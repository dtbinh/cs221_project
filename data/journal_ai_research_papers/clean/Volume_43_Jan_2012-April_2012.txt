Journal of Artificial Intelligence Research 43 (2012) 523-570

Submitted 01/12; published 04/12

Avoiding and Escaping Depressions
in Real-Time Heuristic Search
Carlos Hernandez

chernan@ucsc.cl

Departamento de Ingeniera Informatica
Universidad Catolica de la Santsima Concepcion
Caupolican 491, Concepcion, Chile

Jorge A. Baier

jabaier@ing.puc.cl

Departamento de Ciencia de la Computacion
Pontificia Universidad Catolica de Chile
Vicuna Mackenna 4860, Santiago, Chile

Abstract
Heuristics used for solving hard real-time search problems have regions with depressions.
Such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution. Early real-time search algorithms,
like LRTA , easily become trapped in those regions since the heuristic values of their states
may need to be updated multiple times, which results in costly solutions. State-of-the-art
real-time search algorithms, like LSS-LRTA or LRTA (k), improve LRTA s mechanism
to update the heuristic, resulting in improved performance. Those algorithms, however,
do not guide search towards avoiding depressed regions. This paper presents depression
avoidance, a simple real-time search principle to guide search towards avoiding states that
have been marked as part of a heuristic depression. We propose two ways in which depression avoidance can be implemented: mark-and-avoid and move-to-border. We implement
these strategies on top of LSS-LRTA and RTAA , producing 4 new real-time heuristic
search algorithms: aLSS-LRTA , daLSS-LRTA , aRTAA , and daRTAA . When the objective is to find a single solution by running the real-time search algorithm once, we show
that daLSS-LRTA and daRTAA outperform their predecessors sometimes by one order
of magnitude. Of the four new algorithms, daRTAA produces the best solutions given a
fixed deadline on the average time allowed per planning episode. We prove all our algorithms have good theoretical properties: in finite search spaces, they find a solution if one
exists, and converge to an optimal after a number of trials.

1. Introduction
Many real-world applications require agents to act quickly in a possibly unknown environment. Such is the case, for example, of autonomous robots or vehicles moving quickly
through initially unknown terrain (Koenig, 2001). It is also the case of virtual agents in
games (e.g., Warcraft, Starcraft), in which the time dedicated by the game software to
perform tasks such as path-finding for all virtual agents is very limited. Actually, companies impose limits on the order of 1 millisecond to perform these tasks (Bulitko, Bjornsson,
Sturtevant, & Lawrence, 2011). Therefore, there is usually no time to plan for full trajectories in advance; rather, path-finding has to be carried out in a real-time fashion.
Real-time search (e.g., Korf, 1990; Weiss, 1999; Edelkamp & Schrodl, 2011) is a standard
paradigm for solving search problems in which the environment is not fully known in advance
c
2012
AI Access Foundation. All rights reserved.

fiHernandez & Baier

and agents have to act quickly. Instead of running a computationally expensive procedure
to generate a conditional plan at the outset, real-time algorithms interleave planning and
execution. As such, they usually run a computationally inexpensive lookahead-update-act
cycle, in which search is carried out to select the next move (lookahead phase), then learning
is carried out (update phase), and finally an action is executed which may involve observing
the environment (act phase). Like standard A search (Hart, Nilsson, & Raphael, 1968),
they use a heuristic function to guide action selection. As the environment is unveiled, the
algorithm updates its internal belief about the structure of the search space, updating (i.e.
learning) the heuristic value for some states. The lookahead-update-act cycle is executed
until a solution is found.
Early heuristic real-time algorithms like Learning Real-Time A (LRTA ) and RealTime A (RTA ) (Korf, 1990) are amenable for settings in which the environment is initially
unknown. These algorithms will perform poorly in the presence of heuristic depressions
(Ishida, 1992). Intuitively, a heuristic depression is a bounded region of the search space
in which the heuristic is inaccurate with respect to the heuristic values of the states in
the border of the region. When an agent controlled by LRTA or RTA enters a region of
the search space that conforms a heuristic depression it will usually become trapped. In
order to leave the heuristically depressed region, the agent will need to visit and update
many states in this region, potentially several times. Furthermore, in many applications,
such as games, the behavior of the agent in a depression may look irrational and thus it is
undesirable.
State-of-the-art heuristic real-time search algorithms that are suitable for applications
with initially unknown environments are capable of escaping heuristic depressions more
quickly than LRTA or RTA . They do so by performing more lookahead search, more
learning, or a combination of both. More search involves selecting an action by looking
farther away in the search space. More learning usually involves updating the heuristic of
several states in a single iteration. There are many algorithms that use one or a combination
of these techniques (e.g., Hernandez & Meseguer, 2005; Bulitko & Lee, 2006; Koenig &
Likhachev, 2006b; Hernandez & Meseguer, 2007; Rayner, Davison, Bulitko, Anderson, &
Lu, 2007; Bjornsson, Bulitko, & Sturtevant, 2009; Koenig & Sun, 2009). As a result, these
algorithms perform better than LRTA , spending fewer moves trapped in depressions.
Two algorithms representative of the state of the art in real-time search for initially
unknown environments are LSS-LRTA (Koenig & Sun, 2009) and RTAA (Koenig &
Likhachev, 2006a). These algorithms generalize LRTA by performing more search and
more learning in each episode. Both algorithms have been shown to perform very well in
practice. However, despite the use of more elaborate techniques, they may still perform
poorly in the presence of heuristic depressions. This is because they may sometimes rely on
increasing the heuristic value of states inside the depressions as a mechanism to exit them.
In this paper we study techniques that allow us to improve the performance of real-time
search algorithms by making them explicitly aware of heuristic depressions, and then by
guiding the search in order to avoid and, therefore, escape depressions. Specifically, the
contributions of this paper are as follows.
 We provide new empirical evidence that shows that RTAA outperforms LSS-LRTA
in game map benchmarks in the first trial, which means that whenever there is a
single chance to run one of those real-time heuristic search algorithms to solve a search
524

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

problem, RTAA finds better solutions than LSS-LRTA while making the same search
effort. Before, Koenig and Likhachev (2006b) had shown similar performance results
but in mazes. This is important since LSS-LRTA , and not RTAA , is the algorithm
that has received more attention by the real-time heuristic search community. In this
paper we consider incorporating our techniques to both LSS-LRTA and RTAA .
 We propose a definition for cost-sensitive heuristic depressions, which is a more general
notion than Ishidas (1992) notion of heuristic depression since it incorporates action
costs. We illustrate that our depressions better describe the regions of the search
space in which real-time search algorithms get trapped.
 We propose a simple principle to actively guide search towards avoiding cost-sensitive
heuristic depressions that we call depression avoidance, together with two strategies
to implement depression avoidance which can be incorporated into state-of-the-art
real-time heuristic search algorithms: mark-and-avoid and move-to-border.
 We propose four new real-time search algorithms; two based on mark-and-avoid, aLSSLRTA , aRTAA , and two based on move-to-border: daLSS-LRTA , and daRTAA .
The algorithms are the result of implementing depression avoidance on top of RTAA
and LSS-LRTA .
 We prove that all our algorithms have desirable properties: heuristic consistency is
preserved, they terminate if a solution exists, and they eventually converge to an
optimal solution after running a sufficiently large, finite number of trials.
 We carry out an extensive empirical evaluation of our algorithms over deployed game
benchmarks and mazes. Our evaluation shows that our algorithms outperform existing algorithms in both game maps and mazes. When little time is allowed for the
lookahead phase, two of our algorithms, daLSS-LRTA and daRTAA , outperform
existing ones by an order of magnitude.
Some of the contributions of this paper have been published in conference papers
(Hernandez & Baier, 2011d, 2011c). This article includes new material that has not been
presented before. In particular:
 We describe and evaluate daLSS-LRTA , an algorithm that is presented in this article
for the first time.
 We include full proofs for the termination results (Theorem 6), and a new theoretical
result (Theorem 7) on the convergence of all our algorithms.
 We extend previously published empirical results by including maze benchmarks,
which had not been previously considered, and by including more game domains
and problems.
 Finally, we discuss in detail some scenarios at which our techniques may not perform
particularly good.
525

fiHernandez & Baier

The rest of the paper is organized as follows. In Section 2 we explain basic concepts of
real-time search. We continue presenting LSS-LRTA and RTAA , and we extend the results
available in the literature by comparing them over game maps. We continue elaborating
on the concept of heuristic depression. We then describe our strategies for implementing
depression avoidance and the algorithms that result from applying each of them to LSSLRTA and RTAA . We continue with a detailed theoretical and experimental analysis.
Then, we present a discussion of our approach and evaluation. We finish with a summary.

2. Preliminaries
A search problem P is a tuple (S, A, c, s0 , G), where (S, A) is a digraph that represents the
search space. The set S represents the states and the arcs in A represent all available actions.
A does not contain elements of form (x, x). In addition, the cost function c : A 7 R+
associates a cost to each of the available actions. Finally, s0  S is the start state, and
G  S is a set of goal states. In this paper we assume search spaces are undirected; i.e.,
whenever (u, v) is in A, then so is (v, u). Furthermore, c(u, v) = c(v, u), for all (u, v)  A.
The successors of a state u are defined by Succ(u) = {v | (u, v)  A}. Two states are
neighbors if they are successors of each other.
A heuristic function h : S 7 [0, ) associates to each state s an approximation h(s) of
the cost of a path from s to a goal state. We denote by h (s) the cost of an optimal path
to reach a solution from s.
A heuristic h is consistent if and only if h(g) = 0 for all g  G and h(s)  c(s, s0 ) + h(s0 )
for all states s0  Succ(s). If h is consistent and C(s, s0 ) is the cost of any path between
two states s and s0 , then h(s)  C(s, s0 ) + h(s0 ). Furthermore, if h is consistent it is easy to
prove that it is also admissible; i.e., h(s) underestimates h (s). For more details on these
definitions, we refer the reader to the book authored by Pearl (1984).
We refer to h(s) as the h-value of s and assume familiarity with the A algorithm (Hart
et al., 1968): g(s) denotes the cost of the path from the start state to s, and f (s) is defined
as g(s) + h(s). The f -value and g-value of s refer to f (s) and g(s) respectively.
2.1 Real-Time Search
The objective of a real-time search algorithm is to make an agent travel from an initial
state to a goal state performing, between moves, an amount of computation bounded by a
constant. An example situation is path-finding in a priori unknown grid-like environments.
There the agent has sufficient memory to store its current belief about the structure of
the search space. In addition, the free-space assumption (Zelinsky, 1992; Koenig, Tovey, &
Smirnov, 2003) is taken: the environment is initially assumed as obstacle-free. The agent is
capable of a limited form of sensing: only obstacles in the neighbor states can be detected.
When obstacles are detected, the agent updates its map accordingly.
Many state-of-the-art real-time heuristic search algorithms can be described by the
pseudo-code in Algorithm 1. The algorithm iteratively executes a lookahead-update-act
cycle until the goal is reached. The lookahead phase (Line 46) determines the next state
to move to, the update phase (Line 7) updates the heuristic, and the act phase (Line 8)
moves the agent to its next position. The lookahead-update part of the cycle (Lines 47) is
referred to as the planning episode throughout the paper.
526

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

Algorithm 1: A generic real-time heuristic search algorithm

1
2
3
4
5
6
7
8
9
10

Input: A search problem P , and a heuristic function h.
Side Effect: The agent is moved from the initial state to a goal state if a trajectory exists
h0  h
scurrent  s0
while scurrent 6 G do
LookAhead ()
if Open =  then return no-solution
snext  Extract-Best-State()
Update ()
move the agent from scurrent to snext through the path identified by LookAhead. Stop if
an action cost along the path is updated.
scurrent  current agent position
update action costs (if they have increased)

The generic algorithm has three local variables: scurrent stores the current position of
the agent, c(s, s0 ) contains the cost of moving from state s to a successor s0 , and h is such
that h(s) contains the heuristic value for s. All three variables may change over time. In
path-finding tasks, when the environment is initially unknown, the initial value of c is such
that no obstacles are assumed; i.e., c(s, s0 ) <  for any two neighbor states s, s0 . The initial
value of h(s), for every s, is given as a parameter.
The generic algorithm receives as input a search problem P , and starts off by initializing
some useful variables (Lines 12). In h0 it records the initial value of h, for all states in P ,
and in scurrent it stores the initial position of the agent, s0 . We assume the cost of an arc
cannot decrease. In particular, arc costs increase to infinity when an obstacle is discovered.
In the lookahead phase (Lines 46), the algorithm determines where to proceed next.
The Lookahead() procedure in Line 4 implements a bounded search procedure that expands
states from the current state scurrent . The set of states generated by this call is referred
to as local search space. Different choices can be made to implement this procedure. RealTime A (RTA ) and Learning Real-Time A (LRTA )two early algorithms proposed
by Korf (1990) and other modern real-time search algorithms run a search from the
current state up to a fixed depth (e.g., Bulitko & Lee, 2006). Another common option is
to run a bounded A search; such a choice is taken by Local Search Space LRTA (LSSLRTA ) (Koenig & Sun, 2009), and Real-Time Adaptive A (RTAA ) (Koenig & Likhachev,
2006b). Algorithm 2 shows the pseudo-code for bounded A . Note that at most k states
are expanded, where k is a parameter of the algorithm usually referred to as the lookahead
parameter. The pseudo code of the generic real-time search algorithm assumes that the call
to Lookahead() stores the frontier of the local search space in Open, and, moreover, that
if a goal state is found during search, such a state is not removed from the frontier (in the
bounded A pseudo-code this is guaranteed by the condition in Line 7).
In the last step of the lookahead phase (Line 6, Algorithm 1), the variable containing
the next state to move to, snext , is assigned. Here, most algorithms select the state in the
search frontier that is estimated to be closest to a goal state. When A lookahead is used,
such a state usually corresponds to a state with minimum f -value in Open. Thus A -based
lookahead algorithms use Algorithm 3 to implement the Extract-Best-State() function.

527

fiHernandez & Baier

Algorithm 2: Bounded A lookahead
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

procedure A ()
for each s  S do g(s)  
g(scurrent )  0
Open  
Insert scurrent into Open
expansions  0
while each s0  Open with minimum f -value is such that s0 6 G and expansions < k do
Remove state s with smallest f -value from Open
Insert s into Closed
for each s0  Succ(s) do
if g(s0 ) > g(s) + c(s, s0 ) then
g(s0 )  g(s) + c(s, s0 )
s0 .back = s
if s0  Open then remove s0 from Open
Insert s0 in Open
expansions  expansions + 1

Algorithm 3: Selection of the Best State used by LSS-LRTA , RTAA , and other
algorithms.
1
2

procedure Extract-Best-State ()
return argmins0 Open g(s0 ) + h(s0 )

In the update phase (Line 7, Algorithm 1), the heuristic of some states in the search
space is updated to a value that is a better estimate of the true cost to reach a solution,
while staying consistent. After exploring states in the vicinity of scurrent , the algorithm
gains information about the heuristic value of a number of states. Using this information,
the h-value of scurrent and potentially that of other states in the search spacecan be
updated in such a way that they reflect a better estimation of the cost to reach a solution.
Since after the update the heuristic of some states are updated to a value closer to the true
cost, this phase is also referred to as the learning phase.
The literature describes several ways in which one can implement the update of the
heuristic, e.g., mini-min (e.g., Korf, 1990), max of mins (Bulitko, 2004), and heuristic
bounded propagation (Hernandez & Meseguer, 2005). The learning rules that are most
relevant to this paper, however, are those implemented by LSS-LRTA and RTAA . They
are described in detail in the following subsections.
Finally, after learning, the agent attempts to move to the state selected by the
Extract-Best-State() function, snext . In most implementations, the path to the selected
state has been computed already by the Lookahead() procedure (in the case of Algorithm 2,
the path is reconstructed using the back pointer that is set in Line 13). When the environment is known in advance, the agent can always move to the destination. However, when
the environment is not known in advance, this process can fail (in path-finding, this can
occur due to the discovery of an obstacle). If such an obstacle is found, we assume the
agent stops moving as soon as it has detected an obstacle. In such cases, the algorithm will
528

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

update its memory regarding the environment, which typically involves updating the cost
function. In our pseudo-code, this is reflected in Line 10.

3. LSS-LRTA and RTAA
Now we describe LSS-LRTA and RTAA , the two state-of-the-art real-time heuristic search
algorithms that are most relevant to this paper. We make two small contributions to the understanding of these two algorithms. First, we do an experimental comparison of them over
benchmarks that had not been considered before. Second, we prove two theoretical results
that aim at understanding the differences between their update mechanisms (Propositions 1
and 2). To our knowledge, none of these results appear in the literature.
3.1 LSS-LRTA
Local search space LRTA (LSS-LRTA ) was first introduced by Koenig (2004), and later
presented in detail by Koenig and Sun (2009). It is an instance of Algorithm 1. Its lookahead procedure is a bounded A search (Algorithm 2). The next state to move to corresponds to a state in Open with the lowest f -value; i.e., it uses Algorithm 3 to implement
Extract-Best-State().
LSS-LRTA updates the values of each state s in the local search space in such a way
that h(s) is assigned the maximum possible value that guarantees consistency with the
states in Open. It does so by implementing the Update() procedure as a modified Dijkstras algorithm (Algorithm 4). Since the value of h is raised to the maximum, the update
mechanism of LSS-LRTA makes h as informed as it can get given the current knowledge
about the search space, while maintaining consistency.
Algorithm 4: LSS-LRTA s Modified Dijkstras Procedure. We assume Open list is
a queue ordered by h-value.
1
2
3
4
5
6
7
8
9

procedure ModifiedDijkstra ()
for each state s in Closed do h(s)  
while Closed 6=  do
Extract an s with minimum h-value from Open
if s  Closed then delete s from Closed
for each s0 such that s  Succ(s0 ) do
if s0  Closed and h(s0 ) > c(s0 , s) + h(s) then
h(s0 )  c(s0 , s) + h(s)
if s0 6 Open then Insert s0 in Open

Algorithm 5: RTAA s Update Procedure
1
2
3
4

procedure Update ()
f  minsOpen g(s) + h(s)
for each s  Closed do
h(s)  f  g(s)

529

fiHernandez & Baier

3.2 RTAA
Real-Time Adaptive A (RTAA ) was proposed by Koenig and Likhachev (2006b). It is
an instance of Algorithm 1. Its lookahead phase is identical to that of LSS-LRTA : a
bounded A followed by selecting a state with the lowest f -value in Open as the next state
to move to. However it uses a simpler learning mechanism based on the update rule of
the incremental A search algorithm Adaptive A (Koenig & Likhachev, 2006a). Thus,
it updates the heuristic value of states in the interior of the local search space (i.e., those
stored in A s variable Closed) using the f -value of the best state in Open. The procedure
is shown in Algorithm 5.
RTAA s update procedure is considerably faster in practice than that of LSS-LRTA .
Obtaining the lowest f -value of a state in Open can be done in constant time if A is
implemented with binary heaps. After that, the algorithm simply iterates through the
states in Closed. The worst-case performance is then O(|Closed|). On the other hand,
LSS-LRTA s update procedure first needs to convert Open into a priority queue ordered
by h and then may, in the worst case, need to extract |Open| + |Closed| elements from a
binary heap. In addition, it expands each node that is ever extracted from the priority
queue. The time to complete these operations, in the worst case is Texp  N + Tb  N log N ,
where N = |Open| + |Closed|, Texp is the time taken per expansion, and Tb is a constant
factor associated to extraction from the binary heap. The worst-case asymptotic complexity
of extraction is thus O(N log N ). However, since we usually deal with a small N it may be
the case that the term Texp  N dominates the expression for time.
We will prove that the heuristic values that RTAA learns may be less accurate than
those of LSS-LRTA . To state this formally, we introduce some notation. Let hn , for
n > 0, denote the value of the h variable at the start of iteration n of the main algorithm,
or, equivalently, right after the update phase of iteration n  1. We will also denote the
heuristic function given as input as h0 . Let kn (s, s0 ) denote the cost of an optimal path
from s to s0 that traverses states only in Closed before ending in s0 .
Proposition 1 Let s be a state in Closed right after the call to A has returned in the n-th
iteration of LSS-LRTA . Then,
hn+1 (s) = min kn (s, sb ) + hn (sb ).
sb Open

(1)

Proof: We will show that the value h(s) computed by the modified Dijkstra algorithm for
each state s corresponds to the minimum cost of reaching node s from a certain state in a
particular graph G. The modified Dijkstra procedure can be seen as a run of the standard
Dijkstra algorithm (e.g., Cormen, Leiserson, Rivest, & Stein, 2001) on such a graph.
First we observe that our procedure differs from the standard Dijkstra algorithm in that
a non-singleton set of states, namely those in Open, are initialized with a finite value for h.
In the standard Dijkstra algorithm, on the other hand, only the source node is initialized
with a cumulative cost of 0 whereas the remaining nodes are initialized to . With the
facts above in mind, it is straightforward to see that a run of the modified Dijkstra can be
interpreted as a run of the standard Dijkstra algorithm from node sstart of a directed graph
G that is such that:
 Its nodes are exactly those in Open  Closed plus a distinguished node sstart .
530

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

 It contains an arc (u, v) with cost c if there is an arc (v, u) with cost c in the search
graph of P such that one of v or u is not in Open.
 It contains an arc of the form (sstart , s) with cost h(s) for each s in Open.
 It contains no other arcs.
After running the Dijkstra algorithm from sstart over G, we obtain, for each node s in G
the cost of an optimal path from sstart to s. If we interpret such a cost as h(s), for each s,
Equation 1 holds, which finishes the proof.

For RTAA we can prove a sightly different result.
Proposition 2 Right after the call to A returns in the n-th iteration of RTAA , let s be
the state with lowest f -value in Open, and let s be a state in Closed. Then,
hn+1 (s)  min kn (s, sb ) + hn (sb ).
sb Open

(2)

However, if hn is consistent and s is in the path found by A from scurrent to s , then
hn+1 (s) = min kn (s, sb ) + hn (sb ).
sb Open

(3)

Proof: For (2), we use the fact that if the heuristic is consistent, it remains consistent
after each RTAA iteration (a fact proven by Koenig & Likhachev, 2006a), to write the
inequality hn+1 (s)  minsb Open kn (s, sb ) + hn+1 (sb ). Now note that for every state sb in
Open it holds that hn (s) = hn+1 (s), since the heuristic values of states in Open are not
updated. Substituting hn+1 (s) in the inequality, we obtain the required result.
For (3), we use the a fact proven by Hart et al. (1968) about A : if consistent heuristics
are used, g(s) contains the cost of the cheapest path from the start state to s right after s
is extracted from Open (Line 8 in Algorithm 2).
Because A is run with a consistent heuristic, for any state s0 along the (optimal) path
found by A from scurrent to s ,
g(s0 ) = kn (scurrent , s0 ), and


0

(4)
0



g(s ) = kn (scurrent , s ) + kn (s , s ).

(5)

RTAA s update rule states that:
hn+1 (s0 ) = f (s )  g(s0 ) = hn (s ) + g(s )  g(s0 )

(6)

Substituting with (4) and (5) in (6), we obtain hn+1 (s0 ) = kn (s0 , s ) + hn (s ). Finally,
observe that
kn (s0 , s ) + h(s ) = min kn (s0 , sb ) + hn (sb ).
sb Open

Indeed, if there were an s in Open such that kn (s0 , s ) + h(s ) > kn (s0 , s ) + hn (s ),
then by adding g(s0 ) to both sides of the inequality, we would have that f (s ) > f (s ),
which contradicts the fact that s is the state with lowest f -value in Open. We conclude
henceforth that hn+1 (s0 ) = minsb Open kn (s0 , sb ) + hn (sb ). This finishes the proof.

531

fiHernandez & Baier

Proposition 2 implies that, when using consistent heuristics, RTAA s update may yield
less informed h-values than those of LSS-LRTA . However, at least for some of the states
in the local search space, the final h-values are equal to those of LSS-LRTA , and hence
they are as informed as they can be given the current knowledge about the search space.
Koenig and Likhachev (2006a) show that for a fixed value of the lookahead parameter, the quality of the solutions obtained by LSS-LRTA are better on average than those
obtained by RTAA in path-finding tasks over mazes. This is due to the fact that LSSLRTA s heuristic is more informed over time than that of RTAA . However, they also
showed that given a fixed time deadline per planning episode, RTAA yields better solutions than LSS-LRTA . This is essentially due to the fact that RTAA s update mechanism
is faster: for a fixed deadline, a higher lookahead parameter can be used with RTAA than
with LSS-LRTA .
We extend Koenig and Likhachevs experimental analysis by running a comparison of
the two algorithms on game maps. Table 1 shows average results for LSS-LRTA and
RTAA ran on 12 different game maps. For each map, we generated 500 random test cases.
Observe, for example, that if a deadline of 0.0364 milliseconds is imposed per planning
episode we can choose to run RTAA with a lookahead k = 128, whereas we can choose
to run LSS-LRTA only with lookahead k = 64. With those parameters, RTAA obtains a
solution about 36% cheaper than LSS-LRTA does. Figure 1 shows average solution cost
versus time per episode. The slopes of the curves suggest that the rate at which RTAA
improves solutions is better than that of LSS-LRTA , as more time per episode is given.
In conclusion RTAA seems superior to LSS-LRTA when time is actually important. We
thus confirm for a wider range of tasks that, when time per episode matters, RTAA is
better than LSS-LRTA . These findings are important because mazes (for which previous
evaluations existed) are problems with a very particular structure, and results over them
do not necessarily generalize to other types of problems.
Although we conclude that RTAA is an algorithm superior to LSS-LRTA when it
comes to finding a good solution quickly, it is interesting to note that recent research on
real-time heuristic search is focused mainly on extending or using LSS-LRTA (see e.g.,
Bulitko, Bjornsson, & Lawrence, 2010; Bond, Widger, Ruml, & Sun, 2010; Hernandez &
Baier, 2011d; Sturtevant & Bulitko, 2011), while RTAA is rarely considered. Since LSSLRTA seems to be an algorithm under active study by the community, in this paper we
apply our techniques to both algorithms.

4. Heuristic Depressions
In real-time search problems heuristics usually contain depressions. The identification of
depressions is central to our algorithm. Intuitively, a heuristic depression is a bounded
region of the search space containing states whose heuristic value is too low with respect
to the heuristic values of states in the border of the depression. Depressions exist naturally
in heuristics used along with real-time heuristic search algorithms. As we have seen above,
real-time heuristic algorithms build solutions incrementally, updating the heuristic values
associated to certain states as more information is gathered from the environment.
Ishida (1992) gave a constructive definition for heuristic depressions. The construction
starts with a node s such that its heuristic value is equal to or less than those of the
532

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

RTAA*
LSS-LRTA*
k Avg. Cost Time/ep Exp/ep Per/ep Time Avg. Cost Time/ep Exp/ep Per/ep Time
1 1,146,014 0.0004
1.0
6.1 447.9 1,146,014 0.0012
8.7
14.8 1,259.6
2
919,410 0.0006
2.0
9.4 475.4
625,693 0.0020
13.7
29.3 979.4
4
626,623 0.0011
4.0
17.3 468.8
372,456 0.0034
21.3
54.3 818.1
8
363,109 0.0021
8.0
34.1 383.7
227,526 0.0058
33.8
102.4 653.6
16
188,346 0.0040
16.0
70.1 269.1
127,753 0.0102
56.1
193.5 459.9
32
95,494 0.0078
32.0 152.9 192.8
72,044 0.0187
98.7
397.7 345.3
64
48,268 0.0159
63.9 361.3 145.7
40,359 0.0364 184.9
903.4 279.6
128
25,682 0.0326 126.4 932.3 125.8
22,471 0.0750 370.1 2,338.1 258.2
256
13,962 0.0647 236.8 2,351.8 125.6
12,264 0.1534 733.6 6,003.8 272.2
512
7,704 0.1078 377.6 4,616.7 131.6
7,275 0.2620 1,207.5 11,548.9 312.4

Table 1: Average results for the 12 game maps. For lookahead value k, we report the
solution cost per test case (Avg. Cost), and four measures of efficiency: the runtime
per planning episode (Time/ep) in milliseconds, the number of cell expansions per
planning episode (Exp/ep), the number of heap percolations per planning episode
(Per/ep) and the runtime per test case (Time) in milliseconds. All results were
obtained using a Linux machine with an Intel Xeon CPU running at 2GHz and 12
GB RAM.

Cost vs Time per Episode (Games)

Average Solution Cost (log-scale)

1,000,000

RTAA*
LSS-LRTA*

500,000

100,000

10,000

0

0.05

0.1

0.15

0.2

0.25

0.3

Time per Planning Episode in msec

Figure 1: Average solution cost obtained by LSS-LRTA and RTAA versus planning time
per episode on 12 game maps.

533

fiHernandez & Baier

surrounding states. The region is then extended by adding a state of its border if all states
in the resulting region have a heuristic value lower or equal than those of the states in the
border. As a result, the heuristic depression D is a maximal connected component of states
such that all states in the boundary of D have a heuristic value that is greater than or equal
to the heuristic value of any state in D.
It is known that algorithms like LRTA behave poorly in the presence of heuristic
depressions (Ishida, 1992). To see this, assume that LRTA is run with lookahead depth
equal to 1, such that it only expands the current state, leaving its immediate successors
in the search frontier. Assume further that it visits a state in a depression and that the
solution node lies outside the depression. To exit the depressed region the agent must follow
a path in the interior of the depressed region, say, s1 . . . sn , finally choosing a state in the
border of the region, say se . While visiting sn , the agent chooses se as the next move, which
means that se minimizes the estimated cost to reach a solution among all the neighbors of
sn . In problems with uniform action costs, this can only happen if h(se ) is lower or equal
than the heuristic value of all other neighbors of sn . This fact actually means that the
depression in that region of the search space no longer exists, which can only happen if the
heuristic values of states in the originally depressed region have been updated (increased).
For LRTA , the update process may be quite costly: in the worst case all states in the
depression may need to be updated and each state may need to be updated several times.
Ishidas definition is, nonetheless, restrictive. In fact, it does not take into account the
costs of the actions needed to move from the interior of the depression to the exterior. A
closed region of states may have unrealistically low heuristic values even though the heuristic
values in the interior are greater than the ones in the border. We propose a more intuitive
notion of depression when costs are taken into account. The formal definition follows.
Definition 1 (Cost-sensitive heuristic depression) A connected component of states
D is a cost-sensitive heuristic depression of a heuristic h iff for any state s  D and every
state s0 6 D that is a neighbor of a state in D, h(s) < k(s, s0 ) + h(s0 ), where k(s, s0 ) denotes
the cost of the cheapest path that starts in s, traverses states only in D, and ends in s0 .
Cost-sensitive heuristic depressions better reflect the regions in which an agent controlled
by algorithms such as LRTA get trapped. To illustrate this, consider the two 4-connected
grid-world problems of Figure 2. Gray cells conform an Ishida depression. The union of
yellow and gray cells conform a cost-sensitive heuristic depression. Suppose the agents
initial position is the lower-right corner of the Ishida depression (C4 in Figure 2(a), and
C7 in Figure 2(b)). Assume further that ties are broken such that the priorities, given
from higher to lower, are: down, left, up, and right. For such an initial state, both in
situation (a) and situation (b), the agent controlled by LRTA will visit every state in
the cost-sensitive heuristic depression before reaching the goal. Indeed, cells in the costsensitive depression that are not adjacent to an obstacle are visited exactly 3 times, while
cells adjacent to an obstacle are visited 2 times, before the agent escapes the depression,
and thus the performance of LRTA can be described as a linear function on the size of the
cost-sensitive depression.
It is interesting to note that for problems like the ones shown in Figure 2, the size of
the Ishida depression remains the same while the width of the grid varies. Thus, the size of
534

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

1
A
B
C

2

3

4

5
3

1

6

7

6

5

4

2

6

5

4

3

1

5

4

3

2

0 G

A
B
C

2

3

4

5

6

7

8

10

9

8

7

6

5

4

9

8

7

6

5

4

3

1

8

7

6

5

4

3

2

0 G

(a)

3

9
2

(b)

Figure 2: A 4-connected grid-like search space with unitary costs. Black cells are obstacles.
The cell with a G is the goal cell. Cells show their h-value (Manhattan distance).
Ties are broken by giving priority to the down movement, then left, then up, and
then right. If the initial position of the agent in situation (a) is C4, then the cells
visited by an agent controlled by LRTA are: C3, C2, C1, B1, B2, B3, B4, C4,
C3, B3, C3, C2, B2, C2, C1, B1, C1, B1, B2, B3, B4, A4, A5, A6, B6, and C6.
The solution found for (b) is analogous.

the Ishida depression is not correlated with the performance of LRTA . On the other hand,
the size of the cost-sensitive heuristic depression is a predictor of the cost of the solution

5. Depression Avoidance
A major issue at solving real-time search problems is the presence of heuristic depressions.
State-of-the-art algorithms are able to deal with this problem essentially by doing extensive
learning and/or extensive lookahead. By doing more lookahead, chances are that a state
outside of a depression is eventually selected to move to. On the other hand, by learning
the heuristic values of several states at a time, fewer movements might be needed in order
to raise the heuristic values of states in the interior of a depression high enough as to make
it disappear. As such, LSS-LRTA , run with a high value for the lookahead parameter
exits the depressions more quickly than LRTA run with search depth equal to 1 for two
reasons: (1) because the heuristic function increases for states in D more quickly and (2)
because with a high value for the lookahead parameter it is sometimes possible to escape
the depression in one step.
Besides the already discussed LSS-LRTA and RTAA , there are many algorithms described in the literature capable of doing extensive lookahead and learning. The lookahead
ability of LRTS (Bulitko & Lee, 2006), and TBA (Bjornsson et al., 2009) is parametrized.
By using algorithms such as LRTA (k) (Hernandez & Meseguer, 2005), PLRTA (Rayner
et al., 2007) and LRTALS (k) (Hernandez & Meseguer, 2007) one can increase the number
of states updated based on a parameter. None of these algorithms however are aware of depressions; their design simply allows to escape them because of their ability to do lookahead,
learning, or a combination of both. Later, in Section 9, we give a more detailed overview
of other related work.
To improve search performance our algorithms avoid depressions, a principle we call
depression avoidance. Depression avoidance is a simple principle that dictates that search
535

fiHernandez & Baier

should be guided away from states identified as being in a heuristic depression. There are
many ways in which one could conceive the implementation of this principle in a real-time
heuristic search algorithm. Below we present two alternative realizations of the principle
within the state-of-the-art RTAA and LSS-LRTA algorithms. As a result, we propose
four new real-time search algorithms, each of which has good theoretical properties.
5.1 Depression Avoidance via Mark-and-Avoid
This subsection presents a first possible realization of depression avoidance that we call
mark-and-avoid. With this strategy, we extend the update phase to mark states that we
can prove belong to a heuristic depression. We then modify the selection of the best state
(i.e., the Extract-Best-State() function) to select states that are not marked; i.e., states
that are not yet proven to be part of a depression.
aLSS-LRTA is version of LSS-LRTA that avoids depressions via mark-and-avoid. It is
obtained by implementing the Update() function using Algorithm 6 and by implementing
the Extract-Best() function with Algorithm 7. There are two differences between its
update procedure and LSS-LRTA s. The first is the initialization of the updated flag in
Lines 23. The second is Line 7, which sets s.updated to true if the heuristic value for h
changes as a result of the update process. In the following section, we formally prove that
this means that s was inside a cost-sensitive heuristic depression (Theorem 5).
Algorithm 6: Modified Dijkstra Procedure used by aLSS-LRTA .
1
2
3
4
5
6
7
8
9
10
11
12

procedure ModifiedDijkstra ()
if first run then
for each s  S do s.updated  f alse
/* initialization of update flag */
for each s  Closed do h(s)  
while Closed 6=  do
Extract an s with minimum h-value from Open
if h(s) > h0 (s) then s.updated = true
if s  Closed then delete s from Closed
for each s0 such that s  Succ(s0 ) do
if s0  Closed and h(s0 ) > c(s0 , s) + h(s) then
h(s0 )  c(s0 , s) + h(s)
if s0 6 Open then Insert s0 in Open

To select the next state snext , aLSS-LRTA chooses the state with lowest f -value from
Open that has not been marked as in a depression. If such a state does not exist, the
algorithm selects the state with lowest f -value from Open, just like LSS-LRTA would do.
Depending on the implementation, the worst-case complexity of this new selection mechanism may be different from that of Algorithm 3. Indeed, if the Open list is implemented with
a binary heap (as it is our case), the worst-case complexity of Algorithm 7 is O(N log N )
where N is the size of Open. This is because the heap is ordered by f -value. On the
other hand the worst-case complexity of Algorithm 3 using binary heaps is O(1). In our
experimental results we do not observe, however, a significant degradation in performance
due to this factor.
536

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

Algorithm 7: Selection of the next state used by aLSS-LRTA and aRTAA
1
2
3
4
5
6

function Extract-Best-State ()
if Open contains an s such that s.updated = f alse then
s  argmins0 Opens0 .updated=f alse g(s0 ) + h(s0 )
else
s  argmins0 Open g(s0 ) + h(s0 )
return s ;

Example Figure 3 shows an example that illustrates the difference between LSS-LRTA
and aLSS-LRTA with the lookahead parameter equal to two. After 4 search episodes, we
observe that aLSS-LRTA avoids the depression, leading the agent to a position that is 2
steps closer to the goal than LSS-LRTA .
Algorithm 8: aRTAA s Update Procedure
1
2
3
4
5
6
7

procedure Update ()
if first run then
for each s  S do s.updated  f alse
f  f -value of the best state in Open
for each s  Closed do
h(s)  f  g(s)
if h(s) > h0 (s) then s.updated  true

/* initialization of update flag */

With aLSS-LRTA as a reference, it is straightforward to implement the mark-and-avoid
strategy into RTAA . The update phase of the resulting algorithm, aRTAA , is just like
RTAA s but is extended to mark states in a depression (Algorithm 8). The selection of
the best state to move to is done in the same way as aLSS-LRTA , i.e., with Algorithm
7. As a result aRTAA is a version of RTAA that aims at avoiding depressions using
mark-and-avoid.
5.2 Depression Avoidance via Move-to-Border
Move-to-border is a more finely grained implementation of depression avoidance. To illustrate the differences, consider that, after lookahead, there is no state s in the frontier of the
local search space such that s.updated is false. Intuitively, such is a situation in which the
agent is trapped in a heuristic depression. In this case, aLSS-LRTA behaves exactly as
LRTA does since all states in the search frontier are marked. Nevertheless, in these cases,
we would like the movement of the agent to still be guided away from the depression.
In situations in which all states in the frontier of the local search space are already
proven as members of a depression, the move-to-border strategy attempts to move to a
state that seems closer to the border of a depression. As a next state, this strategy chooses
the state with best f -value among the states whose heuristic has changed the least. The
intuition behind this behavior is as follows: assume (s) is the difference between the actual
cost to reach a solution from a state s and the initial heuristic value of state s. Then, if
s1 is a state close to the border of a depression D and s2 is a state farther away from the
border and deep in the interior of D, then (s2 )  (s1 ), because the heuristic of s2 is
537

fiHernandez & Baier

LSS-LRTA
1

2

3

4

1

A

Iteration 1

D

2

G

3

1

C
D

2

1 5 0
3
2 7 1
5
4

4

3
5
5
6

C 4
G

3

D

4

1

6

2

G

3

4

1 6

B 5

4 1 6
6 5 7
6 2 8
6

2

0 4 1 6

C 4 6 5 7

3

1 6 2 8
6

G

D 5

G

1
6
0
5
1
6
2
5

4

1

A

A

B

B
1 7 2 9
7
0 5 1 7
5 7 6 8

C 6
D

3

A
1
5
0
4
1
5

1

Iteration 4

2

G

B
1 5 0 3
4
3 5
2 7 1 5
5
4 6

A
B

D

A

1

Iteration 3

2 6 1 4
3
1 4 0 2
3 5 2 4

4

B

D

4

C 4

A

C

3

B
2 6 1 4
4
3
1 4 0 2
3 5 2 4

1

Iteration 2

2

A

B
C

aLSS-LRTA

C
D

2

G

3

4

7
5
7
7 2 9
7
7
6

G

Figure 3: First 4 iterations of LSS-LRTA (left) and aLSS-LRTA (right) with lookahead
equal to 2 in a 4-connected grid world with unitary action costs, where the initial
state is D2, and the goal is D4. Numbers in cell corners denote the g-value (upper
left), f -value (upper right), h-value (lower left), and new h-value of an expanded
cell after an update (lower right). Only cells that have been in a closed list show
four numbers. Cells generated but not expanded by A (i.e., in Open) show three
numbers, since their h-values have not been updated. Triangles (N) denote states
with updated flag set to true after the search episode. The heuristic used is the
Manhattan distance. We assume ties are broken by choosing first the right then
bottom then the left and then top adjacent cell. The position of the agent is
given by the dot. A grid cell is shaded (gray) if it is a blocked cell that the agent
has not sensed yet. A grid cell is black if it is a blocked cell that the agent has
already sensed. The best state chosen to move the agent to after lookahead search
is pointed by an arrow.

538

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

aLSS-LRTA
1
A 5

Iteration 1

2

3

4

B

C

C

D

D
E

G

A 5

2

3

4

4 6 5 7 6 8

B

4 6 5 7

C

3 7 4 8

D

A 5

2

3

4 6 5 7

C

3 7 4 8

D

3

4

B
C

2
8
2
7
4
8

G

1

2

3

4

2
2
4 6 5 7 6
2
4 6 5
4
3 7 4

A 5
B
C

2
8
2
7
4
8

D

E

E

G

1
A 5

2

3

4

4 6 5 7 6 8

B

4 8 5 7

C

3 7 4 8

D

G

1

2

3

4

2
2
4 6 5 7 6
4
4 8 5
4
3 7 4

A 5
B
C

2
8
2
7
4
8

D

E

E

G

1

2

3

4

4 6 5 7 6 8

B

4 8 5 9

C

3 7 4 8

D
E

2

2
2
4 6 5 7 6
2
4 6 5
4
3 7 4

A 5

4

4 6 5 7 6 8

B

Iteration 18

4

G

1

E

G

1

A 5

3

D

E

Iteration 17

2
2
4 6 5

A 5

4 6 5

1

Iteration 16

1

B

E

Iteration 15

daLSS-LRTA

G

1
A 5
B
C

2

3

2
2
4 6 5 7 6
4
4 8 5
4
3 7 4

4
2
8
4
9
4
8

D
E

G

G

Figure 4: Iterations 1 and 1518 of aLSS-LRTA (left) and daLSS-LRTA (right) with
lookahead equal to 1 in a 4-connected grid, analogous to our previous example, in
which the objective is the cell E2. In iterations 1 to 14 both algorithms execute in
the same way. Numbers in cells correspond to initial h-value (lower-left), current
h-value (lower-right), and difference between those two amounts (upper-right).
Triangles (N) denote states whose heuristic value has been updated.

539

fiHernandez & Baier

more imprecise than that of s1 . At execution time, h is an estimate of the actual cost to
reach a solution.
daLSS-LRTA and daRTAA differ, respectively, from LSS-LRTA and RTAA in that
the selection of the next state to move to (i.e., function Extract-Best()) is implemented
via Algorithm 9. Note that the worst case complexity of this algorithm is O(N log N ),
where N is the size of Open if binary heaps are used.
Algorithm 9: Selection of the next state by daRTAA and daLSS-LRTA .
1
2
3
4
5
6
7
8

function Extract-Best-State ()
min  
while Open 6=  and min 6= 0 do
Remove state sb with smallest f -value from Open
if h(sb )  h0 (sb ) < min then
s  sb
min  h(sb )  h0 (sb )
return s

Figure 4 illustrates the differences between aLSS-LRTA and daLSS-LRTA . Both algorithms execute in the same way if, after the lookahead phase, there is a state in Open whose
heuristic value has not been updated. However, when this is not the case (i.e., when the
algorithm is trapped in a depression), daLSS-LRTA will move to what seems to be closer
to the border of the depression. In the example of Figure 4, at iteration 15, the algorithm
chooses B4 instead of C3 since B4 is the state for which the h-value has changed the least.
After iteration 18, daLSS-LRTA will move to cells in which less learning has been carried
out and thus will exit the depression more quickly.
All the new algorithms presented in this section are closely related. Table 2 shows a
schematic view of the different components of each algorithm, and the complexity of the
involved algorithms.

6. Theoretical Analysis
In this section we analyze the theoretical properties of the algorithms that we propose.
We prove that all of our algorithms also satisfy desirable properties that hold for their
ancestors. We start off by presenting theoretical results that can be proven using existing
proofs available in the literature; among them, we will show that the consistency of the
heuristic is maintained by all our algorithms during run time. We continue with results that
need different proofs; in particular, termination and convergence to an optimal solution.
As before, we use hn to refer to the value of variable h at the start of iteration n (h0 ,
thus, denotes the heuristic function given as a parameter to the algorithm). Similarly,
cn (s, s0 ) is the cost of the arc between s and s0 . Finally, kn (s, s0 ) denotes the cost of an
optimal path between s and s0 that traverses only nodes in Closed before ending in s0 with
respect to cost function cn .
We first establish that if h is initially consistent, then h is non-decreasing over time.
This is an important property since it means that the heuristic becomes more accurate over
time.

540

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

Algorithm
LSS-LRTA
aLSS-LRTA
daLSS-LRTA
RTAA
aRTAA

daRTAA

Update Phase
Algorithm
Time (heaps)
Modified Dijkstra
(Algorithm 4)
Modified Dijkstra
with Marking (Algorithm 6)
Modified Dijkstra
(Algorithm 4)

O(M log M )

Update
with
best
f -value
(Algorithm 5)
Update
with
best
f -value
plus
marking
(Algorithm 8)
Update
with
best
f -value
(Algorithm 5)

O(N )

O(M log M )
O(M log M )

Next State Selection
Algorithm
Time (heaps)
Best state in Open
(Algorithm 3)
Best
unmarked
state
in
Open
(Algorithm 7)
State in Open that
has changed the
least (Algorithm 9)
Best state in Open
(Algorithm 3)

O (1)
O(L log L)
O(L log L)
O (1)

O(N )

Best
unmarked
state
in
Open
(Algorithm 7)

O(L log L)

O(N )

State in Open that
has changed the
least (Algorithm 9)

O(L log L)

Table 2: Procedures used for the update phase and for the selection of the next state for
each of the algorithms discussed in the paper. Worst-case time complexity for each
procedure is included assuming the Open list is implemented as a binary heap. M
corresponds to |Open| + |Closed|, N is equal to |Closed|, and L is |Open|.

541

fiHernandez & Baier

Theorem 1 If hn is consistent with respect to cost function cn , then hn+1 (s)  hn (s) for
any n along an execution of aLSS-LRTA or daLSS-LRTA .
Proof: Assume the contrary, i.e., that there is a state s such that hn (s) > hn+1 (s). State s
must be in Closed, since those are the only states whose h-value may be updated. As such,
by Proposition 1, we have that hn+1 (s) = kn (s, sb ) + hn (sb ), for some state sb in Open.
However, since hn (s) > hn+1 (s), we conclude that:
hn (s) > kn (s, sb ) + hn (sb ),
which contradicts the fact that hn is consistent. We thus conclude that the h-value of s
cannot decrease.

Theorem 2 If hn is consistent with respect to cost function cn , then hn+1 (s)  hn (s) for
any n along an execution of aRTAA or daRTAA .
Proof: Assume the contrary, i.e., that there is a state s such that hn (s) > hn+1 (s). State
s must be in Closed, since those are the only states whose h-value may be updated. The
update rule will set the value of hn+1 (s) to f (s0 )  g(s) for some s0  Open, i.e.,
hn+1 (s) = f (s0 )  g(s) = g(s0 ) + hn (s0 )  g(s).
But since hn (s) > hn+1 (s), we have that:
hn (s) > g(s0 ) + hn (s0 )  g(s).
Reordering terms, we obtain that:
hn (s) + g(s) > g(s0 ) + hn (s0 ),
which means that the f -value of s is greater than the f -value of s0 . It is known however
that A , run with a consistent heuristic, will expand nodes with non-decreasing f -values.
We conclude, thus, that s0 must have been expanded before s. Since s0 is in Open, then s
cannot be in Closed, which contradicts our initial assumption. We thus conclude that the
h-value of s cannot decrease.

Theorem 3 If hn is consistent with respect to cost function cn , then hn+1 is consistent
with respect to cost function cn+1 along an execution aLSS-LRTA or daLSS-LRTA .
Proof: Since the update procedure used by aLSS-LRTA , daLSS-LRTA and LSS-LRTA
update variable h in exactly the same way, the proof by Koenig and Sun (2009) can be
reused here. However, we provide a rather simpler proof in Section B.1.

Theorem 4 If hn is consistent with respect to cost function cn , then hn+1 is consistent
with respect to cost function cn+1 along an execution aRTAA or daRTAA .
542

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

Proof: Since the update procedure used by aRTAA , daRTAA and RTAA update variable
h in exactly the same way, we can re-use the proof of Theorem 1 by Koenig and Likhachev
(2006b) to establish this result. We provide however a complete proof in Section B.2

The objective of the mark-and-avoid strategy is to stay away from depressions. The
following theorems establish that, indeed, when a state is marked by the aLSS-LRTA or
aRTAA then such a state is in a heuristic depression of the current heuristic.
Theorem 5 Let s be a state such that s.updated switches from false to true between iterations n and n + 1 in an execution of aLSS-LRTA or aRTAA for which h was initially
consistent. Then s is in a cost-sensitive heuristic depression of hn .
Proof: We first prove the result for the case of aLSS-LRTA . The proof for aRTAA is
very similar and can be found in Section B.3.
Let D be the maximal connected component of states connected to s such that:
1. All states in D are in Closed after the call to A in iteration n, and
2. Any state sd in D is such that hn+1 (sd ) > hn (sd ).
Let s0 be a state in the boundary of D. We first show that hn (s0 ) = hn+1 (s0 ). By
definition s0 is either in Closed or Open. If s0  Closed then, since s0 6 D, it must be the
case that s0 does not satisfy condition 2 of the definition of D, and hence hn+1 (s0 )  hn (s0 ).
However, since the heuristic is non-decreasing (Theorems 2 and 1), it must be that hn (s0 ) =
hn+1 (s0 ). On the other hand, if s0 is in Open, its heuristic value is not changed and thus
also hn (s0 ) = hn+1 (s0 ). We have established, hence, that hn (s0 ) = hn+1 (s0 ).
Now we are ready to establish our result: that D is a cost-sensitive heuristic depression
of hn .
Let sd be a state in D. We distinguish two cases.
 Case 1: s0  Closed. Then, by Proposition 1,
hn (s0 ) = kn (s0 , sb ) + hn (sb ),

(7)

for some sb  Open. On the other hand, since the heuristic value has increased
for sd , hn (sd ) < hn+1 (sd ) = mins0b Open kn (sd , s0b ) + h(s0b ); in particular, hn (sd ) <
kn (sd , sb ) + hn (sb ). Since kn (sd , sb ) is the optimal cost to go from sd to sb , kn (sd , sb ) 
kn (sd , s0 ) + kn (s0 , sb ). Substituting kn (sd , sb ) in the previous inequality we have:
hn (sd ) < kn (sd , s0 ) + kn (s0 , sb ) + hn (sb ).

(8)

We now substitute the right-hand side of (8) using (7), and we obtain
hn (sd ) < kn (sd , s0 ) + hn (s0 ).
 Case 2: s0  Open. Because of Proposition 1 we have hn+1 (sd )  kn (sd , s0 ) + hn (s0 ).
Moreover, by definition of D, we have hn+1 (sd ) > hn (sd ). Combining these two
inequalities, we obtain:
hn (sd ) < kn (sd , s0 ) + hn (s0 ).
543

fiHernandez & Baier

In both cases, we proved hn (sd ) < kn (sd , s0 ) + hn (s0 ), for any sd in D and any s0 in the
boundary of D. We conclude D is a cost-sensitive heuristic depression of hn , which finishes
the proof.

Now we turn our attention to termination. We will prove that if a solution exists, then
it will be found by any of our algorithms. To prove such a result, we need two intermediate
lemmas. The first establishes that when the algorithm moves to the best state in Open,
then the h-value of such a state has not changed more than the h-value of the current state.
Formally,
Lemma 1 Let s0 be the state with smallest f -value in Open after the lookahead phase of any
of aLSS-LRTA , daLSS-LRTA , aRTAA , or daRTAA , when initialized with a consistent
heuristic h. Then,
hn+1 (scurrent )  h0 (scurrent )  hn (s0 )  h0 (s0 ).
Proof: Indeed, by Propositions 1 or 2:
hn+1 (scurrent ) = kn (scurrent , s0 ) + hn (s0 )

(9)

Let  be an optimal path found by A connecting scurrent and s0 . Let K0 denote the cost
of this path with respect to cost function c0 . Given that the heuristic h0 is consistent with
respect to the graph with cost function c0 , we have that h0 (scurrent )  K0 + h0 (s0 ) which
can be re-written as:
h0 (scurrent )  K0  h0 (s0 ).
(10)
Adding (10) and (9), we obtain:
hn+1 (scurrent )  h0 (s)  kn (scurrent , s0 )  K0 + hn (s0 )  h0 (s0 ).

(11)

Now, because cn can only increase, the cost of  at iteration n, kn (scurrent , s0 ), is strictly
greater than the cost of  at iteration 0, K0 . In other words, the amount kn (scurrent , s0 )K0
is positive and can be removed from the right-hand side of (11) to produce:
hn+1 (scurrent )  h0 (s)  hn (s0 )  h0 (s0 ),
which is the desired result.



The second intermediate result to prove termination is the following lemma.
Lemma 2 Let n be an iteration of any of aLSS-LRTA , daLSS-LRTA , aRTAA , or
daRTAA , when initialized with a consistent heuristic h. If snext is not set equal to the
state s0 with least f -value in Open, then:
hn (s0 )  h0 (s0 ) > hn (snext )  h0 (snext ).
Proof: Indeed, if aRTAA or aLSS-LRTA are run, this means that snext is such that snext
is not marked as updated, which means that hn (snext ) = h0 (snext ), or equivalently, that
hn (snext )  h0 (snext ) = 0. Moreover, the best state in Open, s0 , was not chosen and hence
544

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

it must be that s0 .updated = true, which means that h(s0 )  h0 (s0 ) > 0. We obtain then
that hn (s0 )  h0 (s0 ) > hn (snext )  h0 (snext ).
The case of daRTAA or daLSS-LRTA is direct by the condition in Line 5 of Algorithm 9. Hence, it is also true that hn (s0 )  h0 (s0 ) > hn (snext )  h0 (snext ).

Now we are ready to prove the main termination result.
Theorem 6 Let P be an undirected finite real-time search problem such that a solution
exists. Let h be a consistent heuristic for P . Then, any of aLSS-LRTA , daLSS-LRTA ,
aRTAA , or daRTAA , used with h, will find a solution for P .
Proof: Let us assume the contrary. There are two cases under which the algorithms do
not return a solution: (a) they return no solution in Line 5 (Algorithm 1), and (b) the
agent traverses an infinite path that never hits a solution node.
For (a) assume any of the algorithms is in state s before the call to A . When it reaches
Line 5 (Algorithm 1), the open list is empty, which means the agent has exhausted the
search space of states reachable from s without finding a solution; this is a contradiction
with the fact that a solution node is reachable from s and the fact that the search problem
is undirected.
For (b) assume that the agent follows an infinite path . Observe that in such an
infinite execution, after some iterationsay, Rthe value of variable c does not increase
anymore. This is because all states around states in  have been observed in the past. As
a consequence, in any iteration after R the agent traverses the complete path identified by
the A lookahead procedure (Line 8 in Algorithm 1).
A second important observation is that, after iteration R, the value of h for the states in
 is finite and cannot increase anymore. Indeed, by Theorems 4 and 3, h remains consistent
and hence admissible, which means that h(s) is bounded by the actual cost to reach a
solution from s, for any s in . Moreover, since c does not change anymore, the call to the
update function will not change the value of h(s), for every s in .
Now we are ready to finish the proof. Consider the algorithm executes past iteration
R. Since the path is infinite and the state space is finite, in some iteration after R the
algorithm decides to go back to a previously visited state. As such, we are going to assume
the agent visits state t0 and selects to move trough states t1 t2    tr1 tr t0    . Since the
heuristic does not change anymore, we simply denote it by h, regardless of the iteration
number. We distinguish two cases.
Case 1 The agent always decides to move to the best state in Open, s0 , and hence
depending on the algorithm that is usedby Proposition 1 or 2, h(s) = k(s, s0 )+h(s0 ),
which implies h(s) > h(s0 ), since action costs are positive. This implies that:
h(t0 ) > h(t1 ) > h(t2 ) > . . . > h(tn ) > h(t0 ),
which is a contradiction; it cannot be the case that h(t0 ) > h(t0 ).
Case 2 At least once, the agent does not move to the best state in Open. Without loss of
generality, we assume this happens only once, for a state ti for some i < r. Let t be
a state with the smallest f -value in Open after the lookahead is carried out from ti .
545

fiHernandez & Baier

By Lemma 1, we can write the following inequalities.
h(t0 )  h0 (t0 )  h(t1 )  h0 (t1 ),
..
.
h(ti1 )  h0 (ti1 )  h(ti )  h0 (ti ),
h(ti )  h0 (ti )  h(t )  h0 (t ),
h(ti+1 )  h0 (ti+1 )  h(ti+2 )  h0 (ti+2 ),
..
.
h(tr )  h0 (tr )  h(t0 )  h0 (t0 ).
Let I be a set containing these inequalities. Now since when in state ti the algorithm
decides to move to ti+1 instead of t , we use Lemma 2 to write:
hn (t )  h0 (t ) > hn (ti+1 )  h0 (ti+1 ).

(12)

The inequalities in I together with (12) entail h(t0 )  h0 (t0 ) > h(t0 )  h0 (t0 ), which
is a contradiction.
In both cases we derive contradictions and hence we conclude the algorithm cannot enter
an infinite loop and thus finds a solution.

We now turn our attention to convergence. The literature often analyzes the properties
of real-time heuristic search when they are run on a sequence of trials (e.g., Shimbo &
Ishida, 2003). Each trial is characterized by running the algorithm from the start state
until the problem is solved. The heuristic function h resulting from trial n is used to feed
the algorithms h variable in trial n + 1.
Before stating the convergence theorem we prove a result related to how h increases
between successive iterations or trials. Indeed, each iteration of our search algorithms
potentially increases h, making it more informed. The following result implies that this
improvement cannot be infinitesimal.
Lemma 3 Let P be a finite undirected search problem, and let Sol be a set of states in P
from which a solution can be reached. Let n be an iteration of any of aLSS-LRTA , daLSSLRTA , aRTAA , or daRTAA . Then hn (s) can only take on a finite number of values, for
every s in P .
Proof: Given Proposition 1, along an execution of any of the algorithms of the LSS-LRTA
family, it is simple to prove by induction on n that:
hn (s) = K + h0 (s000 ),
for any n, where K is sum of the costs of 0 or more arcs in P under cost function cn .
On the other hand, given the update rule of any of the algorithms of the RTAA family
(e.g., Line 6 in Algorithm 8),
hn (s) = K  K 0 + h0 (s000 ),
546

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

for any n, where K and K 0 correspond to the sum of the costs of some arcs in P under cost
function cn .
Since in finite problems there is a finite number of arcs, the quantities referred to by K
and K 0 can only take on a finite number of values. This implies that hn (s), for any s in P ,
can only take on a finite number of values, which concludes the proof.

Below we show that if h converges after a sequence of trials, the solution found with h
is optimal.
Theorem 7 Let P be an undirected finite real-time search problem such that a solution
exists. Let h be a consistent heuristic for P . When initialized with h, a sequence of trials
of any of aLSS-LRTA , daLSS-LRTA , aRTAA , or daRTAA , converges to an optimal
solution.
Proof: First, observe that since the heuristic is admissible, it remains admissible after a
number of trials are run. This is a consequence of Theorems 3 and 4. Hence, for every
state s from which a goal state can be reached, h(s) is bounded from above by the (finite
amount) h (s).
On the other hand, by Lemma 3, the h-values of states from which a solution is reachable
can only increase a finite number of times. After a sequence of trials the value of h thus
converges; i.e., at least for one complete trial, h(s) is not changed, for every s in P . We
can also assume that in such a trial, the value of c does not change either, since once h
converges, the same path of states is always followed and thus no new cost increases are
made.
Let us focus on a run of any of our algorithms in which both h and c do not change.
Observe that this means that hn (s) = h0 (s) for any n (recall h0 is the heuristic given as
input to the algorithm). Independent of the algorithm used, this implies the algorithm
always moves to the best state in Open. Let s1 . . . sm be the sequence of states that were
assigned to snext during the execution (sm is thus a goal state). Observe that since c does
not change along the execution, states s1 . . . sm are actually visited by the agent. Depending
on the algorithm that is used, by Proposition 1 or 2, we know:
h(si ) = k(si , si+1 ) + h(si+1 ),

for all i  {0, . . . , m  1},

(13)

where k(si , si+1 ) is the cost of an optimal path between si and si+1 . Since the heuristic is
consistent
Ph(sm ) = 0, and thus with the family of equations in (13) we conclude h(s0 ) is
equal to m1
i=0 k(si , si+1 ), which corresponds to the cost of the path traversed by the agent.
But we know that h is also admissible, so:
h(s0 ) =

m1
X

k(si , si+1 )  h (s0 ).

i=0

Since h (s0 ) is the cost of an optimal solution, we conclude the path found has an optimal
cost.

547

fiHernandez & Baier

Figure 5: Upper row: the four maze maps used to test our approach; each of 512512 cells.
Lower row: 4 out of the 12 game maps used. The first two come from Dragon
Age: Origins; the remaining 2 are from StarCraft.

7. Empirical Evaluation
We evaluated our algorithms at solving real-time navigation problems in unknown environments. LSS-LRTA and RTAA are used as a baseline for our comparisons. For fairness,
we used comparable implementations that use the same underlying codebase. For example,
all search algorithms use the same implementation for binary heaps as priority queues and
break ties among cells with the same f -values in favor of cells with larger g-values, which
is known to be a good tie-breaking strategy.
We carried out our experiments over two sets of benchmarks: deployed game maps and
mazes. We used twelve maps from deployed video games to carry out the experiments. The
first six are taken from the game Dragon Age, and the remaining six are taken from the game
StarCraft. The maps were retrieved from Nathan Sturtevants pathfinding repository.1 In
addition, we used four maze maps taken from the HOG2 repository.2 They are shown in
Figure 5. All results were obtained using a Linux machine with an Intel Xeon CPU running
at 2GHz and 12 GB RAM.
All maps are regarded as undirected, eight-neighbor grids.
 Horizontal and vertical movements have cost 1, whereas diagonal movements have cost 2. We used the octile distance
(Sturtevant & Buro, 2005) as heuristic.
1. http://www.movingai.com/ and http://hog2.googlecode.com/svn/trunk/maps/. For Dragon
Age we used the maps brc202d, orz702d, orz900d, ost000a, ost000t and ost100d of size 481530, 939718,
656  1491 969  487, 971  487, and 1025  1024 cells respectively. For StarCraft, we used the maps
ArcticStation, Enigma, Inferno JungleSiege, Ramparts and WheelofWar of size 768  768, 768  768,
768  768, 768  768, 512  512 and 768  768 cells respectively.
2. http://hog2.googlecode.com/svn/trunk/maps/

548

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

LSS-LRTA* Variants: Cost vs Time per Episode (Games)
LSS-LRTA*
aLSS-LRTA*

500,000

daLSS-LRTA*

100,000

10,000

0

0.05

0.1

0.15

0.2

0.25

LSS-LRTA* Variants: Cost vs Time per Episode (Mazes)
5,500,000
Average Solution Cost (log-scale)

Average Solution Cost (log-scale)

1,000,000

LSS-LRTA*
aLSS-LRTA*

2,000,000

0.3

Time per Planning Episode in msec

daLSS-LRTA*

500,000

100,000

0

0.05

0.1

0.15

0.2

0.25

Time per Planning Episode in msec

(a)

(b)

Figure 6: Plots showing the average solution cost found by the LSS-LRTA variants versus
average planning time per episode, measured in milliseconds. (a) shows stats on
the game-map benchmarks, and (b) on the mazes benchmarks. Times are shown
in milliseconds. Costs are shown on a log-scale.

For our evaluation we ran all algorithms for 10 different lookahead values. For each map,
we generate 500 test cases. For each test case we choose the start and goal cells randomly.
In the presentation of our results we sometimes use the concept of improvement factor.
When we say that the improvement factor of an algorithm A with respect to B in terms of
average solution cost is n, it means that on average A produces solutions that are n times
cheaper than the ones found by B.
Next we describe the different views of the experimental data that is shown in plots and
tables. We then continue to draw our experimental conclusions.
7.1 An Analysis of the LSS-LRTA Variants
This section analyzes the performance of LSS-LRTA , aLSS-LRTA and daLSS-LRTA .
Figure 6 shows two plots for the average solution costs versus the average planning time
per episode for the three algorithms in games and mazes benchmarks. Planning time per
planning episode is an accurate measure of the effort carried out by each of the algorithms.
Thus these plots illustrate how solution quality varies depending on the effort that each
algorithm carries out.
Regardless of the search effort, we observe aLSS-LRTA slightly but consistently outperforms LSS-LRTA in solution cost. In games benchmarks we observe that for equal
search effort, aLSS-LRTA produces average improvement factors between 1.08 and 1.20 in
terms of solution cost. In mazes, on the other hand, improvement factors are between 1.04
and 1.25. In games, the largest improvements are observed when the lookahead parameter
(and hence the search time per episode) is rather small. Thus aLSS-LRTA s advantage
over LSS-LRTA is more clearly observed when tighter time constraints are imposed on
planning episodes.
549

fiHernandez & Baier

Often times results in real-time search literature are presented in the form of tables,
with search performance statistics reported per each lookahead value. We provide such
tables the appendix of the paper (Tables 5 and 6). An important observation that can be
drawn from the tables is that time per planning episode in LSS-LRTA and aLSS-LRTA
are very similar for a fixed lookahead value; indeed, the time per planning episode of aLSSLRTA is only slightly larger than that of LSS-LRTA . This is interesting since it shows
that the worst-case asymptotic complexity does not seem to be achieved for aLSS-LRTA
(cf. Table 2).
The experimental results show that daLSS-LRTA s more refined mechanism for escaping depressions is better than that of aLSS-LRTA . For any given value of the search effort,
daLSS-LRTA consistently outperforms aLSS-LRTA by a significant margin in solution
cost in games and mazes. daLSS-LRTA also outperforms aLSS-LRTA in total search
time, i.e., the overall time spent searching until a solution is found. Details can be found in
Tables 5 and 6. When the search effort for each algorithm is small, daLSS-LRTA s average
solution quality is substantially better than aLSS-LRTA s; the improvements are actually
close to an order of magnitude.
daLSS-LRTA consistently outperforms LSS-LRTA by a significant margin in total
search time and solution quality, independent of the search effort employed. In terms of
solution cost daLSS-LRTA produces average improvement factors with respect to LSSLRTA between 1.66 and an order of magnitude in the game benchmarks, and produces
average improvement factors between 1.49 and an order of magnitude in the mazes benchmarks. For a fixed lookahead (see Tables 5 and 6 for the specific numbers), the time spent
per planning episode by daLSS-LRTA is larger than time spent per planning episode by
LSS-LRTA because daLSS-LRTA makes more heap percolations than LSS-LRTA . However, for small values of the lookahead parameter, daLSS-LRTA obtains better solutions
using less time per planning episode than LSS-LRTA used with a much larger lookahead.
For example, in game maps, with a lookahead parameter equal to 32, daLSS-LRTA obtains
better solutions than LSS-LRTA with the lookahead parameter equal to 128, requiring, on
average, 2.6 times less time per planning episode. In mazes, with a lookahead parameter
equal to 16, daLSS-LRTA obtains better solutions than LSS-LRTA with the lookahead
parameter equal to 64, requiring, on average, 2.4 times less time per planning episode.
For low values of the lookahead parameter (i.e. very limited search effort) daLSS-LRTA
obtains better solutions in less time per planning episode than aLSS-LRTA used with a
much larger lookahead. For example, in game maps, with a lookahead parameter equal to
1, daLSS-LRTA obtains better solutions than aLSS-LRTA with the lookahead parameter
equal to 16, requiring, on average, 14.1 times less time per planning episode. On the other
hand, in mazes with a lookahead parameter equal to 1, daLSS-LRTA obtains better solutions than aLSS-LRTA with the lookahead parameter equal to 16, requiring, on average,
11.6 times less time per planning episode.
For a fixed lookahead (see Tables 5 and 6), the time taken by daLSS-LRTA per planning
episode is larger than the time taken by aLSS-LRTA per planning episode. This increase
can be explained because, on average, daLSS-LRTA s open list grows larger than that of
aLSS-LRTA . This is due to the fact that, in the benchmarks we tried, daLSS-LRTA
tends to expand cells that have less obstacles around than aLSS-LRTA does. As a result,
550

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

RTAA* Variants: Cost vs Time per Episode (Games)
RTAA*
aRTAA*

500,000

daRTAA*

100,000

10,000
0

0.02

0.04

0.06

0.08

RTAA* Variants: Cost vs Time per Episode (Mazes)
5,500,000
Average Solution Cost (log-scale)

Average Solution Cost (log-scale)

1,000,000

0.1

0.12

Time per Planning Episode in msec

RTAA*
aRTAA*

2,000,000

daRTAA*

500,000

100,000

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
Time per Planning Episode in msec

(a)

(b)

Figure 7: Plots showing the average solution cost found by the RTAA variants versus average planning time per episode. (a) shows stats on the game-maps benchmarks,
and (b) on the mazes benchmarks. Costs are shown on a log-scale.

daLSS-LRTA expands more cells in the learning phase or makes more heap percolations
in the lookahead phase than aLSS-LRTA .
Results show that, among the LSS-LRTA variants, daLSS-LRTA is the algorithm
with the best performance. In fact daLSS-LRTA is clearly superior to LSS-LRTA . Of
the 60,000 runs (12 maps  500 test cases  10 lookahead-values) in game benchmarks,
daLSS-LRTA obtains a better solution quality than LSS-LRTA in 69.9% of the cases, they
tie in 20.9% of the cases, and LSS-LRTA obtains a better-quality solution in only 9.2% of
the cases.
Of the 20,000 (4 maps  500 test cases  10 lookahead-values) runs in mazes benchmarks, daLSS-LRTA obtains a better solution quality than LSS-LRTA in 75.1% of the
cases, they tie in 3.3% of the cases, and LSS-LRTA obtains a better-quality solution in
21.7% of the cases.
7.2 An Analysis of the RTAA Variants
In this section we analyze the relative performance of RTAA , aRTAA , and daRTAA .
Figure 7 shows two plots of the average solution costs versus the average effort carried out
per search episode.
For the same search effort, we do not observe significant improvements of aRTAA over
RTAA . Indeed, only for small values of the average time per search episode does aRTAA
improve the solution quality upon that of RTAA . In general, however, both algorithms
seem to have very similar performance.
On the other hand, the results show that daRTAA s mechanism for escaping depressions
is substantially better than that of aRTAA . For small values for the lookahead parameter (and hence reduced search effort), daRTAA obtains better solutions than the other
variants used with a much larger lookahead. Indeed, for limited search effort, daRTAA is
551

fiHernandez & Baier

approximately an order of magnitude better than the two other algorithms. For example,
in game maps, with a lookahead parameter equal to 1, daRTAA obtains better solutions
than aRTAA with the lookahead parameter equal to 16, requiring, on average, 10.4 times
less time per planning episode.
daRTAA substantially improves RTAA , which is among the best real-time heuristic
search algorithms known to date. In game maps, daRTAA needs only a lookahead parameter of 16 to obtain solutions better than RTAA with the lookahead parameter of 64.
With those values, daRTAA requires about 2.3 times less time per planning episode than
RTAA .
Our results show that daRTAA is the best-performing algorithm of the RTAA family.
Of the 60,000 runs in game-map benchmarks, daRTAA obtains a better solution quality
than RTAA in 71.2% of the cases, they tie in 20.5% of the cases, and RTAA obtains a
better-quality solution in only 8.3% of the cases. Of the 20,000 runs in mazes, daRTAA
obtains a better solution quality than RTAA in 78.0% of the cases, they tie in 2.7% of the
cases, and RTAA obtains a better-quality solution in 19.4% of the cases.
7.3 daLSS-LRTA Versus daRTAA
daRTAA , the best performing algorithm among the RTAA variants, is also superior to
daLSS-LRTA , the best-performing algorithm of the LSS-LRTA variants. Figure 8 shows
average solution costs versus search effort, in game maps and mazes.
As can be seen in the figure, when the lookahead parameter is small (i.e., search effort
is little), the performance of daRTAA and daLSS-LRTA is fairly similar. However, as
more search is allowed per planning episode, daRTAA outperforms daLSS-LRTA . For
example, in games benchmarks, daRTAA , when allowed to spend 0.08 milliseconds per
episode, will obtain solutions comparable to those of daLSS-LRTA but when allowed do
spend 0.18 millisecconds per episode.
Furthermore, the slopes of the curves are significantly more favorable to daRTAA over
daLSS-LRTA . This can be verified in both types of benchmarks and is important since it
speaks to an inherent superiority of the RTAA framework when time per planning episode
is the most relevant factor.
7.4 An Analysis of Disaggregated Data
The performance of real-time algorithms usually varies depending on the map used. To illustrate how the algorithms perform in different maps, Figure 9 shows the improvement on
solution cost of daLSS-LRTA over LSS-LRTA on 4 game and 4 maze benchmarks. They
confirm that improvements can be observed in all domains thus showing that average values
are representative of daLSS-LRTA s behavior in individual benchmarks. Although aLSSLRTA and daLSS-LRTA outperform LSS-LRTA on average, there are specific cases in
which the situation does not hold. Most notably, we observe that in one of the maze benchmarks daLSS-LRTA does not improve significantly with respect to LSS-LRTA for large
values of the lookahead parameter. We discuss this further in the next section. Figure 10
shows also the improvement factors of daRTAA over RTAA . In this plot, the different
algorithms show a similar relative performance in relation to the LSS-LRTA variants.
552

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

Cost vs Time per Episode (Games)

Cost vs Time per Episode (Mazes)
1e+06
Average Solution Cost (log-scale)

Average Solution Cost (log-scale)

daRTAA*
daLSS-LRTA*

100,000

10,000

0

0.05

0.1

0.15

0.2

0.25

daRTAA*
daLSS-LRTA*

100000

0.3

0

0.05

Time per Planning Episode in msec

0.1

0.15

0.2

0.25

Time per Planning Episode in msec

(a)

(b)

Figure 8: Plots showing the average solution cost found by the daRTAA and daLSS-LRTA
versus average planning time per episode. (a) shows stats on the game-maps
benchmarks, and (b) on the mazes benchmarks. Costs are shown on a log-scale.

orz702d

10

maze512-16

orz900d

8
7
6
5

maze512-32

ArticStation

Cost Improvement Factor

Cost Improvement Factor

8

Enigma

7
6
5
4
3

maze512-4
maze512-8

4
3
2

1

2
0.01

0.03

0.06

0.09

0.12

0.15

Time Deadline per Planning Episode

0.01

0.03

0.06

0.09

0.12

0.15

Time Deadline per Planning Episode

Figure 9: Cost improvement factor of daLSS-LRTA over LSS-LRTA , in game maps (left)
and maze benchmarks (right). An improvement factor equal to n indicates that
the solution found by our algorithm is n times cheaper than the one found by the
original algorithm.

553

fiHernandez & Baier

5
orz702d

maze512-16
8

ArticStation

Cost Improvement Factor

Cost Improvement Factor

orz900d
Enigma

4

3

2

maze512-32
maze512-4
maze512-8

6
5
4
3
2
1

0.01

0.03

0.06

0.09

0.12

0.15

Time Deadline per Planning Episode

0.01

0.03

0.06

0.09

0.12

0.15

Time Deadline per Planning Episode

Figure 10: Cost improvement factor of daRTAA over RTAA , in game maps (left) and
maze (right) benchmarks. An improvement factor equal to n indicates that the
solution found by our algorithm is n times cheaper than the one found by the
original algorithm.

7.5 A Worst-Case Experimental Analysis
Although all our algorithms perform a resource-bounded computation per planning episode,
it is hard to tune the lookahead parameter in such a way that both LRTA and daLSSLRTA will incur the same worst-case planning effort. This is because the time spent in
extracting the best state from the open list depends on the structure of the search space
expanded in each lookahead phase.
In this section we set out to carry an experimental worst-case analysis based on a
theoretical worst-case bound. This bound is obtained from the worst-case effort per planning
step as follows. If RTAA performs k expansions per planning episode, then the open list
could contain up to 8k states. This is because each state has at most 8 neighbors. In the
worst case, the effort spent in adding all such states to the open list would be 8k log 8k. On
the other hand, daRTAA would make the same effort to insert those states into the open
list, but would incur an additional cost of 8k log 8k, in the worst-case, to remove all states
from the open list. Therefore, in a worst-case scenario, given a lookahead parameter equal
to k, daRTAA will make double the effort than RTAA makes for the same parameter.
Based on that worst-case estimation, Figure 11 presents the performance of the RTAA
variants, displacing the RTAA curve by a lookahead factor of 2. We conclude that in this
worst-case scenario daRTAA still clearly outperforms RTAA . Gains vary from one order
of magnitude, for low values of the lookahead parameter, to very similar performance when
the lookahead parameter is high.
We remark, however, that we never observed this worst-case in practice. For example,
in our game benchmarks, RTAA , when used with a lookahead parameter 2k spends, on
average 50% more time per planning episode than daRTAA used with lookahead parameter
k.
554

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

0.1

1e+06

0.08

RTAA*(Lookahead * 2)

aRTAA*

aRTAA*

daRTAA*

daRTAA*
Average Cost (log-scale)

Average Time per Planning Episode

RTAA*(Lookahead * 2)

0.06

0.04

0.02

100000

10000
0

0

50

100

150

200

250

300

Lookahead

0

50

100

150

200

250

300

Lookahead

(a)

(b)

Figure 11: Plots showing the average time per planning episode and average solution cost
per lookahead parameter, adjusting the performance of RTAA using a theoretical worst-case bound of 2. As such, for RTAA , the average cost reported for
for a lookahead of k actually corresponds to the cost obtained for a lookahead
2k. Costs are shown on a log-scale.

8. Discussion
There are a number of aspects of our work that deserve a discussion. We focus on two of
them. First, we discuss the setting in which we have evaluated our work, which focused on
showing performance improvements in the first trial for a search in an a priori unknown
domain, without considering other settings. Second, we discuss in which scenarios our
algorithms may not exhibit average performance improvements that were shown in the
previous section.
8.1 The Experimental Setting: Unknown Environments, First Trial
Our algorithm is tailored to solving quickly a search problem in which the environment is
initially unknown. This setting has several applications, including goal-directed navigation
in unknown terrain (Koenig et al., 2003; Bulitko & Lee, 2006). It has also been widely
used to evaluate real-time heuristic search algorithms (e.g., Koenig, 1998; Hernandez &
Meseguer, 2005; Bulitko & Lee, 2006; Hernandez & Meseguer, 2007; Koenig & Sun, 2009).
On the other hand, we did not present an evaluation of our algorithm in environments
that are known a priori. In a previous paper (Hernandez & Baier, 2011d), however, we
showed that aLSS-LRTA obtains similar improvements over LSS-LRTA when the environment is known. However, we omit results on known environments since RTAA and
LSS-LRTA are not representative of the state of the art in those scenarios. Indeed, algorithms like TBA* (Bjornsson et al., 2009) outperform LSS-LRTA significantly. It is not
immediately obvious how to incorporate our techniques to algorithms like TBA*.
We did not present experimental results regarding convergence after several successive
search trials. Recall that in this setting, the agent is teleported to the initial location
555

fiHernandez & Baier

and a new search trial is carried out. Most real-time search algorithmsours includedare
guaranteed to eventually find an optimal solution. Our algorithms do not particularly excel
in this setting. This is because the heuristic value of fewer states is updated, and hence
the heuristic values for states in the search space converges slowly to the correct value. As
such, generally more trials are needed to converge.
Convergence performance is important for problems that are solved offline and in which
real-time approaches may be more adequate for computing an approximation of the optimal
solution. This is the case of the problem of computing an optimal policy in MDPs using
Real-Time Dynamic Programming (Barto, Bradtke, & Singh, 1995). We are not aware,
however, of any application in deterministic search in which searching offline using realtime search would yield better performance than using other suboptimal search algorithms
(e.g., Richter, Thayer, & Ruml, 2010; Thayer, Dionne, & Ruml, 2011). Indeed, Wilt,
Thayer, and Ruml (2010) concluded that real-time algorithms, though applicable, should
not be used for solving shortest path problems unless there is a need for real-time action.
8.2 Bad Performance Scenarios
Although our algorithms clearly outperform its originators LSS-LRTA and RTAA on
average, it is possible to contrive families of increasingly difficult path-finding tasks in
which our algorithms perform worse than their respective predecessors.
Consider for example the 4-connected grid-world scenario of size 7n shown in Figure 12.
The goal of the agent is to reach the state labeled with G, starting from S. Assume
furthermore that to solve this problem we run aRTAA or aLSS-LRTA , with lookahead
parameter equal to 1, and that ties are broken such that the up movement has priority over
the down movement. In the initial state both algorithms will determine that the initial
state (cell E3) is in a heuristic depression and thus will update the heuristic of cell E3. Cell
E3 is now marked as in a depression. Since both cells D3 and F3 have the same heuristic
value and ties are broken in favor of upper cells, the agent is then moved to cell D3. In
later iterations, the algorithm will not prefer to move to cells that have been updated and
therefore the agent will not go back to state E3 unless it is currently in D3 and (at least) C3
is also marked. However, the agent will not go back to D3 quickly. Indeed, it will visit all
states to the right of Wall 1 and Wall 2 before coming back to E3. This happens because,
as the algorithm executes, it will update and mark all visited states, and will never prefer
to go back to a previously marked position unless all current neighbors are also marked.
In the same situation, RTAA and LSS-LRTA , run with lookahead parameter 1 will
behave differently depending on the tie-breaking rules. Indeed, if the priority is given by
up (highest), down, right, and left (lowest), then both RTAA and LSS-LRTA find the
goal fairly quickly as they do not have to visit states to the right of the walls. Indeed,
since the tie-breaking rules prefer a move up, the agent reaches cell A3 after 4 moves,
and then proceeds straight to the goal. In such situations, the performance of aRTAA or
aLSS-LRTA can be made arbitrarily worse than that of RTAA or LSS-LRTA , as n is
increased.
A quite different situation is produced if the tie-breaking follows the priorities given by
up (highest), right, down, and left (lowest). In this case all four algorithms have to visit
the states to the right of both walls. Indeed, once A3 is reached, there is a tie between
556

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

1

2

3

4

5

6

7

8

n2 n1 n
...

B

...
Wall 1

A

D
E
F

G

...
...
S

...

Wall 2

C

...
...

G

Figure 12: A situation in which the relative performance between LSS-LRTA and aLSSLRTA changes depending on the value of n. S is the start state, and G is the
goal. Ties are broken in favor of upper cells.

the h-value of B3 and A4. The agent prefers moving to A4, and from there on it continues
moving to the right of the grid in a zig-zag fashion.
After investigating executions of our da- algorithms in the maze512-4-0 benchmark
(performance is shown in Figures 9 and 10), we believe that the lack of improvement in this
particular benchmark can be explained by the situation just described. This benchmark
is a 512  512 maze in which corridors have a 4-cell width. For low lookahead values, the
number of updates is not high enough to block the corridors. As such, for low values
of the lookahead parameter the increase in performance is still reasonably good. As the
lookahead increases, the algorithm updates more states in one single iteration, and, as a
result, chances are that good paths may become blocked.
Interestingly, however, we do not observe this phenomenon on mazes with wider corridors
or on game maps. A necessary condition to block a corridor that leads to a solution is
that the agent has sufficient knowledge about the borders of the corridor. In mazes with
narrow corridors this may happen with relative ease, as the agent only needs a few moves
to travel between opposite walls. In grids in which corridors are wide however, knowledge
about the existence of obstacles (walls) is hard to obtain by the agent, and, thus, the chances
of updating and blocking, a corridor that leads to a solution are lower.
We believe that it is possible to prove that our algorithms are always better or always
worse for specific search space topologies. We think, nevertheless, that such an analysis
may be hard to carry out, and that its practical significance may be limited. Therefore
we decided to exclude it from the scope of this work. On the other hand, we think that
the impressive performance exhibited by our algorithms in many benchmarks is sufficiently
strong in favor of using our algorithms in domains that do not contain narrow corridors.

9. Related Work
Besides LSS-LRTA and RTAA , there are a number of real-time search algorithms that
can be used in a priori unknown environments. LRTA (k) and LRTALS (k) (Hernandez &
Meseguer, 2005, 2007) are two algorithms competitive with LSS-LRTA that are capable of
557

fiHernandez & Baier

learning the heuristic of several states at the same time; the states for which the heuristic
is learned is independent from those expanded in the lookahead phase. They may escape
heuristic depressions more quickly than LRTA , but its action selection mechanism is not
aware of heuristic depressions. eLSS-LRTA is a preliminary version of aLSS-LRTA we
presented in an extended abstract (Hernandez & Baier, 2011a). It is outperformed by
aLSS-LRTA on average, as it usually becomes too focused on avoiding depressions.
Our algorithms have been designed in order to find good-quality solutions on the first
search trial. Other algorithms described in the literature have been designed with different
objectives in mind. For example, RIBS (Sturtevant, Bulitko, & Bjornsson, 2010) is a realtime algorithm specifically designed to converge quickly to an optimal solution. It will move
the agent as if an iterative-deepening A search was carried out. As such the first solution it
finds is optimal. As a consequence, RIBS potentially requires more time to find one solution
than LSS-LRTA does, but if an optimal solution is required RIBS will likely outperform
LSS-LRTA run to convergence. f -LRTA* (Sturtevant & Bulitko, 2011) is another recent
real-time search algorithm which builds upon ideas introduced by RIBS, in which the gcost of states is learned through successive trials. It has good convergence performance, but
needs to do more computation per planning step than LSS-LRTA .
Incremental A methods, like D* (Stentz, 1995), D*Lite (Koenig & Likhachev, 2002),
Adaptive A* (Koenig & Likhachev, 2006a), and Tree Adaptive A* (Hernandez, Sun, Koenig,
& Meseguer, 2011), are search methods that also allow solving goal-directed navigation
problems in unknown environments. If the first-move delay is required to be short, incremental A* methods cannot be used since they require to compute a complete solution before
starting to move. Real-time search remains the only applicable strategy for this task when
limited time is allowed per planning episode.
Less related to our work are algorithms that abide to real-time search constraints but
that assume the environment is known in advance and that sufficient time is given prior
to solving the problem, allowing preprocessing. Examples are D LRTA (Bulitko, Lustrek,
Schaeffer, Bjornsson, & Sigmundarson, 2008) and kNN-LRTA (Bulitko et al., 2010), tree
subgoaling (Hernandez & Baier, 2011b), or real-time search via compressed path databases
(Botea, 2011).
Finally, the concept of cost-sensitive depression in real-time search could be linked to
other concepts used to describe the poor performance of planning algorithms. For example,
Hoffmann (2005, 2011) analyzed the existence of plateaus in h+ , an effective admissible
domain-independent planning heuristic, and how this negatively affects the performance of
otherwise fast planning algorithms. Cushing, Benton, and Kambhampati (2011) introduced
the concept of -traps that is related to poor performance of best-first search in problems
in which action costs have a high variance. -traps are areas of the search space connected
by actions of least cost. As such, the h-values of states in -traps is not considered in
their analysis. Although we think that the existence of cost-sensitive heuristic depressions
does affect the performance of A , the exact relation between the performance of A and
heuristic depressions does not seem to be obvious.
558

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

10. Summary and Future Work
We have presented a simple principle for guiding real-time search algorithms away from
heuristic depressions. We proposed two alternative approaches for implementing the principle: mark-and-avoid and move-to-border. In the first approach, states that are proven
to be in a depression are marked in the update phase, and then avoided, if possible, when
deciding the next move. In the second approach, the algorithm selects as the next move
the state that seems closer to the border of a depression.
Both approaches can be implemented efficiently. Mark-and-avoid requires very little
overhead, which results in an almost negligible increment in time per planning episode.
Move-to-border, on the other hand, requires more overhead per planning episode, but,
given a time deadline per planning episode, it is able to obtain the best-quality solutions.
Experimentally, we have shown that in goal-directed navigation tasks in unknown terrain, our algorithms outperform their predecessors RTAA and LSS-LRTA . Indeed, the
algorithms based on move-to-borderdaLSS-LRTA and daRTAA are significantly more
efficient than LSS-LRTA and RTAA , especially when the lookahead parameter is a small
value.
The four algorithms proposed have good properties: in undirected, finite search spaces,
they are guaranteed to find a solution if such a solution exists. Moreover, they converge to
an optimal solution after running a number of search trials.
Depression avoidance is a principle applicable to other real-time heuristic search algorithms. Indeed, we think it could be easily incorporated into LRTA (k), LRTALS (k), and
P-LRTA* (Rayner et al., 2007). All those algorithms have specialized mechanisms for updating the heuristic, but the mechanism to select the next state is just like LSS-LRTA s
run with lookahead parameter equal to 1. We think significant improvements could be
achieved if the procedure to select the next movement was changed by daLSS-LRTA s. We
also believe depression avoidance could be incorporated into multi-agent real-time search
algorithms (e.g., Knight, 1993; Yokoo & Kitamura, 1996; Kitamura, Teranishi, & Tatsumi,
1996).

11. Acknowledgments
We thank the JAIR reviewers who provided extensive feedback that helped improve this
article significantly. We also thank the IJCAI-11, SoCS-11, and AIIDE-11 anonymous
reviewers for their thoughtful insights on earlier versions of this work. We are very grateful
to Cristhian Aguilera, who helped out running some of the experiments. Carlos Hernandez
was partly funded a by Fondecyt project #11080063. Jorge Baier was partly funded by the
VRI-38-2010 grant from Pontificia Universidad Catolica de Chile and the Fondecyt grant
number 11110321.

Appendix A. Additional Experimental Data
Tables 36 show average statistics for LSS-LRTA , RTAA , and our 4 algorithms.
559

fiHernandez & Baier

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
5,731,135
4,805,384
3,217,283
1,905,895
1,004,971
513,692
262,760
137,403
71,939
41,089

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
5,165,062
4,038,347
2,746,638
1,504,379
859,669
455,023
239,484
129,765
67,346
38,939

k
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
443,773
804,990
419,616
374,684
257,126
155,573
108,337
75,158
49,065
31,265

RTAA
# Planning
Total
Episodes
Time
5,307,571
2,174
3,885,684
2,410
2,147,060
2,321
954,571
1,912
353,707
1,338
127,555
927
46,777
661
18,787
521
9,012
475
5,973
530
aRTAA
# Planning
Total
Episodes
Time
4,785,257
2,798
3,260,134
2,981
1,832,375
2,829
755,334
2,034
305,372
1,458
114,089
992
43,497
699
18,478
559
9,108
506
6,172
567

daRTAA
# Planning
Total
Episodes
Time
415,327
208
689,014
575
321,418
502
260,163
801
148,616
864
66,818
697
34,119
626
17,686
568
10,370
590
6,954
652

Time per
Episode
0.0004
0.0006
0.0011
0.0020
0.0038
0.0073
0.0141
0.0277
0.0527
0.0888

Exp.
per ep.
1.0
2.0
4.0
8.0
16.0
32.0
63.9
126.3
237.8
397.4

Perc.
per ep.
5.9
9.1
16.8
33.2
67.8
145.6
331.8
816.5
2,016.6
4,101.6

Time
per ep.
0.0006
0.0009
0.0015
0.0027
0.0048
0.0087
0.0161
0.0303
0.0555
0.0918

Exp.
per ep.
1.0
2.0
4.0
8.0
16.0
32.0
63.9
126.3
237.5
399.4

Perc.
per ep.
10.6
18.9
34.0
61.5
113.3
216.0
440.6
988.9
2,272.5
4,394.9

Time
per ep.
0.0005
0.0008
0.0016
0.0031
0.0058
0.0104
0.0183
0.0321
0.0569
0.0937

Exp.
per ep.
1.0
2.0
4.0
8.0
16.0
32.0
63.8
126.2
239.3
408.9

Perc.
per ep.
10.5
27.3
58.5
129.4
261.1
476.7
854.8
1,536.7
2,920.9
5,074.1

Table 3: Average results of RTAA variants over mazes. For a given lookahead parameter value,
we report the average solution cost (Avg. Cost), average number of planning episodes
(# Planning Episodes), total runtime (Total Time), average runtime per search episode
(Time per Episode), average number of expansions per episode (Exp. per ep.), average
number of percolations per planning episode (Perc. per ep.). All times are reported in
milliseconds.

560

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
1,146,014
919,410
626,623
363,109
188,346
95,494
48,268
25,682
13,962
7,704

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
958,795
763,367
516,545
299,786
151,737
81,695
42,883
23,217
12,510
6,892

k
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
109,337
88,947
74,869
62,400
41,145
29,469
18,405
11,924
7,921
5,205

RTAA
# Planning
Total
Episodes
Time
1,058,265
448
747,824
475
422,389
469
184,753
384
67,652
269
24,609
193
9,138
146
3,854
126
1,941
126
1,220
132

aRTAA
# Planning
Total
Episodes
Time
885,506
549
621,438
598
348,785
569
154,037
445
55,706
290
21,533
210
8,357
157
3,631
134
1,845
129
1,178
133
daRTAA
# Planning
Total
Episodes
Time
102,616
53
79,951
66
62,664
102
48,838
165
28,453
199
16,857
229
8,152
196
3,908
158
2,116
149
1,311
145

Time per
Episode
0.0004
0.0006
0.0011
0.0021
0.0040
0.0078
0.0159
0.0326
0.0647
0.1078

Exp.
per ep.
1.0
2.0
4.0
8.0
16.0
32.0
63.9
126.4
236.8
377.6

Perc.
per ep.
6.1
9.4
17.3
34.1
70.1
152.9
361.3
932.3
2,351.8
4,616.7

Time
per ep.
0.0006
0.0010
0.0016
0.0029
0.0052
0.0098
0.0187
0.0368
0.0700
0.1132

Exp.
per ep.
1.0
2.0
4.0
8.0
16.0
32.0
63.9
126.3
235.8
372.7

Perc.
per ep.
11.1
19.9
36.0
66.1
122.5
235.0
485.5
1,114.1
2,586.7
4,826.3

Time
per ep.
0.0005
0.0008
0.0016
0.0034
0.0070
0.0136
0.0241
0.0406
0.0702
0.1107

Exp.
per ep.
1.0
2.0
4.0
8.0
16.0
32.0
63.9
126.4
238.3
385.1

Perc.
per ep.
11.6
29.6
68.0
153.7
327.4
654.9
1,167.4
1,958.9
3,491.5
5,654.4

Table 4: Average results of RTAA variants over game maps. For a given lookahead parameter value, we report the average solution cost (Avg. Cost), average number
of planning episodes (# Planning Episodes), total runtime (Total Time), average
runtime per search episode (Time per Episode), average number of expansions per
episode (Exp. per ep.), average number of percolations per planning episode (Perc.
per ep.). All times are reported in milliseconds.

561

fiHernandez & Baier

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
5,731,135
3,346,675
1,931,251
1,195,330
674,872
391,120
218,303
119,177
64,861
38,182

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
5,165,062
2,561,769
1,670,535
1,027,134
617,302
354,691
205,214
112,288
61,031
36,524

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
443,773
433,576
527,638
317,508
197,066
125,511
85,373
65,009
39,777
28,937

LSS-LRTA
# Planning
Total
Episodes
Time
5,307,571
6,036
2,594,738
4,967
1,247,205
4,009
586,084
3,187
233,400
2,189
96,163
1,613
39,002
1,215
16,649
1,010
8,420
991
5,805
1,143
aLSS-LRTA
# Planning
Total
Episodes
Time
4,785,257
6,174
1,981,509
4,321
1,078,512
3,923
504,696
3,069
213,959
2,217
87,700
1,603
37,106
1,240
16,069
1,028
8,300
1,010
5,879
1,185
daLSS-LRTA
# Planning
Total
Episodes
Time
415,327
357
353,087
603
393,222
1,374
205,868
1,412
100,984
1,293
45,682
1,023
22,725
888
13,772
977
8,201
1,056
6,330
1,310

Time per
Episode
0.0011
0.0019
0.0032
0.0054
0.0094
0.0168
0.0312
0.0607
0.1177
0.1968

Exp.
per ep.
8.5
13.4
20.7
32.9
54.5
95.2
175.6
341.3
655.0
1,079.2

Perc.
per ep.
14.3
28.0
52.1
97.6
182.9
367.4
799.4
1,939.0
4,704.4
8,961.1

Time
per ep.
0.0013
0.0022
0.0036
0.0061
0.0104
0.0183
0.0334
0.0640
0.1217
0.2016

Exp.
per ep.
8.5
13.3
20.7
33.0
54.6
95.8
176.9
344.4
659.1
1,082.0

Perc.
per ep.
19.0
37.7
69.5
126.6
228.3
441.1
918.4
2,134.7
4,997.9
9,283.8

Time
per ep.
0.0009
0.0017
0.0035
0.0069
0.0128
0.0224
0.0391
0.0709
0.1288
0.2070

Exp.
per ep.
6.2
11.7
21.9
40.0
70.7
119.7
209.7
384.7
698.4
1,115.8

Perc.
per ep.
18.9
43.3
96.6
225.8
459.9
816.1
1,477.1
2,936.7
5,972.6
10,136.2

Table 5: Average results of LSS-LRTA variants over mazes. For a given lookahead parameter value,
we report the average solution cost (Avg. Cost), average number of planning episodes (#
Planning Episodes), total runtime (Total Time), average runtime per search episode (Time
per Episode), average number of expansions per episode (Exp. per ep.), average number of
percolations per planning episode (Perc. per ep.). All times are reported in milliseconds.
Results obtained over a Linux PC with a Pentium QuadCore 2.33 GHz CPU and 8 GB
RAM.

562

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
1,146,014
625,693
372,456
227,526
127,753
72,044
40,359
22,471
12,264
7,275

LookAhead
parameter
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
958,795
506,745
313,789
184,632
111,633
66,911
37,215
20,524
11,053
6,460

k
1
2
4
8
16
32
64
128
256
512

Avg.
Cost
109,337
79,417
72,028
51,753
33,351
21,622
13,581
8,693
6,464
4,830

LSS-LRTA
# Planning
Total
Episodes
Time
1,058,265
1,260
488,096
979
242,171
818
113,236
654
45,242
460
18,445
345
7,687
280
3,444
258
1,774
272
1,192
312
aLSS-LRTA
# Planning
Total
Episodes
Time
885,506
1,185
395,546
903
204,478
786
92,594
602
39,857
449
17,271
351
7,217
278
3,234
251
1,677
261
1,137
295
daLSS-LRTA
# Planning
Total
Episodes
Time
102,616
86
69,976
116
58,931
214
38,862
300
20,792
322
10,177
293
4,715
233
2,424
220
1,604
267
1,195
317

Time per
Episode
0.0012
0.0020
0.0034
0.0058
0.0102
0.0187
0.0364
0.0750
0.1534
0.2620

Exp.
per ep.
8.7
13.7
21.3
33.8
56.1
98.7
184.9
370.1
733.6
1,207.5

Perc.
per ep.
14.8
29.3
54.3
102.4
193.5
397.7
903.4
2,338.1
6,003.8
11,548.9

Time
per ep.
0.0013
0.0023
0.0038
0.0065
0.0113
0.0203
0.0386
0.0776
0.1556
0.2592

Exp.
per ep.
8.7
13.7
21.3
34.1
56.6
99.5
186.8
374.5
741.4
1,204.5

Perc.
per ep.
19.8
40.1
73.7
135.6
246.0
479.6
1,036.2
2,553.8
6,339.3
11,823.7

Time
per ep.
0.0008
0.0017
0.0036
0.0077
0.0155
0.0288
0.0494
0.0905
0.1667
0.2651

Exp.
per ep.
6.1
12.1
23.3
44.1
80.3
139.6
236.9
435.4
791.5
1,237.3

Perc.
per ep.
20.8
49.9
118.2
274.0
586.2
1,122.4
1,911.2
3,725.4
7,538.1
12,697.3

Table 6: Average results of LSS-LRTA variants over game maps. For a given lookahead parameter value, we report the average solution cost (Avg. Cost), average number of planning
episodes (# Planning Episodes), total runtime (Total Time), average runtime per search
episode (Time per Episode), average number of expansions per episode (Exp. per ep.), average number of percolations per planning episode (Perc. per ep.). All times are reported
in milliseconds. Results obtained over a Linux PC with a Pentium QuadCore 2.33 GHz
CPU and 8 GB RAM.

563

fiHernandez & Baier

Appendix B. Additional Proofs for Theorems
B.1 Proof of Theorem 3
We establish that, for any pair of neighbor states, s and s0 , hn+1 (s)  cn+1 (s, s0 ) + hn+1 (s0 ).
We divide the rest of the argument in three cases.
Case 1. Both s and s0 are in Closed. Then, by Proposition 1,
hn+1 (s0 ) = kn (s0 , s00 ) + hn (s00 ),

(14)

for some s00  Open. On the other hand, again by Proposition 1,
hn+1 (s) = min kn (s, sb ) + hn (sb ),
sb Open

and thus
hn+1 (s)  kn (s, s00 ) + hn (s00 ),

(15)

since s00 is an element of Open. However, because kn (s, s00 ) is the cost of the shortest path
between s and s00 , we know that
kn (s, s00 )  cn (s, s0 ) + kn (s0 , s00 )

(16)

Adding up (15) and (16), we obtain
hn+1 (s)  cn (s, s0 ) + kn (s0 , s00 ) + hn (s00 )

(17)

Using Equation 14 we substitute kn (s0 , s00 ) + hn (s00 ) in Inequality 17, obtaining:
hn+1 (s)  cn (s, s0 ) + hn (s0 ).

(18)

Since the cost function can only increase, we have that cn (s, s0 )  cn+1 (s, s0 ), and hence:
hn+1 (s)  cn+1 (s, s0 ) + hn (s0 ),

(19)

Finally, since h is non-decreasing (Theorem 1), we have hn (s0 )  hn+1 (s0 ), which allows us
to write
hn+1 (s)  cn+1 (s, s0 ) + hn+1 (s0 ),
(20)
which finishes the proof for this case.
Case 2. One state among s and s0 is in Closed, and the other state is not in Closed.
Without loss of generality, assume s  Closed. Since s0 is not in Closed, it must be in
Open, because s was expanded by A and s0 is a neighbor of s. By Proposition 1 we know:
hn+1 (s) = min kn (s, sb ) + hn (sb ),
sb Open

but since s0 is a particular state in Open, we have:
hn+1 (s)  cn (s, s0 ) + hn (s0 ).
Since cn  cn+1 , we obtain:
hn+1 (s)  cn+1 (s, s0 ) + hn (s0 ),
564

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

which concludes the proof for this case.
Case 3. Both s and s0 are not in Closed. Since hn is consistent:
hn (s)  cn (s, s0 ) + hn (s0 )

(21)

Now we use that the h-value of s and s0 are not updated (hn (s) = hn+1 (s) and hn (s0 ) =
hn+1 (s0 )), and the fact that the cost function increases to write:
hn+1 (s)  cn+1 (s, s0 ) + hn+1 (s0 ),

(22)

which finishes the proof for this case.
In all three cases we proved the desired inequality and therefore we conclude the heuristic
hn+1 is consistent with respect to cost function cn+1 .
B.2 Proof of Theorem 4
We establish that, for any pair of neighbor states, s and s0 , hn+1 (s)  cn+1 (s, s0 ) + hn+1 (s0 ).
We divide the rest of the argument in three cases.
Case 1. Both s and s0 are in Closed. We have that
hn+1 (s) = f (s )  g(s),
0



0

hn+1 (s ) = f (s )  g(s ),

(23)
(24)

for some s in Open. Subtracting (24) from (23), we obtain:
hn+1 (s)  hn+1 (s0 ) = g(s0 )  g(s).

(25)

Since hn is consistent g(s) and g(s0 ) correspond to the cost of the shortest path between
scurrent and, respectively, s and s0 . Thus g(s0 ) = kn (scurrent , s0 ) and g(s) = kn (scurrent , s),
and therefore:
hn+1 (s)  hn+1 (s0 ) = kn (scurrent , s0 )  kn (scurrent , s).
(26)
Let us consider a path from scurrent to s0 that goes optimally to s, and then goes from s to
s0 . The cost of such a path must be at least kn (scurrent , s0 ). In other words:
kn (scurrent , s0 )  kn (scurrent , s) + cn (s, s0 ),
which directly implies:
kn (scurrent , s0 )  kn (scurrent , s)  cn (s, s0 ).

(27)

Now we combine (27) and (26) to obtain:
hn+1 (s)  cn (s, s0 ) + hn+1 (s0 ).

(28)

And, finally, since cn  cn+1 we conclude that:
hn+1 (s)  cn+1 (s, s0 ) + hn+1 (s0 ),
which finishes the proof for this case.
565

(29)

fiHernandez & Baier

Case 2. One state among s and s0 is in Closed, and the other state is not in Closed.
Without loss of generality, assume s  Closed. Since s0 is not in Closed, it must be in
Open, because s was expanded by A and s0 is a neighbor of s.
For some state s in Open, we have that
hn+1 (s) = f (s )  g(s)

(30)

Again we use the fact that, with the consistent heuristic hn , A expands nodes with increasing f -values. Note that s is the state that would have been expanded next by A ,
and that s0 would have been expanded later on. Moreover, as soon as s0 would have been
expanded the g-value for s0 is the optimal cost of the path from scurrent to s0 , kn (scurrent , s0 ).
Therefore, we can write:
f (s )  kn (scurrent , s0 ) + hn (s0 ),
(31)
as kn (scurrent , s0 ) + hn (s0 ) is the f -value of s0 upon expansion. Adding up (30) and (31), we
obtain:
hn+1 (s)  kn (scurrent , s0 )  g(s) + hn (s0 )
However, since s is in Closed, g(s) is the cost of an optimal path from scurrent to s, and
thus:
hn+1 (s)  kn (scurrent , s0 )  kn (scurrent , s) + hn (s0 )
(32)
We use now the same argument of the previous case to conclude that:
kn (scurrent , s0 )  kn (scurrent , s)  cn (s, s0 ).

(33)

Combining (31) and (33) we obtain:
hn+1 (s)  cn (s, s0 ) + hn (s0 )

(34)

Since s0 is not in closed, hn+1 (s0 ) = hn (s). Furthermore, we know that cn  cn+1 . Substituting in (34), we obtain:
hn+1 (s)  cn+1 (s, s0 ) + hn+1 (s0 ),

(35)

which allows us to conclude the proof for this case.
Case 3. Both s and s0 are not in Closed. The proof is the same as that for Case 3 in
Theorem 3.
In all three cases we proved the desired inequality and therefore we conclude the heuristic
hn+1 is consistent with respect to cost function cn+1 .
B.3 An Appendix for the Proof of Theorem 5
This section describes the proof of Theorem 5 for the specific case of aRTAA .
Let D be the maximal connected component of states connected to s such that (1) all
states in D are in Closed after the call to A in iteration n, and (2) any state sd in D is
such that hn+1 (sd ) > hn (sd ). We prove that D is a cost-sensitive heuristic depression of hn .
Let s0 be a state in the boundary of D; as argued for the case of aLSS-LRTA , we can
show that hn (s0 ) = hn+1 (s0 ). Now, let sd be a state in D. We continue the proof by showing
566

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

that hn (sd ) is too low with respect to hn (s0 ), which means that D is a heuristic depression
of hn . For this final part of the proof, we distinguish two cases: (Case 1) s0  Closed, and
(Case 2) s0  Open.
For Case 1, given that hn+1 (s0 ) = hn (s0 ), we know hn (s0 ) = f   g(s0 ), where f  is the
lowest f -value in the open list after the algorithm is run, and hence:
f  = hn (s0 ) + g(s0 )

(36)

On the other hand, since by definition of D the heuristic value has increased for sd ,
hn (sd ) < hn+1 (sd ) = f   g(sd ).

(37)

Substituting f  in Eq. 37 with the right-hand-side of Eq. 36, we get:
hn (sd ) < hn (s0 ) + g(s0 )  g(sd ).

(38)

Because the heuristic is consistent and both s0 and sd are in Closed, g(s0 ) and g(sd ) actually
correspond to the cost of the cheapest path to reach, respectively, s0 and sd from s; i.e.,
g(s0 ) = k(s, s0 ) and g(sd ) = kn (s, sd ). In addition, the triangular inequality kn (s, sd ) +
kn (sd , s0 )  kn (s, s0 ), can be re-written as:
g(s0 )  g(sd )  kn (sd , s0 ).

(39)

Inequalities 38 and 39 imply hn (sd ) < kn (sd , s0 ) + hn (s0 ).
Finally, for Case 2, if s0  Open, by Proposition 2 and the fact that hn+1 (sd ) > hn (sd ),
we also have that hn (sd ) < kn (sd , s0 ) + hn (s0 ).
In both cases, we proved hn (sd ) < kn (sd , s0 ) + hn (s0 ), for any sd in D and any s0 in the
boundary of D. We conclude D is a cost-sensitive heuristic depression of hn .

References
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72 (1-2), 81138.
Bjornsson, Y., Bulitko, V., & Sturtevant, N. R. (2009). TBA*: Time-bounded A*. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),
pp. 431436.
Bond, D. M., Widger, N. A., Ruml, W., & Sun, X. (2010). Real-time search in dynamic
worlds. In Proceedings of the 3rd Symposium on Combinatorial Search (SoCS), Atlanta, Georgia.
Botea, A. (2011). Ultra-fast Optimal Pathfinding without Runtime Search. In Proceedings
of the 7th Annual International AIIDE Conference (AIIDE), Palo Alto, California.
Bulitko, V., & Lee, G. (2006). Learning in real time search: a unifying framework. Journal
of Artificial Intelligence Research, 25, 119157.
Bulitko, V. (2004). Learning for adaptive real-time search. Computing Research Repository,
cs.AI/0407016.
567

fiHernandez & Baier

Bulitko, V., Bjornsson, Y., & Lawrence, R. (2010). Case-based subgoaling in real-time
heuristic search for video game pathfinding. Journal of Artificial Intelligence Research,
38, 268300.
Bulitko, V., Bjornsson, Y., Sturtevant, N., & Lawrence, R. (2011). Real-time Heuristic
Search for Game Pathfinding. Applied Research in Artificial Intelligence for Computer
Games. Springer.
Bulitko, V., Lustrek, M., Schaeffer, J., Bjornsson, Y., & Sigmundarson, S. (2008). Dynamic
control in real-time heuristic search. Journal of Artificial Intelligence Research, 32,
419452.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to Algorithms, Second Edition. The MIT Press and McGraw-Hill Book Company.
Cushing, W., Benton, J., & Kambhampati, S. (2011). Cost based satisficing search considered harmful. CoRR, abs/1103.3687.
Edelkamp, S., & Schrodl, S. (2011). Heuristic Search: Theory and Applications. Morgan
Kaufmann.
Hart, P. E., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimal cost paths. IEEE Transactions on Systems Science and Cybernetics, 4 (2).
Hernandez, C., & Meseguer, P. (2005). LRTA*(k). In Proceedings of the 19th International
Joint Conference on Artificial Intelligence (IJCAI), pp. 12381243.
Hernandez, C., & Meseguer, P. (2007). Improving LRTA*(k). In Proceedings of the 20th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 23122317.
Hernandez, C., & Baier, J. A. (2011a). Escaping heuristic depressions in real-time heuristic
search (extended abstract). In Proceedings of the 10th International Joint Conference
on Autonomous Agents and Multi Agent Systems (AAMAS), Taipei, Taiwan.
Hernandez, C., & Baier, J. A. (2011b). Fast subgoaling for pathfinding via real-time
search. In Proceedings of the 21th International Conference on Automated Planning
and Scheduling (ICAPS), Freiburg, Germany.
Hernandez, C., & Baier, J. A. (2011c). Real-time adaptive A* with depression avoidance. In
Proceedings of the 7th Annual International AIIDE Conference (AIIDE), Palo Alto,
California.
Hernandez, C., & Baier, J. A. (2011d). Real-time heuristic search with depression avoidance.
In Proceedings of the 22nd International Joint Conference on Artificial Intelligence
(IJCAI), Barcelona, Spain.
Hernandez, C., Sun, X., Koenig, S., & Meseguer, P. (2011). Tree adaptive A*. In Proceedings
of the 10th International Joint Conference on Autonomous Agents and Multi Agent
Systems (AAMAS), Taipei, Taiwan.
Hoffmann, J. (2011). Where ignoring delete lists works, part II: Causal graphs. In Proceedings of the 21th International Conference on Automated Planning and Scheduling
(ICAPS), pp. 98105.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
568

fiAvoiding and Escaping Depressions in Real-Time Heuristic Search

Ishida, T. (1992). Moving target search with intelligence. In Proceedings of the 10th National
Conference on Artificial Intelligence (AAAI), pp. 525532.
Kitamura, Y., Teranishi, K.-i., & Tatsumi, S. (1996). Organizational strategies for multiagent real-time search. In Proceedings of the 2nd International Conference on Multiagent Systems (ICMAS), pp. 150156, Kyoto, Japan.
Knight, K. (1993). Are many reactive agents better than a few deliberative ones?. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI),
pp. 432437.
Koenig, S. (1998). Exploring unknown environments with real-time search or reinforcement
learning. In Proceedings of the 11th Conference on Advances in Neural Information
Processing Systems (NIPS), pp. 10031009.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22 (4), 109131.
Koenig, S. (2004). A comparison of fast search methods for real-time situated agents. In
Proceedings of the 3rd International Joint Conference on Autonomous Agents and
Multi Agent Systems (AAMAS), pp. 864871.
Koenig, S., & Likhachev, M. (2002). D* lite. In Proceedings of the 18th National Conference
on Artificial Intelligence (AAAI), pp. 476483.
Koenig, S., & Likhachev, M. (2006a). A new principle for incremental heuristic search:
Theoretical results. In Proceedings of the 16th International Conference on Automated
Planning and Scheduling (ICAPS), pp. 402405, Lake District, UK.
Koenig, S., & Likhachev, M. (2006b). Real-time adaptive A*. In Proceedings of the 5th
International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS), pp. 281288.
Koenig, S., & Sun, X. (2009). Comparing real-time and incremental heuristic search for
real-time situated agents. Autonomous Agents and Multi-Agent Systems, 18 (3), 313
341.
Koenig, S., Tovey, C. A., & Smirnov, Y. V. (2003). Performance bounds for planning in
unknown terrain. Artificial Intelligence, 147 (1-2), 253279.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42 (2-3), 189211.
Pearl, J. (1984). Heuristics: preintelligent search strategies for computer problem solving.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic
search with a priority queue. In Proceedings of the 20th International Joint Conference
on Artificial Intelligence (IJCAI), pp. 23722377.
Richter, S., Thayer, J. T., & Ruml, W. (2010). The joy of forgetting: Faster anytime search
via restarting. In Proceedings of the 20th International Conference on Automated
Planning and Scheduling (ICAPS), pp. 137144.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic
search. Artificial Intelligence, 146 (1), 141.
569

fiHernandez & Baier

Stentz, A. (1995). The focussed D* algorithm for real-time replanning. In Proceedings of the
14th International Joint Conference on Artificial Intelligence (IJCAI), pp. 16521659.
Sturtevant, N. R., & Bulitko, V. (2011). Learning where you are going and from whence
you came: h- and g-cost learning in real-time heuristic search. In Proceedings of the
22nd International Joint Conference on Artificial Intelligence (IJCAI), pp. 365370,
Barcelona, Spain.
Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). On learning in agent-centered search.
In Proceedings of the 9th International Joint Conference on Autonomous Agents and
Multi Agent Systems (AAMAS), pp. 333340, Toronto, Ontario.
Sturtevant, N. R., & Buro, M. (2005). Partial pathfinding using map abstraction and
refinement. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI), pp. 13921397.
Thayer, J. T., Dionne, A. J., & Ruml, W. (2011). Learning inadmissible heuristics during
search. In Proceedings of the 21th International Conference on Automated Planning
and Scheduling (ICAPS), pp. 250257, Freinurg, Germany.
Weiss, G. (Ed.). (1999). Multiagent Systems: A Modern Approach to Distributed Artificial
Intelligence. MIT Press, Cambridge, MA.
Wilt, C. M., Thayer, J. T., & Ruml, W. (2010). A comparison of greedy search algorithms.
In Proceedings of the 3rd Symposium on Combinatorial Search (SoCS).
Yokoo, M., & Kitamura, Y. (1996). Multiagent real-time A* with selection: Introducing
competition in cooperative search. In Proceedings of the 2nd International Conference
on Multiagent Systems (ICMAS), pp. 409416, Kyoto, Japan.
Zelinsky, A. (1992). A mobile robot exploration algorithm. IEEE Transactions on Robotics
and Automation, 8 (6), 707717.

570

fiJournal of Artificial Intelligence Research 43 (2012) 661-704

Submitted 09/11; published 04/12

Learning to Win by Reading Manuals
in a Monte-Carlo Framework
S.R.K. Branavan

branavan@csail.mit.edu

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology

David Silver

d.silver@cs.ucl.ac.uk

Department of Computer Science
University College London

Regina Barzilay

regina@csail.mit.edu

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology

Abstract
Domain knowledge is crucial for effective performance in autonomous control systems.
Typically, human effort is required to encode this knowledge into a control algorithm. In
this paper, we present an approach to language grounding which automatically interprets
text in the context of a complex control application, such as a game, and uses domain
knowledge extracted from the text to improve control performance. Both text analysis and
control strategies are learned jointly using only a feedback signal inherent to the application.
To effectively leverage textual information, our method automatically extracts the text
segment most relevant to the current game state, and labels it with a task-centric predicate
structure. This labeled text is then used to bias an action selection policy for the game,
guiding it towards promising regions of the action space. We encode our model for text
analysis and game playing in a multi-layer neural network, representing linguistic decisions
via latent variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters using
feedback from simulated games. We apply our approach to the complex strategy game
Civilization II using the official game manual as the text guide. Our results show that a
linguistically-informed game-playing agent significantly outperforms its language-unaware
counterpart, yielding a 34% absolute improvement and winning over 65% of games when
playing against the built-in AI of Civilization.

1. Introduction
In this paper, we study the task of grounding document content in control applications
such as computer games. In these applications, an agent attempts to optimize a utility
function (e.g., game score) by learning to select situation-appropriate actions. In complex
domains, finding a winning strategy is challenging even for humans. Therefore, human
players typically rely on manuals and guides that describe promising tactics and provide
general advice about the underlying task. Surprisingly, such textual information has never
been utilized in control algorithms despite its potential to greatly improve performance.
Our goal, therefore, is to develop methods that can achieve this in an automatic fashion.
c
2012
AI Access Foundation. All rights reserved.

fiBranavan, Silver, & Barzilay

The natural resources available where a population settles aects its ability to produce food
and goods. Cities built on or near water sources can irrigate to increase their crop yields, and
cities near mineral resources can mine for raw materials. Build your city on a plains or grassland
square with a river running through it if possible.

Figure 1: An excerpt from the user manual of the game Civilization II.
We explore this question in the context of strategy games, a challenging class of large scale
adversarial planning problems.
Consider for instance the text shown in Figure 1. This is an excerpt from the user
manual of the game Civilization II.1 This text describes game locations where the action
build-city can be effectively applied. A stochastic player that does not have access to this
text would have to gain this knowledge the hard way: it would repeatedly attempt this
action in a myriad of states, thereby learning the characterization of promising state-action
pairs based on observed game outcomes. In games with large state spaces, long planning
horizons, and high-branching factors, this approach can be prohibitively slow and ineffective.
An algorithm with access to the text, however, could learn correlations between words in
the text and game attributes  e.g., the word river and places with rivers in the game 
thus leveraging strategies described in text to select better actions.
To improve the performance of control applications using domain knowledge automatically extracted from text, we need to address the following challenges:
 Grounding Text in the State-Action Space of a Control Application Text
guides provide a wealth of information about effective control strategies, including
situation-specific advice as well as general background knowledge. To benefit from
this information, an algorithm has to learn the mapping between the text of the
guide, and the states and actions of the control application. This mapping allows
the algorithm to find state-specific advice by matching state attributes to their verbal
descriptions. Furthermore, once a relevant sentence is found, the mapping biases the
algorithm to select the action proposed in the guide document. While this mapping
can be modeled at the word-level, ideally we would also use information encoded in
the structure of the sentence  such as the predicate argument structure. For instance,
the algorithm can explicitly identify predicates and state attribute descriptions, and
map them directly to structures inherent in the control application.
 Annotation-free Parameter Estimation While the above text analysis tasks relate
to well-known methods in information extraction, prior work has primarily focused on
supervised methods. In our setup, text analysis is state dependent, therefore annotations need to be representative of the entire state space. Given an enormous state
space that continually changes as the game progresses, collecting such annotations is
impractical. Instead, we propose to learn text analysis based on a feedback signal
inherent to the control application, e.g., the game score. This feedback is computed
automatically at each step of the game, thereby allowing the algorithm to continuously
adapt to the local, observed game context.
1. http://en.wikipedia.org/wiki/Civilization II

662

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

 Effective Integration of Extracted Text Information into the Control Application Most text guides do not provide complete, step-by-step advice for all situations that a player may encounter. Even when such advice is available, the learned
mapping may be noisy, resulting in suboptimal choices. Therefore, we need to design a method which can achieve effective control in the absence of textual advice,
while robustly integrating automatically extracted information when available. We
address this challenge by incorporating language analysis into Monte-Carlo Search, a
state-of-the-art framework for playing complex games. Traditionally this framework
operates only over state and action features. By extending Monte-Carlo search to
include textual features, we integrate these two sources of information in a principled
fashion.
1.1 Summary of the Approach
We address the above challenges in a unified framework based on Markov Decision Processes
(MDP), a formulation commonly used for game playing algorithms. This setup consists of
a game in a stochastic environment, where the goal of the player is to maximize a given
utility function R(s) at state s. The players behavior is determined by an action-value
function Q(s, a) that assesses the goodness of action a at state s based on the attributes of
s and a.
To incorporate linguistic information into the MDP formulation, we expand the action
value function to include linguistic features. While state and action features are known
at each point of computation, relevant words and their semantic roles are not observed.
Therefore, we model text relevance as a hidden variable. Similarly, we use hidden variables
to discriminate the words that describe actions and those that describe state attributes from
the rest of the sentence. To incorporate these hidden variables in our action-value function,
we model Q(s, a) as a non-linear function approximation using a multi-layer neural network.
Despite the added complexity, all the parameters of our non-linear model can be effectively learned in the Monte-Carlo Search framework. In Monte-Carlo Search, the actionvalue function is estimated by playing multiple simulated games starting at the current
game state. We use the observed reward from these simulations to update the parameters
of our neural network via backpropagation. This focuses learning on the current game state,
allowing our method to learn language analysis and game-play appropriate to the observed
game context.
1.2 Evaluation
We test our method on the strategy game Civilization II, a notoriously challenging game
with an immense action space.2 As a source of knowledge for guiding our model, we use the
official game manual. As a baseline, we employ a similar Monte-Carlo search based player
which does not have access to textual information. We demonstrate that the linguisticallyinformed player significantly outperforms the baseline in terms of the number of games
won. Moreover, we show that modeling the deeper linguistic structure of sentences further improves performance. In full-length games, our algorithm yields a 34% improve2. Civilization II was #3 in IGNs 2007 list of top video games of all time.
(http://top100.ign.com/2007/ign top game 3.html)

663

fiBranavan, Silver, & Barzilay

ment over a language unaware baseline and wins over 65% of games against the built-in,
hand-crafted AI of Civilization II. A video of our method playing the game is available at
http://groups.csail.mit.edu/rbg/code/civ/video. The code and data for this work, along
with a complete experimental setup and a preconfigured environment in a virtual machine
are available at http://groups.csail.mit.edu/rbg/code/civ.
1.3 Roadmap
In Section 2, we provide intuition about the benefits of integrating textual information into
learning algorithms for control. Section 3 describes prior work on language grounding, emphasizing the unique challenges and opportunities of our setup. This section also positions
our work in a large body of research on Monte-Carlo based players. Section 4 presents
background on Monte-Carlo Search as applied to game playing. In Section 5 we present a
multi-layer neural network formulation for the action-value function that combines information from the text and the control application. Next, we present a Monte-Carlo method
for estimating the parameters of this non-linear function. Sections 6 and 7 focus on the application of our algorithm to the game Civilization II. In Section 8 we compare our method
against a range of competitive game-playing baselines, and empirically analyze the properties of the algorithm. Finally, in Section 9 we discuss the implications of this research, and
conclude.

2. Learning Game Play from Text
In this section, we provide an intuitive explanation of how textual information can help improve action selection in a complex game. For clarity, we first discuss the benefits of textual
information in the supervised scenario, thereby decoupling questions concerning modeling
and representation from those related to parameter estimation. Assume that every state s
is represented by a set of n features [s1 , s2 , . . . , sn ]. Given a state s, our goal is to select the
best possible action aj from a fixed set A. We can model this task as multiclass classification, where each choice aj is represented by a feature vector [(s1 , aj ), (s2 , aj ), . . . , (sn , aj )].
Here, (si , aj ), i  [1, n] represents a feature created by taking the Cartesian product between [s1 , s2 , . . . , sn ] and aj . To learn this classifier effectively, we need a training set that
sufficiently covers the possible combinations of state features and actions. However, in domains with complex state spaces and a large number of possible actions, many instances of
state-action feature values will be unobserved in training.
Now we show how the generalization power of the classifier can be improved using textual information. Assume that each training example, in addition to a state-action pair,
contains a sentence that may describe the action to be taken given the state attributes. Intuitively, we want to enrich our basic classifier with features that capture the correspondence
between states and actions, and words that describe them. Given a sentence w composed
of word types w1 , w2 , . . . , wm , these features can be of the form (si , wk ) and (aj , wk ) for
every i  [1, n], k  [1, m] and aj  A. Assuming that an action is described using similar words throughout the guide, we expect that a text-enriched classifier would be able to
learn this correspondence via the features (aj , wk ). A similar intuition holds for learning
the correspondence between state-attributes and their descriptions represented by features
(si , wk ). Through these features, the classifier can connect state s and action aj based
664

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

on the evidence provided in the guiding sentence and their occurrences in other contexts
throughout the training data. A text-free classifier may not support such an association if
the action does not appear in a similar state context in a training set.
The benefits of textual information extend to models that are trained using control
feedback rather than supervised data. In this training scenario, the algorithm assesses the
goodness of a given state-action combination by simulating a limited number of game turns
after the action is taken and observing the control feedback provided by the underlying
application. The algorithm has a built-in mechanism (see Section 4) that employs the
observed feedback to learn feature weights, and intelligently samples the space in search for
promising state-action pairs. When the algorithm has access to a collection of sentences, a
similar feedback-based mechanism can be used to find sentences that match a given stateaction pair (Section 5.1). Through the state- and action-description features (si , wk ) and
(aj , wk ), the algorithm jointly learns to identify relevant sentences and to map actions and
states to their descriptions. Note that while we have used classification as the basis of
discussion in this section, in reality our methods will learn a regression function.

3. Related Work
In this section, we first discuss prior work in the field of grounded language acquisition.
Subsequently we look are two areas specific to our application domain  i.e., natural language
analysis in the context of games, and Monte-Carlo Search applied to game playing.
3.1 Grounded Language Acquisition
Our work fits into the broad area of research on grounded language acquisition where
the goal is to learn linguistic analysis from a non-linguistic situated context (Oates, 2001;
Barnard & Forsyth, 2001; Siskind, 2001; Roy & Pentland, 2002; Yu & Ballard, 2004; Chen
& Mooney, 2008; Zettlemoyer & Collins, 2009; Liang, Jordan, & Klein, 2009; Branavan,
Chen, Zettlemoyer, & Barzilay, 2009; Branavan, Zettlemoyer, & Barzilay, 2010; Vogel & Jurafsky, 2010; Clarke, Goldwasser, Chang, & Roth, 2010; Tellex, Kollar, Dickerson, Walter,
Banerjee, Teller, & Roy, 2011; Chen & Mooney, 2011; Liang, Jordan, & Klein, 2011; Goldwasser, Reichart, Clarke, & Roth, 2011). The appeal of this formulation lies in reducing
the need for manual annotations, as the non-linguistic signals can provide a powerful, albeit
noisy, source of supervision for learning. In a traditional grounding setup it is assumed that
the non-linguistic signals are parallel in content to the input text, motivating a machine
translation view of the grounding task. An alternative approach models grounding in the
control framework where the learner actively acquires feedback from the non-linguistic environment and uses it to drive language interpretation. Below we summarize both approaches,
emphasizing the similarity and differences with our work.
3.1.1 Learning Grounding from Parallel Data
In many applications, linguistic content is tightly linked to perceptual observations, providing a rich source of information for learning language grounding. Examples of such parallel
data include images with captions (Barnard & Forsyth, 2001), Robocup game events paired
with a text commentary (Chen & Mooney, 2008), and sequences of robot motor actions de665

fiBranavan, Silver, & Barzilay

scribed in natural language (Tellex et al., 2011). The large diversity in the properties of such
parallel data has resulted in the development of algorithms tailored for specific grounding
contexts, instead of an application-independent grounding approach. Nevertheless, existing
grounding approaches can be characterized along several dimensions that illuminate the
connection between these algorithms:
 Representation of Non-Linguistic Input The first step in grounding words in
perceptual data is to discretize the non-linguistic signal (e.g., an image) into a representation that facilitates alignment. For instance, Barnard and Forsyth (2001) segment images into regions that are subsequently mapped to words. Other approaches
intertwine alignment and segmentation into a single step (Roy & Pentland, 2002), as
the two tasks are clearly interrelated. In our application, segmentation is not required
as the state-action representation is by nature discrete.
Many approaches move beyond discretization, aiming to induce rich hierarchical structures over the non-linguistic input (Fleischman & Roy, 2005; Chen & Mooney, 2008,
2011). For instance, Fleischman and Roy (2005) parse action sequences using a
context-free grammar which is subsequently mapped into semantic frames. Chen
and Mooney (2008) represent action sequences using first order logic. In contrast,
our algorithm capitalizes on the structure readily available in our data  state-action
transitions. While inducing a richer structure on the state-action space may benefit mapping, it is a difficult problem in its own right from the field of hierarchical
planning (Barto & Mahadevan, 2003).
 Representation of Linguistic Input Early grounding approaches used the bagof-words approach to represent input documents (Yu & Ballard, 2004; Barnard &
Forsyth, 2001; Fleischman & Roy, 2005). More recent methods have relied on a richer
representation of linguistic data, such as syntactic trees (Chen & Mooney, 2008) and
semantic templates (Tellex et al., 2011). Our method incorporates linguistic information at multiple levels, using a feature-based representation that encodes both words
as well as syntactic information extracted from dependency trees. As shown by our
results, richer linguistic representations can significantly improve model performance.
 Alignment Another common feature of existing grounding models is that the training
procedure crucially depends on how well words are aligned to non-linguistic structures.
For this reason, some models assume that alignment is provided as part of the training
data (Fleischman & Roy, 2005; Tellex et al., 2011). In other grounding algorithms, the
alignment is induced as part of the training procedure. Examples of such approaches
are the methods of Barnard and Forsyth (2001), and Liang et al. (2009). Both of these
models jointly generate the text and attributes of the grounding context, treating
alignment as an unobserved variable.
In contrast, we do not explicitly model alignment in our model due to the lack of
parallel data. Instead, we aim to extract relevant information from text and infuse it
into a control application.
666

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

3.1.2 Learning Grounding from Control Feedback
More recent work has moved away from the reliance on parallel corpora, using control feedback as the primary source of supervision. The assumption behind this setup is that when
textual information is used to drive a control application, the applications performance will
correlate with the quality of language analysis. It is also assumed that the performance measurement can be obtained automatically. This setup is conducive to reinforcement learning
approaches which can estimate model parameters from the feedback signal, even it is noisy
and delayed.
One line of prior work has focused on the task of mapping textual instructions into a
policy for the control application, assuming that text fully specifies all the actions to be executed in the environment. For example, in our previous work (Branavan et al., 2009, 2010),
this approach was applied to the task of translating instructions from a computer manual
to executable GUI actions. Vogel and Jurafsky (2010) demonstrate that this grounding
framework can effectively map navigational directions to the corresponding path in a map.
A second line of prior work has focused on full semantic parsing  converting a given text
into a formal meaning representation such as first order logic (Clarke et al., 2010). These
methods have been applied to domains where the correctness of the output can be accurately evaluated based on control feedback  for example, where the output is a database
query which when executed provides a clean, oracle feedback signal for learning. This line
of work also assumes that the text fully specifies the required output.
While our method is also driven by control feedback, our language interpretation task
itself is fundamentally different. We assume that the given text document provides highlevel advice without directly describing the correct actions for every potential game state.
Furthermore, the textual advice does not necessarily translate to a single strategy  in fact,
the text may describe several strategies, each contingent on specific game states. For this
reason, the strategy text cannot simply be interpreted directly into a policy. Therefore, our
goal is to bias a learned policy using information extracted from text. To this end, we do
not aim to achieve a complete semantic interpretation, but rather use a partial text analysis
to compute features relevant for the control application.
3.2 Language Analysis and Games
Even though games can provide a rich domain for situated text analysis, there have only
been a few prior attempts at leveraging this opportunity (Gorniak & Roy, 2005; Eisenstein,
Clarke, Goldwasser, & Roth, 2009).
Eisenstein et al. (2009) aim to automatically extract information from a collection of
documents to help identify the rules of a game. This information, represented as predicate logic formulae, is estimated in an unsupervised fashion via a generative model. The
extracted formulae, along with observed traces of game play are subsequently fed to an Inductive Logic Program, which attempts to reconstruct the rules of the game. While at the
high-level, our goal is similar, i.e., to extract information from text useful for an external
task, there are several key differences. Firstly, while Eisenstein et al. (2009) analyze the
text and the game as two disjoint steps, we model both tasks in an integrated fashion. This
allows our model to learn a text analysis pertinent to game play, while at the same time
using text to guide game play. Secondly, our method learns both text analysis and game
667

fiBranavan, Silver, & Barzilay

play from a feedback signal inherent to the game, avoiding the need for pre-compiled game
traces. This enables our method to operate effectively in complex games where collecting a
sufficiently representative set of game traces can be impractical.
Gorniak and Roy (2005) develop a machine controlled game character which responds
to spoken natural language commands. Given traces of game actions manually annotated
with transcribed speech, their method learns a structured representation of the text and
aligned action sequences. This learned model is then used to interpret spoken instructions
by grounding them in the actions of a human player and the current game state. While the
method itself does not learn to play the game, it enables human control of an additional game
character via speech. In contrast to Gorniak and Roy (2005), we aim to develop algorithms
to fully and autonomously control all actions of one player in the game. Furthermore, our
method operates on the games user manual rather than on human provided, contextually
relevant instructions. This requires our model to identify if the text contains information
useful in the current game state, in addition to mapping the text to productive actions.
Finally, our method learns from game feedback collected via active interaction without
relying on manual annotations. This allows us to effectively operate on complex games
where collecting traditional labeled traces would be prohibitively expensive.
3.3 Monte-Carlo Search for Game AI
Monte-Carlo Search (MCS) is a state-of-the-art framework that has been very successfully
applied, in prior work, to playing complex games such as Go, Poker, Scrabble, and real-time
strategy games (Gelly, Wang, Munos, & Teytaud, 2006; Tesauro & Galperin, 1996; Billings,
Castillo, Schaeffer, & Szafron, 1999; Sheppard, 2002; Schafer, 2008; Sturtevant, 2008; Balla
& Fern, 2009). This framework operates by playing simulated games to estimate the goodness or value of different candidate actions. When the games state and action spaces are
complex, the number of simulations needed for effective play become prohibitively large.
Previous application of MCS have addressed this issue using two orthogonal techniques: (1)
they leverage domain knowledge to either guide or prune action selection, (2) they estimate
the value of untried actions based on the observed outcomes of simulated games. This estimate is then used to bias action selection. Our MCS based algorithm for games relies on
both of the above techniques. Below we describe the differences between our application of
these techniques and prior work.
3.3.1 Leveraging Domain Knowledge
Domain knowledge has been shown to be critically important to achieving good performance from MCS in complex games. In prior work this has been achieved by manually
encoding relevant domain knowledge into the game playing algorithm  for example, via
manually specified heuristics for action selection (Billings et al., 1999; Gelly et al., 2006),
hand crafted features (Tesauro & Galperin, 1996), and value functions encoding expert
knowledge (Sturtevant, 2008). In contrast to such approaches, our goal is to automatically extract and use domain knowledge from relevant natural language documents, thus
bypassing the need for manual specification. Our method learns both text interpretation
and game action selection based on the outcomes of simulated games in MCS. This allows it
to identify and leverage textual domain knowledge relevant to the observed game context.
668

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Action selection
according to
policy function

Stochastic state
transition according
to distribution

Figure 2: Markov Decision Process. Actions are selected according to policy function (s, a)
given the current state s. The execution of the selected action ai (e.g., a1 ), causes
the MDP to transition to a new state s0 according to the stochastic state transition
distribution T (s0 | s, a).

3.3.2 Estimating the Value of Untried Actions
Previous approaches to estimating the value of untried actions have relied on two techniques.
The first, Upper Confidence bounds for Tree (UCT) is a heuristic used in concert with the
Monte-Carlo Tree Search variant of MCS. It augments an actions value with an exploration
bonus for rarely visited state-action pairs, resulting in better action selection and better
overall game performance (Gelly et al., 2006; Sturtevant, 2008; Balla & Fern, 2009). The
second technique is to learn a linear function approximation of action values for the current
state s, based on game feedback (Tesauro & Galperin, 1996; Silver, Sutton, & Muller, 2008).
Even though our method follows the latter approach, we model action-value Q(s, a) via a
non-linear function approximation. Given the complexity of our application domain, this
non-linear approximation generalizes better than a linear one, and as shown by our results
significantly improves performance. More importantly, the non-linear model enables our
method to represent text analysis as latent variables, allowing it to use textual information
to estimate the value of untried actions.

4. Monte-Carlo Search
Our task is to leverage textual information to help us win a turn-based strategy game against
a given opponent. In this section, we first describe the Monte-Carlo Search framework
within which our method operates. The details of our linguistically informed Monte-Carlo
Search algorithm are given in Section 5.
4.1 Game Representation
Formally, we represent the given turn-based stochastic game as a Markov Decision Process
(MDP). This MDP is defined by the 4-tuple hS, A, T, Ri, where
 State space, S, is the set of all possible states. Each state s  S represents a complete
configuration of the game in-between player turns.
 Action space, A, is the set of all possible actions. In a turn-based strategy game, a
player controls multiple game units at each turn. Thus, each action a  A represents
the joint assignment of all unit actions executed by the current player during the turn.
669

fiBranavan, Silver, & Barzilay

 Transition distribution, T (s0 | s, a), is the probability that executing action a in state
s will result in state s0 at the next game turn. This distribution encodes the way the
game state changes due to both the game rules, and the opposing players actions.
For this reason, T (s0 | s, a) is stochastic  as shown in Figure 2, executing the same
action a at a given state s can result in different outcomes s0 .
 Reward function, R(s)  R, is the immediate reward received when transitioning to
state s. The value of the reward correlates with the goodness of actions executed up
to now, with higher reward indicating better actions.
All the above aspects of the MDP representation of the game  i.e., S, A, T () and R()
 are defined implicitly by the game rules. At each step of the game, the game-playing
agent can observe the current game state s, and has to select the best possible action a.
When the agent executes action a, the game state changes according to the state transition
distribution. While T (s0 | s, a) is not known a priori, state transitions can be sampled from
this distribution by invoking the game code as a black-box simulator  i.e., by playing the
game. After each action, the agent receives a reward according to the reward function R(s).
In a game playing setup, the value of this reward is an indication of the chances of winning
the game from state s. Crucially, the reward signal may be delayed  i.e., R(s) may have a
non-zero value only for game ending states such as a win, a loss, or a tie.
The game playing agent selects actions according to a stochastic policy (s, a), which
specifies the probability of selecting action a in state s. The expected total reward after
executing action a in state s, and then following policy  is termed the action-value function
Q (s, a). Our goal is to find the optimal policy   (s, a) which maximizes the expected
total reward  i.e., maximizes the chances of winning the game. If the optimal action-value

function Q (s, a) is known, the optimal game-playing behavior would be to select the action

a with the highest Q (s, a). While it may be computationally hard to find an optimal policy

  (s, a) or Q (s, a), many well studied algorithms are available for estimating an effective
approximation (Sutton & Barto, 1998).
4.2 Monte-Carlo Framework for Computer Games
The Monte-Carlo Search algorithm, shown in Figure 3, is a simulation-based search paradigm
for dynamically estimating the action-values Q (s, a) for a given state st (see Algorithm 1
for pseudo code). This estimate is based on the rewards observed during multiple roll-outs,
each of which is a simulated game starting from state st .3 Specifically, in each roll-out,
the algorithm starts at state st , and repeatedly selects and executes actions according to a
simulation policy (s, a), sampling state transitions from T (s0 | s, a). On game completion
at time  , the final reward R(s ) is measured, and the action-value function is updated
accordingly.4 As in Monte-Carlo control (Sutton & Barto, 1998), the updated action-value
3. Monte-Carlo Search assumes that it is possible to play simulated games. These simulations may be
played against a heuristic AI player. In our experiments, the built-in AI of the game is used as the
opponent.
4. In general, roll-outs are run until game completion. If simulations are expensive, as is the case in our
domain, roll-outs can be truncated after a fixed number of steps. This however depends on the availability
of an approximate reward signal at the truncation point. In our experiments, we use the built-in score
of the game as the reward. This reward is noisy, but available at every stage of the game.

670

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Game

Copy game
state to
simulator

Apply action with
best simulation
outcome to game
Single
simulation
rollout

Update rollout
policy from
game feedback
after each rollout

Simulation for

Simulation for

Figure 3: Overview of Monte-Carlo Search algorithm. For each game state st , an independent set of simulated games or roll-outs are done to find the best possible game
action at . Each roll-out starts at state st , with actions selected according to a
simulation policy (s, a). This policy is learned from the roll-outs themselves 
with the roll-outs improving the policy, which in turn improves roll-out action selection. The process is repeated for every actual game state, with the simulation
policy being relearned from scratch each time.

function Q (s, a) is used to define an improved simulation policy, thereby directing subsequent roll-outs towards higher scoring regions of the game state space. After a fixed number
of roll-outs have been performed, the action with the highest average final reward in the
simulations is selected and played in the actual game state st . This process is repeated
for each state encountered during the actual game, with the action-value function being
relearned from scratch for each new game state.5 The simulation policy usually selects
actions to maximize the action-value function. However, sometimes other valid actions are
also randomly explored in case they are more valuable than predicted by the current es5. While it is conceivable that sharing the action-value function across the roll-outs of different game states
would be beneficial, this was empirically not the case in our experiments. One possible reason is that
in our domain, the game dynamics change radically at many points during the game  e.g., when a
new technology becomes available. When such a change occurs, it may actually be detrimental to play
according to the action-value function from the previous game step. Note however, that the action-value
function is indeed shared across the roll-outs for a single game state st , with parameters updated by
successive roll-outs. This is how the learned model helps improve roll-out action selection, and thereby
improves game play. The setup of relearning from scratch for each game state has been shown to be
beneficial even in stationary environments (Sutton, Koop, & Silver, 2007).

671

fiBranavan, Silver, & Barzilay

timate of Q (s, a). As the accuracy of Q (s, a) improves, the quality of action selection
improves and vice versa, in a cycle of continual improvement (Sutton & Barto, 1998).
The success of Monte-Carlo Search depends on its ability to make a fast, local estimate
of the action-value function from roll-outs collected via simulated play. However in games
with large branching factors, it may not be feasible to collect sufficient roll-outs, especially
when game simulation is computationally expensive. Thus it is crucial that the learned
action-value function generalizes well from a small number of roll-outs  i.e., observed
states, actions and rewards. One way to achieve this is to model the action-value function
as a linear combination of state and action attributes:
Q (s, a) = w
~  f~(s, a).
Here f~(s, a)  Rn is a real-valued feature function, and w
~ is a weight vector. Prior work has
shown such linear value function approximations to be effective in the Monte-Carlo Search
framework (Silver et al., 2008).
Note that learning the action-value function Q(s, a) in Monte-Carlo Search is related
to Reinforcement Learning (RL) (Sutton & Barto, 1998). In fact, in our approach, we use
standard gradient descent updates from RL to estimate the parameters of Q(s, a). There is,
however, one crucial difference between these two techniques: In general, the goal in RL is
to find a Q(s, a) applicable to any state the agent may observe during its existence. In the
Monte-Carlo Search framework, the aim is to learn a Q(s, a) specialized to the current state
s. In essence, Q(s, a) is relearned for every observed state in the actual game, using the
states, actions and feedback from simulations. While such relearning may seem suboptimal,
it has two distinct advantages: first, since Q(s, a) only needs to model the current state, it
can be representationally much simpler than a global action-value function. Second, due to
this simpler representation, it can be learned from fewer observations than a global actionvalue function (Sutton et al., 2007). Both of these properties are important when the state
space is extremely large, as is the case in our domain.

5. Adding Linguistic Knowledge to the Monte-Carlo Framework
The goal of our work is to improve the performance of the Monte-Carlo Search framework
described above, using information automatically extracted from text. In this section, we
describe how we achieve this in terms of model structure and parameter estimation.
5.1 Model Structure
To achieve our aim of leveraging textual information to improve game-play, our method
needs to perform three tasks: (1) identify sentences relevant to the current game state, (2)
label sentences with a predicate structure, and (3) predict good game actions by combining
game features with text features extracted via the language analysis steps. We first describe
how each of these tasks can be modeled separately before showing how we integrate them
into a single coherent model.
672

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

procedure PlayGame ()
Initialize game state to fixed starting state
s1  s0
for t = 1 . . . T do
Run N simulated games
for i = 1 . . . N do
(ai , ri )  SimulateGame (st )
end
Compute average observed utility for each action
1 X
at  arg max
ri
Na
a
i:ai =a

Execute selected action in game
st+1  T (s0 | st , at )
end

procedure SimulateGame (st )
for u = t . . .  do
Compute Q function approximation
Q (su , a) = w
~  f~(su , a)
Sample action from action-value function in -greedy fashion:

 uniform (a  A)
with probability 
au  (su , a) =

 arg max Q (su , a) otherwise
a

Execute selected action in game:
su+1  T (s0 | su , au )
if game is won or lost
break
end
Update parameters w
~ of Q (st , a)
Return action and observed utility:
return at , R(s )
Algorithm 1: The general Monte-Carlo algorithm.
673

fiBranavan, Silver, & Barzilay

5.1.1 Modeling Sentence Relevance
As discussed in Section 1, only a small fraction of a strategy document is likely to provide
guidance relevant to the current game context. Therefore, to effectively use information
from a given document d, we first need to identify the sentence yi that is most relevant to
the current game state s and action a.6 We model this decision as a log-linear distribution,
defining the probability of yi being the relevant sentence as:
~

p(y = yi |s, a, d)  e~u(yi ,s,a,d) .

(1)

~ i , s, a, d)  Rn is a feature function, and ~u are the parameters we need to estimate.
Here (y
~
The function ()
encodes features that combine the attributes of sentence yi with the
attributes of the game state and action. These features allow the model to learn correlations
between game attributes and the attributes of relevant sentences.
5.1.2 Modeling Predicate Structure
When using text to guide action selection, in addition to using word-level correspondences,
we would also like to leverage information encoded in the structure of the sentence. For
example, verbs in a sentence might be more likely to describe suggested game actions.
We aim to access this information by inducing a task-centric predicate structure on the
sentences. That is, we label the words of a sentence as either action-description, statedescription or background. Given sentence y and its precomputed dependency parse q, we
model the word-by-word labeling decision in a log-linear fashion  i.e., the distribution over
the predicate labeling z of sentence y is given by:
p(z |y, q) = p(~e |y, q)
Y
=
p(ej |j, y, q),

(2)

j
~

p(ej |j, y, q)  e~v(ej ,j,y,q) ,
~ j , j, y, q)  Rn ,
where ej is the predicate label of the j th word. The feature function (e
in addition to encoding word type and part-of-speech tag, also includes dependency parse
information for each word. These features allow the predicate labeling decision to condition
on the syntactic structure of the sentence.
5.1.3 Modeling the Action-Value Function
Once the relevant sentence has been identified and labeled with a predicate structure, our
algorithm needs to use this information along with the attributes of the current game state s
to select the best possible game action a. To this end, we redefine the action-value function
Q(s, a) as a weighted linear combination of features of the game and the text information:
Q(s0 , a0 ) = w
~  f~(s, a, yi , zi ).

(3)

6. We use the approximation of selecting the single most relevant sentence as an alternative to combining
the features of all sentences in the text, weighted by their relevance probability p(y = yi |s, a, d). This
setup is computationally more expensive than the one used here.

674

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Input layer:

Deterministic feature
layer:

Output layer

Hidden layer encoding
sentence relevance
Hidden layer encoding
predicate labeling

Figure 4: The structure of our neural network model. Each rectangle represents a collection
of units in a layer, and the shaded trapezoids show the connections between layers.
A fixed, real-valued feature function ~x(s, a, d) transforms the game state s, action
a, and strategy document d into the input vector ~x. The second layer contains
two disjoint sets of hidden units ~y and ~z, where ~y encodes the sentence relevance
decisions, and ~z the predicate labeling. These are softmax layers, where only one
unit is active at any time. The units of the third layer f~(s, a, yi , zi ) are a set of
fixed real valued feature functions on s, a, d and the active units yi and zi of ~y
and ~z respectively.

Here s0 = hs, di, a0 = ha, yi , zi i, w
~ is the weight vector, and f~(s, a, yi , zi )  Rn is a feature
function over the state s, action a, relevant sentence yi , and its predicate labeling zi . This
structure of the action-value function allows it to explicitly learn the correlations between
textual information, and game states and actions. The action a that maximizes Q(s, a) is
then selected as the best action for state s: 7
a = arg max Q(s, a).
a

5.1.4 Complete Joint Model
The two text analysis models, and the action-value function described above form the three
primary components of our text-aware game playing algorithm. We construct a single
principled model from these components by representing each of them via different layers
of the multi-layer neural network shown in Figure 4. Essentially, the text analysis decisions
are modeled as latent variables by the second, hidden layer of the network, while the final
output layer models the action-value function.
7. Note that we select action a based on Q(s, a), which depends on the relevant sentence yi . This sentence
itself is selected conditioned on action a. This may look like a cyclic dependency between actions
and sentence relevance. However, that is not the case since Q(s, a), and therefore sentence relevance
p(y|s, a, d), is computed for every candidate action a  A. The actual game action a is then selected
from this estimate of Q(s, a).

675

fiBranavan, Silver, & Barzilay

The input layer ~x of our neural network encodes the inputs to the model  i.e., the
current state s, candidate action a, and document d. The second layer consists of two
disjoint sets of hidden units ~y and ~z, where each set operates as a stochastic 1-of-n softmax
selection layer (Bridle, 1990). The activation function for units in this layer is the standard
softmax function:
.X
p(yi = 1|~x) = e~ui ~x
e~uk ~x ,
k

ith

where yi is the
hidden unit of ~y , ~ui is the weight vector corresponding to yi , and k is
the number of units in the layer. Given that this activation function is mathematically
equivalent to a log-linear distribution, the layers ~y and ~z operate like log-linear models.
Node activation in such a softmax layer simulates sampling from the log-linear distribution.
We use layer ~y to replicate the log-linear model for sentence relevance from Equation (1),
with each node yi representing a single sentence. Similarly, each unit zi in layer ~z represents
a complete predicate labeling of a sentence, as in Equation (2).8
The third feature layer f~ of the neural network is deterministically computed given the
active units yi and zi of the softmax layers, and the values of the input layer. Each unit in
this layer corresponds to a fixed feature function fk (s, a, yi , zi )  R. Finally the output layer
encodes the action-value function Q(s, a) as a weighted linear combination of the units of
the feature layer, thereby replicating Equation (3) and completing the joint model.
As an example of the kind of correlations learned by our model, consider Figure 5.
Here, a relevant sentence has already been selected for the given game state. The predicate
labeling of this sentence has identified the words irrigate and settler as describing the
action to take. When game roll-outs return higher rewards for the irrigate action of the
settler unit, our model can learn an association between this action and the words that
describe it. Similarly, it can learn the association between state description words and the
feature values of the current game state  e.g., the word city and the binary feature nearcity. This allows our method to leverage the automatically extracted textual information
to improve game play.
5.2 Parameter Estimation
Learning in our method is performed in an online fashion: at each game state st , the
algorithm performs a simulated game roll-out, observes the outcome of the simulation, and
updates the parameters ~u, ~v and w
~ of the action-value function Q(st , at ). As shown in
Figure 3, these three steps are repeated a fixed number of times at each actual game state.
The information from these roll-outs is then used to select the actual game action. The
algorithm relearns all the parameters of the action-value function for every new game state
st . This specializes the action-value function to the subgame starting from st . Learning
a specialized Q(st , at ) for each game state is common and useful in games with complex
state spaces and dynamics, where learning a single global function approximation can be
particularly difficult (Sutton et al., 2007). A consequence of this function specialization
is the need for online learning  since we cannot predict which games states will be seen
8. Our intention is to incorporate, into action-value function, information from only the most relevant
sentence. Therefore, in practice, we only perform a predicate labeling of the sentence selected by the
relevance component of the model.

676

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Settlers unit, candidate action 1:
plains

Features:
action = irrigate and action-word = "irrigate"
action = irrigate and state-word = "land"
action = irrigate and terrain = plains
action = irrigate and unit-type = settler
state-word = "city" and near-city = true

city

Settlers unit, candidate action 2:

settler unit

Relevant text: "Use settlers to irrigate land near your city"
Predicted action words:

"irrigate", "settler"

Predicted state words:

"land", "near", "city"

irrigate

Features:
action = build-city
action = build-city
action = build-city
action = build-city
state-word = "city"

and
and
and
and
and

build-city

action-word = "irrigate"
state-word = "land"
terrain = plains
unit-type = settler
near-city = true

Figure 5: An example of text and game attributes, and resulting candidate action features.
On the left is a portion of a game state with arrows indicating game attributes.
Also on the left is a sentence relevant to the game state along with action and
state words identified by predicate labeling. On the right are two candidate
actions for the settler unit along with the corresponding features. As mentioned
in the relevant sentence, irrigate is the better of the two actions  executing it
will lead to future higher game scores. This feedback and the features shown
above allow our model to learn effective mappings  such as between the actionword irrigate and the action irrigate, and between state-word city and game
attribute near-city.

during testing, function specialization for those states cannot be done a priori, ruling out
the traditional training/test separation.
Since our model is a non-linear approximation of the underlying action-value function of
the game, we learn model parameters by applying non-linear regression to the observed final
utilities from the simulated roll-outs. Specifically, we adjust the parameters by stochastic
gradient descent, to minimize the mean-squared error between the action-value Q(s, a) and
the final utility R(s ) for each observed game state s and action a. The resulting update
to model parameters  is of the form:

 =   [R(s )  Q(s, a)]2
2
=  [R(s )  Q(s, a)]  Q(s, a; ),
where  is a learning rate parameter. This minimization is performed via standard error
backpropagation (Bryson & Ho, 1969; Rumelhart, Hinton, & Williams, 1986), resulting in
the following online parameter updates:
w
~  w
~ + w [Q  R(s )] f~(s, a, yi , zj ),
~ui  ~ui + u [Q  R(s )] Q ~x [1  p(yi |)],
~vi  ~vi + v [Q  R(s )] Q ~x [1  p(zi |)].
677

fiBranavan, Silver, & Barzilay

Here w is the learning rate, Q = Q(s, a), and w,
~ ~ui and ~vi are the parameters of the
final layer, the sentence relevance layer and the predicate labeling layer respectively. The
derivations of these update equations are given in Appendix A

6. Applying the Model
The game we test our model on, Civilization II, is a multi-player strategy game set either on
Earth or on a randomly generated world. Each player acts as the ruler of one civilization,
and starts with a few game units  i.e., two Settlers, two Workers and one Explorer. The
goal is to expand your civilization by developing new technologies, building cities and new
units, and to win the game by either controlling the entire world, or successfully sending a
spaceship to another world. The map of the game world is divided into a grid of typically
4000 squares, where each grid location represents a tile of either land or sea. Figure 6 shows
a portion of this world map from a particular instance of the game, along with the game
units of one player. In our experiments, we consider a two-player game of Civilization II
on a map of 1000 squares  the smallest map allowed on Freeciv. This map size is used by
both novice human players looking for an easier game, as well as advanced players wanting
a game of shorter duration. We test our algorithms against the built-in AI player of the
game, with the difficulty level at the default Normal setting.9
6.1 Game States and Actions
We define the game state for Monte-Carlo search, to be the map of the game world, along
with the attributes of each map tile, and the location and attributes of each players cities
and units. Some examples of these attributes are shown in Figure 7. The space of possible
actions for each city and unit is defined by the game rules given the current game state.
For example, cities can construct buildings such as harbors and banks, or create new units
of various types; while individual units can move around on the grid, and perform unit
specific actions such as irrigation for Settlers, and military defense for Archers. Since a
player controls multiple cities and units, the players action space at each turn is defined
by the combination of all possible actions for those cities and units. In our experiments, on
average, a player controls approximately 18 units with each unit having 15 possible actions.
The resulting action space for a player is very large  i.e., 1021 . To effectively deal with this
large action space, we assume that given the state, the actions of each individual city and
unit are independent of the actions of all other cities and units of the same player.10 At
the same time, we maximize parameter sharing by using a single action-value function for
all the cities and units of the player.

9. Freeciv has five difficulty settings: Novice, Easy, Normal, Hard and Cheating. As evidenced by discussions on the games online forum (http://freeciv.wikia.com/index.php?title=Forum:Playing Freeciv),
some human players new to the game find even the Novice setting too hard.
10. Since each player executes game actions in turn, i.e. opposing units are fixed during an individual players
turn, the opponents moves do not enlarge the players action space.

678

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Figure 6: A portion of the game map from one instance of a Civilization II game. Three
cities, and several units of a single player are visible on the map. Also visible are
the different terrain attributes of map tiles, such as grassland, hills, mountains
and deserts.

Nation attributes:
-

City attributes:
-

Amount of gold in treasury
% of world controlled
Number of cities
Population
Known technologies

Map tile attributes:
-

City population
Surrounding terrain and resources
Amount of food & resources produced
Number of units supported by city
Number & type of units present

Unit attributes:

Terrain type (e.g. grassland, mountain, etc)
Tile resources (e.g. wheat, coal, wildlife, etc)
Tile has river
Construction on tile (city, road, rail, etc)
Types of units (own or enemy) present

-

Unit type (e.g., worker, explorer, archer, etc)
Unit health & hit points
Unit experience
Is unit in a city?
Is unit fortied?

Figure 7: Example attributes of game state.

679

fiBranavan, Silver, & Barzilay

6.2 Utility Function
Critically important to the Monte-Carlo search algorithm, is the availability of a utility
function that can evaluate the outcomes of simulated game roll-outs. In the typical application of the algorithm, the final game outcome in terms of victory or loss is used as the
utility function (Tesauro & Galperin, 1996). Unfortunately, the complexity of Civilization
II, and the length of a typical game, precludes the possibility of running simulation roll-outs
until game completion. The game, however, provides each player with a real valued game
score, which is a noisy indicator of the strength of their civilization. Since we are playing
a two-player game, our players score relative to the opponents can be used as the utility
function. Specifically, we use the ratio of the game score of the two players.11
6.3 Features
All the components of our method operate on features computed over a basic set of text and
game attributes. The text attributes include the words of each sentence along with their
parts-of-speech and dependency parse information such as dependency types and parent
words. The basic game attributes encode game information available to human players
via the games graphical user interface. Some examples of these attributes are shown in
Figure 7.
To identify the sentence most relevant to the current game state and candidate action,
the sentence relevance component computes features over the combined basic attributes of
~ are of two types  the first
the game and of each sentence from the text. These features ,
computes the Cartesian product between the attributes of the game and the attributes of
the candidate sentence. The second type consists of binary features that test for overlap
between words from the candidate sentence, and the text labels of the current game state
and candidate action. Given that only 3.2% of word tokens from the manual overlap with
labels from the game, these similarity features are highly sparse. However, they serve as
signposts to guide the learner  as shown by our results, our method is able to operate
effectively even in the absence of these features, but performs better when they are present.
Predicate labeling, unlike sentence relevance, is purely a language task and as such
~ compute
operates only over the basic text attributes. The features for this component, ,
the Cartesian product of the candidate predicate label with the words type, part-of-speech
tag, and dependency parse information. The final component of our model, the action-value
approximation, operates over the attributes of the game state, the candidate action, the
sentence selected as relevant, and the predicate labeling of that sentence. The features of
this layer, f~, compute a three way Cartesian product between the attributes of the candidate
action, the attributes of the game state, and the predicate labeled words of the relevant
~ 
~ and f~ compute approximately 158,500, 7,900, and 306,800 features
sentence. Overall, ,
respectively  resulting in a total of 473,200 features for our full model. Figure 8 shows
some examples of these features.

11. The difference between players scores can also be used as the utility function. However, in practice the
score ratio produced better empirical performance across all algorithms and baselines.

680

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Sentence relevance features:
1 if action = build-city
& tile-has-river = true
& word = "build"

1 if action = irrigate
& tile-is-next-to-city = true
& word = "irrigate"

0 otherwise

0 otherwise

Predicate labeling features:
1 if label = action
& word = "city"
& parent-word = "build"

1 if label = state
& word = "city"
& parent-label = "near"

0 otherwise

0 otherwise

Action-value features:
1 if action = build-city
& tile-has-river = true
& action-word = "build"
& state-word = "river"

1 if action = irrigate
& tile-terrain = plains
& action-word = "irrigate"
& state-word = "city"

0 otherwise

0 otherwise

Figure 8: Some examples of features used in our model. In each feature, conditions that
test game attributes are highlighted in blue, and those that test words in the
game manual are highlighted in red.

7. Experimental Setup
In this section, we describe the datasets, evaluation metrics, and experimental framework
used to test the performance of our method and the various baselines.
7.1 Datasets
We use the official game manual of Civilization II as our strategy guide document.12 The
text of this manual uses a vocabulary of 3638 word types, and is composed of 2083 sentences,
each on average 16.9 words long. This manual contains information about the rules of the
game, about the game user interface, and basic strategy advice about different aspects of
the game. We use the Stanford parser (de Marneffe, MacCartney, & Manning, 2006), under
default settings, to generate the dependency parse information for sentences in the game
manual.
7.2 Experimental Framework
To apply our method to the Civilization II game, we use the games open source reimplementation Freeciv.13 We instrumented FreeCiv to allow our method to programmatically
12. www.civfanatics.com/content/civ2/reference/Civ2manual.zip
13. http://freeciv.wikia.com. Game version 2.2

681

fiBranavan, Silver, & Barzilay

Primary Game
Monte-Carlo
Player

Game
Server

Modied Game
GUI Client

In-memory
File System

Game Simulation 1

Game
Strategy Guide

Game
Server

Modied Game
GUI Client

Game State

Game Simulation 2
Game
Server

Modied Game
GUI Client

Game Simulation 8
Game
Server

Modied Game
GUI Client

Figure 9: A diagram of the experimental framework, showing the Monte-Carlo player, the
server for the primary game which the playing aims to win, and multiple game
servers for simulated play. Communications between the multiple processes comprising the framework is via UNIX sockets and an in-memory file system.

control the game  i.e., to measure the current game state, to execute game actions, to
save/load the current game state, and to start and end games.14
Across all experiments, we start the game at the same initial state and run it for 100
steps. At each step, we perform 500 Monte-Carlo roll-outs. Each roll-out is run for 20
simulated game steps before halting the simulation and evaluating the outcome. Note that
at each simulated game step, our algorithm needs to select an action for each game unit.
Given an average number of units per player of 18, this results in 180,000 decisions during
the 500 roll-outs. The pairing of each of these decisions with the corresponding roll-out
outcome is used as a datapoint to update model parameters. We use a fixed learning rate
of 0.0001 for all experiments. For our method, and for each of the baselines, we run 200
independent games in the above manner, with evaluations averaged across the 200 runs.
We use the same experimental settings across all methods, and all model parameters are
initialized to zero.
Our experimental setup consists of our Monte-Carlo player, a primary game which we
aim to play and win, and a set of simulation games. Both the primary game and the simula14. In addition to instrumentation, the code of FreeCiv (both the server and client) was changed to increase
simulation speed by several orders of magnitude, and to remove bugs which caused the game to crash.
To the best of our knowledge, the game rules and functionality are identical to the unmodified Freeciv
version 2.2

682

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

tions are simply separate instances of the Freeciv game. Each instance of the Freeciv game
is made up of one server process, which runs the actual game, and one client process, which
is controlled by the Monte-Carlo player. At the start of each roll-out, the simulations are
initialized with the current state of the primary game via the game save/reload functionality
of Freeciv. Figure 9 shows a diagram of this experimental framework.
The experiments were run on typical desktop PCs with single Intel Core i7 CPUs (4
hyper-threaded cores per CPU). The algorithms were implemented to execute 8 simulation
roll-outs in parallel by connecting to 8 independent simulation games. In this computational
setup, approximately 5 simulation roll-outs are executed per second for our full model, and
a single game of 100 steps runs in 3 hours. Since we treat the Freeciv game code as a
black box, special care was taken to ensure consistency across experiments: all code was
compiled on one specific machine, under a single fixed build environment (gcc 4.3.2); and
all experiments were run under identical settings on a fixed set of machines running a fixed
OS configuration (Linux kernel 2.6.35-25, libc 2.12.1).

7.3 Evaluation Metrics

We wish to evaluate two aspects of our method: how well it improves game play by leveraging textual information, and how accurately it analyzes text by learning from game feedback.
We evaluate the first aspect by comparing our method against various baselines in terms of
the percentage of games won against the built-in AI of Freeciv. This AI is a fixed heuristic
algorithm designed using extensive knowledge of the game, with the intention of challenging human players.15 As such, it provides a good open-reference baseline. We evaluate our
method by measuring the percentage of games won, averaged over 100 independent runs.
However, full games can sometimes last for multiple days, making it difficult to do an extensive analysis of model performance and contributing factors. For this reason, our primary
evaluation measures the percentage of games won within the first 100 game steps, averaged
over 200 independent runs. This evaluation is an underestimate of model performance 
any game where the player has not won by gaining control of the entire game map within
100 steps is considered a loss. Since games can remain tied after 100 steps, two equally
matched average players, playing against each other, will most likely have a win rate close
to zero under this evaluation.

8. Results

To adequately characterize the performance of our method, we evaluate it with respect to
several different aspects. In this section, we first describe its game playing performance
and analyze the impact of textual information. Then, we investigate the quality of the text
analysis produced by our model in terms of both sentence relevance and predicate labeling.
683

fiBranavan, Silver, & Barzilay

Method
Random
Built-in AI
Game only
Latent variable
Full model
Randomized text

% Win
0
0
17.3
26.1
53.7
40.3

% Loss
100
0
5.3
3.7
5.9
4.3

Std. Err.


 2.7
 3.1
 3.5
 3.4

Table 1: Win rate of our method and several baselines within the first 100 game steps, while
playing against the built-in game AI. Games that are neither won nor lost are still
ongoing. Our models win rate is statistically significant against all baselines. All
results are averaged across 200 independent game runs. The standard errors shown
are for percentage wins.

Method
Game only
Latent variable
Full model

% Wins
24.8
31.5
65.4

Standard Error
 4.3
 4.6
 4.8

Table 2: Win rate of our method and two text-unaware baselines against the built-in AI.
All results are averaged across 100 independent game runs.

8.1 Game Performance
Table 1 shows the performance of our method and several baselines on the primary 100-step
evaluation. In this scenario, our language-aware Monte-Carlo algorithm wins on average
53.7% of games, substantially outperforming all baselines, while the best non-languageaware method has a win rate of only 26.1%. The dismal performance of the Random baseline
and the games own Built-in AI, playing against itself, are indications of the difficulty of
winning games within the first 100 steps. As shown in Table 2, when evaluated on full length
games, our method has a win rate of 65.4% compared to 31.5% for the best text-unaware
baseline.16

15. While this AI is constrained to follow the rules of the game, it has access to information typically not
available to human players, such as information about the technology, cities and units of its opponents.
Our methods on the other hand are restricted to the actions and information available to human players.
16. Note that the performance of all methods on the full games is different from those listed in our previous
publications (Branavan, Silver, & Barzilay, 2011a, 2011b). These previous numbers were biased by a
code flaw in FreeCiv which caused the game to sporadically crash in the middle game play. While we
originally believed the crash to be random, it was subsequently discovered to happen more often in losing
games, and thereby biasing the win rates of all methods upwards. The numbers presented here are with
this game bug fixed, with no crashes observed in any of the experiments.

684

fiObserved game score

Learning to Win by Reading Manuals in a Monte-Carlo Framework

Monte-Carlo rollouts

Figure 10: Observed game score as a function of Monte-Carlo roll-outs for our text-aware
full model, and the text-unaware latent-variable model. Model parameters are
updated after each roll-out, thus performance improves with roll-outs. As can be
seen, our full models performance improves dramatically over a small number
of roll-outs, demonstrating the benefit it derives from textual information.

8.1.1 Textual Advice and Game Performance
To verify and characterize the impact of textual advice on our models performance, we
compare it against several baselines that do not have access to textual information. The
simplest of these methods, Game only, models the action-value function Q(s, a) as a linear
approximation of the games state and action attributes. This non-text-aware method wins
only 17.3% of games (see Table 1). To confirm that our methods improved performance
is not simply due to its inherently richer non-linear approximation, we also evaluate two
ablative non-linear baselines. The first of these, Latent variable extends the linear actionvalue function of Game only with a set of latent variables. It is in essence a four layer
neural network, similar to our full model, where the second layers units are activated only
based on game information. This baseline wins 26.1% of games (Table 1), significantly
improving over the linear Game only baseline, but still trailing our text-aware method by
more than 27%. The second ablative baseline, Randomized text, is identical to our model,
except that it is given a randomly generated document as input. We generate this document
by randomly permuting the locations of words in the game manual, thereby maintaining
the documents statistical properties in terms of type frequencies. This ensures that the
number of latent variables in this baseline is equal to that of our full model. Thus, this
baseline has a model capacity equal to our text-aware method while not having access to
any textual information. The performance of this baseline, which wins only 40.3% of games,
confirms that information extracted from text is indeed instrumental to the performance of
our method.
685

fiBranavan, Silver, & Barzilay

Figure 10 provides insight into how textual information helps improve game performance
 it shows the observed game score during the Monte-Carlo roll-outs for our full model and
the latent-variable baseline. As can be seen from this figure, the textual information guides
our model to a high-score region of the search space far quicker than the non-text aware
method, thus resulting in better overall performance. To evaluate how the performance
of our method varies with the amount of available textual-information, we conduct an
experiment where only random portions of the text are given to the algorithm. As shown
in Figure 11, our methods performance varies linearly as a function of the amount of text,
with the Randomized text experiment corresponding to the point where no information is
available from text.
8.1.2 Impact of Seed Vocabulary on Performance
The sentence relevance component of our model uses features that compute the similarity
between words in a sentence, and the text labels of the game state and action. This assumes
the availability of a seed vocabulary that names game attributes. In our domain, of the 256
unique text labels present in the game, 135 occur in the vocabulary of the game manual.
This results in a sparse seed vocabulary of 135 words, covering only 3.7% of word types
and 3.2% of word tokens in the manual. Despite this sparsity, the seed vocabulary can have
a potentially large impact on model performance since it provides an initial set of word
groundings. To evaluate the importance of this initial grounding, we test our method with
an empty seed vocabulary. In this setup, our full model wins 49.0% of games, showing
that while the seed words are important, our method can also operate effectively in their
absence.
8.1.3 Linguistic Representation and Game Performance
To characterize the contribution of language to game performance, we conduct a series of
evaluations which vary the type and complexity of the linguistic analysis performed by our
method. The results of this evaluation are shown in Table 3. The first of these, Sentence
relevance, highlights the contributions of the two language components of our model. This
algorithm, which is identical to our full model but lacks the predicate labeling component,
wins 46.7% of games, showing that while it is essential to identify the textual advice relevant
to the current game state, a deeper syntactic analysis of the extracted text substantially
improves performance.
To evaluate the importance of dependency parse information in our language analysis,
we vary the type of features available to the predicate labeling component of our model.
The first of these ablative experiments, No dependency information, removes all dependency
features  leaving predicate labeling to operate only on word type features. The performance
of this baseline, a win rate of 39.6%, clearly shows that the dependency features are crucial
for model performance. The remaining three methods  No dependency label, No dependency
parent POS tag and No dependency parent word  each drop the dependency feature they
are named after. The contribution of these features to model performance can be seen in
Table 3.
686

fiWin rate

Learning to Win by Reading Manuals in a Monte-Carlo Framework

Random
text

Percentage of document text given to our model

Figure 11: The performance of our text-aware model as a function of the amount of text
available to it. We construct partial documents by randomly sub-sampling sentences from the full game manual. The x-axis shows the amount of sentences
given to the method as a ratio of the full text. At the leftmost extreme is
the performance of the Randomized Text baseline, showing how it fits into the
performance trend at the point of having no useful textual information.
Method
Full model
Sentence relevance
No dependency information
No dependency label
No depend. parent POS tag
No depend. parent word

% Win
53.7
46.7
39.6
50.1
42.6
33.0

% Loss
5.9
2.8
3.0
3.0
4.0
4.0

Std. Err.
 3.5
 3.5
 3.4
 3.5
 3.5
 3.3

Table 3: Win rates of several ablated versions of our model, showing the contribution of
different aspects of textual information to game performance. Sentence relevance
is identical to the Full model, except that it lacks the predicate labeling component.
The four methods at the bottom of the table ablate specific dependency features
(as indicated by the methods name) from the predicate labeling component of the
full model.

8.1.4 Model Complexity vs Computation Time Trade-off
One inherent disadvantage of non-linear models, when compared to simpler linear models,
is the increase in computation time required for parameter estimation. In our Monte-Carlo
Search setup, model parameters are re-estimated after each simulated roll-out. Therefore,
given a fixed amount of time, more roll-outs can be done for a simpler and faster model. By
its very nature, the performance of Monte-Carlo Search improves with the number of rollouts. This trade-off between model complexity and roll-outs is important since a simpler
687

fiBranavan, Silver, & Barzilay

60%
Full model
Latent variable
Game only
ro
llo

ut

s

50%

0

s

ts

20

0

50

ut

0r
oll
-ou

30%

10

Win rate

40%

o
llro

20%

10%

0%

0

20

40

60

80

100

120

140

Computation time per game step (seconds)

Figure 12: Win rate as a function of computation time per game step. For each MonteCarlo search method, win rate and computation time were measured for 100,
200 and 500 roll-outs per game step, respectively.

model could compensate by using more roll-outs, and thereby outperform more complex
ones. This scenario is particularly relevant in games where players have a limited amount
of time for each turn.
To explore this trade-off, we vary the number of simulation roll-outs allowed for each
method at each game step, recording the win-rate and the average computation time per
game. Figure 12 shows the results of this evaluation for 100, 200 and 500 roll-outs. While
the more complex methods have higher computational demands, these results clearly show
that even when given a fixed amount of computation time per game step, our text-aware
model still produces the best performance by a wide margin.
8.1.5 Learned Game Strategy
Qualitatively, all of the methods described here learn a basic rush strategy. Essentially,
they attempt to develop basic technologies, build an army, and take over opposing cities as
quickly as possible. The performance difference between the different models is essentially
due to how well they learn this strategy.
There are two basic reasons why our algorithms learn the rush strategy. First, since we
are attempting to maximize game score, the methods are implicitly biased towards finding
the fastest way to win  which happens to be the rush strategy when playing against
the built-in AI of Civilization 2. Second, more complex strategies typically require the
coordination of multiple game units. Since our models assume game units to be independent,
688

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Phalanxes are twice as eective at defending cities as warriors.
Build the city on plains or grassland with a river running through it.




You can rename the city if you like, but we'll refer to it as washington.
There are many dierent strategies dictating the order in which
advances are researched

After the road is built, use the settlers to start improving the terrain.
S

S

S

A

A

A

A

A

When the settlers becomes active, chose build road.
S

S

A

S

A

A

Use settlers or engineers to improve a terrain square within the city radius
A

S



A

A

S

A



S

S

S

S

Figure 13: Examples of our methods sentence relevance and predicate labeling decisions.
The box above shows two sentences (identified by green check marks) which
were predicted as relevant, and two which were not. The box below shows
the predicted predicate structure of three sentences, with S indicating state
description,A action description and background words unmarked. Mistakes
are identified with crosses.

they cannot explicitly learn such coordination  putting many complex strategies beyond
the capabilities of our algorithms.
8.2 Accuracy of Linguistic Analysis
As described in Section 5, text analysis in our method is tightly coupled with game playing
 both in terms of modeling, and in terms of learning from game feedback. We have seen
from the results thus far, that this text analysis does indeed help game play. In this section
we focus on the game-driven text analysis itself, and investigate how well it conforms to
more common notions of linguistic correctness. We do this by comparing model predictions
of sentence relevance and predicate labeling against manual annotations.
8.2.1 Sentence Relevance
Figure 13 shows examples of the sentence relevance decisions produced by our method.
To evaluate the accuracy of these decisions, we would ideally like to use a ground-truth
relevance annotation of the games user manual. This however, is impractical since the
relevance decision is dependent on the game context, and is hence specific to each time step
of each game instance. Therefore, we evaluate sentence relevance accuracy using a synthetic
document. We create this document by combining the original game manual with an equal
689

fiSentence relevance accuracy

Branavan, Silver, & Barzilay

1.0
0.8
0.6
0.4
Sentence relevance
Moving average

0.2
0

20

40

60

80

100

Game step

Figure 14: Accuracy of our methods sentence relevance predictions, averaged over 100 independent runs.

number of sentences which are known to be irrelevant to the game. These sentences are
collected by randomly sampling from the Wall Street Journal corpus (Marcus, Santorini,
& Marcinkiewicz, 1993).17 We evaluate sentence relevance on this synthetic document by
measuring the accuracy with which game manual sentences are picked as relevant.
In this evaluation, our method achieves an average accuracy of 71.8%. Given that our
model only has to differentiate between the game manual text and the Wall Street Journal,
this number may seem disappointing. Furthermore, as can be seen from Figure 14, the
sentence relevance accuracy varies widely as the game progresses, with a high average of
94.2% during the initial 25 game steps. In reality, this pattern of high initial accuracy followed by a lower average is not entirely surprising: the official game manual for Civilization
II is written for first time players. As such, it focuses on the initial portion of the game,
providing little strategy advice relevant to subsequent game play.18 If this is the reason for
the observed sentence relevance trend, we would also expect the final layer of the neural
network to emphasize game features over text features after the first 25 steps of the game.
This is indeed the case, as can be seen in Figure 15.
To further test this hypothesis, we perform an experiment where the first n steps of the
game are played using our full model, and the subsequent 100  n steps are played without
using any textual information. The results of this evaluation for several values of n are
given in Figure 16, showing that the initial phase of the game is indeed where information
from the game manual is most useful. In fact, this hybrid method performs just as well
as our full model when n = 50, achieving a 53.3% win rate. This shows that our method
17. Note that sentences from the WSJ corpus contain words such as city which can potentially confuse our
algorithm, causing it to select such sentences are relevant to game play.
18. This is reminiscent of opening books for games like Chess or Go, which aim to guide the player to a
playable middle game, without providing much information about subsequent game play.

690

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

0.5

Game features dominate

1.0

Text features dominate

Text feature importance

1.5

0
20

40

60

80

100

Game step

Figure 15: Difference between the norms of the text features and game features of the
output layer of the neural network. Beyond the initial 25 steps of the game, our
method relies increasingly on game features.

Win rate

60%

40%

20%

0%
20

40

60

80

100

# of initial game steps where text information is used

Figure 16: Graph showing how the availability of textual information during the initial steps
of the game affects the performance of our full model. Textual information is
given to the model for the first n steps (the x axis), beyond which point the
algorithm has no access to text, and becomes equivalent to the Latent Variable
model  i.e., the best non-text model.

is able to accurately identify relevant sentences when the information they contain is most
pertinent to game play, and most likely to produce better game performance.
691

fiBranavan, Silver, & Barzilay

Method
Random labeling
Model, first 100 steps
Model, first 25 steps

S/A/B
33.3%
45.1%
48.0%

S/A
50.0%
78.9%
92.7%

Table 4: Predicate labeling accuracy of our method and a random baseline. Column
S/A/B shows performance on the three-way labeling of words as state, action
or background, while column S/A shows accuracy on the task of differentiating
between state and action words.
game attribute

word

state: grassland

"city"

state: grassland

"build"

state: hills

"build"

action: settlers_build_city

"city"

action: set_research

"discovery"

action: settlers_build_city

"settler"

action: settlers_goto_location

"build"

action: city_build_barracks

"construct"

action: research_alphabet

"develop"

action: set_research

"discovery"

Figure 17: Examples of word to game attribute associations that are learned via the feature
weights of our model.
8.2.2 Predicate Labeling
Figure 13 shows examples of the predicate structure output of our model. We evaluate the
accuracy of this labeling by comparing it against a gold-standard annotation of the game
manual.19 Table 4 shows the performance of our method in terms of how accurately it labels
words as state, action or background, and also how accurately it differentiates between state
and action words. In addition to showing a performance improvement over the random
baseline, these results display a clear trend: under both evaluations, labeling accuracy is
higher during the initial stages of the game. This is to be expected since the model relies
heavily on textual features during the beginning of the game (see Figure 15).
To verify the usefulness of our methods predicate labeling, we perform a final set of
experiments where predicate labels are selected uniformly at random within our full model.
This random labeling results in a win rate of 44%  a performance similar to the sentence
relevance model which uses no predicate information. This confirms that our method is
able to identify a predicate structure which, while noisy, provides information relevant to
game play. Figure 17 shows examples of how this textual information is grounded in the
game, by way of the associations learned between words and game attributes in the final
layer of the full model. For example, our model learns a strong association between the
19. Note that a ground truth labeling of words as either action-description, state-description, or background
is based purely on the semantics of the sentence, and is independent of game state. For this reason,
manual annotation is feasible, unlike in the case of sentence relevance.

692

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

game-state attribute grassland and the words city and build, indicating that textual
information about building cities maybe very useful when a players unit is near grassland.

9. Conclusions
In this paper we presented a novel approach for improving the performance of control
applications by leveraging information automatically extracted from text documents, while
at the same time learning language analysis based on control feedback. The model biases the
learned strategy by enriching the policy function with text features, thereby modeling the
mapping between words in a manual and state-specific action selection. To effectively learn
this grounding, the model identifies text relevant to the current game state, and induces
a predicate structure on that text. These linguistic decisions are modeled jointly using a
non-linear policy function trained in the Monte-Carlo Search framework.
Empirical results show that our model is able to significantly improve game win rate
by leveraging textual information when compared to strong language-agnostic baselines.
We also demonstrate that despite the increased complexity of our model, the knowledge
it acquires enables it to sustain good performance even when the number of simulations is
reduced. Moreover, deeper linguistic analysis, in the form of a predicate labeling of text,
further improves game play. We show that information about the syntactic structure of
text is crucial for such an analysis, and ignoring this information has a large impact on
model performance. Finally, our experiments demonstrate that by tightly coupling control
and linguistic features, the model is able to deliver robust performance in the presence of
the noise inherent in automatic language analysis.

Bibliographical Note
Portions of this work were previously presented in two conference publications (Branavan
et al., 2011a, 2011b). This article significantly extends our previous work, most notably by
providing an analysis of model properties such as the impact of linguistic representation on
model performance, dependence of the model on bootstrapping conditions, and the tradeoff between the models representational power and its empirical complexity (Section 8).
The paper also significantly increases the volume of experiments on which we base our
conclusions. In addition, we provide a comprehensive description of the model, providing
full mathematical derivations supporting the algorithm (Section 5.1 and Appendix A).

Acknowledgments
The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS0835652), the DARPA BOLT Program (HR0011-11-2-0008), the DARPA Machine Reading
Program (FA8750-09-C-0172, PO#4910018860), Batelle (PO#300662) and the Microsoft
Research New Faculty Fellowship. Thanks to the anonymous reviewers, Michael Collins,
Tommi Jaakkola, Leslie Kaelbling, Nate Kushman, Sasha Rush, Luke Zettlemoyer, and the
MIT NLP group for their suggestions and comments. Any opinions, findings, conclusions,
or recommendations expressed in this paper are those of the authors, and do not necessarily
reflect the views of the funding organizations.
693

fiBranavan, Silver, & Barzilay

Appendix A. Parameter Estimation
The parameter of our model are estimated via standard error backpropagation (Bryson &
Ho, 1969; Rumelhart et al., 1986). To derive the parameter updates, consider the slightly
simplified neural network shown below. This network is identical to our model, but for the
sake of clarity, it has only a single second layer ~y instead of the two parallel second layers
~y and ~z. The parameter updates for these parallel layers ~y and ~z are similar, therefore we
will show the derivation only for ~y in addition to the updates for the final layer.

As in our model, the nodes yi in the network above are activated via a softmax function;
the third layer, f~, is computed deterministically from the active nodes of the second layer
via the function ~g (yi , ~x); and the output Q is a linear combination of f~ weighted by w:
~
p(yi = 1 | ~x; ~ui ) =

e~ui ~x
X
,
e~uk ~x
k

f~ =

X

~g (~x, yi ) p(yi | ~x; ~ui ),

i

Q = w
~  f~.
Our goal is to minimize the mean-squared error e by gradient descent. We achieve this by
updating model parameters along the gradient of e with respect to each parameter. Using
i as a general term to indicate our models parameters, this update takes the form:
1
(Q  R)2 ,
2
e
=
i
Q
= (Q  R)
.
i

e =
i

From Equation (4), the updates for final layer parameters are given by:
Q
wi

= (Q  R)
w
~  f~
wi
= (Q  R) fi .

wi = (Q  R)

694

(4)

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Since our model samples the one most relevant sentence yi , and the best predicate labeling
zi , the resulting online updates for the output layer parameters w
~ are:
w
~  w
~ + w [Q  R(s )] f~(s, a, yi , zj ),
where w is the learning rate, and Q = Q(s, a). The updates for the second layers parameters are similar, but somewhat more involved. Again, from Equation (4),
ui,j

Q
ui,j

= (Q  R)
w
~  f~
ui,j
X

w
~
~g (~x, yk ) p(yi | ~x; ~uk )
= (Q  R)
ui,j
= (Q  R)

k

= (Q  R) w
~  ~g (~x, yi )


p(yi | ~x; ~ui ).
ui,j

(5)

Considering the final term in the above equation separately,

p(yi | ~x; ~ui ) =
ui,j

 e~ui ~x
,
ui,j Z


=

where Z =

X

e~uk ~x

k

  eu~ i ~x
e~ui ~x ui,j Z
 u~ ~x 
e i
Z
Z

=
=
=
=
=
=

e~ui ~x

 ~ui ~x 

e
log
Z
ui,j
Z
 ~ui ~x  

e

xj 
log Z
Z
ui,j
 ~ui ~x  

e
1 Z
xj 
Z
Z ui,j
"
#
 ~ui ~x 
e
1  X ~uk ~x
xj 
e
Z
Z ui,j
k
 ~ui ~x  

e
1
~
uk ~
x
xj  xj e
Z
Z
 ~ui ~x  

e
e~ui ~x
xj 1 
.
Z
Z




695

fiBranavan, Silver, & Barzilay

Therefore, from Equation (5),
ui,j


p(yi | ~x; ~ui )
ui,j
 ~ui ~x 


e
e~ui ~x
= (Q  R) w
~  ~g (~x, yi )
xj 1 
Z
Z
= (Q  R) xj w
~  ~g (~x, yi ) p(yi | ~x; ~ui ) [1  p(yi | ~x; ~ui )]
= (Q  R) w
~  ~g (~x, yi )

= (Q  R) xj Q [1  p(yi | ~x; ~ui )] ,
where Q = w
~  ~g (~x, yi ) p(yi | ~x; ~ui ).

The resulting online updates for the sentence relevance and predicate labeling parameters
~u and ~v are:
~ui  ~ui + u [Q  R(s )] Q ~x [1  p(yi |)],
~vi  ~vi + v [Q  R(s )] Q ~x [1  p(zi |)].

696

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Appendix B. Example of Sentence Relevance Predictions
Shown below is a portion of the strategy guide for Civilization II. Sentences that were
identified as relevant by our text-aware model are highlighted in green.

Choosing your location.
When building a new city, carefully plan where you place it. Citizens can
work the terrain surrounding the city square in an x-shaped pattern (see
city radius for a diagram showing the exact dimensions). This area is called
the city radius (the terrain square on which the settlers were standing
becomes the city square). The natural resources available where a
population settles affect its ability to produce food and goods. Cities built on
or near water sources can irrigate to increase their crop yields, and cities
near mineral outcroppings can mine for raw materials. On the other hand,
cities surrounded by desert are always handicapped by the aridness of their
terrain, and cities encircled by mountains find arable cropland at a
premium. In addition to the economic potential within the city's radius, you
need to consider the proximity of other cities and the strategic value of a
location. Ideally, you want to locate cities in areas that offer a combination
of benefits : food for population growth, raw materials for production, and
river or coastal areas for trade. Where possible, take advantage of the
presence of special resources on terrain squares (see terrain & movement
for details on their benefits).
Strategic value.
The strategic value of a city site is a final consideration. A city square's
underlying terrain can increase any defender's strength when that city
comes under attack. In some circumstances, the defensive value of a
particular city's terrain might be more important than the economic value;
consider the case where a continent narrows to a bottleneck and a rival
holds the other side. Good defensive terrain (hills, mountains, and jungle) is
generally poor for food production and inhibits the early growth of a city. If
you need to compromise between growth and defense, build the city on a
plains or grassland square with a river running through it if possible. This
yields decent trade production and gains a 50 percent defense bonus.
Regardless of where a city is built, the city square is easier to defend than
the same unimproved terrain. In a city you can build the city walls
improvement, which triples the defense factors of military units stationed
there. Also, units defending a city square are destroyed one at a time if they
lose. Outside of cities, all units stacked together are destroyed when any
military unit in the stack is defeated (units in fortresses are the only
exception; see fortresses). Placing some cities on the seacoast gives you
access to the ocean. You can launch ship units to explore the world and to
transport your units overseas. With few coastal cities, your sea power is
inhibited.

697

fiBranavan, Silver, & Barzilay

Appendix C. Examples of Predicate Labeling Predictions
Listed below are the predicate labellings computed by our text-aware method on example
sentences from the game manual. The predicted labels are indicated below the words with
the letters A, S, and B for action-description, state-description and background respectively.
Incorrect labels are indicated by a red check mark, along with the correct label in brackets.
After the road is built, use the settlers to start improving the terrain.
S

S

S

A

A

A

A

A

When the settlers becomes active, chose build road.
S

S

S

A

A

A

Use settlers or engineers to improve a terrain square within the city radius

 S (A)

A

A

A

 A (S)

S

S

S

S

S

Bronze working allows you to build phalanx units
S

 B (S)

S

A

A

A

In order to expand your civilization , you need to build other cities

 A (S)

S

S

B

 B (A)

A

In order to protect the city , the phalanx must remain inside

 B(S)

 B(S)

S

 S(A)

A

A

 B(A)

As soon as you've found a decent site , you want your settlers to build a

 B(S)

 B(S)

S

 B (A)

 A (B)

S

A

permanent settlement - a city

 S (A)

A

In a city you can build the city walls improvement

 A (S)

 B (A)

A

A

A

Once the city is undefended , you can move a friendly army into the city and capture it
S

 B (S)

S

A

A

A

A

B

You can build a city on any terrain square except for ocean.
A

S (A)

 B (S)

S

 A (S)

S

You can launch ship units to explore the world and to transport your units overseas
A

S (A)

A

S

 B (S)

 B (S)

S

B

When a city is in disorder, disband distant military units, return them to their home cities,

 A (S)

A

S

A

S (A)

A

S (A)

A

or change their home cities
A

A

A

You can build a wonder only if you have discovered the advance that makes it possible
A

S (A)

S

698

S

S

S

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

Appendix D. Examples of Learned Text to Game Attribute Mappings
Shown below are examples of some of the word to game-attribute associations learned by
our model. The top ten game attributes with the strongest association by feature weight are
listed for three of the example words  attack, build and grassland. For the fourth
word, settler, only seven attributes had non-zero weights in experiments used to collect
these statistics.

attack

build

phalanx (unit)

worker_goto (action)

warriors (unit)

settler_autosettle (action)

colossus (wonder)

worker_autosettle (action)

city walls (city improvement)

pheasant (terrain attribute)

archers (unit)

settler_irrigate (action)

catapult (unit)

worker_mine (action)

palace (city improvement)

build_city_walls (action)

coinage (city production)

build_catapult (action)

city_build_warriors (action)

swamp (terrain attribute)

city_build_phalanx (action)

grassland (terrain attribute)

grassland

settler

settler_build_city (action)

settlers (state attribute)

worker_continue_action (action)

settler_build_city (action)

pheasant (terrain attribute)

city (state_attribute)

city_build_improvement (action)

grassland (terrain_attribute)

city_max_production (action)

plains (terrain_attribute)

settlers (state attribute)

road (terrain_attribute)

city_max_food (action)

workers (state attribute)

settler_goto (action)
worker_build_road (action)
pyramids (city attribute)

699

fiBranavan, Silver, & Barzilay

Appendix E. Features Used by the Model
Features used predict sentence relevance
The following templates are used to compute the features for sentence relevance:
 Word W is present in sentence.
 Number of words that match the text label of the current unit, an attribute in the
immediate neighborhood of the unit, or the action under consideration.
 The units type is U, (e.g., worker) and word W is present in sentence.
 The action type is A, (e.g., irrigate) and word W is present in sentence.
Features used predict predicate structure
The following templates are used to compute the features for the predicate labeling of words.
The label being considered for the word (i.e., action, state or background) is denoted by
L.
 Label is L and the word type is W.
 Label is L and the part-of-speech tag of the word is T.
 Label is L and the parent word in the dependency tree is W.
 Label is L and the dependency type to the dependency parent word is D.
 Label is L and the part-of-speech of the dependency parent word is T.
 Label is L and the word is a leaf node in the dependency tree.
 Label is L and the word is not a leaf node in the dependency tree.
 Label is L and the word matches a state attribute name.
 Label is L and the word matches a unit type name.
 Label is L and the word matches a action name.
Features used to model action-value function
The following templates are used to compute the features of the action-value approximation.
Unless otherwise mentioned, the features look at the attributes of the player controlled by
our model.
 Percentage of world controlled.
 Percentage of world explored.
 Players game score.
 Opponents game score.
 Number of cities.
 Average size of cities.
 Total size of cities.
700

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

 Number of units.
 Number of veteran units.
 Wealth in gold.
 Excess food produced.
 Excess shield produced.
 Excess trade produced.
 Excess science produced.
 Excess gold produced.
 Excess luxury produced.
 Name of technology currently being researched.
 Percentage completion of current research.
 Percentage remaining of current research.
 Number of game turns before current research is completed.
The following feature templates are applied to each city controlled by the player:
 Current size of city.
 Number of turns before city grows in size.
 Amount of food stored in city.
 Amount of shield stored in city (shields are used to construct new buildings and
units in the city).
 Turns remaining before current construction is completed.
 Surplus food production in city.
 Surplus shield production in city.
 Surplus trade production in city.
 Surplus science production in city.
 Surplus gold production in city.
 Surplus luxury production in city.
 Distance to closest friendly city.
 Average distance to friendly cities.
 City governance type.
 Type of building or unit currently under construction.
 Types of buildings already constructed in city.
 Type of terrain surrounding the city.
 Type of resources available in the citys neighborhood.
701

fiBranavan, Silver, & Barzilay

 Is there another city in the neighborhood.
 Is there an enemy unit in the neighborhood.
 Is there an enemy city in the neighborhood.
The following feature templates are applied to each unit controlled by the player:
 Type of unit.
 Moves left for unit in current game turn.
 Current health of unit.
 Hit-points of unit.
 Is unit a veteran.
 Distance to closest friendly city.
 Average distance to friendly cities.
 Type of terrain surrounding the unit.
 Type of resources available in the units neighborhood.
 Is there an enemy unit in the neighborhood.
 Is there an enemy city in the neighborhood.
The following feature templates are applied to each predicate-labeled word in the sentence
selected as relevant, combined with the current state and action attributes:
 Word W is present in sentence, and the action being considered is A.
 Word W with predicate label P is present in sentence, and the action being considered
is A.
 Word W is present in sentence, the current units type is U, and the action being
considered is A.
 Word W with predicate label P is present in sentence, the current units type is U,
and the action being considered is A.
 Word W is present in sentence, and the current units type is U.
 Word W with predicate label P is present in sentence, and the current units type is
U.
 Word W is present in sentence, and an attribute with text label A is present in the
current units neighborhood.
 Word W with predicate label P is present in sentence, and an attribute with text label
A is present in the current units neighborhood.

702

fiLearning to Win by Reading Manuals in a Monte-Carlo Framework

References
Balla, R., & Fern, A. (2009). UCT for tactical assault planning in real-time strategy games.
In Proceedings of IJCAI, pp. 4045.
Barnard, K., & Forsyth, D. A. (2001). Learning the semantics of words and pictures. In
Proceedings of ICCV, pp. 408415.
Barto, A. G., & Mahadevan, S. (2003). Recent advances in hierarchical reinforcement
learning. Discrete Event Dynamic Systems, 13, 341379.
Billings, D., Castillo, L. P., Schaeffer, J., & Szafron, D. (1999). Using probabilistic knowledge
and simulation to play poker. In Proceedings of AAAI/IAAI, pp. 697703.
Branavan, S., Chen, H., Zettlemoyer, L., & Barzilay, R. (2009). Reinforcement learning for
mapping instructions to actions. In Proceedings of ACL, pp. 8290.
Branavan, S., Silver, D., & Barzilay, R. (2011a). Learning to win by reading manuals in a
monte-carlo framework. In Proceedings of ACL, pp. 268277.
Branavan, S., Silver, D., & Barzilay, R. (2011b). Non-linear monte-carlo search in civilization
II. In Proceedings of IJCAI, pp. 24042410.
Branavan, S., Zettlemoyer, L., & Barzilay, R. (2010). Reading between the lines: Learning
to map high-level instructions to commands. In Proceedings of ACL, pp. 12681277.
Bridle, J. S. (1990). Training stochastic model recognition algorithms as networks can lead
to maximum mutual information estimation of parameters. In Advances in NIPS, pp.
211217.
Bryson, A. E., & Ho, Y.-C. (1969). Applied optimal control: optimization, estimation, and
control. Blaisdell Publishing Company.
Chen, D. L., & Mooney, R. J. (2008). Learning to sportscast: a test of grounded language
acquisition. In Proceedings of ICML, pp. 128135.
Chen, D. L., & Mooney, R. J. (2011). Learning to interpret natural language navigation
instructions from observations. In Proceedings of AAAI, pp. 859865.
Clarke, J., Goldwasser, D., Chang, M.-W., & Roth, D. (2010). Driving semantic parsing
from the worlds response. In Proceedings of CoNNL, pp. 1827.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, pp. 449454.
Eisenstein, J., Clarke, J., Goldwasser, D., & Roth, D. (2009). Reading to learn: Constructing
features from semantic abstracts. In Proceedings of EMNLP, pp. 958967.
Fleischman, M., & Roy, D. (2005). Intentional context in situated natural language learning.
In Proceedings of CoNLL, pp. 104111.
Gelly, S., Wang, Y., Munos, R., & Teytaud, O. (2006). Modification of UCT with patterns
in Monte-Carlo Go. Tech. rep. 6062, INRIA.
Goldwasser, D., Reichart, R., Clarke, J., & Roth, D. (2011). Confidence driven unsupervised
semantic parsing. In Proceedings of ACL, pp. 14861495.

703

fiBranavan, Silver, & Barzilay

Gorniak, P., & Roy, D. (2005). Speaking with your sidekick: Understanding situated speech
in computer role playing games. In Proceedings of AIIDE, pp. 5762.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences with less
supervision. In Proceedings of ACL, pp. 9199.
Liang, P., Jordan, M. I., & Klein, D. (2011). Learning dependency-based compositional
semantics. In Proceedings of ACL, pp. 590599.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated
corpus of english: The penn treebank. Computational Linguistics, 19 (2), 313330.
Oates, J. T. (2001). Grounding knowledge in sensors: Unsupervised learning for language
and planning. Ph.D. thesis, University of Massachusetts Amherst.
Roy, D. K., & Pentland, A. P. (2002). Learning words from sights and sounds: a computational model. Cognitive Science 26, 113146.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by
back-propagating errors. Nature, 323, 533536.
Schafer, J. (2008). The UCT algorithm applied to games with imperfect information.
Diploma Thesis. Otto-von-Guericke-Universitat Magdeburg.
Sheppard, B. (2002). World-championship-caliber Scrabble. Artificial Intelligence, 134 (1-2),
241275.
Silver, D., Sutton, R., & Muller, M. (2008). Sample-based learning and search with permanent and transient memories. In Proceedings of ICML, pp. 968975.
Siskind, J. M. (2001). Grounding the lexical semantics of verbs in visual perception using
force dynamics and event logic. Journal of Artificial Intelligence Research, 15, 3190.
Sturtevant, N. (2008). An analysis of UCT in multi-player games. In Proceedings of ICCG,
pp. 3749.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. The MIT
Press.
Sutton, R. S., Koop, A., & Silver, D. (2007). On the role of tracking in stationary environments. In Proceedings of ICML, pp. 871878.
Tellex, S., Kollar, T., Dickerson, S., Walter, M. R., Banerjee, A. G., Teller, S., & Roy, N.
(2011). Understanding natural language commands for robotic navigation and mobile
manipulation. In Proceedings of AAAI, pp. 15071514.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using Monte-Carlo search.
In Advances in NIPS, pp. 10681074.
Vogel, A., & Jurafsky, D. (2010). Learning to follow navigational directions. In Proceedings
of the ACL, pp. 806814.
Yu, C., & Ballard, D. H. (2004). On the integration of grounding language and learning
objects. In Proceedings of AAAI, pp. 488493.
Zettlemoyer, L., & Collins, M. (2009). Learning context-dependent mappings from sentences
to logical form. In Proceedings of ACL, pp. 976984.

704

fiJournal of Artificial Intelligence Research 43 (2012) 173210

Submitted 08/11; published 02/12

Counting-Based Search:
Branching Heuristics for Constraint Satisfaction Problems
Gilles Pesant

gilles.pesant@polymtl.ca

Ecole polytechnique de Montreal, Montreal, Canada

Claude-Guy Quimper

claude-guy.quimper@ift.ulaval.ca

Universite Laval, Quebec, Canada

Alessandro Zanarini

alessandro.zanarini@dynadec.com

Dynadec Europe, Belgium

Abstract
Designing a search heuristic for constraint programming that is reliable across problem
domains has been an important research topic in recent years. This paper concentrates on
one family of candidates: counting-based search. Such heuristics seek to make branching
decisions that preserve most of the solutions by determining what proportion of solutions to
each individual constraint agree with that decision. Whereas most generic search heuristics
in constraint programming rely on local information at the level of the individual variable,
our search heuristics are based on more global information at the constraint level. We
design several algorithms that are used to count the number of solutions to specific families
of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established
benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies
heuristic maxSD as a robust candidate among our proposals. We then evaluate the latter
against the state of the art, including the latest generic search heuristics, restarts, and
discrepancy-based tree traversals. Experimental results show that counting-based search
generally outperforms other generic heuristics.

1. Introduction
Constraint Programming (cp) is a powerful technique to solve combinatorial problems. It
applies sophisticated inference to reduce the search space and a combination of variableand value-selection heuristics to guide the exploration of that search space. Because this
inference is encapsulated in each constraint appearing in the model of a problem, users may
consider it as a black box. In contrast, search in cp is programmable, which is a mixed
blessing. It allows one to easily tailor search to a problem, adding expertise and domain
knowledge, but it may also discourage the average user who would prefer a generic and
fairly robust default search heuristic that works well most of the time. Some generic search
heuristics are indeed available in cp but robustness remains an issue.
Whereas most generic search heuristics in constraint programming rely on information
at the level of the individual variable (e.g. its domain size and degree in the constraint
network), we investigate search heuristics based on more global information. Global
constraints in cp are successful because they encapsulate powerful dedicated inference algorithms but foremost because they bring out the underlying structure of combinatorial
problems. That exposed structure can also be exploited during search. Search heuristics
c
2012
AI Access Foundation. All rights reserved.

fiPesant, Quimper, & Zanarini

following the fail-first principle (detect failure as early as possible) and centered on constraints can be guided by a count of the number of solutions left for each constraint. We
might for example focus search on the constraint currently having the smallest number
of solutions, recognizing that failure necessarily occurs through a constraint admitting no
more solution. We can also count the number of solutions featuring a given variable-value
assignment in an individual constraint, favoring assignments appearing in a high proportion
of solutions with the hope that such a choice generally brings us closer to satisfying the
whole csp.
The concept of counting-based search heuristics has already been introduced, most recently by Zanarini and Pesant (2009). The specific contributions of this paper are: additional counting algorithms, including for other families of constraints, thus broadening the
applicability of these heuristics; experiments that include the effect of some common features
of search heuristics such as search tree traversal order, restarts and learning; considerable
empirical evidence that counting-based search outperforms other generic heuristics.
In the rest of the paper: Section 2 provides background and reviews related work; Sections 3 to 5 present counting algorithms for several of the most usual constraints; Section 6
introduces counting-based search heuristics which can exploit the algorithms of the previous sections; Section 7 reports on an extensive experimental study comparing our proposed
heuristics to state-of-the-art generic heuristics on many problem domains; finally Section 8
concludes the paper.

2. Background and Related Work
We start with the usual general representation formalism for cp.
Definition 1 (constraint satisfaction problem (csp)). Given a finite set of variables X =
{x1 , x2 , . . .}, a finite domain of possible values for each of these variables, D = {D1 , . . . , D|X| },
xi  Di (1  i  |X|), and a finite set of constraints (relations) over subsets of X,
C = {c1 , c2 , . . .}, the constraint satisfaction problem (X, D, C) asks for an assignment of a
value from Di to each variable xi of X that satisfies (belongs to) each cj in C.
And now recall some definitions and notation from Pesant (2005) and Zanarini and Pesant
(2009).
Definition 2 (solution count). Given a constraint c(x1 , . . . , xn ) and respective finite domains Di 1in, let #c(x1 , . . . , xn ) denote the number of n-tuples in the corresponding
relation, called its solution count.
Definition 3 (solution density). Given a constraint c(x1 , . . . , xn ), respective finite domains
Di 1in, a variable xi in the scope of c, and a value d  Di , we will call
(xi , d, c) =

#c(x1 , . . . , xi1 , d, xi+1 , . . . , xn )
#c(x1 , . . . , xn )

the solution density of pair (xi , d) in c. It measures how often a certain assignment is part
of a solution to c.
174

fiCounting-Based Search

Heuristics are usually classified in two main categories: static variable ordering heuristics
(SVOs) and dynamic variable ordering heuristics (DVOs). The former order the variables
prior to search and do not revise the ordering during search. Common SVOs are lexicographic order, lexico, and decreasing degree (i.e. number of constraints in which a variable
is involved), deg. DVOs are generally considered more effective as they exploit information
gathered during search. They often follow the fail-first principle originally introduced by
Haralick and Elliott (1980, p. 263) i.e. To succeed, try first where you are most likely
to fail. The same authors proposed the widely-used heuristic dom that branches on the
variables with the smallest domain; the aim of such a heuristic is to minimize branch depth.
A similar heuristic, proposed by Brelaz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables). Bessiere and
Regin (1996) and Smith and Grant (1998) combined the domain and degree information by
minimizing the ratio dom/deg or dom/ddeg.

2.1 Impact-Based Heuristics
Refalo (2004) proposed Impact Based Search (IBS), a heuristic that chooses the variable
whose instantiation triggers the largest search space reduction (highest impact) that is
approximated as the reduction of the product of the variable domain cardinalities. More
formally the impact of a variable-value pair is:
I(xi = d) = 1 

Paf ter
Pbef ore

where Paf ter and Pbef ore are the products of the domain cardinalities respectively after and
before branching on xi = d (and propagating that decision). The impact is either computed
exactly at a given node of the search (the exact computation provides better information
but is more time consuming) or approximated as the average reduction observed during the
search (hence automatically collected on-the-go at almost no additional cost), that is:
P
k
 i = d) = kK I (xi = d)
I(x
|K|
where K is the index set of the impact observed so far for the assignment xi = d. The
variable impact is defined by Refalo (2004) as
X
 i = d)
I(xi ) =
1  I(x
dDi0

where Di0 is the current domain of the variable xi . Impact initialization is fundamental to
obtain a good performance even at the root of the search tree; therefore, Refalo proposed
to initialize the impacts by probing each variable-value pair at the root node (note that
this subsumes a reduced form of singleton consistency at the root node and can be quite
computationally costly). IBS selects the variable having the largest impact (hence trying
1. It is also referred to as future degree or forward degree in the literature.

175

fiPesant, Quimper, & Zanarini

to maximize the propagation effects and the reduction of the search space) and then selects
the value having the smallest impact (hence leaving more choices for the future variables).
As an interesting connection with impact-based heuristics, Szymanek and OSullivan
(2006) proposed to query the model constraints to approximate the number of filtered
values by each constraint individually. This information is then exploited to design a variable and/or value selection heuristic. Nonetheless, it differs from impact-based search as
they take into consideration each constraint separately, and from counting-based heuristics
(Zanarini & Pesant, 2009) as the information provided is more coarse-grained than actual
solution counts.
2.2 Conflict-Driven Heuristics
Boussemart, Hemery, Lecoutre, and Sais (2004) proposed a conflict-driven variable ordering
heuristic: they extended the concept of variable degree integrating a simple but effective
learning technique that takes failures into account. Basically each constraint has an associated weight that is increased by one each time the constraint leads to a failure (i.e. a
domain wipe-out). A variable has a weighted degree  wdeg  that is the sum of the weights
of constraints in which it is involved. Formally, the weighted degree of a variable is:
X
wdeg (xi ) =
weight[c] | V ars(c) 3 xi  |F utV ars(c)| > 1
cC

where F utV ars(c) denotes the uninstantiated variables of the constraint c, weight[c] is its
weight and V ars(c) the variables involved in c. The heuristics proposed simply choose the
variable that maximizes wdeg or minimizes dom/wdeg. These heuristics offer no general
method to deal with global constraints: a natural extension is to increase the weight of
every variable in a failed constraint but most of them may not have anything to do with
the failure, which dilutes the conflict information. They are also particularly sensitive to
revision orderings (i.e. the ordering of the propagation queue) hence leading to varying
performance. Grimes and Wallace (2006, 2007) proposed some adaptations of dom/wdeg
when combined with restarts or by updating weights on value deletions as well. Balafoutis
and Stergiou (2008b) proposed, among other improvements over the original dom/wdeg,
weight aging, that is the constraint weights are periodically reduced. This limits the inertia
of constraints that got a significant weight early in the search but that are not critical
anymore later on.
Nowadays heuristics dom/wdeg and IBS are considered to be the state of the art of
generic heuristics with no clear dominance by one or the other (Balafoutis & Stergiou,
2008a). Finally we note that both rely on the hypothesis that what is learned early in the
search will tend to remain true throughout the search tree: impacts should not change much
from one search tree node to the other; the same constraints lead to domain wipe-outs in
different parts of the search tree.
2.3 Approximated Counting-Based Heuristics
The idea of using an approximation on the number of solutions of a problem as heuristic
is not new. Kask, Dechter, and Gogate (2004) approximate the total number of solutions
extending a partial solution to a csp and use it in a value selection heuristic, choosing
176

fiCounting-Based Search

the value whose assignment to the current variable gives the largest approximate solution
count. An implementation optimized for binary constraints performs well compared to
other popular strategies. Hsu, Kitching, Bacchus, and McIlraith (2007) and later Bras,
Zanarini, and Pesant (2009) apply a Belief Propagation algorithm within an Expectation
Maximization framework (EMBP) in order to approximate variable biases (or marginals)
i.e. the probability a variable takes a given value in a solution. The resulting heuristics tend
to be effective but quite time-consuming. One way to differentiate our work from these is
that we focus on fine-grained information from individual constraints whereas they work on
coarser information over the whole problem.

3. Counting for Alldifferent Constraints
The alldifferent constraint restricts a set of variables to be pairwise different (Regin,
1994).
Definition 4 (Alldifferent Constraint). Given a set of variables X = {x1 , . . . , xn } with
respective domains D1 , . . . , Dn , the set of tuples allowed by alldifferent(X) are:
{(d1 , d2 , . . . , dn ) | di  Di , di 6= dj i 6= j}
S
We define the associated (0-1) square matrix A = (aid ) with | i=1,...,n Di | rows and
columns such that aid = 1 iff d  Di 2 . If there are more distinct values in the domains than
there are variables, say p more, we add p rows filled with 1s to matrix A. An equivalent
representation is given by the bipartite value graph with a vertex for each variable and value
and edges corresponding to 1 entries in A.
Then as discussed by Zanarini and Pesant (2009), counting the number of solutions to
an alldifferent constraint is equivalent to computing the permanent of A (or the number
of maximum matchings in the value graph), formally defined as
perm(A) =

n
X

a1,d perm(A1,d )

(1)

d=1

where A1,d denotes the submatrix obtained from A by removing row 1 and column d (the
permanent of the empty matrix is equal to 1). If p extra rows were added, the result must
be divided by p! as shown by Zanarini and Pesant (2010).
Because computing the permanent is well-known to be #P -complete (Valiant, 1979),
Zanarini and Pesant (2009) developed an approach based on sampling which gave close
approximations and led to very effective heuristics on hard instances. However it was not
competitive on easy to medium difficulty instances because of the additional computational
effort. The next section describes an approach based on upper bounds, trading approximation accuracy for a significant speedup in the counting procedure.3
2. For notational convenience and without loss of generality, we identify domain values with consecutive
natural numbers.
3. This was originally introduced by Zanarini and Pesant (2010).

177

fiPesant, Quimper, & Zanarini

3.1 Upper Bounds
In the following we assume for notational convenience that matrix A has n rows
P and columns
and we denote by ri the sum of the elements in the ith row of A (i.e. ri = nd=1 aid ).
A first upper bound for the permanent was conjectured by Minc (1963) and later proved
by Bregman (1973):
n
Y
perm(A) 
(ri !)1/ri .
(2)
i=1

Recently Liang and Bai (2004) proposed a second upper bound (with qi = min{d ri2+1 e, d 2i e}):
perm(A)2 

n
Y

qi (ri  qi + 1).

(3)

i=1

Neither of these two upper bounds strictly dominates the other. In the following we
denote by U B BM (A) the Bregman-Minc upper bound and by U B LB (A) the Liang-Bai
upper bound. Jurkat and Ryser (1966) proposed another bound:
perm(A) 

n
Y

min(ri , i).

i=1

However it is considered generally weaker than U B BM (A) (see Soules, 2005 for a comprehensive literature review).
3.1.1 Algorithm
We decided to adapt U B BM and U B LB in order to compute an approximation of solution
densities for the alldifferent constraint. Assigning d to variable xi translates to replacing
the ith row by the unit vector e(d) (i.e. setting the ith row of the matrix to 0 except for
the element in column d). We write Axi =d to denote matrix A except that xi is fixed to d.
We call local probe the assignment xi = d performed to compute Axi =d i.e. a temporary
assignment that does not propagate to any other constraint except the one being processed.
The upper bound on the number of solutions of the alldifferent(x1 , . . . , xn ) constraint
with a related adjacency matrix A is then simply
#alldifferent(x1 , . . . , xn )  min{U B BM (A), U B LB (A)}
Note that in Formulas 2 and 3, the ri s are equal to |Di |; since |Di | ranges from 0 to n,
the factors can be precomputed and stored: in a vector BM f actors[r] = (r!)1/r , r = 0, . . . , n
for the first bound and similarly for the second one (with factors depending on both |Di |
and i). Assuming that |Di | is returned in O(1), computing the formulas takes O(n) time.
Solution densities are then approximated as
min{U B BM (Axi =d ), U B LB (Axi =d )}

P
where  is a normalizing constant so that dDi (xi , d, alldifferent) = 1.
(xi , d, alldifferent) 

178

fiCounting-Based Search

The local probe xi = d may trigger some local propagation according to the level of
consistency we want to achieve; therefore Axi =d is subject to the filtering performed on the
constraint being processed. Since the two bounds in Formulas 2 and 3 depend on |Di |, a
stronger form of consistency would likely lead to more changes in the domains and on the
bounds, and presumably to more accurate solution densities.
If we want to compute (xi , d, alldifferent) for all i = 1, . . . , n and for all d  Di then
a trivial implementation would compute Axi =d for each variable-value pair; the total time
complexity would be O(mP + mn) (where m is the sum of the cardinalities of the variable
domains and P the time complexity of the filtering).
Although unable to improve over the worst case complexity, in the following we propose an algorithm that performs definitely better in practice. We introduce before some
additional notation: we write as Dk0 the variable domains after enforcing -consistency4
on that constraint alone and as Ixi =d the set of indices of the variables that were subject
to a domain change due to a local probe and the ensuing filtering, that is, k  Ixi =d iff
|Dk0 | 6= |Dk |. We describe the algorithm for the Bregman-Minc bound  it can be easily
adapted for the Liang-Bai bound.
The basic idea is to compute the bound for matrix A and to reuse it to speed up the
computation of the bounds for Axi =d for all i = 1, . . . , n and d  Di . Let

BM f actors[|D0 |]

 BM f actors[|Dkk |] if k  Ixi =d
k =


1
otherwise

U B BM (Axi =d ) =

n
Y

BM f actors[|Dk0 |] =

k BM f actors[|Dk |]

k=1

k=1

= U B BM (A)

n
Y

n
Y

k

k=1

Note that k with k = i (i.e. we are computing U B BM (Axi =d )) does not depend on d;
however Ixi =d does depend on d because of the domain filtering.
Algorithm 1 shows the pseudo code for computing U B BM (Axi =d ) for all i = 1, . . . , n
and d  Di . Initially, it computes the bound for matrix A (line 1); then, for a given i, it
computes i and the upper bound is modified accordingly (line 3). Afterwards, for each
d  Di , -consistency is enforced (line 7) and it iterates over the set of modified variables
(line 9-10) to compute all the k that are different from 1. We store the upper bound for
variable i and value d in the structure V arV alU B[i][d]. Before computing the bound for
the other variables-values the assignment xi = d needs to be undone (line 12). Finally, we
normalize the upper bounds in order to correctly return solution densities (line 13-14). Let
I be equal to maxi,d |Ixi =d |, the time complexity is O(mP + mI).
If matrix A is dense we expect I ' n. Therefore most of the k are different from 1 and
need to be computed. As soon as the matrix becomes sparse enough then I  n and only a
small fraction of k need to be computed, and that is where Algorithm 1 has an advantage.
4. Stands for any form of consistency

179

fiPesant, Quimper, & Zanarini

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

UB = U B BM (A);
for i = 1, . . . , n do
varUB = UB * BMfactors[1] / BMfactors[|Di |];
total = 0;
forall d  Di do
set xi = d;
enforce -consistency;
VarValUB[i][d] = varUB;
forall k  Ixi =d \ {i} do
VarValUB[i][d] = VarValUB[i][d] * BMfactors[|Dk0 |] / BMfactors[|Dk |];
total = total + VarValUB[i][d];
rollback xi = d;
forall d  Di do
SD[i][d] = VarValUB[i][d]/total;
return SD;
Algorithm 1: Solution Densities

The sampling algorithm introduced by Zanarini and Pesant (2009) performed very well
both in approximating the solution count and the solution densities, but this is not the case
for upper bounds. The latter in fact produce weak approximations of the solution count
but offer a very good trade-off between performance and accuracy for solution densities:
taking the ratio of two solution counts appears to cancel out the weakness of the original
approximations (see Zanarini & Pesant, 2010 for further details).
3.2 Symmetric Alldifferent
Regin (1999) proposed the symmetric alldifferent constraint that is a special case of
the alldifferent in which variables and values are defined from the same set. This is
equivalent to a traditional alldifferent with an additional set of constraints stating that
variable i is assigned to a value j iff variable j is assigned to value i. This constraint is useful
in many real world problems in which a set of entities need to be paired up; particularly, in
sport scheduling problems teams need to form a set of pairs that define the games.
A symmetric alldifferent achieving domain consistency provides more pruning power
than the equivalent decomposition given by the alldifferent constraint and the set of
xi = j  xj = i constraints (Regin, 1999). Its filtering algorithm is inspired from the
one for alldifferent with the difference being that the matching is computed in a graph
(not necessarily bipartite) called contracted value graph where vertices and values representing the same entity are collapsed into a single vertex (i.e. the vertex xi and the vertex
i are merged into a single vertex i representing both the variable and the value). Regin
proved that there is a bijection between a matching in the contracted value graph and a
solution of the symmetric alldifferent constraint. Therefore, counting the number of
matchings on the contracted value graph corresponds to counting the number of solutions
to the constraint.

180

fiCounting-Based Search

Friedland (2008) and Alon and Friedland (2008) extended the Bregman-Minc upper
bound to consider the number of matchings in general undirected graphs. Therefore, we
can exploit the bound as in the previous section in order to provide an upper bound of the
solution count and the solution densities for the symmetric alldifferent constraint. The
upper bound for the number of matchings of a graph G = (V, E) representing the contracted
value graph is the following:
Y
1
#matchings(G) 
(deg(v))! 2deg(v)
(4)
vV

where deg(v) is the degree of the vertex v and #matchings(G) denotes the number of
matchings on the graph G. Note that in case of a bipartite graph, this bound is equivalent
to the Bregman-Minc upper bound.
The algorithm for counting the number of solutions and computing the solution densities
can be easily derived from what we proposed for the alldifferent.
Example 1. Consider a symmetric alldifferent defined on six variables x1 , . . . , x6 each
one having a domain equal to {1, . . . , 6}. In Figure 1 the associated contracted value graph
is depicted (together with a possible solution to the constraint). In this case, the number of
solutions of the symmetric alldifferent can be computed as 5  3 = 15. In the contracted
value graph each vertex is connected to each other vertex, forming a clique of size 6, therefore
all the vertices have a degree equal to 5. The upper bound proposed by Friedland is equal to:
Y
1
#matchings(G) 
(deg(v))! 2deg(v) = (5!1/10 )6  17.68
vV

In the alldifferent formulation, the related value graph has variable vertices connected to
each of the values (from 1 to 6) thus the ri s are equal to 6. If we consider to rule out all
the edges causing degenerated assignments (xi = i) then we end up with a value graph in
which all the ri s are equal to 5. The Bregman-Minc upper bound would give:
perm(A) 

n
Y

(ri !)1/ri = (5!(1/5) )6  312.62.

i=1

The result is obviously very far from the upper bound given by Formula 4 as well as from
the exact value.

4. Counting for Global Cardinality Constraints
We present in this section how to extend the results obtained in Section 3 to the Global
Cardinality Constraint (gcc), which is a generalization of the alldifferent constraint.
Definition 5 (Global Cardinality Constraint). The set of solutions of constraint gcc(X, l, u)
where X is a set of k variables, l and u respectively the lower and upper bounds for each
value, is defined as:
[
T (gcc(X, l, u)) = {(d1 , . . . , dk ) | di  Di , ld  |{di |di = d}|  ud d  DX =
Dj }
xj X

181

fiPesant, Quimper, & Zanarini

1
3

2

5

4
6

Figure 1: Contracted Value Graph for the constraint symmetric alldifferent of Example
1. Edges in bold represent a possible solution.

We will consider a gcc in which all the fixed variables are removed and the lower and
upper bounds are adjusted accordingly (the semantics of the constraint is unchanged). We
refer to the new set of variables as X 0 = {x  X | x is not bound}; lower bounds are l0
where ld0 = ld  |{x  X | x = d}| and upper bounds u0 are defined similarly; we assume the
constraint maintains -consistency so ld0  0 and u0d  0 for each d  DX .
Inspired by Quimper, Lopez-Ortiz, van Beek, and Golynski (2004) and Zanarini, Milano,
and Pesant (2006), we define Gl the lower bound graph.
Definition 6. Let Gl (X 0  Dl , El ) be an undirected bipartite graph where X 0 is the set of
unbounded variables and Dl the extended value set, that is for each d  DX the graph has ld0
vertices d1 , d2 , . . . representing d (ld0 possibly equal to zero). There is an edge (xi , dj )  El
if and only if d  Di .
Note that a maximum matching on Gl corresponds to a partial assignment of the variables in X that satisfies the gcc lower bound restriction on the number of occurrences of
each value. This partial assignment may or may not be completed to a full assignment
that satisfies both upper bound and lower bound restrictions (here we do not take into
consideration augmenting paths as Zanarini et al., 2006 but instead we fix the variables to
the values represented by the matching in Gl ).
Example 2. Suppose we have a gcc defined on X = {x1 , . . . , x6 } with domains D1 = D4 =
{1, 2, 3}, D2 = {2}, D3 = D5 = {1, 2} and D6 = {1, 3}; lower and upper bounds for the
values are respectively l1 = 1, l2 = 3, l3 = 0 and u1 = 2, u2 = 3, u3 = 2. Considering that
x2 = 2, the lower and upper bounds for the value 2 are respectively l20 = 2 and u02 = 2. The
lower bound graph is shown in Figure 2a: variable x2 is bounded and thus does not appear
in the graph, value vertex 2 is represented by two vertices because it has l20 = 2 (although
l2 = 3); finally value vertex 3 does not appear because it has a lower bound equal to zero. The
matching shown in the figure (bold edges) is maximum. However if we fix the assignments
represented by it (x1 = 2, x4 = 2, x6 = 1) it is not possible to have a consistent solution
since both x3 and x5 have to be assigned either to 1 or 2 hence exceeding the upper bound
restriction. To compute the permanent two additional fake value vertices would be added to
the graph and connected to all the variable vertices (not shown in the figure).
182

fiCounting-Based Search

x1

1

x1

x3

2

x3

x4

20

x4

1

x5

x5

3

x6

x6

30

(a)

(b)

Figure 2: Lower Bound Graph (a) and Residual Upper Bound Graph (b) for Example 2
Every partial assignment that satisfies just the lower bound restriction might correspond
to several maximum matchings in Gl due to the duplicated vertices.
Q For each partial
assignment satisfying the lower bound restriction there are exactly dDX ld0 ! maximum
matchings corresponding to that particular partial assignment. If we take into consideration
Example 2 shown in Figure 2a, variables x1 and x4 may be matched respectively to any
permutation of the vertices 2 and 20 , however no matter which is the permutation, this set
of matchings represents always the assignment of both x2 and x4 to the value 2.
Let Ml 5 be the set of maximum matchings in Gl . We define f : Ml  N, a function that
counts the number of possible ways a maximum matching can be extended to a full gcc
solution. As shown in Example 2, f can be possibly equal to zero. Note that the number of
the remaining variables
that need to be assigned starting from a matching m  Ml is equal
P
0
0
to K = |X |  dDX ld .
The total number of solutions satisfying the gcc is:
P
|Ml | maxmMl (f (m))
U B(Gl ) maxmMl (f (m))
mM f (m)
Q
Q
#gcc(X, l, u) = Q l 0


(5)
0
0
dDX ld !
dDX ld !
dDX ld !
where U B(Gl ) represents an upper bound on the permanent of the 0  1 matrix corresponding to graph Gl .
Note that computing f (m) is as hard as computing the permanent. In fact if l and u
are respectively equal to 0 and 1 for each value, the result is an alldifferent constraint
and equation 5 simplifies to #gcc(X, l, u) = f (m) where m = {} and f (m) corresponds to
the permanent.
As computing f (m) is a #P-complete problem on its own, we focus here on upper bounding f (m). In order to do that, we introduce the upper bound residual graph. Intuitively, it
is similar to the lower bound graph but it considers the upper bound restriction.
Definition 7. Let Gu (X 0  Du , Eu ) be an undirected bipartite graph where X 0 is the set of
unbounded variables and Du the extended value set, that is for each d  DX the graph has
5. if d  DX , ld0 = 0 then Ml = {} and |Ml | = 1

183

fiPesant, Quimper, & Zanarini

u0d  ld0 vertices d1 , d2 , . . . representing d (if u0d  ld0 is equal to zero then there is no vertex
representing d). There is an edge (xi , dj )  Eu if and only if d  Di and u0d  ld0 > 0.
Similarly to the lower bound matching, a matching on Gu that covers K variables may
or may not be completed to a full assignment satisfying the complete gcc. Figure 2b shows
the residual upper bound graph for Example 1: value 2 disappears from the graph since
it has u02 = l20 i.e. starting from a matching in the lower bound graph, the constraints on
value 2 are already satisfied.

graphs each with a comIn order to compute maxmMl (f (m)), we should build |X|
K
bination of K variables, and then choose the one that maximizes the permanent. More
practically, given the nature of the U B M B and U B LB , it suffices to choose K variables
which contribute with the highest factor in the computation of the upper bounds; this can
be easily done in O(n log K) by iterating over the n variables and maintaining a heap with
K entries with the highest factor. We write Gu and Gu for the graphs in which only the
K variables that maximize respectively U B M B and U B LB are present; note that Gu might
be different from Gu .
We recall here that although only K variables are chosen, the graphs Gu and Gu are
completed with fake vertices in such a way to have an equal number of vertices on the two
vertex partitions. AsQin the lower bound graph, the given upper bound has to be scaled
down by a factor of dDX (u0d  ld0 )!. From Equation 5, the number of gcc solutions is
bounded from above by:

#gcc(X, l, u) 

U B(Gl ) min(U B M B (Gu ), U B LB (Gu ))
Q
0
0
0
dDX (ld !(ud  ld )!)

(6)

Scaling and also fake vertices used with the permanent bounds are factors that degrade
the quality of the upper bound. Nonetheless, solution densities are computed as a ratio
between two upper bounds therefore these scaling factors are often attenuated.
Example 3. We refer to the gcc described in Example 2. The exact number of solutions is
19. The U B M B and U B LB for the lower bound graph in Figure 2a are both 35 (the scaling
for the two fake value vertices is already considered). In the upper bound only 2 variables
need to be assigned and the one maximizing the bounds are x1 and x4 (or possibly x6 ): the
resulting permanent upper bound is 6. An upper bound on the total number of gcc solutions
0
0
is then b 356
4 c = 52 where the division by 4 is due to l2 ! = 2! and u3 ! = 2!.
Figure 3 shows the lower bound and residual upper bound graph for the same constraint
where x1 = 1 and domain consistency is achieved. Vertex x1 has been removed and l10 = 0
and u01 = 1. The graph Gl has a permanent upper bound of 6. The number of unassigned
variables in Gu is 2 and the ones maximizing the upper bounds are x4 and x6 , giving an
upper bound of 6. The total number of gcc solutions with x1 = 1 is then bounded above by
b 66
4 c = 9; the approximate solution density before normalizing it is thus 9/52. Note that
after normalization, it turns out to be about 0.18 whereas the exact computation of it is
5/19  0.26.

184

fiCounting-Based Search

x3

x3

x4

2

x4

x5

20

x5

x6

1

x6

3
30

(a)

(b)

Figure 3: Lower Bound Graph (a) and Residual Upper Bound Graph (b) assuming x1 = 1

5. Counting for Regular and Knapsack Constraints
The regular constraint is useful to express patterns that must be exhibited by sequences
of variables.
Definition 8 (Regular Language Membership Constraint). The regular(X, ) constraint
holds if the values taken by the sequence of finite domain variables X = hx1 , x2 , . . . , xk i spell
out a word belonging to the regular language defined by the deterministic finite automaton
 = (Q, , , q0 , F ) where Q is a finite set of states,  is an alphabet,  : Q    Q is
a partial transition function, q0  Q is the initial state, and F  Q is the set of final (or
accepting) states.
Linear equalities and inequalities are expressed as knapsack constraints.
Definition 9 (Knapsack Constraint). The knapsack(x, c, `, u) constraint holds if
`  cx  u
where c = (c1 , c2 , . . . , ck ) is an integer row vector, x is a column vector of finite domain
variables (x1 , x2 , . . . , xk )T with xi  Di , and ` and u are integers.
We assume that l and u are finite as they can always be set to the smallest and largest
value that cx can take. Strictly speaking to be interpreted as a knapsack, the integer values
involved (including those in the finite domains) should be nonnegative but the algorithms
proposed in this section can be easily adapted to lift the restriction of nonnegative coefficients and domain values, at the expense of a larger graph in the case of the algorithm of
Section 5.1. So we are dealing here with general linear constraints.
The filtering algorithms for the regular constraint and the knapsack constraint (when
domain consistency is enforced) are both based on the computation of paths in a layered
acyclic directed graph (Pesant, 2004; Trick, 2003). This graph has the property that paths
from the first layer to the last are in one-to-one correspondence with solutions of the constraint. An exact counting algorithm for the former constraint is derived by Zanarini and
185

fiPesant, Quimper, & Zanarini

Pesant (2009)  in the next section we describe an exact counting algorithm for knapsack
constraints which is similar in spirit, while in Section 5.2 we present an approximate counting algorithm attuned to bounds consistency. 6
5.1 Domain Consistent Knapsacks
We start from the reduced graph described by Trick (2003), which is a layered directed
graph G(V, A) with special vertex v0,0 and a vertex vi,b  V for 1  i  k and 0  b  u
whenever
i
X
 j  [1, i],  dj  Dj such that
cj dj = b
j=1

and
 j  (i, n],  dj  Dj such that `  b 

k
X

cj dj  u  b,

j=i+1

and an arc (vi,b , vi+1,b0 )  A whenever
 d  Di+1 such that ci+1 d = b0  b.
We define the following two recursions to represent the number of incoming and outgoing
paths at each node.
For every vertex vi,b  V , let #ip(i, b) denote the number of paths from vertex v0,0 to
vi,b :
#ip(0, 0) = 1
#ip(i + 1, b0 ) =

X

#ip(i, b),

0i<n

(vi,b ,vi+1,b0 )A

Let #op(i, b) denote the number of paths from vertex vi,b to a vertex vk,b0 with `  b0  u.
#op(n, b) = 1
#op(i, b) =

X

#op(i + 1, b0 ),

0i<k

(vi,b ,vi+1,b0 )A

The total number of paths (i.e. the solution count) is given by
#knapsack(x, c, `, u) = #op(0, 0)
in time linear in the size of the graph even though there may be exponentially many of
them. The solution density of variable-value pair (xi , d) is given by
P
(vi1,b ,vi,b+ci d )A #ip(i  1, b)  #op(i, b + ci d)
.
(xi , d, knapsack) =
#op(0, 0)
6. This was originally introduced by Pesant and Quimper (2008).

186

fiCounting-Based Search

b

8

6;1

7
6

1;3

1;1

3;1

6;1

2;2

3;2

5;1

3;2

5;1

5
4
3

1;10

1;4

2;2

2;4

3;1

2
1
0

1;3

1;22

1;9

1;2

0

1

2

3

4

i

Figure 4: Reduced graph for knapsack constraint 5  3x1 + x2 + 2x3 + x4  8 with D1 =
{0, 1, 2}, D2 = {0, 1, 3}, D3 = {0, 1, 2}, D4 = {1, 2}. Vertex labels represent the
number of incoming and outgoing paths.

value
0
1
2
3

x1
9/22
10/22
3/22


variable
x2
x3
8/22 9/22
8/22 7/22

6/22
6/22


x4

11/22
11/22


Table 1: Solution densities for the example of Fig. 4.
In Figure 4, the left and right labels inside each vertex give the number of incoming and
outgoing paths for that vertex, respectively. Table 1 reports the solution densities for every
variable-value pair.
The time required to compute recursions #ip() and #op() is related to the number of
arcs, which is in O(ku max1ik {|Di |}). Then each solution density computes a summation
over a subset of the arcs but each arc of the graph is involved in at most one such summation,
so the overall time complexity of computing every solution density is O(ku max1ik {|Di |})
as well.
5.2 Bounds Consistent Knapsacks
Knapsack constraints, indeed most arithmetic constraints, have traditionally been handled
by enforcing bounds consistency, a much cheaper form of inference. In some situations,
187

fiPesant, Quimper, & Zanarini

we may not afford to enforce domain consistency in order to get the solution counting
information we need to guide our search heuristic. Can we still retrieve such information,
perhaps not as accurately, from the weaker bounds consistency?
Consider the variable x with domain D = [a, b]. Each value in D is equiprobable. We
associate to x the discrete random variable X which follows a discrete uniform distribution
with probability mass function f (v), mean  = E[X], and variance  2 = V ar[X].

f (v) =
 =
2 =

1
ba+1

if a  v  b
0
otherwise
a+b
2
(b  a + 1)2  1
12

(7)
(8)
(9)

To find the distribution of a variable subject to a knapsack constraint, one needs to
find the distribution of a linear combination of uniformly distributed random variables.
Lyapunovs central limit theorem allows us to approximate the distribution of such a linear
combination.
Theorem 1 (Lyapunovs central limit theorem). Consider the independent random variables X1 , . . . , Xn . Let i be the mean of Xi , i2 be its variance, and ri3 = E[|Xi  i |3 ] be
its third central moment. If
P
1
( n r3 ) 3
lim Pni=1 i 1 = 0,
2 2
n (
i=1 i )
Pn
then
SP=
i=1 Xi follows a normal distribution with mean S =
Pn the random variable
n
2.
2 =


and
variance

i=1 i
i=1 i
S
The probability mass function of the normal distribution with mean  and variance  2
is the Gaussian function:
(x)2

(x) =

e 22

 2

(10)

Note that Lyapunovs central limit theorem does not assume that the variables are taken
from identical distributions. This is necessary since variables with different domains have
different distributions.
Lemma 1 defines an upper bound on the third central moment of the expression kX
where k is a positive coefficient and X is a uniformly distributed random variable.
Lemma 1. Let Y be a discrete random variable equal to kX such that k is a positive
coefficient and X is a discrete random variable uniformly distributed over the interval [a, b].
The third central moment r3 = E[|Y  E[Y ]|3 ] is no greater than k 3 (b  a)3 .
Proof. The case where a = b is trivial. We prove for b  a > 0. The proof involves simple
algebraic manipulations from the definition of the expectation.
188

fiCounting-Based Search

r

3

=

=

kb
X

|i  E[Y ]|3 f (i)

(11)

|kj  kE[X]|3 f (j)

(12)

i=ka
b
X
j=a

fi3
b fi
X
fi
fi
a
+
b
1
fij 
fi
= k
since k > 0
fi
fi
2
ba+1
j=a


a+b




b
2
3
3
X
k3
a+b 
X a + b
=
j +
j


ba+1
2
2
a+b
3

j=a

=

k3
ba+1

j=

 ba

ba
2
2
X
X

j3 +
j3
j=0

(13)

(14)

2

(15)

j=0

ba



2
2k 3 X
j 3 since b  a > 0
ba

j=0

189

(16)

fiPesant, Quimper, & Zanarini

Let m =

ba
2 .

r

3







m
k3 X 3
j
m
j=0


k3 1
1
1
4
3
2
(m + 1)  (m + 1) + (m + 1)
m 4
2
4
 4

3
3
2
k
m
m
m
+
+
m
4
2
4
 4

3
k
m
4
4
+m +m
since m  12
m
4
9 3 3
k m
4

Which confirms that r3 

9 3
32 k (b

(17)
(18)
(19)
(20)
(21)

 a)3  k 3 (b  a)3 .

Lemma 2 defines the distribution of a linear combination of uniformly distributed random variables.
P
Lemma 2. Let Y = ni=1 ci Xi be a random variable where Xi is a discrete random variable
uniformly chosen from the interval [ai , bi ] and ci is a non-negative coefficient.
P When n itends
to infinity, the distribution of Y tends to a normal distribution with mean ni=1 ci ai +b
and
2
Pn 2 (bi ai +1)2 1
variance i=1 ci
.
12
Proof. Let Yi = ci Xi be a random variable. We want to characterize the distribution of
P
n
bi ai
i=1 Yi . Let mi =
2 . The variance of the uniform distribution over the interval [ai , bi ]
2

(m + 1 )2

+1) 1
1
is i2 = (bi ai12
= i 3 2  12
. We have V ar[Yi ] = c2i V ar[Xi ] = c2i i2 . Let ri3 be the
third central moment of Yi . By Lemma 1, we have ri3  c3i (bi  ai )3 . Let L be the term
mentioned in the condition of Lyapunovs central limit theorem:

Pn
L =

lim

3
i=1 ri

n

Pn

1

2 2
i=1 ci i

3

1

(22)

2

Note that the numerator and the denominator of the fraction are non-negative. This
implies that L itself is non-negative. We prove that L  0 as n tends to infinity.
190

fiCounting-Based Search

Approximation of a Combination of Uniformly Distributed Random Variables
0.045
0.04
0.035

density

0.03
0.025
0.02
0.015
0.01
0.005
0
0

5

10

15

20

25
x

30

35

40

45

50

Figure 5: The histogram is the actual distribution of the expression 3x+4y +2z for x, y, z 
[0, 5]. The curve is the approximation given by the Gaussian curve with mean
 = 22.5 and variance  2 = 84.583.

Pn
L 

3 3
i=1 8ci mi

lim
n P
n

2
i=1 ci



lim

n

8
1
3



(mi + 12 )2
3

Pn

1
3

Pn

1
2

3
i=1 ci mi

2
i=1 ci mi

1
3



1
12

 12

(23)

3

(24)

2

v
u Pn 3 3 2
 u
i=1 ci mi
6
 lim 2 3 t
Pn 2 2 3
n
i=1 ci mi
s
Pn Pn
3
 6
j=1 (ci cj mi mj )
i=1
 lim 2 3 Pn Pn Pn
2
n
i=1
j=1
k=1 (ci cj ck mi mj mk )

(25)

(26)

Note that in the last inequality, the terms (ci cj mi mj )3 and (ci cj ck mi mj mk )2 are of the
same order. However, there are n times more terms in the denominator than the numerator.
Therefore, when n tends to infinity, the fraction tends to zero which proves that L = 0 as
n tends to zero.
Pn
By Lyapunovs central limit theorem, as n tends
to
infinity,
the
expression
Y
=
i=1 Yi
Pn
Pn
ai +bi
tends to a normal distribution with mean E[Y ] = i=1 ci E[Xi ] = i=1 ci 2 and variance
P
P
+1)2 1
V ar[Y ] = ni=1 c2i V ar[Xi ] = ni=1 c2i (bi ai12
.
P
Consider the knapsack constraint `  ni=1 ci xi  u. Let xn+1 be a variable with domain
P
Pn
Dn+1 = [`, u]. We obtain xj = c1j (xn+1  j1
i=j+1 ci xi ). Some coefficients in
i=1 ci xi 
this expression might be negative. They can be made positive by setting c0i = ci and
Di0 = [ max(Di ),  min(Di )]. When n grows to infinity, the distribution of xj tends to
a normal distribution as stated in Lemma 2. In practice, the normal distribution is a
191

fiPesant, Quimper, & Zanarini

good estimation even for small values of n. Figure 5.2 shows the actual distribution of the
expression 3x + 4y + 2z for x, y, z  [0, 5] and its approximation by a normal distribution.
Given a variable xi subject to a knapsack constraint, Algorithm 2 returns the assignment
xi = ki with the highest solution density. The for loop computes the average mean j and
the variance j2 of the uniform distribution associated to each variable
Pn xj . Lines 4 and 5
compute the mean and the variance of the distribution of xn+1  j=1 cj xj while Lines 6
P
Pn
and 7 compute the mean and the variance of xi = c1i (xn+1  i1
j=1 cj xj 
j=i+1 cj xj ).
Since this normal distribution is symmetric and unimodal, the most likely value ki in the
domain Di is the one closest to the mean i . The algorithm finds and returns this value as
well as its density di . The density di is computed using the normal distribution. Since the
variable xi must be assigned to a value in its domain, the algorithm normalizes on Line 9
the distribution over the values in the interval [min(Di ), max(Di )].
1
2
3
4
5
6
7
8
9

for j  [1, n] do
min(Dj )+max(Dj )
j 
;
2

(max(D )min(D )+1)2 1

j
j
;
j2 
12
Pn
l+u
M  2  j=1 cj j ;
2 1
P
V  (ul+1)
+ nj=1 c2j j2 ;
12
i i
m  M +c
;
ci

v

V c2i i2
;
c2i

ki  arg minkDi |k  m|;
(ki m)2
(km)2
Pmax(Di )
 2v
di  e 2v / k=min(D
e
;
i)

return hxi = ki , di i
Algorithm 2: P
xi = ki with the highest density as well as its density di for knapsack
constraint `  ni=1 ci xi  u.

10

Lines 1 through 5 take O(n) time to execute. Line 8 depends on the data structure
used by the solver to encode a domain. We assume that the line takes O(log |Di |) time to
execute. The summation on Line 9 can be computed in constant time by approximating
the summation with m,v (max(Di ) + 12 )  m,v (min(Di ) + 12 ) where m,v is the normal
cumulative distribution function with average m and variance v. The constant 12 is added for
the continuity correction. Other lines have a constant running time. The total complexity
of Algorithm 2 is therefore O(n + log |Di |). Note that Line 1 to Line 5 do not depend
on the value of i. Their computation can therefore be cached for subsequent calls to the
function over the same knapsack constraint. Using this technique, finding theP
variable xi 
{x1 , . . . , xn } which has an assignment xi = ki of maximum density takes O( ni=1 log |Di |)
time.
A source of alteration of the distribution are values in the interval which are absent
from the actual domain. Bounds consistency approximates the domain of a variable with
its smallest covering interval. In order to reduce the error introduced by this approximation,
one can compute the actual mean and actual variance of a domain Di on Lines 2 and 3
192

fiCounting-Based Search

instead
P of using the mean and the variance of the covering interval, at a revised overall cost
of O( ni=1 |Di |).

6. Generic Constraint-Centered Counting-based Heuristics
The previous sections provided algorithms to retrieve solution counting information from
many of the most frequently used constraints. That information must then be exploited
to guide search. The solving process alternates between propagating constraints to filter
domains and branching by fixing a variable to a value in its domain. The crucial choice of
variable and value is made through a search heuristic. We considered many search heuristics
based on counting information, which we describe briefly in the next paragraph. We will
experiment extensively with one of the most successful ones in Section 7, so we present it
in more detail. In the following, we denote by C(xi ) the set of constraints whose scope
contains the variable xi . All the heuristics proposed assume a lexicographical ordering as
tie breaking. Counting information is gathered at a search tree node once a propagation
fixed point is reached: it is recomputed only on constraints for which a change occurred to
the domain of a variable within its scope, and otherwise cached information is reused. That
cached counting information is stored in trailing data structures (also known as reversible
data structures) so that it can be retrieved upon backtracking. The heuristics considered
fall into four broad categories:
Combined choice of variable and value Those that select directly a variable-value pair
without an explicit differentiation of variable and value ordering, based on the aggregation,
through simple functions, of the counting information coming from different constraints.
Such heuristics iterate over each variable-value pair, aggregating solution densities from the
relevant constraints and selecting the pair exhibiting the maximum aggregated score. The
type of aggregation used is e.g. the maximum, minimum, sum, or average. For instance:
 maxSD: maxcC(xi ) ((xi , d, c))  selects the maximum of the solution densities.
 maxRelSD: maxcC(xi ) ((xi , d, c)  (1/|Di |))  selects the maximum of the solution
densities subtracting the average solution density for that given variable (i.e. 1/|Di |).
It smoothes out the inherent solution densities differences due to domain cardinalities
(as also the following aggregation function).
i ,d,c)
 maxRelRatio: maxcC(xi ) ( (x
(1/|Di |) )  selects the maximum of the ratio between the
solution density and the average solution density for that given variable.

P

 aAvgSD:

(xi ,d,c)
|C(xi )|

cC(xi )

P

 wSCAvg:

(#c(xi ,d,c))
cC(x ) #c

cC(xi )

P

 computes the arithmetic average of the solution densities.
 computes the average of the solution densities weighted

i

by the constraints solution count. The weights tend to favor branchings on variablevalue pairs that keep a high percentage of solutions on constraints with a high solution
count.
193

fiPesant, Quimper, & Zanarini

Choice of constraint first Those that focus first on a specific constraint (e.g. based on
its solution count) and then select a variable-value pair (as before) among the variables in
the preselected constraints scope. For instance, minSCMaxSD first selects the constraint
with the lowest number of solutions and then restricts the choice of variable to those involved
in this constraint, choosing the variable-value pair with the highest solution density. The
rationale behind this heuristic is that the constraint with the fewest solutions is probably
among the hardest to satisfy.
Restriction of variables Those that preselect a subset of variables with minimum domain size and then choose among them the one with the best variable-value pair according
to counting information.
Choice of value only Those using some other generic heuristic for variable selection and
solution densities for value selection.
Heuristic maxSD The heuristic maxSD (Algorithm 3) simply iterates over all the variablevalue pairs and chooses the one that has the highest density; assuming that the (xi , d, c)
are precomputed, the complexity of the algorithm is O(qm) where q is the number of
constraints and m is the sum of the cardinalities of the variables domains. Interestingly,
such a heuristic likely selects a variable with a small domain, in keeping with the fail-first
principle, since its values have on average a higher density compared to a variable with
many values (consider that the average density of a value is (xi , d, c) = |D1i | ). Note that
each constraint is considered individually.
1
2
3
4
5
6
7
8

max = 0;
for each constraint c(x1 , . . . , xk ) do
for each unbound variable xi  {x1 , . . . , xk } do
for each value d  Di do
if (xi , d, c) > max then
(x? , d? ) = (xi , d);
max = (xi , d, c);
return branching decision x? = d? ;
Algorithm 3: The Maximum Solution Density search heuristic (maxSD)

7. Experimental Analysis
We performed a thorough experimental analysis in order to evaluate the performance of the
proposed heuristics on eight different problems.7 All the problems expose sub-structures
that can be encapsulated in global constraints for which counting algorithms are known.
Counting-based heuristics are of no use for random problems as this class of problems do not
expose any structure; nonetheless real-life problems usually do present structure therefore
the performance of the heuristics proposed may have a positive impact in the quest to
provide generic and efficient heuristics for structured problems. The problems on which
we experimented have different structures and different constraints with possibly different
7. The instances we used are available at www.crt.umontreal.ca/quosseca/fichiers/20-JAIRbenchs.tar.gz.

194

fiCounting-Based Search

arities interconnected in different ways; thus, they can be considered as good representatives
of the variety of problems that may arise in real life.
7.1 Quasigroup Completion Problem with Holes (QWH)
Also referred to as the Latin Square problem, the QWH is defined on a n  n grid whose
squares each contain an integer from 1 to n such that each integer appears exactly once per
row and column (problem 3 of the CSPLib maintained in Gent, Walsh, Hnich, & Miguel,
2009). The most common model uses a matrix of integer variables and an alldifferent
constraint for each row and each column. So each constraint is defined on n variables and
is of the same type; each variable is involved in two constraints and has the same domain
(disregarding the clues). This is a very homogeneous problem. We tested on the 40 hard
instances used by Zanarini and Pesant (2009) with n = 30 and 42% of holes (corresponding
to the phase transition), generated following Gomes and Shmoys (2002).
7.2 Magic Square Completion Problem
The magic square completion problem (problem 19 of CSPLib) is defined on a n  n grid
and asks to fill the square with numbers from 1 to n2 such that each row, each column and
each main diagonal sums up to the same value. In order to make them harder, the problem
instances have been partially prefilled (half of the instances have 10% of the variables
set and the other half, 50% of the variables set). The 40 instances (9  9) are taken
from the work of Pesant and Quimper (2008). This problem is modeled with a matrix
of integer variables, a single alldifferent constraint spanning over all the variables and
a knapsack constraint for each row, column and main diagonal. The problem involves
different constraints although the majority are equality knapsack with the same arity.
7.3 Nonograms
A Nonogram (problem 12 of CSPLib) is built on a rectangular nm grid and requires filling
in some of the squares in the unique feasible way according to some clues given on each row
and column. As a reward, one gets a pretty monochromatic picture. Each individual clue
indicates how many sequences of consecutive filled-in squares there are in the row (column),
with their respective size in order of appearance. For example, 2 1 5 indicates that there
are two consecutive filled-in squares, then an isolated one, and finally five consecutive ones.
Each sequence is separated from the others by at least one blank square but we know little
about their actual position in the row (column). Such clues can be modeled with regular
constraints. This is a very homogeneous problem, with constraints of identical type defined
over m or n variables, and with each (binary) variable involved in two constraints. These
puzzles typically require some amount of search, despite the fact that domain consistency
is maintained on each clue. We experimented with 180 instances8 of sizes ranging from
16  16 to 32  32.

8. Instances taken from http://www.blindchicken.com/ali/games/puzzles.html

195

fiPesant, Quimper, & Zanarini

7.4 Multi Dimensional Knapsack Problem
The Multi dimensional knapsack problem was originally proposed as an optimization problem by the OR community. We followed the same approach as Refalo (2004) in transforming
the optimization problem into a feasibility problem by fixing the objective function to its optimal value, thereby introducing a 0-1 equality knapsack constraint. The other constraints
are upper bounded knapsack constraints on the same variables. We tested on three different set of instances for a total of 25 instances: the first set corresponds to the six instances
used by Refalo, the second set and the third set come from the OR-Library (Weish[1-13]
from Shi, 1979; PB[1,2,4] and HP[1,2] from Freville & Plateau, 1990). The first instance set
have n, that is the number of variables, ranging from 6 to 50 and m, that is the number of
constraints, from 5 to 10; in the second and third instance set n varies from 27 to 60 and
m from 2 to 5. The problem involves only one kind of constraint and, differently from the
previous problem classes, all the constraints are posted on the same set of variables.
7.5 Market Split Problem
The market split problem was originally introduced by Cornuejols and Dawande (1999)
as a challenge to LP-based branch-and-bound approaches. There exists both a feasibility
and optimization version. The feasibility problem consists of m 0-1 equality knapsack
constraints defined on the same set of 10(m1) variables. Even small instances (4  m  6)
are surprisingly hard to solve by standard means. We used the 10 instances tested by Pesant
and Quimper (2008) that were generated by Wassermann (2007). The Market Split Problem
shares some characteristics with the Multi Dimensional Knapsack problem: the constraints
are of the same type and they are posted on the same set of variables.
7.6 Rostering Problem
The rostering problem was inspired by a rostering context. The objective is to schedule
n employees over a span of n time periods. In each time period, n  1 tasks need to be
accomplished and one employee out of the n has a break. The tasks are fully ordered 1 to
n  1; for each employee the schedule has to respect the following rules: two consecutive
time periods have to be assigned to either two consecutive tasks (in no matter which order
i.e. (t, t + 1) or (t + 1, t)) or to the same task (i.e. (t, t)); an employee can have a break after
no matter which task; after a break an employee cannot perform the task that precedes the
task prior to the break (i.e. (t, break, t1) is not allowed). The problem is modeled with one
regular constraint per row and one alldifferent constraint per column. We generated 2
sets of 30 instances with n = 10 each with 5% preset assignments and respectively 0% and
2.5% of values removed.
7.7 Cost-Constrained Rostering Problem
The cost-constrained rostering problem was borrowed from Pesant and Quimper (2008)
and the 10 instances as well. It is inspired by a rostering problem where m employees
(m = 4) have to accomplish a set of tasks in a n-day schedule (n = 25). No employee can
perform the same task as another employee on the same day (alldifferent constraint on
each day). Moreover, there is an hourly cost for making someone work, which varies both
196

fiCounting-Based Search

across employees and days. For each employee, the total cost must be equal to a randomly
generated value (equality knapsack constraint for each employee). Finally, each instance
has about 10 forbidden shifts i.e. there are some days in which an employee cannot perform
a given task. In the following, we refer to this problem also as KPRostering. This problem
presents constraints of different types that have largely different arities.
7.8 Traveling Tournament Problem with Predefined Venues (TTPPV)
The TTPPV was introduced by Melo, Urrutia, and Ribeiro (2009) and consists of finding
an optimal single round robin schedule for a sport event. Given a set of n teams, each
team has to play against every other team. In each game, a team is supposed to play
either at home or away, however no team can play more than three consecutive times at
home or away. The particularity of this problem resides on the venue of each game that
is predefined, i.e. if team a plays against b it is already known whether the game is going
to be held at as home or at bs home. A TTPPV instance is said to be balanced if the
number of home games and the number of away games differ by at most one for each team;
otherwise it is referred to as non-balanced or random. The problem is modeled with one
alldifferent and one regular constraint per row and one alldifferent constraint per
column. The TTPPV was originally introduced as an optimization problem where the sum
of the traveling distance of each team has to be minimized, however Melo et al. (2009)
show that it is particularly difficult to find a single feasible solution employing traditional
integer linear programming methods. Balanced instances of size 18 and 20 (the number of
teams denotes the instance size) were taking from roughly 20 to 60 seconds to find a first
feasible solution with Integer Linear Programming; non-balanced instances could take up to
5 minutes (or even time out after 2 hours of computation). Furthermore six non-balanced
instances are infeasible but the ILP approach proposed by Melo et al. were unable to prove
it. Hence, the feasibility version of this problem already represents a challenge.
For every problem (unless specified otherwise): domain consistency is maintained during
search9 , the counting algorithm for the alldifferent constraint is UB-FC (upper bounds
with forward checking as the consistency level enforced), the search tree is binary (i.e.
xi = j  xi 6= j), and traversed depth-first. All tests were performed on a AMD Opteron
2.2GHz with 1GB and Ilog Solver 6.6; the heuristics that involve some sort of randomization
(either in the heuristic itself or in the counting algorithms employed) have been run 10 times
and the average of the results has been taken into account. We set a timeout of 20 minutes
for all problems and heuristics. We present the results by plotting the percentage of solved
instances against time or backtracks.
7.9 Comparing Counting-Based Search Heuristics
We first compare several of the proposed search heuristics based on counting with respect
to how well they guide search, measured as the number of backtracks required to find a
solution. The important issue of overall runtime will be addressed in the following sections.
9. Even for knapsack constraints, comparative experimental results on the same benchmark instances,
originally reported by Pesant and Quimper (2008), indicated that maxSD performed better with domain
consistency and the associated counting algorithm.

197

fiPesant, Quimper, & Zanarini

Figure 6: Percentage of solved instances with respect to the number of backtracks for the
eight benchmark problems. The search heuristics compared are all based on
solution counting.

198

fiCounting-Based Search

Figure 6 plots the number of solved instances against backtracks for our eight benchmark problems. On the Nonogram, Multi-Knapsack, and Market Split problems, maxSD,
maxRelSD, and maxRelRatio correspond to the same heuristics because domains are binary.
Restricting the use of solution densities to the choice of a value once the variable has been selected by the popular domain size over dynamic degree heuristic (domDeg;maxSD) generally
achieves very poor performance compared to the others. One disappointment which came as
a surprise is that selecting first the constraint with the fewest solutions left (minSCMaxSD)
often behaves poorly as well. For the Multi-Knapsack Problem aAvgSD, which takes the
arithmetic average of the solution densities, performs about one order of magnitude better
than the others. We believe that this might be explained by the fact that all the constraints
share the same variables (in the Latin Square and Nonogram problems constraints overlap
on only one variable): therefore branching while considering all the constraint information
pays off. The maxSD and maxRelSD search heuristics stand out as being more robust on
these benchmarks. They are quite similar but each performs significantly better than the
other on one problem domain. Because it is slightly simpler, we will restrict ourselves to
the former in the remaining experiments.
7.10 Comparing with Other Generic Search Heuristics
The experimental results of the previous section suggest that the relatively simple maxSD
heuristic guides search at least as well as any of the others. We now compare it to the
following ones (see Section 2 as a reference) which are good representatives of the state of
the art for generic search heuristics:
 dom - it selects among the variables with smallest remaining domain uniformly at
random and then chooses a value uniformly at random;
 domWDeg - it selects the variable according to the dom/wdeg heuristic and then the
first value in lexicographic order;
 IBS - Impact-based Search with full initialization of the impacts; it chooses a subset
of 5 variables with the best approximated impact and then it breaks ties based on the
node impacts while further ties are broken randomly; (ILOG, 2005)
Figure 7 and 8 plot the number of solved instances against backtracks and time for our
eight benchmark problems. For the moment we ignore the curves for the heuristics with
restarts.
The maxSD heuristic significantly outperforms the other heuristics on the Latin Square,
Magic Square, Multi Dimensional Knapsack, Cost-Constrained Rostering (KPRostering in
the figure), and TTPPV problems (5 out of 8 problems), both in terms of number of
backtracks and computation time. For the Nonogram Problem it is doing slightly worse
than domWDeg and is eventually outperformed by IBS. The sharp improvement of the latter
around 1000 backtracks suggests that singleton consistency is very powerful for this problem
and not too time consuming since domains are binary. Indeed IBSs full initialization of
the impacts at the root node achieves singleton consistency as a preprocessing step. This
behavior is even more pronounced for the Rostering Problem (see the IBS curves). On that
problem maxSDs performance is more easily compared to domWDeg, which dominates it.
199

fiPesant, Quimper, & Zanarini

Figure 7: Percentage of solved instances with respect to the number of backtracks and to
time (in seconds) for the first four benchmark problems. The search heuristics
compared are maxSD, dom, IBS, and domWDeg, both with and without restarts.

200

fiCounting-Based Search

Figure 8: Percentage of solved instances with respect to the number of backtracks and to
time (in seconds) for the last four benchmark problems. The search heuristics
compared are maxSD, dom, IBS, and domWDeg, both with and without restarts.

201

fiPesant, Quimper, & Zanarini

For the Market Split Problem the differences in performance are not as striking: maxSD
is doing slightly better in terms of backtracks but not enough to outperform domWDeg in
terms of runtime.
For the Magic Square plot against time, there is a notable bend at about the 50% mark
in most of the curves which can be explained by the fact that half of the instances only
have 10% of their cells prefilled and present a bigger challenge. Interestingly, the simpler
dom heuristics performs better than IBS and domWDeg, the latter being unable to solve
half the instances in the allotted time. In contrast with the Nonogram Problem, here the
full impact initialization is a very heavy procedure due to the high number of variable-value
pairs to probe ( n4 that is in our instances 94 = 6561). It is also worth noting that
on the Cost-Constrained Rostering Problem, maxSD solves seven out of the ten instances
backtrack-free and is the only heuristic solving every instance. Similarly for the TTPPV
Problem, almost 90% of the instances are solved backtrack-free by that heuristic. Moreover
six instances happen to be infeasible and maxSD exhibits short proof trees for five of them,
every other heuristic timing out on them.
7.11 Adding Randomized Restarts
It has been remarked that some combinatorial search has a strictly positive probability to
reach a subtree that requires exponentially more time than the other subtrees encountered
so far (so called heavy-tail behavior). Nonetheless, heavy tails can be largely avoided
by adding randomized restarts on top of the search procedure (Gomes, Selman, & Kautz,
1998). This technique is orthogonal to the search heuristic employed and it systematically
restarts the search every time a limit (typically a bound on the number of backtracks) is
reached; obviously, in order to be effective, randomized restarts must be employed along
with a heuristic that presents some sort of randomization or learning such that at each
restart different parts of the search tree are explored. We tested the same heuristics to
assess their performance with randomized restarts. The maxSD and IBS heuristics have
been randomized: particularly, one variable-value pair is chosen at random with equal
probability between the best two provided by the heuristic. Note that, as pointed out by
Refalo (2004), impact information is carried over different runs to improve the quality of the
impact approximation. As for domWDeg, the learned weights are kept between restarts. We
implemented a slow geometric restart policy (Walsh, 1999) (that is 1, r, r2 , . . . with r = 2)
with a scale parameter optimized experimentally and separately for each problem type and
search heuristic.
We turn again to Figure 7 and 8 but this time we also consider the curves for the
heuristics with restarts. Restarts generally help a less informed heuristic such as dom,
sometimes spectacularly so as for the Rostering Problem, but not always as indicated by
the results on the Market Split Problem. For the other heuristics their usefulness is mixed: it
makes little difference for maxSD except for Market Split where it degrades performance and
Rostering where it improves its performance very significantly, now solving every instance
very easily; for IBS it helps on the most difficult instances for half of the problems but for
three others it degrades performance; for domWDeg it is generally more positive but never
spectacular. Note that heavy-tail behavior of runtime distribution is conjectured to depend
both on the problem structure and on the search heuristic employed (Hulubei & OSullivan,
202

fiCounting-Based Search

2006). The Market Split Problem stands out as one where randomized restarts hurt every
search heuristic considered.
7.12 Using Limited Discrepancy Search
Another way to avoid heavy tails is to change the order in which the search tree is traversed,
undoing decisions made at the top of the search tree earlier in the traversal. A popular
way of doing this is by applying limited discrepancy search (LDS) that visits branches in
increasing order of their number of discrepancies, which correspond to branching decisions
going against the search heuristic (Harvey & Ginsberg, 1995). As for restarts, it can be
combined with any search heuristic and may cause dramatic improvements in some cases
but this less natural traversal comes with a price. Figure 9 illustrates the impact of LDS
on two of our benchmark problems, using maxSD as the search heuristic. Either the usual
depth-first search traversal is used (maxSD curve) or limited discrepancy search, grouping
branches that have exactly the same number of discrepancies (LDS 1), by skips of 2 (LDS
2), or by skips of 4 (LDS 4) discrepancies. On the rostering problem LDS undoes bad
early decisions made by our heuristic and now allows us to solve every instance very quickly.
However on the Magic Square problem the impact of LDS on the number of backtracks is
low and it actually significantly slows down the resolution because LDS must revisit internal
nodes, thus repeating propagation steps: the smaller the skip, the larger the computational
penalty.
The same behavior could have been observed on other search heuristics and other problems. So LDS does not necessarily add robustness to our search.
7.13 Analyzing Variable and Value Selection Separately
One may wonder whether the success of counting-based search heuristics mostly depends
on informed value selection, the accompanying variable selection being accessory. In order
to investigate this, we introduce some hybrid heuristics:
 maxSD; random - selects a variable as in maxSD but then selects a value in its domain
uniformly at random;
 IBS; maxSD - selects a variable as in IBS but then selects a value in its domain
according to solution densities;
 domWDeg; maxSD - selects a variable as in domWDeg but then selects a value in its
domain according to solution densities;
Figure 10 and 11 plot the number of solved instances against backtracks and time for
our eight benchmark problems. Comparing maxSD and maxSD; random indicates that most
of the time value selection according to solution densities is crucial, the Rostering Problem
being an exception. Interestingly value selection by solution density improves the overall
performance of IBS; for domWDeg it improves for the Latin Square and Magic Square
problems but not for the rest, often decreasing performance. However such improvements
do not really tip the balance in favor of other heuristics than maxSD, thus indicating that
variable selection according to solution densities is also very important to its success.
203

fiPesant, Quimper, & Zanarini

Figure 9: Percentage of solved instances with respect to the number of backtracks and to
time (in seconds) for two of the benchmark problems. The maxSD search heuristic
is used for every curve but the search tree traversal order is different.

204

fiCounting-Based Search

Figure 10: Percentage of solved instances with respect to the number of backtracks and to
time (in seconds) for the first four benchmark problems. The search heuristics
compared use solution densities either for variable or for value selection.

205

fiPesant, Quimper, & Zanarini

Figure 11: Percentage of solved instances with respect to the number of backtracks and to
time (in seconds) for the last four benchmark problems. The search heuristics
compared use solution densities either for variable or for value selection.

206

fiCounting-Based Search

8. Conclusion
This paper described and evaluated counting-based search to solve constraint satisfaction
problems. We presented some algorithms necessary to extract counting information from
several of the main families of constraints in cp. We proposed a variety of heuristics based
on that counting information and evaluated them. We then compared one outstanding
representative, maxSD, to the state of the art on eight different problems from the literature
and obtained very encouraging results. The next logical steps in this research include
designing counting algorithms for some of the other common constraints and strengthening
our empirical evaluation by considering new problems and comparing against applicationspecific heuristics. The next two paragraphs describe less obvious steps.
Users often need to introduce auxiliary variables or different views of the models that are
linked together by channeling constraints. It is very important to provide all the counting
information available at the level of the branching variables or at least at some level where
direct comparison of solution densities is meaningful. For example in the case of the TTPPV
an earlier model, in which two sets of variables each received solution densities from different
constraints, did not perform nearly as well. Channeling constraints that express a one-tomany relation (such as the one present in the TTPPV) can be dealt with by considering value
multiplicity in counting algorithms (Pesant & Zanarini, 2011). More complex channeling
constraints represent however a limitation in the current framework.
Combinatorial optimization problems have not been discussed in this paper but are very
important in operations research. Heuristics with a strong emphasis on feasibility (such as
counting-based heuristics) might not be well suited for problems with a strong optimization
component, yet may be very useful when dealing with optimization problems that involve
hard combinatorics. Ideally, counting algorithms should not be blind to cost reasoning.
One possibility that we started investigating not only counts the number of solutions that
involve a particular variable-value pair but also returns the average cost of all the solutions
featuring that particular variable-value pair. Another has shown promise when the cost is
linear and decomposable on the decision variables (Pesant & Zanarini, 2011).
To conclude, we believe counting-based search brings us closer to robust automated
search in cp and also offers efficient building blocks for application-specific heuristics.

Acknowledgments
Financial support for this research was provided in part by the Natural Sciences and Engineering Research Council of Canada and the Fonds quebecois de la recherche sur la nature
et les technologies. We wish to thank Tyrel Russell who participated in the implementation
and experimentation work. We also thank the anonymous referees for their constructive
comments that allowed us to improve our paper.

References
Alon, N., & Friedland, S. (2008). The Maximum Number of Perfect Matchings in Graphs
with a Given Degree Sequence. The Electronic Journal of Combinatorics, 15 (1), N13.
207

fiPesant, Quimper, & Zanarini

Balafoutis, T., & Stergiou, K. (2008a). Experimental evaluation of modern variable selection
strategies in Constraint Satisfaction Problems. In Proceedings of the Fifteenth Knowledge Representation and Automated Reasoning Workshop on Experimental Evaluation
of Algorithms for Solving Problems with Combinatorial Explosion, RCRA-08.
Balafoutis, T., & Stergiou, K. (2008b). On Conflict-driven variable ordering heuristics.
In Proceedings of Thirteenth Annual ERCIM International Workshop on Constraint
Solving and Constraint Logic Programming, CSCLP-08.
Bessiere, C., & Regin, J.-C. (1996). MAC and Combined Heuristics: Two Reasons to Forsake FC (and CBJ?) on Hard Problems. In Proceedings of the Second International
Conference on Principles and Practice of Constraint Programming, CP-96, Vol. 1118
of LNCS, pp. 6175. Springer Berlin / Heidelberg.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004). Boosting Systematic Search
by Weighting Constraints. In Proceedings of the Sixteenth Eureopean Conference on
Artificial Intelligence, ECAI-04, pp. 146150. IOS Press.
Bras, R. L., Zanarini, A., & Pesant, G. (2009). Efficient Generic Search Heuristics within
the EMBP framework. In Proceedings of the Fifteenth International Conference on
Principles and Practice of Constraint Programming, CP-04, Vol. 5732 of LNCS, pp.
539553. Springer.
Bregman, L. M. (1973). Some Properties of Nonnegative Matrices and their Permanents.
Soviet Mathematics Doklady, 14 (4), 945949.
Brelaz, D. (1979). New Methods to Color the Vertices of a Graph. Communications of the
ACM, 22 (4), 251256.
Cornuejols, G., & Dawande, M. (1999). A Class of Hard Small 0-1 Programs. INFORMS
Journal of Computing, 11, 205210.
Freville, A., & Plateau, G. (1990). A Branch and Bound Method for the Multiconstraint
Zero One Knapsack Problem.. Investigation Operativa, 1, 251270.
Friedland, S. (2008). An Upper Bound for the Number of Perfect Matchings in Graphs.
http://arxiv.org/abs/0803.0864.
Gent, I. P., Walsh, T., Hnich, B., & Miguel, I. (2009). A Problem Library for Constraints.
http://www.csplib.org. last consulted 2009-08.
Gomes, C., Selman, B., & Kautz, H. (1998). Boosting Combinatorial Search Through
Randomization. In Proceedings of the fifteenth national/tenth conference on Artificial
intelligence/Innovative applications of artificial intelligence, AAAI-98/IAAI-98, pp.
431437. AAAI Press.
Gomes, C., & Shmoys, D. (2002). Completing Quasigroups or Latin Squares: A Structured
Graph Coloring Problem.. In Proceedings of Computational Symposium on Graph
Coloring and Generalizations, COLOR-02, pp. 2239.
Grimes, D., & Wallace, R. J. (2006). Learning to Identify Global Bottlenecks in Constraint
Satisfaction Search. In Learning for Search: Papers from the AAAI-06 Workshop, Vol.
Tech. Rep. WS-06-11, pp. 2431.
208

fiCounting-Based Search

Grimes, D., & Wallace, R. J. (2007). Sampling Strategies and Variable Selection in Weighted
Degree Heuristics. In Proceedings of the Thirteenth International Conference on Principles and Practice of Constraint Programming, CP-07, Vol. 4741 of LNCS, pp. 831
838. Springer.
Haralick, R. M., & Elliott, G. L. (1980). Increasing Tree Seach Efficiency for Constraint
Satisfaction Problems. Artificial Intelligence, 14, 263313.
Harvey, W. D., & Ginsberg, M. L. (1995). Limited Discrepancy Search. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence, IJCAI-95,
pp. 607615. Morgan Kaufmann.
Hsu, E. I., Kitching, M., Bacchus, F., & McIlraith, S. A. (2007). Using Expectation Maximization to Find Likely Assignments for Solving CSPs. In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence, pp. 224230. AAAI Press.
Hulubei, T., & OSullivan, B. (2006). The Impact of Search Heuristics on Heavy-Tailed
Behaviour. Constraints, 11 (2-3), 159178.
ILOG (2005). ILOG Solver 6.1 Users Manual. page 378.
Jurkat, W., & Ryser, H. J. (1966). Matrix Factorizations of Determinants and Permanents.
Journal of Algebra, 3, 127.
Kask, K., Dechter, R., & Gogate, W. (2004). Counting-Based Look-Ahead Schemes for
Constraint Satisfaction. In Springer-Verlag (Ed.), Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming, CP-04, Vol.
LNCS 3258, pp. 317331.
Liang, H., & Bai, F. (2004). An Upper Bound for the Permanent of (0,1)-Matrices. Linear
Algebra and its Applications, 377, 291295.
Melo, R., Urrutia, S., & Ribeiro, C. (2009). The traveling tournament problem with predefined venues. Journal of Scheduling, 12 (6), 607622.
Minc, H. (1963). Upper Bounds for Permanents of (0, 1)-matrices. Bulletin of the American
Mathematical Society, 69, 789791.
Pesant, G. (2004). A Regular Language Membership Constraint for Finite Sequences of Variables. In Proceedings of the Tenth International Conference on Principles and Practice
of Constraint Programming, CP-04, Vol. 3258 of LNCS, pp. 482495. Springer.
Pesant, G. (2005). Counting solutions of csps: A structural approach. In Proceedings of the
Nineteenth International Joint Conference on Artificial Intelligence, IJCAI-05, pp.
260265.
Pesant, G., & Quimper, C.-G. (2008). Counting solutions of knapsack constraints. In Proceedings of the Fifth International Conference on Integration of AI and OR Techniques
in Constraint Programming for Combinatorial Optimization Problems, CPAIOR-08,
pp. 203217.
Pesant, G., & Zanarini, A. (2011). Recovering indirect solution densities for counting-based
branching heuristics. In Achterberg, T., & Beck, J. C. (Eds.), CPAIOR, Vol. 6697 of
Lecture Notes in Computer Science, pp. 170175. Springer.
209

fiPesant, Quimper, & Zanarini

Quimper, C., Lopez-Ortiz, A., van Beek, P., & Golynski, A. (2004). Improved algorithms for
the global cardinality constraint. In Proceedings of the Tenth International Conference
on Principles and Practice of Constraint Programming, CP-04, Vol. LNCS 3258, pp.
542556. Springer.
Refalo, P. (2004). Impact-Based Search Strategies for Constraint Programming. In Proceedings of the Tenth International Conference on Principles and Practice of Constraint
Programming, CP-04, Vol. LNCS 3258, pp. 557571. Springer.
Regin, J.-C. (1994). A Filtering Algorithm for Constraints of Difference in CSPs. In
Proceedings of the Twelfth National Conference on Artificial Intelligence, AAAI-94,
Vol. 1, pp. 362367. American Association for Artificial Intelligence.
Regin, J. (1999). The Symmetric Alldiff Constraint. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI-99, pp. 420425. Morgan
Kaufmann Publishers Inc.
Shi, W. (1979). A Branch and Bound Method for the Multiconstraint Zero One Knapsack
Problem.. Journal of the Operational Research Society, 30, 369378.
Smith, B. M., & Grant, S. A. (1998). Trying Harder to Fail First. In Thirteenth European
Conference on Artificial Intelligence, ECAI-98, pp. 249253. John Wiley & Sons.
Soules, G. W. (2005). Permanental Bounds for Nonnegative Matrices via Decomposition.
Linear Algebra and its Applications, 394, 7389.
Szymanek, R., & OSullivan, B. (2006). Guiding Search using Constraint-Level Advice.
In Proceeding of Seventeenth European Conference on Artificial Intelligence, ECAI06, Vol. 141 of Frontiers in Artificial Intelligence and Applications, pp. 158162. IOS
Press.
Trick, M. A. (2003). A dynamic programming approach for consistency and propagation
for knapsack constraints. Annals of Operations Research, 118, 7384.
Valiant, L. (1979). The Complexity of Computing the Permanent. Theoretical Computer
Science, 8 (2), 189201.
Walsh, T. (1999). Search in a Small World. In Proceedings of the Sixteenth International
Joint Conference on Artificial Intelligence, IJCAI-99, pp. 11721177.
Wassermann, A. (2007).
The feasibility version of the market split problem.
http://did.mat.uni-bayreuth.de/ alfred/marketsplit.html. last consulted 2007-11.
Zanarini, A., Milano, M., & Pesant, G. (2006). Improved algorithm for the soft global cardinality constraint. In Proceedings of the Third International Conference on Integration
of AI and OR Techniques in Constraint Programming for Combinatorial Optimization
Problems, CPAIOR-06, Vol. LNCS 3990, pp. 288299. Springer.
Zanarini, A., & Pesant, G. (2009). Solution Counting Algorithms for Constraint-Centered
Search Heuristics. Constraints, 14, 392413.
Zanarini, A., & Pesant, G. (2010). More robust counting-based search heuristics with
alldifferent constraints. In Lodi, A., Milano, M., & Toth, P. (Eds.), CPAIOR, Vol.
6140 of Lecture Notes in Computer Science, pp. 354368. Springer.

210

fiJournal of Artificial Intelligence Research 43 (2012) 389418

Submitted 10/11; published 03/12

Generalized Biwords for Bitext Compression and
Translation Spotting
Felipe Sanchez-Martnez
Rafael C. Carrasco

fsanchez@dlsi.ua.es
carrasco@dlsi.ua.es

Departament de Llenguatges i Sistemes Informatics
Universitat dAlacant, E-03071, Alacant, Spain

Miguel A. Martnez-Prieto
Joaqun Adiego

migumar2@infor.uva.es
jadiego@infor.uva.es

Departamento de Informatica
Universidad de Valladolid, E-47011, Valladolid, Spain

Abstract
Large bilingual parallel texts (also known as bitexts) are usually stored in a compressed
form, and previous work has shown that they can be more efficiently compressed if the
fact that the two texts are mutual translations is exploited. For example, a bitext can
be seen as a sequence of biwords pairs of parallel words with a high probability of cooccurrence that can be used as an intermediate representation in the compression process.
However, the simple biword approach described in the literature can only exploit one-toone word alignments and cannot tackle the reordering of words. We therefore introduce a
generalization of biwords which can describe multi-word expressions and reorderings. We
also describe some methods for the binary compression of generalized biword sequences,
and compare their performance when different schemes are applied to the extraction of
the biword sequence. In addition, we show that this generalization of biwords allows for
the implementation of an efficient algorithm to look on the compressed bitext for words or
text segments in one of the texts and retrieve their counterpart translations in the other
text an application usually referred to as translation spotting with only some minor
modifications in the compression algorithm.

1. Introduction
The increasing availability of large collections of multilingual texts has fostered the development of natural-language processing applications that address multilingual tasks such
as corpus-based machine translation (Arnold, Balkan, Meijer, Humphreys, & Sadler, 1994;
Lopez, 2008; Koehn, 2010; Carl & Way, 2003), cross-language information retrieval (Grossman & Frieder, 2004, Ch. 4), the automatic extraction of bilingual lexicons (Tufis, Barbu,
& Ion, 2004), and translation spotting (Simard, 2003; Veronis & Langlais, 2000). Other
applications, which are monolingual in nature e.g., syntactic parsing (Carroll, 2003), or
word sense disambiguation (Ide & Veronis, 1998) can also exploit multilingual texts by
projecting the linguistic knowledge available in one language into other languages (Mihalcea
& Simard, 2005).
A bilingual parallel corpus, or bitext, is a textual collection that contains pairs of documents which are translations of one another. The documents in a pair of this nature are
c
2012
AI Access Foundation. All rights reserved.

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

sometimes called the source text and the target text, respectively. However, whenever the
information as to how the document was created is unknown or irrelevant, the documents
are simply called the left text and right text. In the words of Melamed (2001, p. 1), bitexts
are one of the richest sources of linguistic knowledge because the translation of a text into
another language can be viewed as a detailed annotation of what that text means.
Bitexts are usually available in a compressed form in order to reduce storage requirements, to improve access times (Ziviani, Moura, Navarro, & Baeza-Yates, 2000), and to
increase the efficiency of transmissions. However, the independent compression of the two
texts of a bitext is clearly far from efficient because the information contained in both texts
is redundant: in theory, one of the texts might be sufficient to generate a translated version if reliable machine translation systems were already available (Nevill-Manning & Bell,
1992). The improvement in compression performance obtained when taking advantage of
the fact that the two texts in a bitext are mutual translations may be regarded as an indication of the quality of word alignments (Och & Ney, 2003). This indicator, which bounds the
mutual information (Cover & Thomas, 1991) in the two texts of a bitext, does not require
a manually-annotated corpus to evaluate the automatic alignment.
The first article dealing with the compression of bitexts was published by Nevill-Manning
and Bell (1992). This approach compressed one of the texts in isolation, while the other
was compressed by a general prediction by partial matching (PPM; Cleary & Witten, 1984)
encoder based on a model that used the automatic translation of the left text to predict
the words in the right text. This model exploited two types of relations exact word
matches and synonymy relationships provided by a thesaurus and the relative weight of
both predictions depended on the number of letters in the word that had been processed.
This approach obtained better compression ratios than a standard PPM coder operating
on the concatenated texts.
In contrast, Conley and Klein (2008) have proposed text alignment that is, pairings
between the words and phrases in one text and those in the other, as the basis for multilingual text compression. Their algorithm extends the ideas of delta-encoding (Suel &
Memon, 2003) to the case in which the right text R is a translated, automatically aligned,
version of the source text L: L is compressed first, and each block in R is then encoded
as a reference to the parallel block in L. This method requires the computation of wordand phrase-level alignments, together with the lemmatized forms of L and R. The translated text is retrieved from these references, using a bilingual glossary together with other
linguistic resources: a lemmata dictionary of words in L, a dictionary with all the possible
morphological variants of each word in R, and a bilingual glossary. The authors report
slight improvements in the compression of the right text R in comparison to classical compression algorithms such as bzip21 or word-based Huffman (Moffat, 1989) (approximately
1% and 6%, respectively). However, the authors do not take into consideration the size of
the auxiliary files needed for the retrieval of the right text.
In contrast to PPM, some text-compression methods use words rather than characters
as input tokens (Moffat, 1989; Moffat & Isal, 2005). Analogously, Martnez-Prieto, Adiego,
Sanchez-Martnez, de la Fuente, and Carrasco (2009), and Adiego and his colleagues (2009,
2010) propose the use of biwords pairs of words, each one from a different text, with a high
1. http://www.bzip.org

390

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Figure 1: Processing pipeline of a biword-based bitext compression approach.

probability of co-occurrence as input units for the compression of bitexts. This means
that a biword-based intermediate representation of the bitext is obtained by exploiting
alignments, and encoding unaligned words as pairs in which one component is the empty
string. Significant spatial savings are achieved with this technique (Martnez-Prieto et al.,
2009), although the compression of biword sequences requires larger dictionaries than the
traditional text compression methods.
The biword-based compression approach works as a simple processing pipeline consisting
of two stages (see Figure 1). After a text alignment has been obtained without pre-existing
linguistic resources, the first stage transforms the bitext into a biword sequence. The
second stage then compresses this sequence. Decompression works in reverse order: the
biword sequence representing the bitext is first generated from the compressed file, and the
original texts are then restored from this sequence.
A variation of the PPM algorithm that takes words rather than characters as input
tokens and bytes rather than bits as minimal output units (Adiego & de la Fuente, 2006)
can be directly applied in order to compress biword sequences. The bitext is thus compressed
using a single probabilistic model for both texts rather than the independent models used
by older bitext-compression approaches (Nevill-Manning & Bell, 1992; Conley & Klein,
2008). The improvement over general-purpose compressors obtained with this approach
depends on the language pair: for instance, a reduction in the output size of almost 11% is
obtained for SpanishPortuguese, and of about 2.5% for EnglishFrench (Martnez-Prieto
et al., 2009).
A different biword-based scheme called 2lcab has recently been proposed (Adiego et al.,
2009) which creates a two-level dictionary to store the biwords and compresses the biword
sequence with End-Tagged Dense Code (ETDC; Brisaboa, Farina, Navarro, & Parama,
2007). The usage of ETDC permits both Boyer-Moore-type searching (Boyer & Moore,
1977), and random access to the compressed file. If 2lcab is used as a compression booster
for a standard PPM coder, further improvements in compression are obtained, but it is no
longer possible to directly search in the compressed files (Adiego et al., 2010).
The biword sequences obtained with the former biword-based compression methods
contain a large fraction between 10% and 60%, depending on the language pair of
unpaired words, that is, biwords of which one of the words in the pair is the empty word .
The unpaired words are generated in three different cases:
391

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Figure 2: Example of a SpanishEnglish pair of sentences with one-to-many word alignments.
 The aligner is unable to connect a word with any of the words in the parallel text
because, for example, infrequent idiomatic expressions or free translations have been
found.
 The aligner generates a one-to-many alignment because a word has been translated
into a multiword expression. For instance, if the Spanish word volver is translated
into English as to go back, the biword extractor has to select one of the links, build a
pair of words from that link, and leave the other words unpaired.
 The aligner generates some crossing alignments as a result of word reordering in
the translation. For instance, in Figure 2, either the pair (verde, green) or the pair
(casa, house) must be ignored by the biword extractor, thus leaving two unpaired
words; otherwise, the information provided by the sequence will not be sufficient to
retrieve both texts in the original order.
The last two sources of unpaired words are responsible for the different spatial savings
reported by Martnez-Prieto et al. (2009) for bitexts consisting of closely-related languages
(e.g., Spanish and Portuguese) and for those involving divergent language pairs (e.g., French
and English), in which word reorderings and multiword translations are frequent.
In this paper, we describe and evaluate the simple biword extraction approach, and compare it with other schemes used to generate generalized biword sequences that maintain all
or part of the structural information provided by the aligner. A biword essentially becomes
a left word connected with a variable number of right words plus additional information
concerning the relative position of each right word with regard to the preceding one. The
fraction of unpaired words is thus reduced, and better compression ratios can be obtained.
We also show that this generalization of biwords allows for the implementation of an
efficient translation spotting (Simard, 2003; Veronis & Langlais, 2000) algorithm on the
compressed bitext; a task that consists of identifying the words (or text segments) in the
other text that are the translation of the words in the query. Indeed, generalized biword
sequences contain all the information needed in order to retrieve connected passages.
Generalized biwords can also be used as an ingredient in the bilingual language model
employed in some statistical machine translation systems (Koehn, 2010). For instance,
Marino et al. (2006) use bilingual n-grams and consider the translation as a bilingual decoding process. Casacuberta and Vidal (2004) also exploit bilingual n-grams but apply
stochastic finite-state transducers to this task. In both cases, the local reordering of words
is addressed by considering multiword segments of source and target words as the fundamental translation units. Some alternative approaches (Niehues, Herrmann, Vogel, & Waibel,
2011; Matusov, Zens, Vilar, Mauser, Popovic, Hasan, & Ney, 2006; Hasan, Ganitkevitch,
392

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Ney, & Andres-Ferrer, 2008) integrate bilingual language models as an additional feature in
the decoding function that drives the statistical translation process. However, none of the
approaches mentioned includes the structural information provided by the aligners as part
of the bilingual language model.
The remainder of the paper is organized as follows. The following section shows how a
generalized biword sequence can represent a bitext. Section 3 describes two different methods that can be applied to compress a biword sequence. Section 4 introduces the resources
used to evaluate different generalizations of the biwords, whereas Section 5 discusses the
compression results obtained. Section 6 then describes some modifications to one of these
compression techniques in order to allow the compressed bitext to be searched and presents
efficient search and translation spotting algorithms. Finally, some concluding remarks are
presented in Section 7.

2. Extraction of Biword Sequences
Before extracting the sequence of biwords representing a bitext, the alignments between the
words in the left text L = l1 l2    lM and the words in the right text R = r1 r2    rN must
be established by the word aligner. Word aligners usually work after a sentence aligner
has identified which pairs of sentences in the bitext are parallel, that is, a plausible mutual
translation. Sentence alignment algorithms are often based on simple statistical models
for the correlation between sentence lengths (Brown, Lai, & Mercer, 1991; Gale & Church,
1993).
Current word aligners use word-based statistical machine translation models (Brown,
Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, & Roossin, 1990; Brown, Pietra, Pietra,
& Mercer, 1993) to compute the most likely alignment between the words in two parallel
sentences (Koehn, 2010, Ch. 4). In our case, word alignments are computed with the opensource Giza++ toolkit2 (Och & Ney, 2003) which implements a set of methods, including
standard word-based statistical machine translation models (Brown et al., 1993) and a
hidden-Markov-model-based alignment model (Vogel, Ney, & Tillmann, 1996). Giza++
produces alignments such as those depicted in Figure 2, in which a source word (here, a left
word) can be aligned with many target words (here, right words), whereas a target word is
aligned with, at most, one source word.
The result of word alignment is a bigraph G = {L, R, A} in which an edge {li , rj }  A
between word li  L and word rj  R signifies that they are mutual translations according
to the translation model used by the aligner. These complex structures are processed by
splitting the bigraph into connected components: each connected component is either an
unpaired (right or left) word, or a left word  aligned with a sequence  of (one or more) right
words. As will be shown later, a connected component including the structural information
needed to place all the words in their original positions in the bitext is what we term as a
generalized biword.
In order to build a sequence B of generalized biwords, biwords will be sorted primarily
according to their left component  and, secondarily, by the head of their right component
. More precisely, if left() and right() denote the left word and the sequence of right
2. http://code.google.com/p/giza-pp/

393

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

words respectively in a biword , and  represents the empty left or right component in
unpaired words, then  precedes  if and only if:
 left() 6= , left() 6=  and left() precedes left() in L.
 Either left() =  or left() = , and right() 6= , right() 6= , and the initial word
in right() precedes, in R, the initial word in right().
 left() = , left() 6=  and there is no biword  such that  precedes  and  precedes
.
Every generalized biword  = (, , ) in the sequence B consists of:
 a string  in L ,
 an array of strings  in R , and
 an integer array  containing one offset for every string in .
Here, L denotes the set of different words in L enhanced with the empty word , while R
denotes the set of subsequences in R and includes the empty subsequence, represented by
(). The array of offsets  stores the structural information needed to place each word in 
in its original position.
The offset is a non-negative integer that specifies, for every word in  that is not the
first one, the number of words in R located between this word and the preceding one in ,
thus allowing the generation stage during decompression to keep track of the gaps in the
subsequence  that will be filled in with a word from a posterior biword in B. The offset of
the first word w in  6= () is defined as the number of words in R located between w and
the first available gap, that is, the first word in R that belongs to a biword that does not
precede  = (, , ).
The combination of both types of offsets permits the encoding of translations with
word reordering. Indeed, as can be seen in Figure 3, the offset in the biword (casa,
(house),(1)) signifies that there is a one-word gap between house and the which is occupied by the word green with offset 0 in (verde,(green),(0)). The offsets in (vivimos,
(we,live), (0,0)) indicate that we comes directly after the word house and live comes
immediately after we. The pseudo-code of the procedure that extracts the sequence of
generalized biwords and further details on its implementation can be found in Appendix A.
Henceforth, we shall call biwords with shifts those biwords with at least one non-null
offset (biwords without shifts, otherwise). We shall further differentiate between biwords
with simple shifts, where only the first offset is non-null, and biwords with complex shifts,
with non-consecutive words in R.
Generalized biwords are clearly more expressive, and the bitexts will therefore be mapped
onto shorter sequences. However, the enhanced variety of biwords implies that the compression algorithms must use larger dictionaries. The global effect on the compression
ratio must therefore be explored. It is also worth measuring the effect of ignoring certain infrequent alignments in order to avoid biwords with complex shifts. For example,
the generalized biword sequence in Figure 3 contains one biword with complex shifts,
(prefiero,(i,like),(0,1)) which can be split into smaller components, such as (,(i),
394

fiGeneralized Biwords for Bitext Compression and Translation Spotting

(prefiero,(i,like),(0,1))
(,(would),(0))
(volver,(to,go,back),(0,0,0))
(a,(to),(0))
(la,(the),(0))
(casa,(house),(1))
(verde, (green),(0))
(en,(in),(2))
(que,(),())
(vivimos,(we,live),(0,0))
Figure 3: Generalized biword sequence for the word-aligned sentence shown in Figure 2.
(0)) and (prefiero,(like),(0)), so that the sequence only includes biwords with simple
shifts. If only simple shifts are allowed, the compression algorithm needs to encode, at most,
one non-null offset per biword.
In the experiments we shall compare the results obtained when the algorithms described
in the next section (Tre and 2lcab) are used in combination with four different methods
to extract a sequence of biwords:
 1:N Complex: the one-to-many word alignments generated by Giza++ are used to
generate a sequence of generalized biwords.
 1:N Simple: the biwords with complex shifts generated by the one-to-many alignments provided by Giza++ are split into biwords with simple shifts plus unpaired
words; the result is a sequence of biwords with simple shifts or without shifts. Biwords with complex shifts are split by ignoring the least frequent alignments so that
the resulting biwords only contain simple shifts.
 1:1 Non-monotonic: one-to-one word alignments are obtained by computing the intersection of the alignments produced by Giza++ when the left and the right text are
exchanged; the result is a sequence of biwords whose right component contains, at
most, one word (and these biwords cannot, therefore, have complex shifts).
 1:1 Monotonic: the 1:1 non-monotonic sequence is transformed into a sequence of
biwords without shifts by splitting biwords with shifts into unpaired words.
The last method, 1:1 Monotonic, does not use the enhancement provided by the generalization of biwords (i.e., the structural information), and is therefore equivalent to the basic
procedures described earlier (Martnez-Prieto et al., 2009; Adiego et al., 2009, 2010).

3. Compression of Biword Sequences
It is clearly possible to compress the intermediate representation introduced in the previous
section via the application of a wide range of approaches. Here, we describe and evaluate
two different encoding methods, namely Tre (Subsection 3.2) and 2lcab (Subsection 3.3),
that apply a word-based implementation of Huffman coding (Moffat & Turpin, 1997; Turpin
395

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

& Moffat, 2000) in which the input strings are mapped onto integers and then compressed
with Huffman codewords (Huffman, 1952). Both methods encode the offsets as described
below (Subsection 3.1) but differ in how they encode the lexical components of the biword
sequence.
The use of Huffman codewords allows the two methods described here to achieve large
spatial savings, but makes it inefficient to search in the compressed bitext and to retrieve
matches. In Section 6 we shall describe a variant of the 2lcab compression algorithm
(searchable 2lcab) which allows a sequence of words to be retrieved in the left text in
addition to the parallel sequences (and their context) in the right text.
3.1 Structural Encoding
Preliminary tests showed that the biword extraction algorithm was capable of generating
sequences with a high number (usually above 70%) of biwords without shifts, and an array
of null values can thus be considered as the default offset sequence. The offsets can therefore
be encoded as two streams of integer values:
 The positions P = (p1 , p2 , . . . , pN ) of the biwords with shifts in the sequence B.
These positions can be expressed in relation to the previous biword with shifts in B;
for example, the biwords with shifts in Figure 3 are at P = (1, 5, 2).
 The offset values O for the biwords with shifts. In the example, the offsets are O =
(0, 1, 1, 2); the first two offsets belong to the first biword with shifts whereas the
following ones belong to the second and third biword with shifts, respectively.
Both streams are therefore encoded by using two independent sets of Huffman codewords.
3.2 The TRE Compressor
The Translation Relationship-based Encoder (Tre) assigns codewords to the left word and
to the sequences of right words in the biword through the use of two independent methods.
The left text is encoded using word-based Huffman coding (Moffat, 1989). In contrast,
the right text is encoded by using the left text as its context. To do this, Tre uses three
dictionaries: one, L , with the left words, a second one, R , with the sequences of right
words, and a third one, the translation dictionary B , which maps each word   L onto
a subset of entries in R :
B () = {  R :   N : (, , )  B}.
For every   L the sequences in B () are sorted by frequency and assigned an integer
in the range [1, |B ()|], thus signifying that the most frequent translations have the lowest
values.
At the compression stage, the text in every biword (, , ) is mapped onto a pair of
integers a reference to the left word  and the integer value assigned to the sequence
of right words  by B (), and both sequences of integers are then compressed using
independent Huffman codewords. The compression efficiency is improved because the most
frequent translations are all assigned low (and thus recurrent) integer values. Finally, the
compressed file includes a header with:
396

fiGeneralized Biwords for Bitext Compression and Translation Spotting

 The dictionaries L and R , which are independently encoded using PPM compression. A special character is used to separate consecutive entries in the dictionaries
and white-space serves as the delimiter in word sequences.
 The translation dictionary B , that is, a Huffman-compressed sequence of integers
(Moffat & Turpin, 1997) containing, for every entry   L , the size of B () and the
references to the entries in R that store every sequence in B ().
 The independent Huffman codewords used to compress the integer sequences of references, and B () values.
3.3 The 2LCAB Compressor
In contrast to Tre, the 2-Level Compressor for Aligned Bitexts (2lcab; Adiego et al., 2009)
encodes every biword with a single codeword based on a two-level dictionary. The first level
consists of two dictionaries, L and R , containing the left words and the sequences of right
words, respectively, that appear in the biword sequence B. The second level dictionary B
stores the different biwords in B as an integer sequence of alternating references to the
entries in L and R . The text in the sequence B can then be mapped onto a sequence of
references to entries in B .
The header includes L and R which are compressed, as in Tre, with a PPM algorithm (Cleary & Witten, 1984). It also contains the codewords (selected according to the
Huffman compression procedure) for the integers in the sequence describing the dictionary
B , the encoded dictionary B , and a second list of Huffman codewords used to encode the
biword sequence B. This implementation of 2lcab employs (bit-oriented) Huffman coding, but the original work (Adiego et al., 2009), and the application described in Section 6
implement byte-oriented ETDC (Brisaboa et al., 2007). The bit-oriented approach is more
effective, but ETDC permits faster searches on the compressed bitext.

4. Resources and Settings
In order to evaluate the performance of the bitext compressors based on generalized biwords
we have made use of the following bitext collections:
 A 100 MB SpanishCatalan (es-ca) bitext obtained from El Periodico de Catalunya,3
a daily newspaper published in Catalan and Spanish.
 A 100 MB WelshEnglish (cy-en) bitext from the Proceedings of the National Assembly for Wales (Jones & Eisele, 2006).4
 Bitexts (100 MB each) from the European Parliament Proceedings Parallel Corpus
(Europarl; Koehn, 2005) for seven different language pairs: GermanEnglish (de-en),
SpanishEnglish (es-en), SpanishFrench (es-fr), SpanishItalian (es-it), Spanish
Portuguese (es-pt), FrenchEnglish (fr-en), and FinnishEnglish (fi-en).
3. Available on-line at http://www.elperiodico.com.
4. Available on-line at http://xixona.dlsi.ua.es/corpora/UAGT-PNAW/.

397

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

bzip2

gzip

p7zip

ppmdi

wh

22.19%
22.01%
21.51%
21.88%
21.94%
21.65%
27.21%
22.17%
21.83%

31.59%
31.38%
30.80%
31.00%
31.10%
31.10%
37.09%
31.32%
31.11%

21.61%
21.39%
20.75%
21.15%
21.11%
21.06%
24.41%
20.78%
20.33%

20.35%
20.12%
19.57%
19.98%
20.02%
19.76%
25.43%
20.33%
20.07%

23.28%
23.95%
24.04%
23.57%
23.11%
24.57%
27.66%
22.97%
25.18%

Table 1: Compression ratios obtained with four general-purpose compressors and a wordbased text compressor (wh).
As is common in information retrieval applications, the texts were tokenized and converted to lowercase (Manning & Schutze, 1999, Ch. 4). The tokenization placed blank
spaces before and after every punctuation mark, and a word was thus defined as being any
sequence of alphanumeric characters delimited by blank spaces.
Word alignments were computed with the Giza++ toolkit, with all parameters set to
their default values, with the exception of the fertility which was set to 5 (the default being
9). The fertility is the maximum number of words with which a word can be aligned, and a
low value moderates the number of right sequences with one single occurrence in the bitext.

5. Results and Discussion
As a reference, Table 1 shows the compression ratio defined as the quotient between the
lengths of the output and the input texts (Ziv & Lempel, 1977) achieved with the aforementioned bitexts when they are compressed with a variety of general-purpose compressors
and with a word-based compressor operating on the concatenation of the two texts L and
R. The other approaches quoted in the introduction could not be compared because either
the code or the linguistic resources required were not publicly available. The compressors
used as a reference are:
 The bzip2 compressor,5 which splits the text into blocks (100-900 KB), then, applies
the Burrows-Wheeler Transform (BWT; Burrows & Wheeler, 1994) followed by a moveto-front transformation and, finally, encodes the result with a Huffman encoder.
 Two dictionary-based compressors built on different variants of the Ziv-Lempels
LZ77 (Ziv & Lempel, 1977) algorithm. First, gzip,6 a classical compressor that
combines LZ77-based modeling with Huffman coding (Huffman, 1952). Second, the
modern p7zip7 compressor based on the Lempel-Ziv-Markov chain algorithm (Sa5. http://www.bzip.org. Experiments run with version 1.0.5.
6. http://www.gzip.org. Experiments were carried out with version 1.3.12-6.
7. http://www.7zip.com. Experiments were carried out using version 4.58dfsg1.1.

398

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:1
Monotonic
20.38%
19.63%
19.07%
19.21%
18.44%
20.20%
17.02%
21.50%
20.06%

1:1
Non-monotonic
20.06%
18.85%
18.60%
18.86%
18.06%
19.30%
16.95%
20.82%
18.69%

1:N
Simple
20.26%
18.69%
18.78%
19.11%
18.17%
19.31%
16.78%
21.70%
18.05%

1:N
Complex
21.22%
19.33%
19.51%
20.00%
18.79%
20.06%
16.86%
22.24%
18.22%

Table 2: Compression ratios obtained with the Tre compressor and different biword
extraction methods.
lomon, 2007, Sec. 3.24), an algorithm which improves LZ77 with a large dictionary
(up to 4 GB) and range encoding (Martin, 1979).
 ppmdi (Shkarin, 2002) as a representative of the Prediction by Partial Matching
(PPM; Cleary & Witten, 1984) family of compressors. ppmdi uses a high-order
context model and method D (Howard & Vitter, 1992) to handle escape codes. The
implementation available in the Pizza&Chili website8 with the default configuration
(sixth-order context model) has been used.
 A word-based Huffman compressor (Moffat, 1989) that maps the input strings to
integers before encoding the values with Huffman codewords (Moffat & Turpin, 1997;
Turpin & Moffat, 2000). This method was originally designed to compress text, but
also works well with other types of sources. The dictionary that maps words to integers
after its encoding with a ppmdi compressor is part of the output.
As can be seen in Table 1, the lowest compression ratios are obtained with ppmdi,
except for the es-ca pair. The fact that the compression ratios depend only moderately on
the languages involved suggests that these compressors do not benefit from the (variable)
cross-language information provided by the translations.
These ratios must be compared with the performance of the two compressors described
in this section and presented in Tables 2 and 3. Note that although all the bitexts were
aligned in both translation directions, only the results obtained with the direction producing
the best compression are reported here, since the effect of this choice on the compression
ratio proved to be small (an average difference of 0.2 percentage points).
The comparison shows that both Tre and 2lcab outperform the general-purpose compressors in all cases but that of the en-fi pair. The best results are obtained in most
of the cases when one-to-one alignments are used with both techniques. 2lcab achieves
slightly better results than Tre for all language pairs with the exception of it-es and
pt-es, although in these two cases the difference in performance is too small to be considered relevant. The low performance for en-fi is the consequence of the larger translation
8. http://pizzachili.dcc.uchile.cl/utils/ppmdi.tar.gz

399

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:1
Monotonic
19.98%
19.29%
18.89%
19.18%
18.46%
19.75%
16.69%
21.29%
19.43%

1:1
Non-monotonic
19.83%
18.68%
18.50%
18.87%
18.09%
19.03%
16.61%
20.62%
18.30%

1:N
Simple
20.77%
19.08%
19.27%
19.77%
18.75%
19.65%
16.59%
22.46%
17.98%

1:N
Complex
22.12%
19.99%
20.25%
20.96%
19.60%
20.71%
16.70%
23.31%
18.25%

Table 3: Compression ratios obtained with the 2lcab compressor and different biword
extraction methods.
Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

General
purpose
20.35%
20.12%
19.57%
19.98%
20.02%
19.76%
24.41%
20.33%
20.07%

2lcab 1:1
Monotonic
19.98%
19.29%
18.89%
19.18%
18.46%
19.75%
16.69%
21.29%
19.43%

2lcab
(best)
19.83%
18.68%
18.50%
18.87%
18.09%
19.03%
16.59%
20.62%
17.98%

Gain
(Best/Gen.)
2.56%
7.16%
5.47%
5.56%
9.64%
3.69%
32.04%
-1.43%
10.41%

Gain
(Best/Mono.)
0.75%
3.16%
2.06%
1.62%
2.00%
3.65%
0.60%
3.15%
7.46%

Table 4: Summary of the best compression results obtained with: i) general-purpose and
word-based compressors; ii) the 2lcab compressor with no structural information (1:1
Monotonic); iii) the best 2lcab compressor. The columns on the right show the relative improvement of the best 2lcab over the general purpose and monotonic compressors,
respectively.

dictionaries used by Tre, and the larger bilingual dictionary used by 2lcab, in comparison
to the other language pairs. Furthermore, the percentage of unpaired words is also higher
than that of the other language pairs as will be seen below.
Table 4 summarizes the results obtained by the general-purpose compressors and by
2lcab, and the relative gains in compression performance with regard to the generalpurpose compressors performing best, and with regard to that of 2lcab when compressing
the 1:1 Monotonic biword sequence. The greatest improvement, in comparison to the results obtained for the general-purpose compressors, is achieved for the language pair es-ca:
instead of the 24.4% compression ratio obtained by p7zip, 2lcab achieves a compression
ratio of 16.6% which represents a substantial spatial saving (32.04% relative improvement).
This suggests that Tre and 2lcab take advantage of the fact that the texts contain the
400

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:1
Monotonic
0.600
0.506
0.463
0.465
0.421
0.540
0.128
0.684
0.530

1:1
Non-monotonic
0.483
0.395
0.398
0.408
0.363
0.425
0.122
0.610
0.387

1:N
Simple
0.352
0.984
0.290
0.279
0.249
0.316
0.077
0.492
0.286

1:N
Complex
0.319
0.258
0.266
0.247
0.225
0.292
0.072
0.467
0.276

Table 5: Fraction of biwords in the extracted sequence with one empty component.
same information but encoded in different languages, particularly in the case of highly
parallel bitexts (en-cy) and languages with a high syntactic correlation (es-ca).
The generalization of biwords generates shorter biword sequences, essentially because
the sequence extracted contains a lower fraction of unpaired words. Table 5 shows the
fraction of biwords in which one component is the empty word (the other being an unpaired
word). This number is obviously considerably reduced when offsets are used to encode the
structural information implicit in the alignments. Of course, in the case of the 1:N Complex
approach, the fraction coincides with that of the bigraph produced by the aligner.
As expected, the effect of the generalization on the percentage of biwords with an empty
component depends on the languages involved, the reduction being smaller for pairs of
closely-related languages (es-ca, pt-es, and it-es) than for pairs of languages with strong
grammatical divergences (en-de, es-en, and fr-en) since, in the latter case, word reorderings and multiword expressions commonly appear in translations.
In order to gain some insight into the performance of the Tre and 2lcab compressors,
it is interesting to make a separate examination of the contribution to the output size of
the headers, the dictionaries and the codewords. In this respect, it is worth noting that:
1. The number of entries in the left dictionary L does not depend on the method used
to extract the biword sequence.
2. The number of entries in the right dictionary R is identical for the two extraction
methods based on one-to-one alignments because it consists of all the words found in
the right text plus the empty word.
Tables 6 and 7 show the fraction of the output that corresponds to the encoded biword
sequence (columns B), and to the translation dictionary or biword dictionary (columns D),
depending on the compressor used. These numbers reveal that the more general the biwords used are, the more compact the encoded sequences are compared to the headers and
dictionaries. In particular, the translation dictionary size (analogously, the biword dictionary size) does not differ much if non-monotonic alignments are used instead of the basic
method. However, the usage of one-to-many alignments causes the size of the dictionary
to grow considerably, particularly in the case of the biword dictionary B used by 2lcab.
401

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:1
Monotonic
B
D
0.933 0.031
0.942 0.035
0.934 0.041
0.928 0.045
0.923 0.047
0.951 0.029
0.892 0.039
0.884 0.059
0.960 0.021

1:1
Non-monotonic
B
D
0.930
0.034
0.940
0.037
0.932
0.042
0.927
0.046
0.922
0.048
0.948
0.031
0.893
0.039
0.884
0.058
0.958
0.023

1:N
Simple
B
D
0.857 0.070
0.894 0.058
0.884 0.065
0.870 0.073
0.870 0.072
0.903 0.054
0.870 0.045
0.785 0.093
0.933 0.034

1:N
Complex
B
D
0.787 0.078
0.839 0.069
0.826 0.074
0.805 0.081
0.819 0.078
0.845 0.066
0.862 0.046
0.724 0.089
0.916 0.038

Table 6: Fraction of the file compressed with Tre encoding the bitext (B) and the translation dictionary (D).

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:1
Monotonic
B
D
0.896 0.065
0.911 0.063
0.894 0.077
0.886 0.083
0.880 0.087
0.920 0.057
0.859 0.071
0.845 0.094
0.939 0.039

1:1
Non-monotonic
B
D
0.893
0.067
0.908
0.065
0.892
0.079
0.884
0.084
0.880
0.088
0.916
0.060
0.860
0.070
0.844
0.093
0.935
0.042

1:N
Simple
B
D
0.808 0.116
0.844 0.105
0.831 0.115
0.812 0.128
0.814 0.126
0.856 0.098
0.832 0.080
0.725 0.151
0.903 0.061

1:N
Complex
B
D
0.733 0.130
0.788 0.119
0.772 0.126
0.745 0.140
0.762 0.133
0.796 0.114
0.824 0.082
0.666 0.147
0.884 0.066

Table 7: Fraction of the file compressed with 2lcab encoding the bitext (B) and the
biword dictionary (D).

402

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

Comp.
Ratio
19.82%
18.51%
18.58%
18.82%
17.98%
19.10%
16.78%
21.01%
18.05%

1:N Simple
Biword
B
reduc. weight
0.514
0.033
0.495
0.024
0.472
0.028
0.458
0.029
0.483
0.030
0.479
0.022
1.000
0.045
0.539
0.049
1.000
0.034

1:N Complex
Comp. Biword
B
Ratio reduc. weight
19.93%
0.465
0.033
18.56%
0.441
0.024
18.64%
0.434
0.027
18.91%
0.418
0.029
18.05%
0.456
0.030
19.17%
0.416
0.022
16.86%
1.000
0.046
21.09%
0.559
0.048
18.08%
0.631
0.019

Table 8: Compression ratios with Tre when infrequent biwords are split into smaller
biwords, remaining fraction of the original biword dictionary, and relative size of the translation dictionary in the compressed file.

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:N Simple
Comp. Biword
B
Ratio reduc. weight
19.42%
0.435
0.043
18.16%
0.406
0.036
18.21%
0.385
0.038
18.47%
0.334
0.035
17.67%
0.361
0.037
18.71%
0.386
0.033
16.59%
1.000
0.080
20.50%
0.445
0.056
17.68%
0.573
0.029

1:N Complex
Comp. Biword
B
Ratio reduc. weight
19.51%
0.393
0.043
18.22%
0.326
0.031
18.27%
0.354
0.037
18.55%
0.306
0.035
17.73%
0.341
0.036
18.77%
0.334
0.032
16.70%
1.000
0.082
20.55%
0.464
0.055
17.71%
0.542
0.029

Table 9: Compression ratios with 2lcab when infrequent biwords are split into smaller
biwords, remaining fraction of the original biword dictionary, and relative size of the biword
dictionary in the compressed file.

This, in most of the cases, causes 2lcab to perform worse than Tre with one-to-many
word alignments.
The observation that a one-to-many alignment leads to larger dictionaries makes it worth
exploring the effect on the compression ratio when very infrequent biwords are discarded.
Tables 8 and 9 therefore show the compression ratios obtained by Tre and 2lcab, respectively, when infrequent biwords are split into smaller, more frequent, biwords. This
split proceeds iteratively by removing the least frequent alignment in the biword (which
produces a new unpaired word) until all biword frequencies are above a threshold  or the
biwords only contain unpaired words. The threshold is determined by means of a ternary
search which optimizes the compression ratio. The tables also show the fraction of biwords
403

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

that remain in the dictionary after pruning, along with the new fraction of the output that
corresponds to the translation or biword dictionary.
As can be seen in the tables, discarding the most infrequent biwords (about two thirds
of them) usually leads to an improvement in the compression ratios, except in the case
of very similar languages, such as Catalan and Spanish, in which the translation is highly
parallel. This effect is more important in the case of 2lcab because the pruning leads
to a large reduction in the size of the biword dictionary and this compensates the small
increment in the total number of biwords needed to represent the bitext (between 5% and
10% of increment depending on the method used for its generation). With this filtering,
2lcab and Tre obtain the best results when extracting the biword sequence with method
1:N Simple.

6. Translation Spotting with Compressed Bitexts
The exploitation of bitexts by computer-aided translation tools has evolved from simple
bilingual concordancers (Barlow, 2004; Bowker & Barlow, 2004) to advanced translation
search engines (Callison-Burch, Bannard, & Schroeder, 2005a; Bourdaillet, Huet, Langlais,
& Lapalme, 2010). The standard translation unit processed by bilingual concordancers are
sentences, and these concordancers can thus only provide a whole sentence as the result
of a translation search. In contrast, translation search engines have translation spotting
capabilities, i.e. they can retrieve parallel text segments in bitexts.
It would seem that existing translation search engines (Callison-Burch et al., 2005a;
Bourdaillet et al., 2010) do not access bitexts in their compressed forms because storing
the correspondences between the translated segments requires additional data structures
such as word indexes or suffix arrays (Lopez, 2007; Callison-Burch, Bannard, & Schroeder,
2005b); suffix arrays typically require four times the size of the text (Manber & Myers,
1993). In contrast, the generalized biwords require much less space, they integrate the
alignment information into the compressed bitext, and this information can be exploited
to retrieve translation examples. In this section we describe some minor modifications that
need to be done to the 2lcab compression algorithm before it can be applied to this task.
We also describe a search algorithm on the compressed bitexts and evaluate the compression
performance of the new 2lcab implementation (searchable 2lcab).
The application of the 2lcab compression technique to the direct search in compressed
bitexts leads to certain challenges which are not present during the decompression process
because:
 Huffman and PPM compression hinder both direct searching and random access to
the compressed text (Bell, Cleary, & Witten, 1990).
 In a multilevel scheme such as 2lcab, whenever a matching string is found for
instance, in the biword dictionary B  it is necessary to know the strings codeword
in order to search for the encoded string in the higher level for example, the sequence
of biwords B.
The differences induced in 2lcab are described as follows, and are summarized in Table 10.

404

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Data
L
R
B
B
P
O

Content
lword0, lword1, . . .
rword0, rword1, . . .
lpos0, rpos0, lpos1, rpos1, . . .
bpos0, bpos1, . . .
delta0, delta1, . . .
offset0, offset1, . . .

2lcab
PPM
PPM
Huffman
Huffman
Huffman
Huffman

Searchable 2lcab
PPM
PPM
ETDC
ETDC
RRR
DAC & RG

Table 10: Summary of the compression methods applied. Those marked with  sort the
items before the compression. RG is only used with DAC in case biwords with complex
shifts are present.
6.1 Searchable 2LCAB Compression
There are several alternative compression methods, such as ETDC, which allow direct
searches in compressed text. In contrast to the output of the Huffman compression, the
ETDC header only stores words, because each codeword can be derived from the word position henceforth, its rank if the words are sorted according to their relative frequencies
in the document which is to be compressed. This means that there is always a mapping,
denoted code(n), which provides the ETDC codeword for the n-th most frequent word,
along with the corresponding reverse mapping.
For instance, if the b-th byte in the compressed bilingual dictionary B matches a leftword code, its rank in B determines which codeword must be looked for in B.9 Of course,
this rank can be obtained by keeping a record of the number n of words in B scanned so
far, but the standard pattern matching algorithms such as BM (Boyer & Moore, 1977) or
KMP (Knuth, Morris, & Pratt, 1977) are not well suited to this tracking. We therefore
use a finite sequence of bits S = b1 , b2 ,    , b|S| to retrieve the rank of the n-th byte n in
the encoded B . This sequence has bn = 1 for every n such that n is the final byte in a
codeword and it can be built on the fly when B is read from the compressed file.
Succinct data structures (Navarro & Makinen, 2007, Sec. 6), such as RG (Gonzalez,
Grabowski, Makinen, & Navarro, 2005) and RRR (Raman, Raman, & Rao, 2002), provide
an effective way in which to represent a sequence of bits and recover the rank associated
with every matching sequence, because they support a number of operations in the sequence
of bits S in a time that is independent of its length (Clark, 1996):
 The number of bits rankS (i) whose value is one in b1    bi .
 The position selectS (i) of the i-th bit in S whose value is one;
 The value of the i-th bit in S, denoted as accessS (i) = bi .
Moreover, the structural information in P describing which the biwords with shifts are
also needs to be randomly accessed, and the succinct data structure RRR provides a compact
alternative to the Huffman-based method used in 2lcab to compress P, the sequence of
position increments. Indeed, the RRR encoding is especially compact when the information
9. This rank can also be used to discard false matches originated by a coincidence with a right word in B
because, in such cases, the rank will be an even number.

405

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

is unbalanced for instance, above 80% of the bits show identical value, as in this case
in which the number of biwords with shifts is small. Therefore, the integer sequence P will
instead be stored as a binary sequence P = p1 , p2 , . . . , pm such that pi = 1 if the n-th byte
in B is the final byte in the codeword of a biword with shifts.
Finally, the offsets stored in O will be compressed with directly addressable variablelength codes (DAC, Brisaboa, Ladra, & Navarro, 2009) which, in contrast to the Huffman
compression, provide direct access to the n-th encoded element. Information associated
with the n-th biword can thus be retrieved immediately, since DAC encoding does not
require the preceding sequence to be decompressed from the beginning. As the biwords
with complex shifts contain more than one offset, the access to O in these cases is indirect
and provided through an auxiliary RG data structure. This structure builds a sequence of
bits Q = q1 , q2 ,    , q|Q| where |Q| is the total number of offsets stored in O. The sequence
Q has qi = 1 if Oi is the first offset in the array  of a biword (, , ), and qi = 0 otherwise.
As can be seen in Table 10, the searchable 2lcab method replaces the Huffman compression with ETDC and sorts some contents so that the higher-level ETDC compression
does not need to store codewords in its header.
6.2 Translation Spotting
The searchable 2lcab described above is complemented with a search algorithm which,
given a single word w in the left text, proceeds as follows:
1. The word w is looked for in L whose relatively small size permits an uncompressed
copy to be stored in the memory and its identifier n, given by the word position in
L , is used to obtain its ETDC codeword c = code(n).
2. An exact pattern-matching algorithm (Knuth et al., 1977) identifies all the occurrences
of the codeword c in the biword dictionary B . If a match is found at the b-th byte
and r = rankS (b) is odd (indicating a match with a L -codeword, that is, a biword
with a left component w), then, the biword with codeword code(r/2) is added to the
search set Z.
3. The multi-pattern matching algorithm Set-Horspool (Horspool, 1980; Navarro &
Raffinot, 2002) locates all the codewords in the sequence of biwords B that match one
of those contained in Z, and the matching positions are added to a new set M .
4. For every match m  M , the adjacent right component is read from B and, whenever
pm = 1 in P , the offsets are recovered from O and used to place the right words in
the original order. In case the biwords can have complex shifts, the interval Oi    Oj
containing the offsets  starts at i = selectQ (r) and ends at j = selectQ (r + 1)  1,
with r = rankP (m).
In case the query consists of a sequence of words (w1 , w2 ,    wK ) with K > 1, the
Set-Horspool algorithm is executed only for the word wk in the sequence generating the
smallest set of codewords to locate Zk , and the remaining words are then used to filter the
results once the biword context has been retrieved.
Table 11 shows an actual example of the output obtained for a multiple word query
and a compressed biword sequence obtained with the 1:N Complex method. Note that the
406

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Left text:

Right text:

Left text:

Right text:

Left text:

Right text:

Left text:
Right text:

it is only democratic that our citizens should be able to exercise influence , and it goes without saying that they should be entitled to all
the information they need in order to perform their civic duties in
society .
un componente de la democracia es que los ciudadanos puedan influir y
, obviamente , que tengan derecho a acceder a la informacion necesaria
para actuar como ciudadanos en sus sociedades .
it is concerned , then , with protection under criminal law and with
europol units having to receive the information and intelligence they
need in order to perform their tasks .
se trata , por tanto , de proteccion penal y de que las unidades de
europol deben obtener la informacion y los datos que necesiten para
poder realizar su trabajo .
the co-decision procedure must be used in order to perform this
legislative work under conditions which guarantee a genuine debate ,
involving society and citizens .
para que ese trabajo legislativo se realice en condiciones que garanticen un verdadero debate , social y ciudadano , hace falta recurrir al
procedimiento de codecision .
what are the tools , what are the procedures , that we need in order
to perform them ?
 cuales son las herramientas , los procedimientos que necesitamos para
ejecutarlas ?

Table 11: Output obtained after the query in order to perform on the bitext compressed
with the 1:N Complex method. The query terms and their translations are spotted in
boldface.

third match shows a non-contiguous translation, a case which cannot be retrieved with the
original 2lcab implementation (Adiego et al., 2009).
6.3 Experimental Evaluation
The compression ratios obtained with the searchable 2lcab are shown in Table 12. The algorithm is clearly not as effective as the 2lcab described in Section 3, leading to compression
ratios which are slightly worse than those obtained with general purpose and word-based
compressors. However, it is worth to mentioning that these compressed files include the information concerning the alignments between the words, information that is not included in
the files compressed with the standard compressors but is necessary to perform translation
spotting.
Table 12 also shows that the 1:1 Monotonic method is in this case more effective
than the 1:1 Non-monotonic method because the latter needs an additional data structure
(the RRR bit sequence) in order to access the structural information. Moreover, the byte
orientation of ETDC reduces the gain obtained by encoding a lower number of biwords.
407

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Lang.
pair
en-de
es-en
fr-es
it-es
pt-es
fr-en
es-ca
en-fi
cy-en

1:1
Monotonic
22.66%
21.86%
21.30%
21.69%
20.87%
22.37%
19.22%
24.14%
22.30%

1:1
Non-monotonic
23.20%
21.81%
21.47%
21.93%
21.07%
22.19%
19.67%
24.01%
21.86%

1:N
Simple
24.10%
22.26%
22.33%
22.93%
21.78%
22.83%
19.70%
25.91%
21.39%

1:N
Complex
26.02%
23.68%
23.70%
24.57%
23.00%
24.36%
19.87%
27.29%
21.98%

Table 12: Compression ratios obtained with the searchable 2lcab compressor.
We have studied how the time needed to process a query depends on the language pair
and also on the number and frequencies of the words in the query. The average times over
100 different sequences and 10 runs are reported in Figures 4 and 5, in which process times
were measured on an AMD Athlon Dual Core at 2 GHz with 2GB of RAM.
Figure 4 presents the times for two different language pairs. The first one, en-fr, displays the typical behavior of all Europarl bitexts (Koehn, 2005), while the second one,
en-fi, requires particularly longer times, especially for large queries. This divergent behavior seems to originate in the poor quality of the alignments between the words in this
pair of languages. This often makes words participate in a large number of different biwords
and this degrades the performance of the Set-Horspool algorithm. This language pair
consistently leads to the worst compression ratios.
Finally, Figure 5 shows the processing times for two language pairs (en-cy and es-ca)
whose bitexts have been obtained from a totally different source. The processing times are
considerably lower than those required by the Europarl corpus and a manual inspection of
the bitexts revealed that they have a highly parallel structure. This implies that the words
participate only in a small number of biwords and, not surprisingly, 2lcab achieves the
lowest compression ratios with these language pairs.

7. Concluding Remarks
We have introduced the concept of generalized biwords when applied to the compression
of bitexts. Generalized biwords integrate the information concerning word reordering and
multiword expressions in the translated text. We have described a procedure that transforms
the bitext into a sequence of generalized biwords which can be used as an intermediate
representation in the compression process. We have then extended the binary compression
algorithm 2lcab and proposed a new one, called Tre, for the encoding of generalized
biword sequences. We have also designed a variant of the 2lcab compression technique,
and a companion algorithm which facilitates efficient searching and translation spotting on
the compressed bitext.
The compression performance of 2lcab and Tre has been tested with four different
schemes to extract the biword sequence, each of which uses biwords with different structural
408

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Low-frequency sequences

Medium-frequency sequences

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350
300

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350
300

High-frequency sequences

300

250

250

250

200

200

200

150

150

150

100

100

100

50

50

50

0

0
1

2

3 4 5 6
Query length

7

8

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350

0
1

2

3 4 5 6
Query length

7

8

1

2

3 4 5 6
Query length

7

8

(a) EnglishFrench (en-fr) bitext
Low-frequency sequences

Medium-frequency sequences

High-frequency sequences

350

350

350

300

300

300

250

250

250

200

200

200

150

150

150

100

100
1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

50
0
1

2

3 4 5 6
Query length

100
1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

50
0
7

8

1

2

3 4 5 6
Query length

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

50
0
7

8

1

2

3 4 5 6
Query length

7

8

(b) EnglishFinnish (en-fi) bitext

Figure 4: Average time (milliseconds) needed to process a query containing only words
with low, medium or high frequency, as a function of the query length. Times are shown
for two different language pairs and four encoding methods.

409

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Low-frequency sequences

Medium-frequency sequences

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350
300

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350
300

High-frequency sequences

300

250

250

250

200

200

200

150

150

150

100

100

100

50

50

50

0

0
1

2

3 4 5 6
Query length

7

8

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350

0
1

2

3 4 5 6
Query length

7

8

1

2

3 4 5 6
Query length

7

8

(a) EnglishWelsh (en-cy) bitext
Low-frequency sequences

Medium-frequency sequences

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350
300

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350
300

High-frequency sequences

300

250

250

250

200

200

200

150

150

150

100

100

100

50

50

50

0

0
1

2

3 4 5 6
Query length

7

8

1:1 Monotonic
1:1 Non-monotonic
1:N Simple
1:N Complex

350

0
1

2

3 4 5 6
Query length

7

8

1

2

3 4 5 6
Query length

7

8

(b) SpanishCatalan (es-ca) bitext

Figure 5: Average time (milliseconds) needed to process a query for two different language
pairs (en-cy and es-ca).

410

fiGeneralized Biwords for Bitext Compression and Translation Spotting

complexities. The simplest method uses biwords without shifts and is therefore equivalent
to those approaches in which biwords are simple pairs and include no structural information.
The other methods include offsets to integrate the structural information of the alignments.
Our experiments show that generalized biwords lead to better compression ratios because
the reduction in the sequences encoding the bitext compensates the larger dictionaries
needed. The largest reduction in the compression ratios is obtained for pairs of divergent
languages because, in these cases, biwords without shifts cannot tackle the frequent word
reorderings and multiword translations.
Since the enhanced variability of generalized biwords requires larger dictionaries that
increase the header included in the compressed files, we have tested the effect of splitting
infrequent biwords into smaller, more frequent biwords. This reduces the number of different
biwords and allows the 2lcab compressor to obtain lower compression ratios. After this
pruning, 2lcab provides the best results if biwords are obtained from one-to-many word
alignments in which only simple shifts are allowed, that is, the target text is only split into
segments of contiguous words. Both of the algorithms, Tre and 2lcab, provide better
compression ratios than general purpose compressors, particularly in the case of pairs of
languages that share a common language family (es-ca) or bitexts which are highly parallel
(en-cy). The compression ratio can therefore be used to indirectly measure the quality of
word alignment and the degree of parallelism of the bitext.
Some modifications made to the 2lcab compressors allow the compressed bitext to be
searched efficiently, although this adaptation leads to slightly worse compression ratios.
However, the new compressed file includes the alignments between the words in the bitext
and this additional information is needed in order to implement translation spotting.
The relatively small difference in the time needed to process a query by the 1:1 Monotonic
method (the fastest one) and by the 1:N Complex method (the one conveying most information) makes the latter the preferable choice in translation spotting because it identifies
a larger variety of translations in the bitext and provides richer examples.
In our future work we plan to study the effect on translation performance of the integration of generalized, biword-based bilingual language models into current state-of-the-art
statistical machine translation systems.

Appendix A. Biword Extraction Algorithm
Algorithm 1 shows the procedure used to obtain the sequence of generalized biwords B from
a bitext with one-to-many word alignments. The main loop (lines 325) iterates over the
words in the left and right texts while there are still words on both sides to be considered.
Variables m and n point to the next left and right words, respectively, to be processed.
Inside the main loop, the set A
m with the positions of the right words aligned with the left

word lm , and the set An with the positions of the left words aligned with the right word
rn are first computed. As word alignments are one-to-many, A
n contains, at most, one
element.
After every iteration, a single biword is produced. If A
m is empty, i.e., lm is not aligned,
the next biword consists of the left word lm , an empty sequence of right words and an empty

sequence of offsets (line 7). If A
m is not empty but An is, the biword consists of the empty
word, a sequence of right words containing only rn and a sequence of offsets containing only
411

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Algorithm 1 GetBiwords extracts a biword sequence from a one-to-many word alignment between two texts.
Input: Two word sequences L and R, and a one-to-many bigraph G = {L, R, A}
Output: A sequence B of 3-tuples (word, sequence of words, sequence of offsets)
1: B  ()
. Create an empty sequence of 3-tuples
2: m  1; n  1
3: while (m  M )  (n  N ) do
4:
A
m  {j : (lm , rj )  A}
5:
A
n  {i : (li , rn )  A}
6:
if A
m =  then
7:
add (lm , (), ()) to B
8:
mm+1
9:
else if A
n =  then
10:
add (, (rn ), (0)) to B
11:
n NextRight(m, n, G)
12:
else
13:
  ();   ()
. Create empty sequences of words and offsets
14:
kn
15:
for all j  A
m in ascending order do
16:
add rj to ; add j  k to 
17:
k j+1
18:
if n = j then
19:
n NextRight(m, n, A)
20:
end if
21:
end for
22:
add (lm , , ) to B
23:
mm+1
24:
end if
25: end while
26: while m  M do
27:
add (lm , (), ()) to B
28:
mm+1
29: end while
30: while n  N do
31:
A
n  {i : (li , rn )  A}
32:
if A
n =  then
33:
add (, (rn ), (0)) to B
34:
end if
35:
nn+1
36: end while
37: return B

412

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Algorithm 2 NextRight
Input: Integers m and n, one-to-many bigraph G = (L, R, A)
Output: Index of the next right word paired with , with lm or posterior word in L
1: repeat
2:
nn+1
3:
A
n  {i : (li , rn )  A}

4: until (n > N )  (A
n = )  (min(An )  m)
5: return n

a null offset (line 10). Otherwise, the biword consists of lm , a sequence containing all the
right words aligned with lm , and a sequence containing one offset for each right word. The
first offset is relative to n, whereas the following ones are relative to the previous word in
the sequence (see lines 1521).
Index m is simply incremented every time a biword containing a left word is produced.
The update of n is more subtle since some words in positions greater than n may have
already been processed because they are aligned with a left word preceding lm . n is therefore
assigned the value returned by function NextRight (depicted in Algorithm 2) which, given
the current values of m and n, looks for the next n such that rm is paired either with the
empty word  or with a left word not preceding lm .
Finally, two loops take care of the words that remain unprocessed after the main loop.

Appendix B. Bitext Restoration Algorithm
Algorithm 3 provides the pseudo-code with which to restore the right text of the bitext from
the biword representation obtained with Algorithm 1. Restoring the left text is straightforward since biwords are sorted by their left component.
The main loop in Algorithm 3 (lines 315) iterates over the sequence of biwords representing the bitext. Variables m and n point to the next biword to be processed, and to
the next gap in R to be filled in with a word, respectively. It then iterates over the array
of offsets  = (w1 , . . . , w|| ) (lines 710) and places each word j in the sequence of right
words  = (1 , . . . , || ) in the right place. After each biword has been processed, m is
updated to point to the next biword, and n to point to the next gap in R to be filled in
(lines 1214).

Acknowledgments
This work has been supported by the Spanish Government through projects TIN2009-14009C02-01 and TIN2009-14009-C02-02, and by the Millennium Institute for Cell Dynamics and
Biotechnology (grant ICM P05-001-F). During the development of the work reported in this
paper, Miguel A. Martnez-Prieto was at the Department of Computer Science (University
of Chile) on a post-doctoral stay. The authors thanks Nieves R. Brisaboa for her ideas
and cooperation in the development of the initial version of 2lcab, Gonzalo Navarro for
his inspiration for the Tre compression approach, the anonymous referees for suggesting
significant improvements to this paper and Francis M. Tyers for proof-reading it.
413

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Algorithm 3 GetRightText retrieves the right text from the biword representation of
the bitext.
Input: A sequence of biwords B = (1 , . . . , M )
Output: The right text R = r1 r2    rN contained in the sequence B.
1: m  1
2: n  1
3: while m  M do
4:
k =n1
5:
  offset(m )
6:
  right(m )
7:
for j = 1, . . . , || do
8:
k  k + j + 1
9:
rk  j
10:
end for
11:
mm+1
12:
while rn is not undefined do
13:
nn+1
14:
end while
15: end while
16: return R

References
Adiego, J., Brisaboa, N. R., Martnez-Prieto, M. A., & Sanchez-Martnez, F. (2009). A
two-level structure for compressing aligned bitexts. In Proceedings of the 16th String
Processing and Information Retrieval Symposium, Vol. 5721 of Lecture Notes in Computer Science, pp. 114121, Saariselka, Finland. Springer.
Adiego, J., & de la Fuente, P. (2006). Mapping words into codewords on PPM. In Proceedings
of the 13th String Processing and Information Retrieval Symposium, Vol. 4209 of
Lecture Notes in Computer Science, pp. 181192, Glasgow, UK. Springer.
Adiego, J., Martnez-Prieto, M. A., Hoyos-Torio, J. E., & Sanchez-Martnez, F. (2010).
Modelling parallel texts for boosting compression. In Proceedings of the 2010 Data
Compression Conference, p. 517, Snowbird, USA.
Arnold, D., Balkan, L., Meijer, S., Humphreys, R., & Sadler, L. (1994). Machine translation:
An introductory guide. NCC Blackwell, Oxford.
Barlow, M. (2004). Parallel concordancing and translation. In Proceedings of ASLIB Translating and the Computer 26, London, UK.
Bell, T. C., Cleary, J. G., & Witten, I. H. (1990). Text compression. Prentice Hall.
Bourdaillet, J., Huet, S., Langlais, P., & Lapalme, G. (2010). TransSearch: from a bilingual concordancer to a translation finder. Machine Translation, 23 (34), 241271.
Published in 2011.
Bowker, L., & Barlow, M. (2004). Bilingual concordancers and translation memories: a
comparative evaluation. In Proceedings of the Second International Workshop on
414

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Language Resources for Translation Work, Research and Training at Coling 2004, pp.
7079, Geneva, Switzerland.
Boyer, R., & Moore, J. S. (1977). A fast string searching algorithm. Communications of
the ACM, 20 (10), 762772.
Brisaboa, N., Ladra, S., & Navarro, G. (2009). Directly addressable variable-length codes.
In Proceedings of the 16th String Processing and Information Retrieval Symposium,
Vol. 5721 of Lecture Notes in Computer Science, pp. 122130, Saariselka, Finland.
Springer.
Brisaboa, N. R., Farina, A., Navarro, G., & Parama, J. R. (2007). Lightweight natural
language text compression. Information Retrieval, 10 (1), 133.
Brown, P., Lai, J., & Mercer, R. (1991). Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pp.
169176, Berkeley, CA, USA.
Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D.,
Mercer, R. L., & Roossin, P. S. (1990). A statistical approach to machine translation.
Computational Linguistics, 16 (2), 7685.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). The mathematics
of statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Burrows, M., & Wheeler, D. (1994). A block sorting lossless data compression algorithm.
Tech. rep. 124, Digital Systems Research Center, Palo Alto, CA, USA.
Callison-Burch, C., Bannard, C., & Schroeder, J. (2005a). A compact data structure for
searchable translation memories. In Proceedings of the 10th European Association for
Machine Translation Conference, pp. 5965, Budapest, Hungary.
Callison-Burch, C., Bannard, C., & Schroeder, J. (2005b). Scaling phrase-based statistical
machine translation to larger corpora and longer phrases. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Linguistics, pp. 255262, Ann
Arbor, USA.
Carl, M., & Way, A. (Eds.). (2003). Recent Advances in Example-Based Machine Translation, Vol. 21 of Text, Speech and Language Technology. Springer.
Carroll, J. (2003). The Oxford Handbook of Computational Linguistics, chap. 12 Parsing,
pp. 233248. Oxford University Press.
Casacuberta, F., & Vidal, E. (2004). Machine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30 (2), 205225.
Clark, D. (1996). Compact PAT trees. Ph.D. thesis, University of Waterloo, Warteloo, ON,
Canada.
Cleary, J. G., & Witten, I. H. (1984). Data compression using adaptive coding and partial
string matching. IEEE Transactions on Communications, 32 (4), 396402.
Conley, E., & Klein, S. (2008). Using alignment for multilingual text compression. International Journal of Foundations of Computer Science, 19 (1), 89101.
415

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Cover, T. M., & Thomas, J. A. (1991). Elements of Information Theory. Wiley.
Gale, W. A., & Church, K. W. (1993). A program for aligning sentences in bilingual corpora.
Computational Linguistics, 19 (1), 75102.
Gonzalez, R., Grabowski, S., Makinen, V., & Navarro, G. (2005). Practical implementation
of rank and select queries. In Proceedings of the 4th International Workshop on
Efficient and Experimental Algorithms, pp. 2738, Santorini Island, Greece.
Grossman, D. A., & Frieder, O. (2004). Information Retrieval: Algorithms and Heuristics
(2nd edition)., Vol. 15 of The Information Retrieval Series. Springer.
Hasan, S., Ganitkevitch, J., Ney, H., & Andres-Ferrer, J. (2008). Triplet lexicon models for
statistical machine translation. In Proceedings of the 2008 Conference on Empirical
Methods on Natural Language Processing, pp. 372381, Honolulu, USA.
Horspool, R. N. (1980). Practical fast searching in strings. Software: Practice and Experience, 10 (6), 501506.
Howard, P., & Vitter, J. (1992). Practical implementations of arithmetic coding. In Storer,
J. (Ed.), Image and Text Compression, pp. 85112. Kluwer Academic.
Huffman, D. (1952). A method for the construction of minimum-redundancy codes. Proceedings of the Institute of Radio Engineers, 40 (9), 10981101.
Ide, N., & Veronis, J. (1998). Word sense disambiguation: The state of the art. Computational Linguistics, 24 (1), 141.
Jones, D., & Eisele, A. (2006). Phrase-based statistical machine translation between English
and Welsh. In Proceedings of the 5th SALTMIL Workshop on Minority Languages at
the 5th International Conference on Language Resources and Evaluation, pp. 7577,
Genoa, Italy.
Knuth, D. E., Morris, J. H., & Pratt, V. (1977). Fast pattern matching in strings. SIAM
Journal on Computing, 6 (2), 323350.
Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth Machine Translation Summit, pp. 7986, Phuket, Thailand.
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.
Lopez, A. (2007). Hierarchical phrase-based translation with suffix arrays. In Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning, pp. 976985, Prague, Czech Republic.
Lopez, A. (2008). Statistical machine translation. ACM Computing Surveys, 40 (3), 149.
Manber, U., & Myers, G. (1993). Suffix arrays: a new method for on-line string searches.
SIAM Journal on Computing, 22 (5), 935948.
Manning, C. D., & Schutze, H. (1999). Foundations of statistical natural language processing.
MIT Press.
Marino, J., Banchs, R. E., Crego, J. M., de Gispert, A., Lambert, P., Fonollosa, J. A. R.,
& Costa-Jussa, M. R. (2006). N-gram-based machine translation. Computational
Linguistics, 32 (4), 527549.
416

fiGeneralized Biwords for Bitext Compression and Translation Spotting

Martin, G. (1979). Range encoding: an algorithm for removing redundancy from a digitized
message. In Proceedings of Video and Data Recording Conference, Southampton, UK.
Martnez-Prieto, M. A., Adiego, J., Sanchez-Martnez, F., de la Fuente, P., & Carrasco,
R. C. (2009). On the use of word alignments to enhance bitext compression. In
Proceedings of the 2009 Data Compression Conference, p. 459, Snowbird, USA.
Matusov, E., Zens, R., Vilar, D., Mauser, A., Popovic, M., Hasan, S., & Ney, H. (2006).
The RWTH machine translation system. In Proceedings of the TC-STAR Workshop
on Speech-to-Speech Translation, pp. 3136, Barcelona, Spain.
Melamed, I. D. (2001). Emplirical methods for exploting parallel texts. MIT Press.
Mihalcea, R., & Simard, M. (2005). Parallel texts. Natural Language Engineering, 11 (3),
239246.
Moffat, A. (1989). Word-based text compression. Software: Practice and Experience, 19 (2),
185198.
Moffat, A., & Isal, R. Y. K. (2005). Word-based text compression using the Burrows-Wheeler
transform. Information Processing and Management, 41 (5), 11751192.
Moffat, A., & Turpin, A. (1997). On the implementation of minimum redundancy prefix
codes. IEEE Transactions on Communications, 45 (10), 12001207.
Navarro, G., & Makinen, V. (2007). Compressed full-text indexes. ACM Computing Surveys,
39 (1). Article 2.
Navarro, G., & Raffinot, M. (2002). Flexible Pattern Matching in String: Practical on-line
search algorithms for texts and biological sequences. Cambridge University Press.
Nevill-Manning, C., & Bell, T. (1992). Compression of parallel texts. Information Processing
& Management, 28 (6), 781794.
Niehues, J., Herrmann, T., Vogel, S., & Waibel, A. (2011). Wider context by using bilingual language models in machine translation. In Proceedings of the 6th Workshop on
Statistical Machine Translation, pp. 198206, Edinburgh, UK.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Raman, R., Raman, V., & Rao, S. (2002). Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. In Proceedings of the 22nd Annual ACM-SIAM
Symposium on Discrete Algorithms, pp. 233242, San Francisco, CA, USA.
Salomon, D. (2007). Data compression. The complete reference (Fourth edition). Springer.
Shkarin, D. (2002). PPM: One step to practicality. In Proceeding of the 2002 Data Compression Conference, pp. 202211, Snowbird, USA.
Simard, M. (2003). Translation spotting for translation memories. In Proceedings of NAACL
2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pp. 6572, Edmonton, AB, Canada.
Suel, T., & Memon, N. (2003). Algorithms for delta compression and remote file synchronization. In Sayood, K. (Ed.), Lossless Compression Handbook, pp. 269290. Academic
Press.
417

fiSanchez-Martnez, Carrasco, Martnez-Prieto & Adiego

Tufis, D., Barbu, A.-M., & Ion, R. (2004). Extracting multilingual lexicons from parallel
corpora. Computers and the Humanities, 38 (2), 163189.
Turpin, A., & Moffat, A. (2000). Housekeeping for prefix coding. IEEE Transactions on
Communications, 48 (4), 622628.
Veronis, J., & Langlais, P. (2000). Parallel text processing. Alignment and use of translation corpora, chap. Evaluation of Parallel Text Alignment Systems  The ARCADE
Project. Kluwer Academic Publishers.
Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics, pp. 836841, Copenhagen, Denmark.
Ziv, J., & Lempel, A. (1977). An universal algorithm for sequential data compression. IEEE
Transactions on Information Theory, 23 (3), 337343.
Ziviani, N., Moura, E., Navarro, G., & Baeza-Yates, R. (2000). Compression: A key for
next-generation text retrieval systems. IEEE Computer, 33 (11), 3744.

418

fiJournal of Artificial Intelligence Research 43 (2012) 571-620

Submitted 09/11; published 04/12

Reformulating the Situation Calculus and the Event
Calculus in the General Theory of Stable Models and in
Answer Set Programming

Joohyung Lee
Ravi Palla

joolee@asu.edu
Ravi.Palla@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287, USA

Abstract
Circumscription and logic programs under the stable model semantics are two wellknown nonmonotonic formalisms. The former has served as a basis of classical logic based
action formalisms, such as the situation calculus, the event calculus and temporal action
logics; the latter has served as a basis of a family of action languages, such as language A
and several of its descendants. Based on the discovery that circumscription and the stable
model semantics coincide on a class of canonical formulas, we reformulate the situation
calculus and the event calculus in the general theory of stable models. We also present a
translation that turns the reformulations further into answer set programs, so that efficient
answer set solvers can be applied to compute the situation calculus and the event calculus.

1. Introduction
Circumscription (McCarthy, 1980, 1986) and logic programs under the stable model semantics (Gelfond & Lifschitz, 1988) are two well-known nonmonotonic formalisms. As one of the
oldest nonmonotonic formalisms, circumscription has found many applications in commonsense reasoning and model-based diagnoses (e.g., McCarthy, 1986; Shanahan, 1995; Besnard
& Cordier, 1994). The stable model semantics is the mathematical basis of Answer Set Programming (ASP) (Marek & Truszczynski, 1999; Niemela, 1999; Lifschitz, 2008), which is
being widely applied thanks to the availability of several efficient implementations, known
as answer set solvers.
While the two nonmonotonic formalisms have been applied to overlapping classes of
problems, minimal model reasoning ensured by circumscription does not coincide with stable
model reasoning. Moreover, these formalisms have different roots. While circumscription
is defined in terms of translation into classical (second-order) logic, stable models proposed
by Gelfond and Lifschitz (1988) are defined in terms of grounding and fixpoints in the
style of Reiters default logic (Reiter, 1980). These differences in part account for the fact
that the two formalisms have formed rather disparate traditions in knowledge representation
research. In particular, in the area of temporal reasoning, the former has served as a basis of
classical logic based action calculi, such as the situation calculus (McCarthy & Hayes, 1969;
Reiter, 2001), the event calculus (Shanahan, 1995) and temporal action logics (Doherty,
c
2012
AI Access Foundation. All rights reserved.

fiLee & Palla

Gustafsson, Karlsson, & Kvarnstrom, 1998), whereas the latter has served as a basis of a
family of action languages, such as language A (Gelfond & Lifschitz, 1998) and several of its
descendants which can be translated into logic programs under the stable model semantics.
However, a recent generalization of the stable model semantics shed new light on the
relationship between circumscription and stable models. The first-order stable model semantics defined by Ferraris, Lee and Lifschitz (2007, 2011) characterizes the stable models
of a first-order sentence as the models (in the sense of first-order logic) of the sentence
that satisfy the stability condition, expressed by a second-order formula that is similar
to the one used to define circumscription. Since logic programs are viewed as a special
class of first-order sentences under the stable model semantics, this definition extends the
stable model semantics by Gelfond and Lifschitz (1988) to the full first-order level without
limiting attention to Herbrand models. Essentially the same characterization was independently given by Lin and Zhou (2011), via logic of knowledge and justified assumption (Lin
& Shoham, 1992). These definitions are also equivalent to the definition of Quantified Equilibrium Logic given by Pearce and Valverde (2005), which is defined in terms of the logic of
Here-and-There (Heyting, 1930).
The new definition of a stable model motivates us to investigate the relationship between
stable model reasoning and minimal model reasoning. In particular, we focus on their
relationship in the area of temporal reasoning. We show how the situation calculus and the
event calculus can be reformulated in the first-order stable model semantics, and further in
ASP. This is not only theoretically interesting, but also practically useful as it allows us to
leverage efficient answer set solvers for computing circumscriptive action theories.
For this, we develop two technical results. First, we show that circumscription and the
first-order stable model semantics coincide on the class of canonical formulas. This is the
largest syntactic class identified so far on which the two semantics coincide, and is general
enough to cover several circumscriptive action formalisms, such as the situation calculus,
the event calculus, and temporal action logics. The result allows us to reformulate those
action formalisms in the first-order stable model semantics. While minimal model reasoning
sometimes leads to unintuitive results, those circumscriptive action formalisms are carefully
designed to avoid such cases, and our result implies that minimal model reasoning in those
action formalisms can also be viewed as stable model reasoning.
Second, we identify a class of almost universal formulas, which can be turned into the
syntax of a logic program while preserving stable models. It turns out that the reformulations of the situation calculus and the event calculus in the first-order stable model
semantics fall into this class of formulas. We introduce system f2lp that turns formulas in
this class to logic programs, and, in conjunction with the result on canonical formulas, use
the combination of f2lp and answer set solvers to compute the situation calculus and the
event calculus.
Our work makes explicit the relationship between classical logic and logic program traditions in temporal reasoning. Interestingly, the development of the event calculus has
spanned over both traditions. The original version of the event calculus (Kowalski & Sergot, 1986) was formulated in logic programs, but not under the stable model semantics (that
was the time before the invention of the stable model semantics). More extensive developments have been later carried out on the classical logic foundation via circumscription (e.g.,
Shanahan, 1995, 1997, 1999; Miller & Shanahan, 1999; Mueller, 2004), but the relation to
572

fiReformulating the Situation Calculus and the Event Calculus

the logic program formulation remained implicit. Based on the reduction of circumscription to completion, SAT-based event calculus systems were implemented, one by Shanahan
and Witkowski (2004) and another by Mueller (2004). The latter system is called the dec
reasoner,1 which outperforms the former thanks to a more efficient and general compilation
method into propositional logic. While the system handles a large fragment of the event
calculus, it still cannot handle recursive and disjunctive axioms since completion cannot
be applied to such axioms. Our ASP-based approach on the other hand can handle the
full version of the event calculus under the assumption that the domain is given and finite.
Thanks to the efficiency of ASP solvers, our experiments indicate that the ASP-based event
calculus reasoner is significantly faster than the dec reasoner (Appendix B).
Similar to the logic programming tradition of the event calculus, the situation calculus (McCarthy & Hayes, 1969; Reiter, 2001) can be implemented in Prolog, based on the
fact that Clarks completion semantics accounts for definitional axioms. But unlike the
event calculus, to the best of our knowledge, efficient propositional solvers have not been
applied to directly compute the models of situation calculus theories. In this paper, we
reformulate Lins causal action theories (1995) and Reiters basic action theories (2001) in
the first-order stable model semantics and in ASP. For basic action theories, we also provide
an ASP-based encoding method that obtains Reiters successor state axioms from the effect
axioms and the generic inertia axioms adopted in ASP, the idea of which is close to Reiters
frame default (1980).
The paper is organized as follows. The next section reviews the definitions of circumscription and the first-order stable model semantics, and presents the definition of a
canonical formula. Based on this, Sections 3 and 4 reformulate the event calculus and the
situation calculus in the first-order stable model semantics. Section 5 shows a translation
that turns almost universal formulas into logic programs that can be accepted by ASP
solvers. Sections 6 and 7 use this result to turn the reformulations of the event calculus
and the situation calculus given in Sections 3 and 4 into the input language of ASP solvers.
Complete proofs are given in Appendix C.

2. Circumscription and First-Order Stable Model Semantics
We assume the following set of primitive propositional connectives and quantifiers:
 (falsity), , , , ,  .
We understand F as an abbreviation of F  ; symbol > stands for   , and F  G
stands for (F  G)  (G  F ).
2.1 Review: Circumscription
Let p be a list of distinct predicate constants p1 , . . . , pn , and let u be a list of distinct
predicate variables u1 , . . . , un . By u  p we denote the conjunction of the formulas
x(ui (x)  pi (x)) for all i = 1, . . . n, where x is a list of distinct object variables whose
length is the same as the arity of pi . Expression u < p stands for (u  p)  (p  u). For
1. http://decreasoner.sourceforge.net

573

fiLee & Palla

instance, if p and q are unary predicate constants then (u, v) < (p, q) is


x(u(x)  p(x))  x(v(x)  q(x))   x(p(x)  u(x))  x(q(x)  v(x)) .
Circumscription is defined in terms of the CIRC operator with minimized predicates.
For any first-order formula F , expression CIRC[F ; p] stands for the second-order formula
F  u((u < p)  F (u)),
where F (u) is the formula obtained from F by substituting ui for pi . When F is a sentence
(i.e., a formula with no free variables), intuitively, the models of CIRC[F ; p] are the models
of F that are minimal on p.
The definition is straightforwardly extended to the case when F is a many-sorted firstorder formula (Lifschitz, 1994, Section 2.4), which is the language that the event calculus
and the situation calculus are based on.
2.2 Review: First-Order Stable Model Semantics
This review follows the definition by Ferraris et al. (2011). There, the stable models are
defined in terms of the SM operator, whose definition is similar to the CIRC operator in
the previous section. For any first-order formula F and any finite list of predicate constants
p = (p1 , . . . , pn ), formula SM[F ; p] is defined as
F  u((u < p)  F  (u)),
where u is defined the same as in CIRC[F ; p], and F  (u) is defined recursively as follows:
 pi (t) = ui (t) for any list t of terms;
 F  = F for any atomic formula F (including  and equality) that does not contain
members of p;
 (F  G) = F   G ;
 (F  G) = F   G ;
 (F  G) = (F   G )  (F  G);
 (xF ) = xF  ;
 (xF ) = xF  .
The predicates in p are called intensional: these are the predicates that we intend to
characterize by F in terms of non-intensional predicates.2 When F is a sentence, the models
of the second-order sentence SM[F ; p] are called the p-stable models of F : they are the
models of F that are stable on p. We will often simply write SM[F ] in place of SM[F ; p]
when p is the list of all predicate constants occurring in F . According to Lee, Lifschitz, and
2. Intensional predicates are analogous to output predicates in Datalog, and non-intensional predicates are
analogous to input predicates in Datalog (Lifschitz, 2011).

574

fiReformulating the Situation Calculus and the Event Calculus

Palla (2008), answer sets are defined as a special class of stable models as follows. By (F )
we denote the signature consisting of the object, function and predicate constants occurring
in F . If F contains at least one object constant, an Herbrand interpretation of (F ) that
satisfies SM[F ] is called an answer set of F . The answer sets of a logic program  are defined
as the answer sets of the FOL-representation of  (i.e., the conjunction of the universal
closures of implications corresponding to the rules). For example, the FOL-representation
of the program
p(a)
q(b)
r(x)  p(x), not q(x)
is
p(a)  q(b)  x(p(x)  q(x)  r(x))

(1)

and SM[F ] is
p(a)  q(b)  x(p(x)  q(x)  r(x))
uvw(((u, v, w) < (p, q, r))  u(a)  v(b)
x((u(x)  (v(x)  q(x))  w(x))  (p(x)  q(x)  r(x)))),
which is equivalent to the first-order sentence
x(p(x)  x = a)  x(q(x)  x = b)  x(r(x)  (p(x)  q(x)))

(2)

(Ferraris et al., 2007, Example 3). The stable models of F are any first-order models of (2).
The only answer set of F is the Herbrand model {p(a), q(b), r(a)}.
According to Ferraris et al. (2011), this definition of an answer set, when applied to the
syntax of logic programs, is equivalent to the traditional definition of an answer set that is
based on grounding and fixpoints (Gelfond & Lifschitz, 1988).
Note that the definition of a stable model is more general than the definition of an
answer set in the following ways: stable models are not restricted to Herbrand models, the
underlying signature can be arbitrary, and the intensional predicates are not fixed to the
list of predicate constants occurring in the formula. The last fact is not essential in view of
the following proposition. By pr (F ) we denote the list of all predicate constants occurring
in F ; by Choice(p) we denote the conjunction of choice formulas x(p(x)  p(x)) for
all predicate constants p in p, where x is a list of distinct object variables; by False(p) we
denote the conjunction of xp(x) for all predicate constants p in p. We sometimes identify
a list with the corresponding set when there is no confusion.
Proposition 1 Formula
SM[F ; p]  SM[F  Choice(pr (F )\p)  False(p\pr (F ))]

(3)

is logically valid.
Notice that the (implicit) intensional predicates on the right-hand side of (3) are those
in (pr (F )  p). The Choice formula makes the predicates in (pr (F ) \ p) to be exempt
from the stability checking. On the other hand, the False formula makes the predicates in
(p \ pr (F )) to be stabilized (i.e., to have empty extents), though they do not occur in F .
575

fiLee & Palla

Ferraris et al. (2011) incorporate strong negation into the stable model semantics by
distinguishing between intensional predicates of two kinds, positive and negative. Each
negative intensional predicate has the form p, where p is a positive intensional predicate
and  is a symbol for strong negation. Syntactically  is not a logical connective, as
it can appear only as a part of a predicate constant. An interpretation of the underlying
signature is coherent if it satisfies the formula
x(p(x)  p(x)),

(4)

where x is a list of distinct object variables, for each negative predicate p. We usually
consider coherent interpretations only. Intuitively, p(t) represents that p(t) is false. This
is different from p(t) which represents that it is not known that p(t) is true. Similarly,
 p(t) represents that it is not known that p(t) is false, and p(t) represents that it is
not known that p(t) is not known to be true. Note that, unlike in first-order logic, p(t)
is different from p(t). For instance, formula p(a) has only one answer set {p(a)} but p(a)
has no answer sets.
Like the extension of circumscription to many-sorted first-order sentences, the definition
of a stable model is straightforwardly extended to many-sorted first-order sentences.
2.3 Equivalence of the Stable Model Semantics and Circumscription on
Canonical Formulas
Neither the stable model semantics nor circumscription is stronger than the other. For
example,
CIRC[x(p(x)  p(x)); p]
(5)
is equivalent to xp(x), and
SM[x(p(x)  p(x)); p]

(6)

is equivalent to >, so that (5) is stronger than (6). On the other hand,
CIRC[x(p(x)  q(x)); p, q]

(7)

is equivalent to x(p(x)  q(x)), and
SM[x(p(x)  q(x)); p, q]

(8)

is equivalent to x(p(x)  q(x)), so that (8) is stronger than (7).
In this section, we show that the two semantics coincide on a class of formulas called
canonical formulas, which we define below. We first review the notions of positive, negative,
and strictly positive occurrences.
Definition 1 We say that an occurrence of a predicate constant, or any other subexpression, in a formula F is positive if the number of implications containing that occurrence in
the antecedent is even, and negative otherwise. (Recall that we treat G as shorthand for
G  .) We say that the occurrence is strictly positive if the number of implications in F
containing that occurrence in the antecedent is 0.
576

fiReformulating the Situation Calculus and the Event Calculus

For example, in (1), both occurrences of q are positive, but only the first one is strictly
positive.
Definition 2 We say that a formula F is canonical relative to a list p of predicate constants
if
 no occurrence of a predicate constant from p is in the antecedents of more than one
implication in F , and
 every occurrence of a predicate constant from p that is in the scope of a strictly positive
occurrence of  or  in F is strictly positive in F .
Example 1 The formula
x(p(x)  q(x))

(9)

that is shown above is not canonical relative to {p, q} since it does not satisfy the first clause
of the definition (p occurs in the antecedents of two implications as p(x) is shorthand for
p(x)  ). On the other hand, the formula is canonical relative to {q}. The formula
x(p(x)  p(x))

(10)

is not canonical relative to {p} since it does not satisfy the second clause (the second occurrence of p is in the scope of a strictly positive occurrence of , but is not strictly positive in
(10)); the formula
p(a)  (x p(x)  x q(x))
(11)
is canonical relative to {p, q}, while
p(a, a)  x(p(x, a)  p(b, x))

(12)

is not canonical relative to {p, q} since it does not satisfy the second clause (the second
occurrence of p is in the scope of a strictly positive occurrence of , but is not strictly
positive in formula (12)).
The following theorem states that, for any canonical formula, circumscription coincides
with the stable model semantics.
Theorem 1 For any canonical formula F relative to p,
CIRC[F ; p]  SM[F ; p]

(13)

is logically valid.
For instance, for formula (11), which is canonical relative to {p, q}, formulas CIRC[(11); p, q]
and SM[(11); p, q] are equivalent to each other. Also, any sentence F is clearly canonical
relative to , so that CIRC[F ; ] is equivalent to SM[F ; ], which in turn is equivalent to F .
On the other hand, such equivalence may not necessarily hold for non-canonical formulas.
For instance, we observed that, for formula (10) that is not canonical relative to {p}, formulas (5) and (6) are not equivalent to each other. For formula (9) that is not canonical
577

fiLee & Palla

relative to {p, q}, formulas (7) and (8) are not equivalent to each other. We also observe
that formula (12) that is not canonical relative to {p, q}, CIRC[(12); p, q] is not equivalent
to SM[(12); p, q]: the Herbrand interpretation {p(a, a), p(b, a)} satisfies SM[(12); p, q], but
does not satisfy CIRC[(12); p, q].
Note that non-canonical formulas can often be equivalently rewritten as canonical formulas. Since any equivalent transformation preserves the models of circumscription, Theorem 1 can be applied to such non-canonical formulas, by first rewriting them as canonical
formulas. For example, formula (9) is equivalent to
x(p(x)  q(x)),

(14)

which is canonical relative to {p, q}, so that CIRC[(9); p, q] is equivalent to SM[(14); p, q].
For another example, formula (10) is equivalent to
x(p(x)  p(x)),

(15)

which is canonical relative to {p}, so that CIRC[(10); p] is equivalent to SM[(15); p]. It
is clear that this treatment can be applied to any quantifier-free formula (including any
propositional formula) because a quantifier-free formula can be equivalently rewritten as a
canonical formula by first rewriting it into a clausal normal form and then turning each
clause into the form C  D, where C is a conjunction of atoms and D is a disjunction of
atoms.3
Sections 3 and 4 use Theorem 1 to reformulate the event calculus and the situation
calculus in the first-order stable model semantics.

3. Reformulating the Event Calculus in the General Theory of Stable
Models
In this section, we review the syntax of circumscriptive event calculus described in Chapter 2
of the book by Mueller (2006). Based on the observation that the syntax conforms to the
condition of canonicality, we present a few reformulations of the event calculus in the general
theory of stable models.
3.1 Review: Circumscriptive Event Calculus
We assume a many-sorted first-order language, which contains an event sort, a fluent sort,
and a timepoint sort. A fluent term is a term whose sort is a fluent; an event term and a
timepoint term are defined similarly.
Definition 3 A condition is defined recursively as follows:
 If 1 and 2 are terms, then comparisons 1 < 2 , 1  2 , 1  2 , 1 > 2 , 1 = 2 ,
1 6= 2 are conditions;
3. It appears unlikely that knowledge has to be encoded in a non-canonical formula such as (12) that
cannot be easily turned into an equivalent canonical formula. c.f. Guide to Axiomatizing Domains
in First-Order Logic (http://cs.nyu.edu/faculty/davise/guide.html). It is not a surprise that all
circumscriptive action theories mentioned in this paper satisfy the canonicality assumption.

578

fiReformulating the Situation Calculus and the Event Calculus

 If f is a fluent term and t is a timepoint term, then HoldsAt(f, t) and HoldsAt(f, t)
are conditions;
 If 1 and 2 are conditions, then 1  2 and 1  2 are conditions;
 If v is a variable and  is a condition, then v is a condition.
We will use e and ei to denote event terms, f and fi to denote fluent terms, t and ti to
denote timepoint terms, and  and i to denote conditions.
In the event calculus, we circumscribe Initiates, Terminates, and Releases to minimize
unexpected effects of events, circumscribe Happens to minimize unexpected events, and
circumscribe Ab i (abnormality predicates) to minimize abnormalities. Formally, an event
calculus description is a circumscriptive theory defined as
CIRC[ ; Initiates, Terminates, Releases]  CIRC[ ; Happens]
 CIRC[ ; Ab 1 , . . . , Ab n ]  ,

(16)

where
  is a conjunction of universal closures of axioms of the form
  Initiates(e, f, t)
  Terminates(e, f, t)
  Releases(e, f, t)
  1 (e, f1 , t)  2 (e, f2 , t)
(effect constraint)
  []Happens(e1 , t)      []Happens(en , t)  Initiates(e, f, t)
  []Happens(e1 , t)      []Happens(en , t)  Terminates(e, f, t),
where each of 1 and 2 is either Initiates or Terminates ([] means that  is
optional);
  is a conjunction of universal closures of temporal ordering formulas (comparisons
between timepoint terms) and axioms of the form
  Happens(e, t)
(f, t)  1 (f1 , t)      n (fn , t)  Happens(e, t)
(causal constraints)
Happens(e, t)  Happens(e1 , t)      Happens(en , t) (disjunctive event axiom),
where  is Started or Stopped and each j (1  j  n) is either Initiated or
Terminated ;
  is a conjunction of universal closures of cancellation axioms of the form
  Abi (..., t) ;
  is a conjunction of first-order sentences (outside the scope of CIRC) including unique
name axioms, state constraints, event occurrence constraints, and the set of domainindependent axioms in the event calculus, such as EC (for the continuous event calculus) and DEC (for the discrete event calculus) (Mueller, 2006, Chapter 2). It also
579

fiLee & Palla

includes the following definitions of the predicates used in the causal constraints in :
def

Started (f, t)  (HoldsAt(f, t)  e(Happens(e, t)  Initiates(e, f, t)))

(CC1 )

def

Stopped (f, t)  (HoldsAt(f, t)  e(Happens(e, t)  Terminates(e, f, t)))
def

Initiated (f, t)  (Started (f, t)  e(Happens(e, t)  Terminates(e, f, t)))
def

Terminated (f, t)  (Stopped (f, t)  e(Happens(e, t)  Initiates(e, f, t)))

(CC2 )
(CC3 )
(CC4 ).

Remark 1 The following facts are easy to check:
  is canonical relative to {Initiates, Terminates, Releases};
  is canonical relative to {Happens};
  is canonical relative to {Ab 1 , . . . , Ab n }.
These facts are used in the next section to reformulate the event calculus in the general
theory of stable models.
3.2 Reformulating the Event Calculus in the General Theory of Stable Models
Following Ferraris, Lee, Lifschitz, and Palla (2009), about a formula F we say that it
is negative on a list p of predicate constants if members of p have no strictly positive
occurrences in F .4 For example, formula (9) is negative on {p}, but is not negative on {p, q}.
A formula of the form F (shorthand for F  ) is negative on any list of predicates.
We assume that  was already equivalently rewritten so that  is negative on {Initiates,
Terminates, Releases, Happens, Ab 1 , . . . , Ab n }. This can be easily done by prepending 
to strictly positive occurrences of those predicates. The following theorem shows a few
equivalent reformulations of circumscriptive event calculus in the general theory of stable
models.
Theorem 2 For any event calculus description (16), the following theories are equivalent
to each other:5
(a) CIRC[; I, T, R]  CIRC[; H]  CIRC[; Ab 1 , . . . , Ab n ]   ;
(b) SM[; I, T, R]  SM[; H]  SM[; Ab 1 , . . . , Ab n ]   ;
(c) SM[      ; I, T, R, H, Ab 1 , . . . , Ab n ] ;
(d) SM[        Choice(pr (      ) \ {I, T, R, H, Ab 1 , . . . , Ab n })] .
The equivalence between (a) and (b) is immediate from Theorem 1. The equivalence
between (b) and (c) can be shown using the splitting theorem by Ferraris et al. (2009).
The assumption that  is negative on the intensional predicates is essential in showing that
4. Note that we distinguish between a formula being negative (on p) and an occurrence being negative
(Section 2.3).
5. For brevity, we abbreviate the names of circumscribed predicates.

580

fiReformulating the Situation Calculus and the Event Calculus

equivalence (For more details, see the proof in Appendix C.4.). The equivalence between
(c) and (d) follows from Proposition 1 since
{I, T, R, H, Ab 1 , . . . , Ab n } \ pr (      )
is the empty set.6

4. Reformulating the Situation Calculus in the General Theory of Stable
Models
In this section, we review and reformulate two versions of the situation calculusLins
causal action theories (1995) and Reiters basic action theories (2001).
4.1 Review: Lins Causal Action Theories
We assume a many-sorted first-order language which contains a situation sort, an action
sort, a fluent sort, a truth value sort and an object sort. We understand expression P (x, s),
where P is a fluent name, as shorthand for Holds(P (x), s). We do not consider functional
fluents here for simplicity.
According to Lin (1995), a formula (s) is called a simple state formula about s if (s)
does not mention Poss, Caused or any situation term other than possibly the variable s.
We assume that a causal action theory D consists of a finite number of the following
sets of axioms. We often identify D with the conjunction of the universal closures of all
axioms in D. In the following, F , Fi are fluent names, A is an action name, V , Vi are truth
values, s, s0 are situation variables, (s) is a simple state formula about s, symbols a, a0
are action variables, f is a variable of sort fluent, v is a variable of sort truth value, and x,
xi , y, yi are lists of variables.
 Dcaused is a conjunction of axioms of the form
Poss(A(x), s)  ((s)  Caused (F (y), V, do(A(x), s))
(direct effect axioms), and
(s)  Caused (F1 (x1 ), V1 , s)      Caused (Fn (xn ), Vn , s)  Caused (F (x), V, s)
(indirect effect axioms).
 Dposs is a conjunction of precondition axioms of the form
Poss(A(x), s)  (s).

(17)

 Drest is a conjunction of the following axioms:
 The basic axioms:
Caused (f, true, s)  Holds(f, s),
Caused (f, false, s)  Holds(f, s),
true 6= false  v(v = true  v = false).
6. I, T , R, H occur in the domain independent axioms as part of .

581

(18)

fiLee & Palla

 The unique name assumptions for fluent names:
Fi (x) 6= Fj (y), (i 6= j)
Fi (x) = Fi (y)  x = y.

(19)

Similarly for action names.
 The foundational axioms for the discrete situation calculus:

7

s 6= do(a, s),
0

0

0

0

do(a, s) = do(a , s )  (a = a  s = s ),



p p(S0 )  a, s p(s)  p(do(a, s))  s p(s) .

(20)
(21)
(22)

 The frame axiom:
Poss(a, s)  (vCaused (f, v, do(a, s))
 (Holds(f, do(a, s))  Holds(f, s))).
 Axioms for other domain knowledge: (s).
A causal action theory is defined as
CIRC[Dcaused ; Caused ]  Dposs  Drest .

(23)

Remark 2 It is easy to check that Dcaused is canonical relative to Caused .
This fact is used in the next section to reformulate causal action theories in the general
theory of stable models.
4.2 Reformulating Causal Action Theories in the General Theory of Stable
Models
Let Dposs  be the conjunction of axioms (s)  Poss(A(x), s) for each axiom (17) in Dposs .
Instead of the second-order axiom (22), we consider the following first-order formula Dsit ,
which introduces a new intensional predicate constant Sit whose argument sort is situation.8
Sit(S0 )  a, s(Sit(s)  Sit(do(a, s)))  sSit(s).

(24)


In the following, Drest
is the theory obtained from Drest by dropping (22).

Theorem 3 Given a causal action theory (23), the following theories are equivalent to each
other when we disregard the auxiliary predicate Sit:
(a) CIRC[Dcaused ; Caused ]  Dposs  Drest ;

(b) SM[Dcaused ; Caused ]  Dposs  Drest
 SM[Dsit ; Sit] ;

(c) SM[Dcaused ; Caused ]  SM[Dposs  ; Poss]  Drest
 SM[Dsit ; Sit] ;

(d) SM[Dcaused  Dposs   Drest
 Dsit ; Caused , Poss, Sit] .

7. For simplicity we omit two other axioms regarding the partial-order among situations.
8. Suggested by Vladimir Lifschitz (personal communication).

582

fiReformulating the Situation Calculus and the Event Calculus

4.3 Review: Reiters Basic Action Theories
As in causal action theories, we understand P (x, s), where P is a fluent name, as shorthand
for Holds(P (x), s), and do not consider functional fluents.
A basic action theory (BAT) is of the form
  Dss  Dap  Duna  DS0 ,

(25)

where
  is the conjunction of the foundational axioms (Section 4.1);
 Dss is a conjunction of successor state axioms of the form
F (x, do(a, s))  F (x, a, s),
where F (x, a, s) is a formula that is uniform in s
among x, a, s;

9

and whose free variables are

 Dap is a conjunction of action precondition axioms of the form
Poss(A(x), s)  A (x, s),
where A (x, s) is a formula that is uniform in s and whose free variables are among x, s;
 Duna is the conjunction of unique name axioms for fluents and actions;
 DS0 is a conjunction of first-order formulas that are uniform in S0 .
4.4 Reformulating Basic Action Theories in the General Theory of Stable
Models
Note that a BAT is a theory in first-order logic.10 In view of the fact that any first-order
logic sentence F is equivalent to SM[F ; ], it is trivial to view a BAT as a first-order theory
under the stable model semantics with the list of intensional predicates being empty.
In the rest of this section, we consider an alternative encoding of BAT in ASP, in which
we do not need to provide explicit successor state axioms Dss . Instead, the successor state
axioms are entailed by the effect axioms and the generic inertia axioms adopted in ASP by
making intensional both the positive predicate Holds and the negative predicate Holds
(Recall the definitions of positive and negative predicates in Section 2.2). In the following
we assume that the underlying signature contains both these predicates.
An ASP-style BAT is of the form
  Deffect  Dprecond  Dinertia  Dexogenous0  Duna  DS0 ,
where
 , Duna and DS0 are defined as before;
9. We refer the reader to the book by Reiter (2001) for the definition of a uniform formula.
10. For simplicity we disregard the second-order axiom (22).

583

(26)

fiLee & Palla

 Deffect is a conjunction of axioms of the form
+
R
(x, a, s)  Holds(R(x), do(a, s))

(27)


R
(x, a, s)  Holds(R(x), do(a, s)),

(28)

or
+
R
(x, a, s)


R
(x, a, s)

where
and
variables are among x, a and s;

are formulas that are uniform in s and whose free

 Dprecond is a conjunction of axioms of the form
A (x, s)  Poss(A(x), s),

(29)

where A (x, s) is a formula that is uniform in s and whose free variables are among x, s;
 Dinertia is the conjunction of the axioms
Holds(R(x), s)   Holds(R(x), do(a, s))  Holds(R(x), do(a, s)),
Holds(R(x), s)  Holds(R(x), do(a, s))  Holds(R(x), do(a, s))
for all fluent names R;
 Dexogenous0 is the conjunction of
Holds(R(x), S0 ) Holds(R(x), S0 )
for all fluent names R.
Note that axioms in Dinertia are typically used in answer set programming to represent
the common sense law of inertia (Lifschitz & Turner, 1999). Similarly, Dexogenous0 is used
to represent that the initial value of a fluent is arbitrary.11
We will show how this ASP-style BAT is related to Reiters BAT. First, since we use
strong negation, it is convenient to define the following notions. Given the signature  of
a BAT,  Holds is the signature obtained from  by adding Holds to . We say that an
interpretation I of  Holds is complete on Holds if it satisfies
y(Holds(y) Holds(y)),
where y is a list of distinct variables. Given an interpretation I of  Holds , expression I|
denotes the projection of I on .
Let Dss be the conjunction of successor state axioms

Holds(R(x), do(a, s))  +
R (x, a, s)  (Holds(R(x), s)  R (x, a, s)),
+

where +
R (x, a, s) is the disjunction of R (x, a, s) for all axioms (27) in Deffect , and R (x, a, s)

is the disjunction of R (x, a, s) for all axioms (28) in Deffect . By Dap we denote the conjunction of axioms Poss(A(x), s)  A (x, s), where A (x, s) is the disjunction of A (x, s)
for all axioms (29) in Dprecond .

11. The axioms Dinertia and Dexogenous0 are also closely related to the translation of C+ into nonmonotonic
causal logic (Giunchiglia, Lee, Lifschitz, McCain, & Turner, 2004).

584

fiReformulating the Situation Calculus and the Event Calculus

Theorem 4 Let T be a theory (26) of signature  Holds , and I a coherent interpretation of
 Holds that is complete on Holds. If I satisfies

x a s(+
R (x, a, s)  R (x, a, s))

for every fluent name R, then I satisfies
SM[T ; Poss, Holds, Holds]
iff I| satisfies the BAT
  Dss  Dap  Duna  DS0 .

5. Translating Almost Universal Sentences into Logic Programs
Theorems 24 present reformulations of the situation calculus and the event calculus in the
general theory of stable models, which may contain nested quantifiers and connectives. On
the other hand, the input languages of ASP solvers are limited to simple rule forms, which
are analogous to clausal normal form in classical logic. Although any first-order formula can
be rewritten in clausal normal form while preserving satisfiability, such transformations do
not necessarily preserve stable models. This is due to the fact that the notion of equivalence
is stronger under the stable model semantics (Lifschitz, Pearce, & Valverde, 2001).
Definition 4 (Ferraris et al., 2011) A formula F is strongly equivalent to formula G if,
for any formula H containing F as a subformula (and possibly containing object, function
and predicate constants that do not occur in F , G), and for any list p of distinct predicate
constants, SM[H; p] is equivalent to SM[H 0 ; p], where H 0 is obtained from H by replacing
an occurrence of F by G.
In other words, replacing a subformula with another strongly equivalent subformula
does not change the stable models of the whole formula. While strongly equivalent theories
are classically equivalent (i.e., equivalent under classical logic), the converse does not hold.
Consequently, classically equivalent transformations do not necessarily preserve stable models. For instance, consider p and p. When p is intensional, the former has stable models
and the latter does not.
It is known that every propositional formula can be rewritten as a logic program (Cabalar
& Ferraris, 2007; Cabalar, Pearce, & Valverde, 2005; Lee & Palla, 2007), and such translations can be extended to quantifier-free formulas in a straightforward way (Section 5.1).
However, the method does not work in the presence of arbitrary quantifiers, because in the
target formalism (logic programs), all variables are implicitly universally quantified.
In this section, we present a translation that turns a certain class of sentences called
almost universal sentences into logic programs while preserving stable models. It turns
out that the reformulations of the situation calculus and the event calculus in Sections 3
and 4 belong to the class of almost universal sentences, so that we can use ASP solvers for
computing them.
585

fiLee & Palla

5.1 Translating Quantifier-Free Formulas into Logic Programs
Cabalar et al. (2005) define the following transformation that turns any propositional formula under the stable model semantics into a logic program.
 Left side rules:
>F G

7

{F  G}

(L1)

F G

7



(L2)

F  G  H

7

(L3)

(F  G)  H  K

7

(F  G)  H  K

7

{G  F  H}


F H K
GH K



 F  H  K
GH K


H  F  G  K

F G

7

{F  G}

(R1)

F >G

7



(R2)

F  G  H

7

(R3)

F  (G  H)  K

7

F  (G  H)  K

7

{G  F  H}


F GK
F H K


GF H K
H  F  G  K

(L4)

(L5)

 Right side rules:

(R4)
(R5)

Before applying this transformation to each formula on the lefthand side, we assume
that the formula is already written in negation normal form, in which negation is applied
to literals only, by using the following transformation:
 Negation normal form conversion:
>

F
(F  G)
(F  G)
(F  G)

7
7

7

7

7

7



>
F
F  G
F  G
F  G

According to Cabalar et al. (2005), successive application of the rewriting rules above
turn any propositional formula into a disjunctive logic program. This result can be simply
extended to turn any quantifier-free formula into a logic program.
As noted by Cabalar et al. (2005), this translation may involve an exponential blowup in
size, and Theorem 1 from their paper shows that indeed there is no vocabulary-preserving
polynomial time algorithm to convert general propositional theories under the stable model
semantics into disjunctive logic programs. Alternatively, one can use another translation
from the same paper, which is linear in size but involves auxiliary atoms and is more
complex.
586

fiReformulating the Situation Calculus and the Event Calculus

5.2 Quantifier Elimination
We introduce a quantifier elimination method that distinguishes between two kinds of occurrences of quantifiers: singular and non-singular. Any non-singular occurrence of
a quantifier is easy to eliminate, while a singular occurrence is eliminated under a certain
syntactic condition.
Definition 5 We say that an occurrence of QxG in F is singular if
 Q is , and the occurrence of QxG is positive in F , or
 Q is , and the occurrence of QxG is negative in F .
For example, the occurrence of x q(x) is singular in (11), but the occurrence of x p(x) is
not.
Non-singular occurrences of quantifiers can be eliminated in view of the fact that every
first-order sentence can be rewritten in prenex form. The prenex form conversion rules given
in Section 6.3.1 of Pearce and Valverde (2005) preserve strong equivalence, which leads to
the following theorem.12
Theorem 5 (Lee & Palla, 2007, Proposition 5) Every first-order formula is strongly
equivalent to a formula in prenex form.
The prenex form conversion turns a non-singular occurrence of a quantifier into an
outermost  while preserving strong equivalence. Consequently, if a sentence contains no
singular occurrence of a quantifier, then the above results can be used to turn the sentence
into a universal sentence and then into a set of ASP rules. However, in the presence of a
singular occurrence of a quantifier, the prenex form conversion turns the occurrence into
an outermost , which is not allowed in logic programs. Below we consider how to handle
such occurrences.
Obviously, if the Herbrand universe is finite, and if we are interested in Herbrand stable
models (i.e., answer sets) only, quantified formulas can be rewritten as multiple disjunctions
and conjunctions. We do not even need to consider turning the formula into prenex form.
For example, for a formula
r  x(p(x)  q(x))  s

(30)

occurring in a theory whose signature contains {1, . . . , n} as the only object constants (and
no other function constants), if we replace x(p(x)  q(x)) with multiple disjunctions and
then turn the resulting program with nested expressions into a usual disjunctive program
(Lifschitz, Tang, & Turner, 1999), 2n rules are generated. For instance, if n = 3, the
12. Pearce and Valverde (2005) show that a sentence in QNc5 , the monotonic basis of Quantified Equilibrium
Logic, can be turned into prenex form, from which the result follows.

587

fiLee & Palla

resulting logic program is
s  r, not
s  r, not
s  r, not
s  r, not
s  r, not
s  r, not
s  r, not
s  r, not

p(1), not
p(1), not
p(1), not
p(1), not
q(1), not
q(1), not
q(1), not
q(1), not

p(2), not
p(2), not
q(2), not
q(2), not
p(2), not
p(2), not
q(2), not
q(2), not

p(3)
q(3)
p(3)
q(3)
p(3)
q(3)
p(3)
q(3).

Also, the translation is not modular as it depends on the underlying domain; the multiple
disjunctions or conjunctions need to be updated when the domain changes. More importantly, this method is not applicable if the theory contains function constants of positive
arity, as its Herbrand universe is infinite.
One may also consider introducing Skolem constants as in first-order logic, presuming
that, for any sentence F and its Skolem form F 0 , SM[F ; p] is satisfiable iff SM[F 0 ; p] is
satisfiable. However, this idea does not work.13
Example 2 For formula
F = (x p(x)  q)  x(q  p(x)),
SM[F ; q] is equivalent to the first-order sentence
(q  x p(x))  x(q  p(x)),
which is unsatisfiable (the equivalence can be established using Theorems 3 and 11 from Ferraris et al., 2011). Formula F is strongly equivalent to its prenex form

xy (p(x)  q)  (q  p(y)) ,

(31)

However, if we introduce new object constants a and b to replace the existentially quantified
variables as in
F 0 = (p(a)  q)  (q  p(b)),
formula SM[F 0 ; q] is equivalent to
(q  p(a))  (q  p(b)),
which is satisfiable.
Here we present a method of eliminating singular occurrences of quantifiers by introducing auxiliary predicates. Our idea is a generalization of the practice in logic programming
13. Pearce and Valverde (2005) show that Skolemization works with QNc5 , the monotonic basis of Quantified Equilibrium Logic, but as our example shows, this does not imply that Skolemization works with
Quantified Equilibrium Logic.

588

fiReformulating the Situation Calculus and the Event Calculus

that simulates negated existential quantification in the body of a rule by introducing auxiliary predicates. For instance, in order to eliminate  in (30), we will introduce a new
predicate constant p0 , and turn (30) into
(r  p0  s)  x(p(x)  q(x)  p0 ),

(32)

which corresponds to the logic program
s  r, not p0
p0  p(x), q(x).

(33)

The models of SM[(30); p, q, r, s] are the same as the stable models of (33) if we disregard
p0 . This method does not involve grounding, so that the translation does not depend on
the domain and is not restricted to Herbrand models. The method is formally justified by
the following proposition.
Recall that a formula H is negative on p if members of p have no strictly positive
occurrences in H. Given a formula F , we say that an occurrence of a subformula G is
p-negated in F if it is contained in a subformula H of F that is negative on p.
Proposition 2 Let F be a sentence, let p be a finite list of distinct predicate constants,
and let q be a new predicate constant that does not occur in F . Consider any non-strictly
positive, p-negated occurrence of yG(y, x) in F , where x is the list of all free variables of
yG(y, x). Let F 0 be the formula obtained from F by replacing that occurrence of yG(y, x)
with q(x). Then
SM[F ; p]  x(q(x)  yG(y, x))
is equivalent to
SM[F 0  xy(G(y, x)  q(x)); p, q].
Proposition 2 tells us that SM[F ; p] and SM[F 0  xy(G(y, x)  q(x); p, q] have the
same models if we disregard the new predicate constant q. Notice that F 0 does not retain
the occurrence of y.
Example 3 In formula (30), x(p(x)  q(x)) is contained in a negative formula (relative
to any set of intensional predicates). In accordance with Proposition 2, SM[(30); p, q, r, s]
has the same models as SM[(32); p, q, r, s, p0 ] if we disregard p0 .
Any singular, p-negated occurrence of a subformula yG(y, x) can also be eliminated
using Proposition 2 by first rewriting yG(y, x) as yG(y, x). Note that yG(y, x) is
not strongly equivalent to yG(y, x), and in general such a classically equivalent transformation may not necessarily preserve stable models. However, the Theorem on Double
Negations (Ferraris et al., 2009, also reviewed in Appendix C) tells us that such a transformation is ensured to preserve p-stable models if the replaced occurrence is p-negated in
the given formula.
Now we are ready to present our quantifier elimination method, which applies to the
class of almost universal formulas.
589

fiLee & Palla

Definition 6 We say that a formula F is almost universal relative to p if every singular
occurrence of QxG in F is p-negated in F .
For example, formula (30) is almost universal relative to any set of predicates because the
only singular occurrence of x(p(x)  q(x)) in (30) is contained in x(p(x)  q(x)), which
is negative on any list of predicates. Formula F in Example 2 is almost universal relative
to {q} because the singular occurrence of x p(x) is contained in the formula itself, which is
negative on {q}, and the singular occurrence of x(q p(x)) is contained in x(q p(x)),
which is also negative on {q}.
The following procedure can be used to eliminate all (possibly nested) quantifiers in any
almost universal sentence.
Definition 7 (Translation elim-quantifiers) Given a formula F , first prepend  to
every maximal strictly positive occurrence of a formula of the form yH(y, x),14 and then
repeat the following process until there are no occurrences of quantifiers remaining: Select
a maximal occurrence of a formula of the form QyG(y, x) in F , where Q is  or , and x
is the list of all free variables in QyG(y, x).
(a) If the occurrence of QyG(y, x) in F is non-singular in F , then set F to be the formula
obtained from F by replacing the occurrence of QyG(y, x) with G(z, x), where z is a
new variable.
(b) Otherwise, if Q is  and the occurrence of QyG(y, x) in F is positive, then set F to
be
F 0  (G(y, x)  pG (x)),
where pG is a new predicate constant and F 0 is the formula obtained from F by replacing the occurrence of QyG(y, x) with pG (x).
(c) Otherwise, if Q is  and the occurrence of QyG(y, x) in F is negative, then set F
to be the formula obtained from F by replacing the occurrence of QyG(y, x) with
yG(y, x).
We assume that the new predicate constants introduced by the translation do not belong
to the signature of the input formula F . It is clear that this process terminates, and yields
a formula that is quantifier-free. Since the number of times step (b) is applied is no more
than the number of quantifiers in the input formula, and the new formulas added have the
size polynomial to the input formula, it follows that the size of the resulting quantifier-free
formula is polynomial in the size of the input formula.
The following theorem tells us that any almost universal sentence F can be turned into
the form xG, where G is a quantifier-free formula. For any (second-order) sentences F
and G of some signature and any subset  of that signature, we say that F is -equivalent
to G, denoted by F  G, if the class of models of F restricted to  is identical to the class
of models of G restricted to .
14. The maximality is understood here in terms of subformula relation. That is, we select a strictly positive
occurrence of a subformula of F of the form yH(y, x) that is not contained in any other subformula of
F of the same form.

590

fiReformulating the Situation Calculus and the Event Calculus

Theorem 6 Let F be a sentence of a signature , let F 0 be the universal closure of the
formula obtained from F by applying translation elim-quantifiers, and let q be the list
of new predicate constants introduced by the translation. If F is almost universal relative
to p, then SM[F ; p] is -equivalent to SM[F 0 ; p, q].
The statement of the theorem becomes incorrect if we do not require F to be almost
universal relative to p. For instance, if elim-quantifiers is applied to x p(x), it results
in q  (p(x)  q). However, SM[x p(x); p] is not {p}-equivalent to
SM[x(q  (p(x)  q)); p, q]. The former is equivalent to saying that p is a singleton.
The latter is equivalent to q  xp(x)  (q  xp(x)), which is inconsistent.
5.3 f2lp: Computing Answer Sets of First-Order Formulas
Using translation elim-quantifiers defined in the previous section, we introduce translation f2lp that turns an almost universal formula into a logic program. We assume that the
underlying signature contains finitely many predicate constants.
Definition 8 (Translation f2lp)
1. Given a formula F and a list of intensional predicates p, apply translation elim-quantifiers (Definition 7) to F ;
2. Add choice formulas (q(x)  q(x)) for all non-intensional predicates q.
3. Turn the resulting quantifier-free formula into a logic program by applying the translation from Section 3 of the paper by Cabalar et al. (2005), which was also reviewed
in Section 5.1.
As explained in Section 5.1, due to the third step, this transformation may involve an
exponential blowup in size. One can obtain a polynomial translation by replacing Step 3
with an alternative translation given in Section 4 of the paper by Cabalar et al.
The following theorem asserts the correctness of the translation.
Theorem 7 Let F be a sentence of a signature , let p be a list of intensional predicates,
and let F 0 be the FOL representation of the program obtained from F by applying translation
f2lp with p as intensional predicates. If F is almost universal relative to p, then SM[F ; p]
is -equivalent to
SM[F 0  False(p \ pr (F 0 ))].
Example 4 Consider one of the domain independent axioms in the discrete event calculus
(DEC5 axiom):
HoldsAt(f, t)  ReleasedAt(f, t+1)
e(Happens(e, t)  Terminates(e, f, t))  HoldsAt(f, t+1).
Step 1 of translation f2lp introduces the formula
Happens(e, t)  Terminates(e, f, t)  q(f, t),
and replaces (34) with
HoldsAt(f, t)  ReleasedAt(f, t+1)  q(f, t)  HoldsAt(f, t+1).
591

(34)

fiLee & Palla

Step 3 turns these formulas into rules
q(f, t)  Happens(e, t), Terminates(e, f, t)
HoldsAt(f, t+1)  HoldsAt(f, t), not ReleasedAt(f, t+1), not q(f, t).
Turning the program obtained by applying translation f2lp into the input languages of
lparse 15 and gringo 16 requires minor rewriting, such as moving equality and negated
atoms in the head to the body 17 and adding domain predicates in the body for all variables
occurring in the rule in order to reduce the many-sorted signature into the non-sorted one.18
System f2lp is an implementation of translation f2lp, which turns a first-order formula
into the languages of lparse and gringo. The system can be downloaded from its home
page
http://reasoning.eas.asu.edu/f2lp .
First-order formulas can be encoded in f2lp using the extended rule form F  G, where
F and G are first-order formulas that do not contain . The ASCII representation of the
quantifiers and connectives are shown in the following table.
Symbol
ASCII


not


-


&


|


<-


false

>
true

xyz
![X,Y,Z]:

xyz
?[X,Y,Z]:

For example, formula (34) can be encoded in the input language of f2lp as
holdsAt(F,T+1) <- holdsAt(F,T) & not releasedAt(F,T+1) &
not ?[E]:(happens(E,T) & terminates(E,F,T)).

The usual lparse and gringo rules (which have the rule arrow :-) are also allowed
in f2lp. Such rules are simply copied to the output. The program returned by f2lp can
be passed to ASP grounders and solvers that accept lparse and gringo languages.

6. Computing the Event Calculus Using ASP Solvers
Using translation f2lp, we further turn the event calculus reformulation in Section 3.2 into
answer set programs. The following procedure describes the process.
Definition 9 (Translation ec2asp)
1. Given an event calculus description (16), rewrite
all the definitional axioms of the form
def

x(p(x)  G)

(35)

in  as x(G  p(x)), where G is obtained from G by prepending  to
all occurrences of intensional predicates Initiates, Terminates, Releases, Happens,
Ab 1 , . . . , Ab n . Also prepend  to the strictly positive occurrences of the intensional
predicates in the remaining axioms of . Let 0 be the resulting formula obtained
from .
15.
16.
17.
18.

http://www.tcs.hut.fi/Software/smodels
http://potassco.sourceforge.net
For instance, (X=Y) | -q(X,Y) :- p(X,Y) is turned into :- X!=Y, {not q(X,Y)}0, p(X,Y).
Alternatively this can be done by declaring variables using the #domain directive in lparse and gringo
languages.

592

fiReformulating the Situation Calculus and the Event Calculus

2. Apply translation f2lp on       0 with the intensional predicates
{Initiates, Terminates, Releases, Happens, Ab 1 , . . . , Ab n }  p,
where p is the set of all predicate constants p in (35) as considered in Step 1.
The following theorem states the correctness of the translation.
Theorem 8 Let T be an event calculus description (16) of signature  that contains finitely
many predicate constants, let F be the FOL representation of the program obtained from T
by applying translation ec2asp. Then T is -equivalent to SM[F ].
In view of the theorem, system f2lp can be used to compute event calculus descriptions
by a simple rewriting as stated in translation ec2asp.19 The system can be used in place
of the dec reasoner in many existing applications of the event calculus, such as in robotics,
security, video games, and web service composition, as listed in
http://decreasoner.sourceforge.net/csr/decapps.html .
The computational mechanism of the dec reasoner is similar to our method as it is
based on the reduction of event calculus reasoning to propositional satisfiability and uses
efficient SAT solvers for computation. However, our method has some advantages.
First, it is significantly faster due to the efficient grounding mechanisms implemented
in ASP systems. This is evidenced in some experiments reported in Appendix B.
Second, f2lp allows us to compute the full version of the event calculus, assuming that
the domain is given and finite. On the other hand, the reduction implemented in the dec
reasoner is based on completion, which is weaker than circumscription. This makes the
system unable to handle recursive axioms and disjunctive axioms, such as effect constraints
and disjunctive event axioms (Section 3.1). For example, the dec reasoner does not allow
the following effect constraints which describe the indirect effects of an agents walking on
the objects that he is holding:
HoldsAt(Holding(a, o), t)  Initiates(e, InRoom(a, r), t)
 Initiates(e, InRoom(o, r), t)
HoldsAt(Holding(a, o), t)  Terminates(e, InRoom(a, r), t)
 Terminates(e, InRoom(o, r), t).

(36)

Third, we can enhance the event calculus reasoning by combining ASP rules with the
event calculus description. In other words, the event calculus can be viewed as a high
level action formalism on top of ASP. We illustrate this using the example from the work
of Dogandag, Ferraris, and Lifschitz (2004). There are 9 rooms and 12 doors as shown in
Figure 1. Initially the robot Robby is in the middle room and all the doors are closed.
The goal of the robot is to make all rooms accessible from each other. Figure 2 (File robby)
shows an encoding of the problem in the language of f2lp. Atom door(x, y) denotes that
there is a door between rooms x and y; open(x, y) denotes the event Robby opening the door
19. Kim, Lee, and Palla (2009) presented a prototype of f2lp called ecasp that is tailored to the event
calculus computation.

593

fiLee & Palla

Figure 1: Robbys apartment in a 3  3 grid
between rooms x and y; goto(x) denotes the event Robby going to room x; opened(x, y)
denotes that the door between x and y has been opened; inRoom(x) denotes that Robby
is in room x; accessible(x, y) denotes that y is accessible from x. Note that the rules
defining the relation accessible are not part of event calculus axioms (Section 3.1). This
example illustrates an advantage of allowing ASP rules in event calculus descriptions.
The minimal number of steps to solve the given problem is 11. We can find such a
plan using the combination of f2lp, gringo (grounder) and claspD (solver for disjunctive
programs) in the following way. 20
$ f2lp dec robby | gringo -c maxstep=11 | claspD
File dec is an f2lp encoding of the domain independent axioms in the Discrete Event
Calculus (The file is listed in Appendix A).21 The following is one of the plans found:
happens(open(5,8),0) happens(open(5,2),1) happens(open(5,4),2)
happens(goto(4),3) happens(open(4,1),4) happens(open(4,7),5)
happens(goto(5),6) happens(open(5,6),7) happens(goto(6),8)
happens(open(6,9),9) happens(open(6,3),10)

7. Computing the Situation Calculus Using ASP Solvers
Using translation f2lp, we further turn the situation calculus reformulations in Sections 4.2
and 4.4 into answer set programs.
7.1 Representing Causal Action Theories by Answer Set Programs
The following theorem shows how to turn causal action theories into answer set programs.
Theorem 9 Let D be a finite causal action theory (23) of signature  that contains finitely
many predicate constants, and let F be the FOL representation of the program obtained by
applying translation f2lp on

Dcaused  Dposs   Drest
 Dsit

(37)

with the intensional predicates {Caused , Poss, Sit}. Then D is -equivalent to SM[F ].
20. One can use clingo instead of gringo and claspD if the output of f2lp is a nondisjunctive program.
21. The file is also available at http://reasoning.eas.asu.edu/f2lp, along with f2lp encodings of the
domain independent axioms in other versions of the event calculus.

594

fiReformulating the Situation Calculus and the Event Calculus

% File robby
% objects
step(0..maxstep).
astep(0..maxstep-1) :- maxstep > 0.
room(1..9).
% variables
#domain step(T).
#domain room(R).
#domain room(R1).
#domain room(R2).
% position of the
door(R1,R2) <- R1
door(R1,R2) <- R1
door(R1,R2) <- R1
door(R1,R2) <- R2

doors
>= 1 &
>= 4 &
>= 7 &
< 10 &

R2
R2
R2
R2

>=1 & R1 < 4 & R2 < 4 & R2 = R1+1.
>= 4 & R1 < 7 & R2 < 7 & R2 = R1+1.
>= 7 & R1 < 10 & R2 < 10 & R2 = R1+1.
= R1+3.

door(R1,R2) <- door(R2,R1).
% fluents
fluent(opened(R,R1)) <- door(R1,R2).
fluent(inRoom(R)).
% F ranges over the fluents
#domain fluent(F).
% events
event(open(R,R1)) <- door(R,R1).
event(goto(R)).
% E and E1 range over the events
#domain event(E).
#domain event(E1).
% effect axioms
initiates(open(R,R1),opened(R,R1),T).
initiates(open(R,R1),opened(R1,R),T).
initiates(goto(R2),inRoom(R2),T)
<- holdsAt(opened(R1,R2),T) & holdsAt(inRoom(R1),T).
terminates(E,inRoom(R1),T)
<- holdsAt(inRoom(R1),T) & initiates(E,inRoom(R2),T).
% action precondition axioms
holdsAt(inRoom(R1),T) <- happens(open(R1,R2),T).

595

fiLee & Palla

% event occurrence constraint
not happens(E1,T) <- happens(E,T) & E != E1.
% state constraint
not holdsAt(inRoom(R2),T) <- holdsAt(inRoom(R1),T) & R1 != R2.
% accessibility
accessible(R,R1,T) <- holdsAt(opened(R,R1),T).
accessible(R,R2,T) <- accessible(R,R1,T) & accessible(R1,R2,T).
% initial state
not holdsAt(opened(R1,R2),0).
holdsAt(inRoom(5),0).
% goal state
not not accessible(R,R1,maxstep).
% happens is exempt from minimization in order to find a plan.
{happens(E,T)} <- T < maxstep.
% all fluents are inertial
not releasedAt(F,0).

Figure 2: Robby in f2lp

Similar to the computation of the event calculus in Section 6, the Herbrand stable
models of (37) can be computed using f2lp and answer set solvers. The input to f2lp can
be simplified as we limit attention to Herbrand models. We can drop axioms (18)(21) as
they are ensured by Herbrand models. Also, in order to ensure finite grounding, instead of
Dsit , we include the following set of rules situation in the input to f2lp.
nesting(0,s0).
nesting(L+1,do(A,S)) <- nesting(L,S) & action(A) & L < maxdepth.
situation(S) <- nesting(L,S).
final(S) <- nesting(maxdepth,S).

situation is used to generate finitely many situation terms whose depth is up to maxdepth,
the value that can be given as an option in invoking gringo. Using the splitting theorem
(Section C.1), it is not difficult to check that if a program  containing these rules has
no occurrence of predicate nesting in any other rules and has no occurrence of predicate situation in the head of any other rules, then every answer set of  contains
atoms situation(do(am , do(am1 , do(. . . , do(a1 , s0))))) for all possible sequences of actions
a1 , . . . , am for m = 0, . . . , maxdepth. Though this program does not satisfy syntactic conditions, such as -restricted (Gebser, Schaub, & Thiele, 2007), -restricted (Syrjanen, 2004),
or finite domain programs (Calimeri, Cozza, Ianni, & Leone, 2008), that answer set solvers
usually impose in order to ensure finite grounding, the rules can still be finitely grounded
596

fiReformulating the Situation Calculus and the Event Calculus

% File: suitcase
value(t).
value(f).

lock(l1).

lock(l2).

#domain value(V).
#domain lock(X).
fluent(up(X)).
fluent(open).
#domain fluent(F).
action(flip(X)).
#domain action(A).
depth(0..maxdepth).
#domain depth(L).
% defining the situation domain
nesting(0,s0).
nesting(L+1,do(A,S)) <- nesting(L,S) & L < maxdepth.
situation(S) <- nesting(L,S).
final(S) <- nesting(maxdepth,S).
% basic axioms
h(F,S) <- situation(S) & caused(F,t,S).
not h(F,S) <- situation(S) & caused(F,f,S).
% D_caused
caused(up(X),f,do(flip(X),S)) <situation(S) & not final(S) & poss(flip(X),S) & h(up(X),S).
caused(up(X),t,do(flip(X),S)) <situation(S) & not final(S) & poss(flip(X),S) & not h(up(X),S).
caused(open,t,S) <- situation(S) & h(up(l1),S) & h(up(l2),S).
% D_poss
poss(flip(X),S) <- situation(S).
% frame axioms
h(F,do(A,S)) <h(F,S) & situation(S) & not final(S) & poss(A,S)
& not ?[V]:caused(F,V,do(A,S)).
not h(F,do(A,S)) <not h(F,S) & situation(S) & not final(S) & poss(A,S)
& not ?[V]:caused(F,V,do(A,S)).
% h is non-intensional.
{h(F,S)} <- situation(S).

Figure 3: Lins Suitcase in the language of f2lp
597

fiLee & Palla

by gringo Version 3.x, which does not check such syntactic conditions.22 It is not difficult
to see why the program above leads to finite grounding since we provide an explicit upper
limit for the nesting depth of function do.
In addition to situation , we use the following program executable in order to represent
the set of executable situations (Reiter, 2001):
executable(s0).
executable(do(A,S)) <- executable(S) & poss(A,S) & not final(S)
& situation(S) & action(A).

Figure 3 shows an encoding of Lins suitcase example (1995) in the language of f2lp
(h is used to represent Holds), which describes a suitcase that has two locks and a spring
loaded mechanism which will open the suitcase when both locks are up. This example
illustrates how the ramification problem is handled in causal action theories. Since we fix
the domain of situations to be finite, we require that actions not be effective in the final
situations. This is done by introducing atom final(S).
Consider the simple temporal projection problem by Lin (1995). Initially the first lock
is down and the second lock is up. What will happen if the first lock is flipped? Intuitively,
we expect both locks to be up and the suitcase to be open. We can automate the reasoning
by using the combination of f2lp, gringo and claspD. First, we add executable and the
following rules to the theory in Figure 3. In order to check if the theory entails that flipping
the first lock is executable, and that the suitcase is open after the action, we encode the
negation of these facts in the last rule.
% initial situation
<- h(up(l1),s0).
h(up(l2),s0).
% query
<- executable(do(flip(l1),s0)) & h(open,do(flip(l1),s0)).

We check the answer to the temporal projection problem by running the command:
$ f2lp suitcase | gringo -c maxdepth=1 | claspD

claspD returns no answer set as expected.
Now, consider a simple planning problem for opening the suitcase when both locks are
initially down. We add executable and the following rules to the theory in Figure 3. The
last rule encodes the goal.
% initial situation
<- h(up(l1),s0).
<- h(up(l2),s0).
<- h(open,s0).
% goal
<- not ?[S]: (executable(S) & h(open,S)).

When maxdepth is 1, the combined use of f2lp, gringo and claspD results in no
answer sets, and when maxdepth is 2, it finds the unique answer set that contains both
22. Similarly, system dlv-complex allows us to turn off the finite domain checking (option -nofdcheck).
That system was used in a conference paper (Lee & Palla, 2010) that this article is based on.

598

fiReformulating the Situation Calculus and the Event Calculus

h(open, do(flip(l2), do(flip(l1), s0))) and h(open, do(flip(l1), do(flip(l2), s0))), each
of which encodes a plan. In other words, the single answer set encodes multiple plans
in different branches of the situation tree, which allows us to combine information about
the different branches in one model. This is an instance of hypothetical reasoning that is
elegantly handled in the situation calculus due to its branching time structure. Belleghem,
Denecker, and Schreye (1997) note that the linear time structure of the event calculus is
limited to handle such hypothetical reasoning allowed in the situation calculus.
7.2 Representing Basic Action Theories by Answer Set Programs
Since a BAT T (not including the second-order axiom (22)) can be viewed as a first-order
theory under the stable model semantics with the list of intensional predicates being empty,
it follows that f2lp can be used to turn T into a logic program. As before, we focus on
ASP-style BAT.
Theorem 10 Let T be a ASP-style BAT (26) of signature  that contains finitely many
predicate constants, and let F be the FOL representation of the program obtained by applying translation f2lp on T with intensional predicates {Holds,  Holds, Poss}. Then
SM[T ; Holds, Holds, Poss] is -equivalent to SM[F ; (F )  {Poss}].
Figure 4 shows an encoding of the broken object example discussed by Reiter (1991).
Consider the simple projection problem of determining if an object o, which is next to
bomb b, is broken after the bomb explodes. We add executable and the following rules to
the theory in Figure 4.
% initial situation
not h(broken(o),s0) & h(fragile(o),s0) & h(nexto(b,o),s0).
not h(holding(p,o),s0) & not h(exploded(b),s0).
% query
<- executable(do(explode(b),s0)) & h(broken(o),do(explode(b),s0)).

The command
$ f2lp broken | gringo -c maxdepth=1 | claspD

returns no answer set as expected.

8. Related Work
Identifying a syntactic class of theories on which different semantics coincide is important
in understanding the relationship between them. It is known that, for tight logic programs
and tight first-order formulas, the stable model semantics coincides with the completion
semantics (Fages, 1994; Erdem & Lifschitz, 2003; Ferraris et al., 2011). This fact helps us
understand the relationship between the two semantics, and led to the design of the answer
set solver cmodels-1 23 that computes answer sets using completion. Likewise the class
of canonical formulas introduced here helps us understand the relationship between the
stable model semantics and circumscription. The class of canonical formulas is the largest
23. http://www.cs.utexas.edu/users/tag/cmodels

599

fiLee & Palla

% File: broken
% domains other than situations
person(p).
object(o).
bomb(b).
#domain person(R).
#domain object(Y).
#domain bomb(B).
fluent(holding(R,Y)).
fluent(broken(Y)).

fluent(nexto(B,Y)).
fluent(exploded(B)).

fluent(fragile(Y)).

action(drop(R,Y)).

action(explode(B)).

action(repair(R,Y)).

#domain fluent(F).
#domain action(A).
depth(0..maxdepth).
#domain depth(L).
% defining the situation domain
nesting(0,s0).
nesting(L+1,do(A,S)) <- nesting(L,S) & L < maxdepth.
situation(S) <- nesting(L,S).
final(S) <- nesting(maxdepth,S).
% Effect Axioms
h(broken(Y),do(drop(R,Y),S)) <- situation(S) & h(fragile(Y),S) & not final(S).
h(broken(Y),do(explode(B),S)) <- situation(S) & h(nexto(B,Y),S) & not final(S).
h(exploded(B),do(explode(B),S)) <- situation(S) & not final(S).
-h(broken(Y),do(repair(R,Y),S)) <- situation(S) & not final(S).
-h(holding(R,Y),do(drop(R,Y),S)) <- situation(S) & not final(S).
% Action precondition axioms
poss(drop(R,Y),S) <- h(holding(R,Y),S) & situation(S).
poss(explode(B),S) <- situation(S) & not h(exploded(B),S).
poss(repair(R,Y),S) <- situation(S) & h(broken(Y),S).
% inertial axioms
h(F,do(A,S)) <- h(F,S) & not -h(F,do(A,S)) & situation(S) & not final(S).
-h(F,do(A,S)) <- -h(F,S) & not h(F,do(A,S)) & situation(S) & not final(S).
% D_exogeneous_0
h(F,s0) | -h(F,s0).
% Consider only those interpretations that are complete on Holds
<- not h(F,S) & not -h(F,S) & situation(S).

Figure 4: Broken object example in the language of f2lp

600

fiReformulating the Situation Calculus and the Event Calculus

syntactic class of first-order formulas identified so far on which the stable models coincide
with the models of circumscription. In other words, minimal model reasoning and stable
model reasoning are indistinguishable on canonical formulas.
Proposition 8 from the work of Lee and Lin (2006) shows an embedding of propositional circumscription in logic programs under the stable model semantics. Our theorem
on canonical formulas is a generalization of this result to the first-order case. Janhunen
and Oikarinen (2004) showed another embedding of propositional circumscription in logic
programs, and implemented the system circ2dlp,24 but their translation appears quite
different from the one by Lee and Lin.
Zhang, Zhang, Ying, and Zhou (2011) show an embedding of first-order circumscription
in first-order stable model semantics. Theorem 3 from that paper is reproduced as follows.25
Theorem 11 (Zhang et al., 2011, Thm. 3) Let F be a formula in negation normal form
and let p be a finite list of predicate constants. Let F  be the formula obtained from F
by replacing every p(t) by p(t), and let F c be the formula obtained from F by replacing
every p(t) by p(t)  Choice(p), where p is in p and t is a list of terms. Then CIRC[F ; p]
is equivalent to SM[F   F c ; p].
In comparison with Theorem 1, this theorem can be applied to characterize circumscription of arbitrary formulas in terms of stable models by first rewriting the formulas into
negation normal form. While Theorem 1 is applicable to canonical formulas only, it does
not require any transformation, and the characterization is bidirectional in the sense that
it can be also viewed as a characterization of stable models in terms of circumscription.
Zhang et al. (2011) also introduce a translation that turns arbitrary first-order formulas
into logic programs, but this work is limited to finite structures only. On the other hand,
our translation f2lp (Definition 8) works for almost universal formulas only, but is not
limited to finite structures.
The situation calculus and the event calculus are widely studied action formalisms,
and there are several papers that compare and relate them (e.g., Belleghem, Denecker, &
Schreye, 1995; Provetti, 1996; Belleghem et al., 1997; Kowalski & Sadri, 1997).
Prolog provides a natural implementation for basic action theories since definitional
axioms can be represented by Prolog rules according to the Clarks theorem (Reiter, 2001,
Chapter 5). The Lloyd-Topor transformation that is used to turn formulas into Prolog rules
is similar to translation f2lp, but the difference is that the former preserves the completion
semantics and the latter preserves the stable model semantics.
Lin and Wang (1999) describe a language that can be used to represent a syntactically
restricted form of Lins causal situation calculus, called clausal causal theories, which does
not allow quantifiers. They show how to translate that language into answer set programs
with strong negation, the answer sets of which are then used to obtain fully instantiated
successor state axioms and action precondition axioms. This is quite different from our
approach, which computes the propositional models of the full situation calculus theories
directly.
Kautz and Selman (1992) introduce linear encodings that are similar to a propositionalized version of the situation calculus (McCarthy & Hayes, 1969). Lin (2003) introduces
24. http://www.tcs.hut.fi/Software/circ2dlp
25. This is a bit simpler than the original statement because some redundancy is dropped.

601

fiLee & Palla

an action description language and describes a procedure to compile an action domain in
that language into a complete set of successor state axioms, from which a STRIPS-like
description can be extracted. The soundness of the procedure is shown with respect to a
translation from action domain descriptions into Lins causal action theories. However, that
procedure is based on completion and as such cannot handle recursive axioms unlike our
approach.
Denecker and Ternovska (2007) present an inductive variant of the situation calculus
represented in ID-logic (Denecker & Ternovska, 2008)classical logic extended with inductive definitions. ID-logic and the first-order stable model semantics appear to be closely
related, but the precise relationship between them has yet to be shown.

9. Conclusion
The first-order stable model semantics is defined similar to circumscription. This paper
takes advantage of that definition to identify a class of formulas on which minimal model
reasoning and stable model reasoning coincide, and uses this idea to reformulate the situation calculus and the event calculus in the first-order stable model semantics. Together
with the translation that turns an almost universal sentence into a logic program, we show
that reasoning in the situation calculus and the event calculus can be reduced to computing
answer sets. We implemented system f2lp, a front-end to ASP solvers that allows us to
compute these circumscriptive action theories. The mathematical tool sets and the system presented in this paper may also be useful in relating other circumscriptive theories
to logic programs. Also, the advances in ASP solvers may improve the computation of
circumscriptive theories.

Acknowledgments
We are grateful to Yuliya Lierler, Vladimir Lifschitz, Erik Mueller, Heng Zhang, Yan Zhang,
and the anonymous referees for their useful comments and discussions. The authors were
partially supported by the National Science Foundation under Grant IIS-0916116.

Appendix A. File dec in the Language of f2lp
File dec encodes the domain independent axioms in the discrete event calculus. This file
is to be used together with event calculus domain descriptions as shown in Section 6.
% File dec
#domain
#domain
#domain
#domain
#domain
#domain
#domain

fluent(F).
fluent(F1).
fluent(F2).
event(E).
time(T).
time(T1).
time(T2).

time(0..maxstep).
602

fiReformulating the Situation Calculus and the Event Calculus

% DEC 1
stoppedIn(T1,F,T2) <- happens(E,T) & T1<T & T<T2 & terminates(E,F,T).
% DEC 2
startedIn(T1,F,T2) <- happens(E,T) & T1<T & T<T2 & initiates(E,F,T).
% DEC 3
holdsAt(F2,T1+T2) <- happens(E,T1) & initiates(E,F1,T1) & T2>0 &
trajectory(F1,T1,F2,T2) & not stoppedIn(T1,F1,T1+T2) & T1+T2<=maxstep.
% DEC 4
holdsAt(F2,T1+T2) <- happens(E,T1) & terminates(E,F1,T1) & 0<T2 &
antiTrajectory(F1,T1,F2,T2) & not startedIn(T1,F1,T1+T2) &
T1+T2<=maxstep.
% DEC 5
holdsAt(F,T+1) <- holdsAt(F,T) & not releasedAt(F,T+1) &
not ?[E]:(happens(E,T) & terminates(E,F,T)) & T<maxstep.
% DEC 6
not holdsAt(F,T+1) <- not holdsAt(F,T) & not releasedAt(F,T+1) &
not ?[E]:(happens(E,T) & initiates(E,F,T)) & T<maxstep.
% DEC 7
releasedAt(F,T+1) <releasedAt(F,T) & not ?[E]:(happens(E,T) &
(initiates(E,F,T) | terminates(E,F,T))) & T<maxstep.
% DEC 8
not releasedAt(F,T+1) <- not releasedAt(F,T) &
not ?[E]: (happens(E,T) & releases(E,F,T)) & T<maxstep.
% DEC 9
holdsAt(F,T+1) <- happens(E,T) & initiates(E,F,T) & T<maxstep.
% DEC 10
not holdsAt(F,T+1) <- happens(E,T) & terminates(E,F,T) & T<maxstep.
% DEC 11
releasedAt(F,T+1) <- happens(E,T) & releases(E,F,T) & T<maxstep.
% DEC 12
not releasedAt(F,T+1) <- happens(E,T) &
(initiates(E,F,T) | terminates(E,F,T)) & T<maxstep.
{holdsAt(F,T)}.
{releasedAt(F,T)}.

603

fiLee & Palla

Problem
(max. step)

dec
reasoner

dec
reasoner (minisat)

f2lp with
lparse + cmodels

f2lp with
gringo + cmodels

f2lp with
gringo + clasp(D)

f2lp with
clingo

BusRide
(15)











0.2s
(0.07s + 0.13s)
A:13174/R:24687

0.14s

Kitchen
Sink (25)

39.0s
(38.9s + 0.1s)
A:1014/C:12109

38.9s
(38.9s + 0.00s)
A:1014/C:12109

0.24s
(0.18s + 0.06s)
A:11970/R:61932

0.20s

Thielscher
Circuit (40)

6.5s
(6.3s + 0.2s)
A:1394/C:42454

6.3s
(6.3s + 0.0s)
A:1394/C:42454

0.12s
(0.09s + 0.03s)
A:4899/R:35545

0.1s

Walking
Turkey (15)





0.00s
(0.00s + 0.00s)
A:316/R:456

0.00s

Falling w/
AntiTraj (15)

141.8s
(141.4s + 0.4s)
A:416/C:3056

141.7s
(141.7s + 0.00s)
A:416/C:3056

0.03s
(0.03s + 0.00s)
A:3702/R:7414

0.03s

Falling w/
Events (25)

59.5s
(59.5s + 0.0s)
A:1092/C:12351

59.4s
(59.4s + 0.0s)
A:1092/C:12351

0.28s
(0.20s + 0.08s)
A:13829/R:71266

0.22s

HotAir
Baloon (15)

32.2s
(32.2s + 0.0s)
A:288/C:1163

32.3s
(32.3s + 0.0s)
A:288/C:1163

0.0s
(0.0s + 0.0s)
A:1063/R:1835

0.01s

Telephone1
(40)

9.3s
(9.2s + 0.1s)
A:5419/C:41590

9.1s
(9.1s + 0.0s)
A:5419/C:41590

0.00s
(0.00s + 0.00s)
A:355/R:555
C:0
0.15s
(0.07s + 0.08s)
A:5269/R:24687
C:5308
0.44s
(0.19s + 0.25s)
A:11970/R:61932
C:0
0.19s
(0.09s + 0.1s)
A:4899/R:35545
C:0
0.00s
(0.00s + 0.00s)
A:316/R:456
C:0
0.04s
(0.02s + 0.02s)
A:3702/R:7414
C:0
0.46s
(0.20s + 0.26s)
A:1219/R:71266
C:1415
0.0s
(0.0s + 0.0s)
A:492/R:1835
C:681
0.11s
(0.08s + 0.03s)
A:9455/R:13140
C:0

0.01s
(0.00s + 0.01s)
A:448/R:647

Commuter
(15)

0.04s
(0.03s + 0.01s)
A:902/R:7779
C:0
77.29s
(45.74s + 31.55s)
A:32861/R:8734019
C:0
6.19s
(2.99s + 3.20s)
A:121621/R:480187
C:0
0.42s
(0.27s + 0.15s)
A:9292/R:53719
C:0
0.00s
(0.00s + 0.00s)
A:370/R:518
C:0
0.08s
(0.05s + 0.03s)
A:4994/R:9717
C:0
4.95s
(2.57s + 2.38s)
A:1240/R:388282
C:1436
0.01s
(0.01s + 0.00s)
A:494/R:2451
C:689
0.22s
(0.13s + 0.09s)
A:21414/R:27277
C:0

0.07s
(0.06s + 0.01s)
A:9455/R:13140

0.07s

A: number of atoms, C: number of clauses, R: number of ground rules
Figure 5: Comparing the dec reasoner and f2lp with answer set solvers

Appendix B. Comparing the dec Reasoner with ASP-based Event
Calculus Reasoner
We compared the performance of the dec reasoner (v 1.0) running relsat (v 2.2) and
minisat (v 2.2) with the following:
 f2lp (v 1.11) with lparse (v 1.0.17)+cmodels (v 3.79) running minisat (v 2.0
beta),
 f2lp (v 1.11) with gringo (v 3.0.3)+cmodels (v 3.79) running minisat (v 2.0 beta),
 f2lp (v 1.11) with gringo (v 3.0.3) +clasp (v 2.0.2) (claspD (v 1.1.2) used instead
for disjunctive programs), and
 f2lp (v 1.11) with clingo (v 3.0.3 (clasp v 1.3.5)).
f2lp turns an input theory into the languages of lparse and gringo, and lparse and
gringo turn the result into a ground ASP program. cmodels turns this ground program
into a set of clauses and then invokes a SAT solver to compute answer sets, while clasp
computes answer sets using the techniques similar to those used in SAT solvers. clingo is
a system that combines gringo and clasp in a monolithic way.
The first five examples in Figure 5 are part of the benchmark problems from the work
of Shanahan (1997, 1999). The next four are by Mueller (2006). (We increased timepoints
604

fiReformulating the Situation Calculus and the Event Calculus

Problem
(max. step)
ZooTest1
(16)

f2lp with
gringo + cmodels
50.48s
(6.66s + 43.82s)
A:930483/R:2272288
C:3615955
ZooTest2
> 2h
159.51s
(22)
(12.36s + 147.15s)
A:2241512/R:4153670
C:8864228
ZooTest3
> 2h
142.68s
(23)
(13.55s + 129.13s)
A:2505940/R:4556928
C:9914568
A: number of atoms, C: number of clauses, R: number
dec
reasoner (minisat)
> 2h

f2lp with
gringo + clasp
29.01s
(6.66s + 22.35s)
A:153432/R:2271175
210.55s
(12.36s + 198.19s)
A:219220/R:4152137
196.63s
(13.55s + 183.08s)
A:230731/R:4555325
of ground rules

Figure 6: Zoo World in dec reasoner and ASP

to see more notable differences.) More examples can be found from the f2lp homepage. All
experiments were done on a Pentium machine with 3.06 GHz CPU and 4GB RAM running
64 bit Linux. The reported run times are in seconds and were obtained using the Linux
time command (user time + sys time), except for the dec reasoner for which we recorded
the times reported by the system. This was for fair comparisons in order to avoid including
the time spent by the dec reasoner in producing output in a neat format, which sometimes
takes non-negligible time. For the dec reasoner, the times in parentheses are (SAT encoding time + SAT solving time). For the others, they are the times spent by each of the
grounder and the solver. cmodels time includes the time spent in converting the ground
program generated by lparse/gringo into a set of clauses, and calling the SAT solver.
The time spent by f2lp in translating an event calculus description into an answer set
program (retaining variables) is negligible for these problems.  denotes that the system
cannot solve the example due to the limited expressivity. For instance, BusRide includes
disjunctive event axioms, which results in a disjunctive program that cannot be handled
by clingo. Similarly, the dec reasoner cannot handle BusRide (disjunctive event axioms),
Commuter (compound events) and Walking Turkey (effect constraints). As is evident from
the experiments, the main reason for the efficiency of the ASP-based approach is the efficient grounding mechanisms implemented in the ASP grounders. Though the dec reasoner
and cmodels call the same SAT solver minisat, the number of atoms processed by the dec
reasoner is in general much smaller. This is because the dec reasoner adopts an optimized
encoding method (that is based on predicate completion) which avoids a large number of
ground instances of atoms such as Initiates(e, f, t), Terminates(e, f, t), and Releases(e, f, t)
(Mueller, 2004, Section 4.4). On the other hand, in several examples, the number of clauses
generated by cmodels is 0, which means that the answer sets were found without calling
the SAT solver. This is because for these examples the unique answer set coincides with
the well-founded model, which is efficiently computed by cmodels in a preprocessing step
before calling SAT solvers. Out of the 14 benchmark examples by Shanahan (1997, 1999),
10 of them belong to this case when lparse is used for grounding.
605

fiLee & Palla

In the experiments in Figure 5, the solving times are negligible for most of the problems. We also experimented with some computationally hard problems, where solving takes
more time than grounding. Figure 6 shows runs of a medium-size action domain, the Zoo
World (Akman, Erdogan, Lee, Lifschitz, & Turner, 2004). All the tests shown in the table
are planning problems where max. step is the length of a minimal plan. The cut-off time
was 2 hours and the dec reasoner did not terminate within that time for any of the problems. In fact, the entire time was spent on SAT encoding and the SAT solver was never
called. On the other hand, the ASP grounder gringo took only a few seconds to ground
the domain and, unlike in Figure 5, the solvers took much more time than the grounder. As
we can see, cmodels with minisat performed better than clasp on two of the problems.
To check the time taken by minisat on the encoding generated by the dec reasoner, we
ran ZooTest1 to completion. The dec reasoner terminated after 116578.1 seconds (32.38
hours).

Appendix C. Proofs
C.1 Review of Some Useful Theorems
We review some theorems by Ferraris et al. (2011) and Ferraris et al. (2009) which will be
used to prove our main results. In fact, we will provide a version of the splitting theorem
which is slightly more general than the one given by Ferraris et al. (2009), in order to
facilitate our proof efforts.
Lemma 1 Formula
u  p  ((F ) (u)  F )
is logically valid.
Theorem 12 (Ferraris et al., 2011, Thm. 2) For any first-order formula F and any
disjoint lists p, q of distinct predicate constants,
SM[F ; p]  SM[F  Choice(q); p  q]
is logically valid.
Let F be a first-order formula. A rule of F is an implication that occurs strictly positively
in F . The predicate dependency graph of F (relative to p) is the directed graph that
 has all members of p as its vertices, and
 has an edge from p to q if, for some rule G  H of F ,
 p has a strictly positive occurrence in H, and
 q has a positive occurrence in G that does not belong to any subformula of G
that is negative on p.
Theorem 13 (Ferraris et al., 2009, Splitting Thm.) Let F , G be first-order sentences,
and let p, q be finite disjoint lists of distinct predicate constants. If
606

fiReformulating the Situation Calculus and the Event Calculus

(a) each strongly connected component of the predicate dependency graph of F  G relative
to p, q is either a subset of p or a subset of q,
(b) F is negative on q, and
(c) G is negative on p
then
SM[F  G; p  q]  SM[F ; p]  SM[G; q]
is logically valid.
The theorem is slightly more general than the one by Ferraris et al. (2009) in that the
notion of a dependency graph above yields less edges than the one given by Ferraris et al.
Instead of
 q has a positive occurrence in G that does not belong to any subformula of G
that is negative on p,
Ferraris et al.s definition has
 q has a positive occurrence in G that does not belong to any subformula of the
form K.
For instance, according to Ferraris et al., the dependency graph of
((p  q)  r)  p

(38)

relative to p has two edges (from p to r, and from p to p), while the dependency graph
according to our definition has no edges.
On the other hand, the generalization is not essential in view of the following theorem.
Theorem 14 (Ferraris et al., 2009, Thm. on Double Negations) Let H be a sentence, F
a subformula of H, and H  the sentence obtained from H by inserting  in front of F .
If the occurrence of F is p-negated in H, then SM[H; p] is equivalent to SM[H  ; p].
For instance, SM[(38); p] is equivalent to SM[((p  q)  r)  p; p]. The dependency
graph of ((p  q)  r)  p relative to p according to the definition by Ferraris et al. is
identical to the dependency graph of (38) relative to p according to our definition.
Next, we say that a formula F is in Clark normal form (relative to the list p of intensional
predicates) if it is a conjunction of sentences of the form
x(G  p(x)),

(39)

one for each intensional predicate p, where x is a list of distinct object variables, and G has
no free variables other than those in x. The completion (relative to p) of a formula F in
Clark normal form is obtained by replacing each conjunctive term (39) with
x(p(x)  G).
The following theorem relates SM to completion. We say that F is tight on p if the
predicate dependency graph of F relative to p is acyclic.
Theorem 15 (Ferraris et al., 2011) For any formula F in Clark normal form that is tight
on p, formula SM[F ; p] is equivalent to the completion of F relative to p.
607

fiLee & Palla

C.2 Proof of Proposition 1
Using Theorem 12 and Theorem 13,
SM[F ; p]  SM[F ; p  pr (F )]  SM[>; p\pr (F )]
 SM[F ; p  pr (F )]  False(p\pr (F ))
 SM[F  Choice(pr (F )\p)]  False(p\pr (F ))
 SM[F  Choice(pr (F )\p)  False(p\pr (F ))].

C.3 Proof of Theorem 1
In the following, F is a formula, p is a list of distinct predicate constants p1 , . . . , pn , and u
is a list of distinct predicate variables u1 , . . . , un of the same length as p.
Lemma 2 (Ferraris et al., 2011, Lemma 5) Formula
u  p  (F  (u)  F )
is logically valid.
Lemma 3 If every occurrence of every predicate constant from p is strictly positive in F ,
(u  p)  (F  (u)  F (u))
is logically valid.
Proof. By induction. We will show only the case when F is G  H. The other cases are
straightforward. Consider
F  (u) = (G (u)  H  (u))  (G  H).
Since every occurrence of predicate constants from p in F is strictly positive, G contains
no predicate constants from p, so that G (u) is equivalent to G(u), which is the same as
G. Also, by I.H., H  (u)  H(u) is logically valid. Therefore it is sufficient to prove that
under the assumption u  p,
(G  H(u))  (G  H)  (G  H(u))
is logically valid. From left to right is clear. Assume (u  p), G  H(u), and G. We get
H(u), which is equivalent to H  (u) by I.H. By Lemma 2, we conclude H.

The proof of Theorem 1 is immediate from the following lemma, which can be proved
by induction.
Lemma 4 If F is canonical relative to p, then formula
(u  p)  F  (F  (u)  F (u))
is logically valid.
608

fiReformulating the Situation Calculus and the Event Calculus

Proof.
 F is an atomic formula. Trivial.
 F = G  H. Follows from I.H.
 F = G  H. Assume (u  p)  (G  H). Since G  H is canonical relative to p,
every occurrence of every predicate constant from p is strictly positive in G or in H,
so that, by Lemma 3, G (u) is equivalent to G(u), and H  (u) is equivalent to H(u).
 F = G  H. Assume (u  p)  (G  H). It is sufficient to show
(G (u)  H  (u))  (G(u)  H(u)).

(40)

Since G  H is canonical relative to p, every occurrence of every predicate constant
from p in G is strictly positive in G, so that, by Lemma 3, G (u) is equivalent to
G(u).
 Case 1: G. By Lemma 2, G (u). The claim follows since G (u) is equivalent
to G(u).
 Case 2: H. By I.H. H  (u) is equivalent to H(u). The claim follows since G (u)
is equivalent to G(u).
 F = xG. Follows from I.H.
 F = xG. Since every occurrence of every predicate constant from p in G is strictly
positive in G, the claim follows from Lemma 3.

C.4 Proof of Theorem 2
Proof. Between (a) and (b):
Between (b) and (c):

Follows immediately from Theorem 1.

Note first that  is equivalent to SM[; ]. Since

 every strongly connected component in the dependency graph of    relative to
{I, T, R, H} either belongs to {I, T, R} or {H},
  is negative on {H}, and
  is negative on {I, T, R},
it follows from Theorem 13 that (b) is equivalent to
SM[  ; I, T, R, H]  SM[; Ab1 , . . . , Abn ]  SM[; ]
Similarly, applying Theorem 13 repeatedly, we can show that the above formula is
equivalent to (c).
Between (c) and (d):

By Proposition 1.



609

fiLee & Palla

C.5 Proof of Theorem 3
Since Dcaused is canonical relative to Caused , by Theorem 1, (a)

Between (a) and (b):
is equivalent to


SM[Dcaused ; Caused ]  Dposs  Drest
 (22).

(41)

Consequently, it is sufficient to prove the claim that, under the assumption s Sit(s),
formula (22) is equivalent to SM[Dsit ; Sit].
First note that under the assumption, (22) can be equivalently rewritten as

p p(S0 )  a, s(p(s)  p(do(a, s)))  p = Sit .
(42)
On the other hand, under s Sit(s), SM[Dsit ; Sit] is equivalent to
Sit(S0 )  a, s(Sit(s)  Sit(do(a, s)))

 p p < Sit  (p(S0 )  a, s(p(s)  p(do(a, s)))  a, s(Sit(s)  Sit(do(a, s)))) ,
which, under the assumption s Sit(s), is equivalent to
p p(S0 )  a, s(p(s)  p(do(a, s)))  (p < Sit)



and furthermore to (42).
Between (b) and (c): Since (s) does not contain Poss, the equivalence follows from the
equivalence between completion and the stable model semantics.
Between (c) and (d): Since Dcaused contains no strictly positive occurrence of Poss
and Dposs  contains no occurrence of Caused , every strongly connected component in the
predicate dependency graph of Dcaused  Dposs  relative to {Caused , Poss} either belongs
to {Caused } or belongs to {Poss}. By Theorem 13, it follows that (b) is equivalent to

SM[Dcaused  Dposs  ; Caused , Poss]  Drest
 SM[Dsit ; Sit].

Similarly, applying Theorem 13 two more times, we get that the above formula is equivalent
to (c).

C.6 Proof of Theorem 4
Theory T is
  Deffect  Dprecond  DS0  Duna  Dinertia  Dexogenous0 ,
and the corresponding BAT is
  Dss  Dap  DS0  Duna .
Without loss of generality, we assume that T is already equivalently rewritten so that there
are exactly one positive effect axiom and exactly one negative effect axiom for each fluent R,
and that there is exactly one action precondition axiom for each action A.
610

fiReformulating the Situation Calculus and the Event Calculus

Consider
SM[  Deffect  Dprecond  DS0  Duna  Dinertia  Dexogenous0 ; Poss, Holds, Holds].
Since  and Duna are negative on the intensional predicates, the formula is equivalent to
SM[Deffect  Dprecond  DS0  Dinertia  Dexogenous0 ; P oss, Holds, Holds]    Duna .
(43)
Since P oss does not occur in
Deffect  DS0  Dinertia  Dexogenous0 ,
and since Dprecond is negative on {Holds, Holds}, by Theorem 13, (43) is equivalent to
SM[Deffect  DS0  Dinertia  Dexogenous0 ; Holds, Holds]
 SM[Dprecond ; P oss]    Duna ,

(44)

which is equivalent to
SM[Deffect  DS0  Dinertia  Dexogenous0 ; Holds, Holds]
 Dap    Duna .
Therefore the statement of the theorem can be proven by showing the following: if

I |= x a s(+
R (x, a, s)  R (x, a, s))

(45)

I |= 

(46)

SM[DS0  Dexogenous0  Deffect  Dinertia ; Holds, Holds]

(47)

for every fluent R, and
then I satisfies

iff I| satisfies
DS0  Dss .
From Dexogenous0 , it follows that (47) is equivalent to
SM[DS
 Dexogenous0  Deffect  Dinertia ; Holds, Holds],
0

(48)

where DS
is the formula obtained from DS0 by prepending  to all occurrences of Holds.
0
Under the assumption (46),
DS
 Dexogenous0  Deffect  Dinertia
0
is {Holds}-atomic-tight w.r.t. I, 26 so that by the relationship between completion and SM
that is stated in Corollary 11 of (Lee & Meng, 2011), we have that I |= (48) iff I satisfies
DS0 , and, for each fluent R,
26. See Section 7 from the work of Lee and Meng (2011) for the definition.

611

fiLee & Palla

Holds(R(x), do(a, s))  +
R (x, a, s)  (Holds(R(x, s)   Holds(R(x), do(a, s)))

(49)

and
Holds(R(x), do(a, s))  
R (x, a, s)  (Holds(R(x), s)  Holds(R(x), do(a, s))), (50)
where x, a, s are any (lists of) object names of corresponding sorts.
It remains to show that, under the assumption (45), I satisfies (49)  (50) iff I| satisfies

Holds(R(x), do(a, s))  +
R (x, a, s)  (Holds(R(x), s)  R (x, a, s)).

(51)

In the following we will use the following facts.
 I |=Holds(R(x), s) iff I| 6|= Holds(R(x), s).
 if F is a ground formula that does not contain , then I |= F iff I| |= F .
Left to Right: Assume I |= (49)  (50).
 Case 1: I| |= Holds(R(x), do(a, s)). Clearly, I |= Holds(R(x), do(a, s)), so that,
from (49), there are two subcases to consider.
 Subcase 1: I |= +
R (x, a, s). Clearly, I| satisfies both LHS and RHS of (51).
 Subcase 2: I |= Holds(R(x), s). From (50), it follows that I 6|= 
R (x, a, s), and
(x,
a,
s).
Clearly,
I|
satisfies
both
LHS
and
RHS of (51).
consequently, I| 6|= 

R
 Case 2: I| 6|= Holds(R(x), do(a, s)). It follows from (49) that I 6|= +
R (x, a, s), which
(x,
a,
s).
Also
since
I
|=Holds(R(x),
do(a, s)),
is equivalent to saying that I| 6|= +
R
from (50), there are two subcases to consider.
 Subcase 1: I |= 
R (x, a, s). Clearly, I| satisfies neither LHS nor RHS of (51).
 Subcase 2: I |=  Holds(R(x), s). This is equivalent to saying that I| 6|=
Holds(R(x), s). Clearly, I| satisfies neither LHS nor RHS of (51).
Right to Left: Assume I| |= (51).
 Case 1: I |= Holds(R(x), do(a, s)). It follows from (51) that I| satisfies RHS of (51),
so that there are two subcases to consider.
 Subcase 1: I| |= +
R (x, a, s). Clearly, I satisfies both LHS and RHS of (49).
Also from (45), it follows that I 6|= 
R (x, a, s). Consequently, I satisfies neither
LHS nor RHS of (50).
 Subcase 2: I| |= Holds(R(x), s)  
R (x, a, s). Clearly, I satisfies both LHS and
RHS of (49). Since I 6|= 
(x,
a,
s),
I
satisfies neither LHS nor RHS of (50).
R
 Case 2: I |=Holds(R(x), do(a, s)). It follows from (51) that I| 6|= +
R (x, a, s), and
I| 6|= (Holds(R(x), s)  
(x,
a,
s)).
From
the
latter,
consider
the
two
subcases.
R
612

fiReformulating the Situation Calculus and the Event Calculus

 Subcase 1: I| 6|= Holds(R(x), s). Clearly, I satisfies neither LHS nor RHS of
(49), and satisfies both LHS and RHS of (50).
 Subcase 2: I| 6|= 
R (x, a, s). Clearly, I satisfies neither LHS nor RHS of (49),
and satisfies both LHS and RHS of (50).

C.7 Proof of Proposition 2
Lemma 5 Let F be a formula, let p be a list of distinct predicate constants, let G be a
subformula of F and let G0 be any formula that is classically equivalent to G. Let F 0 be the
formula obtained from F by substituting G0 for G. If the occurrence of G is in a subformula
of F that is negative on p and the occurrence of G0 is in a subformula of F 0 that is negative
on p, then
SM[F ; p]  SM[F 0 ; p]
is logically valid.
Proof. Let F  be the formula obtained from F by prepending  to G, and let (F 0 )
be the formula obtained from F 0 by prepending  to G0 . By the Theorem on Double
Negations (Theorem 14), the following formulas are logically valid.
SM[F ; p]  SM[F  ; p],
SM[F 0 ; p]  SM[(F 0 ) ; p].
From Lemma 1, it follows that
(u  p  (G  G0 ))  ((F  ) (u)  ((F 0 ) ) (u))
is logically valid, where u is a list of predicate variables corresponding to p. Consequently,
SM[F  ; p]  SM[(F 0 ) ; p]
is logically valid.



Proof of Proposition 2. In formula
SM[F 0  xy(G(y, x)  q(x)); p, q],

(52)

clearly, F 0 is negative on q and xy(G(y, x)  q(x)) is negative on p. Let H be any
subformula of F that is negative on p and contains the occurrence of yG(y, x). Consider
two cases.
 Case 1: the occurrence of yG(y, x) in H is not strictly positive. Thus the dependency
graph of F 0  xy(G(y, x)  q(x)) relative to {p, q} has no incoming edges into q.
 Case 2: the occurrence of yG(y, x) in H is strictly positive. Since H is negative on p, yG(y, x) is negative on p as well, so that the dependency graph of
F 0  xy(G(y, x)  q(x)) relative to {p, q} has no outgoing edges from q.
613

fiLee & Palla

Therefore, every strongly connected component in the dependency graph belongs to either
p or {q}. Consequently, by Theorem 13, (52) is equivalent to
SM[F 0 ; p]  SM[xy(G(y, x)  q(x)); q]

(53)

Since yG(y, x) is negative on q, formula xy(G(y, x)  q(x)) is tight on {q}. By Theorem 15, (53) is equivalent to
SM[F 0 ; p]  x(yG(y, x)  q(x)).

(54)

By Lemma 5, it follows that (54) is equivalent to
SM[F ; p]  x(yG(y, x)  q(x)).
Consequently, the claim follows.



C.8 Proof of Theorem 6
It is clear that the algorithm terminates and yields a quantifier-free formula K. We will
prove that SM[F ; p]  SM[xK; p  q], where x is the list of all (free) variables of K.
Let F  be the formula obtained from the initial formula F by prepending double
negations in front of every maximal strictly positive occurrence of formulas of the form
yG(x, y). Since F is almost universal relative to p, such an occurrence is in a subformula
of F that is negative on p. Thus by the Theorem on Double Negations (Theorem 14),
SM[F ; p] is equivalent to SM[F  ; p]. Note that F  contains no strictly positive occurrence
of formulas of the form yG(x, y).
For each iteration, let us assume that the formula before the iteration is
H0      Hn ,
where H0 is transformed from F  by the previous iterations, and each Hi (i > 0) is a
formula of the form G(x, y)  pG (x) that is introduced by Step (b). Initially H0 is F 
and n = 0. Let r0 be p, and let ri be each pG for Hi (i > 0). By induction we can prove
that
(i) every positive occurrence of formulas of the form yG(x, y) in Hi is not strictly positive, and is in a subformula of Hi that is negative on ri ;
(ii) every negative occurrence of formulas of the form yG(x, y) in Hi is in a subformula
of Hi that is negative on ri .
We will prove that if Step (a) or Step (c) is applied to turn Hk into Hk0 , then
SM[x0 H0 ; r0 ]      SM[xn Hn ; rn ]

(55)

SM[x00 H00 ; r0 ]      SM[x0n Hn0 ; rn ],

(56)

is equivalent to
where Hj0 = Hj for all j different from k, and xi (i  0) is the list of all free variables of
Hi , and x0i (i  0) is the list of all free variables of Hi0 .
614

fiReformulating the Situation Calculus and the Event Calculus

Indeed, Step (a) is a part of prenex form conversion, which preserves strong equivalence
(Theorem 5). So it is clear that (55) is equivalent to (56).
When Step (c) is applied to turn (55) into (56), since yH(x, y) is in a subformula of
Hk that is negative on rk , the equivalence between (55) and (56) follows from Lemma 5.
When Step (b) is applied to turn Hk into Hk0 and introduces a new conjunctive term
0
Hn+1 , formula (55) is (, r1 , . . . , rn )-equivalent to
0
SM[x00 H00 ; r0 ]      SM[x0n Hn0 ; rn ]  SM[x0n+1 Hn+1
; rn+1 ]

(57)

by Proposition 2 due to condition (i).
Let
00
H000      Hm

(58)

be the final quantifier-free formula, where H000 is transformed from F  . By the induction,
it follows that SM[F ; p] is -equivalent to
00
SM[x000 H000 ; r0 ]      SM[x00m Hm
; rm ],

(59)

where each x00i (0  i  m) is the list of all free variables of Hi00 .
Since every non-strictly positive occurrence of new predicate ri (i > 0) in any Hj00 (0 
j  m) is positive, there is no incoming edge into ri in the dependency graph of (58) relative
to r0 , r1 , . . . , rm . Consequently, every strongly connected component of the dependency
graph belongs to one of ri (i  0). Moreover, it is clear that each Hi00 (i  0) is negative
on every rj for j 6= i. (In the case of H000 , recall that the occurrence of rj for any j > 0
is not strictly positive since F  , from which H000 is obtained, contains no strictly positive
occurrence of formulas of the form yG(x, y).) Thus by the splitting theorem (Theorem 13),
formula (59) is equivalent to
00
SM[x000 H000      x00m Hm
; r0      rm ].

(60)


C.9 Proof of Theorem 7
We use the notations introduced in the proof of Theorem 6. By Theorem 6, SM[F ; p] is
-equivalent to (60) and, by Theorem 12, (60) is equivalent to
00
SM[x000 H000      x00m Hm
 Choice( pred \ p);  pred  r1      rm ]

(61)

(r0 is p), where  pred is the set of all predicate constants in signature . It follows from
Proposition 3 from (Cabalar et al., 2005) that (61) is equivalent to
000
SM[x000 H0000      x00m Hm
 Choice( pred \ p);  pred  r1      rm ],

(62)

where Hi000 is obtained from Hi00 by applying the translation from (Cabalar et al., 2005,
Section 3) that turns a quantifier-free formula into a set of rules. It is easy to see that F 0
is the same as the formula
000
x000 H0000      x00m Hm
 Choice( pred \ p)

615

fiLee & Palla

and  pred  r1      rm is the same as p  pr (F 0 ), so that (62) can be written as
SM[F 0 ; p  pr (F 0 )],
which is equivalent to
SM[F 0  False(p \ pr (F 0 ))].
by Proposition 1.



C.10 Proof of Theorem 8
Assume that T is
CIRC[; Initiates, Terminates, Releases]  CIRC[; Happens]
 CIRC[; Ab 1 , . . . , Ab n ]  ,
which is equivalent to
SM[; Initiates, Terminates, Releases]  SM[; Happens]
 SM[; Ab 1 , . . . , Ab n ]  

(63)

by Theorem 2.
Let def be the set of all definitions (35) in , and let 0 be the formula obtained from 
by applying Step 1. By Theorem 15, it follows that each formula (35) in def is equivalent
to
SM[x(G0  p(x)); p],
where G0 is as described in Step 1. Consequently, (63) is equivalent to
SM[; Initiates, Terminates,
V Releases]  SM[; Happens]
 SM[; Ab 1 , . . . , Ab n ]  (35)def SM[x(G0  p(x)); p]  00 ,

(64)

where 00 is the conjunction of all the axioms in 0 other than the ones obtained from
definitional axioms (35).
Applying Theorem 13 repeatedly, it follows that (64) is equivalent to
V
SM[      00  (35)def x(G0  p(x));
Initiates, Terminates, Releases, Happens, Ab 1 , . . . , Ab n , p] .

(65)

According to the syntax of the event calculus reviewed in Section 3.1,
 every positive occurrence of a formula of the form yG(y) in (65) is contained in a
subformula that is negative on
{Initiates, Terminates, Releases, Happens, Ab 1 , . . . , Ab n , p}, and
 there are no negative occurrences of any formula of the form yG(y) in (65).
Consequently, the statement of the theorem follows from Theorem 7.

616



fiReformulating the Situation Calculus and the Event Calculus

C.11 Proof of Theorem 9
Since (37) is almost universal relative to {Caused , Poss, Sit}, the result follows from Theorems 7 and 3.

C.12 Proof of Theorem 10
From Dexogenous0 , it follows that SM[T ; Holds, Holds, Poss] is equivalent to
SM[T  ; Holds,  Holds, Poss], where T  is obtained from T by prepending  to all
occurrences of Holds in DS0 . From the definition of a uniform formula (Reiter, 2001), it
follows that T  is almost universal relative to {Holds, Holds, Poss}. The result follows
from Theorem 7.


References
Akman, V., Erdogan, S., Lee, J., Lifschitz, V., & Turner, H. (2004). Representing the Zoo
World and the Traffic World in the language of the Causal Calculator. Artificial
Intelligence, 153(12), 105140.
Belleghem, K. V., Denecker, M., & Schreye, D. D. (1995). Combining situation calculus
and event calculus. In Proceedings of International Conference on Logic Programming
(ICLP), pp. 8397.
Belleghem, K. V., Denecker, M., & Schreye, D. D. (1997). On the relation between situation
calculus and event calculus. Journal of Logic Programming, 31 (1-3), 337.
Besnard, P., & Cordier, M.-O. (1994). Explanatory diagnoses and their characterization by
circumscription. Annals of Mathematics and Artificial Intelligence, 11 (1-4), 7596.
Cabalar, P., & Ferraris, P. (2007). Propositional theories are strongly equivalent to logic
programs. Theory and Practice of Logic Programming, 7 (6), 745759.
Cabalar, P., Pearce, D., & Valverde, A. (2005). Reducing propositional theories in equilibrium logic to logic programs. In Proceedings of Portuguese Conference on Artificial
Intelligence (EPIA), pp. 417.
Calimeri, F., Cozza, S., Ianni, G., & Leone, N. (2008). Computable functions in ASP: theory
and implementation. In Proceedings of International Conference on Logic Programming (ICLP), pp. 407424.
Denecker, M., & Ternovska, E. (2007). Inductive situation calculus. Artificial Intelligence,
171 (5-6), 332360.
Denecker, M., & Ternovska, E. (2008). A logic of nonmonotone inductive definitions. ACM
Transactions on Computational Logic, 9 (2).
Doherty, P., Gustafsson, J., Karlsson, L., & Kvarnstrom, J. (1998). TAL: Temporal action
logics language specification and tutorial. Linkoping Electronic Articles in Computer
and Information Science ISSN 1401-9841, 3 (015). http://www.ep.liu.se/ea/cis/
1998/015/.
617

fiLee & Palla

Dogandag, S., Ferraris, P., & Lifschitz, V. (2004). Almost definite causal theories.. In
Proceedings of International Conference on Logic Programming and Nonmonotonic
Reasoning (LPNMR), pp. 7486.
Erdem, E., & Lifschitz, V. (2003). Tight logic programs. Theory and Practice of Logic
Programming, 3, 499518.
Fages, F. (1994). Consistency of Clarks completion and existence of stable models. Journal
of Methods of Logic in Computer Science, 1, 5160.
Ferraris, P., Lee, J., & Lifschitz, V. (2007). A new perspective on stable models. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), pp. 372379.
Ferraris, P., Lee, J., & Lifschitz, V. (2011). Stable models and circumscription. Artificial
Intelligence, 175, 236263.
Ferraris, P., Lee, J., Lifschitz, V., & Palla, R. (2009). Symmetric splitting in the general
theory of stable models. In Proceedings of International Joint Conference on Artificial
Intelligence (IJCAI), pp. 797803.
Gebser, M., Schaub, T., & Thiele, S. (2007). Gringo : A new grounder for answer set
programming. In Proceedings of International Conference on Logic Programming and
Nonmonotonic Reasoning (LPNMR), pp. 266271.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming. In
Kowalski, R., & Bowen, K. (Eds.), Proceedings of International Logic Programming
Conference and Symposium, pp. 10701080. MIT Press.
Gelfond, M., & Lifschitz, V. (1998). Action languages. Electronic Transactions on Artificial
Intelligence, 3, 195210. http://www.ep.liu.se/ea/cis/1998/016/.
Giunchiglia, E., Lee, J., Lifschitz, V., McCain, N., & Turner, H. (2004). Nonmonotonic
causal theories. Artificial Intelligence, 153(12), 49104.
Heyting, A. (1930). Die formalen Regeln der intuitionistischen Logik. Sitzungsberichte
der Preussischen Akademie von Wissenschaften. Physikalisch-mathematische Klasse,
4256.
Janhunen, T., & Oikarinen, E. (2004). Capturing parallel circumscription with disjunctive
logic programs. In Proc. of 9th European Conference in Logics in Artificial Intelligence
(JELIA-04), pp. 134146.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Proceedings of European
Conference on Artificial Intelligence (ECAI), pp. 359363.
Kim, T.-W., Lee, J., & Palla, R. (2009). Circumscriptive event calculus as answer set programming. In Proceedings of International Joint Conference on Artificial Intelligence
(IJCAI), pp. 823829.
Kowalski, R., & Sergot, M. (1986). A logic-based calculus of events. New Generation
Computing, 4, 6795.
Kowalski, R. A., & Sadri, F. (1997). Reconciling the event calculus with the situation
calculus. Journal of Logic Programming, 31 (1-3), 3958.
618

fiReformulating the Situation Calculus and the Event Calculus

Lee, J., Lifschitz, V., & Palla, R. (2008). A reductive semantics for counting and choice
in answer set programming. In Proceedings of the AAAI Conference on Artificial
Intelligence (AAAI), pp. 472479.
Lee, J., & Lin, F. (2006). Loop formulas for circumscription. Artificial Intelligence, 170 (2),
160185.
Lee, J., & Meng, Y. (2011). First-order stable model semantics and first-order loop formulas.
Journal of Artificial Inteligence Research (JAIR), 42, 125180.
Lee, J., & Palla, R. (2007). Yet another proof of the strong equivalence between propositional
theories and logic programs. In Working Notes of the Workshop on Correspondence
and Equivalence for Nonmonotonic Theories.
Lee, J., & Palla, R. (2010). Situation calculus as answer set programming. In Proceedings
of the AAAI Conference on Artificial Intelligence (AAAI), pp. 309314.
Lifschitz, V. (1994). Circumscription. In Gabbay, D., Hogger, C., & Robinson, J. (Eds.),
Handbook of Logic in AI and Logic Programming, Vol. 3, pp. 298352. Oxford University Press.
Lifschitz, V. (2008). What is answer set programming?. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 15941597. MIT Press.
Lifschitz, V. (2011). Datalog programs and their stable models. In de Moor, O., Gottlob,
G., Furche, T., & Sellers, A. (Eds.), Datalog Reloaded: First International Workshop,
Datalog 2010, Oxford, UK, March 16-19, 2010. Revised Selected Papers. Springer.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM
Transactions on Computational Logic, 2, 526541.
Lifschitz, V., Tang, L. R., & Turner, H. (1999). Nested expressions in logic programs. Annals
of Mathematics and Artificial Intelligence, 25, 369389.
Lifschitz, V., & Turner, H. (1999). Representing transition systems by logic programs. In
Proceedings of International Conference on Logic Programming and Nonmonotonic
Reasoning (LPNMR), pp. 92106.
Lin, F. (1995). Embracing causality in specifying the indirect effects of actions. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), pp. 1985
1991.
Lin, F. (2003). Compiling causal theories to successor state axioms and STRIPS-like systems. Journal of Artificial Intelligence Research, 19, 279314.
Lin, F., & Shoham, Y. (1992). A logic of knowledge and justified assumptions. Artificial
Intelligence, 57, 271289.
Lin, F., & Wang, K. (1999). From causal theories to logic programs (sometimes). In
Proceedings of International Conference on Logic Programming and Nonmonotonic
Reasoning (LPNMR), pp. 117131.
Lin, F., & Zhou, Y. (2011). From answer set logic programming to circumscription via logic
of GK. Artificial Intelligence, 175, 264277.
619

fiLee & Palla

Marek, V., & Truszczynski, M. (1999). Stable models and an alternative logic programming
paradigm. In The Logic Programming Paradigm: a 25-Year Perspective, pp. 375398.
Springer Verlag.
McCarthy, J. (1980). Circumscriptiona form of non-monotonic reasoning. Artificial Intelligence, 13, 2739,171172.
McCarthy, J. (1986). Applications of circumscription to formalizing common sense knowledge. Artificial Intelligence, 26 (3), 89116.
McCarthy, J., & Hayes, P. (1969). Some philosophical problems from the standpoint of
artificial intelligence. In Meltzer, B., & Michie, D. (Eds.), Machine Intelligence, Vol. 4,
pp. 463502. Edinburgh University Press, Edinburgh.
Miller, R., & Shanahan, M. (1999). The event calculus in classical logic - alternative axiomatisations. Electronic Transactions on Artificial Intelligence, 3 (A), 77105.
Mueller, E. (2006). Commonsense reasoning. Morgan Kaufmann.
Mueller, E. T. (2004). Event calculus reasoning through satisfiability. Journal of Logic and
Computation, 14 (5), 703730.
Niemela, I. (1999). Logic programs with stable model semantics as a constraint programming
paradigm. Annals of Mathematics and Artificial Intelligence, 25, 241273.
Pearce, D., & Valverde, A. (2005). A first order nonmonotonic extension of constructive
logic. Studia Logica, 80, 323348.
Provetti, A. (1996). Hypothetical reasoning about actions: From situation calculus to event
calculus. Computational Intelligence, 12, 478498.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence, 13, 81132.
Reiter, R. (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Lifschitz, V. (Ed.), Artificial
Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, pp. 359380. Academic Press.
Reiter, R. (2001). Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems. MIT Press.
Shanahan, M. (1995). A circumscriptive calculus of events. Artif. Intell., 77 (2), 249284.
Shanahan, M. (1997). Solving the Frame Problem: A Mathematical Investigation of the
Common Sense Law of Inertia. MIT Press.
Shanahan, M. (1999). The event calculus explained. In Artificial Intelligence Today, LNCS
1600, pp. 409430. Springer.
Shanahan, M., & Witkowski, M. (2004). Event calculus planning through satisfiability.
Journal of Logic and Computation, 14 (5), 731745.
Syrjanen, T. (2004). Cardinality constraint programs.. In Proceedings of European Conference on Logics in Artificial Intelligence (JELIA), pp. 187199.
Zhang, H., Zhang, Y., Ying, M., & Zhou, Y. (2011). Translating first-order theories into logic
programs. In Proceedings of International Joint Conference on Artificial Intelligence
(IJCAI), pp. 11261131.

620

fiJournal of Artificial Intelligence Research 43 (2012) 353388

Submitted 10/11; published 03/12

Computing All-Pairs Shortest Paths
by Leveraging Low Treewidth
Leon Planken
Mathijs de Weerdt

l.r.planken@tudelft.nl
m.m.deweerdt@tudelft.nl

Faculty of EEMCS, Delft University of Technology,
Delft, The Netherlands

Roman van der Krogt

roman@4c.ucc.ie

Cork Constraint Computation Centre,
University College Cork, Cork, Ireland

Abstract
We present two new and efficient algorithms for computing all-pairs shortest paths. The
algorithms operate on directed graphs with real (possibly negative) weights. They make use
of directed path consistency along a vertex ordering d. Both algorithms run in O n2 wd
time, where wd is the graph width induced by this vertex ordering. For graphs of constant
treewidth, this yields O n2 time, which is optimal. On chordal graphs, the algorithms
run in O (nm) time. In addition, we present
a variant that exploits graph separators to

arrive at a run time of O nwd2 + n2 sd on general graphs, where sd  wd is the size of
the largest minimal separator induced by the vertex ordering d. We show empirically that
on both constructed and realistic benchmarks, in many cases the algorithms outperform
FloydWarshalls as well as Johnsons
algorithm, which
represent the current state of the


art with a run time of O n3 and O nm + n2 log n , respectively. Our algorithms can be
used for spatial and temporal reasoning, such as for the Simple Temporal Problem, which
underlines their relevance to the planning and scheduling community.

1. Introduction
Finding shortest paths is an important and fundamental problem in communication and
transportation networks, circuit design, bioinformatics, Internet node traffic, social networking, and graph analysis in generale.g. for computing betweenness (Girvan & Newman, 2002)and is a sub-problem of many combinatorial problems, such as those that can
be represented as a network flow problem. In particular, in the context of planning and
scheduling, finding shortest paths is important to solve a set of binary linear constraints
on events, i.e. the Simple Temporal Problem (STP; Dechter, Meiri, & Pearl, 1991). The
STP in turn appears as a sub-problem to the NP-hard Temporal Constraint Satisfaction
Problem (TCSP; Dechter et al., 1991) and Disjunctive Temporal Problem (DTP; Stergiou
& Koubarakis, 2000), which are powerful enough to model e.g. job-shop scheduling problems. The shortest path computations in these applications can account for a significant
part of the total run time of a solver. Thus, it is hardly surprising that these topics have received substantial interest in the planning and scheduling community (Satish Kumar, 2005;
Bresina, Jonsson, Morris, & Rajan, 2005; Rossi, Venable, & Yorke-Smith, 2006; Shah &
Williams, 2008; Conrad, Shah, & Williams, 2009).
c
2012
AI Access Foundation. All rights reserved.

fiPlanken, De Weerdt, & Van der Krogt

Instances of the STP, called Simple Temporal Networks (STNs), have a natural representation as directed graphs with real edge weights. Recently, there has been specific interest
in STNs stemming from hierarchical task networks (HTNs; Castillo, Fernandez-Olivares,
& Gonzalez, 2006; Bui & Yorke-Smith, 2010). These graphs have the sibling-restricted
property: each task, represented by a pair of vertices, is connected only to its sibling tasks,
its parent or its children. In these graphs the number of children of a task is restricted by a
constant branching factor, and therefore the resulting STNs also have a tree-like structure.
The canonical way of solving an STP instance (Dechter et al., 1991) is by computing
all-pairs shortest paths (APSP) on its STN, thus achieving full path consistency. For
graphs with n vertices and m edges, this can be done in O n3 time with the FloydWarshall
algorithm (Floyd, 1962), based on Warshalls (1962) formulation of efficiently computing the
transitive closure of Boolean matrices. However, the state of the art for computing APSP
on sparse graphs is an algorithm based on the technique originally proposed by Johnson
(1977), which does some preprocessing to allow n runs of Dijkstras (1959) algorithm. Using
a Fibonacci heap (Fredman & Tarjan, 1987), the algorithm runs in O n2 log n + nm time.
In the remainder of this paper, we refer to this algorithm as Johnson.
In this paper we present two new algorithms for APSP with real edge weights (in Section 3). One algorithm, dubbed ChleqAPSP, is based on a point-to-point shortest path
algorithm by Chleq (1995); the other, named Snowball, is similar to Planken, de Weerdt,
and van der Krogts (2008) algorithm for enforcing partial (instead of full) path consistency (P3 C). These new algorithms advance the state of the art in computing APSP. In
graphs of constant treewidth, such as sibling-restricted STNs based on HTNs with
a con
2
stant branching factor, the run time
 of both algorithms is bounded by O n , which is
optimal since the output is  n2 . In addition to these STNs, examples of such graphs
of constant treewidth are outerplanar graphs, graphs of bounded bandwidth, graphs of
bounded cutwidth, and series-parallel graphs (Bodlaender, 1986).
When ChleqAPSP and Snowball are applied to chordal graphs, they have a run time of
O (nm), which is a strict improvement
over the state of the art (Chaudhuri & Zaroliagis,

2000, with a run time of O nmwd2 ; wd is defined below). Chordal graphs are an important
subset of general sparse graphs: interval graphs, trees, k-trees and split graphs are all special
cases of chordal graphs (Golumbic, 2004). Moreover, any graph can be made chordal using
a so-called triangulation algorithm. Such an algorithm operates by eliminating vertices one
by one, connecting the neighbours of each eliminated vertex and thereby inducing cliques
in the graph.
The induced width wd of the vertex ordering d is defined to be equal to the cardinality
of the largest such set of neighbours encountered. The upper
 bound of the run time of
both proposed algorithms on these general graphs, O n2 wd , depends on this induced
width. Finding a vertex ordering of minimum induced width, however, is an NP-hard
problem (Arnborg, Corneil, & Proskurowski, 1987). This minimum induced width is the
tree-likeness property of the graph mentioned above, i.e. the treewidth, denoted w . In
contrast, the induced width is not a direct measure of the input (graph), so the bound of
O n2 wd is not quite proper. Still, it is better than the bound on Johnson if wd  o (log n).1
1. We prefer to write x  o (f (n)) instead of the more common x = o (f (n)). Formally, the right-hand
side represents the set of all functions that grow strictly slower than f (n), and the traditional equality
in fact only works in one direction (see also Graham, Knuth, & Patashnik, 1989, Section 9.2).

354

fiComputing APSP by Leveraging Low Treewidth


To see this, note that the bound on Johnson is never better than O n2 log n , regardless of
the value of m.
In this paper, we also present a variant of Snowball that exploits graph separators and
attains an upper bound on the run time of O nwd2 + n2 sd . This upper bound is even
better than the one for the two other new algorithms, since sd  wd is the size of the largest
minimal separator induced by the vertex ordering d. While theoretical bounds on the run
time usually give a good indication of the performance of algorithms, we see especially for
this last variant that they do not always predict which algorithm is best in which settings.
In Section 4, therefore, we experimentally establish the computational efficiency of the
proposed algorithms on a wide range of graphs, varying from random scale-free networks
and parts of the road network of New York City, to STNs generated from HTNs and job-shop
scheduling problems.
Below, we first give a more detailed introduction of the required concepts, such as
induced width, chordal graphs and triangulation, after which we present the new algorithms
and their analysis.

2. Preliminaries
In this section, we briefly introduce the algorithm that enforces directed path consistency (DPC) and how to find a vertex ordering required for this algorithm. We then
present our algorithms for all-pairs shortest paths, all of which require enforcing DPC (or
a stronger property) as a first step. In our treatment, we assume the weights on the edges
in the graph are real and possibly negative.
2.1 Directed Path Consistency
Dechter et al. (1991) presented DPC, included here as Algorithm 1, as a way check whether
an STP instance is consistent.2 This is equivalent to checking that the graph does not
contain a negative cycle (a closed path with negative total weight). The algorithm takes as
input a weighted directed graph G = hV, Ei and a vertex ordering d, which is a bijection
between V and the natural numbers {1, . . . , n}. In this paper, we simply represent the ith
vertex in such an ordering as the natural number i. The (possibly negative) weight on the
arc from i to j is represented as wij  R. Our shorthand for the existence of an arc
between these vertices, in either direction, is {i, j}  E. Finally, we denote by Gk the graph
induced on vertices {1, . . . , k}; likewise, for a set of vertices V 0  V , GV 0 denotes the graph
induced on V 0 . So, in particular, GV = Gn = G.
In iteration k, the algorithm adds edges (in line 5) between all pairs of lower-numbered
neighbours i, j of k, thus triangulating the graph. Moreover, in lines 3 and 4, it updates
the edge between i and j with the weight of the paths i  k  j and j  k  i, if shorter.
Consequently, for i < j, a defining property of DPC is that it ensures that wij is no higher
than the total weight of any path from i to j that consists only of vertices outside Gj (except
for i and j themselves). This implies in particular that after running DPC, w12 and w21
are labelled by the shortest paths between vertices 1 and 2.
2. Note that other algorithmssuch as BellmanFordcan be used for this purpose as well, and usually
perform better in practice.

355

fiPlanken, De Weerdt, & Van der Krogt

Algorithm 1: DPC (Dechter et al., 1991)
Input: Weighted directed graph G = hV, Ei; vertex ordering d : V  {1, . . . , n}
Output: DPC version of G, or inconsistent if G contains a negative cycle

10

for k  n to 1 do
forall i < j < k such that {i, k} , {j, k}  E do
wij  min {wij , wik + wkj }
wji  min {wji , wjk + wki }
E  E  {{i, j}}
if wij + wji < 0 then
return inconsistent
end
end
end

11

return G = hV, Ei

1
2
3
4
5
6
7
8
9

The run time of DPC depends on a measure wd called the induced width relative to
the ordering d of the vertices. Dechter et al. (1991) define this induced width of a vertex
ordering d procedurally to be exactly the highest number of neighbours j of k with j < k
encountered during the DPC algorithm. This includes neighbours in the original graph (i.e.
{j, k}  E) as well as vertices that became neighbours through edges added during an
earlier iteration of the algorithm. However, the definition can be based on just the original
graph and the vertex ordering, by making use of the following result.
Proposition 1. Suppose that G = hV, Ei is an undirected graph and d : V  {1, . . . , n}
(where d is a bijection) is a vertex ordering. Suppose further that we are given n sets of
edges Ek0 for 1  k  n, defined as follows:

	
Ek0 = {j, k}  V | j < k  path from k to j in G{j}{k,k+1,...,n}
Then, Ek0 is exactly the set of edges visited during iteration k of DPC.
Proof. Note that by definition, each set Ek0 is a superset of the original edges between
vertex k and its lower-numbered neighbours. We use this fact to prove the equivalence by
induction.
The equivalence holds for the first iteration k = n, because En0 is exactly the set of
original edges between vertex n and its lower-numbered neighbours, and there are no earlier
iterations during which DPC may have added other edges {j, k} with j < k. Now, assuming
that the equivalence holds for all sets E`0 with ` > k, we show that it also holds for Ek0 . For
this inductive case, we prove both inclusion relations separately.
() To reach a contradiction, assume that there exists some edge {j, k} 6 Ek0 , with j < k,
which is visited by DPC during iteration k. Because Ek0 includes the original edges between
k and lower-numbered neighbours, this must be a new edge added during some earlier
iteration ` > k, so there must exist edges {j, `} , {k, `}  E`0 . By the induction hypothesis,
j and k are therefore connected in the induced subgraph G{j,k}{`,`+1,...,n} . But then they
356

fiComputing APSP by Leveraging Low Treewidth

must also be connected in the larger subgraph G{j}{k,k+1,...n} and thus by definition be
included in Ek0 : a contradiction.
() Assume, again for reaching a contradiction, that there exists some edge {j, k}  Ek0
not part of E during iteration k of DPC and therefore not visited by the algorithm. Clearly,
{j, k} cannot have been one of the original edges. By definition of Ek0 there must therefore
exist a path with at least one intermediate vertex from j to k in the induced subgraph
G{j}{k,k+1,...n} . Let ` be the lowest-numbered vertex other than j and k on this path;
we have that ` > k > j. Then, by the induction hypothesis, there must exist edges
{j, `} , {k, `}  E`0 , both of which were visited by DPC during iteration `. Once more, we
reach a contradiction, since DPC must have added {j, k} to E during iteration ` > k.
We now formally define the induced width as follows, and conclude with Proposition 1
that this is equivalent to the original procedural definition.
Definition 1. Given an undirected graph G = hV, Ei, a vertex ordering d, and n sets of
edges Ek0 as in Proposition 1, the induced width wd of G (relative to d) is the following
measure:
fi fi
wd = max fiEk0 fi
kV

It follows that the run time of DPC is not a property of the graph per se; rather, it is
dependent on both the graphand the vertex ordering used. With a careful implementation,
DPCs time bound is O nwd2 if this ordering is known beforehand.
The edges added by DPC are called fill edges and make the graph chordal (sometimes
also called triangulated). Indeed, DPC differs from a triangulation procedure only by its
manipulation of the arc weights. In a chordal graph, every cycle of length four or more has
an edge joining two vertices that are not adjacent in the cycle. By Definition 1, the number
of edges in such a chordal graph, denoted by mc  m, is O (nwd ). We now give the formal
definitions of these concepts.

	
Definition 2. Given a graph G = hV, Ei and a set v 1 , v 2 , . . . , v k  V of vertices that
form a cycle in G, a chord
	 of this cycle is an edge between non-adjacent vertices in this
cycle, i.e. an edge v i , v j  E with 1 < j  i < k  1. A graph G = hV, Ei is called chordal
if all cycles of size larger than 3 have a chord.
Definition 3. Given a graph G = hV, Ei, a triangulation T of G, with T  E = , is a
set of edges such that G0 = hV, E  T i is chordal. These edges are called fill edges. T is
a minimal triangulation of G if there exists no proper subset T 0  T such that T 0 is a
triangulation of G.
2.2 Finding a Vertex Ordering
In principle, DPC can use any vertex ordering to make the graph both chordal and directionally path-consistent. However, since the vertex ordering defines the induced width, it
directly influences the run time and the number of edges mc in the resulting graph. As mentioned in the introduction, finding an ordering d with minimum induced width wd = w ,
and even just determining the treewidth w , is an NP-hard problem in general. Still, the
class of constant-treewidth graphs can be recognised, and optimally triangulated, in O (n)
357

fiPlanken, De Weerdt, & Van der Krogt

time (Bodlaender, 1996). If G is already chordal, we can find a perfect ordering (resulting in no fill edges) in O (m) time, using e.g. maximal cardinality search (MCS; Tarjan &
Yannakakis, 1984). This perfect ordering is also called a simplicial ordering, because every vertex k together with its lower-numbered neighbours in the ordering induces a clique
(simplex) in the subgraph Gk . This implies the following (known) result, relating induced
width and treewidth to the size of the largest clique in G.
Proposition 2. If a graph G is chordal, the size of its largest clique is exactly w + 1. If a
non-chordal graph G is triangulated along a vertex ordering d, yielding a chordal graph G0 ,
the size of the largest clique in G0 is exactly wd + 1. The treewidth of G0 equals wd and is
an upper bound for the treewidth of the original graph G: w  wd .
For general graphs, various heuristics exist that often produce good results. We mention
here the minimum degree heuristic (Rose, 1972), which in each iteration chooses a vertex
of lowest degree. Since the ordering produced by this heuristic is not fully known before
DPC starts but depends on the fill edges added, an adjacency-list-based implementation will
require another O (log n) factor in DPCs time bound. However, for our purposes in this
article, we can afford
the comfort of maintaining an adjacency matrix, which yields bounds

of O n2 + nwd2 time and O n2 space.

3. All-Pairs Shortest Paths
Even though, to the best of our knowledge, a DPC-based APSP algorithm has not yet
been proposed, algorithms for computing single-source shortest paths (SSSP) based on DPC
can be obtained from known results in a relatively straightforward manner. Chleq (1995)
proposed a point-to-point shortest path algorithm that with a trivial adaptation computes
SSSP; Planken, de Weerdt, and Yorke-Smith (2010) implicitly also compute SSSP as part
of their IPPC algorithm. These algorithms run in O (mc ) time and thus can
 simply be run
2
once for each vertex to yield an APSP algorithm with O (nmc )  O n wd time complexity.
Below, we first show how to adapt Chleqs algorithm to compute APSP; then, we present
a new, efficient algorithm named Snowball that relates to Planken et al.s (2008) P3 C.
3.1 Chleqs Approach
Chleqs (1995) point-to-point shortest path algorithm was simply called Minpath and computes the shortest path between two arbitrary vertices s, t  V in a directionally pathconsistent graph G. It is reproduced here as Algorithm 2 and can be seen to run in O (mc )
time because each edge is considered at most twice. The shortest distance from the source
vertex s is maintained in an array D; the algorithm iterates downward from s to 1 and then
upward from 1 to t, updating the distance array when a shorter path is found.
Since the sink vertex t is only used as a bound for the second loop, it is clear that D
actually contains shortest distances between all pairs (s, t0 ) with t0  t. Therefore, we can
easily adapt this algorithm to compute SSSP within the same O (mc ) time bound by setting
t = n and returning the entire array D instead of just D[t]. We call the result ChleqAPSP,
included as Algorithm 3, which calls this SSSP algorithm (referred to as Minpaths) n times
to compute all-pairs shortest paths in O (nmc )  O nwd2 time.
358

fiComputing APSP by Leveraging Low Treewidth

Algorithm 2: Minpath (Chleq, 1995)
Input: Weighted directed DPC graph G = hV, Ei;
(arbitrary) source vertex s and destination vertex t
Output: Distance from s to t, or inconsistent if G contains a negative cycle

12

i  V : D[i]  
D[s]  0
for k  s to 1 do
forall j < k such that {j, k}  E do
D[j]  min {D[j], D[k] + wkj }
end
end
for k  1 to t do
forall j > k such that {j, k}  E do
D[j]  min {D[j], D[k] + wkj }
end
end

13

return D[t]

1
2
3
4
5
6
7
8
9
10
11

Algorithm 3: ChleqAPSP
Input: Weighted directed graph G = hV, Ei; vertex ordering d : V  {1, . . . , n}
Output: Distance matrix D, or inconsistent if G contains a negative cycle
1
2

G  DPC(G, d)
return inconsistent if DPC did

5

for i  1 to n do
D[i][]  Minpaths(G, i)
end

6

return D

3
4

359

fiPlanken, De Weerdt, & Van der Krogt

Algorithm 4: Snowball
Input: Weighted directed graph G = hV, Ei; vertex ordering d : V  {1, . . . , n}
Output: Distance matrix D, or inconsistent if G contains a negative cycle
1
2

G  DPC(G, d)
return inconsistent if DPC did

12

i, j  V : D[i][j]  
i  V : D[i][i]  0
for k  1 to n do
forall j < k such that {j, k}  E do
forall i  {1, . . . , k  1} do
D[i][k]  min {D[i][k], D[i][j] + wjk }
D[k][i]  min {D[k][i], wkj + D[j][i]}
end
end
end

13

return D

3
4
5
6
7
8
9
10
11

3.2 The Snowball Algorithm
In this section, we present an algorithm that computes APSP (or full path-consistency),
dubbed Snowball and included as Algorithm 4, that has the same asymptotic worst-case time
bounds as ChleqAPSP but requires strictly less computational work.
Like ChleqAPSP, this algorithm first ensures that the input graph is directionally pathconsistent. The idea behind the algorithm is then that we grow, during the execution of
the outermost loop, a clique {1, . . . , k} of computed (shortest) distances, one vertex at a
time, starting with the trivial clique consisting of just vertex 1; while DPC performed a
backward sweep along d, Snowball iterates in the other direction. When adding vertex k
to the clique, the two inner loops ensure that we compute the distances between k and all
vertices i < k. This works because we know by DPC that for any such pair (i, k), there
must exist a shortest path from i to k of the form i      j  k (and vice versa), such
that {j, k}  E with j < k is an edge of the chordal graph. This means that the algorithm
only needs to look down at vertices i, j < k, and it follows inductively that D[i][j] and
D[j][i] are guaranteed to be correct from an earlier iteration.
The name of our algorithm derives from its snowball effect: the clique of computed
distances grows quadratically during the course of its operation. A small example of the
operation of Snowball is given in Figure 1. Originally, the graph contained a shortest path
4762513. Dashed edges have been added by DPC, and the path 4213 is now also
a shortest path; in particular, w42 holds the correct value. This snapshot is taken for
k = 4; the shaded vertices 13 have already been visited and shortest distances D[i][j] have
been computed for all i, j  3. Then, during the iteration k = 4, for j = 2 and i = 3, the
algorithm sets the correct weight of D[4][3] by taking the sum w42 + D[2][3].
Theorem
 3. Algorithm 4 ( Snowball) correctly computes all-pairs shortest paths in O (nmc ) 
O n2 wd time.
360

fiComputing APSP by Leveraging Low Treewidth

7
6
5
4
3
2
1

Figure 1: Snapshot (k = 4) of a graph during the operation of Snowball.

Proof. The proof is by induction. After enforcing DPC, w12 and w21 are labelled by the
shortest distances between vertices 1 and 2. For k = 2 and i = j = 1, the algorithm then
sets D[1][2] and D[2][1] to the correct values.
Now, assume that D[i][j] is set correctly for all vertices i, j < k. Let  : i = v0 
v1      v`1  v` = k be a shortest path from i to k, and further let hmax =
arg maxh{0,1,...,`} {vh  }. By DPC, if 0 < hmax < `, there exists a path of the same
weight where a shortcut vhmax 1  vhmax +1 is taken. This argument can be repeated to
conclude that there must exist a shortest path  0 from i to k that lies completely in Gk and,
except for the last arc, in Gk1 . Thus, by the induction hypothesis and the observation that
the algorithm considers all arcs from the subgraph Gk1 to k, D[i][k] is set to the correct
value. An analogous argument holds for D[k][i].
With regard to the algorithms time complexity, note that the two outermost loops
together result in each of the mc edges in the chordal graph being visited exactly once. The
inner loop always has fewer than n iterations, yielding a run time of O (nmc ) time. From
 the
2
observation above that mc  nwd , we can also state a looser time bound of O n wd .
We now briefly discuss the consequences for two special cases: graphs of constant
treewidth and chordal graphs. For chordal graphs, which can be recognised in O (m) time,
we can just substitute m for mc in the run-time complexity; further, as described above,
a perfect ordering exists and can be found in O (m) time. This gives the total run-time
complexity of O (nm). Likewise, we stated above that for a given constant , it can be
determined in O (n) time whether a graph has treewidth w  , and if so, a vertex ordering d with wd = w can be found within the same time bound. Then, omitting the
constant factor wd , the algorithm runs in O n2 time. This also follows from the algorithms pseudocode by noting that every vertex k has a constant number (at most w ) of
neighbours j < k.
We note here the similarity between Snowball and the P3 C algorithm (Planken et al.,
2008), presented below. Like Snowball, P3 C operates by enforcing DPC, followed
by a single

backward sweep along the vertex ordering. P3 C then computes, in O nwd2 time, shortest
361

fiPlanken, De Weerdt, & Van der Krogt

Algorithm 5: P 3 C (Planken et al., 2008)
Input: Weighted directed graph G = hV, Ei; vertex ordering d : V  {1, . . . , n}
Output: PPC version of G, or inconsistent if G contains a negative cycle
1
2

G  DPC(G, d)
return inconsistent if DPC did

8

for k  1 to n do
forall i, j < k such that {i, k} , {j, k}  E do
wik  min {wik , wij + wjk }
wkj  min {wkj , wki + wij }
end
end

9

return G

3
4
5
6
7

paths only for the arcs present in the chordal graph. This similarity and a property of chordal
graphs in fact prompt us to present a version of Snowball with improved time complexity.
3.3 Improving Run-Time Complexity Using Separators

In this section, we present an improvement of Snowball for an O nwd2 + n2 sd run time,
where sd is the size of the largest minimal separator in the chordal graph obtained by
triangulation along d.
Definition 4. Given a connected graph G = hV, Ei, a separator is a set V 0  V such that
GV \V 0 is no longer connected. A separator V 0 is minimal if no proper subset of V 0 is a
separator.
This bound is better because, as seen below, it always holds that sd  wd . The improvement hinges on a property of chordal graphs called partial path consistency (PPC).
In a partially path-consistent graph, each arc is labelled by the length of the shortest path
between its endpoints.3 P3 C, presented as Algorithm 5, depends on DPC and computes PPC
in O nwd2 time, which is the current state of the art. Then, we use a clique tree of the
PPC graph to compute the shortest path between all vertices. Figure 2 shows an example
of a chordal graph and its associated clique tree. Such a clique tree has the following useful
properties (Heggernes, 2006, Section 3.2).
Property 1. Every chordal graph G = hV, Ei has an associated clique tree T = hC, Si,
which can be constructed in linear time O (mc ).
Property 2. Each clique tree node c  C is associated with a subset Vc  V and induces a
maximal clique in G. Conversely, every maximal clique in G has an associated clique tree
node c  C.
Property 3. T is coherent: for each vertex v  V , the clique tree nodes whose associated
cliques contain v induce a subtree of T .
3. Full path-consistency (FPC) is achieved if an arc exists for all pairs of vertices u, v  V .

362

fiComputing APSP by Leveraging Low Treewidth

(a) Chordal graph

(b) Clique tree

Figure 2: A chordal graph and its clique tree. Each shaded shape represents a maximal
clique of the graph, containing the vertices at its corners.
Property 4. If two clique tree nodes ci , cj  C are connected by an edge {ci , cj }  S,
Vci  Vcj is a minimal separator in G. Conversely, for each minimal separator V 0 in G,
there is a clique tree edge {ci , cj }  S such that V 0 = Vci  Vcj .
Property
5. All vertices appear in at least one clique associated with a node in T , so:
S
V
=
V
.
cC c
Since we have by Proposition 2 on page 358 that the size of the largest clique in a chordal
graph is exactly wd + 1, it follows from Properties 2 and 4 that sd  wd .

Now, the idea behind SnowballSeparators is to first compute PPC in O nwd2 time using
P3 C, and then traverse the clique tree. PPC ensures that shortest paths within each clique
have been computed. Then, when traversing the clique tree from an arbitrary root node
out, we grow a set Vvisited of vertices in cliques whose nodes have already been traversed.
For each clique node c  C visited during the traversal, shortest paths between vertices in
the clique Vc and vertices in Vvisited must run through the separator Vsep between c and cs
parent. If sd is the size of the largest minimal separator in G, for each
 pair of vertices it
2
suffices to consider at most sd alternative routesfor a total of O n sd routes, yielding the
stated overall time complexity of O nwd2 + n2 sd . We formally present the algorithm based
on this idea as Algorithm 6 with its associated recursive procedure Processcliquetreenode
(on the following page).
Note that because we visit a nodes parent before visiting the node itself, it always
holds that Vcparent  Vvisited . Further note that, for simplicity of presentation, we assume
the graph to be connected. If not, we can simply find all connected components in linear
time and construct a clique tree for each of them.
The improved algorithm has an edge over the original algorithm when separators are
small while the treewidth is not. HTN-based sibling-restricted STNs (which are described
as part of our experimental validation in Section 4.3.5), for instance, have many separators

of size 2. If every task has as many as O ( n) subtasks and every task with its subtasks

induces a clique, we have
wd  O ( n) and sd = 2, implying that SnowballSeparators still

2
has an optimal O n time complexity for these instances.4
Before we proceed to prove that the algorithm is correct and meets the stated run-time
bounds, we introduce the following definition.
4. However, since in general not every task and its subtasks form a clique, this low value of sd will usually
not be attained in practice.

363

fiPlanken, De Weerdt, & Van der Krogt

Algorithm 6: Snowballseparators
Input: Weighted directed graph G = hV, Ei; vertex ordering d : V  {1, . . . , n}
Output: Distance matrix D, or inconsistent if G contains a negative cycle
1
2
3
4
5
6
7
8
9
10

G  P3 C(G, d)
return inconsistent if P3 C did
i, j  V : D[i][j]  
i  V : D[i][i]  0
 {i, j}  E : D[i][j]  wij
 {i, j}  E : D[j][i]  wji
build a clique tree T = hC, Si of G
select an arbitrary root node croot  C of T
(D, Vvisited )  Processcliquetreenode(croot , nil, D, )
return D

Procedure Processcliquetreenode(c, cparent , D, Vvisited )
Input: Current clique tree node c, cs parent cparent , distance matrix D, set of
visited vertices Vvisited
Output: Updated matrix D and set Vvisited

13

if cparent 6= nil then
Vnew  Vc \ Vcparent
Vsep  Vc  Vcparent
Vother  Vvisited \ Vc
forall (i, j, k)  Vnew  Vsep  Vother do
D[i][k]  min {D[i][k], D[i][j] + D[j][k]}
D[k][i]  min {D[k][i], D[k][j] + D[j][i]}
end
end
Vvisited  Vvisited  Vc
forall children c0 of c do
(D, Vvisited )  Processcliquetreenode(c0 , c, D, Vvisited )
end

14

return (D, Vvisited )

1
2
3
4
5
6
7
8
9
10
11
12

364

// recursive call

fiComputing APSP by Leveraging Low Treewidth

Definition 5. We define a distance matrix D as valid for a set U of vertices, and (D, U )
as a valid pair, if for all pairs of vertices (i, j)  U  U , D[i][j] holds the shortest distance
in G from i to j.
We split the correctness proof of the algorithm into three parts: Lemmas 4 and 5
culminate in Theorem 6. The first step is to show that if Processcliquetreenode is called
with a valid pair (D, U ) and some clique node c, the procedure extends the validity to
U  Vc .
Lemma 4. Consider a call to procedure Processcliquetreenode with, as arguments, a clique
node c, cs parent cparent , a distance matrix D, and the set of visited vertices Vvisited . If D
is valid for Vvisited upon calling, then D becomes valid for Vc  Vvisited after running lines
18 of Processcliquetreenode.
Proof. First, note that by Property 2, Vc induces a clique in G. Therefore, edges exist
between each pair (i, k) of vertices in Vc , and since the graph is PPC, wik is labelled with
the shortest distance between i and k. Due to lines 5 and 6 of the main algorithm, D also
contains these shortest distances, so D is valid for Vc .
Now, it remains to be shown that for each pair of vertices (i, k)  Vc  Vvisited the
shortest distances D[i][k] and D[k][i] are set correctly. We show here the case for D[i][k];
the other case is analogous.
The desired result follows trivially if cparent = nil, since the procedure is then called with
Vvisited = . Otherwise, let Vnew = Vc \ Vcparent , Vsep = Vc  Vcparent and Vother = Vvisited \ Vc
as set by the procedure in lines 24. If either i or k lies in Vsep , the correctness of D[i][k]s
value was already proven, so we only need to consider pairs of vertices (i, k)  Vnew  Vother .
For any such pair (i, k), Vsep is a separator between i and k by Property 4, so any
shortest path from i to k is necessarily a concatenation of shortest paths from i to j  and
from j  to k, for some j   Vsep . Since it follows from the definitions of Vnew , Vsep and Vother
that for all (i, j)  Vnew  Vsep and (j, k)  Vsep  Vother , D[i][j] and D[j][k] are correctly
set (by the validity of D for Vc and Vvisited , respectively), the loop on lines 58 yields the
desired result.
Our next step is to prove that through the recursive calls, validity is in fact extended
to the entire subtree rooted at c.
Lemma 5. Consider again a call to procedure Processcliquetreenode with, as arguments, a
clique node c, cs parent Vcparent , a distance matrix D, and the set of visited vertices Vvisited .
0
If D is valid for Vvisited upon calling, then the returned, updated pair (D0 , Vvisited
) is also
valid.
Proof. First, note that by Lemma 4, D is valid for Vvisited after the update in line 10.
Assume that the clique tree has a depth of d; the proof is by reverse induction over the
depth of the clique tree node. If c is a clique tree node at depth d (i.e. a leaf), the loop in
lines 1113 is a no-op, so we immediately obtain the desired result.
Now assume that the lemma holds for all nodes at depth k and let c be a clique tree
node at depth k  1. For the first call (if any) made for a child node c0 during the loop in
lines 1113, this lemma can then be applied. As a consequence, the returned and updated
365

fiPlanken, De Weerdt, & Van der Krogt

pair is again valid. This argument can be repeated until the loop ends and the procedure
returns a valid pair.
With these results at our disposal, we can state and prove the main theorem of this
section.
Theorem 6. Algorithm
6 ( SnowballSeparators) correctly computes all-pairs shortest paths

in O nwd2 + n2 sd time.
Proof. Note that Vvisited =  for the call to Processcliquetreenode in line 9 of Snowball
Separators; therefore, the pair (D, Vvisited ) is trivially valid. By Lemma 5, this call thus
returns a valid updated pair (D, Vvisited ). Since Processcliquetreenode has recursively traS
versed the entire clique tree, Vvisited contains the union cC Vc of all cliques in the clique
tree T = hC, Si, which by Property 5 equals the set of all vertices in G. Therefore, D
contains the correct shortest paths between all pairs of vertices in the graph.
As for the
 time complexity, note that the initialisations in lines 3 and 4 can be carried
2
out in O n time, whereas those in lines 5 and 6 require O (mc ) time. By Property 1, the
clique tree can be built in linear time O (mc ). Since the clique tree contains
at most n nodes,

Processcliquetreenode is called O (n) times. Line 1 requires O wd2 time. To implement
lines 24 and 10 of Processcliquetreenode, we represent the characteristic function for
Vvisited as an array of size n; using Vvisited instead of Vcparent everywhere, we then we simply
iterate over all O (wd ) members of Vc to perform the required computations.
Now, only the complexity of the loop in lines 58 remains to be shown. Note that
|Vsep |  sd by definition, and |Vother | < n always. Further using the observation that each of
the n vertices in the graph appears in Vnew for exactly one invocation of Processcliquetree
node (after
 which it becomes a staunch member of Vvisited ), we obtain a total time bound of
2
O n sd for the loop over all invocations.
While the recursive description above is perhaps easier to grasp and satisfies the claimed
time bounds, we found that efficiency benefited in practice from an iterative implementation.
It also turns out that a good heuristic is to first visit child nodes connected to the already
visited subtree by a large separator, postponing the processing of children connected by a
small separator, because the set of visited vertices is then still small. In this way, the sum of
terms |Vsep  Vvisited | is kept low. In our implementation, we therefore used a priority queue
of clique nodes ordered by their separator sizes. Future research must point out whether it
is feasible to determine an optimal traversal of the clique tree within the given time bounds.
Having presented our new algorithms and proven their correctness and formal complexity, we now move on to an empirical evaluation of their performance.

4. Experiments
We evaluate the two algorithms together with efficient implementations of FloydWarshall
and Johnson with a Fibonacci heap5 across six different benchmark sets.6
5. For Johnson we used the corrected Fibonacci heap implementation by Fiedler (2008), since the widely
used pseudocode of Cormen, Leiserson, Rivest, and Stein (2001) contains mistakes.
6. Available at
http://dx.doi.org/10.4121/uuid:49388c35-c7fb-464f-9293-cca1406edccf

366

fiComputing APSP by Leveraging Low Treewidth

Table 1: Properties of the benchmark sets
type
Chordal
 Figure 3
 Figure 4
Scale-free
 Figure 5
 Figure 6
New York
Diamonds
Job-shop
HTN

#cases

n

m

wd

sd

250
130

1,000
2143,125

75,840499,490
22,788637,009

79995
211

79995
211

130
160
170
130
400
121

1,000
2501,000
1083,906
1112,751
171,321
500625

1,99667,360
2,1763,330
1136,422
1112,751
32110,220
7481,599

88864
150200
251
2
3331
2128

80854
138190
240
2
3311
2127

The properties of the test cases are summarised in Table 1. This table lists the number
of test cases, the range of the number of vertices n, edges m, the induced width wd produced
by the minimum degree heuristic, as well as the size of the largest minimal separators sd
in the graphs. More details on the different sets can be found below, but one thing that
stands out immediately is that sd is often equal to or only marginally smaller than wd .
However, the median size of the minimum separator is less than 10 for all instances except
the constructed chordal graphs.
All algorithms were implemented in Java and went through an intensive profiling phase.7
The experiments were run using Java 1.6 (OpenJDK-1.6.0.b09) in server mode, on Intel
Xeon E5430 CPUs running 64-bit Linux. The Java processes were allowed a maximum
heap size of 4 GB, and used the default stack size. We report the measured CPU times,
including the time that was spent running the triangulation heuristic for ChleqAPSP and
Snowball. The reported run times are averaged over 10 runs for each unique problem instance.
Moreover, we generated 10 unique instances for each parameter setting, obtained by using
different random seeds. Thus, each reported statistic represents an average over 100 runs,
unless otherwise indicated. Finally, each graph instance was ensured to contain no cycles
of negative weight.
4.1 Triangulation
As discussed in Section 2.2, finding an optimal vertex ordering (with minimum induced
width) is NP-hard, but several efficient triangulation heuristics for this problem exist. We
ran our experiments with six different heuristics: the minimum fill and minimum degree
heuristics, static variants of both (taking into account only the original graph), an ordering
produced by running maximum cardinality search (MCS) on the original graph, and a
random ordering. All of these, except minimum fill, have time complexities within the bound
on the run time of ChleqAPSP and Snowball. We found that the minimum degree heuristic
gave on average induced widths less than 1.5% higher than those found by minimum fill,
7. Our implementations are available in binary form at
http://dx.doi.org/10.4121/uuid:776a266e-81c6-41ee-9d23-8c89d90b6992

367

fiPlanken, De Weerdt, & Van der Krogt

Table 2: The summed induced width, triangulation, and total run time of Snowball over all
experiments on general (non-chordal) graphs show that the minimum degree heuristic is the
best choice.
heuristic
min-fill
min-degree
MCS
static min-fill
static min-degree
random

P

wd
321,492
326,222
365,662
388,569
388,707
505,844

triangulation (s)
1,204,384
498
1,520
1,387
1,317
2,436

Snowball (s)

2,047
3,166
3,348
2,746
2,748
5,179

total (s)
1,206,431
3,664
4,868
4,133
4,064
7,615

but with drastically lower run time. The exorbitant time consumption of the minimum
fill heuristic can be partially explained by the fact that we used the LibTW package8 to
compute this ordering, whose implementation can probably be improved. However, it is
also known from the literature that the theoretical bound on the minimum fill heuristic
is worse than that of minimum degree (Kjrulff, 1990). All other heuristics are not only
slower than minimum degree, but also yield an induced width at least 12% higher, resulting
in a longer total triangulation time and a longer total run time of Snowball (see the summary
of the results over all benchmarks given in Table 2). Again, this confirms Kjrulffs earlier
work. In the experimental results included below we therefore only show the results based
on the minimum degree heuristic.
4.2 Chordal Graphs
To evaluate the performance of the new algorithms on chordal graphs, we construct chordal
graphs of a fixed size of 1,000 vertices with a treewidth ranging from 79 up to just less
than the number of vertices, thus yielding a nearly complete graph at the high end. The
results of this experiment are depicted in Figure 3. In this, and other figures, the error
bars represent the standard deviations in the measured run time for the instances of that
size. For graphs up to an induced width of about three quarters of the number of vertices,
Snowball significantly outperforms FloydWarshall (which yields the expected horizontal line),
and overall the run time of both new algorithms is well below that of Johnson across the
entire range. Figure 4 shows the run times on chordal graphs of a constant treewidth and
with increasing number of vertices. Here, the two new algorithms outperform Johnson by
nearly an order of magnitude (a factor 9.3 for Snowball around n = 1300), and even more so
regarding FloydWarshall, confirming the expectations based on the theoretical upper bounds.
4.3 General Graphs

For general, non-chordal graphs, we expect from the theoretical analysis that the O nwd2 time ChleqAPSP and Snowball algorithms are faster than Johnson with its O nm + n2 log n
8. Available from http://treewidth.com/.

368

fiComputing APSP by Leveraging Low Treewidth

100000

time to solve (ms, log scale)

F-W
Johnson
Chleq
Snowball

10000

1000

100
100

1000
induced width (log scale)

Figure 3: Run times on generated chordal graphs with a fixed number of 1000 vertices and
varying treewidth.

1e+06

F-W
Johnson
Chleq
Snowball

time to solve (ms, log scale)

100000

10000

1000

100
300

1000
number of vertices (log scale)

3000

Figure 4: Run times on generated chordal graphs of a fixed treewidth of 211.

369

fiPlanken, De Weerdt, & Van der Krogt

100000
F-W
Johnson
Chleq
Snowball

time to solve (ms, log scale)

10000

1000

100

10
100

200

300

400

500
induced width

600

700

800

900

Figure 5: Run times on the scale-free benchmarks for graphs of 1,000 vertices and varying
induced width.
time bound when wd is low, and that Johnson is faster on sparse graphs (where m is low)
of a large induced width wd . The main question is at which induced width this changeover
occurs. Regarding FloydWarshall with its O n3 bound, we expect that for larger n it is
always outperformed by the other algorithms.
4.3.1 Scale-Free Graphs
Scale-free networks are networks whose degree distribution follows a power law. That is,
for large values of k, the fraction P (k) of vertices in the network having k connections to
other vertices tends to P (k)  ck  , for some constant c and parameter . In other words,
few vertices have many connections while many vertices have only a few connections. Such
a property can be found in many real-world graphs, such as in social networks and in
the Internet. Our instances were randomly generated with Albert and Barabasis (2002)
preferential attachment method, where in each iteration a new vertex is added to the graph,
which is then attached to a number of existing vertices; the higher the degree of an existing
vertex, the more likely it is that it will be connected to the newly added vertex. To see at
which induced width Johnson is faster, we compare the run times on such generated graphs
with 1,000 vertices. By varying the number of attachments for each new vertex from 2
to n/2, we obtain graphs with an induced width ranging from 88 to 866. In these graphs,
the induced width is already quite large for small attachment values: for example, for a
value of 11, the induced width is already over 500.
The results of this experiment can be found in Figure 5. Here we see that up to an
induced width of about 350 (attachment value 5), Snowball is the most efficient. For higher
induced widths, Johnson becomes the most efficient; for wd around 800, even FloydWarshall
becomes faster than Snowball. A consistent observation but from a different angle can be
made from Figure 6, where the induced width is between 150 and 200, the number of edges
370

fiComputing APSP by Leveraging Low Treewidth

time to solve (ms, log scale)

10000

F-W
Johnson
Chleq
Snowball

1000

100

300

400

500

600
700
number of vertices

800

900

1000

Figure 6: Run times on the scale-free benchmarks for graphs of induced widths 150 to 200
and varying vertex count.
is between 2,176 and 3,330 and the number of vertices is varied from 250 to 1,000. Here we
see that for small graphs up to 350 vertices, Johnson is the fastest; then Snowball overtakes
it, and around 750 vertices ChleqAPSP is also faster than Johnson (this holds for all results
up to a sparse graph of 1,000 vertices).
Around the mark of 750 vertices, the results show a decrease in the run time for both
Snowball and ChleqAPSP. This is an artifact of the (preferential attachment) benchmark
generator. Since we cannot generate scale-free graphs with a specific induced width, we
modify the attachment value instead. As it turns out, for graphs of this size only one
attachment value yields an induced width within the desired range; for the graph of size
750, this width is at the high end of the interval, whereas for the graph of size 800 it is near
the low end. This explains the reduced run time for the larger graph.
For these scale-free networks, we conclude that Snowball is the fastest of the four algorithms when the induced width is not too large (at most one third of the number of vertices
in our benchmark set). However, we also observe that the structure of scale-free networks is
such that they have a particularly high induced width for relatively sparse graphs, exactly
because a few vertices have most of the connections. Therefore, Snowball is most efficient
only for relatively small attachment values.
4.3.2 Selections from New York Road Network
More interesting than the artificially constructed graphs are graphs based on real networks,
for which shortest path calculations are relevant. The first of this series is based on the road
network of New York City, which we obtained from the DIMACS challenge website.9 This
network is very large (with 264,346 vertices and 733,846 edges) so we decided to compute
9. http://www.dis.uniroma1.it/~challenge9/

371

fiPlanken, De Weerdt, & Van der Krogt

Figure 7: Coordinates for the vertices in the New York City input graph, and examples of
the extent of subgraphs with respectively 250, 1000, and 5000 vertices.

1e+07
F-W
Johnson
Chleq
Snowball

time to solve (ms, log scale)

1e+06

100000

10000

1000

100

10
100

1000
number of vertices (log scale)

Figure 8: Run times on the New York benchmarks for subgraphs of varying vertex count.

372

fiComputing APSP by Leveraging Low Treewidth

shortest paths for (induced) subgraphs of varying sizes. These were obtained by running
a simple breadth-first search from a random starting location until the desired number of
vertices had been visited. The extent of the subnetworks thus obtained is illustrated for
three different sizes in Figure 7. The results of all algorithms on these subgraphs can be
found in Figure 8. Here we observe the same ranking of the algorithms as on the chordal
graphs of a fixed treewidth and for diamonds: FloydWarshall is slowest with its  n3
run time, then each of Johnson, ChleqAPSP, and Snowball is significantly faster than its
predecessor. This can be explained by considering the induced width of these graphs. Even
for the largest graphs the induced width is around 30, which is considerably smaller than
the number of vertices.
4.3.3 STNs from Diamonds
This benchmark set is based on problem instances in difference logic proposed by Strichman,
Seshia, and Bryant (2002) and also appearing in the smt-lib (Ranise & Tinelli, 2003), where
the constraint graph for each instance takes the form of a circular chain of diamonds. Each
such diamond consists of two parallel paths of equal length starting from a single vertex and
ending in another single vertex. From the latter vertex, two paths start again, to converge
on a third vertex. This pattern is repeated for each diamond in the chain; the final vertex
is then connected to the very first one. The sizes of each diamond and the total number of
diamonds are varied between benchmarks.
Problems in this class are actually instances of the NP-complete Disjunctive Temporal
Problem (DTP): constraints take the form of a disjunction of inequalities. From each DTP
instance, we obtain a STP instance (i.e. a graph) by randomly selecting one inequality from
each such disjunction. This STP is most probably inconsistent, so its constraint graph
contains a negative cycle; we remedy this by modifying the weights on the constraint edges.
The idea behind this procedure is that the structure of the graph still conforms to the type
of networks that one might encounter when solving the corresponding DTP instance, and
that the run time of the algorithms mostly depends on this structure. Moreover, to reduce
the influence of the randomized extraction procedure, we repeat it for 10 different seeds.
For our benchmark set, we considered problem instances which had the size of the
diamonds fixed at 5 and their number varying. The most interesting property of this set
is that the graphs generated from it are very sparse. We ran experiments on 130 graphs,
ranging in size from 111 to 2751 vertices, all with an induced width of 2. This induced width
is clearly extremely small, which translates into ChleqAPSP and Snowball being considerably
faster than Johnson and FloydWarshall, as evidenced by Figure 9.
4.3.4 STNs from Job-Shop Scheduling
We generated each of the 400 graphs in our job-shop set from an instance of a real jobshop problem. These instances were of the type available in smt-lib (Ranise & Tinelli,
2003), but of a larger range than included in that benchmark collection. To obtain these
graphs from the job-shop instances, we again used the extraction procedure described in
the previous section. The most striking observation that can be taken from Figure 10 is
that the difference between Johnson and the two new algorithms is not quite as pronounced,
though Snowball is consistently the fastest of the three by a small margin. The fact that this
373

fiPlanken, De Weerdt, & Van der Krogt

100000
F-W
Johnson
Chleq
Snowball

time to solve (ms, log scale)

10000

1000

100

10

1
100

1000
number of vertices (log scale)

Figure 9: Run times on the diamonds benchmarks for graphs of varying vertex count.

10000

F-W
Johnson
Chleq
Snowball

time to solve (ms, log scale)

1000

100

10

1
100
number of vertices (log scale)

1000

Figure 10: Run times on the job-shop benchmarks for graphs of varying vertex count.

374

fiComputing APSP by Leveraging Low Treewidth

margin is so small is most likely due to the structure of these graphs, which is also reflected
in their relatively high induced width. Note also that the run times for FloydWarshall are
better for graphs of up to 160 vertices, while for larger graphs the other algorithms are
significantly faster.
4.3.5 STNs from HTNs
Finally, we consider a benchmark set whose instances imitate so-called sibling-restricted
STNs originating from Hierarchical Task Networks. This set is therefore particularly interesting from a planning point of view. In these graphs, constraints may occur only between
parent tasks and their children, and between sibling tasks (Bui & Yorke-Smith, 2010). We
consider an extension that includes landmark variables (Castillo, Fernandez-Olivares, &
Gonzalez, 2002) that mimic synchronisation between tasks in different parts of the network, and thereby cause some deviation from the tree-like HTN structure. We generate
HTNs using the following parameters: (i) the number of tasks in the initial HTN tree (fixed
at 250; note that tasks have a start and end point), (ii) the branching factor, determining
the number of children for each task (between 4 and 7), (iii) the depth of the HTN tree
(between 3 and 7), (iv) the ratio of landmark time points to the number of tasks in the
HTN, varying from 0 to 0.5 with a step size of 0.05, and (v) the probability of constraints
between siblings, varying from 0 to 0.5 with a step size of 0.05.
These settings result in graphs of between 500 and 625 vertices, with induced widths
varying between 2 and 128. Though the induced width seems high in light of our claim
above that it is constant, we verified that wd  2  branching factor + #landmarks + 1
for all instances. Filling in the maximal values of 7 and 125 respectively, we find an upper
bound wd  140, well above the actual maximum encountered.
Figure 11 shows the results of these experiments as a function of the induced widths of
the graphs. We can see that only for the larger induced widths, Johnson and ChleqAPSP
come close. These large induced widths are only found for high landmark ratios of 0.5. The
results indicate that for the majority of STNs stemming from HTNs, Snowball is significantly
more efficient than Johnson.
4.4 SnowballSeparators
In Section 3.3 we presented a version of Snowball that has an improved worst-case run time
over vanilla Snowball by taking advantage of the separators in the graph. In this section,
we discuss the results of our experiments comparing these two variants. First, we turn our
attention to the benchmark problems on regular graphs. Our results are summarised in
Figure 12. As one can see, SnowballSeparators actually performs strictly worse on these sets
in terms of run-time performance when compared to the original Snowball.
However, as can be seen in Table 1, the largest minimal separator is often equal to
or only marginally smaller than the induced width. Even though there may be only few
separators this large, and many may be substantially smaller (as noted above, for most
instances the median separator size is below 10), this prompts us to run experiments on
instances where separator sizes are artificially kept small. Indeed, we found that there are
cases where SnowballSeparators shows an improvement over vanilla Snowball when comparing
the number of update operations performedi.e. lines 8 and 9 of Snowball and lines 6 and 7
375

fiPlanken, De Weerdt, & Van der Krogt

F-W
Johnson
Chleq
Snowball

time to solve (ms, log scale)

1000

100

10
0

20

40

60
induced width

80

100

120

Figure 11: Run times on the HTN benchmarks for graphs from 500 to 625 vertices and
varying induced width. Each point is the average of instances with an induced width within
a range [5k, 5k + 4], for some k. This results in between 5 and 11 instances per data point.

100000
Snowball
Snowball-Sep

time to solve (ms, log scale)

10000

chordal

1000
scale-free

ny
diamonds

100
htn

10
job shop

1
100

1000
number of vertices (log scale)

Figure 12: Run times of the Snowball algorithms on the benchmark problem sets listed in
Table 1.

376

fiComputing APSP by Leveraging Low Treewidth

100000

number of updates (x1000, log scale)

Snowball
Snowball-Sep
DPC
P3C

10000

1000

100
50

100

150

200

250
300
induced width

350

400

450

Figure 13: Number of distance matrix updates on chordal instances with 512 vertices,
largest minimal separator size 2 and varying treewidth. Each point represents between 5
and 10 instances.
in Processcliquetreenode, along with lines 3 and 4 of DPC and lines 5 and 6 of P3 C. One
such case is presented in Figure 13. This describes the results on a collection of chordal
graphs of 512 vertices, in which the largest minimal separator is fixed at size 2, and the
treewidth is varied between 16 and 448. The figure also includes the results of DPC and P3 C,
as these are the respective subroutines of Snowball and SnowballSeparators. For these graphs,
SnowballSeparators performs strictly fewer update operations than Snowball on all instances,
although the difference becomes smaller as the induced width increases. While the number of
updates shows a distinct improvement over Snowball, the run times of the SnowballSeparators
algorithms do not show the same improvement. Instead, as can be seen from Figure 14, the
run times of Snowball are strictly better than those of SnowballSeparators on all instances.
Snowball can now even be seen to outperform P3 C which has a better theoretical bound; the
reason is that the adjacency matrix data structure as used by Snowball is very fast, while
the adjacency list used by P3 C, though staying within the theoretical bound, inflicts a larger
constant factor on the run time.
From these experiments, we can conclude that on graphs of these sizes, the additional
bookkeeping required by SnowballSeparators outweighs the potential improvement in the
number of distance matrix updates.
4.5 A Proper Upper Bound on the Run Time
On general graphs, the run time of the proposed algorithms depends on the induced width wd
of the ordering produced by the triangulation heuristic. This induced width is not a direct
measure of the input (graph), so the given upper bound on the run time is not quite proper.
To arrive at a proper bound, in this section we aim to relate the run time to the treewidth,
denoted w , which is a property of the input. However, determining the treewidth, an
377

fiPlanken, De Weerdt, & Van der Krogt

10000

time to solve (ms, log scale)

Snowball
Snowball-Sep
DPC
P3C

1000

100

10
50

100

150

200

250
induced width

300

350

400

450

Figure 14: Run times on chordal instances with 512 vertices, largest minimal separator
size 2 and varying treewidth. Each point represents between 5 and 10 instances.

NP-hard problem, is an intractable task for the benchmark problems we used. We therefore

compare the measured induced width wd  w , an upper bound on the treewidth, to a lower
bound x  w .10 We are unaware of any guarantee on the quality relative to the treewidth
of either the minimum degree triangulation heuristic or the lower bound we used. However,
we can calculate the ratio wd /x to get an upper bound on the ratio wd /w . From this
measure we can then obtain an upper bound on the run time expressed in the treewidth,
at least for the benchmark problems in this paper.
The results of these computations can be found in Figure 15, where we plot these ratios
for the New York, HTN, scale-free and job-shop benchmarks as a function of the lower
bound x. Using a least-squares approach, we then fitted functions wd (x) = cxk (showing
up as a straight line in this log-log plot) to the plotted data points. For functions found
by fitting, we get k = 4.6 for New York, k = 2.3 for HTN, and k = 0.98 for job-shop, all
with small multiplicative constants 0.012 < c < 1.62. As one can see from the plotted data
points for the scale-free instances, they are not amenable to such a fit and we therefore omit
it from the figure.
The decreasing trend for the job-shop data indicates that the quality of the triangulation
(i.e. of the upper bound represented by the induced width) gradually increases: the lower
and upper bound are always less than a factor 2 apart. Indeed, if we plot a line representing
a function wd0 (x) = 2x (yielding a horizontal line in this figure), we find that it describes a
comfortable upper bound on the data points for this benchmark set.
The HTN data prompts us to plot a function wd00 (x) = 25 x2.5 , with an exponent slightly
higher than the one we found from the least-squares fit, and further tweaked slightly by a
10. The lower bound was computed with the LibTW package; see http://treewidth.com/. We used the
MMD + Least-c heuristic.

378

firelative induced width (vs. lower bound)

Computing APSP by Leveraging Low Treewidth

New York
HTN
scale-free
job shop
2x
2
/5x2.5

8

4

2

1
10

100
lower bound on treewidth

Figure 15: An upper bound on the induced width relative to the treewidth can be determined experimentally by comparing it to a lower bound on the treewidth.
multiplicative coefficient to bring it into view. This function as plotted represents an ample
upper bound for the HTN benchmarks (as well as the job-shop ones).
The fit for the data points for the New York benchmark is not good and the trend of
the points themselves is not very clear, because the lower bound only spans an interval
from 1 to 4. Therefore, we cannot give an upper bound for this set of benchmarks with any
acceptable level of confidence.
However, the scale-free data points we plotted, which could not be fitted with a function
yielding a straight line, do mostly follow a clear curving trend. A hypothesis for this
behaviour is that the quality of the upper and lower bound deteriorates mostly for the middle
sizes of the benchmarks; smaller and larger scale-free graphs are easier to triangulate well.11
To give an upper bound, we could plot any line on the outer hull of these data points; e.g.
the horizontal line represented by wd (x) = 8x would work. The most pessimistic assumption
would be to choose a function with the highest slope, and we find that the upper bound
wd00 (x) = 25 x2.5 , found for the HTN benchmarks, also works here.
From this discussion,
 we may conclude that for
 all benchmarks we ran except for New
Snowball
York, wd (x) is O x2.5 which in turn is O w 2.5 ; the run time of the algorithms

and ChleqAPSP on these instances can therefore be bounded by O n2 w 2.5 .
To conclude this section, we remark that an alternative to a triangulation heuristic
would be to use an approximation algorithm with a bound on the induced width that can
be theoretically determined. For example, Bouchitte, Kratsch, Muller, and Todinca (2004)
give a O (log w ) approximation of the treewidth w . Using such an approximation
would

give an upper bound on the run time of Snowball of O n2 w log w . However, the run
11. This mirrors earlier observations by the authors.

379

fiPlanken, De Weerdt, & Van der Krogt


time of obtaining this approximate induced width is O n3 log4 nw 5 log w and has a high
constant as well, so their work isfor nowmainly of theoretical value.

5. Related Work
For dense, directed
graphs with real weights, the state-of-the-art APSP algorithms run in

3
O n / logn time (Chan, 2005; Han, 2008). These represent a serious improvement over
the O n3 bound on FloydWarshall but do not profit from the fact that in most graphs that
occur in practice, the number of edges m is significantly lower than n2 .
This profit is exactly what algorithms for sparse graphs aim to achieve. Recently, an
improvement was published over the O nm + n2 log n algorithm based on Johnsons (1977)
and Fredman and Tarjans
(1987) work: an algorithm for sparse directed graphs running

in O nm + n2 log log n time (Pettie, 2004). In theory, this algorithm is thus faster than
Johnson (in worst cases, for large graphs) when m  o (n log n).12 However, currently no
implementation exists (as confirmed through
personal communication with Pettie, June

2011). The upper bound of O n2 wd on the run time of Snowball is smaller than this
established upper bound when the induced width is small (i.e. when wd  o (log log n)),
and, of course, for chordal graphs and graphs of constant treewidth.
We are familiar with one earlier work to compute shortest paths by leveraging low
treewidth. Chaudhuri and Zaroliagis (2000) present
an algorithm for answering (point-to

O wd3 .
point) shortest path queries with O wd3 n log n preprocessing time and query time

A direct extension of their results to APSP would imply a run time of O n2 wd3 on general
graphs andO nmwd2 on chordal graphs. Our result of computing APSP on general graphs
in O n2 wd and in O (nm) on chordal graphs is thus a strict improvement.
A large part of the state-of-the-art in point-to-point shortest paths is focused on road
networks (with positive edge weights). These studies have a strong focus on heuristics, ranging from goal-directed search and bi-directional search to using or creating some hierarchical
structure, see for example (Geisberger, Sanders, Schultes, & Delling, 2008; Bauer, Delling,
Sanders, Schieferdecker, Schultes, & Wagner, 2008). One of these hierarchical heuristics has
some similarities to the idea of using chordal graphs. This heuristic is called contraction.
The idea there is to distinguish important (core) vertices, which may be possible end points,
from vertices that are never used as a start or end point. These latter vertices are then
removed (bypassed) one-by-one, connecting their neighbours directly.
Other restrictions on the input graphs for which shortest paths are computed can also
be assumed, and sometimes lead to algorithms with tighter bounds.
For example, for

unweighted chordal graphs, APSP lengths can be determined in O n2 time (Balachandhran
& Rangan, 1996; Han, Sekharan, & Sridhar, 1997) if all pairs at distance two are known.
See (Dragan, 2005) for an overview and unification of such approaches. Considering only
planar graphs, recent work shows that APSP be found in O n2 log2 n (Klein, Mozes,
 &
2
Weimann, 2010), which is an improvement over Johnson in cases where m   n log n .
In the context of planning and scheduling, a number of similar APSP problems need
to be computed sequentially, potentially allowing for a more efficient approach using dynamic algorithms. Even and Gazit (1985) provide a method where addition of a single edge
can require O n2 steps, and deletion O n4 /m on average. Thorup (2004) and Deme12. We explain our use of the notation x  o (f (n)) in Footnote 1 on page 354.

380

fiComputing APSP by Leveraging Low Treewidth

trescu and Italiano (2006)
 later give an alternative approach with an amortized run time of
O n2 (log n + log2 n+m
)
. Especially in the context of planning and scheduling, it is not esn
sential that the shortest paths between all time points be maintained. Often, it is sufficient
when the shortest paths of a selection of pairs are maintained. Above, we already mentioned
the P3 C algorithm by Planken et al. (2008) for the single-shot case; Planken et al. (2010)
describe an algorithm that incrementally maintains the property of partial path consistency
on chordal graphs in time linear in the number of edges.

6. Conclusions and Future Work
In this paper we give three algorithms for computing all-pairs shortest paths, with a run
time bounded by (i) O n2 for graphs of constant treewidth, matching earlier results that
also required O n2 (Chaudhuri & Zaroliagis, 2000);
 (ii) O (nm) on chordal graphs, improving over the earlier O nmwd2 ; and (iii) O n2 wd on general
 graphs, showing again an
2
3
improvement over previously known tightest bound of O n wd . In these bounds, wd is the
induced width of the ordering used; experimentally we have determined this to be bounded
by the treewidth to the power 2.5 for most of our benchmarks.
These contributions are obtained by applying directed path consistency combined with
known graph-theoretic techniques, such as a vertex elimination and tree decomposition, to
computing shortest paths. This supports the general idea that such techniques may help
in solving graphically-representable combinatorial problems, but the main contribution of
this article is more narrow, focusing on improving the state of the art for this single, but
important problem of computing APSP.
From the results of our extensive experiments we can make recommendations as to
which algorithm is best suited for which type of problems. Only for very small instances,
FloydWarshall should be used; this is probably mostly thanks to its simplicity, yielding a
very straightforward implementation with low overhead. Snowball can exploit the fact that
a perfect elimination ordering can be efficiently found for chordal graphs, which makes it
the most efficient algorithm for this class of graphs. From all our experiments on different
types of general graphs, we conclude that Snowball consistently outperforms Johnson (and
FloydWarshall), except when the induced width is very high. Our experiments also show
that Snowball always outperforms both ChleqAPSP and SnowballSeparators. Although the
latter has a better bound on its run time, surprisingly its actual performance is worse than
Snowball on all instances of our benchmark sets. This holds even for those instances for
which SnowballSeparators performs significantly fewer updates. Thus, we conclude that the
additional bookkeeping required by SnowballSeparators does not pay off.
Regarding these experiments, it must be noted that, although we did the utmost to
obtain a fair comparison, a constant factor in the measurements depends in a significant
way on the exact implementation details (e.g. whether a lookup-table or a heap is used),
as is also put forward in earlier work on experimentally comparing shortest path algorithms (Mondou, Crainic, & Nguyen, 1991; Cherkassky, Goldberg, & Radzik, 1996). In our
own implementation a higher constant factor for the Snowball algorithms may be caused by
adhering to the object-oriented paradigm, i.e. inheriting from the DPC and P3 C superclasses,
and choosing to reuse code rather than inlining method calls. Nonetheless, we are confident
that the general trends we identified hold independently of such details.
381

fiPlanken, De Weerdt, & Van der Krogt

Note that strictly speaking, the algorithms introduced in this paper compute all-pairs
shortest distances. If one wants to actually trace shortest paths, the algorithms can be
extended to keep track of the midpoint whenever the distance matrix is updated, just like
one does for FloydWarshall. Then, for any pair of vertices, the actual shortest path in the
graph can be traced in O (n) time.
In our current implementation of SnowballSeparators, we used a priority queue to decide
heuristically which clique tree node to visit next, giving precedence to nodes connected by
a large separator to the part of the clique tree already visited. As noted before, we defer
answering the question whether an optimal ordering can be found efficiently to future work.
We remark that using the minimum-degree heuristic for triangulation provides Snowball with
a natural edge, delaying the processing of vertices where the number of iterations of the
middle loop is small until k grows large.
Cherkassky and Goldberg (1999) compared several innovative algorithms for singlesource shortest paths that gave better efficiency than the standard BellmanFord algorithm
in practice, while having the same worst-case bound of O (nm) on the run time. In future
work, we will investigate if any of these clever improvements can also be exploited in Snowball.
SnowballSeparators can be improved further in a way that does not influence the theoretical complexity but may yield better performance in practice. Iterating over Vother can be
seen as a reverse traversal of the part of the clique tree visited before, starting at cs parent.
Then, instead of always using the separator between the current clique node (containing k)
and its parent for all previously visited vertices in Vother , we can keep track of the smallest
separator encountered during this backwards traversal for no extra asymptotic cost. Since
it was shown in Table 1 that the largest minimal separator is often hardly smaller than the
induced width, it might well pay off to search for smaller separators. We plan to implement
this improvement in the near future.
Another possible improvement is suggested by the following observation on DPC. A
variant of DPC can be proposed where edge directionality is taken into account: during
iteration k, only those neighbours i, j < k are considered for which there is a directed path
i  k  j, resulting in the addition of the arc i  j. This set of added arcs would often be
much smaller than twice the number of edges added by the standard DPC algorithm, and
while the graph produced by the directed variant would not be chordal, the correctness of
Snowball would not be impacted.
Furthermore, we would like to also experimentally compare our algorithms to the recent
algorithms by Pettie (2004) and the algorithms for graphs of constant treewidth by Chaudhuri and Zaroliagis (2000) in future work. In addition, we are interested in more efficient
triangulation heuristics, or triangulation heuristics with a guaranteed quality, to be able
to give a guaranteed theoretical bound on general graphs. Another direction, especially
interesting in the context of planning and scheduling, is to use the ideas presented here to
design a faster algorithm for dynamic all-pairs shortest paths: maintaining shortest paths
under edge deletions (or relaxations) and additions (or tightenings).

Acknowledgments
Roman van der Krogt is supported by Science Foundation Ireland under Grant number
08/RFP/CMS1711.
382

fiComputing APSP by Leveraging Low Treewidth

We offer our sincere gratitude to our reviewers for their comments, which helped us
improve the clarity of the article and strengthen our empirical results.
This article is based on a conference paper with the same title, which has received an
honourable mention for best student paper at the International Conference on Automated
Planning and Scheduling (Planken, de Weerdt, & van der Krogt, 2011).

Appendix A. Johnsons Heap
In the experiments in this paper, we presented the results for Johnson
 using a Fibonacci
2
heap, because only then the theoretical bound of O nm + n log n time is attained. In
practice, using a binary heap for a theoretical bound of O (nm log n) time turns out to be
more efficient on some occasions, as we show by the results in this section.
Figure 16 shows the run times of Johnson with a binary heap and with a Fibonacci
heap on all of the benchmark sets listed in Table 1. On the diamonds, HTN, and New
York benchmarks the binary heap is a few percent faster than the Fibonacci heap, but the
slope of the lines in this doubly logarithmic scale is the same, so we can conclude that
the average-case run time has similar asymptotic behavior. However, for larger job-shop
problems, a binary heap is a factor 2 slower than a Fibonacci heap, and on our chordal
graph benchmark problems even a factor 10. Our benchmark problems on scale-free graphs
with a fixed number of vertices help explaining this difference.
In Figure 17, the run time of both variants of Johnson can be found for scale-free graphs
with 1,000 vertices, with the number of edges varying from about 2,000 to almost 80,000.
Here, we see that only for the sparsest scale-free graphs with about 2, 000 edges, the binary
heap is slightly faster, but when more edges are considered, using the Fibonacci heap
significantly outperforms using the binary heap. In particular, the run time of the Fibonacci
heap implementation increases only slowly with the number of edges, while the run time of
the binary heap increases much more significantly. This can be explained by the fact that
when running Dijkstras algorithm as a subroutine in Johnson, each update of a (candidate)
shortest path can be done in amortized constant time with a Fibonacci heap, while in a
binary heap this has a worst-case cost of O (log n) time per update. The number of updates
is bounded by m for each run of Dijkstras algorithm, yielding a bound of O (nm) updates
for Johnson. For the binary heap this O (nm log n) bound accounts for a significant part of
the run time, while with a Fibonacci heap other operations (such as extracting the minimum
element from the heap) have a bigger relative contribution to the run time.
Based on the results over all benchmark sets, we conclude that although Johnson with a
binary heap can help reducing the actual run time in sparse graphs, Johnson with a Fibonacci
heap is overall the better choice if m can be large.

383

fiPlanken, De Weerdt, & Van der Krogt

1e+07
Binary
Fibonacci

time to solve (ms, log scale)

1e+06

chordal

100000

10000

ny

1000

diamonds
scale-free

100

htn
job shop

10

10

100
number of vertices (log scale)

1000

Figure 16: Run times of Johnson with a binary heap and with a Fibonacci heap on the
benchmark problem sets listed in Table 1.

1e+07
Binary
Fibonacci

time to solve (ms, log scale)

1e+06

100000

10000

1000

100

10

2000

4000

8000
16000
number of edges (log scale)

32000

64000

Figure 17: Run times of Johnson with a binary heap and with a Fibonacci heap on scale-free
graphs with 1,000 vertices and increasing number of edges.

384

fiComputing APSP by Leveraging Low Treewidth

References
Albert, R., & Barabasi, A.-L. (2002). Statistical Mechanics of Complex Networks. Reviews
of Modern Physics, 74 (1), 4797.
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity of Finding Embeddings
in a k -Tree. SIAM Journal on Algebraic and Discrete Methods, 8 (2), 277284.
Balachandhran, V., & Rangan, C. P. (1996). All-pairs-shortest-length on strongly chordal
graphs. Discrete applied mathematics, 69 (1-2), 169182.
Bauer, R., Delling, D., Sanders, P., Schieferdecker, D., Schultes, D., & Wagner, D. (2008).
Combining hierarchical and goal-directed speed-up techniques for Dijkstras algorithm. In Experimental Algorithms (WEA 2008), Vol. 5038 of LNCS, pp. 303318.
Springer.
Bodlaender, H. L. (1986). Classes of graphs with bounded tree-width. Tech. rep. RUU-CS86-22, Utrecht University.
Bodlaender, H. L. (1996). A Linear-Time Algorithm for Finding Tree-Decompositions of
Small Treewidth. SIAM Journal on Computing, 25 (6), 13051317.
Bouchitte, V., Kratsch, D., Muller, H., & Todinca, I. (2004). On Treewidth Approximations.
Discrete Applied Mathematics, 136 (2-3), 183196.
Bresina, J. L., Jonsson, A. K., Morris, P. H., & Rajan, K. (2005). Activity Planning for
the Mars Exploration Rovers. In Proc. of the 15th Int. Conf. on Automated Planning
and Scheduling, pp. 4049.
Bui, H. H., & Yorke-Smith, N. (2010). Efficient Variable Elimination for Semi-Structured
Simple Temporal Networks with Continuous Domains. Knowledge Engineering Review, 25 (3), 337351.
Castillo, L., Fernandez-Olivares, J., & Gonzalez, A. (2002). A Temporal Constraint Network
Based Temporal Planner. In Proc. of the 21st Workshop of the UK Planning and
Scheduling Special Interest Group, pp. 99109, Delft, The Netherlands.
Castillo, L., Fernandez-Olivares, J., & Gonzalez, A. (2006). Efficiently Handling Temporal
Knowledge in an HTN planner. In Proc. of the 16th Int. Conf. on Automated Planning
and Scheduling, pp. 6372.

Chan, T. (2005). All-Pairs Shortest Paths with Real Weights in O n3 / log n Time. In
Algorithms and Datastructures, LNCS, pp. 318324. Springer.
Chaudhuri, S., & Zaroliagis, C. D. (2000). Shortest Paths in Digraphs of Small Treewidth.
Part I: Sequential Algorithms. Algorithmica, 27 (3), 212226.
Cherkassky, B. V., Goldberg, A. V., & Radzik, T. (1996). Shortest paths algorithms: theory
and experimental evaluation. Mathematical programming, 73 (2), 129174.
Cherkassky, B. V., & Goldberg, A. V. (1999). Negative-cycle detection algorithms. Mathematical Programming, 85, 277311.
Chleq, N. (1995). Efficient Algorithms for Networks of Quantitative Temporal Constraints.
In Proc. of the 1st Int. Workshop on Constraint Based Reasoning, pp. 4045.
385

fiPlanken, De Weerdt, & Van der Krogt

Conrad, P. R., Shah, J. A., & Williams, B. C. (2009). Flexible execution of plans with
choice. In Proc. of the 19th Int. Conf. on Automated Planning and Scheduling.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to Algorithms, 2nd edition. MIT Press.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal Constraint Networks. Artificial Intelligence, 49 (13), 6195.
Demetrescu, C., & Italiano, G. F. (2006). Fully Dynamic All-Pairs Shortest Paths with
Real Edge Weights. Journal of Computer and System Sciences, 72 (5), 813837.
Dijkstra, E. W. (1959). A note on two problems in connexion with graphs.. Numerische
Mathematik, 1, 269271.
Dragan, F. F. (2005). Estimating all pairs shortest paths in restricted graph families: a
unified approach. Journal of Algorithms, 57 (1), 121.
Even, S., & Gazit, H. (1985). Updating Distances in Dynamic Graphs. Methods of Operations Research, 49, 371387.
Fiedler, N. (2008). Analysis of Java implementations of Fibonacci Heap. http://tinyurl.
com/fibo-heap.
Floyd, R. W. (1962). Algorithm 97: Shortest path. Communications of the ACM, 5 (6),
345.
Fredman, M., & Tarjan, R. E. (1987). Fibonacci Heaps and Their Uses in Improved Network
Optimization Algorithms. Journal of the ACM, 34 (3), 596615.
Geisberger, R., Sanders, P., Schultes, D., & Delling, D. (2008). Contraction hierarchies:
Faster and simpler hierarchical routing in road networks. In Proc. of the Int. Workshop
on Experimental Algorithms, pp. 319333. Springer.
Girvan, M., & Newman, M. E. J. (2002). Community Structure in Social and Biological
Networks. Proc. of the National Academy of Sciences of the USA, 99 (12), 78217826.
Golumbic, M. (2004). Algorithmic Graph Theory and Perfect Graphs. Elsevier.
Graham, R. L., Knuth, D. E., & Patashnik, O. (1989). Concrete Mathematics: A Foundation
for Computer Science (1st edition). Addison-Wesley.
Han, K., Sekharan, C. N., & Sridhar, R. (1997). Unified All-Pairs Shortest Path Algorithms
in the Chordal Hierarchy. Discrete Applied Mathematics, 77 (1), 5971.

Han, Y. (2008). A Note of an O n3 / log n -time Algorithm for All-Pairs Shortest Paths.
Information Processing Letters, 105 (3), 114116.
Heggernes, P. (2006). Minimal triangulations of graphs: A survey. Discrete Mathematics,
306 (3), 297317. Minimal Separation and Minimal Triangulation.
Johnson, D. B. (1977). Efficient Algorithms for Shortest Paths in Sparse Networks. Journal
of the ACM, 24 (1), 113.
Kjrulff, U. (1990). Triangulation of Graphs - Algorithms Giving Small Total State Space.
Tech. rep., Aalborg University.
386

fiComputing APSP by Leveraging Low Treewidth

Klein, P. N., Mozes, S., & Weimann, O. (2010). ShortestPaths in Directed Planar Graphs
with Negative Lengths: A Linear-space O n log2 n -time Algorithm. ACM Transactions on Algorithms, 6 (2), 118.
Mondou, J. F., Crainic, T. G., & Nguyen, S. (1991). Shortest path algorithms: A computational study with the C programming language. Computers & Operations Research,
18 (8), 767786.
Pettie, S. (2004). A New Approach to All-pairs Shortest Paths on Real-weighted Graphs.
Theoretical Computer Science, 312 (1), 4774.
Planken, L. R., de Weerdt, M. M., & van der Krogt, R. P. J. (2008). P3 C: A New Algorithm
for the Simple Temporal Problem. In Proc. of the 18th Int. Conf. on Automated
Planning and Scheduling, pp. 256263.
Planken, L. R., de Weerdt, M. M., & van der Krogt, R. P. J. (2011). Computing allpairs shortest paths by leveraging low treewidth. In Proc. of the 21st Int. Conf. on
Automated Planning and Scheduling, pp. 170177.
Planken, L. R., de Weerdt, M. M., & Yorke-Smith, N. (2010). Incrementally Solving STNs
by Enforcing Partial Path Consistency. In Proc. of the 20th Int. Conf. on Automated
Planning and Scheduling, pp. 129136.
Ranise, S., & Tinelli, C. (2003). The SMT-LIB Format: An Initial Proposal. In Proc. of
Pragmatics of Decision Procedures in Automated Reasoning.
Rose, D. J. (1972). A Graph-Theoretic Study of the Numerical Solution of Sparse Positive
Definite Systems of Linear Equations. In Read, R. (Ed.), Graph theory and computing,
pp. 183217. Academic Press.
Rossi, F., Venable, K. B., & Yorke-Smith, N. (2006). Uncertainty in soft temporal constraint problems: A general framework and controllability algorithms for the fuzzy
case. Journal of AI Research, 27, 617674.
Satish Kumar, T. K. (2005). On the Tractability of Restricted Disjunctive Temporal Problems. In Proc. of the 15th Int. Conf. on Automated Planning and Scheduling, pp.
110119.
Shah, J. A., & Williams, B. C. (2008). Fast Dynamic Scheduling of Disjunctive Temporal
Constraint Networks through Incremental Compilation. In Proc. of the 18th Int. Conf.
on Automated Planning and Scheduling, pp. 322329.
Stergiou, K., & Koubarakis, M. (2000). Backtracking algorithms for disjunctions of temporal
constraints. Artificial Intelligence, 120 (1), 81117.
Strichman, O., Seshia, S. A., & Bryant, R. E. (2002). Deciding Separation Formulas with
SAT. In Proc. of the 14th Int. Conf. on Computer Aided Verification, Vol. 2404 of
LNCS, pp. 209222. Springer.
Tarjan, R. E., & Yannakakis, M. (1984). Simple Linear-time Algorithms to Test Chordality
of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs. SIAM Journal on Computing, 13 (3), 566579.
387

fiPlanken, De Weerdt, & Van der Krogt

Thorup, M. (2004). Fully-dynamic All-Pairs Shortest Paths: Faster and Allowing Negative
Cycles. In Algorithm Theory, Vol. 3111 of LNCS, pp. 384396. Springer.
Warshall, S. (1962). A Theorem on Boolean Matrices. Journal of the ACM, 9 (1), 1112.

388

fiJournal of Artificial Intelligence Research 43 (2012) 257-292

Submitted 09/11; published 02/12

Consistency Techniques for Flow-Based Projection-Safe Global Cost
Functions in Weighted Constraint Satisfaction
J.H.M. Lee
K.L. Leung

JLEE @ CSE . CUHK . EDU . HK
KLLEUNG @ CSE . CUHK . EDU . HK

Department of Computer Science and Engineering
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong

Abstract
Many combinatorial problems deal with preferences and violations, the goal of which is to find
solutions with the minimum cost. Weighted constraint satisfaction is a framework for modeling
such problems, which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments. Typical solution methods for weighted
constraint satisfaction problems (WCSPs) are based on branch-and-bound search, which are made
practical through the use of powerful consistency techniques such as AC*, FDAC*, EDAC* to
deduce hidden cost information and value pruning during search. These techniques, however, are
designed to be efficient only on binary and ternary cost functions which are represented in table
form. In tackling many real-life problems, high arity (or global) cost functions are required. We
investigate efficient representation scheme and algorithms to bring the benefits of the consistency
techniques to also high arity cost functions, which are often derived from hard global constraints
from classical constraint satisfaction.
The literature suggests some global cost functions can be represented as flow networks, and
the minimum cost flow algorithm can be used to compute the minimum costs of such networks in
polynomial time. We show that naive adoption of this flow-based algorithmic method for global
cost functions can result in a stronger form of -inverse consistency. We further show how the
method can be modified to handle cost projections and extensions to maintain generalized versions
of AC* and FDAC* for cost functions with more than two variables. Similar generalization for
the stronger EDAC* is less straightforward. We reveal the oscillation problem when enforcing
EDAC* on cost functions sharing more than one variable. To avoid oscillation, we propose a weak
version of EDAC* and generalize it to weak EDGAC* for non-binary cost functions. Using various
benchmarks involving the soft variants of hard global constraints ALL D IFFERENT, GCC, SAME,
and REGULAR, empirical results demonstrate that our proposal gives improvements of up to an
order of magnitude when compared with the traditional constraint optimization approach, both in
terms of time and pruning.

1. Introduction
Constraint satisfaction problems (CSPs) occur in all walks of industrial applications and computer
science, such as scheduling, bin packing, transport routing, type checking, diagram layout, just
to name a few. Constraints in CSPs are functions returning true or false. These constraints are
hard in the sense that they must be satisfied. In over-constrained and optimization scenarios, hard
constraints have to be relaxed or softened. The weighted constraint satisfaction framework adopt
soft constraints as cost functions returning a non-negative integer with an upper bound . Solution techniques for solving weighted constraint satisfaction problems (WCSPs) are made practic
2012
AI Access Foundation. All rights reserved.

fiL EE & L EUNG

cal by enforcing various consistency notions during branch-and-bound search, such as NC*, AC*,
FDAC* (Larrosa & Schiex, 2004, 2003) and EDAC* (de Givry, Heras, Zytnicki, & Larrosa, 2005).
These enforcement techniques, however, are designed to be efficient only on binary and ternary cost
functions which are represented in table form. On the other hand, many real-life problems can be
modelled naturally by global cost functions of high arities. We investigate efficient representation
scheme and algorithms to bring the benefits of the existing consistency techniques for binary and
ternary cost functions to also high arity cost functions, which are often derived from hard global
constraints from classical constraint satisfaction.
In existing WCSP solvers, these high arity cost functions are delayed until they become binary
or ternary during search. The size of the tables is also a concern. The lack of efficient handling
of high arity global cost functions in WCSP systems greatly restricts the applicability of WCSP
techniques to more complex real-life problems. To overcome the difficulty, we incorporate van Hoeve, Pesant, and Rousseaus (2006) flow-based algorithmic method into WCSPs, which amounts
to representing global cost functions as flow networks and computing the minimum costs of such
networks using the minimum cost flow algorithm. We show that a naive incorporation of global cost
functions into WCSPs would result in a strong form of the -inverse consistency (Zytnicki, Gaspin,
& Schiex, 2009), which is still relatively weak in terms of lower bound estimation and pruning.
The question is then whether we can achieve stronger consistencies such as GAC* and FDGAC*,
the generalized versions of AC* and FDAC* respectively, for non-binary cost functions efficiently.
Consistency algorithms for (G)AC* and FD(G)AC* involve three main operations: (a) computing
the minimum cost of the cost functions when a variable x is fixed with value v, (b) projecting the
minimum cost of a cost function to the unary cost functions for x at value v, and (c) extending
unary costs to the related high arity cost functions. These operations allow cost movements among
cost functions and shifting of costs to increase the global lower bound of the problem, which implies more opportunities for domain value prunings. Part (a) is readily handled using the minimum
cost flow (MCF) algorithm as proposed in van Hoeve et al.s method. However, parts (b) and (c)
modify the cost functions, which can possibly destroy the required flow-based structure of the cost
functions required by van Hoeve et al.s method. To overcome the difficulty, we propose and give
sufficient conditions for the flow-based projection-safety property. If a global cost function is flowbased projection-safe, the flow-based property of the cost function is guaranteed to be retained no
matter how many times parts (b) and (c) are performed. Thus, the MCF algorithm can be applied
throughout the enforcements of GAC* and FDGAC* to increase search efficiency.
A natural next step is to generalize also the stronger consistency EDAC* (de Givry et al., 2005)
to EDGAC*, but this turns out to be non-trivial. We identify and analyze an inherent limitation
of EDAC* similar to the case of Full AC* (de Givry et al., 2005). ED(G)AC* enforcement will
go into oscillation if two cost functions share more than one variable, which is common when a
problem involves high arity cost functions. Sanchez, de Givry, and Schiex (2008) did not mention
the oscillation problem but their method for enforcing EDAC* for the special case of ternary cost
functions would avoid the oscillation problem. In this paper, we give a weak form of EDAC*,
which can be generalized to weak EDGAC* for cost functions of any arity. Most importantly, weak
EDAC* is reduced to EDAC* when no two cost functions share more than one variable. Weak
EDGAC* is stronger than FDGAC* and GAC*, but weaker than VAC (Cooper, de Givry, Sanchez,
Schiex, Zytnicki, & Werner, 2010). We also give an efficient algorithm to enforce weak EDGAC*.
Based on the theoretical results, we prove that some of the soft variants of ALL D IFFERENT,
GCC, SAME, and REGULAR constraints are flow-based projection-safe, and give polynomial time
258

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

algorithms to enforce GAC*, FDGAC* and also weak EDGAC* on these cost functions. Experiments are carried out on different benchmarks featuring the proposed global cost functions. Empirical results coincide with the theoretical prediction on the relative strengths of the various consistency notions and the complexities of the enforcement algorithms. Our experimental results also
confirm that stronger consistencies such as GAC*, FDGAC* and weak EDGAC* are worthwhile
and essential in making global cost functions in WCSP practical. In addition, the reified approach
(Petit, Regin, & Bessiere, 2000) and strong IC are too weak in estimating useful lower bounds
and pruning the search space in branch-and-bound search.
The rest of the paper is organized as follows. Section 2 gives the necessary definitions and
background, while Section 3 gives related work. Generalized versions of existing consistency techniques for global cost functions are presented and compared in Section 4. Enforcement algorithms
for these consistencies are exponential in general. We introduce the notion of flow-based projectionsafety, and describe polynomial time consistency enforcement algorithms for global cost functions
enjoying the flow-based projection-safety property. In Section 5, we prove that the softened form of
some common hard global constraints are flow-based projection-safe and give experimental results
demonstrating the feasibility and efficiency of our proposal both in terms of runtime and search
space pruning. Section 6 summarizes our contributions and shed light on possible directions for
future research.

2. Background
We give the preliminaries on weighted constraint satisfaction problems, global cost functions and
network flows.
2.1 Weighted Constraint Satisfaction
A weighted constraint satisfaction problem (WCSP) is a special case of valued constraint satisfaction (Schiex, Fargier, & Verfaillie, 1995) with a cost structure ([0, . . . , ], , ). The structure
contains a set of integers from 0 to  ordered by the standard ordering . Addition  is defined by
a  b = min(, a + b), and subtraction  is defined only for a  b, a  b = a  b if a 6=  and
  a =  for any a. Formally,
Definition 1 (Schiex et al., 1995) A WCSP is a tuple (X , D, C, ), where:
 X is a set of variables {x1 , x2 , . . . , xn } ordered by their indices;
 D is a set of domains D(xi ) for xi  X , only one value of which can be assigned to xi ;
 C is a set of cost functions WS with different scope S = {xs1 , . . . , xsn }  X that maps a
tuple   L(S), where L(S) = D(xs1 )  . . . D(xsn ), to [0, . . . , ].
An assignment of a set of variables S  X , written as {xs1 7 vs1 , . . . , xsn 7 vsn }, is to
assign each variable xsi  S to a value vsi  D(xsi ). When the context is clear and assuming
an ordering by the variable indices, we abuse notations by considering an assignment also a tuple
 = (vs1 , . . . , vsn )  L(S), where L(S) = D(xs1 )  D(xs2 )  . . . D(xsn ). The notation [xsi ]
denotes the value vsi assigned to xsi  S, and [S  ] denotes the tuple formed by projecting  onto
S   S.
Without loss of generality, we assume C = {W }  {Wi | xi  X }  C + . W is a constant
nullary cost function. Wi is a unary cost function associated with each xi  X . C + is a set of cost
259

fiL EE & L EUNG

functions WS with scope S containing two or more variables. If W and {Wi } are not defined, we
assume Wi (v) = 0 for all v  D(xi ) and W = 0. To simplify the notation, we denote Ws1 ,s2 ,...,sn
for the cost function on variables {xs1 , xs2 , . . . , xsn } if the context is clear.
Definition
L 2 Given a WCSPL(X , D, C, ). The cost of a tuple   L(X ) is defined as cost() =
W  xi X Wi ([xi ])  WS C + WS ([S]). A tuple   L(X ) is feasible if cost() < , and
is a solution of a WCSP if cost() is minimum among all tuples in L(X ).
WCSPs are usually solved with basic branch-and-bound search augmented with consistency
techniques which prune infeasible values from variable domains and push costs into W while
preserving the equivalence of the problems, i.e. the cost of each tuple   L(X ) is unchanged.
Different consistency notions have been defined such as NC*, AC*, FDAC* (Larrosa & Schiex,
2004, 2003), and EDAC* (de Givry et al., 2005).
Definition 3 A variable xi is node consistent (NC*) if each value v  D(xi ) satisfies Wi (v) 
W <  and there exists a value v   D(xi ) such that Wi (v  ) = 0. A WCSP is NC* iff all
variables are NC*.
Procedure enforceNC*() in Algorithm 1 enforces NC*, where unaryProject() moves unary
costs towards W while keeping the solution unchanged, and pruneVal() removes infeasible
values. The variables Q, R, and S are global propagation queues used for further consistency
enforcements explained in later sections. They are initially empty if not specified.

1
2

3
4
5

6
7
8
9
10
11

12
13
14

Procedure enforceNC*()
foreach xi  X do unaryProject (xi );
pruneVal ();
Procedure unaryProject(xi)
 := min{Wi (v) | v  D(xi )};
W := W  ;
foreach v  D(xi ) do Wi (v) := Wi (v)  ;
Procedure pruneVal()
foreach xi  X do
flag := false;
foreach v  D(xi ) s.t. Wi (v)  W =  do
D(xi ) := D(xi ) \ {v};
flag := true;
if flag then
// For further consistency enforcement.
empty if not specified
Q := Q  {xi };
S := S  {xi };
R := R  {xi };

Algorithm 1: Enforce NC*
260

Assume initially

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

Based on NC*, AC* and FDAC* have been developed for binary (Larrosa & Schiex, 2004,
2003) and ternary cost functions (Sanchez et al., 2008). Enforcing these consistency notions requires two equivalence preserving transformations besides NC* enforcement, namely projection
and extension (Cooper & Schiex, 2004).
A projection, written as Project(WS ,Wi ,v,), transforms (WS , Wi ) to (WS , Wi ) with
respect to a value v  D(xi ) and a cost , where   min{WS () | [xi ] = v    L(S)}, such
that:

Wi (u)   if u = v,
 Wi (u) =
Wi (u)
otherwise.

WS ()   if [xi ] = v,
 WS () =
WS ()
otherwise.
An extension, written as Extend(WS ,Wi ,v,), transforms (WS , Wi ) to (WS , Wi ) with
respect to a value v  D(xi ) and a cost , where   Wi (v), such that:

Wi (u)   if u = v,

 Wi (u) =
Wi (u)
otherwise.

WS ()   if [xi ] = v,
 WS () =
WS ()
otherwise.
2.2 Global Constraints and Global Cost Functions
A global constraint is a constraint with special semantics. They are usually with high arity, and thus
cannot be propagated efficiently with standard consistency algorithms. With their special semantics,
special propagation algorithms can be designed to achieve efficiency.
A global cost function is the soft variant of a hard global constraint. The cost of each tuple
indicates how much the tuple violates the corresponding global constraint. One global constraint
can give rise to different global cost functions using different violation measures. A global cost
function returns 0 if the tuple satisfies the corresponding global constraint. The notation SOFT GC
denotes the global cost function derived from a global constraint GC using a violation measure .
For instance, the ALL D IFFERENT constraint has two soft variants.
Definition 4 (Petit, Regin, & Bessiere, 2001) The cost function SOFT ALL D IFFERENTvar returns
the minimum number of variable assignments that needed to be changed so that the tuple contains
only distinct values; while SOFT ALL D IFFERENTdec returns the number of pairs of variables having
the same assigned value.
2.3 Flow Theory
Definition 5 A flow network G = (V, E, w, c, d) is a connected directed graph (V, E), in which
each edge e  E has a weight we , a capacity ce , and a demand de  ce .
An (s, t)-flow f from a source s  V to a sink t  V of a value  in G is defined as a mapping
from E to real numbers such that:
P
P

(s,u)E f(s,u) =
(u,t)E f(u,t) = ;
P
P

(u,v)E f(u,v) =
(v,u)E f(v,u)  v  V \ {s, t};
261

fiL EE & L EUNG

 de  fe  ce  e  E.
For simplicity, we call an (s, t)-flow as a flow if s and t have been specified.
P
Definition 6 The cost of a flow f is defined as cost(f ) =
eE we fe . A minimum cost flow
problem of a value  is to find the flow whose value is  and cost is minimum.
If  is not given, it is assumed to be the maximum value among all flows.
To solve minimum cost flow problems, various approaches have been developed. Two of those
are the successive shortest path and cycle-cancelling algorithms (Lawler, 1976). Both algorithms
focus on the computation in the residual network of the corresponding flow network.
Definition 7 Given a flow f in the network G = (V, E, w, c, d). The residual network Gres =
(V, E res , wres , cres , dres ) is defined as:
 E res = {(u, v)  e | f(u,v) < c(u,v) }  {(v, u)  e | f(u,v) > d(u,v) };

,if f(u,v) < c(u,v)
w(u,v)
res
 w(u,v) =
w(u,v) ,if f(v,u) > d(v,u)

c(u,v)  f(u,v) ,if f(u,v) < c(u,v)
 cres
(u,v) =
f(u,v)  d(u,v) ,if f(v,u) > d(v,u)
 dres
e = 0, for all e  E;
The successive shortest path algorithm successively increases flow values of the edges along the
shortest paths between s and t in the residual network until the value of flow reaches  or no more
paths can be found. The cycle-cancelling algorithm reduces the cost of the given flow to minimum
by removing negative cycles in the induced residual network.
In consistency enforcement with flow, we usually deal with the following problem: consider a
(s, t)-flow f in a network G = (V, E, w, c, d) with minimum cost, and an edge e  E. The problem
is to determine whether increasing (or decreasing) fe by one unit keeps the flow value unchanged,
and compute the minimal cost of the new resultant flow if possible. Again, such a problem can
be solved using the residual network Gres (Regin, 2002; van Hoeve et al., 2006): we compute the
shortest path P from v  to u in Gres , where e = (u , v  )  E. If P exists, the value of the flow is
unchanged if fe is increased by one unit. The new minimum cost can be computed by the following
theorem.
flow by increasing fe
Theorem 1 (Regin, 2002; van Hoeve et al., 2006) Suppose f  is the resultant
P
by one unit. Then the minimum value of cost(f  ) is cost(f ) + weres + eP weres .

Theorem 1 reduces the problem into finding a shortest path from v  to u , which can be made
incremental for consistency enforcement. If we want to reduce a unit flow from an edge, we can
apply similar methods to those used in Theorem 1.

3. Related Work
Global cost functions can be handled using constraint optimization, which focuses on efficient
computation of min{WS () |   L(S)} and enforcing GAC on their hard constraint forms
WS ()  zS , where zS is the variable storing costs (Petit et al., 2001). Van Hoeve et al. (2006)
262

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

develop a framework for global cost functions representable by flow networks, whose computation
is polynomial in the size of networks. Beldiceanu (2000) and Beldiceanu, Carlsson and Petit (2004)
further develop a representation scheme for global cost functions using a graph-based approach and
an automaton approach. Under their framework, the computation of all global cost functions can be
reduced to only considering a fixed set of global cost functions, e.g. the SOFT REGULAR functions.
On the other hand, to efficiently remove more search space during WCSPs solving, various
consistency notions have been developed. Examples are NC* (Larrosa & Schiex, 2004), BAC
(Zytnicki et al., 2009), AC* (Larrosa & Schiex, 2004), FDAC* (Larrosa & Schiex, 2003), and
EDAC* (de Givry et al., 2005). Stronger consistency notions, namely OSAC and VAC (Cooper
et al., 2010), are also defined, but enforcement requires a relaxation of the cost valuation structure
V () to rational numbers, and current implementations are efficient only on binary WCSPs. For
ternary cost functions, AC, FDAC and EDAC are introduced (Sanchez et al., 2008). Cooper (2005)
incorporates the concept of k-consistency into WCSPs to form complete k-consistency. However,
the time and space complexities increase exponentially as the problem size increases, making complete k-consistency impractical to enforce for general WCSPs.

4. Consistency Notions for Global Cost Functions
In this section, we discuss four consistency notions for high-arity cost functions: (1) strong inverse consistency (strong IC), (2) generalized arc consistency (GAC*), (3) full directional generalized arc consistency(FDGAC*), and (4) generalized EDAC*. These consistency notions require
exponential time to enforce in general, but flow-based global cost functions (van Hoeve et al., 2006)
enjoy polynomial time enforcement.
4.1 Strong -Inverse Consistency
Strong -inverse consistency is based on -inverse consistency (IC) (Zytnicki et al., 2009).
Definition 8 (Zytnicki et al., 2009) Given a WCSP P = (X , D, C, ). A cost function WS  C is
-inverse consistent (IC) if there exists a tuple   L(S) such that WS () = 0. A WCSP is IC
iff all cost functions are IC.
The procedure enforceIC() in Algorithm 2 enforces IC. Each cost function WS is made
IC by lines 3 to 6, which move costs from WS to W by simple arithmetic operations.
1
2
3
4
5
6
7

Function enforceIC()
flag := false;
foreach WS  C do
 := min{WS () |   L(S)};
W := W  ;
foreach   L(S) do WS () := WS ()  ;
if  > 0 then flag := true;
return flag;

Algorithm 2: Enforcing IC on a WCSP
The time complexity of enforceIC() in Algorithm 2 depends on the time complexities
of lines 3 and 5. Line 3 computes the minimum cost and line 5 modifies the cost of each tuple
263

fiL EE & L EUNG

to maintain equivalence. In general, these two operations are exponential in the arity of the cost
function. However, the first operation can be reduced to polynomial time for a global cost function.
One such example is flow-based global cost functions (van Hoeve et al., 2006).
Definition 9 (van Hoeve et al., 2006) A global cost function WS is flow-based if WS can be represented as a flow network G = (V, E, w, c, d) such that
min{cost(f ) | f is the max. {s, t}-flow of G} = min{WS () |   L(S)},
where s  V is the fixed source and t  V is the fixed destination.
For examples, the cost function SOFT ALL D IFFERENTdec (S) returns the number of pairs of
variables in S that share the same value, and is shown to be flow-based (van Hoeve et al., 2006). An
example of its corresponding flow network, where S = {x1 , x2 , x3 , x4 }, is shown in Figure 1. All
edges have a capacity of 1. The numbers on the edges represent the weight of the edges. If an edge
has no number, the edge has zero weight. The thick lines show the flow corresponding to the tuple
 = (a, c, b, b) having a cost of 1.
x1
a
x2
s

b

t

1
2

x3
c

1

x4

Figure 1: An example flow network for SOFT

ALL D IFFERENTdec

With flow-based cost functions, the first operation (computing the minimum cost) can be reduced to time polynomial to the network size for those constraints. The second operation can be
reduced to constant time using the S data structure suggested by Zytnicki et al. (2009). Instead of
deducting the projected value  from each tuple in WS , we simply store the projected value in S .
When we want to know the actual value of WS , we compute WS  S .
Enforcing IC only increases W but does not help reduce domain size. Consider the WCSP
in Figure 2. It is IC, but the value c  D(x1 ) cannot be a part of any feasible tuple. All tuples
associated with the assignment {x1 7 c} must have a cost of at least 4: 1 from W , 2 from W1 , and
1 from W1,2 . To allow domain reduction, extra conditions are added to IC to form strong IC.
x1
a
b
c

W1
0
2
2

x2
a
b

 = 4, W = 1
x1 x2 W1,2
W2
a
a
0
1
b
a
0
0
c
a
1

x1
a
b
c

Figure 2: A WCSP which is IC

264

x2
b
b
b

W1,2
0
0
1

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

Definition 10 Given a WCSP P = (X , D, C, ). Consider a non-unary cost function WS  C +
and a variable xi  S. A tuple   L(S) is the -support of a value v  D(xi ) with respect to WS
iff [xi ] = v and W  Wi (v)  WS () < . The cost function WS is strong IC iff it is IC, and
each value in each variable in S has a -support with respect to WS . A WCSP is strong IC if it is
IC and all non-unary cost functions are strong IC.
For instance, the WCSP in Figure 2 is not strong IC. The value c  D(x1 ) does not have a support, since W  W1 (c)  min{W1,2 () | [x1 ] = c    L({x1 , x2 })} =  = 4. Removal of
c  D(x1 ) makes it so.
Strong IC collapses to GAC in classical CSPs when WCSPs collapse to CSPs. Although
its definition is similar to BAC (Zytnicki et al., 2009), their strengths are incomparable. BAC
gathers cost information from all cost functions on the boundary values, while we only consider the
information from one non-unary cost function for all individual values.
The procedure enforceSIC() in Algorithm 3 enforces strong IC, based on the W-AC*3()
Algorithm (Larrosa & Schiex, 2004). The algorithm maintains a propagation queue Q of variables.
Cost functions involving variables in Q are potentially not strong IC. At each iteration, an arbitrary variable xj is removed from Q by the function pop() in constant time. The algorithm enforces
strong IC for the cost functions involving xj from lines 4 to 6. The existence of -support is
enforced by findSupport(). If domain reduction occurs (findSupport() returns true),
or W increases (enforceIC() returns true), variables are pushed onto Q at lines 6 and 7 respectively, indicating that IC are potentially broken. If the algorithm terminates, i.e. Q = , no
variables are pushed into Q at line 6, or Q is not set to X at line 7. It implies all variables are strong
IC and the WCSP is IC. Thus the WCSP is strong IC after execution.

1
2
3
4
5
6
7

8
9
10
11
12
13
14

Procedure enforceSIC()
Q := X ;
while Q 6=  do
xj := pop (Q);
foreach WS  C + s.t. xj  S do
foreach xi  S \ {xj } do
if findSupport (WS , xi ) then Q := Q  {xi };
if enforceIC () then Q := X ;
Function findSupport(WS , xi )
flag := false;
foreach v  D(xi ) do
 := min{WS () | [xi ] = v};
if W  Wi (v)   =  then
D(xi ) := D(xi ) \ {v};
flag := true;
return flag;

Algorithm 3: Enforcing strong IC of a WCSP
The procedure enforceSIC() is correct and must terminate. Its complexity can be analyzed
by abstracting the worst-case time complexities of findSupport() and enforceIC() as
265

fiL EE & L EUNG

fstrong and fIC respectively. Using an augment similar to the proof of Larrosa and Schiexs
(2004) Theorems 12 and 21, the complexity can be stated as follows.
Theorem 2 The procedure enforceSIC() a time complexity of O(r 2 edfstrong +ndfIC ), where
r is the maximum arity of all cost functions, d is maximum domain size, e = |C + | and n = |X |.
Proof: The while loop at line 2 iterates at most O(nd) times. In each iteration, line 6 executes at
most O(r  |N (j)|) times, where N (j) is the set of soft constraints restricting
xj . Since line 7 exeP
cutes at most O(nd) times, the overall time complexity is O(rdfstrong  nj=1 |N (j)| + ndfIC ) =
Pn
O(r 2 edfstrong
Pn + ndfIC ). O( j=1 |N (j)|) = O(re) holds since each cost function counts at most
r times in j=1 |N (j)|. Thus, it must terminate.

Corollary 1 The procedure enforceSIC() must terminate. The resultant WCSP is strong IC,
and equivalent to the original WCSP.
In general, due to enforceIC() and findSupport(), enforcing strong IC is exponential in r. As discussed before, enforceIC() can be reduced to polynomial time for flow-based
global cost functions. Similarly, findSupport() can be executed efficiently and incrementally
for flow-based global cost functions since line 10 can be computed in polynomial time using minimum cost flow.
Another property we are interested in is confluence. A consistency  is confluent if enforcing 
always transforms a problem P into a unique problem P  which is . AC* is not confluent (Larrosa
& Schiex, 2004). With different variable and/or cost function orderings, AC* enforcement can lead
to different equivalent WCSPs with different values of W . BAC is confluent (Zytnicki et al.,
2009). Following the proofs of Propositions 3.3 and 4.3 by Zytnicki et al., it can be shown that
strong IC is also confluent.
Theorem 3 (Confluence) Given a WCSP P = (X , D, C, ), there exists a unique WCSP P  =
(X , D  , C  , ) which is strong IC and equivalent to P .
The above concludes the theoretical analysis of strong IC. In the following, we compare the
strength of strong IC with the classical consistency notions used in constraint optimization. Following Petit et al. (2000), we define the reified form of a WCSP as follows:
Definition 11 (Petit et al., 2000) Given a WCSP P = (X , D, C, ). The reified form, reified(P ),
of P is a constraint optimization problem (COP) (X h , D h , C h , obj), where:
 X h = X  Z, where Z = {zS | WS  C \ {W }} are the cost variables.
 D h (xi ) = D(xi ) for xi  X , and D h (zS ) = {0, . . . ,   W  1} for each zS  Z. If
  W < 1, D h (zS ) = .
h
 C h contains the reified constraints CS{z
, which are the hard constraints associated with
S}
h
h
each WS  C \ {W
L } defined as WS ()  zS for each tuple   L(S). C also contains CZ
defined as W  zS Z zS < .
L
 The objective is to minimize obj, where obj = W  zS Z zS .

266

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

Finding the optimal solution of reified(P ) is equivalent to solving P . However, enforcing GAC
on reified(P ) cannot remove more values than enforcing strong IC of P . It is because strong
IC of P implies GAC of reified(P ) but not vice versa.
In general, we define the strength comparison as follows.
Definition 12 Given a problem P representable by two models (P ) and (P ). A consistency  on
(P ) is strictly stronger than another consistency  on (P ), written as  on (P ) >  on (P ),
or  >  if (P ) = (P ), iff (P ) is  whenever (P ) is , but not vice versa.
Zytnicki et al. (2009) also define consistency strength comparison in terms of unsatisfiability detection, which is subsumed by our new definition. If  on (P ) implies  on (P ), and enforcing 
on (P ) detects unsatisfiability, enforcing  on (P ) can detect unsatisfiability as well.
Given a WCSP P = (X , D, C, ). We show strong IC on P is stronger than GAC on
reified(P ) by the following theorem.
Theorem 4 Strong IC on P > GAC on reified(P ).
Proof: Figure 2 has given an example that a WCSP whose reified COP is GAC may not be strong
IC. We have to show that strong IC on P implies GAC on reified(P ).
First, CZh is GAC. If |C|  1, the constraint is obviously GAC. If |C| > 1, for each vSi  D(zSi ),
to satisfy the constraint, we just let other cost variables take the value 0, i.e. supports for each
vSi  D(zSi ) exist.
h
Besides, CS{z
is GAC. By the definition of IC, there exists a tuple   L(S) such that
S}
h
WS ( ) = 0. The tuple  can form the support of vS  D(zS ) with respect to CS{z
. Besides,
S}
the -support  of v  D(xi ), together with vS = WS ( ), forms a support for v  D(xi ).

For a detailed comparison between strong IC of WCSPs and GAC of the reified approach,
readers can refer to the work of Leung (2009).
When the cost functions are binary, strong IC cannot be stronger than AC*. In the next section,
we show this fact by proving GAC*, a generalized version of AC*, to be stronger than strong IC.
4.2 Generalized Arc Consistency
Definition 13 (Cooper & Schiex, 2004) Given a WCSP P = (X , D, C, ). Consider a cost function
WS  C + and a variable xi  S. A tuple   L(S) is a simple support of v  D(xi ) with respect to
WS with xi  S iff [xi ] = v and WS () = 0. A variable xi  S is star generalized arc consistent
(GAC*) with respect to WS iff xi is NC*, and each value vi  D(xi ) has a simple support  with
respect to WS . A WCSP is GAC* iff all variables are GAC* with respect to all related non-unary
cost functions.
The definition is designed with practical considerations, and is slightly weakerL
than Definition 4.2
in the work of Cooper et al. (2010), which also requires WS () =  if W  xi S Wi ([xi ]) 
WS () = .
GAC* collapses to AC* for binary cost functions (Larrosa & Schiex, 2004) and AC for ternary
cost functions (Sanchez et al., 2008). GAC* is stronger than strong IC, as a WCSP which is GAC*
is also strong IC, but not vice versa. We state without proof as follows.
Theorem 5 GAC* > strong IC.
267

fiL EE & L EUNG

The procedure enforceGAC*() in Algorithm 4 enforces GAC* for a WCSP (X , D, C, ),
based on the W-AC*3() Algorithm (Larrosa & Schiex, 2004). The propagation queue Q stores a set
of variables xj . If xj  Q, all variables involved in the same cost functions as xj are potentially
not GAC*. Initially, all variables are in Q. A variable xj is pushed into Q only after values are
removed from D(xj ). At each iteration, an arbitrary variable xj is removed from the queue by
the function pop() at line 4. The function findSupport() at line 7 enforces GAC* of xi with
respect to WS by finding the simple supports. The infeasible values are removed by the function
pruneVal() at line 10. If a value is removed from D(xi ), the simple supports of other related
variables may be destroyed. Thus, xi is pushed back to Q again by the procedure pruneVal(). If
GAC*() terminates, all values in each variable domain must have a simple support. The WCSP is
now GAC*.

1
2

3
4
5
6
7

8
9
10

11
12
13
14
15
16
17
18

Procedure enforceGAC*()
Q := X ;
GAC* ();
Procedure GAC*()
while Q 6=  do
xj := pop (Q);
foreach WS  C + s.t. xj  S do
foreach xi  S \ {xj } do
if findSupport (WS , xi ) then
// For further consistency enforcement.
initially empty if not specified
S := S  {xi };
R := R  {xi };

Assume

pruneVal ();
Function findSupport(WS , xi )
flag := false;
foreach v  D(xi ) do
 := min{WS () | [xi ] = v};
if Wi (v) = 0   > 0 then flag := true;
Wi (v) := Wi (v)  ;
foreach   L(S) s.t. [xi ] = a do WS () := WS ()  ;
unaryProject (xi );
return flag;

Algorithm 4: Enforcing GAC* for a WCSP
The procedure enforceGAC*() in Algorithm 4 is correct and must terminate. The proof is
similar to that of Theorem 2. By replacing fstrong by fGAC (the worst-case time complexities
of findSupport()) and fIC by O(nd) (the complexity of pruneVal()), the complexity of
Algorithm 4 can be stated as follows.
Theorem 6 The procedure enforceGAC*() has a time complexity of O(r 2 edfGAC +n2 d2 ), where
n, d, e, and r are as defined in Theorem 2.
268

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

Corollary 2 The procedure enforceGAC*() must terminate. The resultant WCSP is GAC*, and
equivalent to the original WCSP.
In general, the procedure enforceGAC*() is exponential in the maximum arity of the cost
function due to findSupport(). The function findSupport() consists of two operations: (1)
finding the minimum cost of the tuple associated with {xi 7 v} at line 13, and (2) performing
projection at lines 15 and 16. The time complexity of the first operation is polynomial for a flowbased global cost function WS . The method introduced by van Hoeve et al. (2006) can be applied to
the first operation as discussed in Section 4.1. However, the second operation modifies WS to WS ,
which requires changing the costs of an exponential number of tuples. Cooper and Schiex (2004)
use a similar technique as the one by Zytnicki et al. (2009) (similar to the technique described in
Section 4.1) to make the modification constant time. However, the resulting WS may not be flowbased, affecting the time complexity of the subsequent procedure calls. To resolve the issue, we
introduce flow-based projection-safety. If WS is flow-based projection-safe, the flow property can
be maintained throughout enforcement.
Definition 14 Given a property T . A global cost function WS is T projection-safe iff WS satisfies
the property T , and for all WS derived from WS by a series of projections and extensions, WS also
satisfies T .
In other words, a T projection-safe cost function WS still satisfies T after any numbers of projections or extensions. This facilitates the use of T to derive efficient consistency enforcement
algorithms. In the following, we consider a special form of T projection-safety, when T is the
flow-based property.
In the following, we first define FB, and show that FB is the sufficient condition of flow-based
projection-safety.
Definition 15 A global cost function satisfies FB if:
1. WS is flow-based, with the corresponding network G = (V, E, w, c, d) with a fixed source
s  V and a fixed destination t  V ;
2. there exists a subjective function mapping each maximum flow f in G to each tuple f 
L(S), and;
3. there exists an injection mapping from an assignment {xi 
7 v} to aP
subset of edges E  E
such that for allP
maximum flow f and the corresponding tuple f , eE fe = 1 whenever
f [xi ] = v, and eE fe = 0 whenever f [xi ] 6= v
Lemma 1 Given WS satisfying FB. Suppose WS is obtained from Project(WS ,Wi ,v,) or
Extend(WS ,Wi ,v,). Then WS also satisfies FB.
Proof: We only prove the part for projection, since the proof for extension is similar. We first show
that WS is flow-based (condition 1). Assume G = (V, E, w, c, d) is the corresponding flow network
of WS . After the projection, G can be modified to G = (V, E, w , c, d), where w (e) = w(e)  
if e  E is an edge corresponding to {xi 7 v} and w (e) = w(e) otherwise. The resulting G is
269

fiL EE & L EUNG

the corresponding flow network of WS , since for the maximum flow f in G with minimum cost:
X

we fe =

eE

X

we fe  

eE

X

fe

eE

= min{WS () |   L(S)}  

X

fe

eE

= min{WS () |   L(S)}.
Moreover, since the topology of G = (V, E, w , c, d) is the same as that of G = (V, E, w, c, d),
WS also satisfies conditions 2 and 3.

Theorem 7 If a global cost function WS satisfies FB, then WS is flow-based projection-safe.
Proof: Initially, if no projection and extension is performed, directly from Definition 15, WS is
flow-based. Assume WS is the cost function formed from WS after a series of projections and/or
extensions. By Lemma 1, WS still satisfies FB and thus flow-based. Result follows.

As shown by Theorem 7, if a global cost function is flow-based projection-safe, it is always
flow-based after projections and/or extensions. Besides, by checking the conditions in Definition
15, we can determine whether a global cost function is flow-based projection-safe.
Note that the computation in the proof is performed under the standard integer set instead of
V () for practical considerations. Further investigation is required if the computation can be restricted on V ().
By using Theorem 7, we can apply the results by van Hoeve et al. (2006) to compute the value
min{WS () | [xi ] = v    L(S)} in polynomial time throughout GAC* enforcement. Besides,
the proof gives an efficient algorithm to perform projection in polynomial time by simply modifying
the weights of the corresponding edges.
Again, we use SOFT ALL D IFFERENTdec as an example. Van Hoeve et al. (2006) have shown
that SOFT ALL D IFFERENTdec (S) satisfies conditions 1 and 2 in Definition 15. Besides, from the
network structure shown in Figure 1, by taking E = {(xi , v)} for each assignment {xi 7 v},
condition 3 can be satisfied. Thus, SOFT ALL D IFFERENTdec is flow-based projection-safe.
x1

1

a
x2
s

b

t

1
2

x3
c

1

x4

Figure 3: The flow network SOFT

ALL D IFFERENT dec ()

after projection

Consider the flow network of the SOFT ALL D IFFERENTdec in Figure 1. Suppose we perform
Project(SOFT ALL D IFFERENTdec (S),W1 ,a,1). The network is modified to the one in Figure 3, the weight of the edge (x1 , a) in which is decreased from 0 to 1. The flow has a cost of 0,
which is the cost of the tuple (a, c, b, b) after projection.
270

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

If a global cost function is flow-based projection-safe, findSupport() has a time complexity
depending on the time complexity of computing the minimum cost flow and the shortest path from
any two nodes in the network. The result is stated by the following theorem.
Theorem 8 Given the time complexities of computing the minimum cost flow and the shortest path
are K and SP respectively. If WS is flow-based projection-safe, findSupport() has a time
complexity of O(K + d  SP), where d = max{|D(xi )| | xi  S} and  is the maximum size of E.
Proof: By Theorem 1, after finding a first flow by O(K), the minimum cost at line 13 can be found
by augmenting the existing flow, which only requires O(SP). Line 15 can be done in constant time,
while line 16 can be done as follows: (a) decrease the weights of all edges corresponding to xi 7 v
by , and (b) augment the current flow to the one with new minimum cost by changing the flow
values of the edges whose weights have been modified in the first step. The first step requires O(),
while the second step requires O(  SP). At most  edges are required to change their flow values
to maintain minimality of the flow cost. Since unaryProject() requires O(d), the overall time
is O(K + d(SP +   SP) + d) = O(K + d  SP).

The time complexity for finding a shortest path in a graph SP varies by applying different
algorithms. In general, SP = O(|V ||E|), as negative weights are introduced in the graph. However,
it can be reduced by applying a potential value on each vertices, as in Johnsons (1977) algorithm.
For example, in Figure 3, we can increase the potential value of vertices a and t by 1, and the weight
of the edges (b, t) and (c, t) by 1. This increases the cost of all paths from s to t by 1, and makes
the weights of all edges non-negative. Dijkstras (1959) algorithm can thus be applied, reducing the
time complexity to O(|E| + |V |log(|V |)).
Although GAC* can be enforced in polynomial time for flow-based projection-safe global cost
functions, the findSupport() function still requires runtime much higher than that for binary or
ternary table cost functions in general. To optimize the performance of the solver, we can delay the
consistency enforcement of global cost functions until all binary or ternary table cost functions are
processed at line 5.
FDAC* for binary cost functions (Larrosa & Schiex, 2003) suggests that a stronger consistency
can be deduced by using the extension operator. We will discuss the generalized version of FDAC*
for non-binary cost functions in the next section.
4.3 Full Directional Generalized Arc Consistency
Definition 16 Given a WCSP P = (X , D, C, ). Consider a cost function WS  C + and a variable
xi  S. A tuple  is the full support of the value v  D(x
L i ) with respect to WS and a subset of
variables U  S \ {xi } iff [xi ] = v and WS ()  xj U Wj ([xj ]) = 0. A variable xi is
directional star generalized arc consistent (DGAC*) with respect to WS if it is NC* and each value
v  D(xi ) has a full support with respect to {xu | xu  S  u > i}. A WCSP is full directional star
generalized arc consistent (FDGAC*) if it is GAC* and each variable is DGAC* with respect to all
related non-unary cost functions.
FDGAC* collapses to GAC when WCSPs collapse to CSPs. Moreover, FDGAC* collapses to
FDAC* (Larrosa & Schiex, 2003) when the arity of the cost functions is two. However, FDGAC*
is incomparable to FDAC for ternary cost functions (Sanchez et al., 2008). FDAC requires full
supports with not only zero unary but also zero binary costs for the next variable in S only, while
we only require all variables with full supports of zero unary costs.
271

fiL EE & L EUNG

By definition, FDGAC* is stronger than GAC* and also strong IC.
Theorem 9 FDGAC* > GAC* > strong IC.
The procedure enforceFDGAC*() enforces FDGAC* for a WCSP, based on the FDAC*() Algorithm (Larrosa & Schiex, 2003). The propagation queues Q and R store a set of variables. If
xj  Q, all variables involved in the same cost functions as xj are potentially not GAC*; if xj  R,
the variables xi involved in the same cost functions as xj are potentially not DGAC*. When values
are removed from the domain of variable xj , xj is pushed onto Q and R; when unary costs of the
values in D(xj ) are increased, xj is pushed to R. At each iteration, GAC* is maintained by the
procedure GAC*(). DGAC* is then enforced by DGAC*(). Enforcing DGAC* follows the ordering
from the largest index to the smallest index such that the full supports of values in the domains
of variables with smaller indices are not destroyed by DGAC*-enforcement on those with larger
indices. The variable with the largest index in R is removed from R by the function popMax().
By implementing R as a heap, popMax() requires only constant time. DGAC* enforcement is performed at line 10 by findFullSupport(). In the last step, NC* is re-enforced by pruneVal().
The iteration continues until all propagation queues are empty, which implies all values in each
variable domain has a simple and full support, and all variables are NC*. The resultant WCSP is
FDGAC*.

1
2
3
4
5

6
7
8
9
10
11
12

13
14
15
16
17
18
19
20

Procedure enforceFDGAC*()
R := Q := X ;
while R 6=   Q 6=  do
GAC* ();
DGAC* ();
pruneVal ();
Procedure DGAC*()
while R 6=  do
xu := popMax (R);
foreach WS  C + s.t. xu  S do
for i = n DownTo 1 s.t. xi  S \ {xu } do
if findFullSupport (WS , xi , S  {xj | j > i}) then R := R  {xi };
S := S  {xi } ;
// For further consistency enforcement.

Function findFullSupport(WS , xi , U )
foreach xj  U do
foreach vj  D(xj ) do
foreach   L(S) s.t. [xj ] = vj do WS () := WS ()  Wj (vj );
Wj (vj ) := 0;
flag := findSupport (WS , xi );
foreach xj  U do findSupport (WS , xj );
unaryProject (xi );
return flag;

Algorithm 5: Enforcing FDGAC* on a WCSP
272

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

The procedure enforceFDGAC*() in Algorithm 5 is correct and must terminate, the proof
of which is similar to those of Theorems 3 and 4 by Larrosa and Schiex (2003). The worst-case
time complexity of enforceFDGAC*() can be stated in terms of that of findFullSupport()
(fDGAC ) and findSupport() (fGAC ) as follows.
Theorem 10 The procedure enforceFDGAC*() has a time complexity of O(r 2 ed(nfDGAC +
fGAC ) + n2 d2 ), where n, d, e, and r are as defined in Theorem 2.
Proof: First we analyze the time complexity of enforcing DGAC*. Consider the procedure
DGAC*() at line 6. The while-loop iterates at most O(n) times. Since no value is removed in
line 10, where i > j, it is not pushed back to R at line
the while-loop, once xi is processed at P
11. Thus, line 10 executes at most O(r nj=0 |N (j)|) = O(r 2 e) times, where N (j) is the set of
cost functions restricting xj . Therefore, the time complexity of DGAC*() is O(r 2 efDGAC ). Since
DGAC*() executes at most O(nd) times throughout the global enforcement iteration. Thus the time
spent on enforcing DGAC* is O(nr 2 edfDGAC )
Although GAC*() is called O(nd) times, it does nothing if no values are removed from variable
domains. Thus we count the number of times calling findSupport(). Since the variables are
pushed into Q only when a value is removed, findSupport() only executes at most O(nd) times
throughout the global enforcement iteration. Similar arguments apply to pruneVal() at line 10
inside GAC*() defined in Algorithm 4. With the proof similar to Theorem 6, the time spent on
enforcing GAC* is O(r 2 edfGAC + n2 d2 ).
The pruneVal at line 5 executes O(nd) times, and each time it requires a time complexity of
O(nd). Therefore, the overall time complexity is O(r 2 ed(nfDGAC + fGAC ) + n2 d2 ).

Corollary 3 The procedure enforceFDGAC*() must terminate. The resultant WCSP is FDGAC*
and equivalent to the original WCSP.
Again, the complexity is exponential in the maximum arity due to the function findSupport()
and findFullSupport(). In the following, we focus the discussion on findFullSupport().
The first part (lines 15 and 16) performs extensions to push all the unary costs back to WS . By
the time we execute line 17, all unary costs Wj , where xj  U , are 0, and enforcing GAC* for xi
achieves the second requirement of DGAC* (each v  D(xi ) has a full support). Line 18 re-instates
GAC* for all variables xj  U . Note that success in line 17 guarantees that Wj (vj ) = 0 for some
value vj appearing in a tuple  which makes WS () = 0.
Again, flow-based projection-safety helps reduce the time complexity of findFullSupport()
throughout the enforcement. The proof of Theorem 7 gives a polynomial time algorithm to perform
extension and maintain efficient computation of min{WS () |   L(S)}. Flow-based projectionsafety can be guaranteed by Theorem 7, which requires checking conditions 1, 2, and 3 in the
definition of flow-based projection-safety. The complexity result follows from Theorems 2 and 8.
Theorem 11 If WS is a flow-based projection-safe global cost function, findFullSupport()
has a time complexity of O(K + rd  SP), where r, , d, K and SP are as defined in Theorems 2
and 8.
Proof: Similarly to Theorem 8, lines 13 to 16 can be performed as follows: (a) for each xj  U
and each value vj  D(xj ), increase the weights of all edges corresponding to {xj 7 vj } by
Wj (vj ), and then reduce Wj (vj ) to 0, and (b) find a flow with the new minimum cost in the new
273

fiL EE & L EUNG

flow network. The first step can be done in O(rd), as the size of U is bounded by the arity of
the cost function r. The second step can be done in O(K), which also acts as preprocessing for
findSupport() at lines 17 and 18. By Theorem 8, lines 17 and 18 can be done in O(rd  SP).
Thus, the overall complexity is O(r  d + K + rd  SP) = O(K + rd  SP).

Similarly to GAC*, the DGAC* enforcement for global cost functions can be delayed until all
binary and ternary table cost functions are processed.
4.4 Generalizing Existential Directional Arc Consistency
EDAC* (de Givry et al., 2005) can be generalized to EDGAC* using the full support definition as
in FDGAC*. However, we find that naively generalizing EDAC* is not always enforceable, due to
the limitation of EDAC*. In the following, we explain and provide a solution to this limitation.
4.4.1 A N I NHERENT L IMITATION

OF

EDAC*

Definition 17 (de Givry et al., 2005) Consider a binary WCSP P = (X , D, C, ). A variable
xi  X is existential arc consistent (EAC*) if it is NC* and there exists a value v  D(xi ) with zero
unary cost such that it has full supports with respect to all binary cost functions Wi,j on {xi , xj }
and {xj }. P is existential directional arc consistent (EDAC*) if it is FDAC* and all variables are
EAC* .
Enforcing EAC* on a variable xi requires two main operations: (1) compute
M
 = min {Wi (a) 
min {Wi,j (a, b)  Wj (b)}},
aD(xi )

Wi,j C

bD(xj )

which determines whether enforcing full supports breaks the NC* requirement, and (2) if  > 0, enforce full supports with respect to all cost functions Wi,j  C by invoking findFullSupport (xi ,
Wi,j , {xj }), implying that NC* is no longer satisfied and hence W can be increased by enforcing
NC*. EDAC* enforcement will oscillate if constraints share more than one variable. The situation
is similar to Example 3 by de Givry et al. (2005). We demonstrate by the example in Figure 4(a),
1 and W 2 . It is FDAC* but not EDAC*. If
which shows a WCSP with two cost functions W1,2
1,2
1
x2 takes the value a, W1,2 (v, a)  W1 (v)  1 for all values v  D(x1 ); if x2 takes the value b,
2 (v, b)  C (v)  1 for all values v  D(x ). Thus, by enforcing full supports of each value
W1,2
1
1
in D(x2 ) with respect to all cost functions and {x1 }, NC* is broken and W can be increased. To
1 , resulting in Figincrease W , we enforce full supports: the cost of 1 in W1 (a) is extended to W1,2
2
1 to W results
ure 4(b). No costs in W1 can be extended to W1,2 . Performing projection from W1,2
2
in Figure 4(c). The WCSP is now EAC* but not FDAC*. Enforcing FDAC* converts the problem
state back to Figure 4(a).
The problem is caused by the first step, which does not tell how the unary costs are separated
for extension to increase W . Although an increment is predicted, the unary cost in W1 (a) has a
1 or W 2 . During computation, no information is obtained on how
choice of moving itself to W1,2
1,2
the unary costs are moved. As shown, a wrong movement breaks DAC* without incrementing W ,
resulting in oscillation.
This problem does not occur in existing solvers which handle only up to ternary cost functions.
The solvers allow only one binary cost functions for every pair of variables. If there are indeed
two cost functions for the same two variables, the cost functions can be merged into one, where the
274

fiC ONSISTENCY T ECHNIQUES

 = 4, W
x1
W1
a
1
a
0
b
b

x1
a
b

x2
a
b

x1
a
a
b
b

W2
0
0

=0
1
x2 W1,2
a
0
b
2
a
1
b
0
x2
a
b
a
b

2
W1,2
1
0
0
2

FOR

x1
a
b

x2
a
b

(a) Original WCSP

S OFT G LOBAL C OST F UNCTIONS

 = 4, W
x1
W1
a
0
a
0
b
b
W2
0
0

x1
a
a
b
b

=0
1
x2 W12
a
1
b
3
a
1
b
0
x2
a
b
a
b

2
W12
1
0
0
2

(b) After Extension

x1
a
b

IN

WCSP S

 = 4, W
x1
W1
a
0
a
0
b
b

x2
a
b

W2
1
0

x1
a
a
b
b

=0
1
x2 W12
a
0
b
3
a
0
b
0
x2
a
b
a
b

2
W12
1
0
0
2

(c) After Projection

Figure 4: Oscillation in EDAC* enforcement
cost of a tuple in the merged function is the sum of the costs of the same tuple in the two original
functions. However, if we allow high arity global cost functions, sharing of more than one variable
would be common and necessary in many scenarios. A straightforward generalization of EDAC*
for non-binary cost functions would inherit the same oscillation problem. In the case of ternary cost
functions, Sanchez et al. (2008) cleverly avoid the oscillation problem by re-defining full supports
to include not just unary but also binary cost functions. During EDAC enforcement, unary costs are
distributed through extension to binary cost functions. However, the method is only designed for
ternary cost functions. In the following, we define a weak version of EDAC*, which is based on the
notion of cost-providing partitions.
4.4.2 C OST-P ROVIDING PARTITIONS

AND

W EAK EDGAC*

Definition 18 A cost-providing partition Bxi for variable xi  X is a set of sets {Bxi ,WS | xi  S}
such that:
 |Bxi | is the number of constraints which scope includes xi ;
 Bxi ,WS  S;
 Bxi ,WSj  Bxi ,WSk =  for any two different constraints WSk , WSj  C + , and;
S
S
 Bx ,W Bx Bxi ,WS = ( WS C + xi S S) \ {xi }.
i

S

i

Essentially, Bxi forms a partition of the set containing all variables constrained by xi . If xj 
Bxi ,WS , the unary costs in Wj can only be extended to WS when enforcing EAC* for xi . This
avoids the problem of determining how the unary costs of xj are distributed when there exists more
than one constraint on {xi , xj }.
Based on the cost-providing partitions, we define weak EDAC*.
Definition 19 Consider a binary WCSP P = (X , D, C, ) and cost-providing partitions {Bxi |
xi  X }. A weak fully supported value v  D(xi ) of a variable xi  X is a value with zero unary
m , there exists a value b  D(x ) such
cost and for each variable xj and a binary cost function Wi,j
j
m
m
m = {}, and W
m = {xj }. A variable xi
that Wi,j (v, b) = 0 if Bxi ,Wi,j
(v,
b)

W
(b)
=
0
if
B
j
x
,W
i
i,j
i,j
is weak existential arc consistent (weak EAC*) if it is NC* and there exists at least one weak fully
supported value in its domain. P is weak existential directional arc consistent (weak EDAC*) if it
is FDAC* and each variable is weak EAC*.
275

fiL EE & L EUNG

Weak EDAC* collapses to AC when WCSPs collapse to CSPs for any cost-providing partition.
Moreover, weak EDAC* is reduced to EDAC* (de Givry et al., 2005) when the binary cost functions
share at most one variable.
We further generalize weak EDAC* to weak EDGAC* for n-ary cost functions.
Definition 20 Given a WCSP P = (X , D, C, ) and cost-providing partitions {Bxi | xi  X }.
A weak fully supported value v  D(xi ) of a variable xi is a value with zero unary cost and full
supports with respect to all cost functions WS  C + with xi  S and Bxi ,WS . A variable xi is weak
existential generalized arc consistent (weak EGAC*) if it is NC* and there exists at least one weak
fully supported value in its domain. P is weak existential directional generalized arc consistent
(weak EDGAC*) if it is FDGAC* and each variable is weak EGAC*.
Weak EDAC* and weak EDGAC* can be achieved using for any cost-providing partitions. Weak
EDGAC* is reduced to GAC when WCSPs collapse to CSPs.
Compared with other consistency notions, weak EDGAC* is strictly stronger than FDGAC*
and other consistency notions we have described. It can be deduced directly from the definition.
Theorem 12 For any cost-providing partitions, weak EDGAC* > FDGAC* > GAC* > strong IC
VAC is stronger than weak EDGAC*, as stated in the theorem below.
Theorem 13 VAC are strictly stronger than weak EDGAC* with any cost-providing partition.
Proof: A WCSP which is VAC must be weak EDGAC* for any cost-providing partition. Otherwise, there must exist a sequence of projections and extensions to increase W , which violates
Theorem 7.3 by Cooper et al. (2010). On another hand, Cooper et al. (2010) give an example which
is EDAC* but not VAC. Results follow.

However, weak EDGAC* is incomparable to complete k-consistency (Cooper, 2005), where k >
2, for any cost-providing partition. It is because EDAC* is already incomparable to complete kconsistency (Sanchez et al., 2008).
To compute the cost-providing partition Bxi of a variable xi , we could apply Algorithm 6, which
is a greedy approach to partition the set Y containing all variables related to xi defined in line 1,
hoping to gathering more costs by gathering more variables at one cost function, increasing the
chance of removing more infeasible values and raising W .

1
2
3
4
5

ProcedureSfindCostProvidingPartition(xi)
Y = ( WS C + xi S S) \ {xi };
Sort C + in decreasing order of |S|;
foreach WS  C + s.t. xi  S do
Bxi ,WS = Y  S;
Y = Y \ S;

Algorithm 6: Finding Bxi
The procedure enforceWeakEDGAC*() in Algorithm 7 enforces weak EDGAC* of a WCSP.
The cost-providing partitions are first computed in line 1. The procedure makes use of four propagation queues P, Q, R and S. If xi  P, the variable xi is potentially not weak EGAC* due to
276

fiC ONSISTENCY T ECHNIQUES

1
2
3
4
5
6
7
8
9

10
11
12
13
14

15
16

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

Procedure enforceWeakEDGAC*()
foreach xi  X do findCostProvidingPartition (xi );
R := Q := S := X ;
while S 6=  
SR 6=   Q 6=  do
P := S  xi S,WS C + (S \ {xi });
weakEGAC* ();
S := ;
DGAC* ();
GAC* ();
pruneVal ();
Procedure weakEGAC*()
while P 6=  do
xi := pop(P);
if findExistentialSupport (xi ) then
R := R  {xi };
P := P  {xj | xi , xj  WS , WS  C + };
Function findExistentialSupport(xi)
flag := false;
L
L
 := minaD(xi ) {Wi (a)  xi S,WS C + min[xi ]=a {WS ()  xj Bx

i ,WS

Wj ([xj ])}};

19

if  > 0 then
flag := true;
foreach WS  C + s.t. xi  S do findFullSupport (WS , xi , Bxi ,WS );

20

return flag;

17
18

Algorithm 7: Enforcing weak EDGAC*

a change in unary costs or a removal of values in some variables. If xj  R, the variables xi involved in the same cost functions as xj are potentially not DGAC*. If xj  Q, all variables in
the same cost functions as xj are potentially not GAC*. The propagation queue S helps build P
efficiently. The procedure weakEGAC*() enforces weak EGAC* on each variable by the procedure
findExistentialSupport() in line 12. If findExistentialSupport() returns true, a
projection has been performed for some cost functions. The weak fully supported values of other
variables may be destroyed. Thus, the variables constrained by xi are pushed back onto P for revision in line 14. DGAC* and GAC* are enforced by the procedures DGAC*() and GAC*(). A
change in unary cost requires re-examining DGAC* and weak EGAC*, which is done by pushing
the variables into the corresponding queues in lines 13 and 14, and lines 11 and 12 in Algorithm 5.
In the last step, NC* is enforced by pruneVal(). Again, if a value in D(xi ) is removed, GAC*,
DGAC* or weak EGAC* may be destroyed, and xi is pushed into the corresponding queues for
re-examination by pruneVal() in Algorithm 1. If all propagation queues are empty, all variables
are GAC*, DGAC*, and weak EGAC*, i.e. the WCSP is weak EDGAC*.
The algorithm is correct and must terminate. We analyze the time complexity by abstracting the
worst-case time complexities of findSupport(), findFullSupport() and
277

fiL EE & L EUNG

findExistentialSupport() as fGAC , fDGAC , and fEGAC respectively. The overall time
complexity is stated as follows.
Theorem 14 The procedure enforceWeakEDGAC*() requires O((nd+)(fEGAC +r 2 efDGAC +
nd) + r 2 edfGAC ), where n, d, e, and r are defined in Theorem 2.
Proof: As line 1 requires only O(nr), we only analyze the overall time complexity spent by each
sub-procedure and compute the overall time complexity.
A variable is pushed into S if a value is removed or weak EGAC* is violated. The former
happens O(nd) times, while the latter occurs O() times (each time weak EGAC* is violated, W
will be increased). Since P is built on S, findExistentialSupport() is executed at most
O(nd + ) times throughout the global enforcement. Thus, the time complexity spent on enforcing
weak EGAC* is O((nd + )fEGAC ).
A variable is pushed into R if either a value is removed, or unary costs are moved by GAC*
or weak EGAC* enforcement. Thus, DGAC*() is called O(nd + ) times. Each time DGAC*() is
called, by Theorem 10, it requires O(r 2 efDGAC ) for DGAC* enforcement. Thus, the time complexity of enforcing DGAC* is O((nd + )r 2 efDGAC ).
A variable is pushed into Q only if a value is removed. Thus, findSupport() inside the
procedure GAC*() is called at most O(nd) times throughout the global enforcement. Using the
proof similar to Theorem 6, the overall time spent on enforcing GAC* is O(r 2 edfGAC + n2 d2 ).
The main while-loop in line 3 terminates when all propagation queues are empty. Thus, the main
while-loop iterates O(nd + ) times. The time complexity for re-enforcing NC* by pruneVal()
at line 9 is O((nd + )nd).
By summing up all time complexity results, the overall time complexity is O((nd+)(fEGAC +

r 2 efDGAC + nd) + r 2 edfGAC ).
Corollary 4 The procedure enforceWeakEDGAC*() must terminate. The resultant WCSP is
weak EDGAC*, and equivalent to the original WCSP.
The procedure enforceWeakEDGAC*() is again exponential due to findSupport(),
findFullSupport() and findExistentialSupport(). In the following, we focus on the
last procedure. It first checks whether a weak fully supported value exists by computing , which
determines whether NC* still holds if we perform findFullSupport() from line 19. If  equals
0, a weak fully supported value exists and nothing should be done; otherwise, this value can be made
weak fully supported by the for-loop at line 19. The time complexity depends on two operations:
(1) computing the value of  in line 16, and; (2) finding full supports by the line 19. These two
operations are exponential in |S| in general. However, if all global cost functions are flow-based
projection-safe, the time complexity of the above operations can be reduced to polynomial time.
In the next section, we put theory into practice. We demonstrate our framework with different
benchmarks and compare the results with the current approach.

5. Towards a Library of Efficient Global Cost Functions
In the previous section, we only show SOFT ALL D IFFERENTdec is flow-based projection-safe. In
the following, we further show that a range of common global cost functions are also flow-based
projection-safe. We give experimental results on various benchmarks with different consistency
notions and different global cost functions.
278

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

5.1 A List of Flow-Based Projection-Safe Global Cost Functions
In this section, we show that a number of common global cost functions are flow-based projectionsafe. They include the soft variants of A LL D IFFERENT, GCC, SAME, and REGULAR constraints.
5.1.1 T HE S OFT VARIANTS

OF

A LL D IFFERENT

The ALL D IFFERENT() constraint restricts variables to take distinct values (Lauriere, 1978). There
are two possible soft variants, namely SOFT ALL D IFFERENTdec () and ALL D IFFERENTvar (). The
former returns the number of pairs of variables that share the same value, while the latter returns
the least number of variables that must be changed so that all variables take distinct values. The
cost function SOFT ALL D IFFERENTdec () is shown to be flow-based projection-safe in Section 4.2.
In fact, this also implies that another cost function
SOFT ALL D IFFERENTvar () is flow-based projection-safe. The SOFT ALL D IFFERENT var () function
also corresponds to a flow network with structure similar to that of SOFT ALL D IFFERENTdec () but
different in weights on the edges connecting to t (van Hoeve et al., 2006). We state the results as
follows.
Theorem 15 The cost functions SOFT
flow-based projection-safe.

ALL D IFFERENT var (S) and SOFT ALL D IFFERENTdec (S) are

5.1.2 T HE S OFT VARIANTS OF GCC
S
Given a set of values  = xi S D(xi ) and functions lb and ub that maps from  to non-negative
integers. Each value v   is associated with a upper bound ubv and a lower bound lbv . The
GCC(S, ub, lb) constraint is satisfied by a tuple   L(S) if the number of occurrences of a value
v   in  (denoted by #(, v)) is at most ubv times and at least lbv times (Regin, 1996). There
are two soft variants of GCC constraints, namely SOFT GCCvar () and SOFT GCCval () (van Hoeve
et al., 2006).
Definition 21 (van Hoeve et al., 2006) Define two functions s(, v) and e(, v): s(, v) returns
lbv  #(, v) if #(, v)  lbv , and 0 otherwise; e(, v) returns #(, v)  ubv if #(, v)  ubv , and
0 otherwise.
P
P
The global
SOFT GCC var (S) returns max{ v s(, v),
v e(, v)}, proP cost functions P
P
vided that v lbv  |S|  v ubv ; while SOFT GCCval (S) returns v (s(, v) + e(, v)).
Van Hoeve et al. (2006) show that both SOFT GCCvar and SOFT GCCdec are flow-based, and the
flow networks have structures similar to the SOFT ALL D IFFERENT cost functions. With a proof
similar to Theorem 15, we can show the following theorem.

Theorem 16 The cost functions SOFT GCCvar (S) and SOFT GCCval (S) are flow-based projectionsafe.
5.1.3 T HE S OFT VARIANTS

OF

S AME

Given two sets of variables S1 and S2 with |S1 | = |S2 | and S1  S2 = . The SAME(S1 ,S2 )
constraint is satisfied by the tuple   L(S1  S2 ) if [S1 ] is a permutation of [S2 ] (Beldiceanu,
Katriel, & Thiel, 2004). The hard SAME() constraint can be softened to the global cost function
SOFT SAMEvar () (van Hoeve et al., 2006):
279

fiL EE & L EUNG

Definition 22 (van Hoeve et al., 2006) Given that the union operation  is the multi-set union, and
1 2 returns the symmetric difference between two multi-sets 1 and 2 , i.e.1 2 = (1 \
2 )  (2 \ 1 ).
S
S
The global cost function SOFT SAMEvar (S1 , S2 ) returns |( xi S1 {[xi ]})( yi S2 {[yi ]})|/2.
Theorem 17 The cost function

SOFT SAMEvar (S1 ,

S2 ) is flow-based projection-safe.

Proof: Van Hoeve et al. (2006) have shown that SOFT SAMEvar satisfies conditions 1 and 2 in
Definition 15. For instance, consider S1 = {x1 , x2 , x3 } and S2 = {x4 , x5 , x6 } with D(x1 ) = {a},
D(x2 ) = {a, b}, D(x3 ) = {b}, D(x4 ) = {a, b} ,and D(x5 ) = D(x6 ) = {a}. The flow network
corresponding to SOFT SAMEvar (S1 , S2 ) is shown in Fig. 5. Solid edges have zero weight and unit
capacity. Dotted edges have unit weight and a capacity of 3. The thick edges show the (s, t)-flow
corresponding to the tuple  = (a, b, b, b, a, a).
x1

s

x4

a

x2

x5

t

b
x6

x3

Figure 5: The flow network corresponding to the SOFT

SAMEvar (S1 ,

S2 ) constraint

Moreover, from the network structure, by taking E = {(xi , v)} for xi  S1 and v  D(xi ),
and E = {(v, yi )} for yi  S2 and v  D(yi ), the cost function satisfies condition 3. Thus, it is
flow-based projection-safe.

5.1.4 T HE S OFT VARIANTS

OF

R EGULAR

The R EGULAR constraint are defined based on regular languages. A regular language L(M ) can
be represented by a finite state automaton M = (Q, , , q0 , F ). Q is the set of states.  is a set of
characters. The symbol q0  Q denotes the initial state and F  Q is the set of final states. The
transition function  is defined as  : Q   7 Q. An automaton can be represented graphically as
shown in Figure 6, where the final states are denoted by double circles.
Given D(xi )   for each xi  S. The REGULAR(S, M ) constraint accepts the tuple  
L(S) if the corresponding string belongs to a regular language L(M ) represented by a finite state
automaton M = (Q, , , q0 , F ) (Pesant, 2004).
b
q0

a

q1
b

a
q2

Figure 6: The graphical representation of a automaton.
280

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

Two soft variants are defined for the REGULAR constraint, namely
SOFT REGULAR edit () (van Hoeve et al., 2006):

IN

WCSP S

SOFT REGULAR var ()

and

Definition 23 (van Hoeve et al., 2006) Define  to be the string formed from the tuple   L(S).
The cost functions SOFT REGULARvar (S) returns min{H( ,  ) |   L(M )}, where H( 1 ,  2 )
returns the number of positions at which two strings  1 and  2 differ; while SOFT REGULARedit (S)
returns min{E( ,  ) |   L(M )}, where E( 1 ,  2 ) returns the minimum number of insertions,
deletions and substitutions to transform  1 to  2 .
Theorem 18 The cost functions SOFT
projection-safe.

REGULAR var (S) and SOFT REGULAR edit (S) are flow-based

Proof: Van Hoeve et al. (2006) show that conditions 1 and 2 are satisfied. For example, consider the
automaton M shown in Figure 6 and S = {x1 , x2 , x3 } with D(x1 ) = {a} and D(x2 ) = D(x3 ) =
{a, b}. The flow networks corresponding to the SOFT REGULARvar (S) and SOFT REGULARedit (S)
functions are shown in Figure 7(a) and 7(b) respectively. The solid edges have zero weight and the
dotted edges have unit weight. The thick edges show the flow corresponding to the tuple (a, b, a).
The graphs are constructed as follows (van Hoeve et al., 2006): the vertices are separated into
n + 1 layers, where n = |X |, and each layer contains |Q| nodes. The source s is connected to q0,0
at the first layer, and the sink t is connected by {qn+1,i | qi  F } at the last layer. Between the ith
and (i + 1)th layers, an zero weighted edge representing v  D(xi ) connects qi,h at the ith layer
and qi+1,k at the (i + 1)th layer if (qk , v) = qh . For SOFT REGULARvar (S), a set of unit-weighted
edges Esub is added to the graph, where Esub = {(qi,k , qi+1,h )u | xi  X  u  D(xi )  v 6=
u s.t. (qk , v) = qh }. For SOFT REGULARedit (S), a set of unit-weighted edges Eedit is added to
the graph, where Eedit = Esub  {(qi,k , qi,h ) | xi  X  v s.t. (qk , v) = qh }  {(qi,k , qi,k )u |
xi  X  u  D(xi )}.
Moreover, each assignment {xi 7 v} maps to a set of edges E labelled as v at the layer xi in the
networks. For example, {x1 7 a} maps to the edges labeled as a at the layer x1 shown in Fig. 7(a).
Thus, the SOFT REGULAR cost functions satisfy condition 3 and are flow-based projection-safe. 
For the SOFT REGULAR cost functions, instead of the general flow computation algorithms, the
dynamic programming approach can be applied to compute the minimum cost (van Hoeve et al.,
2006; Demassey, Pesant, & Rousseau, 2006).
5.2 Experimental Results
In this section, a series of experiments with different benchmarks is conducted to demonstrate the
efficiency and practicality of different consistencies with different global cost functions. We implemented the strong IC, GAC*, FDGAC* and weak EDGAC* enforcement algorithms for these
global cost functions in ToulBar2 version 0.51 . We compare their performance using five benchmarks of different natures. In case of the reified COP models, the instances are solved using ILOG
Solver 6.0.
All benchmarks are crisp in nature, and are softened as follows. For each variable xi introduced, a random unary cost from 0 to 9 is assigned to each value in D(xi ). Soft variants of global
constraints are implemented as proposed. The target of all benchmarks is to find the optimal value
within 1 hour.
1. http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro

281

fiL EE & L EUNG

x2

x1
s

q0

x3

q0

q0
a

a

a

a

a
q1

a

a

q2

q0
a b

b

q1

q1

b

s

q0

q2

x1

x2

a

a,b

a

a

()
x3
a,b

q0

a b

b

a

a

q1

q2

var

q0

a
a

a

q1

q2

q1

b

b

b

b a

q1
a

a

a

q2

SOFT REGULAR

q2

t

b

b

(b)

t

b

SOFT REGULAR

q0

a

q2
b

(a)

q1

b

a

a

q2

b

b a

edit

()

Figure 7: The flow network corresponding to the soft REGULAR constraints
In the experiments, variables are assigned in lexicographical order. Value assignment starts with
the value with minimum unary cost. The test was conducted on a Sun Blade 2500 (2  1.6GHz
USIIIi) machine with 2GB memory. The average runtime and number of nodes of five instances
are measured for each value of n with no initial upper bound. Entries are marked with a * if the
average runtime exceeds the limit of 1 hour. The best results are marked using the  symbol.
5.2.1 B ENCHMARKS BASED

ON

S OFT A LL D IFFERENT

The ALL D IFFERENT() constraint has various applications. In the following, we focus on two: the
all-interval series and the Latin Square problem.
A LL I NTERVAL S ERIES
The all-interval series problem (prob007 in CSPLib) is modelled as a WCSP by two sets of variables {si } and {di } with domains {0, . . . n  1} to denote the elements and the adjacent difference
respectively. Random unary costs ranging from 0 to 9 is placed on each variable. We apply two
soft ALL D IFFERENT cost functions on {si } and {di } respectively, with a set of hard arithmetic
constraints di = |si  si+1 | for each i = 1, . . . , n  1.
The experiment is divided into two parts. We first compare results on enforcing different consistencies using global cost functions derived from ALL D IFFERENT() . Then we compare the result
on using different approaches on modelling SOFT ALL D IFFERENTdec () functions.
The result of the first experiment is shown in Table 1, which agrees with the theoretical strength
of the consistency notions as shown by the number of nodes. FDGAC* and GAC* always outperforms strong IC and the reified modelling, but FDGAC* requires more time than GAC*. One
explanation for this phenomenon is the problem structure. When xi and xi+1 are assigned, di is
282

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

automatically assigned due to the hard constraint di = |xi  xi+1 |. Thus, enforcing FDGAC* on
the variables {di } on every search node is not worthwhile.
(a)
n
8
9
10
11
12

Reified Approach
Time(s) Nodes
1.3
571.0
3.9
1445.0
52.0 15860.6
59.6 13286.2
180.1 31015.2

Strong IC
Time(s) Nodes
0.2
296.4
1.0
542.2
20.2
5706.6
31.8
7536.4
77.8 12886.4

(b)
n
8
9
10
11
12

Reified Approach
Time(s) Nodes
1.6
777.0
3.9
1480.4
56.8 17753.8
70.1 16149.6
214.9 38438.6

SOFT ALL D IFFERENT

var

GAC*
Time(s) Nodes
 0.1
181.0
0.6
300.2
10.8 2589.4
16.4 3273.6
37.6 5204.6

SOFT ALL D IFFERENT

Strong IC
Time(s) Nodes
0.2
396.8
1.0
553.2
21.2
5999.2
38.4
9113.2
96.4 16355.2

()
FDGAC*
Time(s) Nodes
 0.1
86.4
1.2
197.2
15.2 1612.4
21.0 1715.4
46.8 2259.0

Weak EDGAC*
Time(s) Nodes
 0.1
 15.4
 0.1
 20.2
 0.2
 47.4
 0.1
 33.6
 0.8
 47.6

dec

GAC*
Time(s) Nodes
0.2
219.6
0.6
301.8
11.6 2654.6
18.6 3551.8
46.8 6405.0

()
FDGAC*
Time(s) Nodes
 0.1
93.8
1.2
195.0
16.0 1604.2
23.0 1812.6
52.6 2451.6

Weak EDGAC*
Time(s) Nodes
 0.1
 16.0
 0.1
 28.8
 0.8
 70.4
 1.0
 68.6
 1.8
 71.2

Table 1: The time (in seconds) and the number of nodes in solving the all-interval series instances
The second experiment is based on the following fact. The SOFT ALL D IFFERENTdec (S) is flowbased projection-safe. It can be modelled as a flow network for consistency enforcement efficiently.
Another way to model the global cost functions is to apply the decomposition directly. The cost
returned by SOFT ALL D IFFERENTdec (S) is equal to the sum of the costs returned by a set of soft
binary cost functions {Wi,j | i > j  xi , xj  S}, where Wi,j (a, b) returns 0 if a 6= b and 1
otherwise. Thus, binary consistency notions, such as AC* and FDAC* can be applied directly.
We compare the performance on solving the all interval series problem with different modelling
methods on SOFT ALL D IFFERENTdec (). The results are shown in Table 2. Under the same level of
consistency, global cost functions remove an order of magnitude 10 to 100 times more nodes than
the binary decomposition. However, the time required for binary cost functions is much smaller than
global cost functions for AC* and FDAC*. This is because enforcing consistency notions on binary
cost functions is faster than global cost functions, and the removal of nodes is not great enough to
compensate the extra time for consistency enforcement of global cost functions. The runtime of
weak EDGAC*, however, is the fastest among all (2 times over the EDAC* counterpart) since it
is able to utilize global information to prune drastically more search space than any of the binary
decomposition approaches.
L ATIN S QUARES
The Latin Square problem (prob003 in CSPLib) of order n is to fill an initially empty n  n table
using numbers from {0, . . . , n  1} such that each number occurs once in every row and every
column. We model and relax the problem as a WCSP by a set of variables {xij } denoting the
value placed in the cell at the ith row and the j th column with random unary costs. These costs
are essentially restrictions/preferences on the value to be taken by each cell. Thus, our formulation
can model different variants of the Latin Square problem, including the Latin Square Completion
problem. One SOFT ALL D IFFERENT() cost function is posted on the variables at each row and each
283

fiL EE & L EUNG

n
8
9
10
11
12

AC*
Time(s) Nodes
 0.1
317.2
 0.1
596.0
1.4
9113.8
1.6
7672.2
4.6 15897.2

Binary Decomposition
FDAC*
EDAC*
Time(s) Nodes Time(s) Nodes
 0.1
 0.1
231.6
161.8
 0.1
 0.1
358.2
333.0
1.0
5957.4
1.0 5483.2
1.2
4578.4
1.2 4318.6
3.2 10534.8
2.6 7414.4

GAC*
Time(s) Nodes
0.2
219.6
0.6
301.8
11.6 2654.6
18.6 3551.8
46.8 6405.0

Global Cost Functions
FDGAC*
Weak EDGAC*
Time(s) Nodes Time(s) Nodes
 0.1
 0.1
 16.0
93.8
 0.1
 28.8
1.2
195.0
 0.8
 70.4
16.0 1604.2
 1.0
 68.6
23.0 1812.6
 1.8
 71.2
52.6 2451.6

Table 2: The time (in seconds) and the number of nodes in solving the all-interval series instances
with different modelling

column, denoting that same elements on the same rows and columns are allowed but with violation
costs so that the resultant cost is optimal. The result is shown in Table 3, which is similar to Table
1. Besides, the runtime also agrees with the theoretical strength of the consistency notions.

n
4
5
6
7
8

n
4
5
6
7
8

Reified Approach
Time(s)
Nodes
69.0 129958.0
*
*
*
*
*
*
*
*

(a) SOFT ALL D IFFERENT var()
Strong IC
GAC*
FDGAC*
Time(s)
Nodes
Time(s) Nodes Time(s) Nodes
 0.1
 0.1
1.8
3511.0
188.0
21.8
 0.1
490.2 348790.4
26.0 12368.0
66.2
*
*
*
*
3.4
244.4
*
*
*
*
43.2 1429.4
*
*
*
*
*
*

Weak EDGAC*
Time(s) Nodes
 0.1
 16.6
 0.1
 41.2
 1.4
 93.6
 16.2
 425.2
 148.2
 2066.5

Reified Approach
Time(s)
Nodes
62.7 121319.0
*
*
*
*
*
*
*
*

(b) SOFT ALL D IFFERENT dec()
Strong IC
GAC*
FDGAC*
Time(s)
Nodes
Time(s) Nodes Time(s) Nodes
 0.1
 0.1
2.6
3859.8
187.6
21.8
 0.1
531.4 376526.2
25.2 12254.0
66.2
*
*
*
*
3.4
244.4
*
*
*
*
43.4 1429.6
*
*
*
*
*
*

Weak EDGAC*
Time(s) Nodes
 0.1
 16.6
 0.1
 41.2
 1.4
 93.6
 15.8
 425.2
 147.2
 2066.5

Table 3: The time (in seconds) and the number of nodes in solving the Latin Square instances using
SOFT ALL D IFFERENT cost functions

The SOFT ALL D IFFERENTdec () cost functions can also be decomposed into binary disequality
cost functions. We also perform experiments to compare the binary decomposition approach and
our global cost function approach. The result is shown in Table 4. The result confirms that enforcing
stronger consistency on global cost functions is efficient in terms of the number of nodes explored
and also as the problem size grows large.
5.2.2 B ENCHMARKS BASED

ON

S OFT GCC

The GCC() constraint has various applications. In the following, we focus on the Latin Square
problem and round robin tournament problem.
284

fiC ONSISTENCY T ECHNIQUES

n
4
5
6
7
8

AC*
Time(s)
Nodes
 0.1
264.0
3.0
17955.8
639.2 2188035.4
*
*
*
*

Binary Decomposition
FDAC*
Time(s)
Nodes
 0.1
71.8
0.4
3059.6
167.8 346797.6
*
*
*
*

FOR

S OFT G LOBAL C OST F UNCTIONS

EDAC*
Time(s) Nodes
 0.1
39.4
 0.1
828.2
28.2 45817.8
*
*
*
*

IN

WCSP S

Global Constraint Approaches
GAC*
FDGAC*
Weak EDGAC*
Time(s) Nodes Time(s) Nodes Time(s) Nodes
 0.1
 0.1
 0.1
 16.6
187.6
21.8
 0.1
 0.1
 41.2
25.2 12254.0
66.2
 1.4
 93.6
*
*
3.4
244.4
 15.8
 425.2
*
*
43.4 1429.6
 147.2
 2066.5
*
*
*
*

Table 4: The time (in seconds) and the number of nodes in solving the Latin Square instances with
different modelling
(a)
n
4
5
6
7
8

Reified Approach
Time(s)
Nodes
3.8
4865.6
653.7 460989.2
*
*
*
*
*
*

Strong IC
Time(s)
Nodes
2.8
3859.8
621.2 376526.2
*
*
*
*
*
*

(b)
n
4
5
6
7
8

Reified Approach
Time(s)
Nodes
2.2
2815.8
165.2 122840.0
*
*
*
*
*
*

Strong IC
Time(s)
Nodes
1.4
2326.6
153.4 102493.6
*
*
*
*
*
*

GCCvar
GAC*
Time(s) Nodes
 0.1
220.8
38.6 14482.8
*
*
*
*
*
*

SOFT

SOFT

FDGAC*
Time(s) Nodes
 0.1
22
 0.1
66.2
4.8
244.6
58.4 1431.2
*
*

Weak EDGAC*
Time(s) Nodes
 0.1
 17.0
 0.1
 48.2
 1.2
 87.0
 16.4
 331.8
 459.6
 4730.8

FDGAC*
Time(s) Nodes
 0.1
20.4
 0.1
61.2
3.6
211.0
40.4 1243.6
*
*

Weak EDGAC*
Time(s) Nodes
 0.1
 17.0
 0.1
 45.2
 1.0
 82.2
 13.4
 318.4
 285.2
 3700.4

GCCval

GAC*
Time(s)
Nodes
 0.1
131.8
10.0
4818.2
1407.4 357529.8
*
*
*
*

Table 5: The time (in seconds) and the number of nodes in solving the Latin Square instances using
soft GCC constraints

L ATIN S QUARES
We first focus on the Latin Square problem, which is described in Section 5.2.1. We use the same
soft version but we replace SOFT ALL D IFFERENT by either SOFT GCCvar () or SOFT GCCval ()
cost functions which measure the violation differently. The results are shown in Table 5, which
shows a similar result as Table 3. Weak EDGAC* always performs the best in terms of time and
reduction in search space.
ROUND ROBIN T OURNAMENT
The round robin problem problem (prob026 in CSPLib) of order n is to schedule a tournament of n
teams over n  1 weeks. Each week is divided into n/2 periods, and each period is divided into two
slots. A tournament must satisfy the following three constraints: (1) every team plays at least once a
week, (2) every team plays at most twice in the same period over the tournament, and (3) every team
plays every other team. Van Hentenryck, Michel, Perron, and Regin (1999) give a CSP model only
based on GCC constraints: a triple of variables (sij , tij , mij ) represents the match played on the ith
week at the j th period. The assignment {sij 7 a, tij 7 b, mij 7 ab} represents team a is played
against the team b. Ternary constraints link sij , tij and mij together such that sij takes the value a
285

fiL EE & L EUNG

(a)
(N, P, M )
(4,3,2)
(5,4,2)
(6,5,3)
(7,5,3)

Reified Approach
Time(s) Nodes
1.7 1119.2
4.5 2016.6
*
*
*
*

Strong IC
Time(s) Nodes
0.6
827.4
2.2 1242.0
*
*
*
*

(b)
(N, P, M )
(4,3,2)
(5,4,2)
(6,5,3)
(7,5,3)

Reified Approach
Time(s) Nodes
1.5 1046.8
3.5 1821.4
*
*
*
*

SOFT

GAC*
Time(s) Nodes
0.4 470.2
1.8 836.2
*
*
*
*

SOFT

Strong IC
Time(s) Nodes
0.4 794.6
0.6 171.0
*
*
*
*

GCCvar
FDGAC*
Time(s) Nodes
0.2 142.2
0.6 171.6
*
*
*
*

Weak EDGAC*
Time(s) Nodes
 0.1
 33.4
 0.1
 44.6
 583.4
 6508.8
 1283.4
 7476.6

FDGAC*
Time(s) Nodes
0.2 141.0
0.6 171.0
*
*
*
*

Weak EDGAC*
Time(s) Nodes
 0.1
 33.0
 0.1
 42.8
 438.2
 6499.6
 765.0
 7413.6

GCCval

GAC*
Time(s) Nodes
0.4 464.6
1.4 824.6
*
*
*
*

Table 6: The time (in seconds) and the number of nodes in solving the round robin tournament
problems using SOFT GCC cost functions

and tij takes the value b iff mij takes the value ab or ba. The first and the second requirements are
represented by the GCC constraints on {sij , tij | i = w} for each wth week and {sij , tij | j = p}
for each pth period. The third requirement is represented by a GCC constraint on {mij }.
The problem can be generalized by three parameters (N, P, M ): scheduling a tournament of
N teams over M weeks, with each week divided into P periods. Besides placing random unary
costs, we also replace the GCC constraints by the soft variants. We try different combinations of
N , P , and M . The results are shown in Table 6, which agrees with the theoretical strength of each
consistency. It also shows that although enforcing stronger consistency is more expensive, it helps
to reduce search space more. Thus, stronger consistency helps to solve larger instances.
5.2.3 B ENCHMARKS BASED

ON

S OFT S AME

The SAME() constraint can be used to model the following two problems: (1) fair scheduling, and
(2) people-mission scheduling.
FAIR S CHEDULING
The problem is suggested in the Global Constraint Catalog2 . The goal is to schedule n persons
into s shifts over d days such that the schedule is fair, i.e. each person should be assigned to the
same number of the ith shift. For example, the schedule in Figure 8(a) is not fair. The person p1 is
assigned to the AM shift two times but p2 is assigned to the AM shift once only. Figure 8(b) shows
a schedule that is fair to everyone: both p1 and p2 are assigned to the AM shift and Overnight shift
once, and the PM shift twice.
We model and soften the problem by a set of variables {xij }, which denote the shift assigned
to the ith person on the j th day with random unary costs. The SOFT SAMEvar ({xp1 j }, {xp2 j }) cost
functions are placed between each pair of persons p1 and p2 , allowing violation for the fairness of
the schedule to obtain minimum cost. We fix s = 4 and d = 5 and vary n. The results are shown
in Table 7. Similarly to Table 5, weak EDGAC* produces the smallest number of nodes. However,
2. http://www.emn.fr/x-info/sdemasse/gccat/

286

fiC ONSISTENCY T ECHNIQUES

Day 1
AM
AM

p1
p2

Day 2
PM
PM

Day 3
PM
Overnight

FOR

S OFT G LOBAL C OST F UNCTIONS

Day 4
AM
PM

p1
p2

Day 1
AM
AM

(a) Unfair Schedule

Day 2
PM
PM

IN

WCSP S

Day 3
PM
Overnight

Day 4
Overnight
PM

(b) Fair Schedule

Figure 8: Examples of Fair Scheduling
n
5
6
7
8
9
10
11

Reified Approach
Time(s)
Nodes
1983.9 1457812.6
*
*
*
*
*
*
*
*
*
*
*
*

Strong IC
Time(s)
Nodes
74.2
20610.4
1884.0 1038613.2
*
*
*
*
*
*
*
*
*
*

GAC*
Time(s)
Nodes
16.6
3511.8
78.8
11031.8
377.0
36063.0
1630.0 124920.8
*
*
*
*
*
*

FDGAC*
Time(s) Nodes
 0.1
27.4
 0.4
40.4
 1.0
45.0
 2.0
45.4
 2.6
 49.0
 4.0
58.0
 5.8
67.2

Weak EDGAC*
Time(s) Nodes
 0.1
 25.4
 34.0
1.0
 40.6
1.2
 45.0
2.2
 49.0
3.2
 56.8
4.6
 61.6
6.4

Table 7: The time (in seconds) and the number of nodes in solving the fair scheduling problem by
enforcing different consistency notions.

weak EDGAC* requires more time to solve than FDGAC*. We look into the execution and discover
that FDGAC* is so strong that the first lower bound computed is already very close, if not identical,
to the objective value of the optimal solution. Therefore, enforcing weak EDGAC* gives only little
improvement on reducing the search space.
P EOPLE -M ISSION S CHEDULING
This problem extends the doctor-nurse rostering problem described by Beldiceanu, Katriel and
Thiel (2004). Given three groups of n persons, m missions must be assigned to a team containing exactly one person in each group. We are also given a set of constraints restricting the combination of each team in one mission. The problem is to schedule those people into teams for
missions such that no restriction is violated. We model the problem by {xij } denoting the mission
assigned to the ith person in the j th group with random unary costs. The combination restriction
is softened as ternary cost functions. Two global cost functions SOFT SAMEvar ({xi1 }, {xi2 }) and
SOFT SAMEvar ({xi2 }, {xi3 }) are posted to ensure each team exactly contains one person from each
group. We fix m = 6 and vary n. The results are shown in Table 8. Similarly to Table 7, weak
EDGAC* produces the smallest number of nodes, but requires more time than FDGAC*.
5.2.4 B ENCHMARKS BASED

ON

S OFT R EGULAR

The REGULAR() constraint has many applications. In the following, we focus on two: (1) the nurse
rostering problem, and; (2) the STRETCH() constraint modelling.
N URSE ROSTERING P ROBLEM
The nurse rostering problem (Cheng, Lee, & Wu, 1997) is to schedule a group of n nurses into four
shifts, PM shift, AM shift, Overnight, and Day-Off, over a period with most requirements satisfied.
287

fiL EE & L EUNG

n
4
5
6
7

Reified Approach
Time(s)
Nodes
17.5
16992.0
427.8 283950.2
*
*
*
*

Strong IC
Time(s)
Nodes
4.0
5931.6
45.2
51029.8
666.6 553001.2
*
*

GAC*
Time(s) Nodes
1.6
1517.4
11.2
7073.8
156.6 75481.6
*
*

FDGAC*
Time(s)
Nodes
 0.2
247.8
 3.4
831.2
 55.6
11065.2
 1348.0
333937.6

weak EDGAC*
Time(s)
Nodes
 238.8
0.4
 3.4
 693.4
 10957.8
69.2
1714.0  296019.2

Table 8: The time (in seconds) and the number of nodes in solving the people-mission scheduling
problem by enforcing different consistency notions.

(a)
n
3
4
5
6
7
8

Reified Approach
Time(s) Nodes
260.66 118562
*
*
*
*
*
*
*
*
*
*

Strong IC
Time(s) Nodes
152.6 91661.4
*
*
*
*
*
*
*
*
*
*

(b)
n
3
4
5

Reified Approach
Time(s)
Nodes
286.6 122542.4
*
*
*
*

Strong IC
Time(s) Nodes
178.4 91933.8
*
*
*
*

SOFT REGULAR

var

()

GAC*
Time(s) Nodes
2.0
956.2
25.4 6983.4
*
*
*
*
*
*
*
*
SOFT REGULAR

edit

GAC*
Time(s) Nodes
9.2
2850.4
126.2 27267.6
*
*

FDGAC*
Time(s) Nodes
 0.1
28.6
 0.1
32.6
4.0
379.0
63.4
4017.6
207.6 12242.0
821.2 44414.0

weak EDGAC*
Time(s)
Nodes
 0.1
 22.8
 0.1
 28.0
 3.6
 273.6
 37.8
 1927.2
 42.8
 2167.6
 229.2
 10437.0

()
FDGAC*
Time(s) Nodes
 5.6
841.4
 25.4
2568.8
 535.6
47091.2

weak EDGAC*
Time(s)
Nodes
 803.2
6.2

27.6
2424.0
546.8  40244.0

Table 9: The time (in seconds) and the number of nodes in solving the nurse scheduling problem
by enforcing different consistency notions.

In the experiment, the nurses are scheduled over four days such that (1) each nurse must have at most
three AM shifts, at least two PM shifts, at least one Overnight, and at least one day-off; (2) each AM
shift must have two nurses, each PM shift and each Overnight must have one nurse, and; (3) AMshifts are preferred to be packed together, and the same preference is also posted on Day-Offs. We
model this problem by a set of variables {xij } to denote the shift assigned to the ith nurse on the j th
day with random unary costs. Restrictions (1) and (2) are modeled by SOFT GCCval cost functions,
and (3) is modeled by either SOFT REGULARvar or SOFT REGULARedit cost functions. All restrictions are allowed to be violated. The results are shown in Table 9. When SOFT REGULARedit () is
used, FDGAC* wins in term of runtime. However, if SOFT REGULARvar () is used, weak EDGAC*
again requires the least time and the least number of nodes to solve.
M ODELLING

THE

S TRETCH () C ONSTRAINT

Another application of the REGULAR() constraint is to model constraints that describe patterns. One
example is the STRETCH() constraint.
288

fiC ONSISTENCY T ECHNIQUES

n
30
35
40
45
50
55

Reified Approach
Time(s) Nodes
183.5
7346.2
419.4 13845.2
842.4 23485.0
2318.2 55976.0
*
*
*
*

FOR

(a) SOFT REGULARvar()
Strong IC
GAC*
Time(s)
Nodes
Time(s) Nodes
68.2
5203.2
36.4
573.0
162.2
10297.8
80.6
971.6
335.6
18067.2
148.4
1423.2
900.4
42007.0
378.2
3042.0
1142.2
88616.8
165.8 10762.2
2231.4 146901.6
306.0 17130.0
(b)

n
30
35
40
45
50
55

Reified Approach
Time(s) Nodes
216.2
6038.6
561.6 12487.6
1128.1 20585.8
*
*
*
*
*
*

S OFT G LOBAL C OST F UNCTIONS

SOFT REGULAR

Strong IC
Time(s) Nodes
83.2
3861.6
204.2
7626.0
413.0 12789.6
1151.8 30480.6
2122.8 62225.2
*
*

edit

IN

WCSP S

FDGAC*
Time(s) Nodes
 30.0
171.4
 57.6
239.8
 92.2
328.6
 240.6
651.8
130.2 1660.6
208.0 2291.8

weak EDGAC*
Time(s) Nodes
 162.6
35.2
 233.4
69.0
 316.0
108.2
 570.6
246.4
 118.2
 1316.0
 193.8
 1856.8

FDGAC*
Time(s) Nodes
 34.2
123.8
 60.6
164.0
 90.8
208.4
239.6 371.0
204.8 967.6
264.2 972.8

weak EDGAC*
Time(s) Nodes
39.6  122.4
70.8  162.8
101.6  194.0
 207.8
 299.6
 185.0
 823.2
 234.6
 777.6

()

GAC*
Time(s) Nodes
40.6
447.4
86.8
706.0
165.8
1080.0
446.4
2346.2
348.6
9189.0
623.8 13496.8

Table 10: The time (in seconds) and the number of nodes in solving the sliding problem by enforcing different consistency notions.

Definition 24 (Pesant, 2001) Given a value v and a tuple   L(S). A v-stretch is the maximal
subsequence of identical values v in . The STRETCH(S, ub, lb) constraint is satisfied by  if the
length of the v-stretch in  is at most ubv and at least lbv .
For simplicity, we omit the case when the STRETCH() constraint is circular. However, it can be
handled by variable duplication (Pesant, 2004).
The STRETCH() constraint can be described by an automaton and thus modelled using the
REGULAR () constraint (Pesant, 2004). The SOFT REGULAR var () and SOFT REGULAR edit () cost
functions can be directly applied to define two soft variants of the STRETCH() constraint, namely
SOFT STRETCH var () and SOFT STRETCH edit (). They are flow-based projection-safe by inheriting
the same property from SOFT REGULARvar () and SOFT REGULARedit () respectively.
To demonstrate the idea, we conduct experiments using the following sliding problem. The
sliding problem of order n consists a set of variables {x1 , . . . , xn } with domains D(xi ) = {a, b}
and random unary costs. Each subsequence {xi , . . . , xn5+i }, where 1  i  5, is required to
contain a-stretches of length 2 and b-stretches of length 2 or 3. This restriction can be enforced
through STRETCH constraints. We allow violations by modeling the constraints using either the
SOFT REGULAR var or SOFT REGULAR edit cost functions. The results are shown in Table 10. Weak
EDGAC* needs more time than FDGAC* when the instances are small, but weak EDGAC* pays
off for large instances. This experiment also shows that the STRETCH constraint, an important
constraint for modeling patterns, can be efficiently propagated in the WCSP framework.
5.2.5 D ISCUSSIONS
A control comparison should have been conducted to examine the efficiency of ToulBar2 on the
global cost functions encoded explicitly as tables as well. This cannot be done in a meaningful
manner since the tables will be prohibitively large. Consider a simple cost function on 10 variables,
289

fiL EE & L EUNG

each with a domain size of 10. The table already requires storage in the order of 1010 integers or
tens of gigabytes.
Based on our experiments, two conclusions can be made. First, the experiments show that the
reified approach and strong IC are too weak both in terms of search space pruning and runtime
reduction as compared to GAC*, FDGAC*, and weak EDGAC*. Second, the stronger consistency
notions, weak EDGAC*, FDGAC* and GAC*, are worthwhile although they are more expensive
to enforce. As shown from the experiments, GAC* reduces the number of search nodes at least
3 times more than the reified approach and 1.5 times more than strong IC. GAC* has runtime
at least 4 times less than the reified approach and 1.5 times less strong IC. Weak EDGAC* and
FDGAC* can reduce the search space by a much greater extent. Such additional pruning can usually
compensate for the extra effort. Although Table 7 and Table 8 have shown cases where weak
EDGAC* results in slower runtime, FDGAC* only wins by a small margin. In general, weak
EDGAC* is still worthwhile to enforce. Table 10 further confirms that a stronger consistency is
more desirable as the problem becomes large.

6. Conclusion and Remarks
In this section, we summarize our contributions and shed light on possible future directions of
research.
Our contributions are five-fold. First, we introduce strong IC based on IC (Zytnicki et al.,
2009) and give an algorithm to enforce strong IC. Besides, we prove that strong IC is confluent.
We also show that enforcing strong IC on a WCSP is stronger than GAC in the reified approach.
Second, we give an algorithm to enforce GAC* for a WCSP, but enforcement is exponential. For efficient enforcement, we introduce flow-based projection-safety, which preserves the basic structure
of global cost functions. We give sufficient conditions for a global cost function to be flow-based
projection-safe. We also show as a part of the proof how projection and extension can be done
so that the flow property is preserved. Third, we generalize FDAC* (Larrosa & Schiex, 2003) to
FDGAC* and give an enforcement algorithm. Again, flow-based projection-safety helps FDGAC*
enforcement. Fourth, we attempt to generalize EDAC* using similar methods, but find it to be nontrivial. We discover and give an example of a limitation of EDAC*. When cost functions share
more than one variable, oscillation similar to the one demonstrated in Full AC* (de Givry et al.,
2005) will occur. To solve this problem, we introduce cost-providing partitions, which restrict the
distribution of costs when enforcing EDAC*. Based on cost-providing partitions, we define weak
EDGAC*, which can be enforced in polynomial time for flow-based projection-safe global cost
functions. Last but not least, we show that soft versions of ALL D IFFERENT(), GCC(), SAME() and
REGULAR () are flow-based projection-safe. We also prove the practicality of our framework with
empirical results on various benchmarks involving these global cost functions. The empirical results
agree with the theoretical strength of the consistencies in terms of search tree pruning. The results
also show that stronger consistency notions like weak EDGAC* and FDGAC* are more worthwhile
to enforce, especially when solving large problems.
Three directions of future work are possible. The first one is to investigate if other even stronger
consistency notions, such as VAC (Cooper et al., 2010), can also benefit from projection-safety to
make their enforcement practical for global cost functions. Second, the current sufficient conditions
for flow-based projection-safety might still be overly restrictive. For example, the global cost function SOFT SEQUENCE (Maher, Narodytska, Quimper, & Walsh, 2008) does not satisfy the three
290

fiC ONSISTENCY T ECHNIQUES

FOR

S OFT G LOBAL C OST F UNCTIONS

IN

WCSP S

conditions. It is interesting to find out other possible definition of flow-based projection-safety,
which allow efficient projection and extension operations. Third, we only consider the minimum
cost flow computation for finding the minimum cost in a global cost function. It is interesting to
check if other approaches, such as mathematical programming, can be used to achieve the same
results.

Acknowledgments
Work described in this paper was generously supported by grants CUHK413808 and CUHK413710
from the Research Grants Council of Hong Kong SAR.

References
Beldiceanu, N. (2000). Global Constraints as Graph Properties on a Structured Network of Elementary Constraints of the Same Type. In Proceedings of CP00, pp. 5267.
Beldiceanu, N., Carlsson, M., & Petit, T. (2004). Deriving Filtering Algorithms from Constraint
Checkers. In Proceedings of CP04, pp. 107122.
Beldiceanu, N., Katriel, I., & Thiel, S. (2004). Filtering Algorithms for the Same Constraints. In
Proceedings of CPAIOR04, pp. 6579.
Cheng, B., Lee, J. H. M., & Wu, J. (1997). A Nurse Rostering System Using Constraint Programming and Redundant Modeling. IEEE Transactions on Information Technology in
Biomedicine, 1, 4454.
Cooper, M., de Givry, S., Sanchez, M., Schiex, T., Zytnicki, M., & Werner, T. (2010). Soft Arc
Consistency Revisited. Artificial Intelligence, 174, 449478.
Cooper, M., & Schiex, T. (2004). Arc Consistency for Soft Constraints. Artifical Intelligence, 154,
199227.
Cooper, M. C. (2005). High-Order Consistency in Valued Constraint Satisfaction. Constraints,
10(3), 283305.
de Givry, S., Heras, F., Zytnicki, M., & Larrosa, J. (2005). Existential Arc Consistency: Getting
Closer to Full Arc Consistency in Weighted CSPs. In Proceedings of IJCAI05, pp. 8489.
Demassey, S., Pesant, G., & Rousseau, L.-M. (2006). A Cost-Regular Based Hybrid Column Generation Approach. Constraints, 11, 315333.
Dijkstra, E. W. (1959). A Note on Two Problems in Connexion with Graphs. Numerische Mathematik, 1, 269271.
Johnson, D. (1977). Efficient Algorithms for Shortest Paths in Sparse Networks. Journal of the
ACM, 24(1), 113.
Larrosa, J., & Schiex, T. (2003). In the Quest of the Best Form of Local Consistency for Weighted
CSP. In Proceedings of IJCAI03, pp. 239244.
Larrosa, J., & Schiex, T. (2004). Solving Weighted CSP by Maintaining Arc Consistency. Artificial
Intelligence, 159(1-2), 126.
Lauriere, J.-L. (1978). A Language and a Program for Stating and Solving Combinatorial Problems.
Artificial Intelligence, 10, 29127.
291

fiL EE & L EUNG

Lawler, E. (1976). Combinatorial Optimization: Networks and Matroids. Holt, Rinehart and Winston.
Leung, K. L. (2009). Soft Global Constraints in Constraint Optimization and Weighted Constraint
Satisfaction. Masters thesis, The Chinese University of Hong Kong.
Maher, M., Narodytska, N., Quimper, C.-G., & Walsh, T. (2008). Flow-Based Propagators for the
SEQUENCE and Related Global Constraints. In Proceedings of CP08, pp. 159174.
Pesant, G. (2001). A Filtering Algorithm for the Stretch Constraint. In Proceedings of CP01, pp.
183195.
Pesant, G. (2004). A Regular Language Membership Constraint for Finite Sequences of Variables.
In Proceedings of CP04, pp. 482495.
Petit, T., Regin, J.-C., & Bessiere, C. (2000). Meta-constraints on Violations for Over Constrained
Problems. In Proceedings of ICTAI00, pp. 358365.
Petit, T., Regin, J.-C., & Bessiere, C. (2001). Specific Filtering Algorithm for Over-Constrained
Problems. In Proceedings of CP01, pp. 451463.
Regin, J.-C. (1996). Generalized Arc Consistency for Global Cardinality Constraints. In Proceedings of AAAI96, pp. 209215.
Regin, J.-C. (2002). Cost-Based Arc Consistency for Global Cardinality Constraints. Constraints,
7, 387405.
Sanchez, M., de Givry, S., & Schiex, T. (2008). Mendelian Error Detection in Complex Pedigrees
using Weighted Constraint Satisfaction Techniques. Constraints, 13(1), 130154.
Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued Constraint Satisfaction Problems: Hard and
Easy Problems. In Proceedings of IJCAI95, pp. 631637.
Van Hentenryck, P., Michel, L., Perron, L., & Regin, J.-C. (1999). Constraint Programming in OPL.
In Proceedings of the International Conference on the Principles and Practice of Declarative
Programming, pp. 98116.
van Hoeve, W.-J., Pesant, G., & Rousseau, L.-M. (2006). On Global Warming: Flow-based Soft
Global Constraints. J. Heuristics, 12(4-5), 347373.
Zytnicki, M., Gaspin, C., & Schiex, T. (2009). Bounds Arc Consistency for Weighted CSPs. Journal
of Artificial Intelligence Research, 35, 593621.

292

fiJournal of Artificial Intelligence Research 43 (2012) 477522

Submitted 12/11; published 03/12

Proximity-Based Non-uniform Abstractions
for Approximate Planning
Jir Baum
Ann E. Nicholson
Trevor I. Dix

Jiri@baum.com.au
Ann.Nicholson@monash.edu
Trevor.Dix@monash.edu

Faculty of Information Technology
Monash University, Clayton, Victoria, Australia

Abstract
In a deterministic world, a planning agent can be certain of the consequences of its
planned sequence of actions. Not so, however, in dynamic, stochastic domains where
Markov decision processes are commonly used. Unfortunately these suffer from the curse of
dimensionality: if the state space is a Cartesian product of many small sets (dimensions),
planning is exponential in the number of those dimensions.
Our new technique exploits the intuitive strategy of selectively ignoring various dimensions in different parts of the state space. The resulting non-uniformity has strong
implications, since the approximation is no longer Markovian, requiring the use of a modified planner. We also use a spatial and temporal proximity measure, which responds to
continued planning as well as movement of the agent through the state space, to dynamically adapt the abstraction as planning progresses.
We present qualitative and quantitative results across a range of experimental domains
showing that an agent exploiting this novel approximation method successfully finds solutions to the planning problem using much less than the full state space. We assess and
analyse the features of domains which our method can exploit.

1. Introduction
In a deterministic world where a planning agent can be certain of the consequences of its
actions, it can plan a sequence of actions, knowing that their execution will necessarily
achieve its goals. This assumption is not appropriate for flexible, multi-purpose robots and
other intelligent software agents which need to be able to plan in the dynamic, stochastic
domains in which they will operate, where the outcome of taking an action is uncertain.
For small to medium-sized stochastic domains, the theory of Markov decision processes
provides algorithms for generating the optimal plan (Bellman, 1957; Howard, 1960; Puterman & Shin, 1978). This plan takes into account uncertainty about the outcome of taking
an action, which is specified as a distribution over the possible outcomes. For flexibility,
there is a reward function rather than a simple goal, so that the relative desirability or
otherwise of each situation can be specified.
However, as the domain becomes larger, these algorithms become intractable and approximate solutions become necessary (for instance Drummond & Bresina, 1990; Dean,
Kaelbling, Kirman, & Nicholson, 1995; Kim, 2001; Steinkraus, 2005). In particular where
the state space is expressed in terms of dimensions, or as a Cartesian product of sets, its
size and the resulting computational cost is exponential in the number of dimensions. On
c
2012
AI Access Foundation. All rights reserved.

fiBaum, Nicholson & Dix

the other hand, fortunately, this results in a fairly structured state-space where effective
approximations often should be possible.
Our solution is based on selectively ignoring some of the dimensions, in some parts of
the state space, some of the time. In other words, we obtain approximate solutions by
dynamically varying the level of abstraction in different parts of the state space. There
are two aspects to this approach. Firstly, the varying level of abstraction introduces some
artefacts, and the planning algorithm must be somewhat modified so as to eliminate these.
Secondly, more interestingly, an appropriate abstraction must be selected and later modified
as planning and action progress.
Our work is an extension and synthesis of two existing approaches to approximate planning: the locality-based approximation of envelope methods (Dean et al., 1995) and the
structure-based approximation of uniform abstraction (Nicholson & Kaelbling, 1994; Dearden & Boutilier, 1997). Our work extends both of these by exploiting both structure and
locality, broadening the scope of problems that can be contemplated. Baum and Nicholson
(1998) introduced the main concepts while full details of our algorithms and experimental
results are presented in Baums (2006) thesis. There have been some studies of arbitrary
abstraction, for instance by Bertsekas and Tsitsiklis (1996). However, these are generally
theoretical and in any case they tended to treat the approximation as Markovian, which
would have resulted in unacceptable performance in practice. We improve on this by extending the planning algorithm to deal with the non-Markovian aspects of the approximation.
Finally, we use a measure of locality, introduced by Baum and Nicholson (1998), that is
similar to but more flexible than the influence measure of Munos and Moore (1999).
We assume that the agent continues to improve its plan while it is acting and that
planning failures are generally not fatal. We also deal with control error exclusively. Sensor
error is not considered and it is assumed that the agent can accurately discern the current
world state (fully observable), and that it accurately knows the state space, the goal or
reward function, and a distribution over the effect of its actions (no learning).
The remainder of this paper is organised as follows. Section 2 reviews the background,
introduces our abstraction and provides our framework. Section 3 discusses planning under
a static non-uniform abstraction, and Section 4 presents our method for initially selecting
the non-uniform abstraction based on the problem description. Section 5 presents a method
of changing the abstraction based on the policy planned, while Sections 6 and 7 introduce
a proximity measure and a method of varying the abstraction based on that measure,
respectively. Section 8 presents results based both on direct evaluation of the calculated
policy and on simulation. Finally, Section 9 discusses the results and Section 10 gives our
conclusions and outlines possible directions for future work.

2. Planning under Non-uniform Abstractions
In a non-deterministic world where a planning agent cannot be certain of the consequences
of its actions except as probabilities, it cannot plan a simple sequence of actions to achieve
its goals. To be able to plan in such dynamic, stochastic domains, it must use a more
sophisticated approach. Markov decision processes are an appropriate and commonly used
representation for this sort of planning problem.
478

fiProximity-Based Non-uniform Abstractions for Planning

2.1 Illustrative Problems
To aid in exposition, we present two example problems here. The full set of experimental
domains is presented in Section 8.1.
The two illustrative problems are both from a grid navigation domain, shown in Figure 1.
They both have integer x and y coordinates from 0 to 9, three doors which can be either
open or closed and a damage indication which can be either yes or no. The agent can
move in one of the four cardinal directions, open a door if it is next to it, or do nothing.
The doors are fairly difficult to open, with probability of success 10% per time step, while
moving has an 80% chance of success, with no effect in the case of failure. Running into
a wall or into a closed door causes damage, which cannot be repaired. The transitions are
shown in Table 1. The agent starts in the location marked s0 in Figure 1 with the doors
closed and no damage, and the goal is to reach the location marked  with no damage.
x=0
y=0

1

2

3

4

5

6

7



s0

8

9

k3

1

x=0
y=0

2

3

4

5

6

7

s0

8

9

k3

1



2

d1

d2

k1

4

4

5

5

6

6



8





d1
3

7



2

3

9

1



d2



k1





7
8

k2

d3

9

(a)

k2

d3
(b)

Figure 1: The layout of the grid navigation domain. The blue arrows show the optimal
path (a) and a suboptimal path (b) through the 3Keys problem. The 3Doors
problem has the same grid layout (walls and doors) but with no keys.

The 3Keys problem also contains keys, which are required to open the doors. The agent
may have any one or more of them at any time. An additional action allows the agent to
pick up each key in the location as shown in the figure, and the open action requires the
corresponding key to be effective (there is no separate unlock action). The 3Doors problem
contains no keys  the doors are unlocked but closed  and therefore no corresponding
keys or pickup action.
The optimal policy obtained by exact planning for the 3Doors problem simply takes
the shortest path through door 2. For the 3Keys problem, the optimal plan is to collect
keys 3 and 1, pass south through door 1 and east through door 3, shown in Figure 1(a). A
suboptimal plan is shown in Figure 1(b).
479

fiBaum, Nicholson & Dix

x
Stay


y

pre-state
d1
d2
d3

post-state
d1
d2

dmg



x

y













3
3


y+1



















South
2
7




2
2
2
9
y

open






open






















80%
80%

North
2
7




3
3
0
3
y

open






open






















80%
80%

East
4
4
4
4
4
9
x

0
1
2
9























open




















80%
80%
80%
80%

West
5
5
5
5
5
0
x

0
1
2
9























open




















Open
2
2
7
7
4
5


2
3
2
3
9
9










































80%

80%

80%
80%
80%
80%
80%

80%
10%
10%
10%
10%
10%
10%

d3

dmg



























yes
yes


2
2


y1





















yes
yes


5
5
5
5


x+1





































yes
yes


4
4
4
4


x1





































yes
yes


















open
open








open
open








open
open








yes

Table 1: Transitions in the 3Doors problem, showing important and changed dimensions
only. First matching transition is used. Where a percentage is shown, the given
post-state will occur with that probability, otherwise the state is unchanged. Transitions without percentages are deterministic.

480

fiProximity-Based Non-uniform Abstractions for Planning

2.2 Exact Planning
One approach to exact planning in stochastic
domains
involves using Markov Decision


ff
0
Processes (MDPs). A MDP is a tuple S, A, T, R, s , where S is the state space, A is the
set of available actions, T is the transition function, R is the reward function and s0  S
is the initial state. The agent begins in state s0 . At each time step, the agent selects an
action a  A, which, together with the current state, applies T to obtain a distribution over
S. The current state at the next time-step is random according to this distribution and we
write PrT (s, a, s ) for the probability that action a taken in state s will result in the state
s at the next time-step. The agent is also given a reward at each time step, calculated by
R from the current state (and possibly also the action selected). The aim of the agent is
to maximise some cumulative function of these rewards, typically the expected discounted1
sum under a discounting factor . In a fully-observable MDP, the agent has full knowledge.
In particular, the agent is aware of T , R and the current state when selecting an action.
It is well-known that in a fully-observable MDP, the optimal solution can be expressed as
a policy  : S  A mapping from the current state to the optimum action. The planning
problem, then, is the calculation of this . As a side-effect of the calculation, the standard
algorithms also calculate a value function V : S  R, the expected discounted sum of
rewards starting from each state. Table 2 summarises the notation used in this paper.
There are well known iterative algorithms, Bellmans (1957) value iteration, Howards
(1960) policy iteration, and the modified policy iteration of Puterman and Shin (1978)
for computing the optimal policy   . However, as S becomes larger, the calculation of  
becomes more computationally expensive. This is particularly so if state space is structured
as a Cartesian product of dimensions, S = S1 S2   SD , because then |S| is exponential
in D. Since the algorithms explicitly store V and usually , which are functions of S, their
space complexity is therefore also exponential in D. Since they iterate over these arrays,
the time complexity is also at least exponential in D, even before any consideration of how
fast these iterative algorithms themselves converge. Typically, as D grows, planning in S
quickly becomes intractable. Since in practice the amount of computation allowed to the
agent is limited, this necessitates some approximations in the process.
In the 3Doors problem, there are six dimensions (two for the x and y coordinates, three
for the doors and one for damage), so that S = {0 . . . 9}  {0 . . . 9}  {open, closed} 
{open, closed}  {open, closed}  {damage, no damage} and |S| = 12 800. The action space
A is a set of five actions, A = {north, south, east, west, open}. The transition function
specifies the outcomes of taking each action in each state. The reward function R is 0 in
the agent is in the h7, 7i location (marked  in the diagram) with no damage, 1 if it is
in any other location with no damage, and 2 if there is damage. Finally, s0 is the state
where the agent is in the h0, 0i location, the doors are closed, and there is no damage.
Exact planning is listed in our results with |S| and V  (s0 ) for comparison. If there is
no approximation, the planner must consider the whole state space S. |S| is therefore a
measure of the cost of this planning  directly in terms of space and indirectly in terms
of time. On the other hand, since the planning is exact, the optimal value function V  will
1. While our illustrative problems have simple goals of achievement, we use time discounting in order to
remain general and for its mathematical convenience.

481

fiBaum, Nicholson & Dix

symbol
original
S
Sd

meaning

abstract
W  P(S)


state space (specific state space / worldview, resp.)
dimension d of the state space
D  N
number of dimensions
sS
wW
a state
0
s S

initial state
scur  S

current state (in on-line planning)
sd  Sd
wd  Sd
dimension d of the state s or w, resp.
A
set of actions (action space)
a0  A
default action
T
T
transition function (formal)
PrT :SAS[0, 1] PrT :WAW[0, 1] transition function (in use)
R:S R
R:WR
reward function (one-step reward)
V :SR
V :W R
value function (expected discounted sum of rewards)
  [0, 1)
discount factor for the reward
:SA
:WA
policy
 : S  A

optimal policy
V :S R

optimal value function (exact value function of  )
, i : S  A
, i : W  A
approximate policy, ith approximate policy

exact value function of  (note:  may be abstract)
V : S  R
V : S  R
V : W  R
approximate value function of  (approx. to V )
Table 2: Summary of notation. The first column is the notation for the original MDP, the
second is the notation once non-uniform abstraction has been applied.

be obtained, along with the optimal policy   ensuring that the agent will expect to obtain
that value. It is against these figures that all approximations must measure.
2.3 Uniform Abstraction
One method of approximation is to take advantage of these dimensions by ignoring some
of them  those that are irrelevant or only marginally relevant  in order to obtain an
approximate solution. It is uniform in the sense that the same dimensions are ignored
throughout the state space. Since this approach attacks the curse of dimensionality where
it originates, at the dimensions, it should be very effective at counteracting it.
Dearden and Boutilier use this to obtain an exact solution (Boutilier, 1997) or an approximate one (Boutilier & Dearden, 1996; Dearden & Boutilier, 1997). However, their abstractions are fixed throughout execution, and dimensions are also deleted from the problem
in a pre-determined sequence. This makes their approach somewhat inflexible. Similarly,
Nicholson and Kaelbling (1994) propose this technique for approximate planning. They
delete dimensions from the problem based on sensitivity analysis, and refine their abstraction as execution time permits, but it is still uniform. Dietterich (2000) uses this kind of
abstraction in combination with hierarchical planning to good effect: a subtask, such as
Navigate to location t, can ignore irrelevant dimensions, such as location of items to be
482

fiProximity-Based Non-uniform Abstractions for Planning

picked up or even the ultimate destination of the agent. Generally, any time the problem
description is derived from a more general source rather than specified for the particular
problem, uniform abstraction will help. Gardiol and Kaelbling (2008) use it where some dimensions are relevant, but only marginally, so that ignoring them results in an approximate
solution that can be improved as planning progresses.
Unfortunately, however, at least with human-specified problems, one would generally
expect all or most mentioned dimensions to be in some way relevant. Irrelevant dimensions
will be eliminated by the human designer in the natural course of specifying the problem.
Depending on the domain and the situation some marginally relevant dimensions might be
included, but often, these will not be nearly enough for effective approximation.
We do not list comparisons against uniform abstraction in our results for this reason 
in most of our sample domains, it makes little sense. All or almost all of the dimensions
are important to solving the problem. Where this is not the case and methods exist for
effective uniform abstraction, they can be integrated with our approach easily.
2.4 Non-uniform Abstraction
Our approximation, non-uniform abstraction, replaces the state space S with W, a particular type of partition of S, as originally introduced by Baum and Nicholson (1998). We call
W the worldview, so members of W are worldview states while members of S are specific
states.2 Non-uniform abstraction is based on the intuitive idea of ignoring some dimensions
in some parts of the state space. For example, a door is only of interest when the agent is
about to walk through it, and can be ignored in those parts of the state space which are
distant from the door. In a particular member of the worldview wi  W, each dimension is
either taken into account
(concrete, refined in), or ignored altogether (abstract,
Q completely
i and each w i is a singleton subset of the corresponding S
w
coarsened out). wi = D
d
d=1 d
d
for concrete dimensions and equal to Sd for abstract dimensions.3 It is up to the worldview
selection and modification methods to ensure that W remains a partition at all times.
To give an example, in the 3Doors problem one possible worldview has the location and
damage dimensions concrete in every state, while the door dimensions are each concrete
only in states within two steps of the respective door.
Note that the domain is still fully-observable. This is not a question of lack of knowledge
about the dimensions in question, but wilful conditional ignorance of them during planning
as a matter of computational expediency. The approximation also subsumes both exact
planning and uniform abstraction. For exact planning, all dimensions can be set uniformly
concrete, so that |W| = |S| and each worldview state corresponds to one specific state. For
uniform abstraction, the combination of abstract and concrete dimensions can be fixed for
the entire worldview. They can be treated as special cases of our more general approach.4
2. Previously, we used the word envelope for the same concept (Baum & Nicholson, 1998), however,
worldview better describes the approximation used than envelope.
3. We do not allow a dimension to be partially considered, we only abstract to the level of dimensions, not
within them. A dimension such as the x coordinate will either have a particular value, or it will be fully
abstract, but it will never be 59, for instance.
4. Our modified  calculation reduces to the standard algorithm for uniform or fully concrete worldviews,
so our planner obtains the standard results in these cases.

483

fiBaum, Nicholson & Dix

On the other hand, the approximation is no longer Markovian. A dimension that is
abstracted away is indeterminate. In the notation of Markov Decision Processes, this can
only be represented by some distribution over the concrete states, but the dimension is not
stochastic  it has some specific (but ignored) value. The distinction is important because
for a truly stochastic outcome, it can be quite valid to plan to retry some action until it
succeeds (for instance, opening a door in the 3Doors problem). For a dimension which is
merely ignored, the agent will obtain the same outcome (door is closed) each time it moves
into the region where the dimension is not ignored, so that within the worldview, previous
states can appear to matter. We discuss this further in Section 3.
2.5 Comparison to Other Approaches
Non-uniform abstractions began to appear in the literature at first usually as a side-effect
of a structured method, where the state space is represented as a decision tree based on
the individual dimensions, such as Boutilier, Dearden, and Goldszmidt (1995, 2000). Note,
however, that the decision tree structure imposes a restriction on the kinds of non-uniform
abstraction that can be represented: the dimension at the root of the tree is considered
throughout the state space, and so on. This is a significant restriction and results in a
representation much more limited than our representation. A similar restriction affects
de Alfaro and Roys (2007) magnifying-lens abstraction, with the refinement that multivalued dimensions are taken bit-by-bit and the bits interleaved, so that each level of the
decision tree halves the space along a different dimension in a pre-determined order. As they
note, this would work well where these dimensions correspond to a more-or-less connected
space, as in a gridworld, but it would do less well with features like the doors of our
grid navigation domain. Magnifying-lens abstraction calculates upper and lower bounds to
the value function, rather than a single approximation, which is an advantage for guiding
abstraction selection and allows for a definite termination condition (which we lack). On
the other hand, it always considers fully-concrete states in part of the algorithm, limiting
its space savings to the square root of the state space, whereas our algorithm can work
with a mixture of variously abstract states not necessarily including any fully concrete
ones. Another related approach is variable grids used for discretisation, which can be
indirectly used for some discrete domains, as Boutilier, Goldszmidt, and Sabata (1999) do,
if dimensions can be reasonably approximated as continuous (for instance money). Unlike
our approach, variable grids are completely inapplicable to predicates and other binary or
enumerated dimensions. Some, such as Reyes, Sucar, and Morales (2009), use techniques
in some ways quite similar to ours for continuous MDPs, though they are quite different in
other ways: they consider refinement only, not coarsening; they use sampling, rather than
directly dealing with the domain model; and they use a different refinement method, where
each refinement is evaluated after the fact and then either committed or rolled back.
Perhaps the most similar to our approach is one of the modules of Steinkraus (2005),
the ignore-state-variables module. However, the module appears to be completely manual,
requiring input of which variables (dimensions) should be ignored in what parts of the state
space. It also uses the values of the dimensions from the current state scur , rather than a
distribution, which obviously restricts the situations in which it may be used (for instance,
in the 3Doors problem, the doors could not be ignored in the starting state). Finally, since
484

fiProximity-Based Non-uniform Abstractions for Planning

Steinkraus (2005) does not analyse or report the relative contributions of the modules to
the solution, nor on the meta-planning problem of selecting and arranging the modules, it
is difficult to know to what extent this particular module is useful.
Other approaches take advantage of different features of different domains. For instance,
the factored MDP approach (used, for instance, by Boutilier et al., 2000, or Guestrin,
Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and
action spaces can be grouped together so that within each group those actions or action
dimensions affect the corresponding states or state dimensions but interaction between the
groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in
the form of algebraic decision diagrams to produce approximate solutions, while Sanner
and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a
domain, using symbolic dynamic programming, first-order algebraic decision diagrams and
linear value approximation, to pre-compute a generic solution which can then be used to
quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into
macro actions, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean,
and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach,
while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and
loops. Goldman, Musliner, Boddy, Durfee, and Wu (2007) reduce the state space while generating the (limited-horizon, undiscounted) MDP from a different, non-MDP representation
by only including reachable states, pruning those which can be detected as being clearly and
immediately poor, or inferior or equivalent to already-generated states. Naturally, many of
these approaches can be combined. For instance, Gardiol and Kaelbling (2004, 2008) combine state space abstraction with the envelope work of Dean et al. (1995), while Steinkraus
(2005) uses a modular planner with a view of combining as many approaches as may be
appropriate for a given problem. For more details and further approaches and variants we
refer the reader to a recent survey of the field by Daoui, Abbad, and Tkiouat (2010).
2.6 Dynamic Approximate Planning
The top-level algorithm is shown as Algorithm 1. After some initialisation, consisting of
selecting the initial abstraction and setting the policy, value and proximity to a0 , 0 and
proportionally to the size of each worldview state, respectively,5 the planner enters an
infinite loop in which it stochastically alternates among five possible calculations, each of
which is described in the following sections. Here and elsewhere in the algorithm, we use
stochastic choice as a default in the absence of a more directed method.
The agent is assumed to have processing power available while it is acting, so that it
can continually improve its policy, modify the approximation and updates the focus of its
planning based on the current state. This means that the agent does not need to plan so
well for unlikely possibilities, and can therefore expend more of its planning effort on the
most likely paths and on the closer future, expecting that when and if it reaches other parts
of the state space, it can improve the approximation as appropriate.
5. Initialising the approximate policy to action a0 constitutes a domain-specific heuristic  namely, that
there is a known default action a0 which is reasonably safe in all states, such as a do nothing action.

485

fiBaum, Nicholson & Dix

Algorithm 1 High-level algorithm for Approximate Planning with Dynamic Non-uniform
Abstractions
do select initial abstraction /* Algorithm 3 */
for all worldview states w do
(w)  a0 ; V (w)  0; P(w)  |w|
|S|
do policy and value calculation /* Algorithm 2 */
loop
choose stochastically do
do policy and value calculation /* Algorithm 2 */
or
do policy-based refinement /* Algorithm 4 */
or
do proximity calculation /* Algorithm 5 */
or
do proximity-based refinement /* Algorithm 6 */
or
do proximity-based coarsening /* Algorithm 7 */
input latest current state; output the policy

Actual execution of the policy is assumed to be in a separate thread (executive), so
that the planner does not have to concern itself with the timeliness requirements of the
domain: whenever an action needs to be taken, the executive simply uses the policy that it
most recently received from the planner.
Dean et al. (1995) call this recurrent deliberation, and use it with their locality-based
approximation. A similar architecture is used by the CIRCA system (Musliner, Durfee, &
Shin, 1995; Goldman, Musliner, Krebsbach, & Boddy, 1997) to guarantee hard deadlines.
In CIRCA terminology, the planner is the AIS (AI subsystem), and the executive is the
RTS (real-time subsystem).
An alternative to recurrent deliberation is pre-cursor deliberation, where the agent first
plans, and only when it has finished planning does it begin to act, making no further
adjustments to its plan or policy. Effectively, for the planner, the current state is constant
and equal to the initial state throughout planning. In this work the pre-cursor mode is used
for some of the measurements, as it involves fewer potentially confounding variables.
Conceptually, our approach can be divided into two broad parts: the open-ended problem of selecting a good abstraction and the relatively closed problem of planning within
that abstraction. Since the latter part is more closed, we deal with it first, in the next
section, covering Algorithm 2. We then explore the more open-ended part in Sections 57,
covering Algorithms 37.

3. Solving Non-uniformly Abstracted MDPs
Given a non-uniform abstraction, the simplest way to use it for planning is to take one
of the standard MDP algorithms, such as the modified policy iteration of Puterman and
Shin (1978), and adapt it to the non-uniform abstraction minimally. The formulae translate
486

fiProximity-Based Non-uniform Abstractions for Planning

directly in the obvious fashion.  becomes a function of worldview states instead of concrete
states, and so on, as shown in Algorithm 2 (using the simple variant for the update policy
for w procedure). Probabilities of transition from one worldview state to another are
approximated using a uniform distribution over the concrete states (or possibly some other
distribution, if more information is available).
Algorithm 2 policy and value calculation
repeat n times
for all worldview states w do
do update value for w
for all worldview states w do
do update policy for w
do update value for w
procedure update value for w
if PrT (w, (w), w) = 1 then
/* optimisation  V (w) can be calculated directly in this case */
V (w)  R(w)
1
else
P
V (w)  R(w) +  w PrT (w, (w), w )V (w )

procedure update policy
P for w variant simple
(w)  min arg maxa w PrT (w, a, w )V (w )

procedure update policy for w variant with Locally Uniform Abstraction
/* see Section 3.1 for discussion of Locally Uniform Abstraction */
absdims  {d : 
w a . PrT (w, a, w ) > 0  w is abstract in d}
w is abstract in d
d  absdims
LUA  w . w :
dimension d of w = dimension d of w d 
/ absdims
P
|w  w  |


V  w . w W |w | V (w )
P
(w)  min arg maxa w PrT (w, a, w )V (LUA(w ))

Note that in Algorithm 2, A is considered an ordered set with a0 as its smallest element
and the minimum is used when the arg max gives more than one possibility. This has
two aspects: (a) as a domain-specific heuristic, for instance, breaking ties in favour of
the default action when possible, and (b) to avoid policy-basedP
refinement (see Section 5)
based on actions that have equal value. Secondly, for efficiency, w can be calculated only
over states w with PrT (w, a, w ) > 0, since other states will make no contribution to the
sum. Finally, the number n is a tuning parameter which is not particularly critical (we use
n = 10).
Of course, replacing the state space S by a worldview W in this way does not, in general,
preserve the Markov property, since the actual dynamics may depend on aspects of the state
space that are abstracted in the worldview. In the simple variant we ignore this and assume
the Markov property anyway, on the grounds that this is, after all, an approximation.
Unfortunately, the resulting performance can have unacceptably large error, including the
outright non-attainment of goals.
487

fiBaum, Nicholson & Dix

For instance, in the 3Doors problem, such a situation will occur at each of the three
doors whenever they are all abstract at s0 and concrete near the door in question. The
doors are relatively difficult to open, with only a 10% probability of success per try. On
the other hand, when moving from an area where they are abstract to an area where they
are concrete, the assumed probability that the door is already open is 50%. When the
calculations are performed, it turns out to be preferable to plan a loop, repeatedly trying
for the illusory 50% chance of success rather than attempting to open the door at only
10% chance of success. The agent will never reach the goal. Worse still, in some ways, it
will estimate that the quality of the solution is quite good, V (s0 )  19.0, which is in
fact better even than the optimal solutions V  (s0 )  27.5, while the true quality of the
solution is very poor, V (s0 ) = 100 000, corresponding to never reaching the goal (but not
incurring damage, either; figures are for discounting factor  = 0.999 99).
Regions that take into account a particularly bad piece of information may seem unattractive, as described above, and vice versa. We call this problem the Ostrich effect, as
the agent is refusing to accept an unpleasant fact, like the mythical ostrich that buries its
head in the sand. Its solution, Locally Uniform Abstraction, is described in the next section.
If the abstracted approximation is simply treated as a MDP in which the agent does not
know which state it will reach (near closed door or near open door), it will not correspond
to the underlying process, which might reach a particular state deterministically (as it does
here). The problem is especially obvious in this example, when the planner plans a loop.
This is reminiscent of a problem noted by Cassandra, Kaelbling, and Kurien (1996), where
a plan derived from a POMDP failed  the actual robot got into a loop in a particular
situation when a sensor was completely reliable contrary to the model.
3.1 Locally Uniform Abstraction
The ostrich effect occurs when states of different abstraction are considered, for instance
one where a door is abstract and one where the same door is concrete and closed. The
solution is to make the abstraction locally uniform, and therefore locally Markovian for
the duration of the policy generation iterative step. By making the abstraction locally,
temporarily uniform, the iterative step of the policy generation algorithm never has to work
across the edge of an abstract region, and, since the same information is available in all the
states being considered at each point, there is no impetus for any of them to be favoured or
avoided on that basis (for instance, avoiding a state in which a door is concrete and closed
in favour of one where the door is abstract). The action chosen will be chosen based on the
information only and not on its presence or absence.
This is a modification to the update policy for w procedure of Algorithm 2: as
the states are considered one by one, the region around each state is accessed through a
function that returns a locally uniform version. States that are more concrete than the state
being considered will be averaged so as to ignore the distinctions. As different states are
considered, sometimes the states will be taken for themselves, sometimes their estimated
values V will be averaged with adjacent states. This means that some of the dimensions
will only partially be considered at those states  in most cases, this will mean that
the more concrete region must extend one step beyond the region in which the dimension
488

fiProximity-Based Non-uniform Abstractions for Planning

is immediately relevant. For a dimension to be fully considered at a state, the possible
outcomes of all actions at that state must also be concrete in that dimension.
The modified procedure proceeds as follows: first the dimensions that are abstract in
any possible outcome of the state being updated w are collected in the variable absdims.
Then the function LUA is constructed which takes worldview states w and returns potential
worldview states w which are like w but abstract in all the dimensions in absdims. As this
is the core of the modification, it is named LUA for Locally Uniform Abstraction. Since the
potential states returned by LUA are not, in general, members of W, and therefore do not
necessarily have a value stored in V , a further function V is constructed which calculates
weighted
averages of the value function V over potential states. As with the other sum,
P
can
be calculated only over states w with w  w 6=  for efficiency. Finally, the

w
update step is carried out using the two functions LUA and V .
Unfortunately, once the modification is applied, the algorithm may or may not converge depending on the worldview. Failure to converge occurs when the concrete region is
too small  in some cases, the algorithm will cycle between two policies (or conceivably
more) instead of converging. One must be careful, therefore, with the worldview, to avoid
these situations, or else to detect them and modify the worldview accordingly. The policybased worldview refinement algorithm described in Section 5 below ensures convergence in
practice.

4. Initial Abstraction
At the beginning of planning, the planner must select an initial abstraction. Since the
worldview is never completely discarded by the planner, an infelicity at this stage may
impair the entire planning process, as the worldview-improvement algorithms can only
make up for some amount the weakness here.
There are different ways to select the initial abstraction. We propose one heuristic
method for selecting the initial worldview based on the problem description, with some
variants. Consider for example that each door in the 3Doors problem is associated with
two locations, that is, those immediately on either side. It makes sense, then, to consider
the status of the door in those two locations. This association can be read off the problem
specification. Intuitively, the structure of the solution is likely to resemble the structure
of the problem. This incorporates the structure of the transition function into the initial
worldview. The reward function is also incorporated, reflecting the assumption that the
dimensions on which the reward is based will be important.
We use a two-step method to derive the initial worldview, as shown in Algorithm 3.
Firstly, the reward function is specified based on particular dimensions. We make those
dimensions concrete throughout the worldview, and leave all other dimensions abstract. In
the 3Doors problem, these are the x and y and dmg dimensions, so after this step there are
10  10  2 = 200 states in the worldview.
Secondly, the transition function is specified by decision trees, one per action. We use
these to find the nexuses between the dimensions, that is, linking points, those points at
which the dimensions interact. Each nexus corresponds to one path from the root of the
tree to a leaf. For example, in the 3Doors problem, the decision tree for the open action
contains a leaf whose ancestors are x, y, d1 and a stochastic node, with the choices leading
489

fiBaum, Nicholson & Dix

Algorithm 3 select initial abstraction
/* set the worldview completely abstract */
W  {S}
/* reward step */
if reward step enabled then
for all dimensions d mentioned in the reward tree do
refine the whole worldview in dimension d
/* nexus step */
if nexus step enabled then
for all leaf nodes in all action trees do
for all worldview states w matching the pre-state do
refine w in the dimensions mentioned in the pre-state

to that leaf being labelled respectively 4, 2, closed and 10%. This corresponds to a nexus at
sx = 4, sy = 2 and sd1 = closed (the stochastic node is ignored in determining the nexus).
In total, there are four nexuses on each side of each door, in the two locations immediately
adjacent, as shown in Figure 2(a), connecting the relevant door dimension to the x and
y coordinates. The initial worldview is shown in Figure 2(b), with x, y and dmg concrete
everywhere and the doors abstract except that each is concrete in the one location directly
on each side of the door, corresponding to the location of the nexuses on Figure 2(a). After
both steps, |W| = 212, compared to |S| = 1 600 specific states.

x=0

1

2

3

4

5

6

7

8

9

x=0

y=0

y=0

1

1

1

2

3

4

5

6

7

2





2

d1

d2

3





3

d1

d2

4

4

5

5

6

6

7

7

8

8

9

 

9

(a)

8

9

d3 d3
(b)

Figure 2: Nexus step of the initial abstraction, showing (a) the location of the nexuses in
the 3Doors problem (there are four nexuses at each ) and (b) the locations in
which the door dimensions will be concrete in the initial worldview.

490

fiProximity-Based Non-uniform Abstractions for Planning

For the 3Keys problem, the location of the nexuses is the same as in Figure 2(a), except
there are more nexuses in each location and some of them also involve the corresponding
key dimensions. Thus, in the initial worldview, the locations shown in Figure 2(b) will be
concrete not only in corresponding door dimension, but also, when they are closed, in the
corresponding key dimension. In the states in which the doors are open, the key dimension
remains abstract. The initial worldview size for 3Keys is |W| = 224.
Due to the locally-uniform abstraction, these concrete door dimensions will be taken
into account only to a very minimal degree. If the worldview were to be used without
further refinement, it is to be expected that the resulting policies would be very poor. The
results6 bear out this expectation. The worldview initialization methods therefore are not
intended to be used on their own, but rather as the basis for further refinement. Thus,
the real test of the methods is how well they will work when coupled with the worldview
modification methods, described below.

5. Policy-Based Refinement
This section presents the first of the worldview modification methods, policy-based refinement. This method modifies the worldview based directly on the current approximate
policy . In particular, it refines states based on differences between the actions planned at
adjacent, differently-abstract states. Where such differences indicate that a dimension may
be important, any adjacent states that are abstract in that dimension are refined (i.e. that
dimension is made concrete at those states).
The method was previously introduced by Baum and Nicholson (1998), who showed,
using a small navigation domain example (the 3Doors problem of this paper), that this
refinement method resulted in a good policy, though not optimal. Here we present quantitative results and consider more complex domains.
5.1 Motivation
The motivation for this method is twofold. Firstly, as already indicated, the method detects
areas in which a particular dimension is important, because it affects the action planned,
and ensures that it is concrete at adjacent states. Thus regions where a dimension is taken
into account will expand for as long as the dimension matters, and then stop. Secondly, the
method fulfils the requirements for choosing a worldview so as to avoid non-convergence in
the policy calculation, as mentioned in Section 3.1 above.
Dimensions are important where they affect the policy, since the policy is the planners
output. They are less important in parts of the state space where they do not affect
the policy. Thus, which dimensions need to be concrete and which can remain abstract
can be gleaned for each part of the state space by comparing the optimal actions for the
various states. Where the optimal actions are equal, states can be abstract, and where they
differ, states should be concrete. However, we do not have the optimal policy   . With an
approximate policy  on a worldview, it is more difficult. However, the planner can compare
policies in areas where a dimension is concrete, and if it is found to be important there,
expand the area in which it is concrete. As policy-based refinement and policy calculation
6. Omitted here as they uninteresting, but presented by Baum (2006).

491

fiBaum, Nicholson & Dix

alternate, refinement will continue until the area where the dimension is concrete covers the
whole region in which it is important.
Section 3.1 above noted that the planning algorithm requires a worldview chosen with
care. The algorithm described in this section detects situations that are potentially problematic under locally-uniform abstraction and modifies the worldview to preclude them.
Intuitively, the incorrect behaviour occurs where an edge of a concrete region intersects
with a place where there are two fairly-similarly valued courses of action, corresponding to
two different paths to the goal.
5.2 Method
The method uses the transition function as the definition of adjacent states, so that worldview states w and w are considered adjacent if a . PrT (w, a, w ) > 0. This definition is not
symmetrical in general, since the transition function is not, but that is not a problem for
this method, as can be seen below. The algorithm is shown as Algorithm 4.
Algorithm 4 policy-based refinement
candidates  
for all worldview states w do
for all actions a do
for all w : P r(w, a, w ) > 0 do

for all dimensions
 d : w is abstract in d and w is concrete in d do
w
is
abstract
in
d
construct w :
dimension d of w = dimension d of w d 6= d
a
b
a
if w , w . (w ) 6= (wb ) and wa  w 6=  and wb  w 6=  then
/* policy is not the same throughout w */
candidates  candidates  {(w, d)}
for all (w, d)  candidates do
if w  W then
/* replace w with
group of states concrete in d */
 anew
w
is concrete in d
do
for all wnew :
dimension d of wnew = dimension d of w d 6= d
W  W  {wnew }
new
(wnew )  (w); V (wnew )  V (w); P(wnew )  |w|w| | P(w)
W  W \ {w} /* discarding also the stored (w), V (w) and P(w) */

Example In the 3Doors problem, for instance, applying this method during planning
increases the number of worldview states from the initial 212 to 220231, depending on
the stochastic choices (recall that |S| = 1 600 for comparison). It produces concrete regions
which are nice and tight around the doors, as shown in Figure 3, while allowing the algorithm
to converge to a reasonable solution. The solution is in fact optimal for the given initial
state s0 , though that is simply a coincidence, since the s0 is not taken into account by the
algorithm and some other states have somewhat suboptimal actions (the agent would reach
the goal from these states, but not by the shortest route).
492

fiProximity-Based Non-uniform Abstractions for Planning

x=0

1

2

3

4

5

6

7

8

9

x=0

y=0

1

2

3

4

5

6

7

8

9

y=0
d

1

d1

d2

1

k2

2

d1 d1 d1

d2 d2 d2

2

3

d1 d1 d1 d1

d2

3

d1 k1 d1 d1

4

d1 d1

4

d1 d1 d1 d1 d1

5

5

d1 d1 d1

6

6

d1

7

7

d

d

k1
d

d

d

k2

d

8

d3

8

k3

9

d3 d3 d3

9

k3 k3 k3

(a)

d

k2 k2 k2

d

d

d

(b)

Figure 3: Example of non-uniform abstraction for the (a) 3Doors and (b) 3Keys problems
with policy-based refinement. The x, y and dmg dimensions are concrete everyd
where; d1, d2 and d3 indicate where the corresponding door is concrete; while k1,
d
d
k2 and k3 indicate that the corresponding door is concrete and the corresponding
key is also concrete if the door is closed.

The worldview obtained by this method is often quite compact. For instance, rather
than refining a simple 2  3 rectangular region on each side of a door in 3Doors, as a human
might, this algorithm makes only 4 locations concrete on the approach side of each door,
which is enough to obtain a good solution. This can be seen on the north sides of doors
d1 and d2, as well as the west side of door d3 (3 concrete locations, due to the edge). On
the departure side of doors d2 and d3, it is even better  it makes no refinement at all:
south of door d2 and east of door d3, the action is to move toward the goal, regardless of
the status of the door  the actions are equal, so no refinement takes place.
The south side of door d1 seems rather less compact. The concrete area is in fact not
very big  6 locations for 3Doors  but it seems excessive compared with the compact
concrete areas elsewhere. This can occur when there is a nexus close to a region where
the best action to take genuinely depends on the status of the dimension in the nexus,
but the difference is small. If somehow the agent found itself at h4, 3i  and policy-based
refinement is independent of scur  the optimal path genuinely would depend on whether
door d1 is open, the other path being slightly suboptimal in each case. While in theory
such a region could have arbitrarily large extent, it seems to be a relatively minor effect in
practice. Here, for instance, it adds a couple of states, which is about 1% of |W|, and it
was not found to be a real problem in any of the domains (or in the domains used in Baum,
2006).
493

fiBaum, Nicholson & Dix

5.3 Limitations
Policy-based refinement can only deal with cases where a single dimension makes a difference. When two dimensions are needed in combination, it will often miss them. For
instance, in the 3Keys problem each key is quite distant from the corresponding door and
policy-based refinement will therefore never find the relationship between the two. At the
key, there appears to be no reason to pick it up, while at the door there appears to be no
means of unlocking it.
Obviously, this can be fixed ad hoc by rewarding picking up keys for its own sake.
Indeed, some domain formulations in the literature do exactly that, rewarding the agent
for partial achievement of the goal. However, that is not a clean solution. In effect, such
domain specifications cheat by providing such hints.
Another problem is that policy-based refinement does not provide for coarsening the
worldview, or for modifying it in other ways, for instance as execution progresses and the
planner needs to update the plan. Indeed, policy-based refinement ignores the initial state
s0 altogether, or the current state scur in recurrent planning. Thus it produces the same
solution regardless of which part of the problem the agent has actually been asked to solve.
This is a waste of computation in solving those parts which the agent is unlikely to actually
visit, and  perhaps more importantly  carries the penalty of the corresponding loss of
quality in the relevant parts.
The following sections describe proximity-based worldview modification, which is needed
to solve domains where combinations of dimensions are important and which also makes
use of s0 or scur , as appropriate.

6. A Proximity Measure
In general, the worldview should be mostly concrete near the agent and its planned path
to the goal, to allow detailed planning, but mostly abstract elsewhere, to conserve computational resources. In this section we describe a measure (originally in Baum & Nicholson,
1998) which realises this concept, proximity P, which decreases both as a state is further
in the future and as it is less probable.7 This section extends the brief description of Baum
and Nicholson (1998). In the following section we then present new worldview modification
methods based directly on the measure.
6.1 Motivation
The proximity P is a realisation of the intuitive concept of states being near the agent and
likely to be visited, as opposed to those distant from the agent and unlikely. It naturally
takes into account the current state scur in recurrent planning, or the initial state s0 in
pre-cursor planning, unlike policy-based refinement which ignores them altogether. Thus a
planner selecting worldviews based on this proximity measure will produce solutions tailored
to the particular scur or s0 and will ignore parts of the MDP that are irrelevant or nearirrelevant to performance from that state. Thus it saves computation that would otherwise
7. Baum and Nicholson (1998) used the word likelihood for this measure. We now prefer proximity to
avoid confusion with the other meanings of the word likelihood. Munos and Moore (1999) use the word
influence for a somewhat similar measure in continuous domains.

494

fiProximity-Based Non-uniform Abstractions for Planning

be wasted in solving those parts which the agent is unlikely to actually visit, and  perhaps
more importantly  carries the advantage of the corresponding gain of quality in the
relevant parts. This allows the agent to deal with problems such as 3Keys which are beyond
the reach of policy-based refinement.
Implicitly, the agent plans that when and if it reaches those mostly-abstract parts of the
state space, it will improve the approximation as appropriate. The planner thus continually
improves the policy, modifies the approximation and updates the focus of its planning based
on the current state scur . This means refining the regions in which the agent finds itself or
which it is likely to visit, and coarsening away details from regions that it is no longer likely
to visit or those which it has already traversed.
There are three aspects to proximity: temporal, spatial and probabilistic. Firstly, the
temporal aspect indicates states that may be encountered in the near future, on an exponentially decaying scale. The second aspect is spatial  the nearness of states (in terms of
the state space) to the agent and its planned path. The spatial aspect is somewhat indirect,
because any spatial structure of the domain is represented only implicitly in the transition
matrix, but the proximity measure will reflect it. These two aspects are combined in the
proximity to give a single real number between 0 and 1 for each state, denoted P  P
for proximity,  for the spatial aspect and  for the temporal aspect. This number can be
interpreted as a probability  namely the probability of encountering a state  and P
can be interpreted as the probability distribution over states, giving the final, probabilistic
aspect of proximity.
6.2 Calculation
The formula for the proximity P is similar to the formula for the value function. There
are three differences. Firstly, instead of beginning from the reward function it is based on
an is current state function, cur. Secondly, the transition probabilities are time-reversed
(that is, the matrix is transposed). This is because the value calculation is based on the
reward function, which occurs in the future (after taking actions), while the is current state
function is based on the present, before taking actions. Since the order of taking actions and
the function upon which the formula is based is reversed in time, a similar reversal must
b is used
be applied to the transition probabilities. Thirdly, an estimated future policy 
b
b
instead of . In this estimate,  is a stochastic policy defined by making (s) a distribution
over actions which assigns some constant probability to the current (s) and distributes the
remaining probability mass among the other actions equally. This distributed probability
mass corresponds to the probability that the policy will change sometime in the future,
or, alternately, the probability that the currently-selected action is not yet correct. The
formula is therefore:
X

b  ), s)P(s )
Pr(s , (s
(1)
P(s)  cur(s) + P
T

s

where
and

P is the proximity discounting factor (0  P < 1)

1  P if scur = s
cur(s) =
0
otherwise
495

(2)

fiBaum, Nicholson & Dix

P
The constant 1P was chosen for the current-state function so that s P(s) converges
to 1, in other words so that P is a probability distribution. If checked in the near future,
the agent has a probability of P(s) of being in state s, assuming it will follow the policy 
and near future is defined so that the probability of checking at time t is proportional to
Pt (that is, with P interpreted as a stopping probability). As with the value calculation,
one can instead solve the set of linear equations
X

b  ), s)P(s )
Pr(s , (s
(3)
P(s) = cur(s) + P
s

T

or, in matrix notation,
(I  P TbT )P = cur

(4)

b and I is the identity
where Tb is the transition matrix induced by the stochastic policy 
matrix. The implementation uses this matrix form, as shown in Algorithm 5. The proximity
measure needs little adjustment to work with the non-uniformly abstract worldview: s is
simply replaced by w in (1) and (2), with scur = s becoming scur  w.
Algorithm 5 proximity calculation
solve this matrix equation for P as a linear system:
(I  P TbT )P = cur
The measure has two tuning parameters, the replanning probability and the discounting
factor P . The replanning probability controls the spatial aspect: it trades off focus on the
most likely path and planning for less likely eventualities nearby. Similarly, P controls the
temporal aspect: the smaller P is, the more short sighted and greedy the planning will
be. Conversely, if P is close to 1, the planner will spend time planning for the future that
might have been better spent planning for the here-and-now. This should be set depending
on the reward discounting factor , and on the mode of the planner. Here we use P = 0.95,
replanning probability 10%.
Example Proximities for the 3Doors problem are shown in Figure 4 for the initial situation (agent at h0, 0i, all doors closed) and a possible situation later in the execution
(agent at h4, 2i, all doors closed). Larger symbols correspond to higher proximity. One can
immediately see the agents planned path to the goal, as the large symbols correspond to
states the agent expects to visit. Conversely small proximities show locations that are not
on the agents planned path to the goal. For example, the agent does not expect to visit
any of the states in the south-western room, especially once it has already passed by door
1. Similarly, the proximities around the initial state are much lower when the agent is at
h4, 2i, as it does not expect to need to return.
6.3 Discussion
One interesting feature of the resulting numbers is that they emphasise absorbing and nearabsorbing states somewhat more than might be intuitively expected. However, considering
496

fiProximity-Based Non-uniform Abstractions for Planning

x=0

1

2

3

4

5

6

7

8

9

x=0

y=0

y=0

1

1

2

2

3

3

4

4

5

5

6

6

7

7

8

8

9

9

hx = 0, y = 0i

1

2

3

4

5

6

7

8

9

hx = 4, y = 2i

Figure 4: Proximities in the 3Doors problem for s0 and a possible later scur ; symbol size
logarithmic, proximities range from 237 to 21.4 ; P = 0.95, replanning probability 10%.

that absorbing states are in general important, this is a good feature, especially since
normally the planner will try to minimise the probability of entering an absorbing state
(unless it is the goal). This feature should help ensure that absorbing states are kept in
mind as long as there is any chance of falling into them. Dean et al. (1995), for instance,
note that in their algorithm such undesirable absorbing states along the path to the goal
tend to come up as candidates for removal from consideration (due to the low probability of
reaching them with the current policy), and have to make special accommodation for them
so they are not removed from consideration. With the proximity measure emphasising these
states, such special handling is not necessary.
In contrast with this approach, Kirman (1994) uses the probabilities after Es steps,
where Es is (an estimate of) the number of steps the agent will take before switching from
the previous policy to the policy currently being calculated. This assumes that Es can be
estimated well, that the current policy is the policy in the executive, and that the oneplanning-cycle probability is an appropriate measure. In fact one would prefer at least a
two-planning-cycle look-ahead, so that the agent not only begins within the area of focus of
the new policy, but also remains there throughout the validity of that policy, and probably
longer, since the planners foresight should extend beyond its next thinking cycle. More
philosophically, this reliance on the planning cycle length is not very desirable, as it is an
artefact of the planner rather than intrinsic in the domain.
A somewhat related approach is prioritised sweeping (see for instance Barto, Bradtke,
& Singh, 1995). Like the present approach, it defines a measure of states which are in
some way interesting. Unlike this approach, it then applies that measure to determine
497

fiBaum, Nicholson & Dix

the order in which the formulae of the  and V calculation are applied, so that they are
applied preferentially to the interesting states and less frequently to the uninteresting or
unimportant states. It is well-known that the order of calculation in the MDP planning
algorithms can be varied greatly without forfeiting convergence to the optimal policy, and
prioritised sweeping takes advantage of this. Often it is done on a measure such as change
in V in previous calculations, but some approaches use look-ahead from the current state,
which is in some ways a very simple version of proximity (in fact, it corresponds to a
threshold on P with replanning probability set to 1). The proximity measure P might well
be a good candidate for this approach: apply  and V calculation to states chosen directly
according to P as a distribution.8
Munos and Moore (1999) use an influence measure on their deterministic continuous
domains, which is very similar to P. In fact, the main difference is that their measure does
not have the two parameters  it re-uses the same  and has no replanning probability
(effectively it is zero). This means that it cannot take into account replanning, neither in
the difference in the horizon that it entails, nor in the possibility that the policy may change
before it is acted upon. Absorbing states, for instance, would not be emphasised as they
are with proximities.

7. Proximity-Based Dynamic Abstraction
The proximity measure described in the previous section is used to focus the planners attention to areas likely to be useful in the near future. Firstly, that means that the worldview
should be made to match the proximities, by refining and coarsening as appropriate. Secondly, since the proximity measure takes into account the current state, this method will
automatically update the worldview as the agents circumstances change in the recurrent
mode, that is, when planning and execution are concurrent.
7.1 Refinement
High proximity indicates states which the agent is likely to visit in the near future. The
planner should therefore plan for such states carefully. If they are abstract, this is reason
to refine them so as to allow more detailed planning. Such states with high proximity are
therefore considered candidates for refinement.
High proximity is defined by a simple threshold, as shown in Algorithm 6.
When refinement occurs, an anomaly sometimes appears. Like the anomaly which led to
the policy-based refinement method, it arises from different levels of abstraction, but here,
it is not an adjacent more abstract state that causes the problem, but rather a recentlyrefined one. When a state is refined, the values V of the new states are initially estimated
from the states previous value V . However, typically, this means that some of them will
be overestimated and others underestimated. When the policy is being re-calculated, the
state with the overestimated value will be attractive.
Since the problem directly follows from the moment of refinement, it is self-correcting.
After a few iterations, the planner converges to the correct policy and values. However,
8. If retaining the theoretical guarantee of convergence to   is desired, care would have to be taken since

P is zero for states which are not reachable from the current state. In practice, of course, optimality or
otherwise as to unreachable states is immaterial.

498

fiProximity-Based Non-uniform Abstractions for Planning

Algorithm 6 proximity-based refinement
stochastically choose a dimension d
for all worldview states w do
if P(w) > threshold and w is abstract in d then
/* replace w with
group of states concrete in d */
 anew
w
is concrete in d
new
for all w
:
do
dimension d of wnew = dimension d of w d 6= d
W  W  {wnew }
new
(wnew )  (w); V (wnew )  V (w); P(wnew )  |w|w| | P(w)
W  W \ {w} /* discarding also the stored (w), V (w) and P(w) */

while it is doing so, transient anomalies appear in the policy, and in the worst case, the
planner may replan for some other path, then refine more states and re-trigger the same
anomaly. Rather large parts of the state space can be spuriously refined in this way.
This occurs because of the combined  and V calculation phase, which may update
 before V has had a chance to converge. The solution is to create a variant phase, V
calculation only, which replaces the  and V calculation phase until the values stabilise.
We do this for two iterations, which appears to be sufficient. An alternative solution would
have been to copy the difference between values at adjacent, more concrete states where
possible, thus obtaining better estimated values at the newly-refined states. However, since
the simpler solution of V -only calculation works satisfactorily, this more complex possibility
has not been further explored.
7.2 Coarsening
Low proximity indicates states which the agent is unlikely to visit in the near future. The
planner therefore need not plan for such states carefully. Usually, they will already be
abstract, never having been refined in the first place. However, if they are concrete  if
they have been previously refined  this is reason to coarsen them so as to free up memory
and CPU time for more detailed planning elsewhere. Such states with low proximity are
therefore considered candidates for coarsening.
Proximity-based coarsening is useful primarily in an on-line planning scenario with recurrent planning. As the agent itself moves through the state space and the current state
scur changes, so do the states that are likely to be visited in the near future. This is especially useful if the agent finds itself in some unexpected part of the state space, for instance
due to low-probability outcomes, or if the agent has planned a path leading only part way
to the goal (perhaps up to a partial reward). In any case, however, the parts of the state
space already traversed can be coarsened in favour of refinement in front of the agent.9
One might also imagine that as planning progresses, the planner may wish to concentrate
on different parts of the state space and that coarsening might be useful to cull abandoned
explorations and switch focus. However, we have not observed this with any of our domains
9. States already traversed cannot be discarded, even if the agent will never visit them again, since the
worldview is a partition and since the agent does not necessarily know whether it will need to revisit (or
end up revisiting) those states.

499

fiBaum, Nicholson & Dix

and found that in pre-cursor mode, coarsening generally worsens the quality of the policies
with no positive contribution.
Coarsening proceeds in three steps, as shown in Algorithm 7. The first step is very
similar to proximity-based refinement: each time the proximity-based coarsening phase is
invoked, the worldview is scanned for states with low proximity (below a threshold), which
are put in a list of candidates. The second step is more tricky. Coarsening needs to join
several states into one. However, the representation does not allow arbitrary partitions
as worldviews and therefore does not allow the coarsening-together of an arbitrary set of
worldview states. The planner must therefore find a group of states among these lowproximity candidates which can be coarsened into a valid worldview state. Such groups can
be detected by the fact that they differ in one dimension only and have the same size as
the dimension, therefore covering it completely. Finally, the groups that have been found
are each replaced with a single more abstract state.
Algorithm 7 proximity-based coarsening
/* collect candidates for coarsening */
candidates  {w : P < threshold}
/* find groups of candidates that can be coarsened together */
/* partition candidates according to the pattern of abstract and concrete dimensions */
patterns  candidates / {(wa , wb ) : d . wa is concrete in d  wb is concrete in d}
groups  
for all p  patterns do
for all dimensions d do
if states in p are concrete in d then
/* partition p by all dimensions except d, giving potential groups */
potgroups  p / {(wa , wb ) : d 6= d . dimension d of wa = dimension d of wb }
/* add all potential groups that are the same size as dimension d to groups */
groups  groups  {g  potgroups : |g| = |Sd |}
/* replace each group of states with a single, more abstract state */
for all g  groups do
if g  W then
stochastically choose
wa from g
 new
w
is abstract in d
construct wnew :
dimension d of wnew = dimension d of wa d 6= d
W  W  {wnew }
P
1 P
new ) 


(wnew )  (wa ); V (wnew )  |g|
wg V (w); P(w
wg P(w)

W  W \ g /* discarding also the stored (w), V (w) and P(w) for all w  g */

In some cases, it may be impossible to coarsen a section of the worldview despite low
proximity, due to a situation somewhat akin to grid-lock. Probably the simplest example
is the one in Figure 5, which shows a worldview of five states in a three-dimensional binary
specific state space, with three of the states ignoring a different dimension each, while the
remaining two take into account all three. In this situation, no group of states can be
500

fiProximity-Based Non-uniform Abstractions for Planning

S1

0

0

0

0

1

1

1

1

S2

0

0

1

1

1

1

0

0

S3

0

1

1

0

0

1

1

0

w1

w2

w3

w4

w5

Figure 5: A non-uniform worldview that cannot be immediately coarsened. The state space
has three binary dimensions (eight states). The worldview has two concrete states,
w1 and w4 , and three abstract states, w2 , w3 and w5 , each abstract in a different
dimension.

coarsened in any single dimension. Before any coarsening is possible, one of the states must
first be refined, but if they all have low P they are not candidates for proximity-based
refinement. Because of this, integration of a uniform abstraction method into coarsening
would also not be as straightforward as for selecting the initial worldview or for refinement,
unless the worldview is kept uniform. However, even with a non-uniform worldview it would
not be difficult. For instance, the dimension could simply be removed only where possible
rather than everywhere.

8. Results
We run the algorithm over a range of different domains to demonstrate our approach. The
domains divide into two broad groups. The first group consists of the grid navigation
domain only. This was the domain on which intuition was gathered and preliminary runs
were done, however, so while problems in this domain show how well the approach performs,
they cannot show its generality. The second group consists of domains from the literature,
demonstrating how well our approach generalises.
8.1 Experimental Domains
We introduce our domains in this section. The first five problems are in the grid navigation
domain, two already described in Section 2.1, shown in Figure 1, with three additional
problems. The remaining domains are based on domains from the literature, in particular
as used by Kim (2001) and Barry (2009).
As described in Section 2.1, problems in the grid navigation domain shown in Figure 1
all have x and y dimensions of size 10, three door dimensions (binary: open/closed) and
a damage dimension (also binary). So far this is the 3Doors problem, while in the 3Keys
problem, there are keys which the agent must pick up keys to open the corresponding doors.
The three remaining problems are 1Key, shuttlebot and 1010. The 1Key problem is
similar to 3Keys, except that the agent is only capable of holding one key at a time, so
that instead of three binary dimensions for the keys, there is one four-valued dimension
indicating which key the agent holds (or none). The shuttlebot problem introduces a
cyclic goal (with an extra loaded dimension and with the damage dimension tri-valued)
501

fiBaum, Nicholson & Dix

(a) grid navigation domain
keys in world
Problem
3Doors
0
1Key
3
3
3Keys
0
shuttlebot
1010
0

keys held at same time

1
1, 2 or 3



note

cyclic
tiled

dimensions
6
7
9
7
8

|S|
1 600
6 400
12 800
4 800
160 000

(b) robot4 -k domain
Problem
robot4 -10
robot4 -15
robot4 -20
robot4 -25

dimensions
11
16
21
26

|S|
10 240
491 520
20 971 520
838 860 800

(c) factory domain
Problem
s-factory
s-factory1
s-factory3
(d) tireworld domain
locations
Problem
tire-small
5
8
tire-medium
tire-large
19
19
tire-large-n0

initial
n1
n0
n12
n0

dimensions
17
21
25

goal
n4
n3
n3
n3

|S|
131 072
2 097 152
33 554 432

route length
3
4
1
3

dimensions
12
18
40
40

|S|
4 096
262 144
1 099 511 627 776
1 099 511 627 776

Table 3: Experimental domains and problems, each with the dimensionality of the state
space and its size.

while the 1010 variant increases the size of the problem by tiling the grid 10 in each
direction (by having two extra dimensions, xx and yy, of size 10). Table 3(a) summarises
the problems in this domain.
The next two domains are based on those of Kim (2001). Firstly, the robot4 -k domain,
which are based on Kims (2001) ROBOT-k domain but reducing the number of actions to
four. The robot4 -k domain problems consist of a cycle of k rooms as shown in Figure 6,
where each room has a light, analogous to the doors of the 3Doors problem in that they
enable the agent to move. The four actions in our variant are to go forward, turn the light
in the current room on or off, or do nothing. The original formulation allowed the agent
any combination of toggling lights and going forward, for a total of 2k+1 actions, but we
have reduced this as our approach is not intended to approximate in the action space. The
goal is to move from the first room to the last. There are k + 1 dimensions for a state space
of k2k states, as listed in Table 3(b).
502

fiProximity-Based Non-uniform Abstractions for Planning

4
k-1

3

0

2
1

Figure 6: The robot4 -k domain.
drill B
part B:

shape B

drilled

polish B

polish B

shaped

dip B
polished

spray B
handpaint B

painted
glue

connected

bolt
drill A
part A:

shape A

shaped

drilled
polish A

polish A

dip A
polished

spray A

painted

handpaint A

Figure 7: The factory domain.
Kims (2001) factory domain10 is a series of variants on a simple manufacturing problem, represented purely in predicates (that is, dimensions of size 2). The agent is to make a
product from two parts which must be drilled, painted and so on and finally joined together.
Figure 7 shows a very simplified diagram, omitting most of the interactions and options 
for instance, to achieve the painted predicate, the agent may spray, dip or handpaint the
object; to connect the two objects, it may use glue or a bolt (and only the latter requires
that they be drilled); and so on. Unlike the other domains, partial rewards are available to
the agent for achieving certain subgoals. The problems used here are listed in Table 3(c).
The final domain is the tireworld domain from the 2006 ICAPS IPC competition
(Littman, Weissman, & Bonet, 2006) as used by Barry (2009). In this domain, a robotic
car is trying to drive from point A to point B. The car has room to carry one spare tire
and some locations have additional spare tires at them. At these locations, if the car is
not carrying a spare, it can pick one up. If there are n locations for the car, there will be
2n + 2 binary dimensions in the problem, as follows: n dimensions are used to represent
the location of the car. The only valid states are states where only one location dimension
is true, but this is not explicitly stated anywhere in the domain.11 Another n dimensions
are used to represent which locations have a spare tire and which do not. The final two
dimensions represent whether the car is carrying a spare and whether it has a flat tire.
10. This domain was previously used by Hoey, St.-Aubin, Hu, and Boutilier (1999) and is based on the
builder domain of Dearden and Boutilier (1997) which was adapted from standard job-shop scheduling
problems used to test partial-order planners.
11. We touch on this aspect in our discussion in Section 9, but in any case include the domain without
change in order to facilitate comparison with the literature.

503

fiBaum, Nicholson & Dix

n0

n15
n10
n8

n4

n0
n12

n17

n6

n3

n9

n8
n18
n2

n6

n14

n4

n10

n16
n2

n1

n1
n1

n13

n3

n12
n3
n7

n0

tire-small

n7

n11

tire-medium

n5

tire-large

 and goal ()
 locations.
Figure 8: The tireworld domain problems, indicating the initial ()

Barry (2009) uses two of the tireworld problems, labelled small and large. The
small tireworld problem has 5 locations for 12 variables and 14 actions, while the large
one has 19 locations for 40 variables and 100 actions. Curiously, in the large problem, there
is a direct road between the initial and goal locations, so that it only takes a single action
to solve the problem. This makes it difficult to assess whether Barrys method has, in fact,
scaled up. In addition to these two, we created a medium-sized tireworld, with 8 locations
and 18 variables, by removing locations from the large tireworld and moving the initial
location to n0, further from the goal. These variants are listed in Table 3(d) and shown in
Figure 8. The final variant, tire-large-n0, not shown, is identical to the large tireworld
except the initial location is again moved to n0.
8.2 Direct Evaluation of Policies with Pre-cursor Deliberation
With the smaller problems such as 3Doors, 1Key, and 3Keys, we can directly evaluate the
approximate policies produced by the planner running with pre-cursor deliberation. These
problems are small enough that we can use an exact algorithm to calculate the actual value
function corresponding to these approximate policies. As noted in Section 2.6, this can be
useful as it involves fewer potentially confounding variables, but it does not exploit the full
potential of our approach.12
Table 4(a) shows such results for policy-based refinement only  that is, with the
proximity-based methods (Algorithms 5, 6 and 7) disabled. For each problem, the table
lists the size of the problem |S| and the value of the optimal solution at the initial state
12. Proximity-based coarsening (Algorithm 7) is primarily aimed at regions of state space that the agent
has already traversed, but with pre-cursor deliberation there is no traversal. Coarsening would therefore
be expected to bring limited benefit with pre-cursor deliberation and direct evaluation would not be
meaningful to evaluate its performance. It is therefore only evaluated with recurrent deliberation.

504

fiProximity-Based Non-uniform Abstractions for Planning

olu
tio
nv
alu
wo
e
rld
vie
w
siz
e
rel
ati
ve
wo
rld
vie
w
siz
pla
e
nn
er
s
of esti
sol ma
uti te
on
val
ue
act
ua
ls
olu
tio
nv
alu
e

ize

op
tim
al
s

sta
te
sp a
ce
s

dis

cou
nti
ng

fac
tor

V  (s0 ), representing the costs and results for exact planning. These are followed by the
size of the worldview |W| as an absolute number and as a percentage of |S|, the planners
estimate of the value of its solution at the initial state V (s0 ), and the actual value of its
solution at the initial state V (s0 ). In the first half of each part of the table the discounting
factor is  = 0.999 99, while in each second half it is  = 0.95. Averages over 10 runs
are shown and the planner is run to 1000 . 1000 was chosen as an approximation to 
 it is assumed that 1 000 phases is sufficient for the planner to converge. In practice,
convergence generally took place much earlier. It is not detected, however, because of the
overall assumption that the planner continues to plan forever, responding to any changing
inputs, which makes convergence somewhat irrelevant.


problem
|S|
V  (s0 )
|W|
(a) policy-based refinement only
226.4
0.999 99 3Doors
1 600 27.50
326.8
1Key
6 400 79.47
3Keys
12 800 61.98
262.0
222.6
0.95
3Doors
1 600 14.63
1Key
6 400 19.59
296.8
245.7
3Keys
12 800 18.99
(b) proximity-based refinement only
0.999 99 3Doors
1 600 27.50 1 381.7
1Key
6 400 79.47 4 166.7
3Keys
12 800 61.98 4 262.8
0.95
3Doors
1 600 14.63 1 363.2
1Key
6 400 19.59 2 828.2
3Keys
12 800 18.99 4 230.6
(c) both policy- and proximity-based refinement
0.999 99 3Doors
1 600 27.50 1 361.7
1Key
6 400 79.47 4 635.5
3Keys
12 800 61.98 5 948.2
0.95
3Doors
1 600 14.63 1 359.5
1Key
6 400 19.59 4 036.1
3Keys
12 800 18.99 3 748.8

|W|
|S|

V (s0 )

V (s0 )

14%
5.1%
2.0%
14%
5.6%
1.9%

22.50
37 512.20
25 015.60
13.22
15.23
14.56

27.50
100 000.00
100 000.00
14.63
20.00
20.00

86%
65%
33%
85%
44%
33%

27.50
60 031.79
70 018.59
14.63
19.92
19.70

27.50
60 031.79
70 018.59
14.63
19.92
19.70

85%
72%
46%
85%
63%
29%

27.50
20 063.57
30 043.39
14.63
19.67
20.00

27.50
20 063.57
30 043.39
14.63
19.67
20.00

Table 4: Results for direct evaluation of policies with pre-cursor deliberation with three
different refinement methods, evaluated after 1 000 phases.

505

fiBaum, Nicholson & Dix

The results in part (a) of the table divide neatly into two types: without keys (3Doors
problem), the planner succeeds in all ten runs, getting perfect policies for the given starting
state. In the other two problems, 1Key and 3Keys, planning invariably fails. In these two
problems, the agent must pick up a key while far from the door it opens, and this version
of the planner simply cannot think ahead to that extent. For all three of these, the planner
is somewhat optimistic, estimating a better value than it obtains and in some cases even
better than optimum. For instance, in the 3Doors problem with  = 0.999 99, the planners
estimate of the value V (s0 ) is 22.50, which is better than both the true value and the
optimum, V (s0 ) = V  (s0 ) = 27.50. The fractional |W| in the table are due to its being
averaged over ten runs. The final size of the worldview sometimes depends to some extent
on the order in which the dimensions or the states are refined, and this order is randomised
between the runs. For instance, in the 3Doors problem with  = 0.999 99, W has various
sizes ranging from 220 to 231 states at the end of each of the ten runs, with an average of
226.4.
The results in part (a) are similar for the two values of . The main difference is that the
smaller  leads to smaller numbers. For instance, the value indicating failure is 100 000
1
,
when  = 0.999 99 but only 20 when  = 0.95. The values tend to be in multiples of 1
1
and with the smaller value of  here, 1 is 20 rather than 100 000. In some cases, this
smaller range can make differences less obvious: for instance, in the the estimated value
1
column (V (s0 )), it is not very clear whether the numbers are approximations to 1  1
1
(and a failure to reach the goal) or 0  1
minus a small number (representing success).
1
represent rewards or costs
While units represent once-off rewards or costs, multiples of 1
to be obtained in perpetuity. However, this is the expected and desired behaviour. A smaller
 represents a disinterest in the distant future, so that a reward or cost in perpetuity is not
much more important than a once-off reward or cost.
Table 4(b) shows the results of ten runs in pre-cursor mode to 1000 with proximitybased refinement only (no policy-based refinement and no coarsening) for each problem
and . As can be seen, the 3Doors problem is solved optimally in all cases. This is not
surprising, as it is not a complex problem.
The 1Key and 3Keys problems are more interesting. The figures in Table 4(b) arise as the
average of about 31 successful runs, which have values close to or equal to the optimal values
V  (s0 ), and about 23 unsuccessful runs which have values of 100 000. For  = 0.999 99,
the planner found a successful policy in 4 of the 10 runs in the 1Key problem and 3 times
out of 10 in the 3Keys problem. Similarly for the  = 0.95 case (2 times and 3 times,
1
respectively), but since the optimal path is quite long compared with 1
, so that success
means a reward of 19.59 (or 18.99) while failure is punished by 20, the effect is more
difficult to discern.
Table 4(c) shows the results for proximity-based refinement and policy-based refinement combined (no coarsening). Naturally, the 3Doors problem for which either refinement
method alone already obtained the optimal policy shows no improvement. Only the worldview size |W| differs slightly from Table 4(b), as policy-based refinement is sometimes more
directed, so the |W| tend to be slightly smaller than with the more exploratory proximitybased refinement alone, but larger than with policy-based refinement alone.
506

fiProximity-Based Non-uniform Abstractions for Planning

The other two problems, 1Key and 3Keys, show improvement compared with either of
the refinement methods alone. They are now solved in 43 of the runs for  = 0.999 99 
the values 20 063.57 and 30 043.39 represent averages between 2 and 3 unsuccessful runs
and 8 and 7 successful ones, compared with 4 and 3 successful runs for proximity-based
refinement only and no successful runs for policy-based refinement only. For  = 0.95, the
1Key problem is again solved on 8 of the runs, but due to the discounting and the length
of the path, the goal is very near the horizon. Again, with success meaning a reward of
19.59 while a failure receives 20, the distinction is not very great. The 3Keys problem
with  = 0.95 did not find a solution at all with these parameters, so it receives a uniform
20 for each of its runs, for a suboptimality of about one unit.
The behaviour during these runs is generally quite straightforward. Typically, after
initially calculating that the agent cannot reach the goal with the initial worldview, the
worldview size gradually increases, then plateaus  there is no coarsening here, nor movement of the agent, so no other behaviour is really possible. In the successful runs, the
planner plans a route to the goal at some point during the increase, when the worldview becomes sufficient, and V (s0 ) quickly reaches its final value. Rarely, the V (s0 ) may oscillate
once or twice first. We omit the graphs here, but they are presented by Baum (2006).
8.3 Evaluation by Simulation with Recurrent Deliberation
In larger problems, performance can be evaluated by simulation, running the agent in
a simulated world and observing the reward that it collects. In such problems, direct
evaluation is not possible because calculating the actual value function using an exact
algorithm is no longer tractable. Simulation with recurrent delibertion is also the context
in which coarsening can be evaluated. For comparison, this section presents results for the
3Keys problem evaluated by simulation, both without and with coarsening.
Figure 9 shows a representative sample of the results for simulation on the 3Keys problem
with both refinement methods and no coarsening, which is the same combination of options
as shown in Table 4(c) in the previous section, evaluated by simulation rather than directly.
Each small graph shows a different individual run. As can be seen, the agent behaves
reasonably when working in the recurrent planning mode against the simulation.
The left vertical axes on the graphs represent reward R, plotted with thick red lines.
In run 1 of Figure 9, for instance, the agent starts off receiving a reward of 1 for each
step, meaning not at goal, no damage in this domain. From about 180 onwards, it receives
a reward of 0 per step, meaning at goal, no damage. The right vertical axes are the
worldview size |W|, with thin blue lines. They are shown as details throughout this section,
that is, scaled to the actual worldview sizes rather than the full 1|S| ranges. Taking run 1
in Figure 9 again, we see that |W| grows relatively quickly until about 80, then continues
to grow slowly and eventually levels out a little below 5 000. The full state space, for
comparison, is 12 800. In run 2, the agent received reward similarly, but the state space
grew longer, eventually levelling out somewhat above 7000. In runs 3 and 4, the agent
failed to reach the goal and continued receiving the reward of 1 throughout. In run 3, the
worldview size levelled out a little over 3000, while in run 4 it steadily grew to about 5000.
The horizontal axes are simulated world time, corresponding to the discrete time-steps
of the MDP. There are two other time-scales in the simulation: wall clock time, indicating
507

fiBaum, Nicholson & Dix

1:

|W|

R

2:

7000
6000

0

7000
6000

0

5000

5000

4000

4000

3000

3000

2000

-1

|W|

R

2000

-1

1000
0

50

100

150

200

1000

0
250

0

|W|

R

50

100

time

3:

150

200

time

R

4:

7000
6000

0

|W|

7000
6000

0

5000

5000

4000

4000

3000

3000

2000

-1

2000

-1

1000
0

50

100

0
250

150

200

1000

0
250

0

time

50

100

150

200

0
250

time

Figure 9: Simulation results, 3Keys problem, policy-based and proximity-based refinement,
no coarsening (four runs). Reward (left axes, thick red lines) and worldview size
|W| (right axes, thin blue lines; detail) against world time (horizontal axes).

the passage of real time, and the number of phases the planner has performed. The simulation is configured to take 1 time step per 10s of wall clock time. The number of phases
R
is not controlled and is simply given by the speed of the (1.5GHz Intel
) CPU and an implementation coded for flexibility rather than efficiency. Ideally, the agent should gradually
move in the general direction of the goal during planning, as this simplifies the problem,
but not so fast that the agent runs too far ahead of the abstraction in the planner.
The planner algorithm does not terminate, since the planner is assumed to keep planning
(and the agent to keep acting) indefinitely. In goal-oriented domains, such as most of the
examples in this paper, one might consider achieving the goal to be such a termination
condition, but (a) the example domains assume that the agent will continue with the same
goal as a goal of maintenance, albeit trivial, (b) it does not apply at all to non-goal-oriented
domains and (c) even in goal-oriented domains it is not clear how to apply the condition in
the case where the agent fails to reach the goal. In the simulation, therefore, runs were either
terminated manually, when they succeeded or when it appeared that no further progress
508

fiProximity-Based Non-uniform Abstractions for Planning

1:

|W|

R

0

-1

0

2:

9000
8000
7000
6000
5000
4000
3000
2000
1000
0
50 100 150 200 250 300 350 400

0

-1

0

time

|W|

R

9000
8000
7000
6000
5000
4000
3000
2000
1000
0
50 100 150 200 250 300 350 400
time

Figure 10: Simulation results illustrating the effect of coarsening on worldview size, 3Keys
problem, policy-based refinement, proximity-based refinement and coarsening
(two runs).

was likely to be made, or run for a fixed number of world time steps, selected based on the
manually-terminated runs with some allowance for variation.
Because there is no coarsening in Figure 9, the worldview sizes are monotonic increasing.
Different runs refined differently  both the domain and the algorithm are stochastic. At
the beginning of planning, the agent is receiving a reward of 1 per step, because it is not
yet at the goal. As the worldview size increases, the planner eventually finds a policy which
leads to the goal in runs 1 and 2, as can be seen by the better reward 0 obtained in those
runs. There is a simple relationship between worldview size and performance: runs which
worked with a worldview of about 5 000 or larger generally succeeded, those with smaller
worldviews generally did not. On the vast majority of the runs (> 90%), the agent reached
the goal.
When coarsening (Algorithm 7) is activated, compared to the situation when it is turned
off, the reward gathered by the agent declines slightly, but it still reaches the goal on the
vast majority of runs (> 90%). Figure 10 shows two of the runs, one successful and one
unsuccessful, for the 3Keys problem with proximity-based coarsening as well as the two
refinement methods, in contrast with Figure 9 where only the two refinement methods are
used. Note the effect of the interleaving of the refinement and coarsening: the worldview
size |W| (thin blue line) is no longer monotonic, instead being alternately increased and
decreased, which shows up as a jagged line on the graph. Slightly fewer of the runs reach
the goal. Some decline in solution quality is expected, however, since the goal of coarsening
is to reduce the size of the worldview.
For completeness, we have also tested the agent with both proximity-based methods
(Algorithms 6 and 7) active but with the policy-based refinement (Algorithm 4) deactivated.
In this configuration, the agent collects no reward  it generally takes some steps toward
the goal, but without the more directed policy-based refinement, the largely exploratory
proximity-based methods do not discover the keys and consequently cannot reach the goal.
509

fiBaum, Nicholson & Dix

8.4 The Effect of the Discounting Factor
The shuttlebot problem is similar to the 3Doors problem but requires the agent to move
back and forth between two locations repeatedly. It is interesting because in preliminary
runs in pre-cursor mode it was not solved at all for  = 0.999 99 while being solved optimally
for  = 0.95. It was considered whether the  = 0.999 99 case might behave better under
simulation, if the agent took advantage of the possibility of planning only for the nearest
reward and then replanning once that reward is obtained. After all, the agent could function
well even if none of the policies were a good solution by itself. However, as illustrated in
Figure 11, the agents behaviour was very similar to the pre-cursor case: for  = 0.999 99,
(a) refinement only, run 1, and (b) with coarsening, run 2, it would pick up the reward
immediately adjacent to s0 , but no more than that. Again, setting the planners discounting
factor  to 0.95, (c) with coarsening, runs 3 and 4, provided much better performance.13
Note again the effect of the balance of refinement and coarsening in (b) and (c): the
worldview size |W| is nice and steady throughout the runs (though admittedly at a fair
fraction of |S| = 4 800).
8.5 Initial Worldview
For some of the problems, the standard initial worldviews are too large for the planner. Even
modified, smaller initial worldviews obtained by only enabling the nexus step of Algorithm 3
and disabling the reward step are too large. Disabling both the reward and the nexus steps
results in a singleton initial worldview, W = {S}, which treats the entire state space S
as a single (very) abstract worldview state. Unfortunately, this means that the planner
starts with very little in the way of hints as to the direction in which to refine and, at least
initially, no other information on which to base this crucial decision. The upshot is that it
collects no reward. In some cases it remains at the initial state s0 , in others moves around
the state space  sometimes for some distance, other times in a small loop  but it does
not reach the goal or any of the subgoals.14
This is the situation with the factory domain problems of Kim (2001), and in fact the
agent collected no reward during any of the simulated runs, even though in quite a few runs
substantial actions were taken. A similar result occurs for the 10x10 problem (in our grid
navigation domain). No reward was obtained by the agent in this problem, because the
standard initial worldview is somewhat too large for the planner and, again, the singleton
initial worldview does badly. At best, on some runs, the agent took a few limited steps in
the general direction of the goal.
An interesting case is the tireworld domain. Again, tire-large is too large with the
standard initial worldview and fails to obtain a solution with the reward step of the initial
worldview only. However, with a manually-chosen initial worldview that refines the locations
along the path from the start state to the goal before planning begins, the planner solves
not only tire-large, but also tire-large-n0 in 40% of the runs (in one of them after less
than one minute, although that is atypical).
13. The rewards appear as two horizontal lines in runs 3 and 4, one solid and one broken, because the task
is cyclic, and the agent collects a reward of 1 twice in each cycle and a reward of 0 on all the other steps.
14. Further details of these unsuccessful runs, including the |W| behaviour, are given by Baum (2006).

510

fiProximity-Based Non-uniform Abstractions for Planning

(a)  = 0.999 99, no coarsening (one run)
R
|W|
1:
4500

(b)  = 0.999 99, with coarsening (one run)
R
|W|
2:
4500

4000

4000

3500

1

3500

1

3000

3000

2500

2500

2000

2000

1500
0

1500
0

1000

1000

500
0

1000

2000

3000

500

0
4000

0

200

time

400

600

800

(c)  = 0.95, with coarsening (two runs)
R
|W|
3:
4500

4:

|W|

R

4000

3500

1

3000

3000

2500

2500

2000

2000

1500
0

1500
0

1000

1000

500
1000

2000

3000

4500
4000

3500

1

0

0
1000

time

500

0
4000

0

time

1000

2000

3000

4000

0
5000

time

Figure 11: Simulation results illustrating the effect of the discounting factor, shuttlebot
problem, policy-based and proximity-based refinement.

8.6 Worldview Size and Quality
Finally, we consider the effect of worldview size and quality on the robot4 domain, where
the agent moves through a series of rooms with lights. This domain is an excellent example
where the simulated agent works well. In all runs of the robot4 -10 and robot4 -15 problems
the agent thought for a small amount of time, then quickly moved to the goal and stayed
there, with only very small worldviews, as can be seen in Figure 12 for the robot4 -15
problem. Four representative runs are shown, two with the two refinement methods only
(runs 1 and 2) and two with all three methods (runs 3 and 4). In all four runs, the worldview
sizes |W| are reasonable  consider that the full state space contains almost half a million
states, so that a 1 000-state worldview represents just a fifth of a percent. Despite this small
worldview size, however, the planner is effective. After only a few dozen phases, the agent
has reached the goal. The planner works well for robot4 -10 and robot4 -15.
For comparison, Kims (2001) largest ROBOT-k problem is ROBOT-16, though since
ROBOT-16 has 216+1 = 131 072 actions while our robot4 -k domain problems have 4, a
511

fiBaum, Nicholson & Dix

(a) no coarsening (two runs)
R
1:

|W|

2:

1200

|W|

R

1000

1200
1000

1

1
800

800

600

600

400

400

0

0
200

200

0
0 10 20 30 40 50 60 70 80 90 100

0
0 10 20 30 40 50 60 70 80 90 100

time

time

(b) with proximity-based coarsening (two runs)
R
|W|
3:
4:
1200

|W|

R

1000

1200
1000

1

1
800

800

600

600

400

400

0

0
200

200

0
0 10 20 30 40 50 60 70 80 90 100

0
0 10 20 30 40 50 60 70 80 90 100

time

time

Figure 12: Simulation results, robot4 -15 problem,  = 0.999 99, policy-based and proximity-based refinement, with and without proximity-based coarsening.

direct comparison would not be valid. On the other hand, our values of k (10, 15 and
so on) are not necessarily powers of 2, since, unlike that of Kim, our domain specification
always considers room numbers atomic rather than binary numbers, so there is no particular
advantage to powers of 2.
The results for the robot4 -20 problem are beginning to be more interesting than those
for robot4 -10 and robot4 -15. In Figure 13(a), showing two of the runs with no coarsening
(runs 1 and 2), the agent succeeds reasonably promptly and with reasonable worldview
sizes. However, as illustrated in Figure 13(b), when coarsening is active the planner fails to
reach the goal on some of the runs (about 40%, for example, run 3) and succeeds on others
(about 60%, for example, run 4). The state space contains almost 21 million states, so the
successful worldviews in Figure 13 are of the order of 0.01% of the full state space size.
Figure 14 shows four representative runs of the robot4 -25 problem, again (a) two without coarsening (runs 1 and 2) and (b) two with all three methods (runs 3 and 4). This problem has a state space of 25  225  839 million states, and the effect noted for robot4 -20 is
512

fiProximity-Based Non-uniform Abstractions for Planning

(a) no coarsening (two runs)
R
1:

|W|

2:

8000

|W|

R

7000
1

7000

6000

0

1

6000

5000

5000

4000

4000

3000

3000

2000

0

2000

1000
0

50

100

150

200

1000

0
250

0

50

100

time

150

200

|W|

R

7000
1

0

1

6000

5000

5000

4000

4000

3000

3000

2000

0

2000

1000
100

150

200

8000
7000

6000

50

0
250

time

(b) with proximity-based coarsening (two runs)
R
|W|
4:
3:
8000

0

8000

1000

0
250

0

time

50

100

150

200

0
250

time

Figure 13: Simulation results, robot4 -20 problem,  = 0.999 99, policy-based and proximity-based refinement, with and without proximity-based coarsening.

much more pronounced here: without coarsening, the planner tends to much larger worldviews15 These large worldviews then cause the planner to run slowly. As noted in Section 8.3
above, the horizontal axes are world time, not planning time. The relation between the two
varies quite significantly between the runs  from more than two policies per time step
with the smaller worldviews to less than one in ten time steps when the worldviews grew
large.
As far as reaching the goal is concerned, the two cases are similar. Again, the successful
runs are those which maintain a reasonably-sized worldview, such as runs 2 and 4. Runs
where the worldview size grows big invariably fail (runs 1 and 3). The difference is that this
time, the smallest successful worldview, run 4 in Figure 14, used around 2 000 well-chosen
worldview states, which is just 0.000 24% of the full state space. If the worldview grows
beyond a miniscule fraction of the state space  and even the 19 243-state worldview of
15. Note that run 1 is plotted with a different scale on the worldview size |W| axis compared to runs 2, 3
and 4. This makes the details of their behaviour easier to see, but makes the size of run 1 less obvious.

513

fiBaum, Nicholson & Dix

(a) no coarsening (two runs)
R
1:

0

0

|W|

2:

20000
18000
16000
14000
12000
10000
8000
6000
4000
2000
0
50 100 150 200 250 300 350 400

|W|

R

8000
7000

1

6000
5000
4000
3000

0

2000
1000
0

time

0
50 100 150 200 250 300 350 400
time

(b) with proximity-based coarsening (two runs)
R
|W|
4:
3:

|W|

R

8000

8000

7000
1

7000
1

6000

0

6000

5000

5000

4000

4000

3000

3000
0

2000

2000

1000
0

250

500

750

1000

0
1000

0

time

0
20 40 60 80 100 120 140 160
time

Figure 14: Simulation results, robot4 -25 problem,  = 0.999 99, policy-based and proximity-based refinement, with and without proximity-based coarsening. Note the
different scales on the worldview size |W| axis for run (a)1.

run 1 in Figure 14 is only 0.002 3%  the planner will stall and no further progress will be
possible. Even in this challenging environment, the agent reaches the goal in almost half
the runs.

9. Discussion
Section 8 presented results across a range of experimental domains showing that our method
successfully finds solutions to the planning problem using much less than the full state space,
as well as some of its limitations. In this section we discuss these results and analyse the
features of domains which our method can exploit and those which give it difficulty.
On smaller problems, we could directly evaluate the policies produced by our method
in pre-cursor mode, allowing us to better isolate the behaviour of the planner. Without the
proximity-based methods, the worldviews were quite small and the planner could only solve
514

fiProximity-Based Non-uniform Abstractions for Planning

the 3Doors problem. However, even this is better than a uniform abstraction, which could
do very little here. Even an oracle could at best remove one door and its key in 3Keys or
two doors in 3Doors, giving a 25% relative worldview size  however, if the planner then
used a uniform distribution over the removed dimension, the agent would fail anyway, since
opening the doors that were left in the worldview would be harder than hoping for the best
on the assumed-50%-open door which is actually closed. To succeed, it would also have to
deduce that the abstracted doors should be considered closed, a considerable feat. This is a
function of the sample domains. In other circumstances, uniform abstraction could be very
effective, either as a pre-processing step to our approach or integrated with our W selection
methods.
We expected that turning on proximity-based refinement (Algorithms 5 and 6) would
lead to larger worldviews. In general, one would expect larger worldviews to yield better
solutions and smaller worldviews to yield worse solutions, but for lower computational cost.
This means that proximity-based refinement should, in general, improve the solution quality.
The results corresponded to this expectation. The worldviews were indeed larger and the
solution quality was higher.
In larger problems, performance could only be evaluated by simulation, running the
agent in a simulated world and observing the reward that it collects. In such problems,
direct evaluation was not possible because calculating the actual value function using an
exact algorithm was no longer tractable. Section 8.3 therefore presented results for the
3Keys problem for comparison with those obtained by direct evaluation of policies with
pre-cursor deliberation discussed above. As can be seen, the results correspond, crossconfirming the evaluation methods.
In addition, simulation with recurrent delibertion is the context in which coarsening
could be evaluated. When proximity-based coarsening was activated, compared to the
situation when it is turned off, the reward gathered by the agent declines slightly. This
impression of worse performance is somewhat misleading. It is due to the fact that this first
comparison takes place on one of the small problems, which the planner was able to solve
without any coarsening, and in fact without any abstraction at all. Thus, the disadvantages
(lower reward collected) are much more apparent than the advantages (lower computational
cost).
On larger problems, where working without abstractions was not an option, the balance
was reversed. In fact, and somewhat counterintuitively, in some of the larger problems
with coarsening active, the successful runs had smaller worldviews than the unsuccessful
runs. Clearly, it is not the size of the worldview that determines success, but its quality. A
good worldview enabled efficient calculation of policies that progress toward the goal while
remaining small. A poor worldview simply grew larger. In the smaller problems, a growing
worldview may have eventually covered most of the state space in detail, thus masking the
effect. The planner would find a good policy effectively without any real approximation.
In larger problems, where finding a good policy without approximation was not feasible,
a similarly growing worldview simply slowed the planner down until no further progress
was made. In this situation, the worldview-reducing action of proximity-based coarsening
became crucial, ensuring that at the least the worldview remained tractably small and
thereby enabled the planner to deal with the problem.
515

fiBaum, Nicholson & Dix

As can be seen from the results, coarsening was successful in this task some of the
time. When the agent paused to replan part-way to the goal, it reduces the size of the
worldview to keep it more relevant to the changing circumstances. In other runs, however,
the worldview size grew beyond the capabilities of the planner. In some cases, such as in
the 10x10 problem, it settled at a higher balance. In others, it appears to have simply
continued growing. In the latter case, it would not appear to be a simple question of
tuning the parameters: when it did find balance, it was at an appropriate worldview size.
There appears to be some other factor, some other quality of the worldview determining
the success or failure of those runs  whether they find balance and reach the goal or grow
too big and fail.
A number of problems were solved poorly or not at all due to our initial abstraction
selection algorithm (Algorithm 3). On these problems, the algorithm produced either a large
worldview that exceeded available memory (either immediately or very shortly afterwards),
so that planning was not possible, or a small worldview in which planning was ineffective.
It could not be set to produce a medium-sized worldview, none of the four combinations of
options produced one. In some problems, only the singleton worldview was possible  that
is, both steps of initial abstraction selection disabled, resulting in a worldview aggregating
all states into a single, maximally abstract worldview state  leading typically to no reward
being collected by the agent. At best, it would take a few actions in the general direction
of the goal(s). This is considerably worse than previous work. For instance, Kim (2001)
obtains approximate solutions for the problems and for their larger variants, as do others
who use this domain or a variant, including Hoey et al. (1999) and their originators, Dearden
and Boutilier (1997).
It was the necessity of using a singleton initial worldview which understandably greatly
hurt the performance. An infelicity at the worldview initialisation stage could impair the
entire planning process, since the worldview was never completely discarded by the planner.
The worldview-improvement algorithms could have made up for some amount of weakness
in the initial worldview, but a singleton worldview was a poor starting point indeed. This is
similar to an observation made by Dean et al. (1995) in their work using a reduced envelope
of states (that is, a subset of the state space). Their high level algorithms, like those here,
work regardless of the initial envelope (worldview). In practice, however, it is better for
the initial envelope to be chosen with some intelligence, for instance to contain at least a
possible path to the goal (for goal-oriented domains). They find this path using a simple
depth first search, and while this itself is not directly applicable to worldviews with their
gradations of abstraction, the overall concept remains: a reasonable initial worldview is
crucial.
The tireworld results confirm this. Here the initial worldview with the reward step
enabled was small, since the domain only rewards a single dimension, and planning was
ineffective. Once the planner was given a better initial worldview  one which it could
have plausibly calculated  it became quite effective, even in the modified tire-large-n0
where the initial state was deliberately moved further from the goal. It seems, then, that
while our basic approach is general, our worldview selection and modification methods are
less so. This is good  worldview selection is the less fundamental apect of our approach
and can be easily supplemented by additional methods or even just tuning. In domains like
516

fiProximity-Based Non-uniform Abstractions for Planning

tireworld, it seems that a modified predicate solver that can generate plausible trajectories
from the current state to a goal would do well as part of worldview selection.
It is interesting to compare this with the results of Sanner and Boutilier (2009) for
tireworld, which they only describe in passing as extremely poorly approximated before
going on to manually tweak the domain for their planner, adding the information that the
locations are mutually exclusive. This makes planning much easier and largely invalidates
any comparison between approaches.16 It is a fair question, however, to what extent this is a
weakness of their planner and to what extent it is an artefact of the domain. Its combination
of representation and narrative seems rather unfortunate, as the narrative with its obvious
1-of-n intuition will tend to obscure the real features (and the real applicability) of the
propositional representation and hinder rather than help intuition. This would not occur
with a tighter fit between representation and narrative.
Others, of course, solve the tireworld domain well. Some, such as Barry, Kaelbling, and
Lozano-Prez (2010), generate the full policy, while others take advantage of the initial state,
as our planner would do. It is difficult to know to what extent their planners are adapted
to the domain and to what extent they are flexible. It seems that only in recent years has it
become common for planners to be tested on domains to which the researchers do not have
access during development, as with some of the ICAPS IPC domains, rather than being
to a greater or lesser degree hand-tuned, usually unconsciously, to the particulars of one
or another domain, as ours was undoubtedly unconsciously tuned to the grid navigation
domain.
An interesting side point is provided by the shuttlebot problem, which was not solved
at all for  = 0.999 99 (other than collecting the trivial reward immediately adjacent to
s0 ) while being solved optimally for  = 0.95. Since the simulator itself does not have any
intrinsic discounting factor  it reports each reward as it is collected  one can see that
even though the planner was working with a discounting factor  = 0.95, it provided a
better solution to the  = 0.999 99 case than when it worked with  = 0.999 99 in the first
place.
In some ways this better behaviour with the smaller planner discounting factor is reasonable, because while the agents horizon is represented by the world discounting factor, the
horizon of any particular policy  and therefore the planner  is effectively much shorter,
as the policy will be supplanted by a new one relatively soon. Thus, it may be useful on
occasion to set the planners discounting factor  lower than the true world discounting factor in order to facilitate planning. However, this may lead to suboptimal, short-sighted
policies.

10. Conclusions
The theory of Markov decision processes provides algorithms for optimal planning. However,
in larger domains these algorithms are intractable and approximate solutions are necessary.
Where the state space is expressed in terms of dimensions, its size and the resulting computational cost is exponential in the number of dimensions. Fortunately, this also results in
a structured state-space where effective approximations are possible.
16. Similarly, Kolobov, Mausam, and Weld (2008) report results on a variant of tireworld rather than on
tireworld itself, without providing an explanation.

517

fiBaum, Nicholson & Dix

Our approach is based on selectively ignoring some of the dimensions in some parts
of the state space in order to obtain an approximate solutions at a lower computational
cost. This non-uniform abstraction is dynamically adjusted as planning and (in on-line
situations) execution progress, and different dimensions may be ignored in different parts of
the state space. This has strong implications, since the resulting approximation is no longer
Markovian. However, the approach is both intuitive and practical. It is the synthesis of
two existing approaches: the structure-based approximation of uniform abstraction and the
dynamic locality-based approximation of envelope methods. Like the envelope methods, it
can be limited by its reliance on the initial worldview (or envelope): if that is poor, it will
tend to perform poorly overall. Our approach subsumes uniform abstraction completely 
it can be treated as a special case of our more general method.
This paper extends the preliminary work of Baum and Nicholson (1998) by modifying
the worldview based on the proximity measure, both enlarging and reducing its size, and by
evaluating the behaviour against a simulation. This allows us to test the approach on larger
problems, but more importantly demonstrates both the full strength of the approach and its
limits in terms of the domain features that it can exploit and those that it can exploit only
with adjustment or not at all. The abstraction becomes truly dynamic, reacting to changes
in the agents current state and enabling planning to be tailored to the agents situation
as it changes. As shown by the qualitative and quantitative results presented both here
and by Baum (2006), the approach can be effective and efficient in calculating approximate
policies to guide the agent in the simulated worlds.
10.1 Future Work
One possible direction for future research would be to find worldview initialisation and
modification methods that result in smaller yet still useful worldviews, probably domainspecific, to extend the method to further domains, either larger or with different features.
For example, the factory and tireworld domains are goal-oriented and based on predicates,
and a worldview selection or modification method based on a predicate-oriented solver could
find possible paths to the goal and ensure that relevant preconditions are concrete along
that path.
Interestingly, in the 10x10 problem, while the proximity-based methods did not keep
the worldview size small, they did seem to find a balance at a larger but stil moderate
size. Thus another possibility might be to tune the proximity-based methods or develop
self-tuning variants.
At a number of points, for instance phase selection, our algorithm uses stochastic choice
as a default. This could be replaced by heuristics, learning, or other more directed methods.
One could adapt our method to work with other types of MDPs, such as undiscounted
or finite-horizon ones, or combine it with other approaches that approximate different aspects of the domains and the planning problem, as described in Section 2.5. For example,
as mentioned in that section, Gardiol and Kaelbling (2004, 2008) combine hierarchical state
space abstraction somewhat similar to ours with the envelope work of Dean et al. (1995).
Many other combinations would likely be fruitful for planning in domains which have features relevant to multiple methods. Similarly, additional refinement or coarsening methods
518

fiProximity-Based Non-uniform Abstractions for Planning

could be added, for instance as one based on the after-the-fact refinement criterion with
roll-back of Reyes et al. (2009).
On a more theoretical side, one could look for situations in which optimality can be
guaranteed, as Hansen and Zilberstein (2001) do with their LAO* algorithm for the work of
Dean et al. (1995), observing that if an admissible heuristic is used to evaluate fringe states,
rather than a pragmatically chosen V (out), the algorithm can be related to heuristic search
and acquires a stopping criterion with guaranteed optimality (or -optimality). Perhaps a
similar condition could be developed for our approach, with a rather different heuristic.
There are two basic directions in which this work can be further extended in a more
fundamental way, relaxing one of the MDP assumptions, perfect observability or knowledge
of the transition probabilities. A Partially Observable Markov Decision Process (POMDP)
gives the agent an observation instead of the current state, with the observation partly
random and partly determined by the preceding action and the current state. While the
optimal solution is known in principle, it is quite computationally expensive, since it transforms the POMDP into a larger, continuous, many-dimensional MDP on the agents beliefs.
As such, the non-uniform abstraction approach could be applied in two different ways: either to the original POMDP, a fairly direct translation, or to the transformed MDP. The
other extension would be to apply the technique when the agent has to learn the transition
probabilities. In particular, the application of our technique to exploration17 would be very
interesting  the agent would have to somehow learn about distinctions within single abstract states, so as to distinguish which of them should be refined and which should remain
abstract.

References
de Alfaro, L., & Roy, P. (2007). Magnifying-lens abstraction for Markov decision processes.
In Proceedings of the 19th International Conference on Computer Aided Verification,
CAV07, pp. 325338.
Barry, J., Kaelbling, L. P., & Lozano-Prez, T. (2010). Hierarchical solution of large Markov
decision processes. In Proceedings of the ICAPS Workshop on Planning and Scheduling
in Uncertain Domains.
Barry, J. L. (2009). Fast approximate hierarchical solution of MDPs. Masters thesis,
Massachusetts Institute of Technology.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, Special Volume: Computational Research
on Interaction and Agency, 72 (12), 81138.
Baum, J. (2006). Dynamic Non-uniform Abstractions for Approximate Planning in Large
Structured Stochastic Domains. Ph.D. thesis, Clayton School of Information Technology, Monash University. Available at www.baum.com.au/jiri/baum-phd.ps.gz
17. While the learning problem could also be transformed into an MDP on the agents beliefs or experiences,
it would be computationally prohibitive. The standard approaches instead explicitly distinguish exploration, where the agent learns about its domain (but ignores goals) and exploitation, where it achieves
goals (but ignores opportunities to learn).

519

fiBaum, Nicholson & Dix

Baum, J., & Nicholson, A. E. (1998). Dynamic non-uniform abstractions for approximate
planning in large structured stochastic domains. In Lee, H.-Y., & Motoda, H. (Eds.),
Topics in Artificial Intelligence, Proceedings of the 5th Pacific Rim International Conference on Artificial Intelligence (PRICAI-98), pp. 587598.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI
planning with automatically learned macro-operators. Journal of Articial Intelligence
Research, 24, 581621.
Boutilier, C. (1997). Correlated action effects in decision theoretic regression. In Geiger, D.,
& Shenoy, P. (Eds.), Proceedings of the 13th Conference on Uncertainty in Artificial
Intelligence (UAI-97), pp. 3037.
Boutilier, C., & Dearden, R. (1996). Approximating value trees in structured dynamic programming. In Proceedings of the 13th International Conference on Machine Learning,
pp. 5462.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Mellish, C. S. (Ed.), Proceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI-95), Vol. 2, pp. 11041111.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121 (1-2), 49107.
Boutilier, C., Goldszmidt, M., & Sabata, B. (1999). Continuous value function approximation for sequential bidding policies. In Laskey, K., & Prade, H. (Eds.), Proceedings of
the 15th Conference on Uncertainty in Artificial Intelligence (UAI-99), pp. 8190.
Cassandra, A. R., Kaelbling, L. P., & Kurien, J. A. (1996). Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation. Tech. rep. TR CS-96-17, Computer
Science, Brown University.
Daoui, C., Abbad, M., & Tkiouat, M. (2010). Exact decomposition approaches for Markov
decision processes: A survey. Advances in Operations Research, 2010, 120.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. E. (1995). Planning under time
constraints in stochastic domains. Artificial Intelligence, 76 (1-2), 3574.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.
Dietterich, T. G. (2000). Hierarchical reinforcement learning with the MAXQ value function
decomposition. Journal of Artificial Intelligence Research, 13, 227303.
Drummond, M., & Bresina, J. (1990). Anytime synthetic projection: Maximizing the probability of goal satisfaction. In Dietterich, T., & Swartout, W. (Eds.), Proceedings of
the 8th National Conference on Artificial Intelligence (AAAI-90), pp. 138144.
Gardiol, N. H., & Kaelbling, L. P. (2004). Envelope-based planning in relational MDPs. In
Advances in Neural Information Processing Systems 16  NIPS-03.
520

fiProximity-Based Non-uniform Abstractions for Planning

Gardiol, N. H., & Kaelbling, L. P. (2008). Adaptive envelope MDPs for relational equivalence-based planning. Tech. rep. MIT-CSAIL-TR-2008-050, Computer Science and
Artificial Intelligence Laboratory, Massachusetts Institute of Technology.
Goldman, R. P., Musliner, D. J., Boddy, M. S., Durfee, E. H., & Wu, J. (2007). Unrolling
complex task models into MDPs. In Proceedings of the 2007 AAAI Spring Symposium
on Game Theoretic and Decision Theoretic Agents.
Goldman, R. P., Musliner, D. J., Krebsbach, K. D., & Boddy, M. S. (1997). Dynamic
abstraction planning. In Kuipers, B., & Webber, B. (Eds.), Proceedings of the 14th
National Conference on Artificial Intelligence and 9th Innovative Applications of Artificial Intelligence Conference (AAAI/IAAI-97), pp. 680686.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
for factored MDPs. Journal of Artificial Intelligence Research, 19, 399468.
Hansen, E. A., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds
solutions with loops. Artificial Intelligence, 129 (12), 3562.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution of Markov decision processes using macro-actions. In Cooper, G., & Moral,
S. (Eds.), Proceedings of the 14th Annual Conference on Uncertainty in Artificial
Intelligence (UAI-98), pp. 220229.
Hoey, J., St.-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using
decision diagrams. In Proceedings of the 15th Annual Conference on Uncertainty in
Artificial Intelligence (UAI-99), pp. 279288.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press.
Kim, K.-E. (2001). Representations and Algorithms for Large Stochastic Planning Problems.
Ph.D. thesis, Deptartment of Computer Science, Brown University.
Kirman, J. (1994). Predicting Real-time Planner Performance by Domain Characterization.
Ph.D. thesis, Department of Computer Science, Brown University.
Kolobov, A., Mausam, & Weld, D. S. (2008). Regressing deterministic plans for MDP
function approximation. In Workshop on A Reality Check for Planning and Scheduling
Under Uncertainty at ICAPS.
Korf, R. (1985). Macro-operators: A weak method for learning. Artificial Intelligence, 26 (1),
3577.
Littman, M., Weissman, D., & Bonet, B. (2006). Tireworld domain. The Fifth International
Planning Competition (IPC-5) hosted at the International Conference on Automated
Planning and Scheduling (ICAPS 2006).
Munos, R., & Moore, A. (1999). Variable resolution discretization for high-accuracy solutions of optimal control problems. In Dean, T. (Ed.), Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99), pp. 13481355.
Musliner, D. J., Durfee, E. H., & Shin, K. G. (1995). World modeling for the dynamic
construction of real-time plans. Artificial Intelligence, 74, 83127.
521

fiBaum, Nicholson & Dix

Nicholson, A. E., & Kaelbling, L. P. (1994). Toward approximate planning in very large
stochastic domains. In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, pp. 190196.
Parr, R. (1998). A unifying framework for temporal abstraction in stochastic processes.
In Proceedings of the Symposium on Abstraction Reformulation and Approximation
(SARA-98), pp. 95102.
Puterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms for discounted
Markov decision processes. Management Science, 24, 11271137.
Reyes, A., Sucar, L. E., & Morales, E. F. (2009). AsistO: A qualitative MDP-based recommender system for power plant operation. Computacion y Sistemas, 13 (1), 520.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques for first-order MDPs.
Artificial Intelligence, 173 (56), 748788. Advances in Automated Plan Generation.
Srivastava, S., Immerman, N., & Zilberstein, S. (2009). Abstract planning with unknown
object quantities and properties. In Proceedings of the Eighth Symposium on Abstraction, Reformulation and Approximation (SARA-09), pp. 143150.
St-Aubin, R., Hoey, J., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. In Proceedings of Conference on Neural Information
Processing Systems, pp. 10891095.
Steinkraus, K. A. (2005). Solving Large Stochastic Planning Problems using Multiple Dynamic Abstractions. Ph.D. thesis, Department of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology.

522

fiJournal of Artificial Intelligence Research 43 (2012) 1-42

Submitted 08/11; published 01/12

Learning and Reasoning with Action-Related Places
for Robust Mobile Manipulation
Freek Stulp

stulp@clmc.usc.edu

Computational Learning and Motor Control Lab
University of Southern California
3710 S. McClintock Avenue, Los Angeles, CA 90089, USA

Andreas Fedrizzi
Lorenz Mosenlechner
Michael Beetz

fedrizza@cs.tum.edu
moesenle@cs.tum.edu
beetz@cs.tum.edu

Intelligent Autonomous Systems Group
Technische Universitat Munchen
Boltzmannstrae 3, D-85747 Garching bei Munchen, Germany

Abstract
We propose the concept of Action-Related Place (ARPlace) as a powerful and flexible representation of task-related place in the context of mobile manipulation. ARPlace
represents robot base locations not as a single position, but rather as a collection of positions, each with an associated probability that the manipulation action will succeed when
located there. ARPlaces are generated using a predictive model that is acquired through
experience-based learning, and take into account the uncertainty the robot has about its
own location and the location of the object to be manipulated.
When executing the task, rather than choosing one specific goal position based only
on the initial knowledge about the task context, the robot instantiates an ARPlace, and
bases its decisions on this ARPlace, which is updated as new information about the
task becomes available. To show the advantages of this least-commitment approach, we
present a transformational planner that reasons about ARPlaces in order to optimize
symbolic plans. Our empirical evaluation demonstrates that using ARPlaces leads to
more robust and efficient mobile manipulation in the face of state estimation uncertainty
on our simulated robot.

1. Introduction
Recent advances in the design of robot hardware and software are enabling robots to solve
increasingly complex everyday tasks. When performing such tasks, a robot must continually decide on its course of action, where a decision is a commitment to a plan or an
action parameterization based on evidence and the expected costs and benefits associated
with the outcome. (Resulaj, Kiani, Wolpert, & Shadlen, 2009). This definition highlights
the complexity of decision making. It involves choosing the appropriate action and action
parameterization, such that costs are minimized and benefits are maximized. The robot
must therefore be able to predict which costs and benefits will arise when executing an
action. Furthermore, due to stochasticity and hidden state, the exact outcome of an action
is not known in advance. The robot must therefore reason about expected outcomes, and be
able to predict the probability of different outcomes for a given action and action paramec
2012
AI Access Foundation. All rights reserved.

fiStulp, Fedrizzi, Mosenlechner, & Beetz

terization. Finally, a robot commits to decisions based on the current observable evidence,
represented in its belief state. So if the evidence changes, the rationale for committing to
a decision may no longer be valid. The robot therefore needs methods to efficiently reconsider decisions as the belief state changes during action execution, and possibly commit to
another plan if necessary.
Mobile manipulation is a good case in point. Even the most basic mobile manipulation
tasks, such as picking up an object from a table, require complex decision making. To pick
up an object the robot must decide where to stand in order to pick up the object, which
hand(s) to use, how to reach for it, which grasp type to apply, where to grasp, how much
grasp force to apply, how to lift the object, how much force to apply to lift it, where to
hold the object, and how to hold it. Such decision problems are complex as they depend on
the specific task context, which consists of many task-relevant parameters. Furthermore,
these decisions must be continually updated and verified, as the task context, or the robots
knowledge about the context, often changes during task execution.
Consequently, tasks of such complexity require not only robust hardware and low-level
controllers, but also a least-commitment approach to making decisions, abstract planning
capabilities, probabilistic representations, and principled ways of updating beliefs during
task execution. In this article, we demonstrate how implementing these core AI topics
contributes to the robustness and flexibility of our mobile manipulation platform.
The task-relevant decision we consider in this article is to which base position the robot
should navigate to in order to perform a manipulation action. This decision alone presents
several challenges, such as 1) successfully executing the reaching and manipulation action
critically depends on the position of the base; 2) due to imperfect state-estimation, there is
uncertainty in the position of the robot and the target object. As these positions are not
known exactly, but are fundamental to successfully grasping the object, it is not possible to
determine a single-best base position for manipulation; 3) the complete knowledge required
to determine an appropriate base position is often not available initially, but rather acquired
on-line during task execution.
Our solution idea to address these challenges is the concept of Action-Related Places
(ARPlace), a powerful and flexible representation of task-related place in the context of
mobile manipulation. ARPlace is represented as a probability mapping, which specifies the
expected probability that the target object will be successfully grasped, given the positions
of the target object and the robot

ARPlace : { P (Success|fkrob , hf obj , obj i) }K
k=1

(1)

Here, the estimated position of the target object is represented as a multi-variate Gaussian
distribution with mean f obj and covariance matrix obj 1 . The discrete set of robot positions
{fkrob }K
k=1 should be thought of as possible base positions the robot considers for grasping,
i.e. potential positions to navigate to. Typically, this set of positions is arranged in a grid,
as in the exemplary ARPlace depicted in Figure 1.
1. The feature vectors f rob and f obj contain the poses of the robot and the object relative to the tables
edge. Details will be given in Section 3.1. Positions without uncertainty are denoted f , and estimated
positions with uncertainty as hf , i.

2

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Figure 1: ARPlace: the probability of successful manipulation, given the current estiobj
rob K
, obj
mated object position hfcur
cur i. The set of potential robot positions {fk }k=1
are arranged in a grid along the x and y-axis, whereby each position leads to
obj
, obj
a different probability of successful grasping P (Success|fkrob , hfcur
cur i). The
black isolines represent grasp success probability levels of 0.2 and 0.8.

ARPlace has three important properties: 1) it models base places not as a single
position f rob , but rather as a set of positions {fkrob }K
k=1 , each with a different expectation
of the success of the manipulation action; 2) it depends upon the estimated target object
position, so updating f obj or obj during task execution thus leads to different probabilities
in ARPlace; 3) using a probabilistic representation that takes into account uncertainty in
the target object position leads to more robust grasping.
1.1 Example Scenario
In Figure 2, we present an example scenario that demonstrates how these properties
of ARPlace address the challenges stated above, and supports decision-making during
mobile manipulation. The images in the top row show the current situation of the robot
from an outside view, while the images in the lower visualize the robots internal ARPlace
representation. The ARPlace is visualized with the colors red, white and green, which
represent low, medium and high grasp success probabilities respectively. Grasp success
probability levels of 0.2 and 0.8 are depicted as isolines, as in Figure 1.
In this scenario the robots task is to clean the table. In Scene 1 the robot enters the
kitchen and its vision system detects a cup. Because the robot is far away from the cup,
the uncertainty arising from the vision-based pose estimation of the cup is high, indicated
by the large circle around the cup in the lower left image. As the exact position of the cup
is not known, it is not possible to determine a single-best base position for grasping the
cup. The ARPlace representation takes this uncertainty into account by modeling the
base position as a probability mapping.
3

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Scene 1

Scene 2

Scene 3

Scene 4

Figure 2: Example scenario.
In Scene 1 the ARPlace distribution has low probabilities overall, with a maximum
probability of grasp success of only 0.52. Note that although the initial uncertainty in the
cups position precludes the robot from determining a specific base position from which to
reliably grasp the cup, the robot does know the general area to which it should navigate.
During navigation, the robot is able to determine the position of the cup more accurately,
as depicted in Scene 2. As new sensor data comes in, the robot refines the ARPlace and
therefore the ARPlace in Scene 2 has much higher probabilities overall, with a maximum
of 0.96.
In Scene 3 the robot has detected a second cup. Because grasping both cups at once
from a single position is much more efficient than approaching two locations, the robot
merges the two ARPlaces for each cup into one ARPlace representing the probability
of successfully grasping both cups from a single position2 . In Scene 4 further measurements
helped to reduce pose estimation uncertainties of both cups. The maximum grasp success
probability in the ARPlace now reaches 0.97; sufficient for the robot to commit itself to
a goal position and attempt to grasp both cups at once.
This scenario illustrates that real-world tasks can often not be planned from start to
finish, as the initial knowledge is often not complete or accurate enough to determine
an optimal goal position. So rather than committing to a particular base position early
based only on the robots initial knowledge about the task context, a robot instantiates
an ARPlace for a particular task context, and bases its decisions on this ARPlace.
Having the place concept instantiation represented explicitly during the course of action
enables the robot to reconsider and reevaluate these decisions on-line whenever new
information about the task context comes in. For instance, the decision to grasp both
cups from one position in Scene 3 would not have been possible if the robot would have
committed itself to a plan given its initial knowledge when only one cup was detected. Even
if the environment is completely observable, dynamic properties can make a pre-planned
optimal position suboptimal or unaccessible. A least-commitment implementation, where
2. Section 4.4.1 explains how ARPlaces are merged to compute an ARPlace for joint tasks.

4

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

decisions are delayed until they must be taken is more flexible, and leads to more robust
and efficient mobile manipulation. This will be demonstrated in the empirical evaluation.
1.2 Contributions, System Overview and Outline
The system overview for learning, computing, and reasoning with ARPlaces is depicted
in Figure 3. It also serves as an outline for the rest of this article.

Figure 3: System Overview. Numbers refer to sections in this article. Green ovals represent
algorithms and procedures, and blue rectangles the models that result from them.
Procedures and models are briefly described in the contributions in this section;
more detail is given throughout the article. Yellow rectangles cluster conceptually
related procedures, and also delineate the different sections of this article.

The main contributions of this article are:
Representing ARPlace  Section 1. We propose ARPlace as a flexible representation
of place for least-commitment decision making in mobile manipulation.
Model Learning  Section 3. To generate an ARPlace, the robot must be able to
predict the outcome of an action for a given action parameterization. We propose
a generic, off-line learning approach to acquiring a compact prediction model in two
steps: 1) learn to predict whether an action will succeed for a given task parameterization. This is a supervised classification problem which we implement with Support
Vector Machines (Sonnenburg, Raetsch, Schaefer, & Schoelkopf, 2006); 2) generalize
over several task parameterizations by generalizing over the learned SVM classifiers,
5

fiStulp, Fedrizzi, Mosenlechner, & Beetz

which we implement with Point Distribution Models (Cootes, Taylor, Cooper, & Graham, 1995). The resulting success prediction model enables the robot to predict
whether a manipulation action for a given object position will succeed from a given
base position 3 .
Generating ARPlace Section 4. We demonstrate how ARPlaces are generated online, and take object position uncertainty into account through a Monte-Carlo simulation. Furthermore, the ARPlace is conditioned on robot position uncertainty, which
is thus also taken into account.
Reasoning with ARPlace Section 5. We show how ARPlace is integrated in a symbolic transformational planner, to automate decision-making with ARPlaces. In
particular, we consider a scenario that shows how ARPlaces can be merged for joint
manipulation tasks.
Empirical Evaluation  Section 6. We demonstrate how reasoning with ARPlaces
leads to more robust and efficient behavior on a simulated mobile manipulation platform.
Before turning to these contributions, we first compare our approach to related work in
Section 2.

2. Related Work
Most state-of-the-art mobile manipulation platforms use sampling-based motion planners
to solve manipulation problems (LaValle, 2006). Some of the advantages of using symbolic
planning in general, and with ARPlaces in particular, are: 1. Abstraction. Representing and planning with abstract symbolic actions reduces the complexity of the planning
problem. Although computational power is ever increasing, it is still intractable to solve
extended tasks, such as preparing a meal (Beetz et al., 2008), with state-space search alone.
2. Least-commitment. Friedman and Weld (1996) show that setting open conditions to
abstract actions and later refining this choice to a particular concrete action can lead to exponential savings. Note that this principle has also be used to reduce the number of collision
checks for building Probabilistic Roadmaps (Bohlin & Kavraki, 2000). 3. Modular replanning. In symbolic planning, causal links between the actions are explicitly represented. That
the robot navigates to the table in order to perform a grasping motion is not represented in
a plan generated by a sampling-based motion planner. Therefore, during execution, motion
planners cannot reconsider the appropriate base position as a decision in its own right, but
must rather inefficiently replan the entire trajectory if the belief state changes. 4. Reflection. The explicit symbolic representation of causality also allows a robot to reason about
and reflect its own plans and monitor their execution, for instance to report reasons for
3. By using experience-based learning, our approach can be applied to a variety of robots and environments.
Once a model has been learned however, it is obviously specific to the environment in which the experience
was generated. For instance, if only one table height is considered during data collection, the learned
prediction model will be specific to that table height. If different table heights are used during experience
collection, and the table height is included as a task-relevant parameter, the model should be able to
generalize over table heights as well. We refer to Section 3.5 for a full discussion.

6

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

plan failure: I could not find the cup. or I could not determine the position of the cup
with sufficient accuracy to robustly perform the grasp. or An obstacle was blocking my
path.. It is not obvious how to achieve such introspection with motion planning methods.
Also, in contrast to sampling based motion-planning, an ARPlace itself does not generate trajectories or motion itself, but is rather a representation that supports decisions, such
as the decision where the robot should move to in order to manipulate. This goal position
can then be given to a motion planner in order to find a trajectory that gets the robot to
the goal position. In our system for instance, the navigation trajectory is determined by
a Wavefront planner. Also, an ARPlace is not a Reinforcement Learning policy (Sutton
& Barto, 1998). A policy maps states to actions, whereas an ARPlace maps (uncertain)
states to expected probabilities of successfully executing a certain action. ARPlaces are
thus models of actions, and not executable actions in their own right. This distinction will
become most apparent in Section 5, in which ARPlaces are used by a transformational
planner to detect and repair performance flaws in symbolic plans.
From this perspective, most similar to our work are aSyMov (Cambon, Gravot, &
Alami, 2004) and RL-TOPs (Ryan, 2002), in that they use symbolic planners to generate
sequences of motion plans/reinforcement learning policies respectively. The specific contributions of this article are to enable the robot to learn grounded, probabilistic models of
actions to support symbolic decision making, as well as using more flexible transformational
planners that reason with these models. Our focus is thus more on grounding and improving
the representations that enable symbolic planning, rather than the underlying actions that
generate trajectories and/or the actual motion.
Okada, Kojima, Sagawa, Ichino, Sato, and Inaba (2006) also develop representations for
place to enable symbolic planning, and they denote a good base placement for grasping a
spot. Different spots are hand-coded for different tasks, such as manipulating a faucet,
a cupboard, and a trashcan. These symbolic representations of place are then used by a
LISP-based motion planner to perform tool manipulation behavior. ARPlace extends the
concept of a spot by learning it autonomously, grounding it in observed behavior, and
providing a probabilistic representation of place. Berenson, Choset, and Kuffner (2008)
address the issue of finding optimal start and goal configurations for manipulating objects
in pick-and-place operations. They explicitly take the placement of the mobile base into
account. As they are interested in the optimal start and goal configurations, instead of
a probabilistic representation, this approach does not enable least-commitment planning.
Diankov, Ratliff, Ferguson, Srinivasa, and Kuffner (2008) use a model of the reachable
workspace of the robot arm to decide where the robot may stand to grasp an object and to
focus the search. However, uncertainties in the robots base position or the objects position
are not considered, and thus cannot be compensated for. More recent work by Berenson,
Srinivasa, and Kuffner (2009) addresses these issues, but still relies on an accurate model
of the environment, and at a high computational cost. On the other hand, ARPlace is a
compact representation that is computed with negligible computational load, allowing for
continuous updating.
Recently, similar methods to the ones presented in this article have been used to determine successful grasps, rather than base positions for grasping. For instance, Detry et al.
(2009) determine a probability density function that represents the graspability of specific
objects. This function is learned from samples of successful robot grasps, which are biased
7

fiStulp, Fedrizzi, Mosenlechner, & Beetz

by observed human grasps. However, this approach does not take examples of failed grasps
into account. As we shall see in Section 4, the distance between a failed and a successful
grasp can be quite small, and can only be determined by taking failed grasps into account.
Our classification boundaries in Section 3.2 are similar to Workspace Goal Regions, except
that our boundaries refer to base positions, whereas Workspace Goal Regions refer to grasp
positions (Berenson, Srinivasa, Ferguson, Romea, & Kuffner, 2009). Also, we generalize over
these boundaries with a Point Distribution Model, and use it to generate a probabilistic
concept of successful grasp positions.
Kuipers, Beeson, Modayil, and Provost (2006) present a bootstrapping approach that
enables robots to develop high-level ontologies from low-level sensor data including distinctive states, places, objects, and actions. These high level states are used to choose
trajectory-following control laws to move from one distinctive state to another. Our approach is exactly the other way around: given the manipulation and navigation skills of the
robot (which are far too high-dimensional to learn with trajectory-following control laws),
learn places from which these skills (e.g. grasping) can be executed successfully. Our focus
is on action and affordance, not recognition and localization. For us, place means a cluster
of locations from which I can execute my (grasping) skill successfully, whereas for Kuipers
et al. it refers to a location that is perceptually distinct from others, and can therefore be
well-recognized. Furthermore, their work has not yet considered the physical manipulation
of objects, and how this relates to place.
Learning success models can be considered as probabilistic pre-condition learning. Most
research in this field until now far focussed on learning symbolic predicates from symbolic
examples (Clement, Durfee, & Barrett, 2007; Chang & Amir, 2006; Amir & Chang, 2008).
These approaches have not been applied to robots, because the representations that are
learned are not able to encapsulate the complex conditions that arise from robot dynamics
and action parameterization. In robotics, the focus in pre-condition learning is therefore
rather on grounding pre-conditions in robot experience. A more realistic domain is considered by Zettlemoyer, Pasula, and Kaelbling (2005), where a simulated gripper stacks objects
in a blocks world. Here, the focus is on predicting possible outcomes of the actions in a completely observable, unambiguous description of the current state; our emphasis is rather on
taking state estimation uncertainty into account. Dexter learns sequences of manipulation
skills such as searching and then grasping an object (Hart, Ou, Sweeney, & Grupen, 2006).
Declarative knowledge such as the length of its arm is learned from experience. Learning
success models has also been done in the context of robotic soccer, for instance learning the
success rate of passing (Buck & Riedmiller, 2000), or approaching the ball (Stulp & Beetz,
2008). Our system extends these approaches by explicitly representing the regions in which
successful instances were observed, and computing a Generalized Success Model for these
regions.
An interesting line of research that shares some paradigms with ARPlaces, such as
learning the relation between objects and actions or building prediction models, are ObjectAction Complexes (OACs). Geib, Mourao, Petrick, Pugeault, Steedman, Kruger, and
Worgotter (2006) and Pastor, Hoffmann, Asfour, and Schaal (2009) present OACs that
can be used to integrate high-level artificial intelligence planning technology and continuous low-level robot control. The work stresses that, for a cognitive agent, objects and
actions are inseparably intertwined and should therefore be paired in a single interface. By
8

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

physically interacting with the world and applying machine learning techniques, OACs allow
the acquisition of high-level action representations from low-level control representations.
OACs are meant to generalize the principle of affordances (Gibson, 1977).

3. Learning a Generalized Success Model for ARPlace
In this section, we describe the implementation of the off-line phase depicted in Figure 3, in
which a Generalized Success Model (GSM) is learned. The goal is to acquire the function g
P (Success|f rob , f obj )

=

g(f rob , f obj ) 7 {0, 1}

(2)

which predicts the chance of a successful manipulation action, given the relative positions
of the robot and the object, which are stored in the feature vectors f rob and f obj respectively.
Note that during the off-line learning phase, these are known positions. Uncertainty in
positions is taken into account in the on-line phase, as described in Section 4.
Performing mobile manipulation is a complex task that involves many hardware and
software modules. An overview of these modules in our platform is described in Appendix A.
This overview demonstrates the large number of modules required to implement a mobile
manipulation platform. Many of these modules themselves are the results of years if not
decades of research and development within companies, research groups, and open-source
projects. The global behavior of the robot, e.g. whether it can grasp a cup from a certain
base position, depends on all of these modules, and the interactions between them. In some
cases, analytic models of certain modules are available (such as a Capability Map for the
arms workspace, Section 3.1). However, there is no general way of composing such models
to acquire a global model of the systems behavior during task execution. Therefore, we
rather learn this model from observed experience.
However, the component that computes ARPlaces requires exactly such a global model
to predict under which circumstances a manipulation action will fail or succeed. As attempting a theoretical analysis to model all foreseeable events and uncertainties about the world
is at best tedious and error-prone, and at worst infeasible, we therefore use experience-based
learning to acquire global models of the behavior. By doing so, the model is grounded in
the observation of actual robot behavior.
The off-line learning phase consists of three steps: 1) repeatedly execute the action
sequence and observe the result for N different target object positions; 2) learn N Support
Vector Machines classifiers for N specific cup positions 3) generalize over these N classifiers
with a Point Distribution Model.
3.1 Data Acquisition
The robot acquires experience by executing the following action sequence: 1) navigate to
a specified base position at the table; 2) reach for the cup; 3) close the gripper; 4) lift
the cup (Stulp, Fedrizzi, & Beetz, 2009a). In this action sequence, the task context is
determined by the following parameters 1) the pose to which the robot navigates to4 ; 2) the
4. Note that the navigation planner is parameterized such that the robot is always directly facing the table.
This is not a limitation of the planner, but rather a constraint that is added to make the behavior of

9

fiStulp, Fedrizzi, Mosenlechner, & Beetz

pose of the target object on the table. After execution, the robot logs whether the object was
successfully grasped or not. To efficiently acquire sufficient data, we perform the training
experiments in the Gazebo simulator (Gerkey, Vaughan, & Howard, 2003). The robot is
modeled accurately, and thus the simulator provides training data that is also valid for the
real robot. Examples of a failed and successful grasp are depicted in Figure 4.

Figure 4: Two experiment runs with different samples for the robot position. The navigatereach-grasp sequence in the upper row succeeds, but fails in the lower sequence.

The vector field controller we use to perform the reaching movement has proven to be
robust in a wide range of the robots workspace (Beetz et al., 2010). It also has a very
low computational load, is easy to debug, and can quickly be adapted to novel objects.
A disadvantage is that it occasionally gets stuck in local minima, after which the motion
must be restarted. Our probabilistic motion planner for the arm does not suffer from local
minima, but plan generation fails at the border of the workspace; even though the vector
field controller is able to grasp there. Every planner and controller has its advantages and
disadvantages, and there will always be sources of failure in the real world, especially for
complex embodied agents. This article aims at modelling those failures through experiencebased learning, and basing decisions on these models.
The feature space in which the data is collected is depicted in Figure 5. This coordinate
system is relative to the tables edge, and the position of the cup on the table. This
will enable us to apply the model that is learned from the data to different tables at
different locations in the kitchen, in contrast to our previous work (Stulp, Fedrizzi, &
Beetz, 2009b). From now on, we will refer to f obj = [xobj  obj ] as the observable taskrelevant parameters, which the robot observes but cannot influence directly. Here xobj
is the distance of the object to the table edge, and  obj is the angle between the object
orientation and the normal that goes through the table edge and the object, as depicted in
the physical robot more predictable; this makes the robot more safe, which is required to operate the
robot in human environments (cf. Figure 20). In principle, the methods in this paper could take this
orientation into account.

10

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Figure 5. f rob = [xrob y rob ] are the controllable action parameters, because the robot can
use its navigation system to change them.

Figure 5: Relative feature space used in the rest of this paper.
The robot gathers data for 16 target object poses, as depicted in Figure 6. The target
object poses are listed in the matrix Fobj . For a given target object position, we determine
a rectangular area which is our generous estimation of the upper bound from which the
robot can grasp the object. This rectangle is the same for all 16 object poses. Within
this rectangle a uniform grid of almost 200 positions, which are stored in the matrix Frob ,
is defined. Figure 6 depicts the results of data gathering for these positions. Here, the
markers represent the position of the robot base at the table. There are three types of
markers, which represent the following classes:
3.1.1 Theoretically Unreachable (Light Round Markers)
The cup cannot be grasped from many of the positions in the bounding rectangle simply
because the arm is not long enough. More formally, for these base positions, the kinematics
of the arm are such that no inverse kinematic solution exists for having the end-effector
at the position required to grasp the target object. We exploit analytic models of arm
kinematics to filter out base positions in the bounding rectangle from which the cup is theoretically unreachable. The analytic model we use is a capability map, which is a compiled
representation of the robots kinematic workspace (Zacharias, Borst, & Hirzinger, 2007).
Capability maps are usually used to answer the question: given the position of my base,
which positions can I reach with my end-effector? In this article, we use the capability map
to answer the inverse question: given the position of the target object (and therefore the
desired position of my end-effector), from which base positions can I reach this end-effector
position? In Figure 7, the answer to this question is visualized for a specific target object
position. The depicted area is a theoretical kinematic upper bound on the base positions
from which the robot can reach the target.
For each example base position in the bounding box, we use the capability map to
determine if the target object is theoretically reachable. If not, the corresponding base
position is labeled as a failure without executing the navigate-reach-grasp action sequence.
This saves time when gathering data. Another obvious theoretical bound we implemented
was that the robots distance to the table should be at least as big as the robots radius.
Otherwise the robot would bump into the table. Again, we labeled such base positions as
failures without executing them in order to save time.
11

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 6: Results of data acquisition for 16 target object poses, listed in the matrix Fobj .
Markers correspond to the center of the robot base. Green squares and red
circles represent successful and failed grasps respectively. Bright circles were not
executed as a successful grasp is deemed theoretically impossible by capability
maps. The dark green hulls are the classification boundaries (Section 3.2).

3.1.2 Practically Unreachable (Red Filled Round Markers)
The capability map only considers the theoretical reachability of a position, given the kinematics of the robots arm. It does not take self-collisions into account, or the constraints
imposed by our vector-field controller for reaching, or the specific hardware of our gripper,
and the way the gripper interacts with the target object. Red markers in Figure 6 represent
base positions that the capability map deems possible, but that lead to a failure while performing the reaching motion. Some causes for failure are: 1) bumping into the table due to
imprecision in the navigation routine 2) bumping into the cup before grasping it; 3) closing
the gripper without the cup handle being in it; 4) the cup slipping from the gripper 5) the
vector field controller getting caught in a local minimum.
One aim of this article is to demonstrate how such practical problems, that arise from
the interaction of many hard- and software modules, are properly addressed by experiencebased learning. Our approach is to use analytic models when they are available, but use
experience-based learning when necessary. By interacting with the world, the robot observes
12

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Figure 7: Inverse capability map for the right arm for a specific object position.
its global behavior, and learns the difference between what is possible in theory and what
works in practice.
3.1.3 Reachable (Green Square Markers)
These are base positions from which the robot was able to successfully grasp the cup. The
task execution is deemed successful when the cup is more than 10cm above the table when
the action sequence is completed, because this can only be the case if the robot is holding
it. We prefer such an empirical measure over for instance a force-closure measure, as the
latter requires accurate models of the object, which we do not always have. Furthermore,
it has been argued on theoretical grounds (Zheng & Qian, 2005), as well as demonstrated
empirically (Morales, Chinellato, Fagg, & del Pobil, 2004), that force-closure grasps may
not always lead to successful grasps in practice. Of course, force-closure may just as well
be used as a measure of successful grasping; the methods described in this article do not
depend upon this design choice.
The data acquisition yields a set of discrete robot and object positions, associated with
the resulting outcome of the manipulation action, being a success or a failure:
obj N
P (Success|{f rob }M
}j=1 ) = bi,j , with bi,j  {0, 1}
i=1 , {f

(3)

In this article, the number of sampled object positions N = 4  4 = 16 (i.e. the number
of graphs in Figure 6), and the number of sampled robot positions M = 11  17 = 187 (i.e.
the number of data points per graph in Figure 6).
In the remainder of this section, we first generalize over the M discrete robot positions
by training Support Vector Machines (Section 3.2), and then generalize over the N cup
positions with Point Distribution Models (Section 3.3)
3.2 Generalization over Robot Positions
In this step, we generalize over the discrete robot positions, and acquire a compact boolean
classifier that efficiently predicts whether manipulation will succeed:
13

fiStulp, Fedrizzi, Mosenlechner, & Beetz

P (Success|f rob , {f obj }N
j=1 )

=

gj=1...N (f rob ) 7 {0, 1}

(4)

This generalization is implemented as follows. A separate classifier gi=1...N is learned for
each of the N = 16 object poses, i.e. one classifier for each of the 16 data sets depicted in
Figure 4. To acquire these prediction models, we compute a classification boundary around
the successful samples with Support Vector Machines (SVM), using the implementation
by Sonnenburg et al. (2006), with a Gaussian kernel with =0.1 and cost parameter C=40.0.
As successful grasps are rarer, we weight them twice as much as failed grasp attempts.
Figure 6 depicts the resulting classification boundaries for different configurations of taskrelevant parameters as dark-green boundaries. Manipulation is predicted to succeed if the
robots base position lies within a boundary for a given target object pose  Fobj . The
accuracy of these learned classifiers is listed in Section 6.1.
3.3 Generalization over Object Positions
In the next step, we generalize over the discrete object positions:
P (Success|f rob , f obj )

=

g(f rob , f obj ) 7 {0, 1}

(5)

We do so by determining a low-dimensional set of parameters that allows us to interpolate
between the individual classification boundaries that the Support Vector Machines generate. This is done with a Point Distribution Model (PDM), which is an established method
for modelling variations in medical images and faces (Cootes et al., 1995; Wimmer, Stulp,
Pietzsch, & Radig, 2008). The result is one compact model that incorporates the individual boundaries, and is able to interpolate to make predictions for target object poses not
observed during training.
As input a PDM requires n points that are distributed over a contour. How these
landmarks are distributed is described in Appendix B. Given the landmarks on the classification boundaries, we compute a PDM. Although PDMs are most well-known for their
use in computer vision (Cootes et al., 1995; Wimmer et al., 2008), we use the notation
by Roduit, Martinoli, and Jacot (2007), who focus on robotic applications. First, the 16
boundaries of 20 2D points are merged into one 40x16 matrix H, where the columns are the
concatenation of the xrob and y rob coordinates of the 20 landmarks along the classification boundary. Each column thus represents one boundary. The next step is to compute
P, which is the matrix of eigenvectors of the covariance matrix of H. P represents the
principal modes of variation. Given H and P, we can decompose each boundary h1..16 in
the set into the mean boundary and a linear combination of the columns of P as follows
hk = H + P  bk . Here, bk is the so-called deformation mode of the k th boundary. This
is the Point Distribution Model. To get an intuition for what the PDM represents, the
first three deformation modes are depicted in Figure 8, where the values of the first, second
and third deformation modes (columns 1, 2, 3 of B) are varied between their maximal and
minimal value, whilst the other deformation modes are set to 0.
The eigenvalues of the covariance matrix of H indicate that the first 2 components already contain 96% of the deformation energy. For reasons of compactness and to achieve
14

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Figure 8: The first 3 deformation modes of the Point Distribution Model (in B).

better generalization, we use only the first 2 deformation modes, without losing much accuracy.

{fjobj , gj (f rob ) = inboundary(f rob , hj )}N
j=1
{fjobj , gj (f rob ) = inboundary(f rob , H + P  bj )}N
j=1
g(f

rob

,f

obj

) = inboundary(f

rob

, H + P  b(f

obj

))

N Support Vector Machines

(6)

Point Distribution Model

(7)

Regression between

fjobj

and bj

(8)

The PDM has several advantages: 1) instead of having to store N = 16 classification
boundaries hj with each 20 2D points to capture the variation in classification hulls due to
different target object positions, we only store N = 16 deformation modes with 2 degrees of
freedom each. This greatly reduces the dimensionality; 2) the 2 degrees of freedom in b can
be used to interpolate in a principled way between the computed classification boundaries
hj , to generate boundaries for object positions that were not observed during learning; 3) a
simple regression between the two degrees of freedom of the PDM b and the position f obj is
feasible, so that the object position can be related directly to the shape of the classification
boundary. This regression is explained in the next section.
3.4 Relation to Task-Relevant Parameters
In this step, we acquire a function b, that computes the appropriate deformation modes b
for a given object position f obj . To do so, we compute a regression between the matrix of
deformation modes of the specific object positions B, and the 16 object positions themselves
in Fobj , as depicted in Figure 6. We found that a simple second order polynomial regression
model suffices to compute the regression, as it yields high coefficients of determination of
R2 = 0.99 and R2 = 0.96 for the first and second deformation modes respectively. The
coefficients of the polynomial model are stored in two 3x3 upper triangular matrices W1
and W2 , such that B  [ diag([T 1]  W1  [Fobj 1]T ) diag([T 1]  W2  [T 1]T ]
The Generalized Success Model now consists of 1) H, the mean of the classification
boundaries computed with the SVM; 2) P, the principal modes of variation of these clas15

fiStulp, Fedrizzi, Mosenlechner, & Beetz

sification boundaries; 3) W1,2 , the mapping from task-relevant parameters to deformation
modes.
Let us now summarize how the Generalized Success Model is used to predict successful
manipulation behavior:
1. The Generalized Success Model takes the (observed) relative position of the object on
obj
obj
the table fcur
= [xobj
cur cur ] as input (Figure 5).
2. The appropriate deformation values for the given object position are computed with
obj
obj
obj
bcur = b(fcur
) = [ q  W1  qT q  W2  qT ], where q = [fcur
1] = [xobj
cur cur 1]
(Section 3.4).
3. The boundary is computed with hcur = H + P  bcur (Section 3.3).
4. If the relative robot base center f rob = [xrob y rob ] is within boundary hcur , the
model predicts that the robot will be able to successfully grasp the object at position
obj
obj
= [xobj
fcur
cur cur ].
Note that these steps only involve simple multiplications and additions of small matrices,
and thus can be performed very efficiently5 . The reason for this efficiency lies in the fact that
we directly relate task-relevant parameters, such as the position of the cup on the table, to
predictions about the global behavior of the robot, such as whether the manipulation action
will succeed or not. The on-line efficiency is made possible by experience-based learning,
where the wealth of information in the observation of global behavior is compiled into a
compact model off-line. This approach adheres to the proposed strategy of learning taskrelevant features that map to actions, instead of attempting to reconstruct a detailed model
of the world with which to plan actions (Kemp, Edsinger, & Torres-Jara, 2007).
In summary, from observed behavior outcomes, we have learned the mapping in Equation 2 (which we repeat in Equation 9), which maps continuous robot and target object
positions to a boolean prediction about the success of an action:
P (Success|f rob , f obj ) = g(f rob , f obj ) 7 {0, 1}

(9)

This mapping can be used to predict if the current base position of the robot will lead to
successful manipulation, but also to determine appropriate base positions to navigate to.
Equation 9 assumes that the true values of the robot and target object positions are
known to the robot. In Section 4, we discuss how uncertainties in the estimates of these
positions is taken into account during task execution.
3.5 Generality of Generalized Success Model
Before explaining how the Generalized Success Model is used to generate ARPlaces online during task-execution in Section 4, we discuss some of the generalization properties
and limitations of the Generalized Success Model. To do so, we must distinguish between
5. As an indication, with the model that is described in this paper all four steps take 0.2ms on a 2.2GHz
machine in our Matlab implementation.

16

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

the general applicability of our approach to different robots, objects and domains, and the
specificity of the model to these factors once it has been learned. This essentially holds for
any data-driven approach: the model can in principle be learned for any data, independent
of the robot system that generates these data, but once learned, will be specific to the
data generated by that robot system, and thus specific to the robot system itself. For all
practical purposes, we assume that the domain and robot hardware remain fixed, so learning
a domain- and robot-specific model is not a grave limitation.
3.5.1 Generalization over Object Poses
The learned model generalizes over different object poses, as the relative object pose on the
table f obj = [xobj  obj ] is part of the feature space with which the Generalized Success
Model is parameterized (see step 1. in calling a GSM in Section 3.4). The Generalized
actually refers to this capability of generalizing over Success Models for specific object poses.
3.5.2 Grasp-Specific ARPlaces
By being specific to the object, a lot of data would be required to learn an ARPlace for
each object the robot should manipulate. In practice however, we found that only a few
grasps suffice to grasp most everyday objects in kitchen environments with the real robot
platform (Maldonado, Klank, & Beetz, 2010). In particular, this approach required only
2 grasps (one from the top and one from the side) to achieve 47 successful grasps out of
51 attempts with 14 everyday kitchen objects. Therefore, we propose to use grasp-specific
ARPlaces, rather than object-specific ARPlaces. We have learned Generalized Success
Models for both grasps, which are depicted in Figure 9.

Figure 9: The two Point Distribution Models for the side and top grasp. Examples of
objects that can be manipulated with these grasps are depicted.

The two deformation modes for the Point Distribution Model depicted in Figure 9
already contain 99% of the deformation energy, which is even more than for the side grasps.
This is because the success of the side grasp is relatively independent of the orientation of
the object, as the robot does not need to reach around the object. This also leads to more
symmetric classification boundaries for the top grasp, as can be seen in Figure 9.
In summary, only two Generalized Success Models must be learned for two different
grasps, as these two grasps suffice to grasp the 14 everyday kitchen objects that were tested
17

fiStulp, Fedrizzi, Mosenlechner, & Beetz

with the real robot by Maldonado et al. (2010). In the rest of this article, we will focus on
the side grasp; ARPlaces for top grasps are presented by Fedrizzi (2010).

4. Computing Action-Related Places
In the previous section, we demonstrated how the Generalized Success Model is learned
from observed experience for a variety of task parameterizations. The resulting function
maps known robot and object positions to a prediction whether the action execution will
succeed or feel.
In this section, we describe how ARPlaces for manipulation are computed on-line
for specific task contexts. As depicted in Figure 3, this module takes the Generalized
Success Model and the estimated robot pose and target object pose as input, and returns
an ARPlace such as depicted in Figure 1.
4.1 Taking Object Position Uncertainty into Account
In Equation 9, the prediction whether a manipulation action succeeds or fails is based on
known robot and target object positions. However, during task execution, the robot only
has estimates of these positions, with varying levels of uncertainty. These uncertainties
must be taken into account when predicting the outcome, as a manipulation action that
is predicted to succeed might well fail if the target object is not at the position where the
robot expects it to be. Given the Generalized Success Model in Equation 9, the goal of this
section is therefore to compute the mapping

Generalized Success Model

P (Succ|f

rob

,f

obj

ARPlace (with object uncertainty)

) 7 {0, 1}  Monte Carlo  { P (Succ|fkrob , hf obj , obj i) }K
k=1 7 [0, 1]
(10)

which takes estimates of the target object position, and returns a continuous probability
value, rather than a discrete {0, 1} probability value as in Equation 9. In our belief state,
the uncertainties in object positions are modelled as a Gaussian distribution with mean f obj
and covariance matrix obj .
On the robot platform described in Appendix A, f obj and obj are obtained from a
vision-based object localization module (Klank, Zia, & Beetz, 2009). Typical values along
2
2
2
the diagonal of the 6x6 covariance matrix are: x,x
= 0.05, y,y
= 0.03, z,z
= 0.07,
2
2
2
yaw,yaw = 0.8, pitch,pitch = 0.06, roll,roll = 0.06. The uncertainties in position are specified
in meters and the angular uncertainties are specified in radians. The estimation of the object
position is quite accurate, but our vision system has problems to detect the handle, which is
important for estimating the orientation (yaw) of the cup. Due to the constraints enforced
by our assumption that the cup is standing upright on the table, the uncertainty in z, pitch
and roll is set to 0. The remaining 3x3 covariance matrix is mapped to the relative feature
space, which yields obj .
At the end of Section 3.4, we demonstrated how a classification boundary hnew is reobj
obj
constructed, given known task relevant parameters fnew
= [ xobj
new new ]. Because of the
obj
uncertainty in fnew , it does not suffice to compute only one classification boundary given
18

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

the most probable position of the cup as the ARPlace from which to grasp. This might
lead to a failure if the cup is not at the position where it was expected. Therefore, we use
a Monte-Carlo simulation to generate a whole set of classification boundaries. This is done
by taking 100 samples from the Gaussian distribution of the object position, given its mean
position and associated covariance matrix. This yields a matrix of task-relevant parameters
obj  obj ]. The corresponding classification boundaries are computed
Fobj
s
s
s=1...100 = [x
for the samples with hs = H + P  b(fsobj )) from Equation 8. In Figure 10(a), 20 out of
the 100 boundaries are depicted. These were
the task-relevant
parameters
 h
 2 generated with
i
 obj obj
 2 obj obj
2
x

0.03
0
=
.
f obj = [ xobj obj ] = [ 0.2 1.5 ] and obj = 2x x
2
2
0 0.30

 obj xobj

 obj  obj

(a) Sampled classification boundaries (b) Discretized relative sum (c) Final distribution, after condi(hs=1..20 ).
of the boundaries.
tioning on the robot pose uncertainty.

Figure 10: Monte-Carlo simulation of classification boundaries to compute ARPlace.
As described in Appendix C, y is 0 by definition, as FGSM is defined relative to the
cups position along the tables edge. That is why the uncertainty in y, described by
2 , leads to an uncertainty in the origin of F
y,y
GSM . Therefore, when sampling from the
task-relevant parameters, we also sample values of y, and translate FGSM accordingly. Uncertainty in y does not influence the shape of the classification boundary through the PDM,
it simply translates the classification boundary along the tables edge. This sampling of
y has actually already been done in Figure 10(a), where yobj yobj = 0.03. So in fact,
 2

 obj obj
 2 obj obj
 2 obj obj


x
x
x
y
x

0.032 0
0
2
2
2
obj





2
 =
=
0 0.03
0
y obj xobj
y obj y obj
y obj  obj
2

 obj xobj

2

 obj y obj

0

2

 obj  obj

0

0.302

After having computed the sampled classification boundaries, we then generate a discrete
grid of 2.52.5cm cells, which represent the discrete robot positions {fkrob }K
k=1 in Equation 1.
For each cell, the number of classification boundaries that classify each cell as a success is
counted. We are thus computing a histogram of predicted successful grasps. Dividing the
result by the overall number of boundaries yields the probability that grasping the cup will
succeed from this position. The corresponding distribution, which takes the uncertainty of
the cup position into account, is depicted in Figure 10(b).
It is interesting to note the steep decline on the right side of the distribution near the
table, where the probability of a successful grasp drops from 0.8 to 0.2 in about 5cm. This
is intuitive, as the table is located on the right side, and the robot bumps into the table
when moving to the sampled initial position, leading to an unsuccessful navigate-reach19

fiStulp, Fedrizzi, Mosenlechner, & Beetz

grasp sequence. Therefore, none of the 16 boundaries contain the area that is close to the
table, and the variation in P on the right side of the PDM is low. Variations in B do not
have a large effect on this boundary, as can be seen in Figure 10(b). When summing over
the sampled boundaries, this leads to a steep decline in success probability.
Note that an ARPlace is not a normalized probability distribution (which sums to 1),
but rather a probability mapping, in which each element (discrete grid cell) is a probability
distribution itself. Thus the sum of probabilities in each grid cell is 1, i.e. P (Succ) +
P (Succ) = 1.
4.2 Taking Robot Position Uncertainty into Account
The robot not only has uncertainty about the position of the target object, but also about its
own position. This uncertainty must also be taken into account in ARPlace. For instance,
although any position near to the left of the steep incline in Figure 10(b) is predicted to
be successful, they might still fail if the robot is actually more to the right than expected.
Therefore, we condition the probabilities in Figure 10(b) on the robot actually being at a
 rob , y
 rob )6 , and acquire the
certain grid cell (xrob , y rob ) given its position estimate (x
final ARPlace mapping as:
ARPlace prob. mapping, Figure 10(c).
P (Success|hf rob , rob i, hf obj , obj i) =
P (Success|f rob , hf obj , obj i)

P (f rob |hf rob , rob i)
(11)

Prob. mapping Equation 10, Figure 10(b).

Prob. distribution robot uncertainty (Gaussian).

In this equation, hf rob , rob i can be interpreted in two ways. First of all, it can represent
the actual estimate of the robots position at the current time. In this case, P (Success| . . . )
predicts the probability of success when manipulation from the current position. However,
it can also be interpreted as possible goal positions the robot could navigate to in order to
rob , rob i, as we do throughout this paper. In doing so, we
perform the navigation, i.e. hfgoal
goal
rob
make the assumption that the future position uncertainty rob
goal at the goal position fgoal is
rob . We believe this is a fair assumption because;
the same as it is currently, i.e. rob
goal = 
1) it is more realistic than assuming rob
goal = 0; 2) as the robot approaches the navigation
rob
goal, it is continually updating  , and thus P (Success| . . . ). Once it has reached the
rob .
goal, rob
goal will be equivalent to 
4.3 Refining ARPlace On-line
In summary, ARPlaces are computed on-line with a learned Generalized Success Model,
given the task-relevant parameters of the current task context, which includes uncertainties
6. Since the navigation planner is parameterized such that the robot always faces the table (cf. Section 3.1),
we have ignored the orientation of the robot in computing the GSM. Note that we therefore also ignore
the uncertainty in this parameter here, and ARPlaces do not take it into account. We expect that the
improved robustness (evaluated in Section 6.2) could be further improved by taking (the uncertainty) in
this parameter into account.

20

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

in the poses of the robot and target object. This yields a probability mapping that maps
robot base positions to the probability that grasping the target object will succeed.
Learning the Generalized Success Model is a costly step which involves extensive data
collection, and thus is performed off-line. Once learned, this model is very compact, and
is used to efficiently compute ARPlaces on-line7 Therefore, ARPlaces can be updated
as the execution of the task progresses, and can incorporate new knowledge about taskrelevant parameters or changes in the environment. Figure 11 depicts how the ARPlace
probability mapping is affected as new knowledge about task-relevant parameters comes
in. The first row demonstrates how more accurate knowledge about the target objects
position (lower uncertainty, e.g. lower xobj xobj ) leads to a more focussed ARPlace
with higher overall probabilities, and a higher mode. The second and third row depict
similar effects when estimates of the target objects position and orientation change. This
figure serves two purposes: it gives the reader a visual intuition of the effects of several taskrelevant parameters on the shape of the ARPlace, and it demonstrates how the robots
internal ARPlace representation might change as new (more accurate) information about
the target object pose comes in.
Decreasing uncertainty in cup position perpendicular to table edge
xobj =

0.28

xobj =

0.38

 obj =

0.60

0.19

0.10

0.01

Decreasing distance of cup to table edge
0.33

0.23

0.13

Changing orientation of cup on table
1.30

1.95

2.65

Figure 11: These images demonstrate how varying certain task-relevant parameters affects
the shape of the ARPlace distribution.
The decision whether a certain probability of success suffices to execute the manipulation
action critically depends on the domain and task. Failing to grasp a full glass of wine has
7. As an indication, it takes on average 110ms on a 2.2GHz machine in our Matlab implementation to
perform the steps in Section 4.1 and 4.2.

21

fiStulp, Fedrizzi, Mosenlechner, & Beetz

more grave consequences than failing to grasp a tennis ball. In general, ARPlace provides
a representation which enables high-level planners to make rational decisions about such
scenarios, but does not specify how such decisions should be made, or what the minimal
success probability should be in order to perform the task. In Section 5 we present the use
of ARPlace in a concrete scenario.
4.4 Generality of ARPlaces
In Section 3.5, we discussed the generality of learning the Generalized Success Model, and
the specificity of the model with respect to the robot and its skills, once the model has
been learned off-line. In this section, we demonstrate the generality and flexibility of the
ARPlace representation, which is generated on-line using the Generalized Success Model.
We also present various ways in which ARPlaces can be extended, and lay the groundwork for Section 5, which explains how ARPlaces are used in the context of a high-level
transformational planner.
4.4.1 Merging ARPlaces for Multiple Actions
ARPlaces for multiple actions can be composed by intersecting them. Assume we have
computed ARPlaces for two different actions (a1 and a2 ). If the success probabilities of
the ARPlaces is independent, we can compute the ARPlace for executing both actions
in parallel by multiplying the probabilities of the ARPlaces for action a1 and a2 .
In the first two graphs of Figure 12 for instance, the ARPlaces for grasping a cup with
the left and right gripper are depicted. With a piecewise multiplication of the probabilities,
we acquire the merged ARPlace, depicted in the right graph. The robot can use this
merged ARPlace to determine with which probability it can use the left and right gripper
to grasp both cups from one base position (Fedrizzi, Moesenlechner, Stulp, & Beetz, 2009).
Another similar application is merging the ARPlaces for two cup positions, grasped with
the same gripper. This ARPlace represents the probability of being able to grasp a
cup from one position, and placing it on the other position, without moving the base. Such
compositions would be impossible if the robot commits itself to specific positions in advance.

Figure 12: Left distribution: grasp cup with left gripper. Center distribution: grasp cup
with right gripper. Right distribution (element-wise product of the other two
distributions): Grasp both cups with left/right gripper from one base position.

22

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

As navigating to only one position to grasp two cups is much more efficient than navigating to two positions, we have implemented this decision as a transformation rule in the
Reactive Planning Language (McDermott, 1991), which is described in detail in Section 5.
4.4.2 Different Supporting Planes
Defining the feature space of the Generalized Success Model relative to the tables edge
allows the robot to compute ARPlaces for more general table shapes than the one presented so far. This is done by determining an ARPlace for each of the straight edges of a
table, and computing the union of these individual ARPlaces. An example is depicted in
Figure 13.

Figure 13: An ARPlace for a more complex table shape.

4.4.3 Different Uncertainty Distributions
In this article, the uncertainty in the position of the robot and target objects is modelled
by a multi-variate Gaussian distribution. This is not because our approach expects such
a distribution, but because this is how our state estimation systems represent uncertainty.
In Section 4.1, we described how specific target object positions are sampled from this
distribution in a Monte Carlo simulation. In general, our method applies to any distribution
from which such a sampling can be done. These distributions need not be Gaussian, and
might well be multi-modal or even non-parametric. For a particle filter for instance, each
particle could directly be used as a sample to compute the classification boundaries as in
Figure 10(a).
4.4.4 Applicability to Other Domains
We demonstrate the generality of ARPlaces by briefly showing how an ARPlace is able
to represent a task-relevant place for a very different task and domain: approaching the ball
in robotic soccer. This task frequently fails because the robot bumps into the ball before
achieving the desired position at the ball. In Figure 14(a), examples of a successful (S)
and failed (F) attempt are depicted. Here, the robot should approach the ball from the
top. Our goal is to acquire an ARPlace that maps the robots position on the field to the
predicted probability that it will successfully approach the ball.
23

fiStulp, Fedrizzi, Mosenlechner, & Beetz

The procedure for learning an ARPlace is equivalent to that in the mobile manipulation
domain: 1) gather data and log successful and failed episodes (Figure 14(a)); 2) learn classification boundaries and a generalized success model from these data; 3) generate ARPlaces
for specific task contexts (Figure 14(b)). This example demonstrates that the ARPlace
approach is not limited to mobile manipulation, but generalizes to other actions and domains.

(a) Successful and failed
attempts at approaching
the ball.
Data taken
from (Stulp & Beetz,
2008).

(b) The robots ARPlace for approaching the ball. Green
plateau: high probability that the robot will succeed at approaching the ball at the orientation indicated by the thick
black arrow.

Figure 14: An ARPlace for the robot soccer domain.
Note the two bumps to the left and right of the ball. It is not intuitively clear why the
robot should succeed in approaching the ball from these locations, but not the surrounding
ones. We assume that it depends on the particular morphology of the robot, and the
controller used to approach the ball; both are described by Stulp and Beetz (2008). One
of the main advantages of using an approach based on learning is that our assumptions
and intuitions do not play a role in acquiring the model. Whatever the reason may be,
these successful approaches are obvious in the observed data (Figure 14(a)), and hence the
ARPlace represents them.
4.4.5 Using More General Cost Functions
In this article, the probability of success is considered the only utility relevant to determining
an appropriate base position. But in principle, ARPlace is able to represent any kind of
utility or cost, an example of which is given in Figure 15. Here, the task of the robot is to
collect one of the two cups on the table. The probabilistic ARPlaces for the two cups are
depicted in the left graph. Given these parameters, the chance of success is 0.99 for both of
the cups, so there is no reason to prefer fetching one over the other. However, cup B is much
closer to the robot, and therefore it would be more efficient to collect cup B. This preference
can be expressed with an ARPlace. First, we compute the distance of the robot to each
24

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

of the grid cells of the probabilistic ARPlace, as depicted in the center graph. Finally,
we merge the probability P and distance d into one cost u, with u = (1  P )5 + d/0.3.
This expresses that it takes on average 5 seconds to reposition the robot for another grasp
attempt in case of a failure, and that the average navigation speed is 0.3m/s. This cost
thus expresses the expected time the overall task will take8
As depicted in Figure 15, the mode of the ARPlace of the cup that is closer to the
robot is now higher, reflecting the fact that we prefer the robot to fetch cups that are closer.
For an in-depth discussion of utility-based ARPlaces, and how they affect the behavior of
the robot, we refer to Fedrizzi (2010).
Probability: P

Distance: d

Cost: u = (1  P )5 + d/0.3

Figure 15: Example of a more general cost-based ARPlace (right), including both the
probability of success (left) and distance to the robot (center). By including the
distance as part of the cost, the mode of the cost-based ARPlace for the closer
cup is higher than for the more distant cup.

5. Transformational Planning with ARPlace
So far, we have described how ARPlaces are generated on-line by using the learned Generalized Success Model. The ability to predict the (probability of an) outcome of an action
makes ARPlaces a powerful tool when combined with a high level planning system. In
this section, we demonstrate how ARPlace is used in the context of a symbolic transformational planner. Reasoning about ARPlace enables the planner to generate more robust
and efficient plans, and demonstrates the flexibility of the least-commitment ARPlace
representation.
In particular, we consider the task of retrieving two cups from a table. One action
sequence that solves this task is: Plan A: navigate to a location near cup1, pick up cup1
with the left gripper, navigate to a location near cup2, pick up cup2 with the right gripper,
as depicted in Figure 16. However, if the cups are sufficiently close to each other (as in
Figure 12, right), it is much more efficient to replace the plan above with Plan B: navigate
to a location both near cup1 and cup2, pick up cup1 with the left gripper, pick up cup2
with the right gripper, as it saves an entire navigation action.
8. This cost is chosen for its simplicity, to illustrate the generality of the ARPlace representation. More
realistic, complex cost functions can be used.

25

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 16: Improving performance through transformational planning with (merged)
ARPlace. Plan A: navigate to two separate poses for grasping each object,
using ARPlaces for both objects. Plan B: navigate to one pose for grasping
both objects, using the merged ARPlace.

Deciding whether to use two base locations (Plan A) or one (Plan B) is difficult to solve
in a control program without sacrificing generality. To keep our solution general, we do not
want to write two separate control programs for both options, and choose between them
with an if-then-else statement. That would mean we have to provide control programs and
choice points for every option a robot has. The space of choices is prohibitively large in
everyday tasks to allow such an approach. Instead, we use a transformational planner that
takes our general program (Plan A) and, if appropriate, applies generic transformation rules
that change the program locally (to yield Plan B). Our transformational planner consists
of the following components:
Plan projection. A projection mechanism for predicting the outcome of a plan. ARPlace
is a compact representation of such a projection mechanism, as it is able to predict
the probability of success of an action, given its parameters.
Flaw detection. A mechanism for detecting behavior flaws within the predicted plan outcome. Flaws are not only errors that hinder the robot from completing the task, but
they may also be performance flaws, such as suboptimal efficiency. Using two navigation actions to approach to cups that are close to each other (Plan A) is flawed, in
that it is much more efficient to navigate to one position that is close to both cups.
Plan transformation. A mechanism to fix the detected flaws by applying transformation
rules to the plan code. For the problem we consider, a local transformation rule is
applied to Plan A to yield the more efficient Plan B.
In the next sections, we will describe each of these mechanisms in more detail, and
explain how they are implemented to exploit the ARPlace representation. Note that in
this article, we use our transformational planner to exemplify how ARPlace can be used
in the context of a larger planning system. For more information on our transformational
planning framework, and further examples of behavior flaws and transformation rules, we
refer to the work of Mosenlechner and Beetz (2009).
26

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

5.1 Plan Design
To detect flaws and apply transformation rules for their repair, the transformational planner
must be able to reason about the intention of code parts, infer if a goal has been achieved or
not, and deduce what the reason for a possible failure was. To do so, our control programs
are written in rpl (McDermott, 1991), which provides functionality for annotating code
parts to indicate their purpose and make them transparent to the transformational planner.
For the purpose of this article, the most important rpl instructions for semantic annotation
in the context of pick-and-place tasks are achieve, perceive and at-location. A formal
definition of the semantics of these instructions is given by Mosenlechner and Beetz (2009);
here we describe them informally.
(achieve ?expression)  If the achieve statement executes successfully, the logical expression which is passed as its argument is asserted as being true. For instance, after a successful
execution of (achieve (entity-picked-up ?cup)), the object referenced by the variable ?cup
must be in the robots gripper9 .
(perceive ?object)  Before manipulating objects, the robot must find the objects and
instantiate them in its belief state. After successful execution, the statement (perceive ?cup)
asserts that the object referenced by ?cup has been found, and a reference to its internal
representation is returned.
(at-location ?location ?expression)  Manipulation implies the execution of actions at
specific locations. Therefore, it must be assured that pick-up actions are only executed
when the robot is at a specific location. (at-location ?location ...) asserts that code within
its context is either executed at the specified location or fails. Please note that transformations which affect the location where actions are performed directly modify the ?location
parameter of such at-location expressions. Therefore, at-location is the most important
declarative plan expression for optimizing ARPlaces. To specify locations for at-location,
we use so-called designators, symbolic descriptions of entities such as locations, objects and
actions. For instance, a designator for the location where to stand for picking up a cup
can be specified as follows: (a location (to pick-up) (the object (type cup))). This symbolic
description is then resolved by reasoning mechanisms such as ARPlaces and Prolog and an
actual pose is generated when it is needed. In general, an infinite number of poses provide a
valid solution for such a pose. ARPlace gives us a way to evaluate their utility and select
the best pose.
The declarative expressions explained above can be combined to form a tree. Every
achieve statement can contain several further achieve, perceive and at-location statements
as sub-plans. An example plan tree is sketched in Figure 17. In this tree, the goal (achieve
(entity-at-location ?object ?location)) first perceives the object, then picks it up by achieving
entity-picked-up, which executes the pick-up action within an at-location block, and puts
the object down by achieving entity-put-down, which also contains an at-location block.
As we shall see in Section 5.4, behavior flaws are repaired by applying transformation rules
that replace sub-trees within the plan tree by new code.

9. Please note the lisp syntax, where variables are prefixed with a  ?, for example ?cup, and predicates
and functions are pure symbols.

27

fiStulp, Fedrizzi, Mosenlechner, & Beetz

(entity-at-location ?o ?l)

(perceive ?o)

(achieve (entity-put-down ?o ?l))

(achieve (entity-picked-up ?o)
..
.
..
.

.. (at-location ?obj-l)
.

(at-location ?obj-l)
..
.
..
.

..
.

..
.

Figure 17: An example plan tree created by executing a pick-up plan
5.2 Plan Projection
A central component of a transformational planner is plan projection, which simulates the
behavior of the robot that arises when executing a plan. In our approach, plan projection generates a temporally ordered set of events based on the plan code presented in the
previous section. We use the same Gazebo based mechanism for projection that has been
used for generating the training data for learning ARPlaces. In particular, we use information about collisions, perception events and the locations of objects and the robot.
While executing the plan in simulation, we generate an extensive execution trace that is
then used in our reasoning engine that infers behavior flaws that are then fixed by transformation rules (Mosenlechner & Beetz, 2009). The execution trace contains low-level data
representing the position of all objects and the robot, as well as collisions between objects,
the visibility of objects for the robot, and information to reconstruct the state of program
throughout its execution.
ARPlaces are a very efficient way of performing plan projection, as they predict the
probability of a successful outcome without requiring on-line generation of execution traces.
The reason that execution trace sampling is not required on-line, is because the task has
already been executed frequently off-line during data acquisition (cf. Section 3.1). The
results of these task executions have been compiled into the ARPlaces by learning a
GSM, which yields a compact representation of the experience acquired. Therefore, this
experience must not be generated anew during plan generation.
5.3 Behavior Flaws and Reasoning about Plan Execution
Plan projection simulates the robot behavior when executing a plan. The second component of a transformational planner is a reasoning engine that finds pre-defined flaws in
the projected robot behavior. Examples of such flaws are collisions, e.g. caused by underparameterized goal locations, or blocked goals, e.g. when a chair is standing at a location
the robot wants to navigate to. The examples above are behavior flaws that lead to critical
errors in plan execution (i.e. the plan fails), but we also consider behavior that is inefficient
to be flawed (i.e. the plan succeeds, but is unnecessarily inefficient). The task we consider
in this paper is an example of such a performance flaw, as performing two navigation actions where only one is required is highly inefficient. Behavior flaws are specified using a
28

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Prolog-like reasoning engine that is implemented in Common Lisp (Mosenlechner & Beetz,
2009).
The execution trace generated by plan projection is transparently integrated into the
reasoning engine, i.e. the execution trace is queried using Prolog predicates. The information recorded in the execution trace is valuable information in order to find behavior flaws.
Additional information that is used to find behavior flaws is a set of facts that model the semantics of declarative expressions such as achieve or at-location and concepts of the world,
for instance that objects are placed on supporting planes (table, cup-board, ...). To find
behavior flaws, their Prolog specifications are matched against the logical representation of
the execution trace and if solutions are found, the corresponding flaw is present in the plan
and can be fixed.
For instance, the code to match two locations to perform actions that can be merged to
one ARPlace looks as follows:
Listing 1: Flaw definition to match two different pick-up tasks.
1
2
3
4
5

( and
( t a s k g o a l ? t a s k 1 ( a c h i e v e ( e n t i t y p i c k e d up ? o b j e c t 1)))
( t a s k g o a l ? t a s k 2 ( a c h i e v e ( e n t i t y p i c k e d up ? o b j e c t 2))) ( t h n o t
(== ? t a s k 1 ? t a s k 2)) ( o p t i m i z e d a c t i o n l o c a t i o n ? o b j e c t 1
? o b j e c t 2 ? o p t i m i z e d l o c a t i o n ) )

The code above first matches two different pick-up tasks. The predicate optimizedaction-location holds for ?optimized-location being an ARPlace from which the two objects
can be picked up. To bind this variable, the predicate is implemented to calculate such an
ARPlace.
Another example for such a flaw definition is failed navigation, i.e. if the robot is not
standing at the location it was supposed to drive to:

Listing 2: Flaw definition to find locations that were not reached by the robot although it
was told to reach them.
1
2
3
4
5

( and
( t a s k g o a l ? t a s k ( a c h i e v e ( l o c Robot ? g o a l l o c ) ) )
( t a s k s t a t u s ? t a s k Done ? t )
( h o l d s ( l o c Robot ? r o b o t l o c ) ( a t ? t ) )
( not (== ? g o a l l o c ? r o b o t l o c ) ) )

The code above first matches the code that is navigating the robot to the location ?goalloc. Then it infers the actual location of the robot when the navigation task terminated
and binds it to the variable ?robot-loc and finally asserts that the two locations are not
equal. If this Prolog expression can be proven against an execution trace, we have found a
flaw indicating an unachieved goal location.
5.4 Plan Transformations and Transformation Rules
After a behavior flaw has been detected, the last step of a planner iteration is the application
of a transformation rule to fix the behavior flaw. Transformation rules are applied to parts
of the plan tree and cause substantial changes in its structure and the corresponding robot
behavior.
29

fiStulp, Fedrizzi, Mosenlechner, & Beetz

A transformation rule consists of three parts. The input schema is matched against the
plan part that has to be transformed and binds all required code parts to variables in order
to reassemble them in the output part. The transformation part performs transformations
on the matched parts, and the output plan describes how the new code of the respective
plan part has to be reassembled.
input schema
transformation
output plan
Besides the integration of ARPlace into the robot control program through at-location
statements, ARPlace is also integrated into the reasoning engine of our transformational
planner. Using two locations for grasping is considered a performance flaw if one location
would suffice. Informally, we investigate the execution trace for the occurrence of two different pick-up actions, where one is executed at location L1 , and the other one is executed
at location L2 . Then we request a location L3 to perform both actions and the corresponding success probability. L3 is computed by merging the ARPlace as in Figure 12.
If the probability of success of the merged ARPlace is sufficiently high, we apply a plan
transformation, and replace locations L1 and L2 with location L3 .
The transformation rule for optimizing ARPlaces is shown in Listing 3. Please note
that all variables that have been bound while matching the flaw definition are still bound
and can be used in the transformation rule.
Listing 3: Transformation rule for fixing the flaw.
1
2
3
4
5
6
7
8
9
10
11

( d e f t r r u l e f i x unoptimized l o c a t i o n s
: i n p u t schema
( ( and ( t a s k g o a l ? l o c a t i o n t a s k 1
( atl o c a t i o n ( ? l o c a t i o n 1) . ? code 1))
( subt a s k ? l o c a t i o n t a s k 1 ? t a s k 1))
( and ( t a s k g o a l ? l o c a t i o n t a s k 2
( atl o c a t i o n ( ? l o c a t i o n 2) . ? code ) )
( subt a s k ? l o c a t i o n t a s k 2 ? t a s k 2)))
: outputp l a n
( ( atl o c a t i o n ( ? o p t i m i z e d l o c a t i o n ) . ? code 1)
( atl o c a t i o n ( ? o p t i m i z e d l o c a t i o n ) . ? code 2)))

The input schema of the code above consists of two similar patterns, each matching the
at-location sub-plan of the pick-up goals matched in the flaw. The planner replaces the
matching code parts by the corresponding entries of the output plan. In our transformation
rule, the location that has been passed to at-location is replaced by the optimized location
that has been calculated in the flaw definition.
Our behavior flaw is defined to match two different pick-up executions. Then an
ARPlace query is performed to find out the probability for successfully grasping both
objects from one location. If the probability is sufficiently high (> 0.85) the Prolog query
succeeds, i.e. the flaw is detected only if a sufficiently good location for grasping both
objects can be found. Note that sufficiently high depends very much on the scenario
context. In robotic soccer it can be beneficial to choose fast and risky moves, whereas
in safe human-robot interaction, certainty of successful execution is more important than
mere speed. This article focusses on principled ways of integrating such thresholds in a
transformational planner, and relating them to grounded models of the robots behavior.
30

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

What these thresholds should be, and how they are determined, depends on the application
domain and the users.

6. Empirical Evaluation
In this section we 1) determine how many samples are needed to learn an accurate SVM
classifier; 2) compare the robustness of our default strategy for determining base positions
with a strategy that uses ARPlaces; 3) compare the efficiency of plans with and without
fixing performance flaws with our transformational planner; 4) present preliminary results
on the physical robot platform.
6.1 Classification Accuracy and Training Set Size
Figure 18 depicts the accuracy of the SVM classifier for predicting which base positions will
lead to successful grasps for one particular cup position, evaluated on a separate test set
with 150 samples. Without using the capability map to filter out kinematically impossible
base positions, the graph levels off after about 300 examples10 . By filtering out theoretically
impossible base positions with the capability map, the classifier achieves the same accuracy
within 173 examples (Stulp et al., 2009).

Figure 18: Accuracy dependent on training set size for one cup position.
The effect is more dramatic for the entire dataset containing the data for 16 different cup
positions. By applying the capability map, the number of trials that need to be executed
reduces from 2992 (all markers in Figure 6) to 666 (only red/green filled markers in Figure 6).
As the capability map only reduces unsuccessful attempts, it has no influence on the final
classification accuracy, which is 94%.
10. This graph applies to another dataset described by Stulp, Fedrizzi, Zacharias, Tenorth, Bandouch, and
Beetz (2009), which is very similar to the one used in the rest of this article.

31

fiStulp, Fedrizzi, Mosenlechner, & Beetz

6.2 Results from the Simulated Robot
We now compare the robustness of navigation based on probabilistic ARPlaces with a
strategy based on deterministic navigation goals. In this evaluation, the position to which
the robot navigates is the position for which ARPlace returns the highest probability
that grasping the target object will succeed. We compare this strategy to our previous
hand-coded implementation Fixed, which always navigates to a location that has the same
relative offset to the target object, whilst at the same time taking care not to bump into
the table.
In these experiments, we vary the position of the cup (xobj , obj ), as well as the
uncertainties the robot has about its own position and the position of the cup, by varying
the diagonal elements of the covariance matrices associated with the position of the robot
(xrob xrob ,yrob yrob ) and the cup (xobj xobj ,obj obj ). For each combination of
these variables, the robot performs the navigate-reach-grasp-lift sequence. The result is
recorded, just as during data acquisition for learning the Generalized Success Model. To
simulate the uncertainty, we sample a specific perceived robot and cup position from the
distribution defined by their means and covariance matrices. The result of the action is
determined by the true simulated state of the world, but the robot bases its decisions on
the perceived samples.
The results of this evaluation are summarized in the three bar plots in Figure 19, which
depict the success ratios of the ARPlace-based and Fixed strategies. Each ratio is the
number of successful executions, divided by the number of examples, which is 100. The pvalue above each pair of bars is computed with a 2 test between them, which tests whether
the number of successful and failed attempts is sampled from the same distribution for
ARPlace and Fixed.
The first graph depicts the success ratios for increasing uncertainty about the object
position (i.e. xobj xobj = [ 0.00 0.05 0.10 0.15 0.20 ]), for fixed robot position uncertainty xrob = 0.05. In all cases, the ARPlace strategy significantly outperforms the
Fixed strategy. Furthermore, the performance of ARPlace is much more robust towards
increasing object position uncertainty, as ARPlace takes this explicitly into account.
The same trend can be seen when increasing the uncertainty in the robot position (i.e.
xrob xrob = yrob yrob = [ 0.00 0.05 0.10 0.15 0.20 ]), for fixed object position uncertainty
xobj = 0.05. However, when xrob xrob > 0.1 the difference between ARPlace and
Fixed is no longer significant.
Finally, the last graph depicts the success ratios when increasing both robot and object
uncertainty. Again, ARPlace significantly outperforms Fixed when ( < 0.15). If the
robot is quite uncertain about its own and the objects position ( > 0.15), grasp success
probabilities drop below 50% for both strategies.
Summarizing, ARPlace is more robust towards state-estimation uncertainties than our
previous default strategy. The effect is more pronounced for object positions than robot
positions.
6.3 Transformational Planning with Merged ARPlaces
We evaluated the merging of ARPlaces for joint grasping, and the application of transformation rules with our rpl planner, as discussed in Section 4.4.1. Two cups are placed on
32

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Figure 19: Success ratios of the ARPlace and Fixed approaches when changing object
and/or robot pose uncertainties.

the table, where the distance between them is varied between 20 and 60cm, with increments
of 5cm. Our evaluation shows that grasping two cups from separate base positions requires
on average 48 seconds, independent of the relative distance of the cups to each other. By
applying transformation rules, the default plan is optimized to 32 seconds, which is a significant (t-test: p < 0.001) and substantial performance gain of 50% (Fedrizzi et al., 2009).
Above 45cm, two cups cannot be grasped from one position, and plan transformation is not
applied.
6.4 Integration of ARPlace in the Physical Robot System
At a day of open house, our B21 mobile manipulation platform continually performed an
application scenario, where it locates, grasps, and lifts a cup from the table and moves it to
the kitchen oven. Figure 20 shows two images taken during the demonstration. The robot
performed this scenario 50 times in approximately 6 hours, which has convinced us that the
robot hardware and software are robust enough to be deployed amongst the general public.
After the open day, we ran the same experiment, but this time we determined the goal
location for navigating to the table as being the mode of the ARPlace that was computed
before executing the navigation action. Since the main focus of this experiment was on our
error-recovery system described by Beetz et al. (2010), the improved robot performance we
observed cannot quantitatively be attributed to the use of ARPlace or the error-recovery
system. However, a major qualitative improvement we certainly can attribute to using
ARPlace was that the cup can now be grasped from a much larger area on the table.
Without ARPlaces, the cup always had to be placed on the same position on the table to
enable successful grasping.

7. Conclusion
In this article, we present a system that enables robots to learn action-related places
from observed experience, and reason with these places to generate robust, flexible, least33

fiStulp, Fedrizzi, Mosenlechner, & Beetz

1. robot navigates to table

2. robot reaches for cup

Figure 20: A reach and grasp trajectory performed during a public demonstration. (Note
that the operator is holding a camera, not a remote control!)

commitment plans for mobile manipulation. ARPlace is modeled as a probability distribution that maps locations to the predicted outcome of an action.
We believe our system has several advantages. First of all, the learned model is very
compact, with only 2 (deformation) parameters, which are directly related to task-relevant
parameters. Querying the model on-line is therefore very efficient. This is an advantage
of compiling experience into compact models, rather than running a novel search for each
situation.
On the other hand, as the model is acquired through experience-based learning, the
model is grounded in observed experience, and takes into account the robot hardware, its
control programs, and interactions with the environment. It can be applied to any mobile
manipulation platform, independent of the manipulators, navigation base, or the algorithms
that run on them.
The output of this model is a set of positions with associated success probabilities,
instead of one specific position. Rather than constraining itself to a specific position prematurely, the robot can efficiently update ARPlace as new sensor data comes in. This enables
least-commitment planning. The ARPlace representation also enables the optimization of
secondary criteria, such as execution duration, or determining the best position for grasping two objects simultaneously. In previous work, we proposed subgoal refinement (Stulp
& Beetz, 2008) for optimizing such secondary criteria with respect to subgoals.
Finally, by using ARPlaces to determine appropriate base positions, difficult positions
for grasping are avoided, which leads to more robust behavior in the face of state estimation
uncertainty, as demonstrated in our empirical evaluation.
We are currently extending our approach in several directions. We are in the process of
including ARPlace in a more general utility-based framework, in which the probability of
success is only one of the aspects of the task that needs to be optimized. New utilities, such
34

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

as execution duration or power consumption, are easily included in this framework, which
enables the robot to trade off efficiency and robustness on-line during task execution.
We are also applying our approach to more complex scenarios and different domains. For
instance, we are learning higher-dimensional ARPlace concepts, which take more aspects
of the scenario into account, i.e. different object sizes and objects that require different
types of grasps. Instead of mapping specific objects to places, we will map object and grasp
properties to deformation modes. We are also investigating extensions and other machine
learning algorithms that will enable our methods to generalize over this larger space. Objects
which require very different grasps, such as using two hands to manipulate them, will require
more sophisticated methods for acquiring and reasoning about place. Generalization of our
place concept with respect to situations and task contexts is a research challenge which we
have on our mid-term research agenda.

Acknowledgments
We are grateful to Pierre Roduit for providing us with the Matlab code described by Roduit
et al. (2007). We also thank Ingo Kresse, Alexis Maldonado, and Federico Ruiz for assistance
with the robotic platform, and the robot system overview. We are grateful to Franziska
Zacharias for providing a capability map (Zacharias et al., 2007) for our robot. We thank
Dominik Jain and Franziska Meier for fruitful discussions on Section 4.2.
This work was partly funded by the DFG project ActAR (Action Awareness in Autonomous Robots) and the CoTeSys cluster of excellence (Cognition for Technical Systems, http://www.cotesys.org), part of the Excellence Initiative of the German Research
Foundation (DFG). Freek Stulp was also supported by a post-doctoral Research Fellowship
(STU-514/1-1) from the DFG, as well as by the Japanese Society for the Promotion of
Science (PE08571). Freek Stulps contributions to this work were made at the Intelligent
Autonomous Systems Group (Technische Universitat Munchen, Munich, Germany), the
Computational Neuroscience Laboratories (Advanced Telecommunications Research Institute International, Kyoto, Japan), and the Computational Learning and Motor Control Lab
(University of Southern California, Los Angeles, USA),

Appendix A. Robot Platform
The action sequence we consider in this article is: 1) navigate to a specified base position
near the table; 2) reach for the object; 3) close the gripper; 4) lift the object. We now
sequentially describe the various hard- and software components involved in executing these
actions. An overview of these components and the data communicated between them is
depicted in Figure 21.
The main hardware component is a B21r mobile robot from Real World Interfaces
(RWI), with a frontal 180 degrees Sick LMS 200 laser range scanner. Before task execution,
the robot acquires a map of the (kitchen) environment using pmap for map building. To
navigate to the specified base position, the robot uses an Adaptive Monte Carlo Localization
algorithm for localization, and the AMCL Wavefront Planner for global path planning.
For these three software modules (map building, localization and planning), we use the
implementations from the Player project (Gerkey et al., 2003).
35

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 21: Overview of mobile manipulation hardware and software modules.

When the robot is close to the table, it detects and tracks the target object using the
approach proposed by Klank et al. (2009). The stereo-vision hardware consists of two
high dynamic range cameras that are mounted on a PTU-46 pan-tilt unit from Directed
Perception and have a resolution of 1390x1038 pixels.
For manipulation, the robot is equipped with two 6-DOF Powercube lightweight arms
from Amtec Robotics. To control the arms and reach for the target cup, we use the Kinematics and Dynamics Library (Orocos-KDL) (Smits, ) and a Vector Field approach. Within
this vector field, the handle of the cup is an attractor, but the cup itself, the table and all
other obstacles are repellors. Details about the position and shape of these attractors and
repellors are given by Beetz et al. (2010). On-line at every control cycle, the task space velocity at the end-effector is computed given the attractors and repellors, and this velocity is
mapped to joint space velocities using a damped least squares inverse kinematics algorithm.
After reaching the desired end-effector pose, the 1-DOF slide gripper closes.
High-level decision making, monitoring and error-recovery is done by the planning module written in the Reactive Planning Language (McDermott, 1991). It requests ARPlaces
from the module described in this article, reasons about them, and performs navigation and
manipulation requests based on them.
Communication between all modules described above is done over a middleware layer
consisting of Player (Gerkey et al., 2003) and YARP (Metta, Fitzpatrick, & Natale, 2006).
This overview is a simplification of the actual system. For instance, the role of RFID tags
and the Belief State have been omitted. For a complete and more detailed description of
the mobile manipulation platform, we refer to the work of Beetz et al. (2010, Section 1.2).
36

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Appendix B. Landmark Distribution for the Point Distribution Model
A Point Distribution Model (PDM) takes a set of m landmarks on n contours as an input,
represented as a m  n matrix H, and returns the matrices H (mean of the contours), P
(deformation modes), and B (deformation mode weighting per contour), which the original
contours can be reconstructed.
In our application of PDMs, we are free to choose the locations of the landmarks. Therefore, the goal of the procedure described here is to determine landmark locations that leads
to a compact PDM that accurately reconstructs the original contours, i.e. the classification
boundaries. We do so by explicitly optimizing two measures: 1) model compactness: the
amount of energy e stored in the first d degrees of freedom of the PDM, with 0  e  1;
2) reconstruction accuracy: the mean distance l between the landmarks on the original
contours and reconstructed contours. These measures are combined in the cost function
(2  e)l2 , expressing that we want low error and high energy for a given number of degrees
of freedom d.
Given the number of landmarks m and the number of degrees of freedom d, we explicitly
optimize this cost function through search. We do so by varying the position of each
landmark, one landmark at a time, and greedily selecting the position that leads to the
lowest cost. This optimization is first done for d = 1, and the number of degrees of freedom
d is incremented until the optimization leads to an energy that lies above 95%. This ensures
that the number of degrees of freedom d and the distance l between the landmarks remains
low, whilst the energy e is high. Therefore, the resulting PDM model will be compact yet
accurate.
This optimization step is by far the most computationally intensive step in the off-line
learning phase. We are currently investigating the use of alignment methods from computer
vision (Huang, Paragios, & Metaxas, 2006), to replace our iterative optimization approach.

Appendix C. From Robot Coordinate Systems to the Relative Feature
Space
Our robot uses a variety of coordinate systems. The goal is to compute the matrix GSM TO ,
which describes the objects position relative to the feature space of the Generalized Success
Model. GSM TO can then be used to reconstruct classification boundaries for successfully
grasping the object, as described in Section 3.4. We now present the required coordinate
systems, and how they are transformed to yield the Generalized Success Model required
feature space that is depicted in Figure 5.
The coordinate frames that are involved in the transformation are depicted in Figure 22:
the world frame FW , the table frame FT that is centered in the middle top of the table, the
robot frame FR that is centered in the robots base center at the floor, the camera frame
FC that is centered in the cameras sensor chip, the frame of the pan-tilt unit where the
camera is mounted FP T , and the relative feature space FGSM .
To acquire the position of the target object O relative to FGSM , we compute GSM TO as
follows:
GSM

TO = (W TGSM )1 
37

W

TO

(12)

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 22: Relevant coordinate frames

The global position of the object
W

TO =

W

WT
O

TR 

is computed as follows:
R

TP T 

PT

TC 

C

TO

(13)

Here, W TR is the location of the robots base frame relative to the world frame. The
robot uses an AMCL particle filter for estimating its position. R TP T is the pose of the pan
tilt unit relative to the robots base frame. The transformation matrix R TP T is constant
and was specified by manually measuring the distances and angular offsets from the B21
robot base to the pan tilt unit. Because of careful measurement, we assume maximum
errors of 1mm for the distance measurements along the x-, y-, and z-axis, and 2 for the
yaw angle measurement. P T TC is the pose of the cameras sensor relative to the pan tilt
unit. P T TC changes according to the current pan and tilt angles, but can be read from the
pan tilt units driver with high accuracy. C TO is the position of the target object relative
to the camera frame. It is estimated by the vision-based object localization module that is
described by Klank et al. (2009).
In order to compute W TGSM we need to know the global position of the object W TO ,
which we already computed above, and the global position of the table W TT . Currently,
we get the world coordinates of the tables position from a map, but it is also possible to
estimate its position with the vision-based object localization module. We then compute
the normal from the object to the table edge that is closest to the robot, as can be seen in
Figure 5. The origin of FGSM and therefore W TGSM is where the table edge and the object
normal intersect.
The most critical parts in the computations above are the angular estimations of the
robots localization and vision system. First, their estimation uncertainty is rather big.
Second, an error in the localization angle has a significant impact on the estimated object
pose, as follows from Equation 13.
The pose of the cup in frame FGSM is a 6D vector [x, y, z, yaw, pitch, roll]. However,
since we assume the cup is standing upright on the table, we set z to the tables height,
and roll and pitch to 0 . Since the origin of FGSM is perpendicular to the tables edge
that passes through y, y is also 0 by definition. The remaining parameters x and yaw then
correspond to the features xobj and  obj respectively.
38

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

References
Amir, E., & Chang, A. (2008). Learning partially observable deterministic action models.
Journal of Artificial Intelligence Research (JAIR), 33, 349402.
Beetz, M., Stulp, F., Esden-Tempski, P., Fedrizzi, A., Klank, U., Kresse, I., Maldonado,
A., & Ruiz, F. (2010). Generality and legibility in mobile manipulation. Autonomous
Robots Journal (Special Issue on Mobile Manipulation), 28 (1), 2144.
Beetz, M., Stulp, F., Radig, B., Bandouch, J., Blodow, N., Dolha, M., Fedrizzi, A., Jain,
D., Klank, U., Kresse, I., Maldonado, A., Marton, Z., Mosenlechner, L., Ruiz, F.,
Rusu, R. B., & Tenorth, M. (2008). The assistive kitchen  a demonstration scenario
for cognitive technical systems. In IEEE 17th International Symposium on Robot and
Human Interactive Communication (RO-MAN), Muenchen, Germany. Invited paper.
Berenson, D., Choset, H., & Kuffner, J. (2008). An optimization approach to planning
for mobile manipulation. In Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA) 2008, pp. 11871192.
Berenson, D., Srinivasa, S., Ferguson, D., Romea, A. C., & Kuffner, J. (2009a). Manipulation planning with workspace goal regions. In Proceedings of the IEEE International
Conference on Robotics and Automation (ICRA), pp. 13971403.
Berenson, D., Srinivasa, S. S., & Kuffner, J. J. (2009b). Addressing pose uncertainty in
manipulation planning using task space regions. In Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS, pp. 14191425.
Bohlin, R., & Kavraki, L. E. (2000). Path planning using lazy prm. In IEEE International
Conference Robototics and Automation, pp. 521528.
Buck, S., & Riedmiller, M. (2000). Learning situation dependent success rates of actions in a
RoboCup scenario. In Pacific Rim International Conference on Artificial Intelligence,
p. 809.
Cambon, S., Gravot, F., & Alami, R. (2004). A robot task planner that merges symbolic and
geometric reasoning.. In Proceedings of the 16th European Conference on Artificial
Intelligence (ECAI), pp. 895899.
Chang, A., & Amir, E. (2006). Goal achievement in partially known, partially observable domains. In International Conference on Automated Planning and Scheduling
(ICAPS), pp. 203211.
Clement, B. J., Durfee, E. H., & Barrett, A. C. (2007). Abstract reasoning for planning
and coordination. Journal of Artificial Intelligence Research, 28, 453515.
Cootes, T. F., Taylor, C. J., Cooper, D., & Graham, J. (1995). Active shape models - their
training and application. Computer Vision and Image Understanding, 61 (1), 3859.
Detry, R., Baseski, E., Popovic, M., Touati, Y., Krueger, N., Kroemer, O., Peters, J., &
Piater, J. (2009). Learning object-specific grasp affordance densities. In Proceedings
of the International Conference on Development and Learning (ICDL), pp. 17.
Diankov, R., Ratliff, N., Ferguson, D., Srinivasa, S., & Kuffner, J. (2008). Bispace planning:
Concurrent multi-space exploration. In Proc. Int. Conf. on Robotics: Science and
Systems.
39

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Fedrizzi, A. (2010). Action-Related Places for Mobile Manipulation. Ph.D. thesis, Technische
Universiat Munchen.
Fedrizzi, A., Moesenlechner, L., Stulp, F., & Beetz, M. (2009). Transformational planning for mobile manipulation based on action-related places. In Proceedings of the
International Conference on Advanced Robotics (ICAR)., pp. 18.
Friedman, M., & Weld, D. S. (1996). Least-commitment action selection. In Proceedings
3rd International Conference on A.I. Planning Systems, pp. 8693. AAAI Press.
Geib, C., Mourao, K., Petrick, R., Pugeault, M., Steedman, M., Kruger, N., & Worgotter,
F. (2006). Object action complexes as an interface for planning and robot control. In
Proceedings of the 2006 IEEE RAS International Conference on Humanoid Robots,
Genova.
Gerkey, B., Vaughan, R. T., & Howard, A. (2003). The Player/Stage Project: Tools for
multi-robot and distributed sensor systems. In Proceedings of the 11th International
Conference on Advanced Robotics (ICAR), pp. 317323.
Gibson, J. J. (1977). The Theory of Affordances. John Wiley & Sons.
Hart, S., Ou, S., Sweeney, J., & Grupen, R. (2006). A framework for learning declarative
structure. In RSS-06 Workshop: Manipulation for Human Environments.
Huang, X., Paragios, N., & Metaxas, D. N. (2006). Shape registration in implicit spaces
using information theory and free form deformations. IEEE Trans. Pattern Analysis
and Machine Intelligence (TPAMI), 28, 13031318.
Kemp, C., Edsinger, A., & Torres-Jara, E. (2007). Challenges for robot manipulation in
human environments. IEEE Robotics and Automation Magazine, 14 (1), 2029.
Klank, U., Zia, M. Z., & Beetz, M. (2009). 3D Model Selection from an Internet Database
for Robotic Vision. In International Conference on Robotics and Automation (ICRA),
pp. 24062411.
Kuipers, B., Beeson, P., Modayil, J., & Provost, J. (2006). Bootstrap learning of foundational representations. Connection Science, 18, 145158.
LaValle, S. M. (2006). Planning Algorithms, chap. Chapter 5: Sampling-Based Motion
Planning. Cambridge University Press.
Maldonado, A., Klank, U., & Beetz, M. (2010). Robotic grasping of unmodeled objects
using time-of-flight range data and finger torque information. In 2010 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 25862591,
Taipei, Taiwan.
McDermott, D. (1991). A Reactive Plan Language. Research Report YALEU/DCS/RR864, Yale University.
Metta, G., Fitzpatrick, P., & Natale, L. (2006). YARP: Yet Another Robot Platform. International Journal of Advanced Robotics Systems, special issue on Software Development
and Integration in Robotics, 3 (1), 4348.
Morales, A., Chinellato, E., Fagg, A. H., & del Pobil, A. P. (2004). Using experience for
assessing grasp reliability. International Journal of Humanoid Robotics, 1 (4), 671691.
40

fiLearning and Reasoning with Action-Related Places for Robust Mobile Manipulation

Mosenlechner, L., & Beetz, M. (2009). Using physics- and sensor-based simulation for
high-fidelity temporal projection of realistic robot behavior. In 19th International
Conference on Automated Planning and Scheduling (ICAPS09).
Okada, K., Kojima, M., Sagawa, Y., Ichino, T., Sato, K., & Inaba, M. (2006). Vision
based behavior verification system of humanoid robot for daily environment tasks.
In Proceedings of the 6th IEEE-RAS International Conference on Humanoid Robots
(Humanoids), pp. 712.
Pastor, P., Hoffmann, H., Asfour, T., & Schaal, S. (2009). Learning and generalization
of motor skills by learning from demonstration. In Proceedings of the International
Conference on Robotics and Automation (ICRA), pp. 12931298.
Resulaj, A., Kiani, R., Wolpert, D. M., & Shadlen, M. N. (2009). Changes of mind in
decision-making. Nature, 461 (7261), 263266.
Roduit, P., Martinoli, A., & Jacot, J. (2007). A quantitative method for comparing trajectories of mobile robots using point distribution models. In Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 24412448.
Ryan, M. R. K. (2002). Using abstract models of behaviours to automatically generate reinforcement learning hierarchies. In Proceedings of The 19th International Conference
on Machine Learning, Sydney, Australia, pp. 522529.
Smits, R. KDL: Kinematics and Dynamics Library. http://www.orocos.org/kdl.
Sonnenburg, S., Raetsch, G., Schaefer, C., & Schoelkopf, B. (2006). Large scale multiple
kernel learning. Journal of Machine Learning Research, 7, 15311565.
Stulp, F., & Beetz, M. (2008). Refining the execution of abstract actions with learned action
models. Journal of Artificial Intelligence Research (JAIR), 32.
Stulp, F., Fedrizzi, A., & Beetz, M. (2009a). Action-related place-based mobile manipulation. In Proceedings of the International Conference on Intelligent Robots and Systems
(IROS), pp. 31153120.
Stulp, F., Fedrizzi, A., & Beetz, M. (2009b). Learning and performing place-based mobile
manipulation. In Proceedings of the 8th International Conference on Development and
Learning (ICDL)., pp. 17.
Stulp, F., Fedrizzi, A., Zacharias, F., Tenorth, M., Bandouch, J., & Beetz, M. (2009c).
Combining analysis, imitation, and experience-based learning to acquire a concept of
reachability. In 9th IEEE-RAS International Conference on Humanoid Robots, pp.
161167.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: an Introduction. MIT Press.
Wimmer, M., Stulp, F., Pietzsch, S., & Radig, B. (2008). Learning local objective functions
for robust face model fitting. IEEE Transactions on Pattern Analysis and Machine
Intelligence (PAMI), 30 (8), 13571370.
Zacharias, F., Borst, C., & Hirzinger, G. (2007). Capturing robot workspace structure: representing robot capabilities. In Proceedings of the IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pp. 32293236.
41

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Zettlemoyer, L. S., Pasula, H. M., & Kaelbling, L. P. (2005). Learning planning rules in noisy
stochastic worlds. In Proceedings of the Twentieth National Conference on Artificial
Intelligence (AAAI), pp. 911918.
Zheng, Y., & Qian, W.-H. (2005). Coping with the grasping uncertainties in force-closure
analysis. International Journal Robotics Research, 24 (4), 311327.

42

fiJournal of Artificial Intelligence Research 43 (2012) 621-659

Submitted 11/11; published 04/12

A Market-Inspired Approach for Intersection Management
in Urban Road Traffic Networks
Matteo Vasirani
Sascha Ossowski

matteo.vasirani@urjc.es
sascha.ossowski@urjc.es

Centre for Intelligent Information Technology
University Rey Juan Carlos
C/ Tulipan s/n
Madrid, 28933, Spain

Abstract
Traffic congestion in urban road networks is a costly problem that affects all major
cities in developed countries. To tackle this problem, it is possible (i) to act on the supply
side, increasing the number of roads or lanes in a network, (ii) to reduce the demand, restricting the access to urban areas at specific hours or to specific vehicles, or (iii) to improve
the efficiency of the existing network, by means of a widespread use of so-called Intelligent
Transportation Systems (ITS). In line with the recent advances in smart transportation
management infrastructures, ITS has turned out to be a promising field of application for
artificial intelligence techniques. In particular, multiagent systems seem to be the ideal
candidates for the design and implementation of ITS. In fact, drivers can be naturally
modelled as autonomous agents that interact with the transportation management infrastructure, thereby generating a large-scale, open, agent-based system. To regulate such a
system and maintain a smooth and efficient flow of traffic, decentralised mechanisms for
the management of the transportation infrastructure are needed.
In this article we propose a distributed, market-inspired, mechanism for the management of a future urban road network, where intelligent autonomous vehicles, operated by
software agents on behalf of their human owners, interact with the infrastructure in order
to travel safely and efficiently through the road network. Building on the reservationbased intersection control model proposed by Dresner and Stone, we consider two different
scenarios: one with a single intersection and one with a network of intersections. In the
former, we analyse the performance of a novel policy based on combinatorial auctions for
the allocation of reservations. In the latter, we analyse the impact that a traffic assignment strategy inspired by competitive markets has on the drivers route choices. Finally
we propose an adaptive management mechanism that integrates the auction-based traffic
control policy with the competitive traffic assignment strategy.

1. Introduction
Removing the human driver from the control loop through the use of autonomous vehicles integrated with an intelligent road infrastructure can be considered as the ultimate,
long-term goal of the set of systems and technologies grouped under the name of Intelligent
Transportation Systems (ITS). Autonomous vehicles are already a reality. For instance,
three DARPA Grand Challenges 1 have been held so far. The teams participating in the
latest event, the DARPA Urban Challenge, competed to build the best autonomous vehi1. http://archive.darpa.mil/grandchallenge/
c
!2012
AI Access Foundation. All rights reserved.

fiVasirani & Ossowski

cles, capable of driving in traffic, performing complex manoeuvres such as merging, passing,
parking and negotiating with intersections. The results have shown for the first time that
autonomous vehicles can successfully interact with both manned and unmanned vehicular
traffic in an urban environment. Several car-makers expect the technology to be affordable
(and less obtrusive) in about a decade2 . Another initiative that fosters this vision is Connected Vehicle 3 , which promotes research and development of technologies that link road
vehicles directly to their physical surroundings, i.e., by vehicle-to-infrastructure wireless
communications. The advantages of such an integration span from improved road safety
to a more efficient operational use of the transportation network. For instance, vehicles
can exchange critical safety information with the infrastructure, so as to recognise high-risk
situations in advance and therefore to alert drivers. Furthermore, traffic signal systems can
communicate signal phase and timing information to vehicles to enhance the use of the
transportation network.
In this regard, some authors have recently paid attention to the potential of a tighter
integration of autonomous vehicles with the road infrastructure for future urban traffic management. In the reservation-based control system (Dresner & Stone, 2008), an intersection
is regulated by a software agent, called intersection manager agent, which assigns reservations of space and time to each autonomous vehicle intending to cross the intersection.
Each vehicle is operated by another software agent, called driver agent. When a vehicle
is approaching an intersection, the driver requests that the intersection manager reserve
the necessary space-time slots to safely cross the intersection. The intersection manager,
provided with data such as vehicle ID, vehicle size, arrival time, arrival speed, type of turn,
etc., simulates the vehicles trajectory inside the intersection and informs the driver whether
its request is in conflict with the already confirmed reservations. If such a conflict does not
exist, the driver stores the reservation details and tries to meet them; otherwise it may try
again at a later time. The authors show through simulations that in situations of balanced
traffic, if all vehicles are autonomous, their delays at the intersection are drastically reduced
compared to traditional traffic lights.
In this article we explore how different lines of research in artificial intelligence and agent
technology can further improve the effectiveness and applicability of Dresner and Stones
approach, assuming that all vehicles are autonomous and capable of interacting with the
regulating traffic infrastructure. We extend the reservation-based model for intersection
control at two different levels.
1.1 Single Intersection
For a single intersection, our objective is to elaborate a new policy for the allocation of
reservations to vehicles that takes into account the drivers different attitudes regarding their
travel times. Instead of granting the disputed resources (intersection space and time) to the
first agent that requests them, we intend to allocate them to the agents that value them
the most, while maintaining an adequate level of efficiency and fairness of the system. Our
main contribution in this regard is the definition of an auction-based allocation policy for
2. See for example Alan Taub, General Motors Vice President of Global R&D, at the 18th World Congress
on Intelligent Transport Systems, October 17th, 2011.
3. http://www.its.dot.gov/connected vehicle/connected vehicle.htm

622

fiA Market-Inspired Approach for Intersection Management

assigning reservations. This policy models incoming requests as bids over an intersections
available space-time slots and tries to maximise the overall value of the accepted bids. Due
to the combinatorial nature of the auction and the restrictions of our scenario (mainly realtime execution and safety), we define a specific auction protocol, adapt an algorithm for
winner determination for our purposes, and evaluate the behaviour of the approach.
1.2 Network of Intersections.
To extend Dresner and Stones approach to a network of intersections, we focus on the
problem of traffic assignment, conceived as a distributed choice problem where intersection
managers try to affect the decision making of the driver agents. In particular, we use markets
as mediators for our distributed choice and allocation problem (Gerding, McBurney, &
Yao, 2010). Our contribution to the attainment of the above objective is twofold. First,
we build a computational market where drivers must acquire the right to pass through
the intersections of the urban road network, implementing the intersection managers as
competitive suppliers of reservations which selfishly adapt the prices to match the actual
demand. Second, we combine the competitive strategy for traffic assignment with the
auction-based control policy at the intersection level into an adaptive, market-inspired,
mechanism for traffic management of reservation-based intersections.
The article is structured as follows. Section 2 provides an overview of the use of artificial
intelligence and agent technology in the field of ITS. In Section 3 we briefly review the key
elements of the reservation-based intersection control model that our work sets out from.
In Section 4 we present our policy for the allocation of reservations at a single intersection,
inspired by combinatorial auction theory. In Section 5 we extend the reservation-based
model to network of intersections. Finally, we conclude in Section 6.

2. Related Work
To achieve the goals pursued by the ITS vision there is an increasing need to understand,
model, and govern such systems at both the individual (micro) and the societal (macro)
level. Transportation systems may contain thousands of autonomous entities that need
to be governed, which raises significant technical problems concerning both efficiency and
scalability. The inherent distribution of traffic management and control problems, their
high degree of complexity, and the fact that the actors in traffic and transportation systems
(driver, pedestrians, infrastructure managers, etc.) fit the concept of autonomous agent
very well, allow for modelling ITS in terms of agents that interact so as to achieve their
goals, selfishly as well as cooperatively. Therefore, traffic and transportation scenarios are
extraordinarily appealing for multiagent technology (Bazzan & Klugl, 2008). In this section,
we outline some key dimensions of ITS and briefly review relevant literature on the use of
artificial intelligence and multiagent techniques in the field.
2.1 Traffic Control and Traffic Assignment
Traffic control refers to the regulation of the access to a disputed road transport resource.
Traffic control systems manage traffic along arterial roadways, employing traffic detectors,
traffic signals, and various means of communicating information to drivers. Freeway control
623

fiVasirani & Ossowski

systems manage traffic along highways, employing traffic surveillance systems, traffic control
measures on freeway entrance ramps (ramp metering), and lane management.
Traffic control at intersections, based on traffic lights, is the major control measure in
urban road networks. This type of control typically applies off-line optimisation on the basis
of historical data. TRANSYT (Robertson, 1969) is a well-known and frequently applied
signal control strategy, but it cannot adapt dynamically to changing demand patterns.
Other control techniques, such as SCOOT (Hunt, Robertson, Bretherton, & Winton, 1981),
use real-time traffic volume rather than historical data to run optimisation algorithms and
compute the optimal signal plan.
Traffic assignment refers to the problem of the distribution of traffic in a network, considering demands between several locations, and the capacity of the network. In general,
demand may change in a non-predictable way, due to changing environmental conditions,
exceptional events, or accidents. This, in turn, leads to under-utilisation of the overall network capacity, whereby some links are heavily congested while capacity reserves are available
on alternative routes. To address this problem, different traffic management techniques, involving information broadcast as well as control and optimisation, can be employed. For
example, route guidance and driver information systems (RGDIS) may be employed to improve the network efficiency via direct or indirect recommendation of alternative routes (Papageorgiou, Diakaki, Dinopoulou, Kotsialos, & Wang, 2003). These communication devices
may be consulted by a potential road user to make a rational decision regarding whether
or not to carry out (or postpone) the intended trip, the choice of transport mode (car, bus,
underground, etc.), the departure time selection and the route choice.
Traffic control and assignment have different focuses and can therefore be combined
into a single management policy that takes explicitly into account the mutual interactions
between signal control policies and user route choices (Meneguzzer, 1997).
2.2 Isolated and Coordinated Traffic Control
Most traffic control strategies use control devices (e.g., traffic lights, variable message signs,
ramp meters) and surveillance devices (e.g., loop detectors, cameras) to manage a physical
traffic network. In isolated control, only a small portion of the network (e.g. a single
intersection) is modelled, and techniques from control theory are employed to determine
signal cycles so as to minimise the vehicles total delay. For instance, da Silva et al. proposed
a reinforcement learning system for traffic lights that copes with the dynamism of the
environment by incrementally building new models of the environmental state transitions
and rewards (da Silva, Basso, Bazzan, & Engel, 2006). When the traffic pattern changes,
an additional model is created and a new traffic signal plan is learned. The creation of new
models is controlled by a continuous evaluation of the prediction errors generated by each
partial model.
In coordinated control, the settings of several control devices are adapted to each other,
so as to achieve a smooth traffic flow at the network level (i.e., green waves) rather
than at a single intersection. By allowing the individual devices to coordinate their actions
based on the information they receive from sensors and from each other, coherent traffic
control plans are often generated faster and more accurately compared to a human traffic
operator (van Katwijk, Schutter, & Hellendoorn, 2009). For instance, distributed constraint
624

fiA Market-Inspired Approach for Intersection Management

optimisation (DCOP) techniques have recently been applied to the coordination of control
devices (Junges & Bazzan, 2008). Each traffic signal agent is assigned to one or several
variables of the DCOP, which have inter-dependencies and conflicts (e.g., two neighbouring
intersections giving preference to different directions of traffic.). A mediator agent is in
charge of resolving these conflicts when they occur, recommending values for the variables
associated to the agents involved in the mediation.
2.3 Time Perspective
The time perspective refers to the stage in which the decision-making process of an ITS
application takes place. Operational decision-making in ITS refers to short term issues, such
as controlling traffic at an intersection. Tactical decision-making deals with medium-term
issues, such as anticipating congestion by diverting traffic on different routes or influencing
demand patterns. Finally, strategic decision-making typically involves long-term decisions,
e.g. planning the construction of new roads, highways or parking hubs.
Many AI-based ITS partially automate the operational part of road traffic control tasks.
Tactical and strategic decision-making is still mainly a human activity (e.g., carried on by
city planners). Some more recent decision-support systems address tactical questions as
well. InT RY S (Hernandez, Ossowski, & Garca-Serrano, 2002), for instance, is a multiagent
system aimed at assisting operators in a traffic control centre to manage an urban motorway
network. The system is capable of engaging in dialogues with the operators, e.g. to diagnose
the causes of detected traffic problems, to construct coherent sets of driver information
messages, and to simulate the expected effects of such control plans.
2.4 Information to Drivers
Cooperative systems can improve dynamic routing and traffic management (Adler, Satapathy, Manikonda, Bowles, & Blue, 2005), using information services aimed at giving advice to
drivers and efficiently assigning traffic among the network. This is a difficult problem as collective route choice performed by selfish agents often leads to equilibrium strategies that are
far from social welfare optima (Roughgarden, 2003). Providing information about the congestion of links or sharing partial views of vehicle choices, as in context-aware routing (Zutt,
van Gemund, de Weerdt, & Witteveen, 2010), may improve the systems efficiency.
2.5 Domain Knowledge
Domain and topological knowledge can be exploited to structure both the architecture
and the reasoning models of ITS. For instance, Choy et al. propose a cooperative, hierarchical, multiagent system for real-time traffic signal control (Choy, Srinivasan, & Cheu,
2003). The control problem is divided into various sub-problems, each of them handled
by an intelligent agent that applies fuzzy neural decision-making. The multiagent system
is hierarchical, since decisions made by lower-level agents are mediated by their respective
higher-level agents. The InT RY S system (Hernandez et al., 2002) conceives the traffic
dynamics in terms of so-called problem areas, which are defined based on the expertise of
traffic engineers. Each problem area is controlled by a separate traffic control (software)
agent. Knowledge modelling and reasoning techniques are applied to integrate local control
625

fiVasirani & Ossowski

strategies (proposed by the different traffic control agents) into a coherent global plan for
the whole traffic network.
2.6 Learning and Adaptation
ITS often rely on learning techniques to adapt to changing or unknown traffic conditions.
For instance, traffic light agents may use reinforcement learning to minimise the overall
waiting time of vehicles (Steingrover, Schouten, Peelen, Nijhuis, & Bakker, 2005; Wiering,
2000). The control objective is global, although actions are local to the agents. The state of
the learning task is represented as an aggregation of the waiting times of individual vehicles
at the intersection. Traffic light agents learn a value function that estimates expected
waiting times of vehicles given different settings of traffic lights.
Several authors focus on self-organising and self-adapting mechanisms for traffic control (Gershenson, 2005; Lammer & Helbing, 2008), where traffic lights self-organise with
no direct communication between them. The local interactions between neighbouring traffic lights lead to emergent coordination patterns such as green waves. In this way, an
efficient, decentralised traffic light control is achieved, as a combination of two rules, one
that aims at optimising the flow and one that aims at stabilising it. In the T RY SA2
system (Hernandez et al., 2002), traffic agents use a mechanism called structural cooperation (Ossowski & Garca-Serrano, 1999) to locally coordinate their signal plan proposals
without the need to rely on dedicated domain (coordination) knowledge.
2.7 Market-Based Coordination
Being a complex system, traffic is well suited for the application of market-based coordination mechanisms at different levels. These mechanisms replicate the functioning of real
markets (i.e., auctions, bargaining, etc.) in order to coordinate the activities and goals
pursued by a set of agents. The agents that regulate the infrastructure can be built to
act as a team, i.e., they may share a global objective function that represents the system
designers preferences over all possible solutions, as it occurs in multi-robot domains (Dias,
Zlot, Kalra, & Stentz, 2006). In line with this perspective, Vasirani and Ossowski (2009b)
proposed a market-based policy for traffic assignment. The authors put forward a cooperative learning model so as to coordinate the prices of several intersections. The experimental
results showed that, in general, an increase in the profit raised by a team of intersections is
aligned with reduced average travel times. A limitation of this work is the number of interactions with the environment that are required in order for the price vector that maximises
overall profit to be learned.
If we extend the focus to include selfish driver agents and their interaction with the
infrastructure agents, a non-cooperative scenario arises. For instance, an auction-based
policy for intersection control is proposed in the work of Schepperle and Bohm (2007). In
this work, an intersection controlled by an intelligent agent starts an auction for the earliest
time slot among the vehicles that are approaching the intersection on each lane. The authors
assume that the agent that controls an intersection can detect if an approaching vehicle has
another vehicle in front of it. In this case, the former is not allowed to participate in
the auction (i.e., its bids are not processed), so as to ensure that only vehicles that do not
have physical impediments to cross the intersection are allowed to participate in the auction.
626

fiA Market-Inspired Approach for Intersection Management

Furthermore, since a non-combinatorial auction is run to allocate the earliest time-slot, only
one bidder (i.e., driver) is entitled to get a specific time-slot, which can lead to inefficiencies
in the assignments.
The field of transport economics also studies the allocation of resources used to move
road users from place to place (Small & Verhoef, 2007). However, it follows a more static
and analytical approach that requires extensive knowledge of supply and demand functions.
Such information is often hard to obtain and extract, so usually findings from the field are
hard to transfer directly to ITS.
2.8 Discussion
In this work, we mainly focus on the operational time perspective, since our aim is to
manage an advanced traffic infrastructure that regulates the route choices of autonomous
vehicles, while tactical and strategic decisions are left to the human users. In order to make
the proposed mechanisms broadly adoptable, we minimise the domain knowledge necessary
to set up our models. While the software agents that reside in the traffic management
infrastructure need to be aware of the remaining infrastructure agents, they do not require
expert knowledge related to the underlying traffic system. We focus on local adaptation
mechanisms, rather than learning techniques, to enforce emergent coordination among the
software agents that reside in the traffic management infrastructure. Furthermore, we
put forward a market-based coordination framework that involves both the infrastructure
and the drivers. The infrastructure agents coordinate their actions in an indirect way
as competitive market participants that aim to match supply with demand. The driver
agents participate in the allocation of the road network capacity through an auction-based
mechanism that regulates the assignment of the right to cross an intersection. Finally, we
recognise the importance of providing information to drivers in order to influence their
decision making. In particular, we assume the existence of propagation mechanisms, so
that the market price information is available to the drivers, thus potentially influencing
their collective behaviour4 .

3. Reservation-Based Intersection Control
The applications of AI techniques and multiagent technology in the traffic domains that were
detailed in the previous section conceive that the ITS lies in the infrastructure and its components (traffic lights, message signs, sensors, etc.), while the vehicles are usually treated as
particles of a traffic flow that a control policy cannot individually address. Nevertheless, the
continuous advances in software and hardware technologies will make a tighter integration
between vehicles and infrastructure possible. Even today, vehicles can be equipped with
features such as cruise control (Ioannou & Chien, 1993) and autonomous steering (Krogh
& Thorpe, 1986). Small-scale systems of autonomous guided vehicles (AGV) already exist,
for example in factory transport systems. If this trend continues, one day fully autonomous
vehicles will populate our road networks. In this case, given that the system will comprise a
variable (and possibly huge) number of vehicles, an open infrastructure is needed to control
4. Setting up such price index boards is technically feasible already today: for instance, the NYSE indexes
approximately 8500 stocks, whose price variations are spread worldwide almost immediately.

627

fiVasirani & Ossowski

Driver agent
(2)
Send REQUEST

Intersection manager

Reservation distance
lter

(5)

> reservation
distance di ?

Calculate
distance d(r)

no

Get driver
agent's ID

(3)

(1)

yes
Send
REJECTION

(6)

(4)

has
reservation ?

no

yes

(11)

(10)

Send
REJECTION

Update di to

Remove
agent's
reservation

min(di,d(r))

yes

(9)

(7)

(8)
Simulate
trajectory

has
conicts ?
no

(13)

(12)

Send
CONFIRMATION

Update di to 	

Figure 1: Reservation-based protocol with FCFS policy
and schedule the transit of AGVs. In fact, nowadays centralised AGV control systems know
the number of the vehicles, their origins and destinations, before the route planning takes
place. In the case of an urban road traffic scenario, such an approach is certainly unfeasible.
In this section we present some details of the reservation-based system for intersection
control (Dresner & Stone, 2008) that are relevant for this work5 . In particular we outline the
policy executed by intersection managers to process reservations requests (Section 3.1) and
analyse the impact that the distance at which the reservation is sent has on the performance
of the control mechanism (Section 3.2).
3.1 Protocol
The reservation-based control system proposed by Dresner and Stone assumes the existence
of two different kinds of software agents: intersection manager agents and driver agents. The
intersection manager agent controls the space of an intersection and schedules the crossing
of each vehicle. The driver agent is the entity that autonomously operates the vehicle (in
the following we will use the terms intersection manager and driver for short, to refer to
the software agents that control an intersection and a vehicle respectively). The protocol,
using the first-come-first-served policy (FCFS), is summarised in Figure 1. Each driver,
5. We remark that in this work we engineered the basic aspects of the reservation-based system. We did
not consider more advanced features, such as acceleration within the intersections, safety buffers or edge
tiles. The basic functioning of the reservation-based intersection that we assume in this work is the same
in every experimental scenario that we compare. In this way a fair comparison between different policies
for the allocation of reservations is guaranteed.

628

fiA Market-Inspired Approach for Intersection Management

(request reservation
:sender
:receiver
:content(

D-3548
IM-05629
:arrival time
:arrival speed
:lane
:type of turn

08:03:15
23km/h
2
LEFT

)
)
Figure 2: Example of a REQUEST message
when approaching the intersection, contacts the intersection manager by sending a REQUEST
message (1). The message contains the vehicles ID, the arrival time, the arrival speed, the
lane occupied by the vehicle in the road segment that approaches the intersection and the
intended type of turn (see Figure 2 for an example of REQUEST message). The intersection
manager calculates the distance d(r) from which the driver is sending the reservation request
r (2). If the distance is greater than the maximum reservation distance di of the lane that
the driver is occupying (3), the request is rejected without processing it (4). Otherwise,
the intersection manager proceeds to evaluate whether it can be accommodated or not.
First, the drivers ID is parsed (5), and if the driver already has a prior reservation (6), this
reservation is removed (7). Then, with the information contained in the REQUEST message,
the intersection manager simulates the vehicle trajectory, calculating the space needed by
the vehicle over time in order to check if there are potential conflicts (8). If so (9), the
intersection manager updates the maximum reservation distance di (10) and replies with a
REJECTION message (11). Otherwise, the maximum reservation distance di is updated to
infinite (12) and the intersection manager replies with a CONFIRMATION message (13), which
implies that the drivers request is accepted.
The FCFS policy implies that if two drivers send requests that require the same spacetime slots inside the intersection, the driver that sends the request first will obtain the
reservation. In extreme cases this policy is clearly inefficient. Consider the case of a set of
n vehicles, v1 , v2 , . . . , vn , such that v1 s request has conflicts with every other vehicle, but
that v2 , . . . , vn do not have conflicts with one another. If v1 sends its request first, it will
be granted and all other vehicles requests will be rejected. On the other hand, if it sends
its request last, the other n  1 vehicles will have their requests confirmed, whilst only v1
will have to wait. Nevertheless, FCFS has the advantage of being a simple policy, which
only needs the minimum amount of information necessary to implement a reservation-based
intersection control.
3.2 Reservation Distance
The protocol detailed above would be prone to deadlock situations, if it did not make use
of the reservation distance filter. Consider two vehicles, A and B, with A moving in front of
B (see Figure 3). Suppose also that B cannot safely overtake A. If A and B send a request
629

fiVasirani & Ossowski

B sends the request
rst and gets the
reservation

B

A's request is
rejected, thus A
must stop at the
intersection

A
Given that A cannot
cross, also B must
stop at the
intersection

Figure 3: Potential deadlock situation.

for the same space-time slots inside the intersection, the first request that the intersection
manager receives will be accepted, and the second one will be rejected. If vehicle B, which
is behind vehicle A, obtains the reservation, the result will be that vehicle A is not able
to cross because it does not hold a confirmed reservation. This in turn prevents vehicle B
making use of its reservation. If vehicle B always sends its request first, then a deadlock
situation arises, with vehicle A physically blocking vehicle B, and vehicle B blocking vehicle
A by getting the disputed reservation.
To avoid the occurrence of these deadlock situations, Dresner and Stone proposed the
use of the reservation distance as a heuristic criterion for filtering out reservation requests
that could generate deadlock situations. Since the drivers communicate the time at which
they plan to arrive at the intersection, as well as what their speed will be when they get
there (quantities which the drivers have no incentive to misrepresent), it is possible to
approximate a vehicles distance from the intersection, given a reservation request by that
vehicle. This heuristic approximation, called the reservation distance d(r), is calculated as
d(r) = va  (ta  t), where va is the proposed arrival speed of the vehicle, ta is the proposed
arrival time of the vehicle, and t is the current time.
This approximation assumes that the vehicle is maintaining a constant speed. The
reservation processing policy uses it as follows. For each lane i, the policy has a variable di ,
initialised to infinity, that represents the maximum distance from which a driver can send
a reservation request. For each reservation request r from lane i, the policy computes the
reservation distance, d(r). If d(r) > di , r is rejected. If, on the other hand, d(r)  di , r is
processed as normal. If r is rejected after being processed as normal, di  min(di , d(r)).
Otherwise, di  . While the use of the reservation distance does not guarantee that
630

fiA Market-Inspired Approach for Intersection Management

mutually blocking situations never occur, it does prevent these situations from degenerating
into deadlocks.

4. Single Intersection
For a single reservation-based intersection, the problem that the intersection manager has to
solve is allocating the reservations among a set of drivers in a way that a specific objective is
maximised. This objective can be, for instance, minimising the average delay caused by the
presence of the regulated intersection. In this case, the simplest policy to adopt is allocating
a reservation to the first agent that requests it, as occurs with the FCFS policy proposed
by Dresner and Stone in their original work. Another work in line with this objective takes
inspiration from adversarial queuing theory for the definition of several alternative control
policies that aim at minimising the average delay (Vasirani & Ossowski, 2009a)
However, these policies ignore the fact that in the real world, depending on the context
and their personal situation, people value the importance of travel times and delays quite
differently. Since processing the incoming requests to grant the associated reservations can
be considered as the process of assigning resources to agents that request them, one may
be interested in an intersection manager that aims to allocate the disputed resources to
the agents that value them the most. In line with approaches from mechanism design, we
assume that the more a human driver is willing to pay for the desired set of space-time slots,
the more they value the good. Therefore, we rely on combinatorial auction theory (Krishna,
2002) for the definition of an auction-based policy for the allocation of resources.
4.1 Auction-Based Policy
To formalise an auction-based policy for processing incoming reservation requests, it is
necessary to specify the auction design space. This includes the definition of the disputed
resources, the rules that regulate the bidding and the clearing policy.
4.1.1 Auctioned Resources
The first step for the design of any auction is the definition of the resources (or items)
to be allocated. The nature of items determines which type of auction can be employed
to allocate them. In our scenario, the auctioned good is the use of the space inside the
intersection at a given time. We model an intersection as a discrete matrix of space slots.
Let S be the set of the intersection space slots, S = {s1 , s2 , . . . , sm }. Let tnow be the current
time, and T = {tnow + ,   N} the set of future time-steps. The set of items that a
bidder can bid for is the set I = S  T . Due to the nature of the problem, a bidder is
only interested in bundles of items over the set I. In the absence of acceleration in the
intersection, a reservation request (Figure 4) implicitly defines which space slots at which
time the driver needs in order to pass through the intersection6 . Thus, the items must
necessarily be allocated through a combinatorial auction.
6. This computation is easily done by the intersection manager, which knows the geometry of the intersection. If the vehicles were to calculate the trajectory, they would need to know the geometry of every
intersection they pass through.

631

fiVasirani & Ossowski

t4

t4

t3

t3

t2

t2

t4

t4

t3

t3

t2

t2

t1

t1

t1

t1

t0

t0

t0

t0

Figure 4: Bundle of items defined by a reservation request.

4.1.2 Bidding Rules
The bidding rules define the form of a valid bid accepted by the auctioneer (Wurman,
Wellman, & Walsh, 2001). In our scenario, a bid over a bundle of items is implicitly defined
by the reservation request. Given the parameters arrival time, arrival speed, lane and type
of turn, the auctioneer (i.e., the intersection manager) is able to determine which space slots
are needed at which time. Thus, the additional parameter that a driver must include in its
reservation request is the value of its bid, i.e., the amount of money that it is willing to pay
for the requested reservation.
A bidder is allowed to withdraw its bid and to submit a new one. This may happen,
for instance, when a driver that submitted a bid b, estimating to be at the intersection
at time t, realises that, due to changing traffic conditions, it will more likely to be at the
intersection at time t + t, thus making the submitted bid b useless for the driver. In this
case the driver has no guarantees of safety regarding its crossing of the intersection. Thus,
the rational thing to do in this case, as the driver would not want to risk being involved in
a car accident, is resubmitting the bid with the updated arrival time. However, the new bid
must be greater than or equal to the value of the previous one. This constraint avoids the
situation whereby a bidder blocks one or several slots for itself, by acquiring them early
and with overpriced bids. Even though this would oblige others to try to reserve alternative
slots, and thus make the desired slot less disputed, the bidder cannot take advantage of
this, as it cannot withdraw its initial bid and resubmit lower bids in order to obtain the
same reservation at a lower price.
632

fiA Market-Inspired Approach for Intersection Management

new bids

winners
t

bid set

Send
CONFIRMATION

Solve
WDP

t

Collect
incoming
bids

losers

Collect
incoming
bids

Send
REJECTION

new bids

Figure 5: Auction policy

4.1.3 Auction Policy
The auction policy (see Figure 5) starts with the auctioneer waiting for bids for a certain amount of time t. Once the new bids are collected, they constitute the bid set.
Then, the auctioneer executes the algorithm for the winner determination problem (WDP),
and the winner set is built, containing the bids whose reservation requests have been accepted. During the WDP algorithm execution, the auctioneer still accepts incoming bids,
but they will only be included in the bid set of the next round. Then the auctioneer sends a
CONFIRMATION message to all bidders that submitted the bids contained in the winner set,
while a REJECTION message is sent to the bidders that submitted the remaining bids. Then
a new round begins, and the auctioneer collects new incoming bids for a certain amount of
time7 .
4.1.4 Winner Determination Algorithm
Since the auction must be performed in real-time, both the bid collection and the winner
determination phase must be time-bounded, that is, they must occur within a specific time
window. This implies that optimal and complete algorithms for the WDP (Leyton-Brown,
Shoham, & Tennenholtz, 2000; Sandholm, 2002) are not suited for this kind of auction. An
algorithm with anytime properties is needed (Hoos & Boutilier, 2000), so that the longer
the algorithm keeps executing, the better the solution it finds.
7. For safety reasons the auctioneer cannot spend too much time collecting bids, nor can it deallocate
previously granted reservations. Therefore it is possible that a low-valued bid, in the winner set at round
k, impedes the allocation of the disputed reservation to some high-valued bids, submitted at round k + n.
In this case, the second bidder should slow down and resubmit a new (possibly winning) bid. Although
in theory the bid-delay relation (Figure 7) could be worsened by the unrelated sequence of auctions, in
practice the effect is negligible.

633

fiVasirani & Ossowski

Algorithm 1 Winner determination algorithm
B  allBids
W
start  currentT ime
while currentT ime  start < 1 sec do
A
for step = 1 to |B| do
step  step + 1
random  drawU nif ormDistribution(0, 1)
if random < wp then
b  selectRandomlyF rom(B \ A)
else
highest  selectHighestF rom(B \ A)
secondHighest  selectSecondHighestF rom(B \ A)
if highest.age  secondHighest.age then
b  highest
else
random  drawU nif ormDistribution(0, 1)
if random < np then
b  secondHighest
else
b  highest
end if
end if
end if !
AA
{b} \ N (b)
if A.value > W.value then
WA
end if
end for
end while
Algorithm 1 sketches how the winner determination problem is solved. The algorithm
starts initialising the set B containing all the bids received so far. The winner set W
is initialised to the empty set. Once the initialisation has been concluded, the algorithm
executes the main loop for 1 second. Within the main loop, a stochastic search is performed
for a number of steps equal to the number of bids in B. Set A contains the candidate bids
for the winner set. Then, with probability wp (walk probability8 ), a random bid is selected
from the set of bids that are not actually in the candidate winner set (B \ A), while, with
probability 1  wp, the highest and the second highest bids are evaluated. The highest bid
is selected if its age (i.e., the number of steps since a bid was last selected to be added to a
candidate solution) is greater than or equal to the age of the second highest bid. Otherwise,
8. The probability of adding a random, not previously allocated bid to the candidate winner set.

634

fiA Market-Inspired Approach for Intersection Management

Figure 6: Simulator of a single intersection
with probability np (novelty probability9 ) the second highest, and with probability 1  np
the highest bid is selected. Finally bid b is added to the candidate solution A and all its
neighbours N (b), that is, the set of bids over bundles that share with b at least one item,
are removed from A. Finally, if the value of A (i.e., the sum of the bids in A) is greater
than the value of the best-so-far winner set, W, the best solution found so far is updated.
4.2 Simulation Environment
The simulator we use for the evaluation of our auction-based policy is a custom, microscopic,
time-and-space-discrete simulator, with simple rules for acceleration and deceleration. The
simulated area is modelled as a grid, and subdivided in lanes (see Figure 6). Each lane is
3m wide, and subdivided in 12 squared tiles of 0.25m each. Each vehicle is modelled as a
rectangle of 816 tiles, or equivalently, as a rectangle of 2m4m, and has a preferred speed
in the interval [30, 50]km/h. The simulation environment generates the origin-destination
pair randomly. When a vehicle is spawned inside the simulation, it is inserted at the
beginning of one of the 4 incoming links, randomly selected, and a destination is randomly
assigned to it. The destination implies the type of turn (left, right or straight) that the
vehicle will perform at the intersection as well as the lane it will use to travel (the leftmost lane in case of left turn, the right-most lane in case of right turn, any lane for going
straight). The preferred speed is assigned using a normal distribution with mean 40km/h
and variance 5km/h, while being limited by the interval [30, 50].
9. The probability of adding to the candidate winner set the second highest bid rather then the greedy
bid, i.e., the highest in value.

635

fiVasirani & Ossowski

Since the link used to approach the intersection is relatively short, we assume that each
vehicle will travel in its pre-assigned lane, without changing it. Therefore, we only need
a car-following model to simulate the vehicle dynamics, and no lane-changing model is
needed. The car-following model we use is the Intelligent Driver Model (Treiber, Hennecke,
& Helbing, 2000). In this model, the decision of any driver to accelerate or to brake depends
only on its own speed, and on the speed of the vehicle immediately ahead of it. Specifically,
the acceleration dv/dt of a given vehicle depends on its speed v, on the distance s to the
front vehicle, and on the speed difference v (positive when approaching) :
"
# $ #  $2 %
dv
v
s
= a 1

dt
vp
s

(1)

where


s = s0 +

#

v  v
vT +

2 ag

$

(2)

and a is the acceleration, g is the deceleration10 , v is the actual speed, vp is the preferred
speed, s0 is the minimum gap, T is the time headway.
The acceleration is divided into an acceleration towards the preferred speed on a free
road, and braking decelerations induced by the front vehicle. The acceleration on a free
road decreases from the initial acceleration a to 0 when approaching the preferred speed vp .
The braking term is based on a comparison between the preferred distance s , and
the current gap s with respect to the front vehicle. If the current gap is approximately
equal to s , then the braking deceleration essentially compensates the free acceleration
part, so the resulting acceleration is nearly zero. This means that s corresponds to the
gap when following other vehicles in steady traffic conditions. In addition, s increases
dynamically when approaching slower vehicles and decreases when the front vehicle is faster.
As a consequence, the imposed deceleration increases with decreasing distance to the front
vehicle, increasing its own speed, and increasing speed difference to the front vehicle. The
aforementioned parameters were set to vp = 50km/h, T = 1.5s, s0 = 2m, a = 0.3m/s2 ,
b = 3m/s2 . The speed of a vehicle is updated every second, and its position, since the space
is discrete, is updated to the tile closest to the new position in the continuous space.
4.3 Experimental Results
We create different traffic demands by varying the expected number of vehicles () that,
for every O-D pair, are spawned in an interval of 60 seconds, using a Poisson distribution.
We spawned vehicles for a total time of 30 minutes. Table 1 shows the number of vehicles
that have been generated for different values of .
The main goal of this set of experiments is to test whether the policy based on combinatorial auction (CA) enforces an inverse relation between money spent by the bidders
and their delay. The delay measures the increase in travel time due to the presence of the
intersection. It is computed as the difference between the travel time when the intersection
10. a and g are different parameters with different values, since usually a vehicle decelerates (i.e., brakes)
more strongly than it accelerates.

636

fiA Market-Inspired Approach for Intersection Management


# of vehicles

1
29

5
136

10
285

15
438

20
633

25
716

30
832

Table 1: Traffic demands for a single intersection
is regulated by the intersection manager, and the travel time that would arise if the vehicle
could travel unhindered through the intersection. The bid that a driver is willing to submit
is drawn from a normal distribution with mean 100 cents and variance 25 cents, since the
willingness of human drivers to pay is usually normally (or log-normally) distributed (Hensher & Sullivan, 2003). Thus, the agents are not homogeneous in the sense that the amount
of money that they are offering differs from one to another. In this population, we track
the delay of a subset of drivers, which are endowed with 10, 50, 100, 150, 200, 1000, 1500,
2000 and 10000 cents. This endowment is entirely allocated as a bid. We also evaluate the
auction-based policy with respect to the average delay of the entire population of drivers.
For the WDP algorithm, we set the walk probability wp = 0.15 and the novelty probability np = 0.5, as these values produced the best results in auctions of similar type and
size (Hoos & Boutilier, 2000). In all the experiments, we give the intersection manager one
second to execute the WDP algorithm and return a solution. To give more time to bidders to submit their bids, before starting another auction, the intersection manager waits
another second to collect incoming bids11 . To determine if one second is enough for the
winner determination algorithm to produce acceptable results, we performed the following experimental analysis. According to the results reported by Hoos and Boutilier, given
an auction with 100 bids, the winner determination algorithm is able to find the optimal
solution with a probability of 0.6, which tends to 1 if the algorithm is allowed to run for
more than 10 seconds. This is encouraging, but in order to justify the adequacy of the
stochastic algorithm for our particular problem, we need to show that, in the context of
the auction-based policy for reservation-based intersection control, it produces results that
are reasonably close to the optimum, despite the relatively short time (1 second in the experiments) that the algorithm has to return a solution. Given that the average number of
submitted bids for a single auction is between 3 for low traffic demand ( = 1) and 80 for
high traffic demand ( = 30), we performed several experiments to compare the solution
provided by the algorithm with 1 second of run-time with the solution provided by the algorithm with 100 seconds. The solution provided by the second execution of the algorithm
is assumed to be the best approximation of the optimal solution. The result was that the
winner determination algorithm is able to find a solution whose value is at least 95% of the
optimal solution value with a probability between 96.1% for high traffic demand ( = 30)
and 99.2% for low traffic demand ( = 1).
Figure 7 plots (in logarithmic scale) the relation between travel time and bid value
for different values of . All the error bars denote 95% confidence intervals. There is a
sensible decrease of the delay experienced by the drivers that bid from 100 to 150 cents,
which represent 49.8% of drivers whose bid is greater than the mean bid. Still, such delay
reduction tends to settle for drivers that bid more than 1000 cents.
11. Nevertheless, the intersection manager runs a separate thread that receives incoming bids also during
the WDP algorithm execution.

637

fiVasirani & Ossowski

500

90

80

70

60

10

100

1000

Bid (cents)
(a)  = 10

10000

900

450

Avg delay (sec)

Avg delay (sec)

Avg delay (sec)

100

400

350

300

250

800

700

600

500
10

100

1000

Bid (cents)
(b)  = 20

10000

10

100

1000

10000

Bid (cents)
(c)  = 30

Figure 7: Bid-delay relation for various values of  and normally distributed endowments

We remark that the auction-based policy also uses the reservation distance as preprocessing step, which guarantees that a drivers bid cannot be rejected indefinitely. In
fact, a vehicle is allowed to approach the intersection and slow down until it reaches the
intersection edge. At that point, if its request is rejected because another driver submitted
a higher value bid, the reservation distance is updated to the stopped vehicles distance.
Therefore, in the following time step, only this driver will be allowed to submit a bid with its
preferred value. The result is, of course, that this driver will suffer greater delays compared
to other drivers that are willing to pay more12 .
The auction-based allocation policy has proven to be effective regarding its main goal,
that is, rewarding lower delays to those drivers that value their disputed reservations the
most. However, it is worth analysing the impact that such a policy has on the intersections
average delay. Figure 8a plots the average delay for different traffic demands (  [1, 30]).
Again, the error bars denote 95% confidence intervals. When traffic demand is low, the performance of the CA policy and the FCFS is approximately the same. However, when traffic
demand increases, there is a noticeable increase of the average delay when the intersection
manager applies CA. This was somewhat expected, because the CA policy aims to grant
a reservation to the driver that values it the most, rather than maximising the number of
granted requests. Thus, a bid b, whose value is greater than the sum of n bids that share
some items with b, is likely to be selected in the winner set. If so, only 1 vehicle will be
allowed to transit, while n other vehicles will have to slow down and try again. This fact
is highlighted also by the average rejected requests (Figure 8b). Since all the non-winning
bids are rejected, the number of rejected requests with the CA policy is up to four times
greater than with the FCFS policy.
12. Although we focus on technical problems and not social or political ones, one may wonder whether it is
fair that rich drivers can travel faster than poor drivers using a road-infrastructure that is a public
good. Nevertheless, we could argue that through the money raised by the auction-based policy rich
drivers contribute much more to the maintenance and extension of the public road infrastructure than
poor drivers.

638

fiA Market-Inspired Approach for Intersection Management

FCFS
CA

600

Avg delay (sec)

Avg rejected requests (%)

700

500
400
300
200
100
0
5

10

15

20

25

30

FCFS
CA

20

15

10

5

5

10

15



20

25

30



(a)

(b)

Figure 8: Average delay (a) and average rejected requests (b)
4.4 Discussion
The principle of optimising the use of the available resources is not the unique guiding
principle of a traffic controller. In the real world, depending on the context and their
personal situation, drivers value the importance of travel times and delays quite differently.
Thus, it makes sense to elaborate control policies that are aware of these different valuations
and that reward the drivers that value the disputed resources the most. In this respect,
we evaluated a control policy for reservation-based intersections that relies on an auction
mechanism. With such a policy, drivers that submit high-value bids usually experience
significant reductions in their individual delays (about 30% less compared to drivers that
submit low-value bids).
However, since the objective of this policy is not maximising the number of granted
reservations, it pays a social cost, in the form of greater average travel times. This fact
might limit the applicability of the CA policy in high load situations. In this case, additional
mechanisms to reduce the number of vehicles that approach a single intersection are needed.
It is also worth noting how it is possible that a driver, even with a theoretically infinite
amount of money, cannot experience zero delay when approaching an intersection. This
is because an auction carried on in a realistic traffic scenario is quite different from a
synthetic auction that has been set-up for benchmarking purposes (Hoos & Boutilier, 2000).
The auctions that arise in the traffic scenario are affected by the high level of dynamism,
uncertainty and noise, intrinsic to the domain. For example, in high load situations, the
reservation distance plays an important role, since it filters out many potentially winning
bids coming from a greater distance13 . Figure 9 plots how the reservation distance decreases
over time for different traffic demands. In high load situations, the reservation distance
tends to be small, therefore a wealthy driver must reach this reservation distance in order
to participate in the auction and acquire a reservation, thus increasing its travel time. The
estimation of the arrival time also greatly affects the performance of the auction. In fact, in
13. As outlined in Section 3.2, the reservation distance is the maximum distance at which a driver is allowed
to request a reservation.

639

fiReservation distance (m)

Vasirani & Ossowski

350

300

250

200






150

0

=1
= 10
= 20
= 30
5

10

15

20

25

Time (min)

Figure 9: Reservation distance

high load situations, such an estimation is much more noisy and uncertain, and it is likely
that a driver must resubmit a reservation request with the updated arrival time. In this
way, it is possible that an agent wins an auction at time t and then, due to a new estimation
of the arrival time, must resubmit its bid at time t + t. The bidders that participate in
the auction at time t + t are obviously different from those that participated at time t, so
there is no guarantee that the agent might win the auction again.
Furthermore, a real-world scenario such as urban traffic limits the auction design space
and the applicable solution methods for winner determination and payments calculation.
In fact, we gave priority to the winner determination problem, adapting a local search algorithm to our needs, while for the payments calculation we did not adopt any sophisticated
method, i.e., a winner pays a price that is exactly the bid that was submitted. This, as
with any first-price payment mechanism, could in principle lead to malicious behaviours,
with drivers that try to acquire reservations by submitting bids that are lower than the
real valuations they have. In single item auctions it is computationally easy to set up an
incentive compatible payment mechanism, such as the second-price (Vickrey) mechanism.
Unfortunately, extending this mechanism to combinatorial auctions in not (computationally) straightforward, since the equivalent truth-revealing mechanism in the combinatorial
world, the Vickrey-Clarke-Groves (VCG) payment mechanism (Clarke, 1971; Groves, 1973;
Vickrey, 1961), is NP-complete. Therefore, although a driver agent could potentially acquire
a reservation by submitting a bid &b that is lower than its real valuation b, from a practical
point of view this exclusively affects the revenues that the auctioneer should gain if every
bidder were truth-telling, which is not our primary concern. Another possible weakness is
the fact that a bidder could start bidding lower than their real valuation and then raising their bid if they are not able to acquire it, thus leading to a communication overhead
between bidders and auctioneer. Nevertheless, only the bidders within the reservation distance are able to submit a bid, thus the number of bids that the intersection manager may
receive simultaneously is necessarily bounded.
640

fiA Market-Inspired Approach for Intersection Management

5. Network of Intersections
In the single intersection scenario we analysed the performance of an auction-based policy
for the allocation of reservations. In that context, the driver was modelled as a simple agent
that selects the preferred value for the bid that will be submitted to the auctioneer. If we
focus on an urban road network with multiple intersections, it is interesting to notice that
the decision space of a driver is much broader. In fact, drivers are involved in complex and
mutually dependent decisions such as route choice and departure time selection. At the same
time, this scenario opens new possibilities for intersection managers to affect the behaviour
of drivers. For example, an intersection manager may be interested in influencing the
collective route choice performed by the drivers, using variable message signs, information
broadcast, or individual route guidance systems, so as to evenly distribute the traffic over
the network. This problem is called traffic assignment.
In Section 5.1 we evaluate how market-inspired methods (Gerding et al., 2010) can be
applied as traffic assignment strategies for networks of reservation-based intersections. The
idea is that, if there is a market where drivers acquire the necessary reservations to pass
through the intersections of the urban network, this market, and the intersection managers
that operate in it from the supply side, can be designed to work as a traffic assignment
system. In particular, we model the intersection managers so that they apply a competitive
pricing strategy to compete among themselves for the supply of the reservations that are
traded. Finally, in Section 5.2 we combine this traffic assignment strategy with the auctionbased control policy into an integrated mechanism for traffic management of urban road
networks.
5.1 Competitive Traffic Assignment (CTA)
Traffic assignment strategies aim at influencing the collective route choice of drivers in order
to use the road network capacity efficiently. Therefore, we can see the traffic assignment
problem as a distributed choice and allocation problem, since a set of resources (i.e., the
links capacity) must be allocated to a set of agents (i.e., the drivers). To this regard,
markets as mediators for distributed resource allocation problems have been applied to
several socio-technical systems (Gerding et al., 2010).
Setting out from the approach outlined in the work by Vasirani and Ossowski (2011), we
follow this metaphor and model each intersection manager as a provider of the resources, in
this case, the reservations of the intersection it manages. Thus, each intersection manager
is free to establish a price for the reservations it provides. On the other side of the market,
each driver is modelled as a buyer of these resources. Provided with the current prices of
the reservations, it chooses the route, according to its personal preferences about travel
times and monetary costs. Each intersection manager is modelled so as to compete with
all others for the supply of the reservations that are traded. Therefore, our goal as market
designers is making the intersection managers adapt their prices towards a price vector that
accounts for an efficient allocation of the resources.
641

fiVasirani & Ossowski

5.1.1 CTA Pricing Strategy
Let L be the set of incoming links of a generic intersection. For each incoming link l  L,
the intersection manager defines the following variables:
 Current price pt (l): is the price applied by the intersection manager to the reservations
sold to the drivers that come from the incoming link l.
 Total demand dt (l | pt (l)): represents the total demand of reservations from the
incoming link l that the intersection manager observes at time t, given the current
price pt (l). It is given by the number of vehicles that want to cross the intersection
coming from link l at time t.
 Supply s(l): defines the reservations supplied by the intersection manager for the
incoming link l. It is a constant and represents the number of vehicles that cross the
intersection coming from link l that the intersection manager is willing to serve.
 Excess demand z t (l | pt (l)): is the difference between the total demand at time t and
the supply, z t (l | pt (l)) = dt (l | pt (l))  s(l).
Given the set of all the intersection managers that are operating in the market, J , we
define the price vector pt as the vector of the prices applied by each intersection manager
to each of its controlled links:
pt = [ pt1 (l1 ) pt1 (l2 ) . . . pt|J | (lh ) ]

(3)

where p1 (l1 ) is the price applied by intersection manager 1 to its controlled link l1 , p1 (l2 )
is the price applied by the same intersection manager to another link l2 of its intersection,
and p|J | (lh ) is the price applied by the |J |th intersection manager to its last controlled link
lh .
In particular, we say that a price vector pt maps the supply with the demand if the excess
demand z t (l | pt (l)) is 0 for all links of the network. This price vector, which corresponds
to the market equilibrium price, can be computed through a Walrasian auction (Codenotti,
Pemmaraju, & Varadarajan, 2004), where each buyer (i.e., driver) communicates to the
suppliers (i.e., intersection managers) the route that it is willing to choose, given the current
price vector pt . With this information, each intersection manager computes the demand
dt (l | pt (l)) as well as the excess demand z t (l | pt (l)) for each of its controlled links. Then,
each intersection manager adjusts the prices pt (l) for all the incoming links, lowering them
if there is excess supply ( z t (l | pt (l)) < 0 ) and raising them if there is excess demand
( z t (l | pt (l)) > 0 ). The new price vector pt+1 is communicated to the drivers that
iteratively choose their new desired route, on the basis of the new price vector pt+1 . Once
the equilibrium price is computed, the trading transactions take place and each driver buys
the required reservations at the intersections that lay on its route.
The Walrasian auction relies on quite strict assumptions, which make a direct implementation in the traffic domain hard. For instance, the set of buyers is assumed to be fixed
during the auction, which means for the traffic domain that new drivers may not join an
auction until it terminates. Also the fact that no transactions can take place at disequilibrium prices is a strict assumption for the traffic domain. It is unreasonable for all the
642

fiA Market-Inspired Approach for Intersection Management

Algorithm 2 Intersection manager price update
t0
for all l  L do
pt (l)  
s(l)  0.5  opt  $(l)
end for
while true do
for all l  L do
dt (l)  evaluateDemand
z t (l)  dt (l)  s(l)
z t (l)
pt (l)  pt (l) + pt (l) 
s(l)
end for
tt+1
end while
drivers to wait to reach the equilibrium point before choosing the desired route and starting
to travel. Finally, a driver is probably willing to transfer money to an intersection manager
when it is spatially close to it, that is, when it is already travelling along its desired route.
Thus, we implement a pricing strategy that aims to reach the equilibrium price - as in
the Walrasian auction - but that works on a continuous basis, with drivers that leave and
join the market dynamically, and with transactions that take place continuously. To reach
general equilibrium, each intersection manager applies the price update strategy sketched
in Algorithm 2. At time t, each intersection manager independently computes the excess
demand z t (l | pt (l)) and updates the price pt (l) using the formula (Codenotti et al., 2004):
'
(
z t (l | pt (l))
t+1
t
t
p (l)  max , p (l) + p (l) 
(4)
s(l)

where

  is the minimum price that an intersection manager charges for the reservations that
it sells.
 s(l) is the supply of the intersection manager, that is, the number of vehicles above
which the intersection manager considers there is excess demand and starts to raise
prices.
We claim that drivers that travel through road network links with low demand shall not
incur any costs. For this reason, we choose  = 0. To define the supply s(l), we rely on
the fundamental diagram of traffic flow (Gerlough & Huber, 1975). Let opt be the density
that maximises the traffic flow on link l (see Figure 10). We choose s(l) = 0.5  opt  $(l),
where $(l) is the length of link l. In other words, the intersection manager considers that
there is excess demand when the density reaches 50% of optimal density. In this way the
intersection manager aims to avoid exceeding opt by raising prices and diverting drivers to
different routes before reaching opt .
643

fiVasirani & Ossowski

Trafc ow (veh/h)

opt

opt
Density (veh/km)

Figure 10: Fundamental diagram of traffic flow
5.1.2 Driver Model
Unlike the single intersection scenario, in this case we need a reasonable driver model for
the route choice. The route choice problem is modelled as a multi-attribute utility-functionmaximisation problem. Given that the traffic system is regulated by a market mechanism,
the driver must take into consideration different aspects of a route to determine its utility
value. A route  is modelled as an ordered list of links,  = [l1 . . . lN ]. A generic link
lk is characterised by two attributes: the estimated travel time E[T (lk )] and the price of
reservations K(lk ). For sake of simplicity, the estimation is based on the travel time at
free flow, and does not consider real-time information of traffic conditions (see Equation 5,
where $(lk ) is the length of link lk , and vmax (lk ) is the maximum allowed speed on link lk ).
The price of reservations of link lk is always 0, unless the link lk is one of the incoming link
of an intersection (lk = l), in which case the price is pt (l) (Equation 6).
E[T (lk )] =

K(l ) =
k

)

$(lk )
vmax (lk )

pt (l)
0

if lk = l  L
otherwise

(5)

(6)

The summatory of the estimated travel time over all the links of  gives the estimated travel
time of the entire route :
E[T ()] =

N
*

E[T (lk )]

(7)

k=1

Similarly, the summatory of the price of reservations over all the links of  gives the price
of the entire route :
K() =

N
*
k=1

644

K(lk )

(8)

fiA Market-Inspired Approach for Intersection Management

Let C = {1 , . . . , M } be the choice set, that is, the set of routes available to a driver. The
set C is built using a k-shortest paths algorithm (Yen, 1971), with k = 10. Let uT () be
the normalised utility of route  against the estimated travel time attribute (Equation 9),
where MT = max E[T (i )] and mT = min E[T (i )].
i C

i C

uT () =

MT  E[T ()]
MT  mT

(9)

Let uK () be the normalised utility of route  against the reservations cost attribute (Equation 10), where MK = max K(i ) and mK = min K(i ).
i C

i C

uK () =

MK  K()
MK  mK

(10)

The driver multi-attribute utility of route  is then defined as:
U () = wT  uT () + wK  uK ()

(11)

where wT is the weight of the estimated travel time attribute and wK is the weight of the
cost of reservations attribute. Basically, if wT = 1 the driver utility only considers the
attribute related to the estimated travel time (i.e., it prefers the shortest route, no matter
the price of the reservations), if wK = 1 the driver utility only considers the attribute
related to the cost of reservations (i.e., it prefers the cheapest route, no matter the travel
time), while for every other combination of the weights wT and wK the driver considers
the trade-off between estimated travel time and cost of reservations. In the experiments we
draw wT from a uniform distribution over the interval [0, 1], and we set wK = 1  wT .
Once the utility of the routes that form the choice set C has been computed, the driver
must choose one of these alternatives. In this work, we model the driver as a deterministic
utility maximiser that always selects the route with the highest utility value. Since the
price of the incoming links of an intersection is changing dynamically, the term uK () in
Eq. 11 may change during the journey. For this reason, the driver continuously evaluates
the utility of the route it is following and, in case that a different route becomes more
attractive, it may react and change on-the-fly how to reach its destination, selecting a route
different from the original one.
5.1.3 Simulation Environment
The experimental evaluation is performed on a hybrid mesoscopic-microscopic simulator,
where the traffic flow on the roads is modelled at mesoscopic level (Schwerdtfeger, 1984),
while the traffic flow inside the intersections is modelled at microscopic level (Nagel &
Schreckenberg, 1992).
In a mesoscopic model vehicle dynamics is governed by the average traffic density on the
link it traverses rather than the behaviour of other vehicles in the immediate neighbourhood
as in microscopic models. A road network is modelled as a graph, where the nodes represent
intersections and the edges represent the lanes of a road. An edge, also called stretch, is
subdivided into sections (of typically 500m length) for which a constant traffic condition is
assumed. A vehicle i that at time t is driving on a link lk is characterised by its position
645

fiVasirani & Ossowski

xti  [0, $(lk )], and its speed vit . At each time step, a new target speed for each vehicle is
computed, using the formula:
v&it+t = (1 

xti
xti
k
)

y(l
)
+
 y(lk+1 )
$(lk )
$(lk )

(12)

where y(lk ) is the reference speed of link lk and y(lk+1 ) is the reference speed of link lk+1 .
Such reference speeds are calculated by taking into consideration the mean speed of the link
and the vehicles desired speed. The mean speed of the link is calculated with a speed-density
function that for a given links density (lk ) returns the links mean speed (Schwerdtfeger,
1984).
The equation above takes into consideration the fact that the closer the vehicle is to
the next link lk+1 , the higher is the effect of the link reference speed on the vehicle target
speed. If the new target speed v&it+t is higher (lower) than the current speed vit , the vehicle
accelerates (decelerates) with a vehicle-type specific maximum acceleration (deceleration).
The new speed is then denoted by vit+t . Finally, the vehicle position is updated using the
formula:
1
 (vit + vit+t )  t
(13)
2
If xt+t
 $(lk ), the vehicle is placed in the next link of its route, the densities for link lk
i
and lk+1 are updated accordingly, and the position is reset to xt+t
 $(lk ).
i
The mesoscopic model described above does not offer the necessary level of detail to
model a reservation-based intersection. For this reason, when a vehicle enters an intersection, its dynamics switches into a microscopic, cellular-based, simulator (Nagel & Schreckenberg, 1992), similar to the simulation environment used in Section 4.2. Still, the cells that
compose the intersections area are more coarse grained (5 meters), and for simplicity we
assume that the vehicles cross the intersection at a constant speed, so that any additional
tuning of parameters, such as slowdown probability or acceleration/deceleration factors, is
not necessary.
xt+t
= xti +
i

5.1.4 Experimental Results
Although our work does not depend on the underlying road network, we chose a (simplified)
topology of the entire urban road network of the city of Madrid for our empirical evaluation
(see Figure 11). The network is characterised by several freeways that connect the city
centre with the surroundings and a ring road. Each large dark vertex in Figure 11 - if it
connects three or more links - is modelled as a reservation-based intersection. We aim to
recreate a typical high load situation (i.e., the central, worst part of a morning peak), with
more than 11,000 vehicles departing within a time window of 50 minutes (see Table 2). The
vehicles that travel to and from 7 destinations outside the city (marked with O1 up to O7
in Figure 11) form the traffic under evaluation.
The market-inspired traffic assignment strategy is compared with a network of FCFS
reservation-based intersections. In the latter, the drivers route choice only takes into consideration the expected travel time at free flow, since there is no notion of price.
We focus on two different types of metrics, one related to the vehicles and one related
to the network. The network-related metric is the density variation over time at 7 critical
646

fiA Market-Inspired Approach for Intersection Management

Figure 11: Urban road network

Origin
O1
O2
O3
O4
O5
O6
O7

O1

O2

Destination
O3
O4
O5

O6

O7

223
300
208
199
290
224

323
364
233
228
316
231

355
221
229
261
398
214

349
214
368
199
238
253

271
229
362
204
209
337
-

336
248
343
216
386
235

311
191
358
218
374
219

Table 2: OD Matrix (# of vehicles)
intersections (marked with c1 . . . c7 in Figure 11), which connect the freeways going toward
the city centre with the ring road. The vehicle-related metric is the average travel time,
grouped by the origin-destination (O-D) pair. For a given O-D pair, we compute the average
travel time of the vehicles that go from O to D. This measurement is then averaged over 30
runs. Furthermore, for each O-D pair we compute the improvement % of CTA over FCFS
based on the average travel times. Table 3 shows the average travel time of the drivers,
according to their origin-destination pairs, when the reservations are allocated through the
647

fiVasirani & Ossowski

competitive traffic assignment (CTA) and when they are granted with the usual FCFS
policy. Using CTA we observe a net reduction of the average travel time for 30 of 42 origindestination pairs. Such reduction is generally noteworthy for the busiest14 routes, such as
O6 -O2 , O6 -O3 and O7 -O3 . Along the some of the less demanded O-D pairs, FCFS is the
best performing policy. This happens when on the most preferred route from O to D the
traffic density is already low enough to assure free flow, but there exist alternative routes
with even lower demand, and CTA keeps diverting traffic along these potentially longer and
thus slower routes.
To evaluate the effects of the trading activity between drivers and intersection managers
it is worth observing the density variation over time at the critical intersections c1 to c7 ,
plotted in Figure 12. In general, density tends to be lower with CTA compared with the
system regulated by FCFS intersection managers. At the least demanded intersections c1 ,
c2 and c7 , that is, those intersections whose density is below the density that maximises
traffic flow (see Figure 10), there is no substantial difference between CTA and FCFS. These
critical intersections are less demanded due to the topology of the network. In fact, fewer
origins are located in the northern part (O1 , O2 and O7 ).
At the critical intersections c3 , c4 and c6 , the vehicle density with CTA is always below
the density that results from the use of FCFS, especially in the case of intersections c4 and
c6 where with CTA the density exceeds the optimal one by only a small extent and for a
limited period of time.
At intersection c5 , the density has a higher peak around 9:30, but the density starts to
exceed the optimal density later and begins to fall below the optimal density earlier. We
calculated the integral of the density curves, measured in the interval when the curve is
above the optimal density (Eq. 14)
+ t2
+ t2
CTA (t)dt and
FCFS (t)dt
(14)
t1

t1

where CTA and FCFS are the density functions, t1 = min( t | CTA (t) > opt , t | FCFS (t) >
opt ) and t2 = max( t | CTA (t) < opt , t | FCFS (t) > opt ). This metric is lower when
the reservations are allocated through the competitive market (70.24 veh  h/km versus
105.07 veh  h/km).
The result of the application of the market-inspired traffic assignment strategy is a
more balanced urban network, since the price fluctuations force demand to change towards
less expensive intersections. Such fluctuations contribute to creating a system in dynamic
equilibrium, where unused intersections became cheaper while congested ones became more
expensive. The effect is that average travel time decreases, although there are no guarantees
that those drivers that pay more are rewarded with lower travel times.

14. We empirically noticed in the experiments that the southern part of the network tends to be more
congested during the simulation. This is due to the fact that 4 of 7 origins/destinations (O3 , O4 , O5 ,
O6 ) are located in the southern part.

648

fiA Market-Inspired Approach for Intersection Management

Origin
CTA
O1 FCFS
%
CTA
O2 FCFS
%
CTA
O3 FCFS
%
CTA
O4 FCFS
%
CTA
O5 FCFS
%
CTA
O6 FCFS
%
CTA
O7 FCFS
%

Destination
O4
O5

O1

O2

O3

-

12.09
 0.27
11.98
 0.31
-0.8%

19.58
 0.80
22.89
 1.17
14.4%
14.17
 0.72
16.50
 1.06
14.1%

11.26
 0.17
10.15
 0.06
-11.0%
15.57
 0.33
13.35
 0.09
-16.7%
24.79
 0.77
26.94
 1.31
8.0%
26.80
 0.84
32.17
 1.83
16.7%
23.17
 0.50
22.51
 0.40
-2.9%
15.05
 0.22
14.31
 0.10
-5.2%

10.79
 0.14
9.76
 0.03
-10.6%
20.39
 0.60
22.58
 1.06
9.7%
22.83
 0.71
30.61
 1.70
25.4%
27.31
 0.55
57.01
 3.13
52.1%
23.52
 0.33
23.26
 0.40
-1.1%

11.62
 0.41
13.92
 0.82
16.5%
16.30
 0.67
21.54
 1.39
24.3%
25.30
 0.89
41.05
 2.59
38.4%
31.67
 0.82
56.42
 3.01
43.9%

26.70
 1.04
35.13
 1.80
24.0%
19.02
 0.66
25.87
 1.51
26.5%
9.18
 0.08
12.21
 0.62
24.8%
7.47
 0.20
8.83
 0.31
15.4%
16.40
 0.73
24.68
 1.62
33.6%
24.44
 0.97
34.99
 2.15
30.2%

30.75
 0.83
43.57
 1.89
29.4%
23.72
 0.83
31.05
 2.03
23.6%
13.99
 0.37
17.64
 0.92
20.7%
8.21
 0.27
10.05
 0.48
18.3%
12.12
 0.46
19.02
 1.50
36.3%
19.12
 0.69
31.24
 2.06
38.8%

O6

O7

21.17
 0.20
21.35
 0.40
0.8%
24.00
 0.40
38.09
 1.82
37.0%
18.54
 0.32
23.69
 6.34
21.7%
14.35
 0.48
15.74
 0.73
8.8%
11.11
 0.24
10.77
 0.26
-3.1%

14.13
 0.12
13.83
 0.09
-2.2%
20.88
 0.23
19.51
 0.15
-7.0%
24.95
 0.42
31.73
 1.36
21.4%
21.66
 0.75
22.74
 0.99
4.8%
19.47
 0.63
17.66
 0.52
-10.3%
16.58
 0.89
13.73
 0.32
-20.8%

11.69
 0.11
12.00
 0.20
2.5%

-

Table 3: Average travel time in minutes ( 95%CI): CTA vs. FCFS

649

fiVasirani & Ossowski

FCFS
CTA
Optimum

14
12
10
8
6
4
2

FCFS
CTA
Optimum

20

Density (veh/km)

Density (veh/km)

16

0

15

10

5

0

08:00:00

09:00:00

10:00:00

11:00:00

08:00:00

FCFS
CTA
Optimum

40

10:00:00

30

20

10

50

0

11:00:00

FCFS
CTA
Optimum

40

30

20

10

0
08:00

09:00

10:00

11:00

08:00:00

Time
(c) Intersection c3

09:00:00

10:00:00

11:00:00

Time
(d) Intersection c4

40
30
20
10

FCFS
CTA
Optimum

40

Density (veh/km)

FCFS
CTA
Optimum

50

Density (veh/km)

09:00:00

Time
(b) Intersection c2

Density (veh/km)

Density (veh/km)

Time
(a) Intersection c1

0

30

20

10

0
08:00

09:00

10:00

11:00

08:00

Time
(e) Intersection c5

10:00

11:00

Time
(f) Intersection c6

16

Density (veh/km)

09:00

FCFS
CTA
Optimum

14
12
10
8
6
4
2
0
08:00

09:00

10:00

11:00

Time
(g) Intersection c7

Figure 12: Density variation over time at the critical intersections under evaluation
650

fiA Market-Inspired Approach for Intersection Management

5.2 An Integrated Mechanism for Traffic Management (CA-CTA)
In Section 4.1, we introduced an auction-based policy for the control of a single intersection.
The experimental results showed that this policy was quite effective in allocating the reservations to the drivers that value them the most. Drivers that bid high usually experience
a great reduction in delay (about 30%), compared to those drivers that submit low-value
bids. However, this policy on its own showed a couple of drawbacks. First, it fosters the
attainment of a user optimum rather than a global one. It therefore pays a social price, in
the form of greater average delay for the entire population of drivers. Furthermore, it is
possible that even wealthy drivers, in high-load situations, could not get a reservation, for
example due to the decreasing reservation distance.
On the other hand, one of the results of the experimental evaluation of Section 5.1 was
that a traffic assignment strategy can make the task of traffic controllers easier, enforcing
a better distribution of traffic demand. Therefore, it seems reasonable to combine the
auction-based policy with the competitive traffic assignment strategy into an integrated,
market-inspired, mechanism for traffic management.
5.2.1 CA-CTA Mechanism
We adapt the competitive traffic assignment strategy (CTA) to combine it with the auctionbased policy (CA) into an integrated mechanism for traffic management (CA-CTA). Since
the intersection manager is the supplier of the reservations that are allocated through the
combinatorial auction, it may control the reserve price of the auctioned reservations, i.e.,
the minimum price at which the intersection manager is willing to sell. We model the
intersection managers in such a way that they compete for the provision of reservations
to the drivers, raising the reserve price in case of increasing demand or lowering it in case
of decreasing demand. The reservations are allocated through the CA policy defined in
Section 4.1. However, only bids whose value is above the reserve price are accepted in the
bid set.
For each incoming link l of a generic intersection, the intersection manager independently
computes the excess demand z t (l | ptr (l)) and updates the reserve price ptr (l) using the
formula:
'
(
z t (l | ptr (l))
t+1
t
t
pr (l)  max r , pr (l) + pr (l) 
(15)
s(l)

where r is the minimum reserve price, and s(l) is the number of vehicles that the intersection
manager is willing to serve. As in Section 5.1, we choose r = 0 and s(l) = 0.5  opt  $(l),
where $(l) is the length of link l, and opt is the density that maximises the traffic flow on
link l (see Figure 10).
5.2.2 Driver Model

To empirically evaluate CA-CTA we need to define a driver route choice model that takes
into consideration the fact that reservations are now allocated through a combinatorial
auction with a reserve price. We assume that each driver holds a private valuation of the
bids that it is willing to submit to pass through the intersections of its chosen route, defined
by the variable b. Given the monetary constraint, the driver selects the most preferred route
651

fiVasirani & Ossowski

, taking into consideration the estimated travel time associated with the route. A route 
is modelled again as an ordered list of links,  = [l1 . . . lN ], each of them characterised by
two attributes, namely estimated travel time and reserve price.
The travel time estimation is based, as before, on the travel time at free flow (Equation 5). The reserve price of a link is defined as:
) t
pr (l)
if lk = l  L
(16)
K(lk ) =
0
otherwise
The price of link lk is always 0, unless the link lk is one of the incoming link of an intersection
(lk = l), in which case the price is equal to the reserve price ptr (l) established by the
intersection manager. The summatory of the travel time over all the links of  gives the
estimated travel time at free flow of the entire route :
E[T ()] =

N
*

E[T (lk )]

(17)

k=1

Given b, the driver builds the choice-set C as the set of the routes whose intersections have
a reserve price lower than the desired bid b:
,
C = 1 , . . . , M | K(lk )  b lk  i

Once the choice-set is built, the driver selects the shortest route  = argmin E[T (i )].
i C

5.2.3 Experimental Results
We again recreate a typical high load situation, using the same network topology and OD
matrix of Figure 11 and Table 2. We are interested in two different types of properties. From
one side we must evaluate whether or not the integrated management mechanism (traffic
control+traffic assignment) guarantees lower delays to the drivers that submit higher bids
(user optimum). For this purpose, we calculate the average (percentage) increase of the
travel times D, calculated according to Equation 18, where T (i ) is the observed travel
time for vehicle i from its origin to its destination along route i , and mT is the travel time
from the same origin to the same destination along the shortest route if the vehicle could
cross each intersection unhindered15 . For simplicity, we refer to the percentage increase of
the travel time as normalised delay.
D=

T (i )  mT
mT

(18)

On the other hand, we would like to set up a system that is fair to the entire population
of drivers, guaranteeing lower average delays (global optimum). Thus, we compare our
integrated mechanism with a network of intersections governed by intersection managers
that apply the FCFS control policy. We assume that in this case the drivers choose the
shortest route from their origin to their destination, since there are no other incentives to
15. This ratio enables us to aggregate the results of drivers even though they have different origins and/or
destinations.

652

fiA Market-Inspired Approach for Intersection Management

140%

Moving avg (min)

Normalised delay

25
114.30%

110%

80%

67.12%

73.31%

50%

58.10%

20%

0-50

50-100

100-150

150-200

FCFS
CA-CTA
CTA

20

15

10

5

0%

Bid (cents)

20%

40%

60%

80%

100%

Percentage of completed trips

(a)

(b)

Figure 13: Relation between normalised delay and bid (a) and moving average of travel
time (b)
diverge from that route. The aim is to evaluate the global performance (in terms of average
travel time) of our integrated mechanism compared to the straightforward application of the
FCFS policy to a network of intersections, and to detect any potential social cost similar
to that reported in Section 4.3. The metrics we use to assess the performance are the
average delay for every O-D pair, and the moving average of the travel time. The latter
is intended to measure how the average travel time evolves during the simulation. This
metric is initialised to 0 and calculated as follows: once a driver i concludes its trip, the
travel time T (i ) is computed and the moving average travel time T is updated according
to Equation 19, where n is the number of drivers that have completed their trips so far.
T (i )  T
(19)
n+1
In the following tables and figures we refer to the two configurations with the abbreviations
CA-CTA (which stands for combinatorial auction-competitive traffic assignment) and
FCFS.
Figure 13a plots the relation between bid value and normalised delay of the population
of drivers16 . It is still possible to appreciate an inverse relation between these two quantities: the drivers that submit bids between 150 and 200 cents reduce the delay by about
50% compared to those which bid less than 50 cents. Also at the network level, granting
reservations with a combinatorial auction (the CA component of the CA-CTA policy)
ensures that those drivers that submit higher bids experience lower delays (user optimum).
To assess the social cost incurred by CA-CTA at the global level, we measure the moving
average of the travel time, that is, how the average travel time of the entire population of
drivers, computed over all the O-D pairs, evolves during the simulation. We compare CACTA with FCFS and, for completeness, with CTA17 . The results, with 95% confidence
T =T +

16. The error bars denote 95% confidence intervals.
17. In order to evaluate CA-CTA and CTA under the same experimental conditions we ran a new set of
experiments using CTA in combination with the driver model detailed in Section 5.2.2.

653

fiVasirani & Ossowski

Origin
CA-CTA
O1 FCFS
%
CA-CTA
O2 FCFS
%
CA-CTA
O3 FCFS
%
CA-CTA
O4 FCFS
%
CA-CTA
O5 FCFS
%
CA-CTA
O6 FCFS
%
CA-CTA
O7 FCFS
%

Destination
O4
O5

O1

O2

O3

-

12.22
 0.26
11.98
 0.31
-2.0%
-

13.65
 0.31
22.89
 1.17
40.3%
10.51
 0.14
16.50
 1.06
36.4%
-

12.16
 0.21
10.15
 0.06
-19.8%
15.05
 0.69
13.35
 0.09
-12.7%
20.79
 1.23
26.94
 1.31
22.8%
24.59
 1.10
32.17
 1.83
23.6%
25.08
 1.53
22.51
 0.40
-11.4%
15.73
 0.32
14.31
 0.10
-9.9%

12.51
 0.62
9.76
 0.03
-28.2%
18.45
 0.93
22.58
 1.06
18.3%
20.82
 1.26
30.61
 1.70
32.0%
26.72
 0.40
57.01
 3.13
53.1%
24.18
 0.52
23.26
 0.40
-3.9%

10.52
 0.41
13.92
 0.82
24.4%
12.62
 0.63
21.54
 1.39
41.4%
18.12
 1.26
41.05
 2.59
55.8%
22.12
 2.28
56.42
 3.01
60.8%

25.12
 3.40
35.13
 1.80
28.5%
19.58
 1.38
25.87
 1.51
24.3%
9.01
 0.22
12.21
 0.62
26.2%
7.91
 0.48
8.83
 0.31
10.4%
15.78
 1.35
24.68
 1.62
36.1%
26.86
 2.59
34.99
 2.15
23.2%

27.13
 2.03
43.57
 1.89
37.7%
24.17
1.74
31.05
 2.03
22.1%
13.27
 0.46
17.64
 0.92
24.8%
7.32
 0.15
10.05
 0.48
27.2%
10.85
 0.28
19.02
 1.50
42.9%
16.81
 0.99
31.24
 2.06
46.2%

O6

O7

23.13
 0.34
21.35
 0.40
-8.3%
26.54
 0.67
38.09
 1.82
30.3%
18.72
 0.68
23.69
 6.34
21%
13.02
 1.02
15.74
 0.73
17.3%
10.01
 0.28
10.77
 0.26
7.0%
-

13.75
 0.11
13.83
 0.09
0.5%
22.21
 0.37
19.51
 0.15
-13.8%
26.76
 1.02
31.73
 1.36
15.7%
23.12
 1.53
22.74
 0.99
-1.7%
21.88
 1.41
17.66
 0.52
-23.9%
14.55
 0.69
13.73
 0.32
-6.0%
-

11.43
 0.29
12.00
 0.20
4.7%

-

Table 4: Average travel time in minutes ( 95%CI): CA-CTA vs. FCFS
interval error bars, are plotted in Figure 13b. In the beginning, the average travel time
is similar for all the scenarios, but as the number of drivers that populate the network
(i.e., its load) increases, it grows significantly faster with FCFS than with the CA-CTA
policy. In terms of average travel times CTA is the best performing policy. CA-CTA has a
slightly inferior performance, but it does enforce an inverse relationship between bid value
and delay (see Figure 13a). The fact that both CA-CTA and CTA outperforms FCFS
is an indication that, in general, a traffic assignment strategy (the CTA component of
both policies) improves travel time. In fact, with FCFS drivers always select the shortest
654

fiA Market-Inspired Approach for Intersection Management

route, which in some cases is not the best route choice. Furthermore, granting reservations
through an auction (the CA component of the CA-CTA policy) ensures that bid value
and delay reduction are correlated.
Table 4 shows the average travel time of the drivers, according to their O-D pairs, when
the intersection managers use the CA-CTA mechanism, compared to the FCFS policy.
With CA-CTA, there is a net reduction of the average travel time for more than 70% of the
O-D pairs if compared to FCFS. Furthermore, at the 30 intersections at which CA-CTA
outperforms FCFS, the relative improvement (%) is usually more substantial than the
relative losses at the remaining 12 intersections. The travel time reduction is particularly
noteworthy for the busy routes O6 -O2 , O6 -O3 and O7 -O3 with gains that exceed 50%. On
the O-D pairs on which CA-CTA performs worst (especially O5 -O7 and O3 -O2 , with losses
of more than 20%) the assignment strategy is not able to sufficiently reduce demand at
the intersection, thus considerably increasing the travel time due to the social cost of the
combinatorial auction.

6. Conclusions
In this article we studied a distributed mechanism for the control and management of a
future urban road network, where intelligent autonomous vehicles, controlled by drivers,
interact with the infrastructure in order to travel on the links of the network. In this last
section we summarise and discuss the main contributions, and we propose some future lines
of work.
The first objective was the extension of the reservation-based intersection control system (Dresner & Stone, 2008). We focused on modelling a policy that relied on the theory of
combinatorial auctions (Krishna, 2002) to allocate reservations to the drivers. From empirical experimentation, we discovered that the combinatorial auction-based policy guarantees
reduced delay to those drivers that value their time the most, i.e., those that submit higher
bids. However, this new policy showed that it paid a social cost, in term of greater average
delays, especially when traffic demand was high.
The second objective of this work was to go beyond the single intersection setting, and
extending the reservation-based model to a network of intersections. Building on the findings
reported by Vasirani and Ossowski (2011), we realised that a traffic assignment strategy
could make the task of a traffic control policy easier, by better distributing the traffic flow
in the network. We studied a market-inspired traffic assignment strategy that tackled the
problem from the adaptation perspective. In this model, the intersection managers behaved
selfishly, competing with all the others for the supply of the reservations at the intersections.
The experimental evaluation showed that in this way the available resources were efficiently
allocated to the drivers, generating a more balanced network.
Finally, we combined the competitive strategy for traffic assignment with the auctionbased policy for traffic control, in order to develop an adaptive, market-inspired, mechanism
for traffic management. The demand-response pricing policy acted on the distribution of
vehicles in the network, adapting the reserve price (i.e., the minimum price at which the
intersection manager is willing to sell) and generating a system in dynamic equilibrium,
where unused intersections became cheaper while highly demanded ones became more expensive. If demand at particularly disputed intersections was lowered by the reserve price
655

fiVasirani & Ossowski

fluctuations, the social cost of the auction-based control policy was lowered too (at intersection level). Therefore, a more homogeneous distribution of vehicles over the network led to
a better use of network resources, and thus to lower average travel times. In this way, the
entire population of drivers was rewarded with lower average travel times and, at the same
time, the traffic control policy enforced an inverse relation between bid value and delay,
rewarding the drivers that valued the reservations the most with reduced delays.
For future work, other economic models can be implemented, such as continuous double
auctions. Furthermore, this work assumed a driver decision making model that exclusively
took into consideration the route choice, which was modelled as a utility maximisation
problem. In order to capture the inherent complexity of urban traffic systems, it is important
to extend and enrich the driver behavioural model. For example, the driver could be
implemented as a two layer decision maker, where a reactive, rule-based layer provides
short-term decisions about car-following and lane-changing, and a cognitive, BDI-style,
layer is in charge of making the more complex decisions such as route choice and departure
time selection (Rossetti, Bampi, Liu, Vliet, & Cybis, 2000).
Finally, in this article only interactions between the vehicles and the infrastructure take
place. Thus, no collaboration at all is possible between vehicles. Nevertheless, vehicle-tovehicle communication is receiving great attention from the scientific and engineering community (Biswas, Tatchikou, & Dion, 2006). In particular, vehicle-to-vehicle communication
could be used to enrich the action space of a driver, e.g. through the option of dynamically
joining or abandoning coalitions of vehicles, based on the idea of platoons (Varaiya, 1993).

Acknowledgments
This research was partially supported by the Spanish Ministry of Science and Innovation
through the project AT (CONSOLIDER CSD2007-0022, INGENIO 2010) and OVAMAH
(TIN2009-13839-C03-02, Plan E).

References
Adler, J. L., Satapathy, G., Manikonda, V., Bowles, B., & Blue, V. J. (2005). A multiagent approach to cooperative traffic management and route guidance. Transportation
Research Part B - Methodological, 39, 297318.
Bazzan, A. L. C., & Klugl, F. (Eds.). (2008). Multi-agent Architectures for Traffic and
Transportation Engineering. IGI-Global.
Biswas, S., Tatchikou, R., & Dion, F. (2006). Vehicle-to-vehicle wireless communication protocols for enhancing highway traffic safety. IEEE Communications Magazine, 44 (1),
7482.
Choy, M. C., Srinivasan, D., & Cheu, R. L. (2003). Cooperative, hybrid agent architecture
for real-time traffic control. IEEE Transactions on Systems, Man, and Cybernetics Part A, 33 (5), 597607.
Clarke, E. H. (1971). Multipart pricing of public goods. Public Choice, 11 (1), 1733.
656

fiA Market-Inspired Approach for Intersection Management

Codenotti, B., Pemmaraju, S., & Varadarajan, K. (2004). The computation of market
equilibria. SIGACT News, 35 (4), 2337.
da Silva, B. C., Basso, E. W., Bazzan, A. L. C., & Engel, P. M. (2006). Dealing with
non-stationary environments using context detection. In Proceedings of the 23rd International Conference on Machine Learning, pp. 217224. ACM.
Dias, M. B., Zlot, R. M., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination: a survey and analysis. Proceedings of the IEEE, 94 (7), 12571270.
Dresner, K., & Stone, P. (2008). A multiagent approach to autonomous intersection management. Journal of Artificial Intelligence Research, 31, 591656.
Gerding, E., McBurney, P., & Yao, X. (2010). Market-based control of computational
systems: Introduction to the special issue. Journal of Autonomous Agents and MultiAgent Systems, 21, 109114.
Gerlough, D. L., & Huber, M. J. (1975). Traffic-flow theory. Transportation Research
Board.
Gershenson, C. (2005). Self-organizing traffic lights. Complex Systems, 16, 2953.
Groves, T. (1973). Incentives in teams. Econometrica, 41 (4), 617631.
Hensher, D. A., & Sullivan, C. (2003). Willingness to pay for road curviness and road type.
Transportation Research Part D - Transport and Environment, 8, 139155.
Hernandez, J. Z., Ossowski, S., & Garca-Serrano, A. (2002). Multiagent architectures for
intelligent traffic management systems. Transportation Research Part C - Emerging
Technologies, 10 (5), 473506.
Hoos, H. H., & Boutilier, C. (2000). Solving combinatorial auctions using stochastic local
search. In Proceedings of the 17th National Conference on Artificial Intelligence, pp.
2229. AAAI Press.
Hunt, P. B., Robertson, D. I., Bretherton, R. D., & Winton, R. I. (1981). Scoot-a traffic responsive method of coordinating signals. Tech. rep., TRRL Lab. Report 1014,
Transport and Road Research Laboratory, Berkshire.
Ioannou, P., & Chien, C. C. (1993). Autonomous intelligent cruise control. IEEE Transactions on Vehicular Technology, 42 (4), 657672.
Junges, R., & Bazzan, A. L. C. (2008). Evaluating the performance of dcop algorithms in
a real world, dynamic problem. In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multi-Agent Systems, pp. 599606. International
Foundation for Autonomous Agents and Multiagent Systems.
Krishna, V. (2002). Auction Theory. Academic Press.
Krogh, B., & Thorpe, C. (1986). Integrated path planning and dynamic steering control
for autonomous vehicles. In Proceedings of the IEEE International Conference on
Robotics and Automation, pp. 16641669.
Lammer, S., & Helbing, D. (2008). Self-control of traffic lights and vehicle flows in urban
road networks. Journal of Statistical Mechanics: Theory and Experiment, 2008 (04).
657

fiVasirani & Ossowski

Leyton-Brown, K., Shoham, Y., & Tennenholtz, M. (2000). An algorithm for multi-unit
combinatorial auctions. In Proceedings of the 17th National Conference on Artificial
Intelligence, pp. 5661. AAAI Press.
Meneguzzer, C. (1997). Review of models combining traffic assignment and signal control.
Transportation Engineering, 123 (2), 148155.
Nagel, K., & Schreckenberg, M. (1992). A cellular automaton model for freeway traffic.
Journal de Physique I, 2 (12), 22212229.
Ossowski, S., & Garca-Serrano, A. (1999). Social structure as a computational co-ordination
mechanism in societies of autonomous problem-solving agents. In Intelligent Agents
V: Agents Theories, Architectures, and Languages, Vol. 1555 of Lecture Notes in Computer Science, pp. 133148. Springer.
Papageorgiou, M., Diakaki, C., Dinopoulou, V., Kotsialos, A., & Wang, Y. (2003). Review
of road traffic control strategies. In Proceedings of the IEEE, Vol. 91, pp. 20432067.
IEEE.
Robertson, D. I. (1969). Transyt: A traffic network study tool. Tech. rep., Rep. LR 253,
Road Res. Lab., London.
Rossetti, R. J. F., Bampi, S., Liu, R., Vliet, D. V., & Cybis, H. B. B. (2000). An agent-based
framework for the assessment of drivers decision making. In Proceedings of the 3rd
IEEE Conference on Intelligent Transportation Systems, pp. 387392.
Roughgarden, T. (2003). The price of anarchy is independent of the network topology.
Journal of Computer and System Sciences, 67 (2), 341364.
Sandholm, T. (2002). Algorithm for optimal winner determination in combinatorial auctions. Artificial Intelligence, 135 (1-2), 154.
Schepperle, H., & Bohm, K. (2007). Agent-based traffic control using auctions. In Cooperative Information Agents XI, Vol. 4676 of Lecture Notes in Computer Science, pp.
119133. Springer.
Schwerdtfeger, T. (1984). Dynemo: A model for the simulation of traffic flow in motorway
networks. In Proceedings of the 9th International Symposium on Transportation and
Traffic Theory, pp. 6587. VNU Science Press.
Small, K., & Verhoef, E. (2007). The Economics of Urban Transportation. Routledge.
Steingrover, M., Schouten, R., Peelen, S., Nijhuis, E., & Bakker, B. (2005). Reinforcement
learning of traffic light controllers adapting to traffic congestion. In Proceedings of the
17th Belgium-Netherlands Conference on Artificial Intelligence, pp. 216223.
Treiber, M., Hennecke, A., & Helbing, D. (2000). Congested traffic states in empirical
observations and microscopic simulations. Physical Review E, 62 (2), 18051824.
van Katwijk, R. T., Schutter, B. D., & Hellendoorn, J. (2009). Multi-agent control of traffic
networks: Algorithm and case study. In Proceedings of the 12th International IEEE
Conference on Intelligent Transportation Systems, pp. 316321.
Varaiya, P. (1993). Smart cars on smart roads: Problems of control. IEEE Transactions on
Automatic Control, 38, 195207.
658

fiA Market-Inspired Approach for Intersection Management

Vasirani, M., & Ossowski, S. (2009a). Evaluating policies for reservation-based intersection
control. In Proceedings of the 14th Portuguese Conference on Artificial Intelligence,
pp. 3950.
Vasirani, M., & Ossowski, S. (2009b). A market-inspired approach to reservation-based urban road traffic management. In Proceedings of the 8th International Joint Conference
on Autonomous Agents and Multiagent Systems, pp. 617624.
Vasirani, M., & Ossowski, S. (2011). A computational market for distributed control of
urban road traffic systems. IEEE Transactions on Intelligent Transportation Systems,
12, 313321.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. Journal
of Finance, 16, 837.
Wiering, M. (2000). Multi-agent reinforcement learning for traffic light control. In Proceedings of the 17th European Conference on Machine Learning, pp. 11511158.
Wurman, P. R., Wellman, M. P., & Walsh, W. E. (2001). A parametrization of the auction
design space. Games and Economic Behavior, 35 (1-2), 304338.
Yen, J. Y. (1971). Finding the k shortest loopless paths in a network. Management Science,
17 (11), 712716.
Zutt, J., van Gemund, A., de Weerdt, M., & Witteveen, C. (2010). Dealing with uncertainty in operational transport planning. In Intelligent Infrastructures, pp. 349375.
Springer.

659

fiJournal of Artificial Intelligence Research 43 (2012) 329-351

Submitted 11/11; published 03/12

Local Consistency and SAT-Solvers
Peter Jeavons
Justyna Petke

Peter.Jeavons@cs.ox.ac.uk
Justyna.Petke@cs.ox.ac.uk

Department of Computer Science, University of Oxford
Wolfson Building, Parks Road, Oxford, OX1 3QD, UK

Abstract
Local consistency techniques such as k-consistency are a key component of specialised
solvers for constraint satisfaction problems. In this paper we show that the power of
using k-consistency techniques on a constraint satisfaction problem is precisely captured by
using a particular inference rule, which we call negative-hyper-resolution, on the standard
direct encoding of the problem into Boolean clauses. We also show that current clauselearning SAT-solvers will discover in expected polynomial time any inconsistency that can
be deduced from a given set of clauses using negative-hyper-resolvents of a fixed size. We
combine these two results to show that, without being explicitly designed to do so, current
clause-learning SAT-solvers efficiently simulate k-consistency techniques, for all fixed values
of k. We then give some experimental results to show that this feature allows clause-learning
SAT-solvers to efficiently solve certain families of constraint problems which are challenging
for conventional constraint-programming solvers.

1. Introduction
One of the oldest and most central ideas in constraint programming, going right back to
Montanaris original paper in 1974, is the idea of using local consistency techniques to prune
the search space (Bessiere, 2006). The idea of arc-consistency was introduced by Mackworth
(1977), and generalised to k-consistency by Freuder (1978). Modern constraint solvers
generally employ specialised propagators to prune the domains of variables to achieve some
form of generalised arc-consistency, but typically do not attempt to enforce higher levels of
consistency, such as path-consistency.
By contrast, the software tools developed to solve propositional satisfiability problems,
known as SAT-solvers, generally use logical inference techniques, such as unit propagation
and clause-learning, to prune the search space.
One of the most surprising empirical findings of the last few years has been the remarkably good performance of general SAT-solvers in solving constraint satisfaction problems.
To apply such tools to a constraint satisfaction problem one first has to translate the instance into a set of clauses using some form of Boolean encoding (Tamura, Taga, Kitagawa,
& Banbara, 2009; Walsh, 2000). Such encoding techniques tend to obscure the structure of the original problem, and may introduce a very large number of Boolean variables
and clauses to encode quite easily-stated constraints. Nevertheless, in quite a few cases,
such approaches have out-performed more traditional constraint-solving tools (van Dongen,
Lecoutre, & Roussel, 2008, 2009; Petke & Jeavons, 2009).
c
2012
AI Access Foundation. All rights reserved.

fiJeavons & Petke

In this paper we draw on a number of recent analytical approaches to try to account
for the good performance of general SAT-solvers on many forms of constraint problems.
Building on the results of Atserias, Bulatov, and Dalmau (2007), Atserias and Dalmau
(2008), and Hwang and Mitchell (2005), we show that the power of using k-consistency
techniques in a constraint problem is precisely captured by using a single inference rule in
a standard Boolean encoding of that problem. We refer to this inference rule as negativehyper-resolution, and show that any conclusions deduced by enforcing k-consistency can be
deduced by a sequence of negative-hyper-resolution inferences involving Boolean clauses in
the original instance and negative-hyper-resolvents with at most k literals. Furthermore,
by using the approach of Atserias, Fichte, and Thurley (2011), and Pipatsrisawat and
Darwiche (2009), we show that current clause-learning SAT-solvers will mimic the effect of
such deductions in polynomial expected time, even with a random branching strategy. Hence
we show that, although they are not explicitly designed to do so, running a clause-learning
SAT-solver on a straightforward encoding of a constraint problem efficiently simulates the
effects of enforcing k-consistency for all values of k.

2. Preliminaries
In this section we give some background and definitions that will be used throughout the
rest of the paper.
2.1 Constraint Satisfaction Problems and k-Consistency
Definition 1 An instance of the Constraint Satisfaction Problem (CSP) is specified
by a triple (V, D, C), where
 V is a finite set of variables;
 D = {Dv | v  V } where each set Dv is the set of possible values for the variable v,
called the domain of v;
 C is a finite set of constraints. Each constraint in C is a pair (Ri , Si ) where
 Si is an ordered list of mi variables, called the constraint scope;
 Ri is a relation over D of arity mi , called the constraint relation.
Given any CSP instance (V, D, C), a partial assignment is a mapping f from some
S
subset W of V to Dv such that f (v)  Dv for all v  W . A partial assignment satisfies
the constraints of the instance if, for all (R, (v1 , v2 , . . . , vm ))  C such that vj  W for
j = 1, 2, . . . , m, we have (f (v1 ), f (v2 ) . . . , f (vm ))  R. A partial assignment that satisfies
the constraints of an instance is called a partial solution1 to that instance. The set of
variables on which a partial assignment f is defined is called the domain of f , and denoted
Dom(f ). A partial solution g extends a partial solution f if Dom(g)  Dom(f ) and
g(v) = f (v) for all v  Dom(f ). A partial solution with domain V is called a solution.
One way to derive new information about a CSP instance, which may help to determine
whether or not it has a solution, is to use some form of constraint propagation to enforce
1. Note that not all partial solutions extend to solutions.

330

fiLocal Consistency and SAT-Solvers

some level of local consistency (Bessiere, 2006). For example, it is possible to use the notion
of k-consistency, defined below. We note that there are several different but equivalent ways
to define and enforce k-consistency described in the literature (Bessiere, 2006; Cooper, 1989;
Freuder, 1978). Our presentation follows that of Atserias et al. (2007), which is inspired by
the notion of existential k-pebble games introduced by Kolaitis and Vardi (2000).
Definition 2 (Atserias et al., 2007) For any CSP instance P , the k-consistency closure
of P is the set H of partial assignments which is obtained by the following algorithm:
1. Let H be the collection of all partial solutions f of P with |Dom(f )|  k + 1;
2. For every f  H with |Dom(f )|  k and every variable v of P , if there is no g  H
such that g extends f and v  Dom(g), then remove f and all its extensions from H;
3. Repeat step 2 until H is unchanged.
Note that computing the k-consistency closure according to this definition corresponds
precisely to enforcing strong (k+1)-consistency according to the definitions given by Bessiere
(2006), Cooper (1989), and Freuder (1978).
Throughout this paper, we shall assume that the domain of possible values for each
variable in a CSP instance is finite. It is straightforward to show that for any fixed k,
and any fixed maximum domain size, the k-consistency closure of an instance P can be
computed in polynomial time (Atserias et al., 2007; Cooper, 1989).
Note that any solution to P must extend some element of the k-consistency closure of
P . Hence, if the k-consistency closure of P is empty, for some k, then P has no solutions.
The converse is not true in general, but it holds for certain special cases, such as the class of
instances whose structure has tree-width bounded by k (Atserias et al., 2007), or the class
of instances whose constraint relations are 0/1/all relations, as defined in Cooper, Cohen,
and Jeavons (1994), or connected row-convex relations, as defined in Deville, Barette,
and Hentenryck (1997). For these special kinds of instances it is possible to determine in
polynomial time whether or not a solution exists simply by computing the k-consistency
closure, for an appropriate choice of k. Moreover, if a solution exists, then it can be
constructed in polynomial time by selecting each variable in turn, assigning each possible
value, re-computing the k-consistency closure, and retaining an assignment that gives a
non-empty result.
The following result gives a useful condition for determining whether the k-consistency
closure of a CSP instance is empty.
Lemma 1 (Kolaitis & Vardi, 2000) The k-consistency closure of a CSP instance P is
non-empty if and only if there exists a non-empty family H of partial solutions to P such
that:
1. If f  H, then |Dom(f )|  k + 1;
2. If f  H and f extends g, then g  H;
3. If f  H, |Dom(f )|  k, and v 
/ Dom(f ) is a variable of P , then there is some
g  H such that g extends f and v  Dom(g).
A set of partial solutions H satisfying the conditions described in Lemma 1 is sometimes
called a strategy for the instance P (Barto & Kozik, 2009; Kolaitis & Vardi, 2000).
331

fiJeavons & Petke

2.2 Encoding a CSP Instance as a Propositional Formula
One possible approach to solving a CSP instance is to encode it as a propositional formula
over a suitable set of Boolean variables, and then use a program to decide the satisfiability
of that formula. Many such programs, known as SAT-solvers, are now available and can
often efficiently handle problems with thousands, or sometimes even millions, of Boolean
variables (Zhang & Malik, 2002).
Several different ways of encoding a CSP instance as a propositional formula have been
proposed (Prestwich, 2009; Tamura et al., 2009; Walsh, 2000).
Here we consider one common family of encodings, known as sparse encodings (this term
was introduced in Hoos, 1999). For any CSP instance P = (V, D, C), a sparse encoding
introduces a set of Boolean variables of the form xvi for each v  V and each i  Dv . The
Boolean variable xvi is assigned True if and only if the original variable v is assigned the
value i. We will say that a partial assignment f falsifies a clause C if C consists entirely of
literals of the form xvf (v) , for variables v  Dom(f ). Otherwise, we will say that a partial
assignment f satisfies a clause C.
Example 1 Let P be a CSP instance such that V = {u, v, w}, Du = Dv = {0, 1}, Dw =
{0, 1, 2} and C contains a single ternary constraint with scope (u, v, w) specifying that
u  v < w. A sparse encoding of P will introduce seven Boolean variables:
xu0 , xu1 , xv0 , xv1 , xw0 , xw1 , xw2 .
Sparse encodings usually contain certain clauses known as at-least-one and at-most-one
clauses, to ensure that each variable v is assigned a value, say i, and that no other value,
W
j 6= i, is assigned to v. The at-least-one clauses are of the form iDv xvi for each variable
v. The at-most-one clauses can be represented as a set of binary clauses xvi  xvj for all
i, j  Dv with i 6= j.
Example 2 In the case of the CSP instance from Example 1 the at-least-one clauses are:
xu0  xu1 , xv0  xv1 , xw0  xw1  xw2
The at-most-one clauses are:
xu0  xu1 , xv0  xv1 , xw0  xw1 , xw0  xw2 , xw1  xw2
The various different sparse encodings differ in the way they encode the constraints of a
CSP instance. Two methods are most commonly used. The first one encodes the disallowed
variable assignments - the so-called conflicts or no-goods. The direct encoding (Prestwich,
W
2009), for instance, generates a clause vS xvf (v) for each partial assignment f that does
not satisfy the constraint (R, S)  C. Using the direct encoding, the ternary constraint
from Example 1 would be encoded by the following clauses:
xu0  xv0  xw0 ,
xu0  xv1  xw0 ,
xu0  xv1  xw1 ,
xu1  xv0  xw0 ,
332

fiLocal Consistency and SAT-Solvers

xu1  xv0  xw1 ,
xu1  xv0  xw2 ,
xu1  xv1  xw0 ,
xu1  xv1  xw1 .
Another way of translating constraints into clauses is to encode the allowed variable
assignments - the so-called supports. This has been used as the basis for an encoding of
binary CSP instances, known as the support encoding (Gent, 2002), defined as follows.
For each pair of variables v, w in the scope of some constraint, and each value i  Dv ,
W
the support encoding will contain the clause xvi  jA xwj , where A  Dw is the set of
values for the variable w which are compatible with the assignment v = i, according to the
constraint.
Note that the support encoding is defined for binary CSP instances only. However, some
non-binary constraints can be decomposed into binary ones without introducing any new
variables. For instance, the ternary constraint from Example 1 can be decomposed into two
binary constraints specifying that u  v and v < w. Using the support encoding, these
binary constraints would then be represented by the following clauses:
xu0  xv0  xv1 , xu1  xv1 , xv0  xu0 , xv1  xu0  xu1 ,
xv0  xw1  xw2 , xv1  xw2 , xw0 , xw1  xv0 , xw2  xv0  xv1 .
2.3 Inference Rules
Given any set of clauses we can often deduce further clauses by applying certain inference
rules. For example, if we have two clauses of the form C1 x and C2 x, for some (possibly
empty) clauses C1 , C2 , and some variable x, then we can deduce the clause C1  C2 . This
form of inference is known as propositional resolution; the resultant clause is called the
resolvent (Robinson, 1965).
In the next section, we shall establish a close connection between the k-consistency
algorithm and a form of inference called negative-hyper-resolution (Buning & Lettmann,
1999), which we define as follows:
Definition 3 If we have a collection of clauses of the form Ci  xi , for i = 1, 2, . . . , r,
and a clause C0  x1  x2      xr , where each xi is a Boolean variable, and C0 and
each Ci is a (possibly empty) disjunction of negative literals, then we can deduce the clause
C0  C1      Cr .
We call this form of inference negative-hyper-resolution and the resultant clause
C0  C1      Cr the negative-hyper-resolvent.
In the case where C0 is empty, the negative-hyper-resolution rule is equivalent to the
nogood resolution rule described by Hwang and Mitchell (2005) as well as the H5-k rule
introduced by de Kleer (1989) and the nogood recording scheme described by Schiex and
Verfaillie (1993).
Note that the inference obtained by negative-hyper-resolution can also be obtained by a
sequence of standard resolution steps. However, the reason for introducing negative-hyperresolution is that it allows us to deduce the clauses we need in a single step without needing
to introduce intermediate clauses (which may contain up to r  1 more literals than the
333

fiJeavons & Petke

negative-hyper-resolvent). By restricting the size of the clauses we use in this way we are
able to obtain better performance bounds for SAT-solvers in the results below.
Example 3 Assume we have a collection of clauses of the form Ci xi , for i = 1, 2, . . . , r,
and a clause C0  x1  x2      xr , as specified in Definition 3, where each Ci = C0 . The
negative-hyper-resolvent of this set of clauses is C0 .
The clause C0 can also be obtained by a sequence of standard resolution steps, as follows.
First resolve C0  x1  x2      xr with C0  xr to obtain C0  x1  x2      xr1 . Then
resolve this with the next clause, C0  xr1 , and so on for the other clauses, until finally
we obtain C0 . However, in this case the intermediate clause C0 x1 x2   xr1 contains
r  1 more literals than the negative-hyper-resolvent.
Example 4 Note that the no-good clauses in the direct encoding of a binary CSP instance
can each be obtained by a single negative-hyper-resolution step from an appropriate support
clause in the support encoding together with an appropriate collection of at-most-one clauses.
Let A  Dw be the set of values for the variable w which are compatible with the assignment
W
v = i, then the support encoding will contain the clause C = xvi  jA xwj . If there are
any values k  Dw which are incompatible with the assignment v = i, then we can form the
negative-hyper-resolvent of C with the at-most-one clauses xwk  xwj for each j  A, to
obtain the corresponding no-good clause, xvi  xwk .
A negative-hyper-resolution derivation of a clause C from a set of initial clauses  is
a sequence of clauses C1 , C2 , . . . , Cm , where Cm = C and each Ci follows by the negativehyper-resolution rule from some collection of clauses, each of which is either contained in
 or else occurs earlier in the sequence. The width of this derivation is defined to be the
maximum size of any of the clauses Ci . If Cm is the empty clause, then we say that the
derivation is a negative-hyper-resolution refutation of .

3. k-Consistency and Negative-Hyper-Resolution
It has been pointed out by many authors that enforcing local consistency is a form of
inference on relations analogous to the use of the resolution rule on clauses (Bacchus, 2007;
Bessiere, 2006; Hwang & Mitchell, 2005; Rish & Dechter, 2000). The precise strength of the
standard resolution inference rule on the direct encoding of a CSP instance was considered
in the work of Walsh (2000), where it was shown that unit resolution (where one of the
clauses being resolved consists of a single literal), corresponds to enforcing a weak form of
local consistency known as forward checking. Hwang and Mitchell (2005) pointed out that
the standard resolution rule with no restriction on clause length is able to simulate all the
inferences made by a k-consistency algorithm. Atserias and Dalmau (2008) showed that
the standard resolution rule restricted to clauses with at most k literals, known as the kresolution rule, can be characterised in terms of the Boolean existential (k +1)-pebble game.
It follows that on CSP instances with Boolean domains this form of inference corresponds
to enforcing k-consistency. An alternative proof that k-resolution achieves k-consistency for
instances with Boolean domains is given in the book by Hooker (2006, Thm. 3.22).
Here we extend these results a little, to show that for CSP instances with arbitrary
finite domains, applying the negative-hyper-resolution rule on the direct encoding to obtain
334

fiLocal Consistency and SAT-Solvers

clauses with at most k literals corresponds precisely to enforcing k-consistency. A similar
relationship was stated in the work of de Kleer (1989), but a complete proof was not given.
Note that the bound, k, that we impose on the size of the negative-hyper-resolvents,
is independent of the domain size. In other words, using this inference rule we only need
to consider inferred clauses of size at most k, even though we make use of clauses in the
encoding whose size is equal to the domain size, which may be arbitrarily large.
Theorem 1 The k-consistency closure of a CSP instance P is empty if and only if its direct
encoding as a set of clauses has a negative-hyper-resolution refutation of width at most k.
The proof is broken down into two lemmas inspired by Lemmas 2 and 3 in the work
of Atserias and Dalmau (2008).
Lemma 2 Let P be a CSP instance, and let  be its direct encoding as a set of clauses.
If  has no negative-hyper-resolution refutation of width k or less, then the k-consistency
closure of P is non-empty.
Proof. Let V be the set of variables of P , where each v  V has domain Dv , and let
X = {xvi | v  V, i  Dv } be the corresponding set of Boolean variables in . Let  be the
set of all clauses having a negative-hyper-resolution derivation from  of width at most k.
By the definition of negative-hyper-resolution, every non-empty clause in  consists entirely
of negative literals.
Now let H be the set of all partial assignments for P with domain size at most k + 1
that do not falsify any clause in    under the direct encoding.
Consider any element f  H. By the definition of H, f does not falsify any clause of
, so by the definition of the direct encoding, every element of H is a partial solution to
P . Furthermore, if f extends g, then g is also an element of H, because g makes fewer
assignments than f and hence cannot falsify any additional clauses to f .
If  has no negative-hyper-resolution refutation of width at most k, then  does not
contain the empty clause, so H contains (at least) the partial solution with empty domain,
and hence H is not empty.
Now let f be any element of H with |Dom(f )|  k and let v be any variable of P
that is not in Dom(f ). For any partial assignment g that extends f and has Dom(g) =
Dom(f )  {v} we have that either g  H or else there exists a clause in    that is
falsified by g. Since g is a partial assignment, any clause C in    that is falsified by g,
must consist entirely of negative literals. Hence the literals of C must either be of the form
xwf (w) for some w  Dom(f ), or else xvg(v) . Moreover, any such clause must contain the
literal xvg(v) , or else it would already be falsified by f .
Assume, for contradiction, that H does not contain any assignment g that extends f and
has Dom(g) = Dom(f )  {v}. In that case, we have that, for each i  Dv ,    contains a
clause Ci consisting of negative literals of the form xwf (w) for some w  Dom(f ), together
with the literal xvi . Now consider the clause, C, which is the negative-hyper-resolvent
W
of these clauses Ci and the at-least-one clause iDv xvi . The clause C consists entirely
of negative literals of the form xwf (w) for some w  Dom(f ), so it has width at most
|Dom(f )|  k, and hence is an element of . However C is falsified by f , which contradicts
the choice of f . Hence we have shown that for all f  H with |Dom(f )|  k, and for
335

fiJeavons & Petke

all variables v such that v 6 Dom(f ), there is some g  H such that g extends f and
v  Dom(g).
We have shown that H satisfies all the conditions required by Lemma 1, so we conclude
that the k-consistency closure of P is non-empty.
2

Lemma 3 Let P be a CSP instance, and let  be its direct encoding as a set of clauses.
If the k-consistency closure of P is non-empty, then  has no negative-hyper-resolution
refutation of width k or less.
Proof. Let V be the set of variables of P , where each v  V has domain Dv , and let
X = {xvi | v  V, i  Dv } be the corresponding set of Boolean variables in .
By Lemma 1, if the k-consistency closure of P is non-empty, then there exists a nonempty set H of partial solutions to P which satisfies the three properties described in
Lemma 1.
Now consider any negative-hyper-resolution derivation  from  of width at most k. We
show by induction on the length of this derivation that the elements of H do not falsify any
clause in the derivation. First we note that the elements of H are partial solutions, so they
satisfy all the constraints of P , and hence do not falsify any clause of . This establishes
the base case. Assume, for induction, that all clauses in the derivation earlier than some
clause C are not falsified by any element of H.
Note that, apart from the at-least-one clauses, all clauses in  and  consist entirely of
negative literals. Hence we may assume, without loss of generality, that C is the negativehyper-resolvent of a set of clauses  = {Ci  xvi | i  Dv } and the at-least-one clause
W
iDv xvi , for some fixed variable v.
If f  H falsifies C, then the literals of C must all be of the form xwf (w) , for some
w  Dom(f ). Since the width of the derivation is at most k, C contains at most k literals,
and hence we may assume that |Dom(f )|  k. But then, by the choice of H, there must
exist some extension g of f in H such that v  Dom(g). Any such g will falsify some
clause in , which contradicts our inductive hypothesis. Hence no f  H falsifies C, and,
in particular, C cannot be empty.
It follows that no negative-hyper-resolution derivation of width at most k can contain
the empty clause.
2
Note that the proof of Theorem 1 applies to any sparse encoding that contains the
at-least-one clauses for each variable, and where all other clauses are purely negative. We
will call such an encoding a negative sparse encoding. As well as the direct encoding, other
negative sparse encodings exist. For example, we may use negative clauses that involve only
a subset of the variables in the scope of some constraints (to forbid tuples where all possible
extensions to the complete scope are disallowed by the constraint). Another example of
a negative sparse encoding is a well-known variant of the direct encoding in which the
at-most-one clauses are omitted.
Corollary 1 The k-consistency closure of a CSP instance P is empty if and only if any
negative sparse encoding of P has a negative-hyper-resolution refutation of width at most k.
336

fiLocal Consistency and SAT-Solvers

4. Negative-Hyper-Resolution and SAT-Solvers
In this section we adapt the machinery from Atserias et al. (2011), and Pipatsrisawat and
Darwiche (2009) to show that for any fixed k, the existence of a negative-hyper-resolution
refutation of width k is likely to be discovered by a SAT-solver in polynomial-time using
standard clause learning and restart techniques, even with a totally random branching
strategy.
Note that previous results about the power of clause-learning SAT-solvers have generally
assumed an optimal branching strategy (Beame, Kautz, & Sabharwal, 2004; Pipatsrisawat
& Darwiche, 2009) - they have shown what solvers are potentially capable of doing, rather
than what they are likely to achieve in practice. An important exception is the paper
by Atserias et al. (2011), which gives an analysis of likely behaviour, but relies on the
existence of a standard resolution proof of bounded width. Here we show that the results
of Atserias et al. can be extended to hyper-resolution proofs, which can be shorter and
narrower than their associated standard resolution proofs.
We will make use of the following terminology from Atserias et al. (2011). For a clause
C, a Boolean variable x, and a truth value a  {0, 1}, the restriction of C by the assignment
x = a, denoted C|x=a , is defined to be the constant 1, if the assignment satisfies the clause,
or else the clause obtained by deleting from C any literals involving the variable x. For
any sequence of assignments S of the form (x1 = a1 , x2 = a2 , . . . , xr = ar ) we write C|S to
denote the result of computing the restriction of C by each assignment in turn. If C|S is
empty, then we say that the assignments in S falsify the clause C. For a set of clauses ,
we write |S to denote the set {C|S | C  } \ {1}.
Most current SAT-solvers operate in the following way (Atserias et al., 2011; Pipatsrisawat & Darwiche, 2009). They maintain a database of clauses  and a current state
S, which is a partial assignment of truth values to the Boolean variables in the clauses of
. A high-level description of the algorithms used to update the clause database and the
state, derived from the description given in Atserias et al., is shown in Algorithm 1 (a similar framework, using slightly different terminology, is given in Pipatsrisawat & Darwiche,
2009).
Now consider a run of the algorithm shown in Algorithm 1, started with the initial
database , and the empty state S0 , until it either halts or discovers a conflict (i.e.,   |S ).
Such a run is called a complete round started with , and we represent it by the sequence
of states S0 , . . . , Sm , that the algorithm maintains. Note that each state Si extends the
state Si1 by a single assignment to a Boolean variable, which may be either a decision
assignment or an implied assignment.
More generally, a round is an initial segment S0 , S1 , . . . , Sr of a complete round started
with , up to a state Sr such that either |Sr contains the empty clause, or |Sr does not
contain any unit clause. For any clause C, we say that a round S0 , S1 , . . . , Sr satisfies C if
C|Sr = 1, and we say that the round falsifies C if C|Sr is empty.
If S0 , S1 , . . . , Sr is a round started with , and |Sr contains the empty clause, then
the algorithm either reports unsatisfiability or learns a new clause: such a round is called
conclusive. If a round is not conclusive we call it inconclusive 2 . Note that if S0 , S1 , . . . , Sr
is an inconclusive round started with , then |Sr does not contain the empty clause,
2. Note that a complete round that assigns all variables and reports satisfiability is called inconclusive.

337

fiJeavons & Petke

and does not contain any unit clauses. Hence, for any clause C  , if Sr falsifies all the
literals of C except one, then it must satisfy the remaining literal, and hence satisfy C. This
property of clauses is captured by the following definition.
Definition 4 (Atserias et al., 2011) Let  be a set of clauses, C a non-empty clause, and
l a literal of C. We say that  absorbs C at l if every inconclusive round started with 
that falsifies C \ {l} satisfies C.
If  absorbs C at each literal l in C, then we simply say that  absorbs C.
Note that a closely related notion is introduced by Pipatsrisawat and Darwiche (2009) for
clauses that are not absorbed by a set of clauses ; they are referred to as 1-empowering with
respect to . (The exact relationship between 1-empowering and absorption is discussed
in Atserias et al., 2011.)
We will now explore the relationship between absorption and negative-hyper-resolution.
Example 5 Let  be the direct encoding of a CSP instance P = (V, D, C), where V =
{u, v, w}, Du = Dv = Dw = {1, 2} and C contains two binary constraints: one forbids the
assignment of the value 1 to u and v simultaneously, and the other forbids the simultaneous
assignment of the value 2 to u and 1 to w. Let C also contain a ternary constraint that
forbids the assignment of the value 2 to all three variables simultaneously.
 = { xu1  xu2 , xv1  xv2 , xw1  xw2 ,
xu1  xu2 , xv1  xv2 , xw1  xw2 ,
xu1  xv1 , xu2  xw1 , xu2  xv2  xw2 }.
The clause xv1  xw1 is not contained in , but can be obtained by negative-hyperresolution from the clauses xu1  xu2 , xu1  xv1 , xu2  xw1 .
This clause is absorbed by , since every inconclusive round that sets xv1 = true must
set xw1 = f alse by unit propagation, and every inconclusive round that sets xw1 = true
must set xv1 = f alse also by unit propagation.
Example 5 indicates that clauses that can be obtained by negative hyper-resolution from a
set of clauses  are sometimes absorbed by . The next result clarifies when this situation
holds.
Lemma 4 Any negative-hyper-resolvent of a set of disjoint clauses is absorbed by that set
of clauses.
Proof. Let C be the negative-hyper-resolvent of a set of clauses  = {Ci  xi | i =
1, 2, . . . , r} and a clause C 0 = C0  x1  x2      xr , where each Ci is a (possibly empty)
disjunction of negative literals, for 0  i  r. Then C = C0  C1      Cr by Definition 3.
By Definition 4, we must show that   C 0 absorbs C at each of its literals. Assume all
but one of the literals of C are falsified. Since the set of clauses   C 0 are assumed to be
disjoint, the remaining literal l must belong to exactly one of the clauses in this set. There
are two cases to consider.
1. If l belongs to the clause C 0 , then all clauses in  have all but one literals falsified, so
the remaining literal xi in each of these clauses is set to true, by unit propagation.
Hence all literals in C 0 are falsified, except for l, so l is set to true, by unit propagation.
338

fiLocal Consistency and SAT-Solvers

2. If l belongs to one of the clauses Ci  xi , then all of the remaining clauses in  have
all but one literals falsified, so the corresponding literals xj are set to true, by unit
propagation. Hence all literals in C 0 are falsified, except for xi , so xi is set to true, by
unit propagation. But now all literals in Ci  xi are falsified, except for l, so l is set
to true by unit propagation.
2
The next example shows that the negative-hyper-resolvent of a set of clauses that is not
disjoint will not necessarily be absorbed by those clauses.
Example 6 Recall the set of clauses  given in Example 5, which is the direct encoding of
a CSP instance with three variables {u, v, w}, each with domain {1, 2}.
The clause xu2  xv2 is not contained in , but can be obtained by negative-hyperresolution from the clauses xw1  xw2 , xu2  xv2  xw2 , xu2  xw1 .
This clause is not absorbed by , since an inconclusive round that sets xv2 = true will
not necessarily ensure that xu2 = f alse by unit propagation.
The basic approach we shall use to establish our main results below is to show that any
clauses that can be obtained by bounded width negative-hyper-resolution from a given set
of clauses, but are not immediately absorbed (such as the one in Example 6) are likely
to become absorbed quite quickly because of the additional clauses that are added by
the process of clause learning. Hence a clause-learning SAT-solver is likely to fairly rapidly
absorb all of the clauses that can be derived from its original database of clauses by negativehyper-resolution. In particular, if the empty clause can be derived by negative-hyperresolution, then the solver will fairly rapidly absorb some literal and its complement, and
hence report unsatisfiability (see the proof of Theorem 2 for details).
The following key properties of absorption are established by Atserias et al. (2011).
Lemma 5 (Atserias et al., 2011) Let  and 0 be sets of clauses, and let C and C 0 be
non-empty clauses.
1. If C belongs to , then  absorbs C;
2. If C  C 0 and  absorbs C, then  absorbs C 0 ;
3. If   0 and  absorbs C, then 0 absorbs C.
To allow further analysis, we need to make some assumptions about the learning scheme,
the restart policy and the branching strategy used by our SAT-solver.
The learning scheme is a rule that creates and adds a new clause to the database
whenever there is a conflict. Such a clause is called a conflict clause, and each of its literals
is falsified by some assignment in the current state. If a literal is falsified by the i-th decision
assignment, or some later implied assignment before the (i + 1)-th decision assignment, it is
said to be falsified at level i. If a conflict clause contains exactly one literal that is falsified
at the maximum possible level, it is called an asserting clause (Pipatsrisawat & Darwiche,
2009; Zhang, Madigan, Moskewicz, & Malik, 2001).
Assumption 1 The learning scheme chooses an asserting clause.
339

fiJeavons & Petke

Algorithm 1 Framework for a typical clause-learning SAT-solver
Input:  : set of clauses;
S : partial assignment of truth values to variables.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.

while |S 6=  do
if   |S then
Conflict
if S contains no decision assignments then
print UNSATISFIABLE and halt
else
apply the learning scheme to add a new clause to 
if restart policy says restart then
set S = 
else
select most recent conflict-causing unreversed decision assignment in S
reverse this decision, and remove all later assignments from S
end if
end if
else if {l}  |S for some literal l then
Unit Propagation
add to S the implied assignment x = a which satisfies l
else
Decision
apply the branching strategy to choose a decision assignment x = a
add this decision assignment to S
end if
end while
print SATISFIABLE and output S

Most learning schemes in current use satisfy this assumption (Pipatsrisawat & Darwiche, 2009; Zhang et al., 2001), including the learning schemes called 1UIP and Decision (Zhang et al., 2001).
We make no particular assumption about the restart policy. However, our main result
is phrased in terms of a bound on the expected number of restarts. If the algorithm restarts
after r conflicts, our bound on the expected number of restarts can simply be multiplied
by r to get a bound on the expected number of conflicts. This means that the results
will be strongest if the algorithm restarts immediately after each conflict. In that case,
r = 1 and our bound will also bound the expected number of conflicts. Existing SATsolvers typically do not employ such an aggressive restart policy, but we note the remark
in the work of Pipatsrisawat and Darwiche (2009, p.666) that there has been a clear trend
towards more and more frequent restarts for modern SAT solvers.
The branching strategy determines which decision assignment is chosen after an inconclusive round that is not complete. In most current SAT solvers the strategy is based on
some heuristic measure of variable activity, which is related to the occurrence of a variable in
conflict clauses (Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). However, to simplify
the probabilistic analysis, we will make the following assumption.

340

fiLocal Consistency and SAT-Solvers

Assumption 2 The branching strategy chooses a variable uniformly at random amongst
the unassigned variables, and assigns it the value TRUE.
As noted by Atserias et al. (2011), the same analysis we give below can also be applied
to any other branching strategy that randomly chooses between making a heuristic-based
decision or a randomly-based decision. More precisely, if we allow, say, c > 1 rounds of nonrandom decisions between random ones, then the number of required restarts and conflicts
would appear multiplied by a factor of c.
An algorithm that behaves according to the description in Algorithm 1, and satisfies
the assumptions above, will be called a standard randomised SAT-solver.
Theorem 2 If a set of non-empty clauses  over n Boolean variables has a negativehyper-resolution refutation of width k and length m, then the expected number of restarts
requiredby a standard randomised SAT-solver to discover that  is unsatisfiable is less than
mnk 2 nk .
Proof. Let C1 , C2 , . . . , Cm be a negative-hyper-resolution refutation of width k from ,
where Cm is the first occurrence of the empty clause. Since each clause in  is non-empty,
Cm must be derived by negative-hyper-resolution from some collection of negative literals
x1 , x2 , . . . xd and a purely positive clause x1  x2      xd  .
Now consider a standard randomised SAT-solver started with database . Once all of
the unit clauses xi are absorbed by the current database, then, by Definition 4, any further
inconclusive round of the algorithm must assign all variables xi false, and hence falsify the
clause x1  x2     xd . Since this happens even when no decision assignments are made, the
SAT-solver will report unsatisfiability.
It only remains to bound the expected number of restarts required until each clause
Ci is absorbed, for 1  i < m. Let each Ci be the negative-hyper-resolvent of clauses
0 x , together with a clause C = C x x   x
Ci1 , Ci2 , . . . , Cir , each of the form Cij
j
i0
0
1
2
r
from , where C0 is a (possibly empty) disjunction of negative literals. Assume also that
each clause Cij is absorbed by  for j = 0, 1, . . . , r.
If  absorbs Ci , then no further learning or restarts are needed, so assume now that 
does not absorb Ci . By Definition 4, this means that there exists some literal l and some
inconclusive round R started with  that falsifies Ci \ {l} and does not satisfy Ci . Note
that R must leave the literal l unassigned, because one assignment would satisfy Ci and
0 , and hence force all of the literals x used in the
the other would falsify C0 and each Cij
j
negative-hyper-resolution step to be satisfied, because each Cij is absorbed by , so Ci0
would be falsified, contradicting the fact that R is inconclusive.
Hence, if the branching strategy chooses to falsify the literals Ci \ {l} whenever it has
a choice, it will construct an inconclusive round R0 where l is unassigned (since all the
decision assignments in R0 are also assigned the same values in R, any implied assignments
in R0 must also be assigned the same values3 in R, but we have shown that R leaves l
unassigned). If the branching strategy then chooses to falsify the remaining literal l of Ci ,
then the algorithm would construct a conclusive round R00 where Ci0 is falsified, and all
3. See Lemmas 5, 8 and 10 in the work of Atserias et al. (2011) for a more formal statement and proof of
this assertion.

341

fiJeavons & Petke

decision assignments falsify literals in Ci . Hence, by Assumption 1, the algorithm would
then learn some asserting clause C 0 and add it to  to obtain a new set 0 .
Since C 0 is an asserting clause, it contains exactly one literal, l0 , that is falsified at the
highest level in R00 . Hence, any inconclusive round R started with 0 that falsifies Ci \ {l}
will falsify all but one literal of C 0 , and hence force the remaining literal l0 to be satisfied,
by unit propagation. If this new implied assignment for l0 propagates to force l to be true,
then R satisfies Ci , and hence 0 absorbs Ci at l. If not, then the branching strategy can
once again choose to falsify the remaining literal l of Ci , which will cause a new asserting
clause to be learned and added to . Since each new asserting clause forces a new literal to
be satisfied after falsifying Ci \ {l} this process can be repeated fewer than n times before
it is certain that 0 absorbs Ci at l.
Now consider any sequence of k random branching choices. If the first k  1 of these
each falsify a literal of Ci \ {l}, and the final choice falsifies l, then we have shown that the
associated round will reach a conflict, and add an asserting clause to . With a random
branching strategy, as described in Assumption 2, the probability that this happens is at
least the probability that the first k  1 random choices consist of a fixed set of variables
(in some order), and the final choice is the variable associated with l. The number of
random choices that fall in a fixed set follows the hypergeometric distribution, so the overall
1
probability of this is n1 (nk+1)
= 1/(k nk ).
(k1)
To obtain an upper bound on the expected number of restarts, consider the worst case
where we require n asserting clauses to be added to absorb each clause Ci at each of its k
literals l. Since we require only an upper bound,
we will treat each round as an independent
n
trial with success probability p = 1/(k k ), and consider the worst case where we have to
achieve (m  1)nk successes to ensure that Ci for 1  i < m is absorbed. In this case the
total number of restarts will follow a negative binomial distribution, with expected value
(m  1)nk/p. Hence in all cases the expected number of restarts is less than mnk 2 nk . 2
A tighter bound on the number of restarts can be obtained if we focus on the Decision
learning scheme (Atserias et al., 2011; Zhang et al., 2001), as the next result indicates.
Theorem 3 If a set of non-empty clauses  over n Boolean variables has a negative-hyperresolution refutation of width k and length m, then the expected number of restarts required
by a standard randomised SAT-solver
using the Decision learning scheme to discover that

 is unsatisfiable is less than m nk .
Proof. The proof is similar to the proof of Theorem 2, except that the Decision learning scheme has the additional feature that the literals in the chosen conflict clause falsify a
subset of the current decision assignments. Hence in the situation we consider, where the
decision assignments all falsify literals of some clause Ci , this learning scheme will learn a
subset of Ci , and hence immediately absorb Ci , by Lemma 5 (1,2). Hence the maximum
number of learnt clausesrequired is
reduced from (m  1)nk to (m  1), and the probability

is increased from 1/(k nk ) to 1/ nk , giving the tighter bound.
2
Note that a similar argument shows that the standard deviation of the number of restarts
is less than the standard deviation of a negative binomial distribution with parameters m
342

fiLocal Consistency and SAT-Solvers




and 1/ nk , which is less than m nk . Hence, by Chebyshevs inequality (one-tailed version),
the probability that a standard randomised SAT-solver using the decision learning scheme

will discover that  is unsatisfiable after (m + m) nk restarts is greater than 1/2.

5. k-Consistency and SAT-Solvers
By combining Theorem 1 and Theorem 3 we obtain the following result linking k-consistency
and SAT-solvers.
Theorem 4 If the k-consistency closure of a CSP instance P is empty, then the expected
number of restarts required by a standard randomised SAT-solver using the Decision learning scheme to discover that the direct encoding of P is unsatisfiable is O(n2k d2k ), where n
is the number of variables in P and d is the maximum domain size.
Proof. The length m of a negative-hyper-resolution refutation of width k is bounded
Pk
i n
by the number of possible no-goods of length at most
k
for
P
,
which
is
i=1 d i . Hence,



by Theorem 1 and Theorem 3 we obtain a bound of


i n
i=1 d i

Pk

nd
k ,

which is O(n2k d2k ). 2

Hence a standard randomised SAT-solver with a suitable learning strategy will decide
the satisfiability of any CSP instance with tree-width k with O(n2k d2k ) expected restarts,
even when it is set to restart immediately after each conflict. In particular, the satisfiability
of any tree-structured binary CSP instance (i.e., with tree-width 1) will be decided by such
a solver with at most O(n2 d2 ) expected conflicts, which is comparable with the growth rate
of an optimal arc-consistency algorithm for binary constraints. Note that this result cannot
be obtained directly from the work of Atserias et al. (2011), because the direct encoding of
an instance with tree-width k is a set of clauses whose tree-width may be as high as dk.
Moreover, a standard randomised SAT-solver will decide the satisfiability of any CSP
instance, with any structure, within the same polynomial bounds, if the constraint relations
satisfy certain algebraic properties that ensure bounded width (Barto & Kozik, 2009).
Examples of such constraint types include the 0/1/all relations, defined by Cooper et al.
(1994), and the connected row-convex relations, defined by Deville et al. (1997), which
can both be decided by 2-consistency.
It was shown by Gent (2002) that the support encoding of a binary CSP instance can
be made arc-consistent (that is, 1-consistent) by applying unit propagation alone. Hence, a
standard SAT-solver will mimic the effect of enforcing arc-consistency on such an encoding
before making any decisions or restarts. By combining Theorem 4 with the observation in
Example 4 that the direct encoding can be obtained from the support encoding by negativehyper-resolution, we obtain the following corollary concerning the support encoding for all
higher levels of consistency.
Corollary 2 For any k  2, if the k-consistency closure of a binary CSP instance P
is empty, then the expected number of restarts required by a standard randomised SATsolver using the Decision learning scheme to discover that the support encoding of P is
unsatisfiable is O(n2k d2k ), where n is the number of variables in P and d is the maximum
domain size.
343

fiJeavons & Petke

The CSP literature describes many variations on the notion of consistency. In this
paper we have considered k-consistency only. We note that our results can be generalised
to some other types of consistency such as singleton arc-consistency (Bessiere, 2006). The
extension to singleton arc-consistency follows from the recent discovery that if a family of
CSP instances is solvable by enforcing singleton arc-consistency, then the instances have
bounded width (Chen, Dalmau, & Gruien, 2011). In other words, all such instances can
be solved by enforcing k-consistency, for some fixed k. Hence, by Theorem 4, they will be
solved in polynomial expected time by a standard randomised SAT-solver.

6. Experimental Results
The polynomial upper bounds we obtain in this paper are not asymptotic, they apply for
all values of n, m and k. However, they are very conservative, and are likely to be met very
easily in practice.
To investigate how an existing SAT-solver actually performs, we measured the runtime
of the MiniSAT solver (Een & Sorensson, 2003), version 2.2.0, on a family of CSP instances
that can be decided by a fixed level of consistency. For comparison, we also ran our experiments on two state-of-the-art constraint solvers: we used Minion (Gent, Jefferson, &
Miguel, 2006), version 0.12, and the G12 finite domain solver (Nethercote et al., 2007),
version 1.4.
To match the simplified assumptions of our analysis more closely, we ran a further
set of experiments on a core version of MiniSAT in order to get a solver that uses only
unit propagation and conflict-directed learning with restarts. We also modified the solver to
follow the random branching strategy described above. Our solver does not delete any learnt
clauses and uses an extreme restart policy that makes it restart whenever it encounters a
conflict. It uses the same learning scheme as MiniSAT. We refer to this modified solver as
simple-MiniSAT.
As the characteristic feature of the instances tested is their relatively low tree-width,
we also used the Toulbar2 solver (Sanchez et al., 2008). This solver implements the BTD
(Backtracking with Tree-Decomposition) technique which has been shown to be efficient
in practice, in contrast to earlier methods that had been proposed to attempt to exploit
tree-decompositions of the input problem (Jegou & Terrioux, 2003). As the problem of
finding a tree-decomposition of minimal width (i.e., the tree-width) is NP-hard, the BTD
technique uses some approximations (described in Jegou & Terrioux, 2003). We note here
that Toulbar2 is designed for solving optimization problems, namely weighted CSPs, or
WCSPs. In a WCSP instance, certain partial assignments have an associated cost. However,
the Toulbar2 solver can be used to solve standard CSPs by simply setting all costs to 0.
For all of the results, the times given are elapsed times on a Lenovo 3000 N200 laptop
with an Intel Core 2 Duo processor running at 1.66GHz with 2GB of RAM. Each generated
instance was run five times and the mean times and mean number of restarts are shown4 .
Example 7 We consider a family of instances specified by two parameters, w and d. They
have ((d1)w+2)w variables arranged in groups of size w, each with domain {0, ..., d1}.
4. MiniSAT and simple-MiniSAT were run with different seeds for each of the five runs of an instance.
Instances marked with * were run once only. The runtime of simple-MiniSAT on those instances
exceeded 6 hours. Moreover, Toulbar2 was run with parameter B = 1 which enables BTD.

344

fiLocal Consistency and SAT-Solvers

We impose a constraint of arity 2w on each pair of successive groups, requiring that the
sum of the values assigned to the first of these two groups should be strictly smaller than
the sum of the values assigned to the second. This ensures that the instances generated are
unsatisfiable. An instance with w = 2 and d = 2 is shown diagrammatically and defined
using the specification language MiniZinc (Nethercote et al., 2007) in Figure 1 (a) and (b)
respectively5 . A similar format is used for Toulbar2 6 and the same instance encoded in
this format is shown in Figure 1 (c) (note that each hard constraint has cost 0).

(a) Graphical representation.

chain
x1 0 1
x2 0 1
x3 0 1
x4 0 1
x5 0 1
x6 0 1
x7 0 1
x8 0 1
hard( x1 + x2 < x3 + x4 )
hard( x3 + x4 < x5 + x6 )
hard( x5 + x6 < x7 + x8 )

array[1..4] of var 0..1 : X1;
array[1..4] of var 0..1 : X2;
constraint
forall(i in 1..3)(
X1[i] + X2[i] < X1[i + 1] + X2[i + 1]);
solve satisfy;
(b) Specification in MiniZinc.

(c) Specification in cp format.

Figure 1: An example of a CSP instance with w = 2, d = 2 and tree-width = 3.

The structure of the instances described in Example 7 has a simple tree-decomposition as a
path of nodes, with each node corresponding to a constraint scope. Hence the tree-width of
these instances is 2w  1 and they can be shown to be unsatisfiable by enforcing (2w  1)consistency (Atserias et al., 2007). However, these instances cannot be solved efficiently
using standard propagation algorithms which only prune individual domain values.
The structure of the direct encoding of these instances also has a tree-decomposition
with each node corresponding to a constraint scope in the original CSP instance. However,
because the direct encoding introduces d Boolean variables to represent each variable in the
5. In order to run an instance on a CP solver one must usually use a translator to convert the original
model. The MiniZinc distribution provides an mzn2fzn translator while for Minion one can use Tailor
(available at http://www.cs.st-andrews.ac.uk/andrea/tailor/).
6. A cp2wcsp translator and a description of the cp and wcsp formats is available at
http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/SoftCSP.

345

fiJeavons & Petke

original instance, the tree-width of the encoded SAT instances is larger by approximately a
factor of d; it is in fact 2wd  1 (see Figure 2).

(a) Tree-decomposition of the original instance.

(b) Tree-decomposition of its direct
encoding.

Figure 2: Tree-decompositions of the CSP instance from Figure 1.
Table 1 shows the runtimes of simple-MiniSAT and the original MiniSAT solver on
this family of instances, along with times for the two state-of-the-art CP solvers and the
WCSP solver Toulbar2. By far the best solver for this set of instances is Toulbar2,
which is explicitly designed to exploit low tree-width by constructing a tree-decomposition.
For the class of instances we are considering, the widths of the tree-decompositions found
by Toulbar2 matched the tree-widths of the instances tested (i.e., 2w  1).
However, we also note that MiniSAT is remarkably effective in solving these chains
of inequalities, compared to Minion and G12, even though the use of MiniSAT requires
encoding each instance into a large number of clauses with a much larger tree-width than
the original. Although our simplified version of the MiniSAT solver takes a little longer
than the current highly optimised version, it still performs very well on these instances in
comparison with the conventional CP solvers. Moreover, the number of restarts (and hence
the number of conflicts) appears to grow only polynomially with the size of the instance
(see Figure 3). In all cases the actual number of restarts is much lower than the polynomial
upper bound on the expected number of restarts given in Theorem 4.
Our best theoretical upper bounds on the expected run-time were obtained for the
Decision learning scheme (Theorem 4), but the standard version of MiniSAT uses the
1UIP learning scheme with conflict clause minimization. To allow a direct comparison
with these theoretical upper bounds, we implemented the Decision scheme in simpleMiniSAT. As the 1UIP learning scheme has generally been found to be more efficient
in practice (Zhang et al., 2001), we switched off conflict clause minimization in simpleMiniSAT in order to compare the two standard learning schemes and ran a further set of
experiments. We counted the number of restarts for these two modified solvers on instances
of the form described in Example 7 - see Table 2.
346

fiLocal Consistency and SAT-Solvers

group
size
(w)
2
2
2
2
2
2
2
2
2
3
3
3
3
3
4
4
4

domain
size
(d)
2
3
4
5
6
7
8
9
10
2
3
4
5
6
2
3
4

CSP
variables
(n)
8
12
16
20
24
28
32
36
40
15
24
33
42
51
24
40
56

Minion

G12

(sec)
0.055
0.053
0.057
0.084
1.048
47.295
> 20 min
> 20 min
> 20 min
0.055
0.412
> 20 min
> 20 min
> 20 min
0.060
> 20 min
> 20 min

(sec)
0.010
0.011
0.013
0.047
0.959
122.468
> 20 min
> 20 min
> 20 min
0.010
0.034
7.147
> 20 min
> 20 min
0.015
11.523
> 20 min

Toulbar2

MiniSAT

(sec)
0.021
0.023
0.040
0.091
0.199
0.549
1.214
2.523
4.930
0.024
0.103
0.860
5.646
28.663
0.046
1.246
20.700

(sec)
0.003
0.005
0.015
0.043
0.126
0.362
0.895
2.407
5.656
0.004
0.066
1.334
20.984
383.564
0.012
4.631
1,160.873

simpleMiniSAT
(sec)
0.002
0.007
0.034
0.188
0.789
2.884
9.878
34.352
111.912
0.008
0.503
20.054
817.779
> 20 min
0.118
260.656
> 20 min

simpleMiniSAT
restarts
19
157
820
3 039
7 797
17 599
36 108
65 318
114 827
167
5 039
41 478
210 298
731 860
1 617
108 113
1 322 784*

Table 1: Average performance of solvers on instances from Example 7.
group
size
(w)

domain
size
(d)

CSP
variables
(n)

no. of clauses
in the direct
encoding

2
2
2
2
2
2
2
2
2
3
3
3
3
3
4
4

2
3
4
5
6
7
8
9
10
2
3
4
5
6
2
3

8
12
16
20
24
28
32
36
40
15
24
33
42
51
24
40

49
298
1 162
3 415
8 315
17 724
34 228
61 257
103 205
198
3 141
23 611
113 406
408 720
863
34 666

simpleMiniSAT
1UIP
(sec)
0.002
0.008
0.048
0.272
1.399
5.780
24.417
95.278
309.980
0.009
0.643
53.067
2,266.627
> 6 hours
0.141
603.241

simpleMiniSAT
1UIP
restarts
21
203
1 026
4 068
12 029
27 356
56 193
109 862
199 399
192
5 952
63 952
375 849
1 584 012*
1 937
155 842

simpleMiniSAT
Decision
(sec)
0.002
0.010
0.057
0.323
1.526
6.035
20.436
69.144
207.342
0.012
0.750
71.778
2,036.456
> 6 hours
0.192
938.836

simpleMiniSAT
Decision
restarts
23
267
1 424
5 283
14 104
33 621
64 262
113 460
190 063
287
7 308
91 283
391,664
1 365 481*
2 592
253 153

Table 2: Average performance of simple-MiniSAT with the 1UIP and the Decision learning schemes on instances from Example 7.

347

fiJeavons & Petke

Figure 3: Log-log plot of the number of restarts/conflicts used by simple-MiniSAT on the

instances from Example 7. The solid lines show a growth function of d2w2 nd/w
,
3
where n is the number of CSP variables. This empirically derived polynomial
function appears to fit the experimental data well, and is much lower than the
upper bound on the expected number of restarts calculated in Theorem 4 which
is O(d4w2 n4w2 ).

348

fiLocal Consistency and SAT-Solvers

Although the performance of simple-MiniSAT with the Decision learning scheme
and the 1UIP scheme are significantly worse than the performance of the original simpleMiniSAT solver, only about twice as many restarts were required for each instance. Hence,
our theoretical upper bounds are still easily met for both of these standard learning schemes.

7. Conclusions
We have shown that the notion of k-consistency can be precisely captured by a single
inference rule on the direct encoding of a CSP instance, restricted to deriving only clauses
with at most k literals. We used this to show that a clause-learning SAT-solver with a purely
random branching strategy will simulate the effect of enforcing k-consistency in expected
polynomial time, for all fixed k. This is sufficient to ensure that such solvers are able to
solve certain problem families much more efficiently than conventional CP solvers relying
on GAC-propagation.
In principle clause-learning SAT-solvers can also do much more. It is known that, with
an appropriate branching strategy and restart policy, they are able to p-simulate general
resolution (Beame et al., 2004; Pipatsrisawat & Darwiche, 2009), and general resolution
proofs can be exponentially shorter than the negative-hyper-resolution proofs we have considered here (Hwang & Mitchell, 2005). In practice, it seems that current clause-learning
SAT-solvers with highly-tuned learning schemes, branching strategies and restart policies
are often able to exploit structure in the Boolean encoding of a CSP instance even more
effectively than local consistency techniques. Hence considerable work remains to be done
in understanding the relevant features of instances which they are able to exploit, in order
to predict their effectiveness in solving different kinds of CSP instances.

Acknowledgments
We would like to thank Albert Atserias and Marc Thurley for comments on the conference
version of this paper, as well as the anonymous referees. The provision of an EPSRC
Doctoral Training Award to Justyna Petke is also gratefully acknowledged.
A preliminary version of this paper appeared in Proceedings of the 16th International
Conference on Principles and Practice of Constraint Programming - CP2010.

References
Atserias, A., Bulatov, A. A., & Dalmau, V. (2007). On the power of k-consistency. In
International Colloquium on Automata, Languages and Programming - ICALP07,
pp. 279290.
Atserias, A., & Dalmau, V. (2008). A combinatorial characterization of resolution width.
Journal of Computer and System Sciences, 74 (3), 323334.
Atserias, A., Fichte, J. K., & Thurley, M. (2011). Clause-learning algorithms with many
restarts and bounded-width resolution. Journal of Artificial Intelligence Research
(JAIR), 40, 353373.
Bacchus, F. (2007). GAC via unit propagation. In Principles and Practice of Constraint
Programming - CP07, pp. 133147.
349

fiJeavons & Petke

Barto, L., & Kozik, M. (2009). Constraint satisfaction problems of bounded width. In
Symposium on Foundations of Computer Science - FOCS09, pp. 595603.
Beame, P., Kautz, H. A., & Sabharwal, A. (2004). Towards understanding and harnessing
the potential of clause learning. Journal of Artificial Intelligence Research (JAIR),
22, 319351.
Bessiere, C. (2006). Constraint propagation. In Rossi, F., van Beek, P., & Walsh, T. (Eds.),
Handbook of Constraint Programming, chap. 3. Elsevier.
Buning, H., & Lettmann, T. (1999). Propositional logic: deduction and algorithms. Cambridge tracts in theoretical computer science. Cambridge University Press.
Chen, H., Dalmau, V., & Gruien, B. (2011). Arc consistency and friends. Computing
Research Repository - CoRR, abs/1104.4993.
Cooper, M. C. (1989). An optimal k-consistency algorithm. Artificial Intelligence, 41 (1),
8995.
Cooper, M. C., Cohen, D. A., & Jeavons, P. (1994). Characterising tractable constraints.
Artificial Intelligence, 65 (2), 347361.
de Kleer, J. (1989). A comparison of ATMS and CSP techniques. In International Joint
Conference on Artificial Intelligence - IJCAI89, pp. 290296.
Deville, Y., Barette, O., & Hentenryck, P. V. (1997). Constraint satisfaction over connected
row convex constraints. In International Joint Conference on Artificial Intelligence IJCAI97 (1), pp. 405411.
Een, N., & Sorensson, N. (2003). An extensible SAT-solver. In Theory and Applications of
Satisfiability Testing - SAT03, pp. 502518.
Freuder, E. C. (1978). Synthesizing constraint expressions. Communications of the ACM,
21 (11), 958966.
Gent, I. P. (2002). Arc consistency in SAT. In European Conference on Artificial Intelligence
- ECAI02, pp. 121125.
Gent, I. P., Jefferson, C., & Miguel, I. (2006). Minion: A fast scalable constraint solver. In
European Conference on Artificial Intelligence - ECAI06, pp. 98102.
Hooker, J. N. (2006). Integrated Methods for Optimization (International Series in Operations Research & Management Science). Springer-Verlag New York, Inc., Secaucus,
NJ, USA.
Hoos, H. H. (1999). SAT-encodings, search space structure, and local search performance.
In International Joint Conference on Artificial Intelligence - IJCAI99, pp. 296303.
Hwang, J., & Mitchell, D. G. (2005). 2-way vs. d-way branching for CSP. In Principles and
Practice of Constraint Programming - CP05, pp. 343357.
Jegou, P., & Terrioux, C. (2003). Hybrid backtracking bounded by tree-decomposition of
constraint networks. Artificial Intelligence, 146 (1), 4375.
Kolaitis, P. G., & Vardi, M. Y. (2000). A game-theoretic approach to constraint satisfaction. In Conference on Artificial Intelligence - AAAI00 / Innovative Applications of
Artificial Intelligence Conference - IAAI00, pp. 175181.
350

fiLocal Consistency and SAT-Solvers

Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8 (1),
99118.
Montanari, U. (1974). Networks of constraints: Fundamental properties and applications to
picture processing. Information Sciences, 7, 95132.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering an efficient SAT solver. In Design Automation Conference - DAC01, pp.
530535.
Nethercote, N., Stuckey, P. J., Becket, R., Brand, S., Duck, G. J., & Tack, G. (2007).
MiniZinc: Towards a standard CP modelling language. In Principles and Practice of
Constraint Programming - CP07, pp. 529543.
Petke, J., & Jeavons, P. (2009). Tractable benchmarks for constraint programming. Technical Report RR-09-07, Department of Computer Science, University of Oxford.
Pipatsrisawat, K., & Darwiche, A. (2009). On the power of clause-learning SAT solvers with
restarts. In Principles and Practice of Constraint Programming - CP09, pp. 654668.
Prestwich, S. D. (2009). CNF encodings. In Biere, A., Heule, M., van Maaren, H., & Walsh,
T. (Eds.), Handbook of Satisfiability, pp. 7597. IOS Press.
Rish, I., & Dechter, R. (2000). Resolution versus search: Two strategies for SAT. Journal
of Automated Reasoning, 24 (1/2), 225275.
Robinson, J. A. (1965). A machine-oriented logic based on the resolution principle. Journal
of the ACM, 12 (1), 2341.
Sanchez, M., Bouveret, S., de Givry, S., Heras, F., Jegou, P., Larrosa, J., Ndiaye, S., Rollon,
E., Schiex, T., Terrioux, C., Verfaillie, G., & Zytnicki, M. (2008). Max-CSP competition 2008: Toulbar2 solver description. In Proceedings of the Third International
CSP Solver Competition.
Schiex, T., & Verfaillie, G. (1993). Nogood recording for static and dynamic constraint
satisfaction problems. In International Conference on Tools with Artificial Intelligence
- ICTAI93, pp. 4855.
Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling finite linear CSP
into SAT. Constraints, 14 (2), 254272.
van Dongen, M., Lecoutre, C., & Roussel, O. (2008). 3rd international CSP solver competition. Instances and results available at http://www.cril.univ-artois.fr/CPAI08/.
van Dongen, M., Lecoutre, C., & Roussel, O. (2009). 4th international CSP solver competition. Instances and results available at http://www.cril.univ-artois.fr/CPAI09/.
Walsh, T. (2000). SAT v CSP. In Principles and Practice of Constraint Programming CP00, pp. 441456.
Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict driven
learning in Boolean satisfiability solver. In International Conference on ComputerAided Design - ICCAD01, pp. 279285.
Zhang, L., & Malik, S. (2002). The quest for efficient Boolean satisfiability solvers. In
Computer Aided Verification - CAV02, pp. 1736.

351

fiJournal of Artificial Intelligence Research 43 (2012) 87-133

Submitted 07/11; published 01/12

Location-Based Reasoning about Complex Multi-Agent Behavior
Adam Sadilek
Henry Kautz

SADILEK @ CS . ROCHESTER . EDU
KAUTZ @ CS . ROCHESTER . EDU

Department of Computer Science, University of Rochester
Rochester, NY 14627, USA

Abstract
Recent research has shown that surprisingly rich models of human activity can be learned from
GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling
actual successful executions (and not failed or attempted executions) of the activities of interest.
We, in contrast, take on the task of understanding human interactions, attempted interactions, and
intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world
game of capture the flag to illustrate our approach in a well-defined domain that involves many
distinct cooperative and competitive joint activities. We model the domain using Markov logic, a
statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines
constraints imposed by the geometry of the game area, the motion model of the players, and by
the rules and dynamics of the game in a probabilistically and logically sound fashion. We show
that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the
future behaviors of the people involved as well as the events that could have preceded it. Further,
we show that given a model of successfully performed multi-agent activities, along with a set of
examples of failed attempts at the same activities, our system automatically learns an augmented
model that is capable of recognizing success and failure, as well as goals of peoples actions with
high accuracy. We compare our approach with other alternatives and show that our unified model,
which takes into account not only relationships among individual players, but also relationships
among activities over the entire length of a game, although more computationally costly, is significantly more accurate. Finally, we demonstrate that explicitly modeling unsuccessful attempts
boosts performance on other important recognition tasks.

1. Introduction
Our society is founded on the interplay of human relationships and interactions. Since every person is tightly embedded in our social structure, the vast majority of human behavior can be fully
understood only in the context of the actions of others. Thus, not surprisingly, more and more evidence shows that when we want to model behavior of a person, the single best predictor is often the
behavior of people in her social network. For instance, behavioral patterns of people taking taxis,
rating movies, choosing a cell phone provider, or sharing music are best explained and predicted by
the habits of related people, rather than by all the single person attributes such as age, race, or
education (Bell, Koren, & Volinsky, 2007; Pentland, 2008).
In contrast to these observations, most research effort on activity recognition to date has concentrated on modeling single individuals (Bui, 2003; Liao, Fox, & Kautz, 2004, 2005), or statistical
properties of aggregate groups of individuals (Abowd, Atkeson, Hong, Long, Kooper, & Pinkerton,
1997; Horvitz, Apacible, Sarin, & Liao, 2005), or combinations of both (Eagle & Pentland, 2006).
c
2012
AI Access Foundation. All rights reserved.

fiS ADILEK & K AUTZ

Notable exceptions to this isolated individuals approach includes the work of Kamar and Horvitz
(2009) and Gupta, Srinivasan, Shi, and Davis (2009), where simple relationships among people are
just starting to be explicitly considered and leveraged. For instance, Eagle and Pentland (2006)
elegantly model the location of individuals from multi-modal sensory data, but their approach is
oblivious to the explicit effects of ones friends, relatives, etc. on ones behavior. The isolated individuals approximations are often made for the sake of tractability and representational convenience.
While considering individuals independently of each other is sufficient for some constrained tasks,
in many interesting domains it discards a wealth of important information or results in an inefficient and unnatural data representation. On the other hand, decomposing a domain into a set of
entities (representing for instance people, objects in their environment, or activities) that are linked
by various relationships (e.g., is-a, has-a, is-involved-in) is a natural and clear way of representing
data.
To address the shortcomings of nonrelational behavior modeling, we introduce the capture the
flag domain (described below), and argue for a statistical-relational approach to learning models
of multi-agent behavior from raw GPS data. The CTF dataset is on one hand quite complex and
recorded by real-world sensors, but at the same time it is well-defined (as per the rules of the game),
thereby allowing for an unambiguous evaluation of the results.
Being able to recognize peoples activities and reason about their behavior is a necessary precondition for having intelligent and helpful machines that are aware of what is going on in the
human-machine as well as human-human relationships. There are many exciting practical applications of activity recognition that have the potential to fundamentally change peoples lives. For
example, cognitive assistants that help people and teams be more productive, or provide support to
(groups of) disabled individuals, or efficiently summarize a long complex event to a busy person
without leaving out essential information. Other important applications include intelligent navigation, security (physical as well as digital), human-computer interaction, and crowdsourcing. All
these applications and a myriad of others build on top of multi-agent activity recognition and therefore require it as a necessary stepping stone. Furthermore, as a consequence of the anthropocentrism
of our technology, modeling human behavior playsperhaps surprisinglya significant role even
in applications that do not directly involve people (e.g., unmanned space probes).
Furthermore, reasoning about human intentions is an essential element of activity recognition,
since if we can recognize what a person (or a group of people) wants to do, we can proactively
try to help them (orin adversarial situationshinder them). Intent is notoriously problematic to
quantify (e.g., Baldwin & Baird, 2001), but we show that in the capture the flag domain, the notion
is naturally captured in the process of learning the structure of failed activities. We all know perhaps
too well that a successful action is often precededand unfortunately sometimes also followedby
multiple failed attempts. Therefore, reasoning about attempts typically entails high practical utility,
but not just for their relatively high frequency. Consider, for example, a task of real-time analysis
of a security video system. There, detecting that a person or a group of people (again, relations)
intend to steal something is much more important and useful than recognizing that a theft has taken
(or even is taking) place, because then it is certainly too late to entirely prevent the incident, and it
may also be too late or harder to merely stop it. We believe that recognition of attempts in peoples
activities is a severely underrepresented topic in artificial intelligence that needs to be explored more
since it opens a new realm of interesting possibilities.
Before we delve into the details of our approach in Sections 5 and 6, we briefly introduce
the CTF dataset (Section 2), highlight the main contributions of our work (Section 3), and review
88

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

background material (Section 4). We discuss related work, conclude, and outline future work in
Sections 7, 8 and 9 respectively.
This paper incorporates and extends our previous work (Sadilek & Kautz, 2010a, 2010b).

2. Capture The Flag Domain
Imagine two teamsseven players eachplaying capture the flag (CTF) on a university campus,
where each player carries a consumer-grade global positioning system (GPS) that logs its location
(plus noise) every second (see Figure 1). The primary goal is to enter the opponents flag area.
Players can be captured only while on enemy territory by being tagged by the enemy. Upon being
captured, they must remain in place until freed (tagged by a teammate) or the game ends. The
games involve many competitive and cooperative activities, but here we focus on (both successful
and attempted) capturing and freeing. Visualization of the games is available from the first authors
website.
We collected four games of CTF on a portion of the University of Rochester campus (about
23 acres) with Columbus V-900 GPS loggers (one per player) with 1 GB memory card each that
were set to a sampling rate of 1 Hz. The durations of the games ranged approximately from 4 to 15
minutes.
Our work is not primarily motivated by the problem of annotating strategy games, although
there are obvious applications of our results to sports and combat situations. We are, more generally, exploring relational learning and inference methods for recognizing multi-agent activities
from location data. We accept the fact that the GPS data at our disposal is inherently unreliable and
ambiguous for any one individual. We therefore focus on methods that jointly and simultaneously
localize and recognize the high-level activities of groups of individuals.
Although the CTF domain doesnt capture all the intricacies of life, it contains many complex, interesting, and yet well-defined (multi-agent) activities. Moreover, it is based on extensive
real-world GPS data (total of 40,000+ data points). Thus most of the problems that we are addressing here clearly have direct analogs in everyday-life situations that ubiquitous computing needs to
addressimagine people going about their daily lives in a city instead of CTF players, and their
own smart phones instead of GPS loggers.
One of the main challenges we have to overcome if we are to successfully model CTF is the
severe noise present in the data. Accuracy of the GPS data varies from 1 to more than 10 meters. In
open areas, readings are typically off by 3 meters, but the discrepancy is much higher in locations
with tall buildings (which are present within the game area) or other obstructions. Compare the
scale of the error with the granularity of the activities we concern ourselves with: both capturing
and freeing involves players that are within reaching distance (less than 1 meter) apart. Therefore,
the signal to noise ratio in this domain is daunting.
The error has a systematic component as well as a significant stochastic component. Errors
between devices are poorly correlated, because subtle differences between players, such as the angle
at which the device sits in the players pocket, can dramatically affect accuracy. Moreover, since
we consider multi-agent scenarios, the errors in individual players readings can add up, thereby
creating a large discrepancy between the reality and the recorded dataset. Because players can
move freely through open areas, we cannot reduce the data error by assuming that the players move
along road or walkways, as is done in much work on GPS-based activity recognition (e.g., Liao
et al., 2004). Finally, traditional techniques for denoising GPS data, such as Kalman filtering, are

89

fiS ADILEK & K AUTZ

Figure 1: A snapshot of a game of capture the flag that shows most of the game area. Players are
represented by pins with letters. In our version of CTF, the two flags are stationary
and are shown as white circles near the top and the bottom of the figure. The horizontal road in the middle of the image is the territory boundary. The data is shown prior
to any denoising or corrections for map errors. Videos of the games are available at
http://www.cs.rochester.edu/u/sadilek/
90

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

of little help, due to the low data rate (1 sample per second) relative to the small amount of time
required for a player to completely change her speed or direction.
If we are to reliably recognize events that happen in these games in the presence of such severe
noise, we need to consider not only each player, but also the relationships among them and their
actions over extended periods of time (possibly the whole length of the game). Consider a concrete
task of inferring the individual and joint activities and intentions of the CTF players from their GPS
traces. For example, suppose the GPS data shows player A running toward a stationary teammate
B, then moving away. What occurred? Possibly player A has just freed player B, but GPS error
has hidden the fact that player A actually reached B. Another possibility is that player A had the
intention of freeing player B, but was scared off by an opponent at the last second. Yet another possibility is that no freeing occurred nor was even intended, because player B had not been previously
captured.
Understanding a game thus consists of inferring a complex set of interactions among the various
players as well as the players intentions. The conclusions drawn about what occurs at one point in
time affect and are affected by inferences about past and future events. In the example just given,
recognizing that player B is moving in the future reinforces the conclusion that player A is freeing
player B, while failing to recognize a past event of player B being captured decreases confidence in
that conclusion. The game of CTF also illustrates that understanding a situation is as much or more
about recognizing attempts and intentions as about recognizing successfully executed actions. For
example, in course of a 15 minute game, only a handful of capture or freeing events occur. However,
there are dozens of cases where one player unsuccessfully tries to capture an opponent or to free a
teammate. A description of a game that was restricted to what actually occurred would be only a
pale reflection of the original.

Figure 2: Three snapshots of a game situation where both successful and failed capturing occur.
This example also illustrates the need for an approach that exploits both the relational
and the far reaching temporal structure of our domain. (See text for explanation.)

As a concrete example, consider a real game situation illustrated in Figure 2. There we see three
snapshots of a game projected over a map of the campus before any modification of the GPS data.
The game time is shown on each snapshot. Players D, F, and G are allies and are currently on their
home territory near their flag, whereas players L and M are their enemies. In the first snapshot,
players L and M head for the opponents flag but thenin the second framethey are intercepted
by G. At this point it is unclear what is happening because of the substantial error in the GPS data

91

fiS ADILEK & K AUTZ

the three players appear to be very close to each other, but in actuality they could have been 20 or
more meters apart. However, once we see the third snapshot (note that tens of seconds have passed)
we realize that player G actually captured only player M and didnt capture L since G is evidently
still chasing L. The fact that player M remains stationary coupled with the fact that neither D nor F
attempt to capture him suggests that M has indeed been captured. We show that it is possible to infer
occurrences of capturing events even for complex situations like these whereas limited approaches
largely fail. However, we need to be able to recognize not just individual events, we also need
to discover new activities, identify their respective goals, and distinguish between events based on
whether their outcomes are favorable or negative. For instance, in the second frame, player G tries
to capture both L and M. Although he succeeded in the former case, he failed in the latter.
Many different kinds of cooperative and competitive multi-agent activities occur in the games.
The lowest-level joint activities are based on location and movement, and include approaching and
being at the same location. Note, that noise in the GPS data often makes it difficult or impossible
to directly detect these simple activities. At the next level come competitive multi-agent activities
including capturing and attacking; cooperative activities include freeing; and there are activities,
such as chasing and guarding, that may belong to either category or to both categories. There
are also more abstract tactical activities, such as making a sacrifice, and overall strategies, such as
playing defensively. In this paper, we concentrate on activities at the first two levels.

3. Our Contributions
The main contributions of this paper are as follows. We first present a novel method that simultaneously denoises positional data and learns a model of multi-agent activities that occur there. We
subsequently evaluate the model on the CTF dataset and show that it achieves high accuracy in
recognizing complex game events.
However, creating a model by manually writing down new rules or editing existing axioms is
laborious and prone to introduction of errors or unnecessarily complex theories. Thus, we would
like to automate this process by learning (or inducing) new axioms from training data. For people,
it is much easier to provide or validate concrete examples than to directly modify a model. This
leads us to our second contribution: We show how to automatically augment a preexisting model of
(joint) activities so that it is capable of not only recognizing successful actions, but also identifies
failed attempts at the same types of activities. This line of work also demonstrates that explicitly
modeling attempted interactions in a unified way improves overall model performance.
As our third contribution, we demonstrate that the difference (discussed below) between the
newly learned definitions of a failed activity and the original definition of the corresponding successful activity directly corresponds to the goal of the given activity. For instance, as per the rules
of the capture the flag game, a captured player cannot move until freed. When our system induces
the definition of failed capture, the new theory does not contain such a constraint on the movement
of the almost-captured player, thereby allowing him to move freely.

4. Background
The cores of our models described below are implemented in Markov logic (ML), a statisticalrelational language. In this section, we provide a brief overview of ML, which extends finite firstorder logic (FOL) to a probabilistic setting. For a more detailed (and excellent) treatment of FOL,

92

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

ML, and inductive logic programming see the work of Shoenfield (1967), Domingos, Kok, Lowd,
Poon, Richardson, and Singla (2008), and De Raedt and Kersting (2008), respectively.
In order to compare the Markov logic based models to alternative approaches, we consider a
dynamic Bayesian network (DBN) model in the experiments below as one of our baselines. We
therefore review relevant aspects of DBNs in this section as well.
4.1 Markov Logic
Given the inherent uncertainty involved in reasoning about real-world activities as observed through
noisy sensor readings, we looked for a methodology that would provide an elegant combination of
probabilistic reasoning with the expressive, relatively natural, and compact but unfortunately strictly
true or false formulas of first-order logic. And that is exactly what Markov logic provides and thus
allows us to elegantly model complex finite relational non-i.i.d. domains. A Markov logic network
(MLN) consists of a set of constants C and of a set of pairs hFi , wi i such that each FOL formula
Fi has a weight wi  R associated with it. Optionally, each weight can be further scaled by a
real-valued function of a subset of the variables that appear in the corresponding formula. Markov
logic networks that contain such functions are called hybrid MLNs (Wang & Domingos, 2008).
A MLN can be viewed as a template for a Markov network (MN) as follows: the MN contains
one node for each possible ground atom of MLN. The value of the node is 0 if the corresponding
atom is false and 1 otherwise. Two nodes are connected by an edge if the corresponding atoms
appear in the same formula. Thus, the MN has a distinct clique corresponding to each grounding of
g
each formula. By Fi j we denote the j-th grounding of formula Fi . The MN has a feature value fi,j
gj
for each Fi such that
(
g
1 if Fi j is true
fi,j =
0 otherwise
Each weight wi intuitively represents the relative importance of satisfying (or violating, if the
weight is negative) the corresponding formula Fi . More formally, the weight scales the difference
in log-probability between a world that satisfies n groundings of the corresponding formula and one
that results in m true groundings of the formula, all else being equal (cf. Equation 1). Thus the
problem of satisfiability is relaxed in MLNs. We no longer search for a satisfying truth assignment
as in traditional FOL. Instead, we are looking for a truth assignment that maximizes the sum of the
weights of all satisfied formulas.
The weights can be either specified by the knowledge base engineer or, as in our approach,
learned from training data. That is, we provide the learning algorithm with labeled capture instances and pairs of raw and corresponding denoised trajectories along with labeled instances of
game events and it finds an optimal set of weights that maximize the likelihood of the training
data. Weight learning can be done in either generative or discriminative fashion. Generative training maximizes the joint probability of observed (evidence) as well as hidden (query) predicates,
whereas discriminative learning directly maximizes the conditional likelihood of the hidden predicates given the observed predicates. Since prior work demonstrated that Markov network models
learned discriminatively consistently outperform their generatively trained counterparts (Singla &
Domingos, 2005), we focus on discriminative learning in our activity recognition domain.
Once the knowledge base with weights has been specified, we can ask questions about the state
of hidden atoms given the state of the observed atoms. Let X be a vector of random variables
(one random variable for each possible ground atom in the MN) and let  be the set of all possible
93

fiS ADILEK & K AUTZ

instantiations of X. Then, each x   represents a possible world. If (x  )[Pr(X = x) > 0]
holds, the probability distribution over these worlds is defined by
!
X

1
Pr(X = x) = exp
wi ni x{i}
(1)
Z
i

where ni (x{i} ) is the number of true groundings of i-th formula with wi as its weight in a world x
and
!
X
X

Z=
exp
wi ni x{i}
(2)
x

i

Equation 1 can be viewed as assigning a score to each possible world and dividing each score
by the sum of all scores over all possible worlds (the constant Z) in order to normalize.
Maximum a posteriori (MAP) inference in Markov logic given the state of the observed atoms
reduces to finding a truth assignment for the hidden atoms such that the weighed sum of satisfied
clauses is maximal. Even though this problem is in general #P-complete, we achieve reasonable
run times by applying Cutting Plane MAP Inference (CPI) (Riedel, 2008). CPI can be thought of as
a meta solver that incrementally grounds a Markov logic network, at each step creating a Markov
network that is subsequently solved by any applicable methodsuch as MaxWalkSAT or via a
reduction to an integer linear program. CPI refines the current solution by searching for additional
groundings that could contribute to the objective function.
Up to this point, we have focused on first-order Markov logic. In first-order ML, each variable
ranges over objects present the domain (e.g., apples, players, or cars). On the other hand, in finite
second-order Markov logic, we variabilize not only objects but also predicates (relations) themselves (Kok & Domingos, 2007). Our CTF model contains a predicate variable for each type of activity. For example, we have one variable captureType whose domain is {capturing, failedCapturing}
and analogously for freeing events. When grounding the second-order ML, we ground all predicate
variables as well as object variables. There has also been preliminary work on generalizing ML to
be well-defined over infinite domains, which would indeed give it the full power of FOL (Singla &
Domingos, 2007).
Implementations of Markov logic include Alchemy1 and theBeast2 . Our experiments used a
modified version of theBeast.
4.2 Dynamic Bayesian Networks
A Bayesian network (BN) is a directed probabilistic graphical model (Jordan, 1998). Nodes in the
graph represent random variables and edges represent conditional dependencies (cf. Figure 4). For
a BN with n nodes, the joint probability distribution is given by
Pr(X1 , . . . , Xn ) =

n
Y
i=1

1. http://alchemy.cs.washington.edu/
2. http://code.google.com/p/theBeast/

94


Pr Xi |Pa(Xi ) ,

(3)

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

where Pa(Xi ) denotes the parents of node Xi . In a typical setting, a subset of the random variables
is observed (we know their actual values), while the others are hidden and their values need to be
inferred.
A dynamic Bayesian network (DBN) is a BN that models sequential data. A DBN is composed
of slicesin our case each slice represents a one second time interval. In order to specify a DBN,
we either write down or learn intra- and inter-slice conditional probability distributions (CPDs).
The intra-slice CPDs typically constitute the observation model while the inter-slice CPDs model
transitions between hidden states. For an extensive treatment of DBNs, see the work of Murphy
(2002).
There are a number of parameter learning and inference techniques for DBNs. To match the
Markov logic-based framework, in the experiments with the DBN model presented below, we focus
on a supervised learning scenario, where the hidden labels are known at training time and therefore
a maximum likelihood estimate can be calculated directly.
We find a set of parameters (discrete probability distributions)  that maximize the log-likelihood
of the training data. This is achieved by optimizing the following objective function.

? = argmax log Pr x1:t , y1:t |) ,

(4)



where x1:t and y1:t represent the sequence of observed and hidden values, respectively, between
times 1 and t, and ? is the set of optimal model parameters. In our implementation, we represent
probabilities and likelihoods with their log-counterparts to avoid arithmetic underflow.
At testing time, we are interested in the most likely explanation of the observed data. That is, we
want to calculate the most likely assignment of states to all the hidden nodes (i.e., Viterbi decoding
of the DBN) given by

?
(5)
y1:t
= argmax log Pr(y1:t |x1:t ) ,
y1:t

where Pr(y1:t |x1:t ) is the conditional probability of a sequence of hidden states y1:t given a concrete
sequence of observations x1:t between times 1 and t. We calculate the Viterbi decoding efficiently
using dynamic programming (Jordan, 1998).

5. Methodology
In this section, we describe the three major components of our approach. In short, we first manually
construct a model of captures and freeings in CTF and optimize its parameters in a supervised
learning framework (Section 5.1). This constitutes our seed theory that is used for denoising raw
location data and recognition of successful multi-agent activities. We then show, in Section 5.2,
how to automatically extend the seed theory by inducing the structure and learning the importance
of failed captures and freeings as well as the relationships to their successful counterparts. Finally, in
Section 5.3, we use the augmented theory to recognize this richer set of multi-agent activitiesboth
successful and failed attemptsand extract the goals of the activities.
Specifically, we investigate the following four research questions:
Q1. Can we reliably recognize complex multi-agent activities in the CTF dataset even in the presence of severe noise?
Q2. Can models of attempted activities be automatically learned by leveraging existing models of
successfully performed actions?
95

fiS ADILEK & K AUTZ

Q3. Does modeling both success and failure allow us to infer the respective goals of the activities?
Q4. Does modeling failed attempts of activities improve the performance on recognizing the activities themselves?
We now elaborate on each of the three components of our system in turn, and subsequently
discuss, in light of the experimental results and lessons learned, our answers to the above research
questions.
5.1 Recognition of Successful Activities
In this section, we present our unified framework for intelligent relational denoising of the raw GPS
data while simultaneously labeling instances of a player being captured by an enemy or freed by an
ally. Both the denoising and the labeling are cast as a learning and inference problem in Markov
logic. By denoising, we mean modifying the raw GPS trajectories of the players such that the final
trajectories satisfy constraints imposed by the geometry of the game area, the motion model of the
players, as well as by the rules and the dynamics of the game. In this paper, we refer to this trajectory
modification as snapping since we tile the game area with 3 by 3 meter cells and snap each raw
GPS reading to an appropriate cell. By creating cells only in unobstructed space, we ensure the final
trajectory is consistent with the map of the area.
We begin by modeling the domain via a Markov logic theory, where we write the logical formulas that express the structure of the model by hand, and learn an optimal set of weights on the
formulas from training data in a supervised discriminative fashion (details on the experimental setup are in Section 6). In the following two subsections, we will show how to augment this seed
Markov logic theory to recognize a richer set of events and extract the goals of players multi-agent
activities.
In order to perform data denoising and recognition of successful capturing and freeing, we
model the game as weighted formulas in Markov logic. Some of the formulas are hard, in the
sense that we are only interested in solutions that satisfy all of them. Hard formulas capture basic
physical constraints (e.g., a player is only at one location at a time) and inviolable rules of the game
(e.g., a captured player must stand still until freed or the game ends).3 The rest of the formulas
are soft, meaning there is a finite weight associated with each one. Some of the soft constraints
correspond to a traditional low-level data filter, expressing preferences for smooth trajectories that
are close to the raw GPS readings. Other soft constraints capture high-level constraints concerning
when individual and multi-agent activities are likely to occur. For example, a soft constraint states
that if a player encounters an enemy on the enemys territory, the player is likely to be captured.
The exact weights on the soft constraints are learned from labeled data, as described below.
We distinguish two types of atoms in our models: observed (e.g., GPS(P1 , 4, 43.13 , 77.71 )
and hidden (e.g., freeing(P1 , P8 , 6)). The observed predicates in the CTF domain are: GPS, enemies, adjacent, onHomeTer, and onEnemyTer;4 whereas capturing, freeing, isCaptured, isFree,
samePlace, and snap are hidden. Additionally, the set of hidden predicates is expanded by the structure learning algorithm described below (see Table 1 for predicate semantics). In the training phase,
3. Cheating did not occur in our CTF games, but in principle could be accommodated by making the rules highlyweighted soft constraints rather than hard constraints.
4. While the noise in the GPS data introduces some ambiguity to the last two observed predicates, we can still reliably
generate them since the road that marks the boundary between territories constitutes a neutral zone.

96

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Hard Rules:
H1. Each raw GPS reading is snapped to exactly one cell.
H2.

(a) When player a frees player b, then both involved players must be snapped to a common cell at
that time.
(b) A player can only be freed by a free ally.
(c) A player can be freed only when he or she is currently captured.
(d) Immediately after a freeing event, the freed player transitions to a free state.
(e) A player can only be freed while on enemy territory.

H3.

(a) When player a captures player b, then both involved players must be snapped to a common cell
at that time.
(b) A player can only be captured by a free enemy.
(c) A player can be captured only if he or she is currently free.
(d) Immediately after a capture event, the captured player transitions to a captured state.
(e) A player can be captured only when standing on enemy territory.

H4. All players are free at the beginning of the game.
H5. At any given time, a player is either captured or free but not both.
H6. A player transitions from a captured state to a free state only via a freeing event.
H7. A player transitions from a free state to a captured state only via a capture event.
H8. If a player is captured then he or she must remain in the same location.

Soft Rules:
S1. Minimize the distance between the raw GPS reading and the snapped-to cell.
S2. Minimize projection variance, i.e., two consecutive snappings should be generally correlated.
S3. Maximize smoothness (both in terms of space and time) of the final player trajectories.
S4. If players a and b are enemies, a is on enemy territory and b is not, b is not captured already, and they
are close to each other, then a probably captures b.
S5. If players a and b are allies, both are on enemy territory, b is currently captured and a is not, and they
are close to each other, then a probably frees b.
S6. Capture events are generally rare, i.e., there are typically only a few captures within a game.
S7. Freeing events are also generally rare.

Figure 3: Descriptions of the hard and soft rules for capture the flag.
our learning algorithm has access to the known truth assignment to all atoms. In the testing phase,
it can still access the state of the observed atoms, but it has to infer the assignment to the hidden
atoms.
Figure 3 gives an English description of our hard and soft rules for the low-level movement
and player interactions within capture the flag. Corresponding formulas in the language of ML are
shown in Figures 5 and 6.
97

fiS ADILEK & K AUTZ

Predicate
capturing(a, b, t)
enemies(a, b)
adjacent(c1 , c2 )
failedCapturing(a, b, t)
failedFreeing(a, b, t)
freeing(a, b, t)
isCaptured(a, t)
isFailedCaptured(a, t)

Type
hidden
observed
observed
hidden
hidden
hidden
hidden
hidden

isFailedFree(a, t)

hidden

isFree(a, t)

hidden

onEnemyTer(a, t)
onHomeTer(a, t)
samePlace(a, b, t)

observed
observed
hidden

snap(a, c, t)

hidden

Meaning
Player a is capturing b at time t.
Players a and b are enemies.
Cells c1 and c2 are mutually adjacent, or c1 = c2 .
Player a is unsuccessfully capturing b at time t.
Player a is unsuccessfully freeing b at time t.
Player a is freeing b at time t.
Player a is in captured state at time t.
At time t, player a is in a state that follows
an unsuccessful attempt at capturing a.
a in this state has the same capabilities as when free.
At time t, player a is in a state that follows
an unsuccessful attempt at freeing a.
a in this state has the same capabilities as when captured.
Player a is in free state at time t
(isFree(a, t)   isCaptured(a, t)).
Player a in on enemy territory at time t.
Player a in on home territory at time t.
Players a and b are either snapped to a common cell
or to two adjacent cells at time t.
Player a is snapped to cell c at time t.

Table 1: Summary of the logical predicates our models use. Predicate names containing the word
failed are introduced by the Markov logic theory augmentation method described in
Section 5.2.1.

We compare our unified approach with four alternative models. The first two models (baseline
and baseline with states) are purely deterministic and they separate the denoising of the GPS data
and the labeling of game events. We implemented both of them in Perl. They do not involve any
training phase. The third alternative model is a dynamic Bayesian network shown in Figure 4.
Finally, we have two models cast in Markov logic: the two-step ML model and the unified ML
model itself. The unified model handles the denoising and labeling in a joint fashion, whereas the
two-step approach first performs snapping given the geometric constraints and subsequently labels
instances of capturing and freeing. The latter three models are evaluated using four-fold crossvalidation where in order to test on a given game, we first train a model on the other three games.
All of our models can access the following observed data: raw GPS position of each player at
any time and indication whether they are on enemy or home territory, location of each 3 by 3 meter
cell, cell adjacency, and list of pairs of players that are enemies. We tested all five models on the
same observed data. The following describes each model in more detail.
 Baseline Model (B)
This model has two separate stages. First we snap each reading to the nearest cell and afterward we label the instances of player a capturing player b. The labeling rule is simple:
98

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

we loop over the whole discretized (via snapping) data set and output capturing(a, b, t) every
time we encounter a pair of players a and b such that they were snapped (in the first step) to
either the same cell or to two mutually adjacent cells at time t, they are enemies, and a is on
its home territory while b is not. Freeing recognition is not considered in this simple model
since we need to have a notion of persisting player states (captured or free) in order to model
freeing in a meaningful way.
 Baseline Model with States (B+S)
This second model builds on top of the previous one by introducing a notion that players have states. If player a captures player b at time t, b enters a captured state (in logic,
isCaptured(b, t + 1)). Then b remains in captured state until he moves (is snapped to a different cell at a later time) or the game ends. As per rules of CTF, a player who is in captured
state cannot be captured again.
Thus, this model works just like the previous one except whenever it is about to label a
capturing event, it checks the states of the involved players and outputs capturing(a, b, t) only
if both a and b are not in captured state.
Freeing recognition is implemented in an analogous way to capturing recognition. Namely,
every time a captured player b is about to transition to a free state, we check if b has a
free teammate a nearby (again, within the adjacent cells). If that is the case, we output
freeing(a, b, t).
 Dynamic Bayesian Network Model (DBN)
The dynamic Bayesian network model can be viewed as a probabilistic generalization of the
above baseline model with states. The structure of the DBN model for one player is shown
in Figure 4. In each time slice, we have one hidden node and four observed nodes, all of
which represent binary random variables. We want to infer the most likely state S for each
player at any given time t over the course of a game. The state is either free or captured and
is hidden at testing time. There are four observed random variables per time step that model
players motion (M ), presence or absence of at least one enemy (EN ) and ally (AN ) player
nearby, and finally players location on either home or enemy territory (ET ). Each player is
modeled by a separate DBN. Therefore, there are fourteen instantiated DBNs for each game,
but within any one game, all the DBNs share the same set of parameters.
Note that the DBN model does not perform any GPS trajectory denoising itself. To make a fair
comparison with the Markov logic models, we use the denoising component of the Markov
logic theory using only constraints H1 and S1S3 (in Figure 3). This produces a denoised
discretization of the data that is subsequently fed into the DBN model. The random variables
within the DBN that capture the notion of player movement and players being nearby one
another is defined on the occupancy grid of the game area, just like in the two deterministic
baseline models. Namely, a player is said to be moving between time t and t + 1 when he
or she is snapped to two different nonadjacent cells at those times. Similarly, two players are
nearby if they are snapped either to the same cell or to two adjacent cells.
 Two-Step ML Model (2SML)
In the two-step approach, we have two separate theories in Markov logic. The first theory
is used to perform a preliminary snapping of each of the player trajectories individually us99

fiS ADILEK & K AUTZ

ETt

...

ENt

ANt

ETt+1

ENt+1

St

St+1

Mt

Mt+1

ANt+1

...

Figure 4: Two consecutive time slices of our dynamic Bayesian network for modeling the state
of an individual player P from observations. Shaded nodes represent observed random
variables, unfilled denote hidden variables. All random variables are binary. (ETt = 1
when P is on enemy territory at time t, ENt = 1 when there is an enemy nearby at time
t, ANt = 1 when there is an ally nearby at time t, and finally Mt = 1 if P has moved
between time t  1 and t. The value of hidden state St is 1 if P is captured at time t and
0 when P is free.)

ing constraints H1 and S1S3 (in Figure 3). This theory is identical to the one used in the
discretization step in the DBN model above.
The second theory then takes this preliminary denoising as a list of observed atoms in the
form preliminarySnap(a, c, t) (meaning player a is snapped to cell c at time t) and uses the
remaining constraints to label instances of capturing and freeing, while considering cell adjacency in the same manner as the previous three models. The two-step model constitutes a
decomposition of the unified model (see below) and overall contains virtually the same formulas, except 2SML operates with an observed preliminarySnap predicate, whereas the unified
model contains a hidden snap predicate instead. Thus we omit elaborating on it further here.
 Unified ML Model (UML)
In the unified approach, we express all the hard constraints H1H8 and soft constraints S1
S7 (Figure 3) in Markov logic as a single theory that jointly denoises the data and labels game
events. Selected interesting formulas are shown in Figure 6their labels correspond to the
listing in Figure 3. Note that formulas S1S3 contain real-valued functions d1 , d2 , and d3
respectively. d1 returns the distance between agent a and cell c at time t. Similarly, d2 returns
the dissimilarity of the two consecutive snapping vectors5 given agent as position at time t
and t + 1 and the location of the centers of two cells c1 and c2 . Finally, since people prefer to
move in straight lines, function d3 quantifies the lack of smoothness of any three consecutive
segments of the trajectory. Since wp , ws , and wt are all assigned negative values during
training, formulas S1S3 effectively softly enforce the corresponding geometric constraints.
5. The initial point of each snapping (projection) vector is a raw GPS reading and the terminal point is the center of the
cell we snap that reading to.

100

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

The presence of functions d1 through d3 renders formulas S1S3 hybrid formulas. This means
that at inference time, the instantiated logical part of each formula evaluates to either 1 (true)
or 0 (false), which is in turn multiplied by the product of the corresponding function value
and the formula weight.
We will see how we train, test, and evaluate these four models, and how they perform on the
multi-agent activity recognition task in Section 6. Next, we turn to our supervised learning method
for augmenting the unified ML model in order to recognize both successful and failed attempts at
multi-agent activities.
Hard formulas:
a, t c : snap(a, c, t)
0

(H1)
0

0

a, c, c , t : (snap(a, c, t)  c 6= c )  snap(a, c , t)
a1 , a2 , t : freeing(a1 , a2 , t)  samePlace(a1 , a2 , t)  isFree(a1 , t)

(H2)

enemies(a1 , a2 )  isCaptured(a2 , t)  isFree(a2 , t + 1)

onEnemyTer(a1 , t)  onEnemyTer(a2 , t)

a1 , a2 , t : capturing(a1 , a2 , t)  samePlace(a1 , a2 , t)  isFree(a1 , t)

(H3)

enemies(a1 , a2 )  isFree(a2 , t)  isCaptured(a2 , t + 1)

onHomeTer(a1 , t)  onEnemyTer(a2 , t)

a1 , a2 , t : samePlace(a1 , a2 , t)  c1 , c2 : snap(a1 , c1 , t)  snap(a2 , c2 , t)  adjacent(c1 , c2 )



a, t : (t = 0)  isFree(a, t)

(H4)

a, t : isCaptured(a, t)  isFree(a, t)

(H5)

a, t : (isFree(a, t)  isCaptured(a, t + 1))  (=1 a1 : capturing(a1 , a, t))

(H6)

a, t : (isCaptured(a, t)  isFree(a, t + 1))  (=1 a1 : freeing(a1 , a, t))

(H7)

a, t, c : (isCaptured(a, t)  isCaptured(a, t + 1)  snap(a, c, t))  snap(a, c, t + 1)

(H8)

Figure 5: Our hard formulas in Markov logic. See corresponding rules in Figure 3 for an English
description and Table 1 for explanation of the predicates. In our implementation, the
actual rules are written in the syntax used by theBeast, a Markov logic toolkit. (=1
denotes unique existential quantification,  designates exclusive or.)

5.2 Learning Models of Failed Attempts
In the work described above, we manually designed the structure of a Markov logic network that
models the capture the flag domain and allows us to jointly denoise the raw GPS data and recognize
101

fiS ADILEK & K AUTZ

Soft formulas:


a, c, t : snap(a, c, t)  d1 (a, c, t)  wp

(S1)



a,c1 , c2 , t : snap(a, c1 , t)  snap(a, c2 , t + 1)  d2 (a, c1 , c2 , t)  ws

(S2)



a,c1 , c2 , c3 , t : snap(a, c1 , t)  snap(a, c2 , t + 1)  snap(a, c3 , t + 2)  d3 (a, c1 , c2 , c3 , t)  wt
a1 , a2 , t : [(enemies(a1 , a2 )  onHomeTer(a1 , t)

(S3)
(S4)

onEnemyTer(a2 , t)  isFree(a2 , t)
samePlace(a1 , a2 , t))  capturing(a1 , a2 , t)]  wc
a1 , a2 , t : [(enemies(a1 , a2 )  onEnemyTer(a1 , t)

(S5)

onEnemyTer(a2 , t)  samePlace(a1 , a2 , t)  isFree(a1 , t)
 isCaptured(a2 , t))  freeing(a1 , a2 , t)]  wf


a, c, t : capturing(a, c, t)  wcb

(S6)

a, c, t : [freeing(a, c, t)]  wf b

(S7)

Figure 6: Soft formulas in Markov logic. See corresponding rules in Figure 3 for an English description. Each soft formula
is written

 as a traditional quantified finite first-order logic
formula (e.g., a, c, t : snap(a, c, t) ), followed by an optional function (e.g., d1 (a, c, t)),
followed by the weight of the formula (e.g., wp ). This syntax denotes that at inference
time, the instantiated logical part of each formula evaluates to either 1 (true) or 0 (false),
which is then effectively multiplied by the product of corresponding function value and
formula weight.

instances of actual capturing and freeing. Now we show how to automaticallyin a supervised
learning settingextend this theory to encompass and correctly label not only successful actions,
but also failed attempts at those interactions. That is, given the raw GPS data that represent the
CTF games, we want our new model to label instances where player a captures (or frees) player
b as successful captures (successful frees) and instances where player a almost captures (or frees)
player b as failed captures (failed frees). For example, by failed capturing we mean an instance of
players interactions whereup to a pointit appeared that a is capturing b, but when we carefully
consider the events that (potentially) preceded it as well as the impacts of the supposed capture
on the future unfolding of the game, we conclude that it is a false alarm and no capture actually
occurred. In other words, the conditions for a capture were right, but later on, there was a pivotal
moment that foiled the capturing agents attempt.
For both activities (capturing and freeing), our model jointly finds an optimal separation between success and failure. Note that since we cast our model in second-order Markov logic, we
do not learn, e.g., an isolated rule that separates successful freeing from a failed attempt at freeing.
Rathersince capturing and freeing events (both actual and failed) are related and thus labeling
an activity as, say, successful capturing has far-reaching impact on our past, present, and future
102

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

labelingwe learn the separations in a joint and unified way. Namely, both the structure (logical
form) and importance (weight) of each formula in our theory is considered with all its consequences
and influence on other axioms in the theory. Our system thus finds an optimal balance between success and failure in capturing and freeing activities with respect to the training data.
5.2.1 T HE T HEORY AUGMENTATION A LGORITHM
In what follows, we will describe our Markov logic theory augmentation algorithm (Algorithm 1).
For clarity, we will explain how it works in concrete context of the ML models of capture the flag
we discussed in previous sections. However, the underlying assumption that successful actions are
in many ways similar to their failed counterparts, and that minorbut crucialdeviations cause the
failure to occur, often hold beyond capture the flag. Therefore, the same algorithm is applicable to
other domains with different activities, as long as they are modeled in Markov logic.
Algorithm 1 : Extend a ML theory to model successful as well as failed activities.
Input: A: set of activities
MS : ML theory that models successful instances of activities in A
S: set of examples of successful activities
F : set of examples of failed activities
Output: MS+F : augmented ML model with learned weights that models both successful and
attempted activities in A
I: intended goals of the activities
1:
2:
3:
4:
5:
6:
7:

M2S  liftToSecondOrderML(MS , A)
M0S  instantiate(M2S , A)
I  findIncompatibleFormulas(F , M0S )
MS+F  M0S \I
MS+F  learnWeights(S, F , MS+F )
MS+F  removeZeroWeightedFormulas(MS+F )
return MS+F , I

At a high-level, the augmentation algorithm belongs to the family of structure learning methods. Starting with a seed model of successful actions, it searches for new formulas that can be
added to the seed theory in order to jointly model both successfully and unsuccessfully carried out
actions. The declarative language biasessentially rules for exploring the hypothesis space of candidate structuresis defined implicitly by the notion that for any given activity, the structure of
unsuccessful attempts is similar to the successful attempts. Therefore, the augmentation algoritm
goes through an inflation stage, where formulas in the seed theory are generalized, followed by
a refinement stage, where superfluous and incompatible formulas in the inflated model are pruned
away. The refinement step also optimizes the weights within the newly induced theory. We will
now discuss this process in more detail.
The input of our theory augmentation algorithm consists of an initial first-order ML theory MS
that models successful capturing and freeing (such as the unified ML model defined in Section 5.1
that contains formulas shown in Figures 5 and 6), a set of activities of interest A, and a set of
examples of successful (S) as well as failed (F ) captures and frees. MS does not need to have
weights for its soft formulas specified. In case they are missing, we will learn them from scratch in

103

fiS ADILEK & K AUTZ

the final steps of the augmentation algorithm. If the weights are specified, the final weight learning
step for MS+F can leverage them to estimate the initial weight values. A can be specified as a
set of predicate names, e.g., {capturing, freeing}. Each example in sets S and F describes a game
segment and constitutes a truth assignment to the appropriate literals instantiated from MS . Table 2
shows two toy examples of sets S and F for three time steps. Since the goal is to learn a model
of failed (and successful) attempts in a supervised way, the example game segment in F contain
activities labeled with predicates failedCapturing() and failedFreeing().
If MS contains hybrid formulas (such our formulas S1S3 in Figure 6), the appropriate function
definitions are provided as part of S and F as well. Each definition consists of implicit mapping
from input arguments to function values. For instance, function d1 in formula S1 quantifies the L2
distance
between the agent a and cell c at time t in the projected Mercator space: d1 (a, c, t) =
p
(a.gpsXt  c.gpsX)2 + (a.gpsYt  c.gpsY )2 .
Our system goes through the following process in order to induce a new theory MS+F that
augments MS with a definition of failed attempts for each activity already defined in MS .
First we lift MS to second-order Markov logic by variabilizing all predicates that correspond
to the activities of interest (step 1 of Algorithm 1). This yields a lifted theory M2S . More concretely, in order to apply this technique in our domain, we introduce new predicate variables captureType (whose domain is {capturing, failedCapturing}), freeType (over {freeing, failedFreeing}),
and stateType (over {isCaptured, isFailedCaptured, isFree, isFailedFree}). For instance, variabilizing a first-order ML formula freeing(a, b, t)  enemies(a, b) yields a second-order ML formula
freeType(a, b, t)  enemies(a, b) (note that freeType is now a variable). Instantiating back to
first-order yields two formulas: freeing(a, b, t)  enemies(a, b) and failedFreeing(a, b, t) 
enemies(a, b).
As far as agents behavior is concerned, in the CTF domain, isCaptured is equivalent to isFailedFree, and isFree is equivalent to isFailedCaptured. As we will soon see, the theory augmentation
process learns these equivalence classes and other relationships between states from training examples by expanding and subsequently refining formula H5 in Figure 5. While we could work with
only the isCaptured predicate and its negation to represent agents states, we feel that having explicit failure states makes our discussion clearer. Furthermore, future work will need to address
hierarchies of activities, including their failures. In that context, a representation of explicit failure
states may not only be convenient, but may be necessary.
Next, we instantiate all predicate variables in M2S to produce a new first-order ML theory M0S
that contains the original theory MS in its entirety plus new formulas that correspond to failed captures and frees (step 2). Since events that are, e.g., near-captures appear similar to actual successful
captures, our hypothesis is that we do not need to drastically modify the original successful formulas in order to model the failed activities as well. In practice, the above process of lifting and
instantiating indeed results in a good seed theory. While we could emulate the lifting and grounding
steps with a scheme of copying formulas and renaming predicates in the duplicates appropriately,
we cast our approach in principled second-order Markov logic, which ties our work more closely to
previous research and results in a more extensible framework. Specifically, second-order Markov
logic has been successfully used in deep transfer learning (Davis & Domingos, 2009) and predicate invention (Kok & Domingos, 2007). Therefore, an interesting direction of future work is to
combine our theory augmentation and refinement with transfer and inductive learningoperating
in second-order MLto jointly induce models of failed attempts of different activities in different
domains, while starting with a single model of only successful activities in the source domain.
104

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Set S: Successful Capture
enemies(P1 , P2 )
enemies(P2 , P1 )
onEnemyTer(P2 , 2)
onEnemyTer(P2 , 3)
capturing(P1 , P2 , 2)
isFree(P1 , 1)
isFree(P1 , 2)
isFree(P1 , 3)
isFree(P2 , 1)
isFree(P2 , 2)
isCaptured(P2 , 3)
snap(P1 , C5, 1)
snap(P1 , C10, 2)
snap(P1 , C10, 3)
snap(P2 , C9, 1)
snap(P2 , C10, 2)
snap(P2 , C10, 3)
samePlace(P1 , P2 , 2)
samePlace(P2 , P1 , 2)
samePlace(P1 , P2 , 3)
samePlace(P2 , P1 , 3)

Set F: Failed Capture
enemies(P4 , P5 )
enemies(P5 , P4 )
onEnemyTer(P5 , 1)
onEnemyTer(P5 , 2)
onEnemyTer(P5 , 3)
failedCapturing(P4 , P5 , 2)
isFree(P4 , 1)
isFailedCaptured(P4 , 1)
isFree(P4 , 2)
isFailedCaptured(P4 , 2)
isFree(P4 , 3)
isFailedCaptured(P4 , 3)
isFree(P5 , 1)
isFailedCaptured(P5 , 1)
isFree(P5 , 2)
isFailedCaptured(P5 , 2)
isFree(P5 , 3)
isFailedCaptured(P5 , 3)
snap(P4 , C17, 1)
snap(P4 , C34, 2)
snap(P4 , C0, 3)
snap(P5 , C6, 1)
snap(P5 , C34, 2)
snap(P5 , C7, 3)
samePlace(P4 , P5 , 2)
samePlace(P5 , P4 , 2)

Table 2: Two examples of a logical representation of successful (S) as well as failed (F ) capture
events that are input to Algorithm 1. The closed-world assumption is applied, therefore
all atoms not listed are assumed to be false. For clarity, we omit listing the adjacent()
predicate.

Typical structure learning and inductive logic programming techniques start with an initial (perhaps empty) theory and iteratively grow and refine it in order to find a form that fits the training data
well. In order to avoid searching the generally huge space of hypotheses, a declarative bias is either
specified by hand or mined from the data. The declarative bias then restricts the set of possible refinements of the formulas that the search algorithm can apply. Common restrictions include limiting
formula length, and adding a new predicate to a formula only when it shares at least one variable
with some predicate already present in the formula. On the other hand, in our approach, we first
generate our seed theory by instantiating all the activity-related predicate variables. To put it into
105

fiS ADILEK & K AUTZ

context of structure learning, we expand the input model in order to generate a large seed theory,
and then apply bottom-up (data-driven) learning to prune the seed theory, whereby the training data
guides our search for formulas to remove as well as for an optimal set of weights on the remaining
formulas. We conjecture that any failed attempt at an activity always violates at least one constraint
that holds for successful executions of the activity. The experiments below support this conjecture.
The pruning is done in steps 3 and 4 of Algorithm 1. The function findIncompatibleFormulas(F ,
M0S ) returns a set of hard formulas in M0S that are incompatible with the set of examples of failed
interactions F . We say that a formula c is compatible with respect to a set of examples F if F
logically entails c (F |= c). Conversely, if F does not entail c, we say that c is incompatible w.r.t.
F . We explain how to find incompatible formulas in the next section.
In step 4 of Algorithm 1, we simply remove all incompatible formulas (I) from the theory. At
this point, we have our MS+F model, where hard formulas are guaranteed logically consistent with
the examples of failed activities (because we removed the incompatible hard formulas), as well as
with the successful activities (because they were logically consistent to start with). However, the
soft formulas in MS+F are missing properly updated weights (in Markov logic, the weight of each
hard formula is simply set to +). Therefore, we run Markov logic weight learning using theBeast
package (step 5).
Recall that theBeast implements the cutting plane meta solving scheme for inference in Markov
logic, where the ground ML network is reduced to an integer linear program that is subsequently
solved by the LpSolve ILP solver. We chose this approach as opposed to, e.g., MaxWalkSAT that
may find a solution that is merely locally optimal, since the resulting run times are still relatively
short (under an hour even for training and testing even the most complex model). Weights are
learned discriminatively, where we directly model the posterior conditional probability of the hidden predicates given the observed predicates. We set theBeast to optimize the weights of the soft
formulas via supervised on-line learning using margin infused relaxed algorithm (MIRA) for weight
updates while the loss function is computed from the number of false positives and false negatives
over the hidden atoms. Note that if any of the soft formulas are truly irrelevant with respect to the
training examples, they are not picked out by the findIncompatibleFormulas() function, but their
weights are set to zero (or very close to zero) in the weight learning step (line 5 in Algorithm 1).
These zero-weighted formulas are subsequently removed in the following step. Note that the weight
learning process does not need to experience a cold start, as an initial setting of weights can be
inherited from the input theory MS .
Finally, we return the learned theory MS+F , whose formulas are optimally weighted with respect to all training examples. In the Experiments and Results section below, we will use MS+F to
recognize both successful and failed activities. Algorithm 1 also returns the incompatible hard formulas I. We will see how I is used to extract the intended goal of the activities in the Section 5.3,
but first, let us discuss step 3 of Algorithm 1 in more detail.
5.2.2 C ONSISTENCY C HECK : F INDING I NCOMPATIBLE F ORMULAS
Now we turn to our method for finding incompatible formulas (summarized in Algorithm 2). Since
our method leverages satisfiability testing to determine consistency between candidate theories
and possible worlds (examples),6 Algorithm 2 can be viewed as an instance of learning from
interpretationsa learning setting in the inductive logic programming literature (De Raedt, 2008).
6. This is often referred to as the covers relation in inductive logic programming.

106

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Algorithm 2 (findIncompatibleFormulas). Find formulas in a ML theory that are logically inconsistent with examples of execution of failed activities.
Input: F : a set of examples of failed activities
T : unrefined ML theory of successful and failed activities
Output: smallest set of formulas that appear in T and are unsatisfiable in the worlds in F
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

O  extractObjects(F )
Thard  T \ Tsoft
integer n  0
boolean result  false
while result == false do
T c  Thard
remove a new n-tuple of formulas from T c
if for the current n, all n-tuples have been tested then
nn+1
end if
result  testSAT(F , T c , O)
end while
return Thard \ T c

As input, we take a set of examples of failed activities F and a seed theory T (e.g., produced
in step 2 of Algorithm 1). The output is the smallest set of hard formulas that appear in T and
are logically inconsistent with F . The algorithm first extracts the set of all objects O that appear
in F (step 1 in Algorithm 2), while keeping track of the type of each object. For example, suppose there are only two example worlds in F shown in Table 3. Then extractObjects(F ) returns
{P1 , P2 , P7 , P8 , C3 , C5 , 1, 2}.
Example 1
snap(P1 , C5 , 1)
snap(P2 , C5 , 1)
failedCapturing(P1 , P2 , 1)

Example 2
snap(P7 , C3 , 2)
snap(P8 , C3 , 2)
failedFreeing(P2 , P5 , 2)

Table 3: Two simple examples of a logical representation a failed capture event.

In step 2, we limit ourselves to only hard formulas when testing compatibility. We do so since
we can prove incompatibility only for hard formulas. Soft constraints can be violated many times
in the data and yet we may not want to eliminate them. Instead, we want to merely adjust their
weights, which is exactly what we do in our approach. Therefore, Thard contains only hard formulas
that appear in T . Next, on lines 5 through 12, we check if the entire unmodified Thard is compatible
(since for n = 0, we do not remove any formulas). If it is compatible, we return an empty set
indicating that all the hard formulas in the original seed theory T are compatible with the examples.
If we detect incompatibility, we will need to remove some, and perhaps even all, hard formulas in
order to arrive at a logically consistent theory. Therefore, we incrementally start removing n-tuples
of formulas. That is, in the subsequent |Thard | iterations of the while loop, we determine if we can
107

fiS ADILEK & K AUTZ

restore consistency by removing any one of the hard formulas in Thard . If we can, we return the
set Thard \ fi , where fi is the identified and removed incompatible formula. If consistency cannot
be restored by removing a single formula, we in turn begin considering pairs of formulas (n = 2),
triples (n = 3), etc. until we find a pruned theory T c that is consistent with all examples.
In general, we do need to consider n-tuples of formulas, rather than testing each formula in
isolation. This is due to disjunctive formulas in conjunction with an possibly incomplete truth
assignment in the training data. Consider the following theory in propositional logic:
f1 = a  b
f2 = b  c
Data: a  c
(Following the closed-world assumption, the negated atom c would actually not appear in the training data, but we explicitly include it in this example for clarity.) While f1 and f2 are each individually consistent with the data, f1  f2 is inconsistent with the data. More complicated examples
can be constructed, where every group of k formulas is inconsistent with the data, even though the
individual formulas are. In a special case where the truth values of all atoms in the training examples are known, the formulas can be tested for consistency individually, which reduces the original
exponential number of iterations Algorithm 2 executes, in the worst case, to a linear complexity.
An interesting direction for future work is to explore applications of logical methods to lower the
computational cost for the general case of partially observed data.
We also note that some hard formulas model physical constraints or inviolable rules of capture
the flag, and therefore hold universally. Appropriately, these formulas are not eliminated by Algorithm 2. As an example, consider formula H1 in Figure 5, which asserts that each player occupies
exactly one cell at any given time. This formula is satisfied in games that include both successful and failed activities. On the other hand, consider formula H8 in the same figure. It contains a
captured player to the cell he was captured in (following the captured players cannot move rule
of CTF). While this holds for successful capturing events, it does not necessarily hold for failed
attempts at capturing. Therefore, when rule H8 is expanded via second-order ML, only some of the
derived formulas are going to be consistent with the observations.
Specifically, the candidate formula in Equation 6 will be pruned away, as it is inconsistent with
the training examples, i.e., players that were only nearly captured continue to be free to move about.
However, the remaining three variants of formula H8 will not be pruned away. Equation 7 will
always evaluate to true, since if someone attempts to re-capture an already captured player a, a does
indeed remain stationary. Similarly, Equation 8 is also consistent with all the example CTF games
because if there is a failed attempt at capture immediately followed by a successful capture, the
captured player does remain in place from time t onward. Finally, Equation 9 is compatible as well,
since it is the original formula H8 that is consistent with the observations.

a, t, c : isFailedCaptured(a, t)  isFailedCaptured(a, t + 1)  snap(a, c, t)  snap(a, c, t + 1)
(6)

a, t, c : isCaptured(a, t)  isFailedCaptured(a, t + 1)  snap(a, c, t)  snap(a, c, t + 1) (7)

108

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR


a, t, c : isFailedCaptured(a, t)  isCaptured(a, t + 1)  snap(a, c, t)  snap(a, c, t + 1) (8)


a, t, c : isCaptured(a, t)  isCaptured(a, t + 1)  snap(a, c, t)  snap(a, c, t + 1)

(9)

The function testSAT() (line 11 in Algorithm 2) checks whether a given candidate theory T c is
compatible with the examples F by the following process. First, we ground T c using the objects in
O, thereby creating a ground theory G. For example, if T c = {p(x)  q(x)} and O = {B, W },
the grounding would be G = {p(B)  q(B), p(W )  q(W )}. Then we check if G  Fhidden is
satisfiable using the miniSAT solver, where Fhidden is simply the set of hidden atoms that appear in
F . Intuitively, this corresponds to testing whether we can plug in the worlds in F into T c while
satisfying all the hard constraints. Though satisfiability is an NP-complete problem, in practice
testSAT() completes within tenths of a second even for the largest problems in our CTF domain.
For instance, suppose Fhidden = {p(B), q(B)}. Then we test satisfiability of the formula

 

p(B)  q(B)  p(W )  q(W )  p(B)  q(B).
In this case we cannot satisfy it since we are forced to set p(B) to true and q(B) to false, which
renders the first clauseand therefore the whole formulafalse.
An alternative approach to pruning formulas via satisfiability testing, as we have just described,
would be to treat both types of formulas (hard and soft) in the inflated theory M0S as strictly soft
formulas and learning a weight for each formula from examples of both successful and failed game
events. However, this introduces several complications that negatively impact the systems performance as well as model clarity. First, the number of formulas in the inflated theory can be
exponentially larger than in the seed theory. While the instantiation of the second-order ML representation can be quantified to limit this expansion, we still have worst-case exponential blow-up.
By treating all formulas as soft ones, we now need to potentially learn many more weights. This is
especially problematic for activities that occur rarely, as we may not have enough training data to
properly learn those weights. Eliminating the hard candidate formulas by proving them inconsistent
dramatically reduces the number of parameters we have to model. While satisfiability testing is
NP-complete, weight learning in Markov logic entails running inference multiple times, which is
itself a #P-complete problem.
The second reason for distinguishing between soft and hard formulas is the resulting clarity and
elegance of the final learned model MS+F . Even in situations when we have enough training data
to properly learn a large number of weights, we run into overfitting problems, where neither the
structure nor the parameters of the model represent the domain in a natural way. Our experiments
have shown that if we skip the pruning stage (steps 3 and 4 in Algorithm 1), the models recognition
performance does not differ from that of a pruned model in a significant way (p-value of 0.45).
However, we end up with a large number of soft formulas with a mixture of positive and negative
weights that the learning algorithm carefully tuned and balanced to fit the training data. They
however bear little relationship to the concepts in the underlying domain. Not only does this make
it very hard for a human expert to analyze the model, but it makes it even harder to modify the
model.
109

fiS ADILEK & K AUTZ

For these reasons, softening all hard formulas is, in general, infeasible. An interesting direction
of future work will be to identify a small amount of key inconsistent hard formulas to soften, while
eliminating the rest of the inconsistent hard formulas. This however entails searching in a large
space of candidate subsets of softened formulas, where each iteration requires expensive re-learning
of all weights.
Note that Algorithm 2 terminates as soon as it finds a compatible theory that requires the smallest
number of formula-removals. We also experimented with an active learning component to our
system, where we modify Algorithms 1 and 2 such that they present several possible refinements
of the theory to the user who then selects the one that looks best. The proposed modifications are
shown both at the ML theory level with modified sections (formulas) highlighted as well as at the
data level where the program shows the inferred consequences of those modifications. For each
candidate modification, the corresponding consequences are displayed as a collection of animations
where each animation shows what the results of activity recognition would be if we committed to
that particular candidate theory. Note that even people who do not have background in ML can
interact with such a system since the visualization is easy to understand. Interestingly, in the case
of captures and frees, the least modified theory that the off-line version of the algorithm finds is
also the best one and therefore there is no need to query the user. One can view this as a differential
variant of Occams razor. However, for different activities or other domains, the active learning
approach may be worth revisiting and we leave its exploration for future work.
Finally, general structure learning techniques from statistical-relational AI and from inductive
logic programming are not applicable as a substitute for our theory augmentation algorithm for
several reasons. The main reason is that, for efficiency reasons, existing techniques in the literature
typically operate over a very restricted set of formula templates. That is, they consider only Horn
clauses, or only formulas without an existential quantifier, or only formulas with at most k literals or
with at most l variables, and so on. This set of restrictions is part of the language bias of any given
approach. While in principle, structure learning is possible without a language bias, one often has
to carefully define one for the sake of tractability (see the Section 7 for details). In our approach,
the language bias is defined implicitly as discussed in Section 5.2.1.
5.3 Extracting The Goal From Success and Failure
Recall that applying the theory augmentation process (Algorithm 1) on the CTF seed theory of
successful interactions (shown in Figures 5 and 6) induces a new set of formulas that capture the
structure of failed activities and ties them together with the existing formulas in the seed theory.
The logically inconsistent formulas I that Algorithm 2 returns are ones that are not satisfiable in
the worlds with failed activities. At the same time, variants of those formulas were consistent with
the examples of successful actions occurring in the games. Therefore, I represents the difference
between a theory that models only successful activities and the augmented theory of both successful
and failed actions, that has been derived from it. Intuitively, the difference between success and
failure can be viewed as the intended purpose of any given activity a rational agent executes, and
consequently as the goal the agent has in mind when he engages in that particular activity. In the
next section, we will explore the goals extracted from the CTF domain in this fashion.
This concludes discussion of our models and methodology, and now we turn to experimental
evaluation of the framework presented above.

110

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

6. Experiments and Results
We evaluate our approach along the three major directions outlined in Section 5 (Methodology),
while focusing on answering the four research questions formulated ibidem. The structure of this
section closely follows that of the Methodology section.
In a nutshell, we are first interested in how our Markov logic models perform on the standard
multi-agent activity recognition tasklabeling successful activitiesand how their performance
compares to the alternative models. Second, we examine the augmented model that captures both
successful and failed attempts at activities. This is the model MS+F induced by Algorithm 1, which
also lets us extract the intended goal of the activities in question. Third, we compare the performance
of MS+F on the task of jointly recognizing all four activities with that of an alternative model.
Finally, we investigate to what extent the reasoning about failed attempts does help in recognition
of successfully executed activities.
All experiments are performed on our capture the flag dataset consisting of four separate games.
The dataset is summarized in Table 4, where for each game we list the number of raw GPS readings
and the number of instances of each activity of interest. We evaluate the models via four-fold crossvalidation, always training on three games (if training is required for a model) and testing against the
fourth. For each experimental condition below, we report precision, recall, and F1 scores attained
by each respective model over the four cross-validation runs. We have purposefully chosen to
split the data so that each cross-validation fold directly corresponds to a separate game of CTF for
conceptual convenience and clarity. As we discussed above, the events occurring in the games often
have far-reaching consequences. For example, most captured players are never freed by their allies.
Therefore, a capture at the beginning of a game typically profoundly influences the entire rest of the
game. For this reason, splitting the games randomly or even manually would introduce unnecessary
complications, as most of the segments would have dependencies on other segments. By enforcing
that each fold exactly corresponds with a different game, we make each fold self-contained.
To quantify the statistical significance of the pair-wise differences between models, we use a
generalized probabilistic interpretation of F1 score (Goutte & Gaussier, 2005). Namely, we express
F1 scores in terms of gamma variates derived from models true positives, false positives, and false
negatives ( = 0.5, h = 1.0, cf., Goutte & Gaussier, 2005). This approach makes it possible to
compare our results to future work that may apply alternative models on similar, but not identical,
datasets. A future comparison may, for instance, include additional games or introduce random
splits of the data. We note that standard statistical significance tests cannot be applied in those situations. All p-values reported are one sided, as we are interested if models performance significantly
improves as their level of sophistication increases.
6.1 Recognition of Successful Activities
Recall that for both our two-step (2SML) and unified (UML) Markov logic models, we specify the
Markov logic formulas by hand and optimize the weights of the soft formulas via supervised online learning. We run a modified version of theBeast software package to perform weight learning
and MAP inference. theBeast implements the cutting plane meta solving scheme for inference in
Markov logic, where the ground ML network is reduced to an integer linear program that is subsequently solved by the LpSolve ILP solver. We chose this approach as opposed to, e.g., MaxWalkSAT
that can get stuck at a local optimum, since the resulting run times are still relatively short (under
an hour even for training and testing even the most complex model).
111

fiS ADILEK & K AUTZ

Game 1
Game 2
Game 3
Game 4
Total

#GPS
13,412
14,420
3,472
10,850
42,154

#AC
2
2
6
3
13

#FC
15
34
12
4
65

#AF
2
2
0
1
5

#FF
1
1
2
0
4

Table 4: CTF dataset overview: #GPS is the total number of raw GPS readings, #AC and #FC is the
number actual (successful) and failed captures respectively, and analogously for freeings
(#AF and #FF).

At weight learning time, we use the margin infused relaxed algorithm (MIRA) for weight updates while the loss function is computed from the number of false positives and false negatives
over the hidden atoms, as described in the Methodology section. The discretization step for the
dynamic Bayesian network model (DBN) is implemented in Markov logic and is also executed in
this fashion. The DBN model is trained via maximum likelihood as described in Section 4.2. The
two deterministic baselines (B and B+S) do not require any training phase.
At inference time, we are interested in the most likely explanation of the data. In Markov logic,
maximum a posteriori inference reduces to finding a complete truth assignment that satisfies all the
hard constraints while maximizing the sum of the weights of the satisfied soft formulas. At testing
time, theBeast Markov logic solver finds the most likely truth assignment to the hidden atoms as
described above, and in this section we are specifically interested in the values of the capturing and
freeing atoms.
In DBNs, the most likely explanation of the observations is equivalent to Viterbi decoding. The
DBN model assigns either free or captured state to each player for every time step. We then label
all transitions from free to captured state as capturing and all transitions from captured to free as
freeing. Note that the DBN model is capable of determining which player is being freed or captured,
but it does not model which player does the freeing or capturing. In our evaluation, we give it the
benefit of the doubt and assume it always outputs the correct actor.
For all models, inference is done simultaneously over an entire game (on average, about 10
minutes worth of data). Note that we do not restrict inference to a (small) sliding time window. As
the experiments described below show, many events in this domain can only be definitely recognized
long after they occur. For example, GPS noise may make it impossible to determine whether a player
has been captured at the moment of encounter with an enemy, but as the player thereafter remains
in place for a long time, the possibility of his capture becomes certain.
Figures 7 and 8 summarize the performance of our models of successful capturing and freeing
in terms of precision, recall, and F1 score calculated over the four cross-validation runs. For clarity,
we present the results in two separate plots, but each model was jointly labeling both capturing and
freeing activities. We do not consider the baseline model for freeing recognition as that activity
makes little sense without having a notion of player state (captured or free).
We see that the unified approach yields the best results for both activities. Let us focus on
capturing first (Figure 7). Overall, the unified model labels 11 out of 13 captures correctlythere

112

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Capturing	 Recogni/on	 
1.00	 

1.00	 
0.80	 

0.69	 

0.69	 

0.77	 

0.87	 
0.77	 

1.00	 

0.92	 
0.85	 

0.60	 

Precision	 

0.40	 
0.20	 
0.00	 

Recall	 

0.26	 
0.16	 
0.01	  0.02	 

0.03	  0.06	 

B	 

B+S	 

F1	 

DBN	 

2SML	 

UML	 

Figure 7: Comparison of performance of the five models on capturing recognition while doing joint
inference over both capturing and freeing events. See Table 5 for statistical significance
analysis of the pairwise differences between models. (B = baseline model, B+S = baseline
model with states, 2SML = two-step Markov logic model, UML = unified Markov logic
model)

are only two false negatives. In fact, these two capture events are missed by all the models because
they involve two enemies that appear unusually far apart (about 12 meters) in the raw data. Even the
unified approach fails on this instance since the cost of adjusting the players trajectoriesthereby
losing score due to violation of the geometry-based constraintsis not compensated for by the
potential gain from labeling an additional capture.
Note that even the two-step approach recognizes 10 out of 13 captures. As compared to the
unified model, it misses one additional instance in which the involved players, being moderately
far apart, are snapped to mutually nonadjacent cells. On the other hand, the unified model does not
fail in this situation because it is not limited by prior nonrelational snapping to a few nearby cells.
However, the difference between their performance on our dataset is not statistically significant even
at the 0.05 level (p-value of 0.32).
Both deterministic baseline models (B and B+S) perform very poorly. Although they yield a
respectable recall, they produce an overwhelming amount of false positives. This shows that even
relatively comprehensive pattern matching does not work at all in this domain. Interestingly, the
performance of the DBN model leaves much to be desired as well, especially in terms of precision.
While the DBN model is significantly better than both baselines (p-value less than 5.9  105 ), it
also achieves significantly worse performance than both the Markov logic models (p-value less than
0.0002; see Table 5).
Table 5 summarizes p-values of pairwise differences between models of actual (i.e., successful)
capturing. While the difference between the Markov logic-based models (2SML and UML) are not

113

fiS ADILEK & K AUTZ

Freeing	 Recogni+on	 
1.00	 

1.00	 

1.00	 

0.80	 

0.75	 
0.57	 

0.60	 
0.40	 

0.40	 
0.20	 

0.20	 
0.15	 
0.13	 

0.22	 

0.60	 

Precision	 

0.40	 

Recall	 

0.29	 

F1	 

0.00	 
B+S	 

DBN	 

2-SML	 

UML	 

Figure 8: Comparison of performance of our three models on freeing recognition while doing joint
inference over both capturing and freeing events. See Table 6 for statistical significance
analysis of the pairwise differences between models. (B+S = baseline model with states,
2SML = two-step Markov logic model, UML = unified Markov logic model)

B
B+S
DBN
2SML

B+S
0.0192
-

DBN
3.6  106
5.9  105
-

2SML
5.1  107
9.4  106
0.0002
-

UML
2.9  107
1.4  106
8.0  105
0.3230

Table 5: Summary of statistical significance (one sided p-values) of the pairwise differences between F1 scores for models of actual capturing. (B = baseline model, B+S = baseline
model with states, DBN = dynamic Bayesian network model, 2SML = two-step Markov
logic model, UML = unified Markov logic model)

statistically significant (p-value of 0.32), pairwise differences in F1 scores between all other models
are significant at the 0.02 level, and most often even at much lower p-values.
Though the unified model still outperforms its alternatives in the case of freeing recognition as
well, its performance is further from ideal as compared to the capture recognition case (Figure 8).
It correctly identifies only 3 out of 5 freeing events in the games, but does not produce any false
positives. This is partly due to the dependency of freeing on capturing. A failure of a model to
recognize a capture precludes its recognition of a future freeing. Another reason is the extreme
sparseness of the freeing events (there are only five of them in 40,000+ datapoints). Finally, in some

114

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

B+S
DBN
2SML

DBN
0.2739
-

2SML
0.0733
0.1672
-

UML
0.0162
0.0497
0.2743

Table 6: Summary of statistical significance (one sided p-values) of the pairwise differences between F1 scores for models of actual freeing. (B+S = baseline model with states, DBN =
dynamic Bayesian network model, 2SML = two-step Markov logic model, UML = unified
Markov logic model)

instances players barely move after they had been freed. This may occur for a number of reasons
ranging from already occupying a strategic spot to simply being tired. Such freeing instances are
very challenging for any automated system, and even people familiar with the game to recognize
(several situations would have been extremely hard to disambiguate if we didnt have access to our
notes about data collection).
The two-step ML model does a slightly worse job than the unified model on freeing recognition.
It correctly identifies only 2 out of 5 freeings for the same reasons as in the capturing recognition
case. Similarly to models of actual captures, the difference between the unified and two-step freeing
models is not statistically significant (p-value of 0.27).
Table 6 summarizes p-values of pairwise differences between models of actual (i.e., successful) freeing. Here we see that only the difference between B+S and UML models is statistically
significant (p-value of 0.01), whereas the differences between the rest of the model pairs are not
statistically significant. Since there are only five instances of successful freeing, the 2SML model
does not perform significantly better than the B+S model at the 0.05 significance level (p-value of
0.07). However, the UML model achieves better recognition results than even the DBN model with
high confidence (p-value less than 0.05). Therefore, we see that although the 2SML model strictly
dominates the non-Markov logic models when evaluated on capturing recognition, we need the full
power of the unified ML model to strictly outperform the nonrelational alternatives for freeing. This
suggests that as we move to more complex and more interdependent activities, relational and unified
modeling approaches will be winning by larger and larger margins.
Even though the statistical significance tests suggest that 2SML is likely to give similar results to
UML, it is important to note that 2SML, by design, precludes recognition of the activities in question
in certain situations. Namely, as our experiments demonstrate, when the players are snapped to cells
that are too far apart, the two-step model does not even consider those instances as candidates for
labeling, and inevitably fails at recognizing them. Therefore, one needs to look beyond the p-values
obtained when comparing the fully unified models to various alternatives.
As expected from the experiments with capturing recognition, both deterministic baseline models perform very poorly on freeing recognition as well. Not only do they produce an overwhelming
amount of false positives, they also fail to recognize most of the freeing events.
Thus, we see that the models cast in Markov logic perform significantly better than both of the
deterministic baseline models, and also better than the probabilistic, but nonrelational, DBN model.
We note that the DBN model has the potential to be quite powerful and similar DBNs have been
applied with great success in previous work on activity recognition from location data (Eagle &
115

fiS ADILEK & K AUTZ

Pentland, 2006; Liao, Patterson, Fox, & Kautz, 2007). It also has many similarities with the twostep ML model. They both share the same denoising and discretization step, and they both operate
on the same observed data. The key difference is that the DBN model considers players individually,
whereas the two-step ML model performs joint reasoning.
Looking at the actual CTF game data, we see several concrete examples of how this hurts DBNs
labeling accuracy. For instance, consider a situation where two allies had been captured near each
other. Performing inference about individual players in isolation allows the DBN model to infer that
the two players effectively free each other, even though in reality they are both captured and cannot
do so. This occurs because the DBN model is oblivious to the explicit states of ones teammates as
well as opponents. Since capturing and freeing are interdependent, the obliviousness of the DBN
model to the state of the actors negatively impacts its recognition performance for both activities.
The example we just gave illustrates one type of freeing false positives. The hallucinated freeings
create opportunities that often lead to false positives of captures, creating a vicious cycle. False
negatives of freeing (capturing) events often occur for players who the model incorrectly believes
have already been freed (captured) at a prior time.
Since the Markov logic based models are significantly betterwith a high level of confidence
than the alternatives that are not fully relational, the experiments above validate our hypothesis that
we need to exploit the rich relational and temporal structure of the domain in a probabilistic way
and at the same time affirmatively answer research question Q1 (Can we reliably recognize complex
multi-agent activities in the CTF dataset even in the presence of severe noise?). Namely, we show
that although relatively powerful probabilistic models are not sufficient to achieve high labeling
accuracy, we can gain significant improvements by formulating the recognition problem as learning
and inference in Markov logic networks.
Now we turn to the evaluation of our method of learning models of both success and failure in
peoples activities.
6.2 Learned Formulas and Intentions
Applying the theory augmentation process (Algorithm 1) on the CTF seed theory (shown in Figures 5 and 6) induces a new set of formulas that capture the structure of failed activities and ties
them together with the existing formulas in the theory. We call this model MS+F . Figure 9 shows
examples of new weighted formulas modeling failed freeing and capturing attempts that appear in
MS+F .
First, note that our system correctly carries over the basic preconditions of each activity (contrast
formulas S4 with S40 and S5 with S50 in Figures 6 and 9 respectively). This allows it to reliably
recognize both successful and failed actions instead of, e.g., merely labeling all events that at some
point in time appear to resemble a capture as near-capture. This re-use of preconditions directly
follows from the language bias of the theory augmentation algorithm.
Turning our attention to the learned hard formulas, we observe that the system correctly induced
equivalence classes of the states, and also derived their mutual exclusion relationships (H50 ). It
furthermore tied the new failure states to their corresponding instantaneous interactions (H60 and
H70 ).
Finally, the algorithm correctly discovers that the rule If a player is captured then he or she
must remain in the same location (H8, Figure 5) is the key distinction between a successful and
failed capture (since players who were not actually captured can still move). Therefore, it introduces

116

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

an appropriate rule for the failed captures (H80 , Figure 9) explicitly stating that failed capturing does
not confine the near-captured player to remain in stationary. An analogous process yields a fitting
separation between failed and successful freeings. Namely, our model learns that an unsuccessfully
freed player remains stationary. This learned difference between success and failure in players
actions directly corresponds to the goal of the activity and consequently the intent of rational actors. This difference is what our system outputs as the intended goal of capturing activity (and
analogously for freeing).
These experimental results provide an evidence for a resounding yes to both Q2 (Can models
of attempted activities be automatically learned by leveraging existing models of successfully performed actions?) and Q3 (Does modeling both success and failure allow us to infer the respective
goals of the activities?) within the CTF domain.
We note that instead of applying our automated theory augmentation method, a person could,
in principle, manually formulate a Markov logic theory of successful as well as failed activities
by observing the games. After all, this is how we designed the initial seed model of successful
events. However, this process is extremely time consuming, as one tends to omit encoding facts
that to us, humans, seem self-evident but need to be explicitly articulated for the machine (e.g., a
single person cannot be at ten different places at once, or that a player is either free or captured but
not both). It is also surprisingly easy to introduce errors in the theory, that are difficult to debug,
mostly because of the complex weight learning techniques involved. Therefore, we believe that the
theory augmentation method is a significant step forward in enhancing models capabilities while
requiring small amounts of human effort. As the complexity of domains and their models increases,
this advantage will gain larger and larger importance.
6.3 Recognition of Both Successful and Failed Activities
We now compare the performance of our model MS+F to an alternative (baseline) method that
labels all four activities in the following way. Similarly to the baseline with states model for successful interactions defined in Section 5.1, there are two separate stages. First we snap each GPS
reading to the nearest cell by applying only the geometric constraints (H1 and S1S3) of our theory, and afterward we label the instances of our activities. The following labeling rule is applied.
We loop over the whole discretized (via snapping) data set and look for instances where a pair of
players a and b were snapped (in the first step) to either the same cell or to two adjacent cells at
time t, they are enemies, b is not captured already, and a is on its home territory while b is not.
If b moves (is snapped to a different cell at a later time) without having an ally nearby, we output
failedCapturing(a,b,t), otherwise we output capturing(a,b,t). The labeling rule for freeing is defined analogously and all four events are tied together. We also tested a variant of the DBN model
introduced in Section 5.1 that has two additional hidden state values for node St : isFailedFree and
isFailedCaptured. However, the difference in the results obtained with this model was not statistically significant (p-value of 0.38), and therefore we focus on the conceptually more straightforward
baseline model described above.
Model MS+F is evaluated using four-fold cross-validation (always training on three games and
testing against the fourth). Figure 10 compares both models in terms of precision, recall, and F1
score. Note that all four activities are modeled jointly in both models. The F1 score of the augmented
model is significantly better than that of the baseline for all four target activities (p-value less than
1.3  104 ).

117

fiS ADILEK & K AUTZ

a1 , a2 , t : [(enemies(a1 , a2 )  onHomeTer(a1 , t)

(S40 )

onEnemyTer(a2 , t)  samePlace(a1 , a2 , t)  isFree(a1 , t)
 isFree(a2 , t))  failedCapturing(a1 , a2 , t)]  11.206
a1 , a2 , t : [(enemies(a1 , a2 )  onEnemyTer(a1 , t)

(S50 )

onEnemyTer(a2 , t)  samePlace(a1 , a2 , t)  isFree(a1 , t)
 isCaptured(a2 , t))  failedFreeing(a1 , a2 , t)]  1.483
a1 , a2 , t : [failedCapturing(a1 , a2 , t)]  (0.0001)

(S60 )

a1 , a2 , t : [failedFreeing(a1 , a2 , t)]  (0.002)

(S70 )

a, t : isFailedCaptured(a, t)  isFree(a, t)

(H50 )

a, t : isCaptured(a, t)  isFailedFree(a, t)
a, t : isFailedCaptured(a, t)  isFree(a, t)
a, t : isCaptured(a, t)  isFailedFree(a, t)
a, t : (isFree(a, t)  isFailedCaptured(a, t + 1))  (=1 a1 : failedCapturing(a1 , a, t))
a, t : (isCaptured(a, t)  isFailedFree(a, t + 1))  (=1 a1 : failedFreeing(a1 , a, t))

(H60 )
(H70 )

a, t, c : (isFailedCaptured(a, t)  isFailedCaptured(a, t + 1)  snap(a, c, t))  snap(a, c, t + 1)
(H80 )

Figure 9: Example formulas, learned by Algorithm 1, that model unsuccessful capturing and freeing events. The crucial intent recognition formula (H80 ) is highlighted in bold. Formulas
eliminated by Algorithm 2 are preceded by the  symbol, and are not included in the
induced model MS+F . The identity isCaptured(a, t) = isFree(a, t) is applied throughout refining to show the formulas in a more intuitive fashion. For concreteness sake, the
values of the learned weights here come from one cross-validation run (and are similar in
other runs).

We see that the baseline model has, in general, a respectable recall but it produces a large
number of false positives for all activities. The false positives stem from the fact that the algorithm
is greedy in that it typically labels a situation where several players appear close to each other
for certain period of time as a sequence of many captures and subsequent frees even though none
of them actually occurred. Model MS+F gives significantly better results because it takes full
advantage of the structure of the game in a probabilistic fashion. It has a similar over labeling
tendency only in the case of failed captures, where a single capture attempt is often labeled as
several consecutive attempts. While this hurts the precision score, it is not a significant deficiency,

118

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

0.15	 

Baseline	 

AC	 (13)	 

0.23	 

FC	 (65)	 

0.97	 

0.13	 
0.04	 

AF	 (5)	 

0.40	 

0.02	 
0.06	 

FF	 (4)	 
Augmented	 ML	 Model	 

0.46	 

0.09	 

F1	 

0.75	 

0.03	 

Recall	 
0.96	 
0.92	 
1.00	 

AC	 (13)	 
0.86	 

FC	 (65)	 

0.78	 

AF	 (5)	 

0.80	 

FF	 (4)	 

0.75	 

0	 

0.1	 

0.2	 

0.3	 

0.4	 

0.5	 

0.6	 

0.7	 

0.8	 

Precision	 

0.97	 

0.89	 
1.00	 
0.86	 
1.00	 

0.9	 

1	 

Figure 10: Performance of the baseline and augmented (MS+F ) models on joint recognition of
successful and failed capturing and freeing. The F1 score of the augmented model is
significantly better than that of the baseline for all four target activities (p-value less
than 1.3  104 ). AC = actual (successful) capturing, FC = failed capturing, AF =
actual freeing, FF = failed freeing.

as in practice, having a small number of short game segments labeled as possible near-captures is
useful as well.
We also note that even though the original model (UML) did not contain any information on
failed capturing nor failed freeing, the performance of MS+F is respectable even for those two
newly introduced activities. We only provided examples of game situations where those attempts
occur and the system augmented itself and subsequently labeled all four activities. Thus, we see
that we can indeed extend preexisting models in an automated fashion so that the unified model is
capable of recognizing not only individual activities, but also both success and failure in peoples
behavior.
6.4 The Effect of Modeling Failed Attempts on Recognition of Successful Activities
To address research question Q4 (Does modeling failed attempts of activities improve the performance on recognizing the activities themselves?), we want to see how much does the recognition
of attempted activities help in modeling the successful actions (the latter being the standard activity

119

fiS ADILEK & K AUTZ

Capturing	 

F1	 

0.92	 

Recall	 

0.85	 

Precision	 

+0.08	 

1.00	 

F1	 
Freeing	 

+0.04	 

0.75	 

Recall	 

+0.14	 

0.60	 

+0.20	 

Precision	 

1.00	 

0	 

0.1	 

0.2	 

0.3	 

0.4	 

Without	 Modeling	 Failure	 

0.5	 

0.6	 

0.7	 

0.8	 

0.9	 

1	 

With	 Modeling	 Failure	 

Figure 11: Considering unsuccessfully attempted activities strictly improves performance on standard activity recognition. Blue bars show scores obtained with the unified Markov logic
model that considers only successful activities (MS ). The red bars indicate the additive improvement provided by the augmented model that considers both successful and
failed activities (MS+F , the output of Algorithm 1). Each model labels its target activities jointly, we separate capturing and freeing in the plot for clarity. Precision has value
of 1 for both models. F1 scores obtained when explicitly modeling failed attempts are
not statistically different from F1 scores obtained without modeling attempts at a high
confidence level (p-value of 0.20). However, these results still show the importance of
reasoning about peoples attempts when recognizing their activities; see text for details.

recognition problem). Toward that end, we compare the Markov logic model MS that jointly labels
only successful capturing and freeing with model MS+F that jointly labels both successful and
failed attempts at both capturing and freeing (see Section 5.2.1 for a detailed description of the two
models). However, we evaluate them in terms of precision, recall, and F1 score only on successful
interactions, not all four types of activities.
Figure 11 summarizes the results. We see that when evaluated on actual capturing, MS+F
performs better than MS , and similarly for freeing. However, the difference in F1 scores between
a model that captures both attempted and successful activities (MS+F ) and a model of only successful activities (MS ) is not statistically significant (p-value of 0.20). This is partly because MS
already produces very solid results, leaving little room for improvement. Additionally, the CTF
dataset contains relatively few events of interest. In terms of labeling performance at testing time,
the difference between the two models is more than 11% (MS and MS+F recognize, respectively,
120

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

14 and 16 out of 18 successful activities correctly). Thus, we believe the trends shown in Figure 11
are promising and modeling attempted actions does improve recognition performance on both capturing and freeing, but evaluation on a dataset with a larger number of events is needed to show the
difference to be statistically significant at a higher confidence level. However, this does not mean
that recognizing attempts is unimportant. As we show above, our induced augmented model does
recognize failed (as well as successful) activities in the complex CTF domain with high accuracy,
and we argue this to be a significant contribution.
Finally, the comparison of MS and MS+F shows that applying our learning algorithm that augments a model with more recognition capabilities does not hurt model labeling performance. The
fact that binary classification problems are typically easier to solve than their multi-class counterparts has been well reported on in machine learning literature (Allwein, Schapire, & Singer, 2001).
Therefore, introducing new activities into a model, especially in an automated way, is likely to degrade its performance. Contrary to this intuition, our experiments show that MS+F is no worse than
MS on successful activity recognition (i.e., their intersection) with high confidence, even though
MS+F is clearly richer and more useful.

7. Related Work
In the world of single-agent location-based reasoning, the work of Bui (2003) presents and evaluates a system for probabilistic plan recognition cast as an abstract hidden Markov memory model.
Subsequently, the work of Liao et al. (2004) implements a system for denoising raw GPS traces and
simultaneously inferring individuals mode of transportation (car, bus, etc.) and their goal destination. They cast the problem as learning and inference in a dynamic Bayesian network and achieve
encouraging results. In a follow-up work, Liao et al. (2005) introduce a framework for locationbased activity recognition, which is implemented as efficient learning and inference in a relational
Markov network.
The work of Ashbrook and Starner (2003) focuses on inferring significant locations from raw
GPS logs via clustering. The transition probabilities between important places are subsequently
used for a number of user modeling tasks, including location prediction. The work of Eagle and
Pentland (2006) explores harnessing data collected on regular smart phones for modeling human
behavior. Specifically, they infer individuals general location from nearby cell towers and Bluetooth devices at various times of day. Applying a hidden Markov model (HMM), they show that
predicting if a person is at home, at work, or someplace else can be achieved with more than 90% accuracy. Similarly, the work of Eagle and Pentland (2009) extracts significant patterns and signatures
in peoples movement by applying eigenanalysis to smart phone logs.
The work of Hu, Pan, Zheng, Liu, and Yang (2008) concentrates on recognition of interleaving
and overlapping activities. They show that publicly available academic datasets contain a significant
number of instances of such activities, and formulate a conditional random field (CRF) model that
is capable of detecting them with high (more than 80%) accuracy. However, they focus solely on
single-agent household activities.
Peoples conversation has been the primary focus of multi-agent modeling effort (Barbuceanu
& Fox, 1995). In the fields of multi-agent activity recognition and studies of human behavior, researchers have either modeled conversation explicitly (e.g., Busetta, Serafini, Singh, & Zini, 2001),
or have leveraged peoples communication implicitly via call and location logs from mobile phones.
This data has been successfully used to infer social networks, user mobility patterns, model socially

121

fiS ADILEK & K AUTZ

significant locations and their dynamics, and others (Eagle & Pentland, 2006; Eagle, Pentland,
& Lazer, 2009). This is arguably an excellent stepping stone for full-fledged multi-agent activity
recognition since location is, at times, practically synonymous with ones activity (e.g., being at a
store often implies shopping) (Tang, Lin, Hong, Siewiorek, & Sadeh, 2010), and our social networks
have tremendous influence on our behavior (Pentland, 2008).
Additionally, a number of researchers in machine vision have worked on the problem of recognizing events in videos of sporting events, such as impressive recent work on learning models of
baseball plays (Gupta et al., 2009). Most work in that area has focused on recognizing individual
actions (e.g., catching and throwing), and the state of the art is just beginning to consider relational
actions (e.g., the ball is thrown from player A to player B). The computational challenges of dealing
with video data make it necessary to limit the time windows of a few seconds. By contrast, we
demonstrate in this work that many events in the capture the flag data can only be disambiguated
by considering arbitrarily long temporal sequences. In general, however, both our work and that
in machine vision rely upon similar probabilistic models, and there is already some evidence that
statistical-relational techniques similar to Markov logic can be used for activity recognition from
video (Biswas, Thrun, & Fujimura, 2007; Tran & Davis, 2008).
Looking beyond activity recognition, recent work on relational spacial reasoning includes an
attempt to locateusing spacial abductioncaches of weapons in Iraq based on information about
attacks in that area (Shakarian, Subrahmanian, & Spaino, 2009). Additionally, the work of Abowd
et al. (1997) presents a location- and context-aware system, Cyberguide, that helps people explore
and fully experience foreign locations. Other researchers explore an intelligent and nonintrusive
navigation system that takes advantage of predictions of traffic conditions along with a model of
users knowledge and competence (Horvitz et al., 2005). Finally, the work of Kamar and Horvitz
(2009) explore automatic generation of synergistic plans regarding sharing vehicles across multiple
commuters.
An interesting line of work in cognitive science focuses on intent and goal recognition in a probabilistic framework (Baker, Tenenbaum, & Saxe, 2006, 2007). Specifically, they cast goal inference
as inverse planning problem in Markov decision processes, where Bayesian inversion is used to estimate the posterior distribution over possible goals. Recent extensions of this work begin to consider
simulated multi-agent domains (Baker, Goodman, & Tenenbaum, 2008; Ullman, Baker, Macindoe,
Evans, Goodman, & Tenenbaum, 2010; Baker, Saxe, & Tenenbaum, 2011). Comparison of the
computational models against human judgement in synthetic domains shows a strong correlation
between peoples predicted and actual behavior. However, the computational challenges involved in
dealing with the underlying partially observable Markov decision processes are prohibitive in more
complex domains with large state spaces, such as ours.
The focus of our work is on a different aspect of reasoning about peoples goals. Rather than
inferring a distribution over possible, a priori known goals, we automatically induce the goals of
complex multi-agent activities themselves.
Other researchers have concentrated on modeling behavior of people and general agents as reinforcement learning problems in both single-agent and multi-agent settings. The work of Ma (2008)
proposes a system for household activity recognition cast as a single-agent Markov decision process
problem that is subsequently solved using a probabilistic model checker. Wilson and colleagues address the problem of learning agents roles in a multi-agent domain derived from a real-time strategy
computer game (Wilson, Fern, Ray, & Tadepalli, 2008; Wilson, Fern, & Tadepalli, 2010). Experiments in this synthetic domain show strongly encouraging results. While we do not perform role
122

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

learning ourselves, we anticipate that the work of Wilson et al. is going to play an important role
in learning hierarchies of peoples activities. In our capture the flag domain, one can imagine automatically identifying a particular player as, for example, a defender and subsequently leveraging
this information to model his or her behavior in a more personalized way.
The work of Hong (2001) concentrates on recognizing the goal of an agent in the course of her
activities in a deterministic, but relational setting. Interesting work on goal recognition has been
also applied to computer-aided monitoring of complex multi-agent systems, where relationships
between agents are leveraged to compensate for noise and sparse data (Kaminka, Tambe, Pynadath,
& Tambe, 2002). By contrast, in our work we focus on learning the respective goals of a given set
of multi-agent activities in a probabilistic setting. The knowledge is in turn leveraged to achieve a
stronger robustness of the other recognition tasks. Similarly to the approach of Hong, our system
does not need a supplied plan library either.
Our work also touches on anomaly detection since our system reasons about the failed attempts
of the players. Anomaly detection concerns itself with revealing segments of the data that in some
way violate our expectations. For an excellent survey of the subject, we refer the reader to the
results of Chandola, Banerjee, and Kumar (2009). In the realm of anomaly detection within peoples
activities, the work of Moore and Essa (2001) addresses the problem of error detection and recovery
card games that involve two players recorded on video. Their system models the domain with a
stochastic context-free grammar and achieves excellent results.
We note that recognizing a failed attempt at an activity is more fine-grained a problem than
anomaly detection. The failed event is not just anomalous in general.7 Rather, it is the specific
distinction between success and failure in human activities that we are interested in. And the distinction lies in the fact that an unsuccessful attempt does not yield a certain desired state whereas
a successful action does. This desired state is exactly what our approach extracts for each activity
in question. To our knowledge, there exists no prior work on explicit modeling and recognition of
attempted activities or on learning the intended purpose of an activity in a multi-agent setting.
One of the components of our contribution focuses on joint learning and inference across multiple tasks (capturing, freeing, and their respective attempted counterparts). This is in contrast with
the traditional pipeline learning architecture, where a system is decomposed into a series of modules and each module performs partial computation and passes the result on to the next stage. The
main benefits of this set-up are reduced computational complexity and often higher modularity.
However, since each stage is myopic, it may not take full advantage of dependencies and broader
patterns within the data. Additionally, even though errors introduced by each module may be small,
they can accumulate beyond tolerable levels as data passes through the pipeline.
An extensive body of work has shown that joint reasoning improves model performance in a
number of natural language processing and data mining tasks including information extraction (i.e.,
text segmentation coupled with entity resolution) (Poon & Domingos, 2007), co-reference resolution (Poon & Domingos, 2008), information extraction coupled with co-reference resolution (Wellner, McCallum, Peng, & Hay, 2004), temporal relation identification (Yoshikawa, Riedel, Asahara,
& Matsumoto, 2009; Ling & Weld, 2010), and record de-duplication (Domingos, 2004; Culotta
& McCallum, 2005). Similarly to our work, some of the above models are cast in Markov logic.
However, prior work uses sampling techniques to perform learning and inference, whereas we apply
7. A situation where a player in CTF moves through the campus at a speed of 100 mph and on her way passes an enemy
player is certainly anomalous (and probably caused by GPS sensor noise), but we do not want to say that it is a failed
attempt at capturing.

123

fiS ADILEK & K AUTZ

a reduction to integer linear programming. Interestingly, the work in Denis and Baldridge (2007)
jointly addresses the problems of anaphoricity and co-reference via a manual formulation of an
integer linear program.
Joint activity modeling has also been shown to yield better recognition accuracy, as compared to
pipeline baselines as well as baselines that make strong inter-activity independence assumptions.
The work of Wu, Lian, and Hsu (2007) performs joint learning and inference over concurrent singleagent activities using a factorial conditional random field model. Similarly, the work of Helaoui,
Niepert, and Stuckenschmidt (2010) models interleaved activities in Markov logic. They distinguish
between foreground and background activities and infer a time window in which each activity takes
place from RFID sensory data. By contrast, we focus on joint reasoning about multi-agent activities
and attempts in a fully relationaland arguably significantly more noisysetting.
The work of Manfredotti, Hamilton, and Zilles (2010) propose a hierarchical activity recognition
system formulated as learning and inference in relational dynamic Bayesian networks. Their model
jointly leverages observed interactions with individual objects in the domain and the relationships
between objects. Since their method outperforms a hidden Markov model by a significant margin, it
contributes additional experimental evidence that a relational decomposition of a domain improves
model quality.
The work of Landwehr, Gutmann, Thon, Philipose, and De Raedt (2007) casts single-agent
activity recognition as a relational transformation learning problem, building on transformationbased tagging from natural language processing. Their system induces a set of transformation rules
that are then used to infer activities from sensory data. Since the transformation rules are applied
adaptively, at each step, the system leverages not only observed data, but also currently assigned
labels (inferred activities). However, the transformation rules are learned in a greedy fashion and
experiments show that the model does not perform significantly better than a simple HMM. On
the other hand, their representation is quite general, intuitive, and extensible. As we will see, our
Markov logic model has a similar level of representational convenience while performing global
instead of greedyoptimization in a significantly more complex domain.
The denoising component of our model can be formulated as a tracking problem. Prior work
proposed a relational dynamic Bayesian network model for multi-agent tracking (Manfredotti &
Messina, 2009). Their evaluation shows that considering relationships between tracked entities
significantly improves model performance, as compared to a nonrelational particle filter baseline.
By contrast, our work explores joint tracking and activity recognition. However, each GPS reading
is annotated with the identity of the corresponding agent. The work of Manfredotti and Messina
suggests that our model can be generalized, such that the associations between GPS and agent
identities are inferred and need not be observed.
Our Markov logic theory can be viewed as a template for a conditional random field (Lafferty,
2001), an undirected graphical model that captures the conditional probability of hidden labels
given observations, rather than the joint probability of both labels and observations, as one would
typically do in a directed graphical model. In the relational world, directed formalisms include
relational Bayesian networks (Jaeger, 1997) and their dynamic counterparts (Manfredotti, 2009),
probabilistic relational models (Koller, 1999; Friedman, Getoor, Koller, & Pfeffer, 1999), Bayesian
logic programs (Kersting & De Raedt, 2000), and first-order conditional influence language (Natarajan, Tadepalli, Altendorf, Dietterich, Fern, & Restificar, 2005). Conditional random fields have been
extensively applied to activity recognition, and their superior labeling performance over generative
models has been demonstrated in a number of both single-agent and multi-agent domains (Liao
124

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

et al., 2005; Limketkai, Fox, & Liao, 2007; Vail, 2008; Vail & Veloso, 2008; Hu et al., 2008).
Since MLNs are often solved as propositionalized CRFs, and the directed alternatives can be compiled into a Bayesian network, it can be expected that discriminative relational models generally
outperform their generative counterparts on labeling tasks. However, more work needs to be done
to answer this question in its entirety.
Since Markov logic is based on, and in fact subsumes, finite first-order logic, we immediately
gain access to a number of techniques developed in the rich field of traditional logic. Current Markov
logic solvers take advantage of the underlying logical structure to perform more powerful optimizations, such as Alchemys lifted inference in belief propagation and MC-SAT (Poon & Domingos,
2006). Additionally, domain pruning, where one uses hard constraints to infer reduced domains for
predicates, has been shown to lead to significant speed-ups (Papai, Singla, & Kautz, 2011).
We also leverage this relationship between Markov and first-order logic when inducing an augmented model. Furthermore, presence of dependency cycles introduces additional problems in
directed graphical (relational) models. Thus, the fact that, in Markov logic, knowledge can be
expressed as weighted first-order formulas combined with the above factors make it a powerful
framework best suited for the multi-agent reasoning tasks considered in this work.
Traditional hidden Markov models operate over an alphabet of unstructured (i.e., flat) symbols. This makes relational reasoning difficult, as one has to either propositionalize the domain,
thereby incurring combinatorial increase in the number of symbols and model parameters, or ignore
the relational structure and sacrifice information. Logical hidden Markov models (LHMMs) have
been proposed to address this problem (Kersting, De Raedt, & Raiko, 2006). LHMMs are a generalization of standard HMMs that compactly represents probability distributions over sequences of
logical atoms rather than flat symbols. LHMMs have been proven strictly more powerful than their
propositional counterparts (HMMs). By applying techniques from logic-based reasoning, such as
unification, while leveraging the logical structure component of the model, Kersting et al. show that
LHMMs often require fewer parameters and achieve higher accuracy than HMMs.
LHMMs have been recently applied to activity recognition. In the context of intelligent user interfaces, the work of Shen (2009) designs and evaluates a LHMM model for recognition of peoples
activities and workflows carried out on a desktop computer. Other researchers proposed a hierarchical extension of LHMMs along with an efficient particle filter-based inference technique, and
apply it to activity recognition problems in synthetic domains (Natarajan, Bui, Tadepalli, Kersting,
& Wong, 2008). Both lines of work show that LHMMs can be learned and applied efficiently, and
perform better than plain HMMs.
However, LHMMs are a generative model and therefore are not ideal for pure labeling and
recognition tasks, where we typically do not want to make strong independence assumptions about
the observations, nor do we want to explicitly model dependencies in the input space. TildeCRFa
relational extension of traditional conditional random fieldshas been introduced to address this
issue (Gutmann & Kersting, 2006). TildeCRF allows discriminative learning and inference in CRFs
that encode sequences of logical atoms, as opposed to sequences of unstructured symbols. TildeCRF
specifically focuses on efficient learning of models of sequential data via boosting, and is subsumed
by Markov logic, which can produce both discriminative and generative models. We cast our model
in the latter framework to make it more general, extensible, and interpretable.
PRISM, a probabilistic extension of Prolog, has been shown to subsume a wide variety of generative models, including Bayesian networks, probabilistic context-free grammars, HMMs (along
with their logical extension) (Sato & Kameya, 2001, 2008). However, since the focus of PRISM is
125

fiS ADILEK & K AUTZ

on representational elegance and generality, rather than scalability, the sheer size of the state space
and complexity of our CTF domain precludes its application here.
Finally, our Markov logic theory augmentation process is related to structure learning, transfer learning, and inductive logic programming. In fact, Algorithm 1 implements a special case of
structure learning, where we search for a target theory that explains the training data well, while our
declarative bias forces the target theory to differ from the source theory only as much as necessary.
Again, with the intuition that failed attempts are similar to their failed counterparts. A number of
researchers have focused on structure learning specifically in Markov logic networks. This includes
early work on top-down structure learning, where clauses in the knowledge base are greedily modified by adding, flipping, and deleting logical literals (Kok & Domingos, 2005). This search is guided
by the likelihood of the training data under the current model. The work of Mihalkova and Mooney
(2007) exploit patterns in the ground Markov logic networks to introduce a bottom-up declarative
bias that makes their algorithm less susceptible to finding only local optima, as compared to alternative greedy methods. Similarly, the work of Kok and Domingos (2009) introduce a bottom-up
declarative bias based on lifted hypergraph representation of the relational database. This bias then
guides search for clauses that fit the data. Since the hypergraph is lifted, relational path finding
tractable. Interesting work on predicate invention applies relational clustering technique formulated
in second-order Markov logic to discover new predicates from relational databases (Kok & Domingos, 2007). The above systems are capable of modeling relatively rich family of logical formulas.
Other approaches perform discriminative structure learning and achieve excellent results, but focus
on a restricted set of types of formulas (e.g., Horn clauses) (Huynh & Mooney, 2008; Biba, Ferilli, &
Esposito, 2008). The work of Davis and Domingos (2009) successfully uses second-order Markov
logic in deep transfer learning. They lift the model of the source domain to second-order ML and
identify high-level structural patterns. These subsequently serve as declarative bias for structure
learning in the target domain.
By its very nature, the inductive logic programming discipline has extensively studied structure
learning in deterministic, as well as probabilistic settings (e.g., Muggleton, 2002; De Raedt, 2008;
De Raedt, Frasconi, Kersting, & Muggleton, 2008). In fact, our theory augmentation algorithm can
be viewed as an efficient Markov logic based version of theory refinement, a well-established ILP
technique that aims to improve the quality of a theory in terms of simplicity, fit to newly acquired
data, efficiency or other factors (Wrobel, 1996).
Our approach differs from all this work in three main points. First, our declarative bias is defined
implicitly by the seed theory of successful activities. Therefore, our theory augmentation algorithm
is not limited to any hard-wired set of formula types it can consider. Rather, the search space is
defined at run time by extracting motifs from the seed theory. The second distinction lies in computational tractability and exactness of the results. By distinguishing between soft and hard formulas,
we are able to search through candidate formulas in a systematic, rather than greedy manner. Consequently, our final learned model requires fewer parameters, which is especially important when
the amount of training data is relatively small. Additionally, our weight learning does not experience cold starts, as we leverage the seed theory. The final difference is that, to our knowledge, we
are the first to explore structure learning in the context of interplay of success and failure, and their
relationship to the intended goals of peoples actions.

126

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

8. Conclusions
This paper took on the task of understanding the game of capture the flag from GPS data as an
exemplar of the general problem of inferring human interactions and intentions from sensor data.
We have presented a novel methodologycast in Markov logicfor effectively combining data
denoising with higher-level relational reasoning about a complex multi-agent domain. Specifically,
we have demonstrated that given raw and noisy data, we can automatically and reliably detect
and recognize both successful and failed interactions in adversarial as well as cooperative settings.
Additionally, we have shown that success, failure, and the goal of an activity are intimately tied
together and having a model for successful events allows us to naturally learn models of the other
two important aspects of life. Specifically, we have demonstrated that the intentions of rational
agents are automatically discovered in the process of resolving inconsistencies between a theory that
models successful instances of a set of activities and examples of failed attempts at those activities.
We have formulated four research questions and designed experiments within the CTF domain
that empirically answer them. Compared to alternative approaches to solving the multi-agent activity recognition problem, our augmented Markov logic model, which takes into account not only
relationships among individual players, but also relationships among activities over the entire length
of a game, although computationally more costly, is significantly more accurate on real-world data.
Furthermore, we have illustrated that explicitly modeling unsuccessful attempts boosts performance
on other important recognition tasks.

9. Future Work
Multi-agent activity recognition is especially interesting in the context of current unprecedented
growth of on-line social networksin terms of their size, popularity, and their impact on our offline lives. In this paper, we show that location information alone allows for rich models of peoples
interactions, but in the case of on-line social networks, we additionally have access to the content
of users posts and both the explicit and the implicit network interactions. For instance, our recent
study shows that, interestingly, about 30% of Twitter status updates reveal their authors location
(Sadilek, Kautz, & Bigham, 2012). These data sources are now available to machines in massive
volumes and at ever-increasing real-time streaming rate. We note that a substantial fraction of posts
on services such as Facebook and Twitter talk about everyday activities of the users (Naaman, Boase,
& Lai, 2010), and this information channel has become available to the research community only
very recently. Thus, if we are able to reason about human behavior and interactions in an automated
way, we can tap the colossal amounts of knowledge that isat presentdistributed across the whole
population.
We are currently extending our model to handle not only explicit GPS traces, but also be able to
infer the location of people who do not broadcast their GPS coordinates. The basic idea is, again, to
leverage the structure of relationships among people. The vast majority of us participate in on-line
social networks and typically some of our friends there do publish their location. We thus view
the GPS-enabled people as noisy location sensors and use the network interactions and dynamics
to estimate the location of the rest of the users. At present, we are testing this approach on public
tweets.

127

fiS ADILEK & K AUTZ

Acknowledgments
We thank anonymous reviewers for their constructive feedback. We further thank Sebastian Riedel
for his help with theBeast, and to Radka Sadlkova and Wendy Beatty for their helpful comments.
This work was supported by ARO grant #W911NF-08-1-0242, DARPA SBIR Contract #W31P4Q08-C-0170, and a gift from Kodak.

References
Abowd, G. D., Atkeson, C. G., Hong, J., Long, S., Kooper, R., & Pinkerton, M. (1997). Cyberguide:
a mobile context-aware tour guide. Wirel. Netw., 3(5), 421433.
Allwein, E., Schapire, R., & Singer, Y. (2001). Reducing multiclass to binary: A unifying approach
for margin classifiers. The Journal of Machine Learning Research, 1, 113141.
Ashbrook, D., & Starner, T. (2003). Using GPS to learn significant locations and predict movement
across multiple users. Personal Ubiquitous Comput., 7, 275286.
Baker, C., Tenenbaum, J., & Saxe, R. (2006). Bayesian models of human action understanding.
Advances in Neural Information Processing Systems, 18, 99.
Baker, C., Goodman, N., & Tenenbaum, J. (2008). Theory-based social goal inference. In Proceedings of the thirtieth annual conference of the cognitive science society, pp. 14471452.
Baker, C., Saxe, R., & Tenenbaum, J. (2011). Bayesian theory of mind: Modeling joint belief-desire
attribution. In Proceedings of the Thirty-Second Annual Conference of the Cognitive Science
Society.
Baker, C., Tenenbaum, J., & Saxe, R. (2007). Goal inference as inverse planning. In Proceedings
of the 29th annual meeting of the cognitive science society.
Baldwin, D. A., & Baird, J. A. (2001). Discerning intentions in dynamic human action. Trends in
Cognitive Sciences, 5(4), 171  178.
Barbuceanu, M., & Fox, M. (1995). COOL: a language for describing coordination in multi
agent systems. In Proceedings of the First International Conference on Multi-Agent Systems
(ICMAS-95), pp. 1724.
Bell, R., Koren, Y., & Volinsky, C. (2007). Modeling relationships at multiple scales to improve
accuracy of large recommender systems. In KDD, pp. 95104, New York, NY, USA. ACM.
Biba, M., Ferilli, S., & Esposito, F. (2008). Discriminative structure learning of Markov logic
networks.. pp. 5976. Springer.
Biswas, R., Thrun, S., & Fujimura, K. (2007). Recognizing activities with multiple cues. In Workshop on Human Motion, pp. 255270.
Bui, H. H. (2003). A general model for online probabilistic plan recognition. In Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-2003).
Busetta, P., Serafini, L., Singh, D., & Zini, F. (2001). Extending multi-agent cooperation by overhearing. In Cooperative Information Systems, pp. 4052. Springer.
Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. ACM Comput.
Surv., 41, 15:115:58.
128

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Culotta, A., & McCallum, A. (2005). Joint deduplication of multiple record types in relational data.
In Proceedings of the 14th ACM international conference on Information and knowledge
management, pp. 257258. ACM.
Davis, J., & Domingos, P. (2009). Deep transfer via second-order Markov logic. In Proceedings of
the 26th Annual International Conference on Machine Learning, pp. 217224. ACM.
De Raedt, L. (2008). Logical and relational learning. Springer-Verlag New York Inc.
De Raedt, L., Frasconi, P., Kersting, K., & Muggleton, S. (Eds.). (2008). Probabilistic Inductive
Logic Programming - Theory and Applications, Vol. 4911 of Lecture Notes in Computer
Science. Springer.
De Raedt, L., & Kersting, K. (2008). Probabilistic inductive logic programming. (De Raedt et al.,
2008), pp. 127.
Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreference resolution
using integer programming. In Proceedings of NAACL HLT, pp. 236243.
Domingos, P. (2004). Multi-relational record linkage. In In Proceedings of the KDD-2004 Workshop
on Multi-Relational Data Mining.
Domingos, P., Kok, S., Lowd, D., Poon, H., Richardson, M., & Singla, P. (2008). Markov logic.
(De Raedt et al., 2008), pp. 92117.
Eagle, N., & Pentland, A. (2006). Reality mining: sensing complex social systems. Personal and
Ubiquitous Computing, 10(4), 255268.
Eagle, N., & Pentland, A. (2009). Eigenbehaviors: Identifying structure in routine. Behavioral
Ecology and Sociobiology, 63(7), 10571066.
Eagle, N., Pentland, A., & Lazer, D. (2009). Inferring social network structure using mobile phone
data. In Proceedings of the National Academy of Sciences.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational models.
In International Joint Conference on Artificial Intelligence, Vol. 16, pp. 13001309.
Goutte, C., & Gaussier, E. (2005). A probabilistic interpretation of precision, recall and f-score,
with implication for evaluation.. pp. 345359. Springer.
Gupta, A., Srinivasan, P., Shi, J., & Davis, L. S. (2009). Understanding videos, constructing plots:
Learning a visually grounded storyline model from annotated videos. In CVPR.
Gutmann, B., & Kersting, K. (2006). TildeCRF: conditional random fields for logical sequences. In
Machine Learning: ECML 2006, pp. 174185. Springer.
Helaoui, R., Niepert, M., & Stuckenschmidt, H. (2010). A statistical-relational activity recognition
framework for ambient assisted living systems. In Ambient Intelligence and Future TrendsInternational Symposium on Ambient Intelligence (ISAmI 2010), pp. 247254. Springer.
Hong, J. (2001). Goal recognition through goal graph analysis. Journal of Artificial Intelligence
Research, 15, 130.
Horvitz, E., Apacible, J., Sarin, R., & Liao, L. (2005). Prediction, expectation, and surprise: Methods, designs, and study of a deployed traffic forecasting service. In Twenty-First Conference
on Uncertainty in Artificial Intelligence.

129

fiS ADILEK & K AUTZ

Hu, D., Pan, S., Zheng, V., Liu, N., & Yang, Q. (2008). Real world activity recognition with multiple
goals. In UbiComp, Vol. 8, pp. 3039.
Huynh, T., & Mooney, R. (2008). Discriminative structure and parameter learning for Markov
logic networks. In Proceedings of the 25th international conference on Machine learning,
pp. 416423. ACM.
Jaeger, M. (1997). Relational Bayesian networks. In Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence, pp. 266273.
Jordan, M. (1998). Learning in graphical models. Kluwer Academic Publishers.
Kamar, E., & Horvitz, E. (2009). Collaboration and shared plans in the open world: Studies of
ridesharing. In IJCAI.
Kaminka, G. A., Tambe, D. V. P. M., Pynadath, D. V., & Tambe, M. (2002). Monitoring teams
by overhearing: A multi-agent plan-recognition approach. Journal of Artificial Intelligence
Research, 17, 2002.
Kersting, K., & De Raedt, L. (2000). Bayesian logic programs. In Proceedings of the Work-inProgress Track at the 10th International Conference on Inductive Logic Programming.
Kersting, K., De Raedt, L., & Raiko, T. (2006). Logical hidden Markov models. Journal of Artificial
Intelligence Research, 25(1), 425456.
Kok, S., & Domingos, P. (2005). Learning the structure of Markov logic networks. In Proceedings
of the 22nd international conference on Machine learning, pp. 441448. ACM.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. In Proceedings of the 24th international conference on Machine learning, pp. 433440. ACM.
Kok, S., & Domingos, P. (2009). Learning Markov logic network structure via hypergraph lifting. In
Proceedings of the 26th Annual International Conference on Machine Learning, pp. 505512.
ACM.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. In ICML 07: Proceedings of
the 24th international conference on Machine learning, pp. 433440, New York, NY, USA.
ACM.
Koller, D. (1999). Probabilistic relational models. In Inductive Logic Programming, pp. 313.
Springer.
Lafferty, J. (2001). Conditional random fields: Probabilistic models for segmenting and labeling
sequence data. In International Conference on Machine Learning (ICML), pp. 282289.
Morgan Kaufmann.
Landwehr, N., Gutmann, B., Thon, I., Philipose, M., & De Raedt, L. (2007). Relational
transformation-based tagging for human activity recognition. In Proceedings of the 6th International Workshop on Multi-relational Data Mining (MRDM07), pp. 8192.
Liao, L., Patterson, D., Fox, D., & Kautz, H. (2007). Learning and inferring transportation routines.
Artificial Intelligence, 171(5-6), 311331.
Liao, L., Fox, D., & Kautz, H. (2004). Learning and inferring transportation routines. In Proceedings of the Nineteenth National Conference on Artificial Intelligence.

130

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Liao, L., Fox, D., & Kautz, H. (2005). Location-based activity recognition using relational Markov
networks. In IJCAI.
Limketkai, B., Fox, D., & Liao, L. (2007). CRF-filters: Discriminative particle filters for sequential
state estimation. In Robotics and Automation, 2007 IEEE International Conference on, pp.
31423147.
Ling, X., & Weld, D. (2010). Temporal information extraction. In Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
Ma, Z. (2008). Modelling with PRISM of intelligent system. MSc. Thesis, Linacre College, University of Oxford.
Manfredotti, C. (2009). Modeling and inference with relational dynamic Bayesian networks. In
Advances in Artificial Intelligence, pp. 287290. Springer.
Manfredotti, C., & Messina, E. (2009). Relational dynamic Bayesian networks to improve multitarget tracking. In Advanced Concepts for Intelligent Vision Systems, pp. 528539. Springer.
Manfredotti, C., Hamilton, H., & Zilles, S. (2010). Learning RDBNs for activity recognition. In
Neural Information Processing Systems.
Mihalkova, L., & Mooney, R. (2007). Bottom-up learning of Markov logic network structure. In
Proceedings of the 24th international conference on Machine learning, pp. 625632. ACM.
Moore, D., & Essa, I. (2001). Recognizing multitasked activities using stochastic context-free grammar. In In Proceedings of AAAI Conference.
Muggleton, S. (2002). Learning structure and parameters of stochastic logic programs. In Proceedings of the 12th international conference on Inductive logic programming, pp. 198206.
Springer-Verlag.
Murphy, K. P. (2002). Dynamic bayesian networks: representation, inference and learning. Ph.D.
thesis, University of California, Berkeley.
Naaman, M., Boase, J., & Lai, C.-H. (2010). Is it really about me?: message content in social
awareness streams. In CSCW 10: Proceedings of the 2010 ACM conference on Computer
supported cooperative work, pp. 189192, New York, NY, USA. ACM.
Natarajan, S., Tadepalli, P., Altendorf, E., Dietterich, T., Fern, A., & Restificar, A. (2005). Learning
first-order probabilistic models with combining rules. In Proceedings of the 22nd international conference on Machine learning, pp. 609616. ACM.
Natarajan, S., Bui, H. H., Tadepalli, P., Kersting, K., & Wong, W. (2008). Logical hierarchical
hidden Markov models for modeling user activities. In In Proc. of ILP-08.
Papai, T., Singla, P., & Kautz, H. (2011). Constraint propagation for efficient inference in Markov
logic. In Seventeenth International Conference on Principles and Practice of Constraint
Programming.
Pentland, A. S. (2008). Honest Signals: How They Shape Our World. The MIT Press.
Poon, H., & Domingos, P. (2006). Sound and efficient inference with probabilistic and deterministic
dependencies. In Proceedings of the National Conference on Artificial Intelligence, Vol. 21,
p. 458. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.

131

fiS ADILEK & K AUTZ

Poon, H., & Domingos, P. (2007). Joint inference in information extraction. In Proceedings of the
22nd national conference on Artificial intelligence-Volume 1, pp. 913918. AAAI Press.
Poon, H., & Domingos, P. (2008). Joint unsupervised coreference resolution with Markov logic. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp.
650659. Association for Computational Linguistics.
Riedel, S. (2008). Improving the accuracy and efficiency of map inference for Markov logic. In
Proceedings of the Proceedings of the Twenty-Fourth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-08), pp. 468475, Corvallis, Oregon. AUAI Press.
Sadilek, A., & Kautz, H. (2010a). Modeling and reasoning about success, failure, and intent of
multi-agent activities. In Mobile Context-Awareness Workshop, Twelfth ACM International
Conference on Ubiquitous Computing.
Sadilek, A., & Kautz, H. (2010b). Recognizing multi-agent activities from GPS data. In TwentyFourth AAAI Conference on Artificial Intelligence.
Sadilek, A., Kautz, H., & Bigham, J. P. (2012). Finding your friends and following them to where
you are. In Fifth ACM International Conference on Web Search and Data Mining (WSDM).
Sato, T., & Kameya, Y. (2001). Parameter learning of logic programs for symbolic-statistical modeling. In Journal of Artificial Intelligence Research.
Sato, T., & Kameya, Y. (2008). New advances in logic-based probabilistic modeling by PRISM. In
Probabilistic inductive logic programming, pp. 118155. Springer.
Shakarian, P., Subrahmanian, V., & Spaino, M. L. (2009). SCARE: A Case Study with Baghdad.
In Proceedings of the Third International Conference on Computational Cultural Dynamics.
AAAI.
Shen, J. (2009). Activity recognition in desktop environments. Ph.D. Thesis, Oregon State University.
Shoenfield, J. R. (1967). Mathematical Logic. Addison-Wesley.
Singla, P., & Domingos, P. (2005). Discriminative training of Markov logic networks. In Proceedings of the National Conference on Artificial Intelligence, Vol. 20, p. 868. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press; 1999.
Singla, P., & Domingos, P. (2007). Markov logic in infinite domains. In UAI-07.
Tang, K., Lin, J., Hong, J., Siewiorek, D., & Sadeh, N. (2010). Rethinking location sharing: exploring the implications of social-driven vs. purpose-driven location sharing. In Proceedings of
the 12th ACM international conference on Ubiquitous computing, pp. 8594. ACM.
Tran, S., & Davis, L. (2008). Visual event modeling and recognition using Markov logic networks.
In Proceedings of the 10th European Conference on Computer Vision.
Ullman, T., Baker, C., Macindoe, O., Evans, O., Goodman, N., & Tenenbaum, J. (2010). Help
or hinder: Bayesian models of social goal inference. In Advances in Neural Information
Processing Systems (NIPS), Vol. 22.
Vail, D. (2008). Conditional random fields for activity recognition. Ph.D. Thesis, Carnegie Mellon
University.

132

fiL OCATION -BASED R EASONING ABOUT C OMPLEX M ULTI -AGENT B EHAVIOR

Vail, D., & Veloso, M. (2008). Feature selection for activity recognition in multi-robot domains. In
Proceedings of AAAI, Vol. 2008.
Wang, J., & Domingos, P. (2008). Hybrid Markov logic networks. In Proceedings of the 23rd
national conference on Artificial intelligence - Volume 2, pp. 11061111. AAAI Press.
Wellner, B., McCallum, A., Peng, F., & Hay, M. (2004). An integrated, conditional model of information extraction and coreference with application to citation matching. In Proceedings of
the 20th conference on Uncertainty in artificial intelligence, pp. 593601. AUAI Press.
Wilson, A., Fern, A., Ray, S., & Tadepalli, P. (2008). Learning and transferring roles in multi-agent
mdps. In Proceedings of AAAI.
Wilson, A., Fern, A., & Tadepalli, P. (2010). Bayesian role discovery for multi-agent reinforcement learning. In Proceedings of the 9th International Conference on Autonomous Agents
and Multiagent Systems: volume 1-Volume 1, pp. 15871588. International Foundation for
Autonomous Agents and Multiagent Systems.
Wrobel, S. (1996). First order theory refinement. In Advances in inductive logic programming, pp.
1433. IOS Press, Amsterdam.
Wu, T., Lian, C., & Hsu, J. (2007). Joint recognition of multiple concurrent activities using factorial
conditional random fields. In Proc. 22nd Conf. on Artificial Intelligence (AAAI-2007).
Yoshikawa, K., Riedel, S., Asahara, M., & Matsumoto, Y. (2009). Jointly identifying temporal relations with Markov logic. In Proceedings of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference on Natural Language Processing of
the AFNLP: Volume 1-Volume 1, pp. 405413. Association for Computational Linguistics.

133

fiJournal of Artificial Intelligence Research 43 (2012) 211-255

Submitted 8/11; published 02/12

Exploiting Model Equivalences for Solving Interactive Dynamic
Influence Diagrams
Yifeng Zeng

YFZENG @ CS . AAU . DK

Dept. of Computer Science
Aalborg University
DK-9220 Aalborg, Denmark

Prashant Doshi

PDOSHI @ CS . UGA . EDU

Dept. of Computer Science
University of Georgia
Athens, GA 30602, U.S.A.

Abstract
We focus on the problem of sequential decision making in partially observable environments
shared with other agents of uncertain types having similar or conflicting objectives. This problem
has been previously formalized by multiple frameworks one of which is the interactive dynamic
influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent
setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its
belief over the physical state and others models, which changes as the agent acts and observes in
the multiagent setting.
As we may expect, solving I-DIDs is computationally hard. This is predominantly due to the
large space of candidate models ascribed to the other agents and its exponential growth over time.
We present two methods for reducing the size of the model space and stemming its exponential
growth. Both these methods involve aggregating individual models into equivalence classes. Our
first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model
space. The second method further compacts the model space by focusing on portions of the behavioral predictions. Specifically, we cluster actionally equivalent models that prescribe identical
actions at a single time step. Exactly identifying the equivalences would require us to solve all
models in the initial set. We avoid this by selectively solving some of the models, thereby introducing an approximation. We discuss the error introduced by the approximation, and empirically
demonstrate the improved efficiency in solving I-DIDs due to the equivalences.

1. Introduction
Sequential decision making (planning) is a key tenet of agent autonomy. Decision making becomes
complicated due to actions that are nondeterministic and a physical environment that is often only
partially observable. The complexity increases exponentially in the presence of other agents who
are themselves acting and observing, and whose actions impact the subject agent. Multiple related
frameworks formalize the general problem of decision making in uncertain settings shared with
other sophisticated agents who may have similar or conflicting objectives. One of these frameworks is the interactive partially observable Markov decision process (I-POMDP) (Gmytrasiewicz
& Doshi, 2005), which generalizes POMDPs (Smallwood & Sondik, 1973; Kaelbling, Littman,
& Cassandra, 1998) to multiagent settings; another framework is the interactive dynamic influc
2012
AI Access Foundation. All rights reserved.

fiZ ENG & D OSHI

Ri

Ri

Ait

Ait+1
Ajt

Ajt+1

St

St+1

Mj,l-1t

Mj,l-1t+1

Oit

Oit+1

Figure 1: A two time-slice I-DID for agent i modeling another agent j. I-DIDs allow representing
models in a model node (hexagon) and their update over time using the dotted model
update link. Predictions about the other agents behavior from the models are represented
using a dashed policy link.

ence diagram (I-DID) (Doshi, Zeng, & Chen, 2009). In cooperative settings, the decentralized
POMDP (Bernstein, Givan, Immerman, & Zilberstein, 2002) framework models multiagent decision making.
I-DIDs are graphical models for sequential decision making in uncertain multiagent settings.
They concisely represent the problem of how an agent should act in an uncertain environment shared
with others who may act simultaneously in sophisticated ways. I-DIDs may be viewed as graphical
counterparts of I-POMDPs which adopt an enumerative representation of the decision-making problem. I-DIDs generalize dynamic influence diagrams (DID) (Tatman & Shachter, 1990) to multiagent
settings analogously to the way that I-POMDPs generalize POMDPs. Importantly, I-DIDs have the
advantage of a representation that explicates the embedded domain structure by decomposing the
state space into variables and relationships between the variables. Not only is this representation
more intuitive to use, it translates into computational benefits when compared to the enumerative
representation as used in I-POMDPs (Doshi et al., 2009).
Following the paradigm of graphical models, I-DIDs compactly represent the decision problem
by mapping various variables into chance, decision and utility nodes, and denoting the dependencies
between variables using directed arcs between the corresponding nodes. They extend DIDs by
introducing a special model node whose values are the possible models of the other agent. These
models may themselves be represented using I-DIDs leading to nested modeling. Both other agents
models and the original agents beliefs over these models are updated over time using a special
model update link that connects the model nodes between time steps. Solution to the I-DID is a
policy that prescribes what the agent should do over time, given its beliefs over the physical state
and others models. Consequently, I-DIDs may be used to compute the policy of an agent online 
given an initial belief of the agent  as the agent acts and observes in a setting that is populated by
other interacting agents. We show a generic I-DID in Fig. 1 and provide more details in Section 3.
As we may expect, solving I-DIDs is computationally very hard. In particular, they acutely
suffer from the curses of dimensionality and history (Pineau, Gordon, & Thrun, 2006). This is
because the state space in I-DIDs includes the models of other agents in addition to the traditional
212

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

physical states. As the agents act, observe and update beliefs, I-DIDs must track the evolution of the
models over time. Theoretically, the number of candidate models grows exponentially over time.
Thus, I-DIDs not only suffer from the curse of history that afflicts the modeling agent, but also from
that exhibited by the modeled agents. This is further complicated by the nested nature of the state
space.
Consequently, exact solutions of I-DIDs are infeasible for all but the simple problems and ways
of mitigating the computational intractability are critically needed. Because the complexity is predominantly due to the candidate models, we focus on principled reductions of the model space
while avoiding significant losses in the optimality of the decision maker. Our first approach builds
upon the idea of grouping together behaviorally equivalent (BE) models (Rathnasabapathy, Doshi,
& Gmytrasiewicz, 2006; Pynadath & Marsella, 2007). These are models whose behavioral predictions for the modeled agent(s) are identical. Because the solution of the subject agents I-DID
is affected only by the predicted behavior of the other agent regardless of the description of the
ascribed model, we may consider a single representative from each BE class without affecting the
optimality of the solution. Identifying BE models requires solving the individual models. We reduce the exponential growth in the model space by discriminatively updating models. Specifically,
at each time step, we select only those models for updating which will result in predictive behaviors
that are distinct from others in the updated model space. In other words, models that on update
would result in predictions which are identical to those of existing models are not selected for updating. For these models, we simply transfer their revised probability masses to the existing BE
models. Thus, we avoid generating all possible updated models and subsequently reducing them.
Rather, we generate a minimal set of models at each time step.
Restricting the updated models to the exact minimal set would require solving all the models
that are considered initially. Exploiting the notion that models whose beliefs are spatially close tend
to be BE, we solve those models only whose beliefs are not -close to a representative. We theoretically analyze the error introduced by this approach in the optimality of the solution. Importantly,
we experimentally evaluate our approach on I-DIDs formulated for multiple problem domains having two agents, and show approximately an order of magnitude improvement in performance in
comparison to the previous clustering approach (Zeng, Doshi, & Chen, 2007), with a comparable
loss in optimality. One of these problem domains is the Georgia testbed for autonomous control of
vehicles (GaTAC) (Doshi & Sonu, 2010), which facilitates scalable and realistic problem domains
pertaining to autonomous control of unmanned agents such as uninhabited aerial vehicles (UAV).
GaTAC provides a low-cost, open-source and flexible environment for realistically simulating the
problem domains and evaluating solutions produced by multiagent decision-making algorithms.
We further compact the space of models in the model node by observing that behaviorally
distinct models may prescribe identical actions at a single time step. We may then group together
these models into a single equivalence class. In comparison to BE, the definition of our equivalence
class is different: it includes those models whose prescribed action for the particular time step is
the same, and we call it action equivalence (AE). Since there are typically additional models than
the BE ones that prescribe identical actions at a time step, an AE class often includes many more
models. Consequently, the model space is partitioned into lesser number of classes than previously
and is bounded by the number of actions of the other agent.
Unlike the update of BE classes, given the action and an observation AE classes do not update
deterministically. We show how we may compute the probability with which an equivalence class is
updated to another class in the next time step. Although, in general, grouping AE models introduces
213

fiZ ENG & D OSHI

an approximation, we derive conditions under which AE model grouping preserves optimality of the
solution. We demonstrate the performance of our approach on multiple two-agent problem domains
including in GaTAC and show significant time savings in comparison to previous approaches.
To summarize, the main contributions of this article are new approaches that group equivalent
models more efficiently leading to improved scalability in solving I-DIDs. Our first method reduces
the exponential growth of the model space by discriminatively updating models thereby generating
a behaviorally minimal set in the next time step that is characterized by the absence of BE models.
The second method adopts a relaxed grouping of models that prescribe identical actions for that
particular time step. Grouping AE models leads to equivalence classes that often include many
models in addition to those that are BE. We augment both these methods with an approximation that
avoids solving all of the initial models, and demonstrate much improved scalability in experiments.
The remainder of this article is structured as follows. In Section 2, we discuss previous work
related to this article. In Section 3, we briefly review the graphical model of I-DID as well as its
solution based on BE. In Section 4, we show how we may discriminatively update models in order
to facilitate behaviorally-distinct models at subsequent time steps. We introduce an approximation,
and discuss the associated computational savings and error. We introduce the approach of further
grouping models based on actions, in Section 5. All approaches for solving I-DIDs are empirically
evaluated along different dimensions in Section 6. We conclude this article with a discussion of the
framework and the solution approaches including extensions to N > 2 agent interactions, and some
limitations, in Section 7. The Appendices contain proofs of propositions mentioned elsewhere, and
detailed descriptions and I-DID representations of the problem domains used in the evaluation.

2. Related Work
Suryadi and Gmytrasiewicz (1999) in an early piece of related work, proposed modeling other
agents using IDs. The approach proposed ways to modify the IDs to better reflect the observed
behavior. However, unlike I-DIDs, other agents did not model the original agent and the distribution
over the models was not updated based on the actions and observations.
As detailed by Doshi et al. (2009), I-DIDs contribute to an emerging and promising line of
research on graphical models for multiagent decision making. This includes multiagent influence
diagrams (MAID) (Koller & Milch, 2001), network of influence diagrams (NID) (Gal & Pfeffer,
2008), and more recently, limited memory influence diagram based players (Madsen & Jensen,
2008). While MAIDs adopt an external perspective of the interaction, exploiting the conditional
independence between effects of actions to compute the Nash equilibrium strategy for all agents
involved in the interaction, I-DIDs offer a subjective perspective to the interaction, computing the
best-response policy as opposed to a policy in equilibrium. The latter may not account for other
agents behaviors outside the equilibrium and multiple equilibria may exist. Furthermore, both
MAID and NID formalisms focus on a static, single-shot interaction. In contrast, I-DIDs offer
solutions over extended time interactions, where agents act and update their beliefs over others
models which are themselves dynamic.
While I-DIDs closely relate to the previously mentioned ID-based graphical models, another
significant class of graphical models compactly represents the joint behavior as a graphical game
(Kearns, Littman, & Singh, 2001). It models the agents as graph vertices and an interaction in payoff
between two agents using an edge, with the objective of finding a joint distribution over agents
actions possibly in equilibrium. More recently, graphical multiagent models (Duong, Wellman,
214

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

& Singh, 2008) enhance graphical games by allowing beliefs over agent behaviors formed from
different knowledge sources, and conditioning agent behaviors on abstracted history if the game is
dynamic (Duong, Wellman, Singh, & Vorobeychik, 2010).
As we mentioned previously, a dominating cause of the complexity of I-DIDs is the exponential
growth in the candidate models over time. Using the insight that models (with identical capabilities
and preferences) whose beliefs are spatially close are likely to be BE, Zeng and Doshi (2007) utilized a k-means approach to cluster models together and select K models closest to the means of the
clusters in the model node at each time step. This approach facilitates the consideration of a fixed
number of models at each time. However, the approach first generates all possible models before
reducing the model space at each time step, thereby not reducing the memory required. Further, it
utilizes an iterative and often time-consuming k-means clustering method.
The concept of BE of models was proposed and initially used for solving I-POMDPs (Rathnasabapathy et al., 2006), and discussed generally by Pynadath and Marsella (2007). We contextualize BE within the framework of I-DIDs and seek further extensions. A somewhat related notion
is that of state equivalence introduced by Givan et al. (2003) where the equivalence concept is exploited to factorize MDPs and gain computational benefit. Along this direction, another type of
equivalence in probabilistic frameworks such as MDPs and POMDPs, sometimes also called BE, is
bisimulation (Milner, 1980; Givan et al., 2003; Castro, Panangaden, & Precup, 2009). Two states
are bisimilar if any action from these states leads to identical immediate reward and the states transition with the same probability to equivalence classes of states. While bisimulation is a test within a
model given just its definition, BE in multiagent systems is defined and used differently: as a way of
comparing between models using their solutions. Interestingly, both these concepts are ultimately
useful for model minimization.
Other frameworks for modeling the multiagent decision-making problem exist. Most notable
among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for
cooperative settings only and focuses on computing the joint solution for all agents in the team.
Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose
iteratively merging action-observation histories of agents that lead to a small worst-case expected
loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that
exhibit probabilistic equivalence. Such histories generate an identical distribution over the histories
of the other agents and lead to the same joint belief state. While we utilize BE to losslessly cluster
the models of the other agent, we note that BE models when combined with the subject agents policy induce identical distributions over the subject agents action-observation history. More recently,
Witwicki and Durfee (2010) use influence-based abstraction in order to limit an agents belief to the
other agents relevant information by focusing on mutually-modeled features only.
Our agent models are analogous to types in game theory (Harsanyi, 1967), which are defined
as attribute vectors that encompass all of an agents private information. In this context, Dekel et
al. (2006) define a strategic topology on universal type spaces (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993) under which two types are close if their strategic behavior is similar in all
strategic situations. While Dekel et al. focus on a theoretical analysis of the topology and use
rationalizability as the solution concept, we focus on operationalizing BE within a computational
framework. Furthermore, our solution concept is that of best response to ones beliefs.
215

fiZ ENG & D OSHI

3. Background
We briefly review interactive influence diagrams (I-ID) for two-agent interactions followed by their
extension to dynamic settings, I-DIDs (Doshi et al., 2009). Both these formalisms allow modeling
the other agent and to use that information in the decision making of the subject agent.
We illustrate the formalisms and our approaches in the context of the multiagent tiger problem (Gmytrasiewicz & Doshi, 2005)  a two-agent generalization of the well-known single agent
tiger problem (Kaelbling et al., 1998). In this problem, two agents, i and j, face two closed doors
one of which hides a tiger while the other hides a pot of gold. An agent gets rewarded for opening
the door that hides the gold but gets penalized for opening the door leading to the tiger. Each agent
may open the left door (action denoted by OL), open the right door (OR), or listen (L). On listening,
an agent may hear the tiger growling either from the left (observation denoted by GL) or from the
right (GR). Additionally, the agent hears creaks emanating from the direction of the door that was
possibly opened by the other agent  creak from the left (CL) or creak from right (CR)  or silence
(S) if no door was opened. All observations are assumed to be noisy. If any door is opened by
an agent, the tiger appears behind any of the two doors randomly in the next time step. While the
actions of the other agent do not directly affect the reward for an agent, they may potentially change
the location of the tiger. This formulation of the problem differs from that of Nair et al. (2003) in
the presence of door creaks and that it is not cooperative.
3.1 Interactive Dynamic Influence Diagrams
Influence diagrams (Tatman & Shachter, 1990) typically contain chance nodes which represent the
random variables modeling the physical state, S, and the agents observations, Oi , among other
aspects of the problem; decision nodes that model the agents actions, Ai ; and utility nodes that
model the agents reward function, Ri . In addition to these nodes, I-IDs for an agent i include a
new type of node called the model node. This is the hexagonal node, Mj,l1 , in Fig. 2, where j
denotes the other agent and l  1 is the strategy level, which allows for a nested modeling of i
by the other agent j. Agent js level is one less than that of i, which is consistent with previous
hierarchical modeling in game theory (Aumann, 1999a; Brandenburger & Dekel, 1993) and decision
theory (Gmytrasiewicz & Doshi, 2005). Additionally, a level 0 model is an ID or a flat probability
distribution. We note that the probability distribution over the chance node, S, and the model node
together represents agent is belief over its interactive state space. In addition to the model node,
I-IDs differ from IDs by having a chance node, Aj , that represents the distribution over the other
agents actions, and a dashed link, called a policy link.
The model node contains as its values the alternative computational models ascribed by i to the
other agent. The policy link denotes that the distribution over Aj is contingent on the models in
the model node. We denote the set of these models by Mj,l1 , and an individual model of j as,
mj,l1 = hbj,l1 , j i, where bj,l1 is the level l  1 belief, and j is the agents frame encompassing
the decision, observation and utility nodes. A model in the model node may itself be an I-ID or ID,
and the recursion terminates when a model is an ID or a flat probability distribution over the actions.
We observe that the model node and the dashed policy link that connects it to the chance node,
Aj , could be represented as shown in Fig. 3(a) leading to a flat ID shown in Fig. 3(b). The decision
node of each level l  1 I-ID is transformed into a chance node. Specifically, if OPT(m1j,l1 ) is
the set of optimal actions obtained by solving the I-ID (or ID) denoted by m1j,l1 , then P r(aj 
216

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Ri

Ai

S

Aj

Mj.l-1

Oi

Figure 2: A generic level l > 0 I-ID for agent i situated with one other agent j. The shaded hexagon
is the model node (Mj,l1 ) and the dashed arrow is the policy link.

Mj,l-1
S

Aj
Ri

Ai

Mod[Mj]

mj,l-11

mj,l-12

S

A j1
A j2

Aj

Mod[Mj]

Oi

Aj1

Aj2

(b)

(a)

Figure 3: (a) Representing the model node and policy link using chance nodes and dependencies
between them. The decision nodes of the lower-level I-IDs or IDs (m1j,l1 , m2j,l1 ; superscript numbers serve to distinguish the models) are mapped to the corresponding chance
nodes (A1j , A2j ) respectively, which is indicated by the dotted arrows. Depending on the
value of node, M od[Mj ], distribution of each of the chance nodes is assigned to node Aj
with some probability. (b) The transformed flat ID with the model node and policy link
replaced as in (a).

A1j ) =

1
|OPT(m1j,l1 )|

if aj  OPT(m1j,l1 ), 0 otherwise. The different chance nodes (A1j , A2j ) 

one for each model  and additionally, the chance node labeled M od[Mj ] form the parents of the
chance node, Aj . There are as many action nodes as the number of models in the support of agent
is beliefs. The conditional probability table (CPT) of the chance node, Aj , is a multiplexer that
assumes the distribution of each of the action nodes (A1j , A2j ) depending on the value of M od[Mj ].
In other words, when M od[Mj ] has the value m1j,l1 , the chance node Aj assumes the distribution
of the node A1j , and Aj assumes the distribution of A2j when M od[Mj ] has the value m2j,l1 . The
distribution over M od[Mj ] is is belief over js models given the state.
For more than two agents, we add a model node and a chance node representing the distribution
over an agents action linked together using a policy link, for each other agent. Interactions among
others such as coordination or team work could be considered by utilizing models, which predict the
217

fiZ ENG & D OSHI

joint behavior of others, in a distinct model node and possibly updating such models. For example,
the joint behavioral models could be graphical analogs of decentralized POMDPs. Other settings
involving agents acting independently or some of them being cooperative while others adversarial
may be represented as well, and is a topic of research under study. As an aside, Doshi et al. (2009)
show how I-IDs relate to NIDs (Gal & Pfeffer, 2008).

Ri

Ait

Tiger
Locationt

Ajt

Mod[Mjt]
Growl&
Creakit

Ajt,1

Ajt,2

mj,01

mj,02

Rj

Tiger
Locationt,1

Aj

Ajt,2

t,1

Rj

Tiger
Locationt,2

Growljt

Growljt

Figure 4: Level 1 I-ID of i for the multiagent tiger problem. Solutions of two level 0 models (IDs) of
t,2
j map to the chance nodes, At,1
j and Aj , respectively (illustrated using dotted arrows),
transforming the I-ID into a flat ID. The two models differ in the distribution over the
chance node, TigerLocationt .

We setup the I-ID for the multiagent tiger problem described previously, in Fig. 4. We discuss
the CPTs of the various nodes in Appendix B.1. While the I-ID contains two models of j, there
would be as many action nodes of j if there were more models.

Ri

Ri

Ait

Ait+1
Ajt

Ajt+1

St

St+1

Mj,l-1t

Mj,l-1t+1

Oit

Oit+1

Figure 5: A generic two time-slice level l I-DID for agent i. Notice the dotted model update link
that denotes the update of the models of j and of the distribution over the models, over
time.

218

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

I-DIDs extend I-IDs to allow sequential decision making over multiple time steps. We depict a
general, two time-slice I-DID in Fig. 5. In addition to the model nodes and the dashed policy link,
what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 5.
We briefly explain the semantics of the model update next.

St

Ajt

Mj,l-1t

Ajt+1

Mj,l-1t+1
St+1

Mod[Mjt+1]

t

Mod[Mj ]

Ojt+1

Ait
mj,l-1t,1

mj,l-1t,2

Aj

1

Oj

A j2

Oj2

mj,l-1t+1,1

1

mj,l-1t+1,2
mj,l-1t+1,3
mj,l-1t+1,4

A j1

A j2

A j3

A j4

Figure 6: The semantics of the model update link. Notice the growth in the number of models in the
model node at t + 1 shown in bold (superscript numbers distinguish the different models).
Models at t + 1 reflect the updated beliefs of j and their solutions provide the probability
distributions for the action nodes.
Agents in a multiagent setting may act and make observations, which changes their beliefs.
Therefore, the update of the model node over time involves two steps: First, given the models at
time t, we identify the updated set of models that reside in the model node at time t + 1. Because the
agents act and receive observations, their models are updated to reflect their changed beliefs. Since
the set of optimal actions for a model could include all the actions, and the agent may receive any
one of |j | possible observations where j is the set of js observations, the updated set at time step
t+1 will have up to |Mtj,l1 ||Aj ||j | models. Here, |Mtj,l1 | is the number of models at time step t,
|Aj | and |j | are the largest spaces of actions and observations respectively, among all the models.
t+1
t+1
The CPT of chance node M od[Mj,l1
] encodes the indicator function,  (btj,l1 , atj , ot+1
j , bj,l1 ),
updates
which is 1 if the belief btj,l1 in a model mtj,l1 using the action atj and observation ot+1
j
t+1
t+1
to bj,l1 in a model mj,l1 ; otherwise it is 0. Second, we compute the new distribution over the
updated models given the original distribution and the probability of the agent performing the action
and receiving the observation that led to the updated model. The dotted model update link in the
I-DID may be implemented using standard dependency links and chance nodes, as shown in Fig. 6
transforming the I-DID into a flat DID.
In Fig. 7, we show the two time-slice flat DID with the model nodes and the model update link
replaced by the chance nodes and the relationships between them. Chance nodes and dependency
links not in bold are standard, usually found in single agent DIDs.
Continuing with our illustration, we show the two time-slice I-DID for the multiagent tiger
problem in Fig. 8. The model update link not only updates the number of js candidate models due
to its action and observations of growl, it also updates the probability distribution over these models.
The model update link in the I-DID is implemented using standard dependency links as shown in
Fig. 9. For the sake of clarity, we illustrate the update of a single model of j contained in the model
node at time t.
219

fiZ ENG & D OSHI

Ri

Ri

Ait

Ait+1

St

St+1

Oit

Oit+1

Ajt

Ajt+1
t+1

Mod[Mj ]

Mod[Mjt]

Ojt+1
Aj1
Aj

1

Oj

1

A j2

Aj
Oj2

4

A j2
A j3

Figure 7: A flat DID obtained by replacing the model nodes and model update link in the I-DID
of Fig. 5 with the chance nodes and the relationships (in bold) as shown in Fig. 6. The
lower-level models are solved to obtain the distributions for the chance action nodes.

Ri

Ri

Ait

Ait+1
Ajt

Ajt+1

Tiger
Locationt

Tiger
Locationt+1

Mj,l-1t

Mj,l-1t+1

Growl&
Creakt

Growl&
Creakt+1

Figure 8: Two time-slice level l I-DID of i for the multiagent tiger problem. Shaded model nodes
contain the different models of j.

3.2 Behavioral Equivalence and Model Solution
Although the space of possible models is very large, not all models need to be considered by agent i
in the model node. As we mentioned previously, models that are BE (Rathnasabapathy et al., 2006;
Pynadath & Marsella, 2007) could be pruned and a single representative model considered. This is
because the solution of the subject agents I-DID is affected by the predicted behavior of the other
agent; thus we need not distinguish between behaviorally equivalent models. We define BE more
formally below:
Definition 1 (Behavioral equivalence). Two models, mj,l1 and mj,l1 , of the other agent, j, are
behaviorally equivalent if, OPT(mj,l1 ) = OPT(mj,l1 ), where OPT() denotes the solution of
the model that forms the argument.
Thus, BE models are those whose behavioral predictions for the agent are identical.
220

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Tiger
Locationt

Ajt

Mj,l-1t

Ajt+1

Mj,l-1t+1
Tiger
Locationt+1

Mod[Mjt+1]

Mod[Mjt]

mj,l-1t+1,1
Growljt+1

Ait
mj,l-1t,1

A j1

mj,l-1t+1,3

Growlj1

mj,l-1t+1,4
mj,l-1t+1,5
mj,l-1t+1,6

mj,l-1t+1,2

Aj

A j1
2

A j3

A j4

Aj5

Aj6

Figure 9: Because agent j in the tiger problem may receive any one of six possible observations
given the action prescribed by its model, a single model in the model node at time t could
lead to six distinct models at time t + 1.

The solution of an I-DID (and I-ID) is implemented recursively down the levels as shown in
Fig. 10. In order to solve a level 1 I-DID of horizon T , we start by solving the level 0 models, which
may be traditional DIDs of horizon T . Their solutions provide probability distributions over the
other agents actions, which are entered in the corresponding action nodes found in the model node
of the level 1 I-DID at the corresponding time step (lines 3-5). Subsequently, the set of js models
is minimized by excluding the BE models (line 6).
The solution method uses the standard look-ahead technique, projecting the agents action and
observation sequences forward from the current belief state, and finding the possible beliefs that
i could have in the next time step (Russell & Norvig, 2010). Because agent i has a belief over
js models as well, the look-ahead includes finding out the possible models that j could have in
the future. Consequently, each of js level 0 models represented using a standard DID must be
solved in the first time step up to horizon T to obtain its optimal set of actions. These actions are
combined with the set of possible observations that j could make in that model, resulting in an
updated set of candidate models (that include the updated beliefs) that could describe the behavior
of j. SE(btj , aj , oj ) is an abbreviation for the belief update (lines 8-13). Beliefs over these updated
set of candidate models are calculated using the standard inference methods through the dependency
links between the model nodes shown in Fig. 6 (lines 15-18). Agent is I-DID is expanded across all
time steps in this manner. We point out that the algorithm in Fig. 10 may be realized with the help
of standard implementations of DIDs such as H UGIN E XPERT (Andersen & Jensen, 1989). The
solution is a policy tree that prescribes the optimal action(s) to perform for agent i initially given its
belief, and the actions thereafter conditional on its observations.

4. Discriminative Model Updates
Solving I-DIDs is computationally intractable due to not only the large space and complexity of
models ascribed to j, but also due to the exponential growth in candidate models of j over time.
This growth leads to a disproportionate increase in the interactive state space over time. We begin by
introducing a set of models that is minimal in a sense and describe a method for generating this set.
The minimal set is analogous to one of the notions of a minimal mental model space as described by
221

fiZ ENG & D OSHI

I-DID E XACT (level l  1 I-DID or level 0 DID, horizon T )
Expansion Phase
1. For t from 0 to T  1 do
2.
If l  1 then
t
Minimize Mj,l1
3.
For each mtj in Mtj,l1 do
4.
Recursively call algorithm with the l  1 I-DID (or DID)
that represents mtj and the horizon, T  t
5.
Map the decision node of the solved I-DID (or DID), OPT(mtj ), to the
corresponding chance node Aj
6.
Mtj,l1  PruneBEModels(Mtj,l1 )
7.
If t < T  1 then
t+1
Populate Mj,l1
8.
For each mtj in Mtj,l1 do
9.
For each aj in OPT(mtj ) do
10.
For each oj in Oj (part of mtj ) do
11.
Update js belief, bt+1
 SE(btj , aj , oj )
j
t+1
12.
mj  New I-DID (or DID) with bt+1
as the initial belief
j

t+1

{m
}
13.
Mt+1
j
j,l1
t+1
14.
Add the model node, Mj,l1
, and the model update link between
t+1
t
Mj,l1
and Mj,l1
15.
Add the chance, decision, and utility nodes for t + 1 time slice and the
dependency links between them
16.
Establish the CPTs for each chance node and utility node
Solution Phase
17. If l  1 then
18. Represent the model nodes, policy links and the model update links
as in Fig. 6 to obtain the DID
19. Apply the standard look-ahead and backup method to solve the expanded DID
(other solution approaches may also be used)

Figure 10: Algorithm for exactly solving a level l  1 I-DID or level 0 DID expanded over T time
steps.

Pynadath and Marsella (2007). We assume that models of the other agent differ only in their beliefs
and that the other agents frame is known. We later discuss in Section 7 the impact of the frame
being unknown as well. For clarity, we continue to focus on two-agent interactions, and discuss
extensions of the techniques presented here in Section 7 as well.
4.1 Behaviorally Minimal Model Set
Given the set of models, Mj,l1 , of the other agent, j, in a model node we define a corresponding
behaviorally minimal set of models:
222

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Definition 2 (Behaviorally minimal set). Define a minimal set of models, Mj,l1 , as the largest
subset of Mj,l1 , such that for each model, mj,l1  Mj,l1 , there exists no other model in Mj,l1
that is BE to mj,l1 .
Here, BE is as defined in Def. 1. We say that Mj,l1 (behaviorally) minimizes Mj,l1 . As we
illustrate in Fig. 11 using the tiger problem (Kaelbling et al., 1998), the set Mj,l1 that minimizes
Mj,l1 comprises of all the behaviorally distinct representatives of the models in Mj,l1 and only
these models. Because any model from a group of BE models may be selected as the representative
in Mj,l1 , a minimal set corresponding to Mj,l1 is not unique, although its cardinality remains
fixed.
0.075 0.1 0.025

0.05

0.2 0.25

0.05 0.05

0.15 0.05

Pri(Mj,0t|s)

Prj(TL)
0.2

0.6

0.2

Pri(j,0t|s)

Prj(TL)

Figure 11: Illustration of a minimal set using the tiger problem. Black vertical lines denote the
beliefs contained in different models of agent j included in model node, Mj,0 . Decimals
on top indicate is probability distribution over js models, P ri (Mtj,0 |s). In order to
form a behaviorally minimal set, Mtj,0 , we select a representative model from each BE
group of models (models in differently shaded regions). Agent is distribution over the
models in Mtj,0 is obtained by summing the probability mass assigned to the individual
models in each region. Note that Mtj,0 is not unique because any one model within a
shaded region could be selected for inclusion in it.

Agent is probability distribution over the minimal set, Mj,l1 , conditioned on the physical
state is obtained by summing the probability mass over BE models in Mj,l1 and assigning the
accumulated probability to the representative model in Mj,l1 . Formally, let mj,l1  Mj,l1 ,
then:
X
bi (mj,l1 |s) =
bi (mj,l1 |s)
(1)
mj,l1 Mj,l1

where Mj,l1  Mj,l1 is the set of BE models to which the representative mj,l1 belongs. Thus,
if Mj,l1 minimizes Mj,l1 , then Eq. 1 shows how we may obtain the probability distribution over
Mj,l1 at some time step, given is belief distribution over models in the model node at that step
(see Fig. 11).
The behaviorally minimal set together with the probability distribution over it has an important
property: Solution of an I-DID remains unchanged when the models in a model node and the
distribution over the models are replaced by the corresponding minimal set and the distribution
over it, respectively. In other words, transforming the set of models in the model node into its
minimal set preserves the solution. Proposition 1 states this formally:
223

fiZ ENG & D OSHI

Proposition 1. Let X : (Mj,l1 )  (Mj,l1 ) be a mapping defined by Eq. 1, where Mj,l1
is the space of models in a model node and Mj,l1 minimizes it. Then, applying X preserves the
solution of the I-DID.
Proof of Proposition 1 is given in Appendix A. Proposition 1 allows us to show that Mj,l1 is
indeed minimal given Mj,l1 with respect to the solution of the I-DID.
Corollary 1. Mj,l1 in conjunction with X is a sufficient solution-preserving subset of models
found in Mj,l1 .
Proof of this corollary follows directly from Proposition 1. Notice that the subset continues to
be solution preserving when we additionally augment Mj,l1 with models from Mj,l1 .
As the number of models in the minimal set is, of course, no more than in the original set and
typically much less, solution of the I-DID is often computationally much less intensive when the
model set is replaced with its behaviorally minimal counterpart.
4.2 Discrimination Using Policy Graphs
A straightforward way of obtaining Mj,l1 exactly at any time step is to first ascertain the BE
groups of models. This requires us to solve the I-DIDs or DIDs representing the models, then select
a representative model from each BE group to include in Mj,l1 , and prune all others which have
the same solution as the representative.
4.2.1 A PPROACH
Given the set of js models, Mj,l1 , at time t(=0), we present a technique for generating the minimal
sets at subsequent time steps in the I-DID. We first observe that behaviorally distinct models at time
t may result in updated models at t + 1 that are BE. Hence, our approach is to select at time step
t only those models for updating which will result in predictive behaviors that are distinct from
others in the updated model space at t + 1. Models that will result in predictions on update which
are identical to those of other existing models at t + 1 are not selected for updating. Consequently,
the resulting model set at t + 1 is minimal.
We do this by solving the individual I-DIDs or DIDs in Mtj,l1 . Solutions to DIDs or I-DIDs
are policy trees, which may be merged bottom up to obtain a policy graph, as we demonstrate in
Fig. 12. Seuken and Zilberstein (2007) reuse subtrees of smaller horizon by linking to them using
pointers while forming policy trees for the next horizon in the solution of decentralized POMDPs.
The net effect is the formation of a policy graph similar to ours thereby providing an alternative
to our approach of solving the individual models to first obtain the complete policy trees and then
merge post hoc. We adopt the latter approach because the individual models, which are DIDs, when
solved using available implementations produce complete policy trees. The following proposition
gives the complexity of merging the policy trees to obtain the policy graph.
Proposition 2 (Complexity of tree merge). The worst-case complexity of the procedure for merging
policy trees to form a policy graph is O((|j |T 1 )2 |Mj |2 ), where T is the horizon.
Proof. The complexity of the policy tree merge procedure is proportional to the number of comparisons that are made between parts of policy trees to ascertain their similarity. Because the procedure
follows a bottom-up approach and the leaf level has the largest number of nodes, the maximum
224

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

L
GR

GL

OL
*

time t=0

OR

L

*

L

L

GL GR

*

L

OL

L

GR GL

L

GL GR

L

OR

GR GL GR GL

L

OL

L

OR

L

*

GR GL

L

OR

Merge

(a)

GR

OL

Merge

*

GL

GR

*

GL GR

GR GL

L

L

L

L

OR

L

*

GLGR

GL GR

OL

OR

L

L

GL

GL
GR

*

L

OR

L

(b)

[ 0  0.135) [ 0.135  0.865) [ 0.865  0.955) [ 0.955  1]
Actions (node labels):
L = Listen
OL = Open left door
OR = Open right door
Observations (edge labels):
GL = Growl from left door
GR = Growl from right door

L
GR

OL

L

L

GR

GL

L
GR*

OL

GR

GL
L

GL

GR

OR

GL

*

OR

L

* GL *
OR

L

(c)

Figure 12: (a) Example policy trees obtained by solving four models of j for the tiger problem.
Beginning bottom up, we may merge the four L nodes, two OR nodes and two OL
nodes respectively to obtain the graph in (b). Because the two policy trees of two steps
rooted at L (bold circle) are identical, and so are the two policy trees rooted at L (rightmost), we may merge them, respectively, to obtain the policy graph in (c). Nodes at
t = 0 are annotated with ranges of P rj (T L).

number of comparisons are made between leaf nodes. The worst case occurs when none of the leaf
nodes of the different policy trees can be merged. Note that this precludes the merger of upper parts
of the policy trees as well. Each policy tree may contain up to |j |T 1 leaf nodes, where T is the
horizon. Hence, at most O((|j |T 1 )2 |Mj |2 ) comparisons are made, where O(|Mj |2 ) is the number of pairs in the model set. 1 The case when none of the leaf nodes merge must occur when the
models are behaviorally distinct, and they form a minimal set, Mj . In other words, Mj = Mj .

1. If we assume an ordering of the observations (edge labels) thereby ordering the tree, two policy trees may be sufficiently compared in O(|j |T 1 ) time.

225

fiZ ENG & D OSHI

Each node in the policy graph represents an action to be performed by the agent and edges
represent the agents observations. As is common with policy graphs in POMDPs, we associate
with each node at time t = 0, a range of beliefs for which the corresponding action is optimal (see
Fig. 12(c)). This range may be obtained by computing the value of executing the policy tree rooted
at each node at t = 0 in the graph and starting from each physical state. This results in a vector of
values for each policy tree, typically called the -vector. Intersecting the -vectors and projecting
the intersections on the belief simplex provides us with the boundaries of the needed belief ranges.
We utilize the policy graph to discriminate between model updates. For clarity, we formally
define a policy graph next.
Definition 3 (Policy graph). Define a policy graph as:
P G = hV, A, E, , Lv , Le i
where V is the set of vertices (nodes); A is the set of actions which form the node labels; E is the set
of ordered pairs of vertices (edges);  is the set of observations which form the edge labels; Lv :
V  A assigns to each vertex an action from the set of actions, A (node label); and Le : E  
assigns to each edge an observation from the set of observations,  (edge label). Le follows the
property that no two edges whose first elements are identical (begin at the same vertex) are assigned
the same observation.
Notice that a policy graph augments a regular graph with meaningful node and edge labels. For
a policy graph, P G, we also define the transition function, Tp : V    V, for convenience.
Tp (v, o) returns the vertex, v  , such that {v, v  }  E and Le ({v, v  }) = o.
Our insight is that Tp (v, o) is the root node of a policy tree that represents the predictive behavior for the model updated using the action Lv (v) and observation o. As we iterate over js models
in the model node at time t in the expansion phase while solving the I-DID, we utilize Tp in deciding
whether to update a model.
t
We first combine the policy trees obtained by solving the models in node Mj,l1
to obtain the
policy graph, P G, as shown in Fig. 12. Let v be the vertex in P G whose action label, Lv (v), represents the rational action for mj,l1  Mtj,l1 . We can ascertain this by simply checking whether
the belief in mj,l1 falls within the belief range associated with a node. For every observation
o  Le ({v, }), we update the model, mj,l1 , using action Lv (v) and observation o, if v  = Tp (v, o)
has not been encountered previously for this or any other model. We illustrate this below:
Example 1 (Model update). Consider the level 0 models of j in the model node at time t, Mtj,0 =
{h0.01, j i, h0.5, j i, h0.05, j i}, for the multiagent tiger problem. Recall that in a model of j, such
as h0.01, j i, 0.01 is js belief (over TL) and j is its frame. From the PG in Fig. 12(c), the leftmost
node prescribing the action L is optimal for the first and third models, while the second node also
prescribing L is optimal for the second model. Beginning with model, h0.01, j i, Tp (v, GL) = v1
(where Lv (v1 ) = L) and Tp (v, GR) = v2 (Lv (v2 ) = OL). Since this is the first model we consider,
it will be updated using L and both observations resulting in two models in Mt+1
j,0 . For the model,



h0.5, j i, if v is the optimal node (Lv (v ) = L), Tp (v , GR) = v1 , which has been encountered
previously. Hence, the model will not be updated using L and GR, although it will be updated
using L and GL.
226

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Intuitively, for a model, mj,l1 , if node v1 = Tp (v, o) has been obtained previously for this or
some other model and action-observation combination, then the update of mj,l1 will be BE to the
previously updated model (both will have the same policy tree rooted at v1 ). Hence, mj,l1 need not
be updated using the observation o. Because we do not permit updates that will lead to BE models,
the set of models obtained at t + 1 is minimal. Applying this process analogously to models in the
following time steps will lead to minimal sets at all subsequent steps and nesting levels.
4.2.2 A PPROXIMATION
We may gain further efficiency by avoiding the solution of all models in the model node at the first
time step. One way of doing this is to randomly select K models of j, such that K  |M0j,l1 |.
Solution of the models will result in K policy trees, which could be combined as shown in Fig. 12
to form a policy graph. This policy graph is utilized to discriminate between the model updates.
Notice that the approach becomes exact if the optimal solution of each model in M0j,l1 is identical
to that of one of the K models. Because the K models are selected randomly, this assumption is
implausible and the approach is likely to result in a substantial loss of optimality that is mediated
by K.
We propose a simple but effective refinement that mitigates the loss. Recall that models whose
beliefs are spatially close are likely to be BE (Rathnasabapathy et al., 2006). Each of the remaining
|M0j,l1 |  K models whose belief is not within   0 of the belief of any of the K models will
also be solved. This additional step makes it more likely that all the behaviorally distinct solutions
will be generated and included in forming the policy graph. If  = 0, all models in the model node
will be solved leading to the exact solution, while increasing  reduces the number of solved models
beyond K. One measure of distance between belief points is the L1 based metric, though other
metrics such as the Euclidean distance may also be used.
4.3 Transfer of Probability Mass
Notice that a consequence of not updating models using some action-observation combination is
that the probability mass that would have been assigned to the updated model in the model node at
t+1 is lost. Disregarding this probability mass may introduce error in the optimality of the solution.
We did not perform the update because a model that is BE to the potentially updated model
already exists in the model node at time t+1. We could avoid the error by transfering the probability
mass that would have been assigned to the updated model on to the BE model.
t+1
t+1
As we mentioned previously, the node M od[Mj,l1
] in the model node Mj,l1
, has as its values
t+1
the different models ascribed to agent j at time t + 1. The CPT of M od[Mj,l1 ] implements the
t+1
t+1
t
t
function  (btj,l1 , atj , ot+1
j , bj,l1 ), which is 1 if bj,l1 in the model mj,l1 updates to bj,l1 in model




t+1
t+1
mt+1
j,l1 using the action-observation combination, otherwise it is 0. Let mj,l1 = hbj,l1 , j i be the
model that is BE to mt+1
j,l1 . In order to transfer the probability mass to this model if the update is


t+1
pruned, we modify the CPT of M od[Mj,l1
] to indicate that mt+1
j,l1 is the model that results from
t+1
t
t
updating bj,l1 with action, aj and observation oj . This has the desired effect of transfering the


probability that would have been assigned to the updated model (Fig. 6) on to mt+1
j,l1 in the model
node at time t + 1.
227

fiZ ENG & D OSHI

4.4 Algorithm
We present the discriminative update based algorithm for solving a level l  1 I-DID (as well as a
level 0 DID) in Fig. 13. The algorithm differs from the exact approach (Fig. 10) in the expansion
phase. In addition to a two time-slice level l I-DID and horizon T , the algorithm takes as input
the number of random models to be solved initially, K, and the distance, . Following Section 4.2,
we begin by randomly selecting K models to solve (lines 2-5). For each of the remaining models,
we identify one of the K solved model whose belief is spatially the closest (ties broken randomly).
If the proximity is within , the model is not solved  instead, the previously computed solution
0
is assigned to the corresponding action node of the model in the model node, Mj,l1
(lines 6-12).
Subsequently, all models in the model node are associated with their respective solutions (policy
trees), which are merged to obtain the policy graph (line 13), as illustrated in Fig. 12.
In order to populate the model node of the next time step, we identify the node v in P G that
represents the optimal action for a model at time t. The model is updated using the optimal action
aj (= Lv (v)) and each observation oj only if the node, v  = Tp (v, oj ) has not been encountered
in previous updates (lines 16-23). Given a policy graph, evaluating Tp (v, oj ) is a constant time
t+1
operation. Otherwise, as mentioned in Section 4.3, we modify the CPT of node, M od[Mj,l1
],
to transfer the probability mass to a BE model (line 25). Consequently, model nodes at subsequent
time steps in the expanded I-DID are likely populated with minimal sets. Given the expanded I-DID,
its solution may proceed in a straightforward manner as shown in Fig. 10.

4.5 Computational Savings and Prediction Error Bound
The primary complexity of solving I-DIDs is due to the large number of models that must be solved
over T time steps. At some time step t, there could be |M0j,l1 |(|Aj ||j |)t many models of the
other agent j, where |M0j,l1 | is the number of models considered initially. The nested modeling
further contributes to the complexity since solutions of each model at level l  1 requires solving
the lower level l  2 models, and so on recursively up to level 0. In an N +1 agent setting, if the
number of models considered at each level for an agent is bound by |M|, then solving an I-DID at
level l requires the solutions of O((N |M|)l ) many models. Discriminating between model updates
reduces the number of agent models at each level to at most the size of the behaviorally minimal
set, |Mt |, while incurring the worst-case complexity of O((||T 1 )2 |M|2 ) in forming the policy
graph (Proposition 2). Consequently, we need to solve at most O((N |M |)l ) number of models
at each non-initial time step, where M is the largest of the minimal sets across levels. 2 This is
in comparison to O((N |M|)l ), where M grows exponentially over time. In general, M  M,
resulting in a substantial reduction in the computation. Additionally, a reduction in the number of
models in the model node also reduces the size of the interactive state space, which makes solving
the I-DID more efficient.
0
, in order to form the policy
If we choose to solve all models in the initial model node, Mj,l1
graph, all sets of models at subsequent time steps will indeed be minimal. Consequently, there is no
loss in the optimality of the solution of agent is level l I-DID.
For the case where we select K < |M0j,l1 | models to solve, if  is infinitesimally small, we
will eventually solve all models resulting in no error. With increasing values of , larger numbers
2. As we discuss in Section 7, we may group BE models across agents as well due to which the number of models to be
solved further reduces.

228

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

I-DID A PPROX BE (level l  1 I-DID or level 0 DID, T , K, )
1. If l  1 then
Selectively solve M0j,l1
2. Randomly select K models from M0j,l1
3. For each mkj in the K models do
4.
Recursively call algorithm with the l  1 I-DID (or DID) that represents mkj ,
the horizon T , K, and 
5.
Map the decision node of the solved I-DID (or DID), OPT(mkj ), to the chance node Akj
6. For each mkj in the |M0j,l1 |  K models do
7.
Find model among K whose belief, bkj , is closest to bkj in mkj
8.
If ||bkj  bkj ||1   then
9.
Map the decision node of OPT(mkj ) to the chance node, Akj
10.
else
11.
Recursively call algorithm with the l  1 I-DID (or DID) that represents mkj ,
the horizon, T , K, and 
12.
Map the decision node of the solved I-DID (or DID), OPT(mkj ), to the chance node Akj
13. Combine the solutions (policy trees) of all models bottom up to obtain the policy graph, P G
Expansion Phase
14. For t from 0 to T  2 do
15.
If l  1 then
t+1
Populate Mj,l1
minimally
16.
For each mtj in Mtj,l1 do
17.
For each aj in OPT(mtj ) do
18.
For each oj in j (part of mtj ) do
19.
v  vertex in P G to which mtj maps
20.
If Tp (v, oj ) not been encountered previously then
 SE(btj , aj , oj )
21.
Update js belief, bt+1
j
t+1
22.
mj  New I-DID (or DID) with bt+1
as belief
j

t+1
t+1
23.
Mj,l1  {mj }
24.
else
t+1
25.
Update CPT of M od[Mj,l1
] s. t. row mtj , aj , oj has a 1 in column
of BE model
t+1
t+1
t
26.
Add the model node, Mj,l1
, and the model update link between Mj,l1
and Mj,l1
27.
Add the chance, decision, and utility nodes for t + 1 time slice and the dependency
links between them
28.
Establish the CPTs for each chance node and utility node
The solution phase proceeds analogously as in Fig. 10

Figure 13: Algorithm for approximately solving a level l  1 I-DID or level 0 DID expanded over
T time steps using discriminative model updates.

of models remain unsolved and could be erroneously associated with existing solutions. In the
worst case, some of these models may be behaviorally distinct from all of the K solved models.
Therefore, the policy graph is a subgraph of the one in the exact case, and leads to sets of models
229

fiZ ENG & D OSHI

that are subsets of the minimal sets. Additionally, lower-level models are solved approximately as
well. While we seek to possibly bound the prediction error, its impact on the optimality of agent is
level l I-DID is difficult to pinpoint. We formally define the error and discuss bounding it including
some limitations on the bound, in Appendix A.

5. Grouping Models Using Action Equivalence
Grouping BE models may significantly reduce the given space of other agents models in the model
node without loss in optimality. We may further compact the space of models in the model node
by observing that behaviorally distinct models may prescribe identical actions at a single time step.
We may then group together these models into a single equivalence class. In comparison to BE, the
equivalence class includes those models whose prescribed action for the particular time step is the
same, and we call it action equivalence. We define it formally next.
5.1 Action Equivalence
Notice from Fig. 12(c) that the policy graph contains multiple nodes labeled with the same action
at time steps t = 0 and t = 1. The associated models while prescribing actions that are identical at
a particular time step, differ in the entire behavior. We call these models actionally equivalent. For
a general case, we define action equivalence (AE) below:
Definition 4 (Action equivalence). Two models, mj,l1 and mj,l1 , of the other agent are actionally

1
equivalent at time step t if P r(Atj ) = P r(Atj ) where P r(atj ) = |OPT(m
if atj  OPT(mj,l1 ),
j,l1 )|


0 otherwise; and P r(atj ) =

1
|OPT(mj,l1 )|



if atj  OPT(mj,l1 ), 0 otherwise, as defined previously.

Since AE may include behaviorally distinct models, it partitions the model space into fewer
classes.
We show an example aggregation of AE models in Fig. 14. From the figure, the partition of the
t=0,2
t=0,1
model set, Mtj,l1 , induced by AE at time step 0 is {Mt=0,1
j,l1 , Mj,l1 }, where Mj,l1 is the class
of models in the model space whose prescribed action at t = 0 is L, and Mt=0,2
j,l1 is the class of
models whose prescribed action at t = 0 is OR. Note that these classes include the BE models as
well. Thus, all models in an AE class prescribe an identical action at that time step. Furthermore at
t = 1, the partition consists of 3 AE classes and, at t = 2, the partition also consists of 3 singleton
classes.
t,p
t
If Mt,p
j,l1 is an AE class comprising of models mj,l1  Mj,l1 , agent is conditional belief
over it is obtained by summing over is conditional belief over its member models:
bi (Mt,p
j,l1 |s) =

X

bi (mtj,l1 |s)

(2)

mtj,l1 Mt,p
j,l1

5.2 Revised CPT of Mod Node and its Markov Blanket
t
Equation 2 changes the CPT of the node, M od[Mj,l1
], due to the aggregation. Chang and Fung
(1991) note that a coarsening operation of this type will not affect the distributions that do not

230

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Mj,l-1t=0,1
0.16

0.23

L
GR

0.32
L

L

GR

GL

OL

L
GR*

OL

GL*

GR

GL

L

GL

OR

GR

0.29
time t=0

L

OR

time t=1

OL

* GL *

GL,0.23 GL,0.32

time t=2

OL

GL

OR

GR,0.45

L

L
GR*

OR

L

L

L

GR,0.23 GR,0.32

L

Mj,l-1t=0,2

GL,0.45 *,1.0

OR

L

GR

L

*GL

*

OR

(b)

(a)

L

GR,0.23

L

L

OR

GR,0.32 GL,0.23 GL,0.32 GR,0.45
GL,0.45 *,1.0

OL

*,1.0

L

L

GL,0.69 GL,0.31

GR,0.22 GR,0.78

OL

OR

L

*,1.0

OR

L

(c)

Figure 14: (a) Annotations are example probabilities for the models associated with the nodes. (b, c) We
may group models that prescribe identical actions into classes as indicated by the dashed boxes.
Probabilities on the edges represent the probability of transition from a class to a model given
action and observation (ie., CPT of the next M od node).

Ajt

St

Mod[Mjt]

Mod[Mjt+1]

Ojt+1

t
Figure 15: Markov blanket of the chance node, M od[Mj,l1
], shown in bold.

directly involve the model space if the joint probability distribution over the Markov blanket of node,
t
t
M od[Mj,l1
], remains unchanged. In Fig. 15, we show the Markov blanket of M od[Mj,l1
]. 3
The joint distribution over the Markov blanket is:
P
t+1
t t+1
P r(st , atj , ot+1
, mt+1
) = p P r(st , Mt,p
j
j,l1
j,l1 , aj , oj , mj,l1 )
P
t,p
t,p
t+1 t
t t+1
|Mt,p
= p P r(st )P r(Mj,l1 |st )P r(atj |Mj,l1 )P r(mt+1
j,l1
j,l1 , aj , oj )P r(oj |aj )
P
t,p
t,p
t,p
t+1
t+1
= P r(st )P r(oj |atj ) p P r(Mj,l1 |st )P r(atj |Mj,l1 )P r(mj,l1 |Mj,l1 , atj , ot+1
j )
P P
t)
t
t )P r(at |Mt,p )
t,p
= P r(st )P r(ot+1
|a
P
r(m
|s
j
j
p
j
j,l1
j,l1
mt
M
j,l1

t,p
t t+1
P r(mt+1
j,l1 |Mj,l1 , aj , oj )

(3)

j,l1

(from Eq. 2)

t
] to Ojt+1 thus
3. Because we assume that agents frames do not change, we may remove the arc from M od[Mj,l1
simplifying the blanket.

231

fiZ ENG & D OSHI

t
The joint distribution prior to aggregation of M od[Mj,l1
] is:
t+1
t+1 t
t
P r(st , atj , ot+1
j , mj,l1 ) = P r(s )P r(oj |aj )
t
t t+1
P r(mt+1
j,l1 |mj,l1 , aj , oj )

P

mtj,l1 Mtj,l1

P r(mtj,l1 |st )P r(atj |mtj,l1 )

(4)
We equate the right hand sides of Eqs. 3 and 4 to obtain the constraint that must be satisfied by
the CPTs of some of the chance nodes in the Markov blanket in order for the joint distribution to
remain unchanged:
P
t,p
t,p
t+1
t
t t+1
P r(mtj,l1 |st )
p P r(aj |Mj,l1 )P r(mj,l1 |Mj,l1 , aj , oj )
mtj,l1 Mt,p
j,l1
P
t+1
t
t
t
t
t
t t+1
mtj,l1 Mtj,l1 P r(mj,l1 |s )P r(aj |mj,l1 )P r(mj,l1 |mj,l1 , aj , oj )

P

=

(5)

t+1
Notice that Eq. 5 imposes a constraint on the CPTs of the successor nodes, Atj and M od[Mj,l1
].
t+1
t
If the constraint is satisfied  a setting for the CPTs of the nodes, Aj and M od[Mj,l1 ], is found
 grouping of AE models in the initial model node is exact and the optimality of the I-DID is
preserved. An obvious way to satisfy Eq. 5 would be to meet the following intuitive constraint for
each AE class p:
t,p
t+1
t t+1
P r(atj |Mt,p
j,l1 )P r(mj,l1 |Mj,l1 , aj , oj ) =
P

t,p
mt
M
j,l1
j,l1

t
t t+1 )
P r(mtj,l1 |st )P r(atj |mtj,l1 )P r(mt+1
j,l1 |mj,l1 ,aj ,oj
P
P r(mtj,l1 |st )
t,p
t

(6)

m
M
j,l1
j,l1

t,p
P r(atj |mtj,l1 ) is fixed for each model, mtj,l1 , in AE class Mj,l1
and equals P r(atj |Mt,p
j,l1 ).
Therefore, Eq. 6 reduces to:

t,p
t t+1
P r(mt+1
j,l1 |Mj,l1 , aj , oj )

=

P

t,p
mt
M
j,l1
j,l1
P

t
t t+1 )
P r(mtj,l1 |st )P r(mt+1
j,l1 |mj,l1 ,aj ,oj

t,p
mt
M
j,l1
j,l1

P r(mtj,l1 |st )

(7)

Observe that Eq. 7 must hold for all values of the physical state, st . If the right hand side of
t+1
the above equation remains unchanged for any value of st , we may set the CPT of M od[Mj,l1
]
using it. Typically, it is not trivial  often not possible  to find a single CPT for the chance node
t+1
M od[Mj,l1
] that will satisfy the constraint for all st . Chang and Fung (1991) demonstrate that a
close approximation would be to take the average of the right hand side of Eq. 7 over all possible
values of st if these values are close.
t+1
Of course, we may wish to aggregate models in node, M od[Mj,l1
], as well (and so on). While
the overall procedure is analogous, the difference is in the Markov blanket of the node that is to be
t
aggregated. It includes the predecessor chance nodes, M od[Mj,l1
], Atj , and Ojt+1 in addition to its
successors and parents of successors corresponding to those in Fig. 15.
We illustrate the application of Eq. 7 to the example policy graph in Fig. 14(a) below:
Example 2 (Model update). For simplicity, let the left AE class, Mt=0,1
j,l1 , comprise of three models,
t=0,2
t=0,3
mt=0,1
j,l1 , mj,l1 and mj,l1 , all of which prescribe action L. Let is belief over these three models

232

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

be 0.16, 0.23 and 0.32 given physical state TL or TR, respectively (see Fig. 14(a)). We set the
probability of updating say, Mt=0,1
j,l1 , using different action-observation combinations to individual
models at time t=1, using Eq. 7. We show these probabilities in Fig. 14(b); these form the CPT of
t=1 ]. Because P r(mt=0,1 |s) remains same given any s, constraint of Eq. 7 is met
node M od[Mj,l1
j,l1
and the AE based partitioning at t=0 is exact.
Next, we group AE models at t=1 forming 3 AE classes as shown in Fig. 14(c). Again, we may
set the probabilities of updating an AE class given action-observation combinations to individual
models at t=2 using the right hand side of Eq. 7. However, doing so does not meet the constraint
t=0,p t=0 t=1
represented by Eq. 7 because for example, P r(mt=1,1
j,l1 |Mj,l1 , aj , oj ) varies given different
values of the conditionals. As an aside, it is not possible to meet this constraint in this example.
t=2 ], according to the average of the
Consequently, we adjust the CPT of the chance node, M od[Mj,l1
right hand side of Eq. 7 for different values of the conditional variables (see Fig. 14(c)). As a result,
the AE based partitioning at t=1 is not exact.

A manifestation of the approximation is that agent i may now think that j could initially open
the right door, followed by listening and then open the left or right door again. Such a sequence of
actions by j was not possible in the original policy graph shown in Fig. 14(a).
5.3 Algorithm
We provide an algorithm for exploiting AE in order to solve a level l  1 I-DID (as well as a level
0 DID) in Fig. 16. The algorithm starts by selectively solving lower-level I-DID or DID models at
t = 0, which results in a set of policy trees (line 2). We then build the policy graph by merging
the policy trees as mentioned in lines 1-13 of Fig 13. The algorithm differs from Fig. 13 in the
expansion phase. In particular, we begin by grouping together AE models in the initial model
node. This changes the value of the initial M od node to the AE classes (lines 3-9). Subsequently,
updated models that are AE are aggregated at all time steps, and the CPTs of the M od nodes are
revised to reflect the constraint involving AE classes (lines 13-24). As mentioned in Section 5.2,
AE partitioning becomes inexact if we cannot find a CPT for the successor M od node that satisfies
Eq. 7. Given the expanded I-DID, we use the standard look-ahead and backup method to get the
solution.
5.4 Computational Savings
As we mentioned, the complexity of exactly solving a level l I-DID is, in part, due to solving the
lower-level models of the other agent, and given the solutions, due to the exponentially growing
space of models. In particular, at some time step t, there could be at most |M0j,l1 |(|Aj ||j |)t
many models, where M0j,l1 is the set of initial models of the other agent. While K  |M0j,l1 |
models are solved, considering AE bounds the model space to at most |Aj | distinct classes. Thus,
the cardinality of the interactive state space in the I-DID is bounded by |S||Aj | elements at any
time step. This is a significant reduction in the size of the state space. In doing so, we additionally
incur the computational cost of merging the policy trees, which is O((|j |T 1 )2 |M0j,l1 |2 ) (from
Proposition 2). We point out that this approach is applied recursively to solve I-DIDs at all levels
down to 1, as shown in the algorithm.
233

fiZ ENG & D OSHI

I-DID A PPROX AE (level l  1 I-DID or level 0 DID, T , K, )
1. If l  1 then
2. Selectively solve models in M0j,l1 as in Fig. 13 to obtain the policy graph, P G
Expansion Phase
3. If l  1 then
4. For each m0j in M0j,l1 do
5.
v  vertex in P G to which m0j maps
6.
If aj  Lv (v) has not been encountered previously
0,aj
7.
Initialize AE class Mj,l1
0,aj 
8.
Mj,l1  {m0j }
0
9. Mj,l1  Set of all AE classes
10. For t from 0 to T  2 do
11.
If l  1 then
t+1
Populate Mj,l1
using AE classes
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.

t,a

j
in Mtj,l1 do
For each Mj,l1
t,aj
do
For each mtj in Mj,l1
For each aj in OP T (mtj ) do
For each oj in j do
v  vertex in P G to which mtj maps
If aj  Lv (Tp (v, oj )) not been encountered previously then
t+1,a
Initialize AE class Mj,l1 j
t+1
Update js belief, bj  SE(btj , aj , oj )
mt+1
 New I-DID (or DID) with bt+1
as belief
j
j
t+1,aj 
}
Mj,l1  {mt+1
j
Mt+1
j,l1  Set of all AE classes
Update CPT of node M od[Mt+1
j,l1 ] to meet the constraint specified by Eq. 7
if possible, otherwise take average
t+1
t+1
t
Add the model node, Mj,l1
, and the model update link between Mj,l1
and Mj,l1
Add the chance, decision, and utility nodes for t + 1 time slice and the dependency
links between them
Establish the CPTs for each chance node and utility node

The solution phase proceeds analogously as the one in Fig. 10.

Figure 16: Algorithm for possibly inexactly solving a level l  1 I-DID using action equivalence.

6. Empirical Results
We implemented the algorithms in Figs. 10, 13 and 16 and refer to the resulting techniques as
Exact-BE, DMU and AE, respectively. In addition to these, we utilize the previous approximation
technique of k-means clustering (Zeng et al., 2007), referred to as MC, and the exact approach
without exploiting BE, referred to as Exact, as baselines. In MC, models are clustered based on the
spatial closeness of their beliefs, and the clusters are refined iteratively until they stabilize. Because
I-DIDs eventually transform to flat DIDs, we implemented them as a layer above the popular ID
234

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

tool, H UGIN E XPERT V 7.0. The transformed flat DIDs and level 0 DIDs were all solved using
H UGIN to obtain the policy trees.
As benchmark problem domains, we evaluate the techniques on two well-known toy problems
and a new scalable multiagent testbed with practical implications. One of the benchmarks is the
two-agent generalization of the single agent tiger problem introduced previously in Section 3. As
we mentioned, our formulation of this problem (|S|=2, |Ai |=|Aj |=3, |i |=6, |j |=2) follows the
one introduced by Gmytrasiewicz and Doshi (2005), which differs from the formulation of Nair
et al. (2003), in not being cooperative and having door creaks as additional observations. These
observations are informative, though not perfectly, of js actions. The other toy domain is a generalization of Smallwood and Sondiks machine maintenance problem (Smallwood & Sondik, 1973)
to the two-agent domain. This problem (|S|=3, |Ai |=|Aj |=4, |i |=|j |=2) is fully described in
Appendix B.2. I-DIDs for both these problem domains are shown in Section 3 and Appendix B.2,
respectively. Decentralized POMDP solution techniques are not appropriate as baselines in cooperative problems such as machine maintenance because of the absence of a common initial belief
among agents, and I-DIDs take the perspective of an agent in the interaction instead of computing
the joint behavior.
While the physical dimensions of these problems are small, the interactive state space that includes models of the other agent is an order of magnitude larger. Furthermore, they provide the advantage of facilitating detailed analysis of the solutions and uncovering interesting behaviors as previously demonstrated (Doshi et al., 2009). However, beyond increasing horizons, they do not allow
an evaluation of the scalability of the techniques. In this context, we also evaluate the approaches
within the Georgia testbed for autonomous control of vehicles (GaTAC) (Doshi & Sonu, 2010),
which is a computer simulation framework for evaluating autonomous control of aerial robotic vehicles such as UAVs. Unmanned agents such as UAVs are used in fighting forest fires (Casbeer, Beard,
McLain, Sai-Ming, & Mehra, 2005), law enforcement (Murphy & Cycon, 1998), and wartime reconnaissance. They operate in environments characterized by multiple parameters that affect their
decisions, including other agents with common or antagonistic preferences. The task is further
complicated as the vehicles may possess noisy sensors and unreliable actuators. GaTAC provides a
low-cost and open-source alternative to highly complex and expensive simulation infrastructures.
We setup and execute experiments to evaluate the following: (a) We hypothesize that in sets
of models attributed to the other agent, several are BE. This will lead to the exact approach that
groups BE models (Exact-BE) being significantly more efficient than the plain approach (Exact).
(b) Both approximation techniques (DMU and AE) will improve on the previous approximation
technique of k-means clustering (MC). This is because MC generates all models at each time step
before clustering and furthermore MC may retain BE models. (c) Finally, between DMU and AE,
we hypothesize AE to be significantly more efficient because it forms as many classes as there are
actions only. However, the solution quality resulting from DMU and AE will be explored.
6.1 Improved Efficiency Due to BE
We report on the performance of the exact methods (Exact-BE and Exact) when used for solving
both level 1 and 2 I-DIDs formulated for the small problem domains. As there are infinitely many
computable models, we obtain the policy by exactly solving the I-DID given a finite set of models
of the other agent initially, M 0 . In Fig. 17, we show the average rewards gathered by executing the
235

fiZ ENG & D OSHI

policy trees obtained from exactly solving both level 1 and 2 I-DIDs for the two problem domains,
as a function of time allocated toward their solutions.
Each data point is the average of 200 runs of executing the policies, where the true model of the
other agent, j, is randomly selected according to is belief distribution over js models. The time
consumed is a function of the initial number of models and the horizon of the I-DID, both of which
are varied beginning with M 0 = 50 at each level.
Multiagent tiger problem
Level = 1

Level = 2
6.5

6.5

6

6

5.5

5.5

Average Reward

Average Reward

7

5
4.5
4
3.5

5
4.5
4
3.5
3

3
Exact-BE
Exact

2.5

Exact-BE
Exact

2.5

2

2
0

5

10

15

20

25

30

40

60

80

100

120

Time(s)

140

160

180

200

220

240

Time(s)

(a)

(b)

Multiagent machine maintenance problem
Level = 2
0.8

0.7

0.7
Average Reward

Average Reward

Level = 1
0.8

0.6
0.5
0.4
0.3
0.2
10

20

30

40
50
Time(s)

60

70

0.5
0.4
0.3
0.2

Exact-BE
Exact
0

0.6

80

90

(c)

0.1
100

Exact-BE
Exact
150

200

250
Time(s)

300

350

400

(d)

Figure 17: Performance profiles of the exact solutions on the multiagent tiger (a, b) and machine
maintenance problems (c, d). Higher average reward for given time is better. Exact-BE
significantly improves on the plain Exact approach at levels 1 and 2. For longer times
the Exact program runs out of memory. Vertical bars represent the standard deviation
from the mean.

From Fig. 17, we observe that Exact-BE performs significantly better than the Exact approach.
Specifically, Exact-BE obtains the same amount of reward as Exact but in less time, and subsequently, for a given allocated time, it is able to obtain larger reward than Exact. This is because it
is able to solve for a better quality solution in less time as it groups together BE models and retains
a single representative from each class, thereby reducing the number of models held in each model
node. We see significant improvement in performances in solving I-DIDs at both levels.
We uncover the main reason behind the improved performance of Exact-BE in Fig. 18. As we
may expect, after grouping of BE models Exact-BE maintains much fewer classes of models (predicting a particular behavior for the other agent) than the number of individual models maintained
by Exact. This occurs for all horizons and for both the problem domains. The number of models
236

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Multiagent tiger

Multiagent machine maintenance

50

50
Exact
Exact-BE

Exact
Exact-BE
40
Model Classes

Model Classes

40

30

20

10

30

20

10

0

0
9

8

7

6

5

4

3

2

1

9

8

Horizon

7

6

5

4

3

2

1

Horizon

Figure 18: Exact-BE maintains far fewer classes at larger horizons in comparison to the Exact approach. Notice that the number of BE classes reduces as horizon decreases because
solutions tend to involve fewer distinct behaviors. We do not show the increase in models with time for Exact for clarity.

increase as horizon reduces (but time steps increase) due to model updates; however, BE classes
reduce with smaller horizon because of less distinct behaviors in the solutions.
As we increase the number of levels beyond two and model j more strategically, we expect
Exact-BE (and Exact) to result in solutions whose average reward possibly improves but doesnt
deteriorate. However, as the number of models increases exponentially with l, we expect substantially more computational resources to be consumed making it challenging to solve for deeper
levels.
6.2 Comparative Performance of Approximation Methods
While discriminating between model updates as described in Section 4 by itself does not lead to a
loss in optimality, we combined it with the approach of solving K (which we will now call KDM U )
models out of the M 0 models, and then solving those models which are not -close to any of the
KDM U models, to form the policy graph. Thus, we initially examine the behavior of the two
parameters, KDM U and , in how they regulate the performance of the DMU-based approximation
technique for solving level 1 I-DIDs.
We show the performance of DMU for both the multiagent tiger and machine maintenance
problems in Fig. 19. We also compare its performance with an implementation of MC; KM C
represents the total number of models retained after clustering and pruning in the approach. The
performance of MC is shown as flat lines because  does not play any role in the approach. Each
data point for DMU is the average of 50 runs of executing the policies where the true model of the
other agent, j, is randomly picked according to is belief distribution over js models, and solved
exactly if possible. Otherwise, if l > 1, we solve it approximately using DMU with a large KDM U
and small . The plot is for M 0 = 100, and a horizon of 10. As we increase the number of models
randomly selected, KDM U , and reduce distance, , the policies improve and converge toward the
exact. Notice that DMU improves on the performance of MC as we reduce , for KDM U = KM C .
This behavior remains true for the multiagent machine maintenance problem as well.
We evaluate the impact of AE on solving level 1 I-DIDs for both problem domains and compare
it to MC. The experiments were run analogously as before with different values for KAE and .
Both these parameters play roles that are similar to their use in DMU. We observe from Fig. 20 that
237

fiZ ENG & D OSHI

Multiagent tiger

Multiagent machine maintenance
0.75

6.5

0.7

6

0.65
Average Reward

Average Reward

7

5.5
5
4.5
KDMU=25
KDMU=50
KMC=25
KMC=50
Exact-BE M0=100

4
3.5
3
0.90

0.70

0.50

0.30

0.10
0.09

0.6
0.55
0.5
0.45

KDMU=25
KDMU=50
KMC=25
KMC=50
Exact-BE M0=100

0.4
0.35
0.3
0.90

0.07

0.70

0.50

0.30



0.10
0.09

0.07



(a)

(b)

Figure 19: Performance profiles for DMU on horizon T =10, level 1 I-DID with M 0 =100. For
given M 0 , the performance approaches that of the exact method as KDM U increases
and  reduces. The comparison with MC indicates that better performance is achieved
by DMU-based solutions.
Multiagent tiger

Multiagent machine maintenance
0.75

6.5

0.7

6

0.65
Average Reward

Average Reward

7

5.5
5
4.5
KAE=25
KAE=50
KMC=25
KMC=50
Exact-BE M0=100

4
3.5
3
0.90

0.70

0.50

0.30

0.10
0.09

0.6
0.55
0.5
0.45

KAE=25
KAE=50
KMC=25
KMC=50
Exact-BE M0=100

0.4
0.35
0.07

0.3
0.90

0.70

0.50

0.30



0.10
0.09

0.07



(a)

(b)

Figure 20: Performance profiles for AE on horizon T =10, level 1 I-DID with M 0 =100. For given
M 0 , the performance approaches that of the exact method as KAE increases and  reduces. Comparative performance in relation to MC indicates that AE is capable of
achieving better quality solutions (although for relatively small values of ).

as  reduces and more models beyond KAE are solved, the solution generated by AE improves on
MC for the case where KAE = KM C . Of course, as we solve more models initially, AE produces
better quality solutions because the generated policy graph includes more parts of the exact graph.
Additionally, as we may expect, the solution quality approaches that of the exact and becomes
significantly close to the exact (within one standard deviation for the tiger problem) for  < 0.1.
While our previous experiments demonstrated that both DMU and AE are capable of improving on MC, it is not clear how many more initial models beyond KM C were solved to obtain the
improvements. Furthermore, performance of DMU and AE were not compared. In Fig. 21, we
directly compare the performance of DMU, AE and MC. In particular, we measure the average
rewards obtained by corresponding solutions of level 2 I-DIDs as a function of time consumed by
238

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Multiagent tiger

Multiagent machine maintenance

6.5

0.65

6
0.6
Average Reward

Average Reward

5.5
5
4.5
4
3.5

0.5

0.45

AE
DMU
MC

3

0.55

2.5

AE
DMU
MC

0.4
40

60

80

100

120

140

160

180

200

220

240

Time(s)

60

80

100

120

140

160

180

200

220

240

Time(s)

Figure 21: Performance comparison of approximation techniques for solving level 2 I-DIDs. AE
achieves significantly improved efficiency for identical quality solutions over the two
domains.

the approaches. For DMU and AE, the time taken is dependent on the parameters (K and ), tree
merging and the horizon of the I-DID solved. For MC, the time is due to the iterative clustering
until convergence and the K models that are picked. For the problem domains considered, larger
horizon with increasing K and reducing  typically leads to better average rewards. We observe
that both DMU and AE significantly improve on MC  they produce identical quality solutions in
less time than that by MC. Furthermore, between DMU and AE, the latters performance is more
favorable. This is because, while grouping AE models may result in an additional approximation,
further efficiency is made possible by the fewer AE classes and whose number does not exceed a
constant across horizons.
We empirically explore the reason behind the comparative performance of the approximation
techniques. Because the time (and space) consumed by the approaches is predominantly due to the
solution of the models in the model node, we focus on the models retained by the approaches at
different horizons. Fig. 22 shows the models at different horizons for varying  and for both the
problem domains. Note that when  = 0, all initial models are solved and in the case of DMU,
results in the behaviorally minimal set at every horizon. Furthermore, as we mentioned previously,
for non-zero , the merged policy graph is a subgraph of the exact ( = 0) case. As we show, the
resulting sets of models are subsets of the minimal set.
At any horizon, AE maintains no more model classes than the number of js actions, |Aj |.
As we see, this is substantially less than the number maintained by DMU. MC maintains a fixed
number, KM C , of models at each horizon of the I-DID.
6.3 Runtime Comparison
We show the run times of the exact and approximation techniques for solving level 1 and 2 I-DIDs
while scaling in horizons, in Table 1. Notice that a plain exact approach that does not exploit
model equivalences scales poorly beyond small horizons. In contrast, simply grouping BE models
and reducing the exponential growth in models leads to significantly faster executions and better
scaleup. The run times are reported for both these approaches solving the same I-DID exactly.
In obtaining the run times for the approximations, we adjusted the corresponding parameters so
that the quality of the solution by each approach was similar to each other. DMU and AE reported
239

fiZ ENG & D OSHI

Multiagent tiger
8

8
Exact-BE
DMU =0.09
DMU =0.5
DMU =0.9

7

6
Model classes

Model Classes

6

Exact-BE
AE =0.09
AE =0.5
AE =0.9

7

5
4
3
2

5
4
3
2

1

1

0

0
9

8

7

6

5

4

3

2

1

9

8

7

6

Horizon

5

4

3

2

1

Horizon

(a)

(b)
Multiagent machine maintenance

6

6
Exact-BE
DMU =0.09
DMU =0.5
DMU =0.9

Exact-BE
AE =0.09
AE =0.5
AE =0.9

5

4

Model classes

Model classes

5

3
2
1

4
3
2
1

0

0
9

8

7

6

5

4

3

2

1

9

Horizon

8

7

6

5

4

3

2

1

Horizon

(c)

(d)

Figure 22: Number of models maintained in the model node for different horizon in a level 1 I-DID
for the multiagent tiger (a, b) and machine maintenance (c, d) problems. For DMU, as
 reduces, the model space approaches the minimal set. Aggregation using AE further
reduces the model space.

substantially less execution times and better scaleup in comparison to MC for both the domains.
However, the run times of DMU and AE are relatively similar for level 1 I-DIDs. Although, as we
saw previously, there is a difference in the number of models maintained by the two techniques,
solving js level 0 DIDs is quick and the difference does not lead to a significant impact. The
differences in run times are more significant for level 2 I-DIDs because solving js level 1 I-DIDs is
computationally intensive. As we may expect, AE consumes substantially less time in comparison
to DMU  sometimes less than half. Both approaches scale up similarly in terms of the horizon. In
particular, we are able to solve both level 1 and 2 I-DIDs for more than a horizon of 10. Further
scaleup is limited predominantly due to our use of software such as H UGIN for solving the flat
DIDs, which seeks to keep the entire transformed DID in main memory.
6.4 Scalable Testbed: GaTAC
As we mentioned, the objective behind developing GaTAC is to provide a realistic and scalable
testbed for algorithms on multiagent decision making. GaTAC facilitates this by providing an intuitive and easy to deploy architecture that makes use of powerful, open-source software components.
Successful demonstrations of algorithms in GaTAC would not only represent tangible gains but have
the potential for practical applications toward designing autonomous vehicles such as UAVs. 4
4. GaTAC is available for download at http://thinc.cs.uga.edu/thinclabwiki/index.php/
GaTAC_:_Georgia_Testbed_for_Autonomous_Control_of_Vehicles

240

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Level

Domain

T
4
8
14
4
6
12
3
6
10
4
6
10

Tiger
Level 1
MM

Tiger
Level 2
MM

Exact
5.8s
*
*
6.66s
*
*
3m 24s
*
*
5m12s
*
*

Exact-BE
0.47s
10.5s
2h 4m
0.45s
1.73s
9m 40s
10.97s
22m 6s
2h 48m
1.11s
13.59s
20m 36s

DMU
0.13s
1.27s
15m 6s
0.19s
0.53s
2m 9s
4.63s
6m 54s
27m 36s
0.33s
4.3s
3m 36s

AE
0.42s
1.64s
15m 15s
0.22s
0.58s
2m 12s
3.11s
3m 3s
16m 54s
0.58s
1.48s
2m 15s

MC
2.12s
28.45s
*
3.23s
9.88s
*
1m 46s
*
*
2m 21s
*
*

Table 1: Exploiting model equivalences has a significant impact on the execution times. Both DMU
and AE demonstrate improved efficiency. Algorithm involving AE scales significantly
better to larger horizons for deeper strategy levels. All experiments are run on a WinXP
platform with a dual processor Xeon 2.0GHz and 2GB memory. * indicates that the data
point is unavailable because the program ran out of memory.

< Socket, port >

Low-level control

High-level control

Communication
Module
(UDP)
Flight Simulator
Instance #1

< Socket, port >

Autonomous
Control Module
High-level state

Flight dynamics

Host 1
Host 2

Flight Simulator
Server

High-level state
Flight Simulator
Instance #2

Communication
Module
< Socket, port >
(UDP)

Flight dynamics

Host 3

manual control

(a)

(b)

Figure 23: (a) Design of GaTAC showing two networked instances of a flight simulator (FlightGear
with 3D scenery from TerraGear), one autonomously and other manually controlled.
GaTAC is extensible and more instances may be added. (b) Snapshot of a UAV flying
within FlightGear. Different viewpoints including an external view as shown and a
cockpit view are available.

A simplified design of the GaTAC architecture is shown in Fig. 23, where a manually controlled
UAV is interacting with an autonomous one. Briefly, GaTAC employs multiple instances of an opensource flight simulator, called FlightGear (Perry, 2004), possibly on different networked platforms
that communicate with each other via external servers, and an autonomous control module that
interacts with the simulator instances. GaTAC can be deployed on most platforms including Linux
241

fiZ ENG & D OSHI

and Windows with moderate hardware requirements, and the entire source code is available under
GNU Affero public license version 3.
We utilize a relatively straightforward setting consisting
of another hostile fugitive, who is the target of ground reconnaissance (Fig. 24). UAV I must track down the fugitive
before he flees to the safe house. The problem is made complex by assuming that the fugitive is unaware of its own precise location though it knows the location of the safe house,
and I may not be aware of the fugitives location. The problem is further complicated if we realistically assume nondeterministic actions and observations. Our simulations in
I
GaTAC include grids of sizes 3  3 and 5  5 with the same
actors in each. GaTAC may be programmed to support more
complex scenarios comprising of a team of UAVs, multiple
hostile UAVs and reconnaissance targets attempting to blend Figure 24: Example 5  5 theater
of UAV I, which perin with civilians.
forms low-altitude reconWe summarize our formulation of the UAVs problem
naissance of a potentially
domain. We utilize the possible relative positions of the
hostile theater populated
fugitive as states. Hence, possible states would be same,
by a fugitive, J.
north, south, east, west, north-west, and so on. Our representation for a 3  3 theater consists of 25 physical states
for the UAV I. We assume that the fugitive is unaware of its own location resulting in 9 physical
states for it. Extending the theater to a 5  5 grid leads to 81 physical states for the UAV and 25
for the fugitive. We factor the physical state into two variables in the I-DID that model the row and
column positions, respectively. Both UAV I and the fugitive may move in one of the four cardinal
directions, and they may additionally hover at their current positions and listen to get informative
observations. Thus the actions for both I and the fugitive are {move north, move south, move west,
move east, listen}. We may synchronize the actions for the two agents in GaTAC by allocating
equal time duration to the performance of each action. Typically, UAVs have infrared and camera
sensors whose range is limited. Accordingly, we assume that both the UAV I and the fugitive can
sense whether their respective target is north of them (sense north), south of them (sense south),
west or east of them in the same row (sense level) or in the same location as them (sense found).
For I the target is the fugitive, while the fugitives target is the safe house. If we assume that the
fugitive is unaware of Is presence, its transition function is straightforward and simply reflects the
possible nondeterministic change in grid location of the fugitive as it moves or listens. However,
transitions in physical state of I are contingent on the joint actions of both agents. Furthermore, the
probability distribution over the next states is not only due to the nondeterminism of the actions, but
is also influenced by the current relative physical state. To provide an opportunity for the UAV I
to catch the fugitive, we assume that the fugitive can sense the safe house only when it is within a
distance of 1 sector (horizontally or vertically) from it. On the other hand, UAV Is observations of
the fugitive are not limited by this constraint. Thus, if the fugitive is in any location that is north of I
(including north-west or north-east), I receives an observation of sense north. To simulate noise in
the sensors, we assume that the likelihood of the correct observation is 0.8 while all others are equiprobable. The reward function is straightforward with the fugitive receiving a reward if its location
is identical to that of the safe house, and small costs for performing actions to discourage excessive
242

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

action taking. Analogously, UAV I recieves a reward on performing an action and receiving an
observation of sense found, and incurs small costs for actions that lead to other observations.
Listen

Listen

Move_
North

Sense_
North

Sense_
South

Sense_
Level

Safe

Sense_ Sense_
North
South

*

Move_
North

Listen

Move_
West

Listen

*

*

Move_
North

Listen

Listen
*

Sense_ Sense_
North
South

Move_
North

Listen

Sense_
Level

Move_
West

Safe

Move_
West

Listen

Move_
North

Sense_ Sense_
North
South

Listen

Sense_
Level

Safe

Move_
West

Sense_
Level

Found

Move_
North

Move_
South

Move_
West

Listen

*

*

*

Sense_ Sense_
Found
North
Level

Move_
North

Listen

Move_
East

Move_
North

Listen

Listen

Listen

(b)

(a)

Figure 25: (a) Example policies of the fugitive modeled by the UAV. (b) UAV Is optimal policy of
pursuing the fugitive obtained by solving the level 1 I-DID exactly using BE. The policy
is straightforward, using observations to guide the actions. All policies are for a 3  3
grid.

We modeled the problem formulation described above using a level 1 I-DID for the UAV I and
level 0 models for the fugitive. We show two example policies of the fugitive obtained by solving
its level 0 models, in Fig. 25(a). While we considered several models for the fugitive with differing
initial beliefs, the fugitives initial belief of likely being just below the safe house results in the left
policy, while its initial belief of likely being south and east of the safe house leads to the policy on
the right. We show the UAVs policy of reconnaissance in Fig. 25(b), obtained by solving its level 1
I-DID exactly while utilizing BE classes. Thirty models of the fugitive grouped into 16 BE classes
were considered in the I-DID. Here, the UAV initially believes that the fugitive is likely to be in the
same row or south of it.
We simulate the reconnaissance theaters of Fig. 24 in GaTAC. The UAV and the fugitives
behaviors are controlled by their respective policies provided as input to the autonomous control
module. For each simulation run, we generated the UAVs policy by solving its level 1 I-DID using
either Exact-BE, DMU or AE, and sampled one of the fugitives 30 models based on the UAVs
initial belief. The run terminates when either the fugitive reaches the safe house, or the UAV spots
the fugitive by entering the same sector as the fugitive. In the case of DMU and AE, we used
parameters, K = 13 and  = 0.3 for the 3  3 problem size, and K = 17,  = 0.15 for the 5  5
problem. We show the average reward gathered by the UAV across 20 simulation runs for each of
the three approaches in Fig. 26(a) and the associated clock time for solving the I-DIDs in Fig. 26(b).
While we considered several different beliefs for the UAV, positioning itself approximately between
the fugitive and the safe house yielded a fugitive capture rate of 65% among the simulation runs and
an escape rate of 25%. The remaining runs did not result in a capture or an escape.
While exactly solving the I-DID using Exact-BE continues to provide the largest reward among
all approaches, as shown in Fig. 26(a), it fails to scale to a longer horizon or problem size. Both
DMU and AE scale, although AE performs worse than DMU in the context of reward in this domain.
Note that longer horizons result in overall better quality policies in this problem domain, as we may
expect. This is because the UAV is able to plan its initial action better. Finally, the improved reward
obtained by AE relative to DMUs as the horizon increases from 6 to 8 for the 3  3 grid, and
243

fiAverage Reward

Z ENG & D OSHI

50
40

ExactBE
DMU
AE

Problem size

30

33 grid

20
10
0

3

6

8

3x3

55 grid

6

T
3
6
8
6
8

Exact-BE
1m 21s
4m 32s
*
*
*

DMU
8.7s
28s
3h 13m
10m 18s
11h 14m

AE
7.13s
19.24s
23m 54s
2m 12s
41m 30s

5x5

Horizon(T)

(a)

(b)

Figure 26: (a) Simulation performance of the UAV in GaTAC when its level 1 I-DID is solved using
the different approaches and over longer horizons. We scaled the grid size from 3  3
to 5  5. Notice that Exact-BE fails to scale as the horizon and problem size increase
and we do not show its reward. (b) Execution times for solving the level 1 I-DID using
the different approaches. Although AE results in solutions of lower quality compared to
DMU, it does so in much less time. For the longer horizon of 8, this time difference is
about an order of magnitude.

the slight climb in AEs relative reward as a percentage of DMUs from about 57% to 62% for the
horizon of 6 as the grid size is scaled from 3  3 to 5  5 makes us believe that AEs performance
will not necessarily deteriorate compared to DMU for larger problems. Overall, we demonstrate
the scalability of DMU and AE by increasing the horizon to 8 for this larger problem domain, and
further scaling its size. Larger grid sizes or longer horizons resulted in DIDs that could not be solved
by H UGIN given the fixed memory.

I
SN

SN

I
SL
SN

SN

I
SN

(a)

(b)

(c)

Figure 27: UAV Is flight trajectory is in dashed blue while the fugitives is in dashed red. Trajectories (a, b) eventually lead to the fugitive being spotted while in (c), the fugitive reaches
the safe house. The latter is due to an incorrect move by the UAV because of ambiguity
in its observations. A circle represents hovering during which the UAV or the fugtive is
listening and senses the labeled observation.

244

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Finally, we handpick three simulations from the numerous that we carried out and show the
corresponding trajectories of the UAV and the fugitive in Fig. 27. We show two trajectories where
the UAV spots the fugitive and a trajectory where the fugitive successfully escapes to the safehouse.
Figure 27 shows that the trajectories of the UAV can get quite complicated while those of the fugitive
are more straightforward due to its low strategic awareness.

7. Discussion
Graphical models are an appealing formalism for modeling decision making due to the convenience
of representation and the increased efficiency of the solution. DIDs are a key contribution in this
regard. I-DIDs are founded on the normative paradigm of decision theory as formalized by DIDs
and augmented with aspects of Bayesian games (Harsanyi, 1967) and interactive epistemology (Aumann, 1999a, 1999b) to make them applicable to interactions. I-DIDs generalize DIDs to multiagent
settings thereby extending the advantages of DIDs to decision making in multiagent settings. I-DIDs
adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic formalism that takes a decision-makers perspective in an interaction which may be cooperative or
non-cooperative. The broader impact is that understanding the agents decision-making process facilitates planning and problem-solving at its own level and in the absence of centralized controllers
or assumptions about agent behaviors. In a game-theoretic sense, the setting modeled by I-DIDs is
a partially observable stochastic game, and solving it by computing Nash equilibria or otherwise,
has received minimal attention in game theory.
We presented a collection of exact and approximation algorithms for scalably solving I-DIDs.
These algorithms improve on early techniques by providing more effective approaches in order to
reduce the exponential growth of other agents models at each time step. Our main idea is to cluster
models attributed to other agents that are BE. These models attribute identical behaviors across all
time steps to the other agent. We then select representative models from each cluster without loss
of optimality in the solution. Instead of generating the updated models and clustering them, we
showed how we may selectively update those models that will not be BE to existing models in the
next time step. Nevertheless, ascertaining BE requires solving the initial set of models. In order to
approximate this, we proposed solving K randomly picked models followed by all those which are
not -close to any of the K models. We partially bounded the error due to this approximation for
some cases. Despite the lack of a proper bound, our empirical results reveal that the error becomes
unwieldy for large  values only. This is because many problems admit large BE regions for models,
albeit which tend to reduce as horizon increases.
In order to further reduce the number of equivalence classes, we investigated grouping together
models whose prescribed actions at a particular time step are identical. This approach is appealing
because the number of AE classes is upper bounded by the number of distinct actions at any time.
While AE models may be grouped without loss in optimality, we identified the conditions under
which AE leads to an approximation. Our experiments indicate that considerations of BE are of
significance while grouping AE models leads to most reduction in the model space among all the
different approaches. However, they also show that the gap in the quality of the solutions due to
grouping BE and grouping AE models can become large. This difference depends on the domain
characteristics and does not necessarily worsen as the problem is scaled in horizon or size.
Due to the expressiveness of the modeling and its ensuing complexity, our experimentation
focused on settings involving two agents. However, as the number of other agents increases, po245

fiZ ENG & D OSHI

tential computational savings due to exploiting BE and AE assumes greater significance. This is
because the space of interactive states increases exponentially with the number of agents. Therefore, grouping agent models in less numbers of classes would substantially reduce the state space
and consequently the size of the I-DID. We may group models separately for each agent in which
case the computations for ascertaining BE classes grow linearly with the number of agents. On the
other hand, consider the tiger problem where agent is action is affected by somebody opening the
door, without the need for knowing which particular agent opened it. In this case, we may group
together models that are BE but belonging to different agents, leading to increased savings. Our
preliminary experimentation in the context of the multiagent tiger problem in a setting involving
two other agents (total of three agents) one of which is thought to be cooperative while the other
adversarial, indicates that grouping BE models for each other agent leads to a speed up of about 7
for both a 3-horizon and 5-horizon I-DID. Specifically, the computation time reduces from 4.8s for
Exact to 0.6s for Exact-BE when the horizon is 3, and from 72.6s to 10.5s for Exact-BE when the
horizon is 5.
Because identifying exact BE is computationally intensive, we think that improved scalability
may be brought by further investigating approximate BE of models. This would allow us to form
larger clusters and have fewer representative models. While we are investigating multiple ways of
doing this, a significant challenge is storing policy trees that grow exponentially as the horizon is
further scaled. One promising approach in this regard is to compare partial policy trees of bounded
depth and the distance between the belief vectors at the leaves of the trees. This allows us to define
an approximate measure of BE based on the distance between the updated belief vectors given that
the bounded-depth policy trees are identical. However, our preliminary investigations reveal that
deriving the depth of the tree becomes challenging for certain types of problems.
A general limitation of utilizing the spatial closeness of beliefs for approximately identifying
BE models is that the error may be larger if the frames in the models differ. This is because model
beliefs that are close are still less likely to result in the same behavior if say, the reward functions
are different. In the absence of this approximation, our approaches for discriminatively updating
models and grouping AE models continue to apply if frames are also uncertain because they operate
on model solutions  policy trees and actions  and not on model specifications. Another impact of
considering frame uncertainty is that the Markov blanket shown in Fig. 15 changes. A general hurdle
is that further scalability of ID-based graphical models is also limited by the absence of state-ofthe-art techniques for solving DIDs within commercial implementations such as H UGIN E XPERT
that predominantly rely on solving the entire DID in main memory. Although newer versions of
H UGIN allow the use of limited memory IDs (Nilsson & Lauritzen, 2000), recent advances such as
a branch-and-bound approach for solving multistage IDs (Yuan, Wu, & Hansen, 2010) would help
drive further scalability of I-DID solutions.

Acknowledgments
Yifeng Zeng acknowledges support from the Obel Family Foundation (Denmark) and NSFC (#
60974089 and # 60975052). Prashant Doshi acknowledges support from an NSF CAREER grant (#
IIS-0845036) and a grant from U.S. Air Force (# FA9550-08-1-0429). The authors also thank Ekhlas
Sonu and Yingke Chen for help in performing the GaTAC-based simulations, and acknowledge all
the anonymous reviewers for their helpful comments.
246

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Appendix A. Proofs
Proof of Proposition 1. We prove by induction on the horizon. Let {M1j,l1 , . . . , Mqj,l1 } be the
collection of behaviorally equivalent sets of models in Mj,l1 . We aim to show that the value of
each of is actions in the decision nodes at each time step remains unchanged on application of the
transformation, X. This implies that the solution of the I-DID is preserved. Let Qn (bi,l , ai ) give
the action value at horizon n. Its computation in the I-DID could be modeled using the standard dynamic programming approach. Let ERi (s, mj,l1 , ai ) be the expected immediateP
reward for agent
i averaged over js predicted actions. Then, mq Mq
ERi (s, mqj,l1 , ai ) = aj Ri (s, ai , aj )
j,l1

j,l1

P r(aj |mqj,l1 ) = Ri (s, ai , aqj ), because aqj is optimal for all mqj,l1  Mqj,l1 .
P
Basis step: Q1 (bi,l , ai ) = s,mj,l1 bi,l (s, mj,l1 )ERi (s, mj,l1 , ai )
P
P
= s,q bi,l (s) mq Mq
bi,l (mqj,l1 |s)Ri (s, ai , aqj ) (aqj is optimal for all behaviorally equivalent
j,l1
j,l1
models in Mqj,l1 )
P
P
= s,q bi,l (s)Ri (s, ai , aqj ) mq Mq
bi,l (mqj,l1 |s)
j,l1
j,l1
P
= s,q bi,l (s)Ri (s, ai , aqj )bi,l (mqj,l1 |s)
(from Eq. 1)
P
q
q
= s,q bi,l (s, mj,l1 )ERi (s, mj,l1 , ai ) (aqj is optimal for representative mqj,l1 )

= Q1 (bi,l , ai )
Inductive hypothesis: Let, ai ,bi,l Qn (bi,l , ai ) = Qn (bi,l , ai ), where bi,l relates to bi,l using Eq. 1.
Therefore, U n (bi,l ) = U n (bi,l ) where U n (bi,l ) is the expected utility of bi,l for horizon n.
P
Inductive proof: Qn+1 (bi,l , ai ) = Q1 (bi,l , ai ) + oi ,s,mj,l1 ,aj P r(oi |s, ai , aj )
P r(aj |mj,l1 )bi,l (s, mj,l1 )U n (bi,l ) (basis step)
P
P
= Q1 (bi,l , ai ) + oi ,s,q P r(oi |s, ai , aqj ) bi,l (s) mq Mq
bi,l (mqj,l1 |s) U n (bi,l ) (aqj is optij,l1
j,l1
mal for models in Mqj,l1 )
P
P
bi,l (mqj,l1 |s) U n (bi,l ) (using the
= Q1 (bi,l , ai ) + oi ,s,q P r(oi |s, ai , aqj ) bi,l (s) mq Mq
j,l1
j,l1
inductive hypothesis)
P
= Q1 (bi,l , ai ) + oi ,s,q P r(oi |s, ai , aqj ) bi,l (s) bi,l (mqj,l1 |s) U n (bi,l ) (from Eq. 1)
P
= Q1 (bi,l , ai ) + oi ,s,q P r(oi |s, ai , aqj ) bi,l (s, mqj,l1 ) U n (bi,l )
= Qn+1 (bi,l , ai )

Calculating prediction error in Section 4.5. Let mj,l1 be the model associated with a solved
model, mj,l1 , resulting in the worst error. Let  be the exact policy tree obtained by solving
mj,l1 optimally and  be the policy tree for mj,l1 . As mj,l1 is itself solved inexactly due
to approximate solutions of lower level models, let  be the exact policy tree that is optimal for
mj,l1 . If bj,l1 is the belief in mj,l1 and bj,l1 in mj,l1 , then the error is:
E

= |  bj,l1    bj,l1 |
= |  bj,l1    bj,l1 + (  bj,l1    bj,l1 )|
(add zero)
= |(  bj,l1    bj,l1 ) + (  bj,l1    bj,l1 )|
 |(  bj,l1    bj,l1 )| + |(  bj,l1    bj,l1 )| (triangle inequality)

(8)

For the first term, |  bj,l1    bj,l1 |, which we denote by , the error is due to associating
mj,l1 with mj,l1 , both solved exactly. We analyze this error below:
247

fiZ ENG & D OSHI

 = |  bj,l1    bj,l1 |
= |  bj,l1    bj,l1 +   bj,l1    bj,l1 | (add zero)
 |  bj,l1    bj,l1 +   bj,l1    bj,l1 | (  bj,l1    bj,l1 )
= |  (bj,l1  bj,l1 )    (bj,l1  bj,l1 )|
= |(   )  (bj,l1  bj,l1 )|
(Holders inequality)
 ||   ||  ||bj,l1  bj,l1 ||1
 (Rjmax  Rjmin )T  

(9)

In the above inequality, the largest difference between bj,l1 and bj,l1 is , otherwise model,
mj,l1 with belief bj,l1 , would be solved. Notice that the error is regulated by , and as  increases,
we solve less models beyond K and the approximation error worsens.
In subsequent time steps, because the sets of models could be subsets of the minimal sets, the
updated probabilities could be transferred to incorrect models. In the worst case, the error incurred
is bounded analogously to Eq. 9. Hence, the cumulative error in js predicted behavior over T steps
is at most T  , which is similar to that of the previous k-means model clustering approach (Zeng
et al., 2007):
T  (Rjmax  Rjmin )T 2 
The second term, |(  bj,l1    bj,l1 )|, in Eq. 8 represents the error due to the approximate
solutions of models further down in level (for example, is level l  2 models). Since js behavior
depends, in part, on the actions of i (and not on the value of is solution), even a slight deviation
by j from the exact prediction for i could lead to js behavior with the worst error. Hence, it seems
difficult to derive bounds for the second term that are tighter than the usual, (Rjmax  Rjmin )T .
Consequently, the total error in predicting js behavior is bounded if lower-level models are
solved exactly. Otherwise, as we show in Section 6.2, the error is large only for very large . This is
because many problems admit large BE regions for the models thereby not overconstraining , and
the prediction continues to remain exact. However, we noticed that these regions do reduce in size
as the horizon increases. In summary, although the error due to associating different models whose
beliefs are -close is bounded, we are unable to usefully bound the overall error in prediction due to
approximate solutions of lower-level models.

Appendix B. Problem Domains
We provide detailed descriptions of all the problem domains utilized in our evaluations, including
their I-DID models, below.
B.1 Multiagent Tiger Problem
As we mentioned previously, our multiagent tiger problem is a non-cooperative generalization of the
well-known single agent tiger problem (Kaelbling et al., 1998) to the multiagent setting. It differs
from other multiagent versions of the same problem (Nair et al., 2003) by assuming that the agents
hear creaks as well as the growls and the reward function does not promote cooperation. Creaks
are indicative of which door was opened by the other agent(s). While we described the problem
in Section 3, we quantify the different uncertainties here. We assume that the accuracy of creaks
is 90%, while the accuracy of growls is 85% as in the single agent problem. The tiger location is
chosen randomly in the next time step if any of the agents opened any doors in the current step.
248

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Fig. 8 shows an I-DID unrolled over two time-slices for the multiagent tiger problem. We give the
CPTs for the different nodes below:
hati , atj i
hOL, i
hOR, i
h, OLi
h, ORi
hL, Li
hL, Li

TigerLocationt
*
*
*
*
TL
TR

TL
0.5
0.5
0.5
0.5
1.0
0

TR
0.5
0.5
0.5
0.5
0
1.0

Table 2: CPT of the chance node T igerLocationt+1 in the I-DID of Fig. 8.

We assign the marginal distribution over the tigers location from agent is initial belief to the
chance node, T igerLocationt . The CPT of T igerLocationt+1 in the next time step conditioned
on T igerLocationt , Ati , and Atj is the transition function, shown in Table 2. The CPT of the
observation node, Growl&Creakt+1 , is shown in Table 3. CPTs of the observation nodes in level
0 DIDs are identical to the observation function in the single agent tiger problem.
hati , atj i
hL, Li
hL, Li
hL, OLi
hL, OLi
hL, ORi
hL, ORi
hOL, i
hOR, i

TgrLoct+1
TL
TR
TL
TR
TL
TR



hGL, CLi
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

hGL, CRi
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

hGL, Si
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

hGR, CLi
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

hGR, CRi
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

hGR, Si
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

Table 3: CPT of the chance node, Growl&Creakt+1 , in agent is I-DID.
Decision nodes, Ati and At+1
i , contain possible actions of agent i such as L, OL, and OR. Model
t
node, Mj,l1
, contains the different models of agent j which are DIDs if the I-DID is at level 0,
otherwise they are I-DIDs themselves. The distribution over the associated M od[Mjt ] node (see
Fig. 9) is the conditional distribution over js models given physical state from agent is initial
t+1
belief. The CPT of the chance node, M od[Mjt+1 ], in the model node, Mj,l1
, reflects which prior
model, action and observation of j results in a model contained in the model node.
Finally, the utility node, Ri , in the I-DID relies on both agents actions, Ati and Atj , and the
physical states, T igerLocationt . The utility table is shown in Table 4. These payoffs are analogous
to the single agent version, which assigns a reward of 10 if the correct door is opened, a penalty of
100 if the opened door is the one behind which is a tiger, and a penalty of 1 for listening. A result of
this assumption is that the other agents actions do not impact the original agents payoffs directly,
but rather indirectly by resulting in states that matter to the original agent. The utility tables for level
0 models are exactly identical to the reward function in the single agent tiger problem.
249

fiZ ENG & D OSHI

hai , aj i
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

TL
10
-100
10
-100
-1
-1
10
-1
-100

TR
-100
10
-100
10
-1
-1
-100
-1
10

Table 4: Utility table for node, Ri , in the I-DID. Utility table in the I-DID for agent j is the same
with column label, hai , aj i, swapped.

B.2 Multiagent Machine Maintenance Problem
We extend the traditional single agent based machine maintenance (MM) problem (Smallwood &
Sondik, 1973) to a two-agent cooperative version. Smallwood and Sondik (1973) described an MM
problem involving a machine containing two internal components. Either one or both components
of the machine may fail spontaneously after each production cycle (0-fail: no component fails; 1fail: 1 component fails; 2-fail: both components fail). If an internal component has failed, then there
is some chance that when operating upon the product, it will cause the product to be defective. An
agent may choose to manufacture the product (M) without examining it, examine the product (E),
inspect the machine (I), or repair it (R) before the next production cycle. On an examination of the
product, the subject may find it to be defective. Of course, if more components have failed, then the
probability that the product is defective is greater.

Ri

Ri

Ait

Ait+1
Aj

t

Ajt+1

Machine
Failuret

Machine
Failuret+1

Mj,l-1t

Mj,l-1t+1

Defectiveit

Defectiveit+1

Figure 28: Level l I-DID of agent i for the multiagent MM problem.
A level l I-DID for the multiagent MM problem is shown in Fig. 28. We consider M models of
agent j at the lower level which differ in the probability that j assigns to the chance node Machine
Failure. Agent is initial belief over the physical state and js models provides the marginal distribution over M achineF ailuret . In the I-DID, the chance node, M achineF ailuret+1 , has incident
250

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

arcs from the nodes M achineF ailuret , Ati , and Atj . In Table 5, we show the CPT of the chance
node.
hati , atj i
hM/E,M/Ei
hM/E,M/Ei
hM/E,M/Ei
hM,I/Ri
hM,I/Ri
hM,I/Ri
hE,I/Ri
hE,I/Ri
hE,I/Ri
hI/R,*i
hI/R,*i
hI/R,*i

Machine Failuret+1
0-fail
1-fail
2-fail
0-fail
1-fail
2-fail
0-fail
1-fail
2-fail
0-fail
1-fail
2-fail

0-fail
0.81
0.0
0.0
1.0
0.95
0.95
1.0
0.95
0.95
1.0
0.95
0.95

1-fail
0.18
0.9
0.0
0.0
0.05
0.0
0.0
0.05
0.0
0.0
0.05
0.0

2-fail
0.01
0.1
1.0
0.0
0.0
0.05
0.0
0.0
0.05
0.0
0.0
0.05

Table 5: CPT of the chance node, M achineF ailuret+1 , in the level l I-DID of agent i. At level 0
the CPT is analogous to the one in the original MM problem.

With the observation chance node, Def ectivet+1
i , we associate the CPT shown in Table 6.
Note that arcs from M achineF ailuret+1 and the nodes, Ati and Atj , in the previous time step are
incident to this node. The observation nodes in the level 0 DIDs have CPTs that are identical to the
observation function in the original MM problem.
hati , atj i
hM,M/Ei
hM,I/Ri
hE,M/Ei
hE,M/Ei
hE,M/Ei
hE,I/Ri
hI/R,*i

Machine Failuret+1
*
*
0-fail
1-fail
2-fail
*
*

not-defective
0.5
0.95
0.75
0.5
0.25
0.95
0.95

defective
0.5
0.05
0.25
0.5
0.75
0.05
0.05

Table 6: CPT of the observation node, Def ectivet+1
i . Corresponding CPT in agent js l  1 I-DID
is identical but with hati , atj i swapped.

The decision node, Ai , has one information arc from the observation node Def ectiveti indicating that i knows the examination results before making the choice. The utility node Ri is associated
with the utility table in Table 7. The utility table for a level 0 agent is identical to the one in the
original MM problem.
t+1
The CPT of the chance node, M od[Mjt+1 ], in the model node, Mj,l1
, reflects which prior
model, action and observation of j results in a model contained in the model node, analogously to
the tiger problem.
251

fiZ ENG & D OSHI

hati , atj i
hM,Mi
hM,Ei
hM,Ii
hM,Ri
hE,Mi
hE,Ei
hE,Ii
hE,Ri
hI,Mi
hI,Ei
hI,Ii
hI,Ri
hR,Mi
hR,Ei
hR,Ii
hR,Ri

0-fail
1.805
1.555
0.4025
-1.0975
1.5555
1.305
0.1525
-1.3475
0.4025
0.1525
-1.0
-2.5
-1.0975
-1.3475
-2.5
-4

1-fail
0.95
0.7
-1.025
-1.525
0.7
0.45
-1.275
-1.775
-1.025
-1.275
-3.00
-3.5
-1.525
-1.775
-3.5
-4

2-fail
0.5
0.25
-2.25
-1.75
0.25
0.0
-2.5
-2.0
-2.25
-2.5
-5.00
-4.5
-1.75
-2.0
-4.5
-4

Table 7: Utility table for agent i. Agent js utility table in its l  1 I-DID is identical but with
column label, hati , atj i, swapped.

B.3 UAV Reconnaissance Problem
We show a level l I-DID for the multiagent UAV problem in Fig 29. Models of the fugitive (agent
j) at the lower level differ in the probability that the fugitive assigns to its position in the grid. The
UAVs (agent i) initial beliefs are probability distributions assigned to the relative position of the
fugitive decomposed into the chance nodes, F ugRelP osX t and F ugRelP osY t , which represent
the relative location of the fugitive along the row and column, respectively. Its CPTs assume that
each action (except listen) moves the UAV in the intended direction with a probability of 0.67, while
the remaining probability is equally divided among the other neighboring positions. Action listen
keeps the UAV in the same position.
The observation node, SenF ug, represents the UAVs sensing of the relative position of the
fugitive in the grid. Its CPT assumes that the UAV has good sensing capability (likelihood of 0.8
for the correct relative location of the fugitive) if the action is listen, otherwise the UAV receives
random observations during other actions.
The decision node, Ai , contains five actions of the UAV, which includes moving in the four
cardinal directions and listening. The edge incident into the node indicates that the UAV ascertains
the observation on the relative position of the fugitive before it takes an action.
The utility node, Ri , is the reward assigned to the UAV for its actions given the fugitives relative
position and its actions. The UAV gets rewarded 50 if it captures the fugitive; otherwise, it costs -5
for performing any other action.
Because the actual CPT tables are very large, we do not show them here. All problem domain
files are available upon request.
252

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Ri

Ri

Ait

Ait+1
Aj

t

Ajt+1

FugRel
PosXit

FugRel
PosXit+1

FugRel
PosYit

FugRel
PosYit+1

Mj,l-1t

Mj,l-1t+1

SenFugit

SenFugit+1

Figure 29: Level l I-DID of agent i for our UAV reconnaissance problem.

References
Andersen, S., & Jensen, F. (1989). Hugin: A shell for building belief universes for expert systems.
In International Joint Conference on Artificial Intelligence (IJCAI), pp. 332337.
Aumann, R. J. (1999a). Interactive epistemology i: Knowledge. International Journal of Game
Theory, 28(3), 263300.
Aumann, R. J. (1999b). Interactive epistemology ii: Probability. International Journal of Game
Theory, 28, 301314.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control of markov decision processes. Mathematics of Operations Research, 27(4),
819840.
Brandenburger, A., & Dekel, E. (1993). Hierarchies of beliefs and common knowledge. Journal of
Economic Theory, 59, 189198.
Casbeer, D., Beard, R., McLain, T., Sai-Ming, L., & Mehra, R. (2005). Forest fire monitoring with
multiple small uavs. In American Control Conference, pp. 35303535.
Castro, P., Panangaden, P., & Precup, D. (2009). Equivalence relations in fully and partially observable markov decision processes. In International Joint Conference on Artificial Intelligence
(IJCAI), pp. 16531658.
Chang, K.-C., & Fung, R. (1991). Refinement and coarsening of bayesian networks. In Uncertainty
in Artificial Intelligence, pp. 435445.
Dekel, E., Fudenberg, D., & Morris, S. (2006). Topologies on types. Theoretical Economics, 1,
275309.
Doshi, P., & Sonu, E. (2010). Gatac: A scalable and realistic testbed for multiagent decision making). In Fifth Workshop on Multiagent Sequential Decision Making in Uncertain Domains
(MSDM), AAMAS, pp. 6266.
253

fiZ ENG & D OSHI

Doshi, P., Zeng, Y., & Chen, Q. (2009). Graphical models for interactive pomdps: Representations
and solutions. Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS), 18(3),
376416.
Duong, Q., Wellman, M., & Singh, S. (2008). Knowledge combination in graphical multiagent
models. In Uncertainty in Artificial Intelligence (UAI), pp. 153160.
Duong, Q., Wellman, M., Singh, S., & Vorobeychik, Y. (2010). History-dependent graphical multiagent models. In International Conference on Autonomous Agents and Multiagent Systems
(AAMAS), pp. 12151222.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoretic control for
robot teams. In International Conference on Robotics and Automation (ICRA), pp. 1163
1169.
Gal, Y., & Pfeffer, A. (2008). Networks of influence diagrams: A formalism for representing agents
beliefs and decision-making processes. Journal of Artificial Intelligence Research, 33, 109
147.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in markov
decision processes. Artificial Intelligence, 147(1-2), 163223.
Gmytrasiewicz, P., & Doshi, P. (2005). A framework for sequential planning in multiagent settings.
Journal of Artificial Intelligence Research (JAIR), 24, 4979.
Harsanyi, J. C. (1967). Games with incomplete information played by bayesian players. Management Science, 14(3), 159182.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and acting in partially observable
stochastic domains. Artificial Intelligence Journal, 101, 99134.
Kearns, M., Littman, M., & Singh, S. (2001). Graphical models for game theory. In Uncertainty in
Artificial Intelligence (UAI), pp. 253260.
Koller, D., & Milch, B. (2001). Multi-agent influence diagrams for representing and solving games.
In International Joint Conference on Artificial Intelligence (IJCAI), pp. 10271034.
Madsen, N. S., & Jensen, F. V. (2008). An influence diagram framework for acting under influence
by agents with unknown goals. In Fourth European Workshop on Probabilistic Graphical
Models (PGM), pp. 289296.
Mertens, J., & Zamir, S. (1985). Formulation of bayesian analysis for games with incomplete
information. International Journal of Game Theory, 14, 129.
Milner, R. (1980). A Calculus of Communicating Systems. Springer-Verlag.
Murphy, D., & Cycon, J. (1998). Applications for mini vtol uav for law enforcement. In SPIE
3577:Sensors, C3I, Information, and Training Technologies for Law Enforcement.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized pomdps
: Towards efficient policy computation for multiagent settings. In International Joint Conference on Artificial Intelligence (IJCAI), pp. 705711.
Nilsson, D., & Lauritzen, S. (2000). Evaluating influence diagrams using limids. In Uncertainty in
Artificial Intelligence (UAI), pp. 436445.
254

fiE XPLOITING M ODEL E QUIVALENCES FOR S OLVING I NTERACTIVE DYNAMIC I NFLUENCE D IAGRAMS

Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering of histories in decentralized pomdps. In International Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS), pp. 577584.
Perry, A. R. (2004). The flightgear flight simulator. In UseLinux.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations for large pomdps.
Journal of Artificial Intelligence Research (JAIR), 27, 335380.
Pynadath, D., & Marsella, S. (2007). Minimal mental models. In Twenty-Second Conference on
Artificial Intelligence (AAAI), pp. 10381044, Vancouver, Canada.
Rathnasabapathy, B., Doshi, P., & Gmytrasiewicz, P. J. (2006). Exact solutions to interactive pomdps
using behavioral equivalence. In Autonomous Agents and Multi-Agents Systems Conference
(AAMAS), pp. 10251032.
Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (Third Edition). Prentice Hall.
Seuken, S., & Zilberstein, S. (2007). Memory-bounded dynamic programming for dec-pomdps. In
International Joint Conference on Artificial Intelligence (IJCAI), pp. 20092015.
Seuken, S., & Zilberstein, S. (2008). Formal models and algorithms for decentralized decision
making under uncertainty. Autonomous Agents and Multi-Agent Systems, 17(2), 190250.
Smallwood, R., & Sondik, E. (1973). The optimal control of partially observable markov decision
processes over a finite horizon. Operations Research (OR), 21, 10711088.
Suryadi, D., & Gmytrasiewicz, P. (1999). Learning models of other agents using influence diagrams.
In International Conference on User Modeling, pp. 223232.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming and influence diagrams. IEEE
Transactions on Systems, Man, and Cybernetics, 20(2), 365379.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction for weakly-coupled
dec-pomdps. In International Conference on Automated Planning and Scheduling (ICAPS),
pp. 185192.
Yuan, C., Wu, X., & Hansen, E. (2010). Solving multistage influence diagrams using branch-andbound search. In Uncertainty in Artificial Intelligence (UAI), pp. 691700.
Zeng, Y., Doshi, P., & Chen, Q. (2007). Approximate solutions of interactive dynamic influence
diagrams using model clustering. In Twenty Second Conference on Artificial Intelligence
(AAAI), pp. 782787.

255

fiJournal of Artificial Intelligence Research 43 (2012) 419476

Submitted 08/11; published 03/12

Completeness Guarantees for Incomplete Ontology
Reasoners: Theory and Practice
Bernardo Cuenca Grau
Boris Motik
Giorgos Stoilos
Ian Horrocks

bernardo.cuenca.grau@cs.ox.ac.uk
boris.motik@cs.ox.ac.uk
giorgos.stoilos@cs.ox.ac.uk
ian.horrocks@cs.ox.ac.uk

Department of Computer Science, University of Oxford
Wolfson Building, Parks Road, OX1 3QD, Oxford

Abstract
To achieve scalability of query answering, the developers of Semantic Web applications
are often forced to use incomplete OWL 2 reasoners, which fail to derive all answers for
at least one query, ontology, and data set. The lack of completeness guarantees, however,
may be unacceptable for applications in areas such as health care and defence, where
missing answers can adversely aect the applications functionality. Furthermore, even
if an application can tolerate some level of incompleteness, it is often advantageous to
estimate how many and what kind of answers are being lost.
In this paper, we present a novel logic-based framework that allows one to check whether
a reasoner is complete for a given query Q and ontology T that is, whether the reasoner
is guaranteed to compute all answers to Q w.r.t. T and an arbitrary data set A. Since
ontologies and typical queries are often fixed at application design time, our approach allows
application developers to check whether a reasoner known to be incomplete in general is
actually complete for the kinds of input relevant for the application.
We also present a technique that, given a query Q, an ontology T , and reasoners
R1 and R2 that satisfy certain assumptions, can be used to determine whether, for each
data set A, reasoner R1 computes more answers to Q w.r.t. T and A than reasoner R2 .
This allows application developers to select the reasoner that provides the highest degree of
completeness for Q and T that is compatible with the applications scalability requirements.
Our results thus provide a theoretical and practical foundation for the design of future
ontology-based information systems that maximise scalability while minimising or even
eliminating incompleteness of query answers.

1. Introduction
Ecient management and querying of large amounts of data is a core problem for a growing
range of applications in fields as diverse as biology (Sidhu, Dillon, Chang, & Sidhu, 2005),
medicine (Golbreich, Zhang, & Bodenreider, 2006), geography (Goodwin, 2005), astronomy
(Derriere, Richard, & Preite-Martinez, 2006), agriculture (Soergel, Lauser, Liang, Fisseha,
Keizer, & Katz, 2004), and defence (Lacy, Aviles, Fraser, Gerber, Mulvehill, & Gaskill,
2005). In order to facilitate interoperability, such applications often use standard data
models and query languages. In particular, RDF (Hayes, 2004) provides a standard model
for semistructured data, SPARQL (Prudhommeaux & Seaborne, 2008) is a standard query
language for RDF, and ontology languages such as OWL (Horrocks, Patel-Schneider, &
van Harmelen, 2003) and OWL 2 (Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider,
c
2012
AI Access Foundation. All rights reserved.

fiCuenca Grau, Motik, Stoilos & Horrocks

& Sattler, 2008b) can be used to describe background knowledge about the application
domain. Thus, answering SPARQL queries over RDF data sets structured using an OWL
ontology is a key service in ontology-based information systems.
An important question in the design of such systems is the selection of an appropriate
reasoner. Systems such as Pellet (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2007),
HermiT (Motik, Shearer, & Horrocks, 2009b), and RACER (Haarslev & Moller, 2001) are
based on (hyper)tableau algorithms that are provably completethat is, they are guaranteed to compute all answers for each query, ontology, and data set. Completeness, however,
comes at the cost of scalability, as answering queries over OWL 2 ontologies is of high computational complexity (Glimm, Horrocks, Lutz, & Sattler, 2007; Ortiz, Calvanese, & Eiter,
2008; Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007; Lutz, Toman, & Wolter,
2009). Thus, complete systems often fail to meet the scalability demands of applications
that manage data sets consisting of hundreds of millions or even billions of assertions.
Scalability of query answering can be ensured by restricting the expressive power of the
ontology language to the level that makes provably complete reasoning tractable. This has
led to the development of three profiles of OWL 2 (Motik, Cuenca Grau, Horrocks, Wu,
Fokoue, & Lutz, 2009a): OWL 2 EL, OWL 2 RL, and OWL 2 QL. Query answering in all
three profiles can be implemented in polynomial time w.r.t. the size of data (and even in
logarithmic space in the case of OWL 2 QL). Such appealing theoretical properties have
spurred the development of specialised reasoners such as QuONTO (Acciarri, Calvanese,
De Giacomo, Lembo, Lenzerini, Palmieri, & Rosati, 2005) that target specific profiles and
typically reject ontologies that fall outside the target profile.
A dierent solution to the scalability problem is adopted in reasoners such as Oracles Semantic Data Store (Wu, Eadon, Das, Chong, Kolovski, Annamalai, & Srinivasan,
2008), Sesame (Broekstra, Kampman, & van Harmelen, 2002), Jena (McBride, Brian, 2001),
OWLim (Kiryakov, Ognyanov, & Manov, 2005), Minerva (Ma, Yang, Qiu, Xie, Pan, & Liu,
2006), DLE-Jena (Meditskos & Bassiliades, 2008), and Virtuoso (Erling & Mikhailov, 2009).
These reasoners accept all OWL 2 ontologies as inputthat is, they never reject inputs.
Furthermore, to the best of our knowledge, all of these systems are intended to be sound,
which means that all results of a query are indeed correct answers. Finally, these reasoners
typically use scalable reasoning techniques, such as various (deductive) database algorithms.
As a consequence, the reasoners are incomplete: for each reasoner, at least one query, ontology, and data set exist for which the reasoner does not return all answers to the query.
Some of these reasoners were actually designed to be complete for a particular profile of
OWL 2 (typically this is OWL 2 RL due to its close connection with datalog), and they
can often additionally handle certain kinds of axiom that fall outside the target profile.
Since incomplete reasoners can handle large data sets, they often provide the best practical choice for developers of ontology-based applications. For example, OWLim was used
for reasoning in the backend of the BBCs 2010 World Cup website, and Oracles reasoner is
being used by the University of Texas Health Science Center to improve large-scale public
health surveillance. In order to verify that the selected reasoner meets the applications
requirements, developers typically resort to empirical testing, in which they check the reasoners answers w.r.t. the application ontology and queries for representative data sets.
Although primarily intended for testing performance, benchmark suites such as the Lehigh
420

fiCompleteness Guarantees for Incomplete Ontology Reasoners

University Benchmark (LUBM) (Guo, Pan, & Heflin, 2005) and the University Ontology
Benchmark (UOBM) (Ma et al., 2006) have been used for such completeness testing.
Empirical completeness testing, however, has several important limitations. First, tests
are not generic, as data sets used for testing typically have a fixed and/or repetitive structure, which can skew test results. Second, test data is not exhaustive, as completeness is
tested only w.r.t. a limited number of data sets. Finally, query answers may not be verifiable: since complete reasoners fail to handle large data sets, they often cannot compute
the control answers needed to check the answers produced by an incomplete reasoner. As
a consequence, empirical completeness tests provide only limited assurance of a reasoners
ability to meet the requirements of a given application.
In this paper, we present a radically dierent approach to solving these problems. We
observed that, given a query Q and ontology T , even if a reasoner is not complete for the
language of T , the reasoner may be able to correctly answer Q w.r.t. T and an arbitrary data
set A; in such a case, we say that the reasoner is (Q, T )-complete. Given that ontology-based
applications often use a limited set of queries and a fixed ontology (or at least the queries and
the ontology evolve relatively slowly), a scalable reasoner that is generally incomplete, but
is (Q, T )-complete for all relevant combinations of Q and T , may provide a solid foundation
for ontology-based applications, allowing them to enjoy the best of both worlds: regardless
of the data set encountered, such applications will enjoy completeness guarantees normally
available only with computationally-intensive complete reasoners, while at the same time
exhibiting scalability levels normally available only by sacrificing completeness. To develop
an approach for testing the (Q, T )-completeness of a given reasoner, we proceed as follows.
In Section 3 we develop a logic-based framework that allows us to establish formally
provable (Q, T )-completeness guarantees. The following two notions are central to our
framework. First, in order to abstract away from the implementation details of concrete
reasoners, we introduce the notion of an abstract reasoner an idealised reasoner that captures the intended behaviour and salient features (such as soundness and monotonicity) of
a class of concrete reasoners. Second, we introduce the notion of a test suitea finite set
of data sets and queries. Intuitively, given Q and T , our goal is to construct a test suite
such that, if a reasoner correctly answers all queries on all data sets in the test suite, then
the reasoner is guaranteed to be (Q, T )-complete.
Unfortunately, as we show in Section 3.4, for certain Q and T , it is impossible to
construct a finite test suite that will provide the aforementioned completeness guarantees.
Therefore, we investigate assumptions on Q, T , and the reasoner under which testing (Q, T )completeness becomes practically feasible.
In Section 3.5 we consider the case where Q and T can be rewritten into a union of
conjunctive queries Rthat is, when answering Q w.r.t T and a data set A is equivalent
to evaluating R over A. For T expressed in OWL 2 QL, a rewriting R can be computed
using the algorithm by Calvanese et al. (2007); additionally, the algorithm by Perez-Urbina,
Motik, and Horrocks (2010) can sometimes compute R even if T is syntactically outside
this fragment. We show that such R can be converted into a test suite ER that can be used
for testing the (Q, T )-completeness of any reasoner that satisfies some basic assumptions;
roughly speaking, the reasoners answers should not depend on the names of the individuals
occurring in a data set, and its answers must increase monotonically when new data is added.
The size of each test in ER is polynomial in the size of the longest conjunctive query in R,
421

fiCuenca Grau, Motik, Stoilos & Horrocks

so it should be feasible to compute correct answers to the tests using a complete reasoner.
The number of tests in ER , however, can be exponential in the size of R, which may lead to
problems in practice. As a remedy, in Section 3.6 we strengthen our assumptions and require
the reasoner not to drop answers when merging individualsthat is, if the reasoner returns
a given inputs Q, T , and A, then for each (possibly noninjective) mapping  the reasoner
returns (a) given inputs Q, T , and (A)and we show that (Q, T )-completeness of such
reasoners can be checked using a test suite IR obtained from R by a linear transformation.
That Q and T should be rewritable into a union of conjunctive queries eectively prevents T from stating recursive axioms. To overcome this restriction, in Section 3.7 we consider first-order reproducible reasonersthat is, reasoners whose behaviour on Q, T , and A
can be seen as computing certain answers of Q w.r.t. some (possibly unknown) first-order
theory FT and A. Since FT can be a datalog program, most reasoners based on deductive
databases are first-order reproducible. In addition, we require Q and T to be rewritable
into datalog, an extension of datalog that allows for existential quantifiers and disjunction in rule heads. In many cases, T can be transformed into a datalog, program
using equivalence-preserving transformations; furthermore, the algorithm by Perez-Urbina
et al. (2010) can in many cases produce a plain datalog rewriting. We then show how to
transform a datalog, rewriting of Q and T into a test suite that can be used to test
(Q, T )-completeness of first-order reproducible reasoners.
In Section 4 we turn our attention to comparing incomplete reasoners. Roughly speaking, given Q and T , reasoner R1 is more complete than reasoner R2 if, for each data set A,
reasoner R1 computes all the answers to Q w.r.t. T and A that are computed by R2 . We
show that comparing incomplete reasoners is infeasible in general. Therefore, we introduce
the notion of compact reasonersthat is, reasoners whose behaviour on Q, T , and A can be
seen as first selecting some subset T  of T and then using a complete reasoner to evaluate
Q w.r.t. T  and A. Thus, the class of compact reasoners captures all reasoners that reduce
the input ontology T to a set of axioms that match certain parameters, such as fitting into
language fragments. For Q and T that can be rewritten into a union of conjunctive queries
R, we show that the test suite IR can be used to compare compact reasoners.
We have implemented our approaches for computing test suites, and have tested completeness of several well-known reasoners (see Section 5). These show that test suites can
be eciently computed for realistic ontologies. Furthermore, we were able to guarantee
(Q, T )-completeness of the evaluated reasoners for many queries and ontologies. Finally,
when no (Q, T )-completeness guarantee could be provided, we were able to compute a
counter-examplea small data set for which the reasoner at hand was incomplete.

2. Preliminaries
In this section we briefly introduce Description Logics (DLs) (Baader, McGuinness, Nardi, &
Patel-Schneider, 2002)a family of knowledge representation formalisms which underpin
the OWL and OWL 2 ontology languages. We describe description logics in the wider
framework of first-order logic since many of our results hold for arbitrary first-order theories.
We then introduce the datalog, and datalog languages, and we define the syntax and
semantics of unions of conjunctive queries (UCQs). Finally, we introduce the notions of
UCQ, datalog, and datalog, rewritings, which underpin many of our techniques.
422

fiCompleteness Guarantees for Incomplete Ontology Reasoners

2.1 Description Logics and First-Order Logic
Most of the results in this paper hold for arbitrary first-order theories, rather than description logics. Our work, however, is motivated by description logics and ontologies, so we use
the DL terminology throughout the paper; for example, we often talk about TBoxes and
ABoxes instead of first-order theories and sets of facts.
All definitions in this paper are implicitly parameterised by a signature  = P , I ,
which consists of countably infinite disjoint sets of predicates P and individuals (commonly
called constants in first-order logic) I . Each predicate is associated with a nonnegative
arity; predicates of zero arity are commonly called propositional symbols. The notions of
variables, terms, atoms, first-order formulae, and sentences are defined as usual (Fitting,
1996); we do not consider function symbols in this article and we assume all formulae to
be function-free. The atom that is false (true) in all interpretations is written  (). An
atom is a fact if it does not contain variables. We use the standard first-order notions of
satisfiability, unsatisfiability, and entailment (written |=) of sets of first-order sentences.
We assume that P contains the special equality and inequality predicates  and ,
respectively; atoms of the form (t1 , t2 ) and (t1 , t2 ) are commonly written as t1  t2 and
t1  t2 , respectively. We make a technical assumption that  and  are distinct predicates
rather than, as it is common in first-order logic, that t1  t2 is an abbreviation for (t1  t2 );
furthermore, we assume that each theory that uses  and  axiomatises their semantics as
follows, where (5) is instantiated for each predicate P of arity n and each i with 1  i  n.
x, y.[x  y  x  y  ]

(1)

x, y.[x  y  y  x]

(3)

x1 , . . . , xi , . . . , xn , yi .[P (x1 , . . . , xi , . . . , xn )  xi  yi  P (x1 , . . . , yi , . . . , xn )]

(5)

x.[x  x]

(2)

x, y, z.[x  y  y  z  x  z]

(4)

Note that, according to this assumption, each set of facts is satisfiable. For example, the
set of atoms {a  b, a  b} is satisfiable since both a  b and a  b are positive variable-free
atoms that are semantically independent from each other; moreover, axiom (1) is required
to obtain the expected contradiction.
An individual renaming (often just renaming) is a partial function  : I  I that
maps individuals to individuals. The domain and the range of  are written dom() and
rng(); unless otherwise noted, we assume that dom() is finite. For  an object containing
individuals (such as a formula, a set of formulae, or a tuple of individuals), ind() is the
set of individuals occurring in , and () is obtained from  by simultaneously replacing
each individual a  ind()  dom() with (a).
We use the notion of substitutions from first-order logic; that is, a substitution  is a
mapping of variables to terms. For  a term, an atom, or a formula, the result of applying
a substitution  to  is written as ().
A TBox T a is a finite set of first-order sentences that contains axioms (1)(5) whenever
 and/or  are used. An ABox A is a finite set of facts. Note that this definition allows
for atoms of the form a  b and a  b in ABoxes; furthermore, since ABoxes can contain
only positive atoms, each ABox (when considered without a TBox) is satisfiable.
423

fiCuenca Grau, Motik, Stoilos & Horrocks

DL Name
EL
FL
ALC
+(H)
+(R)
+(S)
+(I)
+(Q)
+(O)

Roles
R
R
R

Concepts
, A, C1  C2 , R.C
, A, C1  C2 , R.C
, , A, C, C1  C2 , C1  C2 , R.C, R.C
R.Self

TBox Axioms
C1  C2
C1  C2
C1  C2
R1  R2
RS T
Trans(R)

R
 nS.C,  nS.C
{a}

Table 1: Syntax of standard description logics. Typical extensions of EL, ALC, and FL
are named by appending calligraphic letters (H, R, S, I, Q, and/or O).
A description logic DL is a (usually infinite) recursive set of TBoxes satisfying the
following conditions:
 for each T  DL and each renaming , we have (T )  DL, and
 for each T  DL and each T   T , we have T   DL.

If T  DL, we say that T is a DL-TBox. Finally, FOL is the largest description logic that
contains all finite sets of first-order sentences over the signature in question.
We next present an overview of the DLs commonly considered in the literature. Typically, the predicates in DL signatures are required to be unary or binary; the former are
commonly called atomic concepts and the latter are commonly called atomic roles. DLs
typically use a specialised syntax, summarised in Table 1, that provides a set of constructors
for constructing complex concepts and roles from simpler ones, as well as dierent kinds
of axioms. Using the translation from Table 2, concepts can be translated into first-order
formulae with one free variable, roles can be translated into first-order formulae with two
free variables, and axioms can be translated into first-order sentences. Note that the translation uses counting quantifiers n and n , which can be expressed by using ordinary
quantifiers and equality by well-known transformations.
In the rest of this paper, we commonly write TBoxes and ABoxes in DL syntax; however,
to simplify the presentation, we identify T and A written in DL syntax with (T ) and (A).
2.2 Datalog,
We next introduce a fragment of first-order logic called datalog, as an extension of
datalog by Cal, Gottlob, Lukasiewicz, Marnette, and Pieris (2010). A datalog, rule
(or commonly just a rule) r is a formula of the form (6), where each Bj is an atom dierent
from  whose free variables are contained in x, and
 m = 1 and 1 (x, y1 ) = , or

 m  1 and, for each 1  i  m, formula i (x, yi ) is a conjunction of atoms dierent
from  whose free variables are contained in x  yi .
424

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Mapping DL roles into first-order logic
(R, x, y) = R(x, y)
(R , x, y) = R(y, x)
Mapping DL concepts into first-order logic
(, x, y) = 
(, x, y) = 
(A, x, y) = A(x)
({a}, x, y) = x  a
(C, x, y) = (C, x, y)
(C  D, x, y) = (C, x, y)  (D, x, y)
(C  D, x, y) = (C, x, y)  (D, x, y)
(R.C, x, y) = y.[(R, x, y)  (C, y, x)]
(R.Self, x, y) = R(x, x)
(R.C, x, y) = y.[(R, x, y)  (C, y, x)]
( nS.C, x, y) = n y.[(S, x, y)  (C, y, x)]
( nS.C, x, y) = n y.[(S, x, y)  (C, y, x)]
Mapping TBox axioms into first-order logic
(C  D) = x.[(C, x, y)  (D, x, y)]
(R  S) = x, y.[(R, x, y)  (S, x, y)]
(Trans(R)) = x, y, z.[(R, x, y)  (R, y, z)  (R, x, z)]
(R  S  T ) = x, y, z.[(R, x, y)  (S, y, z)  (T, x, z)]
Mapping ABox axioms into first-order logic
(C(a)) = (C, a, y)
(R(a, b)) = R(a, b)
(a  b) = a  b
(a  b) = a  b
Table 2: Translation of DL syntax into first-order logic

x.[B1  . . .  Bn 

m


i=1

yi .i (x, yi )]

(6)

A rule is safe if each variable in x also occurs in some Bj ; unless otherwise noted, all rules
are assumed to be safe. For brevity, the outer quantifier x is commonly left implicit. The
body of r 
is the set of atoms body(r) = {B1 , . . . , Bn }, and the head of r is the formula
head(r) = m
yi .i (x, yi ). A datalog, program is a finite set of safe datalog, rules.
i=1 
Note that, since  and  are treated as ordinary predicates, they can occur in rules, provided
that their semantics is appropriately axiomatised; furthermore, note that the latter can be
achieved using datalog, rules.
Let r be a datalog, rule. Then, r is a datalog rule if head(r) contains no existential
quantifier. Also, r is a datalog rule if m = 1. Finally, r is a datalog rule if m = 1 and the
head of r is a single atom without existential quantifiers (Ceri, Gottlob, & Tanca, 1989).
In several places in this paper, we check whether a set of first-order sentences entails a
datalog, rule, which can be accomplished using the following simple result.
425

fiCuenca Grau, Motik, Stoilos & Horrocks

Proposition 2.1. Let F be a set of first-order sentences, and let r be a datalog, rule
of the form (6). Then, for each substitution  mapping the free variables of r to distinct
individuals not occurring in F or r, we have F |= r if and only if
F  {(B1 ), . . . , (Bn )} |=

m


i=1

yi .i ((x), yi )

Proof. Let x be the tuple of free variables in r and let  be an arbitrary substitution
mapping the variables in x to distinct individuals not occurring in F or r. The claim of
this proposition follows from the following equivalences:
F |= x.[B1  . . .  Bn 

m


yi .i (x, yi )]

i=1
m


F  {[x.B1  . . .  Bn 
F  {x.[B1  . . .  Bn  

F  {(B1 )  . . .  (Bn )  
F  {(B1 ), . . . , (Bn ), 

m


i

yi .i (x, yi )]} is unsatisfiable

i (skolem. of x)

m


i=1

yi .i ((x), yi )} is unsatisfiable

yi .i ((x), yi )} is unsatisfiable

i=1
m


F  {(B1 ), . . . , (Bn )} |=

yi .i (x, yi )]} is unsatisfiable

i=1
m


i=1

i=1

i

i
i

yi .i ((x), yi ).

2.3 Queries
In order to achieve a high degree of generality, we define a query Q as a finite set of firstorder sentences containing a distinct query predicate Q. Intuitively, the query predicate Q
determines the answers of Q. In order to simplify the notation, we typically assume that the
association between Q and the query predicate is implicit (e.g., we may require each query
to contain precisely one such predicate), and we assume that no query predicate occurs in
a TBox or an ABox.
A tuple of constants a is a certain answer to a query Q with query predicate Q with
respect to a TBox T and an ABox A if the arity of a agrees with the arity of Q and
T  A  Q |= Q(a). The set of all certain answers of Q w.r.t. T and A is denoted as
cert(Q, T , A). If the query predicate of Q is propositional (i.e., if the query is Boolean),
then cert(Q, T , A) is either empty or it contains the tuple of zero length; in such cases, we
commonly write cert(Q, T , A) = f and cert(Q, T , A) = t, respectively.
We use  as the special Boolean query that checks a first-order theory for unsatisfiability.
Thus, cert(, T , A) = t if and only if T  A is unsatisfiable.
A query Q with a query predicate Q is a union of conjunctive queries (UCQ) if it is a
datalog program in which each rule contains Q in the head but not in the body. A UCQ Q
is a conjunctive query (CQ) if it contains exactly one rule.
426

fiCompleteness Guarantees for Incomplete Ontology Reasoners

A union of conjunctive queries Q is ground if, for each rule r  Q, each variable occurring in the body of r also occurs in the head of r. Roughly speaking, when computing
cert(Q, T , A) for a ground Q, all variables in Q can be matched only to the individuals in T
and A, but not to unnamed objects whose existence is guaranteed by existential quantifiers.
Many state of the art reasoners used in practice support only ground UCQs. Note that
Q = {A(x)  Q(x), R(x, y)  Q(x, y)} is not a ground UCQ; in fact, Q is not even a valid
first-order theory since predicate Q does not have unique arity. To obtain a UCQ, one can
pad the head of the first rulethat is, one can introduce a special fresh individual null
and rewrite the rules as Q = {A(x)  Q(x, null ), R(x, y)  Q(x, y)}.
By the properties of first-order logic entailment, cert satisfies the following properties
for each query Q, all TBoxes T and T  , and all ABoxes A and A .
1. Monotonicity: T  T  and A  A imply
 cert(, T , A) = t implies cert(, T  , A ) = t, and
 cert(Q, T , A)  cert(Q, T  , A ).

2. Invariance under renamings: For each renaming  and each tuple of individuals a,
 cert(, T , A) = t implies cert(, (T ), (A)) = t, and

 a  cert(Q, T , A) implies (a)  cert((Q), (T ), (A)).
2.4 Rewritings
Intuitively, a rewriting of a query Q w.r.t. a TBox T is another query that captures all the
information from T that is relevant for answering Q over an arbitrary ABox A (Calvanese
et al., 2007; Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009; Perez-Urbina et al.,
2010). In practice, UCQs (Calvanese et al., 2007) and datalog (Perez-Urbina et al., 2010)
are the most widely used target languages for query rewriting. For the sake of generality,
however, in this paper we use a notion of a datalog, rewriting.
Definition 2.2. Let Q be a query and let T be a TBox. A datalog, rewriting (or simply
a rewriting) of Q w.r.t. T is a triple R = RD , R , RQ  where
 RD is a datalog, program not containing  or Q such that T |= RD ,
 R is a datalog program such that head(r) =  for each r  R , and
 RQ is a UCQ whose query predicate is Q,
such that the following properties hold for each ABox A:
 cert(, T , A) = cert(, RD  R , A), and
 if cert(, T , A) = f, then cert(Q, T , A) = cert(RQ , RD  R , A).
Rewriting R is a datalog rewriting if RD is a datalog program. Furthermore, rewriting R
is a UCQ rewriting if RD = ; such an R is usually written as just R = R , RQ .
427

fiCuenca Grau, Motik, Stoilos & Horrocks

Note that Definition 2.2 requires T |= RD to hold, which precludes rewritings consisting
of axioms that are unsound w.r.t. T . For example, let Q = {A(x)  Q(x)} and T = ;
then, RD = {B(x)  A(x)} does not satisfy the definition of a rewriting since formula
B(x)  A(x) is not a logical consequence of T .
For a wide range of T and Q, a datalog, rewriting of Q w.r.t. T can be computed using
straightforward equivalence-preserving transformations of T ; this can be further optimised
by eliminating axioms from T that are irrelevant to answering Q. Furthermore, several
algorithms for computing UCQ and datalog rewritings have been proposed in the literature.
For example, Calvanese et al. (2007) showed how to compute a UCQ rewriting in cases when
T is expressed in a logic from the DL-Lite family, and this approach can be extended to the
OWL 2 QL profile of OWL 2 (Motik et al., 2009a). Similarly, Perez-Urbina et al. (2010)
proposed an algorithm for computing the simplest possible datalog rewriting when T is
expressed in the description logic ELHIO.
Rewritings produced by known algorithms often contain predicates that do not occur
in T and Q; such predicates are sometimes called fresh. For example, many rewriting
algorithms normalise TBoxes by replacing complex concepts with fresh atomic concepts. A
rewriting R = RD , R , RQ  obtained in such a way is unlikely to satisfy the requirement
that T |= RD . However, predicates occurring in R but not in T can often be eliminated
via unfolding. For example, let Q = {A(x)  Q(x)} and T = {R.S.B  A}, and assume
that a rewriting algorithm produces
RD = {S(x, y)  B(x)  C(x), R(x, y)  C(y)  A(x)}.
To satisfy Definition 2.2, predicate C can be unfolded and RD replaced with
RD = {R(x, y)  S(y, z)  B(z)  A(x)},
for which T |= RD holds. Unfolding, however, may not always be possible (e.g., this might
be the case when fresh predicates occur in recursive axioms), which may limit the applicability of some of the results presented in this paper.

3. Completeness Guarantees for Incomplete Reasoners
In this section, we introduce the formal framework that will allow us to establish completeness guarantees for incomplete reasoners. Our results are not restricted to any particular
description logic, but are applicable to all TBoxes that satisfy the following criterion.
Definition 3.1. A TBox T is admissible if a description logic DL exists such that T is
a DL-TBox, and both checking TBox satisfiability and answering Boolean UCQs w.r.t. an
arbitrary ABox are decidable for DL.
3.1 Concrete and Abstract Reasoners
Concrete reasoners are complex software systems that dier greatly in the functionality
and the supported interfaces, and that use a range of dierent implementation techniques.
To make our results general and independent from specific implementation techniques, we
introduce the notion of an abstract reasoner. An abstract reasoner can be thought of as an
428

fiCompleteness Guarantees for Incomplete Ontology Reasoners

idealised reasoner that captures the intended behaviour and salient features of a class of
concrete reasoners. A concrete reasoner belonging to this class may use arbitrary algorithms,
as long as their observable behaviour mirrors that of the abstract reasoner.
Definition 3.2. An abstract reasoner ans for a description logic DL is a computable function that takes as input an arbitrary DL-TBox T , an arbitrary ABox A, and either the
special unsatisfiability query  or an arbitrary UCQ Q. The return value of ans is defined
as follows:
 ans(, T , A) is either t or f;
 if ans(, T , A) = t, then ans(Q, T , A) is of no interest and can be arbitrary; and
 if ans(, T , A) = f, then ans(Q, T , A) is a finite set of tuples of individuals, where the
arity of each tuple is equal to the arity the query predicate of Q.
An abstract reasoner ans for DL is said to be applicable to a TBox T if T is a DL-TBox.
Intuitively, ans(, T , A) asks the abstract reasoner to check whether T  A is unsatisfiable, and ans(Q, T , A) asks the abstract reasoner to evaluate Q w.r.t. T  A. If T  A
is unsatisfiable, then each tuple of constants of the same arity as the query predicate Q
is an answer to Q on T  A; therefore, the result of ans(Q, T , A) is of interest only if
ans(, T , A) = fthat is, if ans identifies T  A as satisfiable.
Example 3.3. Consider the abstract reasoners rdf, rdfs, rl, and classify which, given as
input a UCQ Q, a TBox T , and an ABox A, compute the answer to Q w.r.t. T and A as
described next.
Abstract reasoner rdf ignores T and evaluates Q w.r.t. A; more precisely, rdf(, T , A) = f
and rdf(Q, T , A) = cert(Q, , A). Thus, rdf captures the behaviour of RDF reasoners.
Abstract reasoner rdfs evaluates Q w.r.t. A and a datalog program Prdfs that is constructed by translating each RDFS axiom  in T into an equivalent datalog rule; more
precisely, rdfs(, T , A) = f and rdfs(Q, T , A) = cert(Q, Prdfs , A). Thus, rdfs captures the
behaviour of RDFS reasoners such as Sesame.
Abstract reasoner rl evaluates Q w.r.t. A and a datalog program Prl that is constructed
by translating each OWL 2 RL axiom  in T into an equivalent datalog rule; more precisely,
rl(, T , A) = cert(, Prl , A) and rl(Q, T , A) = cert(Q, Prl , A). Thus, rl captures the behaviour
of OWL 2 RL reasoners such as Jena and Oracles Semantic Data Store.
Abstract reasoner classify first classifies T using a complete OWL 2 DL reasoner; that is,
it computes a TBox T  containing each subclass axiom A  B such that T |= A  B, and
A and B are atomic concepts occurring in T . The abstract reasoner then proceeds as rl,
but considers T  T  instead of T ; more precisely, classify(, T , A) = rl(, T  T  , Ain ) and
classify(Q, T , A) = rl(Q, T  T  , A). In this way, classify captures the behaviour of OWL 2
RL reasoners such as Minerva and DLE-Jena that try to be more complete by materialising
certain consequences of T .


An ideal abstract reasoner is one such that, for an arbitrary UCQ Q, TBox T , and
ABox A, we have ans(, T , A) = cert(, T , A), and ans(Q, T , A) = cert(Q, T , A) whenever
ans(, T , A) = f. We next introduce and discuss several properties of abstract reasoners
429

fiCuenca Grau, Motik, Stoilos & Horrocks

that are likely to aect how close they come to this ideal and that may also be relevant to
the applicability of our results.
The following notion of soundness describes abstract reasoners that return only answers
that logically follow from Q, T , and A.
Definition 3.4. An abstract reasoner ans for DL is sound if the following conditions hold
for each UCQ Q, DL-TBox T , and ABox A:
 ans(, T , A) = t implies cert(, T , A) = t; and
 ans(, T , A) = f implies ans(Q, T , A)  cert(Q, T , A).
The following notion of monotonicity describes abstract reasoners for which extending
the input TBox and ABox never leads to dropping answers. We also consider a weaker
notion of (Q, T )-monotonicity, in which the input query Q and TBox T are fixed.
Definition 3.5. An abstract reasoner ans for DL is monotonic if the following conditions
hold for each UCQ Q, all DL-TBoxes T and T  , and all ABoxes A and A such that T  T 
and A  A :
 ans(, T , A) = t implies ans(, T  , A ) = t; and
 ans(, T , A) = f and ans(, T  , A ) = f imply ans(Q, T , A)  ans(Q, T  , A ).
Given a UCQ Q and a DL-TBox T , ans is (Q, T )-monotonic if the following conditions
hold for all ABoxes A and A such that A  A :
 ans(, T , A) = t implies ans(, T , A ) = t; and
 ans(, T , A) = f and ans(, T , A ) = f imply ans(Q, T , A)  ans(Q, T , A ).
As discussed in Section 2.3, the logical consequences of a first-order theory are invariant
under renaming and merging of individuals. To define analogous properties for abstract
reasoners, we first introduce the notions of T -stable and (Q, T )-stable renamingsthat is,
renamings that leave all individuals occurring in T (respectively, in Q and T ) unchanged.
Definition 3.6. Let Q be a query, let T be a TBox, and let  be a renaming. Then,  is T stable if (a) = a for each individual a  dom()  ind(T ); furthermore,  is (Q, T )-stable
if (a) = a for each individual a  dom()  ind(Q  T ).
The following notion of weak faithfulness describes abstract reasoners whose answers
are invariant under replacement of individuals with fresh individuals. Furthermore, weak
(Q, T )-faithfulness relaxes this property to the case when Q and T are fixed.
Definition 3.7. An abstract reasoner ans for DL is weakly faithful if the following conditions hold for each UCQ Q, DL-TBox T , ABox A, injective renaming , and tuple a:
 ans(, T , A) = t and ind(T  A)  dom() imply ans(, (T ), (A)) = t; and
 ans(, T , A) = f, ind(Q  T  A)  dom(), and a  ans(Q, T , A) imply
ans(, (T ), (A)) = f and (a)  ans((Q), (T ), (A)).
430

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Given a UCQ Q and a DL-TBox T , ans is weakly (Q, T )-faithful if the following conditions
hold for each ABox A, injective renaming , and tuple a:
 ans(, T , A) = t, ind(T  A)  dom(), and  is T -stable imply ans(, T , (A)) = t;
and
 ans(, T , A) = f, ind(Q  T  A)  dom(),  is (Q, T )-stable, and a  ans(Q, T , A)
imply ans(, T , (A)) = f and (a)  ans(Q, T , (A)).
The following notion of strong faithfulness describes abstract reasoners whose answers
are invariant under merging of individuals. Furthermore, strong (Q, T )-faithfulness relaxes
this property to the case when Q and T are fixed.
Definition 3.8. An abstract reasoner ans for DL is strongly faithful if the following conditions hold for each UCQ Q, DL-TBox T , ABox A, renaming , and tuple a:
 ans(, T , A) = t implies ans(, (T ), (A)) = t; and
 ans(, T , A) = f, a  ans(Q, T , A), and ans(, (T ), (A)) = f imply
(a)  ans((Q), (T ), (A)).
Given a UCQ Q and a DL-TBox T , ans is strongly (Q, T )-faithful if the following conditions hold for each ABox A, renaming , and tuple a:
 ans(, T , A) = t and  is T -stable imply ans(, T , (A)) = t; and
 ans(, T , A) = f,  is (Q, T )-stable, a  ans(Q, T , A), and ans(, T , (A)) = f imply
(a)  ans(Q, T , (A)).
The results that we present in the rest of this paper are applicable only to abstract
reasoners that satisfy various combinations of these properties; as a minimum, we require
(Q, T )-monotonicity and weak (Q, T )-faithfulness. The abstract reasoners described in Example 3.3 all satisfy these properties. Testing if this is the case for concrete reasoners may,
however, be infeasible in practice; indeed, we are not aware of a technique that would allow one to check whether a concrete reasoner satisfies the required properties. We believe,
however, that all concrete reasoners commonly used in practice are intended to be sound,
monotonic, and at least weakly faithful, and that strong faithfulness is a reasonable assumption in most cases. If a concrete reasoner fails to satisfy some of these properties on
certain inputs, this is likely to be due to implementation bugs; thus, any consequent failure
of completeness can be seen as a bug, and detecting such situations should be viewed as a
part of a more general problem of testing software systems.
We next present several examples of abstract reasoners that do not satisfy some of the
mentioned properties.
Example 3.9. Consider an abstract reasoner that behaves as rdf whenever the number
of assertions in the input ABox is smaller than a certain threshold, and that returns the
empty set of answers for larger ABoxes. Intuitively, such an abstract reasoner characterises
a concrete RDF reasoner that processes inputs only up to a certain size. Such a reasoner is
not (Q, T )-monotonic for an arbitrary Q and T .

431

fiCuenca Grau, Motik, Stoilos & Horrocks

Example 3.10. Consider an abstract reasoner that behaves like rdf, but that, for trust
reasons, removes from each input ABox all assertions whose individuals are blacklisted
(e.g., they come from an untrusted source). Such an abstract reasoner is not weakly (Q, T )faithful for an arbitrary Q and T .

Example 3.10 suggests that, for an abstract reasoner to be weakly faithful, it should not
make decisions that depend on specific names of individuals.
Example 3.11. Consider an abstract reasoner rl= that, given as input a UCQ Q, a TBox
T , and an ABox A, proceeds as follows. First, rl= computes the ABox A obtained by
evaluating the datalog program Prl from Example 3.3 over A. Second, rl= computes the
query Q= obtained from Q by adding to the body of each rule r  Q an inequality x  y
for all pairs of distinct variables x and y occurring in r. Third, rl= evaluates Q= over A
by considering A as a databasethat is, as a finite first-order interpretation in which each
individual is mapped to itself (and thus dierent individuals are distinct). Thus, rl= characterises concrete reasoners that evaluate queries by matching dierent variables to dierent
individuals. Abstract reasoner rl= is sound, monotonic, and weakly faithful, but it is not
strongly faithful. For example, given query Q = {R(x, y)  Q(x)}, ABox A = {R(a, b)},
and renaming  = {a  c, b  c}, we have rl= (Q, , A) = {a}, but rl= (Q, , (A)) = . 
Example 3.11 suggests that, for an abstract reasoner to be strongly faithful, it should
allow distinct variables in queries and axioms to be mapped to the same individuals.
We next identify classes of abstract reasoners that we use throughout this paper. Note
that soundness is not required, which contributes to the generality of our results.
Definition 3.12. Given a UCQ Q and a TBox T , CwQ,T (CsQ,T ) is the class of all (Q, T )monotonic and weakly (strongly) (Q, T )-faithful abstract reasoners applicable to T .
Finally, note that all the abstract reasoners introduced in Example 3.3 are sound, monotonic, and strongly (and therefore also weakly) faithful. Consequently, all concrete reasoners
based on reasoning techniques outlined in Example 3.3 can be considered sound, monotonic,
and strongly faithful, modulo implementation bugs.
3.2 Completeness of Abstract Reasoners
We next define the central notion of abstract reasoner completeness for a given query Q
and TBox T . Intuitively, a (Q, T )-complete abstract reasoner is indistinguishable from a
complete abstract reasoner when applied to Q, T , and an arbitrary ABox A.
Definition 3.13. Let DL be a description logic, and let ans be an abstract reasoner for DL.
Then, ans is (Q, T )-complete for a UCQ Q and a DL-TBox T if the following conditions
hold for each ABox A:
 if cert(, T , A) = t, then ans(, T , A) = t;
 if cert(, T , A) = f and ans(, T , A) = f, then cert(Q, T , A)  ans(Q, T , A).
Finally, ans is complete if it is (Q, T )-complete for each UCQ Q and each DL-TBox T .
432

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Example 3.14. Consider the EL-TBox T consisting of the following axioms; the translation
of the axioms into first-order logic is shown after the  symbol.
takesCo.MathCo  St



MathSt  takesCo.MathCo



CalcCo  MathCo



St  Prof  



x, y.[takesCo(x, y)  MathCo(y)  St(x)]
x.[CalcCo(x)  MathCo(x)]

(7)
(8)

x.[MathSt(x)  y.[takesCo(x, y)  MathCo(y)]] (9)
x.[St(x)  Prof(x)  ]

(10)

Axiom (7) states that everyone taking a maths course is a student; axiom (8) states that
each calculus course is also a maths course; axiom (9) states that each maths student takes
some maths course; and axiom (10) states that no person can be both a student and a
professor. Axiom (8) is an RDFS axiom, and all other axioms in T apart from (9) are OWL
2 RL axioms. Consider also query (11) that retrieves students taking a maths course.
Q = {St(x)  takesCo(x, y)  MathCo(y)  Q(x)}

(11)

None of the abstract reasoners rdf, rdfs, rl, and classify from Example 3.3 are complete in
general for answering UCQs over EL-TBoxes. Furthermore, for Q and T from the previous
paragraph, abstract reasoners rdf, rdfs, and rl are not (Q, T )-complete, as all of them return
the empty set of answers for ABox A = {MathSt(c)}. In contrast, in the following sections
we will show that abstract reasoner classify is (Q, T )-completethat is, that it returns all
certain answers to Q, T , and an arbitrary ABox A.

3.3 Test Suites
Checking (Q, T )-completeness of a concrete reasoner by applying the reasoner to all possible
ABoxes and comparing the reasoners answers with that of a complete reasoner is clearly
infeasible in practice since there are infinitely many candidate input ABoxes. To obtain
a practical approach, we need a finite number of tests. We formalise this idea using the
following definition.
Definition 3.15. Let T be a TBox. A T -test suite is a pair S = S , SQ  where
 S is a finite set of ABoxes such that cert(, T , A) = t for each A  S , and
 SQ is a finite set of pairs A, Y where A is an ABox such that cert(, T , A) = f and
Y is a UCQ.
An abstract reasoner ans applicable to T passes a T -test suite S if ans satisfies the
following two conditions:
 for each A  S , we have ans(, T , A) = t, and
 for each A, Y  SQ , if ans(, T , A) = f, then cert(Y, T , A)  ans(Y, T , A).
Let Q be a UCQ, and let C be a class of abstract reasoners applicable to T . Then, S is
exhaustive for C and Q if each ans  C that passes S is (Q, T )-complete.
A T -test suite S is Q-simple if Q is the only query occurring in SQ ; then, SQ is commonly written as just a set of ABoxes, and A, Q  SQ is commonly abbreviated as A  SQ .
433

fiCuenca Grau, Motik, Stoilos & Horrocks

Intuitively, a T -test suite S = S , SQ  determines the tests that an abstract reasoner
should be subjected to. For a reasoner to pass S, it must correctly identify each ABox
A  S as unsatisfiable, and for each ABoxquery pair A, Y  SQ the reasoner must
correctly answer Y w.r.t. T and A.
Given Q and T , our goal is to identify a T -test suite S that is exhaustive for Qthat
is, a test suite such that each abstract reasoner that passes S is guaranteed to be (Q, T )complete. Depending on the properties of abstract reasoners, however, dierent test suites
may or may not achieve this goal. Therefore, the notion of exhaustiveness is relative to a
class of abstract reasoners C: if S is exhaustive for some class of abstract reasoners C, then
S can be used to test an arbitrary abstract reasoner in C. Note that S depends on the
target class of abstract reasoners, but not on the actual abstract reasoner being tested; in
order words, the construction of S depends on the properties that one can assume to hold
for the target abstract reasoner. Furthermore, if an abstract reasoner not contained in C
passes S, this will in general not imply a (Q, T )-completeness guarantee.
Example 3.16. Let Q and T be as specified in Example 3.14, and let A1 A6 be the
following ABoxes.
A1 = {takesCo(c, d), MathCo(d)}
A3 = {takesCo(c, d), CalcCo(d)}
A5 = {MathSt(c)}

A2 = {takesCo(c, c), MathCo(c)}
A4 = {takesCo(c, c), CalcCo(c)}
A6 = {St(c), Prof(c)}

In the following sections, we will show that the Q-simple T -test suite S = S , SQ  defined
by S = {A6 } and SQ = {A1 , . . . , A5 } is exhaustive for the class CwQ,T and Q; consequently,
S can be used to test all abstract reasoners from Example 3.3.
In particular, note that abstract reasoners rdf and rdfs fail all tests in S and SQ , and
that abstract reasoner rl fails the test A5  SQ ; furthermore, all failed tests provide a counterexample of (Q, T )-completeness. In contrast, abstract reasoner classify from Example
3.14 passes the tests in S, which implies that the abstract reasoner is indeed (Q, T )-complete.
Finally, consider a variant of the abstract reasoner classify that, similarly to the abstract
reasoner described in Example 3.9, returns the empty set of answers if the input ABox
contains more than, say, ten assertions. Such an abstract reasoner is not (Q, T )-monotonic
and hence does not belong to CwQ,T . This abstract reasoner clearly passes S; however, since
it does not belong to CwQ,T , passing S (correctly) does not imply that the abstract reasoner
is (Q, T )-complete.

We next state the following property, the proof of which is trivial.

Proposition 3.17. Let Q be a UCQ, let T be a TBox, and let C1 and C2 be classes of
abstract reasoners applicable to T such that C1  C2 .
1. If a T -test suite S is exhaustive for C2 and Q, then S is also exhaustive for C1 and Q.
2. If no T -test suite exists that is exhaustive for C1 and Q, then no T -test suite exists
that is exhaustive for C2 and Q.
Therefore, when proving existence of a T -test suite exhaustive for Q, the most general
result is the one that applies to the largest possible class of abstract reasoners. Furthermore,
434

fiCompleteness Guarantees for Incomplete Ontology Reasoners

in the following section we will identify cases for which no T -test suite exhaustive for Q
can be found; by Proposition 3.17 it suces to provide nonexistence results for the smallest
possible class of abstract reasoners.
We finish this section by pointing out an important practically relevant property of
Q-simple T -test suites, which has been illustrated in Example 3.16.
Proposition 3.18. Let S = S , SQ  be a Q-simple T -test suite and let ans be an abstract
reasoner applicable to T . If ans does not pass S, then ans is not (Q, T )-complete.
Proof. The ABox in S or SQ for which ans does not satisfy the conditions from Definition
3.15 is a counterexample for the (Q, T )-completeness of ans.
Thus, a Q-simple T -test suite S exhaustive for C and Q provides a sucient and necessary test for (Q, T )-completeness of the abstract reasoners in C. In contrast, if S is not
Q-simple, we show in Section 3.7 that then S provides only a sucient, but not also a
necessary test for (Q, T )-completeness of the abstract reasoners in C.
3.4 Negative Results
In Sections 3.5 (resp. Section 3.6) we identify restrictions on a UCQ Q and a TBox T
that guarantee existence of T -test suites exhaustive for CwQ,T (resp. CsQ,T ) and Q. Before
presenting these positive results, we first outline the limits of (Q, T )-completeness testing
and thus justify the restrictions we use in the following sections.
3.4.1 Monotonicity and Weak Faithfulness
Our approaches for testing (Q, T )-completeness of abstract reasoners are applicable only to
reasoners that are (Q, T )-monotonic and weakly (Q, T )-faithful. In this section, we provide
a formal justification for these requirements in the form of the following two theorems.
 Theorem 3.19 shows that exhaustive test suites do not exist if we consider the class
of abstract reasoners satisfying all properties from Section 3.1 apart from (Q, T )monotonicity; this includes soundness, strong faithfulness (which implies weak faithfulness), and monotonicity w.r.t. the TBox only.
 Theorem 3.20 shows that exhaustive test suites do not exist if we consider the class
of abstract reasoners satisfying all properties defined in Section 3.1 with the exception
of (Q, T )-weak faithfulness; these properties include soundness and monotonicity.
The negative results of Theorems 3.19 and 3.20 are very strong: they hold for smallest
classes of abstract reasoners we can define based on the notions introduced in Section 3.1 (by
Proposition 3.17, the smaller the class of abstract reasoners, the more general the negative
result); and they hold regardless of the Q and T considered (modulo a minor technicality:
unlike Theorem 3.19, Theorem 3.20 requires T to be satisfiable).
The proof of Theorem 3.19 can intuitively be understood as follows. We first assume
that S is a T -test suite exhaustive for Q and the class of abstract reasoners to which the
theorem applies. Then, we specify an abstract reasoner ans that does the right thing (i.e.,
it returns the correct answer) when it is given as input the query Q, the TBox T , and
435

fiCuenca Grau, Motik, Stoilos & Horrocks

an arbitrary ABox containing at most as many assertions as the largest test ABox in S;
otherwise, ans returns a sound, but an incomplete answer. We finally show the following
three properties of ans.
 Abstract reasoner ans belongs to the relevant class of abstract reasoners.
 Abstract reasoner ans passes S.
 Abstract reasoner ans is incomplete for at least one input ABox.
These three properties then show that S is not exhaustive for Q and the relevant class of
abstract reasoners. Intuitively, this means that the class of abstract reasoners is too large,
allowing abstract reasoners to treat their input in an erratic way.
Theorem 3.19. Let Q be an arbitrary UCQ, and let T be an arbitrary admissible TBox.
Then, no T -test suite exists that is exhaustive for Q and the class of all sound and strongly
faithful abstract reasoners applicable to T satisfying the following conditions for each TBox
T  with T  T  and each ABox A:
 ans(, T , A) = t implies ans(, T  , A) = t;
 ans(, T , A) = f and ans(, T  , A) = f imply ans(Q, T , A)  ans(Q, T  , A).
Proof. Consider an arbitrary T -test suite S = S , SQ . Let n be the maximum number of
assertions in an ABox from S. Furthermore, let ans be the abstract reasoner that takes as
input a UCQ Qin , an FOL-TBox Tin , and an ABox Ain . The result of ans(, Tin , Ain ) is
determined as follows.
1. Try to find a renaming  such that dom() = ind(T ) and (T )  Tin ; if no such 
exists, then return f.
2. If Ain contains at most n assertions, then check the satisfiability of (T )  Ain using
a sound and complete reasoner; return t if (T )  Ain is unsatisfiable.
3. Return f.
Furthermore, the result of ans(Qin , Tin , Ain ) is determined as follows.
4. Try to find a renaming  such that dom() = ind(Q  T ), (T )  Tin , and (Q) = Qin ;
if no such  exists, then return .
5. If Ain contains at most n assertions, then compute cert((Q), (T ), Ain ) using a sound
and complete reasoner and return the result.
6. Return .
Since T is admissible, checks in steps 2 and 5 can be performed in finite time; furthermore, step 1 can be realised by enumerating all mappings from ind(T ) to ind(Tin ), and step
4 can be realised analogously; consequently, ans can be implemented such that it terminates
on all inputs. To see that ans is sound and monotonic w.r.t. the TBox, consider arbitrary
input Qin , Tin , Tin , and Ain such that Tin  Tin .
436

fiCompleteness Guarantees for Incomplete Ontology Reasoners

 Assume that ans(, Tin , Ain ) = t. Then, on Qin , Tin , and Ain the abstract reasoner
returns in step 2 because (T )  Ain is unsatisfiable; but then, since (T )  Tin , we
have that Tin  Ain is unsatisfiable as well, as required for soundness. Furthermore,
since (T )  Tin  Tin , on Qin , Tin , and Ain the abstract reasoner returns in step 2
as well, so ans(, Tin , Ain ) = t, as required for monotonicity w.r.t. the TBox.
 Assume that a  ans(Qin , Tin , Ain ). Then, on Qin , Tin , and Ain the abstract reasoner
returns in step 5, and therefore we have a  cert((Q), (T ), Ain ); but then, since
(Q) = Qin and (T )  Tin , we have a  cert(Qin , Tin , Ain ), as required for soundness. Furthermore, since (T )  Tin  Tin , on Qin , Tin , and Ain the abstract reasoner
returns in step 5 as well, so a  ans(Qin , Tin , Ain ), as required for monotonicity w.r.t.
the TBox.
To see that ans is strongly faithful, consider an arbitrary renaming . If renaming 
exists such that (Q) = Qin and (T )  Tin , then clearly renaming   exists such that
  (Q) = (Qin ) and   (T )  (Tin ). Consequently, if ans(, Tin , Ain ) returns in step 2, then
ans(, (Tin ), (Ain )) returns in step 2 as well; similarly, if ans(Qin , Tin , Ain ) returns in step
5, then ans((Qin ), (Tin ), (Ain )) returns in step 5 as well; clearly, ans is strongly faithful.
Finally, it is straightforward to see that ans passes S.
Now let A be an ABox containing at least n + 1 assertions such that cert(Q, , A) = ;
such A clearly exists. If T  A is unsatisfiable, then ans(, T , A) = f; furthermore, if T  A
is satisfiable, then ans(Q, T , A) = ; consequently, ans is not (Q, T )-complete. Thus, S is
not exhaustive for Q and the class of abstract reasoners considered in this theorem.
We next prove Theorem 3.20. The proof is similar to the proof of Theorem 3.19, and
the main dierence is in the abstract reasoner ans we construct. In particular, given a test
suite S, we take ans to return the correct answer on the query Q, the TBox T , and each
ABox that contains only the individuals occurring in S; otherwise, the abstract reasoner
returns a sound, but an incomplete answer. Again, the class of the abstract reasoners is
too large, allowing ans to treat inputs in an erratic way.
Unlike Theorem 3.19, the following theorem requires T to be satisfiable; to understand
why, consider an arbitrary unsatisfiable TBox T and UCQ Q. Let S = S , SQ  be the
T -test suite defined by S = {} (i.e., S contains a single empty ABox) and SQ =  (i.e.,
SQ contains no ABoxes), and consider an arbitrary monotonic abstract reasoner ans that
passes S . Since ans passes S, we have ans(, T , ) = t; but then, since ans is monotonic,
for an arbitrary ABox A we have ans(, T , A) = t as well, which in turn implies that ans is
(Q, T )-complete. Failure to satisfy weak faithfulness is thus irrelevant if T is unsatisfiable.
Theorem 3.20. Let T be an arbitrary admissible and satisfiable TBox and let Q be an
arbitrary UCQ. Then, no T -test suite exists that is exhaustive for Q and the class of all
sound and monotonic abstract reasoners applicable to T .
Proof. Consider an arbitrary T -test suite S = S , SQ . Let I be the set of all individuals
occurring in S, Q, and T . Furthermore, let ans be the abstract reasoner that takes as
input a UCQ Qin , an FOL-TBox Tin , and an ABox Ain . The result of ans(, Tin , Ain ) is
determined as follows.
1. If T  Tin , then return f.
437

fiCuenca Grau, Motik, Stoilos & Horrocks

2. Let Ain,I be the set of assertions in Ain that mention only the individuals in I.
3. Check the satisfiability of T  Ain,I using a sound and complete reasoner; return t if
T  Ain,I is unsatisfiable, and return f otherwise.
Furthermore, given a UCQ Qin , the result of ans(Qin , Tin , Ain ) is determined as follows:
4. If T  Tin or Q =
 Qin , then return .
5. Let Ain,I be the set of assertions in Ain that mention only the individuals in I.
6. Compute cert(Q, T , Ain,I ) using a sound and complete reasoner and return the result.
That ans can be implemented such that it terminates on all inputs can be shown as in
the proof of Theorem 3.19. Furthermore, the soundness of ans follows from the following
two observations.
 Assume that ans(, Tin , Ain ) = t. Then, the abstract reasoner returns in step 3 since
T  Ain,I is unsatisfiable; but then, since T  Tin and Ain,I  Ain , we have that
Tin  Ain is unsatisfiable as well, as required.
 Assume that a  ans(Qin , Tin , Ain ). Then, the abstract reasoner returns in step 6,
and therefore we have a  cert(Q, T , Ain,I ); but then, since Q = Qin , T  Tin , and
Ain,I  Ain , we have a  cert(Qin , Tin , Ain ), as required.
For monotonicity, consider an arbitrary Tin and Ain such that Tin  Tin and Ain  Ain ;
clearly, we have T  Tin and Ain,I  Ain,I ; but then, by monotonicity of first-order logic,
ans(, Tin , Ain ) = t implies ans(, Tin , Ain ) = t, and ans(Q, Tin , Ain )  ans(Q, Tin , Ain ). Finally, it is straightforward to see that ans passes S.
Now consider an arbitrary ABox A such that ind(A)  I =  and cert(Q, , A) = ; such
A clearly exists. If T  A is unsatisfiable, since the ABox constructed in step 2 is empty
and T is satisfiable, we have ans(, T , A) = f; furthermore, if T  A is satisfiable, since the
ABox constructed in step 5 is empty, ans(Q, T , A) cannot contain individuals not occurring
in I; consequently, ans is not (Q, T )-complete. Thus, S is not exhaustive for Q and the
class of abstract reasoners considered in this theorem.
3.4.2 Monotonicity and Weak Faithfulness do not Suffice
Next, we show that (Q, T )-monotonicity and (Q, T )-faithfulness in general do not guarantee
existence of a T -test suite exhaustive for Q. In particular, Theorem 3.21 shows that, if T
contains a single recursive axiom, no test suite exists that is exhaustive for the class of
all sound, monotonic, and strongly faithful abstract reasoners (and by Proposition 3.17
for CsQ,T and CwQ,T as well, for each UCQ Q). Although our result is applicable only to a
particular Q and T , it is straightforward to adapt the proof to any TBox with a recursive
axiom that is relevant to the given query. Example 3.22, however, shows that the concept
of relevance is rather dicult to formalise: even if T entails a recursive axiom, this axiom
is not necessarily relevant to answering the query. In order not to complicate matters any
further, we state the following result for fixed Q and T , and we hope that our proof clearly
illustrates the limitations incurred by recursive axioms.
438

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Theorem 3.21. For Q = {A(x)  Q(x)} and T = {R.A  A}, no T -test suite exists
that is exhaustive for Q and the class of all sound, monotonic, and strongly faithful abstract
reasoners applicable to T .

Proof. Consider an arbitrary T -test suite S = S , SQ . Since S is a T -test suite, S
contains only ABoxes A such that T  A is unsatisfiable; clearly, no such ABox exists for T
as stated in the theorem, so S = . Let SQ be an arbitrary, but finite, set of pairs A, Y
with A an ABox and Y a UCQ, and let n be the maximum number of assertions in an ABox
in SQ . Furthermore, consider the following ABox, where ai = aj for all 1  i < j  n + 1:
An+1 = {R(a0 , a1 ), . . . , R(an , an+1 ), A(an+1 )}
We next construct an abstract reasoner pEvaln with the following properties:
(P1) for each A, Y  SQ , we have cert(Y, T , A)  pEvaln (Y, T , A);
(P2) a0  pEvaln (Q, T , An+1 ); and
(P3) pEvaln is sound, monotonic, and strongly faithful.
Note that a0  cert(Q, T , An+1 ), so the above three properties imply that S is not exhaustive
for Q and the class of abstract reasoners considered in this theorem.
Abstract reasoner pEvaln accepts as input an FOL-TBox Tin and an ABox Ain . The
result of pEvaln (, Tin , Ain ) is determined as follows.
1. Return f.

Furthermore, given a UCQ Qin , the result of pEvaln (Qin , Tin , Ain ) is determined as follows.
2. If T  Tin or Q =
 Qin , return .
3. Asat := Ain
4. Repeat the following computation n times:
 Asat := Asat  {(A(x)) |  is a substitution s.t. {(R(x, y)), (A(y))}  Asat }
5. Return cert(Q, , Asat ).

Abstract reasoner pEvaln clearly satisfies Property (P2) because deriving the assertion A(a0 )
requires n+1 iterations of the loop in step 4. Furthermore, pEvaln also satisfies (P1) because
every ABox A occurring in SQ contains at most n individuals and T can be seen as the
rule R(x, y)  A(y)  A(x), which pEvaln applies n times to the input ABox Ain .
We finally show (P3). Abstract reasoner pEvaln is clearly sound. Furthermore, for each
renaming  we have (T ) = T and (Q) = Q, so pEvaln is clearly strongly faithful.
To show that pEvaln is monotonic, consider arbitrary Tin , Tin , Ain , and Ain such that
Tin  Tin and Ain  Ain ; since pEvaln (, Tin , Ain ) = f for each input, the following are the
only relevant cases.
 pEvaln returns  in step 2 on input Qin , Tin , and Ain , in which case either T  Tin or
Q = Qin . Since Tin  Tin , clearly pEvaln also returns  in step 2 on input Qin , Tin ,
and Ain , and monotonicity holds.
439

fiCuenca Grau, Motik, Stoilos & Horrocks

 pEvaln returns in step 5 on input Qin , Tin , and Ain . Then, pEvaln can return in either
step 2 or step 5 on input Qin , Tin and Ain ; in the former case, monotonicity holds
trivially, and in the latter case, pEvaln (Qin , Tin , Ain )  pEvaln (Qin , Tin , Ain ) follows
directly from the fact that Ain  Ain .
The following example shows that the presence of recursive axioms in T does not preclude the existence of a T -test suite exhaustive for Q.
Example 3.22. Consider Q and T defined as follows:
Q = {A(x)  B(x)  Q(x)}
T = {R.A  A, B  R.A}
Note that T contains the axiom mentioned in Theorem 3.21; however, note also that
T |= B  A, and so
R = , {B(x)  Q(x)}

is a UCQ rewriting of Q w.r.t. T . In Section 3.5 we show that the existence of a UCQ rewriting of Q w.r.t. T guarantees existence of a Q-simple T -test suite that is exhaustive for CwQ,T
(and hence also for CsQ,T ) and Q; for example, S = , { {B(a)} } is one such T -test suite.
Intuitively, T |= B  A is the only consequence of T that is relevant for answering Q; hence,
for T  = {B  A} and Q = {A(x)  Q(x)}, we have that cert(Q, T , A) = cert(Q , T , A) for
an arbitrary ABox A. Hence, the recursive axiom in T is irrelevant for answering Q, and
therefore its presence in T does not preclude the existence of a T -test suite that is exhaustive
for CwQ,T and Q.

3.5 Testing (Q, T )-Monotonic and Weakly (Q, T )-Faithful Abstract Reasoners

In this section, we identify a sucient condition that guarantees existence of a Q-simple
T -test suite S exhaustive for CwQ,T and Q; by Proposition 3.17, this result applies to CsQ,T
as well. Roughly speaking, such S can always be computed by instantiating the rules in a
UCQ rewriting of Q w.r.t. T in a suitable way. The requirement that Q should be UCQrewritable w.r.t. T invalidates the negative result of Theorem 3.21 since no UCQ rewriting
of Q = {A(x)  Q(x)} w.r.t. T = {R.A  A} exists.
This result allows one to compute Q-simple T -test suites exhaustive for Q in numerous
practically relevant cases. In particular, a UCQ rewriting is guaranteed to exist if T is
expressed in the DLs underpinning the QL profile of OWL 2 (Motik et al., 2009a; Calvanese
et al., 2007); furthermore, as illustrated in Example 3.22, a UCQ rewriting may exist even
if T is expressed in other fragments of OWL 2 such as the OWL 2 EL (Motik et al., 2009a;
Baader, Brandt, & Lutz, 2005). In practice, such rewritings can be computed using systems
such as QuOnto (Acciarri et al., 2005) and REQUIEM (Perez-Urbina et al., 2010).
We establish the desired result in two steps. First, in Section 3.5.1 we present a general
characterisation of Q-simple T -test suites exhaustive for CwQ,T and Q. Then, in Section
3.5.2 we use this characterisation to establish the desired connection between rewritings
and Q-simple T -test suites exhaustive for Q.
440

fiCompleteness Guarantees for Incomplete Ontology Reasoners

3.5.1 Characterisation of Simple and Exhaustive Test Suites
We next prove that a Q-simple T -test suite S = S , SQ  is exhaustive for CwQ,T and Q if
and only if S contains an isomorphic copy of each data pattern (i.e., a subset of an ABox)
that can produce a certain answer to Q and  w.r.t. T , but that preserves the identity of
the individuals occurring in T and Q. To show that this is not just a sucient, but also
a necessary condition for the existence of an exhaustive T -test suite, we observe that, if S
does not contain one such copy of a data pattern, we can always find an abstract reasoner
in CwQ,T that passes S but that misses certain answers obtained via the missing data pattern
and that is therefore not (Q, T )-complete.
Theorem 3.23. Let Q be a UCQ, let T be an admissible TBox, and let S = S , SQ  be
a Q-simple T -test suite. Then, S is exhaustive for CwQ,T and Q if and only if the following
properties are satisfied for each ABox A.
1. If T  A is unsatisfiable, then there exist an ABox A  S and an injective T -stable
renaming  such that dom() = ind(T  A ) and (A )  A.
2. If T  A is satisfiable, then for each tuple a  cert(Q, T , A) there exist an ABox
A  SQ , a tuple b  cert(Q, T , A ), and an injective (Q, T )-stable renaming  such
that (b) = a, dom() = ind(Q  T  A ), and (A )  A.
Proof. () Let S be an arbitrary Q-simple T -test suite that satisfies Properties 1 and 2;
we next show that S is exhaustive for CwQ,T and Q. Consider an arbitrary abstract reasoner
ans  CwQ,T that passes Sthat is, ans satisfies the following two properties:
(a) ans(, T , A ) = t for each A  S , and
(b) ans(, T , A ) = f implies cert(Q, T , A )  ans(Q, T , A ) for each A  SQ .
We next show that ans is (Q, T )-completethat is, that ans satisfies the two conditions in
Definition 3.13 for an arbitrary ABox A. For an arbitrary such A, we have the following
two possibilities, depending on the satisfiability of T  A.
Assume that T  A is unsatisfiable. Since S satisfies Property 1, there exist an ABox
A  S and an injective T -stable renaming  s.t. dom() = ind(T  A ) and (A )  A.
By Condition (a) we have ans(, T , A ) = t. Since ans is weakly (Q, T )-faithful,  is injective
and T -stable, and dom() = ind(T  A ), we have ans(, T , (A )) = t; finally, since ans is
(Q, T )-monotonic and (A )  A, we have ans(, T , A) = t, as required by Definition 3.13.
Assume that T  A is satisfiable and ans(, T , A) = f. Furthermore, consider an arbitrary tuple a  cert(Q, T , A). Since S satisfies Property 2, there exist an ABox A  SQ ,
a tuple b  cert(Q, T , A ), and an injective (Q, T )-stable renaming  such that (b) = a,
dom() = ind(Q  T  A ), and (A )  A. Since (A )  A, ans(, T , A) = f, and ans is
(Q, T )-monotonic, we have ans(, T , (A )) = f; furthermore, ind(T  A )  dom(),  is
injective and (Q, T )-stable, and ans is weakly (Q, T )-faithful, so ans(, T , (A )) = f implies ans(, T , A ) = f. But then, by Condition (b) we have cert(Q, T , A )  ans(Q, T , A ),
so b  ans(Q, T , A ). Since ans is weakly (Q, T )-faithful,  is injective and (Q, T )-stable,
and dom() = ind(Q  T  A ), we have (b)  ans(Q, T , (A )); since (b) = a, we have
441

fiCuenca Grau, Motik, Stoilos & Horrocks

a  ans(Q, T , (A )); finally, since ans is (Q, T )-monotonic and (A )  A, we then have
a  ans(Q, T , A), as required by Definition 3.13.

() Assume that S is exhaustive for CwQ,T and Q; we next show that Properties 1 and 2
are satisfied for an arbitrary ABox A. To this end, we consider a particular abstract reasoner
ans for which we prove that ans  CwQ,T and that ans passes S; this abstract reasoner will
help us identify the ABox, the tuple, and the renaming required to prove Properties 1 and 2.
Let ans be the abstract reasoner that takes as input a UCQ Qin , an FOL-TBox Tin ,
and an ABox Ain . The result of ans(, Tin , Ain ) is determined as follows.
1. If T =
 Tin , then return f.

2. For each ABox A  S , do the following.
(a) Check the satisfiability of T  A using a sound, complete, and terminating
reasoner.
(b) If T  A is unsatisfiable, and if an injective T -stable renaming  exists such that
dom() = ind(T  A ) and (A )  Ain , then return t.
3. Return f.
Furthermore, the result of ans(Qin , Tin , Ain ) is determined as follows.
4. If T =
 Tin or Q =
 Qin , then return .
5. Out := .
6. For each tuple a of constants occurring in Ain of arity equal to the arity of the query
predicate of Q, and for each A  SQ do the following.
(a) Compute C := cert(Q, T , A ) using a sound, complete and terminating reasoner.
(b) If a tuple b  C and an injective (Q, T )-stable renaming  exist such that
(b) = a, dom() = ind(Q  T  A ), and (A )  Ain , then add a to Out.
7. Return Out.
We next show that ans belongs to CwQ,T ; to this end, we prove that ans terminates on
all inputs, and that it is (Q, T )-monotonic and weakly (Q, T )-faithful.
Termination. Since T is admissible, checking satisfiability of T  A and the computation
of cert(Q, T , A ) are decidable, so the relevant sound, complete and terminating reasoners
exist. Furthermore, checking whether a T -stable (resp. (Q, T )-stable) injective renaming 
exists can be done by enumerating all renamings from ind(T  A ) (resp. ind(Q  T  A ))
to ind(T  Ain ) (resp. ind(Q  T  Ain )). Therefore, ans can be implemented such that it
terminates on each input.
(Q, T )-Monotonicity. Consider arbitrary input Qin , Tin , Ain , and Ain such that Ain  Ain .
 Assume that ans(, Tin , Ain ) = t, so on Tin and Ain the abstract reasoner terminates
in step 2(b) for some A  S and . But then, since (A )  Ain  Ain , on Tin
and Ain the abstract reasoner also terminates in step 2(b), so ans(, Tin , Ain ) = t, as
required.
442

fiCompleteness Guarantees for Incomplete Ontology Reasoners

 Assume that ans(, Tin , Ain ) = f and ans(, Tin , Ain ) = f, and consider an arbitrary
tuple a  ans(Qin , Tin , Ain ). Then a is added to Out in step 7(b) for some A  SQ
and . But then, since (A )  Ain  Ain , on Qin , Tin , and Ain the abstract reasoner
also adds a to Out in step 7(b), so a  ans(Qin , Tin , Ain ), as required.
Weak (Q, T )-Faithfulness. Consider an arbitrary input Qin , Tin , and Ain , and an arbitrary
injective renaming .
 Assume that ans(, Tin , Ain ) = t, dom()  ind(T  A), and  is T -stable. Thus, on
Tin and Ain the abstract reasoner terminates in step 2(b) for some A  S and . Let
 be the renaming such that  (c) = ((c)) for each c  ind(T  A ). Clearly, we have
dom( ) = ind(T  A ), renaming  is T -stable and injective, and  (A )  (Ain ).
Thus, on Tin and (Ain ) the abstract reasoner terminates in step 2(b), so we therefore
have ans(, Tin , (Ain )) = t, as required.
 Assume that ans(, Tin , Ain ) = f, dom()  ind(Q  T  A), and  is (Q, T )-stable,
and consider an arbitrary truple a  ans(Qin , Tin , Ain ). Then a is added to Out in step
7(b) for some A  SQ , , and b. Let  be the renaming defined s.t.  (c) = ((c))
for each individual c  ind(Q  T  A ). Clearly, we have dom( ) = ind(Q  T  A ),
renaming  is (Q, T )-stable and injective,  (A )  (Ain ), and  (b) = (a). Thus,
on Qin , Tin , and (Ain ) the abstract reasoner terminates in step 7(b) and we clearly
have (a)  ans(Qin , Tin , (Ain )), as required.
This concludes the proof that ans  CwQ,T . Furthermore, ans clearly passes S; but then,
since S is exhaustive for CwQ,T and Q, abstract reasoner ans is (Q, T )-complete. We next
prove the main claim of this theorem. To this end, consider an arbitrary ABox A; we have
the following possibilities, depending on the satisfiability of T  A.
 Assume that T  A is unsatisfiable. Then ans(, T , A) = t, so the abstract reasoner
returns in step 2(b) for some ABox A  S and some T -stable renaming  such that
(A )  A and dom() = ind(T  A ). Thus, Property 1 holds as required.
 Assume that T  A is satisfiable, and consider an arbitrary tuple a  cert(Q, T , A).
Then ans(, T , A) = f and a  ans(Q, T , A), so a is added to Out in step 7(b) for
some ABox A  SQ , tuple b  cert(Q, T , A ), and an injective (Q, T )-stable renaming
 such that (b) = a, dom() = ind(Q  T  A ), and (A )  A. Thus, Property 2
holds as required.
The following example illustrates Theorem 3.23.
Example 3.24. Let Q and T be as specified in Example 3.14, and let S = S , SQ  be
specified in Example 3.16. As we show in Section 3.5.2, S is exhaustive for CwQ,T and Q.
Consider now an ABox A = {St(a), MathSt(b), takesCo(a, b1 )}. Clearly, T  A is satisfiable and cert(Q, T , A) = {b}. By Theorem 3.23, this certain answer can be obtained by
evaluating Q w.r.t. T and an ABox in SQ . Indeed, note that ABox A5  SQ is isomorphic
to the subset A = {MathSt(b)} of A via renaming  = {b  c}, and that applying Q to T
and A5 produces c, which is isomorphic to b via .
443

fiCuenca Grau, Motik, Stoilos & Horrocks

Note also that, if we remove A5 from S, we no longer have a T -test suite that is
exhaustive for Q. For example, abstract reasoner rl from Example 3.16 would pass such a
test suite, but it would not return the required certain answers when applied to A5 (and,
consequently, when applied to A either).

3.5.2 Computing Test Suites Exhaustive for CwQ,T

Based on Theorem 3.23, in this section we show that a T -test suite exhaustive for CwQ,T and
Q an be obtained by instantiating a UCQ rewriting R of Q w.r.t. T that is, by replacing all
variables in R with individuals in all possible ways. Please note that such an instantiation
must be full, in the sense that all possible replacements must be considered. This is because
the class CwQ,T can contain abstract reasoners such as rl= from Example 3.11 that are not
strongly faithful and that may incorrectly handle the case when distinct variables are bound
to the same individuals.
Definition 3.25. Let I be a set of individuals, let r be a datalog rule, and let  be a
substitution. Then,  is an instantiation substitution for r w.r.t. I if (x)  I for each
variable x occurring in r. If the latter holds, then the instantiation of r w.r.t.  is the ABox
Ar := {(B) | B  body(r)}.
Let Q be a UCQ, let T be a TBox, let R = R , RQ  be a UCQ rewriting of Q w.r.t.
T , let m be the maximum number of distinct variables occurring in a rule in R, and let I
be a set containing all individuals occurring in R, Q, and T , as well as m fresh individuals.
R,I
R,I
The full instantiation of R w.r.t. I is the pair ER,I = ER,I
and ER,I
Q
 , EQ  where E
are the smallest sets of ABoxes such that
 Ar  ER,I
 for each r  R and each instantiation substitution  for r w.r.t. I, and
 Ar  ER,I
Q for each r  RQ and each instantiation substitution  for r w.r.t. I such
that cert(, R , Ar ) = f.
ER,I is clearly unique up to the renaming of the fresh individuals in I, so I is typically left
R
implicit, and one talks of the full instantiation ER = ER
 , EQ  of R.
Example 3.26. Let Q and T be as specified in Example 3.14, and let R = R , RQ  be
such that R = {St(x)  Prof(x)  } and RQ consists of the following datalog rules:
takesCo(x, y)  MathCo(y)  Q(x)
takesCo(x, y)  CalcCo(y)  Q(x)
MathSt(x)  Q(x)

Then, R is a UCQ rewriting of Q w.r.t. T , and one can see that the Q-simple T -test suite
S = S , SQ  from Example 3.16 is the full instantiation of R.


The following theorem shows that the full instantiation of a UCQ rewriting of Q w.r.t.
T is a Q-simple T -test suite exhaustive for CwQ,T and Q. According to this theorem, the
T -test suite S in Example 3.26 is exhaustive for CwQ,T and Q.
444

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Theorem 3.27. Let Q be a UCQ, let T be an admissible TBox, let R = R , RQ  be a
R
UCQ rewriting of Q w.r.t. T , and let ER = ER
 , EQ  be the full instantiation of R. Then,
ER is a Q-simple T -test suite that is exhaustive for CwQ,T and Q.

Proof. Let I be the set of individuals that ER is obtained from. We first show that ER is
a Q-simple T -test suitethat is, that it satisfies the two properties in Definition 3.15.
 Consider an arbitrary ABox A  ER
 . Then, a rule r  R and an instantiation
substitution  for r exist such that A = Ar ; clearly cert(, {r}, A) = t; since R is a
UCQ rewriting, T  A is unsatisfiable, as required.
 Consider an arbitrary ABox A  ER
Q . Then, cert(, R , A) = f by Definition 3.25;
since R is a UCQ rewriting, T  A is satisfiable, as required.
We next show that ER satisfies Properties 1 and 2 of Theorem 3.23 for an arbitrary
ABox A.
(Property 1) Assume that T  A is unsatisfiable. Since R is a UCQ rewriting, by
Definition 2.2 we have cert(, R , A) = t; but then, a rule r  R and a substitution 
exist such that Ar  A and cert(, {r}, Ar ) = t. Let  be an injective renaming such that
for each individual c occurring in R or T we have (c) = c, and for each individual d
occurring in Ar but not in R and T we have that (d) is a fresh individual in I; such 
exists since the number of variables in r is smaller or equal to the number of fresh individuals
in I. Let  be an instantiation substitution for r such that (x) = ((x)) for each variable
R
x occurring in r; then Ar  ER
 holds since E is the full instantiation of R w.r.t. I. Let 
be any injective renaming that coincides with the inverse of  on each individual occurring
in Ar , R, or T ; such  exists since  is injective and the range of  contains each individual
occurring in Ar , R, and T . Clearly (Ar ) = Ar holds, so (Ar )  A. Furthermore,  is
clearly T -stable. Thus, Property (1) is satisfied for Ar  ER
 and .
(Property 2) Assume that T  A is satisfiable, and consider an arbitrarily chosen tuple
a  cert(Q, T , A). Since R is a UCQ rewriting, by Definition 2.2 we have cert(, R , A) = f
and a  cert(RQ , R , A); but then, clearly a  cert(RQ , , A) as well. Then, a rule r  RQ
and a substitution  exist such that Ar  A and a  cert({r}, , Ar ). Let  be an injective
renaming such that for each individual c occurring in R, Q, or T we have (c) = c, and
for each individual d occurring in Ar but not in R, Q, and T we have that (d) is a fresh
individual in I; such  clearly exists since the number of variables in r is smaller or equal
to the number of fresh individuals in I. Let  be an instantiation substitution for r such
R is
that (x) = ((x)) for each variable x occurring in r; then Ar  ER
Q holds since E
the full instantiation of R w.r.t. I. Let  be any injective renaming that coincides with
the inverse of  on each individual occurring in Ar , R, Q, or T ; such  exists since 
is injective and the range of  contains each individual occurring in Ar , R, Q, and T .
Furthermore, clearly a tuple b  cert({r}, , Ar ) exists such that (head(r)) = Q(b); since
R is a UCQ rewriting and T  Ar is satisfiable, we have b  cert(Q, T , Ar ); furthermore,
since  is injective, (b) = a clearly holds. But then, Property (2) is satisfied for Ar  ER
Q,

, and b.
445

fiCuenca Grau, Motik, Stoilos & Horrocks

3.5.3 Minimising Exhaustive Test Suites
In practice, it is clearly beneficial to compute test suites that are as small as possible.
This goal can be achieved by applying known techniques for minimising UCQ rewritings
(Calvanese et al., 2007; Perez-Urbina, Horrocks, & Motik, 2009). By Theorem 3.27, the
smallest such rewriting can be instantiated to obtain an exhaustive test suite.
State of the art query rewriting systems employ subsumption and condensation techniques in order to reduce the size of a rewriting. A datalog rule r subsumes a datalog rule
r if a substitution  exists such that (r)  r ; intuitively, r is then more general than r .
If a rewriting contains such rules r and r , then r can be safely removed from the rewriting.
Furthermore, if a rule r contains distinct unifiable body atoms Bi and Bj , a condensation
of r is the rule (r) where  is the most general unifier of Bi and Bj . If a rewriting contains
such rule r and (r) subsumes r, the rule can safely be replaced with (r). The following
example illustrates how these techniques can be used to obtain small test suites.
Example 3.28. Let Q and T be as specified in Example 3.14, and let R be the rewriting
of Q w.r.t. T from Example 3.26. Then R = R , RQ  where RQ consists of the following
rules is also a UCQ rewriting of Q w.r.t. T .
takesCo(x, y)  takesCo(x, z)  MathCo(y)  Q(x)

(12)

takesCo(x, y)  CalcCo(y)  Q(x)

(14)

MathSt(x)  Q(x)

(16)

takesCo(x, x)  CalcCo(x)  MathCo(x)  Q(x)

(13)

St(x)  MathSt(x)  Q(x)

(15)

By Theorem 3.27, the full instantiation of R is also a T -test suite exhaustive for CwQ,T and
Q. The rewriting R , however, contains redundancy and hence the resulting test suite is
unnecessarily large. In particular, by applying condensation to query (12), subsumption to
queries (13) and (14), and subsumption again to queries (15) and (16), we can obtain the
simpler rewriting R.

Finally, note that the test suites obtained via full instantiation can contain isomorphic
ABoxes. Clearly, all isomorphic copies of an ABox can safely be eliminated from a test
suite without losing exhaustiveness for CwQ,T and Q.
3.6 Testing (Q, T )-Monotonic and Strongly (Q, T )-Faithful Abstract Reasoners

Due to full instantiation, test suites obtained by Definition 3.25 can be exponentially larger
than the rewriting they are generated from. As a result, even rewritings of moderate size can
yield test suites containing thousands of ABoxes. Intuitively, full instantiation is required
to obtain a test suite exhaustive for the class CwQ,T because this class contains abstract
reasoners such as rl= from Example 3.11, which do not correctly handle the case when
distinct variables in a query are matched to the same individual.
In this section, we show that test suites exhaustive for the class CsQ,T can be obtained by
an injective instantiation of a rewritingthat is, by replacing each variable with a distinct
fresh individual. Test suites obtained in such a way are linear in the size of the rewriting,
and are thus substantially smaller than test suites obtained by full instantiation.
446

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Example 3.29. Let Q and T be as specified in Example 3.14, and let S = S , SQ  be the
Q-simple T -test suite from Example 3.16. Furthermore, consider the abstract reasoner rl=
from Example 3.11 that is weakly, but not strongly (Q, T )-faithful. It is easy to check that
rl= returns complete answers on A1 and A3 , but not on A2 and A4 . Therefore, by Theorem
3.27, for S to be exhaustive for CwQ,T and Q, we must include in SQ ABoxes A2 and A4 ,
which are respectively obtained from ABoxes A1 and A3 by merging individual d into c.
Strongly (Q, T )-faithful abstract reasoners, however, correctly handle inputs obtained
by merging individuals. Based on this observation, in this section we show that the Q-simple
T -test suite S = S , SQ  where SQ = {A1 , A3 , A5 }, obtained by injectively instantiating
the rewriting R from Example 3.26, is exhaustive for CsQ,T and Q.



As in Section 3.5, we first develop a characterisation of Q-simple T -test suites that are
exhaustive for CsQ,T and Q; this result is analogous to Theorem 3.23.

Theorem 3.30. Let Q be a UCQ, let T be an admissible TBox, and let S = S , SQ  be
a Q-simple T -test suite. Then, S is exhaustive for CsQ,T and Q if and only if the following
properties are satisfied for each ABox A.
1. If T  A is unsatisfiable, then there exist an ABox A  S and a T -stable renaming
 such that dom() = ind(T  A ) and (A )  A.
2. If T  A is satisfiable, then for each tuple a  cert(Q, T , A) there exist an ABox
A  SQ , a tuple b  cert(Q, T , A ), and a (Q, T )-stable renaming  such that (b) = a,
dom() = ind(Q  T  A ), and (A )  A.
Proof. () Let S be an arbitrary Q-simple T -test suite that satisfies Properties 1 and 2;
we next show that S is exhaustive for CsQ,T and Q. Consider an arbitrary abstract reasoner
ans  CsQ,T that passes Sthat is, ans satisfies the following two properties:
(a) ans(, T , A ) = t for each A  S , and
(b) ans(, T , A ) = f implies cert(Q, T , A )  ans(Q, T , A ) for each A  SQ .
We next show that ans is (Q, T )-completethat is, that ans satisfies the two conditions in
Definition 3.13 for an arbitrary ABox A. For an arbitrary such A, we have the following
two possibilities, depending on the satisfiability of T  A.
Assume that T  A is unsatisfiable. Since S satisfies Property 1, there exist an ABox
A  S and a T -stable renaming  such that dom() = ind(T  A ) and (A )  A. By
Condition (a) we have ans(, T , A ) = t. Since ans is strongly (Q, T )-faithful and  is T stable, we have ans(, T , (A )) = t; finally, since ans is (Q, T )-monotonic and (A )  A,
we have ans(, T , A) = t, as required by Definition 3.13.
Assume that T  A is satisfiable and ans(, T , A) = f. Furthermore, consider an arbitrary tuple a  cert(Q, T , A). Since S satisfies Property 2, there exist an ABox A  SQ , a
tuple b  cert(Q, T , A ), and a (Q, T )-stable renaming  such that (b) = a, (A )  A, and
dom() = ind(Q  T  A ). Since (A )  A, ans(, T , A) = f, and ans is (Q, T )-monotonic,
we have ans(, T , (A )) = f; furthermore,  is (Q, T )-stable and ans is strongly faithful, so ans(, T , (A )) = f implies ans(, T , A ) = f. But then, by Condition (b) we have
cert(Q, T , A )  ans(Q, T , A ), so b  ans(Q, T , A ). Now ans is strongly (Q, T )-faithful and
447

fiCuenca Grau, Motik, Stoilos & Horrocks

 is (Q, T )-stable, so (b)  ans(Q, T , (A )); since (b) = a, we have a  ans(Q, T , (A ));
finally, since ans is (Q, T )-monotonic and (A )  A, we have a  ans(Q, T , A), as required
by Definition 3.13.
() Assume that S is exhaustive for CsQ,T and Q; we next show that Properties 1 and 2
are satisfied for an arbitrary ABox A. To this end, we consider a particular abstract reasoner
ans for which we prove that ans  CsQ,T and that ans passes S; this abstract reasoner will
help us identify the ABox, the tuple, and the renaming required to prove Properties 1 and 2.
Let ans be the abstract reasoner that takes as input a UCQ Qin , an FOL-TBox Tin ,
and an ABox Ain . The result of ans(, Tin , Ain ) is determined as follows.
1. If T =
 Tin , then return f.
2. For each ABox A  S , do the following.
(a) Check the satisfiability of T  A using a sound, complete, and terminating
reasoner.
(b) If T  A is unsatisfiable, and if a T -stable renaming  exists such that
dom() = ind(T  A ) and (A )  Ain , then return t.
3. Return f.
Furthermore, the result of ans(Qin , Tin , Ain ) is determined as follows.
4. If T =
 Tin or Q =
 Qin , then return .
5. Out := .
6. For each tuple a of constants occurring in Ain of arity equal to the arity of the query
predicate of Q, and for each A  SQ do the following.
(a) Compute C := cert(Q, T , A ) using a sound, complete and terminating reasoner.
(b) If a tuple b  C and a (Q, T )-stable renaming  exist such that (b) = a,
dom() = ind(Q  T  A ), and (A )  Ain , then add a to Out.

7. Return Out.
We next show that ans belongs to CsQ,T . The proofs that ans terminates and that it
is (Q, T )-monotonic are analogous to the proofs in Theorem 3.23. To show strong (Q, T )faithfulness, consider an arbitrary Qin , Tin , and Ain , and an arbitrary renaming .
 Assume that ans(, Tin , Ain ) = t and  is T -stable. Thus, on Tin and Ain the abstract
reasoner terminates in step 2(b) for some A  S and . Let  be the renaming such
that  (c) = ((c)) for each c  ind(T  A ). Clearly, we have dom( ) = ind(T  A ),
renaming  is T -stable, and  (A )  (Ain ). Thus, on Tin and (Ain ) the abstract
reasoner terminates in step 2(b), so we have ans(, Tin , (Ain )) = t, as required.
 Assume that ans(, Tin , Ain ) = f and  is (Q, T )-stable, and consider an arbitrary
tuple a  ans(Qin , Tin , Ain ). Then a is added to Out in step 7(b) for some A  SQ ,
448

fiCompleteness Guarantees for Incomplete Ontology Reasoners

, and b. Let  be the renaming defined such that  (c) = ((c)) for each individual c  ind(Q  T  A ). Clearly, we have dom( ) = ind(Q  T  A ), mapping  is
(Q, T )-stable,  (A )  (Ain ), and  (b) = (a). Thus, on Qin , Tin , and (Ain ) the
abstract reasoner terminates in step 7(b), so (a)  ans(Qin , Tin , (Ain )), as required.
This concludes the proof that ans  CsQ,T . Furthermore, ans clearly passes S; but then,
since S is exhaustive for CsQ,T and Q, abstract reasoner ans is (Q, T )-complete. The main
claim of this theorem can now be shown as in Theorem 3.23.
We next use Theorem 3.30 to show that a Q-simple T -test suite that is exhaustive for
and Q can be obtained as an injective instantiation of a UCQ rewriting of Q w.r.t. T .

CsQ,T

Definition 3.31. Let Q be a UCQ, let T be a TBox, let R = R , RQ  be a UCQ rewriting
of Q w.r.t. T , and let  be a substitution mapping each variable occurring in R into a distinct
R,
fresh individual. The injective instantiation of R w.r.t.  is the pair IR, = IR,
 , IQ 
where IR,
and IR,
are the smallest sets of ABoxes such that
Q

 Ar  IR,
for each r  R , and

 Ar  IR,
for each r  RQ such that cert(, R , Ar ) = f.
Q
IR, is clearly unique up to the renaming of the fresh individuals in , so  is typically left
R
implicit, and one talks of the injective instantiation IR = IR
 , IQ  of R.
Theorem 3.32. Let Q be a UCQ, let T be an admissible TBox, let R = R , RQ  be a
R
UCQ rewriting of Q w.r.t. T , and let IR = IR
 , IQ  be the injective instantiation of R.
Then, IR is a Q-simple T -test suite that is exhaustive for CsQ,T and Q.

Proof. Let  be the substitution that IR is obtained from. We first show that IR is a
Q-simple T -test suitethat is, that it satisfies the two properties in Definition 3.15.
r
 Consider an arbitrary A  IR
 . Then, a rule r  R exist such that A = A ; clearly
cert(, {r}, A) = t; since R is a UCQ rewriting, T  A is unsatisfiable, as required.

 Consider an arbitrary A  IR
Q . Then, cert(, R , A) = f by Definition 3.31; since R is
a UCQ rewriting, T  A is satisfiable, as required.
We next show that IR satisfies Properties 1 and 2 of Theorem 3.30 for an arbitrary
ABox A.
(Property 1) Assume that T  A is unsatisfiable. Since R is a UCQ rewriting, by
Definition 2.2 we have cert(, R , A) = t; but then, a rule r  R and a substitution 
exist such that Ar  A and cert(, {r}, Ar ) = t. Let  be a renaming such that for each
individual c occurring in R or T we have (c) = c, and for each variable x in r we have
((x)) = (x). Clearly, (Ar ) = Ar , so (Ar )  A. Furthermore, it is clear that  is
T -stable. Thus, Property (1) holds for Ar  IR
 and .
(Property 2) Assume that T  A is satisfiable, and consider an arbitrarily chosen tuple
a  cert(Q, T , A). Since R is a UCQ rewriting, by Definition 2.2 we have cert(, R , A) = f
and a  cert(RQ , R , A); but then, clearly a  cert(RQ , , A) as well. Then, a rule r  RQ
449

fiCuenca Grau, Motik, Stoilos & Horrocks

and a substitution  exist such that Ar  A and a  cert({r}, , Ar ). Let  be the renaming
such that for each individual c occurring in R, Q, or T we have (c) = c, and for each
variable x in r we have ((x)) = (x). Clearly, (Ar ) = Ar , so (Ar )  A. Furthermore,
it is clear that  is (Q, T )-stable. Finally, clearly a tuple b  cert({r}, , Ar ) exists such
that (head(r)) = Q(b); since R is a UCQ rewriting and T  Ar is satisfiable, we have
b  cert(Q, T , Ar ); furthermore, (b) = a clearly holds. But then, Property (2) is satisfied

for Ar  IR
,
,
and b.
Q
3.7 Dealing with Recursive Axioms
The negative result in Theorem 3.21 (which applies to both CwQ,T and CsQ,T ) depends on
the presence of a recursive axiom in the TBox; thus, the positive results in Sections 3.5
and 3.6 require the input UCQ to be rewritable w.r.t. the input TBox, which eectively
prohibits recursion in TBox axioms. Instead of disallowing recursive axioms, in this section
we overcome the limitation of Theorem 3.21 by placing additional requirements on the
abstract reasoners by requiring them to be first-order reproducible. Intuitively, the latter
means that the reasoners behaviour can be seen as complete reasoning in some unknown
first-order theory. Such abstract reasoners are not allowed to partially evaluate recursive
axioms, which invalidates the approach used to prove Theorem 3.21.
We show that a T -test suite exhaustive for Q and the class of first-order reproducible
abstract reasoners can be obtained by instantiating a datalog, rewriting of Q w.r.t. T .
Such rewritings exist for a wide range of TBoxes and queries, which in turn allows our
results to be applicable to a range of practically interesting cases. In contrast to test
suites computed from a UCQ rewriting, however, the test suites obtained from a datalog,
rewriting may not be Q-simple. In fact, we show in Section 3.7.2 that, for certain Q
and T , a T -test suite exhaustive for Q and the class of first-order reproducible abstract
reasoners exists, but no such test suite is Q-simple. This has an important practicallyrelevant consequence: if a T -test suite S is not Q-simple, a first-order reproducible abstract
reasoner that passes S is guaranteed to be (Q, T )-complete; however, if an abstract reasoner
does not pass S, in general we cannot conclude that the reasoner is not (Q, T )-complete.
3.7.1 First-Order Reproducible Abstract Reasoners
State of the art concrete reasoners such as Oracles reasoner, Jena, OWLim, Minerva, Virtuoso, and DLE-Jena are all implemented as RDF triple stores extended with deductive
database features. Given T and A as input, these reasoners first precompute all assertions
that follow from T  A in a preprocessing step. In practice, this step is commonly implemented by (a technique that can be seen as) evaluating a datalog program over A. After
preprocessing, these reasoners can then answer an arbitrary UCQ Q by simply evaluating
Q in the precomputed set of assertions.
Motivated by this observation, we next introduce the new class of first-order reproducible
abstract reasonersthat is, abstract reasoners whose behaviour can be conceived as complete reasoning in some unknown first-order theory. Note that this theory is not required to
be a datalog program; for example, it can contain existential quantifiers, which can be used
to capture the behaviour of concrete reasoners such as Jena and OWLim (Bishop, Kiryakov,
450

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Ognyano, Peikov, Tashev, & Velkov, 2011) that handle existential quantifiers in the input
by introducing fresh individuals.
Definition 3.33. An abstract reasoner ans for a description logic DL is first-order reproducible if, for each DL-TBox T , a set of first-order sentences FT exists such that, for each
ABox A,
 ans(, T , A) = cert(, FT , A), and

 if ans(, T , A) = f, then for each UCQ Q, we have ans(Q, T , A) = cert(Q, FT , A).

If FT contains predicates and/or individuals not occurring in T , these are assumed to be
internal to ans and not accessible in queries, TBoxes, ABoxes, test suites, and so on.
Given a TBox T , CfT is the class of all first-order reproducible abstract reasoners applicable
to T .
Example 3.34. Abstract reasoners rdf, rdfs, rl and classify from Example 3.3 are all firstorder reproducible. Indeed, theory FT is empty in the case of rdf, and it is precisely Prdfs
and Prl in the cases of rdfs and rl, respectively. Finally, for abstract reasoner classify, theory
FT is the union of Prl and the program containing the axiom x.[A(x)  B(x)] for each
atomic subsumption A  B entailed by the input TBox.


Please note that a first-order reproducible abstract reasoner ans does not need to actually construct FT : it only matters that some (possibly unknown) theory FT exists that
characterises the reasoners behaviour as specified in Definition 3.33.
Since QFT A |= QFT A whenever A  A , each first-order reproducible abstract
reasoner is (Q, T )-monotonic for arbitrary Q and T . Furthermore, it is straightforward
to see that each first-order reproducible abstract reasoner is also strongly (Q, T )-faithful.
Consequently, we have CfT  CsQ,T for each UCQ Q and each TBox T .
We next show that the negative result in Theorem 3.21 does not directly apply to
the class CfT . In particular, we show that the abstract reasoner pEvaln used to prove
Theorem 3.21 is not first-order reproducible. Intuitively, pEvaln can be understood as
partial evaluation of a datalog programthat is, the rules in the program are applied to
the facts only a fixed number of times rather then until a fixpoint is reached.
Proposition 3.35. For each positive integer n, the abstract reasoner pEvaln defined in the
proof of Theorem 3.21 is not first-order reproducible.
Proof. Let T = {R.A  A}, let Q = {A(x)  Q(x)}, and consider an arbitrary nonnegative integer n. Furthermore, assume that pEvaln  CfT ; then, a finite set of first-order
sentences FT exists such that pEvaln (Q, T , A) = cert(Q, FT , A) for each ABox A.
Let k be a positive integer; furthermore, let rk be the datalog rule and let Ak be the
ABox defined as follows, for a0 , . . . , ak arbitrary distinct but fixed individuals not occurring
in Q  FT :
rk = R(x0 , x1 )  . . .  R(xk1 , xk )  A(xk )  A(x0 )
Ak = {R(a0 , a1 ), . . . , R(ak1 , ak ), A(ak )}
The following condition holds by Proposition 2.1:
FT |= rk

if and only if FT  Ak |= A(a0 )
451

(17)

fiCuenca Grau, Motik, Stoilos & Horrocks

By the definition of pEvaln , we have
a0  pEvaln (Q, T , Ak ) for each 1  k  n, and
a0  pEvaln (Q, T , Ak ) for each k > n.
Since pEvaln (Q, T , A) = cert(Q, FT , A), we have
a0  cert(Q, FT , Ak ) for each 1  k  n, and
a0  cert(Q, FT , Ak ) for each k > n.
Since Q contains only the atom A(x) in the body, we have
FT  Ak |= A(a0 ) for each 1  k  n, and
FT  Ak |= A(a0 ) for each k > n.
By condition (17), we then have
FT |= rk for each 1  k  n
FT |= rk for each k > n.
This, however, contradicts the obvious observation that r1 |= rk for each k  1.
Note that the proof of Proposition 3.35 relies on the fact that the theory FT only depends
on the input TBox, and not on the input query. As shown next, had we defined first-order
reproducible abstract reasoners by allowing FT to depend also on the input query, then the
negative result from Theorem 3.21 would have applied.
Definition 3.36. An abstract reasoner ans for DL first-order q-reproducible if, for each
UCQ Q and each DL-TBox T , a finite set of first-order sentences FQ,T exists such that,
for each ABox A,
 ans(, T , A) = cert(, FQ,T , A), and
 if ans(, T , A) = f, then ans(Q, T , A) = cert(Q, FQ,T , A).
Theorem 3.37. For Q = {A(x)  Q(x)} and T = {R.A  A}, no T -test suite exists
that is exhaustive for Q and the class of all sound, monotonic, strongly faithful, and qreproducible abstract reasoners applicable to T .
Proof. To prove this claim, it suces to show that, for each nonnegative integer n, the
abstract reasoner pEvaln defined in the proof of Theorem 3.21 is first-order q-reproducible.
Consider an arbitrary nonnegative integer n, an arbitrary DL-TBox T  , and an arbitrary
UCQ Q . We define FQ ,T  such that, if T  T  or Q = Q, then FQ ,T  = ; otherwise,
FQ ,T  consists of the following n rules:
A(x0 )  Q(x0 )

R(x0 , x1 )  A(x1 )  Q(x0 )
...

R(x0 , x1 )  R(x1 , x2 )  . . .  R(xn1 , xn )  A(xn )  Q(x0 )
452

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Clearly, pEvaln (, T  , A ) = cert(, FQ ,T  , A ) = f for each UCQ Q , DL-TBox T  and
ABox A , as required. Furthermore, for each Q and T  such that either T  T  or Q = Q
and each ABox A , we have pEvaln (Q , T  , A ) = cert(Q , FQ ,T  , A ) = . Finally, for
Q = Q, each T  such that T  T  , and each ABox A , we clearly have pEvaln (Q , T  , A ) =
cert(Q , FQ ,T  , A ), as required.
3.7.2 Simple vs. Non-Simple Test Suites
Proposition 3.18 from Section 3.3 shows that each Q-simple T -test suite that is exhaustive
for Q and a class of abstract reasoners provides a sucient and necessary test for (Q, T )completeness. We next show that an analogous result does not hold if T contains recursive
axioms, even if we consider only first-order reproducible abstract reasoners. As in Theorem 3.21, we prove the claim for a fixed Q and T since the concept of relevant recursive
axioms might be dicult to formalise; however, our proof can easily be adapted to other
UCQs and TBoxes. Our result essentially states that no T -test suite exists that provides
a necessary and sucient condition for (Q, T )-completeness of each abstract reasoner in
CfT ; consequently, by Proposition 3.18 each T -test suite exhaustive for CfT and Q is not
Q-simple. Furthermore, in Section 3.7.3 we show how to compute a T -test suite exhaustive
for CfT and Q, so the following claim does not hold vacuously.
Theorem 3.38. Let Q = {A(x)  B(x)  Q(x)}, let T = {R.A  A}, and let C be the
class of all sound, monotonic, strongly faithful, and first-order reproducible abstract reasoners applicable to T . Then, no T -test suite S exists that satisfies the following two properties:
1. S is exhaustive for C and Q; and
2. for each abstract reasoner ans  C, if ans is (Q, T )-complete then ans passes S.
Proof. Assume that a T -test suite S = S , SQ  exists that satisfies properties 1 and 2 of
the theorem. Let n be the maximal number of assertions occurring in an ABox in S. We
next define two abstract reasoners ans1 and ans2 ; it is straightforward to check that both
are sound, monotonic, strongly faithful, and first-order reproducible.
Given an arbitrary FOL-TBox Tin , abstract reasoner ans1 uses the datalog program
FT1in defined as follows:
 If T  Tin , then FT1in = .
 If T  Tin , then FT1in contains the following n rules:
r0 =
r1 =
r2 =

B(x0 )  A(x0 )  A(x0 )
B(x0 )  R(x0 , x1 )  A(x1 )  A(x0 )
B(x0 )  R(x0 , x1 )  R(x1 , x2 )  A(x3 )  A(x0 )
...
rn = B(x0 )  R(x0 , x1 )  . . .  R(xn1 , xn )  A(xn )  A(x0 )
Given an arbitrary FOL-TBox Tin , abstract reasoner ans2 uses the datalog program
defined as follows, where predicate Z is private to FT2in (and hence it does not aect
the soundness of the abstract reasoner):
FT2in

453

fiCuenca Grau, Motik, Stoilos & Horrocks

 If T  Tin , then FT2in = .
 If T  Tin , then FT2in contains FT1in as well as the following rules:
rZ1 = R(x0 , x1 )  . . .  R(xn , xn+1 )  A(xn+1 )  Z(x0 )
rZ2 =
R(x0 , x1 )  Z(x1 )  Z(x0 )
rZ3 =
Z(x)  B(x)  A(x)
Now let A be an arbitrary ABox containing at most n assertions. We next show
that, for each assertion  not containing predicate Z, we have FT1in  A |=  if and only
if FT2in  A |= . The () direction is trivial since FT1in  FT2in , so we consider the ()
direction. Furthermore, since rZ3 is the only rule in FT2in \ FT1in that does not contain
Z in the head, the claim is nontrivial only if  is of the form A(a0 ) for some individual a0 occurring in A. Since the antecedent of rZ3 is satisfied for a0 , we have B(a0 )  A
and FT2in  A |= Z(a0 ). But then, for the latter to be implied by rZ1 and rZ2 , individuals
a0 , a1 , . . . , ak with 0  k exist such that R(ai , ai+1 )  A for each 1  i < k, and A(ak )  A.
Since A contains at most n assertions, w.l.o.g. we can assume that k  n. But then, since
FT1in contains rule rk , we have FT1in  A |= A(a0 ) as well, which proves our claim. As a consequence of this claim and the fact that all ABoxes in S contain at most n assertions, we have
cert(, FT1in , A) = cert(, FT2in , A) for each A  S , and cert(Y, FT1in , A) = cert(Y, FT2in , A)
for each A, Y  SQ .
Let A = {B(a0 ), R(a0 , a1 ), . . . , R(an , an+1 ), A(an+1 )}. Then cert(Q, T , A) = {a0 } and
cert(Q, FT1in , A) = , so ans1 is not (Q, T )-complete. Since S is exhaustive for C and Q,
abstract reasoner ans1 does not pass S; by the claim from the previous paragraph, abstract
reasoner ans2 does not pass S either. We next show that ans2 is (Q, T )-complete, which
contradicts the assumption that S satisfies property 2 and thus proves the claim of this
theorem.
Consider an arbitrary ABox A containing m assertions. Clearly, a0  cert(Q, T , A) if and
only if individuals a0 , a1 , . . . , ak with 0  k  m exist such that B(a0 )  A, R(ai , ai+1 )  A
for each 1  i < k, and A(ak )  A. Now assume that k  n; since rk  FT2in , we have
FT2in  A |= A(a0 ) and thus a0  cert(Q, FT2in , A). In contrast, assume that k > n; since
rZ1  FT2in , we have FT2in  A |= Z(akn1 ); since rZ2  FT2in , we have FT2in  A |= Z(ai )
for each 0  i  k  n  1; finally, since rZ3  FT2in , we have FT2in  A |= A(a0 ); but then,
a0  cert(Q, FT2in , A), as required.
As a corollary to Theorem 3.38, we next show that testing abstract reasoners in CfT
cannot be done in general using Q-simple test suites.
Corollary 3.39. For Q = {A(x)  B(x)  Q(x)} and T = {R.A  A}, no Q-simple T test suite exists that is exhaustive for Q and the class of all sound, monotonic, strongly
faithful, and first-order reproducible abstract reasoners applicable to T .
Proof. If S is a Q-simple T -test suite that is exhaustive for Q and the class mentioned in
the Theorem, by Proposition 3.18 each abstract reasoner ans from the class that does not
pass S is not (Q, T )-complete, which contradicts Theorem 3.38.
454

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Theorem 3.38 eectively says that, if an abstract reasoner ans  CfT does not pass a T test suite S, we cannot conclude that ans is not (Q, T )-complete. Please note that this holds
only if ans fails a test of the form A, Y where Q =
 Y: if Q = Y, then A is a counterexample
to (Q, T )-completeness of ans. Thus, S may show ans to be not (Q, T )-complete, but it is
not guaranteed to do so. This is illustrated by the following example.
Example 3.40. Let Q = {A(x)  B(x)  Q(x)} and let T = {R.A  A, R.C  C}. Furthermore, let S = , SQ  be the general test suite defined as follows:
SQ = {

 { A(c) },
{ A(x)  B(x)  Q(x) } ,
 { R(c, d), A(d) }, { A(c)  Q } ,
 { R(c, d), C(d) }, { C(c)  Q } 
}

Let R = RD , , Q where RD = {R(x, y)  A(y)  A(x), R(x, y)  C(y)  C(x)}; clearly,
R is a rewriting Q w.r.t. T . In Section 3.7.3 we show how to compute S from R using a
variant of injective instantiation in a way that guarantees exhaustiveness for CfT and Q.
Now let ans1  CfT be the abstract reasoner defined by FT1 = {R(x, y)  A(y)  A(x)}.
The reasoner does not pass S since cert({C(c)  Q }, FT1 , {R(c, d), C(d)}) = f. Note, however, that the reasoner is (Q, T )-complete. Thus, if a test suite is not Q-simple, passing
it is a sucient, but not a necessary condition for (Q, T )-completeness. In fact, note that
T contains the TBox for Theorem 3.38, so by the theorem we cannot reduce S so that it
correctly identifies all reasoners in CfT that are not (Q, T )-complete.
In practice, however, one can try to mitigate this fundamental theoretical limitation by
eliminating the irrelevant axioms from the rewriting R and thus increasing the likelihood of
obtaining a T -test suite that a (Q, T )-complete abstract reasoner will pass. For example,
using the techniques by Cuenca Grau, Horrocks, Kazakov, and Sattler (2008a) we can
extract the module of R relevant to the query. In the example from the previous paragraph,
this would remove the rule R(x, y)  C(y)  C(x) from R, and injective instantiation will
produce the test suite S = , SQ  where SQ is defined as follows:
SQ = {

 { A(c) },
{ A(x)  B(x)  Q(x) } ,
 { R(c, d), A(d) }, { A(c)  Q } 
}

Abstract reasoner ans1 from the previous paragraph now passes S and is thus guaranteed
to be (Q, T )-complete.
Now let ans2 be the abstract reasoner defined by FT2 = {B(x)  R(x, y)  A(y)  A(x)}.
Clearly, abstract reasoner ans2 is not (Q, T )-complete, so ans2 does not pass SQ . From the
latter, however, we cannot immediately conclude that S is not (Q, T )-complete: the test
that fails does not involve the original query Q. As a possible remedy, we can try to unfold
R to a certain level and then injectively instantiate the result in hope of obtaining a T -test
suite that will identify ans2 as not being (Q, T )-complete. In particular, the first unfolding
of R produces the following query:
B(x)  R(x, y)  A(y)  Q(x)

Instantiating this rewriting produces the following test suite, which does not prove that
ans2 is not (Q, T )-complete.
SQ = {

 { B(c), R(c, d), A(d) }, { A(x)  B(x)  Q(x) }  }
455

fiCuenca Grau, Motik, Stoilos & Horrocks

Another round of unfolding, however, produces the following query:
B(x)  R(x, y)  R(y, z)  A(z)  Q(x)
Instantiating this query produces the following test suite:
S
Q ={

 { B(c), R(c, d), R(d, e), A(e) }, { A(x)  B(x)  Q(x) }  }

Now ans2 does not pass S
Q , so we can conclude that ans2 is not (Q, T )-complete.



To better understand Example 3.40, consider a first-order reproducible abstract reasoner
ans, an arbitrary UCQ Q, and a TBox T such that R = RD , , RQ  is a datalog rewriting
of Q w.r.t. T . Datalog program RD  RQ is equivalent to the (possibly infinite) UCQ RuQ
obtained from RD  RQ via exhaustive unfolding. We now have the following possibilities.
First, assume that ans is not (Q, T )-complete. Since RD  RQ is equivalent to RuQ , each
certain answer a to Q w.r.t. T and an arbitrary ABox A is produced by some r  RuQ .
But then, the injective instantiation Ar of r will provide us with the counterexample for
the (Q, T )-completeness of ans. Thus, we can prove that ans is not (Q, T )-complete by
generating the elements of RuQ in a fair manner (i.e., without indefinitely delaying the
generation of some element of RuQ ) and checking whether cert(Q, T , Ar )  ans(Q, T , Ar );
we are guaranteed to eventually encounter some r  RuQ that invalidates this condition and
thus proves that ans is not (Q, T )-complete.
Second, assume that ans is (Q, T )-complete. Using the above approach, we will determine that cert(Q, T , Ar )  ans(Q, T , Ar ) holds for each r  RuQ . Now if RuQ is finite (i.e., if
the unfolding of RD  RQ terminates), then RuQ is the UCQ rewriting of Q w.r.t. T , so by
the results from Section 3.6 we can conclude that ans is indeed (Q, T )-complete. If, however, RuQ is infinite, then we will never obtain a sucient assurance for the (Q, T )-complete
of ans. In the following section we show a possible remedy to this problem.
3.7.3 Testing First-Order Reproducible Abstract Reasoners
In this section, we show how to compute a T -test suite S = S , SQ  exhaustive for CfT
and Q from a datalog, rewriting R = RD , R , RQ  of Q w.r.t. T . Since first-order
reproducible abstract reasoners are strongly faithful, we need to consider only injective
instantiations of R. Thus, the rules in R and RQ should be instantiated as in Section 3.6.
A rule r  RD , however, is instantiated into a pair A, Y  SQ with A the ABox obtained
by instantiating the body of r and Y the Boolean UCQ obtained by instantiating the head
of r. Intuitively, such tests allow us to check whether the (unknown) first-order theory FT
that captures the behaviour of the abstract reasoner entails r.
Definition 3.41. Let Q be a UCQ with query predicate Q, let T be an admissible TBox,
let R = RD , R , RQ  be a datalog, rewriting of Q w.r.t. T , and let  be a substitution
mapping each variable occurring in R into a distinct fresh individual. The injective instanR,
R,
tiation of R w.r.t.  is the pair IR, = IR,
is the smallest set of ABoxes
 , IQ  where I
and IR,
is the smallest set of pairs of an ABox and a UCQ such that
Q
 Ar  IR,
for each r  R ,

456

fiCompleteness Guarantees for Incomplete Ontology Reasoners

 Ar , Q  IR,
for each r  RQ such that cert(, RD  R , Ar ) = f, and
Q
 Ar , Y  IR,
for each r  RD of the form (6) such that cert(, RD  R , Ar ) = f,
Q
where Y is the UCQ Y = {i ((x), yi )  Q | 1  i  m} with the propositional query
predicate Q .
IR, is clearly unique up to the renaming of the fresh individuals in , so  is typically left
R
implicit, and one talks of the injective instantiation IR = IR
 , IQ  of R.
Example 3.42. Consider the query Q = {A(x)  Q(x)} and the EL-TBox T consisting of
the following axioms, whose translation into first-order logic is shown after the  symbol.
R.A  B



BC



AD 



R.C  A



C  R.D



x, y.[R(x, y)  A(y)  B(x)]
x, y.[R(x, y)  C(y)  A(x)]
x.[B(x)  C(x)]

x.[C(x)  y.[R(x, y)  D(y)]]
x.[A(x)  D(x)  ]

Then, R = RD , R , RQ  as defined next is a datalog rewriting of Q w.r.t. T .
RD = { R(x, y)  A(y)  B(x), R(x, y)  C(y)  A(x), B(x)  C(x) }
R = { A(x)  D(x)   }

RQ = { A(x)  Q(x) }

R
The injective instantiation IR = IR
 , IQ  of R is shown below.

IR
 = { { A(c), D(c) } }

IR
{ A(x)  Q(x) } ,
Q = {  { A(c) },
 { R(c, d), A(d) }, { B(c)  Q } ,
 { R(c, d), C(d) }, { A(c)  Q } ,
 { B(c) },
{ C(c)  Q } 
}



We now show that the injective instantiation of a datalog, rewriting of Q w.r.t. T is
a T -test suite exhaustive for CfT and Q.
Theorem 3.43. Let Q be a UCQ, let T be a TBox, let R = RD , R , RQ  be a datalog,
R
R
rewriting of Q w.r.t. T , and let IR = IR
 , IQ  be the injective instantiation of R. Then, I
is a T -test suite that is exhaustive for CfT and Q.
Proof. Let  be the substitution that IR is obtained from. We first show that IR is a T -test
suite.
r
 Consider an arbitrary A  IR
 . Then, a rule r  R exist such that A = A ; clearly
cert(, {r}, A) = t, so cert(, RD  R , A) = t as well; since R is a datalog, rewriting of Q w.r.t. T , we have that T  A is unsatisfiable, as required.

457

fiCuenca Grau, Motik, Stoilos & Horrocks

 Consider an arbitrary A  IR
Q . Then, cert(, RD  R , A) = f by Definition 3.41;
,
since R is a datalog
rewriting of Q w.r.t. T , we have that T  A is satisfiable, as
required.
To show that IR is exhaustive for CfT and Q, consider an arbitrary abstract reasoner
ans  CfT that passes IR that is, ans satisfies the following two properties:
(a) ans(, T , A ) = t for each A  IR
 , and
(b) ans(, T , A ) = f implies cert(Y, T , A )  ans(Y, T , A ) for each Y, A   IR
Q.
Since ans is first-order reproducible, a set of first-order sentences FT exists such that, for
each ABox A, we have
 ans(, T , A) = cert(, FT , A), and
 if ans(, T , A) = f, then ans(Q, T , A) = cert(Q, FT , A).
By the assumption on FT in Definition 3.33 and the fact that  maps variables to fresh
individuals, we have rng()  ind(FT ) = .
Let R1D and R2D be the smallest sets of rules satisfying the following conditions for each
rule r  RD :
 cert(, FT , Ar ) = t implies r  R1D , where r is obtained from r by replacing the head
with , and
 cert(, FT , Ar ) = f implies r  R2D .
Furthermore, let R1Q and R2Q be the sets of rules obtained from RQ in an analogous way.
Since R1D  R2D is obtained from RD by replacing some head formulae with , we clearly
have R1D  R2D |= RD ; analogously, we have R1Q  R2Q |= RQ .

We next show that FT |= R ; the latter holds if and only if FT |= r for each rule r  R .
Consider an arbitrary rule r  R ; note that head(r) = . Then, by Definition 3.41 we have
r
r
Ar  IR
 ; by (a) we have ans(, T , A ) = t; by Definition 3.33 we have cert(, FT , A ) = t
and hence FT  Ar |= ; finally, since rng()  ind(FT ) = , by Proposition 2.1 we have
FT |= r, as required.
We next show that FT |= R1D ; the latter holds if and only if FT |= r for each rule r  R1D .
Consider an arbitrary rule r  R1D ; note that head(r) = . Then, by the definition of R1D
we have cert(, FT , Ar ) = t and hence FT  Ar |= ; finally, since rng()  ind(FT ) = , by
Proposition 2.1 we have FT |= r, as required.
In a completely analogous way as in the previous paragraph, it is possible to show that
FT |= R1Q .
We next show that FT |= R2D ; the latter holds if and only if FT |= r for each rule
r  R2D . Consider an arbitrary rule r  R2D of the form (6); by the definition of R2D we have
cert(, FT , Ar ) = f, so by Definition 3.33 we have ans(, T , Ar ) = f. Then, by Definition
3.41 we have Ar , Y  IR
x), yi )  Q | 1 i  m}. Note
Q where Y is the UCQ Y = {i ((
that T |= r by Definition 2.2, so by Proposition 2.1 we have that T  Ar |= m
x), yi );
i=1 i ((
by the definition of Y and the fact that Q does not occur in T , we have Y  T  Ar |= Q ;
458

fiCompleteness Guarantees for Incomplete Ontology Reasoners

but then, cert(Y, T , Ar ) = t. The latter observation, ans(, T , Ar ) = f, and (b) then imply
ans(Y, T , Ar ) = t, so by Definition 3.33 we have cert(Y, FT , Ar ) = t. Since Q occurs only

in Y (note that each predicate occurring
m in FT but not in T is private to FT , so Q cannot
r
occur in FT ), we have FT  A |= i=1 i ((x), yi ). Finally, since rng()  ind(FT ) = , by
Proposition 2.1 we have FT |= r, as required.
We next show that Q  FT |= R2Q ; the latter holds if and only if Q  FT |= r for each
rule r  R2Q . Consider an arbitrary rule r  R2Q ; note that head(r) is an atom with predicate
Q, and that by the definition of R2Q we have cert(, FT , Ar ) = f, so by Definition 3.33 we
have ans(, T , Ar ) = f. Furthermore, by Definition 3.41, we have cert(, RD  R , Ar ) = f.
Let a be the tuple of the arguments in (head(r)). Then, by Definition 3.41 we have
Ar , Q  IR
a  cert({r}, , Ar ), but then we have a  cert(RQ , RD  R , Ar )
Q ; clearly, 
by the monotonicity of first-order logic. Since R is a rewriting of Q w.r.t. T , by Definition 2.2 we have a  cert(Q, T , Ar ). The latter observation, ans(, T , Ar ) = f, and (b)
then imply a  ans(Q, T , Ar ). By Definition 3.33 we have a  cert(Q, FT , Ar ); hence,
FT  Ar |= Q(a). Finally, since we have rng()  ind(FT ) = , by Proposition 2.1 we have
FT |= r, as required.
The following table summarises the entailment relationships between various first-order
theories obtained thus far:
FT |= R
Q  FT |= R2Q

FT |= R1D
R1Q  R2Q |= RQ

FT |= R2D
R1D  R2D |= RD

FT |= R1Q

Clearly, this implies the following entailments:
FT |= RD  R

Q  FT |= RD  R  RQ

We now complete the proof of this theorem and show that ans is (Q, T )-complete. To
this end, consider an arbitrary ABox A; we have the following possibilities, depending on
the satisfiability of T  A.
 Assume that T  A is unsatisfiable. Then cert(, RD  R , A) = t by Definition
2.2; by the above mentioned entailments, we have cert(, FT , A) = t; consequently,
ans(, T , A) = t by Definition 3.33, as required.
 Assume that T  A is satisfiable and ans(, T , A) = f, and consider an arbitrary tuple
a  cert(Q, T , A). Then, cert(, RD  R , A) = f and a  cert(RQ , RD  R , A) by
Definition 2.2. By the above mentioned entailments, we have a  cert(Q, FT , A);
hence, a  ans(Q, T , A) by Definition 3.33, as required.
Note that the size of the test suite obtained by Theorem 3.43 is linear in the size of the
rewriting, which, we believe, makes our approach suitable for use in practice.
3.7.4 Testing Ground Queries
As shown in Section 3.7.2, if an abstract reasoner ans  CfT does not pass a T -test suite S
that is not Q-simple, we cannot always conclude that ans is not (Q, T )-complete. From a
practical point of view, it would be highly beneficial to identify situations where not passing
S would show that ans is indeed incomplete for Q and T . Furthermore, in applications where
459

fiCuenca Grau, Motik, Stoilos & Horrocks

prototypical queries are not known at design time, we would like to design completeness
tests that are query-independentthat is, which test an abstract reasoner for completeness
w.r.t. T regardless of the input data and query. In this section, we show that we can
achieve these two goals by focusing on ground queries. This restriction is not unreasonable
in practice, since any SPARQL query can be equivalently expressed as a ground UCQ.
We first define a query-independent notion of exhaustiveness of a test suite.
Definition 3.44. Let T be a TBox, let S be a T -test suite, and let C be a class of abstract
reasoners applicable to T . Then, S is exhaustive for C and all ground UCQs if each ans  C
that passes S is (Q, T )-complete for each ground UCQ Q.
Then, we define the notion of a ground rewriting of T a rewriting that captures all
query answers w.r.t. T , regardless of the input ground query and ABoxand we show how
to instantiate such ground rewritings.

Definition 3.45. A ground rewriting of a TBox T is a pair R = RD , R  such that, for
each ground UCQ Q, the triple RD , R , Q is a datalog rewriting of T w.r.t. Q. An

injective instantiation IR of such R is defined as IR = IR for R = RD , R , .
Note that Definition 3.45 implies that each variable occurring in the head of a rule
in R also occurs in the rule body. Tools such as REQUIEM and KAON2 can easily be
adapted to compute a ground rewriting of a TBox T in practice. We next show that
injective instantiation of a ground rewriting of T yields a T -test suite that provides us with
sucient and necessary check for completeness w.r.t. all ground UCQs.

Theorem 3.46. Let T be a TBox, and let R = RD , R  be a ground rewriting of T .
Then, the following two claims hold.
1. IR is exhaustive for CfT and all ground UCQs.
2. Each abstract reasoner ans  CfT that does not pass IR is not (Q, T )-complete for some
ground UCQ Q.

Proof. (Property 1) Consider an arbitrary abstract reasoner ans  CfT that passes IR . Let
FT be the first-order theory that characterises the behaviour of ans; as in the proof of
Theorem 3.43, the fact that ans passes IR implies FT |= RD  R . Furthermore, consider
an arbitrary ground UCQ Q and an arbitrary ABox A. That ans is (Q, T )-complete can
be shown as in the proof of Theorem 3.43, with the minor dierence that a  cert(Q, T , A)
implies a  cert(Q, RD  R , A) by Definition 3.45.
(Property 2) Note that, since R is a ground rewriting of T , by Definition 3.41 all UCQs
in IR are ground. Thus, if some abstract reasoner ans  CfT does not pass IR , this clearly
shows that ans is not (Q, T )-complete for some ground UCQ Q.

4. Comparing Incomplete Abstract Reasoners
In this section, we investigate techniques that, given a query Q and a TBox T , allow us to
determine whether an abstract reasoner ans2 is more complete than an abstract reasoner
ans1 that is, whether for all ABoxes A, abstract reasoner ans2 computes more answers to
Q and T than abstract reasoner ans1 . This idea is formalised by the following definition.
460

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Definition 4.1. Let Q be a UCQ, let T be a TBox, and let ans1 and ans2 be abstract
reasoners applicable to T . Then, ans1 Q,T ans2 if the following conditions hold for each
ABox A:
1. cert(, T , A) = t and ans1 (, T , A) = t imply ans2 (, T , A) = t; and
2. cert(, T , A) = f, ans1 (, T , A) = f, and ans2 (, T , A) = f imply
ans1 (Q, T , A)  cert(Q, T , A)  ans2 (Q, T , A)  cert(Q, T , A).
Furthermore, ans1 <Q,T ans2 if ans1 Q,T ans2 and an ABox A exists such that at least one
of the following two conditions holds:
3. cert(, T , A) = t, ans1 (, T , A) = f, and ans2 (, T , A) = t; or
4. cert(, T , A) = f, ans1 (, T , A) = f, ans2 (, T , A) = f, and
ans1 (Q, T , A)  cert(Q, T , A)  ans2 (Q, T , A)  cert(Q, T , A).
Example 4.2. Consider the abstract reasoners rdf, rdfs, rl, and classify introduced in Example 3.3 and the query Q and TBox T from Example 3.14. We clearly have the following:
rdf Q,T rdfs Q,T rl Q,T classify
Furthermore, for any two of these abstract reasoners, an ABox exists that distinguishes the
abstracts reasoners w.r.t. Q and T ; for example, for ABox A = {takesCo(c, d), MathsCo(d)},
we have rdfs(Q, T , A ) =  and rl(Q, T , A ) = {c}. As a result, we also have the following:
rdf <Q,T rdfs <Q,T rl <Q,T classify



We would like to check whether ans1 Q,T ans2 and ans1 <Q,T ans2 for any given pair
of abstract reasoners by subjecting the reasoners to a finite set of tests. Towards this goal,
R
we next define the relations R
Q,T and <Q,T that compare abstract reasoners w.r.t. a given
finite set R of ABoxes. Ideally, given Q and T , we would like to compute a finite R such
R
that R
Q,T and <Q,T coincide with Q,T and <Q,T on all abstract reasoners from a class C
of interest. These ideas are captured by the following definitions.
Definition 4.3. Let Q be a UCQ, let T be a TBox, let R be a finite set of ABoxes, and let
ans1 and ans2 be abstract reasoners applicable to T .
Then, ans1 R
Q,T ans2 if Conditions 1 and 2 from Definition 4.1 hold for each ABox
R
A  R. Furthermore, ans1 <R
Q,T ans2 if ans1 Q,T ans2 and either Condition 3 or Condition
4 from Definition 4.1 holds for some ABox A  R.
Definition 4.4. Let Q be a UCQ, let T be a TBox, and let C be a class of abstract reasoners
applicable to T . A finite set R of ABoxes is (Q, T )-representative for C if the following
conditions hold for all ans1 , ans2  C:
1. ans1 R
Q,T ans2 if and only if ans1 Q,T ans2 ; and
461

fiCuenca Grau, Motik, Stoilos & Horrocks

2. ans1 <R
Q,T ans2 if and only if ans1 <Q,T ans2 .
As we show next, to prove that R is (Q, T )-representative, it suces to show the only
if implication in Condition 1 and the if implication in Condition 2 from Definition 4.4.
Proposition 4.5. Let Q be a UCQ, let T be a TBox, let C be a class of abstract reasoners
applicable to T , and let R be a finite set of ABoxes such that
1. ans1 R
Q,T ans2 implies ans1 Q,T ans2 , and
2. ans1 <Q,T ans2 implies ans1 <R
Q,T ans2 .
Then, R is (Q, T )-representative for C.
Proof. Note that ans1 Q,T ans2 trivially implies ans1 R
Q,T ans2 ; thus, Condition 1 of this
proposition clearly implies Condition 1 of Definition 4.4. Furthermore, if some ABox A  R
satisfies Condition 3 or 4 of Definition 4.1, Condition 1 or 2 of Definition 4.1 holds as well;
consequently, Conditions 1 and 2 of this proposition imply Condition 2 of Definition 4.4.
An obvious question is whether a Q-simple T -test suite that is exhaustive for a class
C and Q is also (Q, T )-representative for C. The following example shows that this is not
necessarily the case.
Example 4.6. Let Q and T be as specified in Example 3.14, and let R = {A1 , . . . , A6 } for
the ABoxes as specified in Example 3.16. As shown in Section 3, the Q-simple T -test suite
S = S , SQ  with S = {A6 } and SQ = {A1 , . . . , A5 } is exhaustive for CwQ,T and Q.
Let trivial be the abstract reasoner that returns the empty set on each input, and consider also the RDF-based abstract reasoner rdf from Example 3.3, which ignores the TBox
and evaluates the query directly against the ABox. Clearly, trivial Q,T rdf; furthermore,
trivial <Q,T rdf since for A = {St(c), takesCo(c, d), MathCo(d)} we have rdf(Q, T , A) = {c}
whereas trivial(Q, T , A) = . Both abstract reasoners, however, return the empty set of
answers for all ABoxes in R and thus rdf R
Q,T trivial. Hence, by using R we cannot
dierentiate the two abstract reasoners.

4.1 Negative Result
The following strong result shows that, for numerous TBoxes T , no finite set of ABoxes
exists that can dierentiate two arbitrary abstract reasoners from the class of all sound,
first-order reproducible, monotonic, and strongly faithful reasoners. Note that this result is
stronger than the negative result in Theorem 3.21, as it applies to a smaller class of abstract
reasoners and all TBoxes that imply at least one concept subsumption.
Theorem 4.7. Let T be an arbitrary TBox mentioning an atomic role R and atomic
concepts A and B such that T |= A  B, and let Q = {B(x)  Q(x)}. Then, no finite set
of ABoxes exists that is (Q, T )-representative for the class of all sound, monotonic, strongly
faithful, and first-order reproducible abstract reasoners applicable to T .
462

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Proof. Assume that a finite set of ABoxes R exists that is (Q, T )-representative for the class
of all sound, monotonic, strongly faithful, and first-order reproducible abstract reasoners
applicable to T . Let n be the maximum number of assertions in an ABox in R.
For an arbitrary integer k  1, let ansk be the first-order reproducible abstract reasoner
that, given an FOL-TBox Tin , uses the following datalog program FTkin :
FTkin

=




if Tin | = A  B
A(x0 )  R(x0 , x1 )  . . .  R(xk1 , xk )  B(x0 ) if Tin |= A  B

Clearly, each ansk is sound, monotonic, and strongly faithful; furthermore, ansk (, T , A) = f
for each ABox A. We next show that ansn+1 (Q, T , A)  ansn+2 (Q, T , A) for each ABox
A  R. Consider an arbitrary a0  ansn+1 (Q, T , A); then, individuals a0 , a1 , . . . , an+1 exist
such that R(a1 , a )  A for each 1    n + 1. Since A contains at most n assertions
but the rule in FTn+1 contains n + 1 body atoms, we have ai = aj for some i = jthat is,
A contains an R-cycle. But then, the rule in FTn+2 can be matched to A by mapping x0 to
a0 , so a0  ansn+2 (Q, T , A). Therefore, we have ansn+1 R
Q,T ansn+2 .
For A = {A(a0 ), R(a0 , a1 ), . . . , R(an , an+1 )}, however, we have a0  ansn+1 (Q, T , A) and
ansn+2 (Q, T , A) = ; thus, ansn+1 Q,T ansn+2 does not hold, which contradicts our assumption that R is exhaustive for the class of abstract reasoners from this theorem.
4.2 Compact Abstract Reasoners
Theorem 4.7 suggests that we need to make additional assumptions on the abstract reasoners that we wish to compare using a finite set of ABoxes. In this section, we show that
representative sets of ABoxes can be computed in practice if we further restrict ourselves
to abstract reasoners that we call (Q, T )-compact. Intuitively, such an abstract reasoner
processes Q, T , and A by computing all certain answers of Q, A, and some subset T  of
T , where the subset depends only on T and Q. In other words, the behaviour of compact
abstract reasoners can be simulated by the following process: select the subset of axioms
in the input TBox that can be processed, and then compute all certain answers w.r.t. the
selected fragment of the TBox. The class of (Q, T )-compact abstract reasoners thus captures the properties of concrete reasoners such as Jena or Oracles Semantic Data Store that
discard axioms from the input TBox that fall outside a certain fragment (e.g., existential
restrictions on the right-hand of implications) and then encode the remaining axioms into
a suitable set of rules.
Definition 4.8. Let Q be a UCQ, and let T be a TBox. An abstract reasoner ans applicable
to T is (Q, T )-compact if a TBox T   T exists such that the following properties hold for
each ABox A:
1. cert(, T  , A) = t implies ans(, T , A) = t;
2. cert(, T  , A) = f implies ans(, T , A) = f and ans(Q, T , A) = cert(Q, T  , A).
Abstract reasoner ans is compact if it is (Q, T )-compact for each UCQ Q and each TBox
T to which ans is applicable. Finally, CcQ,T is the class of all (Q, T )-compact and strongly
(Q, T )-faithful abstract reasoners applicable to T .
463

fiCuenca Grau, Motik, Stoilos & Horrocks

Example 4.9. All abstract reasoners defined in Example 3.3 are (Q, T )-compact for the
query Q and EL-TBox T from Example 3.14. Indeed, for abstract reasoner rdf the subset
T  of T is given by T  = ; for abstract reasoner rdfs it is T  = {(8)}; for abstract reasoner
rl it is T  = {(8), (9), (10)}; and for abstract reasoner classify it is T  = T .

The abstract reasoners ansk defined in the proof of of Theorem 4.7 are not (Q, T )compact for the query and the TBoxes to which Theorem 4.7 applies.

Proposition 4.10. Let Q = {B(x)  Q(x)} and let T = {A  B, C  R.}. Then, for
each k  1, abstract reasoner ansk from the proof of Theorem 4.7 is not (Q, T )-compact.
Proof. Let Q and T be as stated in the theorem and consider an arbitrary k  1. Let A1
and A2 be ABoxes defined as follows:
A1 = {A(a0 )}

A2 = {A(a0 ), R(a0 , a1 ), . . . , R(ak1 , ak )}

Clearly, we have the following:
ansk (Q, T , A1 ) = 

ansk (Q, T , A2 ) = {a0 }

One can straightforwardly check, however, that the following holds for each T  with T   T :
cert(Q, T  , A1 ) = cert(Q, T  , A2 )
Thus, ansk is not (Q, T )-compact.
Thus, the negative result from Theorem 4.7 does not immediately apply to a class
containing only compact abstract reasoners.
4.3 Comparing Compact Abstract Reasoners
In this section, we show that a set of ABoxes that is (Q, T )-representative for CcQ,T can be
obtained by computing, for each subset T  of T , a Q-simple T  -test suite that is exhaustive

for CsQ,T . A minor complication arises due to the fact that T  can contain fewer individuals

than T . To deal with such cases correctly, the ABoxes in ST are not allowed to contain

individuals occurring in T but not in T  , and the ABoxes in STQ are not allowed to contain
individuals occurring in T but not in Q  T  . This assumption is without loss of generality:

given a (Q, T  )-test suite ST , one can replace all individuals in T but not in Q  T  with

fresh individuals; the result of such replacement is a (Q, T  )-test suite exhaustive for CsQ,T .
Theorem 4.11. Let Q be a UCQ, and let T be a TBox. Furthermore, for each T   T , let




ST = ST , STQ  be a Q-simple T  -test suite that is exhaustive for CsQ,T and Q such that


no ABox in ST contains an individual from ind(T ) \ ind(T  ) and no ABox in STQ contains
an individual from ind(T ) \ ind(Q  T  ). Then, the set R of ABoxes defined by



R=
ST  STQ
T  T

is (Q, T )-representative for CcQ,T .
464

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Proof. Assume that R satisfies the conditions of the theorem, and let ans1 and ans2 be
arbitrary abstract reasoners in CcQ,T . We next show that ans1 and ans2 satisfy the two
properties in Proposition 4.5.
 Property 1 of Proposition 4.5:

ans1 R
Q,T ans2 implies ans1 Q,T ans2

 Property 2 of Proposition 4.5:

ans1 <Q,T ans2 implies ans1 <R
Q,T ans2

Since ans1 is (Q, T )-compact, a TBox T   T exists that satisfies the conditions of
Definition 4.8. Assume that ans1 R
Q,T ans2 ; we next show that Conditions 1 and 2 of
Definition 4.1 are satisfied for an arbitrary ABox A.
(Condition 1) Assume that cert(, T , A) = t and ans1 (, T , A) = t. By the contrapositive of property 2 of Definition 4.8, then cert(, T  , A) = t. Since R contains all the ABoxes

of some Q-simple T  -test suite that is exhaustive for CsQ,T and Q, by Theorem 3.30 there
exist an ABox A  R and a T  -stable renaming  such that dom() = ind(T   A ) and
(A )  A; since A does not contain individuals from ind(T ) \ ind(T  ), renaming  is also
T -stable. By the definition of a T  -test suite, cert(, T  , A ) = t; furthermore, by property 1

of Definition 4.8 we have ans1 (, T , A ) = t. Since ans1 R
Q,T ans2 we have ans2 (, T , A ) = t.

Since ans2 is strongly (Q, T )-faithful and  is T -stable, we have ans2 (, T , (A )) = t. Finally, since (A )  A and ans2 is (Q, T )-monotonic, we have ans2 (, T , A) = t, as required.
(Condition 2) Assume that cert(, T , A) = f, ans1 (, T , A) = f, and ans2 (, T , A) = f,
and consider an arbitrary tuple a  ans(Q, T , A)  cert(Q, T , A). By the contrapositive
of property 1 of Definition 4.8, then cert(, T  , A) = f; but then, by property 2 of Definition 4.8, we have a  cert(Q, T  , A). Since R contains all the ABoxes of some Q
simple T  -test suite that is exhaustive for CsQ,T and Q, by Theorem 3.30 there exist
an ABox A  R, a tuple b  cert(Q, T  , A ), and a (Q, T  )-stable renaming  such that
dom() = ind(Q  T   A ), (A )  A, and (b) = a; since A does not contain individuals from ind(T ) \ ind(Q  T  ), renaming  is also (Q, T )-stable. By the definition of a
(Q, T  )-test suite, cert(, T  , A ) = f; furthermore, by property 2 of Definition 4.8 we have
b  ans1 (Q, T , A ). Since ans1 R ans2 we have b  ans2 (Q, T , A ). Since ans2 is strongly
Q,T
(Q, T )-faithful and  is (Q, T )-stable, we have that a  ans2 (Q, T , (A )). Finally, since
(A )  A and ans2 is (Q, T )-monotonic, we have a  ans2 (Q, T , A), as required.
Assume that ans1 <Q,T ans2 . By Definition 4.1, then ans1 Q,T ans2 and an ABox
A exists satisfying Conditions 3 and 4 of Definition 4.1. Clearly, ans1 R
Q,T ans2 ; hence,
what remains to be shown is that R contains an ABox that satisfies Conditions 3 and 4
of Definition 4.1. Since ans1 is (Q, T )-compact, a TBox T   T exists that satisfies the
conditions of Definition 4.8.
(Condition 3) Assume that cert(, T , A) = t, and assume also that ans1 (, T , A) = t
and ans2 (, T , A) = f. As in the proof of Condition 1, we can identify an ABox A  R and
a T -stable renaming  such that ans1 (, T , A ) = t and (A )  A. Since ans2 is (Q, T )monotonic and ans2 (, T , A) = f, we have ans2 (, T , (A )) = f; furthermore, since ans2
is strongly (Q, T )-faithful and  is T -stable, we also have ans2 (, T , A ) = f. But then,
Condition 3 of Definition 4.1 is satisfied for A  R.
(Condition 4) Assume that cert(, T , A) = f and ans1 (, T , A) = ans2 (, T , A) = f, and
consider an arbitrary tuple a  [ans1 (Q, T , A)  cert(Q, T , A)] \ ans2 (Q, T , A). As in the
proof of Condition 2, we can identify an ABox A  R, a (Q, T )-stable renaming , and a
465

fiCuenca Grau, Motik, Stoilos & Horrocks

tuple b  cert(Q, T  , A ) such that (A )  A, (b) = a, and b  ans1 (Q, T , A ). Since ans2 is
(Q, T )-monotonic and a  ans2 (Q, T , A), we have a  ans2 (Q, T , (A )); furthermore, since
ans2 is strongly (Q, T )-faithful and  is (Q, T )-stable, we also have b  ans2 (Q, T , A ). But
then, Condition 4 of Definition 4.1 is satisfied for A  R.
Theorems 3.32 and 4.11 immediately suggest an approach for computing a set of ABoxes
that is a (Q, T )-representative for CcQ,T . First, we compute a UCQ rewriting of Q w.r.t.
each subset of T ; then, we instantiate each rule in each such rewriting using an injective
instantiation mapping; finally, we compute R as a union of all ABoxes in all test suites.
Such a nave procedure, however, is not practical since it requires computing an exponential
number of UCQ rewritings. We next present a more practical approach to computing a
set of ABoxes that is (Q, T )-representative for CcQ,T . Intuitively, instead of computing
exponentially many rewritings, one can compute a single UCQ rewriting of Q w.r.t. T that
is subset-closed that is, which contains a rewriting for each subset of T .
Definition 4.12. A UCQ rewriting R = R , RQ  of Q w.r.t. T is subset-closed if for
each T   T a tuple R = R , RQ  exists such that R  R , RQ  RQ and R is a
UCQ rewriting of Q w.r.t. T  .
The following corollary is an immediate consequence of Theorems 3.27, 3.32, and 4.11.

Corollary 4.13. Let Q be a UCQ, let T be a TBox, let R be a subset-closed UCQ rewriting
R
of Q w.r.t. T , and let IR = IR
 , IQ  be the injective instantiation of R. Then, the set of
Q,T
R
ABoxes R = IR
.
  IQ is (Q, T )-representative for Cc

Practical query rewriting systems such as REQUIEM are optimised to produce as small
a UCQ rewriting as possible, so their output is typically not subset-closed. Therefore,
our technique requires the modification of UCQ rewriting algorithms implemented in existing systems. As illustrated by the following example, the required modification typically
involves disabling (at least partially) subsumption-based optimisations.
Example 4.14. Let Q and T be as specified in Example 3.14, and let S = S , SQ  be
the T -test suite from Example 3.16. A system such as REQUIEM can compute such R
for the given Q and T . Note, however, that R is not subset-closed; for example, a UCQ
rewriting of Q w.r.t. T  =  is Q, and it is not a subset of RQ . The rewriting can be made
subset-closed by extending RQ with the following rules:
St(x)  takesCo(x, y)  MathCo(x, y)  Q(x)
St(x)  takesCo(x, y)  CalcCo(x, y)  Q(x)
MathSt(x)  St(x)  Q(x)

Systems such as REQUIEM, however, typically discard such rules by applying subsumption
optimisations described in Section 3.5.3.

As the following example shows, a subset-closed UCQ rewriting of Q w.r.t. T can, in
the worst case, be exponentially larger than the minimal UCQ rewritings of Q w.r.t. T .
Example 4.15. Let Q = {C(x)  Q(x)}, and let T be the following TBox:
T = {B  Ai | 1  i  n}  {A1  . . .  An  C}
466

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Furthermore, let R = R , RQ  be such that R =  and RQ contains the following rules:
C(x)  Q(x)

B(x)  Q(x)

A1 (x)  . . .  An (x)  Q(x)
Clearly, R is a UCQ rewriting of Q w.r.t. T ; however, the number of rules in a subset-closed
UCQ rewriting of Q w.r.t. T is exponential in n.


5. Evaluation
We implemented our techniques for computing exhaustive test suites and for comparing incomplete concrete reasoners in a prototype tool called SyGENiA.1 Our tool uses REQUIEM
for computing UCQ and datalog rewritings.2
We considered two evaluation scenarios. The first one uses the well-known Lehigh
University Benchmark (LUBM) (Guo et al., 2005), which consists of a relatively small
TBox about an academic domain, 14 test queries, and a data generator. The second one
uses a small version of GALEN (Rector & Rogers, 2006)a complex ontology commonly
used in medical applications.
We evaluated the following concrete reasoners: Sesame v2.3-prl,3 DLE-Jena v2.0,4
OWLim v2.9.1,5 Minerva v1.5,6 and Jena v2.6.37 in all of its three variants (Micro, Mini,
and Max).
5.1 Computing Exhaustive Test Suites
Given a UCQ Q and a TBox T , our tool uses REQUIEM to compute a datalog rewriting
R for Q and T . If R is a UCQ rewriting, then our tool computes a simple test suite by
either full or injective instantiation (see Sections 3.5 and 3.6, respectively); otherwise, the
tool computes a non-simple test suite by instantiating R as described in Section 3.7.3.
5.1.1 Simple Test Suites
In the case of the LUBM benchmark, each of the 14 test queries leads to a UCQ rewriting w.r.t. the TBox.8 Therefore, we computed a UCQ rewriting for each query Q in the
benchmark using REQUIEM and instantiated it, both fully and injectively, thus obtaining
Q-simple T -test suites that are exhaustive for Q and CwQ,T and CsQ,T , respectively. The
times needed to compute the test suites and the size of each test suite are shown in Table
3, where S denotes the total number of ABoxes in the corresponding test suites.
1.
2.
3.
4.
5.
6.
7.
8.

http://code.google.com/p/sygenia/
http://www.cs.ox.ac.uk/projects/requiem/home.html
http://www.openrdf.org/
http://lpis.csd.auth.gr/systems/DLE-Jena/
http://www.ontotext.com/owlim/
http://www.alphaworks.ibm.com/tech/semanticstk
http://jena.sourceforge.net/
Since REQUIEM does not currently support individuals in the queries, we replaced the individuals in
queries by distinguished variables.

467

fiCuenca Grau, Motik, Stoilos & Horrocks

Q,T
Cw

CsQ,T

Q1
Time 1.2
S
2
Time 1.2
S
1

Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14
0.7 0.2 6.7 0.2 2.1 0.7
7
2.4 7.4 0.07 0.2 0.2 0.05
20 2 2 352 8 1207 345 3 092 5 3 919 7
25 10
1
0.6 0.2 0.6 0.2 1.2 0.4 1.5 2.5 0.6 0.05 0.1 0.1 0.08
4
1
22
4 169 37
36
1 169
2
3
5
1

Table 3: Computation of simple test suites for LUBM. Times are given in seconds.

Q,T
Cw

CsQ,T

Time
S
Time
S

Q1
14
6 049
2.2
79

Q2
34
12 085
6
151

Q3
67
12 085
40
151

Q4
4.6
79
1.7
25

Table 4: Computation of simple test suites for GALEN. Times are given in seconds.
As shown in the table, simple test suites could be computed in times ranging from 0.05
to 7 seconds, both for CwQ,T and CsQ,T . The optimisations implemented in REQUIEM ensure
that the UCQ rewritings are relatively small, so the resulting test suites also consist of a
relatively small number of ABoxes. Notice, however, the significant dierence between the
numbers of ABoxes in test suites obtained via injective instantiation (which range from 1
to 169 with an average of 32), and those obtained via full instantiation (which range from
1 to 3, 919 with an average of 702). Furthermore, each rule in a rewriting contains at most
6 atoms, therefore each ABox in a test suite also contains at most 6 assertions.
In the case of GALEN, we used the following sample queries, for which REQUIEM can
compute a UCQ rewriting:
Q1
Q2
Q3
Q4

:
:
:
:

HaemoglobinConcentrationProcedure(x)  Q(x)
PlateletCountProcedure(x)  Q(x)
LymphocyteCountProcedure(x)  Q(x)
HollowStructure(x)  Q(x)

We instantiated each UCQ rewriting both fully and injectively. The times needed to compute the test suites and the size of each test suite are shown in Table 4.
As shown in the table, simple test suites for GALEN can be computed in times ranging
from 1.7 to 67 seconds with an average of 33 seconds. Thus, computing test suites for
GALEN is more time consuming than for LUBM. This is unsurprising since the TBox of
GALEN is significantly more complex than that of LUBM. The number of ABoxes in the
test suites ranged from 25 to 151 in the case of injective instantiations and from 79 to over
12, 000 in the case of full instantiations; again, note the significant dierence between the
sizes of the two kinds of test suites. In all cases, however, each individual ABox was very
small, with the largest one containing only 11 assertions.
5.1.2 Non-Simple Test Suites
We also computed non-simple test suites for cases where no UCQ rewriting exists. As
already mentioned, all LUBM queries are UCQ-rewritable. Therefore, we manually added
the following query, for which REQUIEM computes a recursive datalog rewriting.
468

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Time (s)
S

CfT

LUBM
Q15
1.4
22

Q5
5.2
41

GALEN
Q6 Q7
1.3 2.7
23
31

Q8
1.6
12

Table 5: General test suites computed from datalog rewritings for LUBM and GALEN.
System
Completeness Guarantee Completeness w.r.t. LUBM data set
JenaMax/DLE-Jena
Q1 Q14
Q1 Q14
OWLim
Q1 Q5 , Q7 , Q9 , Q11 Q14
Q1 Q14
Jena Mini/Micro
Q1 Q5 , Q7 , Q9 , Q11 Q14
Q1 Q14
Minerva
Q1 Q4 , Q9 , Q14
Q1 Q14
Sesame
Q1 , Q3 , Q11 , Q14
Q1 Q5 , Q11 , Q14

Table 6: Completeness guarantees for UCQ-rewritable queries in LUBM
Q15 :

Organization(x)  Q(x)

Due to the complex structure of the GALEN TBox, test queries that are not UCQ rewritable
can be easily identified. We have evaluated the following four.
Q5
Q6
Q7
Q8

:
:
:
:

WestergrenESRProcedure(x)  Q(x)
ArthroscopicProcedure(x)  Q(x)
TrueCavity(x)  Q(x)
BacterialCellWall(x)  Q(x)

Times needed to compute test suites and the size of each test suite are shown in Table 5.
5.2 Completeness Guarantees
As already discussed, existing concrete reasoners are captured by strongly (Q, T )-faithful
abstract reasoners. Hence, in order to establish completeness guarantees for such concrete
reasoners, we restricted our tests to test suites computed using injective instantiations.
5.2.1 Results for Simple Test Suites
Our results for the original queries of the LUBM benchmark are shown in Table 6. For
each concrete reasoner, the first column of the table shows the queries for which we were
able to prove completeness using our techniques (i.e., the queries that are complete for
an arbitrary data set), and the second column of the table shows the queries on which the
concrete reasoner computes all answers on the canonical LUBM data set with one university.
Our results clearly show that completeness w.r.t. the data set in the LUBM benchmark is
no guarantee of completeness for arbitrary data sets; for example, OWLim, Minerva, and
Jena Mini/Micro are complete for all queries w.r.t. the LUBM data set (and some of these
systems are even complete for the more expressive UOBM benchmark); however, for certain
queries, these systems were found to be incomplete for a data set in our test suites.
Jena Max and DLE-Jena are the only systems that are guaranteed to be complete for
all 14 LUBM queries regardless of the data setthat is, these systems behave exactly like a
complete OWL reasoner for all LUBM queries and the LUBM TBox. According to Jenas
469

fiCuenca Grau, Motik, Stoilos & Horrocks

documentation, Jena Max supports all types of axioms used in the LUBM TBox, hence it
is expected to be complete for the LUBM TBox and queries. Interestingly, when tested
with some of the LUBM data sets, Jena Max could not compute the answers to many of
the queries, so we used smaller LUBM data sets instead. This demonstrates an additional
advantage of our approach: it does not require reasoning w.r.t. very large data sets, since the
ABoxes in test suites typically contain only a small number of assertions. Regarding DLEJena, according to its technical description (Meditskos & Bassiliades, 2008), the system
uses a complete DL reasoner to materialise certain subsumptions in a preprocessing step
and then uses Jena to saturate the ABox, much like the abstract reasoner classify from
Example 3.3. Hence, DLE-Jena is at least as complete as Jena Mini and, in addition, it is
able to draw the inferences that Jena Mini is missing (see below).
OWLim is complete for all LUBM queries that do not involve reasoning with existential
quantifiers in the consequent of implications. It is well known that the latter is not supported
by the system. Jena Mini and Micro exhibited exactly the same behaviour as OWLim,
despite the fact that Jena Mini can handle a larger fragment of OWL than OWLim. Clearly,
the LUBM TBox and queries are not suciently complex to reveal the dierences between
OWLim, and Jena Mini/Micro.
Minerva is guaranteed to be complete for only six queries. Like DLE-Jena, it uses a
DL reasoner to materialise entailed subsumptions between atomic concepts, but it uses a
custom method for saturating the ABox. After investigating several ABoxes from the test
suites we concluded that Minerva cannot correctly handle (at-least) inverse role axioms; for
example, it cannot find the entailment { R  R , R(a, b) } |= R(b, a).
Finally, Sesame is complete for only four queries. This is unsurprising since Sesame is
an RDFS reasoner and is thus complete only for a small fragment of OWL 2 DL.
We next discuss the results of tests based on the GALEN ontology and test queries
Q1 Q4 . We could not run Jena Max since GALEN heavily uses existential restrictions,
which (according to Jenas documentation) might cause problems. Minerva was the only
system that provided completeness guarantee for at least one query (Q4 ); this is because
Minerva precomputes subsumption relationships between atomic concepts that depend on
existential restrictions on the right hand side of TBox axioms, which most other systems
do not handle. Also, unlike LUBM, the version of GALEN that we used does not contain
inverse roles, so Minerva performed much better on this ontology. All other systems were
identified as being incomplete for all test queries.
5.2.2 Results for Non-Simple Test Suites
Results for test queries that are not UCQ-rewritable are summarised in Table 7. Symbol
 indicates that the concrete reasoner was found complete for the given query. Furthermore, whenever a concrete reasoner failed a test suite, we tried to prove the reasoner to be
incomplete as discussed in the examples in Section 3.7.2; in all cases we were successful,
so symbol  indicates that the concrete reasoner was identified as being incomplete for a
given query. Finally, symbol  indicates that the concrete reasoner ran out of memory.
In the case of LUBM, we were able to establish completeness guarantees w.r.t. query
Q15 for OWLim, Jena Micro, DLE-Jena, and Jena Max. Note that all these systems
can handle recursive TBox statements, so completeness for Q15 is not surprising. RDFS,
470

fiCompleteness Guarantees for Incomplete Ontology Reasoners

OWLim
Jena Max
Jena Micro
DLE-Jena
Minerva
Sesame

LUBM
Q15







Q5






GALEN
Q6 Q7











Q8






Table 7: Completeness guarantees for datalog-rewritable queries

CsQ,T

Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14
Time 1.4 1.1 0.2 1.8 0.8 1.2 9.5 7.8 - 0.9 0.05 0.5 0.6 0.04
R
1 24 17 130 136 219 925 777 - 219 2
74 185 1

Table 8: Representative sets of ABoxes for LUBM. Times are given in seconds.
however, cannot express recursive TBox statements involving roles, so Sesamean RDFS
reasonerfails to compute certain answers to some tests.
In the case of GALEN, completeness is guaranteed on query Q8 for OWLim, Jena Micro,
DLE-Jena, and Minerva, and additionally on query Q6 for Minerva. As already mentioned,
answers to queries on GALEN depend on positive occurrences of existential restrictions in
axioms, which most systems cannot handle. We could not run Jena Max on GALEN.
5.3 Comparing Incomplete Concrete Reasoners
We also implemented the techniques for comparing reasoners from Section 4.3. To this end,
we modified REQUIEM to compute subset-closed rewritings, which are then injectively
instantiated to obtain a (Q, T )-representative sets of ABoxes R.
5.3.1 Tests with LUBM
As shown in Table 8, representative sets of ABoxes could be computed in just a few seconds
for most LUBM queries. The only exception was Q9 , for which REQUIEM did not terminate
after disabling rule subsumption optimisations. The size of the representative sets ranged
between 1 and 777 ABoxes. As expected, representative sets contain more ABoxes than the
exhaustive test suites for the same query and TBox (see Table 3).
All combinations of system and query for which the tests in Section 5.2 identified the
system as incomplete are shown in Table 9. The table shows the proportion of certain
answers that each system returned when applied to the LUBM data set, the ABoxes in R,
and the ABoxes in the test suite S used in Section 5.2 to check the systems completeness.
As shown in the table, OWLim and Jena Micro exhibited the same behaviour and were
almost complete. In contrast, Sesame was the least complete for all queries. Furthermore,
please note the dierence between the values obtained for R and those for S; in particular,
Sesame did not compute any certain answer for Q5 on S, whereas the system is able to
compute certain answers for Q5 on some ABoxes (e.g., on the LUBM data set). This is
because the ABoxes in S cannot distinguish Sesame from a trivial reasoner that always
returns the empty set of answers; however, the set R can make such a distinction.
471

fiCuenca Grau, Motik, Stoilos & Horrocks

LUBM
S
R

Q5
1
0.25
0.8

Q6
1
0.86
0.86

LUBM
S
R

Q2
1
0.75
0.75

Q4
1
0.68
0.06

Minerva
Q8
Q10
Q12
1
1
1
0.98 0.86 0.25
0.81 0.84 0.92
Sesame
Q5
Q6
Q7
Q8
1
0.83 0.87 0.83
0
0.003 0.04 0.04
0.36 0.033 0.01 0.004
Q7
1
0.86
0.86

Q13
1
0.2
0.23

OWLim & JMicro
Q6
Q8
Q10
1
1
1
0.99
0.98
0.99
0.96
0.98
0.97

Q9
0.64
0
-

Q10
0.83
0.001
0.028

Q12
0
0.25
0.017

Q13
0
0.2
0.23

Table 9: Reasoner comparison for LUBM

CsQ,T

Time
R

Q1
15
140

Q2
46
266

Q3
70
266

Q4
2
127

Table 10: Representative sets of ABoxes for GALEN
5.3.2 Tests with GALEN
As shown in Table 10, representative sets of ABoxes for GALEN could be computed in
times ranging from 2 to 70 seconds, and each set contains only a small number of ABoxes.
For each system and query, Table 11 shows the proportion of certain answers returned
by the system on R and the test suite S from Section 5.2. Minerva was the most complete
system. Jena Micro was better than DLE-Jena (apart from query Q4 ), while DLE-Jena
and OWLim behaved in almost the same way (again apart from query Q4 ). As expected,
Sesame was the least complete system.
The discrepancies between OWLim, Jena Micro, DLE-Jena and Minerva are rather
surprising. OWLim and Jena theoretically support the same features of OWL; furthermore,
DLE-Jena is an extension of Jena (Meditskos & Bassiliades, 2008) so DLE-Jena should be
at least as complete as Jena, as in the case of LUBM. In order to explain these discrepancies,
we analysed the test suites for queries Q1 Q4 . More precisely, we selected ABoxes on which
OWLim fails to return all certain answers but on which Jena Micro is complete, and then
we identified the minimal set of TBox axioms that entail all certain answers. Our analysis
revealed that, for query Q4 , OWLim fails to find the entailment
T  {Device(a), HollowTopology(b), hasTopology(a, b)} |= HollowStructure(a),

which follows from the following GALEN axioms:

HollowTopology  Topology  hasState.Hollow
Device  SolidStructure

HollowStructure  SolidStructure  hasTopology.(Topology  hasState.Hollow)

Although existential restrictions appear in several axioms, we can observe that no reasoning
over existential variables is actually required, as the first and third axioms imply (by a simple
structural transformation) the following axiom:
SolidStructure  hasTopology.HollowTopology  HollowStructure
472

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Sesame
OWLim
DLE-Jena
JMicro
Minerva

Q1
S
R
0.01 0.18
0.54 0.65
0.54 0.65
0.69 0.82
0.84 0.91

Q2
S
R
0 0.16
0.52 0.63
0.52 0.63
0.68 0.81
0.84 0.90

Q3
S
R
0 0.16
0.52 0.63
0.52 0.63
0.68 0.81
0.84 0.90

Q4
S
R
0.04 0.10
0.52 0.48
0.76 0.9
0.76 0.67
1
1

Table 11: Reasoner comparison on GALEN
This axiom entails the required answer, and both systems can deal with axioms of this form;
however, unlike Jena Micro, OWLim appears to be incapable of dealing with such cases.
Regarding DLE-Jena, according to its technical description (Meditskos & Bassiliades,
2008), the system has replaced several inference rules of Jena with queries to the DL reasoner, so it does not strictly extend Jena. Our investigation of the exhaustive test suite
for query Q4 revealed that DLE-Jena returns many answers that are based on existential
restrictions on the right hand side of TBox axioms which Jena misses; however, the investigation also revealed that DLE-Jena misses several inferences that Jenas TBox reasoner
can capture, which is probably due to the replacement of Jenas inference rules. This also
explains why DLE-Jena performs worse than Minerva on GALEN.
These results clearly show that the behaviour of systems greatly depends on the given
application scenario. For example, DLE-Jena was complete for all LUBM queries, but
it did not perform equally well on GALEN. In contrast, Minerva did not perform well on
LUBM, but it was the most complete system for GALEN. Our results thus allow application
developers to conduct a thorough comparison of reasoning systems for a given application.

6. Conclusion
In this paper we have proposed a theoretical framework and practical techniques for establishing formally provable and algorithmically verifiable completeness guarantees for incomplete ontology reasoners. Our approach radically departs from ad hoc evaluation based on
well-known benchmarks, and it provides a solid foundation for striking the balance between
scalability and completeness in practical applications.
Our approach also opens up numerous and exciting possibilities for future research. For
example, our work opens the door to the design of ontology-based information systems that
are optimised for a class of ontologies, queries, and data relevant to a particular application. Such information systems could maximise scalability of reasoning while still ensuring
completeness of query answers, even for rich ontologies and sophisticated queries.

Acknowledgments
This is an extended version of the paper How Incomplete is your Semantic Web Reasoner?
by Giorgos Stoilos, Bernardo Cuenca Grau, and Ian Horrocks published at AAAI 2010
and the paper Completeness Guarantees for Incomplete Reasoners by the same authors
published at ISWC 2010.
473

fiCuenca Grau, Motik, Stoilos & Horrocks

This research has been supported by the EU project SEALS (FP7-ICT-238975), and by
the EPSRC projects ExODA (EP/H051511/1) and HermiT (EP/F065841/1). B. Cuenca
Grau is supported by a Royal Society University Research Fellowship.

References
Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &
Rosati, R. (2005). Quonto: Querying ontologies. In Proceedings of the 20th National
Conference on Artificial Intelligence (AAAI-05)., pp. 16701671. AAAI Press / The
MIT Press.
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). The DL-Lite family
and relations. J. Artificial Intelligence Research (JAIR), 36, 169.
Baader, F., McGuinness, D., Nardi, D., & Patel-Schneider, P. (2002). The Description Logic
Handbook: Theory, implementation and applications. Cambridge University Press.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proceedings of
the 19th International Joint Conference on AI (IJCAI-05), pp. 364369. MorganKaufmann Publishers.
Bishop, B., Kiryakov, A., Ognyano, D., Peikov, I., Tashev, Z., & Velkov, R. (2011).
OWLIM: A family of scalable semantic repositories. Semantic Web, 2 (1), 3342.
Broekstra, J., Kampman, A., & van Harmelen, F. (2002). Sesame: A generic architecture for
storing and querying RDF and RDF Schema. In Proceedings of the 1st International
Semantic Web Conference (ISWC 2002), pp. 5468.
Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010). Datalog+/-: A
family of logical knowledge representation and query languages for new applications.
In Proc. of the 25th Annual IEEE Symposium on Logic in Computer Science (LICS),
pp. 228242.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning and ecient query answering in description logics: The DL-Lite family.
Journal of Automated Reasoning, 39 (3), 385429.
Ceri, S., Gottlob, G., & Tanca, L. (1989). What you always wanted to know about datalog
(and never dared to ask). IEEE Trans. Knowledge Data Engineering, 1 (1), 146166.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008a). Modular Reuse of
Ontologies: Theory and Practice. Journal of Artificial Intelligence Research, 31, 273
318.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U.
(2008b). OWL 2: The next step for OWL. Journal of Web Semantics (JWS), 6 (4),
309322.
Derriere, S., Richard, A., & Preite-Martinez, A. (2006). An Ontology of Astronomical
Object Types for the Virtual Observatory. In Proc. of the 26th meeting of the IAU:
Virtual Observatory in Action: New Science, New Technology, and Next Generation
Facilities, pp. 1718, Prague, Czech Republic.
474

fiCompleteness Guarantees for Incomplete Ontology Reasoners

Erling, O., & Mikhailov, I. (2009). RDF support in the virtuoso DBMS. In Pellegrini, T.,
Auer, S., Tochtermann, K., & Schaert, S. (Eds.), Networked Knowledge - Networked
Media, pp. 724. Springer Berlin / Heidelberg.
Fitting, M. (1996). First-Order Logic and Automated Theorem Proving, 2nd Edition. Texts
in Computer Science. Springer.
Glimm, B., Horrocks, I., Lutz, C., & Sattler, U. (2007). Conjunctive query answering for
the description logic SHIQ. In Proceedings of the International Joint Conference on
AI (IJCAI), pp. 399404.
Golbreich, C., Zhang, S., & Bodenreider, O. (2006). The Foundational Model of Anatomy
in OWL: Experience and Perspectives. Journal of Web Semantics, 4 (3), 181195.
Goodwin, J. (2005). Experiences of using OWL at the Ordnance Survey. In Proc. of the
OWL: Experiences and Directions Workshop (OWLED 2005), Galway, Ireland.
Guo, Y., Pan, Z., & Heflin, J. (2005). LUBM: A Benchmark for OWL Knowledge Base
Systems. Journal of Web Semantics, 3 (2), 158182.
Haarslev, V., & Moller, R. (2001). RACER System Description. In Gore, R., Leitsch, A., &
Nipkow, T. (Eds.), Proc. of the 1st Int. Joint Conf. on Automated Reasoning (IJCAR
2001), Vol. 2083 of LNAI, pp. 701706, Siena, Italy. Springer.
Hayes, P. (2004). RDF Semantics. World Wide Web Consortium (W3C) Recommendation.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). From SHIQ and RDF to
OWL: the making of a web ontology language. Journal Web Semantics, 1 (1), 726.
Kiryakov, A., Ognyanov, D., & Manov, D. (2005). Owlim-a pragmatic semantic repository
for owl.. In Dean, M., Guo, Y., Jun, W., Kaschek, R., Krishnaswamy, S., Pan, Z., &
Sheng, Q. Z. (Eds.), WISE Workshops, pp. 182192.
Lacy, L., Aviles, G., Fraser, K., Gerber, W., Mulvehill, A., & Gaskill, R. (2005). Experiences
Using OWL in Military Applications. In Proc. of the OWL: Experiences and Directions
Workshop (OWLED 2005), Galway, Ireland.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering in the description
logic EL using a relational database system. In Proceedings of the 21st International
Joint Conference on AI (IJCAI), pp. 20702075.
Ma, L., Yang, Y., Qiu, Z., Xie, G. T., Pan, Y., & Liu, S. (2006). Towards a complete OWL
ontology benchmark. In Proceedings of the 3rd European Semantic Web Conference
(ESWC 2006), pp. 125139.
McBride, Brian (2001). Jena: Implementing the RDF Model and Syntax Specification. In
International Workshop on the Semantic Web 2001.
Meditskos, G., & Bassiliades, N. (2008). Combining a DL reasoner and a rule engine for
improving entailment-based OWL reasoning. In Proceedings of the 7th International
Semantic Web Conference (ISWC 2008), pp. 277292.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2009a). OWL 2
Web Ontology Language Profiles. W3C Recommendation.
475

fiCuenca Grau, Motik, Stoilos & Horrocks

Motik, B., Shearer, R., & Horrocks, I. (2009b). Hypertableau Reasoning for Description
Logics. J. Artificial Intelligence Research (JAIR), 173 (14), 12751309.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity of query answering in expressive description logics via tableaux. Journal of Automated Reasoning, 41 (1), 6198.
Perez-Urbina, H., Horrocks, I., & Motik, B. (2009). Ecient Query Answering for OWL 2.
In Proceedings of the 8th International Semantic Web Conference (ISWC 2009), Vol.
5823 of LNCS, pp. 489504. Springer.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering and rewriting
under description logic constraints. Journal of Applied Logic, 8 (2), 186209.
Prudhommeaux, E., & Seaborne, A. (2008). SPARQL query language for RDF. World
Wide Web Consortium (W3C). W3C Recommendation.
Rector, A. L., & Rogers, J. (2006). Ontological and practical issues in using a description
logic to represent medical concept systems: Experience from galen. In Barahona,
P., Bry, F., Franconi, E., Henze, N., & Sattler, U. (Eds.), Reasoning Web, Second
International Summer School 2006, pp. 197231.
Sidhu, A., Dillon, T., Chang, E., & Sidhu, B. S. (2005). Protein Ontology Development
using OWL. In Proc. of the OWL: Experiences and Directions Workshop (OWLED
2005), Galway, Ireland.
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: A practical
OWL-DL reasoner. Journal of Web Semantics, 5 (2), 5153.
Soergel, D., Lauser, B., Liang, A., Fisseha, F., Keizer, J., & Katz, S. (2004). Reengineering
Thesauri for New Applications: The AGROVOC Example. J. of Digital Information,
4 (4).
Wu, Z., Eadon, G., Das, S., Chong, E. I., Kolovski, V., Annamalai, M., & Srinivasan, J.
(2008). Implementing an inference engine for rdfs/owl constructs and user-defined
rules in oracle. In Proceedings of the 2008 IEEE 24th International Conference on
Data Engineering (ICDE 08), pp. 12391248. IEEE Computer Society.

476

fiJournal of Artificial Intelligence Research 43 (2012) 293-328

Submitted 07/11; published 03/12

SAS+ Planning as Satisfiability
Ruoyun Huang
Yixin Chen
Weixiong Zhang

RUOYUN . HUANG @ WUSTL . EDU
CHEN @ CSE . WUSTL . EDU
WEIXIONG . ZHANG @ WUSTL . EDU

Department of Computer Science and Engineering
Washington University in St. Louis
Saint Louis, Missouri, 63130, USA

Abstract
Planning as satisfiability is a principal approach to planning with many eminent advantages.
The existing planning as satisfiability techniques usually use encodings compiled from STRIPS.
We introduce a novel SAT encoding scheme (SASE) based on the SAS+ formalism. The new
scheme exploits the structural information in SAS+, resulting in an encoding that is both more
compact and efficient for planning. We prove the correctness of the new encoding by establishing
an isomorphism between the solution plans of SASE and that of STRIPS based encodings. We
further analyze the transition variables newly introduced in SASE to explain why it accommodates
modern SAT solving algorithms and improves performance. We give empirical statistical results to
support our analysis. We also develop a number of techniques to further reduce the encoding size of
SASE, and conduct experimental studies to show the strength of each individual technique. Finally,
we report extensive experimental results to demonstrate significant improvements of SASE over the
state-of-the-art STRIPS based encoding schemes in terms of both time and memory efficiency.

1. Introduction
Planning as satisfiability (SAT) is one of the main paradigms for planning. Methods using this
technique usually compile a planning problem into a sequence of SAT instances, with increasing
time horizons (Kautz & Selman, 1999). Planning as satisfiability has a number of distinct characteristics that make it efficient and widely applicable. It makes use of the extensive advancement in
fast SAT solvers. The SAT formulae can be extended to accommodate a variety of complex problems, such as planning with uncertainty (Castellini, Giunchiglia, & Tacchella, 2003), numerical
planning (Hoffmann, Kautz, Gomes, & Selman, 2007) and temporally expressive planning (Huang,
Chen, & Zhang, 2009).
A key factor for the performance of the planning as satisfiability approaches is the SAT encoding
scheme, which is the way a planning problem is compiled into SAT formulae with boolean variables
and clauses. As the encoding scheme has a great impact on the efficiency of SAT-based planning,
developing novel and superior SAT encodings has been an active research topic. Extensive research
has been done to make the SAT encoding more compact. One example of compact encoding is
the lifted action representation (Kautz & Selman, 1996; Ernst, Millstein, & Weld, 1997). In this
compact encoding scheme, an action is represented by a conjunction of parameters. As a result, this
method mitigates the issue of blowing up encoding size. The original scheme does not guarantee
the optimality on makespans. However, an improved lifted action representation that preserves
optimality was proposed (Robinson, Gretton, Pham, & Sattar, 2009). A new encoding is proposed
c
2012
AI Access Foundation. All rights reserved.

fiH UANG , C HEN , & Z HANG

based on a relaxed parallelism semantic (Rintanen, Heljanko, & Niemel, 2006), which also does
not guarantee optimality.
All these previous enhancements are based on the conventional STRIPS formalism for planning.
Recently, the SAS+ formalism (Bckstrm & Nebel, 1996) attracted a lot of attention because of its
rich structural information. The SAS+ formalism represents a planning problem using multi-valued
state variables instead of the propositional facts in STRIPS (Bckstrm & Nebel, 1996). The SAS+
formalism has been used to derive heuristics (Helmert, 2006; Helmert, Haslum, & Hoffmann, 2008),
landmarks (Richter, Helmert, & Westphal, 2008), new search models (Chen, Huang, & Zhang,
2008), and strong mutual exclusion constraints (Chen, Huang, Xing, & Zhang, 2009).
In this paper, we proposed the first SAS+ based SAT encoding scheme (SASE) for classical
planning. Unlike previous STRIPS based SAT encoding schemes that model only actions and facts,
SASE directly models transitions in the SAS+ formalism. Transitions can be viewed as a highlevel abstraction of actions, and there are typically significantly fewer transitions than actions in
a planning task. The proposed SASE scheme describes two major classes of constraints: first the
constraints between transitions and second the constraints that match actions with transitions. We
theoretically and empirically studied the new SAS+ based SAT encoding and compared it against
the traditional STRIPS based SAT encoding. To further improve the performance of SASE, we
proposed a number of techniques to reduce encoding size by recognizing certain structures of actions
and transitions.
We studied the relationship between the solution space of SASE and that of STRIPS based
encoding. The results showed that the solution plans found by SATPlan06, a representative STRIPS
based encoding, and by SASE are isomorphic, meaning that there is a bijective mapping between the
two. Hence, we showed the equivalence between solving the STRIPS based encoding and SASE.
As an attempt to understand the performance gain of SASE, we studied how the new encoding
scheme makes a SAT solving algorithm behave in a more favorable way. The study was quantified
by the widely used VSIDS heuristic (Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). The
transition variables that we introduced have higher frequencies in clauses, and consequently have
higher VSIDS scores. The higher VSIDS scores lead to more branching on transition variables
than action variables. Since the transition variables have high scores and hence stronger constraint
propagation, branching more on the transition variables leads to faster SAT solving. We provided
empirical evidence to support our explanation. Moreover, we introduced an indicator called the
transition index, and empirically showed that there is a strong correlation between the transition
index and SAT solving speedup.
Finally, we evaluated SASE on the standard benchmarks from the recent International Planning
Competitions. Our results show that the new SASE encoding scheme is more efficient in both terms
of time and memory usage compared to STRIPS-based encodings, and solves some large instances
that the state-of-the-art STRIPS-based SAT planners fail to solve.
The paper is organized as follows. After giving some basic definitions in Section 2, we present
our SAS+ based SASE encoding in Section 3 and prove its equivalence to the STRIPS based encoding in Section 4. We study why SASE works better for modern SAT solvers in Section 5. The
techniques to further reduce the encoding size are presented in Section 6. We present our experimental results in Section 7. Finally, we review related works and conclude in Section 8.
294

fiSAS+ P LANNING AS S ATISFIABILITY

2. Background
In this section, we first briefly introduce the STRIPS formalism and review a representative STRIPS
based SAT encoding. Then, we define the SAS+ formalism, from which we develop the new SAT
encoding scheme.
2.1 The STRIPS Formalism
The traditional STRIPS planning representation is defined over binary-valued propositional facts.
A STRIPS planning problem is a tuple  = (F, A, I , G ), where:
 F is a set of propositional facts;
 A is a set of actions. Each action a  A is a triple a = (pre(a), add(a), del(a)), where pre(a)
 F is the set of preconditions, and add(a)  F and del(a)  F are the sets of add facts and
delete facts, respectively;
 A state   F is a subset of facts that are assumed true. Any fact not in  is assumed false in
this state. I  F is the initial state, and G  F is the specification of a goal state or goal
states.
We define three sets of actions. We use ADD(f ) to denote the set of actions that have f as one
of their add effects, meaning ADD(f ) = {a | f  add(a)}. Similarly, two other action sets are
DEL(f ) = {a | f  del(a)} and PRE(f ) = {a | f  pre(a)}.
An action a is applicable to a state  if pre(a)  . We use apply(, a) to denote the state
after applying an applicable action a to , in which variable assignments are changed into ( \
del(a))  add(a). We also write apply(s, P ) to denote the state after applying a set of actions P
in parallel, P  A, to s. A set of actions P is applicable to , when 1) each a  P is applicable
to , and 2) there does not exist two actions a1 , a2  P such that a1 and a2 are mutually exclusive
(mutex) (Blum & Furst, 1997). Two actions a and b are mutex at time step t when one of the
following three conditions holds:
 Inconsistent effects: del(a)  add(b) 6=  or del(b)  add(a) 6= .
 Interference: del(a)  pre(b) 6=  or del(b)  pre(a) 6= .
 Competing needs: There exist f1  pre(a) and f2  pre(b), such that f1 and f2 are mutex at
time step t  1.
Two facts f1 and f2 are mutex at a time step if, for all actions a and b such that f1  add(a), f2 
add(b), a and b are mutex at the previous time step. We call this mutex defined on planning graphs
as P-mutex, in order to distinguish this mutex from another notion of mutex in the next section.
Definition 1 (Parallel solution plan). For a STRIPS planning problem  = (F, A, I , G ), a
parallel solution plan is a sequence P = {P1 , P2 , . . . , PN }, where each Pt  A, t = 1, 2, . . . , N ,
is a set of actions executed at time step t, such that
G  apply(. . . apply(apply(I , P1 ), P2 ) . . . PN ).
295

fiH UANG , C HEN , & Z HANG

2.2 STRIPS Based SAT Encoding (PE)
A SAT instance is a tuple (V, C), where V is a set of variables and C is a set of clauses. Given a
SAT instance (V, C), an assignment  sets every variable v  V true or false, denoted as (v) =
or (v) =. If an assignment  makes every clause in C to be true, then  is a solution to (V, C).
The encoding scheme by SatPlan06 (Kautz, Selman, & Hoffmann, 2006) (denoted as PE in
the following), which is compiled from planning graphs, is a well known and extensively tested
STRIPS based encoding. To facilitate the encoding, SatPlan06 introduces a dummy action dumf
which has f as both its precondition and add-effect. We use A+ to denote the set of actions when
dummy actions are added, which is A  {dumf | f  F}. Unless otherwise indicated, action set
ADD(f ), DEL(f ), and PRE(f ) all include the corresponding dummy actions.
We denote a SatPlan06 encoding up to time step N as PE(, N ), for a given STRIPS task
 = (F, A, I , G ). As a SAT instance, PE(, N ) is defined by (V, C), where V = {Wf,t |f 
F, t  [1, N + 1]}  {Wa,t |a  A+ , t  [1, N ]}. Wf,t = indicates that f is true at t, otherwise
Wf,t =. The clause set C includes the following types of clauses:




I. Initial state: (f, f  I ): Wf,1 ;
II. Goal state: (f, f  G ): Wf,N +1 ;
III. Add effect: (f  F, t  [1, N ]): Wf,t+1 

W

a,f add(a) Wa,t ;

IV. Precondition: (a  A+ , f  pre(a), t  [1, N ])): Wa,t  Wf,t ;
V. Mutex of actions: (a, b  A+ , t  [1, N ], a and b are mutex): W a,t  W b,t ;
VI. Mutex of facts: (f, g  F, t  [1, N + 1], f and g are mutex) : W f,t  W g,t ;
Clauses in class I and II enforce that the initial state is true at the first time step, and that the
goal facts need to be true at the last time step, respectively. Clauses in class III specify that if a fact
f is true at time step t, then there is at least one action a  A+ at time step t  1 that has f as an
add effect. Clauses of class IV specify that if an action a is true at time t, then all its preconditions
are true at time t. Classes V and VI specify mutex between actions and facts, respectively.
PE is one of the most typical SAT encoding schemes for STRIPS planning. It has both action
variables and fact variables, and enforces the same semantics as the one defined by a planning graph.
Later we will show the equivalence between our new SASE encoding and PE.
2.3 The SAS+ Formalism
The SAS+ formalism (Bckstrm & Nebel, 1996) represents a classical planning problem by a set
of multi-valued state variables. A planning task  in the SAS+ formalism is defined as a tuple
 = {X , O, sI , sG }, where
 X = {x1 ,    , xN } is a set of state variables, each with an associated finite domain Dom(xi );
 O is a set of actions and each action a  O is a tuple (pre(a), eff(a)), where pre(a) and
eff(a) are sets of partial state variable assignments in the form of xi = v, v  Dom(xi );
 A state s is a full assignment (a set of assignments that assigns a value to every state variable).
If an assignment (x = f ) is in s, we can write s(x) = f . We denote S as the set of all states.
296

fiSAS+ P LANNING AS S ATISFIABILITY

 sI  S is the initial state, and sG is a partial assignment of some state variables that define
the goal. A state s  S is a goal state if sG  s.
We first define what transition is. In this paper, we build constraints by recognizing that transitions are atomic elements of state transitions. Actions, cast as constraints as well in our case, act as
another layer of logic flow over transitions.
Definition 2 (Transition). For a SAS+ planning task  = {X , O, sI , sG }, given a state variable
x  X , a transition is a re-assignment of x from value f to g, f, g  Dom(x), written as fxg , or
x
from an unknown value to g, written as g
. We may also simplify the notation of fxg as f g
or , when there is no confusion.
Transitions in a SAS+ planning task can be classified into three categories.
 Transitions of the form fxg are called regular. A regular transition fxg is applicable to a
state s, iff s(x) = f . Let s = apply(s, fxg ) be the state after applying transition  to state
s, we have s (x) = g.
 Transitions of the form fxf are called prevailing. A prevailing transition fxf is applicable
to a state s iff s(x) = f , and apply(s, fxf ) = s.
x
x
can be
are called mechanical. A mechanical transition g
 Transitions of the form g
x


applied to an arbitrary state s, and the result of apply(s, g ) is a state s with s (x) = g.

A transition is applicable at a state s only in the above three cases. For each action a, we denote
its transition set as T rans(a), which includes: all regular transitions fxg such that (x = f ) 
pre(a) and (x = g)  eff(a), all prevailing transitions fxf such that (x = f )  pre(a), and all
x
mechanical transitions g
such that (x = g)  eff(a). Given a transition , we use A() to denote
the set of actions a such that   T rans(a). We call A() the supporting action set of .
x
}, for all f, g  Dom(x),
For a state variable x, we introduce T (x) = {fxg }{fxf }{g
which is the set of all transitions that affect x. We also define T as the union of T (x), x  X . T
is the set of all transitions. We also use R(x) = {fxf | f, f  Dom(x)} to denote the set of all
prevailing transitions related to x, and R the union of R(x) for all x  X .
Definition 3 (Transition Mutex). For a SAS+ planning task, two different transitions 1 and 2
are mutually exclusive iff there exists a state variable x  X such that 1 , 2  T (x), and one of
the following holds:
1. Neither 1 nor 2 is a mechanical transition.
2. At least one of 1 and 2 is a mechanical transition, with 1 and 2 transit to different values.
A set of transitions T is applicable to a state s when 1) every transition   T is applicable to
s, and 2) there do not exist two transitions 1 , 2  T such that 1 and 2 are mutually exclusive.
When T is applicable to s, we write apply(s, T ) to denote the state after applying all transitions in
T to s in an arbitrary order.
297

fiH UANG , C HEN , & Z HANG

Definition 4 (Transition Plan). A transition plan is a sequence of {T1 , T2 , . . . , TN }, where each
Tt , t  [1, N ], is a set of transitions executed at time step t, such that
sG  apply(. . . apply(apply(sI , T1 ), T2 ) . . . TN ).
In a SAS+ planning task, for a given state s and an action a, when all variable assignments in
pre(a) match the assignments in s, a is applicable in state s. We use apply(s, a) to denote the state
after applying a to s, in which variable assignments are changed according to eff(a).
Definition 5 (S-Mutex). For a SAS+ planning task  = {X , O, sI , sG }, two actions a1 , a2  O
are S-mutex iff either of the following holds:
1. There exists a transition , such that  is not prevailing ( 6 R), and   T rans(a1 ) and
  T rans(a2 ). Actions a1 and a2 in this case deletes each others precondition.
2. There exist two transitions  and   such that they are mutually exclusive to each other and
  T rans(a1 ) and    T rans(a2 ).
We named this mutex in SAS+ planning S-mutex to distinguish it from the P-mutex defined
in STRIPS planning. We will show in Section 4 that these two types of mutual exclusions are
equivalent. Therefore, in this paper we in general use the single term mutual exclusion (mutex) for
both, unless otherwise indicated.
For a SAS+ planning task, we write apply(s, P ) to denote the state after applying a set of
actions P , P  O, to s. A set of actions P is applicable to s when 1) each a  P is applicable to s,
and 2) there are no two actions a1 , a2  P such that a1 and a2 are S-mutex.
Definition 6 (Action Plan). For a SAS+ task, an action plan is a sequence P = {P1 , . . . , PN },
where each Pt , t  [1, N ], is a set of actions executed at time step t such that
sG  apply(. . . apply(apply(sI , P1 ), P2 ) . . . PN ).
The definition of an action plan for SAS+ planning is essentially the same as that for STRIPS
planning (Definition 1). The relation between transition plan and action plan is the key to the new
encoding scheme introduced in this paper. There always exists a unique transition plan for a valid
action plan. In contrast, given a transition plan, there may not be a corresponding action plan; or
there could be multiple corresponding action plans.
Definition 7 (Step Optimal Plan). For a SAS+ planning task, a step optimal plan is an action
plan P = {P1 , . . . , PN } with the minimum N .
It is worth noting that there are a few different optimization metrics in classical planning research, including step optimality (Definition 7), the number of actions and the total action cost. The
criteria used in recent IPC competitions (The 6th Intl Planning Competition, 2008; The 7th Intl
Planning Competition, 2011) is the total action cost. While step optimality is a widely used criterion, action cost is a more realistic criterion in many domains such as those involving numerical
resources. Nevertheless, action cost assumes plans to be sequential. In other words, it does not
consider concurrency between actions, which is a limitation in many cases.
298

fiSAS+ P LANNING AS S ATISFIABILITY

The step optimality was introduced in GraphPlan (Blum & Furst, 1997), and became more
popular when planning graph analysis was used in several planning systems (Kautz & Selman,
1999; Hoffmann & Nebel, 2001; Do & Kambhampati, 2000). Step optimality takes the concurrency
between actions into consideration, although it assumes a same unit duration for all actions. The
planning methods for step optimality, in particular SAT-based planners, can potentially be made
useful for other optimization metrics, which is a topic of our future work.

3. SAS+ Based SAT Encoding (SASE)
We now introduce our new encoding for SAS+ planning tasks, denoted as SASE. We use the same
search framework as SatPlan: start with a small number of time steps N and increase N by one
after each step until a satisfiable solution is found. For a given N , we encode a planning task as a
SAT instance which can be solved by a SAT solver. A SASE instance includes two types of binary
variables:
1. Transition variables: U,t ,   T and t  [1, N ], which may also be written as Ux,f,g,t when
 is explicitly fxg ;
2. Action variables: Ua,t , a  O and t  [1, N ].
As to constraints, SASE has eight classes of clauses for a SAS+ planning task. In the following,
we define each class for every time step t  [1, N ] unless otherwise indicated.
A. Initial state: x, sI (x) = f ,
B. Goal: x, sG (x) = g,

W

W

f g T (x) Ux,f,g,1 ;

f g T (x) Ux,f,g,N ;

x
C. Progression: hf
 T and t  [1, N  1], Ux,h,f,t 

D. Regression: fxg  T and t  [2, N ], Ux,f,g,t 

W

W

fxg T (x) Ux,f,g,t+1 ;

fx f T (x) Ux,f  ,f,t1 ;

E. Transition mutex: 1 2 such that 1 and 2 are transition mutex, U 1 ,t  U 2 ,t ;
V
F. Composition of actions: a  O, Ua,t  T rans(a) U,t ;
W
G. Action existence:   T \ R, U,t  a,T rans(a) Ua,t ;

H. Action mutex: a1 a2 such that ,   T (a1 )  T (a2 ) and  6 R, U a1 ,t  U a2 ,t ;
Clauses in classes C and D specify and restrict how transitions change over time steps. Clauses
in class E enforce that at most one related transition can be true for each state variable at each time
step. Clauses in classes F and G together encode how actions are composed to match the transitions.
Clauses in class H enforce the mutual exclusions between actions.
Note that there are essential differences between transition variables in SASE and fact variables
in PE. In terms of semantics, a transition variable at time step n in SASE is equivalent to the
conjunction of two fact variables in PE, at time step n and n + 1, respectively. Nevertheless,
fact variables are not able to enforce a transition plan as transition variables do. This is because
299

fiH UANG , C HEN , & Z HANG

transition variables not only imply the values of multi-valued variables, but also enforce how these
values propagate over time steps.
In addition, transition variables are different from action variables regarding their roles in SAT
solving. This is because in SASE, action variables only exist in the constraints for transition-action
matching, but not in the constraints between time steps. Transition variables exist in both. Thus
transition variables appear more frequently in a SAT instance. The inclusion of those high-frequency
variables can help the SAT solvers through the VSIDS rule for variable branching. We shall discuss
this issue further and provide an empirical study, in Section 5.
We now show how SASE works using an example. Consider a planning task with two multivalued variables x and y, where Dom(x) = {f, g, h} and Dom(y) = {d, e}. There are three
y
y
y
x
actions a1 = {fxg , de
}, a2 = {fxg , ed
} and a3 = {gh
, ed
}. The initial state is
{x = f, y = d} and the goal state is {x = h, y = d}. One solution to this instance is a plan of two
actions: a1 at time step 1 and then a3 at time step 2.
In the following we list the constraints between transitions and actions, namely those specified
in classes F and G. The clauses in other classes are self-explanatory. In particular, here we only list
the variables and clauses for time step 1, because these constraints all repeat for time step 2. The
transition variables at time step 1 are { Ux,f,g,1 , Ux,f,f,1 , Ux,g,h,1 , Ux,g,g,1 , Ux,h,h,1 , Uy,d,d,1 , Uy,e,e,1 ,
Uy,e,d,1 , Uy,d,e,1 }, and they repeat for time step 2. The action variables at time step 1 are { Ua1,1 ,
Ua2,1 , Ua3,1 }, and they repeat for time step 2.
The clauses in class F are: U a1,1  Ux,f,g,1 , U a1,1  Ux,d,e,1 , U a2,1  Ux,f,g,1 , U a2,1  Uy,e,d,1 ,
U a3,1  Ux,g,h,1 and U a3,1  Ux,e,d,1 . The clauses in class G are U x,f,g,1  Ua1,1  Ua2,1 , U x,g,h,1 
Ua3,1 , U y,d,e,1  Ua1,1 , and U y,e,d,1  Ua2,1  Ua3,1 .


The solution, in terms of actions, has action variables Ua1,1 and Ua3,2 to be , and all other action variables . In addition, the corresponding transition plan has the following transition variables
: {Ux,f,g,1 , Ux,g,h,2 , Uy,d,e,1 , Uy,e,d,2 }, while all other transition variables are false.


As mentioned above, although there are often multiple transition plans, a transition plan may
not correspond to a valid action plan. In this particular example, there are several different transition
plans that satisfy the initial and the goal states, but some of them do not have a corresponding action
plan. For example, suppose transition variables {Ux,f,g,1 , Ux,g,h,2 , Uy,d,d,1 , Uy,d,d,2 } to be true. This
qualifies as a transition plan, because the goals are achieved. This transition plan however does not
lead to a valid action plan.

4. Correctness of SAS+ Based Encoding
It is important to prove the correctness of the proposed encoding. We achieve this by proving that
SASE for SAS+ planning has the same solution space as that of PE used in STRIPS planning. More
specifically, we show that, for a given planning task and a given time step N , the SAT instance from
SASE is satisfiable if and only if the SAT instance from PE is satisfiable. Here, we assume the
correctness of the PE encoding, in SatPlan06 (Kautz et al., 2006) for STRIPS planning.
PE(, N ) denotes the PE formula that corresponds to the N-step planning problem. SASE(, N )
gives the formula in the case of SASE encoding of the equivalent SAS+ problem.
300

fiSAS+ P LANNING AS S ATISFIABILITY

4.1 Solution Structure of STRIPS Based Encoding
In this section, we study a few properties of the solutions in a STRIPS based encoding. These properties provide some key insights for establishing the relationship between PE and SASE encodings.
Lemma 1 Given a STRIPS task  = (F, A, I , G ), a time step N , and its PE SAT instance
PE(, N ) = (V, C), suppose there is a satisfiable solution denoted as , a fact f  F, and
t  [1, N ] such that: 1) (Wdumf ,t ) =, 2) (Wf,t ) = , and 3) a  DEL(f ), (Wa,t ) =,
then we can construct an alternative solution  to PE(, N ) as follows:


 (v) =

(





, v = Wdumf ,t , v  V

(v), v 6= Wdumf ,t , v  V

(1)

Proof This can be proved by showing that  satisfies every individual clause in C. See Appendix A for more details.

Lemma 2 Given a STRIPS task  = (F, A, I , G ), a time step N , and its PE SAT instance
PE(, N ) = (V, C), suppose there is a satisfiable solution denoted as , a fact f  F, and t 
[1, N ] such that: 1) (Wf,t ) =, 2) there exists an action a  ADD(f ) such that (Wa,t1 ) = ,
then we can construct an alternative solution  to PE(, N ) as follows:




 (v) =

(

, v = Wf,t , v  V

(v), v 6= Wf,t , v  V

(2)

Proof We can prove this by showing that  makes every individual clause (three types) in C to
be true. See Appendix A for more details.

Lemmas 1 and 2 show that under certain conditions, some dummy action variables and fact
variables in PE are free variables. They can be set to be either true or false while the SAT instance
remains satisfied. Although we can manipulate these free variables to construct an alternative solution  from a given solution , both  and  refer to the same STRIPS plan, because there is no
change to any action variable. This leads to an important insight concerning the solutions of PE: a
solution plan to a STRIPS planning problem  may correspond to multiple solutions to PE(, N ).
Proposition 1 Given a STRIPS task  = (F, A, I , G ), a time step N , and its PE SAT instance
PE(, N ) = (V, C), those clauses that define competing needs mutex and fact mutex can be inferred
from other clauses in PE(, N ).
These mutexes are implied by the PE formula, thus by the completeness of resolution, Proposition 1 is true. Proposition 1 implies that when encoding a STRIPS task, it is not necessary to encode
fact mutex and competing needs action mutex, as they are implied by other clauses. Therefore,
while considering the completeness and correctness of PE, we can ignore these redundant clauses.
Analysis with a similar conclusion can be found in the literature (Sideris & Dimopoulos, 2010),
although different approaches are used.
301

fiH UANG , C HEN , & Z HANG

4.2 Equivalence of STRIPS and SAS+ Based Encodings
A classical planning problem can be represented by both STRIPS and SAS+ formalisms that give
rise to the same set of solutions. Given a STRIPS task  = (F, A, I , G ) and its equivalent SAS+
planning task  = (X , O, sI , sG ), the following isomorphisms (bijective mappings) exist:
Q
 f : F  X Dom(X) (a binary STRIPS fact corresponds to an variable assignment in
SAS+);
 a : A  O (a STRIPS action corresponds to a SAS+ action);
 i : I  sI (can be derived from f );
 g : G  sG (can be derived from f ).
Furthermore, since both formalisms represent the same planning task, these mappings preserve
the relations between actions and facts. For example, if f  pre(a) where f  F and a  A in a
STRIPS formalism, we have f (f )  pre(a (a)) in the SAS+ formalism.
First, we show that the parallelism semantics enforced by S-mutex in SAS+ is equivalent to that
of P-mutex in STRIPS.
Lemma 3 Given a SAS+ planning task  = (X , O, sI , sG ) and its equivalent STRIPS task  =
(F, A, I , G ), suppose we have actions a, b  O, and their equivalent actions a , b  A (i.e.
a = a (a ) and b = a (b )), a and b are S-mutex iff a and b are P-mutex.
Proof We can prove this by showing in both directions that given two actions with one type of
mutex, there is also mutux of the other type. Details are in Appendix A.

Lemma 3 connects P-mutex and S-mutex. Based on that we can construct the relations between
the encodings, which are used in the proofs of both Theorems 1 and 2, respectively.
Theorem 1 Given a STRIPS task  and a SAS+ task  that are equivalent, for a time step bound
N , if PE(, N ) is satisfiable, SASE(, N ) is also satisfiable.
Proof We prove this theorem by construction. Suppose we know the solution to PE, we prove
that we can construct a solution to SASE accordingly. Details are in Appendix A.

Theorem 2 Given a STRIPS task  and a SAS+ task  that are equivalent, for a time step bound
N , if SASE(, N ) is satisfiable, PE(, N ) is also satisfiable.
Proof This can be proved by using the same technique used for Theorem 1. See Appendix A for
more details.

From Theorems 1 and 2, we reach the following conclusion.
Theorem 3 A classical planning problem is solvable by the PE encoding if and only if it is solvable by the SASE encoding. Further, for solvable problems, the solution plans found by the two
encodings have the same, optimal makespan.
Theorem 3 reveals that a planning solution can be found by SASE if and only if it can be
found by PE. In terms of SAT solutions, the proofs show that there is an epimorphism (a surjective
mapping) between the solutions to PE and the solutions to SASE. That is, multiple SAT solutions in
302

fiSAS+ P LANNING AS S ATISFIABILITY

PE map to one SAT solution in SASE and every SAT solution in SASE is mapped from at least one
SAT solution in PE. This is due to the existence of free variables in the PE encoding. One solution in
SASE corresponds to a group of solutions in PE with the same assignments to real action variables
but different assignments to the free variables.

5. SAT Solving Efficiency on Different Encodings
In Section 4, we showed that PE and SASE are semantically equivalent in that they have the same solution space. In this section, we study what makes them different regarding SAT solving efficiency.
In particular, we want to understand how PE and SASE make a SAT solver behave differently
on the same planning task. Modern SAT solvers, which nowadays all employ many sophisticated
techniques, are too complicated to be characterized by simple models. In general, it is difficult to
accurately estimate the time that a SAT solver needs to solve a SAT instance. In this section, we
provide an explanation of why the SAT encodings from SASE are more efficient for SAT solvers to
solve than the SAT encodings from PE, and provide empirical evidence to support this explanation.
In this section, we first discuss SASEs problem structure in Section 5.1 and the reason that
the widely used SAT solving heuristic VSIDS (Moskewicz et al., 2001) works better on SASE encodings. The idea of VSIDS is to select those variables that appear frequently in the original and
learnt clauses, since they lead to stronger constraint propagation and space pruning. If we order all
the variables by their VSIDS scores, a large population of the top-ranked transition variables introduced in SASE have higher VSIDS scores than the top-ranked action variables. As a result, those
top-ranked transition variables are selected more often and provide stronger constraint propagation,
speeding up SAT solving.
We study the significance of transition variables, and explain why they make search more efficient. In Section 5.2, we present a comparison on transition variables versus action variables. In
Section 5.3, we show how often transition variables are chosen as decision variables, which is a
direct evidence of transition variables significance.
Finally, in Section 5.4 we empirically define a significance index of transition variables. This
index measures the significance of transition variables within the context of the VSDIS heuristic,
and correlates with the speedup in SAT solving. All analysis in this section uses SatPlan06 as the
baseline.
5.1 VSIDS Heuristic in SAT Solving
The SAT solvers we use are based on the Conflict Driven Clause Learning framework. A decision
variable refers to the one selected as the next variable for branching. Once a decision variable
is chosen, more variables could be fixed by unit propagation. The ordering of decision variables
significantly affects the problem solving efficiency. Most existing complete SAT algorithms use
variants of the VSIDS heuristic (Moskewicz et al., 2001) as their variable ordering strategy.
The VSIDS heuristic essentially evaluates a variable by using the Exponential Moving Average
(EMA) of the number of times (frequency) it appears in all the clauses. This frequency value
keeps changing because of the learnt clauses. Therefore, VSIDS uses a smoothing scheme which
periodically scales down the scores of all variables by a constant in order to reflect the importance of
recent changes in frequencies. The variable that occurs most frequently usually has a higher value,
thus also a higher chance of being chosen as a decision variable. A random decision is made if
there is a tie. Thus, variables associated with more recent conflict clauses have higher priorities. We
303

fiH UANG , C HEN , & Z HANG

Figure 1: Illustration of how the search spaces of two encoding schemes differ from each other.
first consider the frequency in the original clauses only. Then we investigate further by taking the
periodic update into consideration.
Given the fact that the frequency is used as the main measurement, VSIDS will be most effective
when the difference between variables frequencies are large and there are some variables with high
frequencies. If all variables have the same frequency, then picking decision variables will be purely
random. Further, variables with high frequencies are desirable since they lead to stronger constraint
propagation.
The major difference between the SAT instances in PE and SASE is that in the latter encoding,
actions are not responsible for the constraint propagation across time steps. Figure 1 illustrates their
difference. In SASE, the SAT instance can be conceptually reduced to the following search problem
of two hierarchies.
 At the top level, we search for a transition plan as defined in Definition 4. This amounts to
finding a set of transitions for each time step t (corresponding to all  such that U,t is set to
), so that they satisfy the clauses in classes A-E of the SASE encoding.


 At the lower level, we try to find an action plan that satisfies the transition plan. In other
words, for a given transition plan that satisfies clauses in classes A-E, we try to find an action
plan satisfying clauses in classes F-H.
5.2 Transition Variables versus Action Variables
Let us first formulate how the frequency of a variable is measured. Given a SAT instance (V, C),
for each variable v  V , we define a function h(v) to indicate the frequency of v. That is, h(v) is
the number of clauses that v appears in. We sort all variables in V by their h values in a descending
order, and study the variables above a certain h value.
Definition 8 (High h Value Variable Set). Given a SAT instance (V, C), for a h  {h(v) | v 
V }, we denote V (h ) as the set of all variables v  V such that h(v)  h .
We further define percentile and top variable set to quantify our analysis. Instead of a specific
h() value, we use a percentage of all h values to make the analysis comparable across instances.
304

fiSAS+ P LANNING AS S ATISFIABILITY

Definition 9 (Percentile). Given a SAT instance (V, C), the percentile hp (0  p  100), is the h
value of a variable v  V , such that at least p% of the variables v   V have h(v  ) larger than or
equal to hp .
Definition 10 (Top p Variable Set). Given a SAT instance (V, C) and a percentile hp , we call
V (hp ) the top p variable set, denoted as V p .
We use Vo and V to denote the action variables and transition variables in V , respectively. We
also define Vop = Vo  V p , and similarly Vp = V  V p . Table 1 compares the h values of transition
variables and action variables in SAT instances. This table has two parts. For the first part, we list
the average and standard deviation of h values for both transition variables and action variables.
The data are collected from the first satisfiable SAT instance of the largest solvable planning task
in every domain that we consider. From the average h value, it is evident that in most domains
transition variables occur more frequently than action variables. Furthermore, the standard deviation
of transition variables are in general not only larger than action variables standard deviation, but
also even larger than the expected value of transition variables. The high frequencies of transition
variables, along with large standard deviations, are preferred by the VSIDS heuristic and can aid
SAT solving, as we discussed earlier.
The second part lists the average h values for transition variables and action variables, in the
top p variable set with different values of p: 1%, 2%, 5% and 10%. The difference between Vop and
Vp is very large. In most domains, transition variables dominate the top variable sets, while action
variables exist in the top 10% variable set in only a few domains. One exception is the Airport
domain. However, even in this domain, although the average h value of all transition variables is
smaller than the average h value of action variables, among the top 1% variables, the average h value
of transition variables is larger than the average h value of action variables. Since VSIDS picks the
variable with the highest heuristic value, transition variables have higher chances be picked as the
decision variables.
5.3 Branching Frequency of Transition Variables
In Section 5.2, we considered the difference between transition variables and action variables, in
terms of the h values. As mentioned earlier, however, the VSIDS heuristic periodically updates
heuristic values of all variables. The dynamic updating of heuristic values is not captured by the
above analysis. In the following, we present more direct empirical evidence to show that transition
variables are indeed chosen more frequently than action variables for branching, especially at early
stages of SAT solving. That is, a SAT solver spends most of its time deciding an appropriate
transition plan. This analysis takes into consideration VSIDSs dynamic updating strategy.
We empirically test the probabilities that transition variables and action variables are chosen
as branching variables. We measure for every k consecutive decision variables, the number of
transition variables (M ) and action variables (Mo ) selected as the decision variables. If all variables
are selected equally likely, we should have
E(M ) = k

|V |
|Vo |
and E(Mo ) = k
,
|V | + |Vo |
|V | + |Vo |

(3)

which implies:
E(M )
E(Mo )
=
k|V |
k|Vo |
305

(4)

fiH UANG , C HEN , & Z HANG

Instances

N

Airport-48
Depot-14
Driverlog-16
Elevator-16
Freecell-6
Openstacks-2
Parcprinter-20
Pathways-17
Pegsol-25
Pipe-notankage-49
Pipe-tankage-26
Rovers-18
Satellite-13
Scanalyzer-28
Sokoban-6
Storage-13
TPP-30
Transport-17
Trucks-13
Woodworking-30
Zenotravel-16

68
12
18
15
16
23
19
21
25
12
18
12
13
5
35
18
11
22
24
4
7

V
h

8.6
7.6
10.5
6.0
32.3 11.1
22.2
7.4
42.6 58.0
14.1
5.2
12.0 11.8
5.5
8.5
23.0 15.5
22.9 47.1
58.1 116.7
15.5 14.6
31.8
7.8
113.0 151.4
15.6
4.8
4.7
1.9
12.3 16.1
22.8 19.0
5.1
7.6
6.2
5.1
20.2 25.0

Vo
h

19.6 12.9
6.3 3.3
5.6 3.1
10.2 3.8
33.2 7.0
11.5 4.3
15.5 5.9
12.9 3.6
15.2 6.3
41.3 3.4
50.7 12.8
16.0 6.9
2.0 0.3
8.6 1.1
20.0 4.7
6.3 1.6
4.8 0.7
4.5 1.1
6.4 1.2
10.2 3.5
3.9 0.3

1%
98.5
32.5
43.9
27.0
115.6
17.2
44.3
33.6
30.0
77.1
266.4
175.1
35.0
242.8
16.6
10.4
84.2
99.6
56.7
23.1
51.3

h of Vp
h of Vop
2% 5 % 10% 1% 2% 5 %
75.5 59.2 23.4 39.6 35.9 34.7
28.6 23.7 20.9
34.6 26.5 23.4
18.9 18.9 18.9
86.0 49.0 34.7
17.2 16.2 15.2
42.8 26.7 17.8 30.0 30.0 30.0
26.9 15.9 14.5
29.8 17.2 15.5
57.5 36.4 25.3
174.0 86.8 56.0
86.0 35.5 35.5
35.0 35.0 35.0
175.8 129.7 129.7
14.1 12.8 11.2
10.4
9.0
8.1
57.8 34.4 24.6
58.1 52.0 41.6
38.2 20.8 16.3
22.1 18.9 17.2
51.3 36.2 28.5
-

10%
32.4
30.0
10.0
9.0
13.1
-

Table 1: The h values of transition variables versus action variables in all domains. Column N is
the optimal makespan. Column h is the average and Column  is the standard deviation. Column
h of Vp  and h of Vop  refer to the average h value of transition variables and action variables in
V p , while p equals to 1, 2, 5 or 10. - means there is no variable in that percentile range.

We empirically study where we divide the SAT solving process into epoches of length k = 1000
each, for all domains from IPC-3 to IPC-6. We present the results of three representative domains
in Figure 2, and the results on all domains in Figures 9 and 10 in Appendix B. In each domain,
we choose an instance with at least 100,000 decisions. In some domains (e.g. Woodworking),
even the biggest instance has only thousands of decisions. In such a case, we choose the instance
with the largest number of decisions. For every epoch, we plot the branching frequency, which is
M
Mo
k|V | for transition variables and k|Vo | for action variables, respectively. According to (4), these two
branching frequencies should be about the same if the two classes of variables are chosen equally
likely.
The results from Openstacks and Zenotravel show clear distinctions between transition variables
and action variables. While they are evidently different, the branching frequencies in Openstack
has a higher variance. The results of Storage domain show a completely different pattern, where
variables do not distinguish by their branching frequencies.
From Figures 9 and 10, it is evident that, for all these instances except Storage-12 and Woodworking20, the branching frequencies of transition variables are higher than that of action variables. In fact,
in many cases, the branching frequencies of transition variables can be up to 10 times higher than
those of action variables. In Transport-26 and Zenotravel-15, the difference is orders of magnitude
larger. Hence, this empirical study shows that the SAT solvers branch much more frequently on the
newly introduced transition variables than the action variables.
306

fiSAS+ P LANNING AS S ATISFIABILITY

0.7

0.3

0.6

0.25

0.05

0.5

Transition Vars
Action Vars

0.06

0.2

0.04

0.15

0.03

0.1

0.02

0.1

0.05

0.01

0

0

0.4
0.3

Transition Vars
Action Vars

0.2

300000

600000

(a) Openstack-5, N = 22, Unsat

Transition Vars
Action Vars

0
30000

60000

(b) Storage-12, N = 9, Satisfiable

0

50000 100000 150000 200000 250000 300000

(c) Zenotravel-15, N = 7, Satisfiable

Figure 2: Comparison of variable branching frequency (with k = 1000) for transition and action variables
in solving certain SAT instances on instances from three representative domains: Openstack, Storage and
Zenotravel.
5.4 Transition Index and SAT Solving Speedup
The behavior of transition variables, as presented above, suggests that there is a correlation between
the significance of transition variables and the speedup that SASE achieves. Nevertheless, the study
on branching frequency only profiles the connection by showing what happens during SAT solving.
Another interesting study should reveal what leads to the speedup in a more direct way. To quantify
the analysis, we introduce transition index.
As mentioned earlier, the h value does not exactly reflect how VSIDS works, as it updates
dynamically throughout SAT solving. Nevertheless, by putting together all variables and studying
their h values, the statistics on the population leads to the following definition of the transition
index.
Definition 11 (Transition Index). Given a planning problems SAT instance (V, C), we measure
the top p(0  p  100) variable set, and calculate the transition index of p as follows:
|Vp |/|V p |
|V |/|V |
Essentially, the transition index measures the relative density of transition variables in the top
variable set. If the distribution of the transition variables is homogeneous under the total ordering
based on h, |Vp |/|V p | should equal to |V |/|V | for any given p. A transition index larger than 1
indicates that the transition variables have a higher-than-normal density in the top p% variable set.
The larger the transition index is, the more often the transition variables occurring in the top p%
variable set.
Given a planning problems SAT instance, there is correlation between its transition index and
the speedup SASE provides. In Figures 3 and 4 we measure such correlation for all the domains
from IPC-3 to IPC-6. Each dot in one of the figures refers to an individual planning instance. The
y-axis is the speedup of SASE over SatPlan06. The x-axis is the transition index under a given p.
Bootstrap aggregating (Breiman, 1996) is used for the regression lines. For each measurement, we
calculate Spearmans rank correlation coefficient (Myers & Well, 2003), which assesses how well
the relationship between two variables can be described using a monotonic function. If there are
no repeated data values, a perfect Spearmans correlation coefficient of 1 occurs when each of the
variables is a perfect monotone function of the other.
307

fiH UANG , C HEN , & Z HANG

25

25

20

Speed Up

Speed Up

20

Correlation coefficient: 0.364647

15

10

Correlation coefficient: 0.379701

25

20

15

10

15

10

15

10

5

5

5

5

0
0

0
0

0
0

0
0

1

2

3

4

5

1

2

3

4

5

Correlation coefficient: 0.376107

20

Speed Up

Correlation coefficient: 0.36747

Speed Up

25

1

2

3

Transition Index

Transition Index

Transition Index

(a) p = 1

(b) p = 2

(c) p = 5

4

5

1

2

3

4

5

Transition Index

(d) p = 10

Figure 3: The correlation between SAT solving speedup and the transition index with different p.
All problem instances are included. We can see a clear cluster of outliers on the bottom-left of each
graph, all of which are from the Airport and Rovers domains.
The instances included in Figure 3 are those solved by both SatPlan06 and SASE, with Precosat
as the SAT solver. To reduce noise, we do not consider those small instances that both SASE and
SatPlan06 spend less than 1 second to solve. In total we have 186 instances. The speedup of each
instance is SASEs SAT solving time divided by SatPlan06s SAT solving time, which is greater
than 1 in most cases. It can be observed there is a trend that a larger transition index leads to higher
speedup. Such a result links the significance of top ranked (high frequency) transition variables to
the speedup in SAT solving.

25

15

Speed Up

Speed Up

Correlation coefficient: 0.595048

25

20

10

Correlation coefficient: 0.596698

25

20

15

10

15

10

15

10

5

5

5

5

0

0

0

0

5
0

1

2

3

4

5



5
0

1

2

3

4



5

5
0

Correlation coefficient: 0.598874

20

Speed Up

Correlation coefficient: 0.594997

Speed Up

25

20

1

2

3

Transition Index

Transition Index

Transition Index

(a) p = 1

(b) p = 2

(c) p = 5

4

5



5
0

1

2

3

4

5

Transition Index

(d) p = 10

Figure 4: The correlation between SAT solving speedup and the transition index with different p.
Instances from Airport and Rovers domains are not included.
In Figure 3 there is a cluster of instances of small transition indices (to the bottom-left of each
plot in Figure 3). These instances distinguish themselves by having much smaller transition indexes.
In fact, it turns out that these instances are all from either the Airport or the Rovers domain and have
the same property: there are a very high number of action mutual exclusions, contributing to the
majority of clauses. On the other hand, mutual exclusions are binary constraints, which do not
contribute significantly to a SAT problems hardness, because they are trivial for unit propagation.
As mentioned earlier, transition index is merely a heuristic to indicate the significance of transition
variables. In these instances, the enormous number of action mutual exclusion constraints makes the
transition index small. However, they do not make the problems harder, because two-literal clauses
are trivial for SAT solving. As a result, we can ignore these outlier instances in our correlation
analysis. In Figure 4 we removed those instances from the Airport and Rovers domains, resulting
in a total of 159 instances. In this analysis, the correlation becomes even more explicit.
There is, however, one caveat for this study by including all instances from all domains. Some
domains have more than thirty instances solved, while in some other domains, we can only solve
308

fiSAS+ P LANNING AS S ATISFIABILITY

Instances
Pipesworld-20
Storage-20
Openstack-10
Airport-20
Driverlog-15

Before subsumed
count
size
2548
21.72
1449
12.46
221
22.44
1024
6.45
1848
2.82

After subsumed
count
size
516
53.66
249
60.22
141
23.4
604
8.49
1848
2.82

Table 2: Statistics of action cliques, before and after the subsumed action cliques are reduced.
count" gives the number of action cliques, and size" is the average size of the action cliques.

five. As a result, this study is biased toward those domains with more instances solved. It will be
an interesting future study to see how transition index works in a more sophisticated experimental
setting, such as by eliminating certain domain specific factors (Hoffmann, Gomes, & Selman, 2006).

6. Reducing the Encoding Size of SASE
We now propose several techniques to further reduce the size of SAT instances in SASE. We first
represent all mutual exclusions in SASE using a more compact clique representation. We then
develop a few new techniques to recognize the special structures of SASE and further reduce the
encoding size.
6.1 Mutual Exclusion Cliques
Mutual exclusions in SASE naturally define cliques of transitions or actions in which at most one
of them can be true at any time step. There are two types of cliques: 1) for each x  X , T (x)
is a clique of transitions enforced by the class E clauses, and 2) for each transition  that is not
prevailing, A() is a clique of actions enforced by the class H clauses.
It requires O(n2 ) clauses to encode all mutexes within a clique of size n in a pair-wise manner.
To reduce the number of clauses used, we in SASE use a compact representation (Rintanen, 2006),
which uses (n log n) auxiliary variables and (n log n) clauses. For cliques with large n, the
reduction in number of clauses will be significant. To show how it works, consider a simple example.
Suppose that we have a clique {x, y, z} where at most one variable can be true. We introduce
auxiliary variables b0 and b1 and clauses x  b0  b1 , y  b0  b1 and z  b0  b1 .
6.2 Reduction Techniques
Action variables form the majority of all variables, and also lead to many clauses to represent action
mutual exclusions even if the clique technique is used. Thus, it is important to reduce the number
of action variables. We propose three methods when certain structure of a SAS+ planning task is
observed.
6.2.1 R EDUCING S UBSUMED ACTION C LIQUES
We observe that many action cliques share common elements, while transition cliques do not. In the
following, we discuss the case where one action clique is a subset of another. Given two transitions
1 and 2 , if A(1 )  A(2 ), we say clique A(1 ) is subsumed by clique A(2 ).
309

fiH UANG , C HEN , & Z HANG

In preprocessing, for each transition 1  T , we check if A(1 ) is subsumed by another transition 2 s action clique. If so, we do not encode action clique A(1 ). In the special case when
A(1 ) = A(2 ) for two transitions 1 and 2 , we only need to encode one of them.
Table 2 presents the number of cliques and their average sizes, before and after reducing action
cliques, on some representative problems. The reduction is substantial on most problem domains,
except for Driverlog in which no reduction occurred. Note that the average sizes of cliques are
increased since smaller ones are subsumed and not encoded.
6.2.2 U NARY T RANSITION R EDUCTION
Given a transition  such that |T ()| = 1, we say that the only action a in T () is reducible. Since
a is the only action supporting , they are logically equivalent. For any such action a, we remove
Va,t and replace it by U,t , for t = 1,    , N . An effect of this reduction on a few representative
domains can be seen in Table 3.
6.2.3 U NARY D IFFERENCE S ET R EDUCTION
Besides unary transition variables, an action variable may also be eliminated by two or more transition variables. A frequent pattern is the following: given a transition , for all actions in A(), their
transition sets differ by only one transition.
T
Definition 12 Given a transition   T , let I = aA() T rans(a). If for every a  A(),
|T rans(a) \ I| = 1, we call the action set A() a unary difference set.
Consider a transition 1 with A(1 ) = {a1 , a2 , . . . , an }. If A(1 ) is a unary difference set, the
transition sets must have the following form:
T rans(a1 ) = {1 , 2 , . . . , k , 1 }
T rans(a2 ) = {1 , 2 , . . . , k , 2 }
..
.
T rans(an ) = {1 , 2 , . . . , k , n }
In this case, we eliminate the action variables for a1 ,    , an by introducing the following
clauses. For each i, i = 1,    , n, we replace Vai ,t by U1 ,t  Ui ,t , for t = 1,    , N . In such
a case, the action variables can be eliminated and represented by only two transition variables. The
reason that this reduction can be done is that the n actions are in at least one action clique. The
mutual exclusions between these actions maintain the correctness when all but one of the shared
transitions are reduced.
Table 3 shows the number of reducible actions in several representative problems. In Zenotravel,
all action variables can be eliminated when the two reduction methods are used. In Openstack and
Storage, there is only one type of reduction that can be applied.
310

fiSAS+ P LANNING AS S ATISFIABILITY

Instances
Zeno-15
Pathway-15
Trucks-15
Openstack-10
Storage-10

|O|
9420
1174
3168
1660
846

R1
1800
173
36
0
540

R2
7620
810
300
400
0

%
100.00
83.73
10.61
24.10
63.83

Table 3: Number of reducible actions in representative instances. Columns R1  and R2  give the
number of action variables reduced, by unary transition reduction and unary difference set reduction,
respectively. Column % is the percentage of the actions reduced by both methods combined.

7. Experimental Analysis and Results
We experimentally analyzed the performance of planning using SASE in comparison against many
state-of-the-art planners. We tested all problem instances of STRIPS domains in IPC-3 to IPC-6.
PSR and Philosophers were not included because they have derived facts, which cannot be handled
correctly by any of the planners tested. We used the parser by Fast-Downward (Helmert, 2006,
2008) to generate the SAS+ formalism from STRIPS inputs. The preprocessing and encoding parts
of SASE were implemented in Python2.6. All the instances were based on grounded STRIPS. In
nearly all cases, the problem solving took much longer time than the pre-processing, thus we only
reported the overall running time.
We ran all experiments on a PC workstation with a 2.3 GHz AMD Quad-Core Opteron processor. The running time for each instance was set to 1800 seconds, and the memory was limited to
4GB. For all planners, the running time included parsing, preprocessing and problem solving. The
memory consumption was the peak memory usage reported by the SAT solvers.
7.1 Comparison Results
Precosat (build236) (Biere, 2009), the winner of the application track in the SAT09 competition,
was used as the SAT solver for most planners that we tested and compared. Besides Precosat, we
also used CryptoMinisat (Soos, Nohl, & Castelluccia, 2009), the winner of SAT Race 2010, as the
underlying solver of SatPlan06 and SASE. The nine planners considered are listed as follows.
1. SP06 and SP06-Crypto. They are the original SatPlan06 planner (Kautz et al., 2006), only
with the underlying SAT solver changed to Precosat and CryptoMinisat, respectively.
2. SASE and SASE-Crypto. They are SASE encoding introduced in this paper, with all the
optimization methods turned on. The underlying SAT solvers are Precosat and CryptoMinisat,
respectively.
3. SP06L. It is SatPlan06 (Kautz et al., 2006) with long-distance mutual exclusion (londex) (Chen
et al., 2009). We compared against londex since it also derives transition information from the
SAS+ formalism. We used domain transition graphs from Fast-Downwards parser to derive
londex information.
4. SP06C. It is SatPlan06 with the clique technique (Rintanen, 2006) to represent the mutual
exclusions. The clique information was obtained via Fast-Downward. Note that due to the
311

fiH UANG , C HEN , & Z HANG

Number of Instances Solved

400

450

SatPlan06
nplan
SplitE
SASE
LM-cut

400
Number of Instances Solved

450

350

300

250

350

300

SatPlan06
nplan
SplitE
SASE
LM-cut

250

200

200
600

1200

1800

500

1000

Running Time (seconds)

1500

2000

2500

3000

3500

4000

Memory Usage (Megabytes)

420

420

400

400
Number of Instances Solved

Number of Instances Solved

Figure 5: The results on different planners. We include the default version of every planner. The
figures show the number of problems solved by each planner, with increasing limits on running time
or memory consumption.

380
360
340
320
300
280

SatPlan06
SP06L
SP06C
SASE
SP06-Crypto
SASE-Crypto
600
1200
Running Time (seconds)

380
360
340
320
SatPlan06
SP06L
SP06C
SASE
SP06-Crypto
SASE-Crypto

300
280
1800

500

1000

1500
2000
2500
3000
Memory Usage (Megabytes)

3500

4000

Figure 6: The results on the variants of SatPlan06 and SASE. The data is presented as the number of
problems solved by each planner, with increasing limits on running time or memory consumption.
different grounding strategies by SatPlan06 and Fast-Downward, not all of the mutual exclusions defined in SatPlan06 could be covered by cliques.
5. nplan. The nplan solver (Rintanen et al., 2006) is set to use -step to generate plans with
the same optimality metric as other planners. The executable is the most recent release from
nplans homepage. The build-in SAT solver is changed to Precosat.
6. SplitE. It is the split encoding (Robinson et al., 2009) using Precosat. We have obtained
source code from the authors and recompiled it on our 64bit Linux workstation.
7. LM-cut. This is a sequential optimal planner, using LM-Cut heuristic (Helmert & Domshlak,
2009) and A* search. We used the implementation in Fast-Downward.
We present the results as two sets of planners, as in Figures 5 and 6, respectively. For both sets
of data, we show the number of instances that are solvable in the testing domains, with respect to
the given time limit and memory limit.
Figure 5 compares the results of several different solvers using their original version. Our data
suggests that SASE has clear advantages. LM-cut is the least efficient, although the comparison is
312

fiSAS+ P LANNING AS S ATISFIABILITY

420
Number of Instances Solved

Number of Instances Solved

420

400

380

360
SatPlan06
SP06L
SP06C
nplan
SplitE
SASE

340

320
0

200000

400000

600000

800000

1e+06

400

380

360
SatPlan06
SP06L
SP06C
nplan
SplitE
SASE

340

320

1.2e+06 1.4e+06

Number of Variables

0

5e+06 1e+07 1.5e+07 2e+07 2.5e+07 3e+07 3.5e+07 4e+07 4.5e+07
Number of Clauses

Figure 7: Number of problems solved by each planner, with increasing limits on number of variables and number of clauses.
not very meaningful as it uses an optimization metric different from other planners. For running time
and memory consumption, SASE is clearly superior to all the other planners. Among all planners,
nplan is slightly better than the others on smaller instances, but on larger instances, SatPlan06
becomes more competitive.
In Figure 6, we compare the results of different variants of both SatPlan06 and SASE. Both
SP06L and SP06C extend SatPlan06 with additional techniques. They in general make little improvements over the original SatPlan06.
For SAT based planners, we present in Figure 7 the number of instances that are solvable with
increasing limits on the number of variables and number of clauses. Note that the curves are slightly
affected by the given time and memory limit, thus efficient planners like SASE stops at a smaller
number of clauses. The results show SASE has an advantage in terms of the number of variables
and number of clauses over the other planners.
Table 4 presents the number of instances solved in each planning domain, within the given
time and memory limit. In general, SASE solved more instances than the other planners. Due to
some programming bugs, nplan could not find the correct solutions with the optimal makespan in
domains Openstacks, Rovers and Storage. The SplitE parser could not handle problems in Airport
and Pathways. Therefore, we did not evaluate the corresponding encoding on those benchmarks.
Although LM-Cut overall solved fewer instances, in a few domains it performed better than all
the SAT based planners. These domains seemed to allow less concurrencies between action. In
particular, domains Openstacks and Sokoban only have plans that are strictly sequential, meaning
that there are no actions that can be executed at the same time step. The plans for such instances
often require more time steps, making them more challenging for SAT-based planners.
Both SP06L and SP06C used Fast-Downwards parser to obtain domain transition graph information. Therefore, for SP06C and SP06L, it took too much time to pre-process grounded STRIPS
instances twice (one by Fast-Downward and one by original SP06). In consequence, the efficiency
of londex or clique representation may not compensate for pre-processing time, leading to slightly
worse performance than the original SP06 in a few instances. For example, londex was helpful
in TPP, but not in Trucks and Scanalyzer. The clique representation was very helpful in Airport
domain, with 10 more instances solved, but did not help much in Pegsol and Satellite.
313

fiH UANG , C HEN , & Z HANG

Domain
Airport
Depot
Driverlog
Elevator
Freecell
Openstacks
Parcprinter
Pathways
Pegsol
Pipe-notankage
Pipe-tankage
Rovers
Satellite
Scanalyzer
Sokoban
Storage
TPP
Transport
Trucks
Woodworking
Zenotravel
Total

SP06 SP06L SP06C nplan SplitE SASE
35
38
39
20
0
46
17
16
16
19
17
17
16
16
16
17
17
17
30
30
30
30
30
30
5
4
5
6
5
6
5
5
5
0
5
5
29
29
29
30
29
30
11
11
11
12
0
12
21
21
21
21
22
24
38
37
31
40
37
37
16
16
16
22
10
26
13
13
13
0
18
14
17
17
17
18
16
18
15
14
14
18
13
18
5
5
3
11
5
5
15
15
15
0
16
15
27
30
29
28
25
30
19
16
19
22
18
22
7
6
5
10
8
8
30
30
30
30
30
30
15
15
15
15
15
16
386
384
379
369
336
426

SP06c SASEc SASE0
38
42
39
17
15
14
17
17
16
30
30
30
4
6
6
5
5
5
29
30
30
9
10
12
18
19
22
38
35
38
13
23
16
13
17
16
17
17
15
16
17
17
5
5
5
15
15
15
28
29
29
19
21
20
7
8
7
30
30
30
16
16
16
384
407
398

LM
27
7
13
19
5
20
21
5
27
17
11
7
7
7
24
15
6
12
10
16
12
288

Table 4: Number of instances solved in each domain within 1800 seconds. SP06c , SASEc and
LM are short for SP06-Crypto, SASE-Crypto and LM-Cut. Column SASE0  is the result of SASE
without any reduction and optimization.
Comparing with nplan, in general SASE was better, but nplan performed better than SASE
on those domains with few concurrencies. For example, both Sokoban and Trucks have only one
action at nearly every time step. We believe the reason should be the way nplan encodes all mutual
exclusions as linear encoding (Rintanen et al., 2006), which could be further used to improve SASE.
SplitE in general was slightly worse than SP06. It won over SP06 on 5 domains and SP06 was
superior to SplitE on 6 domains. Overall, SplitE was competitive not to nplan or SASE. Rovers
however was the only domain where SplitE performed better than all others. Although CryptoMinisat performed better than Precosat in SAT Race 2010, it was not as good for planning problems.
For both SASE and SP06, CryptoMinisat solved fewer instances.
7.2 Ablation Study
Figure 8 shows the number of solvable problems from all the problems in IPC-3 to IPC-6, with increasing limits on running time, memory consumption, number of variables and number of clauses.
Precosat was used for all planners. Running time was the total time including preprocessing and
problem solving. Memory usage was based on the status report by Precosat. Under the maximum
CPU time (1800s) and memory limit (4Gb), when both clique representation and reduction techniques were not used, SASE solved 398 instances. By turning on either the clique representation or
the action reduction technique, SASE solved 416 and 405 instances, respectively. When both clique
and action reduction techniques were turned on, SASE solves 426 instances.
314

fi440

420

420

400
380
360
340
clique=on, reduction=on
clique=on, reduction=off
clique=off, reduction=on
clique=off, reduction=off

320
300
10

Number of Instances Solved

Number of Instances Solved

440

600
1200
Running Time (seconds)

400
380
360
340

300
1800

500

440

440

420

420

400
380
360
340
clique=on, reduction=on
clique=on, reduction=off
clique=off, reduction=on
clique=off, reduction=off

320
300
500000

1e+06

1.5e+06

clique=on, reduction=on
clique=on, reduction=off
clique=off, reduction=on
clique=off, reduction=off

320

Number of Instances Solved

Number of Instances Solved

SAS+ P LANNING AS S ATISFIABILITY

2e+06

3500

4000

380
360
340
clique=on, reduction=on
clique=on, reduction=off
clique=off, reduction=on
clique=off, reduction=off

300

Number of Variables

1500 2000 2500 3000
Memory Usage (Megabytes)

400

320

2.5e+06

1000

0

5e+06 1e+071.5e+072e+072.5e+073e+073.5e+074e+074.5e+07
Number of Clauses

Figure 8: The results of SASE with the clique and reduction methods turned on or off.
The reduction method improved upon problem solving time, as well as the clique representation.
The clique representation provided a substantial improvement to memory consumption, followed by
action reduction. For numbers of clauses, the clique technique gave a significant reduction. Finally,
both techniques helped reduce the number of variables.

8. Conclusions and Future Research
In this paper, we developed a novel SAS+ based SAT encoding scheme SASE, and showed that it
improves the efficiency of STRIPS based SAT encodings in terms of both time and memory. When
compared with the state-of-the-art SAT based planners, SASE has clear advantages as shown by our
experimental analysis. We proved the correctness of the SASE encoding by showing that there is an
isomorphism between the solution plans of SASE and the solution plans of SatPlan06. We further
analyzed the search space structure of SASE, and explained why it is more efficient. Below, we
briefly discuss some related work and highlight several directions for future research.
8.1 Other Semantics and Encodings
Many enhancements have been developed for SAT based planning since it was introduced (Kautz
& Selman, 1992). The split action representation (Kautz & Selman, 1992; Ernst et al., 1997) uses a
conjunction of multiple variables to represent an action. The optimality is, however, lost. Robinson
et al. (2009) propose a new way of doing splitting without sacrificing the optimality. The results
315

fiH UANG , C HEN , & Z HANG

show that this method has advantages over SatPlan06 (Kautz et al., 2006). There are many published
works with thorough analysis, or improvements along this line of research on SatPlan family encodings. In particular, the power of mutual exclusion, in the context of planning as SAT, has attracted
interests (Chen et al., 2009; Sideris & Dimopoulos, 2010). A new encoding scheme called SMP
is proposed, which preserves the merits of londex and shows certain advantages over the existing
encoding schemes (Sideris & Dimopoulos, 2010).
The planner family of SatPlan and its variants are step-optimal. The step-optimality semantics,
along with a relaxed parallel semantics, are formalized as -step and -step, respectively (Dimopoulos, Nebel, & Koehler, 1997; Rintanen et al., 2006). -step enforces weaker mutual exclusions than
-step, thus may lead to reduced running time due to fewer calls to the SAT solver. As a trade-off,
it loses the optimality of time steps. The semantics in both SatPlan06 and SASE are -step. The
research on various kinds of semantics are orthogonal to our contribution in SASE, and the idea of
SASE can be migrated to new semantics.
Since SAT based planning has also been applied to sequential planning, the idea of SASE can
also be extended to this field. The first planner of this kind is MEDIC (Ernst et al., 1997), which
extends the idea of splitted action representation. This study shows that for sequential planning,
splitting yields very competitive planners. It has also been proposed to utilize the advantages of
sequential and parallel planning (Bttner & Rintanen, 2005).
8.2 Additional Techniques for Planning as SAT
The tremendous amount of two-literal clauses (such as the mutual exclusion clauses in the case of
planning) is a key challenge to approaches based on satisfiability tests. It has been proposed to mitigate the burden of encoding them by recognizing certain structures (Brafman, 2001). In traditional
SAT planning systems like SatPlan06, the mutual exclusions are encoded in a quadratic manner.
Rintanen proposes a log size technique (Rintanen, 2006), called clique representation, for the mutual exclusion constraints, and later a linear size one (Rintanen et al., 2006). The mutual exclusions
in SASE are represented by the clique representation. The log size of the clique representation
is supposed to be less compact than the linear encoding. Our results, however, have shown that
SASE is in general more compact. This is mainly due to the compactness of SAS+ formalism. It
is certainly an open question whether the linear size encoding technique can be adopted to further
improve SASE.
There are also techniques beyond encoding to boost SAT-based planning. Rintanen introduces
how to incorporate symmetry information into SAT instances (Rintanen, 2003). MaxPlan (Xing,
Chen, & Zhang, 2006) does a planning graph analysis to find an upper bound on the optimal make
span and then does SAT queries using decreasing time steps, until it meets an unsatisfiable SAT
instance. A lemma reusing method is proposed (Nabeshima, Soh, Inoue, & Iwanuma, 2006) to reuse
the learnt clauses across multiple SAT solvings. A multiple-step query strategy is introduced (Ray &
Ginsberg, 2008), which however asks for modified SAT procedures. All these boosting methods are
proposed in the context of STRIPS based planning. It is interesting to incorporate these techniques
into SASE and study if they can further improve the performance.
8.3 More Understanding of Structure in General SAT Instances
SAT is intrinsically hard. The performance of modern SAT solvers improves constantly. It is therefore interesting and important to understand why SAT solvers work well on certain instances, and
316

fiSAS+ P LANNING AS S ATISFIABILITY

furthermore, what makes a SAT instance easy or hard. There is much prior research that tries to obtain such an understanding, including backdoor set (Williams, Gomes, & Selman, 2003) and backbone (Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999; Zhang, Rangan, & Looks,
2003; Zhang, 2004). Backdoor set variables are a set of variables, such that when these variables
are assigned, other variables assignments can be derived in polynomial time. Backbone variables
are those variables that have the same assignment in all valid solutions, which can be further exploited to improve SAT solving efficiency. A recent study in the context of planning reveals that
there is clear correlations between the SAT solving efficiency and goal asymmetry (Hoffmann et al.,
2006).
It will be interesting to see if there are connections between SASEs problem structure and
those theories above. For example, is the improvement of SASE because it can lead to a smaller
backdoor set? Second, we have shown that the efficiency of SASE is a result of transition variables
significance, and there is strong correlation between the speedup from SASE and the transition
index. It is interesting to investigate if similar variable set and predictive index can be automatically
found for general SAT solving.
Finally, given the efficiency of SASE, it is promising to apply it to other SAT-based planning
approaches, such as those for complex planning with preferences (Giunchiglia & Maratea, 2007)
and temporal features (Huang et al., 2009).

9. Acknowledgments
This research was supported by National Science Foundation of the United States under grants
NeTS-1017701, DBI-0743797, IIS-0713109, and a Microsoft Research New Faculty Fellowship.
We thank Joerg Hoffmann, Jussi Rintanen and several anonymous reviewers for the helpful comments. We particularly thank Malte Helmert for making the SAS+ parser available. We also thank
the computing resource and supports from engineering IT group of Washington University in St.
Louis.

Appendix A. Proofs
Lemma 1 Given a STRIPS task  = (F, A, I , G ), a time step N , and its PE SAT instance
PE(, N ) = (V, C), suppose there is a satisfiable solution denoted as , a fact f  F, and t 
[1, N ] such that: 1) (Wdumf ,t ) =, 2) (Wf,t ) = , and 3) a  DEL(f ), (Wa,t ) =, then we
can construct an alternative solution  to PE(, N ) as follows:


 (v) =

(





, v = Wdumf ,t , v  V

(v), v 6= Wdumf ,t , v  V

(5)

Proof We show that  satisfies every clause in C just as  does. Since all variables but Wdumf ,t
keep the same value, we only need to examine those clauses that have Wdumf ,t in them. According
to the definition of PE, Wdumf ,t may exist in three types of clauses:
1. Clauses for add effects. In this case, the clauses are of the form Wf,t+1  (Wdumf ,t 
Wa1 ,t      Wam ,t ), which is equivalent to W f,t+1  Wdumf ,t  Wa1 ,t      Wam ,t . Since
 (Wdumf ,t ) = , such clauses are still true.


317

fiH UANG , C HEN , & Z HANG

2. Clauses for preconditions. In this case, the clauses are of the form Wdumf ,t  Wf,t , which
is equivalent to Wdumf ,t  Wf,t . Since  (Wf,t ) = , these clauses remain true for  .


3. Clauses of mutual exclusion between actions. Without loss of generality, let us denote such a
clause Wdumf ,t  Wa,t . For a given f , the actions in all such clauses are mutex with dumf ,
because f is their delete effect. According to the construction, since  (Wa,t ) = (Wa,t ) =,
all such clauses are true.
The three cases above conclude that all clauses that include Wdumf ,t are satisfied by  . Therefore,  is also a solution to PE.

Lemma 2 Given a STRIPS task  = (F, A, I , G ), a time step N , and its PE SAT instance
PE(, N ) = (V, C), suppose there is a satisfiable solution denoted as , a fact f  F, and t 
[1, N ] such that: 1) (Wf,t ) =, 2) there exists an action a  ADD(f ) such that (Wa,t1 ) = ,
then we can construct an alternative solution  to PE(, N ) as follows:




 (v) =

(

, v = Wf,t , v  V

(6)

(v), v 6= Wf,t , v  V

Proof We show that  makes each clause in C to be true. Since all variables but Wf,t keep
the same value, we only need to look at those clauses that have Wf,t in them. According to the
definition of PE, Wf,t may exist in three types of clauses.
1. Clauses for add effects. In this case, f is an add effect of multiple actions. Let us write this
clauses as Wf,t  (Wa1 ,t1 Wa2 ,t1   Wam ,t1 ), which is Wf,t Wa1 ,t1 Wa2 ,t1 
  Wam ,t1 . Since there exists an action a  ADD(f ) such that (Wa,t1 ) = , the clause
is still true in  .


2. Clauses for preconditions. In this case, f is a precondition of an action b. This clause is
written as Wb,t  Wf,t , which is equivalent to Wb,t  Wf,t . Since  (Wf,t ) = , this clause
is still true.


3. Clauses of fact mutex. Without loss of generality, consider a fact g that is mutex with f .
The corresponding clause will be Wf,t  Wg,t . Since  (Wf,t ) = , this clause is true if
 (Wg,t ) =.




We now suppose  (Wg,t ) = and show that it leads to a contradiction. According to clauses
of class III, there must be a variable Wb,t1 , such that g  add(b) and  (Wb,t1 ) = .
According to the definition of mutex, two facts are mutex only when every pair of the actions
that add them are mutex. Thus, Wa,t1 and Wb,t1 are mutex. Therefore,  (Wa,t1 ) =
and  (Wb,t1 ) = , leading to a contradiction. As a result,  (Wg,t ) =, and consequently
this clause is satisfied.








The three cases above conclude that all clauses that include Wf,t are satisfied by  . Therefore,
is also a solution to PE.

318

fiSAS+ P LANNING AS S ATISFIABILITY

Lemma 3 Given a SAS+ planning task  = (X , O, sI , sG ) and its equivalent STRIPS task  =
(F, A, I , G ), suppose we have actions a, b  O, and their equivalent actions a , b  A (i.e.
a = a (a ) and b = a (b )), a and b are S-mutex iff a and b are P-mutex.
Proof We construct the proof by studying it on both directions. Based on Proposition 1, we only
consider inconsistent effects and interference mutex in P-mutex.
: if a and b are P-mutex in , a and b are S-mutex in .
Since a and b are P-mutex, one either deletes precondition or add-effect of the other. Without
loss of generality, suppose a deletes f (i.e. f  del(a )  pre(b )). Consequently, there must be a
transition 1 = fxh  T (a) such that f 6= h and 2 = fxg  T (b). There are two cases to be
considered.
1) 1 6= 2 . 1 and 2 are mutex transitions by Definition 3, since they both change their value
from f . Therefore, a and b are S-mutex, according to the second condition in Definition 5.
2) 1 = 2 . In this case, a and b are S-mutex by the first condition of Definition 5.
Based on the two cases, we conclude that a and b are S-mutex. A similar argument applies to
the case when one action deletes the others add-effect.
: if a and b are S-mutex in , a and b are P-mutex in .
If two actions a and b are S-mutex in , there are two cases.
1) There exists a transition , which is in both T (a) and T (b). Consequently, a and b deletes
each others precondition and thus they are P-mutex.
2) There exist two distinct transitions 1  T (a), 2  T (b) and a multi-valued variable x  X ,
such that {1 , 2 }  T (x). Let us denote these two transitions as vx1 v2 and vx3 v4 . In such a case,
suppose vx1 v2 and vx3 v4 are allowed to be executed in parallel in a STRIPS plan. It obviously
leads to a contradiction, since v1 , v2 , v3 , v4  Dom(x) are values of the same multi-valued variable,
and by the definition of SAS+ formalism, only one of them can be true at the same time. Therefore,
the preconditions of a and b must be mutex, and hence a and b are P-mutex.


Theorem 1 Given a STRIPS task  and a SAS+ task  that are equivalent, for a time step bound
N , if PE(, N ) is satisfiable, SASE(, N ) is also satisfiable.
Proof Since PE(, N ) is satisfiable, we denote one of its solutions as  . We first present how to
construct an assignment to SASE(, N ) from  . Next, we prove that this constructed assignment
satisfies every clause in SASE(, N ).
Construction. There are two steps for the construction. According to Lemmas 1 and 2, there
are in general some free variables in  . In the first step, we construct an alternative solution to
PE(, N ) by changing all free variables in  to be true according to Lemmas 1 and 2. Let us
denote the resulting solution as  . Then, we construct an assignment for SASE(, N ) from  .
The value of each variable in  is defined as follows.
1. For every a  O (which is also in A)1 , we let Ua,t = Wa,t .


and Wg,t+1 =



1. For simplicity, we use a to denote the same action in A instead of using a (a).

319

in  , we set Ux,f,g,t =



2. For every transition f g  T , if Wf,t =
 .

in

fiH UANG , C HEN , & Z HANG

Satisfiability. We prove that every individual clause in SASE is satisfied by  . There are eight
types of clauses.
1. (Forward progression). According to our construction, we need to show that, for any t 
[1, N  2],
_
x
(Wf,t+1  Wg,t+2 )
(7)
hf
 T , (Wh,t  Wf,t+1 ) 
g,fxg T



If  (Wf,t+1 ) =, then (7) is satisfied by  . If  (Wf,t+1 ) = , we consider an action
set Y = {dumf }  DEL(f ), which is a subset of T rans(fxg ). There are two possibilities.
 For every action a  Y ,  (Wa,t+1 ) =. In such a case, Wdumf ,t+1 and Wf,t+2 are
free variables according to Lemmas 1 and 2, respectively. Therefore, according to the
construction in  , which assigns all free variables to true, variables Wf,t+1 , Wf,t+2
and Wdumf ,t+1 are all . In addition, f f is always in T , meaning Wf,t+2 is included
in the right hand side of (7). Therefore, (7) is satisfied by  .




 There exists an action a  Y , such that  (Wa,t+1 ) = . In such a case, let us
consider an arbitrary fact g  add(a). If  (Wg,t+2 ) = , then (7) is satisfied by  .
Otherwise, according to Lemma 2, Wg,t+2 is a free variable and Wg,t+2 is already set to
true in our construction of  . Therefore,  satisfies (7).


2. (Regression). According to our construction, we need to show that, for any t  [2, N  1],
_
(Wh,t1  Wf,t )
(8)
fxg  T , (Wf,t  Wg,t+1 ) 
x
h,hf
T (x)

Consider clauses of class III (add effect) in PE. These clauses indicate that for each fact
f  F, Wf,t implies a disjunction of Wa,t1 for all actions a such that f  add(a). Thus, for
a given f , the following clauses are included in PE, which are satisfied by  :
_

Wf,t 

Wa,t1 .

(9)

aADD(f )

For a given f , we consider the action set

S

h A(hf ),

Wf,t 

_

denoted as Z. Since ADD(f )  Z,

Wa,t1

(10)

aZ
x
x
For any transition hf
, for each action a  A(hf
), since h  pre(a),  satisfies
Wa,t1  Wh,t1 . Therefore, for each h  pre(a), we have
_
Wa,t1  Wh,t1 .
(11)
x
aA(hf
)

By expanding set Z, we can convert (10) to:
320

fiSAS+ P LANNING AS S ATISFIABILITY

_

Wf,t 

_

(

x
h,hf
T

Wa,t1 ).

(12)

x
aA(hf
)

By combining (11) and (12), we have:
_

Wf,t 

Wh,t1 ,

(13)

(Wh,t1  Wf,t ).

(14)

x
h,hf
T

which implies
_

Wf,t 

x
h,hf
T

From (14), we can see that the clauses of regression in (8) are true.
3. (Initial state). We need to show that for each variable x in X such that sI (x) = f :
_
Uf,g,1

(15)

g,f g T

According to our construction, (15) becomes:
_

(Wf,1  Wg,2 ),

g,f g T

which is equivalent to:
_

Wf,1  (

Wg,2 )

(16)

g,f g T (x)



Since f is in the initial state,  (Wf,1 ) =  (Wf,1 ) = . Therefore the first part of the
conjunction in (16) is true. The rest part of (16) can be seen to be true following a similar
argument as that for the progression case.
4. (Goal). The goal clauses can be shown in a similar way as that for the initial state clauses.
5. V
(Composition of actions). The clauses we
V want to prove are, for any action a, Ua,t 
U
,
or
equivalently,
U

a,t
T rans(a) ,t
T rans(a) U,t .
Suppose T rans(a) = {f1 g1 , f2 g2 , . . . , fm gm }. The clause we need to show becomes:
(Wa,t  Wf1 ,t )  (Wa,t  Wg1 ,t )  (Wa,t  Wf2 ,t )  (Wa,t  Wg2 ,t )  . . .
(Wa,t  Wfm ,t )  (Wa,t  Wgm ,t )

(17)

Let us call these two-literal disjunctions in (17) as sub-clauses. All those Wa,t  Wfi ,t subclauses in (17) are exactly the same as the precondition clause (class IV) in PE. So all
Wa,t  Wfi ,t in (17) are satisfied.
Next, let us consider those Wa,t  Wgi ,t sub-clauses. For any g = gi , i = 1,    , m. There
are four cases where Wa,t and Wg,t are assigned different values:
321

fiH UANG , C HEN , & Z HANG

 (Wa,t =, Wg,t =): Wa,t  Wg,t is satisfied.


, Wg,t =

): Wa,t  Wg,t is satisfied.



 (Wa,t =



 (Wa,t =, Wg,t =

): Wa,t  Wg,t is satisfied.



 (Wa,t = , Wg,t =): According to Lemma 2, Wg,t is a free variable. Therefore, since
 (Wg,t ) = , Wa,t  Wg,t is satisfied by  , and hence satisfied by  .


6. (Transition mutex). Consider any mutex clause between two regular transitions 1 = f g
and 2 = f  g . Let f g  T rans(a) and f  g  T rans(b), we see that a and b
are S-mutex. According to Lemma 3, a and b are also P-mutex in PE. Therefore, we have
Wa,t Wb,t . From our
Vb,t . Then, since we have the composition
V construction, we know Ua,t U
of actions, Ua,t  T rans(a) U,t and Ub,t  T rans(b) U,t . A simple resolution of
these clauses yields U1 ,t  U2 ,t , which equals to the transition mutex clause U1 ,t  U2 ,t .
Therefore, the transition mutex clause is true in  . A similar argument applies when the
transitions are prevailing and mechanical.
W
7. (Action existence). The clauses that we want to prove are U,t  T rans(a) Ua,t , for any
transitions . By our construction, the clauses become
_
Wa,t .
(18)
Wf,t  Wg,t+1 
aA(f g )

S
Let  = f g . First, we know by definition that h A(hg ) = ADD(g). Let us denote
ADD(g) as Z. According to clauses of class III in PE, there are clauses:
_
Wa,t .
(19)
Wg,t 
aZ

We divide Z into multiple action sets according to different fact from {f, h1 , . . . , hm }, denoted as Zf , Zh1 ,    , Zhm . In fact, for each h  {f, h1 , . . . , hm }, Zh is equivalent to
A(hg ). Consider any hi , i = 1,    , m. According to the clauses of class IV, for every
action a  PRE(h), there is a clause Wa,t  Whi ,t , which is
Wa,t  Whi ,t .

(20)

Next, we perform resolutions by using (19) and all the clauses in (20), for all such hi and
corresponding actions. We consequently have:
Wg,t  (Wh1 ,t  Wh2 ,t      Whm ,t ) 

_

Wa,t .

(21)

aZf

Further, note that all h1 , h2 , . . . , fm are mutex to f , a resolution using all the mutex clauses
in PEresults in:
(21), Wh1 ,t  Wf,t , Wh2 ,t  Wf,t , . . . , Whm ,t  Wf,t
W
Wg,t  (Wf,t  Wf,t      Wf,t )  aZf Wa,t

Since Zf = A(f g ), the outcome of (22) leads to (18).
322

(22)

fiSAS+ P LANNING AS S ATISFIABILITY

8. (Action mutex). Action mutex clauses are satisfied by  according to Lemma 3.
Combining all the cases concludes that the constructed solution  satisfies all the clauses in
SASE which means SASE is satisfiable. Since for all action a,  (Wa,t ) =  (Wa,t ) =  (Ua,t ),
 and  represent the same solution plan.


Theorem 2 Given a STRIPS task  and a SAS+ task  that are equivalent, for a time step bound
N , if SASE(, N ) is satisfiable, PE(, N ) is also satisfiable.
Proof Assuming  is a satisfiable solution to SASE(, N ), we first construct an assignment 
from  , and show that  satisfies every clause in PE(, N ).
Construction. We construct a solution  as follows:
1. For every a  A (which is also in O), we let Wa,t = Ua,t ;
2. For every dummy action variable dumf , we let Wdumf ,t = Uf f ,t ;
in  , we set Wf,t = Wg,t+1 =





3. For every transition f g  T , if Ux,f,g,t =

in  ;

4. For each fact f , if Uh,f,t = for every transition hf  T (which implies that case 3 will
not assign a value to f ), we set Wf,t to be .
Satisfiability. Next, we prove that every clause in PE is satisfied by  . The clauses for the initial
and goal states are obviously satisfied. Now we consider the add-effect clauses. The clauses that we
want to prove are, for every fact f :
Wf,t 

_

Wa,t1

(23)

aADD(f )

For a given fact f , we consider all the facts h 6= f , such that hf  T . For all such h, there
are two further cases:


x
 There exists a fact h such that hf
 T and Ux,h,f,t1 = in  . In the satisfiable SASE
instance, the action existence clauses in class F specify that the truth of a non-prevailing
transition  indicates a disjunction of all actions in A(). Since Ux,h,f,t1 = , it follows
x
that in the SASE instance there is an action a  A(hf
) such that Ua,t1 = . Then, by
our construction of  , we see that both Wh,t1 and Wf,t are true. Since Wf,t and Wa,t1 ,
a  ADD(f ), are all true, (23) is satisfied by  .




x
 If for every fact h that hf
 T , Ux,h,f,t1 = in  , then, according to our construction,
Wf,t = in  . Thus,  satisfies (23).

The two cases above conclude that  satisfies the add effect clauses. Next, we show that 
satisfies the precondition clauses, Wa,t  Wf,t (i.e. Wa,t  Wf,t ), for all actions a  A and facts
f  pre(a). In SASE, we have clauses of class F, which are Ua,t  U,t , for all actions a  O
x , we have U
and   T rans(a). Let the transition be f,g
a,t  (Uf,t1  Ug,t1 ), which implies
Ua,t  Uf,t1 . By our construction, we know Wa,t  Wf,t1 is true.
Finally, the mutex clauses are satisfied by  according to Lemma 3. Combining all the cases
concludes that the constructed solution  satisfies all the clauses in PE which means PE is satisfiable.

323

fiH UANG , C HEN , & Z HANG

Appendix B. Branching Frequency in All Domains

0.006

0.06

Transition Vars
Action Vars

0.03

Transition Vars
Action Vars

0.005

0.05

0.025

0.004

0.04

0.02

0.003

0.03

0.015

0.002

0.02

0.01

0.001

0.01

0.005

0

0
0

40000

80000

120000

160000

(a) Airport-44, N = 68, Satisfiable
0.2
0.18

Transition Vars
Action Vars

0
0

50000 100000150000200000250000300000350000

0

10000 20000 30000 40000 50000 60000 70000

(b) Depot-8, N = 14, Satisfiable

(c) Driverlog-16, N = 14, Unsat

0.14

0.045

Transition Vars
Action Vars

0.12

0.16

Transition Vars
Action Vars

0.04
0.035

0.1

0.14
0.12
Transition Vars
Action Vars

0.1

0.03

0.08

0.08

0.06

0.06

0.04

0.025
0.02
0.015
0.01

0.04
0.02

0.02
0

0.005

0
5000

10000

15000

(d) Elevator-30, N = 10, Satisfiable
0.07

Transition Vars
Action Vars

0.06

0
0

5000 10000 15000 20000 25000 30000 35000

(e) Freecell-4, N = 12, Unsat
0.2

Transition Vars
Action Vars

0.18
0.16

0

0.04

0.12
0.1

0.02

0.03

0.08

0.015

0.02

0.06

0.01

0
6000

8000 10000 12000 14000

(g) Pathways-15, N = 18, Satisfiable

3000

0.005

0.02
4000

2500

0.025

0.04

2000

2000

0.03

0.14

0

1500

Transition Vars
Action Vars

0.035

0.04

0

1000

(f) Parcprinter-29, N = 23, Satisfiable

0.05

0.01

500

0
200000

600000

1e+06

(h) Pegsol-18, N = 20, Unsat

0

20000 40000 60000 80000 100000120000140000

(i) Pipe-notankage-29, N = 14, Satisfiable

Figure 9: Comparison of variable branching frequency (with k = 1000) for transition and action variables
in solving certain SAT instances in twelve benchmark domains encoded by SASE. Each figure corresponds
to an individual run of MiniSAT. The x axis corresponds to all the decision epochs during SAT solving. The
y axis denotes the branching frequency (defined in the text) in an epoch of k = 1000.

324

fiSAS+ P LANNING AS S ATISFIABILITY

0.09

0.18

Transition Vars
Action Vars

0.08
0.07

0.14

0.06

0.12

0.05

0.1

0.04

0.08

0.03

0.06

0.02

0.04

0.01

0.02

0

0.025

Transition Vars
Action Vars

0.16

0.015
0.01
0.005

0
0

10000 20000 30000 40000 50000 60000 70000

(a) Pipe-tankage-21, N = 13, Unsat
0.14

0
0

2000 4000 6000 8000 10000 12000 14000 16000

(b) Rovers-15, N = 12, Satisfiable
0.25

Transition Vars
Action Vars

0.12

Transition Vars
Action Vars

0.02

0

0.02

Transition Vars
Action Vars

Transition Vars
Action Vars

0.018

0.2

50000 100000 150000 200000 250000 300000

(c) Satellite-12, N = 13, Unsat

0.016

0.1

0.014

0.08

0.15

0.012

0.06

0.1

0.008

0.01
0.006

0.04
0.05

0.004

0.02

0.002

0

0
100000

200000

300000

0

400000

(d) Scanalyzer-27, N = 12, Satisfiable

300000

0.16
0.14

0.05
Transition Vars
Action Vars

0.06
0.04

0.12

0.04

0.1

0.03

0.08
0.06

0.02
0.02

0.04

0.01

0

0.02

0
50000

100000

150000

(g) Transport-26, N = 12, Satisfiable

Transition Vars
Action Vars

0.18

0.06
0.08

5000 10000 15000 20000 25000 30000 35000 40000

(f) TPP-26, N = 10, Unsat
0.2

Transition Vars
Action Vars

0.07

0.1

0

(e) Sokoban-6, N = 33, Unsat
0.08

0.12

600000

0
100000

200000

300000

400000

(h) Trucks-7, N = 17, Unsat

0

500 1000 1500 2000 2500 3000 3500 4000 4500

(i) Woodworking-20, N = 4, Sat

Figure 10: Comparison of variable branching frequency (with k = 1000) for transition and action variables
in solving certain SAT instances in nine other benchmark domains encoded by SASE.

References
Bckstrm, C., & Nebel, B. (1996). Complexity results for SAS+ planning. Computational Intelligence, 11, 625655.
Biere, A. (2009). Pr{e,i}coSAT@SC09. In SAT09 Competition.
Blum, A., & Furst, M. (1997). Fast Planning Through Planning Graph Analysis. Artificial Intelligence, 90, 16361642.
Brafman, R. I. (2001). A simplifier for propositional formulas with many binary clauses. In Proceedings of International Joint Conference on Artificial Intelligence.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24, 123140.
325

fiH UANG , C HEN , & Z HANG

Bttner, M., & Rintanen, J. (2005). Satisfiability Planning with Constraints on the Number of
Actions. In Proceedings of International Conference on Automated Planning and Scheduling.
Castellini, C., Giunchiglia, E., & Tacchella, A. (2003). SAT-based planning in complex domains:Concurrency, constraints and nondeterminism. Artificial Intelligence, 147, 85117.
Chen, Y., Huang, R., Xing, Z., & Zhang, W. (2009). Long-distance mutual exclusion for planning.
Artificial Intelligence, 173, 197412.
Chen, Y., Huang, R., & Zhang, W. (2008). Fast Planning by Search in Domain Transition Graphs.
In Proceedings of AAAI Conference on Artificial Intelligence.
Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding planning problems in nonmonotonic
logic programs. In In Proceeding of the Fourth European Conference on Planning, pp. 169
181. Springer-Verlag.
Do, B., & Kambhampati, S. (2000). Solving Planning Graph by Compiling it into a CSP. In
Proceedings of International Conference on Automated Planning and Scheduling.
Ernst, M., Millstein, T., & Weld, D. (1997). Automatic SAT-compilation of planning problems. In
Proceedings of International Joint Conference on Artificial Intelligence.
Giunchiglia, E., & Maratea, M. (2007). Planning as satisfiability with preferences. In Proceedings
of AAAI Conference on Artificial Intelligence.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence Research, 26, 191246.
Helmert, M. (2008). Concise finite-domain representations for PDDL planning tasks. Artificial
Intelligence, 173, 503535.
Helmert, M., & Domshlak, C. (2009). Landmarks, Critical paths and Abstractions: Whats the
difference anyway?. In Proceedings of International Conference on Automated Planning and
Scheduling.
Helmert, M., Haslum, P., & Hoffmann, J. (2008). Explicit-State Abstraction: A New Method for
Generating Heuristic Functions. In Proceedings of AAAI Conference on Artificial Intelligence.
Hoffmann, J., Gomes, C., & Selman, B. (2006). Structure and Problem Hardness : Goal Asymmetry
and DPLL Proofs in SAT-based Planning. In Proceedings of International Conference on
Automated Planning and Scheduling.
Hoffmann, J., Kautz, H., Gomes, C., & Selman, B. (2007). SAT encodings of state-space reachability problems in numeric domains. In Proceedings of International Joint Conference on
Artificial Intelligence.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253302.
Huang, R., Chen, Y., & Zhang, W. (2009). An Optimal Temporally Expressive Planner: Initial
Results and Application to P2P Network Optimization. In Proceedings of International Conference on Automated Planning and Scheduling.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Proceedings of European Conference
on Artificial Intelligence.
326

fiSAS+ P LANNING AS S ATISFIABILITY

Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and stochastic
search. In Proceedings of AAAI Conference on Artificial Intelligence.
Kautz, H., & Selman, B. (1999). Unifying sat-based and graph-based planning. In Proceedings of
International Joint Conference on Artificial Intelligence.
Kautz, H., Selman, B., & Hoffmann, J. (2006). SatPlan: Planning as Satisfiability. In 5th International Planning Competition, International Conference on Automated Planning and Scheduling.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining
computational complexity from characteristic phase transitions. Nature, 400(8), 133137.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering an
Efficient SAT Solver. In 39th Design Automation Conference.
Myers, J. L., & Well, A. D. (2003). Research Design and Statistical Analysis (2nd edition). Routledge.
Nabeshima, H., Soh, T., Inoue, K., & Iwanuma, K. (2006). Lemma reusing for SAT based planning
and scheduling. In Proceedings of International Conference on Automated Planning and
Scheduling.
Ray, K., & Ginsberg, M. L. (2008). The complexity of optimal planning and a more efficient method
for finding solutions. In Proceedings of International Conference on Automated Planning and
Scheduling.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks Revisited. In Proceedings of AAAI
Conference on Artificial Intelligence.
Rintanen, J. (2003). Symmetry Reduction for SAT Representations of Transition System. In Proceedings of International Conference on Automated Planning and Scheduling.
Rintanen, J. (2006). Biclique-based representations of binary constraints for making SAT planning
applicable to larger problems. In Proceedings of European Conference on Artificial Intelligence.
Rintanen, J., Heljanko, K., & Niemel, I. (2006). Planning as Satisfiability: parallel plans and
algorithms for plan search. Artificial Intelligence, 12-13, 10311080.
Robinson, N., Gretton, C., Pham, D., & Sattar, A. (2009). SAT-Based Parallel Planning Using a
Split Representation of Actions. In Proceedings of International Conference on Automated
Planning and Scheduling.
Sideris, A., & Dimopoulos, Y. (2010). Constraint propagation in propositional planning. In Proceedings of International Conference on Automated Planning and Scheduling.
Soos, M., Nohl, K., & Castelluccia, C. (2009). Extending sat solvers to cryptographic problems. In
International Conference on Theory and Applications of Satisfiability Testing.
The 6th Intl Planning Competition (2008). http://ipc.informatik.uni-freiburg.de/homepage/..
The 7th Intl Planning Competition (2011). http://ipc.icaps-conference.org/..
Williams, R., Gomes, C., & Selman, B. (2003). Backdoors to typical case complexity. In Proceedings of International Joint Conference on Artificial Intelligence.
327

fiH UANG , C HEN , & Z HANG

Xing, Z., Chen, Y., & Zhang, W. (2006). MaxPlan: Optimal Planning by Decomposed Satisfiability and Backward Reduction. In 5th International Planning Competition, International
Conference on Automated Planning and Scheduling.
Zhang, W. (2004). Configuration landscape analysis and backbone guided local search: Part I:
Satisfiability and maximum satisfiability. Artificial Intelligence, 158, 126.
Zhang, W., Rangan, A., & Looks, M. (2003). Backbone Guided Local Search for Maximum Satisfiability. In Proceedings of International Joint Conference on Artificial Intelligence.

328

fiJournal of Artificial Intelligence Research 43 (2012) 135-171

Submitted 08/11; published 02/12

The CQC Algorithm: Cycling in Graphs to Semantically
Enrich and Enhance a Bilingual Dictionary
Tiziano Flati
Roberto Navigli

flati@di.uniroma1.it
navigli@di.uniroma1.it

Dipartimento di Informatica, Sapienza University of Rome
00198, Rome, Italy.

Abstract
Bilingual machine-readable dictionaries are knowledge resources useful in many automatic tasks. However, compared to monolingual computational lexicons like WordNet,
bilingual dictionaries typically provide a lower amount of structured information such as
lexical and semantic relations, and often do not cover the entire range of possible translations for a word of interest. In this paper we present Cycles and Quasi-Cycles (CQC),
a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machine-readable dictionary. The dictionary is represented as
a graph, and cyclic patterns are sought in this graph to assign an appropriate sense tag
to each translation in a lexical entry. Further, we use the algorithms output to improve
the quality of the dictionary itself, by suggesting accurate solutions to structural problems
such as misalignments, partial alignments and missing entries. Finally, we successfully
apply CQC to the task of synonym extraction.

1. Introduction
Lexical knowledge resources, such as thesauri, machine-readable dictionaries, computational
lexicons and encyclopedias, have been enjoying increasing popularity over the last few
years. Among such resources we cite Rogets Thesaurus (Roget, 1911), the Macquarie Thesaurus (Bernard, 1986), the Longman Dictionary of Contemporary English (Proctor, 1978,
LDOCE), WordNet (Fellbaum, 1998) and Wikipedia. These knowledge resources have been
utilized in many applications, including Word Sense Disambiguation (Yarowsky, 1992; Nastase & Szpakowicz, 2001; Martnez, de Lacalle, & Agirre, 2008, cf. Navigli, 2009b, 2012 for a
survey), semantic interpretation of text (Gabrilovich & Markovitch, 2009), Semantic Information Retrieval (Krovetz & Croft, 1992; Mandala, Tokunaga, & Tanaka, 1998; Sanderson,
2000), Question Answering (Lita, Hunt, & Nyberg, 2004; Moldovan & Novischi, 2002), Information Extraction (Jacquemin, Brun, & Roux, 2002), knowledge acquisition (Navigli &
Ponzetto, 2010), text summarization (Silber & McCoy, 2003; Nastase, 2008), classification
(Rosso, Molina, Pla, Jimnez, & Vidal, 2004; Wang & Domeniconi, 2008; Navigli, Faralli,
Soroa, de Lacalle, & Agirre, 2011) and even simplification (Woodsend & Lapata, 2011).
Most of these applications exploit the structure provided by the adopted lexical resources
in a number of different ways. For instance, lexical and semantic relations encoded in
computational lexicons such as WordNet have been shown to be very useful in graph-based
Word Sense Disambiguation (Mihalcea, 2005; Agirre & Soroa, 2009; Navigli & Lapata, 2010;
Ponzetto & Navigli, 2010) and semantic similarity (Pedersen, Banerjee, & Patwardhan,
2005; Agirre, Alfonseca, Hall, Kravalova, Pasca, & Soroa, 2009). Interestingly, it has been
c
2012
AI Access Foundation. All rights reserved.

fiFlati & Navigli

reported that the higher the amount of structured knowledge, the higher the disambiguation
performance (Navigli & Lapata, 2010; Cuadros & Rigau, 2006). Unfortunately, not all the
semantics are made explicit within lexical resources. Even WordNet (Fellbaum, 1998), the
most widely-used computational lexicon of English, provides explanatory information in the
unstructured form of textual definitions, i.e., strings of text which explain the meaning of
concepts using possibly ambiguous words (e.g., motor vehicle with four wheels is provided
as a definition of the most common sense of car ). Still worse, while computational lexicons
like WordNet contain semantically explicit information such as is-a and part-of relations,
machine-readable dictionaries (MRDs) are often just electronic transcriptions of their paper
counterparts. Thus, for each entry they mostly provide implicit information in the form of
free text, which cannot be immediately utilized in Natural Language Processing applications.
Over recent years various approaches to the disambiguation of monolingual dictionary
definitions have been investigated (Harabagiu, Miller, & Moldovan, 1999; Litkowski, 2004;
Castillo, Real, Asterias, & Rigau, 2004; Navigli & Velardi, 2005; Navigli, 2009a), and results
have shown that they can, indeed, boost the performance of difficult tasks such as Word
Sense Disambiguation (Cuadros & Rigau, 2008; Agirre & Soroa, 2009). However, little
attention has been paid to the disambiguation of bilingual dictionaries, which would be
capable of improving popular applications such as Machine Translation.
In this article we present a graph-based algorithm which aims at disambiguating translations in bilingual machine-readable dictionaries. Our method takes as input a bilingual
MRD and transforms it into a graph whose nodes are word senses1 (e.g., car 1n ) and whose
edges (s, s0 ) mainly represent the potential relations between the source sense s of a word
w (e.g., car 1n ) and the various senses s0 of its translations (e.g., macchina 3n ). Next, we introduce a novel notion of cyclic and quasi-cyclic graph paths that we use to select the most
appropriate sense for a translation w0 of a source word w.
The contributions of this paper are threefold: first, we present a novel graph-based algorithm for the disambiguation of bilingual dictionaries; second, we exploit the disambiguation
results in a way which should help lexicographers make considerable improvements to the
dictionary and address issues or mistakes of various kinds; third, we use our algorithm to
automatically identify synonyms aligned across languages.
The paper is organized as follows: in Section 2 we introduce the reader to the main
ideas behind our algorithm, also with the help of a walk-through example. In Section 3
we provide preliminary definitions needed to introduce our disambiguation algorithm. In
Section 4 we present the Cycles and Quasi-Cycles (CQC) algorithm for the disambiguation
of bilingual dictionaries. In Section 5 we assess its disambiguation performance on dictionary
translations. In Section 6, we show how to enhance the dictionary semi-automatically by
means of CQC, and provide experimental evidence in Section 7. In Section 8 we describe an
application to monolingual and bilingual synonym extraction and then in Section 9 describe
experiments. Related work is presented in Section 10. We give our conclusions in Section
11.
1. We denote with wpi the i-th sense of a word w with part of speech p in a reference sense inventory (we
use n for nouns, v for verbs, a for adjectives and r for adverbs), where senses can simply be denoted by
integers (like 1, 2, 3, etc.), but also by letters and numbers (such as A.1, B.4, D.3) indicating different
levels of granularity (homonymy, polysemy, etc.).

136

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

2. A Brief Overview
In this section we provide a brief overview of our approach to the disambiguation of bilingual
dictionary entries.
2.1 Goal
The general form of a bilingual dictionary entry is:
wpi  v1 , v2 , . . . , vk
where:
 wpi is the i-th sense of the word w with part of speech p in the source language (e.g.,
play 2v is the second sense of the verb play);
 each vj is a translation in the target language for sense wpi (e.g., suonare v is a translation for play 2v ). Note that each vj is implicitly assumed to have the same part of
speech p as wp . Importantly, no sense is explicitly associated with vj .
Our objective is to associate each target word vj with one of its senses so that the
concepts expressed by wp and vj match. We aim to do this in a systematic and automatic
way. First of all, starting from a bilingual dictionary (see Section 3.1), we build a noisy
graph associated with the dictionary (see Section 3.2), whose nodes are word senses and
edges are (mainly) translation relations between word senses. These translation relations
are obtained by linking a source word sense (wpi above) to all the senses of a target word
vj . Next, we define a novel notion of graph patterns, which we have called Cycles and
Quasi-Cycles (CQC), that we use as a support for predicting the most suitable sense for
each translation vj of a source word sense wpi (see Section 3.3).
2.2 A Walk-Through Example
We now present a walk-through example to give further insights into the main goal of the
present work. Consider the following Italian-English dictionary entries:
giocare A.1
v
recitare A.2
v
suonare A.1
v
suonare B.4
v
interpretare 4v







play, toy
act, play
sound, ring, play
ring, echo
play, act





giocare
suonare, riprodurre
interpretare, recitare

and the following English-Italian entries:
play 1v
play 2v
play 3v

137

fiFlati & Navigli

Our aim is to sense tag the target terms on the right-hand side, i.e., we would like to
obtain the following output:
giocare A.1
v
recitare A.2
v
suonare A.1
v
suonare B.4
v
interpretare 4v







play 1v , toy 1v
3
act A.1
v , play v
1.A.1
sound v , ring 2.A.2
, play 2v
v
2.A.4
A.1
ring v , echo v
play 3v , act A.1
v

play 1v
play 2v
play 3v





giocare A.1
v
1
suonare A.2
v , riprodurre v
3
A.2
interpretare v , recitare v

where the numbers beside each right-hand translation correspond to the most suitable senses
in the dictionary for that translation (e.g., the first sense of play v corresponds to the sense of
playing a game). For instance, in order to disambiguate the first entry above (i.e., giocare vA.1
 play, toy), we have to determine the best sense of the English verb play given the Italian
verb sense giocare A.1
v . We humans know that since the source sense is about playing a
game, the right sense for playv is the first one. In fact, among the 3 senses of the verb
play v shown above, we can see that the first sense is the only one which translates back into
giocare. In other words, the first sense of play v is the only one which is contained in a path
starting from, and ending in, giocare A.1
v , namely:
giocare A.1
 play 1v  giocare A.1
v
v
while there are no similar paths involving the other senses of playv . Our hunch is that
by exploiting cyclic paths we are able to predict the most suitable sense of an ambiguous
translation. We provide a scoring function which weights paths according to their length
(with shorter paths providing better clues, and thus receiving higher weights) and, at the
same time, favours senses which participate in most paths. We will also study the effect
of edge reversal as a further support for disambiguating translations. Our hunch here is
that by allowing the reversal of subsequent edges we enable previously-missed meaningful
paths, which we call quasi-cycles (e.g., recitare A.2
 play 3v  interpretare 3v  act A.1

v
v
A.2
recitare v ). We anticipate that including quasi-cycles significantly boosts the overall disambiguation performance.

3. Preliminaries
We now provide some fundamental definitions which will be used throughout the rest of the
paper.
3.1 Bilingual Dictionary
We define a bilingual machine-readable dictionary (BiMRD) as a quadruple D =
(L, Senses, T , M), where L is the bilingual lexicon (i.e., L includes all the lexical items for
both languages), Senses is a mapping such that, given a lexical item w  L, returns the set
138

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

language [lNgwidZ] n.
1 lingua; linguaggio: foreign languages, lingue straniere; technical l., la lingua della
tecnica; the l. of poetry, il linguaggio poetico; dead languages, le lingue morte  l.
laboratory, laboratorio linguistico 2 bad l., linguaggio scorretto (o sboccato) 2 sign l.,
lingua dei segni (usata dai sordomuti) 2 strong l., linguaggio violento (o volgare) 2 to
use bad l., usare un linguaggio volgare, da trivio.
2 favella: Animals do not possess l., gli animali non possiedono la favella.
Figure 1: Entry example of the Ragazzini-Biagi dictionary.
of senses for w in D, T is a translation function which, given a word sense s  Senses(w),
provides a set of (possibly ambiguous) translations for s. Typically, T (s)  L, that is, the
translations are in the lexicon. However, it might well be that some translations in T (s)
are not in the lexicon. Finally, M is a function which, given a word sense s  Senses(w),
provides the set of all words representing meta-information for sense s (e.g., M(phoneme 1n )
= {linguistics}).
For instance, consider the Ragazzini-Biagi English-Italian BiMRD (Ragazzini & Biagi,
2006). The dictionary provides Italian translations for each English word sense, and vice
versa. For a given source lemma (e.g., language n in English), the dictionary lists its translations in the target language for each sense expressed by the lemma. Figure 1 shows the
dictionary entry of language n . The dictionary provides:
 a lexicon for the two languages, i.e., the set L of lemmas for which dictionary entries
exist (such as languagen in Figure 1, but also lingua n , linguaggio n , etc.);
 the set of senses of a given lemma, e.g., Senses(language n ) = {language 1n , language 2n }
(the communication sense vs. the speaking ability), Senses(lingua n ) = {lingua 1n , lingua 2n } (the muscular organ and a set of words used for communication, respectively);
Senses(linguaggio n ) = {linguaggio 1n , linguaggio 2n , linguaggio 3n } (the faculty of speaking, the means of communication and machine language, respectively);
 the translations for a given sense, e.g., T (language 1n ) = {lingua n , linguaggio n };
 optionally, some meta-information about a given sense, such as M(phoneme 1n ) = {linguistics}.
The dictionary also provides usage examples and compound translations (see Figure 1),
lexical variants (e.g., acknowledgement vs. acknowledgment) and references to other entries
(e.g., from motorcar to car ).
3.2 Noisy Graph
Given a BiMRD D, we define a noisy dictionary graph G = (V, E) as a directed graph
where:
1. V is the set of senses in the dictionary D (i.e., V =
139

S

wL Senses(w));

fiFlati & Navigli

lingua2n
originaleB.3
n

linguaggio1n

parlatoB.1
n

linguaggio3n

language1n

linguaggio2n
tongue1n
lingua1n

speech1n
eloquio1n

favella2n
parlareD.2
n
idioma1n

Figure 2: An excerpt of the Ragazzini-Biagi noisy graph including language 1n and its neighbours.

2. For each word w  L and a sense s  Senses(w), an edge (s, s0 ) is in E if and only if s0
is a sense of a translation of s in the dictionary (i.e., s0  Senses(w0 ) and w0  T (s)),
or s0 is a sense of a meta-word m in the definition of s (i.e., if s0  Senses(m) for some
m  M(s)).
According to the above definition, given an ambiguous word w0 in the definition of s, we
add an edge from s to each sense of w0 in the dictionary. In other words, the noisy graph
G associated with dictionary D encodes all the potential meanings for word translations in
terms of edge connections. In Figure 2 we show an excerpt of the noisy graph associated
with the Ragazzini-Biagi dictionary. In this sub-graph three kinds of nodes can be found:
 the source sense (rectangular box), namely language 1n .
 the senses of its translations (thick ellipse-shaped nodes), e.g., the three senses of
linguaggio n and the two senses of lingua n .
 other senses (ellipse-shaped nodes), which are either translations or meta-information
for other senses (e.g., speech 1n is a translation sense of eloquio 1n ).

3.3 Graph Cycles and Quasi-Cycles
We now recall the definition of graph cycle. A cycle for a graph G is a sequence of edges
of G that form a path v1  v2      vn (vi  V i  {1, . . . , n}) such that the first node
140

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

s

s

s

...
(a) A cycle

s

...
(b) A
quasi-cycle
with 1 (terminal)
reversed edge

s

(c) A quasi-cycle with
1 (non terminal) reversed edge

s

...
(d) A
quasi-cycle
with 2 reversed edges

...

...
(e) A quasi-cycle with
k reversed edges

...
(f) An illegal quasicycle

Figure 3: Legal and illegal cycles and quasi-cycles.
of the path corresponds to the last, i.e., v1 = vn (Cormen, Leiserson, & Rivest, 1990, p. 88).
The length of a cycle is given by the number of its edges. For example, a cycle of length 3
in Figure 2 is given by the path:
language 1n  linguaggio 2n  lingua 2n  language 1n .
We further provide the definition of quasi-cycle as a sequence of edges in which the
reversal of the orientation of one or more consecutive edges creates a cycle (Bohman &
Thoma, 2000). For instance, a quasi-cycle of length 4 in Figure 2 is given by the path:
language 1n  linguaggio 1n  speech 1n  eloquio 1n  language 1n .
It can be seen that the reversal of the edge (eloquio1n , speech1n ) creates a cycle. Since
the direction of this edge is opposite to that of the cycle, we call it a reversed edge. Finally,
we say that a path is (quasi-)cyclic if it forms a (quasi-)cycle. Note that we do not consider
paths going across senses of the same word; so language 1n  lingua1n  tongue 1n  lingua2n
 language 1n is not considered a legal quasi-cycle.
In order to provide a graphical representation of (quasi-)cycles, in Figure 3 we show
different kinds of (quasi-)cycles starting from a given node s, namely: a cycle (a), a quasicycle with 1 terminal (b) and non-terminal (c) reversed edge (a reversed edge is terminal
if it is incident from s), with more reversed edges ((d) and (e)), and an illegal quasi-cycle
whose reversed edges are not consecutive (f).

4. The CQC Algorithm
We are now ready to introduce the Cycles & Quasi-Cycles (CQC) algorithm, whose pseudocode is given in Table 1. The algorithm takes as input a BiMRD D = (L, Senses, T , M),
141

fiFlati & Navigli

1
2
3
4
5
6
7
8
9
10
11

CQC(BiMRD D = (L, Senses, T , M), sense s of w  L)
for each word w0  T (s)
for each sense s0  Senses(w0 )
paths(s0 ) :=SDFS(s0 , s)
all_paths := s0 Senses(w0 ) paths(s0 )
for each sense s0  Senses(w0 )
score(s0 ) := 0
for each path p  paths(s0 )
l := length(p)
1
v := (l)  N umP aths(all_paths,l)
score(s0 ) := score(s0 ) + v
(w0 ) = argmax score(s0 )

12

return 

s0 Senses(w0 )

Table 1: The Cycles & Quasi-Cycles (CQC) algorithm in pseudocode.
and a sense s of a word w in its lexicon (i.e., w  L and s  Senses(w)). The algorithm
aims at disambiguating each of the words ambiguous translations w0  T (s), i.e., to assign
it the right sense among those listed in Senses(w0 ).
The algorithm outputs a mapping  between each ambiguous word w0  T (s) and the
sense s0 of w0 chosen as a result of the disambiguation procedure that we illustrate hereafter.
First, for each sense s0 of our target translation w0  T (s), the algorithm performs a
search of the noisy graph associated with D and collects the following kinds of paths:
i) Cycles:
s  s0  s1      sn2  sn1 = s
ii) Quasi-cycles:

s  s0  s1  ...  sj  ...  sk  ...  sn2  sn1 = s

1  j  n2, j < k  n1
(1)

where s is our source sense, s0 is our candidate sense for w0  T (s), si is a sense listed in
D (i  {1, . . . , n  2}), sn1 = s, and n is the length of the path. Note that both kinds of
path start and end with the same node s, and that the algorithm searches for quasi-cycles
whose reversed edges connecting sk to sj are consecutive. To avoid redundancy we require
(quasi-)cycles to be simple, that is, no node is repeated in the path except the start/end
node (i.e., si 6= s, si 6= s0 , si 6= si0 i, i0 s. t. i 6= i0 ).
During the first step of the algorithm (see Table 1, lines 2-3), (quasi-)cyclic paths are
sought for each sense of w0 . This step is performed with a depth-first search (DFS, cf.
Cormen et al., 1990, pp. 477479) up to a depth .2 The DFS  whose pseudocode is
2. We note that a depth-first search is equivalent to a breadth-first search (BFS) for the purpose of collecting
paths.

142

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

1
2
3
4

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

DFS(sense s0 , sense s)
paths := 
visited := 
Rec-DFS(s0 , s, s  s0 )
return paths
Rec-DFS(sense s0 , sense s, path p)
if s0  visited or length(p) >  then return
if s0 = s then
paths := paths  {p}
return
push(visited, s0 )
// cycles
for each edge s0  s00
p0 := p  s00
Rec-DFS(s00 , s, p0 )
// quasi-cycles
for each edge s0  s00
p0 := p  s00
if reversedEdgesNotConsecutive(p0 ) then continue
Rec-DFS(s00 , s, p0 )
pop(visited)

Table 2: The depth-first search pseudocode algorithm for cycle and quasi-cycle collection.
shown in Table 2  starts from a sense s0  Senses(w0 ), and recursively explores the graph;
outgoing edges are explored in order to collect cycles (lines 7-9 of Rec-DFS, see Table 2)
while incoming edges are considered in order to collect quasi-cycles (lines 11-14); before
extending the current path p with a reversed edge, however, it is necessary to check whether
the latter is consecutive to all previously reversed edges (if any) present in p and to skip it
otherwise (cf. Formula (1)). The stack visited contains the nodes visited so far, in order to
avoid the repetition of a node in a path (cf. lines 1, 5 and 15 of Rec-DFS). Finally the search
ends when the maximum path length is reached, or a previously visited node is encountered
(line 1 of Rec-DFS); otherwise, if the initial sense s is found, a (quasi-)cycle is collected
(lines 2-4 of Rec-DFS). For each sense s0 of w0 the DFS returns the full set paths(s0 ) of
paths collected. Finally, in line 4 of Table 1, all_paths is set to store the paths for all the
senses of w0 .
The second phase of the CQC algorithm (lines 5-10 of Table 1) computes a score for
each sense s0 of w0 based on the paths collected for s0 during the first phase. Let p be such
a path, and let l be its length, i.e., the number of edges in the path. Then the contribution
of p to the score of s0 is given by:
score(p) :=

(l)
N umP aths(all_paths, l)

(2)

where:
143

fiFlati & Navigli

lingua2n
originaleB.3
n
parlatoB.1
n

language1n

linguaggio2n
tongue1n

speech1n
eloquio1n
favella2n
parlareD.2
n
idioma1n

Figure 4: The Ragazzini-Biagi graph from Figure 2 pruned as a result of the CQC algorithm.

 (l) is a monotonically non-increasing function of its length l; in our experiments,
we tested three different weight functions (l), namely a constant, a linear and an
inversely exponential function (see Section 5).
 the normalization factor N umP aths(all_paths, l) calculates the overall number of
collected paths of length l among all the target senses.
In this way the score of a sense s0 amounts to:
X

score(s0 ) :=

ppaths(s0 )

score(p) =


X

(l)

l=2

N umP aths(paths(s0 ), l)
N umP aths(all_paths, l)

(3)

The rationale behind our scoring formula is two-fold: first  thanks to function   it
favours shorter paths, which are intuitively less likely to be noisy; second, for each path
length, it accounts for the ratio of paths of that length in which s0 participates (second
factor of the right-hand side of the formula above).
After the scores for each sense s0 of the target translation w0 have been calculated, a
mapping is established between w0 and the highest-scoring sense (line 11). Finally, after all
the translations have been disambiguated, the mapping is returned (line 12).
As a result of the systematic application of the algorithm to each sense in our BiMRD
D, a new graph G0 = (V, E 0 ) is output, where V is again the sense inventory of D, and
E 0 is a subset of the noisy edge set E such that each edge (s, s0 )  E 0 is the result of our
disambiguation algorithm run with input D and s. Figure 4 shows the clean, unambiguous
dictionary graph after executing CQC, as compared to the initial noisy graph from Figure
2. In this pruned graph, each sense links to only one sense of each of its translations.
4.1 An Example
As an example, consider the following dictionary entry in the Ragazzini-Biagi dictionary:

144

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

language1n

lingua1n

language1n

lingua1n

favella2n

tongue1n

idioma1n

tongue1n

language1n

lingua2n

language1n

lingua2n

language1n

lingua2n

language1n

lingua2n

favella2n

tongue1n

idioma1n

tongue1n

language1n

lingua2n

language1n

lingua2n

linguaggio2n

linguaggio3n

linguaggio1n

Figure 5: Cycles and quasi-cycles collected from DFS(lingua 1n , language 1n ) (top) and
DFS(lingua 2n , language 1n ) (bottom).

language n. 1 lingua; linguaggio.
In order to disambiguate the Italian translations we call the CQC algorithm as follows:
CQC(D, language1n ). Let us first concentrate on the disambiguation of linguan , an ambiguous word with two senses in the Ragazzini-Biagi. First, two calls are made, namely
DF S(lingua1n , language1n ) and DF S(lingua2n , language1n ). Each function call performs a
DFS starting from the respective sense of our target word to collect all relevant cycles
and quasi-cycles according to the algorithm in Table 2. The set of cycles and quasi-cycles
collected for the two senses from the noisy graph of Figure 2 are shown in Figure 5.
During the second phase of the CQC algorithm, and for each sense of linguan , the
contribution of each path is calculated (lines 8-10 of the algorithm in Table 1). Specifically,
the following scores are calculated for the two senses of lingua n (we assume our weight
function (l) = 1/el ):
score(lingua 1n ) =

2 

score(lingua 2n ) =
+
+
'

1
3
2
1

1
e4



1
N umP aths(all_paths,4)

' 2  0.018 

1
 e12  N umP aths(all_paths,2)
+
1
1
 e3  N umP aths(all_paths,3) +
1
 e14  N umP aths(all_paths,4)
'
1
1
 0.135  1 + 3  0.050  3 + 2  0.018 

1
4

1
4

= 0.009

= 0.194

where N umP aths(all_paths, l) is the total number of paths of length l collected over all
the senses of linguan . Finally, the sense with the highest score (i.e., lingua2n in our example)
is returned.
Similarly, we determine the scores of the various senses of linguaggion as follows:
145

fiFlati & Navigli

score(linguaggio 1n ) =

2 

score(linguaggio 2n ) =
+
+
'

1
2
2
1

score(linguaggio 3n ) =

1 

1
e4



1
N umP aths(all_paths,4)

' 2  0.018 

1
 e12  N umP aths(all_paths,2)
+
1
1
 e3  N umP aths(all_paths,3) +
1
 e14  N umP aths(all_paths,4)
'
1
1
 0.135  2 + 2  0.050  2 + 2  0.018 
1
e2



1
N umP aths(all_paths,2)

1
4

' 1  0.135 

1
4

= 0.009.

= 0.1265.
1
2

= 0.0675.

As a result, sense 2 is correctly selected.

5. Evaluation: Dictionary Disambiguation
In our first set of experiments we aim to assess the disambiguation quality of the CQC
algorithm and compare it with existing disambiguation approaches. We first describe our
experimental setup in Section 5.1, by introducing the bilingual dictionary used throughout
this article, and providing information on the dictionary graph, our tuning and test datasets,
and the algorithms, parameters and baselines used in our experiments. We describe our
experimental results in Section 5.2.
5.1 Experimental Setup
In this section we discuss the experimental setup for our dictionary disambiguation experiment.
5.1.1 Dictionary
We performed our dictionary disambiguation experiments on the Ragazzini-Biagi (Ragazzini
& Biagi, 2006), a popular bilingual English-Italian dictionary, which contains over 90,000
lemmas and 150,000 word senses.
5.1.2 Dictionary Graph
In order to get an idea of the difficulty of our dictionary disambiguation task we determined
the ratio of wrong edges in the graph. To do this we first calculated the ratio of correct edges,
i.e., those edges which link source senses to their right translation senses. This quantity
can be estimated as the overall number of translations in the dictionary (i.e., assuming each
translation has an appropriate sense in the dictionary) divided by the total number of edges:
P
CorrectnessRatio(G) =

|T (s)|

sV

(4)

|E|

The ratio of wrong edges is then calculated as 1  CorrectnessRatio(G), obtaining an
estimate of 66.4% of incorrect edges in the noisy graph of the Ragazzini-Biagi dictionary.
146

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Dataset

# entries

Tuning Dataset
Test Dataset

50
500

# translations

# polysemous

avg. polysemy

perfect alignments

80
1,069

53
765

4.74
3.95

37
739

Table 3: Statistics for the tuning and test datasets.
5.1.3 Dataset
Our datasets for tuning and test consist of dictionary entries, each containing translations
of a source sense into a target language. Each translation item was manually disambiguated
according to its sense inventory in the bilingual dictionary. For example, given the Italian
entry brillante A.2
a , translated as sparkling a , vivacious a , we associated the appropriate English sense from the English-Italian section to sparkling a and vivacious a (senses 3 and 1,
respectively).
For tuning purposes, we created a dataset of 50 entries, totaling 80 translations. We also
prepared a test dataset of 500 entries, randomly sampled from the Ragazzini-Biagi dictionary
(250 from the English-Italian section, 250 from the Italian-English section). Overall, the
test dataset included 1,069 translations to be disambiguated. We report statistics for the
two datasets in Table 3, including the number of polysemous translations and the average
polysemy of each translation. We note that for 44 of the translations in the test set (i.e., 4.1%
of the total) none of the senses listed in the dictionary is appropriate (including monosemous
translations). A successful disambiguation system, therefore, should not disambiguate these
items. The last column in the table shows the number of translations for which a sense exists
that translates back to the source lemma (e.g., car 1n translates to macchina and macchina 3n
translates to car ).
5.1.4 Algorithms
We compared the following algorithms in our experimental framework3 , since (with the
exception of CQC and variants thereof) they represent the most widespread graph-based
approaches and are used in many NLP tasks with state-of-the-art performance:
 CQC: we applied the CQC algorithm as described in Section 4;
 Cycles, a variant of the CQC algorithm which searches for cycles only (i.e., quasicycles are not collected);
 DFS, which applies an ordinary DFS algorithm and collects all paths between s and
s0 (i.e., paths are not closed by completing them with edge sequences connecting s0
to s). In this setting the path s  s0 is discarded, as by construction it can be found
in G for each sense s0  Senses(w0 );
 Random walks, which performs a large number of random walks starting from s0
and collecting those paths that lead to s. This approach has been successfully used to
approximate an exhaustive search of translation circuits (Mausam, Soderland, Etzioni,
3. In order to ensure a level playing field, we provided in-house implementations for all the algorithms
within our graph-based framework, except for Personalized PageRank, for which we used a standard
implementation (http://jung.sourceforge.net).

147

fiFlati & Navigli

Weld, Skinner, & Bilmes, 2009; Mausam, Soderland, Etzioni, Weld, Reiter, Skinner,
Sammer, & Bilmes, 2010). We note that, by virtue of its simulation nature, this
method merely serves as a way of collecting paths at random. In fact, given a path
ending in a node v, the next edge is chosen equiprobably among all edges outgoing
from v.
 Markov chains, which calculates the probability of arriving at a certain source sense
s starting from the initial translation sense s0 averaged over n consecutive steps, that is,
P
(m)
(m)
ps0 ,s = n1 nm=1 ps0 ,s , where ps0 ,s is the probability of arriving at node s using exactly
m steps starting from node s0 . The initial Markov chain is initialized from the noisy
(0)
dictionary graph as follows: for each v, v 0  V , if (v, v 0 )  E, then pv,v0 = 1/out(v),
(0)

where out(v) is the outdegree of v in the noisy graph, otherwise pv,v0 = 0.
 Personalized PageRank (PPR): a popular variant of the PageRank algorithm
(Brin & Page, 1998) where the original Markov chain approach to node ranking is
modified by perturbating the initial probability distribution on nodes (Haveliwala,
2002, 2003). PPR has been successfully applied to Word Sense Disambiguation (Agirre
& Soroa, 2009) and thus represents a very competitive system to compare with. In
order to disambiguate a target translation w0 of a source word w, for each translation
sense s0 , we concentrate all the probability mass on s0 , and apply PPR. We select the
best translation sense as the one which maximizes the PPR value of the source word
(or, equivalently, that of the translation sense itself).
 Lesk algorithm (Lesk, 1986): we apply an adaptation of the Lesk algorithm in which,
given a source sense s of word w and a word w0 occurring as a translation of s, we
determine the right sense of w0 on the basis of the (normalized) maximum overlap
between the entries of each sense s0 of w0 and that of s:
|next (s)  next (s0 )|
,

 0
s0 Senses(w0 ) max{|next (s)|, |next (s )|}
argmax

where we define next (s) = synonyms(s)  next(s), synonyms(s) is the set of lexicalizations of sense s (i.e., the synonyms of sense s, e.g., acknowledgement vs acknowledgment) and next(s) is the set of nodes s0 connected through an edge (s, s0 ).
For all the algorithms that explicitly collect paths (CQC, Cycles, DFS and Random
walks), we tried three different functions for weighting paths, namely:
 A constant function (l) = 1 that weights all paths equally, independently of their
length l;
 A linear function (l) = 1/l that assigns each path a score inversely proportional to
its length l;
 An exponential function (l) = 1/el that assigns a score that decreases exponentially
with the path length.
148

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Algorithm
CQC
Cycles
DFS
Random walks
Markov chains

Best Configuration
Length 
Specific parameters
4
up to 2 terminal reversed edges
4
4
4
400 random walks
2
-

Table 4: Parameter tuning for path-based algorithms.
5.1.5 Parameters
We used the tuning dataset to fix the parameters of each algorithm that maximized the
performance. We tuned the maximum path length for each of the path-based algorithms
(CQC, Cycles, DFS, Random walks and Markov chains), by trying all lengths in {1, . . . , 6}.
Additionally, for CQC, we tuned the minimum and maximum values for the parameters
j and k used for quasi-cyclic patterns (cf. Formula 1 in Section 4). These parameters
determine the position and the number of reversed edges in a quasi-cyclic graph pattern.
The best results were obtained when n  1  k  n  1, i.e. k = n  1, and n  3  j < n  1,
that is, CQC yielded the best performance when up to 2 terminal reversed edges were sought
(cf. Section 3.3 and Figure 3). For Random walks, we tuned the number of walks needed
to disambiguate each item (ranging between 50 and 2,000). The best parameters resulting
from tuning are reported in Table 4. Finally, for PPR we used standard parameters: we
performed 30 iterations and set the damping factor to 0.85.
5.1.6 Measures
To assess the performance of our algorithms, we calculated precision (the number of correct
answers over the number of items disambiguated by the system), recall (the number of
correct answers over the number of items in the dataset), and F1 (a harmonic mean of
R
precision and recall, given by P2P+R
). Note that precision and recall do not consider those
items in the test set for which no appropriate sense is available in the dictionary. In order
to account for these items, we also calculated accuracy as the number of correct answers
divided by the total number of items in the test set.
5.1.7 Baselines
We compared the performance of our algorithms with three baselines:
 the First Sense (FS) Baseline, that associates the first sense listed by the dictionary
with each translation to be disambiguated (e.g., car 1n is chosen for car independently
of the disambiguation context). The rationale behind this baseline derives from the
tendency of lexicographers to sort senses according to the importance they perceive or
estimate from a (possibly sense-tagged) corpus;
 the Random Baseline, which selects a random sense for each target translation;
149

fiFlati & Navigli

 the Degree Baseline, that chooses the translation sense with the highest out-degree,
i.e., the highest number of outgoing edges.
5.2 Results
We are now ready to present the results of our dictionary disambiguation experiment.
5.2.1 Results without Backoff Strategy
In Table 5 we report the results of our algorithms on the test set. CQC, PPR and Cycles are
the best performing algorithms, achieving around 83%, 81% and 75% accuracy respectively.
CQC outperforms all other systems in terms of F1 by a large margin. The results show
that the mere use of cyclic patterns does not lead to state-of-the-art performance, which is,
instead, obtained when quasi-cycles are also considered. Including quasi-cycles leads to a
considerable increase in recall, while at the same time maintaining a high level of precision.
The DFS is even more penalizing because it does not get backward support as happens for
cycling patterns. Markov chains consistently outperform Random walks. We hypothesize
that this is due to the higher coverage of Markov chains compared to the number of random
walks collected by a simulated approach. PPR considerably outperforms the two other
probabilistic approaches (especially in terms of recall and accuracy), but lags behind CQC
by 3 points in F1 and 2 in accuracy. This result confirms previous findings in the literature
concerning the high performance of PPR, but also corroborates our hunch about quasi-cycles
being the determining factor in the detection of hard-to-find semantic connections within
dictionaries. Finally, Lesk achieves high precision, but low recall and accuracy, due to the
lack of a lookahead mechanism.
The choice of the weighting function impacts the performance of all path-based algorithms, with 1/el performing best and the constant function 1 resulting in the worst results
(this is not the case for the DFS, though).
The random baseline represents our lowerbound and is much lower than all other results.
Compared to the first sense baseline, CQC, PPR and Cycles obtain better performance. This
result is consistent with previous findings for tasks such as the Senseval-3 Gloss Word Sense
Disambiguation (Litkowski, 2004). However, at the same time, it is in contrast with results
on all-words Word Sense Disambiguation (Navigli, 2009b), where the first or most frequent
sense baseline generally outperforms most disambiguation systems. Nevertheless, the nature
of these two tasks is very different, because  in dictionary entries  senses tend to be equally
distributed, whereas in open text they have a single predominant meaning that is determined
by context. As for the Degree Baseline, it yields results below expectations, and far worse
than the FS baseline. The reason behind this lies in the fact that the amount of translations
and translation senses does not necessarily correlate with mainstream meanings.
While attaining the highest precision, CQC also outperforms the other algorithms in
terms of accuracy. However, accuracy is lower than F1: this is due to F1 being a harmonic
mean of precision and recall, while in calculating accuracy each and every item in the dataset
is taken into account, even those items for which no appropriate sense tag can be given.
In order to verify the reliability of our tuning phase (see Section 5.1), we studied the F1
performance of CQC by varying the depth  of the DFS (cf. Section 4). The best results
 shown in Table 6  are obtained on the test set when  = 4, which confirms this as the
150

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Algorithm
CQC 1/el
CQC 1/l
CQC 1
Cycles 1/el
Cycles 1/l
Cycles 1
DFS 1/el
DFS 1/l
DFS 1
Random walks 1/el
Random walks 1/l
Random walks 1
Markov chains
PPR
Lesk
First Sense BL
Random BL
Degree BL

P
87.14
87.04
86.33
87.17
86.49
84.56
63.40
63.40
63.56
83.94
83.67
79.38
85.46
83.20
86.05
72.67
28.53
58.39

Performance
R
F1
A
83.32 85.19 83.35
83.22 85.09 83.26
82.54 84.39 82.60
74.93 80.59 75.58
74.34 79.96 75.02
72.68 78.17 73.43
37.85 47.40 39.85
37.85 47.40 39.85
37.95 47.52 39.94
61.17 70.77 62.49
60.98 70.55 62.30
57.85 66.93 59.31
65.37 74.08 66.70
81.25 82.21 81.27
31.90 46.55 34.52
73.17 72.92 73.53
29.76 29.13 28.53
58.85 58.39 58.62

Table 5: Disambiguation performance on the Ragazzini-Biagi dataset.

CQC- e1l

=2
76.94

=3
82.85

=4
85.19

=5
84.50

Table 6: Disambiguation performance of CQC- e1l based on F1.
optimal parameter choice for CQC (cf. Table 4). In fact, F1 increases with higher values
of , up to a performance peak of 85.19% obtained when  = 4. With higher values of 
we observed a performance decay due to the noise introduced. The optimal value of  is
in line with previous experimental results on the impact of the DFS depth in Word Sense
Disambiguation (Navigli & Lapata, 2010).
5.2.2 Results with Backoff Strategy
As mentioned above, the experimented path-based approaches are allowed not to return
any result; this is the case when no paths can be found for any sense of the target word.
In a second set of experiments we thus let the algorithms use the first sense baseline as a
backoff strategy whenever they were not able to give any result for a target word. This
is especially useful when the disambiguation system cannot make any decision because of
lack of knowledge in the dictionary graph. As can be seen in Table 7, the scenario changes
151

fiFlati & Navigli

Algorithm
CQC 1/el
CQC 1/l
CQC 1
Cycles 1/el
Cycles 1/l
Cycles 1
DFS 1/el
DFS 1/l
DFS 1
Random walks 1/el
Random walks 1/l
Random walks 1
Markov chains
PPR
Lesk
First Sense BL
Random BL
Degree BL

Performance with FS
P
R
F1
A
86.52 87.02 86.77 86.81
86.42 86.93 86.67 86.72
85.74 86.24 86.00 86.06
85.55 86.05 85.80 85.87
84.97 85.46 85.21 85.31
83.32 83.80 83.56 83.72
68.00 68.39 68.19 68.94
68.00 68.39 68.19 68.94
68.09 68.49 68.29 69.04
82.06 82.54 82.30 82.51
81.86 82.34 82.10 82.32
78.76 79.22 79.00 79.33
82.75 83.32 83.03 83.26
83.12 83.77 83.44 83.12
82.07 82.63 82.35 82.60
72.67 73.17 72.92 73.53
28.53 29.76 29.13 28.53
58.39 58.85 58.39 58.62

Table 7: Disambiguation performance on the Ragazzini-Biagi dataset using the first sense
(FS) as backoff strategy.

radically in this setting. The adoption of the first sense backoff strategy results in generally
higher performance; notwithstanding this CQC keeps showing the best results, achieving
almost 87% F1 and accuracy when  = 1/el .
5.2.3 Directed vs. Undirected Setting
Our algorithm crucially takes advantage of the quasi-cyclic pattern and therefore relies
heavily on the directionality of edges. Thus, in order to further verify the beneficial impact
of quasi-cycles, we also compared our approach in an undirected setting, i.e., using a noisy
graph whose edges are unordered pairs. This setting is similar to that of de Melo and
Weikum (2010), who aim at detecting imprecise or wrong interlanguage links in Wikipedia.
However, in their task only few edges are wrong (in fact, they remove less than 2% of the
cross-lingual interlanguage links), whereas our dictionary graph contains much more noise,
which we estimated to involve around 66% of the edges (see Section 5.1.2).
To test whether directionality really matters, we compared CQC with its natural undirected counterpart, namely Undirected Cycles: this algorithm collects all (undirected)
cycles linking each target sense back to the source sense in the underlying undirected noisy
graph. We did not implement the DFS in the undirected setting because it is equivalent to
the Undirected Cycles; neither did we implement the undirected versions of Random Walks,
152

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Algorithm
Undirected Cycles 1/el
Undirected Cycles 1/l
Undirected Cycles 1

P
76.67
76.56
76.12

Performance
R
F1
A
67.16 66.73 71.60
67.06 66.63 71.50
66.67 66.25 71.08

Table 8: Disambiguation performance on the Ragazzini-Biagi dataset using an undirected
model.

Algorithm
Undirected Cycles 1/el
Undirected Cycles 1/l
Undirected Cycles 1

Performance with FS
P
R
F1
A
77.50 78.10 77.50 77.80
77.40 78.01 77.40 77.70
77.01 77.61 77.01 77.31

Table 9: Disambiguation performance on the Ragazzini-Biagi dataset using an undirected
model (using the FS baseline).

Markov Chains and PPR, because they are broadly equivalent to Degree in an undirected
setting (Upstill, Craswell, & Hawking, 2003). As shown in Table 8, Undirected Cycles yields
a 66% F1 performance and 71% accuracy (almost regardless of the  function). Consistently
with our previous experiments, allowing the algorithm to resort to the FS Baseline as backoff strategy boosts performance up to 77-78% (with  = 1/el producing the best results,
see Table 9). Nonetheless, Undirected Cycles performs significantly worse than Cycles and
CQC.
The reason for this behaviour lies in the strong disambiguation evidence provided by the
directed flow of information. In fact, not accounting for directionality leads to a considerable
loss of information, since we would be treating two different scenarios in the same way: one
in which s  t and another one in which s  t.
For example, in the directed setting two senses s and t which reciprocally link to one
another (s  t) create a cycle of length 2 (s  t  s); in an undirected setting, instead,
the two edges are merged (s  t) and no supporting cycles of length 2 can be found. As a
result we are not considering the fact that t translates back to s, which is a precious piece
of information! Furthermore an undirected cycle is likely to correspond to a noisy, illegal
quasi-cycle (cf. Figure 3(f)), i.e., one which could contain any sequence whatsoever of plain
and reversed edges. Consequently, in the undirected setting meaningful and nonsensical
paths are lumped together.

6. Dictionary Enhancement
We now present an application of the CQC algorithm to the problem of enhancing the
quality of a bilingual dictionary.
153

fiFlati & Navigli

6.1 Ranking Translation Senses
As explained in Section 4, the application of the CQC algorithm to a sense entry determines,
together with a sense choice, a ranking for the senses chosen for its translations. For instance,
the most appropriate senses for the translations of language (cf. Section 4.1) are chosen
on the basis of the following scores: 0.009 (lingua 1n ), 0.194 (lingua 2n ), 0.009 (linguaggio 1n ),
0.1265 (linguaggio 2n ), 0.0675 (linguaggio 3n ). The higher the score for the target translation,
the higher the confidence in selecting the corresponding sense. In fact, a high score is a clear
hint of a high amount of connectivity conveyed from the target translation back to the source
sense. As a result, the following senses are chosen in our example: lingua 2n , linguaggio 2n . Our
hunch is that this confidence information can prove to be useful not only in disambiguating
dictionary translations, but also in identifying recurring problems dictionaries tend to suffer
from.
For instance, assume an English word w translates to an Italian word w0 but no sense
entry of w0 in the bilingual dictionary translates back to w. An example where such a
shortcoming could be fixed is the following: wood 2n  bosco but no sense of bosco translates
back into wood (here wood 2n and bosco refer to the forest sense). However, this phenomenon
does not always need to be solved. This might be the case if w is a relevant (e.g., popular)
translation for w0 , but w0 is not a frequent term. For instance, idioma 1n (idiom n in english)
translates to language and no sense of language has idioma as its translation. This is correct
because we do not expect language to translate into such an uncommon word as idioma.
But how can we decide whether a problem of this kind needs to be fixed (like bosco)
or not (like idioma)? To answer this question we will exploit the confidence scores output
by the CQC algorithm. In fact, applying the CQC algorithm to the pair wood 2n , bosco 1n we
obtain a score of 0.2 (indicating that bosco 1n should point back to wood )4 , while on the pair
idioma 1n , language 1n we get a score of 0.07 (pointing out that idioma 1n is not at easy reach
from language 1n ).
6.2 Patterns for Enhancing a Bilingual Dictionary
In this section, we propose a methodology to enhance the bilingual dictionary using the
sense rankings provided by the CQC algorithm. In order to solve relevant problems raised
by the Zanichelli lexicographers on the basis of their professional experience, we identified
the following 6 issues, each characterized by a specific graph pattern:
 Misalignment. The first pattern is of the kind sw  sw0 6 sw , where sw is a sense
of w in the source language, sw0 is a sense of w0 in the target language, and  denotes
a translation in the dictionary. For instance, buy 1n is translated as compera 1n , but
compera 1n is not translated as buy 1n . A high-ranking sense such as compera 1n implies
that this issue should be solved.
 Partial alignment. This pattern is of the kind sw  sw0  sw00 w or sw00 w  sw0 
sw where sw and sw00 w are senses in the source language, w00 w is a compound that ends
with w, and sw0 is a sense in the target language. For instance, repellent 1n is translated
as insettifugo 1n , which in turn translates to insect repellent 1n .
4. Note that in practice values greater than 0.3 are very unlikely.

154

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

issue

pattern

misalignment

buy n. 1 (fam.) acquisto; compera.
compera n. 1 purchase; shopping.

sw
sw 0

repellent n. 1 sostanza repellente; insettifugo.
insettifugo n. 2 insect repellent.

sw
sw 0

persistente a. 1 persistent; persisting.
persisting a. (not available in the dictionary).

s w00

sw
sw 0
sw00

pass n. 3 tesserino (di autobus, ecc.).
tesserino n. 1  tessera.
tessera n. 1 card; ticket; pass.

s w0 , s w00

sw
sw0 , sw00

s w0

sw
s w00 w

sw

s w0

sw

s w0

use of reference

use of variant

sw

sw

example

sw
sw 0

s w0

sw

partial alignment

missing lemma

sense
entry

s w0

inconsistent spelling
s w00

sw
sw00

riscontro n. 6 reply; acknowledgment.
acknowledgement, acknowledgment n.
ferma di ricevuta; riscontro.

3 con-

asciugacapelli n. 1 hair-dryer.
hair dryer n. 1 asciugacapelli.

Table 10: The set of graph patterns used for enhancement suggestions.
 Missing lemma. This pattern is of the kind sw  sw0 where sw0 does not exist in the
dictionary. For example, persistente 1a is translated as persistent a , however the latter
lemma does not exist in the dictionary lexicon.
 Use of reference. This pattern is of the kind sw  sw0  sw00  sw where sw0 is a
reference to sw00 . For example, pass 3n is translated as tesserino 1n , while the latter refers
to tessera 1n , which in turn is translated as pass n . However, for claritys sake, double
referencing should be avoided within dictionaries.
 Use of variant. This pattern is of the kind sw  sw0 and sw00  sw , where w00 is a
variant of w0 . For example, riscontro 6n is translated as acknowledgment 1n . However,
this is a just variant of the main form acknowledgement 1n . In the interests of consistency
the main form should always be preferred.
 Inconsistent spelling. This pattern is of the kind sw  sw0 and sw00  sw where
w and w0 differ only by minimal spelling conventions. For example, asciugacapelli 1n
is translated as hair-dryer 1n , while hair dryer 1n is translated as asciugacapelli 1n . The
inconsistency between hair-dryer and hair dryer must be solved in favour of the latter,
which is a lemma defined within the dictionary.
Table 10 presents the above patterns in the form of graphs together with examples.
Next, we collected all the pattern occurrences in the Ragazzini-Biagi bilingual dictionary
and ranked them by the CQC scores assigned to the corresponding translation in the source
155

fiFlati & Navigli

source sense
still 1.A.1
a
burrasca 1n
achievement 2n
..
.

target sense
immobile A.2
a
storm 1n
impresa 2n
..
.

CQC score
0.3300
0.3200
0.3100
..
.

phat 1a
opera 5n

grande A.1
a
society 2n

0.0001
0.0001

Table 11: Top- and bottom-ranking dictionary issues identified using the misalignment pattern.

issue
misalignment
partial alignment
missing lemma
use of reference
use of variant
inconsistent spelling

% accepted
80.0
40.0
21.0
84.5
83.5
98.0

no. absolute
118904
8433
15955
167
1123
12

Table 12: Enhancement suggestions accepted.

entry. An excerpt of the top- and bottom-ranking issues for the misalignment pattern is
reported in Table 11.

7. Evaluation: Dictionary Enhancement
In the following two subsections we describe the experimental setup and give the results of
our dictionary enhancement experiment.
7.1 Experimental Setup
The aim of our evaluation is to show that the higher the confidence score the higher the
importance of the issue for an expert lexicographer. Given such an issue (e.g., misalignment),
we foresee two possible actions to be taken by a lexicographer: apply some change to the
dictionary entry or ignore the issue. In order to assess the quality of the issues, we prepared
a dataset of 200 randomly-sampled instances for each kind of dictionary issue (i.e., 200
misalignments, 200 uses of variants, etc.). Overall the dataset included 1,200 issue instances
(i.e., 200  6 issue types). The dataset was manually annotated by expert lexicographers,
who decided for each issue whether a change in the dictionary was needed (positive response)
or not (negative response). Random sampling guarantees that the dataset has a distribution
comparable to that of the entire set of instances for an issue of interest.
156

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

suggestions accepted / suggestions presented
1.0
0.9
0.8
0.7
0.6
0.5
0.4

score
0

0.1

0.2

0.3

Figure 6: Performance trend of enhancement suggestions accepted by score for the misalignment issue.

7.2 Results
We report the results for each issue type in Table 12. We observed an acceptance percentage
ranging between 80.0 and 84.5% for three of the issues, namely: misalignment, use of reference and use of variant, thus indicating a high level of reliability for the degree of importance
calculated for these issues. We note however that semantics cannot be of much help in the
case of missing lemmas, partial alignment and inconsistent spelling. In fact these issues
inevitably cause the graphs to be disconnected and thus the disambiguation scores equal 0.
To determine whether our score-based ranking impacts the degree of reliability of our
enhancement suggestions we graphed the percentage of accepted suggestions by score for
the misalignment issue (Figure 6). As expected, the higher the disambiguation score, the
higher the percentage of suggestions accepted by lexicographers, up to 99% when the score
> 0.27. We observed similar trends for the other issues.

8. Synonym Extraction
In the previous sections, we have shown how to use cycles and quasi-cycles to extend bilingual
dictionary entries with sense information and tackle important dictionary issues. We now
propose a third application of the CQC algorithm to enrich the bilingual dictionary with
synonyms, a task referred to as synonym extraction. The task consists of automatically
identifying appropriate synonyms for a given lemma. Many efforts have been made to
develop automated methods that collect synonyms. Current approaches typically rely either
on statistical methods based on large corpora or on fully-fledged semantic networks such as
WordNet (a survey of the literature in the field is given in Section 10). Our approach is
closer to the latter direction, but relies on a bilingual machine readable dictionary (i.e., a
resource with no explicit semantic relations), rather than a full computational lexicon. We
exploit cross-language connections to identify the most appropriate synonyms for a given
word using cycles and quasi-cycles.
157

fiFlati & Navigli

The idea behind our synonym extraction approach is as follows: starting from some
node(s) in the graph associated with a given word, we perform a cycle and quasi-cycle
search (cf. Section 4). The words encountered in the cycles or quasi-cycles are likely to be
closely related to the word sense we started from and they tend to represent good synonym
candidates in the two languages. We adopted two synonym extraction strategies:
 sense-level synonym extraction: the aim of this task is to find synonyms of a given
sense s of a word w.
 word-level synonym extraction: given a word w, we collect the union of the synonyms
for all senses of w.
In both cases we apply CQC to obtain a set of paths P (respectively starting from a
given sense s of w or from any sense of w). Next, we rank each candidate synonym according
to the following formula:
score(w0 ) =

X
pP (w0 )

1

(5)

elength(p)

which provides a score for a synonym candidate w0 , where P (w0 ) is the set of (quasi-)cycles
passing through a sense of w0 . In the sense-level strategy P (w0 ) contains all the paths starting
from sense s of our source word w, whereas in the word-level strategy P (w0 ) contains paths
starting from any sense of w. In contrast to other approaches in the literature, our synonym
extraction approach actually produces not only synonyms, but also their senses according
to the dictionary sense inventory. Further, thanks to the above formula, we are able to rank
synonym senses from most to less likely. For example, given the English sense capable1a , the
system outputs the following ordered list of senses:
Lang.
it
it
en
en
it
en
en
..
.
en
it

Word sense
abile 1a
capace 2a
able 1a
skilful 1a
esperto A.1
a
clever 1a
deft 1a
..
.
crafty 1a
destro A.1
a

Score
13.34
8.50
4.42
3.21
3.03
2.61
1.00
..
.
0.18
0.17

In the word-level strategy, instead, synonyms are found by performing a CQC search
starting from each sense of the word w, and thus collecting the union of all the paths
obtained in each individual visit. As a result we can output the list of all the words which
are likely to be synonym candidates. For example, given the English word capable, the
system outputs the following ordered list of words:
158

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Lang.
it
it
en
en
it
en
en
..
.
it
en

Word
abile
capace
clever
able
esperto
skilful
deft
..
.
sapiente
expert

Score
12.00
10.65
3.45
2.90
2.41
2.30
0.54
..
.
0.18
0.16

9. Evaluation: Synonym Extraction
We now describe the experimental setup and discuss the results of our synonym extraction
experiment.
9.1 Experimental Setup
9.1.1 Dataset
To compare the performance of CQC on synonym extraction with existing approaches, we
used the Test of English as a Foreign Language (TOEFL) dataset provided by ETS via
Thomas Landauer and coming originally from the Educational Testing Service (Landauer
& Dumais, 1997). This dataset is part of the well-known TOEFL test used to evaluate the
ability of an individual to use and understand English. The dataset includes 80 question
items, each presenting:
1. a sentence where a target word w is emphasized;
2. 4 words listed as possible synonyms for w.
The examinee is asked to indicate which one, among the four presented choices, is more
likely to be the right synonym for the given word w. The examinees language ability is then
estimated to be the fraction of correct answers. The performance of automated systems can
be assessed in the very same way.
9.1.2 Algorithms
We performed our experiments with the same algorithms used in Section 5.1.4 and compared
their results against the best ones known in the literature. All of our methods are based
on some sort of graph path or cycle collection. In order to select the best synonym for a
target word, we used the approach described in Section 8 for all methods but Markov chains
and PPR. For the latter we replaced equation 5 with the corresponding scoring function of
the method (cf. Section 5.1.4). We also compared with the best approaches for synonym
extraction in the literature, including:
 Product Rule (PR) (Turney, Littman, Bigham, & Shnayder, 2003): this method 
which achieves the highest performance  combines various different modules. Each
159

fiFlati & Navigli

module produces a probability distribution based on a word closeness coefficient calculated on the possible answers the system can output and a merge rule is then applied
to integrate all four distributions into a single one.
 Singular Value Decomposition (LSA) (Rapp, 2003), an automatic Word Sense
Induction method which aims at finding sense descriptors for the different senses of
ambiguous words. Given a word, the twenty most similar words are considered good
candidate descriptors. Then pairs are formed and classified according to two criteria:
i) two words in a couple should be as dissimilar as possible; ii) their cooccurrence
vectors should sum to the ambiguous word cooccurrence vector (scaled by 2). Finally,
words with the highest score are selected.
 Generalized Latent Semantic Analysis (GLSA) (Matveeva, Levow, Farahat, &
Royer, 2005), a corpus-based method which builds term-vectors and represents the
document space in terms of vectors. By means of Singular Value Decomposition and
Latent Semantic Analysis they obtain the similarity matrix between the words of a
prefixed vocabulary and extract the related document matrix. Next, synonyms of a
word are selected on the basis of the highest cosine-similarity between the candidate
synonym and the fixed word.
 Positive PMI Cosine (PPMIC) (Bullinaria & Levy, 2007) systematically explores
several possibilities of representation for the word meanings in the space of cooccurrence vectors, studying and comparing different information metrics and implementation details (such as the cooccurrence window or the corpus size).
 Context-window overlapping (CWO) (Ruiz-Casado, M., E., & Castells, 2005) is
an approach based on the key idea that synonymous words can be replaced in most
contexts. Given two words, their similarity is measured as the number of contexts that
can be found by replacing each word with the other, where the context is restricted to
an L-window of open-class words in a Google snippet.
 Document Retrieval PMI (PMI-IR) (Terra & Clarke, 2003) integrates many
different word similarity measures and cooccurrence estimates. Using a large corpus
of Web data they analyze how the corpus size influences the measure performance and
compare a window- with a document-oriented approach.
 Rogets Thesaurus system (JS) (Jarmasz & Szpakowicz, 2003), exploits Rogets
thesaurus taxonomy and WordNet to measure semantic similarity. Given two words
their closeness is defined as the minimum distance between the nodes associated with
the words. This work is closest to our own in that the structure of knowledge resources
is exploited to extract synonyms.
9.2 Results
In Table 13 and 14 we report the performance (precision and recall, respectively) of our
algorithms on the TOEFL with maximum path length  varying from 2 to 6. The best
results are obtained for all algorithms (except for Markov chains) when  = 6, as this value
makes it easier to find near synonyms that cannot be immediately obtained as translations
160

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Algorithm
CQC
Cycles
DFS
Random walks
Markov chains

Maximum path
2
3
4
100.00 97.56 98.15
100.00 97.50 97.83
97.67 97.78 97.82
97.44 95.12 97.56
94.91 86.76 85.29

length
5
98.36
98.00
98.04
97.62
85.29

6
93.15
96.43
96.36
97.83
85.29

Table 13: Precision of our graph-based algorithms on the TOEFL dataset.

Algorithm
CQC
Cycles
DFS
Random walks
Markov chains

2
47.50
47.50
52.50
47.50
70.00

Maximum path length
3
4
5
6
50.00 66.25 75.00 85.00
48.75 56.25 61.25 67.50
55.00 56.25 62.50 66.25
48.75 50.00 51.25 56.25
73.75 72.50 72.50 72.50

Table 14: Recall of our graph-based algorithms on the TOEFL dataset.

of the target word in the dictionary. We attribute the higher recall (but lower precision)
of Markov chains to the amount of noise accumulated after only few steps. Interestingly,
PPR (which is independent of parameter , and therefore is not shown in Tables 13 and 14)
obtained comparable performance, i.e., 94.55% precision and 65% recall. Thus, CQC is the
best graph-based approach achieving 93% precision and 85% recall. This result corroborates
our previous findings (cf. Section 5).
Table 15 shows the results of the best systems in the literature and compares them with
CQC.5 We note that the systems performing better than CQC exploit a large amount of
information: for example Rapp (2003) uses a corpus of more than 100 million words of
everyday written and spoken language, while Matveeva et al. (2005) draw on more than
1 million New York Times articles with a history label. Even if they do not rely on a
manually-created lexicon, they do have to cope with the extremely high term-space dimension and need to adopt some method to reduce dimensionality (i.e., either using Latent
Semantic Indexing on the term space or reducing the vocabulary size according to some
general strategy such as selecting the top frequent words).
It is easy to see how our work stands above all lexicon-based ones, raising performance
from 78.75% up to 85% recall. In Table 15 we also report the performance of other lexiconbased approaches in the literature (Hirst & St-Onge, 1998; Leacock & Chodorow, 1998;
Jarmasz & Szpakowicz, 2003). We note that our system exploits the concise edition of
the Ragazzini bilingual dictionary which omits lots of translations (i.e., edges) and senses
which are to be found in the complete edition of the dictionary. Our graph algorithm could
5. Further information about the state of the art for the TOEFL test can be found at the following web
site: http://aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions_(State_of_the_art)

161

fiFlati & Navigli

Algorithm
PR
LSA
GLSA
CQC
PPMIC
CWO
PMI-IR
JS
HSO
PairClass
DS
Human
Random
LC

Author(s) / Method
Turney et al. (2003)
Rapp (2003)
Matveeva et al. (2005)
Flati and Navigli (2012)
Bullinaria and Levy (2007)
Ruiz-Casado et al. (2005)
Terra and Clarke (2003)
Jarmasz and Szpakowicz (2003)
Hirst and St-Onge (1998)
Turney (2008)
Pado and Lapata (2007)
Average non-English US college applicant
Random guessing
Leacock and Chodorow (1998)

Resource type
Hybrid
Corpus-based
Corpus-based
Lexicon-based
Corpus-based
Web-based
Corpus-based
Lexicon-based
Lexicon-based
Corpus-based
Corpus-based
Human
Random
Lexicon-based

Recall (%)
97.50
92.50
86.25
85.00
85.00
82.55
81.25
78.75
77.91
76.25
73.00
64.50
25.00
21.88

Table 15: Recall of synonym extraction systems on the TOEFL dataset.

readily take advantage of the richer structure of the complete edition to achieve even better
performance.
Another interesting aspect is the ability of CQC to rank synonym candidates. To better
understand this phenomenon, we performed a second experiment. We selected 100 senses
(50 for each language). We applied the CQC algorithm to each of them and also to their
lemmas. In the former case a sense-tagged list was returned; in the latter the list contained
just words. Then we determined the precision of CQC in retrieving the top ranking K
synonyms (precision@K) according to the algorithms score. We performed our evaluation
at both the sense- and the word-level, as explained in Section 8. In Table 16 we report the
precision@K calculated at both levels when K = 1, . . . , 10. Note that, when K is sufficiently
small (K  4), the sense-level extraction achieves performance similar to the word-level
one, while being semantically precise. However, we observe that with larger values of K the
performance difference increases considerably.

10. Related Work
We now review the literature in the three main fields we have dealt with in this paper,
namely: gloss disambiguation (Section 10.1), dictionary enhancement (Section 10.2) and
synonym extraction (Section 10.3).
10.1 Gloss Disambiguation
Since the late 1970s much work on the analysis and disambiguation of dictionary glosses has
been done. This includes methods for the automatic extraction of taxonomies from lexical resources (Litkowski, 1978; Amsler, 1980), the identification of genus terms (Chodorow, Byrd,
& Heidorn, 1985) and, more in general, the extraction of explicit information from machine162

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Level

Sense

Word

K
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10

Precision
79.00
75.50
70.33
67.01
63.41
60.31
59.24
57.01
55.28
53.06
80.00
74.50
71.67
72.50
71.20
68.83
67.29
65.88
64.22
62.90

Correct/Given
79 / 100
151 / 200
211 / 300
266 / 397
312 / 492
354 / 587
404 / 682
443 / 777
482 / 872
512 / 965
80 / 100
149 / 200
215 / 300
290 / 400
356 / 500
413 / 600
471 / 700
527 / 800
578 / 900
629 /1000

Table 16: Precision@K of the CQC algorithm in the sense and word synonym extraction
task.

readable dictionaries (see, e.g., Nakamura & Nagao, 1988; Ide & Vronis, 1993), as well as
the construction of ambiguous semantic networks from glosses (Kozima & Furugori, 1993).
A relevant project in this direction is MindNet (Vanderwende, 1996; Richardson, Dolan, &
Vanderwende, 1998), a lexical knowledge base obtained from the automated extraction of
lexico-semantic information from two machine-readable dictionaries.
More recently, a set of heuristics has been proposed to semantically annotate WordNet
glosses, leading to the release of the eXtended WordNet (Harabagiu et al., 1999; Moldovan &
Novischi, 2004). Among the heuristics, the cross reference heuristic is the closest technique
to our notion of (quasi-)cyclic patterns. Given a pair of words w and w0 , this heuristic is
based on the occurrence of w in the gloss of a sense s0 of w0 and, vice versa, of w0 in the
gloss of a sense s of w. In other words, a cycle s  s0  s of length 2 is sought. Recently, a
similar consideration has been put forward proposing that probabilistic translation circuits
can be used as evidence to automatically acquire a multilingual dictionary (Mausam et al.,
2009).
Based on the eXtended WordNet, a gloss disambiguation task was organized at Senseval3 (Litkowski, 2004). Most notably, the best performing systems, namely the TALP system
(Castillo et al., 2004), and SSI (Navigli & Velardi, 2005), are knowledge-based and rely on
rich knowledge resources: respectively, the Multilingual Central Repository (Atserias, Vil163

fiFlati & Navigli

larejo, Rigau, Agirre, Carroll, Magnini, & Vossen, 2004), and a proprietary lexical knowledge
base (cf. Navigli & Lapata, 2010).
However, the literature in the field of gloss disambiguation is focused only on monolingual dictionaries, such as WordNet and LDOCE. To our knowledge, CQC is the first
algorithm aimed at disambiguating the entries of a bilingual dictionary: our key idea is
to harvest (quasi-)cyclic paths from the dictionary  viewed as a noisy graph  and use
them to associate meanings with the target translations. Moreover, in contrast to many
disambiguation methods in the literature (Navigli, 2009b), our approach works on bilingual
machine-readable dictionaries and does not exploit lexical and semantic relations, such as
those available in computational lexicons like WordNet.
10.2 Dictionary Enhancement
The issue of improving the quality of machine-readable dictionaries with computational
methods has been poorly investigated so far. Ide and Vronis (1993, 1994), among others,
have been working on the identification of relevant issues when transforming a machinereadable dictionary into a computational lexicon. These include overgenerality (e.g., a
newspaper defined as an artifact, rather than a publication), inconsistent definitions (e.g.,
two concepts defined in terms of each other), meta-information labels and sense divisions
(e.g., fine-grained vs. coarse-grained distinctions). Only little work has been done on the
automatic improvement of monolingual dictionaries (Navigli, 2008), as well as bilingual resources, for which a gloss rewriting algorithm has been proposed (Bond, Nichols, & Breen,
2007). However, to our knowledge, the structure of bilingual dictionaries has never previously been exploited for the purpose of suggesting dictionary enhancements. Moreover, we
rank our suggestions on the basis of semantic-driven confidence scores, thus submitting to
the lexicographer more pressing issues first.
10.3 Synonym Extraction
Another task aimed at improving machine-readable dictionaries is that of synonym extraction. Many efforts have been made to automatically collect a set of synonyms for a word
of interest. We introduced various methods aimed at this task in Section 8. Here we
distinguish in greater detail between corpus-based (i.e., statistical) and lexicon-based (or
knowledge-based) approaches.
Corpus-based approaches typically harvest statistical information about word occurrences from large corpora, inferring probabilistic clauses such as word w is likely to appear
(i.e., cooccur) together with word y with probability p. Thus, word similarity is approximated with word distance functions. One common goal is to build a cooccurrence matrix;
this can be done directly via corpus analysis or indirectly by obtaining its vector space
representation.
The most widespread statistical method (Turney et al., 2003; Bullinaria & Levy, 2007;
Ruiz-Casado et al., 2005; Terra & Clarke, 2003) is to estimate the word distance by counting
the number of times that two words appear together in a corpus within a fixed k-sized
window, followed by a convenient normalization. This approach suffers from the well-known
data sparseness problem; furthermore it introduces the additional window-size parameter k
whose value has to be tuned.
164

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

A similar statistical approach consists of building a vocabulary of terms V from a corpus
C and then representing a document by means of the elements of V contained therein. In
this framework a document is represented as a vector, a corpus as a term-document matrix L
as well as a document-term matrix L0 . The matrix product LL0 represents the cooccurrence
matrix which gives a measure of word closeness.
For computational reasons, however, it is often desirable to shrink the vocabulary size.
Classical algebraic methods, such as Singular Value Decomposition (SVD), can be applied to
synonym extraction (Rapp, 2003; Matveeva et al., 2005), because they are able to produce
a smaller vocabulary V 0 representing the concept space. These methods do not take into
account the relative word position, but only cooccurrences within the same document, so less
information is usually considered. On the other hand, by virtue of SVD, a more significant
concept space is built and documents can be more suitably represented.
Lexicon-based approaches (Jarmasz & Szpakowicz, 2003; Blondel & Senellart, 2002) are
an alternative to purely statistical ones. Graph models are employed in which words are
represented by nodes and relations between words by edges between nodes. In this setting, no
corpus is required. Instead two words are deemed to be synonyms if the linking path, if any,
satisfies some structural criterion, based on length, structure or connectivity degree. Our
application of CQC to the synonym extraction problem follows this direction. However, in
contrast to existing work in the literature, we do not exploit any lexical or semantic relation
between concepts, such as those in WordNet, nor any lexical pattern as done by Wang and
Hirst (2012). Further, we view synonym extraction as a dictionary enrichment task that we
can perform at a bilingual level.

11. Conclusions
In this paper we presented a novel algorithm, called Cycles and Quasi-Cycles (CQC), for
the disambiguation of bilingual machine-readable dictionaries. The algorithm is based on
the identification of (quasi-)cycles in the noisy dictionary graph, i.e., circular edge sequences
(possibly with some consecutive edges reversed) relating a source word sense to a target one.
The contribution of the paper is threefold:
1. We show that our notion of (quasi-)cyclic patterns enables state-of-the-art performance
to be attained in the disambiguation of dictionary entries, surpassing all other disambiguation approaches (including the popular PPR), as well as a competitive baseline
such as the first sense heuristic. Crucially, the introduction of reversed edges allows us
to find more semantic connections, thus substantially increasing recall while keeping
precision very high.
2. We explore the novel task of dictionary enhancement by introducing graph patterns
for a variety of dictionary issues, which we tackle effectively by means of the CQC
algorithm. We use CQC to rank the issues based on the disambiguation score and
present enhancement suggestions automatically. Our experiments show that the higher
the score the more relevant the suggestion. As a result, important idiosyncrasies such
as missing or redundant translations can be submitted to expert lexicographers, who
can review them in order to improve the bilingual dictionary.
165

fiFlati & Navigli

3. We successfully apply CQC to the task of synonym extraction. While data-intensive
approaches achieve better performance, CQC obtains the best result among lexiconbased systems. As an interesting side effect, our algorithm produces sense-tagged
synonyms for the two languages of interest, whereas state-of-the-art approaches all
focus on a single language and do not produce sense annotations for synonyms.
The strength of our approach lies in its weakly supervised nature: the CQC algorithm
relies exclusively on the structure of the input bilingual dictionary. Unlike other research
directions, no further resource (such as labeled corpora or knowledge bases) is required.
The paths output by our algorithm for the dataset presented in Section 5.1 are available
from http://lcl.uniroma1.it/cqc. We are scheduling the release of a software package
which allows for the application of the CQC algorithm to any resource for which a standard
interface can be implemented.
As regards future work, we foresee several developments of the CQC algorithm and its
applications: starting from the work of Budanitsky and Hirst (2006), we plan to experiment
with cycles and quasi-cycles when used as a semantic similarity measure, and compare them
with the most successful existing approaches. Moreover, although in this paper we focused
on the disambiguation of dictionary glosses, exactly the same approach can be applied to
the disambiguation of collocations using any dictionary of choice (along the lines of Navigli,
2005), thus providing a way of further enriching lexical knowledge resources with external
knowledge.

Acknowledgments
The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No.
259234. The authors wish to thank Jim McManus, Simon Bartels and the three anonymous
reviewers for their useful comments on the paper, and Zanichelli for making the RagazziniBiagi dictionary available for research purposes.

References
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, M., & Soroa, A. (2009). A study
on similarity and relatedness using distributional and wordnet-based approaches. In
Proceedings of the Conference of the North American Chapter of the Association of
Computational Linguistics (HLT-NAACL 2009), pp. 1927, Boulder, Colorado, USA.
Agirre, E., & Soroa, A. (2009). Personalizing PageRank for Word Sense Disambiguation.
In Proceedings of the 12th conference of the European chapter of the Association for
Computational Linguistics (EACL 2009), pp. 3341, Athens, Greece.
Amsler, R. A. (1980). The structure of the Merriam-Webster pocket dictionary, Ph.D. Thesis.
University of Texas, Austin, TX, USA.
Atserias, J., Villarejo, L., Rigau, G., Agirre, E., Carroll, J., Magnini, B., & Vossen, P.
(2004). The MEANING Multilingual Central Repository. In Proceedings of International Global WordNet Conference (GWC 2004), pp. 2330, Brno, Czech Republic.
Bernard, J. (Ed.). (1986). Macquarie Thesaurus. Macquarie, Sydney, Australia.
166

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Blondel, V. D., & Senellart, P. P. (2002). Automatic extraction of synonyms in a dictionary.
In Proceedings of the SIAM Text Mining Workshop, Arlington, VA, USA.
Bohman, T., & Thoma, L. (2000). A note on sparse random graphs and cover graphs. The
Electronic Journal of Combinatorics, 7 (1), 19.
Bond, F. C., Nichols, E., & Breen, J. W. (2007). Enhancing a dictionary for transfer rule
acquisition. Linguistic Research, 24 (2), 133151.
Brin, S., & Page, M. (1998). Anatomy of a large-scale hypertextual web search engine. In
Proceedings of the 7th Conference on World Wide Web (WWW), pp. 107117, Brisbane,
Australia.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures of semantic distance. Computational Linguistics, 32 (1), 1347.
Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations from word
co-occurrence statistics: A computational study. Behavior Research Methods, 39 (3),
510526.
Castillo, M., Real, F., Asterias, J., & Rigau, G. (2004). The TALP systems for disambiguating WordNet glosses. In Proceedings of ACL 2004 SENSEVAL-3 Workshop, pp. 9396,
Barcelona, Spain.
Chodorow, M., Byrd, R., & Heidorn, G. (1985). Extracting semantic hierarchies from a large
on-line dictionary. In Proceedings of Association for Computational Linguistics (ACL
1985), pp. 299304, Chicago, IL, USA.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction to algorithms. MIT
Press, Cambridge, MA.
Cuadros, M., & Rigau, G. (2006). Quality assessment of large scale knowledge resources.
In Proceedings of Empirical Methods on Natural Language Processing (EMNLP 2006),
pp. 534541, Sydney, Australia.
Cuadros, M., & Rigau, G. (2008). KnowNet: Building a large net of knowledge from the
web. In Proceedings of the 22nd International Conference on Computational Linguistics
(COLING), pp. 161168, Manchester, UK.
de Melo, G., & Weikum, G. (2010). Untangling the cross-lingual link structure of Wikipedia.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pp. 844853, Uppsala, Sweden. Association for Computational
Linguistics.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Database. MIT Press, Cambridge, MA.
Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-based semantic interpretation for natural language processing. Journal of Artificial Intelligence Research (JAIR), 34, 443
498.
Harabagiu, S., Miller, G., & Moldovan, D. (1999). WordNet 2 - a morphologically and semantically enhanced resource. In Proceedings of Special Interest Group on the Lexicon
of the Association for Computational Linguistics (SIGLEX 99), pp. 18, Maryland,
USA.
167

fiFlati & Navigli

Haveliwala, T. H. (2002). Topic-sensitive PageRank. In Proceedings of 11th International
Conference on World Wide Web (WWW 2002), pp. 517526.
Haveliwala, T. H. (2003). Topic-sensitive pagerank: A context-sensitive ranking algorithm for
web search. IEEE Transactions on Knowledge and Data Engineering, 15 (4), 784796.
Hirst, G., & St-Onge, D. (1998). Lexical chains as representations of context for the detection
and correction of malapropisms. In Fellbaum, C. (Ed.), WordNet: An electronic lexical
database, pp. 305332. MIT Press.
Ide, N., & Vronis, J. (1993). Extracting knowledge bases from machine-readable dictionaries: Have we wasted our time?. In Proceedings of Workshop on Knowledge Bases and
Knowledge Structures, pp. 257266, Tokyo, Japan.
Ide, N., & Vronis, J. (1994). Machine readable dictionaries: What have we learned, where
do we go?. In Proceedings of the COLING 94 International Workshop on Directions
of Lexical Research, Beijing, China.
Jacquemin, B., Brun, C., & Roux, C. (2002). Enriching a text by semantic disambiguation
for Information Extraction. In Proceedings of the Workshop on Using Semantics for
Information Retrieval and Filtering in the 3rd International Conference on Language
Resources and Evaluations (LREC 2002), Las Palmas, Spain.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2003), pp. 212219, Borovets, Bulgaria.
Kozima, H., & Furugori, T. (1993). Similarity between words computed by spreading activation on an english dictionary. In Proceedings of The Association for Computational
Linguistics (ACL 1993), pp. 232239, Utrecht, The Netherlands.
Krovetz, R., & Croft, W. B. (1992). Lexical ambiguity and Information Retrieval. ACM
Transactions on Information Systems, 10 (2), 115141.
Landauer, T. K., & Dumais, S. T. (1997). Solution to Platos Problem: The Latent Semantic Analysis Theory of Acquisition, Induction and Representation of Knowledge.
Psychological Review, 104 (2), 211240.
Leacock, C., & Chodorow, M. (1998). Combining local context and WordNet similarity
for word sense identification. In Fellbaum, C. (Ed.), WordNet: An electronic lexical
database, pp. 265283. MIT Press.
Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries: How
to tell a pine cone from an ice cream cone. In Proceedings of the 5th Special Interest
Group on Design of Communication (SIGDOC), pp. 2426, New York, NY.
Lita, L. V., Hunt, W. A., & Nyberg, E. (2004). Resource analysis for Question Answering. In Proceedings of the 42th Annual Meeting of the Association for Computational
Linguistics (ACL 2004), Interactive poster and demonstration sessions, pp. 162165,
Barcelona, Spain.
Litkowski, K. C. (1978). Models of the semantic structure of dictionaries. American Journal
of Computational Linguistics, 81, 2574.
168

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Litkowski, K. C. (2004). SENSEVAL-3 task: Word-Sense disambiguation of WordNet glosses.
In Proceedings of ACL 2004 SENSEVAL-3 Workshop, pp. 1316, Barcelona, Spain.
Mandala, R., Tokunaga, T., & Tanaka, H. (1998). The use of WordNet in Information
Retrieval. In Proceedings of COLING/ACL Workshop on Usage of WordNet in Natural
Language Processing Systems, pp. 3137, Montreal, Canada.
Martnez, D., de Lacalle, O. L., & Agirre, E. (2008). On the use of automatically acquired
examples for all-nouns Word Sense Disambiguation. Journal of Artificial Intelligence
Research (JAIR), 33, 79107.
Matveeva, I., Levow, G., Farahat, A., & Royer, C. (2005). Generalized latent semantic
analysis for term representation. In Proceedings of the International Conference on
Recent Advances in Natural Language Processing (RANLP 2005), Borovets, Bulgaria.
Mausam, Soderland, S., Etzioni, O., Weld, D., Skinner, M., & Bilmes, J. (2009). Compiling
a massive, multilingual dictionary via probabilistic inference. In Proceedings of Association for Computational Linguistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP 2009), pp. 262270, Singapore.
Mausam, Soderland, S., Etzioni, O., Weld, D. S., Reiter, K., Skinner, M., Sammer, M., &
Bilmes, J. (2010). Panlingual lexical translation via probabilistic inference. Artificial
Intelligence, 174 (9-10), 619637.
Mihalcea, R. (2005). Unsupervised large-vocabulary word sense disambiguation with graphbased algorithms for sequence data labeling. In Proceedings of the Human Language
Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pp. 411418, Vancouver, BC.
Moldovan, D., & Novischi, A. (2002). Lexical chains for Question Answering. In Proceedings
of International Conference on Computational Linguistics (COLING), pp. 17, Taipei,
Taiwan.
Moldovan, D., & Novischi, A. (2004). Word Sense Disambiguation of WordNet glosses.
Computer Speech & Language, 18 (3), 301317.
Nakamura, J.-I., & Nagao, M. (1988). Extraction of semantic information from an ordinary
English dictionary and its evaluation. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), pp. 459464, Budapest, Hungary.
Nastase, V. (2008). Topic-driven multi-document summarization with encyclopedic knowledge and spreading activation. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2008), pp. 763772, Honolulu, Hawaii.
Nastase, V., & Szpakowicz, S. (2001). Word Sense Disambiguation in Rogets Thesaurus
Using WordNet. In Proceedings of the NAACL WordNet and Other Lexical Resources
workshop, pp. 1722, Pittsburgh, USA.
Navigli, R. (2005). Semi-automatic extension of large-scale linguistic knowledge bases. In
Proceedings of the Eighteenth International Florida Artificial Intelligence Research Society Conference (FLAIRS), pp. 548553, Clearwater Beach, Florida.
Navigli, R. (2008). A structural approach to the automatic adjudication of word sense
disagreements. Journal of Natural Language Engineering, 14 (4), 293310.
169

fiFlati & Navigli

Navigli, R. (2009a). Using Cycles and Quasi-Cycles to disambiguate dictionary glosses.
In Proceedings of the 12th conference of the European chapter of the Association for
Computational Linguistics (EACL 2009), pp. 594602.
Navigli, R. (2009b). Word Sense Disambiguation: a Survey. ACM Computing Surveys,
41 (2), 169.
Navigli, R. (2012). A Quick Tour of Word Sense Disambiguation, Induction and Related
Approaches. In Proceedings of the 38th International Conference on Current Trends in
Theory and Practice of Computer Science (SOFSEM 2012), pindlerv Mln, Czech
Republic.
Navigli, R., Faralli, S., Soroa, A., de Lacalle, O. L., & Agirre, E. (2011). Two birds with
one stone: learning semantic models for Text Categorization and Word Sense Disambiguation. In Proceedings of the 20th ACM Conference on Information and Knowledge
Management (CIKM 2011), pp. 23172320, Glasgow, United Kingdom.
Navigli, R., & Lapata, M. (2010). An experimental study on graph connectivity for unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Anaylsis and
Machine Intelligence (TPAMI), 32 (4), 678692.
Navigli, R., & Ponzetto, S. P. (2010). BabelNet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pp. 216225, Uppsala, Sweden.
Navigli, R., & Velardi, P. (2005). Structural semantic interconnections: a knowledge-based
approach to Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 27 (7), 10751088.
Pado, S., & Lapata, M. (2007). Dependency-based construction of semantic space models.
Computational Linguistics, 33 (2), 161199.
Pedersen, T., Banerjee, S., & Patwardhan, S. (2005). Maximizing semantic relatedness to
perform Word Sense Disambiguation. In University of Minnesota Supercomputing
Institute Research Report UMSI 2005/25, Minnesota.
Ponzetto, S. P., & Navigli, R. (2010). Knowledge-rich Word Sense Disambiguation rivaling
supervised system. In Proceedings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL 2010), pp. 15221531, Uppsala, Sweden.
Proctor, P. (Ed.). (1978). Longman Dictionary of Contemporary English. Longman Group,
UK.
Ragazzini, G., & Biagi, A. (Eds.). (2006). Il Ragazzini-Biagi, 4th Edition. Zanichelli, Italy.
Rapp, R. (2003). Word sense discovery based on sense descriptor dissimilarity. In Proceedings
of the Ninth Machine Translation Summit, pp. 315322, New Orleans, LO, USA.
Richardson, S. D., Dolan, W. B., & Vanderwende, L. (1998). MindNet: acquiring and structuring semantic information from text. In Proceedings of International Conference
on Computational Linguistics (COLING 1998), pp. 10981102, Montreal, Quebec,
Canada.
Roget, P. M. (1911). Rogets International Thesaurus (1st edition). Cromwell, New York,
USA.
170

fiCycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary

Rosso, P., Molina, A., Pla, F., Jimnez, D., & Vidal, V. (2004). Information retrieval and text
categorization with semantic indexing. In Proceedings of the Intelligent Text Processing
and Computational Linguistics Conference (CICLing), pp. 596600, Seoul, Korea.
Ruiz-Casado, M., A., E., & Castells, P. (2005). Using context-window overlapping in synonym discovery and ontology extension. In Proceedings of the International Conference
Recent Advances in Natural Language Processing (RANLP 2005), Borovets, Bulgaria.
Sanderson, M. (2000). Retrieving with good sense. Information Retrieval, 2 (1), 4969.
Silber, H. G., & McCoy, K. F. (2003). Efficient text summarization using lexical chains. In
Proceedings of 5th Conference on Intelligent User Interfaces (IUI), pp. 252255, New
Orleans, USA.
Terra, E., & Clarke, C. (2003). Frequency estimates for statistical word similarity measures.
In Proceedings of the 2003 Human Language Technology and North American Chapter of Association of Computational Linguistics Conference (HLT/NAACL 2003), pp.
244251, Edmonton, Canada.
Turney, P. D. (2008). A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of the 22nd International Conference on Computational
Linguistics (COLING), pp. 905912, Manchester, UK.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent modules to solve multiple-choice synonym and analogy problems. In Proceedings
of the International Conference on Recent Advances in Natural Language Processing
(RANLP 2003), pp. 101110, Borovets, Bulgaria.
Upstill, T., Craswell, N., & Hawking, D. (2003). Predicting fame and fortune: PageRank or
Indegree?. In Proceedings of the Australasian Document Computing Symposium, pp.
3140, Canberra, Australia.
Vanderwende, L. (1996). The analysis of noun sequences using semantic information extracted from on-line dictionaries, Ph.D. Thesis. Georgetown University, Washington,
USA.
Wang, P., & Domeniconi, C. (2008). Building semantic kernels for text classification using
Wikipedia. In Proceedings of the 14th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), pp. 713721, Las Vegas, Nevada.
Wang, T., & Hirst, G. (2012). Exploring patterns in dictionary definitions for synonym
extraction. Journal of Natural Language Engineering, 18.
Woodsend, K., & Lapata, M. (2011). WikiSimple: Automatic simplification of wikipedia
articles. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence
(AAAI), pp. 927932, San Francisco, California, USA.
Yarowsky, D. (1992). Word-sense disambiguation using statistical models of the Rogets
categories trained on large corpora. In Proceedings of 14th International Conference
on Computational Linguistics (COLING 1992), pp. 454460, Nantes, France.

171

fiJournal of Artificial Intelligence Research 43 (2012) 43-86

Submitted 07/11; published 01/12

Robust Local Search for Solving RCPSP/max with
Durational Uncertainty
Na Fu
Hoong Chuin Lau
Pradeep Varakantham

na.fu.2007@phdis.smu.edu.sg
hclau@smu.edu.sg
pradeepv@smu.edu.sg

School of Information Systems,
Singapore Management University,
80 Stamford Road, 178902 Singapore

Fei Xiao

feixiao@gmail.com

Google Inc.
1600 Amphitheatre Parkway Mountain View,
CA 94043 USA

Abstract
Scheduling problems in manufacturing, logistics and project management have frequently been modeled using the framework of Resource Constrained Project Scheduling
Problems with minimum and maximum time lags (RCPSP/max). Due to the importance
of these problems, providing scalable solution schedules for RCPSP/max problems is a
topic of extensive research. However, all existing methods for solving RCPSP/max assume
that durations of activities are known with certainty, an assumption that does not hold
in real world scheduling problems where unexpected external events such as manpower
availability, weather changes, etc. lead to delays or advances in completion of activities.
Thus, in this paper, our focus is on providing a scalable method for solving RCPSP/max
problems with durational uncertainty. To that end, we introduce the robust local search
method consisting of three key ideas: (a) Introducing and studying the properties of two
decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty; (b) Deriving the expression for robust
makespan of an execution strategy based on decision rule approximations; and (c) A robust
local search mechanism to efficiently compute activity execution strategies that are robust
against durational uncertainty. Furthermore, we also provide enhancements to local search
that exploit temporal dependencies between activities. Our experimental results illustrate
that robust local search is able to provide robust execution strategies efficiently.

1. Introduction
Research in scheduling has typically considered problems with deterministic durations. In
real-world scheduling problems, unexpected external events such as manpower availability,
weather changes, etc. lead to uncertainty about durations of activities. There has been
a growing interest to account for such data uncertainty (Herroelen & Leus, 2005; Beck &
Wilson, 2007; Rodrguez, Vela, Puente, & Hernandez-Arauzo, 2009) while providing optimized schedules. This paper also focuses on this important issue of durational uncertainty
in scheduling problems. More specifically, we consider scheduling problems where there are
complex resource constraints and temporal dependencies between activities.
c
2012
AI Access Foundation. All rights reserved.

fiFu, Lau, Varakantham, & Xiao

There are broadly two approaches for tackling scheduling problems with durational
uncertainty. One is to adopt a hybrid of proactive and reactive methods, e.g., the work
of Vonder, Demeulemeester, and Herroelen (2007), where an initial baseline schedule is
computed offline, which is then modified (if required) during execution reactively based on
the occurrence of external events. The second approach, e.g., the paper by Mohring and
Stork (2000), is to design schedule policies that provide online decision rules such that at
time t, the policy decides which task(s) may start and which resource(s) to assign. In this
paper, we adopt the latter approach and focus on the computation of a robust schedule
policy.
From the computational perspective, stochasticity adds a great deal of complexity to the
underlying deterministic scheduling problem. For example, in the infinite-resource project
scheduling problem where processing times have two possible discrete values, the problem
of computing the expected makespan (or any point on the cumulative distribution of the
optimal makespan), is #P-hard (Hagstrom, 1988; Mohring,
2001). It has also been shown
P
that for the scheduling problem 1|stoch pj ; dj = d|E[ wj Uj ], the problem of computing a
policy (i.e., execution strategy) maximizing the probability that some job completes exactly
at the deadline is PSPACE-hard (Dean, Goemans, & Vondrak, 2004). Daniels and Carrillo
(1997) consider a one-machine scheduling problem with probabilistic durations, with an
objective to capture the likelihood that a schedule yields actual performance no worse than
a given target level. This has been shown to be NP-hard even though the underlying
deterministic problem can be solved in polynomial time.
The concrete problem of interest in this paper is the Resource Constrained Project
Scheduling Problem with minimum and maximum time lags (abbrev. RCPSP/max), which
is of great importance in manufacturing, logistics and project management. Though these
problems have been shown to be NP-Hard (Bartusch, Mohring, & Radermacher, 1988),
local search based techniques (Demeulemeester & Herroelen, 2002) have achieved great
success in solving these problems. Taking a cue from this and the recent advancements in
robust optimization, we propose a robust local search method for solving the RCPSP/max
problem under durational uncertainty with a risk management perspective. More precisely,
we (a) employ concepts from robust optimization to compute the robust makespan with
proven success probability (or risk of failure) for an execution strategy; and (b) then use
local search methods for computing an execution strategy that seeks to minimize this robust
makespan.
A recent approach (Beck & Wilson, 2007) provides techniques to compute the robust
baseline schedule from a risk management perspective, where durations of activities are
modeled as random variables. Given a value 0 <   1, they were interested to compute a schedule with minimal (probabilistic) makespan where the probability of successful
execution is at least 1   over all realizations of the durational uncertainty. The main
contribution there was to derive a lower bound for the -makespan of a given schedule by
solving a deterministic problem. They considered the Job-shop Scheduling Problem (JSP)
that represents a special case of RCPSP/max (which is the problem of interest in this
paper).
Unlike in JSPs, there are complex resource constraints and activity dependencies in
RCPSP/max problems with durational uncertainty. To account for these, we compute an
execution strategy (also known commonly as schedule policy) called Partial Order Schedule
44

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

(POS) instead of a schedule. We combine techniques from robust optimization with classical
local search to compute a POS that minimizes the robust makespan. The robust makespan
is a value for which the probability of realized makespan for any schedule (derived from
POS) does not exceed it is greater than (1  ), over all realizations of uncertainty. Thus,
we compute an upper bound on makespan values as opposed to lower bound computation
in the work of Beck and Wilson (2007).
More specifically, we make three key contributions in this paper. Firstly, we introduce
two decision rule approximations to define expressions for start times of activities based
on random variables used to represent the durational uncertainties: (a) Segregated Linear
Approximation(SLA) and (b) Generalized Non-Linear Approximation (GNLA). Secondly,
we derive expressions for the upper bound on robust makespan by employing the one sided
Chebyshevs inequality on the decision rule approximations above. Finally, we perform local
search for an execution strategy using the robust makespan upper bound. We also provide
enhancements that consider feedback about robustness of execution strategies to improve
the performance of local search.
In order to demonstrate the effectiveness of our methods, we evaluate the performance on
benchmark problem sets of RCPSP/max and Job-shop Scheduling Problems (JSPs) with
durational uncertainty. Furthermore, we make an in house comparison amongst various
enhancements developed in this paper. Finally, due to the absence of competing algorithms
for solving RCPSP/max problems and to provide an indication of the performance provided
by robust local search, we compare against the existing best solver for JSPs with durational
uncertainty.
In the next section, we present a brief background of the models and solution concepts
referred to in this paper. We then present the decision rule approximations in Section 3 and
the computation of robust makespan upper bound in Section 4. The detailed description of
robust local search and its enhancements are provided in Section 5 and Section 6. Finally,
the experimental setup and results are provided in Section 7.

2. Preliminaries
In this section, we briefly describe the notations along with the scheduling models and
robust optimization concepts of relevance to this paper.
2.1 Definitions and Notations
As given by Ben-Tal and Nemirovski (2002), we also classify the variables in a stochastic
optimization problem into 2 types: Adjustable and Non-Adjustable variables.
Definition 1. Non-Adjustable variables are a priori decisions that must be made before the
actual realization of the uncertainty.
Definition 2. Adjustable variables (also known as recourse variables) are wait-and-see
variables that can adjust themselves when part of the uncertain data become known.
For example, in a scheduling problem such as RCPSP with uncertain task durations,
the non-adjustable variables will represent the execution policy, e.g., the POS proposed by
Policella, Smith, Cesta, and Oddi (2004), that need to be constructed a priori, while the
45

fiFu, Lau, Varakantham, & Xiao

adjustable variables are associated with the actual start times of the tasks, which will be
set with respect to the execution policy and dynamic realizations of uncertainty.
A random variable will be denoted by x and bold face lower case letters such as x
represent vectors.
2.2 RCPSP/max
We now describe the deterministic RCPSP/max scheduling problem along with the extension to handle durational uncertainty. We also explain the execution policy for an uncertain
duration extension of the RCPSP/max.
2.2.1 Deterministic RCPSP/max
The RCPSP/max problem (Bartusch et al., 1988) consists of N activities {a1 , a2 ..., aN },
where each activity aj (j = 1, ...N ) is to be executed for a certain amount of time units
without preemption. Each activity aj has a fixed duration or processing time dj , which is
assumed to be a non-negative real number or non-negative integer number. In addition,
dummy activities a0 and aN +1 with d0 = dN +1 = 0 are introduced to represent the beginning
and the completion of the project, respectively.
A start time schedule ss is an assignment of start times to all activities a1 , a2 ..., aN , i.e.
a vector ss = (st(a1 ), st(a2 ), ...st(aN )) where st(ai ) represents the start time of activity ai
and st(a0 ) is assumed to be 0. Let et(ai ) be the end time of activity ai . Since durations
are deterministic and preemption is not allowed, we then have
st(ai ) + di = et(ai ).

(1)

And the project makespan which is also the start time of the final dummy activity st(aN +1 )
equals
st(aN +1 ) = maxi=1,...N et(ai ).
(2)
Schedules are subject to two kinds of constraints, temporal constraints and resource
constraints. Temporal constraints restrict the time lags between activities. A minimum
time lag Tijmin between the start time of two different activities ai and aj says that
st(aj )  st(ai )  Tijmin

(3)

Specially, Tijmin = 0 means that activity aj cannot be started before activity ai begins. A
maximum time lag Tijmax between the start time of two different activities ai and aj says
that
st(aj )  st(ai )  Tijmax
(4)
Tijmax = 0 means that activity aj cannot be started after activity ai begins.
In this definition, time lags connect start times of two related activities, known as
start-to-start time lags. start-to-end, end-to-end, end-to-start time lags can be easily transformed to the general start-to-start time lags for the deterministic case as given by Bartusch
et al. (1988). A schedule ss = (st(a1 ), st(a2 ), ...st(aN )) is time feasible, if all the time lag
constraints are satisfied at the start times st(ai ) (i = 1, ...N ).
A resource unit is reusable and available for another activity once it is no longer used
by the current activity. Each type of resource has a limited capacity, Ck (k = 1, 2..., K)
46

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

units. Each activity ai requires rik units of resource of type k where k = 1, 2..., K. Let
A(t) = {i  {1, 2...N }|st(ai )  t  et(ai )} be the set of activities which are being processed
at time instant t. A schedule is resource feasible if at each time instant t, the total demand
for a resource k does not exceed its capacity Ck , i.e.
X
rik  Ck .
(5)
iA(t)

A schedule ss is called feasible if it is both time and resource feasible. The objective of
the deterministic RCPSP/max scheduling problem is to find a feasible schedule so that the
project makespan is minimized.

Figure 1: Project Instance.

1

time

Figure 2: Example Schedule.

47

fiFu, Lau, Varakantham, & Xiao

Example 1. In Figure 1, we show a simple example of a deterministic RCPSP problem
which is a special case of RCPSP/max with only precedence constraints (rather than arbitrary time lags) between activities for expository purposes. Each circle indicates an activity
with the number inside the circle representing the activity ID. The two numbers on top of
each activity represent the duration and the number of units of the resource required by the
activity. In this example, there are 9 activities and one type of resource, with the capacity of
the resource limited to 10. It should be noted that the activities 0 and 10 are dummy activities introduced to have a source and sink in the dependency graph. Arrows between activities
represent temporal dependencies. A feasible schedule with makespan of 13 is represented in
Figure 2.
2.2.2 RCPSP/max with Durational Uncertainty and Robust Makespan
In this paper, we consider RCPSP/max problems with durational uncertainty. The duration
of an activity is specified as a sum of its mean value and its deviation: di = d0i + zi , where d0i
is the mean of di and zi is the perturbation part with an expected value of 0 and standard
deviation . It should be noted that irrespective of its distribution type, we can always
represent di as di = d0i + zi where d0i is the mean and zi is the perturbation part with
E(zi ) = 0. In addition, we also assume that these random variables, {zi }, corresponding to
durational uncertainty are independent of each other.
Similar to the deterministic RCPSP/max, the start-to-start constraints are still deterministic. However, unlike the deterministic case, other types of constraints (end-to-start
etc.) cannot be converted into deterministic start-to-start constraints . Instead the equivalent start-to-start constraint is a stochastic one as shown in the following expressions for
an end-to-start constraint. It should be noted that even though the converted constraints
are stochastic, our techniques will still be applicable (with minor modifications) to all types
of time lag constraints. Our robust local search techniques depend on the computation
of maximum and sum of random variables and even with stochastic time lag constraints
that remains the case. In this paper, for purposes of exposition, we present our techniques
assuming the temporal dependencies are provided as start-to-start constraints.
st(aj )  et(ai )  Tijmax
st(aj )  (st(ai ) + di )  Tijmax
st(aj )  st(ai )  Tijmax + di
In the deterministic setting, start time schedules can be computed and values of makespan
can be used to evaluate the performance of the schedule. However, when durational uncertainty is involved, the project makespan becomes a random variable and the schedule
is replaced by an execution strategy. In the following sections, we introduce the Partial
Order Schedule (POS) (Policella et al., 2004), which serves as an execution strategy of the
scheduling project.
Given a level of risk 0 <   1, the goal of our problem is to find such a strategy with a minimum value (across all strategies) of the robust makespan. We define the
robust makespan as a makespan value where the probability that any feasible schedule (i.e.
an assignment of start times to activities) instantiated from the strategy can be completed
before robust makespan is at least 1  .
48

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

2.2.3 Partial Order Schedule
A Partial Order Schedule (POS) was first proposed by Policella et al. (2004). It is defined as
a set of activities, which are partially ordered such that any schedule with total activity order
that is consistent with the partial order is resource and time feasible. Mathematically, a POS
can be represented by a graph where a node represents an activity and the edges represent
the precedence constraints between the activities. Within a POS, each activity retains a set
of feasible start times, which provide the flexibility to respond to unexpected disruptions.
A POS can be constructed from a given RCPSP instance via a chaining algorithm (where
one such algorithm is described below).
1

Figure 3: Example of POS
Example 2. Figure 3 provides a POS for the problem instance introduced in Example 1.
There are 10 units of the resource and that is shown on the left most side of the figure.
Each unit represents a chain. An activity can require multiple resource units and hence
is shown on multiple resource units. For instance, activity 6 is shown on resource units
4, 7 and 8. A solid arrow between activities represents a temporal dependency provided
in the original problem. Solid arrow between activities 1 and 2 is one such example. A
dotted arrow between activities represents a temporal dependency that is introduced since
both activities have to be executed on the same resource unit. It is added to remove resource
conflict. An example for this is the dependency introduced between activity 2 and activity
6. For explanatory purposes we only consider one resource type in this example, however
in the most general case, there exists multiple resource types and a dependency diagram for
every resource type.
2.2.4 Chaining Algorithm
Chaining is a procedure of dispatching activities to different resource units (henceforth
referred to as chains) based on temporal and resource feasibility. During the chaining
process, each activity can be allocated to one or more resource chains based on the number
of resource requirement of the activity. During the chaining process, once an activity is
scheduled to be executed on a resource unit, an additional edge (indicating precedence
49

fiFu, Lau, Varakantham, & Xiao

relationship) is added between the last activity of the selected chain and this activity so as
to eliminate all possible resource conflicts.
In the following, we describe the basic chaining algorithm proposed by Policella et al.
(2004). In this algorithm, a feasible schedule is first obtained using a simple greedy heuristic.
Consequently, the POS is constructed through a chaining method as follows: First, the set
of activities are sorted according to their start times given in the feasible solution; Then, all
activities are allocated to different chains in that order, where each chain corresponds to a
unit of a certain type of resource. A chain is called available for an activity if the end time
of the last activity allocated on this chain is no greater than the start time of the activity
in the feasible schedule. Once an activity is allocated on a chain, a precedence constraint
between this activity and the last activity of the chain is posted. For those activities that
require more than one unit of one or more types of resources, they will be allocated to a
number of chains with the number equal to the overall number of resource units required
by the activity.
Example 3. Take Figure 3 for example. Given the schedule of Figure 2 as an input,activities
are first sorted according to their starting time and the sequence of activities can be presented
as: (7,1,2,8,3,5,4,6,9). The chaining procedure first picks activity 7 and randomly allocates
it to five chains to fulfill its resource requirement. The available chains are those belonging
to dummy activity 0.Thus, five chains 1 through 5 are created which posts the precedence
relationship from the current last activity 0 to activity 7. Activity 7 then becomes the last
activity on those chains. Activity 1 is treated in the same way. The available chains for
activity 2 are those belonging to activity 1. Activity 2 is then randomly assigned to chain 8
through 10 and an edge between activity 1 and activity 2 indicating precedence relationship is
added. This procedure continues until all activities are dispatched to chains that the number
equals its resource requirement, and finally the chained POS 3 is yielded. However, because
the randomness of the chaining procedure, activity 6 is allocated to chains that belong to
three different activities: activity 2, activity 1 and activity 7. This will tie together the
execution of three previously unrelated activities: (activity 2, activity 6),(activity 1, activity
6) and (activity 7, activity 6), which would decrease the flexibility of execution.
To reduce inter-dependencies between activities as much as possible during the chaining
procedure, Policella, Cesta, Oddi, and Smith (2009) developed two heuristics. One direct
advantage of such approaches is that synchronization points of a solution can be reduced:
 Activities that require more than one resource units are allocated to the same subset of
chains. This is achieved by scanning the list of available chains where the last activity
in the chain : (a) requires multiple resource units; and (b) was also previously assigned
another resource unit allocated to the current activity.
 Activities with a precedence constraint defined in the original problem are allocated
to the same set of chains. This is implemented by choosing a chain that has a last
activity with precedence constraint with the current activity.
Example 4. Figure 4 provides the POS computed by using the above mentioned chaining
algorithm for the RCPSP problem described in Example 1. When allocating activity 6,
50

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

1

Figure 4: POS computed with Removed Synchronization Point
the available chains are divided into two sets: {chain 10, chain 9} and {chain 8, chain7,
chain6}. The first set contains chains for which the last activity (i.e. activity 5) is already
ordered in problem definition with respect to activity 6. A chain (for example, chain 10) is
randomly chosen from this set with the last activity on it as activity 5. Then, The remaining
available chains for activity 6 is redivided into two sets: {chain 9} and {chain 8, chain7,
chain6}. The first set contains the chains with activity 5 (i.e. the last activity of the first
picked chain) as the last activity and the second set are the remaining. Activity 6 is first
allocated to chains belonging to the first subset to satisfy all remaining resource requirements.
In this case, the synchronization points caused by activities 1 and 6, activities 7 and 6 being
allocated to different chains has disappeared.
2.3 Job-shop Scheduling Problem (JSP) with Durational Uncertainty
The classical JSP consists of a set of n jobs and a set of M machines. Each job Ji (i = 1, ...n)
consists of a sequence of ni operations denoted as Oij (j = 1, ...ni ) which have to be
processed in a given order. For
P convenience, we enumerate all operations of all jobs by Ok ,
where k = 1, ...N and N = nj=1 nj . Each operation Ok has a positive duration denoted
as dk and must be executed on a dedicated machine denoted as Mk . Once an operation is
started it must be executed for its entire duration. No operations that require the same
resource can overlap in their execution. Thus, operations can be partitioned into two sets:
job sets and resource sets. Job sets referring to operations corresponding to a job and
resource sets referring to all operations that require the same resource.
A solution s is a total ordering of operations on each resource set, which does not conflict
with the job ordering. A path of a solution s is a sequence of operations which follows both
the job ordering and the ordering on various resource sets of the solution s. The length of
a path is equal to the sum of the durations of the operations in the path. The makespan
of a solution s make(s) is the length of the longest path. The minimum makespan of
a JSP problem is defined to be the minimum value of makespans over all solutions, i.e.
mins make(s). Each operation Ok is associated with a start time of st(Ok ) and end time of
51

fiFu, Lau, Varakantham, & Xiao

et(Ok ). A schedule is an assignment of starting times st(Ok ) (k = 1, ...N ) to all operations
on the machines. The objective is to find a schedule which optimizes the total makespan
(makespan is the completion time of the last operation): maxN
k=1 et(Ok ), which is also the
minimum value of the longest path of all solutions. The job shop scheduling problem is
a special case of RCPSP in which resources have unary capacity and each activity (i.e.
operation) consumes only one resource.
We can propagate the same notations from RCPSP/max with durational uncertainty to
the JSP with durational uncertainty, i.e. the processing time of each activity (i.e. operation)
dOk is now modeled as a sum of an expected value d0Ok and a random part zOk : dOk =
d0Ok + zOk . The objective is to find the robust makespan with a given level of risk.
2.4 Segregated Random Variables
A primitive random variable zk is one which has zero mean. Examples of a primitive random
variable include U (a, a) (uniform distribution between constants a and a) and N (0, )
(normal distribution with mean 0 and variance  2 ). As mentioned earlier, we assume
that every uncertain distribution is equal to the sum of its nominal value (mean) and its
deviation, represented by one (or possibly more) primitive random variable z. In a straight
forward representation, there is only one primitive random variable zk associated with an
uncertain variable. In the recent work by Chen, Sim, Sun, and Zhang (2008), each primitive
random variable zk is represented by 2 segregated random variables zk+ (read z-plus) and zk
(z-minus):
z = z +  z 

(6)

z = max {z, 0}

(7)

+



z = max {z, 0} .

(8)

In the following Table 1, we give examples of the respective values of mean p , m and
variance p 2 , m 2 for the segregated variables z + and z  .
z
U (a, a)
N (0, )

V ar(z)

p 2 , m 2

p ,m

a2

5a2
48
(1) 2
2

a
4

2

3
2

Table 1: Values of the mean and variance for the segregated variables under Uniform and
Normal Distribution
The underlying assumption with the use of segregated random variables is that the mean
and variance of the individual segregated variables is provided for the random variables
employed. We are not aware of mean and variance values for segregated variables for
distributions other than normal and uniform.
2.5 Decision Rules for Optimization under Data Uncertainty
In optimization problems with data uncertainty, a decision rule specifies the dependence of
adjustable variables on the uncertainty parameters and the non-adjustable variables. Let z
52

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

and x denote the set of primitive random variables and non-adjustable variables respectively.
An example is the linear decision rule framework proposed by Ben-Tal and Nemirovski
(2002), where the setting value of an adjustable decision variable S(x, z) is assumed to be
affinely dependent on a subset of the N number of primitive random variables:
S(x, z) = c0 +

N
X

ck (x)zk

(9)

k=1

where each ck (x) (1  k  N ) is a coefficient derived from x.
Another example is the segregated linear decision rule framework proposed by Chen
et al. (2008), where each adjustable decision variable
assumed tobe affinely dependent
 + is

+ 
on a set of some N segregated random variables z1 , z1 , . . . , zN
, zN . Hence, a segregated
linear decision rule has the following general form:
S(x, z) = c0 +



PN

k=1


+
 
c+
z
+
c
z
k k
k k .

(10)

As we will show below, a segregated linear decision rule allows us to easily obtain an upper
bound on a subset of random variables (see Eqn 14), which is not possible in the linear
decision rule framework proposed by Ben-Tal and Nemirovski (2002).
Given the mean and variance for each segregated variable E(zk+ ) = E(zk ) = k ,
2
2
, we can express the expected value and variance of
and V ar(zk ) = mk
V ar(zk+ ) = pk
any adjustable variable as:
E[S(x, z)] = c0 +

N
X



c+
k k + ck k



(11)

k=1

N n
o
X
 + 2  
2

V ar[S(x, z)] =
ck pk + ck mk  2c+
.
c

k
k k

(12)

k=1

3. Decision Rules for RCPSP/max with Durational Uncertainty
In RCPSP/max with durational uncertainty, a decision rule specifies the dependence of
activity start times on the durational uncertainty associated with other activities. To make
the comparison with Equation 9, x represents the POS to be generated; each tasks start
time is associated with the adjustable variable S(x, z), where c0 represents the earliest start
time of this task under the POS, and ck (x) encodes how task k is related to this task in the
POS.
In a scheduling context, the start time of an activity is dependent on the start times
of the preceding activities, i.e. Adjustable variables S(x, z) are dependent on one another.
Any activity will either start after the end of an activity (i.e. in series) or after the end
of multiple activities occurring simultaneously (i.e. in parallel). Thus, adjustable variables
are functions of other adjustable variables through the addition operator (to model serial
activities) and/or the maximum operator (to model parallel activities).
53

fiFu, Lau, Varakantham, & Xiao

Given M number of adjustable variables, we may express its sum as an adjustable
variable in the form of a segregated linear decision rule as follows:
PM
=

i=1 Si (x, z)
PM 0 PN nPM + +
i=1 ci +
k=1
i=1 ci,k zk

+

PM

 
i=1 ci,k zk

o
.

(13)

Similarly, given some set C of adjustable variables, we may also express the upper bound
on the maximum of these variables as an adjustable variable in the form of a segregated
linear decision rule:
maxiC {Si (x, z)}
n
o P
n
o
P
N


+
+
 maxiC {c0i } + N
max
{c
}z
+
max
{c
}z
.
iC
iC
k=1
k=1
i,k k
i,k k

(14)

More specifically, the output of solving a RCPSP/max involves a POS that is represented
as a graph with activities as vertices and precedence constraints between activities as the
edges. Given a POS graph, x = (V, E), where V is the set of activities and E is the set of
temporal dependencies (an edge (u, v) represents a temporal dependency that states that
activity v should occur after activity u). For any activity v  V , the decision rule for
computing its start time is defined recursively as follows:
Sv (x, z) = max {d0u + zu + Su (x, z)}.
(u,v)E

(15)

Equation 15 is a recursive expression that is defined as a combination of sum and maximum on a set of random variables. It should be noted that combinations of sum and maximum of random variables cannot be computed exactly and hence we present two operational
decision rule approximations to evaluate the recursive expression of Equation 15: (a) Segregated Linear Approximation(SLA); and (b) General Non-Linear Approximation(GNLA). It
should be noted that the Sv is computable as long as mean and variance of Su is computable
and this is demonstrated with both our approximations.
3.1 Segregated Linear Approximation (SLA)
In this decision rule, the duration for each activity is defined based on the segregated random
variables introduced in Section 2.4. For an uncertain duration d with mean processing
time d0 , we represent d as a sum of three components: its mean d0 , lateness z + (i.e.
 0}),
max{d  d0 , 0}), and earliness z  (i.e. max{d0  d,
d = d0 + z +  z  .

(16)

For a normally distributed duration, i.e., z  N {0, }, the respective values of mean
and variance for the segregated variables can be summarized as:

E[z + ] = E[z  ] = 
2
(  1) 2
V ar[z + ] = V ar[z  ] =
.
2
54

(17)
(18)

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

Now we describe the computation of Sv (x, z) by representing durational uncertainty for
activities using segregated random variables. Upper bounds on both the sum and maximum
of random variables are derived as linear functions of segregated variables as illustrated
below:
 Sum of random variables : In the case of a project network involving k activities,
any two of which have either precedence constraints in between or competing for the
same resource units, a solution in the form of POS requires computation of the sum of
activity durations. The start time of the activity starting after the k-activity project
is expressed as:
P
Sk (x, (z+ , z )) = ki=1 (d0i + zi+  zi ).
(19)
P
Thus, the adjustable variable Sk a mean of ki=1 d0i with uncertainty captured by
Pk
+
a random variable, which has a positive segregated component of
i=1 zi and a
Pk

negative segregated component of
i=1 zi . Mean and variance of the segregated
variables are known and hence the mean and variance of Sk are easy to compute.
 Max of random variables: Consider activities that are executed concurrently, the
upper bound on the start time of an activity starting after the parallel k-activity
project network in SLA is represented by a linear function of the positive segregated
components of duration perturbations:
P
(20)
Sk (x, (z+ , z ))  maxi=1,...k {d0i } + ki=1 zi+ .
Thus, the adjustable variable Sk has an upper bound on the mean of maxi=1,...k {d0i }
with uncertainty
P captured by a random variable with the positive segregated component given by ki=1 zi+ and no negative segregated component. Mean and variance of
the segregated variables are known and hence the mean and variance of Sk are easy
to compute.
Since, in both cases (sum and max) Sk is expressed linearly on a subset of random segregated variables, the recursive computation is straightforward. Compared with other linear
decision rules (Ben-Tal & Nemirovski, 2002), the superiority of SLA (Chen et al., 2008) lies
in this ability to linearly express an upper bound on a subset of random variables by dissecting each uncertainty into its positive and negative components. While this approximation
increases tractability and scalability, it comes at the expense of losing accuracy.
3.2 General Non Linear Approximation (GNLA)
While SLA is efficient, it can typically provide loose upper bounds on robust makespan
due to the linear approximation for computing max of random variables. In this section,
we describe General Non Linear Approximation (GNLA), which is not restricted to only
affine dependencies. For clarity and comparison purposes, we use G to denote the start
time instead of S used in SLA.
Given the mean and variance values of duration uncertainty, we describe the approximation involved in computing mean and variance of the sum and max of activities that will
55

fiFu, Lau, Varakantham, & Xiao

be used in Equation 15. It should be recalled that irrespective of the distribution of the
 we can always represent d as d = d0 + z, where d0 is the mean of d
uncertain duration d,
and z is the perturbation part. Thus, E(z) = 0.
3.2.1 Sum of Random Variables
We compute sum of all stochastic durations in a serial k activity project network as follows:
Gk (x, z) =

k
X

(d0i + zi ).

(21)

i=1

In this case, we have a similar representation to SLA. Mean and variance of Gk are computed
as follows:
Since {zi }i=1,...k are random variables with zero mean, we can then calculate the expected
value as:
k
k
X
X
0
E[ (di + zi )] =
d0i .
i=1

(22)

i=1

Because {zi } are assumed to be independent of each other, the variance value is computed
by the following expression:
k
k
X
X
0
V ar[ (di + zi )] =
V ar[zi ],
i=1

(23)

i=1

and under normal distribution where zi  N (0, i ), we have
k
k
X
X
V ar[ (d0i + zi )] =
i2 .
i=1

(24)

i=1

Note that the expressions for expected value and variance in the case of serial activities are
identical to the ones used by Wu, Brown, and Beck (2009).
3.2.2 Max of Random Variables
For ease of explanation, we begin by considering two activities to be executed in parallel
and then extend the analysis to multiple parallel activities. In GNLA, (unlike in SLA) the
max of random variables itself is not approximated but the expected value and variance of
the max are approximately calculated.
Expected Value and Variance of Max of Two Variables
The decision rule to represent the starting time of an activity, which will begin after the
completion of two parallel activities is defined as:
G2 (z)  max{d01 , d02 } + max{z1 , z2 }.
Note that we tighten the bound in Eqn 20 by replacing z1+ + z2+ with max{z1 , z2 }.
56

(25)

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

We now derive the expressions for expected value and variance of the adjustable variable,
i.e., the RHS term of Eqn 25. Firstly, we focus on the expected value:
E[max{d01 , d02 } + max{z1 , z2 }] = max{d01 , d02 } + E[max{z1 , z2 }].

(26)

In the general case, it is difficult to derive an exact expression for E[max{z1 , z2 }] and hence,
we provide an upper bound.
In the following Propositions 1 and 2, we compute expected value and variance for the
more general case of E(z)  0 (note that we assume E(z) = 0 for all primitive random variables). We calculate for the more general case because it will be required in the computation
of expected value and variance for more than two random variables (next subsection).
Proposition 1. The expected value for the maximum of two general distributions, z1 and
z2 with nonnegativepmeans is less than
1
1
V ar[z1 ] + V ar[z2 ] + (E[z1 ])2 + (E[z2 ])2 .
2 (E[z1 ] + E[z2 ]) + 2
Proof. We begin by considering the following two equalities:
max{z1 , z2 } + min{z1 , z2 } = z1 + z2
max{z1 , z2 }  min{z1 , z2 } = |z1  z2 |.
We now sum the above two equalities.
1
max{z1 , z2 } = (z1 + z2 + |z1  z2 |).
2

(27)

Thus, we can now compute the expected value of the maximum using the following equation:
1
E[max{z1 , z2 }] = (E[z1 ] + E[z2 ] + E|z1  z2 |).
2

(28)

In addition, by using the definition of variance, we obtain:
V ar|z1  z2 | = E(z1  z2 )2  (E|z1  z2 |)2  0.
Therefore,
p
E|z1  z2 | p E(z1  z2 )2
= pE(z12 ) + E(z22 )  2E(z1 )E(z2 )
 pE(z12 ) + E(z22 )
= V ar[z1 ] + V ar[z2 ] + E(z1 )2 + E(z2 )2 .
Substituting the final expression of Eqn 29 into Eqn 28 yields the bound
p
E[max{z1 , z2 }]  12 (E[z1 ] + E[z2 ]) + 12 V ar[z1 ] + V ar[z2 ] + (E[z1 ])2 + (E[z2 ])2 .

(29)

(30)

Hence the proof.
Note that in this paper, we assume E(z) = 0, thus, a tighter bound can be obtained
from Eqn 30:
p
(31)
E[max{z1 , z2 }]  21 V ar[z1 ] + V ar[z2 ].
57

fiFu, Lau, Varakantham, & Xiao

In the special case where {zi } (i = 1, ...k) are normally and identically distributed,
i.e. zi  N (0, ), we know from the work of Clark (1961) that there is a closed form
representation for the expected value of the maximum when k = 2:

E[max{z1 , z2 }] =  .

Now we focus on deriving expressions for variance of the maximum of two general
distributions, i.e., V ar[max(z1 , z2 )].
Proposition 2. The variance for the maximum of two general distributions, z1 and z2 with
nonnegative means is less than V ar(z1 ) + V ar(z2 ) + 21 (E(z1 ))2 + 12 (E(z2 ))2 .
Proof. From Eqn 27, we have
V ar[max(z1 , z2 )] = 41 V ar[z1 + z2 + |z1  z2 |]
= 14 (V ar[z1 + z2 ] + V ar|z1  z2 | + 2COV
p (z1 + z2 , |z1  z2 |))
1
 4 (V ar[z1 + z2 ] + V ar|z1  z2 | + 2 V ar[z1 + z2 ]V ar|z1  z2 |)
 12 (V ar[z1 + z2 ] + V ar|z1  z2 |).

(32)

Firstly, we consider the following two equations.
V ar|z1  z2 | = E(z1  z2 )2  (E|z1  z2 |)2
2

(33)

2

V ar(z1  z2 ) = E(z1  z2 )  (E(z1  z2 ))
Subtracting the second from the first yields

V ar|z1  z2 | = V ar(z1  z2 ) + (E(z1  z2 ))2  (E|z1  z2 |)2 .
Now, we substitute this expression into the last term of Eqn 32 to obtain:
V ar[max(z1 , z2 )]  V ar(z1 ) + V ar(z2 ) + 12 (E(z1 )  E(z2 ))2  21 (E|z1  z2 |)2 .

(34)

When no specific distribution about duration perturbation is known, we can obtain a
bound for V ar[max(z1 , z2 )] as:
V ar[max(z1 , z2 )]  V ar(z1 ) + V ar(z2 ) + 12 (E(z1 ))2 + 12 (E(z2 ))2 .

(35)

Hence the proof.
Note that in this paper, we assume E(z) = 0, thus, a tighter bound can be obtained
from Eqn 35:
V ar[max(z1 , z2 )]  V ar(z1 ) + V ar(z2 ).
(36)
It is interesting to consider the special case when both random variables are normally
distributed. We first state the following lemma1 .
1. This can be found in statistics texts, and found online at http://en.wikipedia.org/wiki/Halfnormal distribution.

58

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

Lemma 3.1. If X is normally distributed X  N (0, ), then Y = |X| is half-normally
distributed, with
r
2
E(Y ) = 
.
(37)

Under normal distribution zi  N (0, i ), since z1  z2 is also normally distributed, and
z1  z2  N (0, 1 + 2 ), we can conclude from Lemma 3.1 that |z1  z2 | follows half-normal
distribution with
r
2
E|z1  z2 | = (1 + 2 )
.
(38)

Thus, if we substitute this expression into Eqn 34, we can express an upper bound on the
variance value for the maximum duration perturbation of two activities, when zi  N (0, i )
as :
V ar[max(z1 , z2 )]  (1 

2
1
)( 2 + 22 )  1 2 .
 1


(39)

Expected Value and Variance of Max of Multiple Variables
Extending from two to k (k > 2) parallel activities, the completion time can be upper
bounded by:
Gk (z)  max {d0i } + max {zi }.
(40)
i=1,...k
i=1,...k
In the following, we first compute the variance value of the above RHS term and then
use a similar procedure to compute the expected value. The basic expression for variance
of RHS is:
V ar[ max {d0i } + max {zi }] = V ar[ max {zi }].
(41)
i=1,...k

i=1,...k

i=1,...k

To obtain the value of V ar[ max {zi }] for general probability distributions, we take
i=1,...k

advantage of the analysis provided for the two-parallel-activity case above. The following
steps outline the overall idea:
(a) Firstly, we group the activity set {a1 , ..., ak } into a couple set {C1 , ..., Cd k e }, where
2

each element Cj (j = 1, ...d k2 e) contains two different activities Cj = {aj1 , aj2 } chosen from
the activity set. Note that when k is an odd, the final element in the couple set contains
just one activity.
(b) For each couple Cj , we apply the maximum operator on duration perturbations of involving activities. Denote cj = max{zj1 , zj2 }, where zj1 and zj2 are duration perturbations
of the two activities involved in Cj , then V ar(cj ) can be calculated based on the expression
for the two-parallel-activity case.
(c) Then we have max {zi } = max {cj }. (Note again just one activity is contained in
i=1,...k

j=1,...d k2 e

Cd k e when k is odd). Then, we can build another couple set from {C1 , ..., Cd k e }, and the
2
2
same method from steps (1) and (2) above is used to compute V ar[ max {cj }] based on
j=1,...d k2 e

Eqn 35 and/or Eqn 36 and/or Eqn 39.
59

fiFu, Lau, Varakantham, & Xiao

There are numerous ways (exponential in k) for generating the couple set {C1 , ..., Cd k e }
2
for k activities in parallel. Each of these couple sets can lead to different levels of tightness of
derived robust makespan. To compute the grouping which provides the best robust fitness
for random variables with generic distributions is an open problem. Instead, we focus on
a heuristic that computes the best grouping under normal distribution zi  N (0, i ). It is
obtained by solving the following optimization problem:
X
max
j1 j2
t
(42)
k
j=1,...b 2 c

where t denotes the grouping technique and is also the decision variable; {Cj } is the couple
set constructed from the activity set under grouping method t; j1 and j2 are the standard
deviations of data perturbation for durations of activities contained in Cj . The intuition for
employing this optimization problem is obtained from the Equation 39. It should be noted
that computing a tighter bound on variance implies considering the highest possible value
of the product of primitive variances. Hence, the reason for employing the optimization
problem of Equation 42.
Proposition 3. The solution t to the optimization problem of Eqn 42 is obtained by
ordering the k activities in a non-increasing order of their variance values and then grouping
all two nearest activities according to the order, i.e. Cj = {aj1 , aj2 }, where j = 1, ...b k2 c and
the standard deviations are in the following order:
11  12  21  22 , ...b k c1  b k c2 .
2

2

(43)

Proof. Suppose we have another grouping method t0 , in which all elements in the couple
set are the same as under t except two couples 2 where the ordering is different, i.e., Cm =
{am1 , an2 } and Cn = {am2 , an1 } (m 6= n), where Cm = {am1 , am2 } and Cn = {an1 , an2 }
under t . Without loss of generality, assume m > n and from Eqn 43, we have
m1  m2  n1  n2 .

(44)

Since t0 is supposed to provide a solution which is no less ( defined in Eqn 42) than t ,
i.e.
11 12 + ... + m1 n2 + ... + n1 m2 + ... + b k c1 b k c2
2
2

11 12 + ... + m1 m2 + ... + n1 n2 + ... + b k c1 b k c2 .
2
2
Therefore, we have
m1 n2 + n1 m2  m1 m2 + n1 n2 ,
which is equivalent to: (m1  n1 )(n2  m2 )  0.
This contradicts Eqn 44 (except the case where all standard deviations are equal, in which
case mixing the order does not affect anything). Thus, there exists no such t0 which is
different from t by at least two couples and has better objective value. The general case
2. It should be noted that if there is an ordering change in only one couple, then the method still produces
the same solution because within a couple the variance computation does not consider the order.

60

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

that t0 has multiple (more than two) couples different from t can be easily derived from to
this case (and is omitted due to space constraints).
Hence the proof.
As for analyzing the expected value E[ max {zi }], we apply the same procedure emi=1,...k

ployed to calculate the variance, i.e., based on the group solution returned by the above
optimization problem, we first calculate the expected value for each couple and then, get
the final bound following Eqn 30 and/or Eqn 31 and/or Eqn 32.
At present, we are unable to show the effectivness of our grouping heuristic (Equation 42) analytically in the most general case. However, we show the intuition behind the
grouping heuristic by providing an analytical comparison3 on an example where there are
four activities (normally distributed durations) executed in parallel, i.e. zi  N (0, i ), and
we assume 1  2  3  4 (no loss of generality).
The representation of makespan under our grouping heuristic (denoted as Mheu ) and
random grouping (denoted as Mran ) are, respectively:
Mheu = max{d01 , d02 , d03 , d04 } + max{max{z1 , z2 }, max{z3 , z4 }}
Mran = max{d01 , d02 , d03 , d04 } + max{max{z1 , z4 }, max{z2 , z3 }}.
Let us first examine mean and variance values of Mheu . From Eqn 31, we have
p
E(max{z1 , z2 })  12 p12 + 22
E(max{z3 , z4 })  12 32 + 42 .

(45)

(46)

From Eqn 39, we have
V ar[max(z1 , z2 )]  (1  1 )(12 + 22 )  2 1 2
V ar[max(z3 , z4 )]  (1  1 )(32 + 42 )  2 3 4 .

(47)

From Eqn 30, Eqn 35, Eqn 46 and Eqn 47, we can obtain bounds of mean and variance
values of of Mheu are 4 :
q
p
p
P
1
1
2
2
2
2
E(Mheu )  const + 4 ( 1 + 2 + 3 + 4 ) + 2 ( 45  1 ) 4i=1 i2  2 (1 2 + 3 4 )
(48)
P
V ar(Mheu )  ( 98  1 ) 4i=1 i2  2 (1 2 + 3 4 ).
Similarly, mean and variance values of of Mran can also be calculated,
q
p
p
P
E(Mran )  const + 14 ( 12 + 42 + 22 + 32 ) + 12 ( 54  1 ) 4i=1 i2  2 (1 4 + 2 3 )
(49)
P
V ar(Mran )  ( 89  1 ) 4i=1 i2  2 (1 4 + 2 3 ).
From Eqn 57, bounds of fitness of Mheu (denoted by F itheu ) and Mran (denoted by
F itran ) can then be respectively represented as a function of RHS of Eqn 48 and Eqn 49.
We then examine the difference value between the two bounds, F itheu  F itran . Let us first
compare the first term of RHS of mean values in Eqn 48 and Eqn 49, since
p
p
p
p
2
2
2
( p
32 + 42 )2  ( 12 + p
22 + 32 )2
1 + 2 +
4 +
(50)
2
2
2
2
2
2
2
2
2
2
= 2 1 3 + 2 4 + 1 4 + 2 3  2 1 3 + 22 42 + 12 22 + 32 42
3. The calculation will use the robust fitness function provided in Definition 57 introduced in Section 4.
4. Note that const in Eqn 48 and Eqn 49 is max{d01 , d02 , d03 , d04 }.

61

fiFu, Lau, Varakantham, & Xiao

and from Proposition 3, we have
1 4 + 2 3  1 2 + 3 4 ,

(51)

12 42 + 22 32  (12 22 + 32 42 ) = (1 4 + 2 3 )2  (1 2 + 3 4 )2  0.

(52)

thus,

From Eqn 51, Eqn 52, Eqn 48 and Eqn 49, we have that the bounds of mean and variance
values of Mheu are lower than Mran . Given the robust fitness function in Eqn 57, we
conclude that
F itheu  F itran  0

(53)

which is independent of  and . In other words, our grouping heuristic can provide tighter
fitness bound than random grouping.

4. Robust Fitness Function
The makespan (start time of the dummy sink activity) for the RCPSP/max with durational uncertainty is a function of non-adjustable variables x and random variables representing durational uncertainty z and is represented using S(x, z) for SLA and G(x, z) for
GNLA. Recall that the robust optimization problem is to find the minimum value F  for
which the following probability bound is observed5 :
P (S(x, z)  F  )  (1  )

(54)

From the one-sided Chebyshevs Inequality, we can obtain a bound for the robust objective value F  as a function of its expected value and variance of the adjustable fitness
function, i.e.:
q
q
(55)
V ar[S(x, z)]  F   P (S(x, z)  F  )  (1  )
E[S(x, z)] + 1

Hence, we can reformulate our robust optimization problem as follows:
min F 
s.t.

E[S(x, z)] +

q

q
1


V ar[S(x, z)]  F 

(56)

From this model, we can now derive the robust fitness function which will be used in
our local search framework:
Definition 3. Given 0 <   1 and the adjustable fitness function S(x, z) defined above,
the robust fitness function, f (x, z, ), is defined as
r
q
1
f (x, z, ) = E[S(x, z)] +
V ar[S(x, z)]
(57)

The goal of the local search mechanism is to find a local minima of f . In addition, local
search typically requires the fitness function to be computed many times and hence it is
imperative that the computation of fitness function is efficient.
5. We show the computation of SLA robust fitness function. By substituting S with G, we obtain the
fitness function for GNLA.

62

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

4.1 Schedule Infeasibility of a Given POS
It should be noted that the fitness function, f assumes that any schedule generated by the
POS, x is always executable. However, due to durational uncertainty and the maximum
time lags, the schedule is not always executable. A direct way to measure IP r(POS) the
probability of infeasibility of the POS (i.e. probability that the POS can lead to an infeasible schedule) lies in the computation of the probability of infeasibility of each activity ai
IP r(ai ), that there does not exist a feasible start time such that all temporal constraints
with respect to ai are satisfied. IP r(POS) can be calculated as the probability that at
least one activity is infeasible. However, due to temporal dependencies between activities
providing a theoretical expression for the overall probability of infeasibility is an open problem. Therefore, we propose a simulation approach, where we simulate POS execution over
multiple trials to compute this probability eciently and approximately. As an illustration,
we experimented with the benchmark J10 instances from the PSPLib (Kolisch, Schwindt,
& Sprecher, 1998) for RCPSP/max with additional durational uncertainty that follows a
normal distribution with mean 0 and variance 1. We generated 1000 sample realizations for
the POS obtained from SLA, and check for infeasibility with respect to the original temporal
(including the maximum time lag) constraints. Examples of the probability of infeasibility
obtained by our simulation for PSP1, PSP4, and PSP13 are 0.18, 0.17 and 0.001. However,
for the other problems PSP3, PSP5 etc. the probability of infeasibility was 0, because the
maximal time lags were much larger than the variance of durational uncertainty.

5. Robust Local Search Algorithm
This section will present how the decision rule approximations introduced by SLA, GNLA
are integrated with the robust fitness function and local search mechanisms to provide a
solution for the problems represented by RCPSP/max with durational uncertainty. Our
proposed algorithm is outlined as follows. Steps 1, 2, 5 and 6 are standard steps in a local
search algorithm. Steps 3 and 4 represent our departure from standard local search to deal
with uncertainty.
1. Generate initial solution
This is usually obtained using a simple greedy heuristic.
2. Generate neighborhood of solutions
Generate a pool of neighbor solutions from the current solution.
3. Employ one of the decision rule approximations (SLA and GNLA) for all
adjustable variables and check feasibility
For each candidate solution x in the solution pool, derive the coefficients Ck (x) for
each adjustable variable. Subsequently, for each solution check constraint violation
and reject those that are not feasible.
4. Evaluate robust fitness function f
For each feasible solution x, evaluate f to obtain the robust objective values. The
solution with the lowest robust objective value is the current best robust solution.
63

fiFu, Lau, Varakantham, & Xiao

5. Apply penalty (optional)
Some advanced local search strategies may require a penalty to be applied to prevent
it from being caught at a local minima. In the case of tabu-search for example, a
tabu-list is updated when a tabu move is applied. In the case of iterated local search,
a perturbation move will be applied to the current local minima.
6. Termination criteria
If the termination criteria is met, return the solution with the lowest robust fitness
function value else repeat the optimization cycle by determining the next move.
Algorithm 1 provides the robust local search algorithm guided by decision rule using
 , S  , S  with G , G , G , we obtain the local search algoSLA. By substituting Snow
now
min
min
rithm using GNLA. Given the RCPSP/max with durational uncertainty and the level of risk
(0 <   1), the algorithm returns the POS with the (locally) minimal robust makespan,
S  (or G by GNLA). In essence, we perform robust local search on the neighborhood set
of activity lists. An activity list (al) is defined as a precedence-constraint feasible sequence
that is used by heuristics to generate earliest start time schedules in solving the standard
RCPSP problem (Kolisch & Hartmann, 2005).
Different activity lists are explored by local moves. In our context, we only consider the
activity list as the sequence of activities which satisfy the non-negative minimal time lag
constraint. Due to the existence of maximal time lag constraint in RCPSP/max, scheduling
activities to their earliest possible start time based on the order position in the activity list
may restrict the schedule so much that it may not even return in a feasible schedule. Thus,
when we schedule each activity sequentially based on order position in the activity list, we
will assign its starting time by randomly picking a time from its domain of feasible start
times.
According to our experiments, this new randomized approach returns more feasible
solutions than the earliest start time one. After finding a feasible schedule, a POS will be
generated by applying the chaining procedure proposed by Policella et al. (2004). Then,
the S  (or G by GNLA) value will be computed according to the POS. Intuitively, using
the randomized approach may return a schedule with a large baseline scheduled completion
time. However, we can apply the shortest path algorithm on the resulting POS to generate
the earliest start time schedule for a smaller makespan.
As mentioned above, it may be difficult to find a feasible schedule that satisfies minimal
and maximal time lag constraints using the activity list. In fact, we believe that in the set
of all activity lists, many may not yield a feasible schedule. We overcome this problem as
follows. We define the set of activity lists which result in feasible (or infeasible) schedules
as F (or I). We seek to design a local search algorithm with the following characteristics:
a) Starting from an activity list in I, the local search should move to an activity list in F
within a short time. b) Starting from an activity list in F , the local search should move
to the activity list with the minimal S  (or G by GNLA)value. c) We also diversify the
exploration of activity lists in F by allowing the local search to move from an activity list
in F to an activity list in I, since activity lists in the F region may not be reachable from
one another by simple local moves. This has the flavor of strategic oscillation proposed in
meta-heuristics research.
64

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

Algorithm 1 Robust Local Search
1: Generate an activity list al randomly
2: Find a start time schedule, ss randomly according to al
3: if al  F then
4:
P OS  chaining(ss)

Compute Snow
according P OS
5:


6:
Update Smin as Snow
7: else
8:
Record the first activity a which cannot be scheduled
9: end if
10: for i  1 to Max Iteration do
if al  I then
11:
12:
Shift activity a ahead in al randomly as al
else
13:
14:
Select two activities b and c in al randomly
15:
Swap b and c in al as al
16:
end if
17:
Find randomized start time schedule ss0 according to al
18:
if al0  F then
P OS 0  chaining(ss0 )
19:
20:
Compute S  according to P OS 0

21:
if al  I or S   Snow
then


22:
Snow  S
23:
al  al0

24:
if S   Smin
then


Smin  S
25:
26:
end if
27:
end if
28:
else if al  I then
29:
al  al0
30:
else
31:
p  rand(0, 1)
if p < 0.01 then
32:
33:
al  al0
34:
Record the first activity a which cannot be scheduled
35:
end if
36:
end if
37: end for

65

fiFu, Lau, Varakantham, & Xiao

The detailed robust local search procedure is given in Algorithm 1. The procedure starts
by randomly generating an activity list al, which is a sequence of activities that satisfy the
non-negative minimum time lag constraint (Line 1). In Line 2, a schedule ss is produced
based on ordering of activities in the activity list al. We first perform domain reduction
on the distance graph using the Floyd-Warshall algorithm, so that the feasible range of the
start time for each activity based on the temporal constraints can be obtained. We then
schedule each activity sequentially based on the order position in the activity list. For each
activity, we first pick a start time randomly from the feasible domain and evaluate resource
constraints for the duration of the activity (i.e. check if the current resource capacity exceeds
the resource amount used by that activity). If yes, we set the start time to that activity,
run the shortest path algorithm to reduce domains for the remaining activities, and update
current resource capacity due to consumption of that activity. If the resource constraints
are not satisfied, we will try to set the start time randomly again for a prescribed maximum
numbers of retries. Once the start time of current activity is set, we proceed iteratively
to the next activity according to the activity list. In Line 4, chaining() is employed to
generate a POS from a baseline schedule (section 2.2.4). M ax Iteration refers to the
maximum number of iterations in the robust local search. We apply two different types of
local moves. To converge quickly to an activity list in F, the first local move is designed
to schedule the activity that is causing a temporal or resource conflict to an earlier time.
It will randomly shift ahead the first activity which cannot be scheduled in the current
activity list (Line 12). When an activity list is in F, the second local move will randomly
pick two activities and swap them in the current activity list, while satisfying the nonnegative minimal time lag constraints (Line 14-15). The move will be accepted, if it results
in a smaller or equal S  value (Line 18-29). To explore different activity lists, we include
a small probability to accept the move which leads to an infeasible schedule (Line 31-35).
The probability to move from an activity list in F to one in I is set at 0.01. The minimal
 .
S  value will be saved as Smin

The worst-case computational complexity analysis is given as follows. For each iteration
in local search, there are three major components: randomized schedule generation, POS
construction and fitness calculation. In the process of randomized schedule generation, we
perform domain reduction and resource checking at each iteration, and thus the complexity
is O(N  (N 3 + H  K  w)) where N is the number of activities, H is the maximum planning
horizon, K is the number of types of resources, and w is the prescribed maximum number
of retries for each activity on setting the randomized start time. The POS construction
process works as follows: the set of activities are first sorted according to their start times
in the generated deterministic schedule and the sorting part costs O(N  logN ); then it
proceeds to allocate each activity the total units needed for each type of resource. Let
maxcap be the maximum capacity among all resources. The cost for computing POS is
then O(N  logN + N  K  maxcap ). When determining the fitness value of generated
POS, we examine edge by edge to check if it is connected in parallel or in serial with
respect to its predecessors and it costs O(N + e) where e is the number of edges in POS
(e < N 2 ). Thus, the worst-case complexity of our proposed robust local search algorithm is
O(T N  (N 3 + H  K  w + K  maxcap )) where T is the number of iterations in local search.
66

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

6. Enhancing Robust Local Search
In this section, we describe two enhancements to improve the basic local search method
described in Section 5. Firstly, we describe ordering generation, which is a pre-processing
step used to identify precedence ordering between activities. This precedence ordering is
then used to focus the local search over activity lists. Secondly, we describe a new chaining
method to generate POS from a feasible schedule.
6.1 Ordering Generation
Ordering Generation is a pre-processing step that identifies precedence relationships between
pairs of activities. The key idea is that for certain pairs of activities, it is always better
(with respect to robust makespan) to have the same ordering among activities. Our goal is
to identify these pairs of activities and employ this ordering to focus the local search over
activity lists and in the chaining method used to compute POS from feasible schedule.
In deciding an ordering between a pair of activities, a and b, there are two key steps:
(i) Sample set generation: Generate two sets of m activity lists. The first set consists of m
activity lists where a occurs before b. The second set is generated by swapping activities a
and b in every activity list in the first set; (ii) Order determination: In this step, we first
compute POS and its robust makespan for all activity lists in the two sets. By comparing
the robust makespan values of corresponding activity lists in the two sets, we determine an
ordering between activities. We explain these steps in the following subsections.
For a problem with n activities, there are Cn2 pairs of activities. If we are to decide the
orders between all pairs, the ordering computation needs to be implemented for Cn2 times,
which is computationally expensive. Based on this observation, we first propose a PairsSelection heuristic to selectively choose a certain number of activities pairs whose ordering
can have a significant impact on the robust makespan.
The Pairs-Selection heuristic picks an activity pair: (a) If it is not precedence related in
the original problem definition; and (b) If there exists at least one type of resource, where
the total demand of both activities exceeds the resource capacity. The intuition behind
picking such an activity pair is that those two activities cannot be executed in parallel and
deciding an ordering relationship is imperative to eliminate the resource conflict. One main
advantage of the heuristic is that the number of pairs of activities that need to be ordered
is significantly reduced. Now, we describe the two steps of ordering generation below:
6.1.1 Sample Set Generation
We first randomly generate m activity lists as an initial sample set denoted by T . Each
element in T is an activity list represented as ali which is a sequence of all activities, where
i = 1, ...m, i.e.
T = {ali |ali = (a1 , a2 , ...an ), i  {1, ...m}}.
For each pair of activities (ak , al ) resulting from the Pairs-Selection heuristic, we define
two sample sets represented as T ak al and T al ak . T ak al has all the activity lists that are
in T , except that if an activity list has al before ak , then those activities are swapped.
T ak al = {aliak al |i  {1, ...m}},
67

fiFu, Lau, Varakantham, & Xiao

(
(a1 , a2 , ..., ak , ...al , ...an ) if ali = (a1 , a2 , ..., al , ...ak , ...an )
.
where aliak al =
ali
if ali = (a1 , a2 , ..., ak , ...al , ...an )
Similarly, T al ak can be constructed by incrementally selecting each activity list from
the initial set T with al  ak and reverse the order if ak  al , i.e.
T al ak = {alial ak |i  {1, ...m}},
(
(a1 , a2 , ..., al , ...ak , ...an ) if ali = (a1 , a2 , ..., ak , ...al , ...an )
where alial ak =
.
ali
if ali = (a1 , a2 , ..., al , ...ak , ...an )
Thus, each activity list in the sample set T ak al share the same positions of all activities
except ak and al with the corresponding activity list in set T al ak , where al precedes ak .
6.1.2 Order Determination
We then determine the activity order of each selected pair of activities based on the sample
sets obtained from last phase. For a pair (ak , al ), we construct a new instance by posting
a precedence constraint ak  al or al  ak to the original instance, and based on the new
instance, we determine the fitness which are denoted as fiak al and fial ak for aliak al and
alial ak , respectively.
Note that aliak al and alial ak share the same elements and the same positions except
the order of ak and al . Thus, the order of ak and al can be considered as a reason why the
fitness of aliak al and alial ak differs. To decide the order of ak and al , we define an index
variable denoted as ivak al that measures the percentage of samples where the one with the
order ak proceeds al wins, i.e.
P

ivak al = 

i

min(

a al
a ak
f k
f l
i
i
ak al
a ak ,0)
|f
f l
|
i
i

m

.

We then define an Index Parameter for activities ak and al denoted as IPak al as a
benchmark for the index variable ivak al in determining the order of ak and al . The parameter IPak al can be prescribed by users and different values (usually larger than 50%)
represent different levels of confidence that the order of ak and al matters in causing fitness
variance, and thus also represents different controllability of ivak al .
If the value of index variable ivak al is larger than the value of IPak al , we set the order
ak  al since there indicates a higher probability that a  b can provide better robustness
than b  a; If ivak al is less than 1  IPak al , we set al  ak ; And in other cases, no order
between ak and al is settled.
6.2 Improved Chaining based on Robustness Feedback
As noted in the Preliminaries section, for each activity a, there may exist multiple
choices of resource chains to which it can be assigned. In addition, different chaining heuristics will lead to POSes that can have different robust makespan values. In this section, we
propose a new chaining heuristic that dispatches activities to resource chains by predicting
the improvement in robust makespan of the generated POS.
68

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

Algorithm 2 Robustness-Feedback Resource Chaining (Activity a, Schedule S, Order G)
1: C  Find set of available chains, C for activity a based on S
2: P  Collect chains from C with last activity of chain preceding a in problem
3: O  Collect chains from C with last activity of chain ordered before a in G
4: if P 6=  then
5:
k  Get first available chain in P
6: else if O 6=  then
7:
k  Get first available chain in O
8: else
9:
k  Get first available chain in C
10: end if
11: Post constraint between last activity of chain k (denoted as last(k)) and activity a
12: if a requires more than one resource unit then
13:
C1  chains in C which have last activity as last(k)
14:
C2  C \ C1
15:
for all resource units required by a do
16:
choose the first available chain belonging to C1
17:
if chain above is not feasible then
18:
choose the first available chain belonging to C2
19:
end if
20:
end for
21: end if
In the latest chaining method which aims to increase flexibility as described in Section 2.2.4, the chains are first randomly picked from a superior subset (i.e., chains where
the last activity is already ordered, or chains sharing the same last element). Since our objective is makespan-related and time becomes a concern, we build on the work of Policella
et al. (2009) and pick the first available chain wherever available. The updated chaining
method is called Robustness-Feedback based Resource Chaining.
Example 5. Figure 5 provides the POS provided by this chaining heuristic when used on
Example 1. As can be seen, compared to the POS in 4, the key difference is the allocation
of activity 5 and 6. With our new heuristic, it can be seen that there is more parallelism
and hence reduced robust makespan with high probability.
When employing the Ordering Generation algorithm in conjunction with the chaining
heuristic, we also consider the information about ordered pairs when allocating resource
units to an activity. The motivation is that once activity a and activity b (for example,
a  b) is ordered, there is a high probability that this precedence relationship can result
in a better solution. Algorithm 2 provides the pseudo code for the Robustness-Feedback
Resource Chaining heuristic with Ordering.

7. Experimental Evaluation
In this section, we first evaluate the scalability and quality of the execution strategies
provided by robust local search and the various enhancements introduced in this paper.
69

fiFu, Lau, Varakantham, & Xiao

1

Figure 5: POS by Robustness Feedback Chaining
Secondly, to establish a benchmark on the performance, we compare against the best known
technique for solving JSP problems with durational uncertainty. It should be noted that
the robust local search method is developed to solve RCPSP/max problems with durational
uncertainty and hence does not exploit the structure present in JSP problems. Furthermore,
as described earlier, the optimization metrics of both approaches are different.
7.1 Experimental Setup
We have two sets of problems that we consider and those are described in the subsections
below. Additionally, we also indicate the algorithms that are compared on each of the data
sets in this section.
7.1.1 RCPSP/max with Durational Uncertainty
The problems considered for RCPSP/max with durational uncertainty were obtained by
extending the three benchmark sets available for RCPSP/max problems, J10, J20 and
J30 as specified in the PSPLib (Kolisch et al., 1998). Each set contains 270 problem
instances with duration for each activity ranging between 1 and 10. The maximum number
of activities for J10, J20 and J30 are 10, 20 and 30, respectively. For each activity ai , we set
the expected value d0i of the stochastic duration as the corresponding deterministic duration
given by the benchmarks, and assume that duration uncertainty is normally distributed,
i.e. zi  N (0, ). Henceforth, we refer to J10, J20 and J30 as these RCPSP/max problems
with durational uncertainty. We run the algorithms on problems with four different duration
variabilities  = {0.1, 0.5, 1, 2} and four increasing levels of risk  = {0.01, 0.05, 0.1, 0.2}.
70

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

On RCPSP/max problems with durational uncertainty, we compare the robust local
search that is guided using the two decision rule approximations SLA and GNLA. Furthermore, we also compare the different enhancements to robust local search on RCPSP/max
problems with durational uncertainty. We compare five different variants of robust local
search for each decision rule approximation: (a) (GNLA) refers to basic robust local search
guided by GNLA decision rule approximation; (b) (GNLA+RC) is the robust local search
with the new Robustness-feedback Chaining heuristic guided by GNLA; (c) (GNLA+) refers
to the basic robust local search with additional local search iterations, where the number
of local search iterations is determined based on the problem set (as described later); (d)
(GNLA+OG) is the Order Generation heuristic on top of GNLA guided robust local search;
and finally (e) (GNLA+OG+RC) has both Order Generation and Robustness-feedback
Chaining heuristics on GNLA guided robust local search.
The number of local search iterations for robust local search was set to 1000. To reduce
the stochasticity effects of robust local search, we average over 10 random executions for
each problem instance. Our code was implemented in C++ and executed on a Core(TM)2
Duo CPU 2.33GHz processor under FedoraCore 11 (Kernel Linux 2.6.29.4-167.fc11.i586).
7.1.2 JSP with Durational Uncertainty
For JSPs, (GNLA) is compared against the probabilistic makespan results provided by
Beck and Wilson (2007). For the benchmark problems, we consider the instances generated
using an existing generator in the work of Watson, Barbulescu, Whitley, and Howe (2002)
with durations drawn uniformly from the interval [1,99]. Specifically, we focus on three
sets of probabilistic JSPs of size {44,66,1010} (where a 44 problems consists of 4
jobs consisting of 4 activities each) and for each set, three uncertainty levels {0.1,0.5,1} are
considered.
7.2 Comparison between SLA and GNLA
We first compare the average robust makespan of 270 problem instances obtained by robust
local search that is guided by our decision rule approximations proposed in Section 3.1 and
Section 3.2. We refer to the robust makespan computed using SLA as S  and using GNLA
as G . Figure 6 provides these results for all three sets of RCPSP/max problems with
durational uncertainty. In these results, we show how the robust makespan is affected by
the level of risk  and the standard deviation  of duration uncertainty. X-axis represents
different combinations of risk and standard deviation of durational uncertainty, as shown in
the table of Figure 6. All runs on every instance takes a couple of seconds and hence we do
not report CPU times here. The key observations and conclusions of interest from Figure 6
are as follows:
 Irrespective of the , as the level of risk  increases, the robust makespan decreases
with both SLA and GNLA. Clearly, the lower risk that the planner is willing to take,
the higher is the robust value of the generated execution strategy. Our method is
capable of quantifying this trade off, which can help the planner to decide on the
desired strategies.
71

fiFu, Lau, Varakantham, & Xiao

105

95

G*

S*

85

75

65

55

45
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

(a) Results of J10
146
136

G*

S*

126
116
106
96
86
76
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

(b) Results of J20
170
160

G*

S*

150
140
130
120
110
100
1

2

3

4

5

6

7

8

9

10

11

12

13

14

(c) Results of J30

Figure 6: Comparison of Robustness between SLA and GNLA.
72

15

16

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

 Irrespective of , as the degree of duration variability  increases, the robust makespan
increases with both SLA and GNLA, and the value becomes more sensitive to  when
the level of risk is constrained to a small value (e.g.  = 0.01).
 For lower values of , more specifically for 0.01, S  provides lower values of robust
makespan than G . On the other hand, for higher values of   {0.05, 0.1, 0.2}, G
provides superior performance to S  . We do not yet understand the reason for drop in
performance for  = 0.01, but this is observed consistently across all the RCPSP/max
benchmark problems.
For each problem instance, we also observe some monotonicity between the absolute
difference of robust makespan S  and G and risk values. When the level of risk  takes a
value around 0.02, S  (SLA) has a slightly lower value than G (GNLA). However, when
risk becomes more than 0.02, the superiority of GNLA increases with higher values of risk.
Figure 7 illustrates this on a randomly picked J10 instance with  = 1 and  = 2. The
same pattern is observed across all problem instances of J10, J20 and J30.
105

65

95

60

85

55

75

50

65

S*

45

S*

55

G*

40

G*

45

35

35

30

25

25

0.005 0.01 0.02 0.03 0.04 0.05 0.1 0.15 0.2 0.25 0.3

0.005 0.01 0.02 0.03 0.04 0.05 0.1 0.15 0.2 0.25 0.3

(a) Results from a randomly selected J10 example (b) Results from a randomly selected J10 example
with =1
with =2

Figure 7: Comparison of Robust Makespan.
Next, in Figure 8, we compare the quality of the execution strategies obtained by using
SLA and GNLA. More precisely, we compare the distributions of the actual makespans of
schedules computed using these decision rule approximations. For this purpose, we generate
a set of 100 samples of realizations of durational uncertainty and test with all 270 instances
of each benchmark set with different levels of risk  = 0.2,  = 0.1 and  = 0.05 to obtain
the respective POS, and then compute the actual makespans of schedules derived from the
respective POS under the given realization samples. This difference between real makespans
obtained from POSs generated by two different decision rule approximations was observed
across the board in all examples of the three sets for all values of  except 0.01. We randomly
select three problem instances from each benchmark set and present the results in Figure 8.
Figure 8 also compares the cumulative frequency distributions of the actual makespans. We
observe that GNLA provided far better realized makespans than SLA - both in absolute
terms, as well as distributionally. For J20, except in 2 cases, rest of the actual makespan
73

fiFu, Lau, Varakantham, & Xiao

82

100%
80

80%
78
76

60%

74

40%

72

20%

70

0%

68

69

71

73

75

77

79

81

(a) Results from randomly selected J10 example with  = 0.2

100%

60

80%
55
60%
50
40%
45

20%

40

0%
41

43

45

47

49

51

53

55

57

(b) Results from a randomly selected J20 example with  = 0.1
110
100%
105
80%
100

60%

95

40%
20%

90
0%
85

86 88 90 92 94 96 98 100 102 104 106 108

(c) Results from a randomly selected J30 example with  = 0.05

Figure 8: Comparison of Actual Makespans and Gap between S  and G .(Lines in left pictures from top down indicating: Computed S  , Actual S  by Simulation, Computed G , Actual G by Simulation.)

74

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

values obtained by SLA were higher than the ones obtained by GNLA. Similar trends were
observed for J10 and J30.
To illustrate the difference of quality in absolute of the two upper bounds, we provide
four lines (computed S  , actual S  , computed G and actual G ) indicating the upper
bounds computed using the algorithms and in simulation over the 100 samples.
7.3 Comparing Robust Local Search Enhancements
Since, we have already shown GNLA performs better than SLA, we will only show the performance of our enhancements over GNLA in this section. It should be noted that enhancements over SLA provided similar results and conclusions with GNLA based enhancements
outperforming SLA based enhancements. Since Ordering Generation heuristic requires
additional rounds of robust makespan computation, we also include a benchmark called
(GNLA+) (which is GNLA plus extra iterations of local search) to make a fair comparison.
To avoid the complexity of considering all pairs of activities, we only consider those pairs
of activities where ordering would improve performance. We proposed the Pairs-Selection
heuristic to select these pairs of activities. The number of extra iterations of local search
for the (GNLA+) benchmark is the number of activity pairs picked by the Pairs-Selection
heuristic times the number of samples m used in the Ordering Generation process. The
experimental results shows that the average number of activity pairs of all 270 instances selected under the Pairs-Selection heuristic for J10, J20 and J30 are 5, 14, and 28 respectively.
In our work, we set m = 100. Thus, the extra iterations of the (GNLA+) benchmark for
J10, J20 and J30 are 500, 1400 and 2800, respectively. The performance of all our enhancements is shown in Figure 9(a), Figure 9(b), Figure 9(c) for J10, J20 and J30 respectively.
In all the charts,  is represented on the X-axis and robust makespan on the Y-axis. So,
lower values are better on the Y-axis.
Given below are some key observations and conclusions made from the results:
 Irrespective of the durational uncertainty, (GNLA+RC) and (GNLA+OG) provide
better robust makespan values than both (GNLA) and (GNLA+) for J10 and J30.
This indicates that the new Robustness Feedback Chaining heuristic and the Order
Determination are able to provide more robust partial ordered schedules for J10 and
J30. This improvement seems to increase further with more number of activities, i.e.
the difference is more obvious for instances in J30 than in J10. Furthermore, the
difference is consistently observed across all the problems. However, the improvement
is not consistent for J20 and there are cases where (GNLA+RC) and (GNLA+OG)
did not out perform (GNLA) and (GNLA+). For instance in J20 problems, (GNLA+)
provides better performance than (GNLA+RC) and (GNLA+OG) for  = 0.01 and
 = 1.5.
 The extra iterations of local search in (GNLA+) do not improve the solution quality
much for J10. However, it improves the solution quality for J20 and J30. This could
be because the optimal solution is obtained within 1000 iterations for the smaller
problems.
 In most cases, (GNLA+RC+OG) provides the lowest robust makespan among all the
enhancements. Thus, the OG and RC enhancements in combination do not degrade
75

fiFu, Lau, Varakantham, & Xiao

(a) Results of J10

76

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

(b) Results of J20

77

fiFu, Lau, Varakantham, & Xiao

(c) Results of J30

Figure 9: Comparison of Robust Local Search Enhancements.

78

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

MNPM
CB
G

Problem Size
44
UL=0.1 UL=0.5 UL=1
1.023
1.046
1.128
1.066
1.123
1.282

Problem Size
66
UL=0.1 UL=0.5 UL=1
1.021
1.073
1.168
1.095
1.190
1.273

Problem Size
1010
UL=0.1 UL=0.5 UL=1
1.024
1.101
1.215
1.210
1.225
1.263

Table 2: Comparison against CB solver(UL:Uncertainty Level).
the performance improvement obtained individually. In some cases, the difference is
significant such as in J10 for  = 0.1 and  = 0.01. On the other hand, there are
cases where (GNLA+RC+OG) does not provide the lowest robust makespan, such as
in J20 for  = 0.5 and  = 0.01.
7.4 Comparing on JSPs with Durational Uncertainty
In this section, we compare the performance of our GNLA approach (referred to as G )
with the best known solver for Job Shop Scheduling Problems proposed by Beck and Wilson (2007) (referred to as CB). For a fair comparison of the two approaches, we employ the
Mean Normalized Makespan (MNPM) metric defined by Beck and Wilson:
M N P M (a, L) =

1 X D(a, l)
|L|
Dlb (l)

(58)

lL

where L is a set of problem instances, D(a, l) is the probabilistic makespan (i.e., robust
makespan in our work) for instance l by algorithm a generated by Monte Carlo simulation,
Dlb (l) is a lower bound on the probabilistic makespan.
We denote the best MNPM values aross different algorithms reported by Beck and
Wilson as CB. We compare them with the MNPM values in our work which are obtained
by replacing D(a, l) in Eqn 58 with an upper bound of robust makespan from the POS
generated from GNLA-guided local search. All runs on 4  4 and 6  6 instances took less
than a minute, while 10  10 instances took about 15 minutes.
Table 2 provides the results. The performance of our solver is comparable to CB solver
across all problem instances. This comparison illustrates that our local search mechanism
is generic (different types of scheduling problems) and is also able to provide performance
on par with near optimal approaches. While the performance is comparable, CB provides
better MNPM values than our approach due to the following key reasons: (a) Our approach
does not exploit the structure specific to JSPs (jobs consisting of a sequence of operations).
We hope to improve our approach to exploit this in the near future. (b) Our robust local
search reasons with upper bounds (due to Chebyshev inequality), which can be loose.

8. Related Work
The Resource-Constrained Project Scheduling Problem with minimum and maximum time
lags, RCPSP/max, (or known as the Resource-Constrained Project Scheduling Problem
with Generalized Precedence Relations, RCPSP-GSR) is a strongly NP-hard combinatorial optimization problem; and even the decision problem of determining whether an
79

fiFu, Lau, Varakantham, & Xiao

RCPSP/max instance has a feasible solution or not is NP-complete (Bartusch et al., 1988).
A survey of recent developments and new applications for RCPSP/max has been given by
Neumann, Schwindt, and Zimmermann (2006).
However, we did not find much study that considers RCPSP/max under uncertainty.
One such paper dealing with variable durations on RCPSP/max is done by Lombardi and
Milano (2009), where activity durations range between given lower and upper bounds.
A precedence constraint posting approach (Policella, Cesta, Oddi, & Smith, 2007) was
adopted. Whereas in our work, we consider RCPSP/max with durational uncertainty where
each activity duration is modeled as a random variable with known mean and variance
values.
Research on scheduling under uncertainty has received much attention in both Artificial
Intelligence and Operations Research communities. For a complete survey of recent AI
papers on robust project scheduling up to 2004, one may refer to the work of Herroelen
and Leus (2005) and of production scheduling (Aytug, Lawley, McKay, Mohan, & Uzsoy,
2005). Broadly, one may classify the techniques to tackle scheduling with uncertainty into
two categories: Proactive Scheduling is to design a priori schedule or a schedule policy that
take into account the possible uncertainty that may occur; Reactive Scheduling modifies
or re-optimizes the baseline schedule when an unexpected event occurs. Here our interest
is on proactive scheduling and we are concerned with robust scheduling which focuses on
obtaining proactive schedules that maintain a high level of performance against uncertainty.
The main idea of proactive techniques is to build a global solution which hopefully does
not need to be revised at execution time. One can divide the research in this area into three
categories, according to how and when the information of uncertainties can be taken into
account in generating more robust and stable schedules than they would be without using
this information (Bidot, Vidal, Laborie, & Beck, 2009): 1. generating one complete generic
schedule which is proved to execute correctly in most scenarios arising during execution;
2. generating a flexible solution in which some decisions are postponed to be made until
execution; 3. generating a conditional solution in which mutually exclusive decisions are
developed,the one being chosen dependent on some observations during execution, like
markov decision processes. In the following, we briefly look at the first two cases since
they are related to our work.
8.1 Generating Generic Schedule
A first method for making a generic schedule that is insensitive to online perturbations is to
produce a complete and robust schedule by taking into account all possible scenarios, i.e. a
schedule with strong controllability (Vidal & Fargier, 1999). Rather than dealing with execution with 100% confidence, probabilistic techniques have been proposed that build schedules
with a probabilistic guarantee against a threshold value of an optimization metric such as
makespan. Another example of such generic schedule generation is fuzzy scheduling (Herroelen & Leus, 2005): instead of stochastic variables and probabilistic distributions, fuzzy
set scheduling use fuzzy numbers for modeling uncertainties based on possibility theory;
a recent work by Rodrguez et al. (2009) modeled uncertain durations as fuzzy numbers
and improved local search to solve the Job Shop Scheduling Problem. In the following,
80

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

we provide further details on the work related to strong controllability and probabilistic
techniques.
8.1.1 Strong Controllable Techniques
Strong Controllability was introduced by Vidal and Fargier (1999) over Simple Temporal
Networks with Uncertainty (STNU) for which controllability is achievable in polynomial
time. With the existence of uncontrollable events that are controlled by exogenous factors, often referred to as Nature, an STNU is strongly controllable if there exists at least
one universal schedule that suits any situation. Such schedule might be computed off-line
beforehand. Strong controllability is the strictest form of STNU. A strongly controllable
network means that the schedule can be executed without regard to the contingent events.
It is useful in applications where contingent events cannot be observed exactly.
8.1.2 Probabilistic Techniques
Instead of generating a global solution suitable for all realizations of uncertainties, probabilistic techniques build a schedule that has a probabilistic guarantee of a deterministic
optimization measure with respect to a threshold value, e.g., find the schedule with the
highest probability that the project makespan will not exceed a particular value.
Daniels and Carrillo (1997) defined a -robust schedule as one that has maximum probability of achieving a given performance level, e.g., the total flow time is no greater than a
given threshold. They presented branch-and-bound and heuristic techniques to find a robust schedule in a one-machine manufacturing context that performs the best within a given
confidence level. As for the Job Shop Scheduling Problem, Beck and Wilson (2007) consider
activity durations as random variables; given a level of risk 0    1, they are interested
in a solution of minimal (probabilistic) makespan which has a probability of execution of
at least 1  .
8.2 Generating Flexible Schedule
Another way of producing robust schedule taking into account of uncertainty is to introduce
flexibility into the schedule. The idea is that only a subset of decisions are made offline and
the rest are postponed to be made online, so that decisions are only made when information
becomes more precise and certain (Bidot et al., 2009). In the following, we discuss three
subcategories of works that deal with generating flexible schedules.
8.2.1 Dynamic Controllable Techniques
An STNU is Dynamic Controllable (Vidal & Fargier, 1999) if there exists a solution that can
always be instantiated incrementally based on the outcomes of contingent edges in the past.
An execution strategy using dynamic controllability is needed to produce an incremental
solution based on the subsequent revelation of contingent events. Morris and Muscettola
(2005) proposed a pseudo-polynomial algorithm to handle dynamic controllability of STNUs
based on constraint satisfaction. Techniques were proposed by Wah and Xin (2004) to
optimize the bounds on durations of contingent edges such that the resulting STNU is
dynamic controllable.
81

fiFu, Lau, Varakantham, & Xiao

8.2.2 Redundancy-based Techniques
Redundancy-based scheduling is another proactive technique for scheduling. The idea is
to generate a schedule that includes the allocation of extra resources and/or time in the
schedule so that these buffers will help absorb the impact of unexpected events without
rescheduling during execution. Davenport, Gefflot, and Beck (2001) proposed techniques
for generating robust schedules based on the insertion of temporal slacks to critical activities that are allocated on possibly breakable resources. Lambrechts, Demeulemeester, and
Herroelen (2010) analytically determined the expected increase in activity duration due to
resource breakdown. Based on this information, simulation-based time buffering was used
to protect the schedule from disruptions caused by resource availability.
8.2.3 Partial Order Schedule (POS)
Even with buffering, baseline schedules may become brittle in face of unpredictable execution dynamics and can quickly get invalidated. Instead of baseline schedule, another line
of work is to consider design of good schedule policies. One such example is the notion
of Partial Order Schedules (POS) defined by Policella et al. (2004) which seeks to retain
temporal flexibility whenever the problem constraints allow it and can often absorb unexpected deviation from predictive assumptions. They considered robustness measures such
as fluidity and flexibility. Generating POS is another example of such flexible approaches: a
subset of sequencing decisions are made offline and the remaining decisions are made online
by using a dispatching rule (Bidot et al., 2009). Different methods of generating POS were
compared in terms of the robustness of the resulting schedules in the work of Rasconi, Cesta,
and Policella (2010). In our work, we apply the concept of POS as the execution policy.
Given an RCPSP/max instance, mean and variance values of the segregated variables of
data perturbations and the level of risk, the objective of our work is to determine POS with
a locally minimal robust value.
8.3 Scenario-based Optimization in Scheduling
Another line of work that deals with scheduling under uncertainty is based on the use of
scenarios (scenario-based optimization). For example, Kouvelis, Daniels, and Vairaktarakis
(2000) introduced the concept of robustness into scheduling problems. They considered uncertain processing times and proposed methods to generate a robust schedule based on the
maximum absolute deviation between the robust solution against all possible scenarios in a
given scenario set. A shortcoming of this kind of approach is that all scenarios are assumed
to be known in advance, and that the scenario space is usually exponentially large. Noteworthy of mention are the two notions of solution robustness and quality robustness, where
solution robustness (or stability) refers to the insensitivity of actual start times, whereas
quality robustness refers to the insensitivity of solution quality (i.e. makespan) to different
scenarios (Herroelen & Leus, 2005). Another pioneering scenario-based optimization work
is by Mulvey, Vanderbei, and Zenios (1995) which handles the tradeoff between solution
robustness (if a solution remains close to the optimal for all scenarios) and model robustness
(if a solution remains feasible for most scenarios).
82

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

8.4 Robust Optimization in Scheduling
A recent development in Operations Research saw the potential of applying the concept of
Robust Optimization to deal with uncertainty. Ben-Tal and Nemirovski (2002) and Bertsimas and Sim (2003) proposed robust optimization models where no assumptions of the
underlying probability distribution of data are needed. The idea is often to approximate
data uncertainty by a tractable (convex) uncertainty set, and optimization is performed on
that set. This results in a robust counterpart formulation as a conic (such as second-order
cone) optimization problem which can be solved in polynomial time. However, only a few
works have been reported in the literature on applying robust optimization to scheduling,
due mainly to a high-degree combinational nature of the problem. One such application
is the process scheduling problem in chemical engineering, such as the works by Janak,
Lin, and Floudas (2007) and Li and Ierapetritou (2008). A notable recent breakthrough
in robust optimization on tractable approximation models to solve stochastic optimization
problems is found by Chen et al. (2008). This work makes use of linear segregated decision rules that are relevant to solving combinatorial scheduling problems with durational
uncertainty and our work exploit this mechanism and incorporate it into local search.

9. Conclusion
Given a level of risk 0 <   1 chosen by the planner, we investigated the problem of finding
the minimum (1  )-guaranteed makespan (i.e. Robust Makespan) and proposed methods
to find a schedule policy (POS) such that when uncertainty is dynamically realized, the
execution policy will result in a solution whose value is as good as robust makespan. We
first put forward a new decision rule utilized in scheduling to help specify the start times for
all activities with respect to execution policy and dynamic realizations of data uncertainty.
Based on the decision rule, new fitness function was then derived to evaluate robustness,
which was finally integrated into a local search framework to produce the solution with
robust makespan. Experimental results illustrate the improved performance of local search
with the new fitness evaluation, which provider tighter bounds on robust makespan and
better partial order schedules compared to the existing method.
For simplicity we have adopted an upper bound approach where we assume independence among the durational uncertainties. One future work is to treat correlations between
durational uncertainties, since a task duration could be correlated with some others in real
life. For example, correlations occur when an external event is not peculiar to a single
task, but more universal, such as weather conditions, seasonal peaks. In such situations,
the durational delays are correlated in the same direction. When this occurs, the decision
rules proposed in this paper break down unfortunately, since even if the covariances of
pairs of duration variables are given, it is very complex to analytically model the extent
to which one duration and any combination (resulting from SUM and MAX operators)
of other durations change together. This in turn complicates the analysis on the variance
of the makespan variable, and hence the robust makespan. Extending our work to handle
covariances is an interesting future direction.

83

fiFu, Lau, Varakantham, & Xiao

Acknowledgments
This paper extends previous research by Lau, Ou, and Xiao (2007) and Fu, Varakantham,
and Lau (2010). The authors wish to thank all reviewers for their insightful comments.

References
Aytug, H., Lawley, M. A., McKay, K., Mohan, S., & Uzsoy, R. (2005). Executing production schedules in the face of uncertainties: A review and some future directions. In
European Journal of Operational Research, Vol. 165(1), pp. 86110.
Bartusch, M., Mohring, R. H., & Radermacher, F. J. (1988). Scheduling project networks
with resource constraints and time windows. Annals of Operations Research, 16 (1-4),
201240.
Beck, J. C., & Wilson, N. (2007). Proactive algorithms for job shop scheduling with probabilistic durations. Journal of Artificial Intelligence Research, 28 (1), 183232.
Ben-Tal, A., & Nemirovski, A. (2002). Robust optimization - methodology and applications.
Mathematical Programming, 92, 453480.
Bertsimas, D., & Sim, M. (2003). Robust discrete optimization and network flows. Mathematical Programming, 98, 4971.
Bidot, J., Vidal, T., Laborie, P., & Beck, J. C. (2009). A theoretic and practical framework
for scheduling in a stochastic environment. Journal of Scheduling, 12, 315344.
Chen, X., Sim, M., Sun, P., & Zhang, J. (2008). A linear decision-based approximation
approach to stochastic programming. Operations Research, 56 (2), 344357.
Clark, C. E. (1961). The Greatest of a Finite Set of Random Variables. Operations Research,
9, 145162.
Daniels, R., & Carrillo, J. (1997). Beta-robust scheduling for single-machine systems with
uncertain processing times. IIE Transactions, 977985.
Davenport, A. J., Gefflot, C., & Beck, J. C. (2001). Slack-based techniques for robust
schedules. In Proceedings of the 6th European Conferences on Planning (ECP).
Dean, B. C., Goemans, M. X., & Vondrak, J. (2004). Approximating the stochastic knapsack
problem: The benefit of adaptivity. In FOCS, pp. 208217.
Demeulemeester, E. L., & Herroelen, W. S. (2002). Project scheduling : a research handbook.
Kluwer Academic Publishers, Boston.
Fu, N., Varakantham, P., & Lau, H. C. (2010). Towards finding robust execution strategies
for rcpsp/max with durational uncertainty. In Proceedings of International Conference
on Automated Planning and Scheduling (ICAPS), pp. 7380.
84

fiRobust Local Search for Solving RCPSP/max with Durational Uncertainty

Hagstrom, J. N. (1988). Computational complexity of pert problems. Networks, 18, 139
147.
Herroelen, W., & Leus, R. (2005). Project scheduling under uncertainty: Survey and
research potentials. In European Journal of Operational Research, Vol. 165(2), pp.
289306.
Janak, S., Lin, X., & Floudas, C. (2007). A new robust optimization approach for scheduling
under uncertainty :ii. uncertainty with known probability distribution. Computers and
Chemical Engineering, 31, 171195.
Kolisch, R., & Hartmann, S. (2005). Experimental investigation of heuristics for resourceconstrained project scheduling: An update.. European Journal of Operational Research.
Kolisch, R., Schwindt, C., & Sprecher, A. (1998). Benchmark Instances for Project Scheduling Problems, pp. 197212. Kluwer Academic Publishers, Boston.
Kouvelis, P., Daniels, R. L., & Vairaktarakis, G. (2000). Robust scheduling of a two-machine
flow shop with uncertain processing times. IIE Transactions, 32, 421432.
Lambrechts, O., Demeulemeester, E., & Herroelen, W. (2010). Time slack-based techniques
for robust project scheduling subject to resource uncertainty. Open access publications
from katholieke universiteit leuven urn:hdl:123456789/272147, Katholieke Universiteit
Leuven.
Lau, H. C., Ou, T., & Xiao, F. (2007). Robust local search and its application to generating
robust schedules. In Proceedings of International Conference on Automated Planning
and Scheduling (ICAPS), pp. 208215.
Li, Z., & Ierapetritou, M. G. (2008). Robust optimization for process scheduling under
uncertainty. Industrial and Engineering Chemistry Research, 47 (12), 41484157.
Lombardi, M., & Milano, M. (2009). A precedence constraint posting approach for the
rcpsp with time lags and variable durations. In Proceedings of the 15th international
conference on Principles and practice of constraint programming, CP09, pp. 569583
Berlin, Heidelberg. Springer-Verlag.
Mohring, R. H. (2001). Scheduling under uncertainty: Bounding the makespan distribution.
In Computational Discrete Mathematics, pp. 7997.
Mohring, R. H., & Stork, F. (2000). Linear preselective policies for stochastic project
scheduling. Mathematical Methods of Operations Research, 52 (3), 501515.
Morris, P., & Muscettola, N. (2005). Temporal dynamic controllability revisited. In Proceedings of the 20th National Conference on Artificial Intelligence, pp. 11931198. AAAI
Press.
Mulvey, J. M., Vanderbei, R. J., & Zenios, S. J. (1995). Robust optimization of large-scale
systems. Operations Research, 43.
85

fiFu, Lau, Varakantham, & Xiao

Neumann, K., Schwindt, C., & Zimmermann, J. (2006). Resource-constrained project
scheduling with time windows. International Series in Operations Research and Management Science, 92, 375408.
Policella, N., Cesta, A., Oddi, A., & Smith, S. (2009). Solve-and-robustify. Journal of
Scheduling, 12, 299314. 10.1007/s10951-008-0091-7.
Policella, N., Cesta, A., Oddi, A., & Smith, S. F. (2007). From precedence constraint posting
to partial order schedules: A csp approach to robust scheduling. AI Communications,
20, 163180.
Policella, N., Smith, S. F., Cesta, A., & Oddi, A. (2004). Generating robust schedules
through temporal flexibility.. In Proceedings of International Conference on Automated Planning and Scheduling (ICAPS), pp. 209218.
Rasconi, R., Cesta, A., & Policella, N. (2010). Validating scheduling approaches against
executional uncertainty. Journal of Intelligent Manufacturing, 21 (1), 4964.
Rodrguez, I. G., Vela, C. R., Puente, J., & Hernandez-Arauzo, A. (2009). Improved local
search for job shop scheduling with uncertain durations. In Proceedings of International Conference on Automated Planning and Scheduling (ICAPS).
Vidal, T., & Fargier, H. (1999). Handling contingency in temporal constraint networks: from
consistency to controllabilities. Journal of Experimental and Theoretical Artificial
Intelligence, 11, 2345.
Vonder, S., Demeulemeester, E., & Herroelen, W. (2007). A classification of predictivereactive project scheduling procedures. Journal of Scheduling, 10 (3), 195207.
Wah, B. W., & Xin, D. (2004). Optimization of bounds in temporal flexible planning
with dynamic controllability. IEEE International Conference on Tools with Artificial
Intelligence, 0, 4048.
Watson, J.-P., Barbulescu, L., Whitley, L. D., & Howe, A. E. (2002). Contrasting structured
and random permutation flow-shop scheduling problems: Search-space topology and
algorithm performance. INFORMS Journal on Computing, 14, 98123.
Wu, C. W., Brown, K. N., & Beck, J. C. (2009). Scheduling with uncertain durations: Modeling beta-robust scheduling with constraints.. Computers and Operations Research,
36, 23482356.

86

fi