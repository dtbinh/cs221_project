Journal of Artificial Intelligence Research 54 (2015) 535-592

Submitted 07/15; published 12/15

Pay-As-You-Go Description Logic Reasoning
by Coupling Tableau and Saturation Procedures
Andreas Steigmiller
Birte Glimm

andreas.steigmiller@uni-ulm.de
birte.glimm@uni-ulm.de
Institute of Artificial Intelligence, University of Ulm, Germany

Abstract
Nowadays, saturation-based reasoners for the OWL EL profile of the Web Ontology
Language are able to handle large ontologies such as SNOMED very efficiently. However,
it is currently unclear how saturation-based reasoning procedures can be extended to very
expressive Description Logics such as SROIQthe logical underpinning of the current
and second iteration of the Web Ontology Language. Tableau-based procedures, on the
other hand, are not limited to specific Description Logic languages or OWL profiles, but
even highly optimised tableau-based reasoners might not be efficient enough to handle large
ontologies such as SNOMED. In this paper, we present an approach for tightly coupling
tableau- and saturation-based procedures that we implement in the OWL DL reasoner
Konclude. Our detailed evaluation shows that this combination significantly improves the
reasoning performance for a wide range of ontologies.

1. Introduction
The current version of the Web Ontology Language (OWL 2) (W3C OWL Working Group,
2009) is based on the very expressive Description Logic (DL) SROIQ (Horrocks, Kutz,
& Sattler, 2006). Sound and complete tableau algorithms, which are easily extensible and
adaptable, are typically used to handle (standard) reasoning tasks. Moreover, the use
of a wide range of optimisation techniques allows for handling many expressive, real-world
ontologies. Since standard reasoning tasks for SROIQ have N2EXPTIME-complete worstcase complexity (Kazakov, 2008), it is, however, not surprising that larger ontologies easily
become impractical for existing systems.
In contrast, the OWL 2 profiles define language fragments of SROIQ for which the
standard reasoning tasks are tractable and specialised reasoning procedures are available.
For example, the OWL 2 EL profile is based on the DL EL++ , which can efficiently be
handled by completion- or consequence-based reasoning procedures (Baader, Brandt, &
Lutz, 2005; Kazakov, 2009). These saturation algorithms have also been extended to handle
more expressive DLs such as Horn-SHIQ (Kazakov, 2009) for which they are often able to
outperform the more general tableau algorithms. Even saturation procedures for DLs with
non-deterministic language features, such as ALCI (Simanck, Kazakov, & Horrocks, 2011),
ALCH (Simanck, Motik, & Horrocks, 2014), SHIQ (Bate, Motik, Cuenca Grau, Simanck,
& Horrocks, 2015), have been developed and some of their implementations into reasoning
systems show a remarkable performance. In particular, they allow for a one-pass handling
of several reasoning tasks such as classification (i.e., the task of arranging the classes of
an ontology in a hierarchy), whereas the idea of tableau procedures is based on pairwise
testing individual class subsumptions. Although optimised classification algorithms have
c
2015
AI Access Foundation. All rights reserved.

fiSteigmiller & Glimm

been developed for tableau-based reasoning systems (Baader, Hollunder, Nebel, Profitlich,
& Franconi, 1994; Glimm, Horrocks, Motik, Shearer, & Stoilos, 2012), they still use a
multitude of separate consistency tests in order to decide subsumption relations. Since
a complete handling of DLs providing disjunctions, cardinality restrictions, and inverse
roles causes several difficulties, saturation procedures have not yet been extended to very
expressive DLs such as SROIQ. Hence, only the less efficient tableau-based reasoning
systems can currently be used to handle ontologies with these more expressive language
features.
Unfortunately, a combination of tableau algorithms and saturation procedures is not
straightforward since both techniques work quite differently. Hence, ontology engineers
have to decide whether they only use the restricted features of certain language fragments
such that their ontologies can be handled by specialised reasoners with saturation-based
procedures or they have to face possible performance losses by using more general reasoning
systems based on tableau algorithms. This is especially unfavourable if such language
features are only required for a few axioms in the ontology. In this case, completeness
is no longer ensured for the specialised procedures, while the fully-fledged, tableau-based
reasoners are possibly not efficient enough. Ideally, reasoning systems with a better pay-asyou-go behaviour could be used, where the part of the ontology that is not affected by the
axioms outside the tractable fragment can still be handled efficiently. This led to the recent
development of approaches that combine saturation procedures and fully-fledged reasoners
in a black box manner (Armas Romero, Cuenca Grau, & Horrocks, 2012; Song, Spencer,
& Du, 2012; Zhou, Nenov, Cuenca Grau, & Horrocks, 2014; Zhou, Cuenca Grau, Nenov,
Kaminski, & Horrocks, 2015). These approaches try to delegate as much work as possible
to the specialised and more efficient reasoner, which allows for reducing the workload of the
fully-fledged tableau algorithm, while still guaranteeing completeness.
In this paper, we present a much tighter coupling between saturation and tableau algorithms, whereby further performance improvements are achieved. After introducing some
preliminaries (Section 2), we present a saturation procedure that is adapted to the data
structures of a tableau algorithm (Section 3). This allows for easily passing information
between the saturation and the tableau algorithm within the same reasoning system. Moreover, the saturation partially handles features of very expressive DLs in order to efficiently
derive as many consequences as possible (Section 3.1). We then show how parts of the
ontology can be identified for which the saturation procedure is possibly incomplete and
where it is necessary to fall-back to the tableau procedure (Section 3.2). Subsequently, we
present several optimisations that are based on passing information from the saturation to
the tableau algorithm (Section 4) and back (Section 5). Finally, we discuss related work
(Section 6) and present the results of a detailed evaluation including comparisons with other
approaches and state-of-the-art reasoners (Section 7) before we conclude (Section 8).
This paper is based on a previous conference publication (Steigmiller, Glimm, & Liebig,
2014a), but contains significantly extended explanations, additional examples, and proofs
for the correctness of the integrated saturation procedure with its incompleteness detection.
Due to the space limitations of the conference publication, the information passing between
the tableau algorithm and the saturation procedure could only be sketched, whereas it is
described in detail in this paper. Moreover, the coupling with the saturation procedure
has been extended in this paper to consider and handle all language features of the DL
536

fiPay-As-You-Go Description Logic Reasoning

SROIQ. Furthermore, we show a new and more detailed evaluation that is based on an
updated implementation and we compare the results with other approaches where fullyfledged and specialised reasoners are combined.

2. Preliminaries
In order to describe our techniques in more detail, we first give, based on the original presentation of Horrocks et al. (2006), a brief introduction into the DL SROIQ in this section.
For a detailed introduction to DLs, we refer to the Description Logic Handbook (Baader,
Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007). Subsequently, we describe a tableau algorithm as it is typically used by reasoning systems and also refer to the work of
Horrocks et al. (2006) for further details.
2.1 The Description Logic SROIQ
We first define the syntax of roles, concepts (also called classes), and individuals, and then
we go on to axioms and ontologies/knowledge bases. Additionally, we sketch typically
used restrictions for the combination of the different axioms, which are necessary to ensure
decidability for many inference problems of SROIQ. Note that we do not describe all
details of these restrictions since they are well-known in the DL literature (Horrocks et al.,
2006) but not particularly relevant for the proposed optimisation techniques. Subsequently,
we define the semantics of these components.
Definition 1 (Syntax of Individuals, Concepts, and Roles). Let NC , NR , and NI be countable, infinite, and pairwise disjoint sets of concept names, role names, and individual
names, respectively. We call  = (NC , NR , NI ) a signature. The set Rols() of SROIQroles over  (or roles for short) is NR  {r | r  NR }, where a role of the form r is
called the inverse role of r. Since the inverse relation on roles is symmetric, we can define a
function inv, which returns the inverse of a role and, therefore, we do not have to consider
roles of the from r . For r  NR , let be inv(r) = r and inv(r ) = r.
The set of SROIQ-concepts (or concepts for short) over  is the smallest set built
inductively over symbols from  using the following grammar, where a  NI , n  IN0 , A 
NC , and r  Rols():
C ::= > |  | A | {a} | C | C1 u C2 | C1 t C2 | r.C | r.C | r.Self | > n r.C | 6 n r.C.
The roles, concepts, and individuals are used to build ontology axioms as follows:
Definition 2 (Syntax of Axioms and Ontologies). For C, D concepts, a general concept
inclusion (GCI) axiom is an expression C v D. A finite set of GCIs is called a TBox.
A role inclusion (RI) axiom is an expression of the form u v r, where r is a role and u
is a composition of roles, i.e., u = s1  . . .  sn with the roles s1 , . . . , sn and n  1. For r, s
roles, a role assertion (RA) axiom is of the form Disj(r, s). An RBox is a finite set of RIs
and RAs. Given an RBox R, we use v as the transitive-reflexive closure over all r v s
and inv(r) v inv(s) axioms in R. We call a role r a sub-role of s and s a super-role of r if
r v s.
An (ABox) assertion is an expression of the form C(a) or r(a, b), where C is a concept,
r is a role, and a, b  NI are individual names. An ABox is a finite set of assertions. A
537

fiSteigmiller & Glimm

knowledge base or ontology K is a tuple (T , R, A) with T a TBox, R an RBox, and A
an ABox.
Note, it is also possible to allow other types of RAs for the RBox, e.g., axioms that
specify roles as asymmetric, irreflexive, transitive, reflexive, or symmetric. However, such
axioms can be expressed indirectly in other ways and, therefore, we omit their presentation
here. For example, an axiom of the form Refl(r), for which the role r has to be interpreted
as reflexive, can be encoded with the axioms r0 v r and > v r0 .Self.1 Analogously, we
only allow the most frequently used ABox assertions since, in the presence of nominals, all
ABox assertion can also be expressed with GCIs (which we also utilise below to eliminate all
ABox assertions to simplify the presentation of algorithms). Furthermore, SROIQ usually
allows the usage of the universal role u, but u can also be simulated by a fresh transitive,
reflexive, and symmetric super role, i.e., a role that is implied by all other roles. In the
following, we use K also as an abbreviation for the collection of all axioms in the knowledge
base. For example, we write C v D  K instead of C v D  T and T  K.
In order to ensure decidability (Horrocks, Sattler, & Tobies, 1999; Horrocks & Sattler,
2004), only simple roles are allowed in concepts of the form > n r.C, 6 n r.C, and r.Self
and in axioms of the form Disj(r, s), where, roughly speaking, a role is simple if it is not
implied by any RI that uses role composition. Furthermore, the RBox has to be regular,
i.e., RI axioms are only allowed in a limited form (Horrocks & Sattler, 2004), which restricts
cyclic dependencies between RIs.
Next, we define the semantics of concepts and then we go on to the semantics of axioms
and ontologies/knowledge bases.
Definition 3 (Semantics of Individuals, Concepts, and Roles). An interpretation I =
(I , I ) consists of a non-empty set I , the domain of I, and a function I , which maps
every concept name A  NC to a subset AI  I , every role name r  NR to a binary
relation rI  I  I , and every individual name a  NI to an element aI  I . For
I
each role name r  NR , the interpretation of its inverse role (r ) consists of all pairs
h,  0 i  I  I for which h 0 , i  rI .
For any interpretation I, the semantics of SROIQ-concepts over a signature  is defined by the function I as follows:
>I
(C)I
(r.Self)I
(r.C)I
(r.C)I
(6 n r.C)I
(> n r.C)I

=
=
=
=
=
=
=

I
I \ C I
{  I
{  I
{  I
{  I
{  I

I = 
({a})I =
I
I
I
(C u D) = C  D
(C t D)I =
I
| h, i  r }
| if h,  0 i  rI , then  0  C I }
| there is a h,  0 i  rI with  0  C I }
| ]{ 0  I | h,  0 i  rI and  0  C I }  n}
| ]{ 0  I | h,  0 i  rI and  0  C I }  n},

{aI }
C I  DI

where ]M denotes the cardinality of the set M .
Finally, we can define the semantics of ontologies/knowledge bases.
1. Note that we have to use the fresh sub-role r0 of r in the axiom > v r0 .Self since r might be complex,
but r.Self expressions are only allowed with simple roles.

538

fiPay-As-You-Go Description Logic Reasoning

Definition 4 (Semantics of Axioms and Ontologies). Let I = (I , I ) be an interpretation,
then I satisfies a TBox/RBox axiom or ABox assertion , written I |=  if
1.  is a GCI C v D and C I  DI , or
2.  is an RI s1  . . .  sn v r and sI1  . . .  sIn  rI , where  denotes the composition of
binary relations for sI1  . . .  sIn , or
3.  is an RA of the form Disj(r, s) and rI  sI = , or
4.  is an ABox assertion C(a) and aI  C I , or
5.  is an ABox assertion r(a, b) and haI , bI i  rI .
I satisfies a TBox T (RBox R, ABox A) if it satisfies each GCI in T (each RI/RA axiom
in R, each assertion in A). We say that I satisfies K = (T , R, A) if I satisfies T , R, and
A. In this case, we say that I is a model of K and we write I |= K. We say that K is
consistent if K has a model.
2.2 Normalisation and Preprocessing
In the remainder we assume that each knowledge base is normalised into the following form:
1. The TBox contains only axioms of the form A1 u A2 v C and H v C with H = A,
H = {a}, or H = >, where C is in negation normal form and A, A1 , A2 denote concept
names.
2. The RBox contains only RAs and simple role inclusion axioms of the form r1 v r2 .
3. The ABox is empty.
This assumption is without loss of generality. For Item 1: Any concept can be transformed
into an equivalent one in negation normal form (NNF) by pushing negation inwards, making
use of de Morgans laws and the duality between existential and universal restrictions, and
between at-most and at-least cardinality restrictions (Horrocks, Sattler, & Tobies, 2000).
We use nnf(C) to denote the equivalent concept to C in NNF. Furthermore, a GCI C v
D  T that does not correspond to the normal form can equivalently be written as > v
nnf(C tD). This rewriting of GCIs creates (possibly many) disjunctions, which potentially
causes a lot of non-determinism in the reasoning procedure and, therefore, easily decreases
the reasoning performance. To counteract this, a preprocessing step called absorption is
often used (Horrocks & Tobies, 2000; Hudek & Weddell, 2006; Steigmiller, Glimm, & Liebig,
2013, 2014b; Tsarkov & Horrocks, 2004), which tries to rewrite the axioms into possibly
several simpler concept inclusion axioms. For example, instead of treating A u r.B v C as
> v A t r.(B) t C, a sophisticated absorption algorithm can avoid the non-determinism
by rewriting the axiom into B v r .F and A u F v C, where F is a fresh atomic
concept that is used to preserve the semantics of the original axiom. For Item 2: RIs that
use compositions can be eliminated using an encoding based on automata (Horrocks &
Sattler, 2004) or regular expressions (Simanck, 2012). Note that such explicit encodings
of propagations over complex roles might blow up the knowledge base exponentially, but
539

fiSteigmiller & Glimm

it cannot be avoided in the worst-case, i.e., we could only try to delay it until the actual
reasoning process (Kazakov, 2008) and this is indeed utilised by many reasoners (although
such a blow up seems hardly be caused by real-world ontologies). For Item 3, C(a) (r(a, b))
can equivalently be expressed as {a} v C ({a} v r.{b}).
2.3 Tableau Algorithm for SROIQ
Model construction calculi, such as tableaux, decide the consistency of a knowledge base K
by trying to construct an abstraction of a model for K, a so-called completion graph. In
the following, we describe, based on the original presentation of the SROIQ tableau algorithm (Horrocks et al., 2006), the model construction process and the used data structures,
beginning with completion graphs.
Definition 5 (Completion Graph). For a concept C, we use sub(C) to denote the set of
all sub-concepts of C (including C). Let K be a normalised SROIQ knowledge base and
let Cons(K) be the set of concepts occurring in the TBox T of K, i.e., Cons(K) = {C, D |
C v D  K}. We define the closure clos(K) of K as:
clos(K) = {C  sub(D) | D  Cons(K)}  {nnf(C) | C  sub(D), D  Cons(K)}.
 ). Each node v  V is
A completion graph for K is a directed graph G = (V, E, L, 6=
labelled with a set L(v)  fclos(K), where
fclos(K) = clos(K)  {6 m r.C |6 n r.C  clos(K) and m  n}.
Each edge hv, v 0 i  E is labelled with the set L(hv, v 0 i)  Rols(K), where Rols(K) are the
 is used to keep track of inequalities
roles occurring in K. The symmetric binary relation 6=
between nodes in V .
In the following, we often use r  L(hv1 , v2 i) as an abbreviation for hv1 , v2 i  E and
r  L(hv1 , v2 i).
Definition 6 (Successor, Predecessor, Neighbour). If hv1 , v2 i  E, then v2 is called a
successor of v1 and v1 is called a predecessor of v2 . Ancestor is the transitive closure of
predecessor, and descendant is the transitive closure of successor. A node v2 is called an ssuccessor of a node v1 if r  L(hv1 , v2 i) and r is a sub-role of s; v2 is called an s-predecessor
of v1 if v1 is an s-successor of v2 . A node v2 is called a neighbour (s-neighbour) of a node
v1 if v2 is a successor (s-successor) of v1 or if v1 is a successor (inv(s)-successor) of v2 .
For a role r and a node v  V , we define the set of vs r-neighbours with the concept C in
their label, written mneighbs(v, r, C), as {v 0  V | v 0 is an r-neighbour of v and C  L(v 0 )}.
Note, many inference problems for the DL SROIQ can easily be reduced to consistency
checking and, therefore, they can indirectly be handled although often only consistency
checking reasoning task is specified for the tableau algorithm. For example, in order to
test the satisfiability of a concept C, we introduce a fresh individual a for which we assert
the concept C by an axiom of the form {a} v C. The nodes that represent these (fresh)
individuals are typically called root nodes.
In order to test the consistency of a knowledge base, the completion graph is initialised by
creating one node for each individual/nominal in the input knowledge base. In particular,
540

fiPay-As-You-Go Description Logic Reasoning

if v1 , . . . , v` are the nodes for the individuals a1 , . . . , a` of K, then we create an initial
completion graph G = ({v1 , . . . , v` }, , L, ) and add for each individual ai the nominal {ai }
and the concept > to the label of vi , i.e., L(vi ) = {{ai }, >} for all 1  i  `.
The tableau algorithm works by decomposing/unfolding concepts in the completion
graph with a set of expansion rules (see Table 1). Each rule application can add new
concepts to node labels and/or new nodes and edges to the completion graph, thereby
explicating the structure of a model for the input knowledge base. The rules are repeatedly
applied until either the graph is fully expanded (no more rules are applicable), in which case
the graph can be used to construct a model that is a witness to the consistency of K, or an
obvious contradiction (called a clash) is discovered (e.g., both C and C in a node label),
proving that the completion graph does not correspond to a model. The input knowledge
base K is consistent if the rules (some of which are non-deterministic) can be applied such
that they build a fully expanded and clash-free completion graph.
 ) for a knowledge base K contains
Definition 7 (Clash). A completion graph G = (V, E, L, 6=
a clash if there are the nodes v and w such that
1.   L(v), or
2. {C, nnf(C)}  L(v) for some concept C, or
3. v is an r-neighbour of v and r.Self  L(v), or
4. Disj(r, s)  K and w is an r- and an s-neighbour of v, or
5. there is some concept 6 n r.C  L(v) and {w1 , . . . , wn+1 }  mneighbs(v, r, C) with
 wj for all 1  i < j  n + 1, or
wi 6=
 w.
6. there is some {a}  L(v)  L(w) and v 6=
Unrestricted application of the -rule and >-rule can lead to the introduction of infinitely
many new tableau nodes and, thus, prevent the calculus from terminating. To counteract
that, a cycle detection technique called (pairwise) blocking (Horrocks & Sattler, 1999) is
used that restricts the application of these rules. To apply blocking, we distinguish blockable
nodes from nominal nodes, which have either an original nominal from the knowledge base
or a new nominal introduced by the calculus in their label.
Definition 8 (Pairwise Blocking). A node is blocked if either it is directly or indirectly
blocked. A node v is indirectly blocked if an ancestor of v is blocked; and v with predecessor
v 0 is directly blocked if there exists an ancestor node w of v with predecessor w0 such that
1. v, v 0 , w, w0 are all blockable,
2. w, w0 are not blocked,
3. L(v) = L(w) and L(v 0 ) = L(w0 ),
4. L(hv 0 , vi) = L(hw0 , wi).
In this case, we say that w directly blocks v and w is the blocker of v.
541

fiSteigmiller & Glimm

v1 -rule if
then
v2 -rule if
then
if
then
if
then
if

u-rule
t-rule
-rule

then
Self-rule if
then
-rule if

ch-rule

>-rule

then
if
then
if 1.
2.
then

6-rule

if 1.
2.
then

o-rule

if
then
NN-rule if 1.
2.

then

H  L(v), H v C  K with H = A, or H = {a}, or H = >, C 
/ L(v),
and v is not indirectly blocked
L(v) = L(v)  {C}
{A1 , A2 }  L(v), A1 u A2 v C  K, C 
/ L(v), and
v is not indirectly blocked
L(v) = L(v)  {C}
C1 u C2  L(v), v is not indirectly blocked, and {C1 , C2 } 6 L(v)
L(v) = L(v)  {C1 , C2 }
C1 t C2  L(v), v is not indirectly blocked, and {C1 , C2 }  L(v) = 
L(v 0 ) = L(v 0 )  {H} for some H  {C1 , C2 }
r.C  L(v), v is not blocked, and
v has no r-neighbour v 0 with C  L(v 0 )
create a new node v 0 and an edge hv, v 0 i with
L(v 0 ) = {>, C} and L(hv, v 0 i) = {r}
r.Self  L(v), v is not blocked, and v is no r-neighbour of v
create a new edge hv, vi with L(hv, vi) = {r}
r.C  L(v), v is not indirectly blocked, and
there is an r-neighbour v 0 of v with C 
/ L(v 0 )
0
0
L(v ) = L(v )  {C}
6 n r.C  L(v), v is not indirectly blocked, and
there is an r-neighbour v 0 of v with {C, nnf(C)}  L(v 0 ) = 
L(v 0 ) = L(v 0 )  {H} for some H  {C, nnf(C)}
> n r.C  L(v), v is not blocked, and
 vj
there are not n r-neighbours v1 , . . . , vn of v with C  L(vi ) and vi 6=
for 1  i < j  n, where v1 , . . . , vn are not blocked if v is a nominal node
create n new nodes v1 , . . . , vn with L(hv, vi )i = {r}, L(vi ) = {>, C}
 vj for 1  i < j  n.
and vi 6=
6 n r.C  L(v), v is not indirectly blocked,
]mneighbs(v, r, C) > n and there are two r-neighbours v1 , v2 of v with
 v2
C  (L(v1 )  L(v2 )) and not v1 6=
a. if v1 is a nominal node, then merge(v2 , v1 )
b. else if v2 is a nominal node or an ancestor of v1 , then merge(v1 , v2 )
c. else merge(v2 , v1 )
 v0
there are two nodes v, v 0 with {a}  (L(v)  L(v 0 )) and not v 6=
merge(v, v 0 )
6 n r.C  L(v), v is a nominal node, and there is a blockable
r-neighbour v 0 of v such that C  L(v 0 ) and v is a successor of v 0 ,
there is no m such that 1  m  n, (6 m r.C)  L(v),
and there exist m nominal r-neighbours v1 , . . . , vm of v
 vj for all 1  i < j  m
with C  L(vi ) and vi 6=
1. guess m with 1  m  n and L(v) = L(v)  {6 m r.C}
0 with L(hv, v 0 i) = {r},
2. create m new nodes v10 , . . . , vm
i
0
L(vi ) = {>, C, {ai }} with each ai  NI new in G and K, and
 v 0 for 1  i < j  m.
vi0 6=
j

Table 1: Tableau expansion rules for normalised SROIQ knowledge bases
542

fiPay-As-You-Go Description Logic Reasoning

During the expansion it is sometimes necessary to merge two nodes or to delete (prune)
a part of the completion graph (Horrocks & Sattler, 2007). Roughly speaking, when a node
w is merged into a node v, e.g., by an application of the 6-rule, written merge(w, v), we
add L(w) to L(v), move all the edges leading to w so that they lead to v and move all
the edges leading from w to nominal nodes so that they lead from v to the same nominal
nodes; we then remove w (and blockable sub-trees below w) from the completion graph,
written prune(w), to prevent a further rule application on these nodes.
Note, in order to ensure termination of the tableau algorithm, it is in principle necessary
to apply certain crucial rules with a higher priority. For example, the o-rule is applied
with the highest priority and the NN-rule has to be applied before the 6-rule. The priority
of other rules is not relevant as long as they are applied with a lower priority than for these
crucial rules.

3. Saturation Compatible with Tableau Algorithms
In this section, we describe a saturation method that is an adaptation of the completionbased procedure (Baader et al., 2005) such that it generates data structures that are compatible for further usage within a fully-fledged tableau algorithm. Roughly speaking, the
saturation approximates completion graphs in a compressed form and, therefore, it directly
allows the extraction and transfer of results from the saturation to the tableau algorithm.
To be more precise, we ensure that the saturation generates nodes that are, similarly to the
nodes in completion graphs, labelled with sets of concepts. The saturated labels can then
be used to initialise the labels of new nodes in completion graphs or to block the processing
of successors. Moreover, in some cases, it is directly possible to build a model from the data
structures of the saturation, which makes the construction of completion graphs with the
tableau algorithm unnecessary.
Note, the adapted saturation method is not designed to cover a certain OWL 2 profile or a specific DL language. In contrast, we saturate those parts of knowledge bases
that can easily be supported with an efficient algorithm (see Section 3.1), i.e., we simply
ignore unsupported concept constructors or only process them partially, and afterwards
(see Section 3.2), we dynamically detect which parts have not been completely handled
by the saturation. Hence, the results of the saturation are possibly incomplete, but since
we know how and where they are incomplete, we can use the results from the saturation
appropriately.
For an easy integration into a highly optimised tableau procedure, the (usually simpler)
saturation procedure is adapted to work on the same (normalised and preprocessed) knowledge base and data structures as the tableau algorithm (but, in principle, also the opposite
direction would be possible). This enables, for example, a direct use of node labels from
the saturation in the tableau algorithm. For this coupling technique, the use of a good
absorption algorithm is crucial since the saturation only handles deterministic parts of the
knowledge base.
3.1 Saturation Based on Tableau Rules
The adapted saturation method generates so-called saturation graphs, which approximate
completion graphs in a compressed form, e.g., by reusing nodes.
543

fiSteigmiller & Glimm

Definition 9 (Saturation Graph). A saturation graph for a knowledge base K is a directed
graph S = (V, E, L) with the nodes V  {vC | C  fclos(K)}. Each node vC  V is labelled
with a set L(v)  fclos(K) such that L(vC )  {>, C}. We call vC the representative node
for the concept C. Each edge hv, v 0 i  E is labelled with a set L(hv, v 0 i)  Rols(K).
Obviously, a saturation graph is a data structure that is very similar to a completion
 -relation, which can be omitted since
graph. A major difference is, however, the missing 6=
the saturation is not designed to completely handle cardinality restrictions. Furthermore,
each node in the saturation graph is the representative node for a specific concept, which
allows for reusing nodes as successors. For example, instead of creating new successors
for existential restrictions, we reuse the representative node for the existentially restricted
concept as a successor.
Since nodes, edges, and labels are used as in completion graphs, we use the terms
(r-)neighbour, (r-)successor, (r-)predecessor, ancestor, and descendant analogously. Please
note, however, that all nodes in the saturation graph can have several predecessors due to
the reuse of nodes, whereas in completion graphs, only the nominal nodes can have several
predecessors.
We initialise the saturation graph with the representative nodes for those concepts that
have to be saturated. For example, if the satisfiability of the concept C has to be tested,
then we add the node vC with the label L(vC ) = {>, C} to the saturation graph. If we are
later also interested in the saturation of a concept D, then we simply extend the existing
saturation graph by vD . For knowledge bases that contain nominals, we further initialise the
saturation graph with a node v{a} with L(v{a} ) = {>, {a}} for each nominal {a} occurring
in the knowledge base. The saturation then simply applies the rules depicted in Table 2 to
the saturation graph.
Definition 10 (Saturation). Let S = (V, E, L) be a saturation graph for a knowledge base
K, then the saturation of S exhaustively applies the rules of Table 2 to S. A saturation
graph is called fully saturated if the rules are not further applicable. We use the function
saturate to denote the saturation of a saturation graph S, i.e., saturate(S) returns S 0 , where
S 0 is the fully saturated extension of S.
Note that if a saturation rule refers to the representative node for a concept C and
the node vC does not yet exist, then we assume that the saturation graph is automatically
extended by this node. Although the saturation rules are very similar to the corresponding
expansion rules in the tableau algorithm, there are some differences. For example, the
number of nodes is limited by the number of (sub-)concepts occurring in the knowledge
base due to the reuse of nodes for satisfying existentially restricted concepts. Consequently,
the saturation terminates since the rules are only applied when they can add new concepts
or roles to node or edge labels. Moreover, a cycle detection such as blocking is not required,
which makes rule application very fast. Note also that the -rule propagates concepts only
to the predecessors of a node. This restriction is necessary in order to allow the reuse of
nodes for existentially restricted concepts.
To efficiently derive as many sound inferences as possible, some saturation rules in
Table 2 only partially support more expressive features of SROIQ. After a full saturation,
we then check where the saturation graph is possibly incomplete. Although there are often
544

fiPay-As-You-Go Description Logic Reasoning

v1 -rule: if
then
v2 -rule: if
then
u-rule: if
then
-rule:
if
then
-rule:
if
then
t-rule: if
then
>-rule: if
then
Self-rule: if
then
o-rule:
if

-rule:

H  L(v), H v C  K with H = A, H = {a}, or H = >, and C 
/ L(v)
L(v) = L(v)  {C}
{A1 , A2 }  L(v), A1 u A2 v C  K, and C 
/ L(v)
L(v) = L(v)  {C}
C1 u C2  L(v) and {C1 , C2 } 6 L(v)
L(v) = L(v)  {C1 , C2 }
r.C  L(v) and r 
/ L(hv, vC i)
L(hv, vC i) = L(hv, vC i)  {r}
r.C  L(v), there is an inv(r)-predecessor v 0 of v, and C 
/ L(v 0 )
L(v 0 ) = L(v 0 )  {C}
/ L(v)
C1 t C2  L(v), there is some D  L(vC1 )  L(vC2 ), and D 
L(v) = L(v)  {D}
> n r.C  L(v) with n  1 and r 
/ L(hv, vC i)
L(hv, vC i) = L(hv, vC i)  {r}
r.Self  L(v) and v is not an r-successor of v
L(hv, vi) = L(hv, vi)  {r}
{a}  L(v), there is some D 
/ L(v), and
D  L(v{a} ) or there is a descendant v 0 of v with {{a}, D}  L(v 0 )
then L(v) = L(v)  {D}
if

/ L(v), and
1. {C, nnf(C)}  L(v), or
2. v is an r-successor of itself and {r.Self, r .Self}  L(v) 6= , or
3. v 0 is an r-successor of v with {a}  L(v)  L(v 0 ) and
r.Self  L(v) or r .Self  L(v), or
4. {> n r.C, 6 m s.D}  L(v) with n > m, r v s, and D  L(vC ), or
5. > n r.C  L(v) with n > 1 and {a}  L(vC ), or
6. v 0 is an r-successor of v, r v s, and Disj(r, s)  K, or
7. v 0 is an r-successor of v and v 00 is an s-successor of v and
{a}  L(v 0 )  L(v 00 ) and Disj(r, s)  K, or
8. there exists a successor node v 0 of v with   L(v 0 ), or
9. there exists a node v{a} with   L(v{a} )
then L(v) = L(v)  {}

Table 2: Saturation rules for (partially) handling SROIQ knowledge bases

several ways to integrate the support of more expressive concept constructors, we chose
a simple one that only allows a partial saturation, but which can be implemented very
efficiently. For instance, the t-rule adds only those concepts that are implied by both
disjuncts. Hence, the addition of concepts by this rule is obviously sound, but its handling
of disjunctions is often incomplete. For at-least cardinality restrictions, we build the edges
to (possibly reused) successor nodes similarly to the -rule. Thereby, the actual cardinality
is ignored, which possibly causes incompleteness if also at-most cardinality restrictions for
related super-roles are in the same label. In order to (partially) handle a nominal {a}
in the label of a node v, we use an o-rule that adds concepts that are derived for v{a}
545

fiSteigmiller & Glimm

or for descendant nodes that also have {a} in their label (instead of merging such nodes
as in tableau procedures). As a consequence, the unsatisfiability of concepts of the form
r.(Au{a})ur.(Au{a}) cannot be discovered. This simple implementation does, however,
not require the repeated saturation of the same concepts extended by small influences
from the nominals. While tractable and complete saturation algorithms with nominals are
possible (Kazakov, Krotzsch, & Simanck, 2012), many ontologies use nominals in such a
simple way that this o-rule is already sufficient (e.g., by using nominals only in concepts of
the form r.{a}).
We further need an explicit -rule that uses similar conditions as the ones for clashes
in completion graphs for SROIQ (cf. Definition 7). The -rule is used to handle several
independent concepts in a one-pass manner within the same saturation graph and to distinguish nodes for unsatisfiable concepts from nodes that are (possibly) still satisfiable. The
-rule not only detects trivial reasons for unsatisfiability such as C and C in the label
of a node (Condition 1), but also more involved cases. Violations regarding concepts of
the form r.Self and r .Self are handled by Conditions 2 and 3. The former handles
straightforward self-loops, while the latter handles cases that would lead to a loop if the
saturation were to merge neighbouring nodes with the same nominal in their label. Conditions 4 and 5 handle problematic cases with cardinality restrictions. The actual cardinalities
are ignored by the saturation, but it is clear that a clash would occur in a completion graph
in the presence of conflicting at-least and at-most cardinalities (Condition 4) or for at-least
number restrictions of the form > n r.C with n > 1, where the node representing C contains
a nominal. In the latter case, only one instance of C, the nominal, can exist in any model
of the knowledge base. Conditions 6 and 7 handle problems with Disj(r, s) axioms. The
former condition treats the more trivial case, where an r-successor is also an s-successor
due to s being a super-role of r. The latter condition considers that the saturation does
not merge nodes with the same nominal in their label, which would merge the labels of the
edges from and to these nodes. Note that a node vC can be both an r- and an s-successor
due to node reuse in the saturation if the concepts r.C and s.C are in the label of the
same node. This is, however, no problem even in the presence of Disj(r, s) axioms since this
is not required for models of the knowledge base. The -rule also propagates  to ancestor
nodes (Condition 8) and, in case  occurs in the label of a nominal node,  is propagated
to every other node since the knowledge base is inconsistent (Condition 9).
It is in principle possible to detect also several other kinds of clashes for the incompletely
handled parts in the saturation (e.g., for a concept C that has to be propagated to a successor
node v with C  L(v)), but the presented conditions of the -rule, in combination with the
detection of incompleteness (see Section 3.2), are already sufficient to identify all potential
causes of unsatisfiability. Hence, we omit further clash conditions for ease of presentation.
Example 1. Let us assume that the TBox T1a contains the following axioms:
A v s .B

B v s.{a}

If we are interested in the satisfiability of the concept A, the saturation graph is initialised
with the representative node for A, say vA , with L(vA ) = {>, A} and v{a} with L(v{a} ) =
{>, {a}} for the representation of the individual a. The v1 -rule (cf. Table 2) is applicable
for the first axiom and vA , which results in the addition of s .B to L(vA ) for which the
546

fiPay-As-You-Go Description Logic Reasoning


vA s vB

s


vA s vB

v{a}

L(vA ) = {>, A, s .B}
L(vB ) = {>, B, s.{a}}
L(v{a} ) = {>, {a}}

s
v{a}

r
L(vA ) = {>, A, s .B, B t {a}, C}
L(vB ) = {>, B, s.{a}, C, 6 1 s.C}
L(v{a} ) = {>, {a}, C, > 2 r.B}

Figure 1: Generated saturation graphs for testing the satisfiability of A w.r.t. T1a (left) and
T1c (right) from Example 1

application of the -rule generates the node vB with L(vB ) = {>, B} and an s -labelled edge
between vA and vB . Now, the v1 -rule can be applied to unfold B in the label of vB to s.{a}
for which the -rule is again applicable. Hence, we obtain the fully saturated saturation graph
depicted on the left-hand side of Figure 1. Note that the saturation procedure starts the rule
application with only the nodes vA and v{a} ; other nodes, e.g., vB , are created on demand.
Now let T1b = T1a plus the axioms:
A v B t {a}

BvC

{a} v C

The v1 -rule extends L(vA ) with B t {a} and L(vB ) as well as L(v{a} ) with C. Now the
t-rule is applicable and adds the concept C to the label of vA because C is in the label of
the representative nodes for both disjuncts of the disjunction B t {a}. Note that although
the concept C is added to node labels, a node for C is not created since C is not used in a
way that requires this.
Finally, let T1c = T1b plus the axioms:
B v 6 1 s.C

{a} v > 2 r.B

The v1 -rule extends L(vB ) with 6 1 s.C and L(v{a} ) with > 2 r.B. The latter addition
triggers the >-rule, which adds an r-edge from v{a} to vB such that the saturation graph
depicted on the right-hand side of Figure 1 is obtained. Note that the saturation is inherently
incomplete. For example, there is no saturation rule that handles the concept 6 1 s.C. The
tableau rules would merge vA and v{a} since vB can have only one s-successor with C in
its label, which possibly leads to conclusions that the saturation misses. We cover how such
incomplete handling of nodes can be detected in the next section.
With a suitable absorption technique, the saturation is usually able to derive and add the
majority of those concepts that would be added by the tableau algorithm for an equivalent
node. This is especially the case for ontologies that primarily use features of the DL EL++
for which saturation-based procedures are particularly well-suited. Since EL++ covers many
important and often used constructors (e.g., u, ), the saturation does already the majority
of the work for many ontologies as confirmed by our evaluation in Section 7.
3.2 Saturation Status Detection
Similarly to other saturation procedures, the presented method in Section 3.1 easily becomes
incomplete for more expressive DLs. In order to nevertheless gain as much information as
547

fiSteigmiller & Glimm

possible from the saturation, we identify nodes for which the saturation was possibly incomplete. We call such nodes critical. In principle, such nodes can be detected by testing
whether an actual tableau rule is still applicable. However, since we saturate some more expressive concept constructors partially, this approach is often too conservative. For example,
for at-least cardinality restrictions of the form > n r.C with n > 1, the saturation already
creates or reuses a successor node with the concept C and, therefore, all consequences that
are propagated back from this successor node are already considered. Nevertheless, the
tableau expansion rule for this at-least cardinality restriction is still applicable, since we
have not created n successors that are stated as pairwise different. This is, however, only
relevant if there are some restrictions that limit the number of allowed r-successors with
the concept C in their label. For the DL SROIQ, such limitations are only possible with
nominals and at-most cardinality restrictions. Therefore, it is sufficient to check for such
limitations instead of testing whether the tableau expansion rules are applicable. Similar
relaxations are also possible for other concept constructors.
We use the rules of Table 3 and 4 to detect the saturation status of a saturation graph,
where incompletely handled nodes are identified and other information that is relevant for
supporting the tableau algorithm is extracted. To be more precise, the rules are applied to
a saturation graph and gather nodes in the sets So , S , and S! , where So represents nodes
that depend on nominals, S represents nodes with tight at-most restrictions, and S!
represents critical nodes that are potentially not completely handled by the saturation. In
order to specify the saturation status in more detail, we first define the number of merging
candidates to facilitate the treatment of possibly incompletely handled at-most cardinality
restrictions.
Definition 11 (Merging Candidates). Let S = (V, E, L) be a saturation graph. For a role
s and a concept D, the number of merging candidates
Pfor a node v  V w.r.t. s and D,
written as the function ]mcands(v, s, D), is defined as >n r.CL n with
L ={> n r.C  L(v) | r v s and D  L(vC )}
{> 1 r.C | r.C  L(v), r v s, and D  L(vC )}.
For an at-most cardinality restriction 6 m s.D in the label of a node v, the merging
candidates are those s-successors that have the concept D in their label. This is used by
the C -rule (see Table 3) to identify nodes with tight at-most restrictions, which is the case
for a node v with an at-most cardinality restriction 6 m s.D in its label if the number of
merging candidates for v w.r.t. s and D is m, i.e., m = ]mcands(v, s, D). For such nodes, it is
still not necessary to merge some of the merging candidates, but every additional candidate
might require merging and, therefore, these nodes cannot be used arbitrarily.
The Co -rule adds to the set So all nodes that directly or indirectly depend on nominals,
i.e., it identifies all nodes that directly have a nominal in their label or have a descendant
node with a nominal in its label.
The rules of Table 4 are used to identify critical nodes for which the saturation procedure
might be incomplete, i.e., these nodes are added to the set S! as follows:
 The C - and Ct -rule identify nodes as critical for which the - or the t-rule of the
tableau algorithm is applicable. Note, for the C -rule it is only necessary to check
548

fiPay-As-You-Go Description Logic Reasoning

C -rule: if
then
Co -rule: if
then

v
/ S , 6 m s.D  L(v), and ]mcands(v, s, D) = m
S = S  {v}
v
/ So and either {a}  L(v) or v has a successor node v 0 with v 0  So
So = So  {v}

Table 3: Rules for detecting nodes with tight at-most restrictions and nodes with nominal
dependency in the saturation graph

whether the concepts can be propagated to the successor nodes since the propagation
to predecessors is ensured by the saturation procedure.
 The C6-rule checks for every node v whether there is a potentially unsatisfied at-most
cardinality restriction of the form 6 m s.D in the label of v, i.e., ]mcands(v, s, D) > m.
Analogously to the ch-rule in the tableau algorithm, the Cch -rule identifies nodes as
incompletely handled if they have s-successor nodes with neither D nor nnf(D) in
their label. In addition, we have to consider that the successors may have to be
merged into a predecessor. Note that this has to be checked from the perspective
of the predecessors due to the reuse of nodes. Therefore, we check with the C6and Cch -rule on a node v whether there exists an inv(s)-successor node v 0 that has
a tight at-most restriction for s, i.e., 6 m s.D  L(v 0 ) and ]mcands(v, s, D) = m.
If v is a merging candidate, i.e., D  L(v), or it would be necessary to apply the
ch-rule for v, then we consider v critical. For example, if v has an s -successor v 0
with {> 3 s.D, 6 3 s.D}  L(v 0 ), then the C6-rule (Cch -rule) identifies v as critical if
D  L(v) ({D, nnf(D)}  L(v) 6= ).
 We also need several rules for the detection of incompleteness related to nominals.
First, we check with the Coo -rule whether there are two nodes in the saturation graph
that have the same nominal but different concepts in their label. In this case, the
handling of the nominal is possibly incomplete since merging these nodes would also
merge their labels. Note that if we saturate several independent concepts in the same
saturation graph, then merging all nodes with the same nominal in their label is not
always necessary. However, detecting when a merge is really required would involve
a more expensive test. Since many ontologies for less expressive DLs use nominals in
very simple ways, we opt for a simple and efficient solution. In addition, if a node v is
nominal dependent, i.e., it has a descendant node with a nominal in its label, then arbitrary consequences could be propagated to it via other individuals/nominals. Hence,
the Co! -rule adds v to the set S! of critical nodes if a representative node for an individual is not completely handled since we cannot guarantee that the saturation has
derived all consequences for v. In contrast, the Co6-rule checks for possible interactions between nominals and at-most cardinality restrictions. Such an interaction is
handled by the NN-rule of the tableau algorithm, but cannot easily be handled by the
saturation and we rather identify such nodes as critical.
 Finally, the C -rule marks all predecessors of critical nodes as critical.
549

fiSteigmiller & Glimm

C -rule: if
then
Ct -rule: if
then
C6-rule: if
then
Cch -rule: if
then
C6-rule: if
then
Cch -rule: if
then
Coo -rule: if
then
Co! -rule: if
then
Co6-rule: if

C -rule:

then
if
then

v
/ S! , r.C  L(v), there is an r-successor v 0 of v, and C 
/ L(v 0 )
S! = S!  {v}
v
/ S! , C t D  L(v), and {C, D}  L(v) = 
S! = S!  {v}
v
/ S! , 6 m s.D  L(v), and ]mcands(v, s, D) > m
S! = S!  {v}
v
/ S! , 6 m s.D  L(v), there is an s-successor v 0 of v, and
L(v 0 )  {D, nnf(D)} = 
S! = S!  {v}
v
/ S! , D  L(v), v 0 is an inv(s)-successor of v, 6 m s.D  L(v 0 ), and
]mcands(v 0 , s, D) = m
S! = S!  {v}
v
/ S! , v 0 is an inv(s)-successor of v, 6 m s.D  L(v 0 ), and
L(v)  {D, nnf(D)} = 
S! = S!  {v}
v
/ S! , {a}  L(v), {a}  L(v 0 ), and L(v) 6 L(v 0 )
S! = S!  {v}
v
/ S! , v  So , and there exist some node v{a}  S!
S! = S!  {v}
v
/ S! , v 0 is an inv(s)-successor of v, {a}  L(v 0 ), 6 m s.D  L(v 0 ),
and nnf(D) 
/ L(v)
S! = S!  {v}
v
/ S! , there is a successor v 0 of v, and v 0  S!
S! = S!  {v}

Table 4: Rules for detecting incompleteness in the saturation graph
The sets So , S , and S! are now used to define the saturation status of a saturation
graph as follows:
Definition 12 (Saturation Status). The saturation status S of a saturation graph S =
(V, E, L) is defined as the tuple (So , S , S! ). We use status as the function that creates S
from S by the exhaustive application of the rules in Table 3 and 4. A node v  V is critical
if v  S! , v is nominal dependent if v  So , and v has tight at-most restrictions if v  S .
We call v clashed if   L(v).
Note that a concept C is unsatisfiable if its representative node vC is clashed. The
satisfiability of C can, however, only be guaranteed if vC is not critical and the knowledge
base is consistent. Consistency is required, because a concept is satisfiable only if the
knowledge base is consistent, which can only be determined by the saturation if no nominal
node is critical.
Of course, if the satisfiability/completeness of saturated concepts is considered in the
context of arbitrary other concepts that are not handled by the saturation (e.g., in a completion graph constructed by the tableau algorithm), then nominal dependency becomes
more relevant. In particular, if new consequences are propagated to nominal nodes, then
550

fiPay-As-You-Go Description Logic Reasoning

the nominal dependent nodes in the saturation graph could be affected and they have to
be considered as incompletely handled. Hence, also the satisfiability hinges on the status
of the nominal nodes a node depends on.
A problem in practice is that a critical node for a nominal also makes all nominal
dependent nodes critical. Hence, we can easily get many critical nodes for ontologies that
use nominals if the saturation cannot completely handle all individuals. However, we can
improve the saturation graph after the initial consistency check with the tableau algorithm
(see Section 5 for details) by replacing the node labels of critical nominal nodes in the
saturation graph with the ones from the obtained completion graph. Although we have
to distinguish deterministically and non-deterministically derived concepts in these labels,
we know that they correspond to a clash-free and fully expanded completion graph and,
therefore, we can consider them as not critical.
Example 2. Consider again the TBoxes from Example 1. We start with the fully saturated
saturation graph for T1a (left-hand side of Figure 1). Only the Co -rule from Table 3 is
applicable, which identifies all nodes as nominal dependent and adds them (iteratively) to
So , but all nodes are completely handled by the saturation and no node is critical or has
tight at-most restrictions.
The situation changes for the extension T1b of T1a . Since B t {a}  L(vA ), but neither
disjunct is, vA is identified as critical (vA  S! ) by the Ct -rule from Table 4. The other
nodes are, however, still completely handled by the saturation.
Finally, consider the extension T1c of T1b (right-hand side of Figure 1). The C -rule
from Table 3 now adds vB to S since the number of merging candidates for vB w.r.t. s and
C is 1, i.e., ]mcands(vB , s, C) = 1. This is used to identify nodes as critical that use vB
as an s -successor such that merging with an s-successor of vB is potentially required. In
particular, the concept s .B  L(vA ) is now problematic because it connects vB with vA
via the role s, 6 1 s.C  L(vB ), and ]mcands(vB , s, C) = 1. Hence, if vA were not already
identified as critical due to the incompletely handled disjunction, the C6-rule would add vA
to S! . Note, however, that vB and v{a} are still completely handled by the saturation.
3.3 Correctness
It is straightforward to see that the saturation rules of Table 2 only produce sound inferences.
In particular, the saturation rules add only those concepts to a label of a node which are
also added by the tableau algorithm for an equivalently labelled node in the completion
graph. The termination of the saturation rules is ensured since the number of nodes and
edges as well as the size of the labels is bounded by the number of concepts, roles, and size
of the closure of concepts in the knowledge base. Furthermore, the rules are only applied
if they add new facts in the saturation graph. Analogously, the application of the rules of
Table 3 and 4 for the generation of a saturation status is terminating, because each rule
application adds the node to a corresponding set (either So , S , or S! ) and the rules are
only applicable if the node does not already belong to the corresponding set.
It remains to show the completeness, i.e., we show that if a node vD and the nodes
representing individuals are neither critical nor clashed, we can build a model of the knowledge base in which the extension of D is non-empty. Note that a direct transformation of
the saturation graph into a completion graph is not possible since the reuse of nodes in the
551

fiSteigmiller & Glimm

saturation graph possibly causes problems with certain features of SROIQ. For example, if
two roles r and s are stated disjoint and s is not a super role of r, then the saturation graph
can contain a node that is an r- and an s-successor of another node. However, in principle,
it would be possible to rebuild a completion graph by recursively creating corresponding
successor nodes from the used nodes in the saturation graph until we would reach nominal
nodes or the nodes would be blocked.
Given a fully saturated saturation graph, where all nodes representing individuals are
neither critical nor clashed, we show completeness for a non-critical node vD by providing
an interpretation I = (I , I ) that is a model of the knowledge base with a non-empty
extension of D, i.e., DI 6=  and the interpretation witnesses the satisfiability of D. For
ease of presentation, we assume that existentially quantified concepts of the form r.C are
equivalently expressed as > 1 r.C. This is w.l.o.g., since normalised knowledge bases contain
only simple roles.
Since the occurrence of a nominal {a} in the label of a node vC means that the node vC
represents the same element as v{a} in I , we only need one representative element, which
we ensure by defining a suitable equivalence relation over the nodes in the saturation graph.
Definition 13 (Canonical Saturation Model). For a saturation graph S = (V, E, L), let 
be the following relation: {(vC , vC ) | vC  V }  {(vC , v{a} ) | vC  V, {a}  L(vC ), L(v{a} ) =
*
L(vC )} and let 
be the transitive, reflexive, and symmetric closure of . Since the relation
*
 is an equivalence relation over the nodes in V , we use v[C] , for vC  V , to denote the
*
*
equivalence class of vC by 
. We use the relation 
to (recursively) define the elements of
I
 . We first define the set Nom(S) = {v[{a}] | a  NI } of nodes with nominals in their
labels. Further non-nominal elements of I are then obtained by unravelling parts of the
saturation graph into paths as usual (Horrocks & Sattler, 2007). We set
PathsS (D) = {v[D] }  Nom(S) 
i
{p  v[C]
| p  PathsS (D), > n r.C  L(v), v  tail(p), v[C] 
/ Nom(S), 1  i  n},
i )=v .
where  denotes concatenation and tail(v[C] ) = tail(p  v[C]
[C]

We can now define the interpretation I = (I , I ) as follows:
I = PathsS (D),
and, for each a  NI , we set I to
aI = v[{a}] ,
for each A  NC to
AI = {p | v  tail(p) and A  L(v)},
552

fiPay-As-You-Go Description Logic Reasoning

and for each r  NR to
i
and > n s.C  L(v)
rI = {hp, qi  PathsS (D)  PathsS (D) | q = p  v[C]

for a v  tail(p) with i  n and s v r} 
i
{hq, pi  PathsS (D)  PathsS (D) | q = p  v[C]
and > n inv(s).C  L(v)

for a v  tail(p) with i  n and s v r} 
{hp, xi  PathsS (D)  Nom(S) | there is a v  x and a v 0  tail(p)
such that v is an r-successor of v 0 } 
{hx, pi  Nom(S)  PathsS (D) | there is a v  x and a v 0  tail(p)
such that v is an inv(r)-successor of v 0 } 
{hp, pi  PathsS (D)  PathsS (D) | there is a v  tail(p)
such that v is an r-neighbour of itself }

Note that the domain elements are (paths of) equivalent classes and to ensure that the
extension of a role r correctly contains all edges derived by the saturation, we use all nodes
of an equivalent class for the construction of rI .
In order to show that I |= K, we first show that, for every p  I , it holds that p  C I
if C  L(v) with v  tail(p).
Lemma 1. Let S = (V, E, L) be a fully saturated saturation graph for a concept D w.r.t.
K and vD as well as the nodes representing individuals are neither critical nor clashed.
Furthermore, let I = (I , I ) denote an interpretation constructed as described in Definition 13. For each p  I it holds that p  C I if C  L(v) with v  tail(p).
Proof 1. We observe that all nodes involved in the construction of the interpretation can
neither be critical nor clashed. In particular, if a node v  tail(p) for a p  I were clashed,
then v would be a descendant node of v[D] or of a node representing an individual and, hence,
they would be identified as clashed with the -rule, which contradicts our assumption that
these nodes are not clashed. Analogously, if v were critical, then v[D] or a node representing
an individual would be identified as critical by the C -rule (which is also contradictory to
our assumption). Hence, for considering the different types of concepts for proving the
lemma in the following, it is safe to assume that all nodes used for the construction of the
interpretation are neither clashed nor critical and, hence, I = .
The base case for C = A with A  L(v) and v  tail(p) trivially holds for all p  I by
the definition of I and I , i.e., p  AI if A  L(v) with v  tail(p). Also note that I = >I
due to the definition of the saturation algorithm (in particular due to the initialisation of
nodes) and due to the fact that we never remove concepts from labels in a saturation graph.
Other base cases hold for all elements p  I with v  tail(p) and C  L(v) as follows:
 For C = {a}, we observe that p = v[{a}] due to the use of equivalence classes, the
*
definition of 
, I , I , and the fact that the Coo -rule cannot identify v as critical by
assumption. Hence, p  C I .
553

fiSteigmiller & Glimm

 For C = B, we observe that p 
/ B I due to the definition of I and the fact that the
used nodes are not clashed in the saturation graph. Hence, p  C I .
 For C = r.Self, we observe that the node v is an r-successor of itself due to the
application of the Self-rule. By the definition of I , we have hp, pi  rI . Hence,
p  CI .
 For C = r.Self, we observe that the nodes in the saturation graph are not clashed.
Hence, we can exclude loops that are caused by the last part of the definition of I since
any such node would be clashed due to Condition 2 of the -rule. For nominal nodes
we observe that nodes of V with the same nominal in their label are represented by
one element in I . This possibly introduces loops for neighbouring nominal nodes in
the saturation graph. This is, however, excluded by Condition 3 of the -rule. Hence,
we have hp, pi 
/ rI and, therefore, p  C I .
The complex cases hold for all elements p  I with v  tail(p) and C  L(v) by
induction as follows:
 For C = C1 u C2 , the application of the u-rule ensures that L(v)  {C1 , C2 }. By
induction, we have p  C1I and p  C2I . Hence, p  C I .
 For C = C1 t C2 , we observe that there must be a concept C 0  L(v) with C 0 
{C1 , C2 } since the Ct -rule would otherwise have identified the node v  tail(p) as
critical, which contradicts our assumption. Hence, by induction, we have p  C 0I
and, as a consequence, p  C I .
 For C = > n r.C 0 , we observe that the saturation algorithm creates and saturates
the node vC 0 as r-successor of v for n  1 and we consider two cases: First, for
v[C 0 ]  Nom(S), we have that n = 1 since v would be clashed for n > 1 due to
Condition 5 of the -rule, which contradicts our assumption. By the definition of
PathsS (D), I , and I , there exists the element q with q  Nom(S) such that hp, qi 
rI . Furthermore, by induction, we have q  C 0I and, consequently, p  C I . Second,
i
1 , . . . , p  vn
for v[C
/ Nom(S), by the construction of PathsS (D), p  v[C
0] 
0]
[C 0 ]  PathsS (D)
I
and, by definition of  , these elements are n r-successors of p. Finally, by induction,
1 , . . . , p  vn
0I and, consequently, p  C I .
we have p  v[C
0]
[C 0 ]  C
 For C = r.C 0 , we observe that the application of the -rule guarantees that all inv(r)predecessors have C 0 in their label. Furthermore, also all r-successors have C 0 in their
label, otherwise the C -rule would have identified v as critical, which is contradictory
to our assumption. Hence, by definition of PathsS (D), I , I , and by induction, it
holds for every r-neighbour element q that q  C 0I and, consequently, p  C I .
 For C = 6 n r.C 0 , the C6-rule guarantees that every node v  p has at most n merging
candidates, i.e., at most n r-successor nodes with C 0 in their labels, otherwise the
node would be critical, which contradicts our assumption. Analogously, the Cch -rule
guarantees that every r-successor of v has either C 0 or nnf(C 0 ) in its label. For
p = v[D] with p 6 Nom(S), we observe that p does not have any predecessors and,
554

fiPay-As-You-Go Description Logic Reasoning

by definition of PathsS (D), I , and I , and by induction, we have at most n rneighbour elements q1 , . . . , qn with q1 , . . . , qn  C 0I since for every other r-neighbour
element q it holds that q  nnf(C 0 )I and, therefore, q 
/ C 0I . Hence, p  C I for
p = v[D] . For p  Nom(S), it holds, for every inv(r)-predecessor element q of p, that
q
/ C 0I due to induction, the definition of PathsS (D), I , and I , and the Co6-rule
(otherwise the node would be identified as critical). Hence, p  C I for p  Nom(S).
i . Moreover, the C
For p 
/ {v[D] }  Nom(S), we have p = q  v[D
0]
ch -rule guarantees that
0
0
for w  tail(q) either C or nnf(C ) is in its label. By induction and the definition
of PathsS (D), I , and I , we have either q  C 0I or q  nnf(C 0 )I . We consider
both cases: First, if q 
/ C 0I because of w  tail(q) with nnf(C 0 )  L(w), then we can
argue analogously to the case where p = v[D] . Second, if q  C 0I because of w  tail(q)
with C 0  L(w), then it is guaranteed by the C6-rule that v has at most n  1 rsuccessors with C in their labels and, by definition of PathsS (D), I , and I , we have
at most n r-neighbour elements q1 , . . . , qn of p for which it holds by induction that
q1 , . . . , qn  C 0I . Consequently, p  C I .
By using Lemma 1, we can now show the completeness, i.e., we can show that the
constructed interpretation satisfies all axioms in the knowledge base:
Lemma 2 (Completeness). Let S = (V, E, L) be a fully saturated saturation graph for a
concept D w.r.t. K and vD as well as the nodes representing individuals are neither critical
nor clashed, then there exists an interpretation I = (I , I ) such that I |= K and DI 6=  .
Proof 2. We assume that the interpretation I = (I , I ) is built from S as in Definition 13.
Note that due to the definition of the saturation algorithm, there is a node vD  V with
D  L(vD ), by the definition of I , v[D]  I and, by the definition of tail and I , v[D] 
DI . Hence, DI 6= . We can now observe that I satisfies every axiom  of the normalised
knowledge base K (cf. Section 2.2) as follows:
 For  = H v C with H = {a}, H = A, or H = >, we observe that, for every p  I
*
with v  tail(p), we have H I only if H  L(v) due to the definition of 
, PathsS (D),
I
I
 , and  . Due to the applications of the v1 -rule, we have C  L(v) if H  L(v),
and, by Lemma 1, we have p  C I if p  H I . Hence, I |= .
 For  = A1 u A2 v C, we analogously observe that, for every p  I with v  tail(p),
*
we have (A1 uA2 )I only if {A1 , A2 }  L(v) due to the definition of 
, PathsS (D), I ,
I
and  . Due to the applications of the v2 -rule, we have C  L(v) if {A1 , A2 }  L(v),
and, by Lemma 1, we have p  C I if p  H I . Hence, I |= .
 For  = r v s, we observe that I |=  due to the definition of successors/predecessors
and by the definition of PathsS (D), I , and I .
 For  = Disj(r, s), we assume for a contradiction that there are the elements p, q  I
such that hp, qi  rI  sI . By definition of a saturation graph and I from the edges
in S, either q  Nom(S) or r v s. However, in both cases v  tail(p) is clashed due
to Condition 6 and Condition 7 of the -rule, respectively. Since it contradictory to
our assumption that v is clashed, we have I |= .
555

fiSteigmiller & Glimm

Since I satisfies, for all elements of I , every axiom  of K, I |= K.
From the language features that are completely supported by the presented saturation
algorithm and from the fact that r.C concepts on the left-hand side of GCIs can be
transformed to universal restrictions on the right-hand side (with only propagations to
predecessors if inverse roles are not used), one can observe that completeness of the presented
saturation algorithm is guaranteed for at least ELH knowledge bases.

4. Supporting Tableau Algorithms
In this section, we present a range of optimisations to directly and indirectly support reasoning with tableau algorithms for the DL SROIQ. As already mentioned, reasoning systems
for more expressive DLs are usually very complex and they integrate many sophisticated
optimisations which are necessary to make reasoning for many real-world ontologies practicable. As a consequence, it is important for the development of new optimisations to
consider the interaction with already existing techniques. For example, a very important
and well-known optimisation technique is dependency directed backtracking which allows
for only evaluating relevant non-deterministic alternatives with the tableau algorithm.
A typical realisation of dependency directed backtracking is backjumping where every fact
that is added to the completion graph is labelled with those non-deterministic branches on
which the fact depends (Baader et al., 2007; Tsarkov, Horrocks, & Patel-Schneider, 2007).
If a clash is discovered, then we can jump back to the last non-deterministic decision that
is referenced by the clashed facts in the completion graph and, consequently, we do not
have to evaluate non-deterministic alternatives for which it is clear that they would result in the same clashes. Hence, new optimisation techniques that manipulate completion
graphs must obviously also add these dependencies correctly, otherwise dependency directed
backtracking cannot be completely supported in the presence of these optimisations.
The optimisation techniques that we present in this section are fully compatible with
dependency directed backtracking and, to the best of our knowledge, they also do not
negatively influence other well-known optimisations. Moreover, since the saturation optimisations allow for doing a lot of reasoning work very efficiently, they often reduce the
effort for other optimisation techniques. For example, these optimisations directly perform
many simple expansions in the completion graph and, therefore, the effort for conventional
caching methods is often reduced. In particular, tableau-based reasoning systems often
cache satisfiable node labels in order to block the expansion of successors of identically
labelled nodes in subsequent completion graphs. If we can reuse the labels of non-critical
and non-clashed nodes from the saturation graph, then we directly know that a further
processing in the completion graph is not required for them, even without checking whether
corresponding node labels are in the satisfiability cache.
4.1 Transfer of Saturation Results to Completion Graphs
Since the presented saturation method uses compatible data structures, we can directly
transfer the saturation results into completion graphs. This improves the tableau algorithm
by a faster clash detection and optimises the construction of a completion graph. For
example, we can directly use unsatisfiability information that is detected by the -rule in
556

fiPay-As-You-Go Description Logic Reasoning

the saturation. In particular, if the application of a tableau expansion rule adds a concept
C to the completion graph, then we can check the saturation status of vC and, in case it
is clashed, we can immediately initiate the backtracking with the dependencies from the
unsatisfiable concept C in the completion graph. Analogously, we can utilise other derived
consequences form the saturation. For instance, if an expansion rule adds a concept C
to a label of a node in the completion graph, then we can add all concepts of L(vC ) to
the same label. Of course, in order to further support dependency directed backtracking,
we also have to add the correct dependencies. However, since all concepts of L(vC ) are
deterministic consequences of C, we can simply use that D deterministically depends on C
for every additionally added concept D  L(vC ).
As a nice side effect, the addition of derived concepts from the saturation improves
the backtracking and processing of disjunctions. Basically, the t-rule from the saturation
extracts shared (super-)concepts from all disjuncts of a disjunction. For example, for the
disjunction A1 t A2 and the axioms A1 v B and A2 v B, we derive with the saturation that
L(vA1 t A2 )  {A1 t A2 , B}, i.e., B is a super-concept of both disjuncts and we can add it as
a deterministic consequence of the disjunction A1 t A2 . Although we still have to process
the disjunction, we can add some of the consequences (e.g., B) deterministically. Hence,
backtracking does not identify the processing of alternatives of a disjunction as relevant if
only such deterministic consequences are involved in the clash.
The transfer of derived consequences to directly add as many consequences as possible is
helpful in several ways. First, the application of expansion rules from the tableau algorithms
might become unnecessary. For example, if a disjunct of a disjunction has already been
added, then it is not necessary to apply the t-rule. Second, if specific concepts are in the
label of a node, then, at least for some expansion rules of the tableau algorithm, optimised
rule applications are possible. For instance, if the concepts r.C, r.D, and 6 1 r.> are
in the label of the same node, then the second application of the -rule by the tableau
algorithm can directly add the existentially restricted concept to the already present rsuccessor instead of creating a new one that has to be merged afterwards. Third, if concepts
are propagated back to ancestor nodes, then it is necessary to again check whether one of
the modified ancestor nodes is blocked before rules on descendant nodes can be applied.
Due to the transfer of derived consequences, many of the concepts that are propagated
back from successors are already added and, therefore, the amount of blocking tests is
significantly reduced. Last but not least, the transfer of derived consequences allows for
blocking much earlier. Blocking of a node v is usually only possible if a node could be
replaced by another non-blocked node from the completion graph that does not influence
any ancestor of v. A simple blocking condition that guarantees completeness for more
expressive DLs is pairwise blocking. However, pairwise blocking can be refined to achieve
more precise blocking conditions that possibly allow for blocking earlier (Horrocks & Sattler,
2001). Since many of the concepts that are propagated back from successors are added by
the transfer of derived consequences from the saturation, it is likely that the creation and
processing of new successor nodes does not influence ancestor nodes. As a result, it might
be possible to block nodes even without the creation and processing of many successors.
Besides the transfer of derived consequences, it is in some cases also possible to directly
block the processing of successor nodes in the completion graph. For this, the node in the
completion graph, say v, has to be labelled with the same concepts as a node v 0 in the
557

fiSteigmiller & Glimm

saturation graph and v 0 must neither be clashed, critical, nor nominal dependent. If there
exists such a v 0 , then the processing of the successors of v can be blocked since v could be
expanded in the same way as v 0 in the saturation graph. Obviously, we have to enforce
that v 0 is not nominal dependent, because a dependent nominal could be influenced in the
completion graph such that new consequences are propagated back to v and this would
not be considered if the processing of successor nodes is blocked. Furthermore, it is indeed
necessary to create the successors before blocking their processing, because they may have
to be merged into the ancestor node. However, if the saturation node v 0 does not have a
tight at-most restriction, i.e., for each at-most cardinality restriction 6 m r.C  L(v 0 ), v 0
has at most m  1 r-successors that have not nnf(C) in their label, then also the creation
of successor nodes can be blocked, because every at-most cardinality restriction in the label
of the node allows for at least one additional neighbour before some nodes have to be
merged. Since nodes can easily have a large number of successors (e.g., due to at-least
cardinality restrictions with big cardinalities), blocking the creation of new successors can
be a significant improvement in terms of memory consumption and building time of the
completion graph. Of course, if new concepts are propagated to v such that the label of v
differs from v 0 , then the blocking becomes invalid and the processing of the successors has
to be reactivated or we have to find another compatible blocker node.
4.2 Subsumer Extraction
For tableau-based reasoning systems, many higher level reasoning task are often reduced
to consistency checking. For example, a very naive classification algorithm tests the satisfiability of all classes and then checks the pairwise subsumption relations between these
classes (which are also reduced to satisfiability/consistency tests) in order to build the
class hierarchy of an ontology. In practice, the number of required satisfiability tests can
be significantly reduced by optimised classification approaches such as enhanced traversal
(Baader et al., 1994) or known/possible set classification (Glimm et al., 2012). These optimised classification algorithms use specific testing orders and exploit information that can
be extracted from the constructed models. To optimise their testing order, the algorithms
are usually initialised with told subsumptions, i.e., with the subsumption relations that can
syntactically be extracted from ontology axioms, and, typically, the more told subsumers
can be extracted, the larger is the benefit for the classification algorithms. However, a more
detailed extraction of told subsumers from ontology axioms is usually less efficient than a
simple one. For instance, the ontology axioms A1 v r.C u D and r.C v A2 imply that
A2 is a subsumer of A1 , but this can only be detected, when parts of axioms are compared
with each other.
With the saturation, we can significantly improve the told subsumers for the initialisation of the tableau-based classification algorithm since also (some) semantic consequences
are considered. As new and more accurate told subsumers, we can simply use, for each
concept A that has to be classified, all the (atomic) concepts in L(vA ). Moreover, if vA
is clashed, then we know that A is unsatisfiable without performing a satisfiability test.
Analogously, if vA is neither clashed nor critical and the knowledge base is consistent, we
know that A is satisfiable and that L(vA ) contains all subsumers. Note that if vA is nominal
dependent and a representative node for a nominal is critical, then also vA is identified as
558

fiPay-As-You-Go Description Logic Reasoning

critical. Hence, for the extraction of subsumers, we only have to consider the criticality
status of the considered node, whereas nominal dependency does not matter. If no node
for an ontology is critical, we already get all subsumers from the saturation and, therefore,
only a transitive reduction (i.e., the elimination of those subsumptions that are indirectly
implied through the transitivity property of the subsumption relation) is necessary to build
the class hierarchy. Thus, with the preceding saturation we automatically get a one-pass
classification for simple ontologies.
Note that our completeness proof in Section 3.3 principally only covers the satisfiability
of concepts. However, it is quite obvious that the presented saturation approach also computes all subsumers if the nodes are neither critical nor clashed. In particular, if we assume
that a subsumption A v B is not derived by the saturation but follows from a knowledge
base K, then we obtain a contradiction by considering the saturation of the knowledge base
K0 that extends K by the axiom B v . Since B v  can only be added with the v1 -rule
if B is in a node label and K0 differs from K only through the axiom B v , the saturation
would derive the same consequences and, hence, would be incomplete w.r.t. testing the
satisfiability of A, which is contradictory w.r.t. our assumption and our completeness proof.
4.3 Model Merging
Many ontologies contain axioms of the form C  D, which can be seen as an abbreviation
for C v D and D v C. As described in Section 2.2, we utilise this to get a normalised
knowledge base where we do not have to consider such axioms. Treating axioms of the
form A  D with A an atomic concept as A v D and D v A can, however, downgrade
the performance of tableau algorithms since absorption might not apply to D v A, i.e.,
the axiom has to be internalised into > v nnf(D t A). To avoid this, many implemented
tableau algorithms explicitly support A  D axioms by an additional unfolding rule, where
the concept A in the label of a node is unfolded to D and A to nnf(D) (exploiting that
D v A is equivalent to A v nnf(D)) (Horrocks & Tobies, 2000).2 Unfortunately, using
such an unfolding rule also comes at a price since the tableau algorithm is no longer forced
to add either A or nnf(D) to each node in the completion graph, i.e., we might not know
for some nodes whether they represent instances of A or A. This means that we cannot
exclude A as possible subsumer for other (atomic) concepts if the nodes in completion graphs
do not contain A (and also not A), which is an important optimisation for classification
procedures (Glimm et al., 2012).
To compensate for this, we can create a candidate concept A+ for A, for example by
partially absorbing D (Steigmiller et al., 2014b), which is then automatically added to a
node label in the completion graph if the node is possibly an instance of A, i.e., the candidate
concepts indicate which completely defined concepts are possibly satisfied. Hence, if A+
is not added to a node label, then we know that A is not a (possible) subsumer of the
concepts in the label of such a node (even if we allow the knowledge base to contain concept
equivalence axioms of the form A  D). Formally, we can define the requirements on such
candidate concepts as follows:
2. Note that this only works as long as there are no other axioms of the form A v D0 , A u A0 v D0 ,
A0 u A v D0 , or A  D0 with D0 6= D in the knowledge base.

559

fiSteigmiller & Glimm

Definition 14 (Candidate Concept). Let K be a knowledge base containing a complete
definition of the form A  D. We say that A+ is a candidate concept for A if for every fully
 ) (fully saturated saturation graph
expanded and clash-free completion graph G = (V, E, L, 6=
+
S = (V, E, L)), it holds that A  L(v) if K |= C1 u. . .uCn v A, where {C1 , . . . , Cn } = L(v)
({C1 , . . . , Cn } = L(v) and v as well as the nodes representing individuals are neither critical
nor clashed).
Of course, with axioms of the form > v A+ , we can enforce that A+ is added to all node
labels and, hence, it represents a valid candidate concept for A. To be useful in practice, it
is, however, desired that such concepts are added to as few node labels as possible (without
introducing additional overhead) and, therefore, reasoners usually employ sophisticated
absorption techniques to generate better candidate concepts (Steigmiller et al., 2014b). As
a consequence, they are very handy for the identification of non-subsumptions as illustrated
in the following example.
Example 3. Let us assume that the TBox T2 consists of the axioms
A1 v r.B

A2 v s.B u (r. t B)

A3  s.B u r.B,

and we are interested in the classification of T2 . In order to get an (automatic) indication of
concepts that could be subsumed by the completely defined concept A3 , we create a candidate
concept for A3 by (partially) absorbing the negation of A3 s definition. Hence, by partially
absorbing s.B t r.B to B v s .A+
3 (the part r.B cannot be absorbed trivially), we
+
obtain the candidate concept A3 for A3 . Note that this absorption only adds B v s .A+
3 to
in
a
label
indicates
T2 without removing or rewriting other axioms. Now, the absence of A+
3
that A3 is not a subsumer of the concepts in this label, which can be used for classification.
In particular, if we saturate A1 , A2 , A3 , and B, then s .A+
3 is added to the label of the
is
propagated
to
the
representative
nodes for A2 and A3 .
representative node for B and A+
3
In particular, since the label of the representative node for A1 does not contain A+
3 , we know
that A1 v A3 does not hold without any special consideration of the axiom A3  s.B ur.B.
However, we still have to determine whether A2 is satisfiable and which concepts are the
subsumers of A2 to complete the classification of T2 . For this, we (have to) fall back to the
tableau algorithm and, in principle, we have to perform a satisfiability test for A2 and a
subsumption test for each possible subsumer of A2 , i.e., for the (atomic) concepts that are
(possibly non-deterministically) added to the root node in the satisfiability test, we have to
checks whether they are actual subsumers in all models.
Although the candidate concepts already allow for a significant pruning of subsumption
tests, there are still ontologies where these candidate concepts are added to many node
labels, especially if only a limited absorption of D for an axiom of the form A  D is
possible. Hence, A can still be a possible subsumer for many concepts.
The saturation graph can, however, again be used to improve the identification of (more
or less obvious) non-subsumptions. Basically, if a candidate concept A+ for A  D is in
the label of a node v in the completion graph, then we test whether merging v with the
saturated node vnnf(D) is possible. Since D is often a conjunction, we can also try to merge
v with the representative node for a disjunct of nnf(D). If the models can be merged
as defined below, then v is obviously not an instance of A.
560

fiPay-As-You-Go Description Logic Reasoning

Definition 15 (Model Merging). Let S = (V, E, L) be a fully saturated saturation graph
 ) a fully expanded and clash-free completion graph for a knowledge
and G = (V 0 , E 0 , L0 , 6=
base K. A node v  V is mergeable with a node v 0  V 0 if
 v is not critical, not nominal dependent, and not clashed;
 L(v)  L0 (v 0 ) does not contain {C, nnf(C)} for some concept C;
 L(v)  L0 (v 0 ) does not contain concepts A1 and A2 such that A1 u A2 v C  K and
C
/ (L(v)  L0 (v 0 ));
 v 0 is not an r-neighbour of v 0 for a concept r.Self  L(v);
 v is not an r-neighbour of v for a concept r.Self  L0 (v 0 );
 C  L0 (w0 ) for every r-neighbour w0 of v 0 and r.C  L(v);
 C  L(w) for every r-successor w of v and r.C  L0 (v 0 );
 nnf(C)  L0 (w0 ) for every r-neighbour w0 of v 0 and 6 m r.C  L(v); and
 nnf(C)  L(w) for every r-successor w of v and 6 m r.C  L0 (v 0 ).
Note that the conditions are designed such that they can be checked very efficiently and
it is clear that some of the conditions can be relaxed further. For instance, it is not necessary
to enforce that v is not nominal dependent. In principle, we only have to ensure that there
is no interaction with the generated completion graph, which can, for example, also be
guaranteed if the concept tested for satisfiability does not use nominals in the completion
graph. In addition, if the model merging fails due to concepts in the completion graph that
have an interaction with the tested node in the saturation graph, then we can simply extend
the saturation graph with a new node, where also the problematic concepts are considered,
and retest the model merging with this node. For instance, if a node v 0 in the completion
graph is not mergeable with a node v in the saturation graph due to an axiom A1 u A2 v C
in the knowledge base for which A1  L0 (v 0 ), A2  L(v), and C 
/ (L0 (v 0 )  L(v)), then we
can saturate a new node w with L(w)  L(v)  {C} and check whether w is mergeable.
In contrast, if concepts from the tested node in the saturation graph interact with the
completion graph, then it is often not easily possible to extend the model merging approach
such that non-subsumption can be guaranteed. In particular, we are not interested in
modifying the completion graph since it also has to be used for other model merging tests.
In addition, a recursive model merging test, where we check whether the neighbours of a
node in the completion graph are mergeable with propagated concepts from the saturation
graph, is non-trivial since we have to exclude interactions with already tested nodes. For
example, if a node v 0 in the completion graph is not mergeable with a node v in the
saturation graph due to an r-neighbour w0 of v 0 and a concept r.C in the label of v for
which C 
/ L(w0 ), then a recursive model merging could test whether w0 is mergeable with
vC . However, it would also be necessary to guarantee that the merging of w0 with vC does
not cause new consequences that are propagated back to v 0 , which is especially non-trivial
if there are several universal restrictions in the label of v that would affect w0 .
561

fiSteigmiller & Glimm

Example 4. To continue the classification of the TBox T2 from Example 3, we (have to)
build a completion graph for A2 with the tableau algorithm, which is straightforward. In
particular, we can directly see that A2 is satisfiable and only A3 can be a possible subsumer
of A2 (since the candidate concept A+
3 is propagated to the root node for A2 from the existentially restricted s-successor with B in its label). To apply model merging, the saturation of
the different alternatives/disjuncts that correspond to A3 is required, i.e., we now assume
that the concepts s.B and r.B have also been saturated, which is trivial since no new
consequences are implied and all created and referred nodes can completely be handled by
the saturation. If the tableau algorithm has added the disjunct r. to satisfy r. t B,
then the model merging fails since vs.B has an interaction with the r-successor in the
completion graph that has been constructed to satisfy s.B and for vr.B an interaction
with r. can obviously not be excluded. Hence, it would be required to test whether A3 is
a subsumer of A2 by checking the satisfiability of A2 u A3 . In contrast, if B has been
added, then none of the model merging conditions are satisfied for vr.B and, therefore, we
can directly conclude that A3 is not a subsumer of A2 .
Note, although other proposed (pseudo) model merging techniques (Haarslev, Moller,
& Turhan, 2001) work, in principle, in a very similar way, there are also some significant
differences. For example, the presented merging test is only applied if corresponding candidate concepts are in the label of nodes, which already reduces the number of tests. In
addition, we test the merging against nodes from the saturation graph and, therefore, we
do not have any significant overhead in creating appropriate (pseudo) models. In contrast,
for other approaches it is often necessary to build separate completion graphs for those
concepts for which the model merging is to be applied. Moreover, the presented approach
is also applicable to very expressive DLs such as SROIQ, whereas other approaches often
deactivate model merging if certain language features are used (e.g., nominals). Of course,
very expressive DLs may produce more critical nodes and, therefore, they potentially reduce
the model merging possibilities, but it is not necessary to completely deactivate it, which
results in a very good pay-as-you-go behaviour.

5. Saturation Improvements
Obviously, the support of the tableau algorithm with the saturation works better when
as few nodes as possible are marked critical. However, since our saturation procedure
does not completely support all language features, we easily get critical nodes even when
the unsupported language features are only rarely used in the knowledge base. This is
especially problematic if the critical nodes are referenced by many other nodes, whereby
they also have to be considered critical. In the following, we present different approaches
about how the saturation can be improved such that the number of critical nodes can be
reduced. As a result, a better support of the tableau algorithm is possible.
5.1 Supporting More Expressive Language Features
As known from the literature, saturation procedures can be extended to more expressive
Horn DLs, e.g., Horn-SHIQ (Kazakov, 2009) or even Horn-SROIQ (Ortiz, Rudolph, &
Simkus, 2010). Although it has been shown that such extensions can be very efficient for
562

fiPay-As-You-Go Description Logic Reasoning

ontologies in these fragments, it is not completely clear how they perform for ontologies
that use language features outside of these fragments, for example, if they are used to
partially saturate ontologies as for our approach. In particular, the worst-case complexity
for such procedures is not polynomial and, therefore, they can easily cause the construction
of very large saturation graphs with corresponding large memory requirements. However,
in practical implementations, we can simply limit the number of nodes that are processed
by the saturation by directly marking the remaining nodes as critical. Hence, we can easily
support some features of such Horn-languages without risking that the memory consumption
is increased too much without gaining some benefits.
In particular, it is very interesting to relax the restriction that concepts are only propagated to predecessor nodes for universal restrictions of the form r.C. This is required for
the saturation procedure presented in Section 3 to enable the reuse of nodes, but it can be
extended such that a full support of universal restrictions is possible. Of course, we are not
allowed to directly modify existing r-successors, but we can easily create and saturate copies
of the existing r-successors that we extend by the propagated concept C. In addition, we can
remove the edges to the previous r-successors such that the incompleteness detection rule
C for the concept r.C does not mark the node as critical, which is obviously not the case
if now all newly connected r-successors include the concept C and are completely handled.
Note that these copies and extensions of nodes can be realised very efficiently. Basically,
we first apply the default saturation rules and, afterwards, we extend only those successors
where the saturation has not already added the concept C. In addition, we can use, for each
successor node that has to be extended, a mapping for the concepts, for which the node has
to be extended, to the copied and extended nodes, whereby we can reuse already created
node extensions. Thus, if several predecessors propagate the same concepts to the same
successors, then we create a node with the corresponding extension only once. This can be
seen as an (efficient) implementation of the so-called node contexts, which serve as basis for
many saturation procedures that can fully handle universal restrictions (Simanck et al.,
2011, 2014; Bate et al., 2015). In particular, our extension mapping, i.e., the mapping
from nodes to copies of the nodes extended by the additional concepts, can be seen as a
representation of such node contexts. For example, if we have a node vA as an r-successor
of v and r.B  L(v), then we create a copy of the node vA , say vA,B , for which B is added
and which is then used as r-successor of v instead of vA . With our extension mapping, we
then also store that the extension of vA by B can be found in the node vA,B such that we
can reuse it. Note, however, that we create the copy only if B is not already in the label of
vA . Moreover, by directly copying the nodes (with the derived consequences), a repetition
of many rule applications is not necessary.
Support for at-most restrictions of the form 6 1r.> can be achieved analogously. The
labels of corresponding r-successors can easily be merged into a new node, which can then be
used to replace the other r-successors. Again, we can use a mapping such that the merging
of certain successors always results in the same (possibly new) node. If there is a remaining
r-successor v 0 that also has to be merged to a predecessor v 00 for a node v, then we add all
the concepts in the label of v 0 to the label of v 00 and we make v also an inv(r)-successor of
v 00 . Thus, Horn-SHIF can (almost) completely be supported with rather small extensions
of the presented saturation procedure.
563

fiSteigmiller & Glimm

More difficult is the support of nominals. Already a complete nominal support for the
DL EL++ would potentially introduce some significant overhead. In particular, it would be
necessary to store, for every node v and every nominal {a}, which descendants of v are using
the nominal {a}, i.e., if a descendant of v has a nominal {a} in its label, then we would have
to store for v that the nominal {a} is used by this descendant. If we would find a node v,
for which it is stored that a nominal {a} is used by several descendant nodes, say v 1 , . . . , v n ,
then we would have to create a new node u where the labels of v 1 , . . . , v n were merged, and
we would have to reproduce the paths of predecessors from the merged nodes up to v
such that potentially new consequences can also be propagated to v. However, since the
majority of EL ontologies use nominals only in much simpler ways (e.g., with concepts of
the form r.{a}) for which the presented saturation procedure is already sufficient, a more
sophisticated nominal handling does currently not seem to be required.
Saturation procedures can further be extended to non-Horn DLs, for instance, saturation procedures have been proposed for the DLs ALCH (Simanck et al., 2011), ALCI
(Simanck et al., 2014), and even for SHIQ (Bate et al., 2015). For this, they also have
to handle non-determinism that is, for example, caused by disjunctions, which is typically
realised by simply considering/saturating all non-deterministic alternatives. If the same
concepts are derived for all alternatives, then they are interpreted as actual consequences
of the knowledge base. If the number of alternatives is very large, then such a (naive)
saturation approach might become impractical. Although also the tableau algorithm has
to consider all alternatives in the worst-case, it is doing it successively, i.e., it is trading
memory requirements against a potentially increased runtime. Moreover, tableau algorithms usually implement a large amount of optimisations to reduce the non-deterministic
alternatives that have to be considered. Most notably, dependency directed backtracking
allows for evaluating only those alternatives of non-deterministic decisions that are indeed
relevant, i.e., which are involved in the creation of clashes. Since saturation algorithms do
not track dependencies between derived facts, their ability to determine which alternatives
of non-deterministic decisions do not have to be considered (since they would result in the
same clashes) is very limited. Unfortunately, the tracking of dependency information makes
a simple reuse of nodes (which is the foundation of saturation procedures) impossible or, at
least, much more involved.
Although it has been shown that saturation procedures extended to non-deterministic
language features can work very well for a range of ontologies (Simanck et al., 2011), more
investigations are required in order to understand whether (or in which cases) they are
better than tableau algorithms for more expressive DLs. However, the development and
the implementation of saturation-based reasoning systems for more expressive DLs seems
challenging and, to the best of our knowledge, a saturation-based procedure/reasoner for
expressive DLs such as SROIQ does not yet exist. Hence, it can be an interesting compromise, as presented in this paper, to keep the (basic) saturation algorithm deterministic
and to process the remaining parts with the tableau algorithm, which is typically coupled
with several well-established optimisations (e.g., semantic branching, Boolean constraint
propagation, dependency directed backtracking, unsatisfiability caching) to handle nondeterminism. Alternatively, one can process non-deterministic language features with the
saturation procedure only as long as certain limits are not reached (e.g., a memory limit or
564

fiPay-As-You-Go Description Logic Reasoning

L(vA2 ) =
L(vA1 ) =



>, A1 , s.A2

	



vA1

s

vA2

r

vA3

s
L(v{a} ) =



>, {a}, B, s .B

v{a}

	

L(v{b} ) =



	

>, A2 , s.{b}, r.A3 , A1 t A3

r

v{b}

L(vA3 ) =



>, A3 , s.{c}

	

s
r

v{c}

L(v{c} ) =

>, {b}, r.{a}, r.{c}, 6 1r.>



>, {c}

	

	

Figure 2: Incompletely handled saturation graph for testing the satisfiability of the concept
A1 from Example 5

an upper bound for the number of saturated, non-deterministic alternatives), and can simply
mark remaining nodes as critical such that they are processed by the tableau algorithm.
5.2 Improving Saturation with Results from Completion Graphs
As already mentioned, even if there is only one node for an individual that is critical, then
the presented saturation procedure also marks all nominal dependent nodes as critical.
This easily limits the improvement from the saturation for ontologies that intensively use
nominals. Analogously, if there are few nodes with incompletely handled concepts (e.g.,
disjunctions) and these nodes are referenced by many other nodes, then all these other
nodes are also critical although they do not necessarily have concepts in their label that
cannot be handled completely. Both issues are also illustrated in the following example:
Example 5. Let us assume that the TBox T3 contains the following axioms:
A1 v s.A2
A3 v s.{c}
{a} v B

A2 v s.{b}

A2 v r.A3

A2 v A1 t A3

{b} v r.{c}

{b} v 6 1r.>



B v s .B
{b} v r.{a}

For testing the satisfiability of the concept A1 w.r.t. TBox T3 , we generate the saturation
graph that is depicted in Figure 2. Note, the node v{b} for the individual b cannot be
completely handled by the saturation due to the concept 6 1r.> in the label of v{b} , which
would require that v{a} and v{c} are merged. Therefore, v{b} is critical and we also have
to consider all nodes as critical that refer to such critical nodes, which is, for example, the
case for the node vA2 . Moreover, since one node for an individual is critical, we cannot
exclude that more consequences are propagated to other individuals and, therefore, possibly
also to other nominal dependent nodes. For instance, the merging of v{a} and v{c} would
propagate the concept B to the label of vA3 . Thus, also vA3 is critical although it does
not directly contain a concept that cannot be handled by the saturation. Analogously, the
label of vA2 contains the disjunction A1 t A3 , which is also not completely processed by the
565

fiSteigmiller & Glimm

saturation and, therefore, we have to mark all ancestor nodes of vA2 as critical (if this is
not already the case), even if they do not contain problematic concepts. As a consequence,
we obtain a saturation status S = (So , S , S! ), where v{b} has a tight at-most restriction,
i.e., S = {v{b} }, and all nodes are nominal dependent as well as critical, i.e., So = S! =
{v{a} , v{b} , v{c} , vA1 , vA2 , vA3 }.
Of course, the saturation can be extended in several ways to better support features of
more expressive DLs (see Section 5.1), but, to the best of our knowledge, there exists no
saturation algorithm that completely covers all the features of very expressive DLs such as
SROIQ. Hence, if a knowledge base uses some of the unsupported features, then we easily
run into the problem that the saturation becomes incomplete and we possibly get many
critical nodes.
An approach to overcome the issues with critical nodes is to patch, i.e., update, the
saturation graph with results from fully expanded and clash-free completion graphs that
are generated for consistency or satisfiability checks. Roughly speaking, the idea is to
replace the labels of critical nodes in the saturation graph with corresponding labels from
these completion graphs, for which we know that they are completely handled by the tableau
algorithm. We call such nodes patched nodes. Then, we apply the saturation rules again and
we update the saturation status, which hopefully results in an improved saturation graph
with fewer critical nodes. Note, however, that simply adding non-deterministically derived
concepts from labels of completion graphs to the saturation easily leads to unsound results.
Hence, we distinguish deterministically and non-deterministically derived concepts when
updating the saturation by simultaneously managing two saturation graphs: one where only
the deterministically derived concepts are added, i.e., the deterministic saturation graph,
and a second one, where also the non-deterministically derived concepts and consequences
are considered, i.e., the non-deterministic saturation graph. If the non-deterministic
consequences have only a locally limited influence, i.e., the non-deterministically added
concepts propagate new consequences only to a limited number of ancestor nodes, then,
by comparing both saturation graphs, we can possibly identify ancestor nodes that are
not further influenced, which can then be considered as non-critical. By reducing the
number of critical nodes in the saturation, this approach then allows for further improving
the construction of new completion graphs by transferring new and more results from the
updated saturation.
In order to describe the approach in more detail, we first define a saturation patch,
which constitutes the data structure for managing the information that is necessary for
updating a saturation graph.
Definition 16 (Saturation Patch). Let fclos(K) (Rols(K)) denote the concepts (roles) that
possibly occur in completion graphs for the knowledge base K as defined in Definition 5. A
saturation patch P for a saturation graph S = (V, E, L) w.r.t. K is a tuple P = (Vp , Ld , Ln ,
Mc , Vo ), where
 Vp  V denotes the set of patched nodes in the saturation graph,
 Ld : Vp  2fclos(K) is the mapping of patched nodes to a set of deterministically derived
concepts,
566

fiPay-As-You-Go Description Logic Reasoning

 Ln : Vp  2fclos(K) is analogously the mapping of patched nodes to a set of nondeterministically derived concepts,
 Mc : Vp  Rols(K)  fclos(K)  IN0 is the mapping of at-most cardinality restrictions
of the form 6 m r.C on patched nodes (represented as a tuple of the node v, the role
r, and the qualification concept C) to the number of merging candidates, and
 Vo  Vp denotes the patched nodes that are nominal dependent.
A saturation patch obviously has to identify the nodes that should be patched/updated,
which is realised with the set Vp . For each node in Vp , the mappings Ld and Ln contain the
concepts from the nodes label in the completion graph that are derived deterministically
and non-deterministically, respectively. Hence, these mappings determine how a nodes
label in the saturation graph can be extended such that it is no longer critical. We also
have to store the number of merging candidates (Mc ) and the patched nodes that are
nominal dependent (Vo ) because this information is required for the generation of an updated
saturation status. Note that we consider the number of merging candidates and the nominal
dependencies as non-deterministic information since it is often not possible to correctly
extract the corresponding deterministic information from completion graphs. For example,
state-of-the-art reasoners are usually searching blocker nodes by checking more detailed
conditions as defined for pairwise blocking, whereby a node can possibly also be blocked if
the label is a subset of the label from the blocker node (Horrocks & Sattler, 2001). If the
blocker node is directly or indirectly using nominals, i.e., it is nominal dependent, then also
the blocked node has to be considered as nominal dependent. Hence, we have to consider
the nominal dependency as non-deterministic information since the nominal dependency
could be caused by a concept that is in the label of the blocker node but not in the label of
the blocked one.
Especially the root nodes of completion graphs constructed for satisfiability and consistency tests are very suitable for the extraction of patches. For instance, in a fully expanded
and clash-free completion graph for testing the satisfiability of a concept C, the root node
has C in its label and can be used as a patch for the node vC in the saturation graph. The
completion graph of a consistency check can be used to patch representative nodes for nominals. Of course, for the patching of nominal dependent nodes, we have to ensure some kind
of consistency, i.e., the dependent nominals have to be compatible with the representative
nodes of these nominals in the saturation graph and the already applied patches for these
nodes. A simple form of compatibility can be defined as follows:
Definition 17 (Saturation Patch Compatibility). Let P 1 = (Vp1 , L1d , L1n , Mc1 , Vo1 ), . . . , P n =
(Vpn , Lnd , Lnn , Mcn , Von ) be saturation patches for a saturation graph S w.r.t. a knowledge base
i } for 1  i  n. We say that P 1 , . . . , P n are compatible if
K, where Vpi = {v1i , . . . , vm
i
 ) can be built for K such
a fully expanded and clash-free completion graph G = (V, E, L, 6=
1
1
n
n
that it contains the nodes w1 , . . . wm1 , . . . , w1 , . . . wmn with L(wji ) = Lid (vji )  Lin (vji ) for
1  j  mi and 1  i  n.
In principle, we are not limited to the root and nominal nodes for the extraction of
patches, but a more detailed analysis of the completion graph is required for other nodes.
For example, the tableau algorithm does not apply the -rule for a concept r.C in the
567

fiSteigmiller & Glimm

label of a node v if v already has an r -predecessor v 0 with C in its label. Hence, if the
predecessor v 0 directly or indirectly uses nominals, then also v has to be considered as
nominal dependent. Moreover, for other nodes in the completion graph, it is often not
clear which concepts have to be considered as non-deterministically derived consequences.
For instance, if we create, for the concepts r.C and r.D in the label of a node v, the
r-successor v 0 and we extract a patch for vC from v 0 , then D has to be identified as a
non-deterministically derived concept. For this, it is in principle necessary to track and
analyse the dependencies between facts and their causes in the completion graph. If this
is efficiently supported by a reasoning system, then the extraction of patches can also be
extended to other nodes in the completion graph. Otherwise, the patch creation can simply
be restricted as appropriate.
The saturation patches are applied to a saturation graph as follows:
Definition 18 (Saturation Patch Application). Let S = (V, E, L) be a saturation graph and
P = (Vp , Ld , Ln , Mc , Vo ) a saturation patch for S. The deterministic (non-deterministic)
application of P to S yields a deterministically (non-deterministically) extended saturation
graph Sd (Sn ) of S that is obtained by saturating the saturation graph (V, E, L0 ), where
L0 = {v 7 L(v) | v  V \ Vp }  {v 7 Ld (v) | v  Vp } (L0 = {v 7 L(v) | v  V \ Vp }  {v 7
Ld (v)  Ln (v) | v  Vp }).
Since we are interested in a deterministic and in a non-deterministic saturation graph, we
create a copy of the saturation graph as soon as we have a patch with non-deterministically
derived concepts and, then, we use the non-deterministic application of patches only for the
copy. Although we can also fully saturate the non-deterministic saturation graph by simply
using the presented saturate function, this potentially derives unwanted consequences since
the application of all saturation rules for all nodes possibly propagates new consequences
to patched nodes. This can be unfavourable if patched consequences are derived from the
processing of different non-deterministic alternatives. In particular, if a node v and an
ancestor of v are patched, then the saturation rules might propagate new consequences
obtained from the patching of v up to the ancestors. If an ancestor is, however, patched
with concepts from another completion graph, where different non-deterministic alternatives
are processed, then we possibly mix consequences of different alternatives in the saturation
graph, which easily limits the effectiveness of our approach. For example, if v contains the
disjunction r.A t r.A, and we patch v with the non-deterministic extension r.A, then
the patching of the r -predecessor v 0 of v with the non-deterministic extension A allows
the application of the -rule for the concept r.A in the label of the node v such that the
concept A is propagated to v 0 . As a consequence, we would infer with the saturation that
v 0 is (possibly) clashed since A and A are in its label. Since all (new) consequences in the
non-deterministic saturation graph are considered to be non-deterministic, this does not
produce incorrect results. In order to, nevertheless, avoid the derivation of such unwanted
consequences, we can saturate the non-deterministic saturation graph that contains the
patched nodes V with a modified saturate\V (S) function, where only the -rule is applied
to the nodes in V and the -rule is modified such that it does not propagate concepts to a
node v  V . For this (and for a more precise detection of the saturation status), we gather
all patches in one combined patch and keep this patch in addition to the deterministic
568

fiPay-As-You-Go Description Logic Reasoning

and non-deterministic saturation graph. The patches can straightforwardly be combined
by using the -operator defined as follows:
Definition 19 (Saturation Patch Composition). Given two saturation patches P and P 0
with P = (Vp , Ld , Ln , Mc , Vo ) and P 0 = (Vp0 , Ld 0 , Ln 0 , Mc0 , Vo0 ), the saturation patch P  P 0 is
defined as the tuple consisting of
 Vp  Vp0 ,
 Ld  {v 7 C | v 7 C  Ld 0 and v 
/ Vp },
/ Vp },
 Ln  {v 7 C | v 7 C  Ln 0 and v 
 Mc  {hv, s, Ci 7 n | hv, s, Ci 7 n  Mc0 and v 
/ Vp }, and
 Vo  (Vo0 \ Vp ).
Note that if both patches contain information about the same node, then we keep the
information for this node only from one (the new) patch instead of mixing the information.
Thus, the information from the other patch gets lost for all common nodes, which is,
however, not problematic since both patches describe valid extensions.
In addition to the modified saturation function, we should reprocess the ancestors for
patched nodes in the non-deterministic saturation graph if the patching removes previously
added non-deterministic consequences in order to avoid the mixing of consequences from
different non-deterministic alternatives. For example, if a non-deterministically derived
concept such as r.C was added or propagated to a node label and is removed by patching
this node, then the r -predecessor should also be rebuilt from the deterministic saturation
graph such that unnecessary non-deterministic consequences (e.g., C) can also be removed.
For practical implementations, we can obviously limit the number of ancestor nodes that
are updated or processed for new non-deterministic consequences in the non-deterministic
saturation graph in order to limit the overhead of the patch application. If the limit is
reached, then the remaining ancestors can simply be marked as critical. Also note that we
can reuse all the data of the deterministic saturation graph in the non-deterministic one for
nodes that are not influenced by a patch with non-deterministic consequences.
In order to be able to use patched saturation graphs for the support of the tableau
algorithm, e.g., for the transfer of results into completion graphs, we have to update the
saturation statuses after the application of the patches. Similarly to the rule application of
the non-deterministic saturation graph, we do not want to propagate a status to a patched
node from the successors. Therefore, we analogously use a modified status\V function
instead of status, where the rules of Table 3 and 4 are only applied to nodes that are not
in V . This also requires that we use a modified ]mcands0 function in status\V since, for
the patched nodes, we have to use the correct information as given by the patch. To be
more precise, when Mc denotes the mapping to the number of merging candidates in the
considered patch, then ]mcands0 (v, s, D) has to return Mc (hv, s, Di) if v is a patched node,
and ]mcands(v, s, D) otherwise. In addition, we have to correctly initialise the sets S ,
So , and S! for the patched nodes with the information from the applied patches. For the
non-deterministic saturation graph, all patched nodes are obviously non-critical since their
labels have been extracted from fully expanded and clash-free completion graphs. Hence,
569

fiSteigmiller & Glimm

we only have to initialise S and So for a (combined) patch P = (Vp , Ld , Ln , Mc , Vo ), which
can be realised by setting So = {v | v  Vo }, and
S = {v | v  Vp and 6 m r.C  (Ld (v)  Ln (v)) such that Mc (hv, r, Ci) = m}.
For the deterministic saturation graph, we additionally have to set S! to {v | Ln (v) 6= } in
order to mark all patched nodes directly as critical if they could depend on non-deterministic
consequences. After the initialisation, we can call the function status\Vp to obtain a full
status for the corresponding saturation graph, which can then be used to further improve
the support of the tableau algorithm.
Analogously to the deterministic and non-deterministic saturation graphs, every new
saturation status can incrementally be updated from the last generated status for the last
saturation graphs by sequentially updating the ancestors for the newly patched nodes.
Hence, also the generation of new saturation statuses is not causing a significant overhead
in practice.
The patching of saturation graphs enables a more sophisticated support of tableau algorithms. On the one hand, the patching reduces the number of critical nodes and, therefore,
the optimisations described in Section 4, such as blocking the expansion of successors nodes
in the completion graph or the extraction of subsumers, are better applicable. On the
other hand, we can now also use the non-deterministic saturation graph for the support,
e.g., in the classification process. If a node vA in the non-deterministic saturation graph
is not critical, then the label of vA in the non-deterministic saturation graph describes all
possible subsumers of A. Thus, if vA is not critical in the non-deterministic saturation
graph, then its label can be used to prune possible subsumers. Moreover, we can use the
non-deterministic saturation graph to find identical labels that can be used for blocking
the processing/expansion of successors nodes in the completion graph. Of course, we still
require that the corresponding nodes in the (non-deterministic) saturation graph are not
critical. In contrast, the restriction that the nodes in the saturation graph are not allowed
to be nominal dependent for the blocking can be relaxed such that it works sufficiently well
for many real world ontologies. Basically, we patch all nodes that represent individuals
in the saturation graph after the consistency check with the corresponding nodes in the
obtained fully expanded and clash-free completion graph. Furthermore, we ensure, on the
one hand, that each subsequent saturation patch is compatible to this initial patch, i.e.,
we only create patches for nominal dependent nodes if all the labels of the nodes for the
individuals in a completion graph are identical or subsets of the corresponding labels of the
initial completion graph from the consistency check. On the other hand, we do not create
patches for nodes if they depend on new nominals, i.e., on nominals that are introduced by
the NN-rule. This ensures that the nodes in the saturation graphs can be used for blocking
as long as we expand the nodes for the individuals in the same way as in the initial completion graph. Thus, if nominal dependent nodes are used for blocking, we collect the blocked
nodes in a queue and we reactive these nodes if it becomes necessary to expand the nodes
for the individuals in another way as in the completion graph for the initial consistency
check. Of course, with a more exact tracking of the dependent nominals, e.g., by exactly
saving on which nominals a node possibly depends, we can refine and improve this technique significantly. Obviously, if we use a node for blocking for which it is exactly known on
which nominals it depends, then we only have to reactivate the processing of this node if the
570

fiPay-As-You-Go Description Logic Reasoning

nodes for the corresponding individuals are expanded differently. Although this approach
keeps the patching of the saturation graphs consistent, i.e., the compatibility of the patches
is automatically ensured, it is more restrictive than required in Definition 17. However,
it allows for identifying potential incompatibilities with other techniques of tableau-based
reasoning systems, e.g., with variants of completion graph caching techniques (Steigmiller,
Glimm, & Liebig, 2015).
Due to the non-deterministic decisions of the tableau algorithm, a critical node in the
saturation graph can be patched in several ways. Moreover, we can patch an already
patched node to (hopefully) improve the non-deterministic saturation graph, i.e., we try
to reduce the number of nodes that are influenced by the non-deterministic consequences
and/or marked as critical. Thus, we need a strategy that decides for which nodes we have
to extract patches from a fully expanded and clash-free completion graph such that the
non-deterministic saturation graph can be improved. As already described, we can only
extract patches from nodes for which all information can be safely extracted and they do
not make the non-deterministic saturation graph inconsistent. In addition, the strategy
has to keep the number of patches as small as possible since we have to update the data
structures for every patch.
A simple example for such a strategy is to create only patches when they reduce the
number of non-deterministic propagation concepts for the patched nodes. With this strategy
we would prefer a patch that adds the non-deterministic set of concepts {r.C, A1 , A2 }
in comparison with a patch that has the non-deterministic extension {s.D, t.D}. This
strategy ensures, at least, that we do not create arbitrary patches, which avoids an oscillation
between different possibilities, and we clearly favour the creation of patches that do not
influence other nodes. However, we cannot guarantee that the non-deterministic saturation
graph is actually improved. For example, the concept r.C could propagate C to several
predecessors and also the processing of C could further influence many ancestors, whereas
the patch with {s.D, t.D} might only influence few predecessors. Therefore, if the node is
already patched with {s.D, t.D} and we create a new patch with {r.C, A1 , A2 } due to the
fewer propagation concepts, then we even worsen the non-deterministic saturation graph.
In order to counteract this, we should also extract patches from the saturation graph if we
detect that a critical node in the deterministic saturation graph is labelled with the same
concepts as in the non-deterministic saturation graph and the node in the non-deterministic
saturation graph is not critical. With this kind of internal patch we can ensure that if the
saturation has identified a node that is neither critical nor influenced by non-deterministic
consequences, then we remember this solved state of the node and we do not overwrite
its state by integrating other patches in the non-deterministic saturation graph. Of course,
the strategy for the creation and extraction of patches optimally also considers the nominal
dependency and tight at-most restrictions by trying to reduce the number of such nodes.
Example 6. As mentioned, all nodes in the saturation graph of Figure 2, which is generated
for testing the satisfiability of the concept A1 w.r.t. TBox T3 (p. 564), are critical. As a
consequence, we have to check the satisfiability of A1 with the tableau algorithm in detail.
For this, we first check the consistency of the individuals a, b, and c, which results in a simple
completion graph, where the nodes for a and c are merged. From this completion graph, we
extract an initial saturation patch P 1 for the individuals, i.e., P 1 = (Vp1 , Ld 1 , Ln 1 , Mc1 , Vo1 )
571

fiSteigmiller & Glimm

with Vp1 = {v{a} , v{b} , v{c} }, Ld 1 = {v{a} 7 {>, {a}, {c}, B, s .B}, v{c} 7 {>, {a}, {c}, B,
s .B}, v{b} 7 {>, {b}, r.{a}, r.{c}, 6 1 r.>}}, Ln 1 = , Mc1 = {hv{b} , r, >i 7 1}, and
Vo1 = {v{a} , v{b} , v{c} }. Note, although the nodes for the individuals a and c are merged in
the completion graph, we have to patch v{a} and v{c} separately since the saturation does
not support the merging of nodes. Also note that the completion graph for the consistency
check is deterministic and, therefore, the mapping of nodes to non-deterministically derived
concepts is not required, i.e., each node has to be mapped to  for Ln 1 . However, for ease
of presentation, we omit uninteresting patch data and we simply use  for Ln 1 .
By deterministically applying P 1 to our initial saturation graph, we obtain a new deterministic saturation graph, where the nodes are extended by the data from the applied patch.
In particular, v{c} is extended by the concepts {a}, B, and s .B in this deterministic saturation graph, whereby the concept B is also propagated to vA3 and, as a consequence, the
label of vA3 is extended to the set {>, A3 , s.{c}, B, s .B}. The saturation status for the
new deterministic saturation graph reveals that the nodes v{a} , v{b} , v{c} , and vA3 are not
critical. Thus, we have, in principle, already shown the satisfiability of the concept A3 . In
contrast, vA1 is still indirectly critical due to the incompletely handled disjunction A1 t A3
in the label of vA2 .
In order to test the satisfiability of A1 now with the tableau algorithm, we initialise
a new completion graph with a node v for which the concept A1 is asserted. Since the
disjunction A1 t A3 will be added to this completion graph for the s-successor v 0 of v,
the tableau algorithm has to choose between the disjuncts A1 and A3 . Independently from
the decision, we can obtain a fully expanded and clash-free completion graph that shows
the satisfiability of A1 , but the non-deterministic decision influences the patching of the
saturation graph. For example, by non-deterministically adding A3 to v 0 , the tableau algorithm has to add an s-edge to the node representing c due to A3 v s.{c}  T3 and
then B is propagated to the label of v 0 and, subsequently, also to the label of v due to
B v s .B  T3 and since the node for c has B in its label. Thus, we can extract a
patch P 2 = ({vA1 }, {vA1 7 {>, A1 , s.A2 }}, {vA1 7 {B, s .B}}, , {vA1 }). Since P 2 contains non-deterministically derived consequences, we apply the patch deterministically and
non-deterministically. Although the node vA1 can be considered as fully handled in the nondeterministic saturation graph, it remains critical in the deterministic saturation graph and,
therefore, its usage for supporting (e.g., blocking the expansion of successor nodes in new
completion graphs, identification of (possible) subsumers) the tableau algorithm is limited.
In contrast, if the disjunct A1 were non-deterministically added to v 0 , then we could extract
a saturation patch P 3 = ({vA1 }, {vA1 7 {>, A1 , s.A2 }}, , , {vA1 }) and by applying P 3 ,
we could also consider the node vA1 as non-critical in the deterministic saturation graph.
Hence, we prefer the saturation patch P 3 and we would also extract and apply P 3 , even if
we extracted and applied P 2 from an earlier constructed completion graph.

As of now, we only considered patching from fully expanded and clash-free completion
graphs. Of course, it is also possible to integrate unsatisfiability results from completion
graphs into the saturation graphs. In particular, if the tableau algorithm cannot find a
fully expanded and clash-free completion graph for a concept C, then we can create a patch
where we deterministically extend vC by the concept . Such a management of unsatisfiable
572

fiPay-As-You-Go Description Logic Reasoning

concepts with the saturation graph has the benefit that  is also propagated to other nodes
and we can immediately identify many other unsatisfiable concepts.
It is also worth pointing out that, especially with the extraction and application of
patches, the support of the tableau algorithm with the information provided through the
saturation graphs can be seen as an intelligent caching technique. Although this only corresponds to a limited caching for certain nodes that are not further influenced by predecessors,
it also works, to some extent, with nominals and inverse roles. Moreover, it is very fast and
can automatically propagate unsatisfiability and satisfiability statuses to other concepts.

6. Related Work
There are already some approaches that combine the reasoning techniques of fully-fledged
DL reasoners with specialised procedures for specific fragments. For instance, the reasoning
system MORe (Armas Romero et al., 2012) uses module extraction to identify a part of
an ontology that can be completely handled by a more efficient reasoning system and the
fully-fledged reasoner is then only used for the remaining parts of the ontology. Note, our
approach works more from the opposite direction: we apply the saturation and simply
ignore (or partially process) unsupported features and, then, we detect which parts are not
completely handled. Since MORe uses other reasoners as black-boxes, it is, in principle,
possible to combine arbitrary reasoning procedures by adapting the module extraction.
However, as of now, all fully-fledged OWL 2 reasoners are based on variants of tableau
calculi and the efficient reasoning systems for interesting fragments are usually using variants
of saturation procedures (e.g., completion- and consequence-based reasoning), whereby the
combination of tableau and saturation algorithms currently seems to be the only interesting
one.
Due to the black-box approach, the technique realised in MORe is very flexible. For
example, it is easily possible to exchange the fully-fledged reasoner with a reasoning system
for which it is known that it works best for certain kinds of ontologies. Our approach, on
the other hand, has to be implemented into one single reasoning system and requires the
support of certain techniques, such as binary absorption, to work well. Moreover, compatible
data structures have to be used for both kinds of procedures, which usually means that an
appropriate saturation algorithm has to be integrated into a tableau-based reasoning system.
Our approach, however, has also various advantages. For example, our saturation uses the
same representation of ontologies as tableau algorithms and, therefore, the ontology has to
be loaded only once. In contrast, the reasoners used by MORe have to separately load the
ontology (or parts thereof) since they are used as black-boxes and, usually, they also do not
have compatible data structures. Furthermore, our approach is much more tolerant for the
usage of features outside the efficiently supported fragment. Some of our optimisations can
also be used when all saturated nodes are critical, which could, for example, be the case if
the ontology contains non-absorbable GCIs. In addition, we have presented an extension
that allows for fixing critical parts in the saturation, whereby unsupported features are
not problematic if they are only rarely used in the ontology. In contrast, MORe has to
reduce the module for the efficient reasoner as long as the module contains unsupported
features. Thus, our approach promises a better pay-as-you-go behaviour. Moreover, we can
use intermediate results from the saturation, whereas the technique in MORe relies on the
573

fiSteigmiller & Glimm

externally provided interfaces of the reasoners, which usually only provides basic information
such as the satisfiability of concepts and the subsumers of classes. Therefore, our integration
of the saturation procedure obviously allows for more sophisticated optimisation techniques
such as the transfer of inferred consequences and the blocking of the processing with the
tableau algorithm.
Although both approaches are in principle applicable to different reasoning tasks, our
technique automatically improves reasoning as long as the reasoning task is reduced to
consistency checking with the tableau algorithm. For example, in order to support the
satisfiability testing of complex concepts, our approach does not need any adaptations. For
MORe, however, it would be necessary to check whether the complex concept is in the
module that can be handled by the efficient reasoner in order to achieve an improvement.
Last but not least, we do not need the module extraction technique in our approach, which
can also take a significant amount of time. This is especially an advantage for ontologies
that are almost completely in the efficiently supported fragment since our approach does
not have a similar overhead as the module extraction for such ontologies.
Another reasoning system that combines different reasoning techniques is WSReasoner
(Song et al., 2012), which uses a weakening and strengthening approach for the classification of ontologies. To be more precise, the ontology is first rewritten into a simpler one
(the weakening of the ontology), where not supported language features are (partially) expressed in the fragment that can be handled by the efficient reasoner. Then, a strengthened
version of the weakened ontology is created, where axioms are added such that at least
also the consequences of the original ontology are implied. The weakened and the strengthened ontologies are then classified by the specialised reasoner and possible differences in
the obtained subsumtion relations are verified with a fully-fledged reasoner. Also for WSReasoner, the fragment specific reasoner (usually based on a saturation procedure) and the
fully-fledged reasoner (usually based on a tableau calculus) are used as black-boxes, which
makes them, in principle, exchangeable. However, the weakening and strengthening also
has to be adapted to the language fragment of the efficient reasoner.
Although the technique of WSReasoner is different to the one of MORe, the advantages
and disadvantages in comparison with our approach are in principle the same. However,
the approach of WSReasoner is not as easily extendible to more language features and,
as of now, it is only presented for the DL ALCHIO (with the elimination/encoding of
transitive roles also for SHIO). Moreover, since the nominals are simplified to fresh atomic
concepts, the approach cannot straightforwardly be used for all reasoning tasks. If such a
simplification is, however, applicable, then it often improves the reasoning performance for
corresponding ontologies.
Similarly to WSReasoner, PAGOdA (Zhou et al., 2014, 2015) also uses the weakening
and strengthening approach, however, for different reasoning tasks and by delegating a
different fragment of the ontology to a specialised reasoner. In particular, PAGOdA is
designed for ABox reasoning (e.g., conjunctive query answering) and delegates the majority
of the computational workload to an efficient datalog reasoner. If the lower bound from
the datalog reasoner does not match with the upper bound, then PAGOdA delegates the
query with the relevant parts of the ABox to a fully-fledged reasoner. In order to keep the
relevant parts as small as possible, PAGOdA uses additional optimisations such as relevant
subset extraction, summarisation, and dependency analysis. However, these additional
574

fiPay-As-You-Go Description Logic Reasoning

optimisations also carry the risk that every use of the fully-fledged reasoner introduces
additional overhead, which could be problematic for ontologies where a lot of work has still to
be done by the fully-fledged reasoner. Moreover, maintaining several naive representations
of the entire ABox can easily multiply memory requirements.

7. Implementation and Evaluation
We extended Konclude3 (Steigmiller, Liebig, & Glimm, 2014) with the saturation procedure
shown in Section 3 and with the optimisations presented in Section 4 and 5. Konclude is
a tableau-based reasoner for SROIQ (Horrocks et al., 2006) with extensions for handling
nominal schemas (Steigmiller et al., 2013). Konclude integrates many state-of-the-art optimisations such as lazy unfolding, dependency directed backtracking, caching, etc. Moreover,
Konclude uses partial absorption (Steigmiller et al., 2014b) in order to significantly reduce
the non-determinism in ontologies and, therefore, Konclude is well-suited for the proposed
coupling with saturation procedures.
Our integration of the saturation in Konclude completely covers the language features
of the DL Horn-SHIF by using the saturation extensions described in Section 5.1, where
universal restrictions that propagate concepts to successors and the merging of successors/predecessors due to functional at-most restrictions are handled. The number of nodes
that are additionally processed for the handling of these saturation extensions is mainly
limited by the number of concepts occurring in the knowledge base. However, Koncludes
saturation procedure only supports a very limited handling of ABox data. This is due to
a design decision with which we try to avoid several representations of individuals (and
derived consequences for these) in the reasoning system. Since the saturation could easily
be incomplete for the ABox (e.g., since disjunctions are asserted to few individuals or due
to datatypes), the ABox has often also be handled by the tableau algorithm and several
representations of the ABox can multiply memory requirements. Hence, Konclude primarily handles ABox individuals with the tableau algorithm and uses patches from completion
graphs (as presented in Section 5.2) to improve those parts in the saturation graph that
depend on nominals.
In addition, Konclude saturates the concepts that might be required for a certain reasoning task upfront in a batch processing mode, whereby the switches between the tableau and
the saturation algorithm can be reduced significantly. Moreover, we sort the concepts that
occur in the knowledge base and saturate them in a specific order to maximise the amount
of data that can be shared between the saturated nodes. For example, if the knowledge base
contains the axiom A v B, then we first saturate B and we use the data from vB to initiate
vA . In particular, by copying the node labels, many rule applications can be skipped, which
significantly improves the performance of the saturation procedure. Furthermore, this also
reduces the effort for the saturation status detection. For instance, if vB does not satisfy
an at-most restriction, then this at-most restriction is also not satisfied for vA .
In the following, we present a detailed evaluation that shows the effects of Koncludes
integrated saturation procedure and the presented optimisations for the support of the fullyfledged tableau algorithm. In addition, we compare the reasoning times of Konclude with the
ones of other state-of-the-art reasoners that support TBox reasoning for (almost) all features
3. Konclude is freely available at http://www.konclude.com/

575

fiSteigmiller & Glimm

Repository

#
Axioms
Ontologies

Q0.5
Gardiner
292
5, 842
96
NCBO BioPortal
403 27, 180
1, 116
NCIt
185 178, 818 167, 667
OBO Foundry
502 37, 349
1, 292
Oxford
394 73, 921
3, 433
TONES
203
7, 707
352
Google crawl
414
6, 869
255
OntoCrawler
548
2, 574
136
OntoJCrawl
1, 696
6, 281
311
Swoogle crawl
1, 638
2, 778
132
ORE2014 dataset
16, 555 16, 017
594
ALL
22, 830 16, 647
594

Classes

Q0.5
1, 788
14
7, 518
339
69, 720 68, 862
6, 753
509
8, 543
500
2, 864
96
1, 127
39
124
17
1, 772
50
416
21
3, 846
87
4, 020
74

Properties
 Q0.5
44
7
48
13
116 123
24
4
53
11
40
5
102
44
93
20
62
9
36
10
101
53
97
50

Individuals
 Q0.5
85
2
1, 766
0
0
0
20, 905
6
18, 273
5
65
0
824
1
633
0
838
0
879
0
1, 801 50
2, 271 29

Table 5: Statistics of ontology metrics for the evaluated ontology repositories ( stands for
average and Q0.5 for median)

of the DLs SROIQ, namely FaCT++ 1.6.3 (Tsarkov & Horrocks, 2006), HermiT 1.3.8
(Glimm, Horrocks, Motik, Stoilos, & Wang, 2014), MORe 0.1.6 (Armas Romero et al., 2012),
and Pellet 2.3.1 (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2007). The evaluation uses
a large test corpus of ontologies,4 which has been obtained by collecting all downloadable
and parsable ontologies from
 the Gardiner ontology suite (Gardiner, Horrocks, & Tsarkov, 2006),
 the NCBO BioPortal (Whetzel et al., 2011),
 the National Cancer Institute thesaurus (NCIt) archive (National Cancer Institute,
2003),
 the Open Biological Ontologies (OBO) Foundry (Smith et al., 2007),
 the Oxford ontology library (Information Systems Group, 2012),5
 the TONES repository (Information Management Group, 2008),
 the subsets of the OWLCorpus (Matentzoglu, Bail, & Parsia, 2013) that were gathered
by the crawlers Google, OntoCrawler, OntoJCrawl, and Swoogle,6 and
 the ORE2014 dataset (Matentzoglu & Parsia, 2014).
4. The test corpus and the evaluated version(s) of Konclude v0.6.1 can be found online at http://www.
derivo.de/en/products/konclude/paper-support-pages/tableau-saturation-coupling.html
5. Note that the Oxford ontology library also contains other repositories (e.g., the Gardiner ontology suite),
which we ignored in order to avoid too much redundancy.
6. In order to avoid too many redundant ontologies, we only used those subsets of the OWLCorpus which
were gathered with the crawlers OntoCrawler, OntoJCrawl, Swoogle, and Google.

576

fiPay-As-You-Go Description Logic Reasoning

Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

Expressiveness
ALE+
ALEH+
ALEHIF+
SRIF
SRI
ALCH
SH
SH
ALCOIF
SHOIN

Axioms
1, 170, 573
60, 633
61, 782
847, 794
731, 482
141, 957
229, 713
109, 959
165, 000
32, 157

Classes
518, 196
23, 136
23, 136
187, 520
106, 398
58, 771
95, 701
54, 974
41, 648
3, 533

Properties
16
950
950
70
469
124
110
9
148
84

Individuals
1
0
0
220, 948
0
0
0
0
85
160

Table 6: Ontology metrics for selected benchmark ontologies

Note that the ORE2014 dataset is a collection of ontologies from several sources and it
redundantly contains many of the ontologies that are also contained by the other repositories. However, many ontologies of the ORE2014 dataset have been adapted and approximated to fit the requirements of certain OWL 2 profiles (e.g., by removing datatypes that
are not in the OWL 2 datatype map, by enforcing a regular role hierarchy, and by adding
declarations for undeclared entities). We used the OWL API for parsing and we converted
all ontologies to self-contained OWL/XML files, where we created, for each of the 1,380
ontologies with imports, a version with resolved imports and another version, where the
import directives are simply removed (which allows for testing the reasoning performance
on the main ontology content without imports, which are frequently shared by many ontologies). Table 5 shows an overview of our obtained test corpus with overall 22,830 ontologies
including statistics of ontology metrics for the source repositories.
In addition to our test corpus, we present results for explicitly selected ontologies (shown
in Table 6) which are frequently used in many evaluations. This allows for directly showing
the effects of our approach for well-known benchmark ontologies and enables a more concrete
comparison. Note that Table 6 is separated into EL (upper part) and non-EL ontolgies
(lower part). As EL ontologies, we chose the well-known Gazetteer and EL-GALEN, where
the latter one is obtained by removing functionality and inverses from the Full-GALEN
ontology, which we also selected for benchmarking. In addition, we evaluated Biomodels
and Cell Cycle v2.01, which are large but mainly deterministic ontologies from the NCBO
BioPortal, NCI v06.12d and NCI v12.11d, which are different versions of the NCI-Thesaurus
ontology from the NCIt archive, SCT-SEP, which denotes the SNOMED CT anatomical
model ontology (Kazakov, 2010), FMA v2.0-CNS, which is a version of the Foundational
Model of Anatomy (Golbreich, Zhang, & Bodenreider, 2006), and OBI, which represents a
recent version of the Ontology for Biomedical Investigations (Brinkman et al., 2010).
The evaluation was carried out on a Dell PowerEdge R420 server running with two Intel
Xeon E5-2440 hexa core processors at 2.4 GHz with Hyper-Threading and 144 GB RAM
under a 64bit Ubuntu 12.04.2 LTS. Our evaluation focuses on classification, which is a central reasoning task supported by many reasoners and, thus, it is ideal for comparing results.
In principle, we only measured the classification time, i.e., the time spent for parsing and
loading ontologies as well as writing classification output to files is not included for the pre577

fiSteigmiller & Glimm

sented results. This is an advantage for reasoners that already perform some preprocessing
while loading, which is, however, not the case for Konclude since Konclude uses a lazy processing approach where also the preprocessing is triggered with the classification request.
This also seems to be confirmed by the accumulated loading times over all ontologies in
the evaluated repositories, which are 6, 304 s for Konclude, 10, 877 s for MORe, 13, 210 s
for FaCT++, 22, 458 s for Pellet, and 61, 293 s for HermiT. Note that HermiT directly
clausifies the ontologies while loading (i.e., it converts the axioms into HermiTs internal
representation based on DL-clauses), which can easily take a lot of time if ontologies intensively use cardinality restrictions. We also ignored all errors that were reported by (other)
reasoners, i.e., if a reasoner stopped the processing of an ontology (e.g., due to unsupported
axioms or program crashes), then we only measured the actual processing time. This is also
a disadvantage for Konclude since Konclude processed all ontologies (however, Konclude
also ignored parts of role inclusion axioms if they were not regular as specified by OWL 2
DL). In contrast, MORe reported errors for 803, FaCT++ for 944, Pellet for 1, 285, and
HermiT for 1, 483 ontologies in our corpus. The reasoners often cancelled the processing
due to unsupported or malformed datatypes. Another frequently reported error consisted
of different individual axioms for which only one individual was specified. In addition, HermiT completely refused processing ontologies with irregular role inclusion axioms (which
are, however, only rarely present in our test corpus).
For the evaluation of the ontology repositories, we used the time limit of 5 minutes.
For the selected benchmark ontologies, we cancelled the classification task after 15 minutes
since these ontologies are relatively large. Moreover, we averaged the results for the selected
benchmark ontologies over 3 separate runs, which was not necessary for the evaluated repositories since the large amount of ontologies automatically compensates the non-deterministic
behaviours of the reasoners, i.e., the accumulated (classification) times for separate runs over
many ontologies are almost identical. Although some reasoners support parallelisation, we
configured all reasoners to use only one worker thread, which allows for a comparison independently of the number of CPU cores and facilitates the presentation of the improvements
through saturation.
7.1 Evaluation of Saturation Optimisations
The presented optimisations are integrated in Konclude in such a way that they can separately be activated and deactivated. Hence, we can evaluate and compare the performance
improvements for the different optimisations. Please note that deactivating optimisations
in Konclude can cause disproportionate performance losses since appropriate replacement
optimisations, which could compensate the deactivated techniques to some extent, are often not integrated in Konclude. For example, many reasoning systems use the completely
defined concepts optimisation (Tsarkov & Horrocks, 2005) to identify those classes of an
ontology for which all subsumption relations can directly be extracted from the ontology
axioms and, thus, satisfiability and subsumption tests are not necessary to correctly insert
these classes into the class hierarchy. Clearly, such an optimisation is not necessary for Konclude, because we can extract all subsumers of a class from the saturation if the saturated
representative node is not critical. Hence, the performance with deactivated optimisations
578

fiPay-As-You-Go Description Logic Reasoning

might be worse than it has to be. Nevertheless, we evaluated the versions of Konclude,
where
 all saturation optimisations are activated (denoted by ALL), and
 none of the saturation optimisations are activated (denoted by NONE),
in combination with the activation/deactivation (denoted by +/) of the following modifications:
 RT (standing for result transfer), where the transfer of (possibly intermediate) results from the saturation into completion graphs (as presented in Section 4.1) is activated/deactivated. More precisely, we initialise new nodes in a completion graph
with the consequences available in the saturation graph and we block the processing
of (successor) nodes as long as they are identically labelled as the non-critical nodes in
the (deterministic or non-deterministic) saturation graph. As described in Section 5.2,
nominal dependent nodes are handled by reactivating the processing if the nodes for
the dependent nominals become modified in the completion graph. For this, we implemented an exact tracking of nominal dependent nodes in the completion graph as
well as in the saturation graph.
 SE (standing for subsumer extraction), where the extraction of subsumers from the
saturation (as presented in Section 4.2) is activated/deactivated. If the representative nodes for atomic concepts are not critical, then Konclude use them to extract all
subsumers (besides completely defined concepts) and, otherwise, the derived atomic
concepts are only used as told subsumers. Note that, if completely defined concepts
are not in a node label, then the candidate concepts are interpreted as if the corresponding completely defined concepts are non-deterministically derived, i.e., as possible subsumers. If the SE optimisation is deactivated, then Konclude extracts some
simple told subsumers from the axioms of the knowledge base in order to initialise the
classification algorithm, which is also in Konclude based on the known and possible
sets classification (Glimm et al., 2012).
 MM (standing for model merging), where the model merging with the saturation
graph (as presented in Section 4.3) is activated/deactivated. The candidate concepts
are obtained in Konclude with the partial absorption technique (Steigmiller et al.,
2014b) and the model merging is only applied for the first initialisation of the known
and possible subsumers of an atomic concept. In particular, we avoid a repeated
model merging for the same possible subsumption relation on different nodes (in
possibly different completion graphs) since this could result in a significant overhead
while possibly only few new non-subsumptions are identified.
 ES (standing for extended saturation), where the handling of universal restrictions
and of functional at-most restrictions for successors in the saturation (as presented in
Section 5.1) is activated/deactivated. Note that the integrated saturation procedure
becomes complete for Horn-SHIF knowledge bases if this optimisation is activated,
whereas completeness is only guaranteed for ELH knowledge bases if it is deactivated.
579

fiSteigmiller & Glimm

 PS (standing for patched saturation), where the patching of the saturation graph with
data from completion graphs (as presented in Section 5.2) is activated/deactivated.
To ensure patch compatibility for nominal dependent nodes, we use the completion
graph caching technique integrated in Konclude (Steigmiller et al., 2015) such that
only those nominal nodes are identified for which possibly different consequences are
derived as in the initial completion graph. Since Konclude supports an exact tracking
of nominal dependency in the completion graph, we save the dependent nominals in
patches and propagate them in the saturation graphs such that the processing of a
node only has to be reactivated if a node for a dependent nominal becomes modified.
Konclude further incorporates an exact tracking of which facts are the causes of which
derived facts (Steigmiller, Liebig, & Glimm, 2012) and this is used to also extract
patches from non-root nodes (as also sketched in Section 5.2). Moreover, if it can be
discovered that a satisfiability test for a concept does not result in a fully expanded and
clash-free completion graph, i.e., the concept is unsatisfiable, then Konclude patches
the saturation graphs with the -concept such that other unsatisfiable concepts are
also revealed.
For example, NONE+MM denotes the version of Konclude, where all saturation optimisations except the model merging with the saturation graph are deactivated.
Based on the version NONE, Table 7 shows the performance improvements for the activation of the saturation optimisations RT, SE, and MM. In addition, the results for ALL
are shown, where all optimisations are activated simultaneously. Please note that ES and
PS are optimisations to further improve the saturation procedure and, therefore, their evaluation only makes sense in combination with other saturation optimisations. The most
significant improvements are achieved by the transfer of saturation results into completion
graphs (RT), which often reduces the effort for the tableau algorithm significantly. The
model merging optimisation (MM) primarily improves the classification performance for
the NCI-Thesaurus ontologies in the NCIt archive, but does not have a similar significant
impact for the other repositories. Since many NCI-Thesaurus ontologies contain complete
definitions of the form A  r.B1 u s.B2 , the model merging with the candidate concepts
(as demonstrated in Section 4.3) allows for pruning many subsumptions while performing
satisfiability tests for the atomic concepts. There are also improvements through the extraction of subsumers from the saturation (SE), but, compared to the improvements of the other
optimisations, they are only significantly better for the NCBO BioPortal. In particular, the
NCBO BioPortal contains many large but relatively simple ontologies that can almost be
completely handled by the saturation and, therefore, it is not necessary to perform satisfiability tests for every class with the tableau algorithm if the SE optimisation is activated in
order to determine the (possible) subsumers. Nevertheless, if all saturation optimisations
are activated, then we are often able to achieve much larger performance improvements for
almost all repositories. On the one hand, this is caused by the additionally activated ES and
PS optimisations, but on the other hand, the reasoning system can utilise several synergy
effects from the saturation (obviously, the concepts have to be saturated only once for all
optimisations).
Table 8 analogously shows the performance improvements by activating the saturation
optimisations RT, SE, and MM for the selected benchmark ontologies. The saturation optimisations can significantly improve the classification performance for several ontologies.
580

fiPay-As-You-Go Description Logic Reasoning

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset
ALL

NONE
526
2, 259
28, 603
3, 020
7, 976
1, 734
798
27
3, 405
3, 477
115, 494
167, 320

NONE+RT
508
2, 039
28, 434
812
4, 639
1, 481
463
29
1, 166
2, 670
80, 673
122, 914

NONE+SE
414
580
27, 940
877
5, 866
1, 568
670
30
2, 232
2, 820
98, 630
141, 628

NONE+MM
490
2, 326
3, 163
2, 829
8, 013
756
794
31
2, 504
2, 283
115, 232
138, 421

ALL
108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

Table 7: Accumulated classification times (in seconds) with separately activated saturation
optimisations for the evaluated ontology repositories
Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

NONE

NONE+RT

NONE+SE

NONE+MM

ALL

34.8
761.0
 900.0
241.5
 900.0
 900.0
17.7
 900.0
 900.0
1.3

30.1
5.5
 900.0
50.6
 900.0
 900.0
14.6
339.8
 900.0
0.8

14.0
1.6
 900.0
18.2
7.6
 900.0
8.8
279.4
 900.0
0.7

37.9
762.6
 900.0
148.7
 900.0
17.9
16.0
383.1
 900.0
2.1

13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
72.7
0.6

Table 8: Classification times (in seconds) with separately activated saturation optimisations
for the evaluated benchmark ontologies

In particular, with the optimisations, Konclude can handle all ontologies in a reasonable
amount of time, whereas Konclude timed out for five of these ontologies if the saturation optimisations were not used. Very difficult ontologies such as Full-GALEN and FMA v2.0-CNS
can only be handled if more sophisticated saturation optimisations are used (e.g., SE, PS).
It can also be observed that, for many ontologies, only specific optimisations are crucial,
which is, however, also not very surprising. For example, it is clear that the MM optimisation cannot improve the performance for deterministic ontologies since they do not have
possible subsumers for which the model merging could be applied.
Table 9 shows the performance changes for the separate deactivation of saturation optimisations based on the ALL configuration. The evaluation of optimisations is also very
interesting from this perspective, because the saturation of many concepts can easily require
a significant amount of reasoning time and, by separately deactivating single optimisations,
the overhead of the saturation is not only associated with a separately activated optimisa581

fiSteigmiller & Glimm

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset
ALL

ALL
108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

ALLRT
134
624
2, 041
1, 052
3, 987
143
790
30
2, 017
1, 427
56, 128
68, 374

ALLSE
201
1, 980
2, 580
453
3, 701
366
731
62
1, 445
1, 209
56, 469
69, 286

ALLMM
90
618
27, 952
473
2, 398
1, 377
706
30
702
2, 456
37, 760
71, 562

ALLES
396
582
2, 000
774
3, 658
226
412
30
764
1, 348
51, 400
61, 590

ALLPS
106
709
1, 960
453
2, 537
633
733
35
879
1, 201
61, 036
70, 281

Table 9: Accumulated classification times (in seconds) with separately deactivated saturation optimisations for the evaluated ontology repositories

Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

ALL
13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
72.7
0.6

ALLRT
13.6
1.5
12.7
17.2
7.5
15.0
8.7
280.4
60.3
0.8

ALLSE
27.9
4.8
25.1
47.1
 900.0
15.7
13.0
337.1
28.2
0.8

ALLMM
13.2
1.4
11.8
15.6
7.1
 900.0
7.6
161.9
180.3
0.7

ALLES
13.7
1.5
 900.0
16.2
7.3
13.8
7.4
167.5
66.6
0.7

ALLPS
13.5
1.5
12.7
16.6
6.9
13.2
7.7
168.7
 900.0
0.7

Table 10: Classification times (in seconds) with separately deactivated saturation optimisations for selected benchmark ontologies

tion. Furthermore, this allows for evaluating whether some optimisations are superfluous
and which effects are caused by the saturation improvements ES and PS, which are only useful in combination with other saturation optimisations. Table 9 reveals that some saturation
optimisations are completely irrelevant for some repositories. Moreover, the deactivation of
optimisations can also improve the performance for several repositories, e.g., the deactivation of RT results in better reasoning times for the ontologies in the TONES repository and
the deactivation of MM causes some minor performance improvements for the OntoJCrawl
ontologies. However, by considering all repositories, each optimisation is indeed justified.
In particular, if any of the presented saturation optimisations is deactivated, then the reasoning times increase by at least 55 %. This is also caused by several difficult ontologies in
the ORE2014 dataset, such as variants of the KB Bio 101 ontology (Chaudhri, Wessel, &
Heymans, 2013), which can only be handled by Konclude if almost all saturation optimisations are used. The patching of the saturation graph (PS) with the data from the initial
582

fiPay-As-You-Go Description Logic Reasoning

consistency test is often required for the complete/sufficient handling of nominals within
the saturation procedure, the saturation extensions (ES) enable a primitive handling of
(qualified) cardinality restrictions even for very big cardinalities (due to the reuse of nodes
in the saturation graph), and the result transfer (RT) as well as the subsumer extraction
(SE) reduce or avoid the work for the tableau algorithm, which is particularly useful for
very big and highly cyclic ontologies. Although the MM optimisation is similar important
for the ORE2014 dataset, it is the only optimisation that significantly reduces the effort of
Konclude for the NCI-Thesaurus ontologies from the NCIt archive.
The performance changes for the separate deactivation of saturation optimisations for
the evaluated benchmark ontologies are depicted in Table 10. Again, it can be observed
that often only specific optimisations are important for the ontologies. For example, only
the deactivation of the SE optimisations significantly decreases the performances for the
Biomodels and Cell Cycle v2.01 ontologies. Since Full-GALEN is highly cyclic and has
many consequences that are caused by functional cardinality restrictions as well as inverse
roles, the tableau algorithm has difficulties to find appropriate blocker nodes in the completion graph and, therefore, it can only be handled if the saturation is extended to these
language features (as realised by the ES optimisation). In contrast, FMA v2.0-CNS has
many unsatisfiable classes and, as soon as the tableau algorithm can find such an unsatisfiable class, the saturation graph can be patched (realised with the PS optimisation) and the
-concept can directly be propagated to many other classes, whereby many satisfiability
tests with the tableau algorithm become unnecessary.
7.2 Evaluation of the Saturation Effort
Table 11 shows the distribution of the processing times w.r.t. Koncludes processing stages
for the classification of the evaluated repositories with the version ALL. Unsurprisingly, the
majority of the processing time (61.5 %) is spent for the classification process itself. In contrast, the saturation of all those concepts that are potentially required for the classification
requires only 12.1 % together with the detection of the saturation status. The latter one
can, however, usually be neglected in terms of processing time since our implementation is
very efficient. For example, if a node is detected as critical, then the criticality status is
immediately propagated to all dependent nodes and, as a consequence, they do not have
to be tested. Moreover, we use a criticality testing queue that is filled during saturation if
concepts are added to node labels that potentially influence the criticality status. Hence,
the status detection does not have to iterate through all node labels. Although it is in
principle possible to design ontologies where the saturation can be relatively inefficient (in
particular w.r.t. the memory requirements), such ontologies hardly occur in practice. In
particular, with the data sharing for node labels that is realised in Konclude, the saturation
does not cause significant problems for the evaluated repositories, which is also reflected
by the short processing time for the saturation stage. Consistency checking can usually
also be performed efficiently, but several evaluated repositories (e.g., the Swoogle crawl)
also contain very difficult ontologies for which the tableau algorithm cannot find a fully
expanded and clash-free completion graph within the time limit. Building the internal representation as well as preprocessing are also realised very efficiently in Konclude and do not
cause problems for the evaluated repositories.
583

fiSteigmiller & Glimm

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset
ALL

Building Preprocessing
10.8
30.6
27.4
17.1
7.5
11.0
16.3
5.0
5.6
10.6
2.6
3.3
11.6
7.9
38.3
10.7
8.2
4.6
2.2
1.0
4.8
5.8
5.4
6.3

Saturation
32.5
23.6
11.2
6.7
12.2
5.6
19.4
17.5
4.4
1.8
12.5
12.1

Consistency
1.8
3.1
1.9
46.6
20.3
0.6
8.3
21.1
48.1
26.7
13.4
14.6

Classification
24.3
28.8
68.4
25.4
51.3
87.9
52.9
12.5
34.7
68.2
63.4
61.5

Table 11: Distribution of processing time w.r.t. different processing stages (in %)

7.3 Comparison with other Approaches
As mentioned in Section 6, there exist other approaches that also use saturation-based
reasoning techniques to improve fully-fledged tableau algorithms. For example, MORe
uses module extraction to delegate as much work as possible to an efficient reasoner that is
specialised for a specific fragment in order to classify ontologies. Since an early development
version of MORe is available, we evaluated MORe with our test corpus and we compare the
results to our approach in the following. We used MORe in combination with ELK 0.4.1
(Kazakov, Krotzsch, & Simanck, 2014) and HermiT 1.3.8, but other combinations are also
possible since these reasoners are used as black-boxes.
The left-hand side of Table 12 shows the accumulated classification times (in seconds) for
the versions of Konclude where all saturation optimisations are deactivated (version NONE
in Column 2) and all saturation optimisations are activated (version ALL in Column 3)
for the different repositories. Furthermore, the improvement from the version NONE to the
version ALL is given in percent (Column 4 of Table 12). For example, by using all saturation
optimisations presented here, the accumulated reasoning time for all repositories is reduced
by 77.5 % for Konclude. On the right-hand side of Table 12, we have analogously depicted
the accumulated reasoning times for HermiT (Column 5) and MORe (Column 6), and also
the percentage of HermiTs reasoning time that can be reduced by MORe (Column 7).
Note, the accumulated loading times for all repositories are 61, 293 s for HermiT and
10, 877 s for MORe, where the difference of 50, 416 s can be explained by the additional
preprocessing that is directly performed in HermiTs loading stage, whereas MORe starts
the processing of the ontologies not before the classification request. Of course, MORe also
uses HermiT internally to process parts of ontologies that cannot be handled by the OWL
2 EL reasoner ELK, but the required time for loading these parts with HermiT is then
counted as reasoning/classification time for MORe. Hence, we made the comparison fair
by adding the additional preprocessing time in the loading stage to HermiTs classification
time, i.e., the shown classification times for HermiT are extended by the difference between
the loading times of HermiT and MORe.
584

fiPay-As-You-Go Description Logic Reasoning

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset
ALL

NONE [s]
526
2, 259
28, 603
3, 020
7, 976
1, 734
798
27
3, 405
3, 477
115, 494
167, 320

 [%]
79.5
88.5
93.2
75.2
68.9
85.6
86.0
0.0
79.0
65.9
74.2
77.5

ALL [s]
108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

HermiT [s]
1, 773
5, 901
26, 435
6, 654
12, 865
2, 342
1, 917
1, 863
8, 555
4, 857
294, 124
367, 283

MORe [s]
1, 537
4, 187
26, 600
4, 474
8, 083
2, 184
1, 629
893
4, 546
4, 270
166, 589
224, 982

 [%]
13.3
29.0
0.6
32.8
37.2
6.7
15.0
52.1
46.9
12.1
43.9
38.7

Table 12: Comparison of improvements through saturation between the approaches realised
in Konclude and MORe for the accumulated classification times of the evaluated
ontology repositories (in seconds and %)

Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

NONE [s]
34.8
761.0
 900.0
241.5
 900.0
 900.0
17.7
 900.0
 900.0
1.3

 [%]
51.2
98.0
 98.7
93.3
 99.2
 98.5
53.8
 80.1
 75.9
53.8

ALL [s]
13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
72.7
0.6

HermiT [s]
 900.0
 900.0
 900.0
788.8
 900.0
211.9
92.7
 900.0
 900.0
32.5

MORe [s]
18.2
2.6
 900.0
648.8
 900.0
208.0
83.3
 900.0
 900.0
2.37

 [%]
 98.0
 99.7

17.7

1.9
10.1


93.0

Table 13: Improvements through saturation between the approaches in Konclude and
MORe for the classification times of selected benchmark ontologies (in seconds
and %)

Table 12 reveals that MORe can significantly improve the reasoning time of HermiT for
almost all repositories. In particular, MORe saves 52.1 % of HermiTs classification time
for the ontologies from OntoCrawler. Nevertheless, there are still many ontologies in these
repositories, where MORe is not able to reduce the effort of HermiT such that they can be
classified within the time limit (HermiT timed out for 786 and MORe for 590 ontologies,
respectively). In contrast, Konclude integrates a more sophisticated interaction between
the tableau algorithm and the saturation procedure and, therefore, the improvements by
the saturation optimisations are significantly better for many repositories. As a result, the
7. The class hierarchy computed by MORe for OBI does not coincide with the results of HermiT and
Konclude.

585

fiSteigmiller & Glimm

ALL version of Konclude reached the time limit only for 69 ontologies. Note, already the
version NONE of Konclude, where all saturation optimisations are deactivated, outperforms
HermiT and MORe for many ontologies, which is probably due to the difference in the integrated optimisations. For example, Konclude uses several caching techniques to save and
reuse intermediate results, which usually improves the reasoning performance a lot. Moreover, the partial absorption (Steigmiller et al., 2014b) integrated in Konclude significantly
reduces non-determinism also for very expressive ontologies. Also note that MORe does
not yet completely support all language features of SROIQ and, therefore, it does not always output the correct class hierarchy (for the evaluated ontology repositories, 1, 441 class
hierarchies computed by MORe do not coincide with the results of HermiT and Konclude).
Table 13 analogously shows the performance improvements for the selected benchmark
ontologies. Again, it can be observed that the improvements through the saturation are
often better for Konclude than for MORe, especially if ontologies are considered for which
the version NONE of Konclude still requires a lot of reasoning time.
7.4 Comparison with State-of-the-Art Reasoners
We also evaluated the classification times for the state-of-the-art reasoners FaCT++ and
Pellet, which are compared with the other reasoners HermiT, Konclude, and MORe in
Table 14. Note, Table 14 only shows the accumulated classification times that are actually
reported by the reasoners, i.e., we did not compensate differences in the loading times.
It can be observed that Konclude outperforms all other reasoners for all evaluated
repositories, which is mainly due to the integrated saturation optimisations. FaCT++ is the
only reasoner that can efficiently handle the majority of all NCI-Thesaurus ontologies in the
NCIt archive also without saturation optimisations. Nevertheless, the model merging with
the saturation graph allows for pruning many possible subsumers in Konclude, whereby
the classification performance can further be improved and, therefore, Konclude is able
to outperform FaCT++ also for the NCIt archive. Considering all repositories, Konclude
produced the fewest timeouts (69), followed by MORe (590), HermiT (786), FaCT++ (822),
and Pellet (1, 470).
Nevertheless, there are a few ontologies for which Koncludes performance is not optimal
and for which other reasoners are sometimes able to outperform Konclude. For example,
Konclude requires 54.6 s for the classification of the atom-complex-proton-2.0 ontology from
the TONES repository, whereas Pellet only requires 11.4 s (FaCT++ and HermiT timed
out). In particular, the handling of (large) cardinalities can easily cause problems since,
in the worst-case, the tableau algorithm is used to create and merge the corresponding
numbers of successor nodes. Although our saturation has a limited handling of at-least cardinality restrictions by using only one representative node as successor, this easily becomes
incomplete if also at-most cardinality restrictions are used in ontologies. As a remedy, one
could try to extend the saturation procedure to better handle cardinality restrictions or
to combine the tableau algorithm with algebraic methods, where cardinality restrictions
are handled as a system of linear (in)equations (Haarslev, Sebastiani, & Vescovi, 2011).
Moreover, too much non-determinism, e.g., caused by non-absorbable GCIs, can still cause
serious issues for tableau-based systems. Examples of such ontologies are variants of enzyo
from the Swoogle crawl, which cannot be classified by any of the evaluated reasoners except
586

fiPay-As-You-Go Description Logic Reasoning

Repository
Gardiner
NCBO BioPortal
NCIt
OBO Foundry
Oxford
TONES
Google crawl
OntoCrawler
OntoJCrawl
Swoogle crawl
ORE2014 dataset
ALL

FaCT++
1, 108
5, 413
4, 393
7, 225
20, 761
1, 684
2, 178
964
11, 580
2, 864
241, 402
299, 573

HermiT
1, 688
5, 570
25, 203
6, 258
12, 255
1, 943
1, 761
1, 723
6, 757
4, 073
249, 637
316, 867

Konclude
108
260
1, 942
748
2, 484
250
112
27
715
1, 187
29, 841
37, 674

MORe
1, 537
4, 187
26, 600
4, 474
8, 083
2, 184
1, 629
893
4, 546
4, 270
166, 589
224, 982

Pellet
4, 006
9, 877
17, 647
12, 031
27, 461
1, 755
7, 496
9, 999
31, 776
9, 212
397, 794
528, 891

Table 14: Comparison of accumulated classification times between state-of-the-art reasoners
(in seconds) for the evaluated ontology repositories
Ontology
Gazetteer
EL-GALEN
Full-GALEN
Biomodels
Cell Cycle v2.01
NCI v06.12d
NCI v12.11d
SCT-SEP
FMA v2.0-CNS
OBI

FaCT++
 900.0
 900.0
 900.0
2.78
 900.0
13.9
57.8
 900.0
 900.0
 900.0

HermiT
 900.0
 900.0
 900.0
788.8
 900.0
206.1
78.8
 900.0
 900.0
31.5

Konclude
13.3
1.4
12.0
16.2
7.2
13.9
8.2
173.1
28.3
0.6

MORe
18.2
2.6
 900.0
648.8
 900.0
208.0
83.3
 900.0
 900.0
2.39

Pellet
480.2
135.1
 900.0
 900.0
 900.0
69.6
306.9
 900.0
 900.0
 900.0

Table 15: Comparison of classification times between state-of-the-art reasoners (in seconds)
for selected benchmark ontologies

for FaCT++ (but also FaCT++ needs a significant amount of time and even reaches the
time limit of 5 minutes for some variants) although they have less than 20, 000 axioms and
only an expressiveness that is ranging from ALIN to ALCOIN . Also very large SROIQ
ontologies from the Oxford ontology library, such as Mus musculus consisting of 221, 484
axioms, seem currently to be out of reach for existing reasoning systems. Due to their
size and complexity, it is even difficult to analyse in which kinds of problems the reasoners
are running, but an intensive use of nominals often limits the applicability of optimisation
techniques and, hence, often results in poor performance.
Analogously, Table 15 shows the comparison of the classification times between all evaluated reasoners for the selected benchmark ontologies in seconds. Again, with the activated
8. FaCT++ 1.6.3 crashed for the classification of Biomodels after 2.7 seconds.
9. The class hierarchy that is computed by MORe for OBI does not coincide with the results of the other
reasoners.

587

fiSteigmiller & Glimm

saturation optimisations, Konclude can outperform the other reasoners for almost all ontologies and is able to classify all these benchmark ontologies within the time limit. Compared
to the other reasoners, MORe can also achieve good results for several of these ontologies.
In particular, MORe only timed out for 4 ontologies, whereas HermiT and Pellet could
not classify 6 ontologies in time, and FaCT++ failed for the classification of 7 ontologies.
Hence, a support through saturation seems to pay off.

8. Conclusions
In this paper, we have presented a technique for tightly coupling saturation- and tableaubased procedures. Unlike standard completion- and consequence-based saturation procedures, the approach is applicable for arbitrary OWL 2 DL ontologies. Furthermore, it has
a very good pay-as-you-go behaviour, i.e., if only few axioms use features that are problematic for saturation-based procedures (e.g., disjunction), then the tableau procedure can still
benefit significantly from the saturation.
The very good pay-as-you-go behaviour seems to be confirmed by our evaluation over
several thousand ontologies, where the integration of the presented saturation optimisations
into the reasoning system Konclude significantly improves the classification performance.
In particular, with these optimisations, Konclude is able to outperform many other state-ofthe-art reasoners for a wide range of ontologies often by more than one order of magnitude.

Acknowledgments
The first author acknowledges the support of the doctoral scholarship under the Postgraduate Scholarships Act of the Land of Baden-Wuerttemberg (LGFG). This work was done
within the Transregional Collaborative Research Centre SFB/TRR 62 A CompanionTechnology for Cognitive Technical Systems funded by the German Research Foundation (DFG).

References
Armas Romero, A., Cuenca Grau, B., & Horrocks, I. (2012). MORe: Modular combination
of OWL reasoners for ontology classification. In Proc. 11th Int. Semantic Web Conf.
(ISWC12), Vol. 7649 of LNCS, pp. 116. Springer.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proc. 19th Int. Joint
Conf. on Artificial Intelligence (IJCAI05), pp. 364369. Professional Book Center.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2007).
The Description Logic Handbook: Theory, Implementation, and Applications (Second
edition). Cambridge University Press.
Baader, F., Hollunder, B., Nebel, B., Profitlich, H.-J., & Franconi, E. (1994). An empirical
analysis of optimization techniques for terminological representation systems. J. of
Applied Intelligence, 4 (2), 109132.
588

fiPay-As-You-Go Description Logic Reasoning

Bate, A., Motik, B., Cuenca Grau, B., Simanck, F., & Horrocks, I. (2015). Extending
consequence-based reasoning to SHIQ. In Proc. 28th Int. Workshop on Description
Logics (DL15).
Brinkman, R. R., Courtot, M., Derom, D., Fostel, J., He, Y., Lord, P. W., Malone, J.,
Parkinson, H. E., Peters, B., Rocca-Serra, P., Ruttenberg, A., Sansone, S., Soldatova,
L. N., Jr., C. J. S., Turner, J. A., Zheng, J., & OBI consortium (2010). Modeling
biomedical experimental processes with OBI. J. Biomedical Semantics, 1 (S-1), S7.
Chaudhri, V. K., Wessel, M. A., & Heymans, S. (2013). KB Bio 101: A challenge for OWL
reasoners. In Proc. 2nd Int. Workshop on OWL Reasoner Evaluation (ORE13).
CEUR.
Gardiner, T., Horrocks, I., & Tsarkov, D. (2006). Automated benchmarking of description
logic reasoners. In Proc. 19th Int. Workshop on Description Logics (DL06), Vol. 198.
CEUR.
Glimm, B., Horrocks, I., Motik, B., Shearer, R., & Stoilos, G. (2012). A novel approach to
ontology classification. J. of Web Semantics, 14, 84101.
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: An OWL 2
reasoner. J. of Automated Reasoning, 53 (3), 125.
Golbreich, C., Zhang, S., & Bodenreider, O. (2006). The foundational model of anatomy in
OWL: Experience and perspectives. J. of Web Semantics, 4 (3), 181195.
Haarslev, V., Moller, R., & Turhan, A.-Y. (2001). Exploiting pseudo models for TBox
and ABox reasoning in expressive description logics. In Proc. 1st Int. Joint Conf. on
Automated Reasoning (IJCAR01), Vol. 2083 of LNCS, pp. 6175. Springer.
Haarslev, V., Sebastiani, R., & Vescovi, M. (2011). Automated reasoning in ALCQ via
SMT. In Proc. 23rd Int. Conf. on Automated Deduction (CADE11), pp. 283298.
Springer.
Horrocks, I., Kutz, O., & Sattler, U. (2006). The even more irresistible SROIQ. In Proc.
10th Int. Conf. on Principles of Knowledge Representation and Reasoning (KR06),
pp. 5767. AAAI Press.
Horrocks, I., & Sattler, U. (1999). A description logic with transitive and inverse roles and
role hierarchies. J. of Logic and Computation, 9 (3), 385410.
Horrocks, I., & Sattler, U. (2001). Optimised reasoning for SHIQ. In Proc. 15th European
Conf. on Artificial Intelligence (ECAI02), pp. 277281. IOS Press.
Horrocks, I., & Sattler, U. (2004). Decidability of SHIQ with complex role inclusion axioms.
Artificial Intelligence, 160 (1), 79104.
Horrocks, I., & Sattler, U. (2007). A tableau decision procedure for SHOIQ. J. of Automated Resoning, 39 (3), 249276.
Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning for expressive description logics. In Proc. 6th Int. Conf. on Logic Programming and Automated Reasoning
(LPAR99), Vol. 1705 of LNCS, pp. 161180. Springer.
589

fiSteigmiller & Glimm

Horrocks, I., Sattler, U., & Tobies, S. (2000). Reasoning with individuals for the description
logic SHIQ. In Proc. 17th Int. Conf. on Automated Deduction (CADE00), Vol. 1831
of LNCS, pp. 482496. Springer.
Horrocks, I., & Tobies, S. (2000). Reasoning with axioms: Theory and practice.. In Proc.
7th Int. Conf. on Principles of Knowledge Representation and Reasoning (KR00),
pp. 285296. Morgan Kaufmann.
Hudek, A. K., & Weddell, G. E. (2006). Binary absorption in tableaux-based reasoning for
description logics. In Proc. 19th Int. Workshop on Description Logics (DL06), Vol.
189. CEUR.
Information Management Group (2008). TONES ontology repository. University of Manchester. Available at http://owl.cs.manchester.ac.uk/repository/. Accessed: July
2012; Mirrored at http://ontohub.org/repositories/tones.
Information Systems Group (2012). Oxford ontology library. University of Oxford. Available
at http://www.cs.ox.ac.uk/isg/ontologies/. Accessed: August 2012;.
Kazakov, Y. (2008). RIQ and SROIQ are harder than SHOIQ. In Proc. 11th Int.
Conf. on Principles of Knowledge Representation and Reasoning (KR08), pp. 274
284. AAAI Press.
Kazakov, Y. (2009). Consequence-driven reasoning for Horn-SHIQ ontologies. In Proc.
21st Int. Conf. on Artificial Intelligence (IJCAI09), pp. 20402045. IJCAI.
Kazakov, Y. (2010).
ConDOR project site.
condor-reasoner/. Accessed: July 2013;.

https://code.google.com/p/

Kazakov, Y., Krotzsch, M., & Simanck, F. (2012). Practical reasoning with nominals in the
EL family of description logics. In Proc. 13th Int. Conf. on Principles of Knowledge
Representation and Reasoning (KR12). AAAI Press.
Kazakov, Y., Krotzsch, M., & Simanck, F. (2014). The incredible ELK - from polynomial
procedures to efficient reasoning with EL ontologies. J. of Automated Reasoning, 53,
161.
Matentzoglu, N., Bail, S., & Parsia, B. (2013). A corpus of OWL DL ontologies. In Proc.
26th Int. Workshop on Description Logics (DL13), Vol. 1014. CEUR.
Matentzoglu, N., & Parsia, B. (2014). ORE 2014 reasoner competition dataset. Zenodo.
Available at http://dx.doi.org/10.5281/zenodo.10791.
National Cancer Institute (2003). Nci thesaurus archive. Available at http://ncit.nci.
nih.gov/. Accessed: December 2012;.
Ortiz, M., Rudolph, S., & Simkus, M. (2010). Worst-case optimal reasoning for the HornDL fragments of OWL 1 and 2. In Proc. 12th Int. Conf. on Principles of Knowledge
Representation and Reasoning (KR10), pp. 269279. AAAI Press.
Simanck, F. (2012). Elimination of complex RIAs without automata. In Proc. 25th Int.
Workshop on Description Logics (DL12), Vol. 846. CEUR.
Simanck, F., Kazakov, Y., & Horrocks, I. (2011). Consequence-based reasoning beyond
horn ontologies. In Proc. 22nd Int. Joint Conf. on Artificial Intelligence (IJCAI11),
pp. 10931098. IJCAI/AAAI.
590

fiPay-As-You-Go Description Logic Reasoning

Simanck, F., Motik, B., & Horrocks, I. (2014). Consequence-based and fixed-parameter
tractable reasoning in description logics. J. of Artificial Intelligence, 209, 2977.
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: A practical
OWL-DL reasoner. J. of Web Semantics, 5 (2), 5153.
Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters, W., Goldberg, L. J.,
Eilbeck, K., Ireland, A., Mungall, C. J., Consortium, T. O., Leontis, N., Rocca-Serra,
P., Ruttenberg, A., Sansone, S.-A., Scheuermann, R. H., Shah, N., Whetzeland, P. L.,
& Lewis, S. (2007). The OBO Foundry: coordinated evolution of ontologies to support
biomedical data integration. J. of Nature Biotechnology, 25, 12511255.
Song, W., Spencer, B., & Du, W. (2012). WSReasoner: A prototype hybrid reasoner for
ALCHOI ontology classification using a weakening and strengthening approach. In
Proc. 1st Int. Workshop on OWL Reasoner Evaluation (ORE12), Vol. 858. CEUR.
Steigmiller, A., Glimm, B., & Liebig, T. (2013). Nominal schema absorption. In Proc. 23rd
Int. Joint Conf. on Artificial Intelligence (IJCAI13), pp. 11041110. AAAI Press.
Steigmiller, A., Glimm, B., & Liebig, T. (2014a). Coupling tableau algorithms for expressive
description logics with completion-based saturation procedures. In Proc. 7th Int.
Joint Conf. on Automated Reasoning (IJCAR14), Vol. 8562 of LNCS, pp. 449463.
Springer.
Steigmiller, A., Glimm, B., & Liebig, T. (2014b). Optimised absorption for expressive
description logics. In Proc. 27th Int. Workshop on Description Logics (DL14), Vol.
1193. CEUR.
Steigmiller, A., Glimm, B., & Liebig, T. (2015). Completion graph caching for expressive
description logics. In Proc. 28th Int. Workshop on Description Logics (DL15).
Steigmiller, A., Liebig, T., & Glimm, B. (2012). Extended caching, backjumping and merging for expressive description logics. In Proc. 6th Int. Joint Conf. on Automated
Reasoning (IJCAR12), Vol. 7364 of LNCS, pp. 514529. Springer.
Steigmiller, A., Liebig, T., & Glimm, B. (2014). Konclude: system description. J. of Web
Semantics, 27 (1).
Tsarkov, D., & Horrocks, I. (2004). Efficient reasoning with range and domain constraints.
In Proc. 17th Int. Workshop on Description Logics (DL04), Vol. 104. CEUR.
Tsarkov, D., & Horrocks, I. (2005). Optimised classification for taxonomic knowledge bases.
In Proc. 18th Int. Workshop on Description Logics (DL05), Vol. 147. CEUR.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.
In Proc. 3rd Int. Joint Conf. on Automated Reasoning (IJCAR06), Vol. 4130 of
LNCS, pp. 292297. Springer.
Tsarkov, D., Horrocks, I., & Patel-Schneider, P. F. (2007). Optimizing terminological reasoning for expressive description logics. J. of Automated Reasoning, 39, 277316.
W3C OWL Working Group (27 October 2009). OWL 2 Web Ontology Language: Document Overview. W3C Recommendation. Available at http://www.w3.org/TR/
owl2-overview/.
591

fiSteigmiller & Glimm

Whetzel, P. L., Noy, N. F., Shah, N. H., Alexander, P. R., Nyulas, C., Tudorache, T., &
Musen, M. A. (2011). BioPortal: enhanced functionality via new web services from
the national center for biomedical ontology to access and use ontologies in software
applications. Nucleic Acids Research, 39 (Web-Server-Issue), 541545.
Zhou, Y., Cuenca Grau, B., Nenov, Y., Kaminski, M., & Horrocks, I. (2015). PAGOdA:
Pay-as-you-go ontology query answering using a datalog reasoner. J. of Artificial
Intelligence Research, 54, 309367.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL query
answering using a triple store. In Proc. 28th AAAI Conf. on Artificial Intelligence
(AAAI14), pp. 11421148.

592

fiJournal of Artificial Intelligence Research 54 (2015) 159-192

Submitted 02/15; published 09/15

Leveraging Online User Feedback to Improve
Statistical Machine Translation
Llus Formiga

lformiga@verbio.com

Verbio Technologies, S.L.,
Loreto, 44, 08029 Barcelona

Alberto Barron-Cedeno
Llus Marquez

albarron@qf.org.qa
lmarquez@qf.org.qa

Qatar Computing Research Institute
Hamad Bin Khalifa University,
Tornado Tower, Floor 10, P.O. Box 5825, Doha, Qatar

Carlos A. Henrquez
Jose B. Marino

carlos.henriquez@upc.edu
jose.marino@upc.edu

TALP Research Center - Universitat Politecnica de Catalunya,
Jordi Girona, 1-3, 08034 Barcelona

Abstract
In this article we present a three-step methodology for dynamically improving a statistical machine translation (SMT) system by incorporating human feedback in the form of
free edits on the system translations. We target at feedback provided by casual users, which
is typically error-prone. Thus, we first propose a filtering step to automatically identify
the better user-edited translations and discard the useless ones. A second step produces
a pivot-based alignment between source and user-edited sentences, focusing on the errors
made by the system. Finally, a third step produces a new translation model and combines
it linearly with the one from the original system. We perform a thorough evaluation on
a real-world dataset collected from the Reverso.net translation service and show that every step in our methodology contributes significantly to improve a general purpose SMT
system. Interestingly, the quality improvement is not only due to the increase of lexical
coverage, but to a better lexical selection, reordering, and morphology. Finally, we show
the robustness of the methodology by applying it to a different scenario, in which the new
examples come from an automatically Web-crawled parallel corpus. Using exactly the same
architecture and models provides again a significant improvement of the translation quality
of a general purpose baseline SMT system.

1. Introduction
Statistical machine translation (SMT) has become a widespread technology, used by millions of people to satisfy a multiplicity of needs in their daily interactions and information
seeking. In contrast to business-oriented translation services, on-line machine translation
services (e.g., Google Translate, see Google Inc., 2015; Bing, see Microsoft Inc., 2015; Reverso, see Reverso-Softissimo, 2015) offer free general-purpose translations with fairly acc
2015
AI Access Foundation. All rights reserved.

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

ceptable levels of quality and for a large number of language pairs. The fact that they are
easily accessible from any computer, tablet or smartphone connected to the Internet has
contributed to create a huge community of heterogeneous users.
However, SMT systems have significant limitations and produce translation errors at
different levels (e.g., morphology and agreement, phrase structure and reordering, lexical
selection, and fluency). This is not only due to the inherent complexity of the task but also
to the limitations of the currently available translation models and training corpora, which
might not be fully representative of the domain, genre and style of the texts to be translated.
This behavior may cause frustration and fatigue to the users; but the users themselves are
in the right position to spot such mistakes. The response of machine translation (MT)
systems developers has been to allow users to provide feedback by proposing corrections to
the system-generated translations. Gathering new and improved information from users
edits has shown to be a valuable resource to improve the translation systems in both online cost-free services (Simard, Goutte, & Isabelle, 2007; Ambati, Vogel, & Carbonell, 2010;
Potet, Esperanca-Rodier, Blanchon, & Besacier, 2011) and professional computer-assisted
translation frameworks (Bertoldi, Cettolo, & Federico, 2013; Mathur, Mauro, & Federico,
2013). The raising interest of this topic in the MT community has emerged also in the
form of research projects (e.g., MateCat, FAUST, and Casmacat, see European Comission
- 7th Framework Program, 2010) specialized workshops (e.g., the Workshop on Post-editing
Technology and Practice at MT Summit XIV) and special issues (such as the Machine
Translation Journal Special Issue of Machine Translation on MT Post-editing).
In this article we explore the use of real translation feedback from non-professional
users. Our aim is to automatically improve the general translation quality of the underlying MT system. This approach differs significantly from the more common setting in which
professional post-editors are used to produce high-quality translations from an imperfect
automatic output. In our setting users are casual, with limited skills, and sometimes
having low command of the languages being translated. They perform free edits on the
machine-translated text to produce a supposedly better alternative translation. This translation is sometimes a proper post-edition, but frequently it is partial, contains errors, or it
is simply a piece of unrelated text. The challenge in this particularly noisy setting is being
able to filter out part of the noise and select the potentially useful translation edits.
One advantage of this crowd-sourcing approach to MT enrichment is its potential to
reach a vast community of contributors. This scenario conveys a mutual-interest framework:
on the one side, an active user wills to correct translations as long as the system responds
better to her needs in the future; on the other side, a system requires the input of an
intelligent agent, which is able to provide information to improve its translation models.
If a high level of engagement is achieved, a committed live community can constantly
contribute to improve the free on-line translators. Another fundamental aspect necessary
to reach circle is a mechanism that efficiently and accurately incorporates user feedback into
the translation engine. Accurately, because we want the MT system not to repeat the same
mistakes and, at the same time, not to worsen its overall translation quality; efficiently to
engage users, we need the system to react quickly (if not instantaneously) to their feedback.
Even though the exploitation of user edits (UE) is a widespread practice with increasing
interest, few methods aim at improving existing MT models. Most of the work is highly focused on translation dictionaries and centered in minimizing the out-of-vocabulary (OOV)
160

fiLeveraging Online User Feedback to Improve SMT

ratio of the translations (Cettolo, Federico, Servan, & Bertoldi, 2013). Studying the performance of the enriched translation models on a variety of aspects of translation quality
(e.g., morphology, word ordering, lexical selection, etc.) is an issue that deserves further
attention from the MT community.
In this work we explore to which extent we can use translation edits collected from
non-professional users of a commercial on-line translation portal to improve the translation
quality of a general purpose SMT system. The main contribution is twofold. First, we
address the noisy crowd-sourcing scenario by training supervised classifiers to identify useful
UE instances. Second, we devise SimTer, a pivot-based method for aligning user-edited
translations to both the source text and the original automatic translations, with an aim
to detect the specific corrected errors and to build enriched translation models accordingly.
Both aspects, UE filtering and pivot-based selection of phrase pairs, are novel and we show
that they contribute significantly to a better translation quality. We support this claim by
extensive experimentation and analysis. The improvement achieved is remarkable compared
to a simple corpus-concatenation strategy, since we work with a relatively low quantity of
UEs. Additionally, we conduct a manual evaluation of the output of the enriched system.
This study reveals the strengths and weaknesses of the enriched system and the source of
its improved results, which are not only achieved by means of the reduction of the OOV
ratio. Interestingly, other more linguistically-founded aspects of translation quality are also
improved. Finally, we show the generality of our approach by successfully applying the same
architecture and models to a noisy domain-adaptation scenario, where the new examples
come from an automatically-crawled bilingual corpora and filtering out noisy examples is a
key aspect of the adaptation.
The rest of the article is organized as follows. Section 2 puts the current article in context
by overviewing related work. Section 3 describes and locally evaluates our classification
approach for identifying useful UE instances. Section 4 discusses our proposal for improving
machine translation models with UEs. Section 5 presents our experiments on real datasets
showcasing the proposed methodology. Finally, the conclusions are presented in Section 6.

2. Related Work
In this section we overview the most relevant work related to the two main contributions
of this article: automatically identifying useful user edits (UEs) and improving existing
translation models (TMs) on the basis of such UEs.
Identifying the useful UE instances is necessary because many of the ones collected
from non-professional users may not represent better translations as compared to the ones
produced by the system itself. To the best of our knowledge, no other research on this
particular topic exists. Some analog tasks can be found either on obtaining quality material
by filtering automatically gathered corpora or on domain adaptation. Usually, the subsampling selection method (Foster, Goutte, & Kuhn, 2010; Axelrod, He, & Gao, 2011)
is used when dealing with corpus selection problems. It consists of a simple rationale:
a language model (LM) is created with reliable in-domain data and, subsequently, those
parts of the corpus having a lower perplexity with respect to the model are selected. In our
scenario, we have to select the best UE instead.
161

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

In our previous research on this topic (Pighin, Marquez, & May, 2012; Barron-Cedeno
et al., 2013) we defined a basic set of features to perform such identification. We aim at
capturing different aspects, such as: (i) whether the UE text is a more adequate translation
from the source sentence than the automatic translation (computed with simple surface
similarity features), (ii) whether the UE includes mistakes or typos, and (iii) whether the
source contains mistakes or typos that prevent from obtaining sensible translations. The
identification is then cast as a supervised classification problem using the above mentioned
features. Our work extends this approach, which is explained and evaluated in Section 3.
Assuming a set of good edited translations (revised by an expert), the enrichment of
the translation system involves two separate steps. First, an alignment between the source
sentences and the edited translation is computed (alignment). Second, improved translation
models are learned using these alignments (adaptation). Regarding the alignment step, it is
widely accepted that the best possible alignment is obtained by adding to the training corpus, new sentence pairs in which the edited sentence is treated at the target-side and models
are estimated from schratch (Hardt & Elming, 2010). However, the large amount of training
data makes this approach computationally expensive; an obstacle to the goal of reacting
quickly to the user feedback. To overcome this problem, several incremental alignment
models have been proposed in the literature(Levenberg, Callison-Burch, & Osborne, 2010).
With the exception of the stream-based translation approach, which adds or updates the
original TM scores according to the new material (Ortiz-Martnez, Garca-Varea, & Casacuberta, 2010; Martnez-Gomez, Sanchis-Trilles, & Casacuberta, 2012; Mathur et al., 2013),
the adaptation step is usually carried out by creating specific translation tables from the
edited translations (using the standard phrase-extraction and phrase-scoring algorithms)
and then combining them with the original translation tables. It is important to note that
most of the work on incremental adaptation has been tested in the scenarios where references are used instead of UEs. Hence, related work mainly addresses simulated or artificial
scenarios where the conclusions might not be totally representative. Only a few models use
human-edited translations. Mathur et al. (2013) along with Bertoldi et al.(2013) rely on
a business-oriented specific corpus in computer-assisted translation. Simard et al. (2007)
introduced the so-called automatic post-editors (i.e., monolingual SMT systems designed
to improve translation errors). Potet et al. (2011) considered a small corpus of UEs from
non-professional users, but not an incremental methodology. On the basis of all previous
aspects we describe next a few selected papers.
Hardt and Elming (2010) proposed an approach to produce approximate alignments.
They defined an approximate alignment as the one that allows to extend a phrase table,
even if it is not perfect or exact. Given a hsource, user-editi pair and baseline alignment
(Och & Ney, 2003), they explored all the available links by selecting the ones producing the
highest probability according to IBM model 4. Moreover, they improved the alignments
applying two heuristics: the first non-aligned word in the source is aligned to the first
non-aligned word in the UE and unlinked fragment pairs surrounded by corresponding
alignments are linked. One drawback of this method is that the alignments produced are
noise-sensitive, as they are built upon heuristics. This makes the methodology unpredictable
and unstable to deal with words that have not been seen by the baseline alignment model.
For the adaptation step, they built a separate phrase table with UEs and decoded with both
phrase tables. They used approximate and exact GIZA++ alignments and showed that the
162

fiLeveraging Online User Feedback to Improve SMT

performance of the approximate alignment yields half of the improvement of that obtained
by the GIZA++ alignment. However, only simulated data, using references instead of actual
human UEs, were considered in this work.
Ortiz-Martnez et al. (2010) and Martnez-Gomez et al. (2012) applied an incremental version of the Expectation Maximization (EM) algorithm (Neal & Hinton, 1998) that
minimizes an error function with small sequences of mini-batched data. This paradigm is
commonly known as stream-based translation, as small portions of data are processed over
time. More specifically, they incrementally adapted seven models including language models, length penalty, phrase translation models, and distortion. This strategy is reported to
perform reasonably well in non-stationary environments where fast adaptation is required,
such as interactive machine translation (Ortiz-Martnez, Sanchis-Trilles, Gonzalez-Rubio, &
Casacuberta, 2013). For a general-purpose web-based scenario, the resulting models are too
sensitive to the lately integrated data. In this approach no normalization was carried out
due to its computational cost. Mathur and Federico (2013) opted for leaving the original
features unaltered and added extra (normalized) feature that reflects the impact of the UEs
in the adaptation of the system.
Simultaneously to Ortiz-Martnez et al. (2010), Levenberg et al. (2010) proposed an
on-line strategy for phrase-based model enrichment using the stepwise EM alignment algorithm (Cappe & Moulines, 2009). The incorporation of new knowledge in this approach
is based on the re-estimation of scores of the phrase table by re-computing the counts
throughout a computationally efficient dynamic suffix array (Callison-Burch, Bannard, &
Schroeder, 2005; Lopez, 2008). A suffix array contains the starting index of each suffix of
the string containing the phrases in lexicographical order, allowing for an easy computation of on-the-fly translation probabilities for a given source phrase. A dynamic variant
of the suffix array supports deletions and insertions, making it suitable for a stream-based
approach. Moses uses this algorithm to provide an incremental training strategy (Haddow
& Germann, 2011).
mGIZA++ is a parallel implementation of the IBM and HMM models (Gao & Vogel, 2008).
As a byproduct, it performs forced alignment,1 which is an alternative to the step-wise
and incremental EM approaches. mGIZA++ builds multiple alignment models in parallel,
allowing for filtering and merging them afterwards to produce the exact alignment for
the aggregation. Consequently, one can obtain forcedly-aligned material and improved
alignment models with the same quality as if the concatenated dataset had been used from
the beginning. This tool is efficient in terms of processing time and storage requirements,
making it suitable for exact incremental alignment.
In contrast to the alignment methods described above, the pivot-based approach tries
to identify the specific edits performed by the user on the original translation and project
them to the source (Henrquez, Marino, & Banchs, 2011). More precisely, it uses the original
translation as a pivot to obtain alignments between the source and the UE translation, with
which to enrich the existing translation models. An advantage of this approach is that it
allows to spot incorrect fragments in the translation, which are the potential source of errors
in the MT system. Therefore, the new alignments obtained from the edited fragments are
the ones to be promoted within the TMs of the enriched translator. Formiga et al. (2012) and
1. The term forced alignment refers to coercively aligning unseen parallel text by selecting the maximum
probability given by the model, even if its value is low.

163

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Blain, Schwenk, Senellart and Systran (2012) studied this idea further to perform adaptation
with good results. The recently developed MateCat tool (Matecat, 2015), a Web-based CAT
tool, is a good example in this category. This approach has three advantages: it is fast, it
does not rely on baseline alignment models, and it has low memory requirements.
In all previous pivot-based alignment implementations, the edit distance is computed
with a translation-error-rate (TER) alignment algorithm, which takes into account reordering operations and paraphrasing. As this strategy is the basis of the methodology presented
in this article, we explain it in detail in Section 4.1. Concerning the adaptation strategy,
Formiga et al. (2012) combined UE-specific translation models with the baseline models using Foster and Kuhns (2007) interpolation with empirically-set weights. Blain et al. (2012)
studied two decoding strategies considering the baseline and UE phrase-tables separately:
back-off, if a phrase is not found in the baseline translation model, the phrase table is considered and multiple decoding, if the same phrase is found in both translation models, both
translations and scores are used. They found multiple decoding to be the best strategy while
restricting the TER alignment only to the substitution operations (i.e., neglecting addition,
deletion, and shifting edits). Some refined combination methods have been presented recently. Sennrich (2012) used the L-BFGS algorithm (Byrd, Lu, Nocedal, & Zhu, 1995) to
find the optimal interpolation values for each feature function of the TMs. Bisazza, Ruiz
and Federico (2011) defined a fill-up strategy to complement the missing information of the
original TMs. Nevertheless, these methods have not been challenged under an incremental
scenario.

3. Learning to Classify User Edits
This section describes our strategy to identify useful user-edits for improving the machine
translation system. We approach the task as a binary classification problem identifying the
cases in which the edited translation is more adequate than the system-produced translation
as positive. In doing so, we follow some of our previous work on human feedback filtering
(cf. Section 2).
3.1 Training Corpus
As training material we used the EnglishSpanish Faust Feedback Filtering (FFF+ ) 2 corpus, developed within the FAUST EU project.It contains 550 examples of real translation
requests and user-edits from the Reverso.net translation Web service. Each example contains seven fields:
SRC source sentence in English (users translation request to the system);
TGT automatic translation of SRC into Spanish;
UE a potentially improved user-edit of TGT provided by the user;
BTGT automatic translation of TGT back into English;
BUE automatic translation of UE back into English;
LANG the language set in the translators interface; and
2. Available at ftp://mi.eng.cam.ac.uk/data/faust/UPC-Mar2013-FAUST-feedback-annotation.tgz.

164

fiLeveraging Online User Feedback to Improve SMT

CL class label, i.e., whether UE is a more adequate translation of SRC than TGT.

Observe that the language set in the translators interface is a very likely indicator of
the users native language; a factor that may indicate that user edits in that language
are more reliable. The 550 examples of FFF+ were independently labeled as acceptable
or unnaceptable edits by two human annotators both were native Spanish speakers with
high command of English. All cases of disagreement (100) were discussed until consensus
was reached. The main criterion to decide whether the user-edit is acceptable is based
on translation adequacy (i.e., the degree to which the meaning of the source sentence is
conveyed in the translation). Concretely, UE is considered acceptable if it is strictly more
adequate than TGT, even if still imperfect. We believed that this lax criterion of acceptability
could work well as a proxy for usefulness, when thinking of enriching the MT system. For
a detailed description of the annotation guidelines, including examples, one may refer to
the work of FAUST (2013, Section 3.2). The levels of inter-annotator agreement achieved
(Cohens kappa 0.50.6) can be considered only moderately high. This fact evinces the
inherent difficulty of this task, even for humans. The positivenegative distribution of the
corpus is almost balanced: 50.5% versus 49.5%, indicating that the edits provided by casual
users are very noisy.

3.2 Learning Features
We considered five sets of features to characterize the examples fields and the relationships
between them: surface comparison, back-translation, noise-based, similarity-based, and quality estimation. The first four sets require no external resources other than a MT system
from the target back to the source language (having such a system at hand is very likely,
as the necessary resources to build it are practically the same as for the original-direction
system). This fact makes them particularly appealing for under-resourced languages. The
fifth set is composed of a combination of well-known MT quality estimation measures. In all
cases, features were extracted after text pre-processing, including case-folding and diacritics
elimination but original texts were incorporated into the translation system. Following,
we provide a short description of the main principles guiding each family of features. The
full list, comprising more than 90 individual features, is described in detail by FAUST (2013,
Section 3.2).

3.2.1 Surface Features
They consider the text strings SRC, TGT and UE, and compute surface similarities among
them at the level of word tokens and characters (length, length ratios, Levehnstein distance,
vocabulary containment, etc.). There is also a binary feature to account for the language
of the interface (LANG).
165

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

3.2.2 Back-Translation Features
These features account also for surface properties, but considered on the back-translations
of TGT (BTGT) and UE (BUE). Levenshtein distances both at the token and character level as
well as vocabulary intersections are included.3
3.2.3 Noise-Based Features
These are binary features intended to determine the likelihood of the text fragments to include noisy sections. We consider instances of SRC or UE that include characters repetitions
(longer than three characters) or tokens whose length is too high to be a regular word (>10
chars). Some of them try to determine a length-based translation difficulty by asuming
that the longer sentences are harder to translate (we consider features for the ranges [1  5],
[6  10], and [11, ) words).
3.2.4 Similarity-Based Features
This set includes different similarity measures between SRC, TGT, UE and their back-translations.
Borrowing concepts from cross-language information retrieval we estimate cosine similarities
across languages by using character 3-grams (Mcnamee & Mayfield, 2004). From machine
translation itself we consider models for parallel corpora alignment based on both pseudocognates and lengths (Simard, Foster, & Isabelle, 1992; Gale & Church, 1993; Pouliquen,
Steinberger, & Ignat, 2003). These features intend to be an alternative to Levenshtein-based
features in surface and back-translation sets.
3.2.5 Quality-Estimation-Based Features
We applied the 26 system-independent quality estimation (QE) measures provided by
Asiya (Gimenez & Marquez, 2010) to the SRCTGT and SRCUE pairs. These measures intend to estimate translation quality without human references, making them appealing for
our current framework. The QE measures are quite shallow, but they incorporate linguistic
information at the level of part of speech, syntactic phrases, and named entities. A bilingual
external dictionary is also used. Consequently, we can say that this set of features contains
more linguistically-oriented generalizations than the previous ones. Perplexity, the number
of out-of-vocabulary words of the translation sentence, as well as bilingual-dictionary-based
similarity between SRC and UE (or TGT) are included among the 26 measures.
3.3 Classifier Learning and Intrinsic Evaluation
We trained support vector machines (SVM) with the previously described features to learn
the classifiers. We used SVMlight (Joachims, 1999) with linear, polynomial, and RBF kernels
and we tuned the classifiers with 90% of the FFF+ corpus. The remaining 10% was left
aside for testing purposes. Feature values were clipped to fit into the range   3   2
to decrease the impact of outliers. Normalization was then applied by means of z-score:
x = (x  )/. Later on, the mean and standard deviation of the tuning dataset were used
to normalize the remaining test set instances.
3. All back-translations were produced by Spanish-to-English MT engines using the same Reverso.net
technology.

166

fiLeveraging Online User Feedback to Improve SMT

Table 1: Cross-validation results for linear SVMs trained with incremental sets of features,
with and without the application of feature selection. Best results are italic faced.

feature sets
surface + bt
+ noise
+ similarity
+ QE

F1
64.6
70.1
69.3
72.0

all features
P
R Acc
63.5 65.7 63.0
63.7 78.0 65.9
64.3 75.2 65.9
67.2 77.6 69.1

after feature selection
F1
P
R Acc
67.8 65.7 70.1 65.9
73.5 67.0 81.5 69.9
73.6 68.1 79.9 70.5
76.1 71.0 81.9 73.5

We evaluated on the basis of standard measures: classification accuracy, precision (ratio of predicted useful instances between all instances classified as useful), recall (ratio of
predicted useful instances between all useful instances in the dataset), and F1 (harmonic
mean of precision and recall). Our training strategy aimed at optimizing F1 and consisted
of two iterative steps: (a) parameter tuning: a grid search for the most appropriate SVM
parameters (Hsu, Chang, & Lin, 2003), and (b) feature selection: a wrapper strategy, implementing backward elimination to discard redundant or irrelevant features (Witten & Frank,
2005, p. 294). In short, this process performs an iterative search to remove the worst feature
from the dataset at a time, according to the performance obtained with the classifier that
neglects such feature. See further details in the work of Barron-Cedeno et al. (2013).
We present here an incremental evaluation to see the contribution of every feature
family and the effect of feature selection. The base feature set is composed of surface and
back-translation (bt) features. Then, we incrementally add noise-based, similarity-based
and quality estimation (QE) features. Table 1 presents the results. Figure 1 displays the
corresponding precisionrecall curves. The learning setting is restricted to linear SVMs in
this experiment. A first observation is that the feature selection procedure consistently
provides better accuracy and F1 scores; i.e., it allows to discard irrelevant features and also
some harming ones. Results show that all feature families contribute positively to the final
performance of the classifiers, the gains are accumulative. This improvement is especially
noticeable when quality estimation features come into play and feature filtering is applied.
The numerical results are backed by the shape of the precision-recall curves: including all
feature sets leads to better results, with precision levels clearly above 70% at recall levels
of 6070%.
A complementary study on using non-linear kernels for the task (not included here for
brevity), revealed that even though slightly better accuracy and F1 results can be obtained
by using non-linear kernels, the shape of the precisionrecall curve is better for the linear
classifier in the high-precision zone.4 Avoiding false positives is a very important property
when thinking of selecting useful user edits for MT improvement. Therefore, we used linear
classifiers in all the translation experiments in Section 5. The extended study, including
kernel variants, is available at the description in the report of FAUST (2013, Section 3.4).
This report also includes more detailed experiments on the relevance of individual features
4. For values above 0.6 precision, the curve for the linear classifier is smoother and contains a much larger
area below it.

167

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

surface+bt
+noise
+similarity
+QE

1

Recall

0.8

0.6

0.4

0.2

0

0.5

0.6

0.7
0.8
Precision

0.9

1

Figure 1: Precision-Recall curves for different learning settings and feature sets over the
development partition. Black dots represent the optimal F1 values.

and a comparison of the example rankings produced by different classifier variants on the
set of examples.
Finally, we evaluated the different classifiers on the 10% test partition from the FFF+
corpus. Absolute resuts are slightly lower: the linear SVM with all feature sets obtains F1
and accuracy values of 73.2 and 69.9, respectively,5 but the same results are observed. That
is, precisionrecall curves for linear SVMs including all feature sets are better than the rest,
allowing to obtain higher precision scores at similar levels of recall.
3.4 Discussion
The classifiers analyzed in this section showed modest levels of precision, only slightly over
70% at acceptable levels of recall. Significantly higher precision values can be reached only
at the price of lowering recall below 10%. This behavior reflects the difficulty of confidently
characterizing positive examples with the type of features used to train the classifiers. It
is worth noting that the task is also difficult for humans. The agreement achieved between
annotators (Cohens kappa below 0.6) can be considered only moderately high, but certainly
not fully satisfactory, evincing the inherent difficulty of the task. Translation quality is a
multi-faceted concept, which encompasses adequacy (i.e., whether the translation conveys
the meaning of the source), fluency (i.e., whether the translation is a fluent utterance in
the target language) and many other aspects; some of them defined on an application basis
(e.g., vocabulary usage, language register, post-editing effort, etc.). As a result, human
perception of translation quality is a highly subjective matter, depending on small details,
which are difficult to capture and delimit in a set of simple annotation guidelines. This effect
is amplified in our corpus by the noisy nature of the input text, where the input sentences
5. The FFF+ test set is very small and susceptible of statistical unstability when computing performance
scores.

168

fiLeveraging Online User Feedback to Improve SMT

often lack the necessary context to make fully reliable decisions. Fortunately, classifying
good user edits is not the end task. The ultimate goal of the UE classifiers is to perform
selection, i.e., to rank UE instances by acceptability and set an appropriate threshold to
select useful UEs to improve the SMT engine. In the following sections we empirically
show that, regardless of the limited performance of the local classifiers, the proper inclusion
of the selected highest-ranked user-edited translations into a general purpose SMT can
significantly improve its quality. Finally, one might argue that rather than training classifiers
to optimize accuracy on the local task, it would be better to define a joint UE-selection
MT-enrichment learning setting, where the classifiers were optimized directly against the
translation quality of the enriched SMT system. Although this is attractive in theory, it
would be extremely inefficient and practically infeasible in our pipeline.

4. A Method for Incorporating User-Edits into the SMT System
In this section we describe a method for incorporating user-edits into the machine translation
model. Our approach is developed under the assumption that the translation model and the
user-edited materials are the only data at our disposal to improve the translation system.
Our approach is divided in two steps: (i) using the original automatic translation as a
pivot to align the source text to the edited translation and extracting new phrase pairs
(Section 4.1), and (ii) the interpolation of the new phrase pairs with the original translation
model (Section 4.2). Some validation experiments are discussed in Section 4.3.
4.1 Pivot-Based Word Alignment
In order to detect and correct translation errors, we consider three pieces of information:
the source input text SRC, the target output translation given by the translation system
TGT, and its user-edited version UE. A monolingual alignment between TGT and UE allows for
predicting translation errors, by identifying the parts that have been edited. The projection
of this alignment to SRC allows for the extraction of new translation pairs, representing the
corrections provided in UE.
We propose a three-step alignment procedure. First, we compute the TER path (Snover,
Madnani, Dorr, & Schwartz, 2009) between TGT and UE, aligning the identical words from
both sides. Second, we estimate the best alignment among the possible combinations of
unaligned TGT and UE words by maximizing a similarity function between pairs of words in
TGT and UE sim(wt , wu ). Finally, once the monolingual alignment is made, we pivot the
alignment towards SRC: taking advantage of the decoder-provided word alignment between
SRC and TGT, we link words between SRC and UE if and only if there is a word in TGT that
connects them. This alignment process, which we call as SimTer is described in Algorithm 1.
The comparison between the translated and edited sentences is based on translation
edit rate, TER (Snover et al., 2009).6 In addition to the minimum number of edits, TER
obtains an alignment and edit path: the required sequence of edits that change the output
translation into the reference the user-edited sentence in our case. Figure 2 shows an
6. TER is an error metric that estimates the number of edits required to convert a translation output
into one of its references. Although based on the Levenshtein distance, TER additionally allows word
movements before considering the usual addition, deletion, and substitution operations in order to reduce
the number of changes.

169

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Algorithm 1

SimTer. A pivot-based algorithm to align SRC and UE through TGT

1: Translate SRC into TGT with the decoder and obtain the corresponding word alignments

align(ws , wt ) for every ws  SRC and wt  TGT;
2: Compute the TER path between TGT and UE and align identical words (cf. Section 4.1);
3: Align every non-aligned word wt  TGT to words wu  UE such that the summation of the

similarities of all new pairs, Sim(wt , wu ), is maximized;
4: For every pair of alignments al(ws , wt ) and al(wt , wu ), create a new alignment al(ws , wu );
It will not be easy to find the right mix
It the right mix will not be easy to find
s

e

e

e

e

e

e

e dd

Finding the right mix will not be easy
It will not be easy to find the right mix

Finding the right mix will not be easy

Figure 2: TER-path computed for a monolingual sentence pair. d, e and s stand for deletion,
exact (no change), and substitution. The minimum number of edits is three: two deletions
and one substitution.

example. First, in the upper part of the figure, TER allows word movements, so the right
mix is moved next to the first word. The minimum number of edits is then computed,
resulting in one substitution and two deletions. Finally, in the lower part of the figure,
the word movements roll back to their original positions and we obtain a word alignment
between the two sentences.
Although TER is able to align most of the words correctly, it may fail in some circumstances such as when the words affected by an edit are actually aligned according to their
position in the sentence, rather than their semantic similarity. In the example of Figure 2,
finding and to find should be aligned. As a counter measure to this issue, we propose
to consider the similarity between wt and wu as a linear combination of simple similarity
features:
8
X
sim(wt , wu ) =
i hi (wt , wu ) ,
(1)
i=1

where hi (wt , wu ) are the similarity features and i are the corresponding contribution
weights. Each hi models a different relationship between wt and wu as follows:
h1 - A binary feature that indicates if wt and wu are identical.
h2,3 - Two binary features accounting for the existence of links between wt1 and wu1 (h2 )
or wt+1 and wu+1 (h3 ).
170

fiLeveraging Online User Feedback to Improve SMT

h4 - A feature to penalize multiple links (onetomany). It considers the possibility of
link(wt0 , wu ) aligning a user-edited word wu to the TGT word wt when a link between
wt0 and wu has already been set. The feature penalizes this new link proportionally
to the distance between wt and wt0 :


kpos(wt )  pos(wt0 )k
h4 (wt , wu ) =  max
,
(2)
|TGT|t
link(wt0 ,wu )
where pos() is the position of  in TGT and |TGT|t is the number of tokens in TGT.
h5,6 - Two lexical features designed to evaluate the strength of the semantic relationship
between wt and wu according to their proximity to ws . This is done by considering
the bidirectional conditional probabilities between both (ws , wt ) and (ws , wu ) in the
translation table. Feature h5 (wt , wu ) is approximated as a normalized conditional
probability based on bilingual dictionaries:

h5 (wt , wu ) =

X
s:link(ws ,wt )

=

p(wt | ws )p(ws | wu )
0
0 UE p(wt | ws )p(ws | wu )
wu

(3)

p(ws | wu )
,
0
0 UE p(ws | wu )
wu

(4)

P
X

P
s:link(ws ,wt )

where a normalization factor is included in order to consider the contribution of p(ws |
wu ) only in the context of sentence UE. Feature h6 is analogous to h5 , but considering
p(ws | wt ) and p(wu | ws ) instead. If ws is an unknown word (i.e., it is replicated from
SRC to TGT without mapping in both bilingual dictionaries), we take h5 = h6 = 0.
h7 - This feature is applied only when wt is an unknown word that has been duplicated by
the translation system from the input sentence into the output. If wu and wt are the
same, we force them to be linked by giving h7 an arbitrarily large value. Otherwise, the
feature takes a real value as a function of the Levenshtein distance (LD) at character
level between the unknown wt and wu :
h7 (wt , wu ) = 1 

LD(wt , wu )
,
|wu |

(5)

where |wu | represents the length of wu in characters. This feature becomes a penalty
when the Levensthein distance exceeds the length of wu , preventing its linking to
longer unrelated words.
h8 - A penalty feature to prevent alignments between an unknown word wt and a stopword
wu . It takes a large negative value if wu is a stopword; zero otherwise. We take as
stopwords determiners, articles, pronouns, prepositions, and auxilary verbs.
The  weights relative to each hi feature are obtained through a downhill simplex
algorithm (Nelder & Mead, 1965). We give more details in Section 4.3.2.
171

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Figure 3: Process to compute the interpolation weight () and re-tune the TM feature
coefficients ().

4.2 Incorporation of the Aligned Phrase Pairs
After computing the SimTer word alignment between source and edited translations, we
use this new parallel corpus (SRC,UE) to enrich and retrain the translator. We have to
deal with two types of newly extracted phrase pairs (translation units): (i) previouslyseen
phrase pairs are those already included in the original translation model; they are relatively
up-weighted within the translation models, favoring their selection when facing a similar
situation, and (ii) new phrase pairs which are those missing in the original translation model
and represent the principal resource to improve the translator. New phrase pairs must be
added to the final translation model in order to provide the decoder with new translation
options when facing similar source sentences.
We include the new phrase pairs into the original model using Foster and Kuhns (2007)
interpolation method, initially designed to address domain adaptation problems. The relatively small translation models extracted from the user edits are estimated by means of
symmetrization and phrase-extraction standard algorithms with grow-diag-final-and heuristic. Then, the original and the new translation models are linearly interpolated according
to:
TM(phrasei ) =  TMoriginal (phrasei ) + (1  ) TMpe (phrasei )
(6)
where TM, TMoriginal and TMpe are the final, original, and UE-based translation model
scores for a phrasei pair. The setting of the interpolation parameter  is strongly coupled
to the re-tuning of the classical set of weights () used to combine the SMT features. We
applied the iterative two-step process outlined in Figure 3. The process starts with the set
of weights 1 from the original translation system and iterates while the updates to the 
weight yield BLEU improvements. This is a two-step iterative process. First, the best i
172

fiLeveraging Online User Feedback to Improve SMT

Table 2: Statistics of the EnglishSpanish corpora used to obtain the SimTer similarity
function weights .
Corpus
EPPS
EPPS UE

Sent.
Eng
Spa
Eng
Spa

1.90 M
100

Words
49.40 M
52.66 M
2,862
3,022

Vocab.
124.03 k
154.67 k
1,017
1,089

avg.len.
26.05
27.28
28.6
30.2

weighting is computed using the best i1 available. In the case of i = 1 no interpolation
is taken into account (0 = 0). Afterwards, the optimum i interpolation parameter is
computed using the best set of translation feature weights i obtained.
The combination procedure is simpler for the reordering models. It follows a fill-up
strategy that preserves every entry and score from the original model and adds new entries
and scores only for the new phrases (Bisazza et al., 2011). Units that appear both in the
original model and the one obtained from user-edits preserve the reordering score of the
original model.
4.3 Validation Experiments
The objective of the experiments described in this section is twofold: (i) tuning the metaparameters for our algorithms and (ii) validating the proposed methodology in a well established domain-adaptation task. We consider these experiments preliminary, as translation
references are used instead of proper user edits.
4.3.1 Datasets
We selected different datasets for these experiments. In order to optimize the  parameters
of the similarity function in Equation (1), we used the Europarl v6 corpus, EPPS (Koehn,
2005), to build a base phrase-based SMT system. To evaluate the alignments, a small
manually-aligned corpus, EPPS UE (Lambert, de Gispert, Banchs, & Marino, 2005), was
used to perform the pivot-translations and subsequent SimTer alignments. This small corpus
belongs to the same domain as EPPS but there is no intersection between them. Table 2
shows some statistics.
In order to tune the  and  parameters, and to validate the proposed methodology, we
used the corpora from the WMT12 campaign (Callison-Burch, Koehn, Monz, Post, Soricut,
& Specia, 2012). It contains parallel sentences from the EPPS corpus already mentioned,
News Commentary (NC), and United Nations (UN). It also includes a monolingual version
of Europarl and monolingual corpora based on News (broken down by years: 2007-2011).
Tables 3 and 4 provide descriptive statistics of these datasets, computed after cleaning and
pre-processing. Additionally, we used the WMT08-11 test material for tuning the  and
the TMs s (dev), and WMT12/13 tests for testing the methodology (test12 and test13).
Table 5 shows the statistics for the tuning/test material.
173

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Table 3: Statistics of the additional WMT12 EnglishSpanish parallel corpora for training
the translation models (used for tuning and validation purposes).
Corpus
NC
UN

Eng
Spa
Eng
Spa

Sent.
Words Vocab.
Preliminary Experiments
3.73 M
62.70 k
0.15 M
4.33 M
73.97 k
205.68 M 575.04 k
8.38 M
239.40 M 598.54 k

avg.len.
24.20
28.09
24.54
28.56

Table 4: Statistics of the Spanish monolingual corpora used to build the language models.
Corpus
EPPS
NC
UN
News.07
News.08
News.09
News.10
News.11

Sent.
2.12 M
0.18 M
11.20 M
0.05 M
1.71 M
1.07 M
0.69 M
5.11 M

Words
61.97 M
5.24 M
372.21 M
1.33 M
49.97 M
30.57 M
19.58 M
151.06 M

Vocab.
174.92 k
81.56 k
725.73 k
64.10 k
377.56 k
287.81 k
226.76 k
668.63 k

avg.len.
29.18
28.55
33.24
28.91
19.19
28.63
28.54
29.55

Table 5: Statistics of the development and test corpora used to tune and test the translation
system.
Corpus
dev
test12
test13

Eng
Spa
Eng
Spa
Eng
Spa

Sent.
Words Vocab.
WMT based dev/test
189.01 k 18.61 k
7,567
202.80 k 21.75 k
63.78 k 14.34 k
3,003
69.45 k 16.47 k
56.09 k 13.34 k
3,000
62.05 k 15.16 k

avg.len.
25.0
26.8
21.2
23.1
18.7
20.7

4.3.2 Tuning the Parameters of the Similarity Function
We built the baseline SMT system following the standard pipeline of a Moses training with
EPPS (Koehn & Hoang, 2007). We applied the resulting system to translate the source
side of the manually-aligned corpus (EPPS UE). Then, we carried out a downhill simplex
process to adjust the  weights (Nelder & Mead, 1965), except for 8 that was fixed to
1. Recall that h8 assigns large costs to prevent alignments between unknown words and
function words. When activated, it works a as hard constraint, pruning the hypotheses
space, so the value of 8 can be chosen arbitrarily to any positive number different from
zero. SimTer was applied at each of the three completed iterations. Our final goal is to
produce an alignment between the source sentence and its edited translation. Therefore, we
evaluated using Alignment Error Rate (AER) with respect to the manual sourcereference
174

fiLeveraging Online User Feedback to Improve SMT

Table 6: Contribution weights of the similarity function features.
1
2
3
4
5
6
7
8
0.08 0.75 0.91 3.08 0.47 2.02 1.50 1.00
Table 7: Translation quality for different values of .

BLEU

0.1
27.86

0.2
28.18

0.3
28.33

0.4
28.49

0.5
28.68

0.6
28.75

0.7
28.69

0.8
28.62

0.9
28.45

alignment (Och & Ney, 2003). AER was reduced from 20.12% to 17.57% (13% relative error
reduction), reaching a performance equivalent to that of mGIZA++ with the same corpora.
Table 6 includes the resulting s. As the 4 value shows, the penalization for distant links
(h4 ) is the most important feature. As for the lexical features, h6 is significantly more
relevant than h5 . Interestingly, the feature h1 (same word) was not considered relevant
compared to the others.
4.3.3 Tuning and Validating the Combination Method
The last step before fully testing our strategy is to compute the  parameter from equation (6). As mentioned before, we used the corpus from the WMT12 campaign (CallisonBurch et al., 2012). We trained a baseline English to Spanish system as a factored Moses
phrase-based system (Koehn & Hoang, 2007) from words into words and POS tags (Formiga
et al., 2012).7 The base system considered EPPS and UN concatenated as a whole corpus.
Regarding the monolingual data, a language model (LM) was built for each corpus and
then interpolated by minimizing the perplexity on the development set (Schwenk & Koehn,
2008). In this experiment we translated the English sentences of the NC parallel corpus
and took the Spanish references to simulate user edited translations (UE). We performed
a SimTer word alignment to build UE-specific translation and reordering models. Finally,
we applied the  optimization method depicted in Section 4.2.
Table 7 shows the BLEU scores obtained with different values of . When combining
the translation models, the BLEU improved from 27.86 to 28.75, achieving its highest value
with  = 0.6 (i.e., a 6040% distribution of the weight for the base and edited translation
models, respectively). After setting  and , we validated our adaptation method with
the obtained hyper-parameters. We also compared our combination method (referred to as
Linear Interpolation ( = 0.6)) to other methods available in Moses, namely:
Concatenation NC, EPPS, and UN are aggregated as a single training corpus.
Multiple Tables Either Two parallel decodings corresponding to each TM are launched
separately, selecting the one with the best score.
Multiple Tables Both One decoding is launched considering all the options available
in both phrase tables, doubling the number of translation features in the log-linear
model.
7. The text was POS-tagged with the Freeling suite of NLP analyzers (Padro, Collado, Reese, Lloberes, &
Castellon, 2010).

175

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Moses Incremental training The base phrase-table is updated with approximate alignments (see Levenberg et al., 2010 in Section 2) . The alignments are computed with
the Moses incremental inc-giza-pp algorithm instead of the SimTer algorithm.

Table 8: Results with different combinations of base and SimTer-specific translation models.
In BLEU and NIST columns,  and  indicate significant differences over the
Concatenation system with 0.95 and 0.90 confidence levels, respectively. Best
absolute results are depicted in bold face. Moses incremental training is shown
for comparison purposes although it does not use SimTer alignment

test12
Base
Concatenation
Linear Interpolation ( = 0.6)
Multiple tables either
Multiple tables both
Moses Incremental training
test13
Base
Concatenation
Linear Interpolation ( = 0.6)
Multiple tables either
Multiple tables both
Moses Incremental training

BLEU

NIST

TER

METEOR

ULC

32.77
33.03
33.25
33.20
31.72
32.58

8.63
8.66
8.70
8.70
8.51
8.61

48.65
48.48
48.24
48.19
49.42
48.86

55.48
55.64
55.84
55.76
54.88
55.40

70.78
71.16
71.66
71.61
69.21
71.04

28.74
28.96
29.15
29.13
28.02
28.57

8.01
8.07
8.07
8.07
7.91
7.97

51.60
51.29
51.28
51.29
52.40
52.01

52.82
53.11
53.15
53.03
52.26
52.67

70.94
71.56
71.73
71.66
69.52
70.74

We used BLEU, NIST, TER and METEOR (Papineni, Roukos, Ward, & Zhu, 2002;
NIST, 2002; Snover et al., 2009; Denkowski & Lavie, 2011) as automatic translation
quality measures. Additionally, we considered a linear combination of the former ones:
ULC (Gimenez & Marquez, 2010). Table 8 presents the obtained results. The adaptation
strategy outperforms Baseline and Concatenation configurations practically for every test
corpora and quality measure. More precisely, we found significant differences under BLEU
(test12 and test13) and NIST (test12) metrics.8 The performance of the either combination was very close to our linear interpolation method. However, the either combination is
computationally expensive, almost doubling the time required by the linear interpolation
method. Table 9 includes the translation times for reference.9 The incremental approach
is the fastest one, but it yields no improvement over the baseline.
In summary, the alignment and combination methods proposed in this work offer significantly better results without sacrificing computational efficiency, compared to the other
alternative methods provided in Moses. Therefore, we do not consider multiple decoding
and incremental strategies in the remaining experiments of the paper.
8. In this work, significances are computed through paired bootstrap resampling (Riezler & Maxwell, 2005)
9. These figures were computed on a Linux server with 96 GB of RAM and 24-core CPU Xeon processors
1.6 GHz (134064 Bogomips in total). Multi-threading was not used to compute the decoding times.

176

fiLeveraging Online User Feedback to Improve SMT

Table 9: Translation times in seconds (Collecting+Decoding) for each combination method.
Combination
method
Moses Incremental training
Linear Interpolation ( = 0.6)
Multiple tables both
Multiple tables either

Num.
sent.
3,003

Total time
test12
test13
7,502.52
6,553.45
8,284.79
7,684.65
13,353.80 12,291.70
13,097.40 11,364.80

Time per Sentence
test12
test13
2.50
2.18
2.76
2.56
4.10
4.45
4.36
3.79

Table 10: Statistics of the EnglishSpanish parallel corpora used in the FAUST scenario.
Corpus
FAUST UE
FAUST dev Clean

FAUST test Raw

FAUST test Clean
FAUST Monolingual

Sent.
Eng
Spa
Eng
Spa ref0
Spa ref1
Eng
Spa ref0
Spa ref1
Eng
Spa ref0
Spa ref1
Spa

6,610
1,998

998

1,996
98,199

Words
43,310
47,800
24,588
24,588
25,270
9,941
10,135
10,333
19,773
20,270
20,666
1.15 M

Vocab.
8,250
10,430
3,758
3,758
3,743
4,184
4,484
4,499
4,737
4,484
4,499
89,378

avg. length
6.6
7.2
12.3
12.3
12.6
10.0
10.2
10.4
9.9
10.2
10.4
11.67

5. Experiments with Real Data
In this section we present experiments on using our methodology to improve already existing
MT systems with real data. We describe two experiments. In the first scenario, the new
material comes from a collection of user-edited translations submitted to the Reverso.net
MT Web service (cf. Section 5.1). In the second scenario, new material is selected (cf.
Section 5.2) from CommonCrawl (Smith et al., 2013)
5.1 User-Provided Edited Translations (FAUST)
In the the FAUST project (cf. Section 3) the goal was to improve the quality of on-line MT
services by leveraging users feedback, mainly in the form of suggested improved translations.
In these experiments, we take advantage of parallel and monolingual data supplied by a set
of user translation queries and their edits. These users belong to an on-line community
motivated to edit the response to their translation queries.
The FAUST parallel corpora are composed of two non-overlapping collections of translation requests gathered from the Reverso.net website: FAUST UE and FAUST dev/test.10
FAUST UE includes triplets composed of input source, MT output, and the user edit. We
use this corpus for training purposes. The FAUST dev/test corpus includes target refer10. A sample of FAUST UE is available at ftp://mi.eng.cam.ac.uk/data/faust/FaustFeedbackSample.
xls.gz under FAUST User-edited corpus. FAUST dev/test are available at ftp://mi.eng.cam.ac.
uk/data/faust/FAUST-1.0.tgz.

177

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

ences provided by two professional translators. Moreover, the translators processed the
source inputs to reduce noise (e.g., removing slang words, misspellings, smileys, etc.). This
process resulted in two versions of the dev/test corpus: Raw, where the source inputs are
the original ones, and Clean. In our experiments we considered the FAUST dev Clean version for tuning (less error prone), and the real FAUST test Raw for testing. The FAUST
monolingual corpus is composed of 98,199 translation requests to the Reverso.net website
that had Spanish as the source language. No selection with respect to the target language
was made. Table 10 shows some statistics of the FAUST corpora.
For our first experiment we took the Concatenation MT system from Section 4.3.3
and adapted its target language model to the FAUST scenario as follows: (i) we built a
specific web-domain language model from the FAUST Monolingual corpus, (ii) we obtained
a language model by means of a new interpolation of all language models according to
perplexity minimization on the FAUST dev Clean corpus, and (iii) we tuned the weights of
the translation features using MERT again to maximize BLEU on the FAUST dev Clean
corpus. Our goal was to set a strong baseline system for the experimental comparison on
this datset. We will refer to it as Base FAUST.
In order to select the most suitable feedback material to improve the Base FAUST
system, we ranked the FAUST UE collection of 6,610 user-edited instances according to the
SVM classifier scores (cf. Section 3). The SVM labeled 61% of the data as useful feedback
(we call that point TH0, for decision threshold=0). However, there is no guarantee that
this level of selection maximizes the translation quality of the adapted system. Therefore, we
carried out an analysis of the quality as a function of the percentage of selected user-edited
instances.
5.1.1 Results
Figure 4 depicts the performance obtained on the FAUST test Raw corpus with different
percentages of selected user-edited data. This figure focuses on two evaluation metrics:
NIST, which is based on a classical n-gram matching approach with an improved brevity
penalty providing more robustness to noise than BLEU and TER, which tries to mimic
the editing effort that would be addressed by humans in order to obtain a high-quality
translation. Very similar curves are observed when using FAUST test Clean (not shown for
brevity). Table 11 presents a more complete comparative results on the FAUST Raw corpus,
with the set of extended evaluation metrics from Section 4.3.3: BLEU, NIST, METEOR,
TER, and ULC. Different adaptation and filtering strategies are also presented in the
table, including: (i) different filtering methods (FFF+ and Subsampling), (ii) adaptation
methods (Concatenation vs. SimTer), and (iii) different percentages of included user-edits:
50%, 61%(TH0), and 100%. Significances are computed in the same way as described in
Section 4.3.3. We only performed subsampling by computing the perplexity between the
existing models and the UE part, as we wanted to select the best user edits.
When analyzing the results, it is worth noting that we already have a strong baseline,
tuned in-domain by using domain specific monolingual data. Several observations can be
drawn. The first block (SimTer 0.6 & Subsamp.) shows that adding new material by
subsampling filtering provides none or little improvement to the baseline depending on the
evaluation metric under analysis. More precisely n-gram based metrics BLEU and NIST
178

fiLeveraging Online User Feedback to Improve SMT

8.3

8.25
8.22

8.24

51.5

51.25

51.1

51.0
8.11

50.5

8.1

8.09

50.64

TER

NIST

8.2

8.1

50.02

50.0

49.9

49.67

49.5
RAW

8
0

25

50

75

RAW
100

0

25

50
75
% of best PE selected

100

Figure 4: NIST and TER scores in function of the percentage of best ranked user-edits
used.

Table 11: Results obtained before and after considering the feedback instances depending on
the amount of user-edits used and the different filtering and adaptation methods.
 and  indicate significant differences over the Baseline system with 0.99 and
0.95 confidence levels. Best absolute results are highlighted.
Translation system
Adaptation
Filtering
Baseline

Subsamp.
SimTer 0.6
FFF+

Concatenation

FFF+

% of edits

BLEU

NIST

TER

METEOR

ULC

0%
+25%
+50%
+75%
+50%
+TH0-61%
100%
+50%
+TH0-61%

33.34
33.26
33.41
32.95
34.01
33.68
33.18
32.89
33.13

8.11
8.11
8.16
8.13
8.25
8.22
8.10
8.15
8.08

51.25
51.27
50.48
50.58
49.67
50.02
50.64
50.29
50.63

55.10
55.06
55.55
55.32
56.06
56.14
55.54
55.79
55.46

71.05
70.96
71.85
71.39
73.23
72.78
71.68
71.83
71.44

do not capture any improvement as TER and METEOR slightly do. The most important
evidence is provided by the second block (SimTer 0.6 & FFF+), which is the strategy we
propose in this paper. The results evince the appropriateness of the FFF+ filtering as it
yields significantly better results in all metrics compared to Subsampling. However, it is
remarkable that the FFF+ and Subsampling learning curves obtain the best results with
only the 50% of the total user-edits considered. The filtering strategy results crucial in
obtaining a final improvement. Not only regarding the method, but also because using all
the edits with no filtering (+100%) has no impact on the n-gram-based metrics (BLEU and
NIST), and only marginally improves with the other metrics (TER and METEOR). The
last block (Concatenation & FFF+) confirms the important contribution of the SimTer 0.6
adaptation strategy compared to the straightforward approach of adding the new material
by concatenation. In this case, using exactly the same filtered material, SimTer 0.6 yields
better results compared to the concatenation strategy.
179

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Alignment / Interpolation Method

Alignment / Interpolation Method
51.5

8.30

51.0

8.20

50.5

NIST

TER

8.25

8.15

50.0

8.10
49.5
8.05
SimTer 0.6
RAW

8.25

mGIZA++ 0.6

8.24

SimTer PPL

8.13

mGIZA++ PPL

8.12

SimTer 0.6
RAW

49.67

mGIZA++ 0.6

49.94

SimTer PPL

51.09

mGIZA++ PPL

51.17

Figure 5: NIST and TER translation performance over the FAUST Raw and Clean tests
achieved with different alignment/phrase-table interpolation methods.
In short, the UE-enriched translation models following our SimTer 0.6 & FFF+ yield a
significant final improvement of (+0.67) BLEU points and (-1.57) TER points on the test
set. Introducing user edits without any pre-selection nor boosting scheme does not allow
the MT system to achieve consistent improvements.
Additionally, we compared our alignment/phrase-table interpolation approach to other
competitive variants present in the literature. Concerning the alignment, we considered
the forced alignment capability of mGIZA++ as defined by Gao and Vogel (2008): the malignments are obtained from a re-trained IBM 4 model that was iterated with all the data
the original training and the edited material. Regarding the weight-interpolation strategy,
we considered the perplexity minimization method (PPL) of Sennrich (2012), especially
suited for domain adaptation. In this approach, a development translation model (TM)
(i.e., phrase-table) was built from a small development set. This development TM is used
to minimize the perplexity when combining different TMs in our case base and UE-based
models. The strength of this approach is the granularity of the weights: instead of giving
a different weight to each phrase-table, it assigns different weights to each feature function
within the phrase-table, trying to lower the perplexity as much as possible. We used the
optimization method L-BFGS with numerically approximated gradients (Byrd et al., 1995).
Figure 5 presents the results obtained in the four alignment/interpolation combined
scenarios. Concerning the comparison of alignment methods, NIST shows no significant
differences between the SimTer pivot-based and the mGIZA++ approaches. However, TER
reflects a bigger difference in favor of SimTer. This behavior is coherent with our alignment
strategy, focused on finding the particular edits given by the user. In terms of computational
time, there are no big differences: both alignments are computed in less than a minute for
6,610 sentences. One of the advantages of using SimTer is that it does not require nor
depend on previous alignment models (mGIZA++ or alike). 11 Comparing them in terms
of alignment error rate is beyond the scope of this paper. SimTer is specifically tailored
to find the differences between the original and the edited translations in order to extract
11. We are not claiming that SimTer can work well as a general purpose alignment algorithm, being competitive to mGIZA++ or other state-of-the-art aligners.

180

fiLeveraging Online User Feedback to Improve SMT

Table 12: Real examples of translations changed (but not necessarily improved) by the
SimTer 0.6 & FFF+ (50%) system and their categorization according to the
linguistic phenomena studied

Function
Words
Additions/
Omissions
(worse)
Lexical

Reorder.

Bad Feedback
Morphol.
Combined

Source

Baseline

Applicants
authorized
representative
information:
Tell me how Im supposed
to breathe with no air.

El representante autorizado solicitante informacion:
DIME como se supone que
tengo para respirar sin aire.

SimTer 0.6
& FFF+ (50%)
Representante autorizado
del solicitante de la informacion:
DIME como me supone
para respirar sin aire.

Do you use all the letters
in the English alphabet to
write in Persian?
These measures were updated and developed in the
2005 strategy review.

utilizar todas las letras del
alfabeto persa para escribir
en espanol?
Estas medidas fueron actualizados y la revision de la
estrategia desarrollada en
el 2005.
verdad?
Si un extranjero llega (. . . ),
me dejan por el?
Informamos que vamos a
darle garanta de 5 anos
sobre los materiales y la
mano de obra.

utilizar todas las letras
del alfabeto ingles para escribir en persa?
Estas medidas se han actualizado y desarrollado
en la revision de la estrategia de 2005.
Hiza intentoHazlo?
Si un extranjero llega (. . . ),
vas a dejarme por el?
Informamos que vamos a
proporcionarle 5 anos de
garanta sobre materiales
y mano de obra.

Did it?
If an alien comes (. . . ), will
you leave me for him?
Please be informed that we
will be provide you 5 years
warranty on material and
workmanship.

new phrase pairs, useful to improve the translation models. Thus, the two tools serve a
different purpose.12 Finally, regarding the interpolation strategy, setting the interpolation
weights with the perplexity minimization method from Sennrich (2012) does not provide
enough boosting to the UE-based models. This issue is particularly observed when looking
at the weights set by the L-BFGS algorithm: they are in the order of  0.99 for the baseline
model and  1  103 for the UE-based model. On the contrary, an optimization based on
the quality of the provided translation addresses the point directly.
5.1.2 Qualitative Output Analysis
The results presented so far are based on automatic evaluation metrics. We now complement the study with a set of human assessments in order to verify if the improvement is
also perceived by humans and to identify the characteristics that make the new translations
better. Five expert annotators analyzed 414 instances from the FAUST test Clean corpus.
The annotators observed triplets composed of source sentence, the translation produced
by the Baseline, and the translation of the better performing SimTer 0.6 & FFF+ (50%)
system. They had to determine which of the two translations was better or, instead, if they
had the same quality. An additional option I cannot tell was possible as well. Annotators
12. For the sake of completeness, we can mention that some experiments conducted to evaluate the performance of SimTer as a general aligner showed AER results significantly lower than those of mGIZA++
(Henriquez, 2014)

181

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Table 13: Results of the comparative analysis carried out by five human annotators on the
translation of 414 sentences of the FAUST test Clean corpus by the Baseline
and SimTer 0.6 & FFF+ (50%) systems. For each criterion (row), Better indicates that user-adapted models provide a better translation compared to the
non-adapted system, Worse indicates it conversely and Changed indicates that
the translations are different.
Adequacy
Fluency
Function Words
Additions / Omissions
Lexical
Reordering
Bad Feedback
Morphology

Better
34.54%
32.61%
Better
50.00%
31.54%
47.59%
57.50%

35.77%

Same
40.58%
49.76%
Same
18.56%
20.53%
31.99%
18.37%

40.73%

Worse
15.70%
17.63%
Worse
31.52%
47.93%
20.40%
24.13%
100.00%
23.45%

Cannot Tell
9.18%

Changed
13.04%
17.63%
60.39%
21.01%
5.31%
19.57%

Table 14: Progression of OOV ratio and BLEU for the FAUST test Clean corpus along
with the different acceptance levels of user-edited instances.
Selection
Ratio
0% (no-feedback)
25%
50%
61%
75%
100% (all-feedback)

OOV
Words
563
559
557
554
550
550

Total
Words

22,898

OOV
Ratio
2.46%
2.44%
2.43%
2.42%
2.40%
2.40%

BLEU
37.85
37.86
38.65
38.32
38.47
37.79

did not know which system produced which translation, and the order of presentation of the
two options was randomized. The overall quality assessment was based on translation adequacy and fluency, but annotators were also asked to provide detailed information on which
linguistic aspects made one translation better than the other one; e.g., changes on function
words, addition or omission of spurious words, lexical coverage, reordering, morphology,
and presence of harmful elements (bad feedback, i.e., mistranslations clearly introduced by
erroneous user edits). Table 12 includes real samples of the studied phenomena. Cohens
kappa agreement for all the annotators on a selection of 10 common phrases was  = 0.57.
Table 13 presents the overall results. The percentage of sentences in which the translation changes significantly in terms of adequacy or fluency is around 50%-60%. The number
of translations in which adequacy and fluency is improved by the SimTer 0.6 & FFF+ (50%)
system doubled the number of cases in which it lowered its quality. This fact confirms the
results obtained with the automatic evaluation measures. Table 13 shows that, about the
60% of the sentences underwent a lexical modification. Other aspects received less bus
significant impact: reordering (21.01% of sentences affected), morphology (19.57%), addi182

fiLeveraging Online User Feedback to Improve SMT

Table 15: Statistics of the EnglishSpanish parallel CommonCrawl corpus.
Corpus
CommonCrawl

Sent.
Eng
Spa

1.84 M

Words
46.54 M
50.33 M

Vocab.
750.01 K
775.75 K

avg. length
25.22
27.30

tions/omissions (17.63%), function words (13.04%) and, the least frequent, bad feedback
(5.31%). All the aspects are improved, except for additions/omissions and, bad feedback.
It is worth mentioning that the most frequent changes (lexical and reordering) are also
changes whose benefit doubles the cases of a quality decrease. Contrary to what could
be thought, the lexical correction addresses mistranslations in a greater deal than Out-ofVocabulary words (OOV), as OOVs were reduced only by 0.03% under the best performance
(cf. Table 14 for a detailed analysis).
5.2 Using a Web-Crawled Parallel Corpus
In this application scenario, we use the CommonCrawl corpus, a collection of parallel texts
automatically mined from the Web (Smith et al., 2013). This corpus offers two interesting
characteristics for our experiments: (i) its vocabulary and expressions go far beyond the
controlled scenario of EPPS acts and the formality of the News UN corpora and (ii) it is a
very noisy corpus. The large vocabulary size in Table 15 gives an intuition of the nature of its
content, with a high presence of noise and spurious words. In addition, its size allows to set
a trade-off between quantity (amount of new material selected) and quality (the threshold
of the selection algorithm). Moreover, the selection method from the FAUST corpus is also
applicable due to an analogy between scenarios: under CommonCrawl, we can compare the
automatic translation of the source sentences against the references automatically obtained
by the crawler and select the cases in which the latter is better. To perform the selection
we use the same classifiers from the FAUST scenario without retraining or adaptation 
as we want to study the generalization ability and no specific training material for the
CommonCrawl corpus is available for the selection task. This experiment represents a
double challenge: (i) determining if the presented proposal is also suitable for crawled
parallel corpora, and (ii) studying whether the trained selection models generalize well
across both corpora and domain. The CommonCrawl experimental setting can be seen as
an artificial post-editing scenario: the references represent the edits, which ideally should
provide more adequate translations.
In this experiment, we consider as baseline the best obtained system so far for translation
of news texts (cf. Section 4.3.3). We call this baseline Base News SimTer 0.6. This baseline
system might be considered as already strong, since it is 0.25 BLEU points better pure
baseline system (trained once with all the data). In order to assess the trade-off between
quantity and quality when using more parallel text, we enrich the baseline by adapting the
original models with different portions of the CommonCrawl corpus and filtered either with
the subsampling or FFF+ strategies. As in the FAUST scenario, we analyzed the translation
performance depending on several factors: (i) the ratio of CommonCrawl data selected,
(ii) the data selection strategy (FFF+ vs Subsampling), and (iii) the adaptation strategy.
Table 16 and Figure 6 show the evaluation results over the test12 and test13 datasets
from Section 4.3.1. The curves in Figure 6 show a consistent pattern with that observed
183

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

in Figure 4 for the FAUST scenario, reinforcing the evidences: both FFF+ selection and
SimTer 0.6 adaptation are important in order to obtain a final gain. Using CommonCrawl
without selection does not result in any performance gain, but worsens the results slightly.
Subsampling improves some metrics at 75% point but at the cost of worsening others.
Moreover, concatening the 25% best ranked CommonCrawl to the training data Concat.
FFF+(25%) provides a slight improvement. However, the improvements obtained from
Concat. FFF+(25%) and SimTer 0.6 Subsampling(75%) are not significant.
Table 16: Results obtained with the base and CommonCrawl-enriched SMT systems depending on different filtering and adaptation methods. For BLEU and NIST, ,
 and  indicate significant differences over the News SimTer 0.6 system (experiment baseline) with 0.99, 0.95 and 0.90 confidence levels, respectively. The
best results on each corpus are boldfaced.
Translation system
Adaptation
Filtering
test12
News SimTer 0.6 
SubSamp.
SimTer 0.6
FFF+

Concat.

FFF+

test13
News SimTer 0.6


SubSamp.

SimTer 0.6
FFF+

Concat.

FFF+

% of edits

BLEU

NIST

TER

METEOR

ULC

0%
+25%
+50%
+75%
+25%
+TH0-60%
+100%
+25%
+TH0-60%

33.25
33.21
33.38
33.47
33.73
33.74
33.19
33.41
33.20

8.70
8.67
8.68
8.70
8.78
8.75
8.68
8.71
8.68

48.24
48.38
48.25
48.27
47.84
47.87
48.46
48.09
48.18

55.84
55.48
55.54
55.81
56.21
56.05
55.50
55.88
55.72

71.66
71.86
72.04
72.32
72.52
72.42
71.20
72.44
72.15

0%
+25%
+50%
+75%
+25%
+TH0-60%
+100%
+25%
+TH0-60%

29.15
29.17
29.22
29.29
29.61
29.57
29.32
29.27
29.00

8.07
8.05
8.03
8.05
8.13
8.10
8.07
8.07
8.04

51.28
51.49
51.43
51.36
50.99
51.12
51.36
51.11
51.14

53.15
52.87
52.71
52.88
53.39
53.13
52.87
53.14
53.01

71.73
71.71
71.63
71.85
72.41
72.07
71.53
72.26
71.94

Results also show that selecting only the 25%-best CommonCrawl data produces the
best improvement. This is approximately +0.50 BLEU and 0.40 TER on the test sets. It
is important to recall that Base News SimTer 0.6 is the strong baseline. It is remarkable
that the same selection models trained on the data from the FAUST scenario generalize well
to the CommonCrawl domain adaptation scenario. The optimal 25% selection threshold
represents a much stricter selection than that required in the FAUST scenario (50%). However, while in FAUST we were selecting around 3,000 sentences among 6,000, in this case
we are selecting around 460 thousand sentences over 1.84 million. Hence, the final selection
threshold is a compromise between the aggressiveness of the method and the minimum
amount of new material necessary to cause a real impact. We also analyzed the effect of
184

fiLeveraging Online User Feedback to Improve SMT

8.9

NIST

8.74 8.75

8.74

8.7

48.46

TER

8.8

48.9

49.0

8.78

48.5

8.68

8.7

47.84

48.0
47.5

test_12

8.6
0

25
50
75
% of best CommonCrawl corpus used

47.9 47.87

100

test_12
0

8.2

48.04

25
50
75
% of best CommonCrawl corpus used

100

52.5
52.06

8.1

8.07

52.0
8.08

8.1

8.09

TER

NIST

8.13

51.5

8.07

50.99

51.19 51.12

51.29

51.36

51.0
8
0

25
50
75
% of best CommonCrawl corpus used

test_13

50.5

test_13
100

0

25
50
75
% of best CommonCrawl corpus used

100

Figure 6: NIST and TER scores on the test12 and test13 corpora as a function of the
percentage of best ranked CommonCrawl segments used.
selecting the 25% of the CommonCrawl data randomly (averaged over 10 times) or selecting the 25% of the data that our selection classifiers considered the worst. This analysis
allows to assess how good our selection algorithm is on ranking the parallel examples of
CommonCrawl corpus. Figure 7(a) shows the results, which indicate that the classifiers are
able to generate sensible rankings to detect both best and worst examples. This allows to
properly enrich the translation models, avoiding the negative effect of using the really bad
instances. The results obtained with 25% of the examples selected at random are below the
results obtained selecting the best 25% according to the classifier.
Lastly, we repeated the comparison of our alignment/adaptation strategy with the methods considered in the FAUST scenario experiment. Figure 7(b) shows the obtained results.
Similar to Figure 5, the differences are small in favor of SimTer. However, SimTer 60-40%
performs significantly better than the other alignment/adaptation strategies in both test
sets (p < 0.01).

185

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

test_12
test_13

Selection Method
52.7
52.1
51.5
50.9
50.4
49.8
49.2
48.6
48
47.4

TER

NIST

Selection Method
8.8
8.6
8.5
8.4
8.3
8.2
8.1
8
7.9
7.8
25%_best

baseline

all

8.78*
8.13*

8.7
8.07

8.68
8.07

25%_rand 25%_worst

8.65
8.03

8.44
7.83

test_12
test_13

25%_best

baseline

50.99
47.84

51.28
48.24

all

51.36
48.46

25%_rand 25%_worst

51.57
48.55

52.81
49.72

(a) Achieved by using different selection criteria: baseline (0%), all (100%), best 25%, random 25%
and worst 25%.
Alignment / Interpolation Method

TER

NIST

Alignment / Interpolation Method
8.8
8.7
8.6
8.5
8.4
8.3
8.2
8.1
8

51.3
50.9
50.5
50
49.6
49.1
48.7
48.2
47.8
47.4

SimTer 0.6 mGIZA++ PPL SimTer PPL mGIZA++ 0.6
test_12
test_13

8.78*
8.13*

8.72
8.08

8.72
8.07

8.74
8.08

SimTer 0.6 mGIZA++ PPL SimTer PPL mGIZA++ 0.6
test_12
test_13

50.99
47.84

51.18
48.07

51.19
48.08

51.26
48.08

(b) Achieved by applying different alignment/adaptation methods.

Figure 7: NIST and TER scores on the test12 and test13 corpora.  indicates significant
differences over the Baseline system with 0.99 confidence level.

6. Conclusions
In this article we proposed a new automatic strategy to incrementally train machine translation (MT) models with the edited translations coming from casual unreliable users. Our
strategy builds upon three main blocks, namely: (i) automatic identification of useful useredited instances (UE); (ii) alignment of the UEs with the source text, focusing on the
errors made by the original MT system; and (iii) incorporation of the new parallel segments through specific translation models trained with UEs. Our proposal is novel in the
application of some techniques from information retrieval, quality estimation and domain
adaptation to the problem of MT system enrichment. The datasets explored have also
interesting and challenging properties.
The selection of useful UEs is important to filter out noisy feedback from the users. We
accomplish this by training a classifier from supervised data using features derived from
similarity metrics used in information retrieval and MT quality estimation. Although the
186

fiLeveraging Online User Feedback to Improve SMT

classification results achieved are only moderate, classification scores allow to approximately
rank UEs by quality and tune different selection thresholds. Experiments show that this
is a useful strategy to select examples which, combined with the other two steps, yields
significant improvements over the original MT system.
Regarding the sourcetranslationUE alignment, we proposed SimTer, a simple incremental approach based on pivoting, which uses a TER alignment augmented with similarity
features. This approach has two advantages: it does not depend on previous softwarespecific alignment models (e.g., GIZA++, mGIZA++ or Berkeley Aligner) and the monolingual nature of the alignment implicitly allows the algorithm to focus on correcting translation errors, rather than achieving an optimal alignment between words. By experimenting
with two real datasets, we showed the positive contribution of SimTer in the MT enrichment pipeline. For our particular application, using SimTer is better than using existing
general-purpose aligners, such as mGIZA++, especially in the CommonCrawl scenario. The
experiments also confirmed the validity of the proposal, in terms of computational efficiency.
The third step deals with building UE-specific translation models using standard phrase
extraction and scoring tools. After experimentally analyzing different ways of combining the
UE-based and the original translation models, we concluded that a simple linear interpolation is a good and efficient strategy. By properly tuning the parameters, this combination
has a real impact in the final translation models, something that, for instance, perplexity
minimization is not able to achieve.
The complete architecture was thoroughly tested with real UEs collected from nonprofessional users through a commercial on-line translation portal (the so called FAUST
scenario). We experimented with different thresholds to select examples and alternative
ways to perform the alignment and the integration of the new aligned sentences. Results
showed that our approach significantly improves the translation quality of a basic, general
purpose SMT system, being generally superior to alternative methods. Apart from evaluating with several automatic quality measures, we also conducted a manual analysis in order
to verify the quality of improvements and to gain more insight on the cases in which the enriched MT system performs better or worse. The improvements did not come mainly from
a reduction of the out-of-vocabulary words, which are actually reduced only marginally.
The major improvements in translation quality came from a much better lexical selection,
reordering, and morphology. On the down side, the enriched system introduces from time
to time incorrect words or expressions learned from wrongly selected and aligned examples.
It also performed poorly in terms of adding and omitting spurious words, slightly worsening
quality over the baseline system.
The approach is general enough to be applied to different scenarios. We finally used
CommonCrawl, a collection of parallel texts automatically extracted from the Web, to
enrich a general purpose baseline SMT system. The same three steps were applied; a
selection was also necessary in this case because the corpus is very noisy, due to its automatic
extraction. Exactly the same classifiers trained with the FAUST corpus were used to identify
examples in which the automatically extracted target sentence is better than the automatic
translation of the source provided by the baseline translation system. The classifiers showed
robustness even when noisy references were used instead of UEs evincing their capacity to
deal with human or automatically-generated noise.The same conclusions can be drawn in
187

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

the adaptation experiment, which shows that our methodology works well across different
corpora sources and types of noise.

Acknowledgments
The major part of this work was carried out when the authors worked at TALP Research
Center - Universitat Politecnica de Catalunya. We would like to thank Nadir Durrani for
the proofreading of the paper and also the anonymous reviewers for their valuable feedback.
This work was partially funded by the Spanish Ministerio de Economa y Competitividad,
under contracts TEC2012-38939-C03-02 and TIN2012-38523-C02-02, as well as from the
European Regional Development Fund (ERDF/FEDER) and the European Communitys
FP7 (2007-2013) program under the following grants: 247762 (FAUST, FP7-ICT-2009-4247762) and 246016 (ERCIM Alain Bensoussan Fellowship).

References
Ambati, V., Vogel, S., & Carbonell, J. (2010). Active Learning and Crowd-Sourcing for
Machine Translation. In Proceedings of the LREC, pp. 21692174.
Axelrod, A., He, X., & Gao, J. (2011). Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural
Language Processing, pp. 355362, Edinburgh, Scotland, UK. Association for Computational Linguistics.
Barron-Cedeno, A., Marquez, L., Henrquez, Q. C. A., Formiga, L., Romero, E., & May,
J. (2013). Identifying useful human correction feedback from an on-line machine
translation service. In Proceedings of the Twenty-Third International Joint Conference
on Artificial Intelligence, IJCAI 13, pp. 20572063. AAAI Press.
Bertoldi, N., Cettolo, M., & Federico, M. (2013). Cache-based online adaptation for machine
translation enhanced computer assisted translation. In Proc. of MT Summit, pp. 35
42.
Bisazza, A., Ruiz, N., & Federico, M. (2011). Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of IWSLT, pp. 136143.
Blain, F., Schwenk, H., Senellart, J., & Systran, S. (2012). Incremental adaptation using
translation information and post-editing analysis. In Proceedings IWSLT 2012, pp.
229236.
Byrd, R. H., Lu, P., Nocedal, J., & Zhu, C. (1995). A Limited Memory Algorithm for Bound
Constrained Optimization. SIAM Journal on Scientific Computing, 16 (5), 11901208.
Callison-Burch, C., Bannard, C., & Schroeder, J. (2005). Scaling phrase-based statistical
machine translation to larger corpora and longer phrases. In Proceedings of the 43rd
Annual Meeting on Association for Computational Linguistics, ACL 05, pp. 255262,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the
188

fiLeveraging Online User Feedback to Improve SMT

Seventh Workshop on Statistical Machine Translation, pp. 1051, Montreal, Canada.
Association for Computational Linguistics.
Cappe, O., & Moulines, E. (2009). On-line expectation-maximization algorithm for latent
data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71 (3), 593613.
Cettolo, M., Federico, M., Servan, C., & Bertoldi, N. (2013). Issues in Incremental Adaptation of Statistical MT from Human Post-edits. In Proceedings of MT Summit XIV
Workshop on Post-editing Technology and Practice, pp. 111118.
Denkowski, M., & Lavie, A. (2011). Meteor 1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In Proceedings of the 6th Workshop
on Statistical Machine Translation, pp. 8591, Edinburgh, Scotland.
European Comission - 7th Framework Program (2010). Matecat, FAUST and Casmacat Projects. http://www.matecat.com http://www.faust-fp7.eu http://www.
casmacat.eu. Accessed: 2015-02-01.
FAUST (2013). Final report on the methods for the evaluation of translation requests, system outputs, and modelling of user feedback.
Tech. rep. D4.6,
FAUST  Feedback Analysis for User-Adaptive Statistical Translation. ftp://svrftp.eng.cam.ac.uk/pub/pub/faust-pub/Deliverables/FAUSTD4.6.pdf.
Formiga, L., Henrquez Q., C., Hernandez, A., Marino, J., Monte, E., & Fonollosa, J. (2012).
The talp-upc phrase-based translation systems for wmt12: Morphology simplification
and domain adaptation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pp. 275282, Montreal, Canada. Association for Computational
Linguistics.
Foster, G., Goutte, C., & Kuhn, R. (2010). Discriminative instance weighting for domain
adaptation in statistical machine translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing, pp. 451459, Cambridge, MA.
Association for Computational Linguistics.
Foster, G., & Kuhn, R. (2007). Mixture-Model Adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Translation, pp. 128135.
Gale, W., & Church, K. (1993). A Program for Aligning Sentences in Bilingual Corpora.
Computational Linguistics, 19, 75102.
Gao, Q., & Vogel, S. (2008). Parallel implementations of word alignment tool. In Software
Engineering, Testing, and Quality Assurance for Natural Language Processing, pp.
4957, Columbus, Ohio. Association for Computational Linguistics.
Gimenez, J., & Marquez, L. (2010). Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, pp. 7786.
Google Inc. (2015). Google Translate. http://translate.google.com. Accessed: 201502-01.
Haddow, B., & Germann, U. (2011).
Moses Incremental Training.
http://www.statmt.org/moses/?n=Advanced.Incremental. Accessed: 2015-08-11.
189

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Hardt, D., & Elming, J. (2010). Incremental re-training for post-editing smt. In In proc. of
AMTA 2010: the Ninth conference of the Association for Machine Translation in the
Americas, Denver, CO. USA.
Henriquez, C. (2014). Improving statistical machine translation through adaptation and
learning. Ph.D. thesis, Universitat Politecnica de Catalunya.
Henrquez, C., Marino, J., & Banchs, R. (2011). Deriving translation units using small
additional corpora. In Proceedings of the 15th Conference of the European Association
for Machine Translation, pp. 121128.
Hsu, C.-W., Chang, C.-C., & Lin, C.-J. (2003). A practical guide to support vector classification. Tech. rep., Department of Computer Science, National Taiwan University.
http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.
Joachims, T. (1999). Making large-scale support vector machine learning practical. In
Scholkopf, B., Burges, C. J. C., & Smola, A. J. (Eds.), Advances in Kernel Methods,
pp. 169184. MIT Press, Cambridge, MA, USA.
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. In
Machine Translation Summit X, pp. 7986, Phuket, Thailand.
Koehn, P., & Hoang, H. (2007). Factored Translation Models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 868876, Prague, Czech
Republic.
Lambert, P., de Gispert, A., Banchs, R., & Marino, J. B. (2005). Guidelines for word
alignment evaluation and manual alignment. Language Resources and Evaluation,
39 (4), 267285.
Levenberg, A., Callison-Burch, C., & Osborne, M. (2010). Stream-based translation models
for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational
Linguistics, pp. 394402, Los Angeles, California. Association for Computational Linguistics.
Lopez, A. (2008). Tera-scale translation models via pattern matching. In Proceedings of the
22Nd International Conference on Computational Linguistics - Volume 1, COLING
08, pp. 505512, Stroudsburg, PA, USA. Association for Computational Linguistics.
Martnez-Gomez, P., Sanchis-Trilles, G., & Casacuberta, F. (2012). Online adaptation
strategies for statistical machine translation in post-editing scenarios. Pattern Recognition, 45 (9), 3193  3203. Best Papers of Iberian Conference on Pattern Recognition
and Image Analysis (IbPRIA2011).
Matecat (2015). Matecat official repository. https://github.com/matecat/MateCat. Accessed: 2015-07-24.
Mathur, P., Mauro, C., & Federico, M. (2013). Online learning approaches in computer
assisted translation. In Proceedings of the Eighth Workshop on Statistical Machine
Translation, pp. 301308, Sofia, Bulgaria. Association for Computational Linguistics.
190

fiLeveraging Online User Feedback to Improve SMT

Mcnamee, P., & Mayfield, J. (2004). Character N-Gram Tokenization for European Language Text Retrieval. Information Retrieval, 7 (1-2), 7397.
Microsoft Inc. (2015). Bing Translator. http://www.bing.com/translator. Accessed:
2015-02-01.
Neal, R. M., & Hinton, G. E. (1998). A view of the em algorithm that justifies incremental,
sparse, and other variants. In Learning in graphical models, pp. 355368. Springer.
Nelder, J. A., & Mead, R. (1965). A Simplex Method for Function Minimization. The
Computer Journal, 7, 308313.
NIST (2002). Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. Tech. rep., National Institute of Standards and Technology.
http://www.itl.nist.gov/iad/mig/tests/mt/doc/ngram-study.pdf.
Och, F. J., & Ney, H. (2003). A Systematic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29, 1951.
Ortiz-Martnez, D., Garca-Varea, I., & Casacuberta, F. (2010). Online learning for interactive statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational
Linguistics, pp. 546554, Los Angeles, California. Association for Computational Linguistics. http://www.aclweb.org/anthology/N10-1079.
Ortiz-Martnez, D., Sanchis-Trilles, G., Gonzalez-Rubio, J., & Casacuberta, F. (2013).
Progress report on adaptive translation models. Tech. rep. D4.2, Casmacat: Cognitive Analysis and Statistical Methods for Advanced Computer Aided Translation.
Padro, L., Collado, M., Reese, S., Lloberes, M., & Castellon, I. (2010). FreeLing 2.1: Five
Years of Open-Source Language Processing Tools. In Proceedings of the 7th Language
Resources and Evaluation Conference (LREC 2010), La Valletta, MALTA.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Pighin, D., Marquez, L., & May, J. (2012). An Analysis (and an Annotated Corpus) of User
Responses to Machine Translation Output. In Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC12), Istanbul, Turkey.
Potet, M., Esperanca-Rodier, E., Blanchon, H., & Besacier, L. (2011). Preliminary experiments on using users post-editions to enhance a smt system. In Proceedings of the
15th Conference of the European Association for Machine Translation, pp. 161168.
Pouliquen, B., Steinberger, R., & Ignat, C. (2003). Automatic Identification of Document
Translations in Large Multilingual Document Collections. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP2003), pp. 401408, Borovets, Bulgaria.
Reverso-Softissimo (2015). Reverso  Free online translator and dictionary based on SDL
technology. http://www.reverso.net. Accessed: 2015-02-01.
Riezler, S., & Maxwell, J. (2005). On some pitfalls in automatic evaluation and significance testing for MT. In ACL-05 Workshop on Intrinsic and Extrinsic Evaluation
191

fiFormiga, Barron-Cedeno, Marquez, Henrquez & Marino

Measures for Machine Tranlsation and/or Summarization (MTSE05) at the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan,
USA.
Schwenk, H., & Koehn, P. (2008). Large and diverse language models for statistical machine
translation.. In Proceedings of IJCNLP, pp. 661666, Hyderabad, India.
Sennrich, R. (2012). Mixture-Modeling with Unsupervised Clusters for Domain Adaptation
in Statistical Machine Translation. In Proceedings of the 16th EAMT Conference.
Simard, M., Foster, G. F., & Isabelle, P. (1992). Using cognates to align sentences in bilingual corpora. In in Proceedings of the Fourth International Conference on Theoretical
and Methodological Issues in Machine Translation, pp. 6781.
Simard, M., Goutte, C., & Isabelle, P. (2007). Statistical phrase-based post-editing. In
Human Language Technologies 2007: The Conference of the North American Chapter
of the Association for Computational Linguistics; Proceedings of the Main Conference,
pp. 508515, Rochester, New York. Association for Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez, A.
(2013). Dirt cheap web-scale parallel text from the Common Crawl. In Proceedings of
the 2013 Conference of the Association for Computational Linguistics (ACL 2013),
Sofia, Bulgaria. Association for Computational Linguistics.
Snover, M., Madnani, N., Dorr, B., & Schwartz, R. (2009). TER-Plus: Paraphrase, Semantic,
and Alignment Enhancements to Translation Edit Rate. Machine Translation, 23 (2),
117127.
Witten, I., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and Techniques (2 edition). Morgan Kaufmann, San Francisco, CA.

192

fiJournal of Artificial Intelligence Research 54 (2015) 437-469

Submitted 11/14; published 11/15

Constraining Information Sharing to Improve
Cooperative Information Gathering
Igor Rochlin

IGOR . ROCHLIN @ GMAIL . COM

School of Computer Science,
College of Management, Rishon LeZion, Israel.

David Sarne

DAVID . SARNE @ GMAIL . COM

Department of Computer Science,
Bar-Ilan University, Ramat-Gan, Israel.

Abstract
This paper considers the problem of cooperation between self-interested agents in acquiring
better information regarding the nature of the different options and opportunities available to them.
By sharing individual findings with others, the agents can potentially achieve a substantial improvement in overall and individual expected benefits. Unfortunately, it is well known that with
self-interested agents equilibrium considerations often dictate solutions that are far from the fully
cooperative ones, hence the agents do not manage to fully exploit the potential benefits encapsulated in such cooperation. In this paper we introduce, analyze and demonstrate the benefit of five
methods aiming to improve cooperative information gathering. Common to all five that they constrain and limit the information sharing process. Nevertheless, the decrease in benefit due to the
limited sharing is outweighed by the resulting substantial improvement in the equilibrium individual information gathering strategies. The equilibrium analysis given in the paper, which, in itself
is an important contribution to the study of cooperation between self-interested agents, enables
demonstrating that for a wide range of settings an improved individual expected benefit is achieved
for all agents when applying each of the five methods.

1. Introduction
In many settings agents can benefit from cooperating in information gathering (Rochlin, Aumann,
Sarne, & Golosman, 2014; Kephart & Greenwald, 2002; Rochlin, Sarne, & Mash, 2014; Hazon,
Aumann, Kraus, & Sarne, 2013). For example, consider two travel agents, from the same city,
that plan to participate in an international tourism conference, taking place in a highly traveled
destination. There are many airlines offering flights to nearby destinations, each setting a price
according to various external factors such as seat availability and agreements it has with its airlines
partners. Similarly, depending on the airport of arrival, one can get to the conference by train, bus,
ferry, taxi or any combination of these for different segments of the trip. Each of these means of
transportation may be characterized by a different availability and fare, depending, for example, on
the time of the day when it is required. Checking the feasibility and cost of the different alternatives
for traveling to the conference, thus, potentially involves several time consuming activities, such
as checking locations on the map and checking the companies web-sites for routes, timetables,
fares and availability, and thus incurs some opportunity cost. Since both agents can benefit from
the information each of them gathers regarding the different options for getting to the conference,
they have a strong incentive to share their findings, i.e., execute the information gathering process
(hereafter denoted IGP) cooperatively.
c
2015
AI Access Foundation. All rights reserved.

fiROCHLIN & S ARNE

Cooperative information gathering is used in many real-life applications of different domains.
For example, consider two friends, both interested in buying a big TV screen. The friends can visit
the shopping mall, together, while each of them checks offers in different stores, and eventually
they meet and share their findings. Alternatively, consider an oil drilling company sending multiple
agents to explore possible drilling sites, in order to develop the best site discovered. Similarly
when a position needs to be filled, HR personnel can interview candidates in parallel and recruit the
best candidate found. Students can jointly look for references for an assignment they receive and
eventually use the best source found by any of them.
The benefits of multi-agent cooperative information gathering are twofold. First, since each
alternative (hereafter termed opportunity) reviewed can benefit many agents, the relative cost
of information gathering is reduced, while the overall welfare increases. Secondly, the task can
potentially be divided according to the expertise of the different agents, if such expertise exists.1
Cooperative information gathering can be seen as a type of a public goods game, where all
agents contribute through their individual IGP and the collective result influences the welfare of all
of them. Like in public goods games, the costs of IGP in cooperative information gathering, are
basically born by individual agents although benefits (better information in our case) are societal.
In public goods games, in general, inefficiencies in private giving commonly occur whenever the
agents are self-interested (de Jong, Tuyls, & Verbeeck, 2008; de Jong & Tuyls, 2011). Similarly,
we have shown in prior work that cooperative information gathering, carried out by self-interested
agents, does not result in the amount of cooperation as in the optimal fully cooperative case (Rochlin
et al., 2014). In particular we have demonstrated that methods and instruments (termed enhancers)
that are easily proved to be beneficial in the fully cooperative case, can actually have a negative
impact, both on individual and overall performance, in the self-interested case. These enhancers
included an increase in the size of the group of agents that gather information jointly, an increase
in the number of opportunities each agent has time to potentially gather information on, an improvement in some of the agents information gathering competence, an increase in the level of
heterogeneity in the individual information gathering competence of the group members and the
ability to communicate throughout the process. Alas, our prior work was mostly descriptive in the
sense that it outlined the potential problems that may arise in cooperative information gathering of
self-interested agent. The research reported in the current paper aims to provide solutions to these
problems, in the form of five somehow non-intuitive methods that essentially constrain and limit the
ISP such that the individual benefit of all participating agents substantially increase.
The five cooperative information gathering methods reported in this paper differ in the constraints they put on the information sharing process (henceforth denoted ISP). The first, denoted
Enforced probabilistic information sharing prevents individual agents from taking part in the ISP
according to some probabilistic function. The second, denoted Threshold restricted information
sharing, prevents all agents that have found highly favorable values along their individual IGP
from taking part in the ISP. The third, denoted Cost filtered information sharing, introduces some
cost for taking part in the ISP (where the proceeds are wasted and are not returned to the agents) and
allows agents to choose whether to take part in the ISP or not. The fourth, denoted Random finding
sharing, allows all agents to take part in the ISP, however restricts each of them to disclosing only
one of the values from the set known to each, in random. Finally, the fifth, denoted Subgroup restricted information sharing, initially divides the agents into subgroups and allows only local ISPs,
1. For buyers cooperation, the agents can also benefit from a volume discount through their cooperation; however this
property holds only for that specific domain.

438

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

i.e., information sharing in the subgroup level. Each of these methods may seem counter intuitive,
because the absence of some of the agents in the ISP or the restriction on the amount of information
that can be shared could be harmful to all agents. Yet, in many settings, the use of these methods can
be highly beneficial. This is because of the paradox embedded in the ISP option - while the sharing
of information benefits all agents, the very fact that all information gathered is going to be shared,
discourages agents from investing much resources in their individual IGP (Rochlin et al., 2014).
Therefore, with the use of these methods the individual benefit of each agent from taking part in
the ISP decreases, however the IGP carried out by the agent individually becomes more efficient.
Therefore, by intelligently managing the tradeoff between the two, a more beneficial equilibrium
can be achieved, which improves both the overall and individual benefits.
The paper provides a comprehensive analysis of the individual information gathering strategies
used by the agents, given the strategy of others, under the different methods. For the Enforced probabilistic information sharing, Random finding sharing and Subgroup information sharing methods
the agents individual strategy is proven to be similar in structure to the one used with the standard
cooperative information gathering method  the agent will resume information gathering as long
as the best value obtained so far is lower than some reservation value (a threshold), regardless of
how much more information can potentially be gathered. For the Threshold restricted information
sharing method the agents individual strategy is proven to be threshold-based, however the threshold changes as a function of the amount of information that potentially can still be gathered. For
the Cost filtered information sharing method, the individual strategies are proven to be based on a
single reservation value for determining the benefit in additional information gathering and a set of
intervals for deciding whether to take part in the ISP. These allow the characterization of the resulting equilibria. Using synthetic environments, we numerically demonstrate that all five methods
result in substantial improvement to each of the agents individual expected benefit for a wide range
of settings.
The results contribute to the advancement of theories of cooperation in MAS. As discussed later
in the paper, the methods can be easily applied and their use can benefit both individuals planning
to engage in cooperative information gathering and designers of multi-agent systems (MAS) where
cooperative information gathering is likely to take place.
In the following section we formally introduce the cooperative information gathering model. In
Section 3 we detail the model analysis, the equilibrium strategies for the different model variants
considered and supply numerical examples for the benefit that can be achieved when using them.
Related work is reviewed in Section 4, emphasizing the uniqueness of the analysis provided in the
paper. Finally, we conclude and discuss directions for future research in Section 5.

2. The Model
The model considers a set K = {A1 , ..., Ak } of fully-rational self-interested agents. Each of the
agents needs to gather information pertaining to the value (e.g., benefit) of different opportunities to
which it has access and eventually choose one. The values of the different opportunities are a priori
unknown and information is gathered for one opportunity at a time. The individual information
gathering problem, as defined above, is standard and follows the assumptions commonly used in
literature (Chhabra & Das, 2011; Kephart & Greenwald, 2002; Hazon et al., 2013; Rothschild, 1974;
McMillan & Rothschild, 1994; Morgan & Manning, 1985). The uncertainty associated with the
value of opportunities available to any agent Ai is modeled, as in most costly information gathering
439

fiROCHLIN & S ARNE

literature (McMillan & Rothschild, 1994; Burdett & Malueg, 1981; Carlson & McAfee, 1984;
Lippman & McCall, 1976; Morgan, 1983), through a probability distribution function (p.d.f.) fi (x)
(i.e., the value of each opportunity in the individual IGP of Ai is drawn from fi (x)), with which all
agents are familiar (Tang, Smith, & Montgomery, 2010; Waldeck, 2008; Janssen, Moraga-Gonzalez,
& Wildenbeest, 2005). Due to the resource consuming nature of the process it is considered costly
in the sense that revealing the value of an opportunity incurs a fixed cost, denoted ci . The model
assumes that any agent Ai is constrained by a number of opportunities accessible to this agent,
denoted ni . The cost ci , the distribution fi (x) and the number of opportunities ni , are defined in
the agents level to support settings where different agents have different skills and capabilities.
The agent thus needs to gather information, i.e., explore the value of some of the opportunities
and eventually pick one of the values revealed (i.e., recall is permitted) (Carlson & McAfee, 1984;
McMillan & Rothschild, 1994).
In settings where all opportunities are applicable to all agents the agents have an incentive to
cooperate in information gathering in the sense that all individual findings are eventually shared with
all others. While there are many ways to share the information, the focus of this paper is on setups
where the ISP takes place at some pre-specified time, after all agents have completed their individual
IGPs and each needs to decide on the opportunity it will choose. The choice of sharing findings at
the end of the individual IGPs is mostly natural and customary in real life. More importantly, the
alternative of sharing information throughout the process has a major setback in the sense that each
individual agent finds information sharing to be beneficial only when it is on the receiving end,
i.e., it is the one being informed that a favorable opportunity has been found; when it is on the
reporting end, the agent loses from such communication since the report can potentially encourage
the other agents to terminate their individual information gathering. On the other hand sharing the
information after concluding the individual IGPs is always beneficial for the agent as it gains more
information, and at the same time the information it discloses does not affect the behavior of others
thereafter since they also have already concluded their IGPs.
As in prior models of cooperative information gathering, we also assume that: (a) the agents are
truthful in the sense that they always report the true values they obtain;2 (b) each agent Ai has some
fall-back value v0i , i.e., even if not becoming acquainted with any opportunity values (in case of
not gathering any information individually and not receiving any of the others findings) the agent
can presumably benefit v0i ;3 and (c) either the opportunities each agent can check are unique or the
agents can a priori divide the opportunities among them such that each will be assigned a different
set. It is assumed that information gathering costs and opportunity values are additive and each agent
Ai is interested in maximizing its expected benefit, denoted EBi . The benefit of an agent is therefore
the best value obtained by the group minus the costs accumulated individually along the agents
individual IGP. Finally, it is assumed that when engaging in a cooperative information gathering
process all agents are a priori acquainted with the probability distribution functions and information
gathering costs of all agents, i.e., the only possible difference in the information available to the
different agents throughout the cooperative IGP is their own findings and the findings of others.
The cooperative information gathering model as detailed above can be found in full or with
some variations in prior literature (Hazon et al., 2013; Rochlin et al., 2014; Gatti, 1999; Carlson &
2. The truthfulness assumption is commonly justified by a substantial potential reputation loss, and is easily enforceable
using fines.
3. And similarly, if taking part in the ISP then necessarily disclosing that value in the absence of any better one.

440

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Application

Individual goal

Opportunity

Value

Product acquisition

Minimize individual expense

Complex service/product

Cost of purchase

Choosing
an
oildrilling
site

Maximize
oil revenues
minus cost of
exploratory
drills
Minimize the
cost of production and R&D
expenses
Maximize individual utility
(grade minus
individual
effort)

Potential
drilling sites

Amount of
oil found

Production
technology

Cost of production with
a
specific
technology
Expected
grade if this
source
is
used

R&D

Information
Search
(students
assignment)

Information
source (e.g.,
online, textbook, library
resource)

Information gathering cost
Time spent finding
options and evaluating them
Time and resources
spent in the exploratory drills

Source of uncertainty

R&D cost of specific technology

Uncertainty concerning implementation aspects of a
desired technology

Time spent evaluating
different
sources

Differences in coverage of
the topic, relevance, accuracy, level of details

Sellers competition, seasonal effects, service constraints
Uncertainty regarding the
amount of oil in each
drilling site

Table 1: The mapping of different applications to the cooperative information gathering problem.
McAfee, 1984).4 Taking the travel agents example, given in the previous section, the opportunities
represent different alternatives for reaching the conference location and their value is their total cost.
The information gathering cost is the agents cost of the time needed to explore the alternatives. The
goal of each agent is to minimize her expected expense, defined as the cost of the best alternative
found by the two plus the cost of the time spent individually to review the different alternatives.
Similarly, the model can be mapped to all the other applications mentioned in the introduction (e.g.,
see Table 1).

3. Analysis
We divide the analysis according to the five cooperative information gathering enhancing methods.
For each method, first we determine the individual optimal information gathering strategy of an
agent taking part in the process, as the best response to the other agents strategies. Then, we show
how the collective behavior is derived and extract the equilibrium set of strategies.
The appropriate equilibrium concept depends on how we define the type space. If we were to
define an agents type by the specific vector of values it would encounter if fully exhausting its
IGP, then the appropriate concept would be the ex-ante Bayesian Nash equilibrium, since agents
are a priori unaware of their types and this information is revealed to them (individually) along
the IGP. However, we prove in this paper that in this definition, an agents type would not affect
its strategy (strategies turn out to be based on thresholds that are set prior to conducting the IGP).
Therefore, while outcomes are stochastic, one could in theory build a direct stochastic mapping
from individual strategies to the global outcome and therefore there is no need to go through the
type in doing so. Therefore there is no added value here from the Bayesian Nash equilibrium
characterization. The equilibrium concept would then be subgame-perfection or Stackelberg (where
the system designer sets the rules for participation in the ISP, and then agents respond by choosing
their optimal strategies). All these result in a similar equilibrium characterization in our case.
4. While some model variants consider the task to be executed by a representative agent, acting on behalf of the group,
the essence of gathering costly information and trading-off costs and benefit is the same.

441

fiROCHLIN & S ARNE

Finally, we demonstrate how the expected individual benefit of all agents improves, when the
method is used, compared to the standard cooperative IGP. In order to illustrate the performance
achieved with the different methods, we use a setting where the agents are homogeneous in terms of
their information gathering environment. Meaning that each agent samples opportunities from the
same probability distribution function f (y) (i.e., f1 (y) = .. = fk (y) = f (y)), that all agents are
constrained to the same number of opportunities n they can sample overall (i.e., n1 = .. = nk = n)
and they all share the same information gathering cost c (i.e., c1 = .. = ck = c) and same fallback
v0 (i.e., v01 = .. = v0k = v0 ). Such setting is quite common in real-life as often (and especially in the
Internet age) people can potentially access the same opportunities and with a similar effort (e.g., time
spent navigating through a web-site). For example, in our travel agents running example, it is likely
that all agents have access to the same web-sites and resources that need to be used for identifying
and gathering information on the different options available for getting to the conference, and that
none of them, being a trained and experienced travel agent, has some specific advantage in doing
so. For simplicity and ease of exposition of the figures, we use f (y) to be the uniform distribution
function (between 0 and 1). We stress that even though such a homogeneous setting is standard in
costly information gathering literature (McMillan & Rothschild, 1994; Lippman & McCall, 1976),
and common in real-life ISP as argued above, its use in our case is merely for illustration purposes
and all the results concerning individual strategies and equilibrium structures that are given in this
paper are based on formal theoretical proofs.
3.1 Enforced Probabilistic Information Sharing
In this method each agent Ai is a priori assigned some probability PiIS which, after it has completed
its individual IGP, is used to determine whether it is allowed to take part in the ISP. It is assumed that
all agents are a priori acquainted with the probabilities PiIS used for enabling information sharing of
each agent. The determination whether an agent will take part in the ISP must be made in proximity
to the time information should actually be shared and requires some enforcing mechanism since
once the individual IGP is completed, agents obviously will benefit from taking part in the ISP,
as it does not incur any cost and at the same time can improve their best finding. Furthermore,
since the process takes place after all individual IGPs have been terminated, the information an
agent discloses has no influence over the individual IGP strategies used by the other agents. Such
enforcement is easy to achieve through simple means. For example, in our travel agents running
example this is equivalent to having all agents send their findings to a designated secured server. The
server will select those eligible for information sharing, according to the pre-defined probabilities,
and will remove from its database all the information coming from those that are not. Then, the
server will allow those eligible for information sharing access to the data it stores.
An agents state throughout its individual IGP is represented by the subset of opportunities on
which it has already gathered information, and their associated values, and consequently the remaining opportunities for which the values are still unknown. An agents strategy is thus the mapping
from a world state to a choice {resume, terminate} where resume suggests that the agent needs
to gather information about an additional opportunity (a random one, since all opportunities available to a given agent are a priori alike) and terminate means that the agent needs to proceed to the
ISP. Theorem 1 proves that the state representation in this case can be compacted to the best value
found so far (including the fallback v0i ), v, and that the optimal strategy can be represented in terms
of a single reservation value, independent of the number of remaining opportunities.

442

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Theorem 1. Given the probability distribution function of the maximal value obtained by all other
agents that take part in the ISP, denoted fi (x), agent Ai s optimal individual information gathering
strategy is to set a reservation value ri  v0i , where ri is the solution to:5
 
 
ci = PiIS 
fi (y)
(max(y, x)  max(ri , x))fi (x)dxdy
(1)
y=ri
x=
 
IS
+ (1  Pi ) 
(y  ri )fi (y)dy
y=ri

The agent should always choose to gather information on an additional opportunity (if one is available) if the best value obtained so far is below ri and otherwise it should proceed to ISP.
Proof. See appendix A.
Theorem 1 specifies the optimal strategy of an agent given the strategy of others. The solution
of a set of k equations similar to (1), one for each agent Ai , will provide a set of pure equilibria of
the form {ri |1  i  k} if any exist. A mixed equilibrium in our case is defined by a probability
pi (v, j) assigned to each state (v, j), defining whether the agent will resume or terminate information gathering in that state. This may seem infeasible to extract, based on the infinite number of
states (as value distributions are continuous). Nevertheless, in order for such a solution to hold, the
agents expected benefit from both actions (resume and terminate information gathering when in
that state) must be equal. Based on the optimality of the reservation-value rule, this can hold only
for states where the value v equals ri as calculated according to (1). However, due to the continuous
nature of v, the probability of actually reaching states that satisfy the above condition is zero, thus
assigning such probabilities will have no effect on the other agents. The only exception for the
above is the agents strategy at the beginning of its individual IGP. Here, the state is a priori known
to be (v0i , 0) hence adding some probability for actually gathering information on one opportunity
and then continuing according to ri (or otherwise going straight to ISP due to the indifference to resuming or terminating information gathering) will have an actual effect on the others. Consequently,
a mixed equilibrium for our problem is of the form:
{(pi , ri )|1  i  k}
where pi is the probability that agent Ai will initiate its individual IGP (0  pi  1) and ri is the
reservation value to be used by the agent. A solution will be considered stable (i.e., in equilibrium),
if none of the agents will find it beneficial to deviate from it individually. An equilibrium in pure
strategies for the problem would require pi  {0, 1} i. Any other solution is a mixed equilibrium
strategy.
Now that the individual strategy in equilibrium has been defined in its complete form (i.e.,
including the probabilistic aspect), we can formulate fi (x) (the probability distribution function of
the maximal value obtained along the IGP of all other agents that will take part in the ISP). For
this purpose we make use of the probability that the maximum value that will be found by all the
agents that take part in the ISP, except Ai , will be smaller than or equal to x, denoted Fi (x). The
5. Notice that if the equation has a solution then it is necessarily of the form v0i  ri . In case there is no solution to the
equation (e.g., in case of a substantial ci ), onwards, then we can simply set ri = v0 and no information gathering
will take place.

443

fiROCHLIN & S ARNE

calculation of Fi (x) makes use of the probability that the maximum value obtained along the IGP of
an agent Aj (that chooses to engage in IGP and uses rj ), is less than x, denoted Fjreturn (x), calculated
according to:6




0
nj
return
F
(x)
Fj
(x) =
j

 F (r )nj +
j j

x < v0j
v0j  x  rj
nj
1Fj (rj )
1Fj (rj ) (Fj (x)  Fj (rj )) x > rj

(2)

The case where x < v0j is trivial, as v0j is a lower bound for the best value the agent ends up with.
For the case where v0j  x  rj , the value of all nj opportunities must result in a value below
x. When x > rj there are two possible scenarios. The first is where all nj opportunities result
in a value below the reservation value rj , i.e., with a Fj (rj )nj probability. The second, is where
the information gathering terminates right after revealing value y at the lth opportunity such that
rj < y < x (otherwise, if y < rj the information gathering should resume) and all the former l  1
values obtained are smaller than rj (otherwise the lth opportunity is not reached). The probability of
the latter case occurring (summing over all values of l  nj ) can be calculated using the geometric
nj
1F (r )nj
series l=1
(Fj (x)  Fj (rj ))Fj (rj )l1 = 1Fj j (rj j ) (Fj (x)  Fj (rj )).
The probability distribution function of the maximum value obtained throughout agent Ai s IGP,
denoted fireturn (x), is by definition, the first derivative of Fireturn (x):
fireturn (x) =

d(Fireturn (x))
dx

Thus, we can now formulate the probability that the maximum value that will be found by all
the agents taking part in ISP, except Ai , will be smaller than or equal to x, Fi (x):
Fi (x) =



(PjIS (pj Fjreturn (x) + (1  pj )) + (1  PjIS ))

(3)

Aj Kj=i

The calculation is based on the probability that any given agent will either: (a) not take part in the
ISP (hence it will not contribute any value) or, (b) take part in the ISP and its best value obtained in
its individual IGP will be below x. The probability of (a) is (1  PiIS ). To calculate the probability
of (b) we first need to calculate the probability that an agents best value will be below x. This
can happen either if the agent initiated IGP and the best value it will obtain will be lower than (or
equal to) x, i.e., with a probability of pj Fjreturn (x), or if the agent opted out from IGP, i.e., with
a probability of 1  pj . Therefore the probability of b is given by PjIS (pj Fjreturn (x) + (1  pj )).
Consequently, the probability distribution function fi (x) is the derivative of Fi (x):
dFi (x)
fi (x) =
dx
These enable us to calculate the expected benefit of agent Ai when the other agents use the set
of strategies {(pi , ri ) |1  i  k}. If agent Ai chooses to engage in IGP then its expected benefit,
6. The maximum value found includes also the fallback v0j . In the degenerate case where Fj (v0i ) = 0 we use
Fjreturn (x) = 0 for v0i  x  rj .

444

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

denoted EBi (IGP ), is given by:7
 
 
EBi (IGP ) = PiIS 
fireturn (y)
max(v0i , y, x)fi (x)dxdy+
y=
x=
 
1  Fi (ri )ni
(1  PiIS ) 
max(v0i , y)fireturn (y)dy  ci
1  Fi (ri )
y=

(4)

where the first term on the right hand side is the expected maximum between the best value found
by the agent itself (i.e., associated with a distribution fireturn (y)) and the best value returned by the
other agents (associated with a distribution fi (x)) if agent Ai participates in the ISP (i.e., with a PiIS
probability). The second term is the expected best (i.e., maximum) opportunity-value found by
the agent along its information gathering if the agent is not allowed to take part in the ISP (i.e., with
a 1  PiIS probability). The last term is the expected cost incurred throughout the IGP carried out
n
 i
i (ri ) i
by Ai , calculated as: ci nj=1
(Fi (ri ))j1 = ci 1F
1Fi (ri ) , as the number of opportunities on which
information is gathered is a geometric random variable bounded by ni , with a 1  Fi (ri ) success
probability  the IGP terminates only upon receiving a value greater than ri (or all ni opportunities
explored) and the probability for a value greater than ri is 1  Fi (ri ).
When the agent opts not to execute individual IGP at all, its expected benefit, denoted EBi (
IGP ), is simply the expected value of the maximum value returned by the other agents, taking
part in the ISP, if it takes part by itself in the process, or otherwise v0i , i.e.:
 
IS
EBi (IGP ) = Pi 
max(v0i , x)fi (x)dx + (1  PiIS )v0i
(5)
x=

At this point, we have everything that is needed to formulate the equilibrium stability conditions.
A set of strategies {(pi , ri )|1  i  k} will be in equilibrium only if the following conditions hold:
(a) For every agent Ai for which pi = 0, EBi (IGP )  EBi ( IGP ).
(b) For every agent Ai for which pi = 1, EBi (IGP )  EBi ( IGP ).
(c) For every agent Ai for which 0 < pi < 1, EBi (IGP ) = EBi (IGP ).
Therefore, in order to find the equilibrium, the stability of 3k possible solutions of type {(pi , ri )
|1  i  k} differing in the value each pi obtains (pi = 0, pi = 1 and 0 < pi < 1) needs to be
checked. For every combination, the reservation values of the different agents and the probability
pi of each agent that uses a non-pure mixed strategy (i.e., with 0 < pi < 1) should be calculated by
solving a set of equations of type (1) (one for each agent characterized by pi = 0) and EBi (IGP ) =
EBi (IGP ) (one for every agent Ai for which 0 < pi < 1). Once the appropriate reservation
values and probabilities are obtained for a given set, the stability conditions need to be validated.
We note that there is no guarantee that an equilibrium will actually exist (either pure or mixed,
since there are an infinite number of strategies). Also, there is no guarantee that if one exists there
will be no other equilibria (i.e., multiple equilibria may exist). In the latter case, if there is one
7. If none of the other agents engage in IGP (i.e., pj = 0 for all Aj = Ai ) then fi (x) = 0 and therefore the expected

n
i (ri ) i
benefit EBi (IGP ) should be calculated as EBi (IGP ) = y= max(v0i , y)fireturn (y)dy  ci 1F
, where
1Fi (ri )

ri is the solution to ci = y=r (y  ri )fi (y)dy (according to a single agents optimal IGP (Rochlin et al., 2014)),
i
and EBi (IGP ) = v0i .

445

fiROCHLIN & S ARNE

Figure 1: Enforced probabilistic information sharing - effect of P IS on the individual expected
benefit, for different: (a) numbers of agents, k, in a setting: c = 0.35 and n = 5; and (b)
information gathering costs, c in a setting: k = 15 and n = 4.

equilibrium that dominates the others in terms of the individual expected benefit each and every
agent obtains then it will likely be the one used. Otherwise, there is no way of deciding which of
the equilibria is the one to use, and this question is not included in the scope of the current paper.
We emphasize that the above analysis generalizes the analysis of the standard cooperative information gathering model (Hazon et al., 2013; Rochlin et al., 2014) in the sense that the latter is a
specific case of the first, where the probability each agent will be allowed to take part in the ISP is
one (i.e., PiIS = 1 1  i  k). Furthermore, when the probability each agent will be allowed to
take part in the ISP is zero, the solution obtained is the same as the one known for the single-agent
information gathering problem (McMillan & Rothschild, 1994) (since each agent relies solely on
the values it obtains throughout its individual IGP).
Figure 1 depicts the agents individual expected benefit as a function of the probability PiIS
used, for different group sizes (k) and information gathering costs (c). The setting used is the
homogeneous setting described at the beginning of the section and the value of PiIS is the same for
all agents (i.e., PiIS = P IS i). The other model parameters were set to: c = 0.35 and n = 5
(Figure 1(a)) and k = 15 and n = 4 (Figure 1(b)). As depicted in the figure, the maximum expected
benefit (agent-wise, as all agents are alike in this case) is obtained when the participation of the
agents in the ISP is not certain but rather determined probabilistically (i.e., 0 < P IS < 1). The
typical pattern exhibited in the figure is an increase and then a decrease in the expected individual
benefit as the probability P IS increases. This is explained as follows. When P IS = 0 each agent
actually executes an individual IGP without any information sharing with others. As P IS increases,
the agent relies more on other agents findings. Thus, pi and ri become lower, which is bad for
the group since everybody gains less from the participation of the agent in the ISP. However, at
the same time the probability the agent will actually take part in the ISP increases, thus, overall,
the individual expected benefit increases. Nevertheless, for some value of P IS , the loss due to the
resulting decrease in pi and ri becomes more dominant than the benefit due to the increase in the
value of P IS .
446

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Another interesting behavior observed in Figure 1 is that the increase in information gathering
costs, c, and the increase in the number of agents, k, results in a decrease in the the value of P IS
that maximizes expected benefit. This may seem non-intuitive since the greater the information
gathering cost the greater the potential benefit that can be achieved by information sharing and
similarly, the greater the number of agents the greater the chances of obtaining favorable values in
the ISP. Therefore further limiting information sharing in settings with high k and c values may
seem unnatural. The phenomena is explained by the fact that the positive effect of the increase
in P IS over the participation probabilities pi and the reservation value ri used by each agent in
equilibrium in this case, which are substantially poor to begin with, is greater than the loss due to
the uncertain information sharing.
While the above method requires that an agent either completely avoid or fully participate in
the ISP, many other variants can be considered, e.g., partially limiting information sharing. For
example, an agent can be requested, with some probability, to contribute the information it has
gathered, without receiving the other group members information. Alternatively, an agent can be
allowed to receive information however not share its own gathered data. These variants can, in some
settings, result in substantially superior performance as we illustrate in Appendix B.
3.2 Threshold Restricted Information Sharing
With this method the agents that have found highly favorable values along their individual IGP are
prevented from taking part in the ISP. This is implemented by setting a threshold ViIS , for each
agent Ai , which requires that the agent opt-out of the ISP if it obtained a value greater than ViIS
in its individual IGP. It is assumed that all agents are a priori acquainted with the thresholds of all
agents. In our travel agents running example this can take the same form as with the Enforced probabilistic information sharing, with slight modifications, e.g., having all agents send their findings to
a designated secured server. The server will select those eligible for information sharing, according
to the pre-set thresholds, and will remove from its database all the information coming from those
that are not. Then, the server will allow those eligible for information sharing access to the data it
stores.
The choice of excluding agents with findings that are greater than the threshold may seem
counter intuitive. Seemingly those with favorable findings, i.e., those that actually performed well
and can contribute most to the others are being punished. One may argue that a more suitable choice
would be to set the threshold such that agents that have not found good values would be excluded
from the ISP, thereby encouraging them to try harder. Nonetheless, this also suggests some discouragement to agents in their IGPs as they now will have greater expectations that high values will be
reported in the ISP. Obviously, at the time the ISP ought to take place, opting out is dominated by
taking part in the ISP, both individually and globally. All agents will benefit from the participation
of more agents in the ISP. Moreover, the fact that we require that agents with more favorable values
not to take part in the ISP seems to intensify the potential negative effect of this method as discussed
above. Nevertheless, since each of the agents may find itself in a situation where it is requested not
to take part in the information sharing, and due to the decrease in the potential for improvement
encapsulated in information sharing to those agents that will eventually take part in it, it is likely
that each agent will be individually motivated towards a more efficient IGP.
Overall, the Threshold restricted information sharing method is a bit more complicated to enforce since agents may choose to declare a value different than the best they have found, in an effort
447

fiROCHLIN & S ARNE

Figure 2: A schematic illustration of the optimal strategy for Threshold restricted information sharing.

to take part in information sharing. Nevertheless, whenever the true value can be validated (e.g., in
the travel agents example, each agent can be requested to post a receipt indicating the way she got
to the conference and the amount paid, after the ISP) then the method can be easily enforced using
fines.
In this case, as we prove in the following paragraphs, the agents strategy needs to take into
consideration not only the best value obtained thus far, but also the number of opportunities on which
information has already been gathered. This is because whenever a value v > ViIS is encountered,
whereby the agent is excluded from the ISP, the agent can still benefit from resuming its individual
IGP, and the benefit from additional information gathering depends on the number of remaining
opportunities.
The structure of each agents optimal strategy, given the strategy of others, is given in Theorem
2.
Theorem 2. Given the probability distribution function of the maximal value obtained by all other
agents that take part in the ISP, fi (x), and the number of opportunities on which information has
already been gathered, j  ni , agent Ai s optimal individual information gathering strategy can
be described by the pair (ri (j), riresume ), such that v0i  ri (j)  ViIS  riresume (see Figure 2),
where:
 
)
( IS
resume
(y  r)fi (y)dy
(6)
ri
= max Vi , r|ci =
y=r

and ri (j) is the solution to:


 
fi (y)
(max(y, x)  max(ri (j), x))fi (x)dxdy
y=ri (j)
x=
 
 
+
fi (y)
(EB j+1
(y)  max(ri (j), x))fi (x)dxdy
i
ViIS

ci =

y=ViIS

(7)

x=

where the EB ji (y) is given by:
{
v
j

EBi (v) =
ci + y=EB j+1
(max(v0i , y, v))fi (y)dy
i

riresume  v  j > ni
v < riresume  j  ni

(8)

Given the best value obtained so far, v, Agent Ai should resume its IGP if v < ri (j) or ViIS <
v  riresume and otherwise terminate (and proceed to ISP if v  ViIS ).
448

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Proof. See appendix C.
Intuitively, the reservation value riresume is used to determine if the IGP should be resumed in
cases in which a favorable value v > ViIS has been found and the agent will now be on its own.
The reservation value ri (j) is used to determine if the IGP should be resumed when v  ViIS , i.e.,
when the agent can still potentially take part in the ISP. Resuming the IGP in the latter case can lead
to better values, however at the same time can also lead to exclusion from the ISP, in which case
the agent will end up on its own. Since the fallback in case of a finding v > ViIS depends on the
number of remaining opportunities j, we use a different reservation value ri (j) for each j value. In
case riresume = ViIS , there is no benefit for the agent to resume the IGP once a value v > ViIS is
found. In this case ri (j) = r for any j.
Once again, the solution of a set of equations consisting of (6-8) will provide a set of pure
equilibria of the form {(ri (1), .., ri (ni )), riresume |1  i  k}, if any exist. Similarly, for the
same considerations given in Section 3.1, a mixed equilibrium for this case will be of the form:
{(pi , (ri (1), .., ri (ni )), riresume )|1  i  k}.
While the equilibrium analysis in this case generally resembles the one given in 3.1, the calculation of Fireturn (x), which is the probability that the maximum value obtained by agent Ai s
individual IGP (including v0j ) will be less than x is substantially more complicated. In order to
formulate Fireturn (x) we use Fireturn (x, v, j) to denote the probability that agent Ai will obtain a
maximum value of x or below, given its current state (v, j), where v is the best value obtained so
far in its IGP (including v0i ) and j is the number of opportunities on which information has been
gathered. The function Fireturn (x, v, j) can be calculated recursively according to:

v>x )
 0
(
IS
resume
return
1
v  x  ri (j) < v  Vi  ri
 v  j = ni
Fi
(x, v, j) =
 
return (x, max(v i , y, v), j + 1)f (y)dy
F
otherwise
i
0
y= i
(9)
The case where x < v is trivial since the maximum value the agent will obtain will be at least
v. Therefore, the probability of obtaining x or below is zero. Similarly, when ri (j) < v  ViIS
or riresume  v or when there are no additional opportunities (j = ni ) the agent will inevitably
terminate its IGP (according to Theorem 2) and the maximum value that will be obtained will be
v. In such case, if v  x the function obtains 1. In all other cases, the IGP resumes, hence the
probability is given recursively based on the new state (x, max(v, y), j + 1) the agent will be in
after gathering information on one additional opportunity. Using Fireturn (x, v, j) we can calculate
the probability that the maximum value that will be obtained by agent Ai s individual IGP will be
less than (or equal to) x, as: Fireturn (x) = Fireturn (x, , 0).
Thus, we can now formulate the probability that the maximum value provided by agent Ai in

the ISP will be less than x, denoted Fireturn (x):

0
x < v0i


return
return
return
IS
F
(x) + (1  Fi
(Vi )) v0i  x  ViIS
Fi
(x) =
(10)
 i
1
otherwise
The case where x < v0i is trivial, as v0i is a lower bound for the best value the agent ends up with.
The case where x  ViIS is satisfied either if Ai s best value is less than x (in which case it will take
449

fiROCHLIN & S ARNE

part in the ISP since x  ViIS ), i.e., with a probability of Fireturn (x), or if the agent will not take part
in the ISP, i.e., with a probability of 1Fireturn (ViIS ). The case where x > ViIS is straightforward 
since agent Ai will take part in the ISP only if its best value will be lower than ViIS , the probability
of obtaining a value v (where x > ViIS ) from it is 1.

Using Fireturn (x), we can calculate the function Fi (x) (a modification of (3)):
Fi (x) =





(pj Fjreturn (x) + (1  pj ))

(11)

Aj Kj=i

The probability distribution function fi (x) is the first derivative of Fi (x) as before. Similarly, the
probability distribution function fireturn (x) is the first derivative of Fireturn (x).
We now turn to calculating the expected cost incurred throughout the IGP carried out by agent
Ai , denoted ECi . In order to calculate ECi we use ECi (v, j) to denote the expected cost of
agent Ai , when in state (v, j), where v is the best value obtained after gathering information on j
opportunities. The value of ECi (v, j) can be calculated recursively according to:
{
ECi (v, j) =

0 
ri (j)  v < ViIS  riresume  v  j = ni

i
ci + y= ECi (max(v0 , y, v), j + 1)fi (y)dy
otherwise
(12)

The first case is where the agent unavoidably terminates its IGP (according to Theorem 2). The
second case is where, the IGP resumes, hence the expected cost is given recursively based on the
new state (max(v, y), j + 1) the agent will be in after gathering information on one additional
opportunity. This allows us to calculate ECi (x), as: ECi (x) = ECic (, 0).
These enable us to calculate the expected benefit of agent Ai when the other agents use the set
of strategies {(pi , (ri (1), .., ri (ni )), riresume )|1  i  k  i = j}. If agent Ai will choose to engage
in IGP then its expected benefit, EBi (IGP ), is given by:
 
max(v0i , y)fireturn (y)dy
(13)
EBi (IGP ) = ECi +

+

ViIS

fireturn (y)

y=



y=ViIS



max(v0i , y, x)fi (x)dxdy

x=

which is similar to (4), except that the differentiation between the second and third terms on the
right is according to ViIS rather than PiIS .
When the agent opts not to gather information at all, its expected benefit, EBi (IGP ), is
simply the expected value of the maximum value returned by the other agents taking part in the ISP:
 
EBi (IGP ) =
max(y, v0i )fi (y)dy
(14)
y=

The equilibrium stability conditions remain as in Section 3.1, replacing the calculation of EBi (
IGP ) and EBi (IGP ) with (13) and (14). As with the former method, there is no guarantee
that an equilibrium will actually exist (either pure or mixed) and that if one exists there will be no
other equilibria. Also, as with the method presented above, the analysis of the Threshold restricted
method generalizes the analysis of the standard cooperative information gathering model in the
450

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Figure 3: Threshold restricted information sharing - effect of V IS on the individual expected benefit, for different information gathering costs, c, in a setting: k = 2 and n = 1.

sense that the latter is a specific case where the threshold set for taking part in the ISP is ViIS  
i. Similarly, when ViIS   i, the solution obtained is the same as the one known for the
single-agent information gathering problem (McMillan & Rothschild, 1994).
Figure 3 illustrates the agents individual expected benefit as a function of the thresholds ViIS
used, for different information gathering costs (c). The setting used is the homogeneous setting
described at the beginning of the section, with the parameters c = 0.4 and n = 1. The values ViIS
were similar for all agents (i.e., ViIS = V IS i). As depicted in the figure, the maximum expected
individual benefit is obtained when the value of the threshold for excluding agents from the ISP
is, in some cases, substantially small, leaving many favorable findings outside the ISP. The typical
pattern exhibited in the figure is similar to the one depicted in Figure 1: an increase and then a
decrease in the expected individual benefit as the threshold V IS increases. This is explained by the
fact that when V IS = 0 each agent actually executes an individual IGP without sharing information
with others. As V IS increases, the agent relies more on the information gathered by the other agents
and the same considerations explained in 3.1 regarding the tradeoff between the negative effect over
pi and ri and the positive effect over the chance of actually taking part in the ISP hold.
3.3 Cost Filtered Information Sharing
This method introduces a cost cIS an agent incurs if it chooses to take part in the ISP. For example,
in our travel agents running example this is equivalent to having all agents that are interested in
taking part in the ISP present some evidence of donating a fixed amount of money to some charity,
as a prerequisite for accessing the designated secured server that is used for sharing. Unlike the two
prior methods, here the agents get to decide whether to opt-out of information sharing; hence it does
not require any enforcement whatsoever. It does require, however, a means for introducing a cost
for the ISP, e.g., through a donation or by setting a meeting place for sharing information such that
each participant will need to spend time and money for getting there.
451

fiROCHLIN & S ARNE

It is assumed that all agents are a priori acquainted with the ISP participation costs of all agents.
The introduction of such a cost (that is not eventually returned to the agents) requires an appropriate
balance in the form of some compensation to the agents for taking part in the ISP. Without such
compensation, no agent will be willing to take part in the ISP, as proven in the following proposition.
Proposition 1. If the agents incur some cost when taking part in the ISP, then in the absence of
some compensation for taking part in the ISP, none of the agents will take part in the ISP.
Proof. Consider the highest value v that if known to any of the agents, after completing their individual IGP, warrants the participation of the agent in the ISP. We show that in the absence of an
appropriate compensation for the agent, such a value v cannot hold  since none of the other agents
will contribute a value greater than v to the ISP, the agent that found v will not gain anything from
the ISP however it will incur a cost; consequently it will choose not to take part in the ISP.
One option to compensate the agents for taking part in the ISP is to offer the agent with the best
value that takes part in the ISP a compensation B. The amount B can be collected from the agents
(e.g., in equal shares) prior to the IGP, such that once collected it is considered a sunk cost and the
agents strategies become affected only by the chance of receiving the bonus B. The structure of
the best response strategy of any individual agent in this case, given the strategy of others, is given
in Theorem 3.
Theorem 3. Given the probability distribution function of the maximal value obtained by all other
agents that take part in the ISP, fi (x), agent Ai s optimal individual information gathering strategy
can be described by the pair (ri , RiIS ), where RiIS is the set of intervals such that for any x 
RiIS  x  v0i :
 x
 
IS

fi (y)dy
(15)
(y  x)fi (y)dy + B 
c 
y=

y=x

and is to set a value ri  v0i , where ri is the solution to:
 
ci = (EBi (y)  EBi (ri ))fi (y)dy

(16)

y=ri

where EBi (v) is given by:


v
v
EBi (v) = cIS + B  y= fi (y)dy

 
+ y= max(v0i , y, v)fi (y)dy

v
/ RiIS
otherwise

(17)

The agent should resume its individual IGP as long as the value found so far is below ri , and
otherwise it should terminate its IGP. Upon terminating the IGP (or obtaining the value of all
opportunities) the agent should participate in the ISP if the best value it has found in its individual
IGP is in one of the intervals of the set RiIS and otherwise it should opt out of taking part in the ISP.
Proof. The set RiIS as defined in (15) contains all the values v for which
(  the expected benefit)from

taking part in the ISP - calculated as the potential value improvement,
(y  x)fi (y)dy plus
y=x

452

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

x
the expected compensation B  y= fi (y)dy, both independent of the reservation value ri used
by the agent - is greater than the cost cIS incurred. The remainder of the proof, concerning the
optimality of a reservation-value-based strategy and the correctness of (16) is the same as the one
provided for Theorem 1, differing only in the way the expected benefit if information gathering is
resumed is calculated.
The solution of a set of equations consisting of (16)-(17) for each agent will provide a set of
pure equilibria of the form {(ri , RiIS )|1  i  k}, if any exist. For the same considerations given
in Section 3.1, a mixed equilibrium for this case will be of the form {(pi , ri , RiIS )|1  i  k}.
The equilibrium analysis of the Cost filtered information sharing follows the analysis given
for the previous methods, therefore we provide only the differences. The calculation of Fireturn (x)
remains as in (2). The probability distribution function fireturn (x) is the first order derivative of

Fireturn (x). The function Fireturn (x), is now calculated as a modification of (10):


Fireturn (x) = Fireturn (x) +
fireturn (y)dy
yxy R
/ iIS

and consequently the function Fi (x) is given by:
Fi (x) =





(pj Fjreturn (x) + (1  pj ))

Aj Kj=i

The probability distribution function fi (x) is the first order derivative of Fi (x) as before.
These enable us to calculate the expected benefit of agent Ai when the other agents use the set
of strategies {(pj , rj , RjIS )|1  j  k  i = j}. If agent Ai will choose to engage in IGP then its
expected benefit, EBi (IGP ), is given by:
 
1  Fi (ri )ni
+ EBi (max(y, v0i ))fireturn (y)dy
(18)
EBi (IGP )=ci
1  Fi (ri )
y=
The first term is the expected cost incurred throughout the IGP carried out by Ai , calculated as:
n
 i
i (ri ) i
ci nj=1
(Fi (ri ))j1 = ci 1F
1Fi (ri ) , as the number of opportunities on which information is gathered is a geometric random variable bounded by ni , with a 1  Fi (ri ) success probability. The
second term is the expected benefit from executing its IGP. When the agent opts not to gather information at all, its expected benefit, EBi (IGP ), is simply the maximum between the expected
value of the maximum value returned by the other agents and v0i , if the agent will choose to take
part in the ISP:
EBi (IGP ) = EBi (v0i )

(19)

The equilibrium stability conditions remain as in Section 3.1, replacing the calculation of EBi (
IGP ) and EBi (IGP ) with (18) and (19). As with the former methods, there is no guarantee
that an equilibrium will actually exist (either pure or mixed) and that if one exists there will be no
other equilibria. Also, as with the methods presented above, the analysis of the cost filtered method
generalizes the analysis of the standard cooperative information gathering model in the sense that
the latter is a specific case where cIS = B = 0. Similarly, when B = 0 the solution obtained is the
453

fiROCHLIN & S ARNE

Figure 4: Cost filtered information sharing - effect of cIS on the individual expected benefit, for
different information gathering costs, c in a setting: k = 2, n = 3 and B = 0.04.

same as the one known for the single-agent information gathering problem (regardless of the value
of cIS , based on Proposition 1).
Figure 4 illustrates the agents individual expected benefit as a function of the cost cIS used, for
different information gathering costs (c). The setting used is the homogeneous setting described at
the beginning of the section, using the parameters k = 2, n = 3 and B = 0.04. As illustrated in
the figure, the maximum expected individual benefit is obtained when a substantial cost is incurred
for taking part in the ISP. The typical pattern exhibited in the figure is similar to the one depicted in
Figures 1 and 3, and explained by similar considerations.
3.4 Random Finding Sharing
In this method each agent Ai , after it has completed its individual IGP, is allowed to take part in the
ISP, however it is restricted to disclose only one of the values it has come across in its IGP, which
is randomly selected. The downside of such a restriction is obvious - in most cases the agents will
not benefit from the best individual findings but rather become exposed to a small, not necessarily
optimal, subset of the information gathered. Still, as we demonstrate in the following paragraphs,
for some settings the individual performance improves. In our travel agents running example the
implementation of the method is quite straight forward and relies, as before, on having the agents
send their findings to a designated secured server used for sharing. The only difference, however,
is that the server will pick only a single random finding of each agent and will discard the rest of
the findings. Here, again, no enforcement is necessary since no individual agent can benefit from
disclosing more information than it is required to disclose.
The structure of the best response strategy of any individual agent in this case, given the strategy
of others, is given in Theorem 4.
Theorem 4. Given the probability distribution function of the maximal value obtained in the ISP
based on the reports of all other agents, denoted fi (x), agent Ai s optimal individual information
454

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

gathering strategy is to follow a reservation value ri  v0i , where ri is the solution to:
 
 
ci =
fi (y)
(max(y, x)  max(ri , x))fi (x)dxdy
y=ri

(20)

x=

The agent should always choose to gather information on an additional opportunity (if one is still
available) if the best value obtained so far is below ri and otherwise it should proceed to ISP.
Proof. The proof is similar to the one given for Theorem 1, differing only in the way the expected
benefit if gathering information on an additional opportunity is calculated.
The solution of a set of k equations of type (20), one for each agent, will provide a set of pure
equilibria of the form {ri |1  i  k} if any exist. For the same considerations given in Section 3.1,
a mixed equilibrium for this case will be of the form {(pi , ri )|1  i  k}.

The equilibrium analysis for this method follows the one given in 3.1, except that Fireturn (x)
is used as in 3.2. The function Fireturn (x) is given by (2). The probability distribution function

fireturn (x) is the first order derivative of Fireturn (x). In order to formulate Fireturn (x) we use a some
what different state definition for Fireturn (x, v, j, l). A state (v, j, l) is now defined according to the
number of values that are below x obtained by agent Ai after gathering information on j opportuni
ties, denoted l, and the best value obtained so far (including v0i ), v. The function Fireturn (x, v, j, l)
can be calculated recursively according to:

0
x < v0i



l

v  ri  ni = j

j
x

Fireturn (x, v, j, l) =
(21)
return
F
(x,
max(y,
v),
j+1,
l+1)f
(y)dy
otherwise

i
i
y=



 +  F return (x, max(y, v), j+1, l)f (y)dy
i
y=x i
The case where x < v0i is trivial, as v0i is a lower bound for the best value the agent ends up with.
The second case in (21) is straightforward  when the best value obtained so far is v  ri , or when
there are no additional opportunities (ni = j), the agent necessarily terminates its IGP (according
to Theorem 4). In such cases, if l values of the total j values are below x, then the probability that
Ai will return a value below x is jl . In all other cases, the IGP will resume; hence the probability is
given recursively based on the new state (x, max(y, v), j + 1, l + 1) in case the new value obtained
is y  x and (x, max(y, v), j + 1, l) otherwise.

Thus, we can now formulate Fireturn (x):




Fireturn (x) = Fireturn (x, , 0, 0)

(22)

and consequently Fi (x) is given by (11) and the probability distribution function fi (x) is the first
order derivative of Fi (x) as before.
These enable us to calculate the expected benefit of agent Ai when the other agents use the set of
strategies {(pj , rj )|1  j  k  i = j}. If agent Ai will choose to engage in IGP then its expected
benefit, EBi (IGP ), is given by:
 
 
1  Fi (ri )ni
EBi (IGP ) =  ci
+
fireturn (y)
max(v0i , y, x)fi (x)dxdy
(23)
1  Fi (ri )
y=
x=
455

fiROCHLIN & S ARNE

Figure 5: Random finding sharing and Standard information sharing (according to Rochlin et al.,
2014) as a function of: (a) information gathering costs, c, in a setting: k = 5 and n = 3;
and (b) number of agents, k, in a setting: c = 0.1 and n = 3.

When the agent opts not to gather information at all, its expected benefit, EBi (IGP ), is
simply the expected value of the maximum value returned by the other agents:
 
EBi (IGP ) =
max(v0i , x)fi (x)dx
(24)
x=

The equilibrium stability conditions remain as in Section 3.1, replacing the calculation of EBi (
IGP ) and EBi (IGP ) with (23) and (24). As with the former method, there is no guarantee that
an equilibrium will actually exist (either pure or mixed) and that if one exists there will be no other
equilibria.
Figure 5 depicts the agents individual expected benefit as a function of information gathering
costs c (left graph) and the number of agents k (right graph). The setting used is the homogeneous
setting described in the beginning of the section, with the parameters k = 5 and n = 3 (Figure 5(a))
and c = 0.1 and n = 3 (Figure 5(b)). Each graph depicts the performance both with Random finding
sharing and the standard non-restricted sharing method (according to Rochlin et al., 2014). The
figure shows that the random finding sharing strategy dominates the standard information sharing
strategy, as far as the expected individual benefit is concerned.
3.5 Subgroup Restricted Information Sharing
In this method the agents are divided into sub groups that share their findings separately, i.e., each
agent executes its individual IGP separately and at the end each subgroup will carry out a separate
ISP.
 Formally, consider
wthe case where the |K| agents are divided into w subgroups {K1 ,    , Kw }
( w
K
=

and
j
j=1
j=1 Kj = K). In our travel agents running example this can be done by
providing each sub-group a designated server that will serve only the subgroup members or, in
physical environments, setting different meeting places for sharing the information after the group
has been partitioned into sub-groups.
456

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Figure 6: Subgroup restricted information sharing - individual expected benefit as a function of the
number of subgroups, w, in a setting: c = 0.45, k = 20 and n = 10.

The best response strategy of any individual agent Aji of subgroup Kj = {Aj1 , . . . Aj|kj | }  K
as well as the equilibrium analysis within the subgroup level are the same as given by (Rochlin
et al., 2014), as all agents within the subgroup fully share their findings. The optimal strategy can
thus be obtained by checking the expected benefit of all possible divisions of K into subgroups and
selecting the partition associated with the highest expected benefit. The computational complexity
of evaluating all subgroups is combinatorial in the number of agents. Although the focus of the
paper is not on the computational aspects but rather on analyzing the structure of the equilibrium
cooperative strategies, we note that in most ISP settings the computational complexity becomes a
non-issue since the number of agents taking part in the ISP is relatively small.
We note that this method results in a similar performance as in (Rochlin et al., 2014) when the
number of subgroups is w = 1 (i.e., all k agents will take part in the ISP as one group). Similarly,
when the number of subgroups is w = |K| (i.e., each subgroup contains only one agent) the solution obtained is the same as the one known for the single-agent information gathering problem
(McMillan & Rothschild, 1994).
Figure 6 illustrates the agents individual expected benefit as a function of the number of subgroups used (w). The setting used is the homogeneous setting described in the beginning of the
section, using the parameters k = 20, n = 10 and c = 0.45. The graph uses partitioning into equalsize subgroups, i.e., w obtains the values {1, 2, 4, 5, 10}. As depicted in the figure, the maximum
expected individual benefit is obtained in this example when the number of subgroups is w = 2.

4. Related Work
The model analyzed in this paper is based on two important concepts: multi-agent cooperation and
costly information gathering. Multi-agent cooperation has been shown to be widely effective for
better achieving agents individual goals (Stone & Kraus, 2010) or improve their performance measures (Kraus, Shehory, & Taase, 2003; Dutta & Sen, 2003), especially when there are differences
in the agents capabilities, knowledge and resources or when an agent is incapable of completing
457

fiROCHLIN & S ARNE

a task by itself (Stone & Kraus, 2010; Saad, Han, Debbah, & Hjorungnes, 2009; Conitzer, 2012;
Breban & Vassileva, 2001). It is also the main driving force behind many coalition formation models in the area of cooperative game theory and MAS (Shehory & Kraus, 1998). Yet, the majority
of cooperation and coalition formation MAS-related research tends to focus on the way coalitions
are formed and consequently concerns issues such as the optimal division of agents into disjoint
exhaustive coalitions, division of coalition payoffs and enforcement methods for interaction protocols. While coalition formation and coordination models can be widely found in the electronic
market domain, most work in this domain emphasizes mechanisms for forming cooperation for the
purpose of aggregating demands in order to obtain volume discounts (Tsvetovat, Sycara, Chen, &
Ying, 2000; Yamamoto & Sycara, 2001; Sarne & Kraus, 2003). Several authors have considered
the problem of determining the strategy of a group once formed (Ito, Ochi, & Shintani, 2002; Sarne,
Manisterski, & Kraus, 2010; Rochlin, Sarne, & Zussman, 2011; Mash, Rochlin, & Sarne, 2012),
however their focus was mostly on fully-cooperative agents. None of these works considered the cooperation problem of a group of self-interested agents in costly exploration settings where findings
can benefit all agents.
Group-based cooperation of self-interested agents can also be found in public goods games and
allocation games in general (Aumann, 1998; Nagel & Tang, 1998; McKelvey & Palfrey, 1992;
de Jong et al., 2008; de Jong & Tuyls, 2011). Common to these games is that according to their
equilibrium each agent individually should opt out of the cooperation as soon as possible or invest
the minimum allowed. Therefore the research on cooperation in this domain is mostly studied for
repeated games (Selten & Stoecker, 1986) or settings with bounded-rational participants (e.g., people) for which cooperation to some extent is commonly exhibited. Much effort has been placed on
developing reciprocity-based mechanisms, e.g., tit-for-tat (Axelrod, 1984) that facilitate cooperation
even when agents find it momentarily beneficial to act selfishly. This way, long-term considerations
override short-term greedy behavior. Many have extended the basic mechanism to support various
variants of the model, such as asymmetric costs, heterogeneously repeating instances and other factors (Sen, 1996). Few works has dealt with inducing cooperation in non-repeated settings showing
that rewards are somewhat less effective than sanctions in enforcing cooperation (Walker & Halloran, 2004). The main difference between public goods games and our work is that in our case the
public goods are in fact information that can lead to better economic decisions. Obtaining more
information requires carrying out an active sequential information gathering process. Therefore, in
our settings there is much room for individual information gathering, to some extent, even if all
the others are free riders. Moreover, with the simplistic settings used in public goods games the
issue of information sharing and the ways it is carried out (when considering self-interested agents)
becomes irrelevant.
The second concept upon which this paper relies, i.e., costly information gathering, is of great
importance when there is no central source that can supply an agent full immediate reliable information on the environment and the state of the other agents (Sarne & Aumann, 2014). In general, the
introduction of information gathering costs into MAS models leads to a more realistic description of
these environments. This is because agents are typically required to invest/consume some of their
resources in order to obtain information concerning opportunities available in their environment
(Bakos, 1997; Sarne & Kraus, 2008; Kephart & Greenwald, 2002; Rochlin & Sarne, 2013; Rochlin,
Sarne, & Zussman, 2013; Manisterski, Sarne, & Kraus, 2008).
Optimal strategies for settings where individuals need to search for an applicable opportunity
when information gathering is costly have been widely studied (Grosfeld-Nir, Sarne, & Spiegler,
458

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

2009; Elmalech, Sarne, & Grosz, 2015; Elmalech & Sarne, 2012), prompting several literature reviews (Smith, 2011; McMillan & Rothschild, 1994; Morgan & Manning, 1985). These models,
which are often termed costly search models or economic search models have been developed
to the point where their total contribution is referred to as search theory. Over the years, many
information gathering model variants have been considered, focusing on different aspects of the
model, such as the decision horizon (finite versus infinite) (Lippman & McCall, 1976), the presence of the recall option (McMillan & Rothschild, 1994), the option to disambiguate noisy signals
(Chhabra, Das, & Sarne, 2014a; Alkoby, Sarne, & Das, 2015; Chhabra, Das, & Sarne, 2014b), the
distribution of values and the extent to which findings remain valid along the process (Landsberger
& Peled, 1977). In particular, these models have been integrated in the study of strategic information platforms (Hajaj, Hazon, Sarne, & Elmalech, 2013; Sarne, 2013; Hajaj & Sarne, 2014).
Another strand of search-based models is two-sided search (Sarne & Arponen, 2007; Hendrix &
Sarne, 2007) that deals with the distributed (search-based) formation of pairwise (or general size)
partnerships (Nahum, Sarne, Das, & Shehory, 2015). While the analysis of such models derive
from equilibrium considerations, these are very different from our model as they do not reflect any
cooperative aspect.
Many cooperative information gathering models have been studied, extending the theories to
multi-agent (or multi-goal) environments (Sarne & Kraus, 2005). Examples include, among others,
an attempt to purchase several commodities while facing imperfect information concerning prices
or operating several robots in order to evaluate opportunities in different locations. These works
differ from ours either in that they consider fully cooperative agents that attempt to maximize the
overall utility (Sarne et al., 2010; Gatti, 1999; Burdett & Malueg, 1981; Carlson & McAfee, 1984),
and thus lack any equilibrium considerations, or they assume that any agents IGP is constrained
by the findings of the other agents, rather than augmented/improved by such findings as in our
case (Rochlin, Sarne, & Laifenfeld, 2012; Rochlin & Sarne, 2014b). Consequently they constitute substantially different equilibrium strategies. Models that do consider cooperative information
gathering, which rely on assumptions similar to ours (e.g., Rochlin et al., 2014; Hazon et al., 2013),
focus primarily on the extraction of the equilibrium strategies and investigate the influence of the
different model parameters on the agents performance in equilibrium. None of these works, however, suggested methods for improving cooperative information gathering in such settings, of the
kind that we suggest and analyze in this paper.
More broadly, our problem can be seen as part of the field of planning under uncertainty, hence it
is related to Markov decision processes (MDP) (Bellman, 1957; Puterman, 1994) and decentralized
Markov decision processes (Bernstein, Givan, Immerman, & Zilberstein, 2002). In these models the
goal is to maximize the expected cumulative reward, which is also the objective in our case. Alas, the
use of MDPs in our case is complicated by the continuous nature of the value probability distribution
functions. More importantly, our analysis and proofs result in threshold-based (or interval-based)
solutions which are both simpler in terms of strategy and state representation and can be derived
with a substantially lesser complexity compared to solving via MDPs.
Finally, we note that the non-intuitive findings whereby methods that essentially limit information sharing and cooperation actually have a positive impact in the self-interested case follows,
in spirit, earlier results in other settings. In particular, ones in which it has been shown that socalled inefficiencies can increase market performance, under certain circumstances. For example,
Masters (1999) shows that an increase in minimum wage, which is often considered inefficiency
in economics, can have positive employment effects. In transportation economics (e.g., congestion
459

fiROCHLIN & S ARNE

games) equilibrium is frequently not the overall optimum. In such cases, it has been shown that taxation can change the equilibrium to a more desirable one (Penn, Polukarov, & Tennenholtz, 2009b,
2009a; Fotakis, Karakostas, & Kolliopoulos, 2010). Similarly, taxes can facilitate more desirable
equilibria in Boolean games (Endriss, Kraus, Lang, & Wooldridge, 2011) and in centralized matching schemes (Anshelevich, Das, & Naamad, 2013). In this work we show that a somewhat similar
phenomenon also occurs in the context of cooperative information gathering, though the model and
analysis are, of course, totally different from the above mentioned.

5. Discussion and Conclusions
As demonstrated in Section 3, each of the five methods proposed and analyzed in this paper can
substantially increase the benefit self-interested agents achieve through information sharing when
gathering information cooperatively. Each of the five methods is based on a different restriction
made on the agents ability or willingness to take part in the ISP. Intuitively such restrictions may
seem to have a negative effect on performance. Yet, since each agent gains less from the information
sharing itself, it has a greater incentive to invest more resources in individual information gathering,
hence overall performance improves.
The results suggest important inputs for the designers of markets and systems where cooperative
information gathering is applicable, by enabling them to predict the strategies that will be used and
the resulting system performance. These primarily facilitate the proper design of the system and
the determination of what elements should and should not be included in such systems in order
to achieve specific goals and promote certain behavior. In particular, the introduction of some
seemingly non-beneficial elements may actually be productive. We note that the paper generally
does not attempt to find the optimal parameter values for each method (e.g., probability, group
partitioning, threshold and cost of taking part in the information sharing, or the payment received
if the agent is associated with the best value), since the concept of optimality in this sense is not
properly defined. Indeed in settings where there is an equilibrium solution that is preferred by all
agents (e.g., in the examples given in the former sections, where all agents are homogeneous) the
choice of parameter values is clear. Nevertheless in general, it is possible that a certain value will
be preferred by one of the agents whereas others will prefer another. In the latter case it is the role
of the system designer to decide on these parameters based on her goals.
There are numerous extensions of the model that can be considered. Some of them are straightforward and require minor changes in the analysis. For example, if agents are buyers and each of
them is interested in more than a single unit of the product they are searching for, the only change
required in the individual strategy equations is multiplication of the expense of purchasing the item
by the number of items in which the agent is interested. Another example is the composition of
several methods. For example, in the case of Subgroup restricted information sharing, each of the
subgroups can adopt any of the other four methods in the subgroup level. Other extensions, while
of much interest, are more complex to analyze. For example, consider a model where the agents
can continuously share their findings along their individual IGPs. In this case, as discussed in Section 1, it is essential to first define the method that will provide an incentive for agents to share
their findings despite the negative influence it will have in terms of discouraging others from further
information gathering.
460

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Acknowledgments
Preliminary results of this work appeared in the Proceedings of the Thirteenth International Conference on Autonomous Agents and Multiagent Systems (Rochlin & Sarne, 2014a). The authors would
like to thank Barbara Grosz for insightful comments that helped improve this paper substantially.
The first coauthor was a student at Bar-Ilan University when the research reported in this paper was
carried out. This research was partially supported by the ISRAEL SCIENCE FOUNDATION (grant
No. 1083/13) and the ISF-NSFC joint research program (grant No. 2240/15).

Appendix A. Proof of Theorem 1
Proof. We first prove the reservation-value nature of the optimal strategy. Then we continue with an
inductive proof, to show that the reservation value used by each agent will remain stationary along
its IGP and can be calculated according to Equation 1.
In the absence of any other new information along the IGP, each agents strategy will be the
mapping of S(x, j)  {terminate, resume}, where x is the set of values obtained so far (including v0i ) and j is the number of opportunities on which information has already been gathered. Since
the agent is interested merely in the maximum opportunity value, its strategy is affected only by
the maximum value in x, hence the strategy can be defined as S(v, j)  {terminate, resume},
where v is the maximum value in x. Obviously, if according to the optimal strategy the agent needs
to resume IGP upon reaching state (v, j) then the same should be true for any state (v  , j) where
v  < v. Similarly, if according to the optimal strategy the exploration should terminate at state (v, j)
then the same should hold for any state (v  , j) where v  > v. Therefore, for each given number
of opportunities for which information has been gathered, j, the optimal individual IGP strategy of
agent Ai can be characterized by the reservation value rij such that the agent should resume IGP if
the best value obtained so far is below rij and otherwise terminate the IGP.
We begin with the case of j = ni  1. If the best value obtained thus far by agent Ai is v then
gathering information on one last opportunity, according to this strategy, will incur a cost ci and the
expected value the agents will obtain will be:

PiIS







fi (y)
y=
 
+ (1  PiIS ) 



(max(y, v, z)fi (z))dzdy

z=

max(y, v)fi (y)dy

y=

The first term relates to the case where agent Ai will participate in the ISP, with a probability of
PiIS , whereas the second term relates to the case where it will be required to opt out from the ISP,
i.e., with a probability of 1  PiIS (where y is the new value that will be obtained by the agent and
z is the best value that will obtained from the other agents IGP).
On the other hand, terminating the IGP at this point will result in a benefit:

PiIS





max(v, z)fi (z)dz + (1  PiIS )  v

z=

461

fiROCHLIN & S ARNE

Therefore, the agent should gather information on the last opportunity if and only if:
 
IS
Pi 
max(v, z)fi (z)dz + (1  PiIS )  v <
z=
 
 
PiIS 
fi (y)
(max(y, v, z)fi (z))dzdy
y=
z=
 
IS
+ (1  Pi ) 
max(y, v)fi (y)dy  ci

(25)

y=

The left hand side of the equation captures the expected benefit if the individual IGP is terminated
and the right hand side captures the expected benefit if information is gathered for the last opportunity. Both terms distinguish between the case where agent Ai participates in the ISP i.e., with
a probability of PiIS , and when it is not allowed to. Using simple mathematical manipulations
obtains:
 
 
IS
0 < Pi 
fi (y)
(max(y, z)  max(v, z)fi (z))dzdy
(26)
y=v
z=
 
+ (1  PiIS ) 
(y  v)fi (y)dy  ci
y=v

The value v for which (26) becomes an equality is in fact the value of ri according to (1). Therefore, since the right hand side of (26) is a decreasing function of v, then the agents should gather
information on the last opportunity whenever the value of v is less than the value of r according to
(1). This establishes the first part of the proof.
Now assume the same ri (according to (1)) holds for any j  > j, for some j, and consider
the agents decision regarding gathering information on one more opportunity, if the best value
obtained thus far is v and the number of opportunities for which the values were already obtained is
j. If v > ri  v0i and the agent gathers information on one additional opportunity, then regardless
of the value obtained next it will definitely terminate its individual IGP thereafter (as it already has
a value greater than ri and according to the induction assumption the optimal strategy thereafter is
the reservation value ri ). Therefore the benefit obtained from further information gathering is given
by:
 
 
IS
Pi 
fi (y)
(max(y, z)  max(v, z)fi (z))dzdy
y=
z=
 
+ (1  PiIS ) 
(y  v)fi (y)dy  ci
y=

Alas, since the latter term decreases as v increases, and obtains zero for v = ri (according to (1)),
then since v > ri  v0i the term obtains a negative value, hence additional information gathering
cannot be the preferred choice.
Similarly, consider the case where v0i  v < ri for j and the agent chooses not to gather additional information. Here the expected benefit from resuming information gathering is necessarily
greater than if resuming in state (v, j  > j). However, according to the induction assumption the
agent should resume information gathering in state (v, j  > j), leading to a contradiction. Therefore, the optimal strategy for j is also a reservation value strategy and the optimal reservation value
is calculated, once again, according to (1).
462

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Figure 7: Enforced probabilistic information sharing - effect of P H on the individual expected
benefit, for different: (a) numbers of agents, k, in a setting: c = 0.35 and n = 5; and (b)
information gathering costs, c, in a setting: k = 15 and n = 4.

Appendix B. Partially Limiting Information Sharing
Consider the case where each agent Ai , after it has completed its individual IGP, shares its own
findings however can only obtain the information of others with a probability of PiH , which is
a priori assigned. The structure of the best response strategy of any individual agent in this case,
given the strategy of others, is identical to the one given in Theorem 1, replacing PiIS with PiH .
Similarly the rest of the analysis given above hold for this case, except for the calculation of Fi (x),
which is now given by:


Fi (x) =

(pj Fjreturn (x) + (1  pj ))

(27)

Aj Kj=i

Figure 7 depicts the agents individual expected benefit as a function of the probability PiH
used, for different group sizes (k) and information gathering costs (c) using the same setting used
in Figure 1. A comparison of Figures 1 and 7 reveals that in this case the partial limitation over the
information sharing dominates the enforced probabilistic information sharing strategy, as far as the
expected individual benefit is concerned. Nonetheless, this is certainly not a general result.

Appendix C. Proof of Theorem 2
Proof. The proof generally resembles the one given for Theorem 1. If the best value agent Ai has
found so far (including v0i ) is v > ViIS it is not allowed to take
 part in the ISP. If j = ni  1
then resuming the IGP will result in expected benefit ci + y= max(y, v)fi (y)dy whereas
terminating the IGP will guarantee v. The agent should explore further for any v for which the first
463

fiROCHLIN & S ARNE

term is greater than the latter, which can be represented, after some mathematical manipulations, as:




ci <

(y  v)fi (y)dy

y=v

which is equivalent to any value v greater than riresume according to 6.8
The remainder of the proof which deals with showing the reservation-value nature of the optimal
strategy and that if v > ViIS the reservation value used by each agent remains stationary along its
IGP is the same as in Theorem 1.
Now consider the case where the best value agent Ai has found so far is v0i  v  ViIS and the
number of opportunities on which information has already been gathered is j. Resuming the IGP
will result in:

 ci +



ViIS



fi (y)
y=


(max(y, v, z)fi (z))dzdy +

z=



y=ViIS

EB j+1
(y)fi (y)dy
i

where EB j+1
(y) is the expected benefit of agent Ai , given the best value it has obtained thus far, y
i
(including v0i ), and the number of opportunities on which information has already been gathered, j.
The term EBij (v) is calculated recursively according to (8). If there are no remaining opportunities
(j > n) or the best value found is v  riresume the expected benefit is simply v. Otherwise, the
IGP resumes (incurring a cost ci ) with the revised best value (max(y, v)), however with j + 1
opportunities on which information has already been gathered, i.e., with an expected benefit of
EBij+1 (max(y, v)). On the other hand, terminating the IGP at this point will result in the expected
benefit:
 
max(v, z)fi (z)dz
z=

Therefore, the agent should explore further for any v for which the first term is greater than the
second, which after some mathematical manipulations becomes:

ci <

+

ViIS

y=v


y=ViIS





(max(y, z)  max(v, z))fi (z)dzdy
fi (y)
z=
 
fi (y)
(EBij+1 (y)  max(v, z))fi (z)dzdy
z=

which is equivalent to any value greater than ri (j) according to 7. The reason a different threshold
is used for different j values is that the calculation of the threshold depends among others on the
expected benefit in case the value to be obtained is v > ViIS , in which case the expected benefit
depends on the number of remaining opportunities.

8. If the value of riresume resulting from ci = y=v (yv)fi (y)dy is lower than ViIS , the agent will inevitably terminate
the IGP (as v > riresume ) hence any value riresume  ViIS can be used, in particular riresume = ViIS as in the
theorem.

464

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

References
Alkoby, S., Sarne, D., & Das, S. (2015). Strategic free information disclosure for search-based
information platforms. In Proceedings of the 2015 International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2015), pp. 635643.
Anshelevich, E., Das, S., & Naamad, Y. (2013). Anarchy, stability, and utopia: creating better
matchings. Autonomous Agents and Multi-Agent Systems, 26(1), 120140.
Aumann, R. (1998). On the centipede game. Games and Economic Behavior, 23(1), 97105.
Axelrod, R. (1984). The evolution of cooperation. Basic Books.
Bakos, Y. (1997). Reducing buyer search costs: Implications for electronic marketplaces. Management Science, 42, 16761692.
Bellman, R. (1957). A Markovian decision process. Indiana University Mathematics Journal, 6(4),
679684.
Bernstein, D., Givan, D., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized
control of Markov decision processes. Mathematics of Operations Research, 27(4), 819840.
Breban, S., & Vassileva, J. (2001). Long-term coalitions for the electronic marketplace. In Proceedings of the E-Commerce Applications Workshop, Canadian AI Conference.
Burdett, K., & Malueg, D. A. (1981). The theory of search for several goods. Journal of Economic
Theory, 24(3), 362376.
Carlson, J. A., & McAfee, R. P. (1984). Joint search for several goods. Journal of Economic Theory,
32(2), 337345.
Chhabra, M., Das, S., & Sarne, D. (2014a). Competitive information provision in sequential search
markets. In Proceedings of the International conference on Autonomous Agents and MultiAgent Systems (AAMAS 2014), pp. 565572.
Chhabra, M., Das, S., & Sarne, D. (2014b). Expert-mediated sequential search. European Journal
of Operational Research, 234(3), 861873.
Chhabra, M., & Das, S. (2011). Learning the demand curve in posted-price digital goods auctions.
In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2011), pp. 6370.
Conitzer, V. (2012). Computing game-theoretic solutions and applications to security. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, pp. 21062112.
de Jong, S., & Tuyls, K. (2011). Human-inspired computational fairness. Autonomous Agents and
Multi-Agent Systems, 22(1), 103126.
de Jong, S., Tuyls, K., & Verbeeck, K. (2008). Fairness in multi-agent systems. Knowledge Engineering Review, 23(2), 153180.
Dutta, P., & Sen, S. (2003). Forming stable partnerships. Cognitive Systems Research, 4(3), 211
221.
Elmalech, A., Sarne, D., & Grosz, B. J. (2015). Problem restructuring for better decision making in
recurring decision situations. Autonomous Agents and Multi-Agent Systems, 29(1), 139.
465

fiROCHLIN & S ARNE

Elmalech, A., & Sarne, D. (2012). Evaluating the applicability of peer-designed agents in mechanisms evaluation. In Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology-Volume 02, pp. 374381.
Endriss, U., Kraus, S., Lang, J., & Wooldridge, M. (2011). Designing incentives for boolean games.
In Proceedings of the 10th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2011), pp. 7986.
Fotakis, D., Karakostas, G., & Kolliopoulos, S. G. (2010). On the existence of optimal taxes for network congestion games with heterogeneous users. In Proceedings of the Third international
conference on Algorithmic game theory (SAGT10), pp. 162173.
Gatti, J. (1999). Multi-commodity consumer search. Journal of Economic Theory, 86(2), 219244.
Grosfeld-Nir, A., Sarne, D., & Spiegler, I. (2009). Modeling the search for the least costly opportunity. European Journal of Operational Research, 197(2), 667674.
Hajaj, C., Hazon, N., Sarne, D., & Elmalech, A. (2013). Search more, disclose less. In Proceedings
of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2013).
Hajaj, C., & Sarne, D. (2014). Strategic information platforms: selective disclosure and the price of
"free". In ACM Conference on Economics and Computation (EC14), pp. 839856.
Hazon, N., Aumann, Y., Kraus, S., & Sarne, D. (2013). Physical search problems with probabilistic
knowledge. Artificial Intelligence, 196, 2652.
Hendrix, P., & Sarne, D. (2007). The effect of mediated partnerships in two-sided economic search.
In Klusch, M., Hindriks, K., Papazoglou, M., & Sterling, L. (Eds.), Cooperative Information Agents XI (CIA 2007), Vol. 4676 of Lecture Notes in Computer Science, pp. 224240.
Springer Berlin Heidelberg.
Ito, T., Ochi, H., & Shintani, T. (2002). A group-buy protocol based on coalition formation for
agent-mediated e-commerce. International Journal of Computing and Information Sciences,
3(1), 1120.
Janssen, M. C. W., Moraga-Gonzalez, J. L., & Wildenbeest, M. R. (2005). Truly costly sequential
search and oligopolistic pricing. International Journal of Industrial Organization, 23(5-6),
451466.
Kephart, J., & Greenwald, A. (2002). Shopbot economics. Journal of Autonomous Agents and
Multi-Agent Systems, 5(3), 255287.
Kraus, S., Shehory, O., & Taase, G. (2003). Coalition formation with uncertain heterogeneous
information. In Proceedings of the Second International Conference on Autonomous Agents
and Multi-agent Systems (AAMAS 2003), pp. 18.
Landsberger, M., & Peled, D. (1977). Duration of offers, price structure, and the gain from search.
Journal of Economic Theory, 16(1), 1737.
Lippman, S., & McCall, J. (1976). The economics of job search: A survey. Economic Inquiry,
14(3), 347368.
Manisterski, E., Sarne, D., & Kraus, S. (2008). Enhancing cooperative search with concurrent
interactions. Journal of Artificial Intelligence Research (JAIR), 32(1), 136.
466

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Mash, M., Rochlin, I., & Sarne, D. (2012). Join me with the weakest partner, please. In Proceedings
of the 2012 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT
2012), pp. 1724.
Masters, A. M. (1999). Wage posting in two-sided search and the minimum wage. International
Economic Review, 40(4), 809826.
McKelvey, R., & Palfrey, T. (1992). An experimental study of the centipede game. Econometrica,
60(4), 803836.
McMillan, J., & Rothschild, M. (1994). Search. In Proceedings of Handbook of Game Theory with
Economic Applications, pp. 905927.
Morgan, P. (1983). Search and optimal sample sizes. Review of Economic Studies, 50(4), 659675.
Morgan, P., & Manning, R. (1985). Optimal search. Econometrica, 53(4), 923944.
Nagel, R., & Tang, F. (1998). Experimental results on the centipede game in normal form: an
investigation on learning. Journal of Mathematical Psychology, 42(2-3), 356384.
Nahum, Y., Sarne, D., Das, S., & Shehory, O. (2015). Two-sided search with experts. Autonomous
Agents and Multi-Agent Systems, 29(3), 364401.
Penn, M., Polukarov, M., & Tennenholtz, M. (2009a). Random order congestion games. Mathematics of Operations Research, 34(3), 706725.
Penn, M., Polukarov, M., & Tennenholtz, M. (2009b). Taxed congestion games with failures. Annals
of Mathematics and Artificial Intelligence, 56(2), 133151.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley-Interscience.
Rochlin, I., Aumann, Y., Sarne, D., & Golosman, L. (2014). Efficiency and fairness in team search
with self-interested agents. In Proceedings of the International conference on Autonomous
Agents and Multi-Agent Systems (AAMAS 2014), pp. 365372.
Rochlin, I., & Sarne, D. (2014a). Constraining information sharing to improve cooperative information gathering. In Proceedings of the 13th International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2014), pp. 237244.
Rochlin, I., & Sarne, D. (2014b). Utilizing costly coordination in multi-agent joint exploration.
Multiagent and Grid Systems, 10(1), 2349.
Rochlin, I., Sarne, D., & Laifenfeld, M. (2012). Coordinated exploration with a shared goal in costly
environments. In Proceedings of the ECAI 2012 - 20th European Conference on Artificial
Intelligence. Including Prestigious Applications of Artificial Intelligence (PAIS-2012) System
Demonstrations Track, pp. 690695.
Rochlin, I., Sarne, D., & Mash, M. (2014). Joint search with self-interested agents and the failure
of cooperation enhancers. Artificial Intelligence, 214, 4565.
Rochlin, I., Sarne, D., & Zussman, G. (2011). Sequential multilateral search for a common goal.
In Proceedings of the 2011 IEEE/WIC/ACM International Conference on Intelligent Agent
Technology (IAT 2011), pp. 349356.
Rochlin, I., Sarne, D., & Zussman, G. (2013). Sequential multi-agent exploration for a common
goal. Web Intelligence and Agent Systems, 11(3), 221244.
467

fiROCHLIN & S ARNE

Rochlin, I., & Sarne, D. (2013). Information sharing under costly communication in joint exploration. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, pp.
847853.
Rothschild, M. (1974). Searching for the lowest price when the distribution of prices is unknown.
Journal of Political Economy, 82(4), 689711.
Saad, W., Han, Z., Debbah, M., & Hjorungnes, A. (2009). Coalitional games for distributed collaborative spectrum sensing in cognitive radio networks. In Proceedings of the IEEE INFOCOM,
pp. 21142122.
Sarne, D. (2013). Competitive shopbots-mediated markets. ACM Transactions on Economics and
Computation, 1(3), 17.
Sarne, D., & Arponen, T. (2007). Sequential decision making in parallel two-sided economic search.
In Proceedings of the Sixth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2007), p. 69.
Sarne, D., & Aumann, Y. (2014). Exploration costs as a means for improving performance in
multiagent systems. Annals of Mathematics and Artificial Intelligence, 72(3-4), 297329.
Sarne, D., & Kraus, S. (2005). Cooperative exploration in the electronic marketplace. In Proceedings of the Twentieth National Conference on Artificial Intelligence and the Seventeenth
Innovative Applications of Artificial Intelligence Conference (AAAI 2005), pp. 158163.
Sarne, D., Manisterski, E., & Kraus, S. (2010). Multi-goal economic search using dynamic search
structures. Autonomous Agents and Multi-Agent Systems, 21(1-2), 204236.
Sarne, D., & Kraus, S. (2003). The search for coalition formation in costly environments. In
Proceedings of the Cooperative Information Agents VII, 7th International Workshop, CIA,
pp. 117136.
Sarne, D., & Kraus, S. (2008). Managing parallel inquiries in agents two-sided search. Artificial
Intelligence, 172(4-5), 541569.
Selten, R., & Stoecker, R. (1986). End behavior in sequences of finite prisoners dilemma supergames a learning theory approach. Journal of Economic Behavior & Organization, 7(1),
4770.
Sen, S. (1996). Reciprocity: a foundational principle for promoting cooperative behavior among
self-interested agents. In Proceedings of the Second International Conference on Multi-Agent
Systems, pp. 322329.
Shehory, O., & Kraus, S. (1998). Methods for task allocation via agent coalition formation. Artificial
Intelligence, 101(1-2), 165200.
Smith, L. (2011). Frictional matching models. Annual Reviews in Economics, 3(1), 319338.
Stone, P., & Kraus, S. (2010). To teach or not to teach? decision making under uncertainty in ad
hoc teams. In Proceedings of the Ninth International Conference on Autonomous Agents and
Multiagent Systems (AAMAS 2010), pp. 117124.
Tang, Z., Smith, M. D., & Montgomery, A. (2010). The impact of shopbot use on prices and
price dispersion: Evidence from online book retailing. International Journal of Industrial
Organization, 28(6), 579590.
468

fiC ONSTRAINING I NFORMATION S HARING TO I MPROVE C OOPERATIVE I NFORMATION G ATHERING

Tsvetovat, M., Sycara, K., Chen, Y., & Ying, J. (2000). Customer coalitions in electronic markets.
In Proceedings of Agent-Mediated Electronic Commerce III, Current Issues in Agent-Based
Electronic Commerce Systems (includes revised papers from AMEC 2000 Workshop), pp.
121138.
Waldeck, R. (2008). Search and price competition. Journal of Economic Behavior and Organization, 66(2), 347357.
Walker, J., & Halloran, M. (2004). Rewards and sanctions and the provision of public goods in
one-shot settings. Experimental Economics, 7(3), 235247.
Yamamoto, J., & Sycara, K. (2001). A stable and efficient buyer coalition formation scheme for
e-marketplaces. In Proceedings of the 5th international conference on Autonomous agents
(AGENTS 01), pp. 576583.

469

fiJournal of Artificial Intelligence Research 54 (2015) 593-629

Submitted 07/15; published 12/15

Compressing Optimal Paths with Run Length Encoding
Ben Strasser

STRASSER @ KIT. EDU

Karlsruhe Institute of Technology
Karlsruhe, Germany

Adi Botea

ADIBOTEA @ IE . IBM . COM

IBM Research
Dublin, Ireland

Daniel Harabor

DANIEL . HARABOR @ NICTA . COM . AU

NICTA
Sydney, Australia

Abstract
We introduce a novel approach to Compressed Path Databases, space efficient oracles used to
very quickly identify the first edge on a shortest path. Our algorithm achieves query running times
on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from
the literature. Space consumption is competitive, due to a compression approach that rearranges
rows and columns in a first-move matrix and then performs run length encoding (RLE) on the
contents of the matrix. One variant of our implemented system was, by a convincing margin, the
fastest entry in the 2014 Grid-Based Path Planning Competition.
We give a first tractability analysis for the compression scheme used by our algorithm. We
study the complexity of computing a database of minimum size for general directed and undirected
graphs. We find that in both cases the problem is NP-complete. We also show that, for graphs which
can be decomposed along articulation points, the problem can be decomposed into independent
parts, with a corresponding reduction in its level of difficulty. In particular, this leads to simple and
tractable algorithms with linear running time which yield optimal compression results for trees.

1. Introduction
A Compressed Path Database (CPD) is an index-based data-structure for graphs that is used to very
quickly answer first-move queries. Such a query takes as input a pair of nodes, namely a source node
s and a target node t, and asks for the first edge on a shortest st-path (i.e., a path from s to t). CPDs
have successfully been applied in a number of contexts important to AI. For instance, Copa (Botea,
2012), a CPD-based pathfinding algorithm, was one of the joint winners in the 2012 edition of the
Grid-Based Path Planning Competition, or shorter GPPC (Sturtevant, 2012b). A related algorithm,
MtsCopa, is a fast method for moving target search over known and partially known terrain (Botea,
Baier, Harabor, & Hernandez, 2013; Baier, Botea, Harabor, & Hernandez, 2014).
Given a graph G = (V, E), a trivial CPD consists of a square matrix m with dimensions
|V |  |V |. The matrix m, constructed during a precomputation step, stores in each cell m[s, t] the
identity of a first edge on a shortest st-path. We call this a first-move matrix. By convention we
say that rows of m correspond to fixed source nodes and the columns to fixed target nodes. This is
optimal in terms of query time but the O(|V |2 ) space consumption quickly becomes prohibitive for
larger graphs. The challenge is to design a compact representation of m that trades a small increase
in query times for a large decrease in space consumption.
c
2015
AI Access Foundation. All rights reserved.

fiS TRASSER , B OTEA , & H ARABOR

A number of different techniques to compress a first-move matrix have been suggested for this
purpose (Sankaranarayanan, Alborzi, & Samet, 2005; Botea, 2011; Botea & Harabor, 2013a). In
each case the objective is to conserve space by grouping together entries of m which all share a
common source node and which all store the same first-edge information.
In this work we present the Single-Row-Compression (SRC) and the Multi-Row-Compression
(MRC) indexing algorithms for compressing all-pairs shortest paths. In 2014s GPPC, SRC outperformed all competitors in terms of query running time. The contributions presented in this article
go in three main directions: a new approach to compressing a first-move matrix; experiments that
demonstrate advancing state-of-the-art in terms of both response time and memory consumption;
and a thorough theoretical analysis, discussing NP-hardness results and islands of tractability.
We introduce a new matrix compression technique based on run-length encoding (RLE). The
main idea of our algorithm is simple: we compute an order for the nodes in the input graph and
assign numeric IDs to nodes (e.g., from 1 to |V |) in this order. The purpose of the ordering is that
nodes which are located in close proximity in the graph have a small ID difference. This ordering
is used to order the rows and the columns of a first-move matrix, which is also computed during
preprocessing. Then, we apply run-length encoding (RLE) to each row of the first-move matrix. We
study three types of heuristic orderings: graph-cut order, depth-first order and input-graph order. We
also study two types of run-length encoding. The first involves a straightforward application of the
algorithm to each row. The second type is a more sophisticated multi-row scheme that eliminates
redundancies between adjacent RLE-compressed rows. To answer first-move queries we employ a
binary search on a fragment on the compressed result.
We undertake a detailed empirical analysis including comparisons of our techniques with stateof-the-art variants of CPDs (Botea, 2012), and Hub-Labeling (Delling, Goldberg, Pajor, & Werneck,
2014). Copa is a recent and very fast CPD oracle which was among the joint winners at the 2012
International Grid-Based Path Planning Competition (GPPC). Using a variety of benchmarks from
the competition we show that our techniques improve on Copa, both in terms of storage and query
time. Hub-Labeling is a technique initially developed to speedup queries on roads, but they also
work on other graphs, such as gridmaps. Hub-Labeling is to the best of our knowledge the fastest
technique known on roads. In experiments, we show that our approach leads to better query times
than Hub-Labeling on graphs where we can reasonably compute m.
As our technique relies on an all-pairs-shortest-path pre-computation, it plays a tradeoff between
its query-response speed, the preprocessing time and the memory required to store the compressed
path database. Thus, our algorithm is faster, but it also requires a larger preprocessing time and
more memory than some of the other techniques from the literature. In other words, when memory
and preprocessing time are available, our technique will provide a state-of-the-art speed performance. On the other hand, when larger and larger graphs create a memory and preprocessing time
bottleneck, other techniques should be considered. See a detailed comparison in the experiments
section.
In the theoretical analysis, we formally define and study optimal RLE-compression of first-move
matrices produced from input graphs. We consider the case of directed input graphs and the case of
undirected weighted input graphs. We show that both versions are NP-complete. Focusing on such
distinct types of graphs, each result brings something new compared to the other. Related (Kou,
1977; Oswald & Reinelt, 2009) and weaker, less specific (Mohapatra, 2009) results on RLE-based
matrix compression are available in the literature. However, as known, the NP-hardness of a class
of problems does not necessarily imply the NP-hardness of a subset of the class. Thus, despite
594

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

previous related results (Mohapatra, 2009), it has been an open question whether the optimal RLEcompression of a first-move matrix computed from an input graph is tractable.
We also show that, for graphs which can be decomposed along articulation points, the problem
can be decomposed into independent subproblems. If optimal orderings are available for the subproblems, a global optimal ordering can easily be obtained. In particular, a depth-first preorder is
optimal on trees, and the general ordering problem is fixed-parameter tractable in the size of the
largest 2-connected component.
Our approach and part of the evaluation have previously been reported in a shorter conference
paper (Strasser, Harabor, & Botea, 2014). The theoretical analysis was the topic of another conference paper (Botea, Strasser, & Harabor, 2015). Putting these together in the current submission
provides a unique source that describes our method, its performance and its theoretical properties.
Compared to the previous conference papers, we now provide complete proofs for all the theoretical
results. We have included more details and more examples in the presentation, for a better clarity.
We report additional results, such as the performance in the pathfinding competition GPPC 2014,
which were originally published in a paper about the competition (Sturtevant, Traish, Tulip, Uras,
Koenig, Strasser, Botea, Harabor, & Rabin, 2015).

2. Related Work
Many techniques from the literature can be employed in order to quickly answer first-move queries.
Standard examples include optimal graph search techniques such as Dijkstras algorithm (Dijkstra,
1959) and A* (Hart, Nilsson, & Raphael, 1968). Significant improvements over these methods can
be achieved by preprocessing the input graph, as done in CPDs, for instance. As shortest paths
have numerous applications in various fields, a plethora of different preprocessing-based algorithms
have been proposed. For an overview, we refer the interested reader to a recent survey article (Bast,
Delling, Goldberg, MullerHannemann, Pajor, Sanders, Wagner, & Werneck, 2015). A common
approach consists of adding online pruning rules to Dijkstras algorithm, which rely on data computed in the preprocessing phase, significantly reducing the explored graphs size. As this approach
significantly differs from the technique described in this paper, we omit the details and refer the
interested reader to the aforementioned survey article.
SILC (Sankaranarayanan et al., 2005) and Copa (Botea & Harabor, 2013a) are CPD-based techniques for a fast first-move computation. SILC employs a recursive quad-tree mechanism for compression while Copa uses a simpler and more effective (Botea, 2011) decomposition with rectangles.
Hub Labels (HL) were initially introduced as 2-Hop Labels (Cohen, Halperin, Kaplan, & Zwick,
2002). For nearly a decade there has not been much research on the topic, until Abraham, Delling,
Goldberg, and Werneck (2011) showed that the technique is practical on huge road networks, and
coined the term Hub Labels. This realization drastically increased the interest in HL and thus
spawned numerous follow up works, such as (Abraham, Delling, Goldberg, & Werneck, 2012;
Delling, Goldberg, & Werneck, 2013; Abraham, Delling, Fiat, Goldberg, & Werneck, 2012; Akiba,
Iwata, & Yoshida, 2013). In our context, the most relevant one is probably RXL (Delling et al.,
2014), which is a HL variant. The authors show that their algorithm works well not only on road
graphs but on a variety of graphs from different sources including graphs derived from maps used
during GPPC. We compare our algorithm against RXL.
The HL index consists of a forward and backward label for each node, that contains a list of hub
nodes and the exact distances to them. For each st-pair there must exist a meeting hub h that is a
595

fiS TRASSER , B OTEA , & H ARABOR

forward hub of s and a backward hub of t and is on a shortest st-path. A shortest distance query
from a node s to a node t is answered by enumerating all common hubs of s and t. A labeling is
good if all labels only contain very few hubs. Computing a labeling minimizing the index size is
NP-hard (Babenko, Goldberg, Kaplan, Savchenko, & Weller, 2015).
Most works do not consider HL in its most general form, but consider a more restrictive variant
called Hierarchical Hub Labels (HHL). This term was introduced by Abraham et al. (2012) but
labels used in previous work (Abraham et al., 2011) were already hierarchical. A labeling is called
hierarchical if an ordering of the vertices exists, such that every hub h of a vertex v comes after v
in the order. Given a fixed node order, an optimal labeling can be computed efficiently (Abraham
et al., 2012). The difficult task with HHL consists of computing the node order. Computing a node
order minimizing the index size is also an NP-hard task (Babenko et al., 2015).
HHL is deeply coupled with a different popular speedup technique for shortest path computations called Contraction Hierarchies (CH) (Geisberger, Sanders, Schultes, & Delling, 2008). CH
does not achieve the query speeds of HHL but has significantly smaller index sizes. However, for
most applications even CH query times are already faster than necessary, which makes CH a very
strong competitor. CH iteratively contracts nodes while inserting shortcuts to maintain all shortest
path distances in the remaining graph. By following the so inserted shortcuts only a small fraction
of the graph needs to be explored from every node. A node order is good for CH if the search
spaces of every node is small. Again, computing an optimal order is NP-hard (Bauer, Columbus,
Katz, Krug, & Wagner, 2010). The first HL paper on road graphs (Abraham et al., 2011) computed
the label of v by explicitly storing all nodes reachable from v in the CH search space and then applying some pruning rules. Later papers have refined these rules, but every hierarchical label can be
viewed as an explicitly stored and pruned CH search space. A consequence is that node orders that
are good for CH are also good for HHL and vice versa, even though the formal optimization
criteria differ and therefore an optimal order for one of them with respect some criterion can be
slightly suboptimal for the other.
The node orders used for HHL and the original CH depend on the weights on the input graph.
Substantial changes to the weights requires recomputing the node ordering. More recent work
(Bauer, Columbus, Rutter, & Wagner, 2013; Dibbelt, Strasser, & Wagner, 2014) has introduced
Customizable Contraction Hierarchies (CCH) and shown that node orders exist and work well that
only depend on the structure of the input graph. These node orders exploit that the input graph has
small balanced node-separators or has a comparative small treewidth.
In our paper we also consider two types of node orders. The first is a depth first search preorder
and the second is based on small balanced edge-cuts. They are thus also independent of the input
graphs weights. However, do not confuse our orders with the CCH node orders. They are not
interchangeable. Using a CCH ordering will result in bad performance with our technique, just as
using one of our node orders with CCH will not work well. In fact, using a preorder with CCH
maximizes the maximum search space in terms of vertices instead of minimizing it. That is, an
order that works well with our technique is a CCH worst case node order. Further, our orders can
also not be interchanged with the weight-dependent orders needed for HHL and CH.
As described in the literature, HL answers distance queries. However, as hinted by Abraham
et al. (2012), it is easy to extend hub labels to first move queries. To achieve this, the entries in
the forward and backward labels are extended with a third component: a first move edge ID. If h
is a forward hub of s then the corresponding entry is extended using the first edge ID of a shortest
sh-path. If h is backward hub of t then the entry is extended with the first edge of a shortest ht-path.
596

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

For a st-query first the corresponding meeting hub h is determined. If s 6= h then the first move is
the edge ID stored in the forward label s and otherwise the first move is contained in the backward
label of t. This slightly increases memory consumption but should have a negligible impact on
performance. Note that distance values are needed even if one only wishes to compute first-moves
as we need the distances to determine the right hub if s and t have several hubs in common.
In the context of program analysis it is sometimes desirable to construct an oracle that determines if a particular section of code can ever be reached. PWAH (van Schaik & de Moor, 2011) is
one such example. Similarly to our work, the authors precompute a quadratic matrix and employ
a compression scheme based on run-length encoding. The main difference is that such reachability
oracles only return a yes-no answer for every query rather than the identity of a first-edge.
Another speedup technique with low average query times is Transit Node Routing (TNR) (Bast,
Funke, & Matijevic, 2009; Bast, Funke, Matijevic, Sanders, & Schultes, 2007; Antsfeld, Harabor,
Kilby, & Walsh, 2012). However, two independent studies (Abraham et al., 2011; Arz, Luxen, &
Sanders, 2013) have come to the conclusion that (at least on roads) TNR is dominated by HL in
terms of query time. Further, TNR does not optimize short range queries. A scenario that often
arises is a unit that chases another unit. In most situations both units tend to be very close, which
results in many short range queries. TNR is rather ineffective in this scenario.
Bulitko, Bjornsson, and Lawrence (2010) present a subgoal-based approach to pathfinding. Similarities to our work include a preprocessing stage where paths on a map are precomputed, after
which the results are compressed and stored into a database. The database is used to speed up the
response time when a path query is posed to the system. There are substantial differences between
the two approaches as well. Our method precomputes all-pairs shortest paths, eliminating graph
search entirely in the production mode (i.e., the stage where the system is queried to provide full
shortest paths or fragments of shortest paths). In contrast, Bulitko et al. restrict their precomputed
database to a subset of nodes, which in turn requires some additional search in the production mode.
The compression method is different in each case. Our system provides optimal paths, which is not
guaranteed in the case of Bulitko et al.s method. Besides Bulitko et al. (2010) work, pathfinding
with sub-goals turned out to be a popular and successful idea in more recent work (Hernandez &
Baier, 2011; Bulitko, Rayner, & Lawrence, 2012; Lawrence & Bulitko, 2013; Uras, Koenig, &
Hernandez, 2013).
Pattern databases (PDBs) (Culberson & Schaeffer, 1998) are lookup tables that provide heuristic
estimations of the true distance from a search node to a goal state. They are obtained by abstracting
an original search space into a smaller space. Optimal distances in the abstracted space, from every
state to a pre-established goal, are precomputed and stored into the pattern database as estimations
of the distances in the original space. As such, both techniques are memory-based enhancements
in problems where the solution can be represented as a path in a graph. There are several key
distinctions between PDBs and CPDs. PDBs are lossy abstractions, and they are specific to a goal
or subset of goals. CPDs are lossless compressions, and encode shortest paths for every starttarget
pair. Given their lossy nature, PDBs need to be used as heuristic within in a search algorithm, such
for example as A*, as opposed to a complete and optimal method on its own. PDBs are commonly
used in large graphs, such as implicitly defined search spaces, where exploring the entire graph in
the preprocessing is impractical. In PDBs, the coarseness of the abstraction impacts the accurracy of
heuristic estimations. A finer abstraction has a better quality, but it can also result in a larger PDB.
Work on addressing this bottleneck include compressing pattern databases (Felner, Korf, Meshulam,
597

fiS TRASSER , B OTEA , & H ARABOR

& Holte, 2007; Samadi, Siabani, Felner, & Holte, 2008). In contrast, CPDs compress all-pairs
shortest paths.

3. Preliminaries
We denote by G = (V, E) a graph with node set V and edge1 set E  V  V . We denote by
deg(u) the number of outgoing edges of u.2 The maximum out-degree is denoted by . A node
order o : V  [1, |V |] assigns to every node v a unique node ID o(v). The out-going edges of every
node are ordered in an arbitrary but fixed order and their position (index in the ordering) is referred
to as their out-edge ID.
Further, there is a weight function w : E  R>0 3 . An st-path is a sequence of edges a1 . . . ak
such that a1 starts at s and ak ends at tP
and for every i the edge ai ends at the node where ai+1
starts. The weight (or cost) of a path is i w(ai ). An st-path is shortest if no other st-path exists
with a strictly smaller weight. The distance between two nodes s and t is the weight of a shortest
st-path, if one exists. If no st-path exists, the distance is . Notice that there may be multiple
shortest st-paths but all of them have the same weight.
Without loss of generality we can assume that no duplicate edges (multi-edges) exist in our
graphs, as if there were, we could just drop all but a shortest edge, as the other edges are not used
by any shortest path. Further, using a similar argument, we can assume without loss of generality
that no reflexive loops exist.
For s 6= t, an st-first-move is the first edge of a shortest st-path. If there are multiple shortest
st-paths, there may also be multiple st-first-moves. If no st-path exists, no st-first-move exists. The
formal problem we consider is the following: Given a pair of nodes s and t, find an st-first-move.
If there are several valid first-moves, the algorithm can freely choose which to return.
Given a oracle that answers first move queries, we can easily extract shortest paths. Compute
an st-first move a. In other words, a is the first edge of the shortest path. Next, set s to the end of a.
As long as s 6= t, apply this procedure iteratively. Notice, that this only works as edge weights are
guaranteed to be non-zero. If we allowed zero-weights, we could run into an infinite-loop problem,
as the following example illustrates: Consider a graph G with two nodes x and y connected by edges
xy and yx with weights zero. Denote by t some other node in G. A valid xt-first-move is using xy.
Further a valid yt-first-move is using yx. If the oracle always returned these two first-moves, our
path extraction algorithm would oscillate between x and y and would not terminate.
A depth first search (DFS) is a way of traversing a graph and constructing a special sort of
spanning tree using backtracking. A depth-first preorder is a node order that orders nodes in the
way that a DFS first sees them. The search is parameterized on the root node and on the order
in which the neighbors of each node are visited. In this work we will regularly refer to depthfirst preorders without stating these parameters. We always implicitly assume that the root is some
arbitrary node and that the neighbors are visited in some arbitrary order.
1. The term arc is also used in the literature. Sometimes, the distinction is made on whether the graph is directed (in
which case some authors prefer to say arcs) or undirected. In this paper, we stick with the term edge in all cases.
2. In a directed graph, every ordered pair (u, v)  E is an outgoing edge of u. In an undirected graph, every edge
incident to u is an outgoing edge of u.
3. We assume that this is a function E  R>0 to be able to apply Dijkstras algorithm during the preprocessing phase.
However, one could consider arbitrary weights without negative cycles and replace every occurrence of Dijkstras
algorithm with the algorithm of Bellman and Ford (Bellman, 1958; Ford, 1956).

598

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

Run length encoding (RLE) compresses a string of symbols by representing more compactly
substrings, called runs, consisting of repetitions of the same symbol. For instance, string aabbbaaa
has three runs, namely aa, bbb, and aaa. A run is replaced with a pair that contains the start and the
value of the run. The start is the index of the first element in the substring, whereas the value is the
symbol contained in the substring. In our example, the first run aa has the start 1 and the value a.
Run bbb has the start 3 and the value b, whereas the last run has the start 6 and the value a.4
When the first and the last run have the same value, there is no need to encode both. The first
run can easily be reconstructed in constant time in this case. First, decide whether the first run has
been removed or not, this can be done by checking if the first run among the preserved ones has the
start equal to 1. Secondly, if needed, reconstruct the first run, using 1 as a start position and a value
equal to the value of the last encoded run. Another way of looking at this is that, if the first and
the last run have the same value, we can allow them to merge, as if we wrapped around the string
to form a cycle. When we allow this, we say we are using cyclic runs. Otherwise (never consider
merging the ends of a string), we say we use sequential runs. See Example 1 below.
Given an ordered sequence of elements (string), we say that two positions are: adjacent if they
are next to each other; cyclic-adjacent if they are adjacent or one is the first and the other is the last
position in the ordering; separated otherwise.
Let  be an ordered sequence of elements (symbols) over a dictionary (or alphabet) . Given a
symbol   , let an -run be an RLE run containing symbol . For every string , we denote by
N () the total number of occurrences of symbol  in . Further, the number of sequential -runs
is denoted by Rs () and the number of of cyclic by Rc (). Notice that 0  Rs ()Rc ()  1. In
other words, the number of sequential runs and the number of cyclic runs never differ by more than
1. Finally, we denote by Rs () the total number of sequential runs and by Rc () the total number
of cyclic runs. In this paper, we assume that first-move compression uses cyclic runs, unless we
explicitly say otherwise.
Example 1. Consider again the string  = aabbbaaa. Compressing  yields 1, a; 3, b; 6, a. This
means that after position 1 the string consists of as. Similarily after position 3 there are bs and
finally after position 6 all elements are as until the string ends. We have Na () = 5 and Nb () = 3.
There are three sequential runs, namely aa, bbb and aaa. The first and the third ones are a-runs,
whereas the middle one is a b-run. Thus, we have Ras () = 2, Rbs () = 1, and Rs () = 2 + 1 = 3.
At the same time,  has one cyclic a-run. Indeed, if we put next to each other the two ends of
the string, as if the string were cyclic, all occurrences of a in the string become one solid block (i.e.,
one cyclic a-run). Thus, Rac () = 1, Rbc () = 1, and Rc () = 1 + 1 = 2.

4. Basic Idea
As mentioned in the introduction, our algorithm starts by building a |V |  |V | all-pairs first-move
matrix m. The entry at position m[i, j] is an ij-first-move. The central idea of our algorithm is
to compress each row of m using RLE. The compression is performed gradually, as the matrix
rows are being computed, so that the uncompressed matrix does not have to be kept in memory. To
answer an st-first-move query, we run a binary search for t in the row of s. However, to achieve a
good compression ratio, we first reorder the columns of m to decrease the total number of runs. As
the columns correspond to nodes, we can regard the problem of reordering the columns as a problem
4. Alternative encodings exist, such as the value followed by the run length. E.g., a, 2; b, 3; a, 3 in the example.

599

fiS TRASSER , B OTEA , & H ARABOR

1
b,5

a,2
e,3
3

c,3
5

d,6
(a) Input

2
f ,4
4

t
s 12345
1a a a a
2 ae f e
3 e ed c
4 f f dd
5 c c c c
(b) First-Move Matrix

1
2
3
4
5

1/a
1/a 3/e 4/f 5/e
1/e 4/d 5/c
1/f 3/d
1/c

(c) Compressed Path Database

Figure 1: A toy example of our algorithm
of computing a good node order. Computing an optimal node order that minimizes the number of
runs is NP-hard, as we show in our theoretical analysis. Fortunately, a simple depth-first preorder
works well in practice.
Sometimes, in a formal analysis, some technical details are annoying in the sense that they
can make the presentation somewhat more complicated. The question of what symbol we should
use for m[i, i] is such an example. In our practical implementation, we say that we do not care about
the symbol, as we never query for it. To reduce the number of runs we therefore assign it either the
value of m[i  1, i] or m[i + 1, i]. In our theoretical analysis, we make a similar assumption (i.e.,
a dont care symbol) in Sections 5 and 6. As we will state in Section 7, the assumption there is
that m[i, i] is a symbol different from any edge symbol. In every case, our assumptions have the
purpose of keeping the analysis as simple as possible.
Example 2. Figure 1a shows a toy weighted and undirected graph, with 5 nodes and 6 edges. For
each edge, we show its weight (cost), as a number, and a unique label, as a letter. A first-move
matrix m of this graph, corresponding to the node ordering 1, 2, 3, 4, 5, is shown in Figure 1b.
Recall that the entry m[r, c], where r is a row and c is a column, is the id of the first move of a
shortest path from node r to node c. For example, m[3, 1] = e because e is the first step of ea,
an optimal path from node 3 to node 1. Another optimal path would be the single-step path b, as
both ea and b have an optimal weight (cost) of 5. Thus, we are free to choose between m[3, 1] = e
and m[3, 1] = b. We prefer e because this leads to a better compression of row 3 in m, since the
first two symbols of the third row, being identical, will be part of the same RLE run. We will show
in Section 9 that breaking such ties in an optimal way is feasible and computationally easy. The
compression of m for the given node ordering (or equivalently, matrix column ordering) is shown
in Figure 1c.
Notice that the ordering of the nodes impacts the size of a compressed matrix. In Example 2,
swapping nodes 3 and 4, as illustrated in Figure 2, would further reduce the number of RLE runs
on row 2, as the two e symbols will become adjacent. The total number of runs decreases from 11
runs to 10 runs. Thus, the challenge is to find an optimal or at least a good enough node ordering,
where the objective function is the size of the compressed first-move matrix.
The compression strategy with RLE illustrated in Example 2 is a key component of our approach. We study it theoretically in the next three sections, showing that computing an optimal
node ordering is NP-hard in general, and identifying tractability islands. We present a number of
effective heuristic node orderings in Section 8. A variant of our implemented method, called SRC,
600

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

1
b,5

a,2
e,3
4

c,3
5

d,6
(a) Input

2
f ,4
3

t
s 12345
1a a a a
2 af e e
3 f f d d
4 e e dc
5 c c c c
(b) First-Move Matrix

1
2
3
4
5

1/a
1/a 3/f 4/e
1/f 3/d
1/e 3/d 4/c
1/c

(c) Compressed Path Database

Figure 2: The same toy example as in Figure 1 but with a different node ordering (i.e., nodes 3 and
4 swapped).
performs the compression as illustrated in the example. Another version of our program, called
MRC, goes beyond the idea of compressing each row independently, implementing a multi-row
compression strategy. These are discussed in Section 9 and evaluated empirically in Section 12.

5. First-Move Compression for Directed Graphs
Recall that the ordering of the columns of a first-move matrix m affects the number of RLE runs in
the matrix. In this section we show that obtaining an optimal ordering is intractable in general when
the input graph is directed. Our construction works with uniform edge weights. For simplicitly we
therefore omit the weights in this section.
Definition 1. The FMComp-d (First Move CompressionDirected) problem:
Input: A directed graph G = (V, E); a matrix m of size |V |  |V | where each cell m[i, j] encodes
the first move on an optimal path from node i to node j; an integer k.
Question: Is there an ordering of the columns of m such that, if we apply RLE on each row, the
total number of cyclic RLE runs summed up for all rows is k?
Theorem 1. The FMComp-d problem is NP-complete.
Proof. It is easy to see that the problem belongs to NP, as a solution can be guessed and verified in
polynomial time.
The NP-hardness is shown as a reduction from the Hamiltonian Path Problem (HPP) in an
undirected graph. Let GH = (VH , EH ) be an arbitrary undirected graph, and define n = |VH |
and e = |EH |. Starting from GH , we build an instance of the FMComp-d problem. According to
Definition 1, such an instance includes a directed graph, which we call GF , the first-move matrix
m of GF , and a number.
GF = (VF , EF ) is defined as follows. For each node u  VH , define a node in VF . We call
these nodes in VF type-n nodes, to indicate they are created from original nodes in VH . For each
edge (u, v)  EH , define a new node nuv  VF (type-e nodes). For each new node nuv , define
two edges in EF , one from nuv to u and one from nuv and v. There are no other edges in EF . See
Figure 3 for an example.
Table 1 shows the first-move matrix of the running example. Given a type-n node u, all other
nodes are unreachable from u in the graph GF . Thus, the matrix row corresponding to u has only
601

fiS TRASSER , B OTEA , & H ARABOR

nxy

y

x

x

y

w

z

nxw
w

z

nwz
Figure 3: Left: sample graph GH . Right: GF built from GH . In GF , x, y, w, z are type-n nodes.
Nodes nij have type e.

x
y
w
z
nxy
nxw
nwz

x
2
2
2
0
0
2

y
2
2
2
1
2
2

w
2
2
2
2
1
0

z
2
2
2
2
2
1

nxy
2
2
2
2
2
2

nxw
2
2
2
2
2
2

nwz
2
2
2
2
2
2
-

Nr. cyclic runs
1
1
1
1
3
4
3

Table 1: First-move matrix for the running example. Both rows and columns follow the node
ordering x, y, w, z, nxy , nxw , nwz .
one non-trivial symbol,5 which we chose to be symbol 2, and which denotes that a node is not
reachable. Such rows have one RLE run each, regardless of the node ordering.
A matrix row corresponding to a type-e node nuv has three distinct (non-trivial) symbols in total:
one symbol for the edge to node u, another symbol for the edge to node v, and the non-reachable
symbol 2 for every other node. Without any generality loss, we use symbol 0 for the edge to u, and
symbol 1 for the edge to v. It is easy to see that, when nodes u and v are cyclic-adjacent in a given
ordering, the nuv s row has 3 RLE runs. When u and v are separated, the row will have 4 RLE runs.
See Table 1 for a few sample orderings.
We claim that HPP has a solution iff FMComp-d has a solution with 4e + 1 RLE runs. Let
vi1 , vi2 . . . , vin be a solution of HPP (i.e., a Hamiltonian path in GH ), and let P  EH be the set of
all edges included in this solution. We show that the node ordering in VF starting with vi1 , . . . , vin ,
followed by the type-e nodes in an arbitrary order, will result in 4e+1 = 3(n1)+4(en+1)+n
runs, with 3n  3 runs in total for the type-e rows6 corresponding to edges in P ; 4(e  n + 1) runs
in total for the remaining type-e rows; and n runs in total for the type-n rows.
5. By trivial symbol we mean the that dont care symbol . Recall that it has no impact on the number of runs. For
simplicity, we can safely ignore this symbol in our discussion.
6. We say that a row has a type-n (or type-e) iff its associated node has that type.

602

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

Indeed, for each edge (u, v)  P , the type-e row in m corresponding to node nuv  VF will
have 3 RLE runs, since u and v are adjacent in the ordering. There are n  1 edges in a Hamiltonian
path, with a total number of RLE runs of 3(n  1) for all these rows.
For an edge (u, v) 
/ P , the two nodes are separated and therefore the corresponding matrix row
will have 4 runs. This sums up to 4(e  n + 1) RLE runs for all rows corresponding to edges not
included in the Hamiltonian path.
Conversely, consider a node ordering that creates 4e + 1 = 3(n  1) + 4(e  n + 1) + n RLE
runs in total. We show that the ordering has all type-n nodes as a contiguous block,7 and that their
ordering is a Hamiltonian path in GH . This is equivalent to saying that there exist n  1 pairs of
type-n nodes u and v such that u and v are cyclic-adjacent in the ordering, and (u, v)  EH .
Here is a proof by contradiction. Assume there are only p < n  1 pairs of type-n nodes u and v
such that u and v are cyclic-adjacent in the ordering, and (u, v) is an edge in EH . For each of these
p pairs, the row corresponding to the type-e node nuv will have 3 RLE runs. The remaining e  p
type-e rows will be 4 RLE runs each. As mentioned earlier, the type-n rows have n runs in total,
regardless of the ordering. Thus, the total number of RLE runs is 3p + 4(e  p) + n = 4e  p + n >
4e  (n  1) + n = 4e + 1. Contradiction.

6. Compression for Undirected Weighted Graphs
We turn our attention to undirected weighted graphs, showing that computing an optimal ordering
is NP-complete.
Definition 2. The FMComp-uw problem (First Move CompressionUndirected, Weighted) is defined as follows.
Input: An undirected weighted graph G = (V, E); a matrix m of size |V |  |V | where a cell m[i, j]
stores the first move on an optimal path from node i to node j; an integer k.
Question: Is there an ordering of ms columns such that, if we apply run length encoding (RLE) on
each row, the total number of cyclic RLE runs in the matrix is at most k?
As a stepping stone in proving the NP-hardness of FMComp-uw, we introduce a problem that
we call SimMini1Runs (Definition 3), and prove its NP-completeness. SimMini1Runs is inspired
by the work of Oswald and Reinelt (2009), who have studied the complexity of a problem involving
the so-called k-augmented simultaneous consecutive ones property (C1Sk ) for a 0/1 matrix (i.e.,
a matrix with only two symbols, 0 and 1). By definition, a 0/1 matrix has the C1Sk property if,
after replacing at most k 1s with 0s, the columns and the rows of the matrix can be ordered so that,
for each row and for each column, all 1s on that row or column come as one contiguous block.
Oswald and Reinelt (2009) have proven that checking whether a 0/1 matrix has the C1Sk property
is NP-complete. Our proof for SimMini1Runs is related, as we point out later in the proof.
Given a 0/1 matrix o, an ordering of its columns, and an ordering of its rows, let the global
sequential 1-runs count Gs1 (o) be the number of sequential 1-runs summed over all rows and all
columns. That is,
X
Gs1 (o) =
R1s (),


7. Here, the notion of a contiguous block allows the case when part of the block is at the end of the sequence, and the
other part is at the beginning, as if the sequence were cyclic.

603

fiS TRASSER , B OTEA , & H ARABOR

o=

r1
r2



c1

c2

c3

0
1

1
0

1
1



Figure 4: Running example 0/1 matrix o. Rows are labelled as ri , whereas cj represent column
labels.
where  is iterated through os rows and columns. For instance, Gs1 (o) = 6 for the matrix o shown
in Figure 4.
Definition 3. The Simultaneous Mini 1-Runs (SimMini1Runs) problem is defined as follows.
Input: A 0/1 matrix o so that every row and column contain at least one value of 1; an integer k.
Question: Is there an ordering of the columns, and an ordering of the rows, so that Gs1 (o)  k?
Theorem 2. SimMini1Runs is NP-complete.
The proof is available in Appendix A.
Lemma 1. Let  be a 0/1 string so that it starts with a 0, or it ends with a 0, or both. Then
R1s () = R0c ().
Proof. Case (i):  starts with a 0 and ends with a 1. As the two end symbols are different, sequential
runs and cyclic runs are identical. As 0-runs and 1-runs alternate, their numbers are identical. Case
(ii), when  starts with a 1 and ends with a 0, is similar to the previous one.
Case (iii):  has a 0 at both ends. As 0-runs and 1-runs alternate, and we have 0-runs at both
ends, it follows that R1s () = R0s ()  1 = R0c ().
Theorem 3. FMComp-uw is NP-complete.
Proof. The NP-hardness is shown with a reduction from SimMini1Runs. Consider an arbitrary
SimMini1Runs instance o with m rows and n columns. Figure 4 shows a running example. We
build an undirected weighted graph G = (V, E) as follows. V has 3 types of nodes, to a total of
m + n + 1 nodes. Each column of o generates one node in V . We call these c-nodes. Each row
generates one node as well (r-nodes). There is an extra node p called the hub node.
One r-node ri and one c-node cj are connected through a unit-cost edge iff o[ri , cj ] = 1. In
addition, there is an edge with a weight of 0.75 between p and every other node. No other edges
exist in graph G. See Figure 5 for an example.
Let m be the first-move matrix of G. The row of p has a fixed number of runs, namely m + n,8
regardless of the ordering of ms columns. Let v be a c-node or r-node. Apart from vs adjacent
nodes, all other nodes are reached through a shortest path of cost 1.5 whose first move is the edge
(v, p). The matrix m for the running example is shown in Figure 6.
Let T1 be the total number of occurrences of symbol 1 in matrix o. We claim that there is an
ordering of os rows and columns that results in at most k sequential 1-runs (summed up for all rows
and all columns) iff there is an ordering of the columns of m resulting in at most k + 2T1 + m + n
8. Recall that we can ignore the dont care symbol m[p, p] = , which has no impact on the number of RLE runs.

604

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

p

c1

c2

c3

r1

r2

Figure 5: Graph in the running example. Dashed edges have a weight of .75, whereas solid lines
are unit-cost edges.

r1

m=

r1



r2









c1
c2
c3
p

r2

c1

c2

c3

p

 0 0 1 2 0
0  1 0 2 0
0 1  0 0 0
1 0 0  0 0
1 2 0 0  0
1 2 3 4 5 










Figure 6: The first-move matrix for the running example. Without any generality loss, 0 is the move
towards p. The incident edges of a given node are counted starting from 1.
cyclic RLE runs in total (summed up for all rows). Thus all rows of m, except for ps row, have at
most k + 2T1 runs in total.
Let ri1 , . . . rim and cj1 , . . . cjn be the row and column orderings in o that result in at most k
sequential RLE runs on all rows and columns. We show that the ordering ri1 , . . . rim , cj1 , . . . cjn , p
of ms columns generates at most k + 2T1 + m + n cyclic runs. Clearly, for every row or column
 in o, there is a corresponding row  0 in m (see again Figures 4 and 6 for an example). According
to the steps explained earlier and illustrated in Figures 4 and 6,  0 is obtained from  as follows.
All original 0s are preserved. All original 1s are replaced with distinct consecutive integers starting
from 1. In addition,  0 is padded with 0s at one or both of its ends. Since  0 has 0s at one or both its
ends, it follows that R1s () = R0c ( 0 ).9 It follows that Rc ( 0 ) = R0c ( 0 )+N1 () = R1s ()+N1 ().
Summing up Rc ( 0 ) over all rows  0 of m, except for ps row, we obtain
X
 0 (m)\{p}

Rc ( 0 ) =

X

R1s () +

(o)

X
(o)

N1 ()  k + 2T1 ,

where  denotes the set of rows of a matrix,  is the set of columns, and  =   . It follows that
ms rows have at most k + 2T1 + m + n cyclic RLE runs in total (that is, summed up for all rows).
Conversely, assume an ordering of ms columns with at most k + 2T1 + m + n cyclic RLE runs
in total (for all rows). This means that summing up the runs of all rows of m, except for node ps
row, results in at most k + 2T1 runs. As there are exactly 2T1 distinct runs different from 0-runs, it
9. R1s () = R0c () from Lemma 1, and R0c () = R0c ( 0 ) by construction.

605

fiS TRASSER , B OTEA , & H ARABOR

follows that there are at most k 0-runs in total:
X
 0 (m)\{p}

R0c ( 0 )  k.

Let ri1 , . . . rim , cj1 , . . . cjn , p be a re-arragement of ms columns so that: all r-nodes come in
one contiguous block, and their relative ordering is preserved; all c-nodes are one contiguous block,
and their relative ordering is preserved.
Since G restricted to c-nodes and r-nodes is bi-partite, this rearrangement cannot possibly increase the number of RLE runs. (If anything, it could eliminate some 0-runs). This is not hard
to prove. For example, if the current matrix row corresponds to an r-node a as a source node, we
have that m[a, b] = 0 for every other r-node b, since a, p, b is an optimal path from a to b. Also,
m[a, p] = 0. Our rearrangement moves all nodes b into a block that is cyclic-adjacent to p, which
does not create any new run. The case with a c-node source is similar.
We order os columns as cj1 , . . . cjn , and os rows as ri1 , . . . rim . With these orderings, the
relation between a row or column  of o and its corresponding row  0 in m is as follows. All
non-zero values in  0 are converted into 1s in . Some of  0 0s from one or both of its ends are cut
away in . Since  0 contains some 0s at one or both ends, R1s () = R0c ( 0 ), according to Lemma 1.
It follows that
X
X
R1s () =
R0c ( 0 )  k.
(o)(o)

 0 (m)\{p}

7. Fighting Complexity with Decomposition
So far all our results have been negative. We have shown that computing an optimal order on a large
class of graphs is NP-hard. In this section we identify tractability islands. We show that the problem
can be decomposed along articulation points (which are related to cuts of size 1). In particular, this
implies (as shown in this section) that a depth-first preorder is an optimal node ordering on trees.
Further we are able to construct optimal orders efficiently on a broader class of graphs than trees:
We show that the problem is fixed-parameter tractable in the size of the largest component of the
graph that has no articulation points.
Definition 4. We say that a node x of a graph G is an articulation point if removing x and its adjacent edges from G would split the graph into two or more disjoint connected subgraphs G1 . . . Gn .
Figure 7 shows an example. In the rest of this section we focus on graphs G with articulation
points x. We consider cyclic runs. In previous sections, we treated m[s, s] as a dont care symbol,
with no impact on the number of runs. In this section, we make a different assumption. Every cell
m[s, s] gets its own distinct symbol, called the s-singleton, which always creates its own run, and
which can not be merged with adjacent symbols into a common run. This makes the proofs easier
and clearly does not have a significant impact on the number of runs.
Definition 5. We call an x-block ordering any node ordering where x comes first, the nodes of G1
come next as a contiguous block, and so on all the way to block Gn .
606

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

Figure 7: A graph with an articulation point x. Removing x would decompose the graph into four
disjoint components, depicted as G1 to G4 .

In the example shown in Figure 7, the ordering o = x, a, b, c, d, e, f, g is an example of an
x-block ordering.
We use o|G0 to denote the projection of the node ordering o to a subset of nodes corresponding
to a subgraph G0 of G. We use i to denote the subgraph induced by the nodes from Gi  {x}.10
We say that an order o is a rotation of another order o0 when o is obtained from o0 by taking a block
of o0 s elements from the beginning and appending it to the end. For instance, d, e, f, g, x, a, b, c is
a rotation of x, a, b, c, d, e, f, g. More formally, o is a rotation of o0 when two sub-orders  and 
exist such that o0 = ,  and o = , .
Lemma 2. Let x be an articulation point of a graph G. Every node order o can be rearranged into
an x-block ordering o0 without increasing the number of runs on any row.
Given a graph G, a node ordering o and a row subset S, let N (o, G, S) be the number of runs
restricted to subset S. Clearly, N (o, G, G) is the total number of runs.
Lemma 3. Given any x-block ordering o, we have that:
1. N (o, G, Gi ) = N (o|i , i , Gi ); and
P
2. N (o, G, {x}) = 1  n + i N (o|i , i , {x}); and
P
3. N (o, G, G) = 1  n + i N (o|i , i , i ).
The proofs to Lemmas 2 and 3 are available in Appendix B.
Theorem 4. Given an optimal order oi for every subgraph induced by i , we can construct an
optimal global ordering o for G as following. Obtain new orderings o0i by rotating oi such that x
comes first, and then removing x. Then, o = x, o01 , . . . , o0n is optimal.
Proof. We show, by contradiction, that the global ordering o is optimal. Notice that o|i is optimal
for i . Assume there is a strictly better ordering o0 . According to Lemma 2, there exists an x-block
10. A subgraph induced by a subset of nodes S contains all nodes from S and all edges whose both ends belong to S.

607

fiS TRASSER , B OTEA , & H ARABOR

ordering o00 at least as good as o0 . We have
N (o, G, G) = 1  n +

X

 1n+

X

N (o|i , i , i )

i

i

N (o00 |i , i , i )

= N (o00 , G, G)  N (o0 , G, G)
which is a contradiction with o0 being strictly better (i.e., N (o0 , G, G) < N (o, G, G)).
Lemma 4. If G is a tree and o is a depth-first preorder of G (with arbitrary root) then o is a rotated
x-block order for every node x.
Proof. Every preorder induces a rooted tree. With respect to this root every node x (except the root)
has a parent p and a possibly empty sequence of direct children c1 . . . cn ordered in the way that
the depth-first search visited them. When removing x, G is decomposed into the subgraphs Gp ,
Gc1 . . . Gcn . If x is the root then Gp is the empty graph. The order o has the following structure:
some nodes of Gp , x, all nodes of Gc1 . . . all nodes of Gcn , the remaining nodes of Gp . Clearly this
is a rotated x-block ordering.
Theorem 5. If G = (V, E) is a tree and o a depth-first preorder of G then N (o, G, G) = 3|V |  2.
Proof. A direct consequence of Lemma 4 is that every node v has as many runs as d(v) + 1, where
d(v) is the degree of the node. The +1 comes from the v-singleton. We have thus
N (o, G, G) =

X
vV

(d(v) + 1) = 2|E| + |V | = 3|V |  2.

Theorem 6. Computing an optimal order for graph G is fixed-parameter tractable in the size of
largest two-connected component of G (i.e., the largest component with no articulation points).
Proof. Recursively decompose G at articulation points until only two-connected parts are left. As
the size of the parts does not depend on the size of G we can enumerate all orders and pick the
best one. Given optimal orders for every part we can use Theorem 4 to construct an optimal global
order.
Being able to decompose graphs along articulation points is useful on real-world road networks.
These graphs tend to have a large two-connected component with many small trees attached. For example the Europe graph made available during the 9th DIMACS challenge (Demetrescu, Goldberg,
& Johnson, 2009) has about 18M nodes in total of which only 11.8M are within the largest twoconnected component. Our result allows us to position 6.2M nodes in the order fast and optimally
using only local information.
608

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

8. Heuristic Node Orderings
In Sections 5 and 6 we have shown that computing an optimal order is NP-hard in theory. Fortunately, NP-hardness does not rule out the existence of good heuristic orderings that can be
computed quickly. Indeed, a simple depth-first preorder works very well in practice. This observation can partially be explained by the fact that, as shown in Section 7, a depth-first preorder is
optimal for trees. However, we can also explain it using more informal and intuitive terms.
An ordering is good if neighboring nodes in the graph are assigned neighboring IDs. This is
consistent with the previous observation (Sankaranarayanan et al., 2005; Botea, 2011) that, if two
target nodes are close to each other, chances are that the first move from a current node towards
any of these targets is the same. A depth-first preorder achieves the goal of assigning close IDs to
neighboring nodes in low degree graphs. A node is either an interior node, the root, or a leaf in the
DFS tree. Most of the nodes of a graph tend to be interior nodes. For these, a depth-first preorder
will assign to two neighboring nodes adjacent IDs. Denote by v an internal node, by p its parent
and by c the first child of v. The ID of p will be the ID of v minus 1, whereas the ID of c is the ID
of v plus one. We can guarantee nothing for other children. However, if the average node degree is
low, as is the case in for example road graphs, then there just are not that many other children.
Besides using depth-first preorders, we also propose another heuristic that is based on the intuition of assigning close IDs to close nodes. It is based on cuts. The above formulated intuitive
optimization criterion can also be formulated as following: For every edge, both endpoints should
have a close ID. Obviously this can not be fulfilled for all edges at once. For this reason the proposed ordering tries to identify a small set of edges for which this property may be violated. It does
this using balanced edge cuts. Given a graph with n nodes we want to assign IDs in the range [1, n]
using recursive bisection. In the first step our algorithm bisects the graph into two parts of nearly
equal node counts and a small edge cut size. It then divides the ID range in the middle and assigns
the lower IDs to one part and the upper IDs to the other part. It continues by recursively bisecting
the parts and further dividing the associated ID ranges until only parts of constant size are left. As
described so far the algorithm is free to decide to which part it assigns the lower and to which the
upper ID ranges. For this reason we augment it by tracking for every node v two counters h(v) and
`(v) representing the number of neighbors with guaranteed higher and a lower IDs. Initially these
counters are zero. At every bisection after the ranges are assigned the algorithm iterates over the
edge cut increasing the counters for the border nodes. When deciding which of two parts p and q
gets which ranges it uses these counters to estimate the ID distance of both parts to the nodes around
them. It evaluates
X
X
X
X
h(v) 
`(v) <
h(v) 
`(v)
vq

vq

vp

vp

and assigns the higher IDs to p if the condition holds. When the algorithm encounters a part that is
too small to be bisected it assigns the IDs ordered by `(v)  h(v).

9. Compression
Let a1 . . . an denote an uncompressed row in the first-move matrix. As stated previously, SRC
compresses it as a list of runs ordered by their start. As the compressed rows vary in size, we need
an additional index array that maps each source node s onto the memory offset of the first run in the
609

fiS TRASSER , B OTEA , & H ARABOR

row corresponding to s. We arrange the rows consecutively in memory and therefore the end of ss
row is also the start of s + 1s row. We therefore do not need to store the row ends.
9.1 Memory Consumption
We required node IDs to be encodable in 28 bits and out-edge IDs in 4 bits. We encode each runs
start in the upper 28 bits of a 32-bit machine word and its value in the lower 4 bits. The total memory
consumption is therefore 4  (|V | + 1 + r) bytes where r is the total number of runs over all rows and
|V | + 1 is the number of offsets in the index array. Notice that, in our implementation, we assume
that 4 bytes per index entry are sufficient, which is equivalent to saying that r < 232 . The formula
can easily be adapted to other sizes (i.e., number of bits) for node IDs, edge IDs, and index entries.
For instance, when the sum of one node ID and one edge ID is K bytes, and J bytes are sufficient
to encode the index of any run (in other words, the number r fits into J bytes), the formula becomes
J  (|V | + 1) + K  r bytes.
9.2 Computing Rows
Rows are computed individually by running a variant of Dijkstras one-to-all algorithm for every
source node s and then compressed as described in detail in Section 9.3. However, depending on the
graph it is possible that shortest paths are not unique and may differ in their first edge. It is therefore
possible that multiple valid uncompressed rows exist that tie-break paths differently. These rows
may also differ in their number of runs and therefore have different compressed sizes. To minimize
the compressed size of a row, instead of using Dijkstras algorithm to compute one specific row
a1 . . . an we modify it to compute sets A1 . . . An of valid first move edges. We require that for each
at  At a shortest st-path must exist that uses at as its first edge. Our algorithm maintains alongside
the tentative distance array d(t) for each node t a set of valid first move edges At . If the algorithm
relaxes an edge (u, v) decreasing d(v) it performs Av  Au . If d(u) + w(u, v) = d(v) then it
performs Av  Av  Au . As we restricted the out-degree of each node to 15 we can store the At
sets as 16-bit bitfields. Set union is performed using a bitwise-or operation.
9.3 Compressing Rows with Run Length Encoding
For every target the compression method is given a set of valid first move edges and may pick the one
that minimizes the compressed size. We formalize this subproblem as following: Given a sequence
of sets A1 . . . An find a sequence a1 . . . an with ai  Ai that minimizes the number of runs. We
show that this subproblem can be solved optimally using a greedy algorithm. Our algorithm begins
by determining the longest run T
that includes a1 . This
T is done by scanning over the A1 . . . Ai Ai+1
until the intersection is empty: j[1,i] Aj 6=  but j[1,i+1] Aj = . The algorithm then chooses
a value from the intersection (it does not matter which) and assigns it to a1 . . . ai . It continues
by determining the longest run that starts at and contains ai+1 in the same way. This procedure
is iterated until the rows end is reached. This approach is optimal because we can show that an
optimal solution with a longest first run exists. No valid solution can have a longer first run. An
optimal solution with a shorter first run can be transformed by increasing the first runs length and
decreasing the second ones without modifying their values. As subsequences can be exchanged
without affecting their surroundings we can conclude that the greedy strategy is optimal.
610

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

1
b,5

a,2
e,3
4

c,3
5

d,6

2
f ,4
3

1
2
3
4
5

1/a
1/a 3/f 4/e
1/f 3/d
1/e 3/d 4/c
1/c

(a) Input

(b) SRC

1
2
3
4
5

X
X 3/f 4/e
Y 1/f
Y 1/e 4/c
Z

(c) MRC per row info.

X 1/a
Y 3/d
Z 1/c

(d) MRC per group info.

Figure 8: MRC applied to the toy graph from Figure 2, reproduced here for convenience, at the left.
Part (b) illustrates the SRC input and the full runs Rs of every row. Part (c) show the groups (X,
Y , or Z) of each row and the row-specific runs R0 s . Finally, part (d) depicts the runs R0 g shared by
the rows in each group.

9.4 Merging Rows using Groups
To compress individual rows we have exploited that shortest paths from s to t1 and t2 often have
the same first move if t1 and t2 are close. A similar observation can be made for close source nodes
s1 and s2 . Their compressed rows tend to resemble each other. We want to further compress the
data by exploiting this redundancy. We call this technique multi-row compression (MRC) and we
illustrate it in Figure 8. We partition the nodes into groups and store for each group the information
shared by all nodes in the group. At each row we only store the information unique to it. Denote
by g(s) the unique group of node s. Two runs in different rows with the same start and same value
will have the same 32-bit pattern. Denote by Rs the set of runs in the row of s. Instead of storing
for each row T
s the whole set Rs we store for each group h the intersection of all rows. That is, we
0
store R h = ih Ri . For each row s we store R0 s = Rs \ Rg(s) . Recall that a query with target t
consists of finding max{x  Rs | x < t0 } (where t0 = 15t + 16). Notice that this formula can be
rewritten using basic set logic as max{max{x  R0 s | x < t0 }, max{x  R0 g(s) | x < t0 }} which
can be implemented using two binary searches if all R0 i are stored as ordered arrays. Note that we
need a second index array to lookup the R0 g for the groups g.
9.5 Computing Row Groups
By design close source nodes have close node IDs and thus neighbouring rows. This motivates
restricting ourselves to row-run groupings. That is, for each group h there are rows i and j such
that all rows in [i, j] belong to the group. An optimal row-run grouping can be computed using
dynamic programming. Denote by S(n) the maximum number of runs saved compared to using no
group-compression restricted to the first n rows. Notice that S(1) = 0. Given S(1) . . . S(n) we
want to compute S(n + 1). Obviously the n + 1s row must be part of theT
last group. Suppose that
the last group has length ` then we save in total S(n + 1  `) + (`  1)  | i[n+1`,n+1] Ri | runs.
As there are only n different values for ` we can enumerate, with brute force, all possible values,
resulting in an algorithm with a running time in (n2 ). We observe that the intersection of large
groups often seems to be nearly empty and therefore we only test values for `  100 resulting in a
(n) heuristic.
611

fiS TRASSER , B OTEA , & H ARABOR

10. Queries
Given a source node s and a target node t (with s 6= t) the algorithm determines the first edge of
a shortest st-path. It does this by first determining the start and the end of the compressed row
of s using the index array. It then runs a binary search to determine the run containing t and the
corresponding out-edge ID. More precisely the algorithm searches for the run with the largest start
that is still smaller or equal to t. Recall that we encode each run in a single 32-bit machine word
with the higher 28 bits being the runs start. We can reinterpret these 32-bits as unsigned integers.
The algorithm then consists of a binary search in an ordered 32-bit integer for the largest element
not larger than 16t + 15 (i.e., t in the higher 28 bits and all 4 lower bits set).
Extracting a path using CPDs is an extremely simple recursive procedure: beginning at the start
node we extract the first move toward the target. We follow the resultant edge to a neighbouring
node and repeat the process until the target is reached.

11. Experimental Setup
To evaluate our work we consider two types of graphs: road graphs and grid-based graphs. In
all cases we assume that node IDs can be encoded within 28-bit integers. Further we assume that
  15, and that we use a distinct value (15) to indicate an invalid edge. This allows us to encode
out-edge IDs within 4 bits. Note that the concatenation of a node ID and an out-edge ID fits into a
single 32-bit machine word.
Our experiments were performed on a quad core i7-3770 CPU @ 3.40GHz with 8MB of combined cache, 8GB of RAM running Ubuntu 13.10. All algorithms were compiled using g++ 4.8.1
with -O3. All reported query times use a single core.
11.1 Grid Graphs
We have chosen three benchmark problem sets drawn from real computer games. The first two sets
of benchmark instances have appeared in the 2012 Grid-Based Path Planning Competition. The third
benchmark set consists of two worst case maps in terms of size. These two maps are available as part
of Nathan Sturtevants extended problem repository at http://movingai.com/benchmarks/
but are not part of the 2012 competition set.
 The first benchmark set features 27 maps that come from the game Dragon Age Origins.
These maps have 16K nodes and 119K edges, on average.
 The second benchmark set features 11 maps that come from the game StarCraft. These
maps have 288K nodes and 2.24M edges, on average.
 The third benchmark set comprises two large grids which we evaluate separately. These are
the largest maps available for the two games. From the extended Dragon Age Origins
problem set we choose the map called ost100d. It has 137K nodes and 1.1M edges. From
the extended StarCraft problem set we choose the map called TheFrozenSea. It has
754K nodes and 5.8M edges. Note that ost100d, while being the largest Dragon Age
Origins map, is smaller than the average StarCraft map.
All grid maps under evaluation are undirected and feature two types
 of edges: straight edges which
have a weight of 1.0 and diagonal edges which have a weight of 2.
612

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

11.2 Road Graphs
In the case of road graphs we have chosen several smaller benchmarks made available during the
9th DIMACS challenge (Demetrescu et al., 2009).
 The New York City map (henceforth, NY) has 264K nodes and 730K edges.
 The San Francisco Bay Area (henceforth, BAY) has 321K nodes and 800K edges.
 Finally, the State of Colorado (henceforth, COL) has 436K nodes and 1M edges.
For all three graphs travel time weights (denoted using a -t suffix) and geographic distance weights
(denoted using -d) are available.
11.3 Comparisons
We implemented our algorithm in two variants: single-row-compression (SRC) not using the row
merging optimization, and multi-row-compression (MRC), using this optimization. We compare
both these approaches with two recent and state-of-the-art methods: Copa (Botea & Harabor, 2013b)
and RXL (Delling et al., 2014). We evaluate two variants of Copa. The first variant, which we
denote Copa-G, appeared at the 2012 GPPC and is optimised for grid-graphs. We use the original
C++ implementation which is available from the competition repository (Sturtevant, 2012a). The
second variant, which we denote Copa-R, is optimised for road graphs. This algorithm is described
in (Botea & Harabor, 2013a); we used the original C++ implementation of this program version as
well.
RXL is the newest version of the Hub-Labeling algorithm. We asked the original authors to
run the experiments for us presented below. These experiments were carried out on a Xeon E52690 @ 2.90 GHz. To compensate for the lower clock speed, when compared to our test machine,
we scale the query times of RXL by a factor of 2.90/3.40 = 85%. It is important to note that this
implementation of RXL computes path distances instead of first-moves. As discussed in Section 2
this should not make a significant difference for query times. However it is unclear to us whether it is
possible to incorporate the additional data needed for first-move computation into the compression
schemes presented by Delling et al. (2014). The reported RXL database sizes should therefore be
regarded as lower bounds.

12. Results
We evaluate our two algorithms (SRC and MRC) in terms of preprocessing time, compression
performance and query performance. We also study the impact of a range of heuristic node orderings
using these same metrics. There are three such variants, each distinguished by suffix. The suffix +cut
indicates a node ordering based on the balanced edge-separators graph cutting technique described
in Section 8. The suffix +dfs indicates a node ordering based on depth-first search traversal, again as
described in Section 8. The suffix +input (or shorter +inp) indicates the order of the nodes is taken
from the associated input file. In the case of grid graphs we ordered nodes lexicographically, first by
y- and then by x-coordinates. Where applicable we compare our work against the state-of-the-art
first-move algorithms Copa-R and Copa-G. We also compare against the very recent hub labeling
technique known as RXL and a more space efficient (but not as fast) variant called CRXL.
613

fiS TRASSER , B OTEA , & H ARABOR

Benchmark
DIMACS
Dragon Age Origins
StarCraft
ost100d
TheFrozenSea

Average Preprocessing Time (seconds)
Compute Order
Single Row Compression
Multi Row Compression
+cut
+dfs
+input
+cut
+dfs
+input
+cut
+dfs
+input
16
<1
0
1950
1982
2111
1953
1985
2125
2
<1
0
32
35
38
33
36
40
18
<1
0
1979
2181
2539
1993
2195
2574
19
<1
n/m
101
100
n/m
104
105
n/m
110
<1
n/m
3038
3605
n/m
3133
3690
n/m

Table 2: Preprocessing time for road and grid graphs. We give results for (i) the average time required to
compute each node ordering; (ii) the total time required to compute the entire database for each of SRC and
MRC. Values are given to the nearest second. The ost100d and TheFrozenSea preprocessing experiments
were run on an AMD Opteron 6172 with 48 cores @ 2.1 GHz to accelerate the APSP computation. The
experiments on the smaller graphs clearly show that the input order is fully dominated. We therefore omit the
numbers for the two larger test graphs. n/m stands for not measured.
Graph
|V |

|E|
|V |

CopaG

+cut

Min < 1K
7
Q1
2K 7.2
Med
5K 7.4
Avg 31K 7.4
Q3
52K 7.6
Max 100K 7.7

<1
<1
1
12
18
75

<1
<1
<1
5
6
31

Min
Q1
Med
Avg
Q3
Max

60
128
183
351
510
934

20
28
69
148
189
549

105K
173K
274K
288K
396K
494K

7.7
7.7
7.8
7.8
7.8
7.8

DB Size (MB)
Query Time (nano seconds)
MRC
SRC
MRC
SRC
UM CopaG
+dfs +inp +cut +dfs +inp
+cut +dfs +inp +cut +dfs
Dragon Age: Origins (27 maps)
<1 <1 <1 <1 <1
<1
34
19 26 26
14 19
<1 <1 <1 <1 <1
2
63
22 31 35
16 22
1
1
<1 1
2
12.5
81
30 44 54
20 31
7
23
6
8
53
480.5
156
34 50 72
25 36
10
29
7
12
65
1352
266
36 62 106 28 45
39 106 35 44 349 5000
316
95 116 176 67 78
StarCraft (11 maps)
35
89
25 42 187 5512.5 304
63 93 130 47 63
61 144 33 71 281 14964
324
70 103 142 51 69
111 393 83 126 956 37538
334
95 121 187 66 77
203 444 172 222 983 41472
358
105 130 195 66 82
282 621 222 308 1318 78408
396
126 146 226 72 90
626 1245 630 660 2947 122018 436
197 195 311 108 118

+inp
18
26
38
54
82
138
88
102
133
132
156
208

Table 3: Performance of SRC and MRC on grid graphs. We use two problem sets taken from the 2012
GPPC and compare against Copa-G, one of the winners of that competition. We measure (i) the size of the
compressed database (in MB) and; (ii) the time needed to extract a first query (in nanos). All values are
rounded to the nearest whole number (either MB or nano, respectively). As a baseline, column UM shows
the size of a naive, non-compressed first-move matrix.

12.1 Preprocessing Time
Table 2 gives the average preprocessing time for SRC and MRC on the 6 road graphs and the two
competition sets. The time in each case is dominated by the need to compute a full APSP table.
As we have previously commented, APSP compression is the central point of our work; not the
APSP-computation. Our preprocessing approach involves executing Dijkstras algorithm repeatedly
resulting in a total running time of O(n2 log n) on sparse graphs with non-negative weights; using
modern APSP techniques (e.g., Delling, Goldberg, Nowatzyk, and Werneck 2013) succeeded in
significantly reducing the hidden constants behind the big-O and are able to exploit more specific
graphs structures (e.g., road graphs) to get the running time down. However, these techniques do
not give a benefit over repeatedly running Dijkstras algorithm in the asymptotic worst-case.
614

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

Graph
Name

|V |

|E|
|V |

BAY-d
BAY-t
COL-d
COL-t
NY-d
NY-t

321K
321K
436K
436K
264K
264K

2.5
2.5
2.4
2.4
2.8
2.8

DB Size (MB)
Query Time (nano seconds)
Copa Hub Labels
MRC
SRC
Copa Hub Labels
MRC
SRC
UM
-R RXL CRXL +cut +dfs +cut +dfs
-R RXL CRXL +cut +dfs +cut +dfs
317 90
19
141 129 160 144 51521 527 488 3133 89 100 62 69
248 65
17
102 95 117 107 51521 469 371 1873 74 87 52 60
586 138 24
228 206 268 240 95048 677 564 3867 125 111 68 85
503 90
22
162 150 192 175 95048 571 390 2131 88 97 58 65
363 99
21
226 207 252 229 34848 617 621 4498 112 122 75 83
342 66
18
192 177 217 198 34848 528 425 2529 98 111 67 75

Table 4: Comparative performance of SRC, MRC, Copa-R and two recent Hub Labeling algorithms. We
also report the size UM of an uncompressed matrix. We test each one on six graphs from the 9th DIMACS
challenge. We measure (i) database sizes (in MB); (ii) the time needed to extract a first query (in nanos).
Values are rounded to the nearest whole number. Graph sizes are rounded to the nearest thousand nodes.

Creating a node order is fast; +dfs requires only fractions of a second. Even the +cut order
requires no more than 18 seconds on average using METIS (Karypis & Kumar, 1998). Meanwhile,
the difference between the running times of SRC and MRC indicate that multi-row compression
does not add more than a small overhead to the total time. For most of our test instances the
recorded preprocessing overhead was on the order of seconds.
12.2 Compression and Query Performance
In Table 3 we give an overview of the compression and query time performance for both Copa-G
and a range of SRC and MRC variants on the competition benchmark sets. To measure the query
performance we run 108 random queries with source and target nodes picked uniformly at random
and average their running times.
MRC outperforms SRC in terms of compression but at the expense of query time. Node orders
significantly impact the performance of SRC and MRC. In most cases +cut yields a smaller database
and faster queries. SRC and MRC using +cut and +dfs convincingly outperform Copa-G on the
majority of test maps, both in terms of space consumption and query time.
A naive, non-compressed first-move matrix is impractical due to the very large memory requirements. The size an uncompressed matrix would have is 4  |V |2 bits, reflecting our assumption that
an outgoing edge can be stored in 4 bits. For Tables 3, 4, and 5 we specify in column UM the memory consumption of an uncompressed matrix. For example, for the ost100d game graph with 137K,
a non-compressed matrix requires a bit more than 9GB of memory, which is more than two orders of magnitude higher than the 49MB, respectively 39MB that SRC+cut respectively MRC+cut
need. For larger graphs, the difference is more striking. For example TheFrozenSea, our largest
game graph, with 754K nodes leads to a 576MB MRC+cut database compared to 284GB in a noncompressed matrix. On the other hand, for the smaller graphs where the matrix would fit in memory,
fetching up moves would be extremely fast, being one table lookup per move. For comparison, to
fetch one move, our method performs a binary search in a compressed string whose length is no
larger, and usually much smaller than |V |.
In Table 4 we look at the performance for the 6 road graphs and compare Copa-R, SRC, MRC,
RXL and CRXL. The main observations are that on road graphs +dfs leads to smaller CPDs than
+cut. Surprisingly, the lower average row lengths do not yield faster query times. Copa-R is dominated by RXL, SRC, and MRC. SRC+cut outperforms all competitors by several factors in terms of
615

fiS TRASSER , B OTEA , & H ARABOR

Graph
Name
ost100d
FrozenSea

|V |

|E|
|V |

137K
754K

7.7
7.7

DB Size (MB)
MRC
SRC
UM
RXL CRXL +cut +dfs +cut +dfs
62
24
39
50
49
57
9436
429 135
576 634 753 740 284405
Hub Labels

Query Time (nano seconds)
Hub Labels
MRC
SRC
RXL CRXL +cut +dfs +cut +dfs
598 5501
89 110
58
71
814 9411 176 192 104 109

Table 5: Performance of SRC and MRC on large grid graphs from Nathan Sturtevants extended repository.
We compare against the Hub Labeling methods RXL, and CRXL. We also report the size UM of an uncompressed matrix. We run tests on TheFrozenSea, drawn from the game StarCraft, and ost100d, which comes
from the game Dragon Age Origins. We measure (i) database sizes (in MB); (ii) the time needed to extract a
first query (in nanos). Values are rounded to the nearest whole number. RXL & CRXL exploit that the graphs
are undirected while SRC & MRC do not. For directed graphs the space consumption of RXL would double.

Graph
BAY-d
BAY-t
COL-d
COL-t
NY-d
NY-t
FrozenSea
ost100d

Average Row and Label
Length
Space (Bytes)
SRC
SamPG
SRC
SamPG
+cut +dfs + Plain +cut +dfs
51
129 108
816
516 432
34
94
79
544
376 316
59
160 131
944
640 524
35
114
96
560
456 384
70
248 203
1120
992 812
44
214 175
704
856 700
92
260 256
1472
1040 1024
80
91
108
1280
364 432

Table 6: We report the average number of hubs per label (length), number of runs per row (length), and
average space usage per node for SamPG+Plain and SRC.

speed. RXL wins in terms of the database size. However the factor gained in space is smaller than
the factor lost in query time compared to SRC. CRXL clearly wins in terms of space but is up to two
orders of magnitude slower than the competition. On road graphs distance weights are harder than
travel time weights. This was already known for algorithms that exploit similar graph features as
RXL. However, it is interesting that seemingly unrelated first-move compression based algorithms
incur the same penalties.
In Table 5 we evaluate the performance of SRC, MRC, RXL and CRXL on the larger game
maps. We dropped Copa-R because from the experiments on the smaller graphs it is clear that it is
fully dominated. Other than on road graphs, the space consumption for SRC and MRC is lower for
the +cut order than for +dfs. As a result the +cut order is clearly superior to +dfs on game maps.
On ost100d both SRC and MRC beat RXL in terms of query time and of space consumption. On
TheFrozenSea RXL needs less space than SRC and MRC. However, note that on game maps RXL
gains a factor of 2 by exploiting that the graphs are undirected which SRC and MRC do not.
CRXL employs powerful compression techniques specific to shortest paths. RXL does not use
them but it is not uncompressed as it uses some basic encoding techniques such as delta encoding. A
basic HL variant that stores all nodes and distances explicitly needs more memory. We refer to this
basic variant as SamPG+Plain. SamPG is the ordering algorithm used in RXL11 and Plain refers
11. The RXL-paper describes several node orders. However, SamPG is the order they suggest using. All other RXL and
CRXL numbers in our paper use SamPG.

616

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

1
b,5

a,2
e,3

2
f ,4

4
c,3
5

d,6

3

(a) Input Graph

(b) Computing rectangles

(c) List of rectangles

Figure 9: Copas rectangle decomposition on the toy example from Figures 2 and 8 for the first
moves from source node 2. Similar decompositions are needed for every other source node.

to an elementary HL encoding with 4 bytes per hub ID and 4 bytes per distance value. We want
to comparse SRC with SamPG+Plain. We therefore report in Table 6 the average number of hubs
per label and the average number of runs per row using SRC. The reported number of hubs is per
label. Note that on directed graphs every node needs two labels: a forward and a backward label.
On undirected graphs the two labels coincide and only one has to be stored. This contrasts with
SRC which cannot exploit that the input graph is undirected. The numbers in the table therefore
assume that two HL-labels are needed per node for better comparability. For HL we need to store
a 32-bit distance value, a 28-bit node ID and a 4-bit out-edge ID and this times 2 because of there
being two labels per node. The total space consumption is thus 16h bytes where h is the average
number of hubs per label. For SRC we need to store a 28-bit node ID and a 4-bit out-edge ID per
run. This results in 4r bytes where r is the average number of runs per row. For Table 6 it can be
seen that SamPG+Plain consistently occupies more space than SRC, even though the experiments
thus far suggest that RXL is more compact. The basic compression techniques in RXL are therefore
important and are enough to make the performance ordering of the algorithms tip with respect to
space consumption.
RXL has some advantages not visible in the tables. For example it does not require computing
an APSP in the preprocessing step significantly reducing preprocessing time. Further it computes
besides the first move also the shortest path distance.
12.3 Discussion
We compared SRC and MRC with Copa and RXL. Copa is a recent and successful technique for creating compressed path databases. As one of the joint winners at the 2012 Grid-Based Path Planning
Competition (GPPC-12) we regard Copa as the current state-of-the-art for a range of pathfinding
problems including the efficient storage and extraction of optimal first-moves. RXL is the newest
version of the Hub-Labeling algorithm and to our knowledge the state-of-the-art in terms of minimizing query times on road graphs.
617

fiS TRASSER , B OTEA , & H ARABOR

SRC and MRC have been illustrated in Figures 1, 2 and 8. Figure 9 illustrates how Copa
works in preprocessing, for a better understanding of this section, without any intention of a fully
detailed description. Copa assumes that every node in the graph is labelled with x, y coordinates.
Our toy example has 3 rows and 3 columns, as shown in Figure 9 (a). Copa iterates through all
nodes in the graph. Let n be the current node (source node) at a given iteration. Copa splits the
map into rectangles, and labels each rectangle with the id of an outgoing edge from n. When a
target t belongs to a given rectangle, the optimal move from n towards t is precisely the label of that
rectangle. Figure 9 (a) shows the map decomposition for the source node 2, with rectangles depicted
with a dashed line. There are three rectangles constructed in the figure: one at the bottom left, with
size 2  2 and label e; one at the top left, with size 1  1 and label a; and one at the bottom right,
having size 1  1 and label f . In part (b), each rectangle is represented with 5 symbols each: upper
row, left column, width, height, label. Here we show just the rectangles for source node 2, but this
is concatenated with the lists of all other nodes. Some of the rectangles can safely be removed (list
trimming) but we skip this in our example. Then, each of the 5 columns in Figure 9 (a) is treated
as a separate string, and it is further compressed with sliding window compression and run-length
encoding.
We performed experiments on a large number of realistic grid-graphs used at GPPC-12 and find
that both SRC and MRC significantly improve on both the query time and compression power of
Copa. For a large number of experiments and on a broad range of input maps we were able to extract
a first move in just tens or hundreds of nano-seconds (a factor of 3 to 5 faster than Copa). There are
two main reasons why SRC and MRC are performant vs. Copa: Our approach uses less memory
and our query running time is logarithmic (cf. linear) in the label size.
Our approach requires less memory than Copa. Part of the explanation stems from the differences between the sizes of the building blocks in each approach. In SRC and MRC, the building
block is an RLE run represented with two numbers: the start of the run, which is a node id and
thus requires log2 (|V |) bits, and the value of the run, which is an out-edge id and requires log2 ()
bits. In Copa, a building block is a rectangle that requires 2 log2 (|V |) + log2 () bits. In the actual
implementations, both SRC and MRC store only a single 32-bit machine word per run, which allows for graphs with up to 228 nodes. The Copa code used in the 2012 Grid-Based Path Planning
Competition stores a rectangle on 48 bits, corresponding to a max node count of 222 .
Clearly, the size of the building blocks is not the only reason for the different compression
results. The number of RLE runs in SRC or MRC can differ from the total number of rectangles
in Copa. When more than one optimal out-edge exists, SRC and MRC select an edge that will
improve the compression, whereas Copa sticks with one arbitrary optimal out-edge. On the other
hand, besides rectangle decomposition, Copa implements additional compression methods, such as
list trimming, run length encoding and sliding window compression, all performed on top of the
original rectangle decomposition (Botea & Harabor, 2013a).
Our approach has an asymptotic query time of O(log2 (k)) where k is the number of compressed
labels that must be searched. By comparison, given a source node, Copa stores its corresponding
list of rectangles in the decreasing order of their size. Rectangles are checked in order. While, in the
worst-case, the total number of rectangle checks is linear in the size of the list, the average number
is much improved due to the ordering mentioned (Botea, 2011; Botea & Harabor, 2013a).
The reason why a CPD is faster than RXL is due to the basic query algorithm. The algorithm
underlying RXL consists of a merge-sort like merge of two integer arrays formed by the forward
label of s and the backward label of t. This is a fast and cache friendly operation but needs to look
618

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

at each entry resulting in an inherently linear time operation. SRC on the other hand builds upon a
binary search which is slightly less cache friendly as memory accesses are not sequential it but has
a logarithmic running time.
One can regard the compressed SRC rows as one-sided labels. For each st-pair the first move
can be determined using only the label of s. HL on the other hand needs the forward label of s and
the backward label of t. HL-labels tend to have less entries than the SRC labels. However, each
HL-entry needs more space as they need to store the distance values in addition to node-IDs.

13. Results from the 2014 Grid-Based Path Planning Competition
We have recently submitted the algorithms SRC+cut and SRC+dfs to the 2014 edition of the GridBased Path Planning Competition GPPC (Sturtevant, 2014). In this section we give a brief overview
of the competition and a short summary of results. A full description of the methodology employed
by the organisers, as well as a full account of the results, is given in (Sturtevant et al., 2015).
13.1 Competition Setup
The Grid-Based Path Planning Competition features over one hundred grid maps from which more
than three hundred thousand distinct problem instances are drawn. Individual maps differ in size,
ranging from several thousand to several million nodes. The topography of the maps is also varied
with many maps originating from the computer games StarCraft, Dragon Age: Origins and Dragon
Age 2. Other maps appearing as part of the competition are synthetically generated grids; mazes,
rooms and randomly placed obstacles of varying density. In the 2014 edition of the competition
there were a total 14 different entries, submitted by 6 different teams. Several entries are variants of
the same algorithm and are submitted by the same team.
 6 entries employ symmetry breaking to speed up search. The entries BLJPS, BLJPS2, JPS+
and JPS+Bucket can roughly be described as extensions of Jump Point Search (Harabor &
Grastien, 2011) and JPS+ (Harabor & Grastien, 2014). The entry NSubgoal makes use of
multi-level Subgoal Graphs (Uras & Koenig, 2014). Finally the entry named BLJPS Sub is a
hybrid algorithm that makes use of both Jump Point Search and Subgoal Graphs.
 1 entry (CH) employs a variation on Contraction Hierarchies (Dibbelt et al., 2014).
 3 entries directly improve the performance of the A* algorithm; either through the use of faster
priority queues (A* Bucket) or by trading optimality for speed (RA* and RA*-Subgoal).
 4 entries use Compressed Path Databases. Two of these (SRC+dfs-i and SRC+cut-i) are
incremental algorithms that return the optimal path one segment at a time; that is, they must
be called repeatedly until the target location is returned. The other two algorithms (SRC+dfs
and SRC+cut) are non-incremental and when queried return a complete path.
Unfortunately, 3 entries contained some bugs and therefore did not finish on all instances.
SRC+cut was one of them. We therefore omit them from the tables and the discussion.
13.2 Results
A summary of results from the competition is given in Table 7. We observe the following:
619

fiS TRASSER , B OTEA , & H ARABOR

Entry


RA*
BLJPS
JPS+
BLJPS2

RA*-Subgoal
JPS+ Bucket
BLJPS2 Sub
NSubgoal
CH
SRC-dfs
SRC-dfs-i

Averaged Query Time over All Test Paths (s)
Slowest Move
First 20 Moves
Full Path
in Path
of path
Extraction
282 995
282 995
282 995
14 453
14 453
14 453
7 732
7 732
7 732
7 444
7 444
7 444
1 688
1 688
1 688
1 616
1 616
1 616
1 571
1 571
1 571
773
773
773
362
362
362
145
145
145
1
4
189

Preprocessing Requirements
DB Size
Time
(MB)
(Minutes)
0
0.0
20
0.2
947
1.0
47
0.2
264
0.2
947
1.0
524
0.2
293
2.6
2 400
968.8
28 000
11649.5
28 000
11649.5

Table 7: Results from the 2014 Grid-Based Path Planning Competition. Figures are summarised
from the official competition results, which appear in (Sturtevant et al., 2015). Entries denoted
by  indicate approximate algorithms that are not guaranteed to always find the shortest path. The
measurments in bold indicate which entry performed best with regard to this single criterion. The
entries whose name are in bold are those that are not fully Pareto-dominated with respect to every
criterion. The preprocessing running times are the time needed to process all 132 test maps.
1. Our CPD-based entries are the fastest methods at the competition across all query time metrics. This includes the fastest (average) time to required to extract a complete optimal path, the
fastest (average) time to extract the first 20 steps of an optimal path and the fastest (average)
time required to extract any single step of an optimal path.
2. Performing a first move query with our algorithm is faster than the resolution of the competitions microsecond timer. Even iteratively extracting 20 edges of a path is only barely
measurable without a finer timer resolution. During testing we observed that a significant
amount of the query running time was spent within the benchmarking code provided by the
competition. We therefore opted to submit two variants. SRC+dfs extracts the path as a
whole. The benchmarking code is thus only run once per path. On the other hand SRC+dfs-i
extracts the path one edge at a time. This allows measuring the time needed by an individual
first move query. Unfortunately, it also requires executing the benchmarking code once per
edge. The difference in path extraction running times between SRC-dfs and SRC-dfs-i (i.e.,
44s) is the time spent in the benchmarking code.
3. Our algorithm is the only competitor that is able to answer first-move queries faster than a
full path extraction.
4. Because our entries are database driven they require generous amounts of preprocessing time
and storage space. SRC+dfs therefore had the largest total preprocessing time and the largest
total storage cost of all entries at the competition.

14. Conclusion and Future Work
We study the problem of creating an efficient compressed path database (CPD): a shortest path
oracle which, given two nodes in a weighted directed graph, always returns the first-move of an
620

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

optimal path connecting them. Starting with an all-pairs first-move matrix, which we assume is
given, we create oracles that are more compact and that can answer arbitrary first-move queries
optimally and many orders faster than is otherwise possible using conventional online graph search
techniques. We employ a run-length encoding (RLE) compression scheme throughout and analyse
the problem from both a theoretical perspective and an empirical one. Our main idea is simple: we
look to re-order the nodes (i.e., columns) of the input first-move matrix in a good way such that
its RLE-compressed size is subsequently reduced.
On the theoretical side we show that the problem of finding an optimal node ordering, in the
general case for both directed and directed graphs, is NP-complete. In more specific cases, such as
graphs that can be decomposed along articulation points, the same problem can be efficiently tackled
by solving a series of independent sub-problems. In particular we show that a depth-first traversal
of a tree provides an optimal node ordering. These results give a first theoretical underpinning to the
problem of creating space-efficient CPDs using RLE. Our work also extends theoretical results from
the areas of information processing and databases (Oswald & Reinelt, 2009; Mohapatra, 2009).
On the empirical side we study the efficacy of three heuristic node orderings: (i) a depth-first
ordering; (ii) a graph-cut ordering based on balanced edge separators; (iii) a naive baseline given by
the ordering specified by the input graph. Given an ordering and a first-move matrix, we describe
two novel approaches for creating CPDs. The first of these, SRC, uses simple run-length encoding
to compress the individual rows of the matrix. The second approach, MRC, is more sophisticated
and identifies commonalities between sets of labels compressed by SRC.
In a range of experiments we show that SRC and MRC can compress the APSP matrix for graphs
with hundreds of thousands of nodes in as little as 1-200MB. Associated query times regularly
require less than 100 nanoseconds. We also compare our approaches with Copa (Botea, 2012;
Botea & Harabor, 2013a), and RXL (Delling et al., 2014). In a range of experiments on both grid
and road graphs we show that SRC and MRC are not only competitive with Copa but often several
factors better, both in terms of compression and query times. We also show that SRC and MRC
outperform RXL in terms of query time. We also summarise results from the 2014 Grid-Based
Path Planning Competition. In particular we can report that SRC was the fastest method at the
competition across all query-time metrics and that SRC performed better than the resolution of the
competitions microsecond timer.
There appear several promising directions in which the current work could be extended. One
immediate possibility is to harness the available results on efficient appproximate TSP algorithms
in order to compute better and more space-efficient node orderings. Another immediate possibility
is to improve the current MRC compression scheme by devising an algorithm that optimizes the
assignment of first-move IDs.
Looking more broadly, a strength that all CPDs have, in addition to fast move extraction, is
that they can compress any kind of path  not just those that are network-distance optimal.12 In
multi-agent pathfinding for example it is sometimes useful to guarantee properties like there must
always be a local detour available (Wang & Botea, 2011). Another example are turn-costs in road
graphs. Thus one possible a possible direction for future work is to create CPDs that store only
paths satisfying such constraints.
A weakness of our approach is that in the preprocessing an APSP-computation is required.
While Delling et al. (2013) have shown that an APSP can sometimes be computed reasonably fast
12. Suboptimal paths, however, introduce the additional challenge of avoiding infinite loops when extracting such a path
from a CPD.

621

fiS TRASSER , B OTEA , & H ARABOR

on graphs with many nodes, APSP remains inherently quadratic in the number of nodes on any
graph class as its output size is quadratic. Our approach would therefore hugely profit from an
algorithm that can directly compute the compressed CPD without first computing the first-move
matrix in an intermediate step.

15. Acknowledgments
We thank Patrik Haslum, Akihiro Kishimoto, Jakub Marecek, Anika Schumman and Jussi Rintanen
for feedback on earlier versions of parts of this work. We would like to thank Daniel Delling &
Thomas Pajor for running the Hub-Labeling experiments for us.

Appendix A. Proof to Theorem 2
Theorem 2. SimMini1Runs is NP-complete.
Proof. The membership to NP is straightforward. The hardness proof uses a reduction from the
Hamiltonian Path Problem (HPP) in an undirected graph. Let G = (V, E) be an arbitrary undirected
graph, without duplicate edges, and define n = |V | and e = |E|. Figure 10 shows a toy graph used
as a running example.
Starting from G, we build a SimMini1Runs instance as follows. We define a 0/1 matrix m with
e rows and n columns. Let r be the row corresponding to an edge (u, v), and let cu and cv be the
columns associated with nodes u and v. Then m[r, cv ] = m[r, cu ] = 1 and m[r, c] = 0 for all other
columns. Notice that m has at least a value of 1 on every row and column. Figure 11 shows the
matrix in the running example.
x

y

w

z

Figure 10: Sample graph G.
Let r be the matrix row corresponding to an edge (u, v). It is easy to see that, when a given
ordering of the columns (nodes) makes the two nodes u and v adjacent, the number of sequential

(x, y)
(x, w)
(x, z)
(w, z)

x
1
1
1
0

y
1
0
0
0

w
0
1
0
1

z
0
0
1
1

Figure 11: Matrix m built for G.
622

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

(x, z)
(w, z)
(x, w)
(x, y)

y
0
0
0
1

x
0
0
1
1

w
0
1
1
0

z
1
1
0
0

Figure 12: The matrix after: i) converting some 1s into 0s (shown in bold); ii) re-ordering columns
on the Hamiltonian path; and iii) re-ordering rows lexicographically.

(x, z)
(w, z)
(x, w)
(x, y)

y
0
0
0
1

x
1
0
1
1

w
0
1
1
0

z
1
1
0
0

Figure 13: Matrix after restoring back the previously replaced 1s (shown in bold).
RLE 1-runs13 for row r is 1. If the nodes are not adjacent, the number of sequential RLE 1-runs for
row r is 2.
We claim the HPP has a solution iff SimMini1Runs has a solution with t = 3e  n + 2 RLE
1-runs. Let vi1 , . . . , vin be a solution of HPP (i.e., a Hamiltonian path), and let P be the set of all
edges included in this solution. In the running example, let P contain (y, x), (x, w) and (w, z).
In every row corresponding to an edge not contained in P , switch one of its two 1-entries to
0. Then, order the columns with respect to the sequence of the nodes in the Hamiltonian path and
rearrange the rows in lexicographical order. Figure 12 illustrates these changes.
The construction of the matrix, the trick of converting some of the 1s into 0s, and the ordering
of the rows and columns are reused from Oswald and Reinelts proof for the hardness of deciding
whether a 0/1 matrix has the C1Sk property (Oswald & Reinelt, 2009). The rest of the proof,
coming below, is significantly different.
Now, restore the previously replaced 1s, as shown in Figure 13. Each of the e  n + 1 1s that
were replaced and restored have no adjacent 1s in the matrix.14 As such, each of them counts as two
1-runs, one horizontal and one vertical. This sums up to a total of 2(e  n + 1) 1-runs corresponding
to the 1s replaced and restored. In addition, each row and column has one more 1-run. It follows
that the matrix has 3e  n + 2 1-runs.
Conversely, consider a row and column ordering that creates 3e  n + 2 RLE 1-runs in total. We
show that the matrix has at least e + 1 vertical 1-runs, regardless of the row ordering. Consider the
rows, in order, starting from the top. The first row introduces exactly 2 vertical 1-runs, one for each
column where it contains a value of 1. Each subsequent row introduces at least one more vertical
1-run. Otherwise, the new row would be identical to the previous one, which contradicts the fact
that the graph has no duplicate edges.
13. All runs used in this proof are sequential.
14. If they had an adjacent 1 on a column, this would imply that we have two identical rows, which would further mean
that G has duplicate edges. If they had an adjacent 1 on a row, it would mean that the edge at hand belongs to the
Hamiltonian path, which contradicts the fact that 1s were replaced and restored on the complementary set of edges.

623

fiS TRASSER , B OTEA , & H ARABOR

As there are at least e + 1 vertical 1-runs, the number of horizontal 1-runs will be at most
(3e  n + 2)  (e + 1) = 2e  n + 1. We show that the column ordering is a Hamiltonian path.
Assuming the contrary, there are only p < n  1 edges such that their nodes are adjacent in the
ordering. It follows that the number of horizontal 1-runs is p + 2(e  p) = 2e  p > 2e  n + 1.
Contradiction.

Appendix B. Proofs to Lemma 2 and Lemma 3
We start by pointing out two simple but important properties stemming from the notion of an articulation point:
Remark 1. Given a graph G, let x be an articulation point, and let G1 . . . Gn be the corresponding
connected components obtained after removing x.
1. Given a source node s  Gi , the first optimal move from s towards anywhere outside Gi is
the same.15
2. Given two distinct components Gi and Gj , the first optimal move from x towards anywhere
in Gi is different from the first optimal move from x towards anywhere in Gj .
Remark 1 follows easily from the obvious observation that there is no way of going from one
subgraph Gi to another subgraph Gj other than passing through x. See Figure 7 for an illustration.
Lemma 2. Let x be an articulation point of a graph G. Every node order o can be rearranged into
an x-block ordering o0 without increasing the number of runs on any row.
Proof. We construct the desired ordering o0 by applying the following steps:
1. Rotate o so that x comes first.
2. For every i  {1 . . . n} project the resulting order onto Gi , obtaining a suborder o0i .
3. Define o0 as: o0 = x, o01 , . . . , o0n .
It is clear by construction that the nodes in every subgraph Gi are consecutive in o0 . It remains
to show that the number of runs per row does not grow.
Denote by s a source node. We distinguish between two cases:
 Case when s  Gi for some i. Being just a rotation,
step 1 has no impact on the number of
S
cyclic runs. Steps 2 and 3 take all nodes from k6=i Gk and put them into one or two blocks
that are cyclic S
adjacent to x. We know from Remark 1, point 1 that m[s, x] = m[s, n] for
all nodes n in k6=i Gk . Thus, the re-arrangement brings next to x nodes n with the same
first-move symbol as x. Clearly, this does not increase the number of runs.
 Case when s = x. As in the previous case, the rotation performed at step 1 does not increase
the number of cyclic runs. After step 1, cyclic runs and sequential runs are equivalent, since
the first position contains a distinct symbol, namely the x-singleton. Steps 2 and 3 separate
15. Assuming that we split the ties among optimal paths in a consistent manner, which is easy to ensure.

624

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

each Gi into a contiguous block. This does not increase the number of sequential runs since,
according to Remark 1, point 2, every two blocks corresponding to Gi and Gj , with i 6= j,
have no common symbol. It follows that the number of cyclic runs does not increase either.

Lemma 3. Given any x-block ordering o, we have that:
1. N (o, G, Gi ) = N (o|i , i , Gi ); and
P
2. N (o, G, {x}) = 1  n + i N (o|i , i , {x}); and
P
3. N (o, G, G) = 1  n + i N (o|i , i , i ).
Proof. We prove each point as follows.
1. As o is an x-block ordering, the nodes come in the order x, G1 , . . . Gi , . . . Gn . Consider a
node s  Gi and its corresponding row in the first-move matrix. As pointed out in Remark 1,
every path from s to a node outside Gi has to pass through x, and therefore the first move from
s to anywhere outside Gi is the same. It follows that all nodes in the sequence x, G1 , . . . Gi1 ,
together with all nodes in the sequence Gi+1S
, . . . Gn , form one cyclic run. In effect, removing
from consideration all nodes contained in k6=i Gk leaves the number of runs unchanged,
which completes the proof of this case.
2. This is the case focused on x as a start node. According to Remark 1, if Gi 6= Gj , then the
first move from x towards anywhere in Gi is different from the first move from x towards
anywhere in Gj . It follows that two runs from two adjacent subsets Gi and Gi+1 never merge
into one run. Thus,
X
N (o, G, {x}) = 1 +
(N (o|i , i , {x})  1)
i

= 1n+

X
i

N (o|i , i , {x}).

3. This case follows from the previous two, with standard arithmetic manipulation.
X
N (o, G, G) = N (o, G, {x}) +
N (o, G, Gi )
i

= 1n+

X

= 1n+

X
(N (o|i , i , {x}) + N (o|i , i , Gi ))

i

N (o|i , i , {x}) +

X

i

X
= 1n+
(N (o|i , i , {x}  Gi )
i

= 1n+

X

N (o|i , i , i ).

i

625

N (o|i , i , Gi )

i

fiS TRASSER , B OTEA , & H ARABOR

References
Abraham, I., Delling, D., Fiat, A., Goldberg, A. V., & Werneck, R. F. (2012). HLDB: Locationbased services in databases. In Proceedings of the 20th ACM SIGSPATIAL International
Symposium on Advances in Geographic Information Systems (GIS12), pp. 339348. ACM
Press. Best Paper Award.
Abraham, I., Delling, D., Goldberg, A. V., & Werneck, R. F. (2011). A hub-based labeling algorithm
for shortest paths on road networks. In Proceedings of the 10th International Symposium on
Experimental Algorithms (SEA11), Vol. 6630 of Lecture Notes in Computer Science, pp.
230241. Springer.
Abraham, I., Delling, D., Goldberg, A. V., & Werneck, R. F. (2012). Hierarchical hub labelings
for shortest paths. In Proceedings of the 20th Annual European Symposium on Algorithms
(ESA12), Vol. 7501 of Lecture Notes in Computer Science, pp. 2435. Springer.
Akiba, T., Iwata, Y., & Yoshida, Y. (2013). Fast exact shortest-path distance queries on large networks by pruned landmark labeling.. In Proceedings of the 2013 ACM SIGMOD International
Conference on Management of Data (SIGMOD13), pp. 349360. ACM Press.
Antsfeld, L., Harabor, D., Kilby, P., & Walsh, T. (2012). Transit routing on video game maps.. In
AIIDE.
Arz, J., Luxen, D., & Sanders, P. (2013). Transit node routing reconsidered. In Proceedings of the
12th International Symposium on Experimental Algorithms (SEA13), Vol. 7933 of Lecture
Notes in Computer Science, pp. 5566. Springer.
Babenko, M., Goldberg, A. V., Kaplan, H., Savchenko, R., & Weller, M. (2015). On the complexity of hub labeling. In Proceedings of the 40th International Symposium on Mathematical
Foundations of Computer Science (MFCS15), Lecture Notes in Computer Science. Springer.
Baier, J., Botea, A., Harabor, D., & Hernandez, C. (2014). A fast algorithm for catching a prey
quickly in known and partially known game maps. Computational Intelligence and AI in
Games, IEEE Transactions on, PP(99).
Bast, H., Delling, D., Goldberg, A. V., MullerHannemann, M., Pajor, T., Sanders, P., Wagner, D., &
Werneck, R. F. (2015). Route planning in transportation networks. Tech. rep. abs/1504.05140,
ArXiv e-prints.
Bast, H., Funke, S., & Matijevic, D. (2009). Ultrafast shortest-path queries via transit nodes. In
The Shortest Path Problem: Ninth DIMACS Implementation Challenge, Vol. 74 of DIMACS
Book, pp. 175192. American Mathematical Society.
Bast, H., Funke, S., Matijevic, D., Sanders, P., & Schultes, D. (2007). In transit to constant shortestpath queries in road networks. In Proceedings of the 9th Workshop on Algorithm Engineering
and Experiments (ALENEX07), pp. 4659. SIAM.
Bauer, R., Columbus, T., Katz, B., Krug, M., & Wagner, D. (2010). Preprocessing speed-up
techniques is hard. In Proceedings of the 7th Conference on Algorithms and Complexity
(CIAC10), Vol. 6078 of Lecture Notes in Computer Science, pp. 359370. Springer.
Bauer, R., Columbus, T., Rutter, I., & Wagner, D. (2013). Search-space size in contraction hierarchies. In Proceedings of the 40th International Colloquium on Automata, Languages, and
626

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

Programming (ICALP13), Vol. 7965 of Lecture Notes in Computer Science, pp. 93104.
Springer.
Bellman, R. (1958). On a routing problem. Quarterly of Applied Mathematics, 16, 8790.
Botea, A. (2011). Ultra-fast optimal pathfinding without runtime search. In Proceedings of the Seventh AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE11), pp. 122127. AAAI Press.
Botea, A. (2012). Fast, optimal pathfinding with compressed path databases. In Proceedings of the
Symposium on Combinatorial Search (SoCS12).
Botea, A., Baier, J. A., Harabor, D., & Hernandez, C. (2013). Moving target search with compressed
path databases. In Proceedings of the International Conference on Automated Planning and
Scheduling ICAPS.
Botea, A., & Harabor, D. (2013a). Path planning with compressed all-pairs shortest paths data. In
Proceedings of the 23rd International Conference on Automated Planning and Scheduling.
AAAI Press.
Botea, A., & Harabor, D. (2013b). Path planning with compressed all-pairs shortest paths data. In
Proceedings of the International Conference on Automated Planning and Scheduling ICAPS.
Botea, A., Strasser, B., & Harabor, D. (2015). Complexity Results for Compressing Optimal Paths.
In Proceedings of the National Conference on AI (AAAI15).
Bulitko, V., Bjornsson, Y., & Lawrence, R. (2010). Case-based subgoaling in real-time heuristic
search for video game pathfinding. J. Artif. Intell. Res. (JAIR), 39, 269300.
Bulitko, V., Rayner, D. C., & Lawrence, R. (2012). On case base formation in real-time heuristic
search. In Proceedings of the Eighth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE-12, Stanford, California, October 8-12, 2012.
Cohen, E., Halperin, E., Kaplan, H., & Zwick, U. (2002). Reachability and distance queries via
2-hop labels. In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 02, pp. 937946, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14(3),
318334.
Delling, D., Goldberg, A. V., Nowatzyk, A., & Werneck, R. F. (2013). PHAST: Hardwareaccelerated shortest path trees. Journal of Parallel and Distributed Computing, 73(7), 940
952.
Delling, D., Goldberg, A. V., Pajor, T., & Werneck, R. F. (2014). Robust distance queries on massive
networks. In Proceedings of the 22nd Annual European Symposium on Algorithms (ESA14),
Vol. 8737 of Lecture Notes in Computer Science, pp. 321333. Springer.
Delling, D., Goldberg, A. V., & Werneck, R. F. (2013). Hub label compression. In Proceedings
of the 12th International Symposium on Experimental Algorithms (SEA13), Vol. 7933 of
Lecture Notes in Computer Science, pp. 1829. Springer.
Demetrescu, C., Goldberg, A. V., & Johnson, D. S. (Eds.). (2009). The Shortest Path Problem: Ninth
DIMACS Implementation Challenge, Vol. 74 of DIMACS Book. American Mathematical
Society.
627

fiS TRASSER , B OTEA , & H ARABOR

Dibbelt, J., Strasser, B., & Wagner, D. (2014). Customizable contraction hierarchies. In Proceedings
of the 13th International Symposium on Experimental Algorithms (SEA14), Vol. 8504 of
Lecture Notes in Computer Science, pp. 271282. Springer.
Dijkstra, E. W. (1959). A note on two problems in connexion with graphs. Numerische Mathematik,
1, 269271.
Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.. J.
Artif. Intell. Res. (JAIR), 30, 213247.
Ford, Jr., L. R. (1956). Network flow theory. Tech. rep. P-923, Rand Corporation, Santa Monica,
California.
Geisberger, R., Sanders, P., Schultes, D., & Delling, D. (2008). Contraction hierarchies: Faster
and simpler hierarchical routing in road networks. In Proceedings of the 7th International
Conference on Experimental Algorithms (WEA08), pp. 319333.
Harabor, D. D., & Grastien, A. (2011). Online graph pruning for pathfinding on grid maps. In Burgard, W., & Roth, D. (Eds.), Proceedings of the Twenty-Fifth AAAI Conference on Artificial
Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press.
Harabor, D. D., & Grastien, A. (2014). Improving jump point search. In Chien, S., Do, M. B.,
Fern, A., & Ruml, W. (Eds.), Proceedings of the Twenty-Fourth International Conference on
Automated Planning and Scheduling, ICAPS 2014, Portsmouth, New Hampshire, USA, June
21-26, 2014. AAAI.
Hart, P. E., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4, 100107.
Hernandez, C., & Baier, J. A. (2011). Fast subgoaling for pathfinding via real-time search.. In
Proceedings of the International Conference on Automated Planning and Scheduling ICAPS11.
Karypis, G., & Kumar, V. (1998). Metis, a software package for partitioning unstructured graphs,
partitioning meshes, and computing fill-reducing orderings of sparse matrices, version 4.0..
Kou, L. T. (1977). Polynomial complete consecutive information retrieval problems. SIAM Journal
on Computing, 6(1), 6775.
Lawrence, R., & Bulitko, V. (2013). Database-driven real-time heuristic search in video-game
pathfinding. Computational Intelligence and AI in Games, IEEE Transactions on, 5(3), 227
241.
Mohapatra, A. (2009). Optimal Sort Ordering in Column Stores is NP-Complete. Tech. rep., Stanford University.
Oswald, M., & Reinelt, G. (2009). The simultaneous consecutive ones problem. Theoretical Computer Science, 410(21-23), 19861992.
Samadi, M., Siabani, M., Felner, A., & Holte, R. (2008). Compressing pattern databases with
learning. In Proceedings of the 2008 Conference on ECAI 2008: 18th European Conference
on Artificial Intelligence, pp. 495499, Amsterdam, The Netherlands, The Netherlands. IOS
Press.
628

fiC OMPRESSING O PTIMAL PATHS WITH RUN L ENGTH E NCODING

Sankaranarayanan, J., Alborzi, H., & Samet, H. (2005). Efficient query processing on spatial networks. In Proceedings of the 13th Annual ACM International Workshop on Geographic Information Systems (GIS05), pp. 200209.
Strasser, B., Harabor, D., & Botea, A. (2014). Fast First-Move Queries through Run Length Encoding. In Proceedings of the Symposium on Combinatorial Search (SoCS14).
Sturtevant, N. (2012a). 2012 Grid-Based Path Planning Competition. https://code.google.
com/p/gppc-2012/.
Sturtevant, N. (2012b). The Website of the Grid-Based Path Planning Competition 2012. http:
//movingai.com/GPPC/.
Sturtevant, N. (2014). The Website of the Grid-Based Path Planning Competition 2014. http:
//movingai.com/GPPC/.
Sturtevant, N., Traish, J., Tulip, J., Uras, T., Koenig, S., Strasser, B., Botea, A., Harabor, D., &
Rabin, S. (2015). The grid-based path planning competition: 2014 entries and results. In
Proceedings of the 6th International Symposium on Combinatorial Search (SoCS15). AAAI
Press.
Uras, T., & Koenig, S. (2014). Identifying hierarchies for fast optimal search. In Brodley, C. E., &
Stone, P. (Eds.), Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,
July 27 -31, 2014, Quebec City, Quebec, Canada., pp. 878884. AAAI Press.
Uras, T., Koenig, S., & Hernandez, C. (2013). Subgoal graphs for optimal pathfinding in eightneighbor grids.. In Proceedings of the International Conference on Automated Planning and
Scheduling ICAPS-13.
van Schaik, S. J., & de Moor, O. (2011). A memory efficient reachability data structure through bit
vector compression. In Proceedings of the 2011 ACM SIGMOD International Conference on
Management of Data, SIGMOD 11, pp. 913924, New York, NY, USA. ACM.
Wang, K.-H. C., & Botea, A. (2011). MAPP: a Scalable Multi-Agent Path Planning Algorithm with
Tractability and Completeness Guarantees. Journal of Artificial Intelligence Research (JAIR),
42, 5590.

629

fiJournal of Artificial Intelligence Research 54 (2015) 369435

Submitted 9/15; published 11/15

Continuing Plan Quality Optimisation
Fazlul Hasan Siddiqui
Patrik Haslum

fazlul.siddiqui@anu.edu.au
patrik.haslum@anu.edu.au

The Australian National University &
NICTA Optimisation Research Group
Canberra, Australia

Abstract
Finding high quality plans for large planning problems is hard. Although some current
anytime planners are often able to improve plans quickly, they tend to reach a limit at
which the plans produced are still very far from the best possible, but these planners fail
to find any further improvement, even when given several hours of runtime.
We present an approach to continuing plan quality optimisation at larger time scales,
and its implementation in a system called BDPO2. Key to this approach is a decomposition
into subproblems of improving parts of the current best plan. The decomposition is based
on block deordering, a form of plan deordering which identifies hierarchical plan structure.
BDPO2 can be seen as an application of the large neighbourhood search (LNS) local search
strategy to planning, where the neighbourhood of a plan is defined by replacing one or more
subplans with improved subplans. On-line learning is also used to adapt the strategy for
selecting subplans and subplanners over the course of plan optimisation.
Even starting from the best plans found by other means, BDPO2 is able to continue
improving plan quality, often producing better plans than other anytime planners when
all are given enough runtime. The best results, however, are achieved by a combination of
different techniques working together.

1. Introduction
The classical AI planning problem involves representing models of the world (initial and
goal states) and available actions in some formal modelling language, and reasoning about
the preconditions and effects of the actions. Given a planning problem, a planning system
(or planner, for short) generates a sequence of actions, whose application transforms the
world from the initial state to a desired goal state. Thus, planning makes an intelligent
system autonomous through the construction of plans of action to achieve its goals.
A key concern in automated planning is producing high quality plans. Planners using
optimal or bounded suboptimal (heuristic) search methods offer guarantees on plan quality,
but are unable to solve large problems. Fast planners, using greedy heuristic search or other
techniques, on the other hand, can solve large problems but often find poor quality plans.
The gap between the capabilities of these two kinds of planners means that producing high
quality plans for large problems is still a challenge. An example of this gap is shown in
Figure 1. We seek to address this gap by proposing a new approach to continuing plan
improvement, that is able to tackle large problems and works at varying time scales.
Anytime search tries to strike a balance between optimal (or bounded suboptimal) and
greedy heuristic search methods. Anytime search algorithms do so by finding an initial
solution, possibly of poor quality, quickly and then continuing to search for better solutions
c
2015
AI Access Foundation. All rights reserved.

fi20
0

10

Plan cost

30

Siddiqui & Haslum

**
**********
*
*
****
0

50

100

150

Problem (sorted)

Figure 1: Illustration of the plan quality gap. The dashed line represents the best (lowestcost) plan for 156 problems from Genome Edit Distance (GED) domain (Haslum, 2011)
found by different non-optimal planners, including anytime planners. The solid line represents the corresponding highest known lower bound. The difference between these two is
the optimality gap. The ? points represent plans found by optimal planners, while the
vertical bars show the optimality gap obtained by a problem-specific algorithm (GRIMM).

the more time they are given. Anytime search algorithms such as, for example, RWA*
(Richter, Thayer, & Ruml, 2010) or AEES (Thayer, Benton, & Helmert, 2012b) have been
successfully used in anytime planners. However, these planners are often not effective at
making use of increasing runtime beyond the first few minutes. Xie, Valenzano, & Muller
(2010) define the unproductive time of a planner as the amount of time remaining when
it finds its best plan, out of the total time given. They show that in four IPC-2011 domains
(Barman, Elevators, Parcprinter, and Woodworking), the unproductive time of the LAMA
planner (which uses RWA*), given 30 minutes per problem, is more than 90%.
We have observed similar results, as shown in Figure 2. The figure shows the average
IPC quality score as a function of time for several anytime planners and plan optimisation
methods, including the LAMA planner. (A full description of the experiment setup, and
results for even more anytime planners, is presented in Section 3, from page 392.) LAMA
finds a first solution quickly: for 92.3% of the problems it solves (within a maximum of 7
hours CPU time per problem), the first plan is found in less than 10 minutes. The quality of
LAMAs plans improve rapidly early on, but the later trend is one of flattening out, i.e.,
decreasing increase. (The drop at the beginning is due to the figure showing the average
plan quality over solved problems: as initial, low-quality, plans for more problems are found
the average drops, before increasing again as better plans are found.) Between 1 and 7
hours CPU time, LAMA improves the plans for 21.3% of solved problems. Yet for a further
51.6% of problems better plans exist, and are found by other methods. In the same time
interval, LAMAs average plan quality score increases by only 2.7%, while an increase of
370

fiContinuing Plan Quality Optimisation

0.96
0.95

Average Quality Score (Relative IPC Quality Score / Coverage)

0.94
0.93
0.92
0.91




































































































0.9
0.89
0.88
0.87
0.86
0.85
0.84


0.83









































BDPO2 on PNGS on base plans
BDPO2 on base plans
PNGS on base plans
IBCS on base plans
BSS on base plans
LAMA from scratch
IBaCoP2 from scratch








0.82



7

6.5

6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0.81

Time (hours)

Figure 2: Average IPC quality score as a function of time per problem, on a set of 182
large-scale planning problems. The quality score of a plan is cref/c, where c is the cost
of the plan and cref the reference cost (least cost of all plans for the problem); hence
a higher score represents better plan quality. Anytime planners (LAMA, IBaCoP2) start
from scratch, while post-processing (PNGS, BDPO) and bounded-cost search (IBCS, BeamStack Search) methods start from a set of base plans. Their curves are delayed by 1 hour to
account for the maximum time given to generating each base plan. The experiment setup
and results for additional planners are described in Section 3.1 (page 392).

371

fiSiddiqui & Haslum

Figure 3: General framework of BDPO2

at least 14.6% is possible. Memory-limited branch-and-bound algorithms, like Beam Stack
Search (Zhou & Hansen, 2005) may run indefinitely, but find improvements very slowly.
The increase in average plan quality made by BSS over the entire time depicted in Figure
2 is only 1.8%.
Plan optimisation approaches based on post-processing start with a valid plan and seek
to improve it. Figure 2 shows results for Plan Neighbourhood Graph Search (Nakhost
& Muller, 2010). PNGS searches for shortcuts in a subgraph of the state space of the
problem, constructed around the current plan. (The PNGS implementation used in this
experiment also applies Nakhosts and Mullers action elimination technique.) Applying
PNGS results in substantial plan quality improvements quickly  94.8% of improved plans
are found in less than 10 minutes  but then stops, as it runs out of memory.
In summary, this experiment shows that current anytime plan optimisation methods
become unproductive as runtime increases, or suffer from a very slow rate of plan quality
improvement.
We present a post-processing approach to plan optimisation, and its implementation in
a system called BDPO2. (The source code for BDPO2 is provided as an on-line appendix
to this article.) As a post-processor, BDPO2 does not work on its own: it depends on other
methods providing an initial plan. In the experiment, the set of input plans (referred to as
base plans) are the best plans found by LAMA after 1 hour, or the plan found by IBaCoP2
in the 2014 IPC. What Figure 2 shows is that switching to our approach after some time can
overcome the limitation of current anytime planning techniques, and continue to improve
plan quality as allotted time increases. The best result, as shown, is obtained by chaining
several techniques together, applying first PNGS on the base plans, and then BDPO2 on
the best result produced by PNGS. This result could not be achieved by previous anytime
planning approaches alone.
BDPO2 uses Large Neighborhood Search (LNS), a local search technique. The local
search explores a neighbourhood around the current solution plan for a better quality valid
plan. In LNS, the neighbourhood of a solution defined by destroy and repair methods,
which together replace a part of the current solution, while keeping the rest of it unchanged.
In BDPO2, the destroy step selects a subsequence of some linearisation of a deordering of
the current plan (we call this a window) and the repair step applies a bounded-cost
planner to the subproblem of finding a better replacement for this subplan. This focus
on solving smaller subproblems makes local search, and LNS in particular, scale better
to large problems. The size and structure of the neighborhood, however, plays a crucial
372

fiContinuing Plan Quality Optimisation

role in the performance of local search (Hoffmann, 2001). In our setting, the neighbourhood is determined by the strategies used to select windows and subplanners. The destroy
methods used in LNS algorithms often contain an element of randomness, and the local
search may accept moves to lower-quality solutions (Ropke & Pisinger, 2006; Schrimpf,
Schneider, Stamm-Wilbrandt, & Dueck, 2000). In contrast, we explore the neighbourhood
systematically, examining candidate windows generated and ordered by several heuristics,
and accept only moves to strictly better plans. We also introduce into LNS the idea of
delayed restarting, meaning that we search for and combine multiple local improvements
before restarting the next iteration from the new best plan. We have found that delayed
restarts allow better exploration of subplans from different parts of the current plan, and
helps avoid local minima that otherwise occur when the system attempts to re-optimise the
same part of the plan in successive iterations.
The BDPO2 framework, shown in Figure 3, broadly consists of four components: plan
decomposition, LNS (i.e., the repeated destroy and repair steps), windowing, and on-line
adaptation. The first step, decomposition, uses deordering to produce a partially ordered
plan. Deordering enables the windowing strategies to find subplans that are easier to
improve on, leading to much better anytime performance. We use block deordering (Siddiqui
& Haslum, 2012), which simultaneously decomposes a given plan into coherent subplans,
called blocks, and relaxes ordering constraints between blocks. Block deordering removes
some of the inherent limitations of existing, step-wise deordering techniques, and is able
to deorder sequential plans in some cases where no step-wise deordering is possible. The
windowing component is a collection of strategies for extracting windows from the block
deordered plan, and ranking policies which order the windows so that the system attempts
to optimise more promising windows first.
BDPO2 extends our earlier system, BDPO (Siddiqui & Haslum, 2013b), mainly by using
a variety of alternatives for each task: where BDPO used a single windowing strategy (with
no ranking) and a single subplanner, BDPO2 uses portfolios of window generation and
ranking strategies and several subplanners. This improves the capability and robustness of
the system, since no single alternative (windowing strategy, subplanner, etc.) dominates
all others across all problems. Furthermore, we take advantage of the fact that the system
solves many subproblems over the course of the local search to learn on-line which are the
best alternatives for the current problem. In particular, we use the UCB1 multi-armed
bandit learning policy (Auer, Cesa-Bianchi, & Fischer, 2002) for subplanner selection, and
a sequential portfolio of window ranking policies.
The remainder of this article is structured as follows: Section 2 describes block deordering. The theory of block deordering presented here is slightly different from our earlier
account (Siddiqui & Haslum, 2012), allowing for more deordering in some cases and better contrasting it with traditional partially ordered plan semantics. Section 3 presents an
overview of the BDPO2 system and main empirical results, while Sections 4 and 5 give
more details of the windowing and on-line adaptation components, respectively, including
empirical analysis of their impact on the performance of the system as a whole. Section 6
reviews related work, and Section 7 presents conclusions and outlines ideas for future work.
373

fiSiddiqui & Haslum

2. Plan Decomposition
Our approach to continuing plan quality improvement is based on optimising a plan by
parts, one at a time. Every subplan that we consider for local optimisation is a subsequence
of some linearisation of a partially ordered plan. Therefore, a key step is removing unnecessary ordering constraints from the, typically sequential, input plan. This process is called
plan deordering. The importance of deordering is demonstrated by one of our experiments
(presented in Section 3.6, page 402), in which we apply BDPO2 to input plans that are
already of high quality: The total plan quality improvement (measured by the increase in
the average IPC plan quality score) achieved by BDPO2 without any deordering is 28.7%
less than that achieved with BDPO2 using our plan deordering technique.
The standard notion of a valid partially ordered plan requires all unordered steps in the
plan to be non-interfering (i.e., for two subsequences of the plan to be unordered, every
interleaving of steps from the two must form a valid execution). This limits the amount of
deordering that can be done, in some cases to the extent that no deordering of a sequential
plan is possible. (An example of this situation is shown in Figure 6 on page 381.) To
remedy this, we have introduced block deordering (Siddiqui & Haslum, 2012), which creates
a hierarchical decomposition of the plan into non-interleaving blocks and deorders these
blocks. This makes it possible to deorder plans further, including in some cases where
conventional, step-wise, deordering is not possible. (Again, an example can be found in
Figure 6 on page 381.) In this section, we present a new, and slightly different account of the
theory and practice of block deordering. First, it relaxes a restriction on block deordered
plans, thereby allowing more deordering of some plans. Second, it contrasts the semantics
of block decomposed partially ordered plans with the traditional partially ordered plan
semantics in a clearer way.
Sections 2.12.3 describe necessary background, while Sections 2.42.6 introduce block
decomposed partially ordered plans and the block deordering algorithm.
2.1 The Planning Problem, Sequential Plan and its Validity
We consider the standard STRIPS representation of classical planning problems with action
costs. A planning problem is a tuple  = hM, A, C, I, Gi, where M is a set of atoms
(alternatively called fluents or propositions), A is a set of actions, C : A  R0+ is a cost
function on actions, which assigns to each action a non-negative cost, I  M is the initial
state, and G  M is the goal.
An action a is characterised by a triple hpre(a), add(a), del(a)i, where pre(a), add(a),
and del(a) are the preconditions, add and delete effects of a respectively. We also say that
action a is a consumer of an atom m if m  pre(a), a producer of m if m  add(a), and
a deleter of m if m  del(a). An action a is applicable in a state S if pre(a)  S, and
if applied in S, results in the state apply(a, S) = (S \ del(a))  add(a). A sequence of
actions  = hai , ai+1 , ..., aj i is applicable in a state Si if (1) pre(ak )  Sk for all i  k  j,
and (2) Si+1 = apply(ai , Si ), Si+2 = apply(ai+1 , Si+1 ), and so on; the resulting state is
apply(, Si ) = Si+j+1 .
A valid sequential plan (also totally ordered plan) seq = ha1 , ..., an i for a planning
problem  is a sequence of actions that is applicable in I and such that G  apply(seq , I).
The actions of seq must be executed in the specified order.
374

fiContinuing Plan Quality Optimisation

2.2 The Partially Ordered Plan and its Validity
Plans can be partially ordered, in which case actions can be unordered with respect to
each other. A partially ordered plan (p.o. plan) is a tuple, pop = hS, i, where S is a
set of steps (each of which is labelled by an action from A) and  represents a strict (i.e.,
irreflexive) partial order over S. The unordered steps in pop can be executed in any order.
+ denotes the transitive closure of . An element hsi , sj i  (also si  sj ) is a basic
ordering constraint iff it is not transitively implied by other constraints in . For a plan
step s, we use pre(s), add(s) and del(s) to denote the preconditions, add and delete effects
of the action associated with s. We also use the terms producer, consumer, deleter, and
cost for plan steps, referring to their associated actions. We include in S two more steps,
sI and sG . sI is ordered before all other steps, consumes nothing and produces the initially
true atoms, while sG is ordered after all other steps, consumes the goal atoms and produces
nothing.
A linearisation of pop is a total ordering of the steps in S that respects . A p.o. plan
pop is valid (for a planning problem ) iff every linearisation of pop is a valid sequential
plan (for ). In other words, a p.o. plan can be viewed as a compact representation of a
set of totally ordered plans, namely its linearisations.
Every basic ordering constraint, si  sj , in pop has a set of associated reasons, denoted
by Re(si  sj ). These reasons explain why the ordering is necessary for the plan to be
valid: If Re(si  sj ) is non-empty, then some step precondition may be unsatisfied before
its execution in some linearisations of pop that violate si  sj . The reasons are of three
types:
PC(m) (producerconsumer of atom m): The first step, si , produces m which is a precondition of the second step, sj . Thus, if the order is changed, and sj executed before si ,
the precondition of sj may not have been established when it is required.
CD(m) (consumerdeleter of m): The second step, sj deletes m, which is a precondition of
si . Thus, if the order is changed, m may be deleted before it is required.
DP(m) (deleterproducer of m): The first step, si deletes m, which is produced by the
second step, sj . If the order is changed, the add effect of the producer step may be
undone by the deleter, causing a later step to fail. It is, however, not necessary to
order a producer and deleter if no step that may occur after the producer in the plan
depends on the added atom.
Note that an ordering constraint can have several associated reasons, including several
reasons of the same type but referring to different atoms. The producerconsumer relation
PC(m)  Re(si  sj ) is usually called a causal link from si to sj for m (McAllester &
Rosenblitt, 1991), and denoted by a triple hsi , m, sj i. A causal link hsi , m, sj i is threatened
if there is any deleter of m that may be ordered between the last producer of m before sj
and sj , since this implies a possibility of m being false when required for the execution of
sj . The formal definition is as follows.
Definition 1. Let pop = hS, i be a p.o. plan, and hsp , m, sc i be a causal link in pop .
hsp , m, sc i is threatened if there is a step sd that deletes m such that neither (1) sc + sd
nor (2) s0p : m  add(s0p )  sd + s0p + sc is true.
375

fiSiddiqui & Haslum

As mentioned above, a p.o. plan, pop = hS, i for a planning problem  is valid iff
every linearisation of pop is a valid sequential plan for . However, the following theorem
gives an alternative, equivalent, condition for p.o. plan validity.
Theorem 1 (e.g., Nebel & Backstrom, 1994). A p.o. plan is valid iff every step precondition
can be supported by a causal link such that there is no threat to that causal link.
This condition is the same as Chapmans (1987) modal truth criterion, that
sc  S, m  pre(sc ) :
sp  S : (PC(m)  Re(sp  sc )
st : m  del(st )  sc + sd  s0p : m  add(s0p )  sd + s0p + sc



.

2.3 Deordering
The process of deordering converts a sequential plan into a p.o. plan by removing ordering
constraints between steps, such that the steps of the plan can be successfully executed in
any order consistent with the partial order and still achieve the goal (Backstrom, 1998). We
will refer to this as step-wise deordering, to distinguish it from the block decomposition and
deordering that we introduce later in this section. Since current state space search planners
can produce sequential plans very efficiently, deordering plays an important role in efficient
generation of p.o. plans.
Let pop = hS, i be a valid p.o. plan. A (step-wise) deordering of pop is a valid plan
0
0
pop
= hS, 0 i such that (0 )+ + . That is, pop
is the result of removing some basic
ordering constraints without invalidating the plan. A sequential plan seq = ha1 , ..., an i can
be represented as a p.o. plan with one step si  S for each action ai in seq and an ordering
si  sj whenever i < j, so that no two steps in S which are unordered. Thus, deordering a
sequential plan is no different from (further) deordering a p.o. plan.
Computing a (step-wise) deordering with a minimum number of ordering constraints is
NP-hard (Backstrom, 1998), but there are several non-optimal algorithms (e.g., Pednault,
1986; Veloso, Perez, & Carbonell, 1990; Regnier & Fade, 1991). We have used a variant of
the explanation-based generalisation algorithm by Kambhampati and Kedar (1994). The
algorithm works in two phases: In the first phase it constructs a validation structure, which
has exactly one causal link hsp , m, sc i for each precondition m of each step sc . sp is chosen
as the earliest producer of m preceding sc in the input plan, with no intervening threatening
step (i.e., that deletes m) between sp and sc . (The algorithm by Veloso, Perez and Carbonell
is similar, but selects the latest producer instead.) In the second phase, the algorithm builds
a partial ordering, keeping only those orderings in the original plan which either correspond
to causal links in the validation structure or that are required to prevent a threatening step
from becoming unordered w.r.t. the steps in such a causal link.
Kambhampati and Kedars deordering algorithm, due to its greedy strategy, does not
guarantee optimality. An example where it fails to transform a totally ordered plan to
a least-constrained plan is shown in Figure 4. However, a recent study found that the
algorithm did produce optimal step-wise plan deorderings of all plans on which it was
tested (Muise, McIlraith, & Beck, 2012).
However, our motivation for plan deordering is to find a deordering that is adequate for
generating useful candidate subplans for local optimisation. More important than achieving
376

fiContinuing Plan Quality Optimisation

Figure 4: An example on which Kambhampati and Kedars (1994) algorithm fails to find
the least constrained plan. (Derived from Figure 14 in Backstroms 1998 article on plan
deordering.) Figure (a) is a sequential input plan, (b) is the plan produced by the algorithm
after choosing the earliest producer (for the validation structure) of the preconditions p and
q of D, and (c) is the minimally ordered version of (a). For simplicity, the goal atoms
produced by the steps A, B, C and D are not shown in the figure.

an optimal step-wise deordering is overcoming the inherent limitation of step-wise deordering, which only allows plan steps to be unordered when they are non-interfering. Block
deordering, described in the next two sections, can remove further orderings from input
plans by forming blocks, which helps generate a decomposed plan that is more suitable for
extracting subplans for local optimisation.
2.4 Block Decomposition
In a conventional p.o. plan, whenever two subplans are unordered every interleaving of steps
from the two forms a valid execution. This limits deordering to cases where individual steps
are non-interfering. To remove this restriction, we have proposed block decomposed partial
ordering, which restricts the interleaving of steps by dividing the plan steps into blocks, such
that the steps in a block must not be interleaved with steps not in the block. However,
steps within a block can still be partially ordered. This is illustrated with an example in
Figure 5. The figure shows the difference in linearisations between a p.o. plan and a block
decomposed p.o. plan. b, a, c, d is a valid linearisation in the standard partial ordering but
not in the block decomposed p.o. plan. The formal definition of a block is as follows.
Definition 2. Let pop = hS, i be a p.o. plan. A block w.r.t. , is a subset b  S of steps
such that for any two steps s, s0  b, there exists no step s00  (S \b) such that s + s00 + s0 .
A decomposition of a plan into blocks can be recursive, i.e., a block can be wholly
contained in another. However, blocks cannot be partially overlapping. Two blocks are
ordered bi  bj if there exist steps si  bi and sj  bj such that si  sj and neither block is
contained in the other (i.e., bi 6 bj and bj 6 bi ).
Definition 3. Let pop = hS, i be a p.o. plan. A set B of subsets of S is a block
decomposition of pop iff (1) each b  B is a block w.r.t.  and (2) for every bi , bj  B,
either bi  bj , bj  bi , or bi and bj are disjoint. A block decomposed plan is denoted by
bdp = hS, B, i.
377

fiSiddiqui & Haslum

Figure 5: A normal p.o. plan (left) represents the set of all sequential plans that are linearisations of the plan steps, in this example ha, b, c, di, hb, a, c, di, hb, c, a, di, and hb, c, d, ai.
A block decomposed p.o. plan (shown on the right with dashed outlines for blocks) allows
unordered blocks to be executed in any order, but not steps from different blocks to be
interleaved. Thus, only ha, b, c, di, hb, c, a, di, and hb, c, d, ai are possible linearisations of
this plan.

The semantics of a block decomposed plan is defined by restricting its linearisations (for
which it must be valid) to those that respect the block decomposition, i.e., that do not
interleave steps from disjoint blocks. If bi  bj , all steps in bi must precede all steps in bj
in any linearisation of the block decomposed plan.
Definition 4. Let bdp = hS, B, i be a block decomposed p.o. plan for planning problem
. A linearisation of bdp is a total order lin on S such that (1) lin and (2) every
b  B is a block w.r.t. lin . bdp is valid iff every linearisation of bdp is a plan for .
Blocks behave much like (non-sequential) macro steps, having preconditions, add and
delete effects that can be a subset of the union of those of their constituent steps. This
enables blocks to encapsulate some plan effects and preconditions, reducing interference
and thus allowing more deordering. The following definition captures those preconditions
and effects that are visible from outside the block, i.e., those that give rise to dependencies
or interference with other parts of the plan. These are what we need to consider when
deciding if two blocks can be unordered. (Note that a responsible step is a step in a block
that causes it to produce, consume or threaten an atom.)
Definition 5. Let bdp = hS, B, i be a block decomposed p.o. plan, and b  B be a block.
The block semantics are defined as:
 b adds m iff b does not have precondition m, and there is a responsible step s  b with
m  add(s), such that for all s0  b, if s0 deletes m then s0  s.
 b has precondition m iff there is a responsible step s  b with m  pre(s), and there
is no step s0  b such that there is a causal link hs0 , m, si without an active threat.
 b deletes m iff there is a responsible step s  b with m  del(s), and there is no step
s0  b with s  s0  that adds m.
Note that if a block consumes a proposition, it cannot also produce the same proposition.
The reason for this is that taking the black box view of block execution, the proposition
simply persists: it is true before execution of the block begins and remains true after it has
finished. If the steps within a block are totally ordered, the preconditions and effects of a
block according to Definition 5 are nearly the same as the cumulative preconditions and
378

fiContinuing Plan Quality Optimisation

effects of an action sequence defined by Haslum and Jonsson (2000), the only difference
being that a consumer block cannot also be a producer of the same proposition.
A conventional p.o. plan, to be valid, must not contain any threat to a causal link. In
contrast, a block decomposed p.o. plan allows a threat to a causal link to exist in the plan,
as long as the causal link is protected from that threat by the block structure. A causal
link is protected from a threat iff either (i) the causal link is contained by a block that does
not contain the threat, or (ii) the threat is contained by a block that does not contain the
causal link and does not delete the threatened atom (i.e., encapsulates the delete effect). A
threat to a causal link is active if the link is not protected from it, otherwise inactive. The
formal definition is as follows.
Definition 6. Let bdp = hS, B, i be a block decomposed p.o. plan, and st  S be a threat
to a causal link hsp , m, sc i in bdp . hsp , m, sc i is protected from st  S iff there exist a block
b  B such that either of the following is true: (1) sp , sc  b; st 
/ b; or (2) st  b, sp , sc 
/ b,
and m 
/ del(b).
An example of how the block decomposition protects a causal link can be seen in Figure
7(i) on page 382.
The following theorem provides an alternative criterion for the validity of a block decomposed p.o. plan, in analogy with the condition for a conventional p.o. plan given in
the theorem cited above. The only difference is that a block decomposed p.o. plan allows
threats to causal links, as long as those threats are inactive. Let bdp = hS, B, i be a block
decomposed p.o. plan. Analogously with Chapmans modal truth criterion, this condition
can be stated as follows:
sc  S, m  pre(sc )
sp  S : (m  add(sp )
st  S : (m  del(st )  st 6+ sp  sc 6+ st  hsp , m, sc i is protected from st )).
Theorem 2. A block decomposed p.o. plan is valid iff every step precondition is supported
by a causal link that has no active threat.
Proof. Let bdp = hS, B, i be a block decomposed p.o. plan of a planning problem . Let
us first prove the if part, i.e., that if every step precondition is supported by a causal
link that has no active threat then every linearisation of bdp is a valid plan for . Let
seq = h. . . , sc , . . .i be an arbitrary linearisation of bdp with a total order seq on S, where
m  pre(sc ). Then, according to the validity criteria for a sequential plan, we have to show
that m must be satisfied before the execution of sc in seq . Since every step precondition
is supported by a causal link in bdp that has no active threat, m must be supported by a
causal link hsp , m, sc i that has no active threat. Moreover, since seq then sp seq sc .
Let st be a threat to hsp , m, sc i in bdp . Clearly, sp seq st seq sc is the only possibility
that may cause m to be unsatisfied before the execution of sc . Since hsp , m, sc i has no active
threat, hsp , m, sc i is protected from st , and therefore, according to Definition 6, either (1)
sp , sc  b and st 
/ b, or (2) st  b, sp , sc 
/ b, and m 
/ del(b), must hold. If (1) is true,
then sp seq st seq sc can not occur in any valid linearisation of bdp , since it interleaves
steps sp , sc  b with st 
/ b, and thus b is not a block w.r.t. seq . In the second case, since
379

fiSiddiqui & Haslum

m
/ del(b) then there must be a producer of m, s0p  b, such that st seq s0p . Moreover,
since sp , sc 
/ b, sp seq st seq sc can only be true if sp seq st seq s0p seq sc . This also
makes m true before the execution of sc in seq .
Let us now prove the only if part, i.e., that if bdp is valid then every step precondition
is supported by a causal link that has no active threat. Let sc  S, m  pre(sc ), and
seq = h. . . , sc , . . .i be a linearisation of bdp with a total order seq on S. We consider two
possible situations: (1) there is no producer s0 from which a causal link hs0 , m, sc i in bdp
can be constructed, or (2) there is at least one such producer that can construct the causal
link with sc for the atom m but that causal link has an active threat in bdp . We will show
that none of the above situations can happen as long as bdp is valid. According to situation
(1), there is no s0 in seq as well such that s0 seq sc . This causes m to be unsatisfied before
the execution of sc in seq , i.e., seq become invalid. Consequently, bdp become invalid
(since one of its linearisation is invalid), which contradicts with our assumption. Therefore,
there must exist at least one producer s0 that can construct a causal link hs0 , m, sc i in bdp .
Now, for situation (2), assume sp is the last producer of m before the execution of sc in
seq , i.e., s0p  S \ sp : m  add(s0p )  (s0p seq sp  sc seq s0p ). Let sp be the producer
in the causal link hsp , m, sc i in bdp (which is possible, since sp is not ordered after sc in
bdp ). Assume hsp , m, sc i has an active threat st in bdp . Since hsp , m, sc i has an active
threat st (i.e., hsp , m, sc i is not protected from st ), then neither (i) sp , sc  b; st 
/ b, nor
(ii) st  b; sp , sc 
/ b, and m 
/ del(b), is true. Therefore, sp seq st seq sc is a possible
linearisation of bdp . Moreover, since there is no more producer of m in between sp and sc ,
m must be unsatisfied before the execution of sc , i.e., seq becomes invalid. Consequently,
bdp is invalid since one of its linearisations is invalid. Therefore, hsp , m, sc i must not have
any active threat.
2.5 Block Deordering
Block deordering (Siddiqui & Haslum, 2012) is the process of removing orderings between
plan steps by adding blocks to a block decomposed p.o. plan. It may also add to the plan
some new ordering constraints, but those are transitively implied by the other ordering
constraints. Block deordering can often remove ordering constraints where step-wise deordering can not. This is because the no-interleaving restriction among the blocks affords
us a simplified, black box, view of blocks that localises some interactions, in which only
the preconditions and effects of executing the block as a whole are important. Thus, it allows further deordering by being able to ignore some dependencies and effects that matter
only internally within the block. In addition to providing more linearisations, by improving
deordering, the blocks formed by block deordering often correspond to coherent, more selfcontained subplans, and form the basis for the windowing strategies (described in detail
in Section 4) that we use to generate candidate subplans for local optimisation.
This subsection presents the conditions under which adding blocks to a block decomposition allows the removal of basic ordering constraints. The complete block deordering
algorithm is presented in the next subsection.
As a simple example of block deordering, Figure 6(i) shows a sequential plan for a small
Logistics problem. This plan can not be deordered into a conventional p.o. plan, because
each plan step has a reason to be ordered after the previous. Block deordering, however,
380

fiContinuing Plan Quality Optimisation

Figure 6: A sequential plan and a block deordering of this plan with two unordered blocks
b1 and b2. Ordering constraints are labelled with their reasons: producerconsumer (PC),
i.e., causal link, deleterproducer (DP), and consumerdeleter (CD). Note that no ordering
constraint in the sequential plan can be removed without invalidating it. Thus, step-wise
deordering of this plan is not possible.

is able to break the ordering s3  s4 by removing the only reason PC(at P1 A) based on
the formation of two blocks b1 and b2 as shown in Figure 6(ii). Neither of the two blocks
delete or add the atom at P1 A (although it is a precondition of both). This removes
the interference between them, and allows the two blocks to be executed in any order but
without any interleaving. Therefore, the possible linearisations of the block decomposed
p.o. plan are only hs1, s2, s3, s4i and hs4, s1, s2, s3i. Note that if b2 is ordered before b1,
then b1 can be optimised by removing step s3.
Besides the necessary orderings between a pair of steps in a plan due to reasons PC,
CD, and DP (stated in Section 2.2), a valid block decomposed p.o. plan must maintain one
more type of necessary ordering, called threat protection ordering. If removing an ordering
sx + sy causes a block containing both steps to have delete effect, which it did not have
with this ordering, and that delete effect causes a causal link outside the block to become
unprotected (not satisfying either of the two conditions of Definition 6), then sx + sy is a
threat protection ordering, which may not be removed. A threat protection ordering can be
introduced during the block deordering process, and once introduced can not be removed.
This is demonstrated in Figure 7, where removing this kind of ordering leads to an invalid
block decomposed p.o. plan. The threat protection ordering is defined formally as follows.
Definition 7. Let bdp = hS, B, i be a block decomposed p.o. plan, and hsp , m, sc i be a
causal link that is protected from st  S in bdp . Let b  B; st , s0  b; sp , sc 
/ b; m  add(s0 );
m
/ del(b); and st + s0 . st + s0 is a threat protection ordering if breaking this ordering
causes m  del(b) and that causes hsp , m, sc i to become unprotected from st .
381

fiSiddiqui & Haslum

Figure 7: Two block decompositions of a plan containing five steps: s1, s2, s3, s4, and s5.
In decomposition (i), there are three (transitively reduced) necessary orderings: s1  s2,
s2  s3, and s4  s5, where Re(s1  s2) = {DP(m), DP(n)}, Re(s2  s3) = {PC(m)},
and Re(s4  s5) = {PC(n)}. This decomposition is valid since every step precondition is
satisfied by a causal link without active threats. The threat from s1 to causal link hs4, n,
s5i is inactive, since the link is protected by block bx = {s1, s2, s3} which contains s1 but
does not delete m, and is disjoint from the causal link. By forming two blocks, by = {s1}
and bz = {s2, s3} it would be possible to remove s1  s2, as shown in (ii), since hs2, m, s3i
is then protected from s1 by bz . However, in this decomposition the delete effect of block bx
becomes del(bx ) = {m, n}, and the block therefore no longer protects hs4, n, s5i. Therefore,
this decomposition and deordering is invalid. The ordering s1  s2 is a threat protection
ordering, which must not be broken. Note that in (i) s2 has no consumers of its produced
atom n, yet acts as a white knight for hs4, n, s5i to protect n from the deleter s1.

The notion of threat protection ordering was missing from our earlier block deordering
procedure (Siddiqui & Haslum, 2012), which relied (implicitly) on the stronger restriction
that the delete effects of a block do not change due to subsequent deordering inside the block.
Explicitly checking only the necessary threat protection orderings allows more deordering
inside created blocks to take place.
To remove a basic ordering, si  sj , from a block decomposed p.o. plan bdp = hS, B, i,
we create two blocks, bi and bj , where si  bi , sj  bj , and bi  bj = . Note that one of the
two blocks can consist of a single step. Both blocks must be consistent with the existing
decomposition, i.e., B  {bi , bj } must still be a valid block decomposition, in the sense of
Definition 2. In the remainder of this subsection, we define four rules which state conditions
on blocks bi and bj that allow different reasons for the ordering si  sj to be eliminated.
Since the ordering si  sj can exist for several reasons (including several reasons of the
same type, referring to different atoms), it is only if blocks bi and bj can be found that
allow us to remove every reason in Re(si  sj ) that the ordering between the steps can be
removed.
Rule 1. Let bdp = hS, B, i be a valid block decomposed p.o. plan, si  sj be a basic
ordering whose removal does not cause any threat protection ordering to be removed, and
PC(m)  Re(si  sj ). Let bi be a block, where si  bi , sj 
/ bi , and s0  bi : si  s0 . PC(m)
can be removed from Re(si  sj ) if m  pre(bi ) and sp 
/ bi such that sp can establish a
causal link to bi and to sj .
382

fiContinuing Plan Quality Optimisation

Figure 8: Formation of a block {s,p} and addition of a causal link hr,m,qi in (ii) in order
to remove the reason PC(m) behind the basic ordering constraint p  q from (i). Different
situations, (iii and iv), where a threat, t, may be active to hr,m,qi.

As an explanation of Rule 1, if PC(m)  Re(si  sj ), then bi must not produce m. Since
si produces m and is not followed by a deleter of m within bi (because si  sj is a basic
ordering and sj 
/ bi ) the only way for this to happen is if bi consumes m. Since the plan is
valid, there must be some producer, sp 
/ bi , that necessarily precedes the step (in bi ) that
+
consumes m. Note that sp  sj . Then adding the causal link PC(m) to Re(sp  sj ) (i.e.,
adding hsp , sj i to  if not already present) allows PC(m) to be removed from Re(si  sj ).
Theorem 3. Deordering according to Rule 1 preserves plan validity.
Proof. Let bdp = hS, B, i be a valid block decomposed p.o. plan. Therefore, according to
Theorem 2, every step precondition of bdp is supported by a causal link that has no active
threat. Let p  q be a basic ordering constraint (where p, q  S), bp , bq  B be the blocks
that meet the conditions for removing PC(m)  Re(p  q), and bp , bq are not ordered for
any other ordering constraints. We will show that removing PC(m) from Re(p  q) results
0
in a new plan, bdp
= hS, B 0 , 0 i, that meets the condition of Theorem 2, and therefore
remains valid.
Assume PC(m)  Re(p  q) is removed, and the precondition of q is now supplied by the
step r based on the newly established causal link hr,m,qi after deordering and formulating
0
bp = {s,...,p}, bq = {q} in bdp
, as shown in Figure 8 (ii). We have to show that hr,m,qi
0
0
has no active threat in bdp , and therefore, bdp
is valid. Assume, there is an active threat,
0
+
t, to hr,m,qi in bdp . Then, of course, t  r and q + t. We will examine every other
situation, where t can be an active threat to hr,m,qi.
Situation (1): assume s + t, as shown in Figure 8 (iii). Since t is not an active threat
to hr,m,si in bdp , then according to Theorem 2, either t is contained by a block that does
383

fiSiddiqui & Haslum

Figure 9: Formation of blocks for removing the reason CD(m) behind the basic ordering
p  q.

not delete the threatened atom and does not contain hr,m,si, or hr,m,si is contained by a
0
block b0 = {r, s, ...} that does not contain t. For the first case, it also holds true in bdp
, and
0
therefore, t can not be an active threat to hr,m,qi. For the second case, b can not partially
overlap with bp = {s,...,p}, therefore, either bp  b0 or b0  bp . If bp  b0 , bp must contain r,
which can not happen according to the PC removing criteria (i.e., r 
/ bp must hold) stated
in Rule 1. If b0  bp , then b0 must contain at least r, s, and p, because b0 can not partially
overlap with bp = {s,...,p}. Since t is also not an active threat to hp,m,qi in bdp , hp,m,qi
must be contained by some block b00 = {p, q, ...} that does not contain t. Now, since b0 and
b00 can not partially overlap, b0 or b00 (whichever is bigger) must contain at least r, s, p, and
q, for which b0 or b00 (whichever is bigger) protects hr,m,qi from t.
Situation (2): assume t + p, also shown in Figure 8 (iii). Since t is also not an active
threat to hp,m,qi in bdp , like before, we can show that either t is contained by a block that
encapsulates the threatened atom (i.e., does not delete m) and does not contain hp,m,qi,
or hp,m,qi is contained by a block b0 = {r, s, p, q, ...} that does not contain t. In both cases,
hr,m,qi is protected from t.
Situation (3): assume s + t + p shown in Figure 8 (iv). This is only possible if t  bp ,
since t can not interleave with the steps in bp if t 
/ bp . Therefore, t  bp , which causes
hr,m,qi to be protected from t. This is because bp does not contain hr,m,qi and does not
delete m (since m  add(p) and t + p).
Therefore, we can conclude that t can never be an active threat to hr,m,qi under any
situation.
Rule 2. Let bdp = hS, B, i be a valid block decomposed p.o. plan, si  sj a basic ordering
whose removal does not cause any threat protection ordering to be removed, and CD(m) 
384

fiContinuing Plan Quality Optimisation

Re(si  sj ). Let bi and bj be two blocks, where si  bi , sj  bj , and bi  bj = . Then
CD(m) can be removed from Re(si  sj ) if bi does not consume m.
Theorem 4. Deordering according to Rule 2 preserves plan validity.
Proof. Let bdp = hS, B, i be a valid block decomposed p.o. plan, and p  q be a basic
ordering constraint, where p, q  S with CD(m)  Re(p  q). In order to meet the
condition of Rule 2, let us assume bp is a block that includes r and p such that hr,m,pi is a
causal link and every other consumer of m in bp (if they exist) is ordered after r in bdp (as
shown in Figure 9 (i)). Therefore it meets the condition that bp must not consume m. Also,
assume bq is a block that contains {q} and bp , bq are not ordered for any other ordering
constraints. Therefore, CD(m)  Re(p  q) as well as p  q are removed, which results a
0
0
new plan bdp
= hS, B 0 , 0 i. We will show that bdp
is valid according to Theorem 2.
Since bdp is valid, there is no active threat to any causal link in bdp according to
Theorem 2, but due to the deordering of p  q, the deleter q becomes a new threat only to
0
. However, hr,m,pi is contained by bp that does not contain
the causal link hr,m,pi in bdp
q, and therefore, according to Definition 6, hr,m,pi is protected from q, i.e., q becomes an
0
remains valid.
inactive threat. As a result, bdp
Rule 3. Let bdp = hS, B, i be a valid block decomposed p.o. plan, si  sj a basic ordering
whose removal does not cause any threat protection ordering to be removed, and CD(m) 
Re(si  sj ). Let bi and bj be two blocks, where si  bi , sj  bj , and bi  bj = . Then
CD(m) can be removed from Re(si  sj ) if bj does not delete m.
Theorem 5. Deordering according to Rule 3 preserves plan validity.
Proof. Let bdp = hS, B, i be a valid block decomposed p.o. plan, and p  q be a basic
ordering constraint, where p, q  S with CD(m)  Re(p  q). In order to meet the condition
of Rule 3, let us assume bq is a block that includes q and s such that DP(m)  Re(q  s)
and every other deleter of m in bq (if they exist) is ordered before s in bdp (as shown in
Figure 9 (ii)). Therefore it meets the condition that bq must not delete m. Also, assume bp
is a block that contains {p}, and bp , bq are not ordered for any other ordering constraints.
Therefore, CD(m)  Re(p  q) as well as p  q is removed, which results a new plan
0
0
is valid according to Theorem 2.
= hS, B 0 , 0 i. We will show that bdp
bdp
Since bdp is valid, there is no active threat to any causal link in bdp according to
Theorem 2, but due to the deordering of p  q, the deleter q becomes a new threat only to
0
the causal link hr,m,pi in bdp
. However, q is contained by bq that does not contain hr,m,pi,
and does not delete m; therefore, according to Definition 6, hr,m,pi is protected from q, i.e.,
0
q becomes an inactive threat. As a result, bdp
satisfies the condition of Theorem 2 and
therefore remains valid.
Rule 4. Let bdp = hS, B, i be a valid block decomposed p.o. plan, si  sj a basic ordering
whose removal does not cause any threat protection ordering to be removed, and DP(m) 
Re(si  sj ). Let bj be a block, where sj  bj but si 
/ bj . Then DP(m) can be removed from
Re(si  sj ) if bj includes every step s0 such that PC(m)  Re(sj  s0 ).
Theorem 6. Deordering according to Rule 4 preserves plan validity.
385

fiSiddiqui & Haslum

Figure 10: Formation of blocks for removing the reason DP(m) behind the basic ordering
p  q.

Proof. Let bdp = hS, B, i be a valid block decomposed p.o. plan, and let p  q be a basic
ordering constraint (where p, q  S). Let bq be a block that includes all the steps  r and
s such that hq,m,ri, hq,m,si are the causal links in bdp (as shown in Figure 9 (ii)). Hence,
it meets the condition of Rule 4. Also, assume bp is a block that contains {p} and bp , bq
are not ordered for any other ordering constraints. As a result, DP(m)  Re(p  q) as well
0
0
= hS, B 0 , 0 i. We will show that bdp
as p  q is removed, which results a new plan bdp
satisfies the condition of Theorem 2 and therefore remains valid.
Since bdp is valid, there is no active threat to any causal link in bdp according to
Theorem 2, but due to the deordering of p  q, the deleter p becomes a new threat to the
0
. However, those causal links are contained by
only causal links hq,m,ri and hq,m,si in bdp
bq that does not contain p, and therefore, according to Definition 6, are protected from p,
0
i.e., p becomes an inactive threat. As a result, bdp
remains valid.

Even when, by applying the four rules above, we can find blocks bi and bj that remove
all reasons for an ordering si  sj , thus permitting the ordering to be removed, it is not
guaranteed that the two blocks bi and bj will be unordered. They may be ordered because
bi contains some step other than si that is ordered before some step in bj (whether sj or
another). Even if they are not, if there is a block b  B that contains bi (or bj but not both),
and b is still ordered before bj (resp. after bi ) due to some constraint in + other than
hsi , sj i, then blocks bi and bj will still be ordered, in the sense that bi will appear before bj
in any linearisation consistent with the block decomposition.
386

fiContinuing Plan Quality Optimisation

2.6 Block Deordering Algorithm
The previous subsection described four conditions (Rules 14) under which adding blocks
to a decomposition allows reasons for ordering constraints, and thus ultimately the ordering
constraints themselves, to be removed while preserving plan validity. Next, we describe the
algorithm that uses these rules to perform block deordering, i.e., to convert a sequential
plan seq into a block decomposed p.o. plan bdp .
The algorithm is divided into two phases. First, we apply a step-wise deordering procedure to convert seq into a p.o. plan pop = (S, 0 ). We have used Kambhampati and
Kedars (1994) algorithm for this, because it is simple and has been shown to produce very
good results (Muise et al., 2012), even though it has no optimality guarantee.
After the step-wise plan deordering, we extend ordering to blocks: two blocks are ordered
bi  bj if there exist steps si  bi and sj  bj such that si  sj and neither block is contained
in the other (i.e., bi 6 bj and bj 6 bi ). In this case, all steps in bi must precede all steps in bj
in any linearisation of the block decomposed plan. We also extend the reasons for ordering
(PC, CD and DP) to ordering constraints between blocks, with the set of propositions
produced, consumed and deleted by a block given by Definition 5. Recall that a responsible
step is a step in a block that causes it to produce, consume or delete a proposition. For
example, if b produces p, there must be a step s  b that produces p, such that no step in
the block not ordered before s deletes p; we say step s is responsible for b producing p.
The next phase is block deordering, which converts the p.o. plan pop = (S, ) into a
block decomposed p.o. plan bdp = (S, B, 0 ). This is done by a greedy procedure, which
examines each basic ordering constraint bi  bj in turn and attempts to create blocks that
are consistent with the decomposition built so far and that will allow this ordering to be
removed. The core of this algorithm is the Resolve procedure (Algorithm 1). It takes as
input two blocks, bi and bj , that are ordered (one or both blocks may consist of a single step),
and tries to break the ordering by extending them to larger blocks, b0i and b0j . The procedure
examines each reason for the ordering constraint and extends one of the blocks to remove
that reason, following the rules given in the previous subsection. After this, the sets of
propositions produced, consumed and deleted by the new blocks (b0i and b0j ) are recomputed
(following Definition 5) and any new reasons for the ordering constraint that have arisen
because of steps that have been included are added to Re(b0i  b0j ). This is repeated
until either no reason for the ordering remains, in which case the new blocks returned by
the procedure can safely be unordered, or some reason cannot be removed, in which case
deordering is not possible (signalled by returning null). The function Intermediate(bi , bj )
returns the set of steps ordered between bi and bj , i.e., {s | bi + s + bj }. Where Algorithm
1 refers to a nearest step s0 preceding or following another step s, it means a step with a
smallest number of basic ordering constraints between s0 and s.
If we applied the Resolve procedure to each basic ordering constraint we would obtain
a collection of blocks with which we can break some orderings. But this collection is not
necessarily a valid decomposition, since some of the blocks may have partial overlap. To
find a valid decomposition, we use a greedy procedure. We repeatedly examine each basic
ordering constraint bi  bj and call Resolve to find two extended blocks b0i  bi and b0j  bj
that allow the ordering to be removed. In each iteration, constraints are checked in order
from the beginning of the plan. A block, once added into bdp , will not be removed to
387

fiSiddiqui & Haslum

Algorithm 1 Resolve ordering constraints between a pair of blocks.
1: procedure Resolve(bi , bj )
2:
Initialise b0i = bi , b0j = bj .
3:
while Re(b0i  b0j ) 6=  do
4:
for each r  Re(b0i  b0j ) do
5:
if r = PC(p) then
// try Rule 1
6:
Find a responsible step s  b0i and a nearest s0 6 b0i that consumes
p such that s0 + s.
7:
if such s0 exists then
8:
Set b0i = b0i  {s0 }  Intermediate(s0 , b0i ).
9:
else return null
10:
else if r = DP(p) then
// try Rule 4
11:
Find a responsible step s  b0j and all s0 6 b0j such that
each hs, p, s0 i is a causal link.
12:
if such s0 exists then
13:
Set b0j = b0j  {s0 }  Intermediate(b0j , s0 ).
14:
else return null
15:
else if r = CD(p) then
// try Rule 3
16:
Find a responsible step s  b0j and a nearest s0 6 b0j that produces p,
such that s + s0 .
17:
if such s0 exists then
18:
Set b0j = b0j  {s0 }  Intermediate(b0j , s0 ).
19:
else
// try Rule 2
20:
Find a responsible step s  b0i and a nearest s0 6 b0i that produces
p, such that s0 + s.
21:
if such s0 exists then
22:
Set b0i = b0i  {s0 }  Intermediate(s0 , b0i ).
23:
else return null.
24:
Recompute Re(b0i  b0j ).
25:

return (b0i , b0j ).

accommodate another block that partially overlaps with the existing block throughout the
procedure, even if the later (rejected) block could produce more deordering than the one
created earlier. Since the choice of deordering to apply is greedy, the result is not guaranteed
to be optimal. If b0i or b0j cannot be added to the decomposition (because one or both of
them partially overlaps with an existing block), we consider all blocks ordered immediately
after bi , and check if all these orderings can be broken simultaneously, using the union of the
blocks returned by Resolve for each ordering constraint. (Symmetrically, we also check the
set of blocks immediately before bj , though this is only very rarely useful.) As an additional
388

fiContinuing Plan Quality Optimisation

heuristic, we discard the two blocks if there is a basic ordering constraint between a step
that is internal to one of the blocks (i.e., that has both preceding and following steps within
the block) and a step outside the block.
If the ordering can be removed, the inner loop exits and the ordering relation is updated
with any new constraints between b0i and blocks ordered after bj and between b0j and blocks
ordered before bi . This is done by checking for the three reasons (PC, CD and DP) based
on the sets of propositions produced, consumed and deleted by b0i and b0j . The inner loop
is then restarted, with ordering constraints that previously could not be broken checked
again. This is done because removing ordering constraints can make possible the resolution
of other constraints, since removal of orderings can change the set of steps intermediate
between two steps.
The main loop repeats until no further deordering consistent with the current decomposition is found. Each iteration runs in polynomial time, but we do not know of an upper
bound on the number of iterations. Note, however, that our procedure is anytime, in the
sense that if interrupted before running to completion, the result at the end of the last completed iteration is still a block deordering of the plan. In BDPO2, we use a time-limit of 5
minutes for the whole deordering procedure. However, for almost every problem considered
in our experiments (described in Section 3.1), block deordering finishes in a few seconds
(except for a few problems in the Visitall domain, for which it takes a couple of minutes).
In summary, deordering makes the structure of a plan explicit, showing us which parts
are necessarily sequential (because of dependency or interference) and which are independent and non-interfering. Block deordering improves on this by creating an on-the-fly
hierarchical decomposition of the plan, encapsulating some dependencies and interferences
within each block. Considering blocks, instead of primitive actions, as the units of partial
ordering thus enables deordering plans to a greater extent, including in cases where no deordering is possible using the standard, step-wise, partial order plan notion. The impact of
block decomposition on the anytime performance of our plan quality optimisation system
is discussed in Section 3.6.

3. System Overview
BDPO2 is a post-processing-based plan quality optimisation system. Starting with an initial
plan, it seeks to optimise parts of the plan, i.e. subplans, replacing them with lower-cost
subplans. We refer to the subplans that are candidates for replacement as windows. When
a better plan has been found and certain conditions are met, it starts over from the new
plan. This can be viewed as a local search, using the large neighborhood search (LNS)
strategy, in which the neighborhood of a plan is defined as the set of plans that can be
reached by replacing a window with a new subplan. The local search is plain hill-climbing:
each move is to a strictly better neighbouring plan. As in other LNS algorithms, searching
for a better plan in the neighbourhood is done by formulating local optimisation problems,
which are solved using bounded-cost subplanners.
Block deordering, described in the previous section, helps identify candidate windows
by providing a large set of possible plan linearisations; the block decomposition is also used
by some of our windowing strategies. Each window is a subsequence of some linearisation
of the block deordered input plan. However, we represent a window in a slightly different
389

fiSiddiqui & Haslum

way, by a partitioning of the blocks into the part to be replaced (w), and those ordered
before (p) and after (q) that part.
Definition 8. Let bdp = (S, B, ) be a block decomposed p.o. plan. A window in bdp
is a partitioning of B into sets p, w, q, such that bdp has a linearisation consistent with
{bp  bw  bq | bp  p, bw  w, bq  q}.
Each window defines a subproblem, which is the problem of finding a plan that can
fill the gap left by removing the steps in w from a linearisation of bdp consistent with the
window. This problem is formally defined as follows.
Definition 9. Let bdp = (S, B, ) be a block decomposed p.o. plan for planning problem
, hp, w, qi a window in bdp , and s1 , . . . , s|p| , s|p|+1 , . . . , s|p|+|w| , s|p|+|w|+1 , . . . , sn a linearisation of bdp consistent with that window. The subproblem corresponding to hp, w, qi,
sub , has the same atoms and actions as . The initial state of sub , Isub , is the result of
progressing the initial state of  through s1 , . . . , s|p| (i.e., applying s1 , . . . , s|p| in I), and the
goal of sub , Gsub , is the result of regressing the goal of  through sn , . . . , s|p|+|w|+1 .
Theorem 7. Let bdp = (S, B, ) be a block decomposed p.o. plan for planning problem , hp, w, qi a window in bdp , sub the subproblem corresponding to the window,
and s1 , . . . , s|p| , s|p|+1 , . . . , s|p|+|w| , s|p|+|w|+1 , . . . , sn the linearisation that sub is constructed
0
0
0 = s0 , . . . , s0 be a plan for 
from. Let w
sub . Then s1 , . . . , s|p| , s1 , . . . , sk , s|p|+|w|+1 , . . . , sn
1
k
is a valid sequential plan for .
Proof. The proof is straightforward. The subsequence s1 , . . . , s|p| is applicable in the initial
state of , I, and, by construction of sub , results in the initial state of sub , Isub . Hence
s1 , . . . , s|p| , s01 , . . . , s0k is applicable in I, and, again by construction of sub , results in a state
sG that satisfies the goal of sub , Gsub . Since Gsub is the result of regressing the goal of ,
G, through s|p|+|w|+1 , . . . , sn in reverse, it follows that this subsequence is applicable in sG ,
and applying it results in a state satisfying G. (For the relevant properties of regression,
see, for example, Ghallab, Nau, & Traverso, 2004, Section 2.2.2.)
The subproblem corresponding to a window hp, w, qi always has a solution, in the form
of a linearisation of the steps in w. To improve plan quality, however, the replacement
subplan must have a cost that is strictly lower than the cost of w, C(w). This amounts to
solving bounded-cost subproblems. The subplanners we have used for this in BDPO2 are
described in Section 3.3. We return to the question of when and how multiple windows
within the same plan can be simultaneously replaced in Section 3.5.
Algorithm 2 describes how BDPO2 performs one step of the local search, by exploring
the neighbourhood of the current plan. The first step is to block deorder the current plan
(line 3). Next, optimisation using a bounded-cost subplanner is tried systematically on
candidate windows (lines 419), until a restart condition is met (line 18), until no more
local improvements are possible, or until a time limit is reached. A point of difference
with other LNS algorithms is that we have used delayed restart, meaning that exploration
of the neighbourhood can continue after a better plan has been found. This helps avoid
local minima, by driving exploration to different parts of the current plan. The restart
conditions, and the impact they have on the local search, are described in Section 3.4.
390

fiContinuing Plan Quality Optimisation

Algorithm 2 The neighbourhood exploration procedure in BPDO2.
1: procedure BDPO2(in , tlimit , banditPolicy, rankPolicy, optSubprob)
2:
Initialize: telapsed = 0, last = in , trialLimit[1...n] = 1, windowDB = 
3:
bdp = BlockDeorder(in )
4:
while telapse < tlimit and last is not locally optimal do
5:
if more windows needed then
6:
ExtractMoreWindows(bdp , windowDB, optSubprob)
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

p = SelectPlanner(banditPolicy)
w = SelectWindow(p, rankPolicy, trialLimit, windowDB)
if w = null and no more windows to extract then trialLimit[p] += 1
if w = null then continue
wnew , searchResult = OptimiseWindow(p, w)
UpdateWindowDB(p, w, wnew , optSubprob, searchResult, windowDB)
if C(wnew ) < C(w) then
new = Merge(bdp , windowDB)
if C(new ) < C(last ) then last = new
UpdateBanditPolicy(p, w, wnew , searchResult, banditPolicy)
UpdateRankPolicy(p, searchResult, rankPolicy)
if C(last ) < C(in ) and restart condition is true then
return BDPO2 (last , tlimt  telapsed , banditPolicy, rankPolicy, optSubprob)
return last

A key design goal of the procedure is to avoid unproductive time, meaning spending
too much time in one step or trying to optimise one window while other options that could
lead to an improvement are left waiting. Therefore, all steps are done incrementally, with
a time limit on any step that could take an unbounded time.
A database (windowDB) stores each unique window extracted from the block deordered
plan, and records its status (how many times optimisation of this window has been tried
with each subplanner and the result), and structural summary information about the window. The window database is populated incrementally (lines 56), by applying different
windowing strategies with a limit on the time spent and the number of windows added.
The limits we have used are 120 seconds and 20 windows, respectively. This balances time
between window extraction and optimisation, to prevent the procedure spending unproductive time. The windowing strategies are described in Section 4. We also compute a lower
bound on the cost of any replacement plan for the window, using the admissible LM-Cut
heuristic (Helmert & Domshlak, 2009). A window is proven optimal if the current subplan cost equals this bound, or a previous attempt to optimise the window exhausted the
bounded-cost search space. Already optimal windows are, of course, excluded from further
optimisation. More windows are added to the database when the number of windows eligible to be selected for optimisation by any one subplanner (defined in the next paragraph)
drops below a threshold. We have used 75% of the current window database size as the
threshold.
391

fiSiddiqui & Haslum

The subplanner to use is selected using the UCB1 multi-armed bandit policy (Auer et al.,
2002), which learns over repeated trials to select more often the subplanner that succeeds
more often in finding improvements. The next window to try is chosen, among the eligible
ones in the database, according to a ranking policy. Windows eligible for optimisation by
the chosen subplanner are those that (1) are not already proven optimal; (2) have not been
tried with the chosen subplanner up to its current trial limit; and (3) do not overlap with
any improved window already found. The ranking policy is a heuristic aimed at selecting
windows more likely to be improved by the chosen subplanner. We use several ranking
policies and switch from one to the next when the subplanner fails to find an improvement
for a number of consecutive tries, since this indicates the current ranking policy may not be
recommending the right windows for the current problem. The threshold we have used for
switching the ranking policy is 13. (This is 2/3 of the maximum number of windows added to
the window database in each call to ExtractMoreWindows.) The ranking policies are
described in Section 4.6. The subplanner is given a time limit, which is increased each time
it is retried on the same window. We have used a limit of 15 seconds, increasing by another
15 seconds for each retry. A limit on the number of times it can be retried on the same
window is kept for each subplanner. Initially set to 1, the limit is increased only when the
subplanner has been tried on every window in the database (excluding windows that have
already been proven optimal or that overlap with windows for which a better replacement
has been found) and no strategy can generate more new windows (line 9). If a lower-cost
replacement subplan for the window is found, this together with all improvements already
found in the current neighbourhood are fed into the Merge procedure, which tries to
combine several replacements to achieve a greater overall plan cost reduction. The Merge
procedure is described in Section 3.5.
When the procedure restarts with a new best plan, the learned bandit policy for subplanner selection and the current ranking policy (for each subplanner) are carried over to
the next iteration. We also keep a database of the subproblems (defined by their initial
state and goal) whose plan cost has been proven optimal, to avoid trying fruitlessly to optimise them further. The window database, which contains only information specific to the
current input plan, is reset.
The remainder of this section is organised as follows: The next two sections describe the
settings that we have used for our experiments and an overview of main results, respectively.
We then describe the subplanners used in BDPO2 (Section 3.3), the restart conditions
(Section 3.4) and the Merge procedure (Section 3.5). Section 3.6 discusses the impact of
block deordering on the performance of the system. The windowing strategies and ranking
policies are described in Section 4, while more details of the on-line adaptation methods
used are presented in Section 5.
3.1 Experiment Setup
Before presenting the overview of results, we outline below the three different experimental
setups that we have used. For experiment setup 2 and 3 we used 182 large-scale instances
from 21 IPC domains. The selection of domains and instances is described below. For
experiment 1, we included additional medium-sized instances for a total of 219 instances
from the same 21 domains. We used all domains from the sequential satisficing track of
392

fiContinuing Plan Quality Optimisation

the 2008, 2011, and 2014 IPC, except for the CyberSec, CaveDiving and CityCar domains.
(The CyberSec domain is too slow for our system to parse. The other two have conditional
effects, which our implementation does not handle.) We also used the Alarm Processing for
Power Networks (APPN) domain (Haslum & Grastien, 2011). The plans used as input to
BDPO2 are the plan produced by IBaCoP2 (Cenamor, de la Rosa, & Fernandez, 2014) in
the 2014 IPC for the problems from that competition, and the best plan found by LAMA
(Richter & Westphal, 2010, IPC 2011 version) in 1 hour CPU time for all other problems.
We refer to these as the base plans. For experiments 2 and 3, we selected from each domain
the 10 last instances for which a base plan exists. (In some domains less than 10 instances
are solved by LAMA/IBaCoP2, which is why the total is 182 rather than 210.) For domains
that appeared in more than one competition, we used instances only from the IPC 2011 set.
All experiments were run on 6-Core, 3.1GHz AMD CPUs with 6M L2 cache, with an 8
GB memory limit for every system. When comparing the anytime performance of BDPO2
and other systems that require an input plan, we count the time to generate each base plan
as 1 hour CPU time. This is the maximum time allocated to generating each base plan;
most of them were found much more quickly.
In our first experiment, we did not use the BDPO2 system. Instead, we ran each of two
subplanners, PNGS and IBCS, for up to 30 seconds on every subproblem corresponding to
a window extracted (by our six windowing strategies) from all base plans, excluding only
subproblems for which the window was proven optimal by the lower bound obtained from
the admissible LM-Cut heuristic (Helmert & Domshlak, 2009). This experiment provided
information to inform the design of the combined window extraction procedure, the window
ranking policies, and other aspects of the system. We do not present its results here, but
will refer to it later when we discuss these system components in detail.
In experiment 2, we compare BDPO2 and eight other anytime planners and plan optimisation systems: LAMA (Richter & Westphal, 2010, IPC 2011 version); AEES (implemented
in the Fast Downward code base; cf. Thayer et al., 2012b); IBCS (as described in Section
3.3); Beam-Stack Search (BSS) (Zhou & Hansen, 2005); PNGS, including Action Elimination (Nakhost & Muller, 2010); IBaCoP2 (Cenamor et al., 2014); LPG (Gerevini &
Serina, 2002); and Arvand (Nakhost & Muller, 2009). BDPO2 uses PNGS and IBCS as
subplanners, and is configured as described above. AEES uses LM-Cut (Helmert & Domshlak, 2009) as its admissible heuristic, and the FF heuristic, with and without action costs
for its inadmissible estimates. BSS uses the LM-Cut heuristic. Our implementation of BSS
does not use divide-and-conquer solution reconstruction, and was run with a beam width
of 500. The other systems are described further in Section 6.
Each system was run for up to 7 hours CPU time per problem. BDPO2 and PNGS
both use the base plans as input, and IBCS and Beam-Stack Search both use the base
plan cost as the initial cost bound. As mentioned above, we allocated 1 hour CPU time
for generating each base plan. Therefore, when comparing these systems with planners
starting from scratch (LAMA, AEES, IBaCoP2, LPG and Arvand), we add a 1 hour start
up delay to their runtime. Beam-Stack Search is much slower than the other planners used
in the experiment. Therefore, we ran it for up to 24 hours CPU time, and in reporting its
results we divide its runtime by 4. In other words, the results shown are for a hypothetical
implementation of Beam-Stack Search that does the same amount of search, but faster by
a constant factor of 4.
393

fiSiddiqui & Haslum

Experiment 3 uses the same setup as experiment 2, except that the input to BDPO2 is
the best plan found by running PNGS for up to 1 hour CPU time, with an 8 GB memory
limit, on the base plans. (As mentioned previously, in the vast majority of cases PNGS runs
out of memory in much less time than that, but in a few cases it does run up to the 1 hour
limit.) We use this setup primarily to run different configurations of BDPO2 to analyse
the impact of different designs (e.g., the planner selection and window ranking policies,
immediate vs. delayed restart, and so on) in a setting where input plans are already of good
quality. When comparing the anytime result of BDPO2 in this experiment to the other
systems, we add 2 hours to its runtime.
3.2 Overview of Results
Figure 11 shows a headline result, in the form of the average plan quality achieved by
BDPO2 and other systems as time-per-problem increases. The IPC quality score of a plan
is calculated as cref /c, where c is the cost of the plan and cref is the cost of the best plan
for the problem instance found over all runs of all systems used in our experiments. Thus,
a higher score reflects a lower-cost plan. The results in Figure 11 are from experiment 2
and 3, described in the previous section. It is the same as shown in Figure 2 (on page 371),
but including results for all the compared anytime planning systems. None of the planners
starting from scratch find a solution to all 182 problems: LAMA solves 155 problems,
IBaCoP2 144, Arvand 134, AEES 87 and LPG 49. For these planners, the average quality
score shown in Figure 11 is the average over only those problems for which they have, at
that time, found at least one plan. (As previously mentioned, this is also the reason why the
average quality sometimes falls: when a first plan, of low quality, for a previously unsolved
problem is found, the average can decrease.) In other words, this metric is unaffected by
the differences in coverage. Likewise, none of the post-processing or bounded cost search
methods improve on all base plans: BDPO2 finds a plan of lower cost than the base plan
for 147 problems, PNGS for 133, IBCS for 66 and Beam-Stack Search for 14. For these
systems, the average quality shown in Figure 11 is taken over all 182 problems, using the
base plan quality score for those problems that a system has not improved on.
The majority of the compared systems show a trend similar to that of LAMA, i.e.,
improving quickly early on but then flattening out and ultimately stagnating. The reasons
vary: Memory is a limiting factor for some algorithms, notably PNGS, which exhausts the
8 GB available memory before reaching the 7 hour CPU time limit for 93.7% of problems,
and LAMA, which does the same for 67% of problems. AEES runs out of memory on just
over 50% of problems. On the other hand, planners that use limited-memory algorithms,
such as Beam-Stack Search, LPG and Arvand (both of which use local search), never run
out of memory and thus could conceivably run indefinitely. However, the rate at which they
find plan quality improvements is small: From 4 to 7 hours, the average quality produced by
LPG and Arvand increases by 0.0049 and 0.0094, respectively. (The latter excludes three
problems that were solved by Arvand for the first time between 4 and 7 hours; including
those brings the average down, making the increase less than 0.002.) The increase in average
quality achieved by BDPO2, starting from the high-quality plans generated by PNGS from
the base plans, over the same time interval is 0.0115.
394

fiContinuing Plan Quality Optimisation

0.96
0.94
0.92


Average Quality Score (Relative IPC Quality Score / Coverage)

0.9













































































































































0.88
0.86



0.84





0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
0.64



0.62



BDPO2 on PNGS on base plans
BDPO2 on base plans
PNGS on base plans
IBCS on base plans
BSS on base plans
LAMA from scratch
AEES from scratch
IBaCoP2 from scratch
Arvand from scratch
LPG from scratch

0.6
0.58



0.56
0.54










































































7

6.5

6

5.5

5

4.5

4

3.5

3

2.5

2

1.5



0.5



0

0.5



1

0.52





Time (hours)

Figure 11: Average IPC quality score as a function of time per problem, on a set of 182
large-scale planning problems. The quality score of a plan is cref/c, where c is the cost of the
plan and cref is the least cost of all plans for the problem; hence a higher score represents
better plan quality. The LAMA, AEES, LPG, Arvand and IBaCoP2 planners start from
the scratch, whereas the post-processing (PNGS, BDPO2) and bounded cost search (IBCS,
Beam-Stack Search) methods start from a set of base plans; their curves are delayed by 1
hour, which is the maximum time allocated to generating each base plan. The experimental
setup is described in detail in Section 3.1.

395

fiSiddiqui & Haslum

BDPO2
BDPO2
on PNGS
= < ? = < ?
Appn
50
20 40
10
Barman
100 90
10
Childsnack
100 30
70
Elevators
60 60
10 10
Floortile
67
78 22
GED
30
20
Hiking
50
70 20
Maintenance 100
100
Nomystery
100
100 100
100
Openstacks
Parcprinter
100
22 100
22
43
43 14
Parking
Scanalyzer
75 12
38 12
Sokoban
100
100
Tetris
80 40
60 20
Thoughtful
80 30
50 20
Tidybot
43
29
Transport
60 60
40 40
Visitall
60 60
30 30
Woodworking 70 30
50 20
Overall
66 24 4 47 12 3
Domains

LAMA

AEES

Arvand

LPG

IBCS

BSS

PNGS

IBaCoP2

= < ? = < ? = < ? = < ? = < ? = < ? = < ? = <
40
10
40
10 70 20 20

?

10
10

10
11

80 70
40 30

20 20
11 11

10

33

10
50
29

50

50 50

33

11

50

50

50 50

50

22 33

67

22

88 88
11 33

43 43

20
71 29

33

33

43

43

12 12
67
11
29
75 12
67

29 14

10

12

14

10 10
18 14

8

1

1

1

2

1

8

1 12 2 3 13 1

20 10
8 2

1

Table 1: For each plan improvement method, the percentage of instances where it found a
plan of cost matching the best plan (=); found a plan strictly better than any other method
(<); and found a plan that is known to be optimal, i.e., matched by the highest lower bound
(?). The percentage is of the same instances in each domain shown in Figure 12. (Zeros are
omitted to improve readability.) BDPO2 on PNGS is the result of BDPO2 in experiment
3; the other results are from experiment 2 (see Section 3.1).

We draw two main conclusions: First, BDPO2 achieves the aim of continuing quality
improvement even as the time limit grows. In fact, it continues to find better plans, though
at a decreasing rate, even beyond the 7 hour time limit used in this experiment. Second, the
combination of PNGS and BDPO2 achieves a better result than either does alone. Partly
this is because they work well on different sets of problems and the figure is showing an
average, but BDPO2 sometimes produces a better result when started with the best plan
found by PNGS also in domains where BDPO2 already outperforms PNGS when both start
from the same base plans (e.g., Elevators and Transport). However, we have also seen the
opposite in some domains (e.g., Floortile and Hiking), where starting BDPO2 with a worse
input plan often yields a better final plan. This can be seen in Figure 12, which provides
a more detailed view. It shows for each problem the cost of the best plan found by each
system at the 7 hour total time limit, scaled to the interval between the base plan cost
and the highest known lower bound (HLB) on any plan for the problem. (Lower bounds
were obtained by a variety of methods, including several optimal planners; cf. Haslum,
2012.) 18 of the 182 problems are excluded from Figure 12: in 3 cases, the base plan cost
already matches the lower bound, so no improvement is possible; for another 15 problems,
no method improves on the base plans within the stipulated time. (The Pegsol domain
does not appear in the graph, because all base plans but one are optimal, and no method
improves on the cost of the last one.)
396

fiContinuing Plan Quality Optimisation

Base Plans















 

























Best cost achieved (normalised)



































Nomystery

Maintenance

Ged

Floortile

Barman

Hiking



Elevators



Childsnack



Appn

HLB







LAMA from scratch
AEES from scratch
Arvand from scratch
LPG from scratch
IBaCoP2 from scratch
PNGS on base plans
IBCS on base plans
BSS on base plans
BDPO2 on base plans
BDPO2 on PNGS on base plans

Base Plans









 


























Best cost achieved (normalised)





 
 


























HLB



LAMA from scratch
AEES from scratch
Arvand from scratch
LPG from scratch
IBaCoP2 from scratch
PNGS on base plans
IBCS on base plans
BSS on base plans
BDPO2 on base plans
BDPO2 on PNGS on base plans
Woodworking

Visitall

Transport

Tidybot

Thoughtful

Tetris

Sokoban

Scanalyzer

Parking

Parcprinter

Openstacks

 



Figure 12: Best plan cost, normalised to the interval between the cost of the base plan
and the corresponding highest known lower bound, achieved by different anytime plan
optimisation methods in experiment 2, and by BDPO2 in experiments 2 & 3 (see Section
3.1).

397

fiSiddiqui & Haslum

Table 1 provides a different summary of the information in Figure 12, showing for each
domain and system the percentage of instances for which it found a plan with a cost (1)
matching the best plan for that instance; (2) strictly better than any other method; and
(3) matching the lower bound, i.e., known to be optimal. In aggregate, the combination
of BDPO2 after PNGS over the base plans achieves the best result on all three measures.
However, in 5 domains (GED, Hiking, Openstacks, Parking, and Tidybot), LAMA finds
more plans that are strictly better than any other method. We tried using LAMA as
one of the subplanners in BDPO2, but this does not lead to better results overall. In some
domains, such as OpenStacks and GED, the smallest improvable subplan is often the whole,
or almost the whole, plan, and LAMA finds an improvement of the plan only after searching
for a longer time. Although BDPO2 increases the time limit given to subplanners with each
retry, the average time limit, across all local optimisation attempts in this experiment, is
only 18.48 seconds. Thus, our strategy of searching for quick improvements of plan parts
does not work well in these domains.
3.3 Subplanners Used for Window Optimisation
The subplanners used by BDPO2 are used to find a plan for the window subproblem, as
stated in Definition 9, with a cost less than the cost of the current window, C(w). We have
considered three subplanners:
(1) Iterated bounded-cost search (IBCS), using a greedy search with an admissible heuristic for pruning.
(2) Plan neighbourhood graph search (PNGS), including the action elimination technique (Nakhost & Muller, 2010).
(3) Restarting weighted A? (Richter et al., 2010), as implemented in the LAMA planner.
However, in the experimental setups described in the previous section, BDPO2 uses only
two subplanners, IBCS and PNGS. There are two reasons for choosing these two: First,
they show good complementarity across domains. For example, IBCS is significantly better than PNGS in the APPN, Barman, Floortile, Hiking, Maintenance, Parking, Sokoban,
Thoughtful and Woodworking domains, while PNGS is better in the Elevators, Scanalyzer,
Tetris, Transport and Visitall domains. Second, the learning policy that we use for subplanner selection learns faster with a smaller number of options. Therefore, adding a third
subplanner will only improve the overall performance of BPDO2, given a limited time per
problem, if that subplanner complements the other two well, i.e., it performs well on a
significant fraction of instances where both the other two do not. On the set of benchmark
problems used in our experiment, this was not the case. (A different set of benchmarks
could of course yield a different outcome.) An experiment comparing the effectiveness of all
three subplanners, individually as well as the combination of IBCS and PNGS under the
learning policy, in BDPO2 is presented in Section 5.2 on page 420.
To solve the bounded-cost problem, IBCS uses a greedy best-first search guided by the
unit-cost FF heuristic, pruning states that cannot lead to a plan within the cost bound
using the f-value based on the admissible LM-Cut heuristic (Helmert & Domshlak, 2009).
It is implemented in the Fast Downward planner. The search is complete: if there is no plan
398

fiContinuing Plan Quality Optimisation

within the cost bound, it will prove this by exhausting the search space, given sufficient
time and memory. The bounded-cost search can return any plan that is within the cost
bound. To get the best subplan possible within the given time limit, we iterate it: whenever
a plan is found, as long as time remains, the search is restarted with the bound set to be
strictly less than the cost of the new plan.
PNGS (Nakhost & Muller, 2010) is a plan improvement technique. It searches a subgraph of the state space around the input plan, limited by a bound on the number of states,
for a lower cost plan. If no better plan is found the exploration limit is increased (usually
doubled); this continues until the time or memory limit is reached. Like with IBCS, we
iterate PNGS to get the best subplan possible within the given time limit. If it improves
the current subplan, the process is repeated around the new best plan.
LAMA (Richter & Westphal, 2010) finds a first solution using greedy best-first search.
It then switches to RWA? (Richter et al., 2010) to search for better quality solutions.
3.4 Restart
The restart condition determines a trade-off between exploring the neighbourhood of the
current solution and continuing the local search into different parts of the solution space.
The most obvious choice, and the one used in other LNS algorithms, is to restart with the
new best solution as soon as one is found. We call this immediate restart. However, we have
found that continuing to explore the neighbourhood of the current plan even after a better
plan has been found, and merging together several subplan improvements, as described in
Section 3.5 below, often produces better results. We call this delayed restart.
Setting the right conditions for when to make a delayed restart is critical to the success
of this approach. We have used a disjunction of two conditions: First, if the union of
improved windows found in the neighbourhood covers 50% of the steps in the input plan.
Recall that when we continue the exploration loop (Algorithm 2) after an improvement has
been found, windows that overlap with any already improved window are excluded from
further optimisation. This drives the procedure to search for improvements to different
parts of the current plan, and helps avoid a certain myopic behaviour that can occur
with immediate restarts: When restarting with the new best plan, we get a new block
decomposition and a new set of windows; this can lead to attempting to re-optimise the
same part of the plan that was just improved, even over several restarts, which may lead to a
local optimum that is time-consuming to escape. The second condition is that 39 consecutive
subplanner calls have failed to find any further improvement. The threshold of 39 is three
times the threshold for switching the ranking policy (cf. description of Algorithm 2 at the
beginning of this section). This means that after 39 attempts we have tried to optimise
the 13 most promising windows, among the remaining eligible ones, recommended by all
ranking policies, without success. This suggests there are no more improvable windows
to be found, or that none of our ranking policies are good in the current neighbourhood.
Making a restart at this point allows the exploration to return to parts of the plan that
intersect already improved windows, thus increasing the set of eligible windows.
The average plan quality, as a function of time-per-problem, achieved by BDPO2 using
immediate restart and delayed restart based on the conditions above is shown by the top
two lines in Figure 13 (page 403). In this experiment, both configurations were run using
399

fiSiddiqui & Haslum

Algorithm 3 Merge Improved Windows
1: procedure Merge(bdp , windowDB)
2:
Initialise bdp = bdp
3:
W = improved windows from windowDB sorted by cost reduction (C(w)  C(wnew ))
4:
while W 6=  do
5:
(hp, w, qi, wnew ) = pop window with highest C(w)  C(wnew ) from W
6:
bdp = ReplaceIfPossible(bdp , hp, w, qi, wnew )
7:
W = RemoveConflictingWindows(W, bdp )
8:

return bdp

the same setup as experiment 3, described in Section 3.1 on page 392. As can be seen,
delayed restart yields better results overall. Compared to BDPO2 with immediate restart,
it achieves a total improvement that is 12% higher. However, we found immediate restart to
work better for a few instances, especially in the Visitall and Woodworking domains, where
BDPO2 with immediate restart found a better final plan for nearly 20% of the instances.
The average number of iterations (i.e., steps in the LNS) done by BDPO2 using the
delayed restart condition is 3.48 per problems across all the domains considered in the
experiment; the highest average in a single domain is 8.7, in Thoughtful solitaire. With
immediate restart the average over all domains increases to 4.87. In other words, both
configurations of BDPO2 spend significant time exploring the neighbourhood of each plan.
The anytime performance curve in Figure 13 shows that the additional time spent in each
neighbourhood when using delayed restarts pays off.
3.5 Merging Improved Windows
Delayed restarting would not have any benefit without the ability to simultaneously replace
several improved windows in the current plan. The improved windows are always nonoverlapping (because once a better subplan for a window is found, windows that overlap
with it are no longer considered for optimisation) but their corresponding subproblems may
have been generated from different linearisations of the block deordered plan. Because of
this, the replacement subplans may have additional preconditions or delete effects that the
replaced windows did not, or lack some of their add effects. Thus, there may not be a
linearisation that permits two or more windows to be simultaneously replaced.
The Merge procedure shown in Algorithm 3 is a greedy procedure. It maintains at all
times a valid block deordered plan (bdp ), meaning that each precondition of each block
is supported by a causal link with no active threat. (Recall that a block in this context
can be block that consists of a single step.) Initially, this is the input plan (bdp ), for
which causal links, and other ordering constraints, are computed by block deordering. The
procedure gets the improved windows (W ) from the window database, and tries to replace
them in the current plan bdp in order of their contribution to decreasing plan cost, i.e.,
the cost of the replaced window (C(w)) minus the cost of the new subplan (C(wnew )).
The first replacement always succeeds, since, by construction of the subproblem, there
is a linearisation of the input plan in which wnew is valid (cf. Theorem 7). Subsequent
replacements may fail, in which case Merge proceeds to the next improved window in W .
400

fiContinuing Plan Quality Optimisation

Since replacing a window with a different subplan may impose new ordering constraints,
any remaining improved windows that conflict with partial order of the current plan are
removed from W .
The ReplaceIfPossible function takes the current plan (bdp ), and returns an updated plan (which becomes the current plan), or the same plan if the replacement is not
possible. The replacement subplan (wnew ) is made into a single block whose steps are totally ordered. The preconditions and effects of this block, and those of the replaced window
(w), are computed according to Definition 5 (page 378). For any atom in pre(wnew ) that
is also in w, the existing causal link is kept; likewise, causal links from an effect in add(w)
that are are also in add(wnew ) are kept. These links are unthreatened and consistent with
the order, since the plan is valid before the replacement. For each additional precondition
of the new subplan, m  pre(wnew ) \ pre(wi ), and for each causal link hbp , m, bc i in bdp
where the producer is in the replaced window (bp  w), the consumer is not (bc 6 w), and
the atom of the link is not produced by the replacement subplan (m 6 add(wnew )), a new
causal link must be found. Given a consumer (bc ) and an atom it requires (m  pre(bc )),
the procedure tries the following two ways of creating an unthreatened causal link:
(C1) If there is a block b0 + bc with m  add(b0 ), and for every threatening block (i.e.,
b00 with m  del(b00 )), either b00  b0 or bc  b00 can be added to the existing plan ordering
without contradiction, then b0 is chosen, and the ordering constraints necessary to resolve
the threats (if any) are added.
(C2) Otherwise, if there is a block b0 with m  add(b0 ) that is unordered w.r.t. bc , and for
every threatening block either b00  b0 or bc  b00 can be enforced, then b0 is chosen, and the
causal link (implying the new ordering b0  bc ) and threat resolution ordering constraints
(if any) are added to the plan.
The two are tried in order, C1 first and C2 only if C1 fails. If neither rule can find the
required causal link, the replacement fails. wnew may also threaten some existing causal
links in bdp that w did not. For each threatened link, hbp , m, bc i, the procedure tries to
resolve the threat in three ways:
(T1) If the consumer bc was ordered before w in the linearisation of the corresponding
subproblem (bc  p), and bc  wnew is consistent, the threat is removed by adding this
ordering.
(T2) If the producer bp was ordered after w in the linearisation of the corresponding subproblem (bp  q), and wnew  bp is consistent, the threat is removed by adding this ordering.
(T3) If a new, unthreatened causal link supplying m to bc can be found by one of the two
rules C1 or C2 above, the threatened link is replaced with the new causal link.
The rules are tried in order, and if none of them can resolve the threat, the replacement
fails.
Some non-basic ordering constraints between blocks not in w may disappear when w
is replaced with wnew ; likewise, some ordering constraints between w and the rest of the
plan may become unnecessary, because wnew may not delete every atom that w deletes and
may not have all preconditions of w, and thus can be removed. This may make pairs of
blocks b, b0 in the plan that were ordered before the replacement unordered, and thus create
new threats. All such new threats are checked by ReplaceIfPossible, and if found are
resolved by restoring the ordering constraint that was lost.
401

fiSiddiqui & Haslum

Lemma 8. If the current plan bdp is valid, and wnew solves the subproblem corresponding
to window hp, w, qi, the plan returned by ReplaceIfPossible is valid.
Proof. The procedure ensures that every precondition of every step is supported by a causal
link with no active threat: such a link either existed in the plan before replacement (and
any new threats to it created by the replacement are resolved by ordering constraints), or
was added by the procedure. Thus, if the replacement succeeds, the resulting plan is valid
according to Theorem 2. If the replacement fails, the plan returned is the current plan,
bdp , unchanged, which is valid by assumption.
Theorem 9. If the input plan, bdp is valid, then so is the plan returned by Merge.
Proof. Immediate from Lemma 8 by induction on the sequence of accepted replacements.

3.6 The Impact of Plan Decomposition
The neighbourhood explored in each step of the LNS in BDPO2 is defined by substituting
improved subplans into the current plan. Each subplan considered for local optimisation is
a subsequence of some linearisation of the block deordering of the current plan. Obviously,
we can also restrict windows to be consecutive subsequences of the totally ordered input
plan; in fact, similar approaches to plan optimisation have adopted this restriction (Ratner
& Pohl, 1986; Estrem & Krebsbach, 2012; Balyo, Bartak, & Surynek, 2012). In this section,
we address the question of how much the block deordering contributes to the performance
of BDPO2.
In the preliminary experiment (setup 1, as described in Section 3.1 on page 392) we
observed that more than 75% of the subproblems for which an improved subplan was found
correspond to a non-consecutive part of the sequential input plan. However, this in itself does not prove that optimising only the 25% of subplans that can be found without
deordering would not lead to an equally good end result.
Therefore, we conducted another experiment, using the same setup as experiment 3 (described in Section 3.1). In this experiment, we ran BDPO2 separately with different degrees
of plan decomposition: (1) With block deordering (as in the default BDPO2 configuration,
the one used in experiments 2 and 3 presented in Section 3.2 on page 394). (2) With standard, i.e., step-wise, plan deordering only. In this configuration, we used Kambhampati
and Kedars (1994) algorithm (described in Section 2.3) for plan deordering. (3) Without
any deordering, i.e., passing the totally ordered input plan directly to the LNS process.
In addition, each of these configurations was run once with immediate restarting and once
with delayed restarting, as described in Section 3.4.
Figure 13 shows the average IPC plan quality score as a function of time-per-problem
achieved by each of these configurations of BDPO2. It shows a simple and clear picture:
With immediate restart, LNS applied to block deordered plans outperforms LNS applied
to step-wise deordered plans, which in turn outperforms its use on totally ordered plans.
The total improvement, as measured by the increase in the average IPC plan quality score,
achieved by BDPO2 without deordering is 28.7% less than what is achieved by the best
configuration. We can also see that deordering is an enabler for delayed restarting: With
block and step-wise deordering, delayed restarting further boosts the performance of LNS
402

fiContinuing Plan Quality Optimisation

0.962












0.958








0.954







0.95



0.946










0.942






















0.938









0.934



0.93

6

5.5

5

4.5

1

0.5

0

0.922

4



3.5



3



0.926

BDPO2 with delayed restart over block deordered plans
BDPO2 with immediate restart over block deordered plans
BDPO2 with delayed restart over standard partially ordered plans
BDPO2 with immediate restart over standard partially ordered plans
BDPO2 with delayed restart over totally ordered plans
BDPO2 with immediate restart over totally ordered plans
2.5



2




1.5

Average Quality Score (Relative IPC Quality Score / Coverage)



Time (hours)

Figure 13: Average IPC quality score as a function of time per problem for BDPO2 applied
to the totally ordered input plan; the standard (step-wise) deordering of the plan; and the
block deordering of the plan. For each plan type, the system was run in two configurations: once with delayed restarting and once with immediate restarting (cf. Section 3.4 on
page 399). This experiment was run with setup 3, as described in Section 3.1 on page 392.
The time shown here is only the runtime of BDPO2 (i.e., without the 2 hour delay for
generating the input plans, as shown in Figure 11). Note also that the y-axis is truncated:
All curves start from the average quality of the input plans, which is 0.907.

403

fiSiddiqui & Haslum

plan optimisation by 12% and 14.7%, respectively, while on totally ordered plans it has no
significant effect.
Deordering increases the number of linearisations and therefore enables many more
distinct candidate windows to be created. However, recall that BDPO2s neighbourhood
exploration procedure (Algorithm 2) interleaves incremental window generation with optimisation attempts; many of the windows that could be generated from the current plan may
never be generated before a restart occurs. Thus, the average number of windows generated
in each iteration does not reflect the difference in performance. (With block deordering, the
average number of windows generated is 277.23, of which 183.19 remain after filtering, while
on the totally ordered plans it is 376.8, and 149.94 after filtering; both are using immediate
restart.) But deordering helps the windowing strategies generate windows that are more
easily optimised. Recall that neighbourhood exploration will retry the same subplanner on
the same window (with a higher time limit) only after all windows have been tried by that
subplanner. The average number of optimisation attempts, using either subplanner, on each
window selected for optimisation at least once, is around 1.7 when either block deordering
or standard deordering is used on the input plan. Without any deordering, however, the
average number of attempts is higher, and very high in a few domains: leaving out the
highest 5% of neighbourhoods encountered, the average is slightly more than 2; in more
than 10% of the plan neighbourhoods the average number of attempts is over 5, and in a
few cases more than 10. In other words, generating windows from a totally ordered plan
causes the procedure to spend, on average, more time on each window before an improving
plan is found.
On the other hand, as noted in Section 3.2, in some domains subplanners need more
runtime to find better plans for improvable windows, and the BDPO2 configuration without
deordering does find a better plan than the default configuration for 26 of the 182 problems.
In the current BDPO2 system, the subplanner time limit is increased only when a window
is retried. A procedure that either attempts candidate windows more likely to be improved
(for example, as indicated by the window ranking policies described in Section 4.6) more
frequently, or varies the amount of time given to optimise each window may perform better.
The optimal amount of deordering to do on each plan may well be different from problem to problem. But averaged across the set of benchmark problems, more deordering is
unarguably better than none.

4. Windowing Strategies and Ranking of Windows
A window is a subplan of some linearisation of the block deordered plan, extracted in order
to attempt local optimisation. This section describes the strategies we use to generate and
rank windows, and an experimental evaluation of their impact on our systems performance.
Recall from Definition 8 (page 390) that a window is represented by a triple hp, w, qi,
where w is the set of blocks to be replaced, and p and q are the sets of blocks ordered
before and after w, respectively, in the linearisation. A block decomposed p.o. plan can
have many linearisations, producing many possible windows  typically far too many to
attempt to optimise them all. A windowing heuristic is a procedure that extracts a reduced
set of windows, hopefully including the most promising ones, in a systematic way. We
404

fiContinuing Plan Quality Optimisation

Figure 14: A block deordered plan and its transformation into extended blocks: blocks b1
and b3 are merged into a single block, as are blocks b5 and b6.

Windowing heuristics
Rule-based
Cyclic thread
Causal followers

Generated
Basic
Ext.
108
35
59
47
72
45

Not filtered out
Basic
Ext.
59
22
45
31
41
20

Improved
Basic
Ext.
23
9
15.5
11
15.5
7

Impr./Gen.
Basic
Ext.
0.21
0.26
0.26
0.23
0.22
0.16

Table 2: The total number (in thousands) of windows that were generated, not filtered out,
and finally improved, using different windowing heuristics over different block types (basic
and extended). The number of possible windows over the sequential input plans, before
even considering deordering, is over 1.47 million. The rightmost pair of columns shows the
rate of success, meaning the fraction of improved windows out of the generated windows.
The numbers are from the results of experiment 1 (described in Section 3.1 on page 392).
present three windowing heuristics, called the rule-based, cyclic thread, and causal followers
heuristics. Each of them is described in detail in the following subsections.
Each heuristic is applied over two types of block  basic and extended  one at a time.
Basic blocks are the blocks generated by a block deordering. (For the purpose of windowing,
any step that is not included in a block created by block deordering is considered to be
a block on its own.) Extended blocks are created by merging basic blocks in the block
deordered plan that form complete non-branching subsequences. If block bi is the only
immediate predecessor of block bj , and bj the only immediate successor of bi , they are
merged into one extended block. Algorithm 4 shows the procedure for extended block
formation. (IP(b) denotes the set bs immediate predecessors, while IS(b) is bs immediate
successors.)
Algorithm 4 Computing extended blocks.
1: Bext  Bbasic
2: while bi , bj  Bext : IP(bj ) = {bi }, IS(bi ) = {bj } do
3:
Bext  Bext  {bi  bj } \ {bi , bj }
The process is further illustrated by an example in Figure 14. Note that blocks b5
and b2 are not merged into one extended block. This is because although b5 is the only
immediate successor of b2, b2 is not the only immediate predecessor of b5. Extended blocks
are useful because they allow some windowing heuristics to capture larger windows. Our
experiment results show that windows of different sizes are more useful in different domains:
405

fiSiddiqui & Haslum

Algorithm 5 Extract More Candidate windows
/* global array strategy[1..6] stores the state of each windowing strategy */
1: procedure ExtractMoreWindows(bdp , windowDB, optSubprob)
2:
W =
3:
tlimit = initial time limit Tincrement
4:
while telapsed < tlimit or |W | < nWindowsLimit do
5:
i = NextWindowingStrategy()
6:
if i = null then break /* all windowing strategies are exhausted */
7:
W = strategy[i].GetWindows(bdp , windowDB, optSubprob,
nWindowsLimit  |W |, tlimit  telapsed )
8:
if telapsed  tlimit and W =  then tlimit += Tincrement
9:

windowDB.Insert(W )

For example, larger windows are more likely to be improved in the Pegsol, Openstacks and
Parcprinter domains, while optimising smaller windows is better in the Elevators, Transport,
Scanalyzer and Woodworking domains.
A windowing strategy is a windowing heuristic applied to a block type. Thus, we use
a total of six different strategies. Each of these strategies contributes some improvable
windows that are not generated by any of the other strategies (cf. Section 4.4, and in
particular Table 3 on page 411). Thus, all of them are, in this sense, useful. On the other
hand, the size of the set of windows that each generates and the fraction of improvable
windows in this set varies between the strategies, and in that sense some are more useful
than others.
Table 2 shows results from our first experiment, in which we systematically tried two
subplanners (PNGS and IBCS) on every window generated (and not filtered out) by any
windowing strategy over 219 input plans. The table shows the total number (in thousands)
of windows that were generated, that remain after filtering, and that were finally improved
by at least one of the two subplanners. In this experiment, windows were filtered out only
if the window cost matched the lower bound given by the admissible LM-Cut heuristic
(Helmert & Domshlak, 2009). The experiment setup is further described in Section 3.1 (on
page 392). The first observation is that all strategies are very selective. The number of
windows that could potentially be generated, even without considering deordering, i.e., only
by taking all subsequences of the totally ordered input plans, is over 1.47 million. Thus,
even the most prolific strategy generates less than a tenth of the possible windows. Second,
we used the rate of success, meaning the fraction of windows generated that were improved
by any of the subplanners used in the experiment, to order the strategies. The order is as
follows:
1. Rule-based heuristic over extended blocks.
2. Cyclic thread heuristic over basic blocks.
3. Cyclic thread heuristic over extended blocks.
4. Causal followers heuristic over basic blocks.
5. Rule-based heuristic over basic blocks.
6. Causal followers heuristic over extended blocks.
406

fiContinuing Plan Quality Optimisation

The neighbourhood exploration procedure (Algorithm 2 on page 391) adds windows to
the database incrementally, by calling the ExtractMoreWindows procedure shown in
Algorithm 5. This procedure selects the next strategy to try, cycling through them in
the order above, and asks this strategy to generate a specified number of windows, in
a limited time. Each strategy keeps its own state (what part of the heuristic has been
applied and up to what part of the plan), so that the next time it is queried it can resume
generating new windows. When all windows that are possible under a given strategy have
been generated, we say the strategy is exhausted. The windowing strategies discard (1)
windows that are known to be optimal, either because their cost matches the lower bound
given by the admissible LM-Cut heuristic (Helmert & Domshlak, 2009), or because they
are in the stored set of optimally solved subproblems, and (2) windows that overlap with an
already improved window. These windows are not eligible for optimisation (cf. Section 3),
so generating them is redundant. If the selected strategy finishes without generating enough
windows and time remains, the next not-yet-exhausted strategy in the order is queried, and
so on, until either |W | = nWindowsLimit or time is up. If no windows are generated, and
some strategies are still not exhausted, the time limit is increased.
4.1 Rule-Based Windowing Heuristic
Our first version of BDPO (Siddiqui & Haslum, 2013b) used a single windowing strategy,
based on applying a fixed set of rules over extended blocks. Because this strategy complements the new windowing heuristics well, we have kept it in BDPO2.
Each rule when applied to a block b in a block deordered plan bdp selects a set of
blocks from to go into the replaced part (w) based on their relation to b. To ensure that
the window is consistent with the block deordering (i.e., has a consistent linearisation, as
stated in Definition 8 on page 390), any blocks that are constrained to be ordered between
blocks in the window must also be included. We call these the intermediate blocks, formally
defined as follows.
Definition 10. Let bdp = hS, B, i be a block decomposed p.o. plan. The intermediate
blocks of B  B are IB(B) = {b |  b0 , b00  B : b0  b  b00 }.
Let b be a block in bdp , and let Un(b) be the set of blocks that are not ordered w.r.t. b,
IP(b) the immediate predecessors of b, and IS(b) its immediate successors. The rules used
by the windowing heuristic are:
1. w0  {b}.
2. w0  {b}  IP(b).
3. w0  {b}  IS(b).
4. w0  {b}  Un(b).
5. w0  {b}  Un(b)  IP(b).
6. w0  {b}  Un(b)  IS(b).
7. w0  {b}  Un(b)  IP(b)  IS(b).
8. w0  {b}  Un(b)  IP({b}  Un(b)).
9. w0  {b}  Un(b)  IS({b}  Un(b)).
407

fiSiddiqui & Haslum

Figure 15: Window formation by applying the 1st rule of the rule-based windowing heuristic
over block b1, i.e., w  {b1}, p  Un(b1). The unordered block b2 is placed in its
predecessor set. Note that this window can be optimised by removing s3 because this step
has no causal link to its successors.

10. w0  {b}  Un(b)  IP({b}  Un(b))  IS({b}  Un(b)).
Given the blocks selected by one of the rules above, the partitioning of blocks into hp, w, qi
is made by setting w = w0  IB (w0 ) and assigning to p any block that is ordered before
or unordered with w, and to q any block ordered after w. Figure 15 shows an example of
rule-based windowing, where the 1st rule is applied to block b1. Applied to all blocks, the
above rules can produce duplicates; of course, only unique windows are kept.
The first rules, which include fewer blocks, generally produce smaller windows, while
the later rules tend to produce larger window (though there is no exact relation, since the
number of actions in a block varies). The heuristic applies all rules to each block in the
block deordered plan bdp in turn. Rules are applied in the order 1,10,2,9,3,8,4,7,5,6, i.e.,
starting with the first, the last, the second, the second last, and so on. The blocks are
ordered by size (descending), with ties broken by their order in the input plan (in opposite
direction for extended blocks).
Recall that ExtractMoreWindows repeatedly asks each windowing strategy to generate a limited number of windows. The ordering of blocks and rules described above helps
to ensure that the heuristic generates a varied set of windows, including both small and
large, covering different parts of the current plan, each time it is queried.
4.2 The Cyclic Thread Windowing Heuristic
To discover new windowing heuristics, we noted some key changes in the decomposed plan
structure that frequently occur when a plan is improved. One significant observation is
that if multiple steps of an input plan have the same add effects, then those steps together
with the steps necessarily ordered in between them form a subplan that can often be im408

fiContinuing Plan Quality Optimisation

proved. We call this cyclic behavior. In one experiment, we found that cycles of this type
are either removed from the plan or replaced with different cycles in more than 87% of
the improvements across most domains. The definition of cyclic behavior is based on an
individual atom. Intuitively, an atom has cyclic behavior if it has multiple producers (as
defined below).
Definition 11. Let bdp = hS, B, i be a block decomposed p.o. plan, and Pm  S be the
set of producers of an atom m, i.e., sPm m  add(s). m has cyclic behavior iff |Pm | > 1.
Note that Pm contains the init step sI iff m  I. However, since a window never contains
the initial step sI , candidate windows are formed from extended producers instead. A step
s 
/ {sI , sG } is an extended producer of atom m iff s produces m, or s consumes m and
there is no s0 6= sI that produces m and ordered before s in the block deordered plan. The
formal definition is as follows.
Definition 12. Let bdp = hS, B, i be a block decomposed p.o. plan. A step s  S is an
extended producer of an atom m iff s 
/ {sI , sG } and:
1. m  add(s) or
2. m  pre(s) and kS\sI if m  add(k) then s + k.
In order to form candidate windows with respect to an atom m having cyclic behavior,
we first extract all the blocks that contain at least one extended producer of an atom m.
A cyclic thread (cf. Definition 14) is then formed by taking a linearisation of those blocks,
consistent with the input plan.
Definition 13. Let bdp = hS, B, i be a block deordering of a sequential plan seq , and
bx , by  B are two blocks such that bx  by = . Let hbx , by i be a linearisation of {bx , by }.
hbx , by i is consistent with seq if at least one step in bx appears before a step in by in seq .
The way we linearise the blocks so that it is consistent with the input plan is clarified
by the following example. Assume bx : {sa , sc } and by : {sb , sd } are two blocks that we
have to linearise, and that the orderings of their constituent steps in the input plan is
sa in sb in sc in sd . The linearisation starts with the block that contains the first
element of in , i.e., bx in this case (since it contains sa ); in is then updated to in \bx , and
the linearisation continues in the same fashion until in is empty. The resulting linearisation
of the example blocks will be hbx , by i. If multiple (nested) blocks contain the first element
of in , the innermost one is picked. The formal definitions of thread and cyclic thread are
as follows.
Definition 14. Let bdp = hS, B, i be a block deordering of a sequential plan seq , EPm  S
be the set of extended producers of an atom m, and Bm  B be the set of blocks, where
each element of Bm contains at least one element of EPm . The thread of m, Tm , is the
linearisation of blocks in Bm such that the linearisation is consistent with seq . The thread
is called cyclic iff m has cyclic behavior.
For example, in the plan shown in Figure 15(i), atom (at t1 A) has cyclic behaviour,
since it holds in the initial state and is added by step s3. Its extended producers are s1, s3
and s4, so the cyclic thread is T(at t1 A) = hb1, b2i.
409

fiSiddiqui & Haslum

Finally, candidate windows are formed by taking a consecutive subsequence of blocks
(and intermediate blocks, as necessary) from a cyclic thread. Like in rule-based windowing,
blocks that are unordered with respect to the window are assigned to the set of blocks that
will precede the window.
Definition 15. Let Tm = b1 , ..., bk be a cyclic thread of an atom m. The cyclic thread-based
windows over the cyclic thread Tm are Wl,m = {B  IB(B) | B = bi , ..., bi+l is a consecutive
subsequence of Tm }, while the unordered blocks are always placed in its predecessor set.
Also like the rule-based windowing heuristic, the cyclic thread heuristic generates windows in an order that aims to ensure it returns a varied set of windows each time it is
called. It first identifies all cyclic threads in the block deordered plan and then generates
a stream of candidate windows from one cyclic thread after another. As mentioned, each
candidate window is formed by taking a consecutive subsequence of blocks (and the intermediate blocks as required to form a consistent window) from the cyclic thread. Given a
thread of |Tm | blocks, subsequences are generated according to the following order of sizes:
1, |Tm |, 2, |Tm |  1, . . . , |Tm |/2. In other words, the subsequence lengths are ordered as the
smallest, the biggest, the second smallest, the second biggest, and so on. For each size in
this order, all windows are generated moving from the beginning to the end of the thread.
4.3 Causal Followers Windowing Heuristic
The third strategy that we have use to obtain a broader range of potentially improvable
windows is similar to the cyclic thread heuristic in that it creates windows that are subsequences of a linearisation of blocks connected by a particular atom, but different in that
these connections are via causal links.
Definition 16. Let bdp = hS, B, i be a block decomposed p.o. plan, and c be the set of
causal links in . The causal followers of an atom m for a producer p  S are CFhm,pi =
{p, sj , ..., sk |{hp, m, sj i, ..., hp, m, sk i}  c } \ {sI , sG }. The causal followers of m (for all
producers), CFm , is the sequence hCFhm,p1 i , ..., CFhm,pn i i, where p1 , ..., pn is a linearisation
of all the producers of m.
In other words, the causal followers of an atom m is a list of sets of steps. In each
set of steps, one is the producer s and the others are consumers sj of m, and s has a
causal link to every sj for m, i.e., PC(m)  Re(s  sj ). For example, the atom (at t1 B)
in the block deordered plan in Figure 15(i) appears in two causal links, both with the
same producer: hs1, (at t1 B), s2i and hs1, (at t1 B), s3i. Thus, the causal followers are
CF(at t1 B) = h{s1, s2, s3}i.
From the block deordered plan we extract the sequence of sets of blocks corresponding
to the causal follower steps, according to the definition below. For example, the sequence
of causal follower blocks of CF(at t1 B) in the plan in Figure 15(i) is CFB(at t1 B) = h{b1}i,
since all steps in CF(at t1 B) are contained in block b1.
Definition 17. Let bdp = hS, B, i be a block decomposed p.o. plan, and CFhm,pi be
the causal followers of an atom m with respect to a producer p  S. The causal follower
blocks with respect to a producer p  S of an atom m, CFBhm,pi , is the set of blocks,
where each block contains at least one element of CFhm,pi . The causal follower blocks of m
410

fiContinuing Plan Quality Optimisation

Exclusive
All

Basic block
66.52%
91.86%

Ext. block
8.14%
33.48%

Rule-based
24.50%
63.22%

Cyclic thread
6.09%
34.01%

Causal followers
17.78%
66.34%

Table 3: Percentage of improvable windows found using the two block types and three
windowing heuristics, out of the total number of improvable windows found using all blocks
types and windowing heuristics. The first row gives the percentage of improvable windows
found by one block type but not the other (or by one windowing heuristic but not the
others), while the second row gives the percentage of all improvable windows found by one
block type (or windowing heuristic). The results are from the first experiment, described
in Section 3.1.
(for all producers), CFBm , is the sequence hCFBhm,p1 i , ..., CFBhm,pn i i, where p1 , ..., pn is a
linearisation of all the producers of m in bdp .
Candidate windows are formed by taking consecutive subsequences of the sequence of
causal follower blocks (with intermediate blocks, as necessary). The formal definition is
given below. Like in the other windowing heuristics, blocks that are unordered with respect
to the window are assigned to the set of blocks that will precede the window.
Definition 18. Let bdp = hS, B, i be a block decomposed p.o. plan, and CFBm =
hCFBhm,p1 i , ..., CFBhm,pn i i be the causal follower blocks of m. The causal followers-based
windows over CFBm are Wl,m = {B  IB(B) | B = CFBhm,pi i  ...  CFBhm,pi+l i is a
consecutive subsequence of CFBm of length l}, while the unordered blocks are always placed
to its predecessor list.
The order in which windows are generated by the causal followers heuristic is based
on the same principle as in the cyclic thread heuristic. It generates a stream of candidate
windows from the causal follower blocks CFBm associated with each atom m in turn. These
windows are consecutive subsequences of sets of blocks from CFBm , of lengths chosen
according to the pattern 1, l, 2, l  1, ..., (l/2), where l is the length of CFBm .
4.4 The Impact of Windowing Heuristics
No one single windowing heuristic or block type, nor any combination of them, is guaranteed
to find all improvable windows. The first row of Table 3 shows the percentage of improvable
windows found using one block type but not the other (or by one windowing heuristic but
not the others), out of the total number of improvable windows found using all blocks types
and windowing heuristics. (The results are from the first experiment, described in Section
3.1). It shows that every windowing heuristic and block type contributes some improvable
windows that are not found by other strategies. For example, 24.5% of improvable windows
are found only by the rule-based windowing heuristic (using both basic and extended blocks).
On the other hand, 36.78% of all improvable windows are not found by this heuristic. Each
of the windowing heuristics has its strengths and limitations. The rule-based heuristic,
for example, can only generate windows that contain sequences of extended blocks up to
a fixed length, while the cyclic thread and causal followers heuristics only make windows
from blocks connected by a single atom.
411

fiSiddiqui & Haslum

0.963

0.955

0.951




0.947














































0.943




















0.939


































0.935



















0.931





0.927



3

2.5

2

1.5

1

0.5

0

6



0.919

5.5




5

0.923

BDPO2 (combined windowing heuristics)
BDPO2 (random windowing)
BDPO2 (rulebased windowing only)
BDPO2 (causal followers windowing only)
BDPO2 (cyclic thread windowing only)
4.5



4



3.5

Average Quality Score (Relative IPC Quality Score / Coverage)

0.959

Time (hours)

Figure 16: Average IPC quality score as a function of time for separate runs of BDPO2 using
each of the three windowing heuristics alone, all three heuristics combined, and random
window generation. Each run is done using the same setup as experiment 3, described in
Section 3.1 (on page 392). The x-axis here shows only the runtime of BDPO2 (i.e., without
the 2 hour delay for generating the input plans, as shown in Figure 11). Note also that the
y-axis is truncated: The average quality of the input plans is 0.907.

412

fiContinuing Plan Quality Optimisation

Figure 16 shows the impact of different windowing heuristics on the anytime performance
of BDPO2, as measured by the average IPC plan quality score achieved as a function of timeper-problem. In this experiment, we ran BDPO2 with each of the three windowing heuristics
alone, and with all three combined in a sequential portfolio, as described at the beginning
of this section. (The combined portfolio of windowing heuristic is the same configuration
of BDPO2 that is presented in the experimental results in Section 3.2, page 394.) We also
compare these with a non-heuristic, random windowing strategy, in which each window
is formed by taking a random subsequence of blocks from a random linearisation of the
block deordered plan. Subsequences are chosen so that the distribution of window sizes
(measured by the number of actions in the window) is roughly the same as that produced
by the combined heuristics. The experiment uses setup 3 (described in Section 3.1 on page
392), i.e., the input plans to BDPO2 are already of high quality. (Their average IPC plan
quality score is 0.907.)
As predicted by the data in Table 3, using any of the three windowing heuristics on
its own results in a much worse system performance, since each fails to find a substantial
fraction of improvable windows. In fact, random window generation is better than any
of the heuristics on their own. However, the combined portfolio of heuristics outperforms
random windowing by a good margin: the total quality improvement achieved with the
random windowing strategy is 17.1% less than that of the best BDPO2 configuration. This
demonstrates that the heuristics capture information that is useful to guide the selection of
windows.
4.5 Possible Extensions to the Windowing Strategies
Since a window is formed by partitioning plan steps into three disjoint sets of blocks, the
number of possible windows is exponential. The challenge for a good windowing heuristic is
to extract a reduced set that contains windows more likely to be improved. Every windowing
strategy has some limitations. Hence, there is always a scope for developing new windowing
heuristics or extending the existing ones; one such extension is discussed in this section.
The combination of strategies we use may miss some improvable windows. For example,
a long sequence of blocks that do not form part of a cyclic thread or causal followers sequence
with respect to a single atom will not be captured by these heuristics. An example of this is
shown in Figure 17, where three candidate windows, W1, W2 and W3, found by the causal
followers windowing heuristic are not improvable separately. In this situation, forming a
window as the union of separate windows, found by one or several strategies, can overcome
the limitations of those strategies. In the example, the union of W1 and W2 is improvable.
This type of composite windows could be formed in the later stages of the plan improvement
process, after all the individual windowing heuristics have been exhausted. However, the
number of composite windows that can be created from a large set of candidate windows is
combinatorial and thus optimising all of them will take a long time.
4.6 Window Ranking
Although the windowing strategies generate only a fraction of all possible windows, the
number of candidate windows is still often large (cf. Table 2). In order to speed up the
413

fiSiddiqui & Haslum

Figure 17: Three candidate windows, W1, W2, and W3, found by the causal followers
windowing heuristic for atoms (at t1 B), (in p1 t2), and (in p2 t3) respectively. None of
them are improvable. However, the composite window formed by merging W1 and W2 is
improvable by substituting the delivery of package p1 (from location B to C) provided by
truck t2 with truck t1. This is because the atom (at t2 C) is not required by any of its
successors (i.e., the goal in this example).

plan improvement process, it is helpful to order windows so that those more likely to be
improved are optimised first. This is the role of window ranking.
Ranking windows is made difficult by the fact that the properties of improvable windows
vary from one to another, and a lot from domain to domain. For example, as mentioned at
the beginning of this section, larger windows are more likely to be improved in the Pegsol,
Openstacks and Parcprinter domains, while smaller windows are better for the Elevators,
Transport, Scanalyzer, and Woodworking domains. In the Sokoban domain, on the other
hand, medium-sized windows are better. Moreover, an improvable window may not be
improved by a particular subplanner within the given time bound. We have noted that in
some domains, e.g., Pegsol or Scanalyzer, subplanners require, on average, more time to
find a lower-cost plan.
We have developed a set of window ranking policies by examining structural properties
of the generated candidate windows generated and the results of our first experiment (cf.
Section 3.1) in which we ran two subplanners (IBCS and PNGS) on each generated window
with a 30 second time limit, excluding only windows whose cost is already shown to be
optimal by the admissible LM-Cut heuristic (Helmert & Domshlak, 2009). Investigating
the properties of improved and unimproved windows, we identified four metrics that work
relatively well across domains:
414

fiContinuing Plan Quality Optimisation

0.74
Random ranking
Outgoing causal links per length (min to max)
Incoming causal links per length (min to max)
Pairwise ordering disagreement (min to max)
Gap between cost & admissible heuristic (max to min)

Fraction of improvable windows out of the selected windows



0.72

0.7

0.68

0.66

0.64

0.62






























400

375

350

325

300

275

250

225

200

175

150

125

100

75

50

25

0.6

Number of selected (top ranked) windows

Figure 18: Fraction of improvable windows, across all domains, out of the selected top
windows from the ranked orders generated by each of the ranking policies (see text).

(1) The total number of causal links whose producers reside in a window and whose consumers are outside the window, divided by the length of the window  the lower the
value the higher the rank. We call this property outgoing causal links per length.
(2) The total number of causal links whose consumers reside in a window and whose producers are outside the window, divided by the length of the window  the lower the
value the higher the rank. We call this property incoming causal links per length.
(3) The gap between the cost of a window and the lower bound on the cost of any plan for
the corresponding subproblem given by the admissible heuristic  the higher the value
the higher the rank.
(4) The number of pairwise ordering (of steps) disagreements between a window hp, w, qi
and the sequential input plan  the lower the value the higher the rank. To calculate
this we first take the linearisation of hp, w, qi that is used to generate the corresponding
subproblem. Then, for every pair of plan steps, if the ordering between them in the
linearisation is not the same as in the input plan we call this a pairwise ordering
disagreement. The lower the total number of such disagreements is for a window, the
higher its rank. In other words, if the ordering of steps in a window is very different
from the input plan then it is less likely to be improved.

415

fiSiddiqui & Haslum

0.64
Fraction of improvable windows out of the selected windows

Random ranking
Outgoing causal links per length (min to max)
Incoming causal links per length (min to max)
Pairwise ordering disagreement (min to max)
Gap between cost & admissible heuristic (max to min)



0.62
0.6
0.58
0.56
0.54
0.52
0.5
0.48
0.46
0.44
0.42












0.4

















0.38




0.36
0.34
0.32

400

375

350

325

300

275

250

225

200

175

150

125

100

75

50

25

0.3

Number of selected (top ranked) windows

Figure 19: Fraction of improvable windows in the Parking domain, out of the selected top
windows from the ranked orders generated by each of the ranking policies (see text).

We can infer from the first two ranking policies that the more disconnected a window is
from other blocks in the decomposed plan the more likely it is to be improved. Figure 18
compares these ranking policies with the performance of a random ordering of the windows.
On average across all domains, all four ranking policies are good at picking out improvable
windows. For example, if we take the top 25 windows from the order generated by the
incoming causal links per length policy, nearly 74% of those windows are improvable (by
at least one subplanner), while the top 25 windows from the random order contain only
61% improvable windows. The random ranking in Figure 18 is the best result out of three
separate random rankings for each of the values on the x-axis. As expected, it exhibits
roughly the same ratio of improvable windows over all ranges (from 25 to 400). Nearly 61%
of the selected windows, across all domains, are improvable. However, the performance of
individual ranking policies varies by domain, and for each policy we find some domain in
which it is not good. For example, Figure 19 shows ranking results for instances of the
Parking domain only: Here, the outgoing causal links per length policy does not work
well. Considering the top 90 windows in the ranked order, it is even worse than random.
However, the other ranking policies are quite beneficial in this domain.
BDPO2 uses the first three ranking policies in a sequential portfolio (as explained in
Section 3). For each subplanner, BDPO2 uses a current ranking policy to select the next
416

fiContinuing Plan Quality Optimisation

0.963

Average Quality Score (Relative IPC Quality Score / Coverage)

0.961
0.959
0.957



0.955



0.953












0.951





0.949








0.947





0.945





0.943




0.941





0.939




0.937





0.935


0.933




0.931

BDPO2 (rankbased)
BDPO2 (randomranked)
6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0



Time (hours)

Figure 20: Average IPC plan quality score as a function of time in two separate runs: with
and without window ranking. In the second case, the order of the candidate windows is
randomised. Each run is done using experimental setup 3, as described in Section 3.1 on
page 392. The time shown here is the runtime of BDPO2 only (excluding the 2 hour delay
for generating the input plans, as shown in Figure 11). Also, the y-axis is truncated: All
curves start from the average quality score of the input plans, which is 0.907.

window for the chosen subplanner (from those eligible for optimisation by that subplanner).
If no improvement is found by that subplanner in a certain number of attempts (13, in our
current configuration), the system switches a different ranking policy, to produce a different
ordering of the candidate windows for that subplanner.
The use of window ranking has a beneficial effect on the anytime performance of the
plan improvement process, as shown in Figure 20. We achieve higher quality scores, and in
particular, achieve them faster, when using window ranking compared to random ranking.
In this experiment, we ran BDPO2 once with the portfolio of ranking policies, as described
417

fiSiddiqui & Haslum

above, and once with windows chosen for optimisation in a random order. The experiment
used the same setup as experiment 3 (described in Section 3.1 on page 392).
We tried many alternative methods of combining the ordered lists generated by different
ranking policies, in order to achieve a ranking with more stable performance across domains.
The problem of combining rankings, often called rank aggregation, has been studied in many
disciplines, such as social choice theory, sports and competitions, machine learning, information retrieval, database middleware, and so on. Rank aggregation techniques range from
quite simple (based on rank average or number of pairwise wins) to complex procedures
that in themselves require solving an optimisation problem. We tried five simple but popular rank aggregation techniques, namely Bordas (1781) method, Kemenys (1978) optimal
ordering, Copelands (1951) majority graph, MC4 (Dwork, Kumar, Naor, & Sivakumar,
2001), and multivariate Spearmans rho (Bedo & Ong, 2014). The result of those experiments, however, is that rank aggregation does not produce better, or more stable, window
rankings, especially in cases where one individual policy is relatively bad. Hence our choice
of using the ranking policies in a cyclic portfolio instead.

5. On-line Adaptation
The LNS approach to optimisation by repeatedly solving local subproblems gives us the
opportunity for adapting the process on-line to the current problem. We have noted that
different subplanners, windowing strategies, and ranking policies work better in different
domains. For example, Figure 21 shows the fraction of local improvements found by each
of three subplanners in different domains. As can be seen, the IBCS subplanner is more
productive, compared to PNGS and LAMA, in the APPN, Barman, Maintenance, Parking,
Sokoban, and Woodworking domains. PNGS, on the other hand, is better in the Scanalyzer
and Visitall domains, and LAMA in the Elevators and Openstacks domains. Therefore, if we
can learn over the course of the local search the relative success rate of different subplanners
on the current problem, the system will perform better. In a similar fashion, window
generation strategies and ranking policies may also be adapted to the current problem, so
that the system is more likely to select subplans for optimisation that are improvable.
We use an on-line machine learning technique  the multi-armed bandit (MAB) model,
to be specific  to select the subplanner for each local optimisation attempt. This technique, and its impact on the anytime performance of BPO2 is described in the following
subsections.
For window selection, on-line adaptation is limited to switching between alternative
ranking policies. The window selected for optimisation by a subplanner is the top one
in the order given by the current ranking policy for that subplanner (cf. Section 4.6).
As long as improvements are found among these windows, we can consider the current
policy to be useful. When a subplanner reaches a certain number of attempts with no
improvements found, we switch to using the next policy for that subplanner. The number
of windows in each neighbourhood that are optimised is typically small compared to the
number of candidate windows generated. On average across all problems in experiment 3
(cf. Section 3.1 on page 392) optimisation by at least one subplanner is tried on 24.8% of
generated windows. Because of this, adapting the ranking policy has more influence over
418

fiContinuing Plan Quality Optimisation

Figure 21: The percentage of improved windows found by each of the subplanners (PNGS,
IBCS, and LAMA), out of the total number of improved windows found by all the subplanners. In this experiment, BDPO2 was run three times, each time with one subplanner. The
setup was the same as for experiment 3 (described in Section 3.1 on page 392).

which windows are tried than adapting the windowing strategies. The effect of adaptive
window ranking on the anytime performance of BDPO2 is shown in Figure 20 (page 417).
5.1 Bandit Learning
The multi-armed bandit (MAB) model is a popular machine learning formulation for dealing
with the exploration versus exploitation dilemma. In a MAB problem, an algorithm is
presented with a sequence of trials. In each round, the algorithm chooses one from a set of
alternatives (often called arms) based on the past history, and receives a reward for this
choice. The goal is to maximise the total reward over time. A bandit learning algorithm
balances exploiting the arms with the highest observed average reward with exploring poorly
understood arms to discover if they can yield better reward.
MAB has found numerous applications in diverse fields (e.g., control, economics, statistics, and learning theory) after the influential paper by Robbins (1952). Many policies have
been proposed for the MAB problem under different assumptions, for example, with independent (Auer et al., 2002) or dependent arms (Pandey, Chakrabarti, & Agarwal, 2007),
exponentially or infinitely many arms (Wang, Audibert, & Munos, 2008), finite or infinite
time horizon (Jones & Gittins, 1974), with or without contextual information (Slivkins,
2014), and so on.
We cast the problem of selecting the subplanner for each local optimisation attempt as a
multi-armed bandit problem. The goal of this is to maximise the total number of improved
windows over time. We use a learning algorithm based on the optimistic exploration strategy, which chooses an arm in the most favorable environments that has a high probability of
being the best, given what has been observed so far. This strategy is often called optimism
in the face of uncertainty. At each trial t, and for each arm k, the strategy is to use past
observations and a probabilistic argument to define high-probability confidence intervals
for the expected reward k . The most favorable environment for arm k is thus the upper
419

fiSiddiqui & Haslum

confidence bound (UCB) on k . A simple policy based on this strategy is to play the arm
having the highest UCB.
A number of algorithms have been developed for optimistic exploration of bandit arms,
such as UCB1, UCB2 and UCB1-NORMAL by Auer et al. (2002), UCB-V by Audibert,
Munos and Szepesvari (2009), and KL-UCB by Garivier and Cappe (2011). We use the
UCB1 algorithm for planner selection. The UCB1 algorithm
selects at each trial t the arm
q
2 ln t
with highest upper confidence bound Bk,t = 
bk,t +
nk , the sum of an exploitation term
and an exploration term, respectively. 
bk,t is the empirical mean of the rewards received
from arm k up
to
trial
t,
and
n
is
the
number
of times arm k has been tried so far. The
k
q
2 ln t
second term,
nk , is a confidence interval for the average reward, within which the true
expected reward falls with almost certain probability. Hence, Bk,t is a upper confidence
bound. The UCB1 algorithm can achieve logarithmic regret uniformly over the number of
trials and without any preliminary knowledge about the reward distributions (Auer et al.,
2002).
Applied to subplanner selection in BDPO2, the algorithm works as follows: First, we
select each subplanner p once, to initialise the average reward 
bp . After each optimisation
attempt, we give a reward of 1 to the chosen subplanner if it found an improvement and a
reward of 0 otherwise. We could use some other scheme for assigning rewards rather than
simply 0 and 1, for example, making the reward proportional to the amount of improvement
(or time taken to find it). However, we have observed that assigning varying rewards
to subplanners makes the bandit learning system more complicated, and does not help
in achieving better overall result. Next, we select for each
q attempt a subplanner p that
maximises the upper confidence bound of p, Bp,t = 
bp + 2nlnp t , as explained above. Here,
np is the number of times p has been tried so far, and t is the total number of optimisation
attempts (by all subplanners) done so far. We can see that Bp,t grows with t but shrinks
when t and np increase uniformly. This ensures that each alternative is tried infinitely often
but still balances exploration and exploitation. In other words, the more we try p, the
smaller the size of the confidence interval and the closer Up gets to its mean value 
bp . But

p cannot be tried once Up becomes smaller than p , where p is the planner with the best
average reward.
5.2 The Impact of Bandit Learning
The response of the bandit policy for subplanner selection is shown in Figure 22. The figure
shows the fraction of the total number of optimisation attempts that one subplanner, IBCS,
was selected, and the fraction of the total number of window improvements found by that
subplanner. Since BDPO2 in this experiment uses only two subplanners, IBCS and PNGS,
the corresponding fraction for PNGS is 1  y. As an example, in the third problem (from
the left) in the APPN domain, 100% of window improvements are found by IBCS, and the
bandit policy selects this subplanner for 84% of the total number optimisation attempts.
PNGS is chosen for the other 16%, but finds no improvement. We can see that the bandit
policy selects the more promising subplanner more often across the problems. However,
the bandit policy is somewhat conservative, because it ensures that we do not rule out
any subplanners that fare poorly early on. Moreover, as the current plan is improved it
420

fiContinuing Plan Quality Optimisation

1
improvement ratio
exploitation ratio



0.9
Exploitation and improvement ratio by IBCS



0.8
0.7




0.6
0.5





 

   









 

             


  
 
 
 

 



 
  








 


  












0.4



0.3



 
 





 



0.2
0.1

Woodworking

Visitall

Transport

Thoughtful

Tetris

Sokoban

Parking

Scanalyzer

Parcprinter

Nomystery

Maintenance

Hiking

Ged

Floortile

Elevators

Childsnack

Barman

Appn

0

Figure 22: The response of the bandit policy to subplanner success rates. The exploitation
ratio is the fraction of the total number of optimisation attempts for which the IBCS
subplanner was chosen, out of the total number of attempts by both subplanners. The
improvement ratio is the fraction of the total number of improved windows found by IBCS,
out of total number of improved windows found by both subplanners. Since IBCS and
PNGS are the only two subplanners used in this experiment, the corresponding ratios for
PNGS are the opposite (i.e., 1  y). The experiment was run with the same setup as
experiment 2, described in Section 3.1 on page 392.

becomes harder to find further improvements (within the given time bound), so the average
reward for both subplanners decreases. This forces the bandit policy to switch between the
subplanners more often.
Figure 23 shows the impact of combining the subplanners using the UCB1 bandit policy,
compared to simply alternating between subplanners or using each subplanner alone, on the
anytime performance of BDPO2. In this experiment we ran BDPO2 once with each of IBCS,
PNGS and LAMA as the only subplanner, once combining two of them (IBCS and PNGS)
using a simple alternation policy, which selects each of the two in turn, and once combining
the two using the bandit policy. Each run was done with experiment setup 3 (as described
in Section 3.1 on page 392), i.e., with input plans of a high quality. (The IPC plan quality
score of each plan is calculated as before; see page 394). The average score of the input
plans is 0.907.) As expected, combining the IBCS and PNGS subplanners in some fashion
leads to more quality improvement across the entire time scale than achieved by running
BDPO2 with any individual subplanner. The figure also shows that combining multiple
subplanners using the bandit policy is a better strategy than simply alternating between
421

fiSiddiqui & Haslum

0.963

Average Quality Score (Relative IPC Quality Score / Coverage)

0.959




















0.955






















0.951















0.947






0.943







0.939




0.935


0.931




0.927



















































0.923

BDPO2 (PNGS+IBCS: Bandit)
BDPO2 (PNGS+IBCS: Alternating)
BDPO2 (PNGS only)
BDPO2 (IBCS only)
BDPO2 (LAMA only)









0.919


6

5.5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0.915

Time (hours)

Figure 23: Average IPC quality score as a function of time per problem in five different
runs of BDPO2: using only each one of the three subplanners, using two of them (IBCS
and PNGS) combined with the UCB1 bandit policy, and without (using simple alternation
instead). This experiment was run with setup 3 as described in Section 3.1 (on page 392).
Note that the y-axis is truncated: All curves start at the average quality of the input plans,
which 0.907. The time shown here is the runtime of BDPO2 only, excluding the 2 hour
delay for generating the input plans shown in Figure 11).

422

fiContinuing Plan Quality Optimisation

them. The total quality improvement achieved by BDPO2 using the alternation policy is
6.8% less than that by BDPO2 using the bandit policy.

6. Related Work
We survey four areas of related work: Anytime search algorithms and post-processing approaches, which have in common with our approach the aim of continuing plan quality
improvement; uses of local search in planning; and finally, uses of algorithm portfolios in
planning.
6.1 Anytime Search
Large state-space search problems, of the kind that frequently arise in planning problems,
often cannot be solved optimally because optimal search algorithms exhaust memory before
finding a solution. Anytime search algorithms try to deal with such problems by finding a
first solution quickly, possibly using a greedy or suboptimal heuristic search, then continue
(or restart) searching for a better quality solution. Anytime algorithms are attractive because they allow users to stop computation at any time, i.e., after a good enough solution
has been found, or after too long a wait. This contrasts with algorithms that require the
user to decide in advance on a deadline, a suboptimality bound, or some other parameter
that fixes the trade-off between time and solution qualty.
Bounded suboptimal search is the problem of finding a solution with cost less than or
equal to a user specified factor w of optimal. Weighted A* (WA*) search (Pohl, 1970)
and Explicit Estimation Search (EES) (Thayer & Ruml, 2011) are two algorithms of this
kind that have been most used in planning. Iteratively applying any bounded suboptimal
search algorithm with a lower value of w whenever a new best solution is found provides an
anytime improvement of plan quality. Restarting WA* (Richter et al., 2010) does this, using
a schedule of decreasing weights. RWA* is used by the LAMA planner (Richter & Westphal,
2010) LAMA finds the first plan using a greedy best-first search (Bonet & Geffner, 2001).
It also uses several search enhancements, like preferred operators and deferred evaluation
(Richter & Helmert, 2009). EES conducts a bounded suboptimal best-first search restricted
to expanding nodes that may lead to a solution with a cost no more than a given factor w
times optimal. Among the open nodes in this set, it expands the one estimated to have the
fewest remaining actions between it and a goal. It uses both an admissible heuristic for plan
cost and more informative but inadmissible estimates to guide the search. AEES (Thayer
et al., 2012b) is an anytime version of EES. To achieve an anytime behavior, AEES lowers
the value of w whenever a new best solution is found.
The bounded-cost search (Stern, Puzis, & Felner, 2011) problem, of which the subproblems solved in our approach are an example, requires finding a solution with a cost less
than or equal to a user-specified cost bound C. The aim of a bounded-cost search algorithm is to find such a solution as quickly as possible. Iteratively applying any bounded-cost
search algorithm with a bound less than the cost of the best solution found so far provides
anytime quality improvement. This is what the IBCS algorithm, used as one of the subplanners in BDPO2, does. The BEES and BEEPS algorithms (Thayer, Stern, Felner, &
Ruml, 2012a) adapt EES to the setting of bounded cost search. These algorithms expand
423

fiSiddiqui & Haslum

the best open node among those whose inadmissible cost estimate is at most C, falling back
to to expanding the node with the best admissible estimate only if the set is empty.
Branch-and-bound algorithms explore the search space in some systematic fashion, using
an admissible heuristic (lower bound on cost) to prune nodes that cannot lead to a solution
better than the best found so far. Branch-and-bound can be implemented with a linearmemory, depth-first search strategy as well as on top of other strategies. In the experiment
reported in Section 3.2 (page 394) we used Beam-Stack Search (BSS) (Zhou & Hansen,
2005) as a bounded-cost search algorithm by providing as an initial upper bound the cost of
the base plan for each problem. BSS combines backtracking branch-and-bound with beam
search, which behaves like a breadth-first search but limits the size of the open list in each
layer by a user-specified parameter, known as the beam width. When forced to backtrack,
BSS reconstructs nodes pruned from the open list so that the search is complete. The beam
width parameter can be used to control the memory consumption of BSS so that it never
exceeds available memory. For planning problems, however, whose state spaces are often
dense in transpositions and where accurate admissible heuristics are expensive to compute,
repeatedly reconstructing paths to unexplored nodes becomes time-consuming.
Anytime search planners aim to provide continuing improvement of plan quality given
more time, and often succeed in doing that in the early stages of the search. However, as we
have observed in the results of our experiments, these algorithms often stagnate, reaching
a point where they do not find any better plans even after several hours of CPU time. (cf.
Figure 11 on page 395 and Section 3.2 on page 394.) For example, in our experiment LAMA
and AEES found better plans for only 8.7% and 6.1%, respectively, of the total number of
problems between 3 hours and 6 hours CPU time, while BDPO2 found better plans for
30.4% of the problems during the same time interval. Memory is one limiting factor, but
not the only one. For almost half the problems, AEES ran for a full 7 hours CPU time
without running out of memory, yet found very few improved plans. BSS found plans with
a cost less than the initial upper bound (the cost of the base plans) for only 14 out of 182
problems even after 24 hours CPU time per problem.
6.2 Local Search
Local search explores a space by searching only a small neighbourhood of a current element
in the search space for one that is, in some way, better, then moving to the neighbour and
repeating the process. Compared to systematic search algorithms, the advantage of local
search is that it needs much less memory. Therefore, local search algorithms are widely
used to solve hard optimisation problems. However, local search algorithms cannot offer
any guarantees of global optimality, or bounded suboptimality. In planning, local search has
been used mainly to find plans quickly, and rarely to improve plan quality, though some of
the post-processing methods discussed in the next section can be viewed as local searches.
FF (Hoffmann & Nebel, 2001) is a forward-chaining heuristic state space search planner.
The heuristic used by FF estimates the distance from a state to the nearest goal state. FF
uses a local search strategy, called enforced hill-climbing, that in each state uses a breadthfirst search to find a neighbour state (which may be several steps away from the current
state) with a strictly better heuristic value, i.e., that is believed to be closer to the goal.
It then commits to that state and starts a new search for a neighbour with a better yet
424

fiContinuing Plan Quality Optimisation

heuristic value. If the local search fails, due to getting trapped in a dead end, FF falls back
on a complete best-first search algorithm. The RW-LS planning algorithm (Xie, Nakhost,
& Muller, 2012) is similar to FFs hill-climbing approach, but uses a combination of greedy
best-first search and exploration by random walks to find a better next state in each local
search step. Nakhost and Muller (2009) developed a planning system, called Arvand, that
uses random walk-based local exploration in conjunction with the FF search heuristic. They
showed that Arvand outperforms FF on hard problems in many domains. The execution
of Arvand consists of a series of search episodes. Each episode starts with a set of random
walks from the initial state. The endpoint of each random walk is evaluated using the
heuristic function to choose the next state. The search episode then continues with a set
of random walks from this state. This process repeats until either the goal is reached,
or enough transitions are made without heuristic progress, in which case the process is
restarted. The IPC 2011 and 2014 versions of Arvand apply post-processing to improve the
quality of each generated plan. The post-processing techniques are Action Elimination and
Plan Neighborhood Graph Search (Nakhost & Muller, 2010); they are discussed in the next
subsection. Because Arvands search is randomised, the system can continue generating
alternative plans, which are then optmised, indefinitely, storing at all times the best plan
generated so far. This provides a certain anytime capability. It is in this manner that it
was used in the experiment reported in Section 3.2 on page 394.
The LPG planner (Gerevini & Serina, 2002) is based on local search in the space of
action graphs, which represent partial plans. The neighbourhood is defined by operators
that modify an action graph, such as inserting or removing actions. The function that
evaluates nodes in the neighbourhood combines terms that estimate both how far an action
graph is from becoming a valid plan, termed search cost, and the expected quality of
the plan it may become. The choice of neighbour to move to also involves an element of
randomness. LPG also performs a continuing search for better plans; in this, it is similar to
the anytime search algorithms discussed in the last subsection. Whenever it finds a plan,
the local search restarts with a partial plan obtained by removing some randomly selected
actions from the current plan. A numerical constraint forcing the cost of the next plan to
be lower is also added. This provides some guidance towards a better quality next plan.
There is a close relationship between local search approaches to planning and plan repair
or adaptation methods (Garrido, Guzman, & Onaindia, 2010). The LPG planner originated
as a method of plan repair (Gerevini & Serina, 2000), and iterative repair methods can also
be used for plan generation (Chien, Knight, Stechert, Sherwood, & Rabideau, 2000).
A key difference between our use of local search and its previous uses in planning is that
we carry out a local search only in the space of valid plans. This permits the neighbourhood
evaluation to focus exclusively on plan quality. Searching a space of partial plans (represented by states) as done in FF, or incomplete (invalid) plans, as done in LPG, requires
neighbourhood evaluation to consider how close an element is to becoming a valid plan, and
balancing that with quality.
The large neighbourhood search (LNS) strategy formulates the problem of finding a
good neighbor as an optimisation problem, rather than simply enumerating and evaluating
neighbours. This allows a much larger neighbourhood to be considered. LNS has been used
very successfully to solve hard combinatorial optimisation problems like vehicle routing with
time windows (Shaw, 1998) and scheduling (Godard, Laborie, & Nuijten, 2005). Theoretical
425

fiSiddiqui & Haslum

and experimental studies have shown that the increased neighborhood size may improve the
effectiveness (quality of solutions) of local search algorithms (Ahuja, Goodstein, Mukherjee,
Orlin, & Sharma, 2007). If the neighbourhood of the current solution is too small then it
is difficult to escape from local minima. In this case, additional meta-heuristic techniques,
such as Simulated Annealing or Tabu Search, may be needed to escape the local minimum.
In LNS, the size of the neighborhood itself may be sufficient to allow the search process to
avoid or escape local minima.
In the LNS literature, the neighborhood of a solution is usually defined as the set of
solutions that can be reached by applying a destroy heuristic and a repair method.
The destroy heuristic selects a part of the current solution to be removed (unassigned),
and the repair method rebuilds the destroyed part, keeping the rest of the current solution
fixed. The destroy heuristic often includes an element of randomness, enabling the search
to explore modifications to different parts of the current solution. The role of the destroy
heuristic in our system is played by the windowing strategies, which select candidate windows (subplans) for re-optimisation. We explore these windows systematically. Some LNS
algorithms (e.g., Ropke & Pisinger, 2006; Schrimpf et al., 2000) allow the local search to
move to a neighbouring solution with a lower quality (e.g., using simulated annealing). We
consider only strictly improving moves. However, in difference to previous LNS algorithms,
we do not immediately move to a better plan and restart neighbourhood exploration after a
local improvement has been found. Instead, we use delayed restarting, which allows a better
solution to be found in one local search step by destroying and repairing multiple parts of
the current plan. Experimentally, we found that delayed restarting produces better quality
plans, and produces them faster, than immediate restarts (cf. Section 3.4 on page 399).
6.3 Plan Post-Processing
By a post-processing method, we mean one that takes a valid plan as input and attempts to
improve it, by making some modifications. This is also related to plan repair and adaptation
(Chien et al., 2000; Fox, Gerevini, Long, & Serina, 2006; Garrido et al., 2010), but with
the key difference that plan repair or adaptation starts from a plan that is not valid for
the current situation and focuses on making it work; the discrepancy between the current
state or goals and those the plan was originally built for provide guidance to where repairs
are needed. In contrast, post-processing for plan optimisation may require modifications
anywhere in the current plan.
Nakhost and Muller (2010) proposed two post-processing techniques  Action Elimination (AE) and Plan Neighborhood Graph Search (PNGS). Action elimination identifies and
removes some unnecessary actions from the given plan. PNGS constructs a plan neighborhood graph, which is a subgraph of the state space of the problem, built around the
path through the state space induced by the current plan by expanding a limited number of
states from each state on that path. It then searches for the least-cost plan in this subgraph.
If this finds a plan better than the current, the process is repeated around the new best
plan; otherwise, the exploration limit is increased, until a time or memory limit is exceeded.
Furcys (2006) Iterative Tunneling Search with A* (ITSA*) is similar to PNGS. ITSA*
explores an area, called a tunnel, of the state space using A* search, restricted to a fixed
distance from the current plan. These methods can be seen as creating a neighborhood
426

fiContinuing Plan Quality Optimisation

that includes only small deviations from the current plan, but anywhere along the plan. In
contrast, BDPO2 focuses on one section of the decomposed plan at a time, often grouping
together different parts of the input plan, but puts no restriction on how much that section
changes; hence, it creates a different neighbourhood. Our experiments show that the best
results are obtained by exploring both neighbourhoods. For example, PNGS often finds
plan improvements quickly, but running it for an additional 6 hours improves its average
IPC plan quality score, over that of the best plans it finds in the first hour, only by 0.01%.
Running instead BDPO2, using PNGS as the only subplanner and taking the best plans
found by PNGS in 1 hour as input, improves the average plan quality score by 3% in 6
hours.
Ratner and Pohl (1986) used local optimisation for shortening solutions to sequential
search problems. To select the subpath to optimise, they used a sliding window of a predefined size dmax over consecutive segments of the current path. Estrem and Krebsbach
(2012) instead used a form of windowing heuristic: They select for local optimisation pairs
of states on the current path that maximise an estimate of redundancy, based on the ratio
between the estimated distances between the two states, given by a state space heuristic,
and the cost of the current path. Balyo, Bartak and Surynek (2012) used a sliding window
approach to minimise parallel plan length (that is, makespan, assuming all actions have
unit duration). Rather than take segments of a single path in the state space, we use block
deordering of the input plan to create candidate windows for local optimisation. As shown
by the experimental results, this is very important for the success of BDPO2: The total
improvement of average plan quality achieved without deordering was 28.7% less than that
achieved by BDPO2 using block deordering of input plans (cf. Section 3.6 on page 402).
The planning-by-rewriting approach (Ambite & Knoblock, 2001) also uses local modifications of partially ordered plans to improve their quality. Plan modifications are defined by
domain-specific rewrite rules, which have to be provided by the domain designer or learned
from many examples of both good and bad plans. Hence, this technique can be effective for
solving many problem instances from the same domain. Using a planner to solve subproblems may be more time-consuming than applying pre-defined rules, but makes the process
automatic. However, if we consider solving many problems from the same domain it may be
possible to reduce average planning time by learning (generalised) rules from the subplan
improvements we discover and using these where applicable to avoid invoking a subplanner.
6.4 Portfolio Planning and Automatic Parameter Tuning
A portfolio planning system runs several subplanners in sequence (or in parallel) with short
timeouts, in the hope that at least one of the component planners will find a solution in
the time allotted to it. Portfolio planning systems are motivated by the observations that
no single planner dominates all others in all domains, and that if a planner does not solve
a planning task quickly, often it does not solve it at all. Therefore, many of todays most
successful planners run a sequential portfolio of planners (Coles, Coles, Olaya, Celorrio,
Linares Lopez, Sanner, & Yoon, 2012).
Gerevini, Saetti and Vallati (2009) introduced the PbP planner, which learns a portfolio
over a given set of planners for a specific domain, as well as domain-specific macro-actions.
Fast Downward Stone Soup (FDSS, Helmert, Roger, Seipp, Karpas, Hoffmann, Keyder,
427

fiSiddiqui & Haslum

Nissim, Richter, & Westphal, 2011) uses a fixed portfolio, computed to optimise performance
on a large sample of training domains, for all domains. IBaCoP2 (Cenamor et al., 2014)
dynamically configures a portfolio using a predictive model of planner success.
Another recent trend is the use of automatic algorithm configuration tools, like the
ParamILS framework (Hutter, Hoos, Leyton-Brown, & Stutzle, 2009), to enhance planner
performance on a specific domain. ParamILS does a local search in the space of configurations, using a suite of training problems to evaluate performance under different parameter
settings. The combinatorial explosion caused by many parameters with many different values is managed by varying one parameter at a time. ParamILS has been used to configure
the LPG planner (Vallati, Fawcett, Gerevini, Hoos, & Saetti, 2011) and the Fast Downward planner (Fawcett, Helmert, Hoos, Karpas, Roger, & Seipp, 2011). The PbP2 portfolio
planner (Gerevini, Saetti, & Vallati, 2011), successor to PbP, includes a version of LPG
customised to the domain with ParamILS in the learned portfolio.
BDPO2, of course, uses a portfolio of subplanners, and, as we have shown, selecting the
right subplanner for the current problem is important (cf. Section 5). Much more important,
however, is the focus on subproblems that our approach brings: comparing Figures 11 (page
395) and 23 (page 422), it is clear that using even a single subplanner within BDPO2 is
more effective than using any of the subplanners on its own. The multiple window ranking
policies used in BDPO2 (cf. Section 4.6) can also be viewed as a simple sequential portfolio.
Compared to previous portfolio planners, the iterated use of subplanners, windowing strategies and other components in our approach offers a possibility to learn the best portfolio
or configuration on-line; that is, rather than spend time on configuring the system using
training problems, we can learn from the experience of solving several subproblems, while
actually working on optimising the current plan.
Finally, although we have not explored it in great depth, our results suggest that combining different anytime search and post-processing methods, in what is effectively a kind of
sequential portfolio (such as running BDPO2 on the result of running PNGS on the result
of LAMA or IBaCoP2, as in the results of experiment 3, shown in Figure 2 on page 371),
often achieves better quality final plans than investing all available time into any single
method.

7. Conclusions and Future Work
Plan quality optimisation, particularly for large problems, is a central concern in automated
planning. Anytime planning, which aims to deliver a continuing stream of better plans
given more time, is an attractive idea, offering the flexibility to stop the process at any
point, such as when the best plan found is good enough or the wait for the next plan
becomes too long. We have presented an approach to anytime plan improvement, and its
realisation in the BDPO2 system. This approach is based on the large neighbourhood local
search strategy (Shaw, 1998), using windowing heuristics to select candidate windows from a
block deordering of the current plan, for local optimisation using off-the-shelf bounded-cost
planning techniques.
Experiments demonstrate that BDPO2 achieves continuing plan quality improvement
even at large time scales (several hours CPU time), when other anytime planners stagnate.
Key to achieving this is our focus on optimising subproblems, corresponding to windows.
428

fiContinuing Plan Quality Optimisation

As mentioned in Section 4.5, extending the windowing heuristics and improving the on-line
learning of effective window rankings is one way to improve the approach. Also, complementing the window ranking, which estimates how promising a window is, with an
estimate of how difficult windows are to optimise, and using this to inform the time allocated to subplanners, which is currently uniform for all windows, may contribute to better
performance. The best result, however, is achieved by chaining several techniques together
(for example, applying BDPO2 to the best plan found by PNGS applied to the best plan
found by LAMA or IBaCoP2). This result cannot be achieved by any of the previous anytime planning approaches alone. Thus, another area of future work is to examine in greater
depth what is the best way to combine different plan improvement methods, and how this
can be learned on-line while optimising a plan. For example, we have conducted a study
of the optimal time to switch from base plan generation, using LAMA, to post-processing
using PNGS or BDPO, as a function of the total runtime (Siddiqui & Haslum, 2013a).
As we have demonstrated experimentally, the block deordering step is essential for the
good performance of BDPO2 (cf. Section 3.6 on page 402). Block deordering creates a
decomposition of the plan into non-interleaving blocks while removing ordering constraints
between blocks. This lifts a limitation of conventional, step-wise, deordering, which
requires all unordered steps in the plan to be non-interfering. As we have shown, a validity
condition for block decomposed partially ordered plans can be stated that is almost the
same as Chapmans (1987) modal truth criterion, but allowing threats to a causal link to
remain unordered as long as the link is protected by the block structure (Theorem 2 on
page 379). Therefore, block deordering can yield less order-constrained plans, including in
some cases where no conventional deordering is possible.
The plan structure uncovered by block decomposition can also have other uses. Recently it was used in the planner independent macro generation system BloMa (Chrpa &
Siddiqui, 2015) to find longer macros that capture compound activities in order to improve
planners coverage and efficiency. In some domains (e.g., Barman, ChildSnack, Scanalyzer,
Parcprinter, Gripper, Woodworking, etc.), block deordering often identifies structurally similar subplans, which also have symmetric improvement patterns. This could potentially be
exploited in learning plan rewrite rules (Ambite, Knoblock, & Minton, 2000). The structure
of block deordered plans, which often comprises a nested, hierarchical decomposition into
meaningful subplans, is reminiscent of Hierarchical Task Network (HTN) representations.
Hence, block deordering technique could potentially be applied to generating (or helping to
generate) HTN structures in a domain independent way, reducing the knowledge-engineering
effort. Recent work by Scala and Torasso (2015) extends deordering to plans for planning
domains with numeric state variables, identifying numeric dependencies that capture the
additional reasons for necessary orderings. Defining the conditions on blocks sufficient to
encapsulate these dependencies would allow block deordering also of numeric plans. There
may be a synergy between block deordering and numeric planning, since numeric dependencies often involve groups of plan steps, rather than a single producerconsumer pair.
Acknowledgment
This work was partially supported by the Australian Research Council discovery project
DP140104219 Robust AI Planning for Hybrid Systems. NICTA is funded by the Aus429

fiSiddiqui & Haslum

tralian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.

References
Ahuja, R. K., Goodstein, J., Mukherjee, A., Orlin, J. B., & Sharma, D. (2007). A very
large-scale neighborhood search algorithm for the combined through-fleet-assignment
model. INFORMS Journal on Computing, 19 (3), 416428.
Ambite, J. L., & Knoblock, C. A. (2001). Planning by rewriting. Journal of Artificial
Intelligence Research (JAIR), 15 (1), 207261.
Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules. In
Proc. of the 5th International Conference on Artificial Intelligence Planning Systems,
AIPS 2000, Breckenridge, CO, USA, April 14-17, 2000, pp. 312. AAAI Press.
Audibert, J.-Y., Munos, R., & Szepesvari, C. (2009). Explorationexploitation tradeoff using
variance estimates in multi-armed bandits. Theoretical Computer Science, 410 (19),
18761902.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Backstrom, C. (1998). Computational aspects of reordering plans. Journal of Artificial
Intelligence Research (JAIR), 9, 99137.
Balyo, T., Bartak, R., & Surynek, P. (2012). On improving plan quality via local enhancements. In Proc. of the 5th International Symposium on Combinatorial Search, SOCS
2012, Niagara Falls, Canada, July 19-21, 2012. AAAI Press.
Bedo, J., & Ong, C. S. (2014). Multivariate Spearmans rho for aggregating ranks using
copulas. CoRR, abs/1410.4391.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (12), 533.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2014). IBaCoP and IBaCoP2 planners. In
Proc. of the 8th International Planning Competition, IPC 2014, Deterministic Part,
pp. 3538.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32 (3), 333377.
Chien, S., Knight, R., Stechert, A., Sherwood, R., & Rabideau, G. (2000). Using iterative
repair to improve the responsiveness of planning and scheduling. In Proc. of the
5th International Conference on Artificial Intelligence Planning Systems, AIPS 2000,
Breckenridge, CO, USA, April 14-17, 2000, pp. 300307. AAAI Press.
Chrpa, L., & Siddiqui, F. H. (2015). Exploiting block deordering for improving planners efficiency. In Proc. of the 24th International Joint Conference on Artificial Intelligence,
IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 15371543. AAAI Press.
Coles, A. J., Coles, A., Olaya, A. G., Celorrio, S. J., Linares Lopez, C., Sanner, S., & Yoon,
S. (2012). A survey of the seventh international planning competition. AI Magazine,
33 (1), 8388.
430

fiContinuing Plan Quality Optimisation

Copeland, A. H. (1951). A reasonable social welfare function. In University of Michigan
Seminar on Applications of Mathematics to the social sciences.
de Borda, J. C. (1781). Memory on election ballot. History of the Royal Academy of
Sciences, Paris, 657664.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods for
the web. In Proc. of the 10th International Conference on World Wide Web, WWW
2001, Hong Kong, May 1-5, 2001, pp. 613622, New York, NY, USA. ACM.
Estrem, S. J., & Krebsbach, K. D. (2012). AIRS: Anytime iterative refinement of a solution. In Proc. of the 25th International Florida Artificial Intelligence Research Society
Conference, Marco Island, Florida. May 23-25, 2012.
Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:
Domain-specific configuration using Fast Downward. In Proc. of the 2011 ICAPS
Workshop on Planning and Learning, PAL 2011, Freiburg, Germany, June 11-16,
2011, pp. 1320. AAAI Press.
Fox, M., Gerevini, A., Long, D., & Serina, I. (2006). Plan stability: Replanning versus plan
repair. In Proc. of the 16th International Conference on Automated Planning and
Scheduling, ICAPS 2006, Cumbria, UK, June 6-10, 2006., pp. 212221. AAAI Press.
Furcy, D. (2006). ITSA*: Iterative tunneling search with A*. In Proc. of the 2006 AAAI
Workshop on Heuristic Search, Memory-Based Heuristics and Their Applications,
July 1620, 2006, Boston, Massachusetts, pp. 2126. AAAI Press.
Garivier, A., & Cappe, O. (2011). The KL-UCB algorithm for bounded stochastic bandits
and beyond. CoRR, abs/1102.2490.
Garrido, A., Guzman, C., & Onaindia, E. (2010). Anytime plan-adaptation for continuous
planning. In Proc. of the joint 28th Workshop of the UK Special Interest Group on
Planning and Scheduling and 4th Italian Workshop on Planning and Scheduling, pp.
4754.
Gerevini, A., Saetti, A., & Vallati, M. (2009). An automatically configurable portfolio-based
planner with macro-actions: PbP. In Proc. of the 19th International Conference on
Automated Planning and Scheduling, ICAPS 2009, Thessaloniki, Greece, September
19-23, 2009, pp. 350353. AAAI Press.
Gerevini, A., Saetti, A., & Vallati, M. (2011). PbP2: Automatic configuration of a portfoliobased multi-planner. In 7th International Planning Competition (IPC 2011), Learning
Track. http://www.plg.inf.uc3m.es/ipc2011-learning.
Gerevini, A., & Serina, I. (2002). LPG: A planner based on local search for planning graphs
with action costs. In Proc. of the 6th International Conference on Artificial Intelligence
Planning and Scheduling, AIPS 2002, April 23-27, 2002, Toulouse, France, pp. 281
290. AAAI Press.
Gerevini, A. E., & Serina, I. (2000). Fast plan adaptation through planning graphs: Local
and systematic search techniques. In Proc. of the 5th International Conference on
Artificial Intelligence Planning Systems, AIPS 2000, Breckenridge, CO, USA, April
14-17, 2000, pp. 112121. AAAI Press.
431

fiSiddiqui & Haslum

Ghallab, M., Nau, D. S., & Traverso, P. (2004). Automated Planning: Theory & Practice.
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
Godard, D., Laborie, P., & Nuijten, W. (2005). Randomized large neighborhood search for
cumulative scheduling. In Proc. of the 15th International Conference on Automated
Planning and Scheduling, ICAPS 2005, Monterey, California, USA, June 5-10 2005,
pp. 8189. AAAI Press.
Haslum, P. (2011). Computing genome edit distances using domain-independent planning.
In Proc. of the 2011 ICAPS Workshop on Scheduling and Planning Applications,
SPARK 2011, Freiburg, Germany, June 11-16, 2011. AAAI Press.
Haslum, P. (2012). Incremental lower bounds for additive cost planning problems. In Proc.
of the 22nd International Conference on Automated Planning and Scheduling, ICAPS
2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012, pp. 7482. AAAI Press.
Haslum, P., & Grastien, A. (2011). Diagnosis as planning: Two case studies. In Proc. of
the 2011 ICAPS Workshop on Scheduling and Planning Applications, SPARK 2011,
Freiburg, Germany, June 11-16, 2011. AAAI Press.
Haslum, P., & Jonsson, P. (2000). Planning with reduced operator sets. In Proc. of the
5th International Conference on Artificial Intelligence Planning Systems, AIPS 2000,
Breckenridge, CO, USA, April 14-17, 2000, pp. 150158. AAAI Press.
Helmert, M., Roger, G., Seipp, J., Karpas, E., Hoffmann, J., Keyder, E., Nissim, R., Richter,
S., & Westphal, M. (2011). Fast Downward Stone Soup (planner abstract). In
Proc. of the 7th International Planning Competition, IPC 2011, Deterministic Part.
http://www.plg.inf.uc3m.es/ipc2011-deterministic.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats
the difference anyway?. In Proc. of the 19th International Conference on Automated
Planning and Scheduling, ICAPS 2009, Thessaloniki, Greece, September 19-23, 2009,
pp. 162169. AAAI Press.
Hoffmann, J. (2001). Local search topology in planning benchmarks: An empirical analysis.
In Proc. of the 17th International Joint Conference on Artificial Intelligence, IJCAI
2001, Seattle, Washington, USA, August 4-10, 2001, pp. 453458, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research (JAIR), 14, 253302.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: an automatic
algorithm configuration framework. Journal of Artificial Intelligence Research (JAIR),
36 (1), 267306.
Jones, D. M., & Gittins, J. (1974). A dynamic allocation index for the sequential design of
experiments. University of Cambridge, Department of Engineering.
Kambhampati, S., & Kedar, S. (1994). A unified framework for explanation-based generalization of partially ordered and partially instantiated plans. Artificial Intelligence,
67 (1), 2970.
432

fiContinuing Plan Quality Optimisation

McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proc. of the 9th
National Conference on Artificial Intelligence, AAAI 1991, Anaheim, CA, USA, July
14-19, 1991, Volume 2., pp. 634639. AAAI Press / The MIT Press.
Muise, C. J., McIlraith, S. A., & Beck, J. C. (2012). Optimally relaxing partial-order plans
with maxsat. In Proc. of the 22nd International Conference on Automated Planning
and Scheduling, ICAPS 2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012, pp. 358
362. AAAI Press.
Nakhost, H., & Muller, M. (2009). Monte-carlo exploration for deterministic planning.
In Proc. of the 21st International Joint Conference on Artificial Intelligence, IJCAI
2009, Pasadena, California, USA, July 11-17, 2009, Vol. 9, pp. 17661771.
Nakhost, H., & Muller, M. (2010). Action elimination and plan neighborhood graph search:
Two algorithms for plan improvement. In Proc. of the 20th International Conference
on Automated Planning and Scheduling, ICAPS 2010, Toronto, Canada, May 12-16,
2010, pp. 121128. AAAI Press.
Nebel, B., & Backstrom, C. (1994). On the computational complexity of temporal projection, planning, and plan validation. Artificial Intelligence, 66 (1), 125160.
Pandey, S., Chakrabarti, D., & Agarwal, D. (2007). Multi-armed bandit problems with
dependent arms. In Proc. of the 24th International Conference on Machine Learning,
ICML 2007, Corvallis, Oregon, USA, June 20-24, 2007, Vol. 227, pp. 721728. ACM.
Pednault, E. P. D. (1986). Formulating multiagent, dynamic-world problems in the classical
planning framework. Reasoning about actions and plans, 4782.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence,
1 (3), 193204.
Ratner, D., & Pohl, I. (1986). Joint and LPA*: Combination of approximation and search. In
Proc. of the 5th National Conference on Artificial Intelligence, AAAI 1986, Philadelphia, PA, August 11-15, 1986. Volume 1: Science., pp. 173177. Morgan Kaufmann.
Regnier, P., & Fade, B. (1991). Complete determination of parallel actions and temporal
optimization in linear plans of action. In Proc. of the European Workshop on Planning,
EWSP 1991, Sankt Augustin, FRG, March 18-19, 1991, Vol. 522 of Lecture Notes in
Computer Science, pp. 100111. Springer.
Richter, S., & Helmert, M. (2009). Preferred operators and deferred evaluation in satisficing
planning. In Proc. of the 19th International Conference on Automated Planning and
Scheduling, ICAPS 2009, Thessaloniki, Greece, September 19-23, 2009, pp. 273280.
AAAI Press.
Richter, S., Thayer, J. T., & Ruml, W. (2010). The joy of forgetting: Faster anytime search
via restarting. In Proc. of the 20th International Conference on Automated Planning
and Scheduling, ICAPS 2010, Toronto, Canada, May 12-16, 2010, pp. 137144. AAAI
Press.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. Journal of Artificial Intelligence Research (JAIR), 39, 127
177.
433

fiSiddiqui & Haslum

Robbins, H. (1952). Some aspects of the sequential design of experiments. In Herbert
Robbins Selected Papers, Vol. 58, pp. 527535. Springer.
Ropke, S., & Pisinger, D. (2006). An adaptive large neighborhood search heuristic for
the pickup and delivery problem with time windows. Transportation Science, 40 (4),
455472.
Scala, E., & Torasso, P. (2015). Deordering and numeric macro actions for plan repair.
In Proc. of the 24th International Joint Conference on Artificial Intelligence, IJCAI
2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 16731681. AAAI Press.
Schrimpf, G., Schneider, J., Stamm-Wilbrandt, H., & Dueck, G. (2000). Record breaking
optimization results using the ruin and recreate principle. Journal of Computational
Physics, 159 (2), 139171.
Shaw, P. (1998). Using constraint programming and local search methods to solve vehicle
routing problems. In Proc. of the 4th International Conference on Principles and
Practice of Constraint Programming, CP 1998, , Pisa, Italy, October 26-30, 1998,
Vol. 1520 of Lecture Notes in Computer Science, pp. 417431. Springer.
Siddiqui, F. H., & Haslum, P. (2012). Block-structured plan deordering. In Proc. of the 25th
Australasian Joint Conference on Advances in Artificial Intelligence, AI 2012, Sydney,
Australia, December 4-7, 2012, Vol. 7691 of Lecture Notes in Computer Science, pp.
803814, Berlin, Heidelberg. Springer.
Siddiqui, F. H., & Haslum, P. (2013a). Local search in the space of valid plans. In Proc.
of the 2013 ICAPS Workshop on Evolutionary Techniques in Planning and Scheduling, EVOPS 2013, Rome, Italy, June 10-14, 2013, pp. 2231. http://icaps13.icapsconference.org/wp-content/uploads/2013/05/evops13-proceedings.pdf.
Siddiqui, F. H., & Haslum, P. (2013b). Plan quality optimisation via block decomposition.
In Proc. of the 23rd International Joint Conference on Artificial Intelligence, IJCAI
2013, Beijing, China, August 3-9, 2013, pp. 23872393. AAAI Press.
Slivkins, A. (2014). Contextual bandits with similarity information. Journal of Machine
Learning Research, 15 (1), 25332568.
Stern, R. T., Puzis, R., & Felner, A. (2011). Potential search: A bounded-cost search
algorithm. In Proc. of the 21st International Conference on Automated Planning and
Scheduling, ICAPS 2011, Freiburg, Germany June 11-16, 2011, pp. 234241. AAAI
Press.
Thayer, J., Stern, R., Felner, A., & Ruml, W. (2012a). Faster bounded-cost search using
inadmissible heuristics. In Proc. of the 22nd International Conference on Automated
Planning and Scheduling, ICAPS 2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012,
pp. 270278. AAAI Press.
Thayer, J. T., Benton, J., & Helmert, M. (2012b). Better parameter-free anytime search
by minimizing time between solutions. In Proc. of the 5th International Symposium
on Combinatorial Search, SOCS 2012, Niagara Falls, Canada, July 19-21, 2012, pp.
120128. AAAI Press.
434

fiContinuing Plan Quality Optimisation

Thayer, J. T., & Ruml, W. (2011). Bounded suboptimal search: A direct approach using
inadmissible estimates. In Proc. of the 22nd International Joint Conference on Artificial Intelligence, IJCAI 2011, Barcelona, Catalonia, Spain, July 16-22, 2011, pp.
674679. AAAI Press.
Vallati, M., Fawcett, C., Gerevini, A., Hoos, H., & Saetti, A. (2011). ParLPG: Generating domain-specific planners through automatic parameter configuration in LPG. In
Proc. of the 7th International Planning Competition, IPC 2011, Deterministic Part.
http://www.plg.inf.uc3m.es/ipc2011-deterministic.
Veloso, M. M., Perez, A., & Carbonell, J. G. (1990). Nonlinear planning with parallel
resource allocation. In Proc. of the DARPA Workshop on Innovative Approaches to
Planning, Scheduling and Control, San Diego, California, November 5-8, 1990, pp.
207212. Morgan Kaufmann.
Wang, Y., Audibert, J., & Munos, R. (2008). Algorithms for infinitely many-armed bandits.
In Proc. of the 22nd Annual Conference on Neural Information Processing Systems,
NIPS 2008, Vancouver, British Columbia, Canada, December 8-11, 2008, pp. 1729
1736. Curran Associates, Inc.
Xie, F., Nakhost, H., & Muller, M. (2012). Planning via random walk-driven local search. In
Proc. of the 22nd International Conference on Automated Planning and Scheduling,
ICAPS 2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012, pp. 315322. AAAI Press.
Xie, F., Valenzano, R. A., & Muller, M. (2010). Better time constrained search via randomization and postprocessing. In Proc. of the 23rd International Conference on
Automated Planning and Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013,
pp. 269277. AAAI Press.
Young, H. P., & Levenglick, A. (1978). A consistent extension of condorcets election principle. SIAM Journal on Applied Mathematics, 35 (2), 285300.
Zhou, R., & Hansen, E. A. (2005). Beam-stack search: Integrating backtracking with beam
search. In Proc. of the 15th International Conference on Automated Planning and
Scheduling, ICAPS 2005, Monterey, California, USA, June 5-10, 2005, pp. 9098.
AAAI Press.

435

fiJournal of Artificial Intelligence Research 54 (2015) 233275

Submitted 06/15; published 10/15

Decision Making with Dynamic Uncertain Events
Meir Kalech

KALECH @ BGU . AC . IL

Department of Information Systems Engineering,
Ben-Gurion University of the Negev, Beer-Sheva, Israel

Shulamit Reches

SHULAMIT. RECHES @ GMAIL . COM

Department of Applied Mathematics,
Jerusalem College of Technology, Israel

Abstract
When to make a decision is a key question in decision making problems characterized by
uncertainty. In this paper we deal with decision making in environments where information arrives dynamically. We address the tradeoff between waiting and stopping strategies. On the one
hand, waiting to obtain more information reduces uncertainty, but it comes with a cost. Stopping
and making a decision based on an expected utility reduces the cost of waiting, but the decision
is based on uncertain information. We propose an optimal algorithm and two approximation algorithms. We prove that one approximation is optimistic - waits at least as long as the optimal
algorithm, while the other is pessimistic - stops not later than the optimal algorithm. We evaluate
our algorithms theoretically and empirically and show that the quality of the decision in both approximations is near-optimal and much faster than the optimal algorithm. Also, we can conclude
from the experiments that the cost function is a key factor to chose the most effective algorithm.

1. Introduction
There are many real-world domains in which an agent has to choose an alternative among multiple
candidates, based on their utility. The problem becomes more complicated when the utility depends
on events that occur dynamically and therefore the decision itself is based on dynamically changing
and uncertain information. In such domains, the question is whether to stop at a particular point
and make the best decision given the information available, or wait until more information arrives
to enable making a better decision. This problem is not trivial when there is a cost associated with
waiting.
For example, consider the problem of finding the best stock to buy in the stock market. The
values of the stocks may change over time due to future events, such as publication of the companys
sales report or a change in the interest rate, etc. The longer we wait, the more information becomes
available and, as a result, a decision can be made with more certainty. In many real-world domains,
there is a cost to waiting. For instance, in the above example, the cost of the stock reflects the loss
caused by not investing money in one of the candidate stocks. Thus, there is a tradeoff between a
waiting strategy that enables one to acquire more information and decreases the uncertainty and a
stopping strategy, which reduces the cost.
Another example relates to scheduling systems for meetings. Determining the best time for
a meeting could depend on many factors, such as the times of other meetings, location, and the
schedule of the attendees. Typically, these factors may change dynamically and influence a decision
on the best time for the meeting; obviously, the longer one waits, the more information becomes
c
2015
AI Access Foundation. All rights reserved.

fiK ALECH & R ECHES

available and the probability of choosing the best time for the meeting is higher. However, waiting
incurs a cost of the possibility that the chosen time slot might no longer be available. The goal of
this paper is to determine the best time to make a decision which maximizes the expected gain and
considers the cost of waiting.
This question, of whether to wait to get more information or not, is also raised in the context of
real estate investment. There are many unknown factors that may influence the decision of which
real estate property to buy. For example, an infrastructure development in the area (like a railway
station), raising/reducting municipal taxes in the area, construction of a polluting factory in the area,
etc. There is a question whether to choose a real estate property based on the expected gain or to
wait and get the information about the next factor but taking the risk that the properties prices may
increase.
The tradeoff between uncertainty and cost is related to the optimal stopping problem (Ferguson,
1989; Peskir & Shiryaev, 2006), the problem of decision making under bounded-resource (Horvitz,
2001, 2013), the problem of decision making with multiple informative but expensive observations
(Krause & Guestrin, 2009; Tolpin & Shimoni, 2010), the Max K-Armed Bandit problem (Cicirello
& Smith, 2005) and the ranking and selection problem (Powell & Ryzhov, 2012). Our work copes
with the challenge of the stopping problem where multiple alternatives are affected by uncertain
information that arrives dynamically. The decision whether to stop or wait, in our problem, depends
on the utility affected by the result of certain events that will occur in the next time stamps. Since
each alternative is affected by different events we should consider the combination of all possible
events, which makes our problem hard and different from others.
In this paper, we:
1. Develop a model for representing the arrival of dynamic information and its influence on the
utilities of the candidates.
2. Present an optimal exponential algorithm (OP T IM AL) that guarantees the best decision
tradeoff between certainty and waiting costs.
3. Propose two polynomial approximation algorithms to solve the problem and provide bounds
on their error. We prove that both algorithms evaluate the expected utility from stopping optimally. However, one approximation algorithm is optimistic (OP T IM IST IC) in
the sense that its waiting evaluation is overestimated. The other algorithm is pessimistic
(P ESSIM IST IC), namely its waiting evaluation is underestimated.
4. Empirically evaluate the optimal and the two approximation algorithms and illustrate the
advantages of each one of them.
We empirically evaluate the three algorithms by simulating a stock market scenario. We compare the optimal and approximation algorithms to four other baseline algorithms; one algorithm
makes the decision at the beginning of the decision process, the second algorithm decides after all
the information is obtained, the third algorithm makes the decision at a random time and the fourth
one makes the decision after half of the time steps. We examine the algorithms in terms of the
quality of the decision (utility) and runtime.
Our empirical evaluation shows that the cost function much influences on the quality of the
decision. As the cost function increases more moderately (i.e. a root function), the pessimistic
algorithm becomes less effective and the optimistic algorithm improves the quality decision and it
234

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

is even slightly better than the pessimistic one. However, for cost functions that grow linearly or
polynomially with the time the pessimistic approach is much better than the optimistic one, and in
most cases there is even no significant difference between the quality of the decision made by the
pessimistic approximation algorithm and the optimal algorithm. The quality of both approximations is much better than the baseline algorithms. The runtime of the approximation algorithms is
polynomial rather than an exponential runtime of the optimal algorithm.
This work is an extension of our previous work (Kalech & Pfeffer, 2010; Reches, Kalech, &
Stern, 2011). In this work we expand both the theoretical and empirical parts of the research.
In particular, in the theoretical aspect we find and prove the approximation error of the expected
gain and the expected wait of the algorithms, and prove the complexity and other properties of
the pessimistic and optimistic algorithms. We greatly expanded the evaluation by presenting the
influence of different parameters, such as the cost of waiting, and the distribution on the variables.
Furthermore, we empirically show the pessimistic and optimistic behaviour of the algorithms.
The paper is organized as follows. In the next section we present the basic fundamentals of the
problem and formally define it. In Section 3 we present the optimal algorithm. The optimistic approach is illustrated in Section 4 and the pessimistic approach in Section 5. An empirical evaluation
of the algorithms is provided in Section 6. In Section 7 we discuss related work and our conclusion
is presented in Section 8.

2. Model Description
To describe the model clearly we use an example inspired by the stock market. Assume a decision
maker wishes to choose one stock to purchase among three stocks (c1 , c2 , c3 ). The value of the
stocks is influenced by future events, such as the consumer price index (CPI), interest rates, etc.
The decision maker cannot evaluate the influence of the future events on the stocks with certainty
but only with some degree of probability. Obviously, the sooner the decision is taken, the lesser
the loss from not investing the money. On the other hand, the longer the waiting time, the more
information that can be gathered by knowing the outcome of the expected events and consequently
a decision with a greater degree of certainty can be made.
In our model, each decision will be designated by a candidate; throughout the paper we refer
to the candidate set C = {c1 , c2 , ..., cm }. An agent desires to choose an alternative from the set
C. A candidates utility is influenced by information that arrives dynamically. We represent the
dynamic information by random variables. A random variable is an event that occurs at a specific
time. The different outcomes of the event may influence the utility of the candidate in different
ways. As described extensively later, a candidate may be influenced by multiple events. Let us
define formally the timed variable.
Definition 1 (timed variable) A timed variable is a pair Xi , t, where Xi is a discrete, finite random variable taking values xi1 , ..., xik and t  T represents its time stamp, where T is a discrete
time horizon T = {0, ..., h} with horizon h.
Given a timed variable Xi , t, t is the time stamp in which the random variable Xi is assigned
one of its possible values xi1 , ..., xik .
For example, the timed variables in the stock market are future events that influence the utility of
the stocks. X1 , 1 may represent an expected decrease in the percentage of the interest rate at time
1, where time 1 represents one month from now. X1 takes the discrete values {x11 = 0.1, x12 =
235

fiK ALECH & R ECHES

0}. Another timed variable, X2 , 2, represents the expected prospectus of a specific company in
two months from now, which takes the values {x21 = positive, x22 = negative}. One more timed
variable X3 , 3, represents the expected change in the percentage of CPI in three months. X3 takes
the values {x31 = 0.1, x32 = 0.2}. In our example, X1 , 1 and X3 , 3 influence candidate c1 and
X2 , 2 influences candidate c2 . A timed variable may affect two or more candidates.
Definition 2 (assignment) An assignment to the timed variable X, t is an outcome xi of X. A
global assignment at time t, denoted  t , is an assignment of values to all timed variables whose
time stamp is less than or equal to t.
For example, the global assignment at time 3 ( 3 ) may be X1 = 0, X2 = positive, and
X3 = 0.1, i.e., at time 1 (after a month) the interest did not change, at time 2 (after two months)
the prospectus of the company was positive, and at time 3 (after three months) the CPI increased by
0.1%.
Each candidates utility depends on a set of timed variables, where different sets lead to different
utilities. We use a tree to represent the effect of the timed variables on the utility.
Definition 3 (candidate tree) A candidate tree cti for candidate ci is a tree. nj,i is a node in cti ,
where i stands for the index of the candidate and j for its index in the tree. The internal nodes are
associated with timed variables. The random variable corresponding to node n is denoted X(n)
and its time is denoted (n). If nj,i is a descendant of nk,i then (nk,i ) < (nj,i ). The edges going
out of node n represent the possible assignments of X(n). Each edge X = x is labeled by the
probability outcome denoted by p(X = x). A leaf n is labeled by its utility U(n), representing the
utility of the candidate affected by the assignments from the root to the leaf. CT represents the set
of candidate trees.
We call the assignments on the path that starts at the root of candidate tree cti and ends at
node nj,i at time t = (ni,j ) the local assignment of candidate ci  denoted by it . Note that
the candidate tree represents an estimate of the effect of the timed variables on the utility of the
candidates. Obviously, this estimate may change over time, due to the addition or removal of times
variables or re-estimation of the probabilities or the utilities. In this case the candidate trees should
be updated.

X3,t=3 0.4
n1,1

X1,t=1
n0,1

0.6

0.8

0.2

0.9

n2,1

n3,1

n5,1

80

55

X4,t=3 0.3
n1,2

X5,t=4
n4,1
0.8

0.1

Figure 1: Candidate tree ct1 .

X6,t=4
n4,2
0.6

n5,2

n3,2
75

65

0.7

0.4

0.2

n2,2

n6,1
60

X2,t=2
n0,2

40

n6,2
70

45

Figure 2: Candidate tree ct2 .

Figures 1 and 2 present two candidate trees built at time stamp t = 0. These are an extension of
the three timed variables demonstrated above. n0,1 is the root of candidate tree ct1 . Its time stamp
is t = 1, which represents the fact that the random variable X1 will obtain its outcomes at time 1.
236

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

The node n1,1 in ct1 represents the timed variable X3 , t = 3. One possible outcome of the change
in the CPI (X3 ) is 0.1% (left edge). The probability of this outcome is 0.8. An alternative outcome
of the change in the CPI is 0.2% (right edge) and the probability of this outcome is 0.2. The value
80 in the left leaf of ct1 represents the utility of candidate c1 when the local assignment at time 3
(13 ) is: the interest decreased by 0.1% (X1 = 0.1) and the CPI increased by 0.1% (X3 = 0.1).
The probability of this utility is 0.4  0.8 = 0.32.
The utility of a candidate is known certainly at the leaves. However, the expected utility of a
candidate can be calculated beforehand at any time (depth in the candidate tree) and will consider
the subtree from that depth. The expected utility can be trivially computed by a recursive function.
The expected utility of a leaf is its utility and for an internal node it is the expected utilities of its
children. Formally:
Definition 4 (expected utility) Given a node n  cti , the function EU(cti , n) returns the expected
utility of n:
{
U(n)
n is a leaf
EU(cti , n) = 
j p(X(n) = xj )EU(cti , nj ) otherwise
where nj represents the successor node of n via assignment X(n) = xj .
For instance, the expected utility of the root in Figure 1 is: EU(ct1 , n0,1 ) = 0.4  (0.8  80 +
0.2  55) + 0.6  (0.9  60 + 0.1  65) = 66.3.
The expected utility is an estimate of the real utility based on the information known at the
current time. Waiting for the next time reduces the uncertainty about the candidates utilities and
hence increases the probability of making a good decision. However, waiting incurs a cost. The cost
can be either a function of the assignments or a function of the time. For the sake of simplicity, in
this paper we represent the cost as a function of the time. In reality, the cost of waiting for a specific
time stamp is usually not higher than the utility gained at that time. Thus, to enforce a realistic cost,
we bound the cost by the maximum utility.
Definition 5 (cost) Given a time stamp t  T , CST (t) is an increasing function that returns the
approximated cost of waiting until time t.
The expected gain of a node is the difference between the expected utility of the node and the
cost of waiting until that node.
Definition 6 (expected gain) Given node n

cti , GN (cti , n)
=
EU (cti , n) 
CST ((parent(n))).1 If n is the root of the candidate tree then: GN (cti , n) = EU(cti , n).
The reason we reduce the cost of the parent of n, rather than of n is that EU(cti , n) represents
the expected gain of the subtree rooted in n without waiting for the outcomes of X(n).
There is a tradeoff between the first component of GN , the expected utility, and the second
component, the waiting cost. The objective of this paper is to present an algorithm that will find the
1. Since the utility and cost are not necessarily given in the same scale they should be normalized before reduction. The
normalization is domain dependent.

237

fiK ALECH & R ECHES

time which maximizes the gain2 . Unfortunately, we are unable to separate the computation of the
optimal time to make the decision and the selection of the best candidate, since the utilities of the
candidates depend on future events. Therefore, we define a policy to determine what to do in all
situations that the decision maker might face.
Beside the outcomes of the timed variables, the cost function also influences the decision of
whether to stop or to wait. A cost function that returns much smaller values than the difference
between the expected utilities through time will eventually lead to a wait decision. On the other
hand, a cost function that returns values which are too high will eventually lead to a stop decision. For the examples in this paper we use the cost function CST (t) = t. This function ensures
proportional cost values (0,1,2,3,4) in relation to the utility values (4580). For instance, according
to the estimate at time stamp t = 0, given the global assignment: X1 = 0.1, X2 = positive,
the expected gain of candidate c1 is GN (ct1 , n1,1 ) = 75  1 = 74 and the expected gain of c2 is
GN (c2 , n1,2 ) = 68  2 = 66. As a result, if the decision maker decides to stop it will choose c1 ,
otherwise it will wait for the next time stamp. The policy dictates the decision on whether to stop
or wait.
Definition 7 (policy) A policy is a function  :   {stop, wait}, where  is the set of all global
assignments.
If the policy specifies to stop, the decision maker chooses the candidate with the current highest
expected gain.
Finally, we define the expected gain for the decision maker by using policy , referred as global
expected gain. To understand this definition we first introduce another definition of the nodes
corresponding to a certain time.
Definition 8 (N ODEStj ) The set N ODEStj represents the following nodes of ctj : (1) leaves
whose parents time is less than or equal to t: {n  ctj |(parent(n))  t, n is a leaf}. (2)
internal nodes whose parents time is less or equal to t and their time is greater than t {n 
ctj |(parent(n))  t  (n) > t, n is an internal node}.
For example, in Figure 1 N ODES31
=
{n2,1 , n3,1 , n4,1 }, N ODES41
=
{n2,1 , n3,1 , n5,1 , n6,1 }.
The global expected gain function obtains the candidate trees, the global assignment, and a
policy. If the policy specifies to stop, then the global expected gain is the maximum expected gain
among the candidates. Otherwise, it recursively computes the expectation of the global expected
gain of the different combinations of the roots children in the next time stamp. Formally:
Definition 9 (global expected gain) A global expected gain is a function that returns the expected
gain from choosing policy :
GEG(CT,  t , ) =


GN (ctj , nj )
if ( t ) = stop
 max
j

GEG(CT y ,  ty , )P r(y) if ( t ) = wait

 y{N ODES 1 ,...,N ODES m }
t+1

t+1

where
2. Although in the decision making theory it is common to maximize the expected utility, here we use the term expected
gain, which incorporates the cost in order to distinguish it from the expected utility.

238

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

1. nj is the root of ctj .
2. CT y is the set of candidate trees rooted by the nodes in y.
3. P r(y) is the probability of the nodes in y given the assignment  t .
4.  ty represents the union of  t and the assignments of X(nj ) which are represented by y.
In the running example, for the global assignment  t=2 and a policy ( t=2 ) = stop and
y = (n1,1 , n1,2 ), GEG(CT y ,  t , ) = max(73, 66) = 73. In case of a policy ( t ) = wait,
we sum GEG for all possible assignments of the next time. The set of N ODES rooted by the
nodes n1,1 and n1,2 at time t = 3 are N ODES31 = {n2,1 , n3,1 } and N ODES32 = {n2,2 , n3,2 },
correspondingly, and y  {n2,1 , n3,1 }  {n2,2 , n3,2 }. Therefore:
GEG(CT {n1,1 ,n1,2 } ,  t , ) =
GEG(CT {n2,1 ,n2,2 } ,  t , )  0.8  0.8+
GEG(CT {n2,1 ,n3,2 } ,  t , )  0.8  0.2+
GEG(CT {n3,1 ,n2,2 } ,  t , )  0.2  0.8+
GEG(CT {n3,1 ,n3,2 } ,  t , )  0.2  0.2
Based on the above definitions, we can define the timed decision making problem (TDM):
Definition 10 (Timed Decision Making (TDM) problem) Given a set of candidate trees CT , the
TDM problem is to find a policy that maximizes GEG(CT,  0 , ).
Table 1 summarizes the notation we use in describing the model.
The hardness of the TDM problem is due to the computation of the expected gain from waiting for the next time stamp. This computation needs to take into consideration the utility of each
candidate for each possible assignment and for each time stamp. Specifically, at time t0 , the decision maker should decide whether to stop and choose the best candidate or to wait for the next
time stamp by comparing the expected gain from stopping and the expected gain from waiting. The
expected gain from stopping at time stamp t0 can be computed immediately by taking the maximum
of the expected gain of the candidate trees (maxGN (ctj , nj ), where n1 , ..., nm are the roots of the
j

candidate trees). On the other hand, the computation of the expected gain from waiting for the next
time stamp is hard. The expected wait considers the combination of the possible assignments of
the candidate trees at time t1 . For each such combination, we need to take into consideration the
utility of stopping at time t1 and the utility of waiting for time stamp t2 which include additional
combinations of assignments and so on. The problem is hard because the number of combinations
is exponential in the number of candidates. Specifically, at a certain time stamp, for k possible
assignments of different variables in m different candidate trees, the number of combinations is k m .
In order to prove that TDM is NP-hard, we present the timed decision making problem as a
decision problem . Given CT,  0 and a non-negative integer K. Answer yes if there exist a
policy  such that the global expected gain GEG(CT,  0 , )  K.
Theorem 1: TDM problem is NP-hard. (The proof appears in Appendix A).
One option for representing our problem would be to use Markov Decision Processes (MDP). In
such a model, the states at time t would be global assignments at time t ( t ) and the actions would
be either to select the best candidate at that time (stop) or wait one more time step. The transition
239

fiK ALECH & R ECHES

Parameter
C = {c1 , ..., cm }
Xi , t
cti
CT
t
it
nj,i
X(n)
(n)
T = (0, ..., h)
U (nij )
EU(cti , n)
CST (t)
GN (cti , n)
A policy
N ODEStj
GEG
EW( t , )
ES( t , )
PT H(cti , n)
P rPT H(cti , n)
i

Description
A set of m candidates.
A timed variable, where Xi is a discrete, finite random variable and t is its
time stamp.
A candidate tree of ci .
A set of candidate trees.
An assignment of values to all timed variables whose time stamp is less
than or equal to t.
A local assignment. The assignments on the path that stats at the root of
candidate cti and end at node whose time is t t.
A node in the candidate tree cti of candidate ci .
The random variable corresponding to node n.
The time of the random variable corresponding to node n.
The time horizon.
The utility of candidate cti affected by the assignments from its root to the
leaf nij .
The expected utility of candidate tree cti from node n.
The approximated cost of waiting until time t.
The expected gain of node n in candidate tree cti is the difference between
the expected utility of the node and the cost of waiting until that node.
A function  :   {stop, wait}.
A set of nodes whose parents time is less than or equal to t and they are
leaves, or their time is greater then t.
Global expected gain.
The expected gain from waiting for the next time stamp (t+1), using policy
.
The expected gain from stopping at time stamp t, using policy .
A path, set of local assignments from the root of cti to node n.
The probability of a path.
Local policy of candidate ci .
Table 1: The notation used for the description of the model.

240

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

function from a time t state to a time t + 1 state for a wait action would be given by the product of
the probabilities of time t + 1 assignments. A stop action leads to a terminal state in which a reward
is received equal to the gain of the winning candidate.
The usual advantage of an MDP formulation is the possibility of using dynamic programming
methods, such as value and policy iteration. However, in our problem, dynamic programming
provides no benefits because the same state cannot be reached by different paths and the number of
states is exponential in the total number of timed variables in all the trees.
One of the methods which addresses large MDPs is by means of factored MDPs (Boutilier,
Dearden, & Goldszmidt, 2000; Guestrin, Koller, Parr, & Venkataraman, 2003). This approach is
not viable in our domain because the utility of stopping is a maximization over the utilities in all the
trees which depends on all the timed variables. As we show in Section 4, there is a special structure
in our problem that is not readily apparent in the MDP or a factored MDP formulation.

3. Optimal Algorithm
The optimal gain can be calculated straightforwardly by a decision tree approach. The optimal
decision tree merges the candidate trees into a single decision tree whose depth is the maximal time
of the timed variables in the candidate trees. In the decision tree, we define three kinds of nodes:
Decision nodes, in which the decision maker must decide whether to stop or to wait; if the decision
is to stop, then which candidate to choose.
Stop nodes, where the decision maker has stopped and chosen one of the candidates.
Wait nodes, where the decision maker has decided to wait.
Each node is marked with a time stamp. Edges leading out of the wait nodes are labeled by conjunctions of assignments. Every node in the tree is marked by a set of assignments, which are the
assignments on the path leading to the node.
The tree is constructed at offline time as follows:
Procedure 1 Optimal:
1. The root is a decision node with time stamp 0.
2. For each time stamp, the children of decision nodes with time stamp t are the wait nodes with
time stamp t; for each candidate there is a stop node child. If t is the final time stamp, no wait
node child is included.
3. Stop nodes are leaves of the tree. If the stop node corresponds to node n  cti at time t, the
value of the node is GN (cti , n) (Definition 6).
4. The children of a wait node with time stamp t given a global assignment  t are determined
as follows:
(a) The local assignment it passes through a path of assignments ending at a node. Let us
denote this node by ni for candidate tree cti .

(b) Let Xn = i X(ni ).
241

fiK ALECH & R ECHES

Decision Time Horizon
66.84

0
66.84

66.3

58.9
0.4

0.6

73.8

57.7

73.8
8

59.3

73.26

0.3

65.6
6 0.64

76.4

76.4 71.4
4
4

76.4 36.4
4
4

75.2

75.2

70.2

75.2
2
1

70.2

75.2

75.2

35.2

51.4 36.4
4
4

50.2

70.2

50.2
2
1
50.2

50.2

35.2

0.7

0.8

72.36
66
0.2

58.1

68.5

65.6
6
0.8

56.2

75.2 75.2

65.2

50.2

56.9

71.4 70.2
4
2
0.9 0.1
70.2

70.2

52.6
6

58.1

59.5
1

0.2

71.4

76.4 51.4 75.2 51.4 51.4 56.2
2
4
4
2
4
4
0.4 0.6
0.4 0.6

2

59.5

68.5

76.4

51.4

51.4 71.4 70.2
4
4
2
1

52.6
6

0.04

71.4

76.4

75.2
2
1

0.16

72.6
6

62.24
0.3

72.6

74.6
0.16

57.7

0.7

74.6

72.6
6

1

62.2

56.9

56.9

36.4 55.7
4
0.9 0.1
55.2

59.5

57.9

60.2

52.4
4

3

59.5

0.36 0.54 0.04 0.06
65.2

55.2

65.2 60.2

4

40.2
75.2 65.2 75.2 40.2 50.2 65.2 50.2 40.2 55.2 70.2 60.2 70.2 55.2 35.2 60.2 35.2 55.2 65.2 55.2 40.2 60.2 65.2 60.2

Figure 3: Optimal decision tree for ct1 and ct2 candidate trees.
(c) For each possible joint outcome of the timed variables in Xn , the wait node has a child,
labeled by the joint probability. The child is a decision node with time stamp t + 1.
Once the tree has been constructed, it can be evaluated using a simple bottom-up process. The
gains at the leaves, i.e., the stop nodes, have already been calculated. The gain of a wait node is
the expectation of the utilities of its children. The gain of a decision node is the maximum gain
of its children and the optimal decision is the one that leads to a maximal gain. The decision tree
is generated and evaluated in advance before any assignments have been undertaken. Its solution
represents a policy (Definition 7).
Figure 3 presents the optimal decision tree for a decision problem with candidate trees ct1
(Figure 1) and ct2 (Figure 2). The time line on the right of the graph represents the time horizon
of the decision. The rectangular nodes represent decision nodes; the shaded ellipse nodes represent
wait nodes and the empty ellipse nodes represent stop nodes3 . The stop nodes come in pairs, one
for each candidate; the node for c1 is on the left. The numbers in the nodes represent the expected
gains and are computed using the bottom-up algorithm. In the example, we used a cost function
that linearly grows in time: CST (t) = 1.2t, so the cost of reaching a leaf is 4.8 (since there are four
time stamps).
For example, consider the dashed triangle on the right-hand side of the figure. The root of the
subtree shown in this triangle is a wait node with time stamp 3. In determining the children of this
node, we consider all timed variables in the candidate trees with a time stamp of 4. There are two
such timed variables, X5 , 4 for candidate c1 and X6 , 4 for candidate c2 . We need to split all
the joint outcomes of these two candidates so that the wait node has four children. Each of these
children is a decision node with time stamp 4. Since this is the last time stamp, these decision nodes
only have stop nodes as their children.
One particular course of events is shown in bold in the figure. Since the expected gain of waiting
(66.84) is higher than the expected gain from stopping, which is the expected utility from choosing
3. The stop nodes at the final time do not have ellipses for readability. We have also omitted assignment labels on the
edges.

242

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

the best stop node (66.3), the agent will wait. Accordingly, the assignment X1 = 0 (left) will then
occur. At the next decision node child, the expected gain from stopping and choosing candidate
c1 (73.88) is higher than the expected gain of waiting (73.26), so the agent may stop at t = 1 and
choose c1 . Assume the sequence of assignments occurred as depicted in bold in the figure. This
eventually will lead to a leaf node with a gain of 75.2 as shown. Note, however, that this gain
incorporates the cost of waiting 4 time stamps. Thus, if the agent had been omniscient and known
the outcomes of the timed variables in advance, it would have obtained a gain of 80, since it would
not have had to wait at all. For a rational agent which stops at time 1, the gain for choosing c1 is
80  1.2 = 78.8.
Denote   as the optimal policy. Given a global assignment  t , we use EW( t ,   ) to represent
the expected gain from waiting for the next time stamp by executing the optimal algorithm. It equals
the wait node given  t . Similarly, we use ES( t ,   ) to represent the expected gain from stopping
at time stamp t by executing the optimal algorithm; this equals the maximal stop node given  t . For
example, in Figure 3, EW( 0 ,   ) = 66.84 and ES( 0 ,   ) = 66.3.
The optimal decision tree explicitly represents the state space in an MDP model, as described
in Section 2. Since the state space is represented in a tree (rather than a graph), an intelligent value
iteration process is equivalent to the backward induction algorithm we use for the optimal decision
tree.
3.1 Analysis
The time complexity of the optimal algorithm is affected by the fact that the optimal decision tree
considers the different combinations of paths of the candidate trees. Let the maximum size of a
candidate tree be M and the number of candidates be m. Notice that the size M of a candidate tree
is exponential in the depth of the local candidate tree. Specifically, given the depth of the candidate
tree, h (the horizon), and the number of outcomes for each timed variable, k (the branching factor)
then M = k h . The total number of timed variables is M m and the size of the depth of the optimal
decision tree is bounded by m log(M ). The worst-case time complexity of computing the optimal
tree is O(M m ).
As mentioned above, the backward induction on the optimal decision tree is equivalent to the
value iteration for MDPs. Since the state space is represented in a tree, the value iteration simply
scans the state space. Thus, the complexity of a value iteration is the size of the state space. The
complexity of a state space, as described in Section 2, is the sum of the global assignment alternatives at each time. In the worst case, where every candidate depends on different timed variables at
m
m
each time, there are k m alternatives at time 1, (k 2 ) alternatives at time 2, and (k h ) alternatives
at time h. This complexity is identical to the complexity of optimal decision tree O(M m ) (since
M  O(k h )).
The best-case complexity is archived when all candidates are affected by the same timed variable, since we should not consider the combinations between the same timed variables. In this case
the trees of the candidates are identical except for the utilities in the leaves and the complexity is
thus mM .
Beyond the exponential complexity of the optimal algorithm, another disadvantage of this algorithm stems from the fact that every change in the candidate trees demands rebuilding the decision
tree. Unfortunately, due to its exponential complexity rebuilding is not feasible. To cope with the
exponential complexity of the optimal algorithm and the fact that it is not feasible to rebuild the de243

fiK ALECH & R ECHES

cision tree when timed variables change, we propose two approximation algorithms in the following
sections.
Beyond the exponential complexity of the optimal algorithm, another disadvantage of this algorithm stems from the fact that every change in the candidate trees demands rebuilding the decision
tree. The optimal algorithm computes the whole combinations of the future events in advance and
as a result, it makes an optimal solution as long as the initial evaluation about the probabilities and
utilities of each event is valid. Since the complexity of the optimal algorithm is exponential, it may
be infeasible to rebuild a new decision tree for each time stamp. However, in realistic scenarios the
evaluation about the events and the utilities may change over time. To cope with the exponential
complexity of the optimal algorithm and the fact that it is not feasible to rebuild the decision tree
when timed variables change, in the following sections we propose two approximation polynomial
algorithms.

4. An Optimistic Approach
The optimal algorithm presented in Section 3 considers all candidates simultaneously and thus
grows exponentially in the number of candidates. In this section, we present an alternative algorithmic framework that considers the candidates separately and dynamically. This alternative viewpoint
will lead to a more efficient approximation algorithm.
4.1 OPTIMISTIC Algorithm
The main idea behind the alternative framework is calculating the utility of each candidate tree
separately and then combining the utilities together to obtain an evaluation of the global gain. In
this way we avoid the complexity of comparing each assignment to all the assignments of the other
candidates; each candidate contributes separately to the overall utility. Specifically, a candidate
contributes to the overall utility only when it actually prevails over all the other candidates. Thus
we can estimate the utility from a node in a candidate tree by the product of its expected gain and
the probability that the candidate will win, given that the node is reached. Then, in order to estimate
the overall gain, we sum over this utility for all the candidates. To formally describe the algorithm
we present the following definitions:
Definition 11 (path) Given a node n  cti , the function PT H(cti , n) returns a set of local assignments from the root to n {Xi1 = xi1 , Xi2 = xi2 , ...} in candidate tree cti .
Definition
12 (probability of path) Given

jPT H(cti ,n) P r(j).

a

node

n



cti ,

P rPT H(cti , n)

=

The probability that ci will prevail over a specific candidate cj at time t is the sum of its probabilities to prevail over cj for each possible assignment, i.e., for each node in N ODEStj . The
probability that a candidate ci will prevail over the other candidates, given a specific node nx,i  cti
and at a specific time stamp, t = (nx,i ), is the sum of the probabilities that it will prevail over each
candidate in the current time t. Formally4 :
4. For the mathematical calculations of the probabilities in the approximation algorithms, we assume that the candidates
have disjointed sets of timed variables that are probabilistically independent; which means that two candidates are not
affected by the same time variable. Nevertheless, as will be shown by the results of the experiments, the algorithms
perform well even when there are common variables.

244

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

Definition 13 (probability of winning) Given a node nx,i  cti , the probability of ci to win is5 :
P r(ciwins|nx,i ) =

IsW in(EU(cti , nx,i ), EU (ctj , ny,j ))P rPT H(ctj , ny,j )

j{1,...,m},i=j ny,j N ODES j
t

where
1. t = (parent(nx,i )).
2. IsW in(EU (cti , nx,i ), EU(ctj , ny,j )) =
{

1 if EU(cti , nx,i ) > EU(ctj , ny,j )
0 else

For example, using Figures 1 and 2 above, recall N ODES31 = {n2,1 , n3,1 , n4,1 }.
P r(c2 wins|n2,2 ) =
IsW in(EU (ct2 , n2,2 ), EU(ct1 , n2,1 ))  0.4  0.8+
IsW in(EU (ct2 , n2,2 ), EU(ct1 , n3,1 ))  0.4  0.2+
IsW in(EU (ct2 , n2,2 ), EU(ct1 , n4,1 ))  0.6 =
0  0.32 + 1  0.08 + 1  0.6 = 0.68
We define the relative expected gain of each candidate as its contribution to the global expected gain given a specific node.
Definition 14 (relative expected gain) Given a node nx,i  cti , the relative expected gain of candidate ci is GN (cti , nx,i )  P r(ci wins|nx,i ).
Notice that the probability of candidate ci to win at a time stamp t is:

P r(ci wins) =
P r(ci wins|nx,i )  P r(nx,i )
nx,i N ODESti

and thus according to the law of total probability:


P r(ci wins|nx,i )  P r(nx,i ) = 1
i nx,i N ODESti

The computation of the relative expected gain of nx,i is presented in Algorithm 1. In line 3
we go over the candidate trees except for candidate tree cti . In line 5 we go over the candidate
trees nodes that have the time as the same as nx,i s time. We then sum over the probabilities of
those nodes whose expected utility is less than that of nx,i (lines 68). This sum represents the
probability of ci to win cj , given the node nx,i . Finally, in line 10 we multiply the probability that ci
will prevail over all the candidates (given the node nx,i ), since a winning candidate should prevail
over all the candidates. We return the product of this probability and the expected gain of the node
5. Ties between candidates are broken in a consistent manner.

245

fiK ALECH & R ECHES

Algorithm 1 RELATIVE EXPECTED GAIN
(input: candidate trees CT = {ct1 , ..., ctm }
input: node nx,i
output: relative expected gain of nx,i )

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

t  (nx,i )
prob  1
for all ctj  CT (i = j) do
temp  0
for all ny,j  N ODEStj do
if EU (cti , nx,i ) > EU (ctj , ny,j ) then
temp  temp + P rPT H(ctj , ny,j )
end if
end for
prob  prob  temp
end for
return prob  GN (cti , nx,i )

nx,i . For instance, the relative expected gain from choosing candidate c2 given the node n2,2 in time
3 (with a cost function CST (t) = 1.2t) is (75  3.6)  0.68 = 48.552.
The optimistic approach determines the policy (Definition 7) by constructing a separate decision
tree for each candidate. The policy then is determined based on the local assignment for each time.
As mentioned earlier the local assignment for a certain candidate tree is derived from the global
assignment. The estimate of the global expected gain from stopping and from waiting depends on
the policy and the expected gain of each candidate separately. We call such a policy local policy.
Obviously, it may be possible that at a certain time the local policies over the candidates will be
different.
Definition 15 (local policy) A local policyi for candidate ci is a rule that dictates either stopping
or waiting for a local assignment it .
For each assignment at time t, the optimistic decision maker decides on its policy by building
individual decision trees based on the relative expected gain for each candidate. The expected gain
from stopping at each time is the sum of the relative expected gain of the candidates. The optimistic
procedure is invoked first with time 0:
Procedure 2 Optimistic:
1. Generate an individual candidate decision tree for each candidate ci based on cti in a manner
similar to the optimal decision tree except that the relative expected gain is used instead of
the expected gain.
2. Denote the stop node of the current time stamp t for each candidate tree cti by ES i ( t , i )
and the wait node of the current time stamp t for each candidate tree cti by EW i ( t , i ).
Denote also:

ES( t , ) =
ES i ( t , i ) and
i{1,...,m}

EW( t , ) =
EW i ( t , i ).
i{1,...,m}

246

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

3. If ES( t , )  EW( t , )
then  = stop, return argmax ES i ( t , i )
i{1,...,m}

else  = wait.
4. Prune each candidate tree according to the global assignment such that the tree will be rooted
by the node reached by the local assignment. Invoke Procedure 2 in the next time stamp t + 1.
If we consider the stop values at the root, exactly one candidate will have a probability of winning of 1 and all others will have a probability of 0. Therefore, the expected gain of the winning
candidate equals the sum of the values of the stop nodes in the root of all the decision trees. Intuitively, the value of a wait node for a candidate is an estimate of the candidates contribution to
the benefit of waiting. Therefore, the algorithm evaluates the expected utility from waiting by summing up the expected wait of the candidates. If the summation value is greater than the maximum
immediate expected gain, then the total expected gain of waiting is greater than the expected gain
of stopping. In this case the decision will be to wait. Otherwise, the decision will be to stop and to
choose the candidate with the highest expected gain.
If the agent decides to wait then the decision trees must be updated according to the new assignments obtained after waiting. The new assignments prune some parts of the candidate trees.
For instance, consider candidate tree ct1 in Figure 1. Assume that the agent decided to wait for the
outcome of the variables at time 1. Assume the assignment of the timed variable X1 at time 1 is
the left, then the right subtree of ct1 can be pruned since it no longer influences the utility of ct1 .
The N ODESt1 set, which is associated with a specific time, changes as a result of this pruning
as well as the computation of the relative expected gain. Therefore rebuilding the decision trees is
necessary.
Figures 4 and 5 present the decision tree of ct1 and ct2 , respectively (Figures 1 and 2). The leaves
contain two numbers; the first represents the expected utility while the second (in bold) represents
the relative expected gain.
For example, let us compute the relative expected gain of the rightmost bottom-level node n6,1
in Figure 1. The cost function is CST (t) = 1.2  t. This node, with a utility of 65, is greater than
the two nodes in ct2 n3,2 with a utility of 40 and n6,2 with a utility of 45. The total probability of
the two nodes in ct2 to be defeated by n6,1 is 0.3  0.2 + 0.7  0.6 = 0.48. This is the probability
that c1 will defeat c2 given its utility of 65. To compute the relative expected gain of this node
(see Definition 14), we multiply the gain, which is 65  4.8 = 60.2, by the probability of winning,
resulting in 28.9 (see the rightmost bottom-level node in Figure 4).
Based on the decision trees in Figures 4 and 5 we find that the Optimistic algorithm decision
at time 0 is to wait, since the sum of the wait nodes in t = 0 (89.3) is greater than the sum of the
stop nodes (66). Suppose that timed variables X1 , 1 have been assigned by the left outcome.(as
in the example of the optimal algorithm). The decision trees are now updated. Candidate c1 s tree
is pruned so that it only includes the subtree rooted at n1,1 and as a result the relatives expected
gain of candidate trees c1 and c2 being updated. Figures 6 and 7 present the obtained trees by the
Optimistic algorithm at this case. According to this trees, the algorithm at time stamp t = 0, decides
again to wait, since the sum of the wait nodes (84), is higher than the sum of the stop nodes(73.8).
At the next decision node child, the expected gain from stopping and choosing candidate c1 (73.88)
is higher than the expected gain of waiting (73.2), so the agent may stop at t = 1 and choose c1 .
247

fiK ALECH & R ECHES

Decision Time Horizon
0

24.2
24.2

(58.9) 0

1

1
24.2

(58.9) 0
0.3

24.2
0.7

39.4

38.8

(68) 39.4
0.8

(55) 0

1
0

47.7 (40) 0

0

47.7

0.4
0

44.3

(40) 0

(70) 44.3

0.6
0

4

(45) 0

Figure 5: Candidate decision tree for ct2 (built at
t = 0).

Decision Time Horizon
1

11.4

(58.9) 0
0.3

11.4
0.7

13.4

10.5

11.6

(68) 13.4
0.8

(75) 14.5

(55) 0

1
0

10.5

14.3 (40) 0
1
14.3

(75) 57.1

2
10.5

0.2

14.5

Figure 6: Candidate decision tree for ct1 (rebuilt
at t = 1).

17.7

(55) 4.1

1

(75) 47.7

3

17.7

1

Figure 4: Candidate decision tree for ct1 (built at
t = 0).

17.7

0.2

48.5

(75) 48.5

2

17.7

0
1

3

5.3

(55) 10.5
0.4

0

13.3

(40) 0

(70) 13.3

0.6
0

4

(45) 0

Figure 7: Candidate decision tree for ct2 (rebuilt
at t = 1).

4.2 Analysis
The time complexity of the optimistic approximation is only polynomial in the number of candidates
since we build a decision tree for every candidate separately.
248

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

Theorem 1 The time complexity of building the decision trees in the optimistic approximation is
O(M 2 m2 ), where m is the number of candidates and M is the maximal size among the candidate
trees.
Proof: When evaluating each candidate tree, we must compute the probability of winning at
O(M ) nodes. For each such node, we perform a summation over O(M ) nodes in each of the other
candidate trees and its cost O(M 2 m). We should perform this for all m candidate trees. Thus, the
total cost of the algorithm is O(M 2 m2 ). 2
This result compares favorably to O(M m ) for the optimal algorithm if the number of candidates
is large. In addition, due to its polynomial complexity and due to the fact that the optimistic algorithm rebuilds the decision trees to update the probabilities, it can easily update the decision tree
by new dynamic events or by updated probabilities and utilities. For instance, assume that at time
t = 0 the prediction is that the interest rate will increase at time t = 2 in 0.1% with a probability of
0.8 and in 0.2% with a probability of 0.2. At time t = 1 this prediction may change, for instance, to
0.15% with a probability of 0.6 and 0.2% with a probability of 0.4. Since the optimistic algorithm
rebuilds in polynomial time the decision trees it can easily consider the updated probabilities and
values.
We will show now that if Procedure 2 returns a stopping policy, an optimal algorithm would
decide the same. When the algorithm waits, it implies that the expected gain from waiting is greater
than the expected gain from stopping. In this case, the optimal waiting expectation could be lower.
Theorem 2 Given a global assignment  t , if  is a policy obtained by Procedure 2 and   is an
optimal policy, then ES( t , ) = ES( t ,   ) and EW( t , )  EW( t ,   ).
Proof: First we prove that ES( t , ) = ES( t ,   ). Procedure 2 calculates ES( t , ) at time
stamp
the stop nodes nx,i of all the candidate trees cti at this time: ES( t , ) =
 t by summing
i t
ES ( , i ). These nodes are the relative expected gain from stopping at this time and,
i{1,...,m}

according to definition 14, are GN (cti , nx,i )P r(ci wins|nx,i ). Since the global assignment of this
time is known, exactly one candidate ci (the candidate with the highest expected gain at time t)
confirms P r(ci wins|nx,i ) = 1 and all others confirm P r(cj wins|nx,j  ctj ) = 0. As a result,
ES( t , ) = GN (cti , nx,i ), where ci is the candidate with the highest expected gain at time t. Thus,
ES( t , ) = ES( t ,   ).
t ,   ). The optimistic approach estimates the expected
We now prove that EW( t , )
  EW(
i t
t
waiting EW( , ) by the sum i EW ( , i ). Since for every decision tree of cti and for any
global assignment  t the optimistic approach chooses the policy i that maximizes EW i ( t , i )
(looks for the optimal time to stop for each local assignment and takes the combination
 of the utilities) independently of the other candidate trees, there is a possibility that the sum i EW i ( t , i )
includes a relative expected gain of one candidate from stopping at a specific time stamp i and a
relative expected gain of another candidate from waiting till time stamp i + 1 for the same global
assignment. Since each relative expected gain is optimal, then EW( t , )  EW( t ,   ).2
Corollary 1 Based on the last theorem, given a policy obtained by Procedure 2, if ( t ) = stop,
an optimal policy would decide the same. This is because when an optimistic policy  decides to
stop, ES( t , ) > EW( t , ). Then, based on the last theorem, EW( t , )  EW( t ,   ) and
ES( t , ) = ES( t ,   ), thus ES( t ,   ) > EW( t ,   ), namely an optimal policy will declare a
249

fiK ALECH & R ECHES

stopping policy too. Therefore, the optimistic approach guarantees the optimal expected gain from
stopping.
We now prove the approximation error of the expected wait. Notice that the following theorem
does not discuss the error of the optimistic algorithm but focuses on the worst case error when
estimating the waiting gain by the optimistic algorithm.
Theorem 3 Given a time horizon T = (0, ..., h), if  is a policy obtained by Procedure 2 and  
f 1
is a policy obtained by the optimal algorithm, EW( 0 , )  EW( 0 ,   )  i=1
EU(cti , ni ) +
CST (h), where ni is the root of candidate tree cti , EU (cti , ni ) is the expected utility of the node
nij in candidate tree cti and f = M in(m, h) (m is the number of candidates).

Proof: According to the optimistic approach, EW( t , ) = i EW i ( t , i ). Since the expected
wait of each candidate EW i ( t , i ) is computed independently, the global expected wait EW( t , )
may include, for a specific assignment, the stopping gain of one candidate and the waiting gain of
another candidate simultaneously (even though this combination is impossible).
The worst case scenario, whereby EW( t , ) has the highest value occurs when for each time
stamp, exactly one local policy i is i ( t ) = stop. In this situation, the expected wait is the sum
of the expected stop in different time stamps of f candidates, where f = M in(m, h). Thus, (1)

EW( t , )  fi=1 EU(cti , ni ), where ni is the root of candidate tree cti . Now, EW( t ,   ) 
EU(ctj , nj )  CST (h) when EU (ctj , nj ) is the highest expected utility among the roots of the
candidate trees. Thus, (2) EW( t ,   )  EU(cti , nj ) + CST (h) and as a result, by summing
 1
(1) and (2), EW( t , )  EW( t ,   )  fi=1
EU(cti , ni ) + CST (h). In particular, for t = 0:

1
EW( 0 , )  EW( 0 ,   )  fi=1
EU (cti , ni ) + CST (h). 2
Finally, we prove the approximation error of the global expected gain. It is actually the cost of
waiting till level lh1 , when lh is the last level.
Theorem 4 Given time horizon T = (0, ..., h), a policy  obtained by Procedure 2,a cost function
CST (t), a set of candidate trees CT , a global assignment  t and an optimal policy   , the global
expected gain GEG holds: GEG(CT,  0 ,   )  GEG(CT,  0 , )  CST (h  1).
Proof: According to Corollary 1, Procedure 2 guarantees an optimal policy for ( t ) = stop.
As a result, an error can be obtained only when ( t ) = wait and   ( t ) = stop. Since
waiting for the next time stamp decreases uncertainty, the error is only the cost of waiting. In
the worst case scenario, Procedure 2 may wait until the last time stamp while an optimal policy
would stop immediately. However, the policy obtained by Procedure 2 at time h  1 is optimal,
GEG(CT,  h1 , ) = GEG(CT,  h1 ,   ). The reason behind this is that because the value of
EW( h1 , ) considers only local policies i ( h1 ) = wait, the estimated expected wait from
this policy, which is the sum of the local expected wait, is optimal, since it does not include the
relative gain of waiting and stopping for the same assignment.
As a result, the absolute approximation error is GEG(CT,  0 ,   )  GEG(CT s,  0 , ) <
CST (h  1).2

5. A Pessimistic Approach
In this section we present an alternative approximation algorithm which, in contrast to the former,
presents a pessimistic approach. This algorithm considers the expected utility of each time stamp
250

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

separately. As a result, we avoid an exponential complexity of the optimal algorithm which considers all the possible combinations between waiting and stopping for each time stamp.
5.1 PESSIMISTIC Algorithm
The approximation gain can be calculated by a united decision tree. In this approach we merge
candidate utility functions into a single decision tree, where each level in the tree represents a time
stamp associated with a timed variable, i.e., level li in the tree represents a time point ti where we
decide whether to stop or wait. In the decision tree there are two nodes on each level:
Stop node, where the decision maker stops and chooses one of the candidates. A stop node, ES i ,
is the expected utility of stopping at level li .
Wait node, where the decision maker decides to wait. The wait node, EW i , is the expected utility
of waiting for the next time level. This is the maximum between the stop node and the wait
node of level li+1 .
In our approximate solution for each time stamp we compute the expected utility of stopping
(ES i ) at that level. When stopping, the optimal choice is the candidate with the highest expected
utility. To compute the expected utility of stopping (ES i ) optimally, we should compute the expected
utility of the winning candidate in each possible assignment and multiply it by the probability of
that assignment. A brute force approach will consider all the combinations between the assignments
of the timed variables and for each one return the product of the winners expected utility and the
probability of the assignment. This approach is obviously exponential in the number of candidates
since the size of the assignment combinations is exponentially affected by the number of candidates.
Alternatively, we can relax the time complexity by sorting the expected utilities of each candidate. In this way we can easily find the winner and multiply its expected utility by the probabilities
of the assignments of the other candidates with a lower expected utility. Since the expected utilities
are sorted, this computation is linear in the number of candidates. In Theorem 5 we analyze the
time complexity in detail.
To describe this calculation in Algorithm 2, we use the definition of N ODES (see Definition
8). The algorithm obtains time t and a set of candidate trees CT and returns the expected utility
of stopping at that time. For each one of the candidate trees, lines 811 sort the nodes with times
less or equal to t (N ODEStj ) according to their expected utility. The sorting is done in an inverse
order and the ordered nodes are inserted into array sj []. All the arrays are added to set S. In order
to iterate over the arrays, we initiate pointers to the arrays; indx[] contains m pointers to m arrays,
where indx[j] contains a pointer to array sj []. All the pointers are initiated to point at the first node
in the corresponding array (lines 1214). In the main loop (lines 1519), we find the node with the
highest expected utility among the nodes that are pointed at. To compute its probability of winning,
we multiply its own probability with the lower probabilities of the nodes of the other candidates.
Namely, for each one of the candidate trees, we sum over the probabilities of the nodes that have
a lower expected utility than the winner (line 17) and multiply this summation with the probability
of the node that currently wins. The sum of the probabilities in the array of candidate ci (si []) that
are lower than that of cj is actually the probability that cj is greater than ci and thus its probability
to beat it. Since the arrays are sorted, this summation is actually done from the current pointer to
the end of the arrays. This follows the law of total probability. Finally, in line 18, we increment the
251

fiK ALECH & R ECHES

Algorithm 2 EXPECTED STOPPING
(input: time t)
(input: candidate trees CT = {ct1 , ..., ctm })
output: expected stopping ES( t , )

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

Internal variables:
S
indx[m]
i1
j1
exp  0
best
for all ctj  CT do
j
sj []  sort
 N ODESt in inverse order
S  S sj []
end for
for all j  m do
indx[j]  1
end for
while j  m, indx[j] did not reach the end of sj [] do
best  k, where k confirmssk [indx[k]]  si [indx[i]] i  {1, ..., m}
exp

exp
+
EU (ctbest , sbest [indx[best]])

|si []|
P
rPT
H(ct
,
s
[k])
i i
i=best
k=indx[i]
18:
indx[best]  indx[best] + 1
19: end while
20: return exp  CST (t)

s0=66.3



P rPT H(ctbest , sbest [indx[best]])



w1=65.904

s1=65.1

w1=65.904
s2=65.255

w2=65.904

s3=65.17

w3=65.904

s4=65.904
Figure 8: Pessimistic approach: decision tree based on ct1 and ct2 .
pointer of the local winner to the next node to find the winner in the next iteration. We continue this
loop until one of the candidate nodes has been scanned. In this case, all the unscanned utilities of
the other arrays are less than the last utility in the array that has been completely scanned. In line
20 the function subtracts the cost of waiting from exp. This algorithm will be demonstrated in the
net page.
The next procedure describes the pessimistic decision tree for time stamp t. Such a tree is rebuilt
for each time stamp. It is invoked first with time 0:
Procedure 3 Pessimistic:
252

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

1. Generate a decision tree in a bottom-up manner:ES i for i  {0, ..., h} is computed based on
Algorithm 2. Then, EW h1 is equal to ES h and EW i for i  {0, ..., h  2} is the maximum
between ES i+1 and EW i+1 . This building iterates up to the root at time stamp t. Denote ES t
and EW t by ES( t , ) and EW( t , ) respectively.
2. If ES( t , )  EW( t , )
then,  = stop, return argmax ES i ( t , i )
i{1,...,m}

else  = wait.
3. Prune each candidate tree according to the global assignment such that the tree will be rooted
by the node reached by the local assignment and invoke Procedure 3 with time stamp t + 1.
Let us demonstrate the approximate decision tree (Figure 8). Figure 1 and Figure 2 represent
two candidate trees and CST (t) = 1.2  t. We generate the decision tree in a bottom-up manner
since each waiting node is actually the maximum of the nodes in the next level. In the last level l4 ,
there is only one node, ES 4 . In order to calculate the expected utility of stopping, we use Algorithm
2. N ODES41 = {n2,1 , n3,1 , n5,1 , n6,1 }, N ODES42 = {n2,2 , n3,2 , n5,2 , n6,2 }. Algorithm 2 sorts
these sets into s1 and s2 : s1 = [80, 65, 60, 55], s2 = [75, 70, 45, 40]. In the first iteration (lines
1519), the pointer to the winner is best = 1 since s1 [1] = 80 > s2 [1] = 75. Thus,
exp = s1 [1]  P rPT H(ct1 , n2,1 )
(P rPT H(ct2 , n2,2 ) + P rPT H(ct2 , n3,2 )+
P rPT H(ct2 , n5,2 ) + P rPT H(ct2 , n6,2 )) =
80  (0.4  0.8)  (0.3  0.8 + 0.7  0.4 + 0.7  0.6 + 0.3  0.2) = 25.6
Then the pointer of s1 is incremented to point to s1 [2]. In the next iteration, best = 2 since s2 [1] =
75 > s1 [2] = 65. Thus,
exp = exp + s2 [1]  P rPT H(ct2 , n2,2 )
(P rPT H(ct1 , n6,1 ) + P rPT H(ct1 , n5,1 ) + P rPT H(ct1 , n3,1 )) =
exp + 75  (0.8  0.3)  (0.6  0.1 + 0.6  0.9 + 0.4  0.2) =
exp + 12.24 = 37.84
Lastly, exp = 70.704 and the expected utility of stopping is ES 4 = 70.704  4.8 = 65.904.
EW 3 = ES 4 , since there is no wait node in time t4 . Similarly, according to Algorithm 2, we
calculate ES 3 based on N ODES31 = {n2,1 , n3,1 , n4,1 }, N ODES32 = {n2,2 , n3,2 , n4,2 }: ES 3 =
65.172. EW 2 = max(ES 3 , EW 3 ) = 65.904. The complete decision tree is presented in Figure 8.
At runtime, the decision maker decides to wait or stop according to ES 0 and EW 0 (in the first
iteration when t = 0). The agent decides to stop if ES 0 > EW 0 and then it chooses the candidate
with the highest expected utility. If the agent decides to wait, several assignments will occur. At
this point the decision tree needs to be recomputed as several nodes have become irrelevant. In the
presented example the agent decide to stop at time stamp t = 0 since the expected stop, ES 0 (66.3)
is higher than the expected wait EW 0 (65.904).
5.2 Analysis
First, we show the time complexity of the pessimistic approach.
253

fiK ALECH & R ECHES

Theorem 5 Given time horizon T = (0, ..., h), the time complexity of building the decision tree in
the pessimistic approximation is O(h  (m2 M + mM log M )), where m is the number of candidates
and M is the maximal size among the candidate trees.
Proof: At time stamp ti , the algorithm sorts the nodes in the set N ODEStji for each candidate tree
ctj . Since the maximum number of nodes in N ODEStji is M , the worst case complexity of this sort
is M log M . Since we perform this sort for each candidate tree, the complexity is O(mM log M ).
To compare sorted sets in set S, the algorithm goes over the candidates and finds the maximum
among the pointed nodes of the candidates. This computation is m2 . The algorithm stops once it
reaches the end of one candidates array (line 15). The worst case is M . Finding P rPT H of each
node can be calculated once before the loop with a complexity of mM log M . Thus, the worst case
time complexity of Algorithm 2 is O(m2 M + mM log M ). We perform Algorithm 2 for each time
stamp and as a result the time complexity is O(h  (m2 M + mM log M )).2
Similar to the optimistic algorithm, the pessimistic algorithm rebuilds the decision tree in each
time stamp in polynomial time and thus can address changed and additional timed variables.
We now show that if Procedure 3 decides to wait an optimal algorithm would operate similarly.
If the expected gain from stopping is greater than the expected gain from waiting, Procedure 3
returns a stopping policy. In this case an optimal policy could return a waiting policy.
Theorem 6 Given a cost function CST (t), a set of candidate trees CT , a global assignment  t and
an optimal policy   , a policy  taken by Procedure 3 and an optimal policy   , the global expected
gain GEG holds: ES( t , ) = ES( t ,   ) and EW( t , )  EW( t ,   ).
Proof: For each time t, Algorithm 2 calculates the expected gain from stopping, ES( t , ), by
summing for each possible assignment the expected utility of the candidate with the highest value
(the winner candidate) times the probability of the assignment. Since for each time stamp the sum of
the probabilities of all the possible assignments is 1, according to the law of total probability (Beaver
& Mendenhall, 1983), ES( t , ) = ES( t ,   ). The waiting node is the maximum between the
wait node and the stop node at the next time level. Therefore, the algorithm does not take into
consideration the combination of waiting and stopping for different assignments. In contrast, with
an optimal policy the algorithm considers such combinations and takes the maximal value for each
assignment. Thus, the value of its expected wait may be higher and as a result the wait nodes value
is less than or equal to the optimal expected gain from waiting, EW( t , )  EW( t ,   ).2
Corollary 2 Based on the last theorem, if ( t ) = wait, an optimal policy would result in the
same decision. This is due to the fact that if  is a policy obtained by Procedure 3 and ( t ) =
wait, then EW( t , ) > ES( t , ). Based on the last theorem EW( t , )  EW( t ,   ) and
ES( t , ) = ES( t ,   ), and thus EW( t ,   ) > ES( t ,   ) therefore, policy   decides to wait.
Consequently, the pessimistic approach guarantees the optimal expected gain from waiting.
We now prove the approximation error of the expected wait.
Theorem 7 Given a time horizon T = (0, ..., h), a cost function CST (t), a set of candidate trees
CT , a global assignment  t and a global assignment  t , if  is a policy obtained by Procedure
3 and   is a policy obtained by the optimal algorithm, EW( t ,   )  EW( t , )  CST (h) 
CST (t + 1).
254

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

Proof: The optimal expected gain that can be obtained from stopping or waiting at a specific time
stamp t is the expected gain from waiting until the last time stamp h (where there is no uncertainty) without considering the cost of this waiting, ES( h , ) + [CST (h)  CST (t)]. Thus, the
expected gain from waiting at time t holds, (1) EW( t ,   )  ES( h , )+[CST (h)CST (t+1)]
(since the expected wait EW( t ,   ) already includes the cost of waiting from time t to time stamp
t + 1). On the other hand, because the wait nodes are calculated as the maximum among their
children and since in the last level lh there is only a stop node, ES( h , )  EW( t , ) and thus,
(2) EW( t , )  ES( h , ). As a result, (by summing the two inequalities (1) and (2)),
EW( t ,   )  EW( t , )  CST (h)  CST (t + 1). 2
Finally, we prove the approximation error of the global expected gain.
Theorem 8 Given time horizon T = (0, ..., h), a policy  taken by Procedure 3 and an optimal
policy   , a cost function CST (t), a set of candidate trees CT , a global assignment  t holds:
GEG(CT,  0 ,   )  GEG(CT,  0 , ) < CST (h)  CST (1).
Proof:
According to Corollary 2 , the pessimistic policy  is not optimal except for assignment  t , ( t ) = stop and an optimal policy   holds   ( t ) = wait. In this case,
GEG(CT,  t ,   ) = EW( t ,   ) and GEG(CT,  t , ) = ES( t , ). If ( t ) = stop at time
stamp t, then ES( t , ) > EW( t , ) and according to Theorem 7, EW( t ,   )  EW( t , ) 
CST (h)  CST (t + 1). Thus, EW( t ,   )  ES( t , )  CST (h)  CST (t + 1). As a
result, GEG(CT,  t ,   )  GEG(CT,  t , ) < CST (h)  CST (t + 1). In particular, for t=0,
GEG(CT,  0 ,   )  GEG(CT s,  0 , ) < CST (h)  CST (1).2

6. Evaluation
Before presenting an empirical evaluation, we summarize the theoretical analysis of our algorithms
in Table 2.
Policy

OPTIMAL
OPTIMISTIC
PESSIMISTIC

#trees Complexity
m-#candidates,
M-size of the candidate
tree
1
O(M m )
m
O(M 2 m2 )
1
O(m2 M + mM log M )

Approximation
error of GEG
(h-max time horizon)
0
CST (h  1)
CST (h) CST (1)

Expected wait

Approximation error of EW

optimal
overestimate
underestimate

0
f

i=1 EU (cti , ni ) + CST (h)
CST (h)  CST (t + 1)

Table 2: Summary of the theoretical evaluation of the algorithms.
The three algorithms, OPTIMAL, OPTIMISTIC, and PESSIMISTIC, are based on a decision
tree approach. However, while the optimal and the pessimistic algorithms use a single decision
tree that merges all the candidates, the optimistic algorithm implements m decision trees, one for
each candidate tree. The time complexity and the approximation error of the global expected gain
are presented in columns three and four, respectively. The fifth column presents an evaluation of
the expected wait. Obviously, the optimal algorithm computes the expected wait optimally. The
optimistic algorithm overestimates the expected wait and thus a waiting decision is not optimal
since the real expected wait may be less than the stop. The pessimistic algorithm underestimates
the expected wait and although a waiting decision is optimal, a stopping decision is not since the
real expected wait may be higher and consequently the optimal decision would be to wait. The last
column presents the approximation error of the expected wait.
255

fiK ALECH & R ECHES

As shown in table 2, the error of the expected wait estimated by the optimistic algorithm is
much higher than that of the pessimistic algorithm. As a result we estimate that the pessimistic
performances will be closer to the optimal algorithm in most of the situations. However, since
the expected wait of the optimistic algorithm is overestimated, it may frequently choose to wait
and obtain information and thus in case that the cost function increases moderately in time, the
performance of the optimistic algorithm will increase.
6.1 Experimental Settings
We experimentally validated our algorithm within a systematic artificial framework inspired by
the stock market. We varied the number of candidate stocks (230) and the time horizon of the
economic events (15) (i.e., the timed variables). We ran each combination 25 times. In each test,
the possible profits from the stocks (the utility) were randomly selected from a uniform distribution
over the range [$10K . . . $100K]. Later we present experiments with additional distributions. We
ran each scenario (of 25 tests) with 25 random assignments for the timed variables. Each data point
in the graphs is an average of 625 tests (25 random utilities  25 random assignments).
We compared the three algorithms (OPTIMAL, OPTIMISTIC and PESSIMISTIC) to four baseline algorithms:
1. A trivial stopping strategy; determining the winning candidate at the beginning based only on
the expected utility (STOP).
2. A trivial waiting strategy; determining the winning candidate at the end based on full information (WAIT).
3. An algorithm that stops in the middle (horizon/2) and chooses the best candidate based on
the expected gain (MIDDLE).
4. An algorithm that stops at a random time (RANDOM).
We compared the above algorithms using two metrics: (1) runtime, and (2) the outcome utility.
The runtime of OPTIMAL is the runtime of building the decision tree; the runtime of the approximations is the average runtime of building the decision trees in each level. To normalize the utility,
we divided it by the utility gained by an omniscient decision maker with no cost. All the following
experiments presented in the next sections apart from those presented in Section 6.4 deal with disjoint timed variables, namely, two candidate trees do not share the same timed variable. Notice that,
using disjoint time variables, is the worst case scenario for OPTIMAL since finding the optimal
time to stop requires taking into consideration all the combinations between the timed variables of
the candidate trees. When the timed variables are disjoint the number of comparisons is exponential
in the number of candidates (see Table 2).
6.2 The Effect of the Cost
The cost function is a key factor in selecting the most affective algorithm. To examine this factor, we
set a simple cost function that grows linearly with the time CST (t) = a  t and varied the coefficient
of the time stamp (a) from 0.01K to 2.91K, with jumps of 0.15K. We fixed both the number of
candidates and the horizon at 5. STOP strategy, as presented in Figure 9, is not affected by the cost
since it stops at time t = 0 in any case. On the other hand, the utility of WAIT, MIDDLE and
256

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

1
Depth in percents

1
Normalized utility

OPTIMAL

0.95
0.9
0.85
0.8

0.8
0.6
0.4

0.2
0

0.01
0.16
0.31
0.46
0.61
0.76
0.91
1.04
1.16
1.31
1.46
1.61
1.76
1.91
2.04
2.16
2.31
2.46
2.61
2.76
2.91

0.01
0.16
0.31
0.46
0.61
0.76
0.91
1.04
1.16
1.31
1.46
1.61
1.76
1.91
2.04
2.16
2.31
2.46
2.61
2.76
2.91

0.75

Cost per time stamp (in thousands)

Cost per time stamp (in thousands)

Figure 9: Normalized utility over the cost of Figure 10:
each time step, where CST (t) = x 
K  t. Time horizon is 5 levels and the
number of candidates is 5.

Depth of decision over the cost of
each time step, where CST (t) = x 
K  t. Time horizon is 5 levels and
the number of candidates is 5.

RANDOM, linearly decreases while the cost increases. Figure 10 shows that OPTIMAL and both
approximations make the decision earlier as the cost increases since it becomes less worthwhile to
wait. However, the depth of the decision decreases faster in OPTIMAL and PESSIMISTIC than in
OPTIMISTIC. The depth of the decision influences the utility of the algorithms. It is interesting to
see that the gap between the utility of OPTIMISTIC and PESSIMISTIC grows as the cost increases,
similarly to the gap in the depth. This can be explained by the fact that OPTIMISTIC overestimates
the expected wait and thus it makes the decision later and the loss from the cost is more significant.
Nevertheless, from cost = 2 both approximations and the optimal algorithm stop at time stamp 0
and achieve the same utility.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP
OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

0.96

Normalized utility

Normalized utility

1

OPTIMAL

0.95

0.9
0.85
0.8

0.95
0.94
0.93

0.92

0.75

2

2

3

4
#candidates

5

6

Figure 11: Normalized
utility
over
the
number of candidates,
where

CST (t) = 0.28K  3 t for time
horizon of 5 levels.

3

4
#candidates

5

6

Figure 12: Normalized utility over the number of candidates,
where CST (t) =

0.28K  3 t for time horizon of 5 levels: zoom in the utility range of 0.92
0.96.

We further ran experiments with additional cost functions. Figures 11 and 13 present
non
3
linear cost functions. The cost in Figure 11 increases moderately (CST (t) = 0.28K  t) and the
cost in Figure 13 increases fast (CST (t) = 0.28K  t2 ). We show that, for the cost function that
257

fiK ALECH & R ECHES

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

0.97

Normalized utility

1

Normalized utility

OPTIMAL

0.95
0.9

0.85
0.8
0.75

0.96
0.95
0.94
0.93

0.92
0.91

2

3

4
#candidates

5

6

1/7

Figure 13: Normalized utility over the num- Figure 14:
ber of candidates, where CST (t) =
0.28K  t2 for time horizon of 5 levels.

1/6

1/5
1/4
1/3
1/2
cost function: cost(t)=0.28k*t^x

1

2

Normalized utility over the cost function: cost(t) = 0.28K  tx , where
time horizon is 5 levels and number
of candidates is 5.

increases more moderately (a root function), the pessimistic algorithm becomes less effective and
OPTIMISTIC becomes better than PESSIMISTIC. As shown in Figure 11 and in a zoom-in view
in Figure 12, in such functions OPTIMAL is significantly better than PESSIMISTIC and in most
situations it is better than OPTIMISTIC (tested with a 95% confidence value).
To further examine the influence of the cost function on the algorithms, we varied consistently
the cost function. We choose the cost function cost(t) = 0.28K tx while changing x in the range of
{ 17 , 16 , 15 , 14 , 13 , 21 , 1, 2}. Obviously, by decreasing x the function increases more moderately. Figure
14 presents the results. It is clear shown that OPTIMISTIC is better than PESSIMISTIC for root
functions smaller than square root. Then, as the cost function increases faster the gap between
OPTIMISTIC and PESSIMISTIC increases in a favor of PESSIMISTIC.

expected utility

0.28K*t

0.28K*t^(1/3)

Averaged expected utility

9.5
9
8.5
8

7.5
7
0

1

2

3

4

5

Time

Figure 15: Averaged expected utility over time, where time horizon is 5 levels and number of candidates is 5.

258

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

To examine the reason for the behavior of the approximations as dependent in the cost function,
we observed the growth of the averaged expected utility as a function of the time (Figure 15). The
x-axis is the time horizon and the y-axis is the averaged expected utility. Obviously, the expected
utility increase as the time increases since the more events are discovered the uncertainty decreases.
It seems that the averaged expected utility grows moderately, approximately logarithmically, as a
function of the time. In addition to the averaged expected utility, we present in Figure 15 two
cost functions. The first is the linear cost function CST (t) = 0.28K  t, where the pessimistic

approximation is better, and the second is the root cost function CST (t) = 0.28K  3 t, where
the optimistic approximation is better. We set both cost functions to start at the same value of
the expected utility on the y-axis. Figure 15 compares between the growth behavior of the two
cost functions and the expected utility. This comparison may explain the fact that when the cost
function grows linearly the pessimistic algorithm, which usually stops earlier, is better than the
optimistic algorithm, since the cost function grows faster than the utility function. However, when
the cost function grows more moderately (a root function), meaning, the cost function and the utility
function has a similar trend, the optimistic algorithm, which usually stops later, becomes better.
In the rest of the experiments we will examine other factors that influence the performance of
the algorithms. As we showed, the difference between the algorithms for a root cost function is
small and thus it might be hard to examine the impact of the other factors. Therefore, in the rest of
the experiments we use a linear cost function by fixing the waiting cost of all the events to a constant
value of $2.8K for each time stamp (CST (t) = 2.8K  t).
6.3 The Effect of Number of Candidates
We present a subset of the results with a time horizon of 5 levels. Figure 16 presents the utility for a
test setting of up to six candidates. Due to memory limitations, the optimal algorithm failed to deal
with larger candidate sets. The utility gained by PESSIMISTIC is very close to OPTIMAL and the
difference between them is not significant. This result is much better than the results of the baseline
algorithms and even better than OPTIMISTIC.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Runtime (ms)

Normalized utility

1
0.95

0.9
0.85
0.8
0.75
2

3

4
#candidates

5

6

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

10000
1000
100
10
1
0.1
0.01
0.001
2

3

4
#candidates

5

6

Figure 16: Normalized utility over 6 candidates Figure 17: Runtime over 6 candidates with a
time horizon of 5 levels.
with a time horizon of 5 levels.
The runtime is presented in a logarithmic scale in Figure 17. The runtime of all the algorithms
is polynomial, except for OPTIMAL, which is exponential. For instance, the average runtime of
OPTIMAL for six candidates is 5836 milliseconds, while that of the other algorithms is less than
two milliseconds.
259

fiK ALECH & R ECHES

We further compared the algorithms, excluding the optimal algorithm, for larger sets of up to 30
candidates. The utility of PESSIMISTIC is always significantly better than the others, as shown in
Figure 18. This may be explained by the approximation error of the expected wait. By comparing
the approximation error of the two algorithms (see Table 2), it is clear that the approximation error
of the expected wait of OPTIMISTIC is much greater than that of PESSIMISTIC and thus OPTIMISTIC is expected to make its decision later than PESSIMISTIC. Note that there is no statistically
significant difference between OPTIMISTIC and MIDDLE. Later we will present experiments with
larger cost values and horizon where OPTIMISTIC is much better than MIDDLE.
Although the complexity of both PESSIMISTIC and OPTIMISTIC is polynomial, PESSIMISTIC is better than OPTIMISTIC in terms of runtime, as shown in Figure 19. This can be
justified by the complexity analysis of the algorithms, as shown in Table 2. While OPTIMISTIC is
square in both M and m, PESSIMISTIC is square only in m but not in M .
To illustrate the significance of these results, consider for instance a stock market with five
candidate stocks. Based on our experiments, the average utility of the optimal algorithm is $92.8K,
which is 97.6% of the utility obtained by an omniscient decision maker. PESSIMISTICs utility is,
on average, less than the optimal by an amount of only $300, while OPTIMISTIC reduces the utility
by an amount of $2, 800. Obviously, the baseline algorithms reduce the utility drastically. The wait
strategy, for instance, produces a profit of only $80.5K.
OPTIMISTIC

PESSIMISTIC

STOP

OPTIMISTIC

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

WAIT

MIDDLE

RANDOM

10

0.95

Runtime (ms)

Normalized utility

1

0.9

0.85
0.8

1
0.1
0.01
0.001

0.75
2

4

6

8

2

10 12 14 16 18 20 22 24 26 28 30
#candidates

4

6

8

10 12 14 16 18 20 22 24 26 28 30
#candidates

Figure 18: Normalized utility over 30 candi- Figure 19: Runtime over 30 candidates with a
dates with a time horizon of 5 levels.
time horizon of 5 levels.

6.4 Shared Timed Variables
The previous experiments were run on settings where different candidates are not affected by the
same timed variable. In the next experiment we show that, even if different candidates are affected
by the same timed variables, both approximations achieve a similar utility as previously. We generated candidate trees with a horizon of 5 time stamps. We set 50% of the variables to affect multiple
candidates. In Figures 20 and 21 we present the normalized utility for 6 to 30 candidates. By
comparing these results to the results without shared variables (Figures 16 and 18) we can see that
the baseline algorithms MIDDLE and RANDOM significantly improve their utility. There is no
statistically significant difference between each one of the other algorithms with and without shared
variables (tested with a 95% confidence value). The reason for the improvement of MIDDLE and
RANDOM is that the more shared variables the less uncertainty and thus the expected utilities of
the candidates is more accurate. On the other hand the computation of the expected stopping of the
260

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

1
Normalized utility

Normalized utility

1

OPTIMISTIC

0.95
0.9
0.85
0.8

0.75

0.95
0.9
0.85
0.8
0.75

2

3

4
#candidates

5

6

2

4

6

8

10 12 14 16 18 20 22 24 26 28 30
#candidates

Figure 20: 50% Shared timed variables: Nor- Figure 21: 50% Shared timed variables: Normalized utility over 6 candidates with
malized utility over 30 candidates
a time horizon of 5 levels.
with a time horizon of 5 levels.

approximation algorithms relies on the independence between the variables and thus the decrease
in the uncertainty does not improve their results.
We also checked the difference between the algorithms and found that there is no statistically
significant difference between OPTIMAL and PESSIMISTIC. Both algorithms are better than OPTIMISTIC but there is no statistically significant difference between OPTIMISTIC and MIDDLE.
The results of the approximation algorithms are significantly better than the other baseline algorithms (tested with a 95% confidence value).
As analyzed in Section 3, the optimal algorithm addresses shared timed variables very efficiently
and in fact it reduces its computational complexity. In Figure 22 we show the runtime with 50%
shared timed variables. Compared to Figure 17 we can see that OPTIMAL runs in two orders of a
magnitude faster than experiments with no shared timed variables.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

1
Depth in percents

10
Runtime (ms)

OPTIMAL

1
0.1
0.01

0.8
0.6
0.4
0.2

0

0.001
2

3

4
#candidates

5

2

6

3

4
#candidates

5

6

Figure 22: 50% Shared timed variables: runtime Figure 23: Normalized depth over 6 candidates
over 6 candidates with a time horizon
with a time horizon of 5 levels.
of 5 levels.

6.5 The Depth of the Decision
Figure 23 illustrates the attributes of the PESSIMISTIC and OPTIMISTIC algorithms when run
on candidate trees with a time horizon of 5 levels. The y-axis represents the depth (in percentage
261

fiK ALECH & R ECHES

relative to the maximal horizon) in which the algorithms stop and decide. Figure 23 presents the
results for candidate trees with a time horizon of 5 levels. As analyzed, PESSIMISTIC always stops
before the optimal algorithm, since its expected wait is underestimated. OPTIMISTIC always stops
after the optimal algorithm since by overestimating the expected wait it continues to wait, although
an optimal algorithm decides to stop.
6.6 The Effect of the Horizon

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

1

Normalized utility

Normalized Utility

1

OPTIMISTIC

0.95
0.9
0.85
0.8
0.75

0.95
0.9
0.85

0.8
0.75

1

2

3

4

5
6
Horizon

7

8

9

10

1

3

5

7

9

11 13 15 17 19 21 23 25 27 29
Horizon

Figure 24: Normalized utility over horizon (1- Figure 25: Normalized utility over horizon (130) where number of candidates is 5.
10) where number of candidates is 5.

Next we examine the influence of the horizon of the candidate trees on the utility. To grow
candidate trees with a large horizon, we generated a chain; a tree such that every left branch leads
to a leaf while every right branch leads to another internal node. Thus, the size of the candidate tree
grows linearly with the horizon. The experimental setting for these experiments includes 5 candidates. Figures 24, 26, and 28 present the results for horizon 110 by comparing all the algorithms.
Since it was not feasible to run OPTIMAL for a larger horizon, we ran the rest of the algorithms for
horizon 130 (see Figures 25, 27 and 29). As shown in Figures 24 and 25, the utility is not dramatically affected by the horizon for the optimal, approximations and stop algorithms. This is different
for WAIT, MIDDLE and RANDOM, which lose a constant cost every time stamp since they do
not intelligently compute where to stop. The stop strategy is not affected by the horizon since it
always makes the decision at the first time, thus we see that for low horizon levels a wait strategy
is better, but for high levels the stop strategy outperforms the wait strategy as well as MIDDLE and
RANDOM.
Although the chain topology of the candidate trees grows linearly with the horizon, the runtime
of OPTIMAL increases exponentially in the horizon, since at each level it splits the possible assignments of the candidates timed variables (Figure 26, the runtime is presented in a logarithmic scale).
As shown in Figure 27, OPTIMISTIC and PESSIMISTIC grow polynomially but OPTIMISTIC
grows faster. Naturally, the relative depth of the decision of OPTIMAL and both approximations
decrease in the horizon, since the cost function grows in the horizon and thus it is less worthwhile
to wait for more information (Figure 28 and 29). Nevertheless, the decision time does not converge
to 0 since once the cost is not very high, the cost of waiting a few time stamps may be less than
the expected utility. Obviously, as discussed above, the higher the cost the less the wait. Note that
262

fiOPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

OPTIMISTIC

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

100

10000
1000
100
10
1
0.1
0.01
0.001

Runtime (ms)

Runtime (ms)

D ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

10
1

0.1
0.01
0.001

1

2

3

4

5
6
Horizon

7

8

9

10

1

3

5

7

9

11 13 15 17 19 21 23 25 27 29
Horizon

Figure 26: Runtime over horizon (1-10) where Figure 27: Runtime over horizon (1-30) where
number of candidates is 5.
number of candidates is 5.

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

PESSIMISTIC

STOP

WAIT

MIDDLE

RANDOM

1
Depth in percents

Depth in percents

1

OPTIMISTIC

0.8
0.6
0.4
0.2
0

0.8
0.6

0.4
0.2

0

1

2

3

4

5
6
Horizon

7

8

9

10

1

3

5

7

9

11 13 15 17 19 21 23 25 27 29
Horizon

Figure 28: Depth of decision over horizon (1- Figure 29: Depth of decision over horizon (130) where number of candidates is 5.
10) where number of candidates is 5.

the decrease of the OPTIMISTIC compared to the PESSIMISTIC is moderate (Figure 29) since the
approximation error of the expected wait of OPTIMISTIC is greater than that of PESSIMISTIC.
6.7 The Effect of the Utility Distribution
In the above experiments, we simulated the utilities by taking them from a uniform distribution. To
simulate a varied range of domains we present additional results where the utilities are taken from a
Beta distribution with symmetric and asymmetric cases. A Beta distribution with  =  provides a
symmetric distribution. For  =   2 a Beta distribution is similar to a normal distribution defined
in an interval of [0, 1] and thus reflects the distribution of many real-world domains. The larger the
value of  =  the lower the variance. Running experiments with a Beta distribution allows us to
examine: (1) the influence of the variance - by controlling the value of  = , and (2) the influence
of the skewness - by setting  to a fixed value and varying .
In the first experiment we set  =  = 2 and the number of candidates and the horizon at 5. By
comparing the results (Figure 31) to the results of the experiments with uniform distribution (Figure
16), we can see that all the algorithms except for WAIT, improve the utility. This can be explained
by the fact that the variance between the candidates utilities in a Beta distribution is smaller than
the variance in a uniform distribution, and thus by choosing a non-optimal candidate, the utility is
263

fiK ALECH & R ECHES

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1
0.95
0.9
0.85
0.8
0.75
2

3

4

5

#candidates

Figure 31: Utilities were taken from a Beta distribution with  = 2 and  = 2
Figure 30: A Beta distribution with  = 2 and
skewness=0. Normalized utility over
 = 2.
candidates with a time horizon of 5
levels and 5 candidates.

closer to that of the optimal candidate. Therefore, if an algorithm stops and selects a non-optimal
candidate, the utility by such a selection is closer to the optimal (and higher) in a Beta distribution
than in uniform distribution. This also explains why in a Beta distribution the utility of STOP is
higher than that of OPTIMISTIC, while in a uniform distribution it is lower. This is also supported
by the depth of the decision. The decision is made earlier in a Beta distribution than in a uniform
distribution. The utility of the WAIT strategy is the same in both distributions because once the
decision is made at the end it is optimal and thus only the cost is reduced, which is the same in both
distributions.
OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1

0.95
0.9
0.85

0.8
0.75
2

3

4
The value of =

5

6

Figure 32: Utilities were taken from a Beta distribution with varied  = . Normalized utility over
candidates with a time horizon of 5 levels and 5 candidates.

To further examine this insight we run experiments with a varied range of  =  of 2 to
6. By increasing  and  the variance is decreased. Figure 32 presents the average utility for
264

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

this experiment. It seems that the utility of most algorithms is not significantly influenced by the
variance. A possible explanation is the tradeoff between two trends. On the one hand, the smaller the
variance between the utilities of the candidates, the less difference between the utilities of the chosen
candidate and the best candidate. On the other hand, the difference between the expected utility of
the candidates, decreases with the variance, and thus the possibility of selecting a wrong candidate
increases. Consequently, the probability for an error in choosing the best candidate increases but the
payoff for an error is decreased and thus the utility is not significantly influenced by the variance.
An exception is the WAIT algorithm which decreases with the variance. The reason is that it makes
the decision only at the end and thus always chooses the best candidate. However, since a Beta
distribution with  =  is a symmetric distribution then as the variance increases the best utility
decreases (close to the middle), and on the other hand, it pays a full cost for waiting to the last time
stamp.

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1
0.95
0.9
0.85
0.8
0.75
2

3

4

5

#candidates

Figure 34: Utilities were taken from a Beta distribution with  = 2 and  = 6
Figure 33: A Beta distribution with  = 2 and
skewness=0.69. Normalized utility
 = 6.
over candidates with a time horizon
of 5 levels and 5 candidates.

We repeated the experiments with a Beta distribution while changing the parameters of the
distribution only for the first candidate to  = 6 and  = 2. This difference influences the skewness
of the distribution (-0.69) and gives a high probability of gaining higher values (Figure 33). Since
 and  parameters of the other candidates remain the same ( = 2 and  = 2), their skewness is 0
and thus the first candidate is likely to be chosen. As shown in Figure 34, low skewness of the first
candidate significantly increases the utility of all the algorithms. The reason is that independently
of the stopping time, in most cases, the expected utility of the first candidate is the highest and thus
it is selected by the algorithms. Then only the cost of waiting reduces the utility. This insight is
significantly shown in the high utility of the STOP algorithm.
We further experimented with the influence of the skewness on the algorithms. Figure 35
presents the normalized utility by changing the skewness of the first candidate. The lower the
skewness, the more likely the first candidate will be chosen. The increase in the utility of all algorithms is clear since the more likely the first candidate will be chosen, the fewer errors there will be
in choosing the best candidate.
265

fiK ALECH & R ECHES

OPTIMAL

OPTIMISTIC

PESSIMISTIC

WAIT

MIDDLE

RANDOM

STOP

Normalized utility

1
0.95
0.9
0.85
0.8

0.75
0.00

-0.29

-0.47
Skewness

-0.60

-0.69

Figure 35: Utilities were taken from a Beta distribution with  = 2 and varied  and skewness.
Normalized utility over candidates with a time horizon of 5 levels and 5 candidates.

6.8 Conclusions
To summarize, the conclusions from the experiments are:
1. As the cost function increases more moderately (a root function), the PESSIMISTIC algorithm becomes less effective and the OPTIMISTIC becomes better than the PESSIMISTIC.
2. The utility of PESSIMISTIC is very close to OPTIMAL and in most cases (for functions that
grow polynomially) there is no statistically significant difference between them.
3. The runtime of OPTIMAL is exponential and actually feasible only for a few candidates with
small candidate trees.
4. The runtime of the approximations is polynomial but PESSIMISTIC runs much faster than
OPTIMISTIC.
5. There is no statistically significant difference between the experiments that include shared
timed variable and experiments that include only disjoint timed variables (except for MIDDLE and RANDOM).
6. OPTIMAL runs much faster in experiments which include candidates with shared timed variable.
7. PESSIMISTIC strategy makes the decision slightly earlier than OPTIMAL.
8. OPTIMISTIC strategy makes the decision much later than OPTIMAL. The gap is increased
by increasing the horizon of the candidate trees and the cost of waiting. For a very large
horizon and cost the depth is very close to time=0.
9. There are cases (low horizon, high number of agents, cost functions that grow polynomially
and shared timed variables) where MIDDLE is better than the OPTIMISTIC algorithm.
266

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

10. The greater the expected utility of one candidate is higher than the others, the greater the
utility achieved by the different algorithms.
11. The normalized utilities achieved by the algorithms is almost not affected by the variance of
the candidates utilities. However, a significant improvement is achieved with a Beta distribution ( =   2) in relation to uniform distribution.

7. Related Work
In this section we discuss the relation between this work and other research related to the optimal
stoping problem and explorationexploitation problems.
7.1 Optimal Stopping Problem
Our problem is related to the Optimal Stopping Problem (OSP). In OSP the goal is to choose a
time to take a particular action in order to maximize the expected reward (Ferguson, 1989; Gilboa
& Schmeidler, 1989; Peskir & Shiryaev, 2006). The classical stopping problem is defined by two
objects: (i) a sequence of independent random variables, X1 , X2 , ... , with a known joint distribution, and (ii) a sequence of real-valued reward functions, y0 , y1 (x1 ), y2 (x1 , x2 ), ...,. For each
n = 1, 2, ..., after observing X1 = x1 , X2 = x2 , ..., Xn = xn , an agent may stop and receive the
known reward, yn (x1 , ..., xn ), or it may continue and observe Xn+1 . If the agent chooses not to
make any observation, it will receive the constant amount, y0 . Take for example the house-selling
problem where an agent wishes to sell a house. Each day it receives an offer Xi . The agent should
decide either to accept the offer or to wait for the next offer. Waiting is associated with a cost of
living. Offers are assumed to be independent. The goal is to get the highest offer (Lippman &
McCall, 1976).
This class of problems seems to be similar to our problem, since they both address the problem of finding the best time to stop. However, in a deeper perspective, there are three significant
differences between these problems:
(1) Stopping reward: In OSP, an agent has to decide whether to stop and obtain a known
reward, based on the prior random variables or to wait until the next time stamp and observe the
next random variable. If the agent chooses to stop, and not to make any observation, it will receive
a constant reward that is not dependent on further random variables. In contrast to OSP, in our
model, the utility (which is not considering the waiting cost) of each candidate does not depend
on the decision to stop or wait, but on future events. Even the gain is not solely dependent on the
stopping time, since different future events influence the gain differently. A waiting decision enables
the decision maker to acquire more information about the reward of each candidate, although this
reward is not affected by the waiting decision. For example in the house-selling problem, an agent
wishes to sell a house. Each day it receives an offer Xi . The agent should decide either to accept
the offer or to wait for the next offer. If he decides to stop and accept the offer he obtains the reward
of the offer. This reward is not affected by the future offers. In our model, on the other hand, the
reward from stopping depends on future random variables. This difference in the reward obviously
affects the way each approach maximizes the reward. For example, assume an omniscient agent
that knows the outcomes of all the variables in advance; it will certainly stop at the time stamp with
the highest value in the OSP. Contrastingly, in our problem, the best time to stop is at the first time
267

fiK ALECH & R ECHES

stamp since at that time the agent knows the exact utility of the candidates rather than the expected
utility.
The second aspect is related to the independency of the stopping rewards. In OSP, the stopping
rewards of each time stamp are assumed to be independent. For example, in the house-selling
problem, the offers are independent. In our problem, although the variables are independent, the
rewards from stopping are dependent. This difference is significant since, with the independency
assumption, the stopping rule of OSP depends on the probability that the agent did not stop until
the current time stamp multiplied by the probability that it will stop now. In our model, on the other
hand, the rewards from stopping depend on future random variables and are calculated by taking
the expected rewards of the candidates and are therefore dependent. As a result, we are not able to
use the OSP model to solve our problem and vice versa.
(2) Multiple candidates: In OSP there is one reward from observing X1 , X2 , ..., Xn . Our
problem, on the other hand, considers multiple candidates. As a result, each candidate has a different
reward from observing the variables. There are two different challenges that we face: (1) finding
the best time to stop, and (2) choosing the candidate with the highest expected utility at that time.
Although these are two different challenges, they cannot be treated in two steps: finding the best
time to stop in advance, and when that time is reached choosing the candidate with the highest
expected utility at that time. Such an approach would have made the challenge much simpler and
thus the fact that there are multiple candidates would be insignificant. Nevertheless, the problem
cannot be solved in two separate steps since the decision of whether to stop or wait depends on
the assignments that will occur the next time. Since each candidate may be affected by different
assignments, we should consider the combination of all assignments which makes this problem
hard.
(3) Joint distribution: In the classical stopping problem the random variables have a known
joint distribution. In the previous example, all the offers are assumed to have the same known
distribution and thus in case of infinity there is convergence. Our problem is different since it
considers random variables with different distributions.
In the last few years several researchers have generalized the classical stopping problem in order
to deal with cases of multiple distributions, i.e., multiple optimal stopping problems. Riedel (2009)
presents a unified and general theory of optimal stopping under multiple priors in discrete time
and extends the theory to continuous time (da Rocha & Riedel, 2010; Cheng & Riedel, 2010). He
developed a theory of optimal stopping with more than one joint distribution and with unknown distribution of the variables using and extending suitable results from the martingale theory (Williams,
1991). Still the two differences (Stopping reward, Multiple candidates) specified above, are maintain. In addition, Riedel presents specific constraints on the random variables such as considering
the Martingale theory and assuming that the set of stopping rewards are time-consistent (Cheridito
& Stadje, 2009). In contrast, in our work the distribution of the variables is known in advance and
we do not require any specific constraints on the random variables.
These three differences demonstrate that in spite of the similarity between our problem and OSP,
these two models are not comparable and cannot be reduced to each other.
7.2 ExplorationExploitation Problems
In addition to the classical stopping problem, we also consider a subset of the family of
explorationexploitation problems as a kind of stopping problem. In these problems an agent
268

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

has to decide when to stop acquiring information (exploration) about a specific issue and make a
decision (exploitation). Sequential hypothesis testing is a method which is based on statistical
tests. This method enables a stopping rule to be defined as soon as significant results are observed.
This method is based mainly on uncertain information, using multiple observations and samples.
For instance, Wald and Woldforwitz (1948) present a problem where a chance variable, Xs distribution, only can be either p0 (X) or p1 (X). The required decision is to choose between the two
options based on acquired observations of this variable. The research goal obviously, is to make a
decision with a minimal number of observations.
In the multi-armed bandit problem (Katehakis & Veinott, 1987) an agent allocates trials over
slot machines where each machine provides a random reward from a distribution specific to that
machine. The objective is to allocate trials over the slot machines in order to maximize the expected
reward from using the machines. One version of the multi armed bandit which is the most relevant
to our research is the Max K-Armed Bandit. In the Max K-Armed Bandit problem (Cicirello &
Smith, 2005; Streeter & Smith, 2006) the objective is to allocate trials over the K arms in order to
identify the best arm. Cicirello and Smith (2005) extend the problem where the agent runs trials
repeatedly, where in each trial it tries to improve the reward it has achieved thus far.
Similarly, in the ranking and selection problem an agent is supposed to select an alternative
among several options. To demonstrate the problem, Powell and Ryzhov present the following example (Powell & Ryzhov, 2012): a physician should choose a type of drug among several medicines
which reduces the cholesterol of a patient,. In order to decide about the best drug he might require
making some physical experiments or he might need to run a number of medical laboratory simulations. Testing an alternative might involve running a time-consuming computer simulation, or
require a physical experiment. Obviously, the experimental process is costly and thus the challenge
is to allocate the experiments that most efficiently and accurately make the selection. Usually there
is a limited budget for evaluating the alternatives and when the budget is exhausted, the agent has
to choose the alternative that appears to be the best, according to the obtained knowledge (Swisher,
Jacobson, & Yucesan, 2003). Frazier and Powell (2008) present a version of this problem where the
prior information units and the new obtained information units about each alternative are sampled
from a specific distribution with unknown mean and variance. The model provides a new heuristic
sampling and stopping rule that relies on the distribution of the samples.
An additional problem in statistical analysis is change detection which tries to identify a change
in the parameters of a stochastic system. The changes can be in the probability distribution of a
stochastic process or time series (Basseville & Nikiforov, 1993). In general, the problem concerns
both detecting whether or not a change has occurred (several changes also might occur), and identifying the time of any such change. The model has to decide when to stop obtaining observations
and find the closest time stamp that the distribution has changed. The problem in some works is to
detect the disorder time as quickly as possible after it happens and minimize the rate of false alarms
at the same time (Dayanik, Poor, & Sezer, 2007).
Another problem is decision making concerning multiple observations that are informative
but expensive. The challenge with decision making problems is to decide which variables to observe in order to maximize the expected utility. Krause and Guestrin studied this problem in the
domain of sensor placement and consider a sensor network where the utility of a sensor is determined by the certainty about the measured quantity. The task is to efficiently select the most informative subsets of observations. Specifically, they propose optimal nonmyopic value of information
in chain graphical models (Krause & Guestrin, 2009). Bilgic and Getoor (2011) address a similar
269

fiK ALECH & R ECHES

problem for efficiently acquiring classification features in domains in which costs are associated
with acquisition. The objective is to minimize the sum of the information acquisition cost. They
propose a data structure known as the value of information lattice (VOILA). VOILA exploits dependencies between missing features, making it possible to share information value computations
between different feature subsets.
Similarly, another work (Tolpin & Shimoni, 2010; Radovilsky & Shimoni, 2010) deals with
selection under uncertainty and develops algorithms based on the value of information (VOI) with a
semi-myopic approximation scheme for problems with real-valued utilities. In particular, Tolpin and
Shimoni (2010) interpret VOI as the expected difference between the expected utility of a meta-level
action and the expected utility of the current base-level action. Radovilsky and Shimoni (2010) deal
with optimizing the selection of a set of observations. Their aim is to bring an objective function to
optimum while taking into consideration the cost of observation and the remaining uncertainty after
executing the observation.
Recently, Chen et al. (2014) proposed to use the computation of the Same-decision Probability
(SDP) in order to compute whether additional information should be gathered. In particular, they
compute a stopping criterion by computing SDP, the SDP is the probability that the same decision
will be made even with further observations. If more information should be gathered they propose
which pieces of information to gather next.
Our work is also related, in some aspects, to Horvitzs work (2001, 2013) on decision making
under bounded resources. The execution of a task is associated with a utility and a cost, depending
on resources. When the resources are bounded, the question is which stopping point is the best that
will, for the most part, satisfy the task. Horvitz presents the use of an expected value of computation
to determine the best time to stop. Similarly, monitoring anytime algorithms (Boddy & Dean,
1994; Zilberstein, 1996; Zhang, 2001) search for the best possible answer under the constraint of
limited time and/or resources. A major question that arises in utilizing this class of algorithms is
how to optimally decide when to stop. For instance, Finkelstein and Markovitch (2001) developed
algorithms that design an optimal query schedule to detect when a given goal has been fulfilled.
Their aim was to minimize the number of queries (which are time consuming) to reach the goal.
The joint objective of the above works is to maximize a goal function while considering the cost
of the observations/acquisition/actions and their extent of uncertainty. We also attempt to maximize
the utility by choosing the best candidate and we consider the cost and uncertainty of the timed
variables. However, our work differs from the above works in two significant aspects:
1. Time and variables: some of the previous studies such as change detection problem and
K-Armed bandit problem consider a set of observations without considering their order. Most
works do not consider the time of the observations. In our work the variables are associated with
time and their selection is dynamically determined according to the time progress and the outcomes
of the previous variables. This point is important due to the fact that the candidates may be influenced by disjoint variables. We cannot order the variables according to the time and select a subset
of variables since the available variables at time t depend on the assignment at time t  1. Thus, the
decision of whether to stop or wait depends on assignments that will occur at the next time period.
Since each candidate may be affected by different assignments we should consider the combination
of all assignments which complicates our problem and makes it dissimilar to previous work.
2. Finite and small horizon: in our model the utility of a candidate is affected by a finite and
small set of discrete random variables. As a result, the decision maker can actually achieve absolute
information about the optimal choice by waiting until the last time stamp. At the last time stamp,
270

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

there is complete knowledge about the assignments of all the random variables which explore the
exact value of the utility. Due to the cost of the information the research problem in our model is
to determine the optimal time to make the decision before reaching the end. In the related research,
on the other hand, the basic assumption is that only partial information can be obtained and thus
it is impossible to compute the exact utility of each candidate. The obtained information contains
some observations or samples about the different alternatives that can help the decision maker to
statistically approximate the utility distribution of the different alternatives. The potential population
of the observations may be infinite or very large and thus it is impossible in practice to obtain all
the information necessary to compute the exact utility. The research problem of the above models
is thus to use statistical methods to decide on the required information of the different alternatives.

8. Summary and Future Work
In this paper we presented the problem of decision making among multiple candidates where the
information arrives dynamically. We focused on the question of when to stop and make a decision
that maximizes the utility while taking into consideration the cost of waiting. We presented three
algorithms; an optimal algorithm that is exponential in the number of candidates and two alternative
polynomial approximation algorithms. We proved that one approximation algorithm is optimistic.
Namely its computation of the expected utility from waiting is equal to, or higher than, the expected
utility computed by an optimal algorithm, while the other algorithm is pessimistic and thus stops
earlier. An empirical evaluation of our algorithms showed that the cost function much influences
on the results. As the cost function increases more moderately (a root function), the PESSIMISTIC
algorithm becomes less effective and the OPTIMISTIC becomes better than the PESSIMISTIC.
For polynomial cost functions there is no significant difference between the outcome utility of the
optimal algorithm and the pessimistic algorithm. We also illustrated the exponential growth of the
optimal algorithm and the polynomial growth of the optimistic and the pessimistic algorithms.
In the future we plan to continue in two directions: 1) In this work we assumed discrete variables, however in practice, these variables may be continuous. One option for solving this problem
is to discretize the variables, however, by doing so we lose optimality. We plan to find an optimal way to address this question, and 2) we plan to further investigate the problem presented in
this paper in domains involving multi-agent decision making. In these domains multiple agents
should share the same decision based on different variables and utilities. A multi-agent version of
our approximation grows exponentially in the number of agents and thus we plan to reduce this
complexity.

Appendix A. TDM problem is NP-hard
Proof: We present a reduction from the 3-SAT problem (Cook, 1971). An instance of 3-SAT is
 
given by a propositional logic formula (z1 , ..., zn ) = 1 ... k , where each i is a clause with a
disjunction of exactly three literals. The aim being to answer yes if there is some assignment to
the Boolean variables z1 , ..., zn that satisfies the formula. We construct an instance of T DM as
follows.

1. For each Boolean variable zi we create a timed variable Xi .
271

fiK ALECH & R ECHES

Figure 36: Structure of the candidate trees as accordance with the next 3-SAT formula:
(z1 , z2 , z3 , z4 ) = (z1  z2  z3 )  (z1  z3  z4 )  (z1  z2  z4 ).

2. For every clause j j  {1, ..., k} we create a candidate tree ctj with three timed
variables:Xj1 , Xj2 and Xj3 , corresponding to the variables in that clause. For example, for
the clause j = (z1  z4  z5 ) we create a candidate tree ctj with variables: X1 , X4 and
X5 .
3. The root of each candidate tree ctj includes additional timed variable Yj , where its time stamp
is (Yj ) = 1 with three possible assignments. The probability of each assignment is 13 and
each one of the assignments leads to one of the above timed variables:Xj1 , Xj2 and Xj3 .
4. Every timed variable Xi that corresponds to a literal zi has only two possible assignments
Xi = 1 with utility a > 0 and Xi = 0 with utility 2a, each of them with probability 0.5 and
(Xi ) = 2. Each timed variable which is corresponding to a literal zi has also two possible
assignments Xi = 1 with utility 2a > 0 and Xi = 0 with utility a, each one of them with
probability of 0.5 and (Xi ) = 2.
Figure 36 presents an example to the structure of the candidate trees as accordance with 3SAT formula. A left outgoing edge of a random variable Xi represents the assignment Xi = 1
and a right outgoing edge represents the assignment Xi = 0.
5. We set the time horizon T = [0, 2].
6. We set the cost function to be: CST (t) = 0.1  a  t.
7. We set a constant C such that C = 1.8a.
We now prove that there exists a policy  such that the global expected gain GEG(CT,  0 , ) 
C, if and only if (x1 , ..., xn ) is not satisfiable.
The expected utility from stopping at time t = 0 as well as at t = 1 is exactly 3a
2 which is less
than C. But the highest expected utility of 2a can be obtained by waiting to time stamp 2. The
expected gain is then less than or equal to 1.8a (after considering the cost of waiting). Therefore, in
the rest of the proof we will consider the waiting policy that waits to time stamp 2.
1. To guarantee GEG(CT,  0 , )  C at time stamp 2, we must confirm that for each assignment
and for each combination between the trees branches, at least one candidate tree has a utility
of 2a.
272

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

2. By construction, this may happens if and only if for all assignments there is at least one
candidate tree in which all its utilities are 2a.
3. By construction, in all candidate trees an assignment that guarantees a utility of 2a entails at
least one false clause, meaning all literals in the clause obtain 0 and as a result (z1 , ..., zn )
is not satisfiable.
As a result we obtain that Timed Decision Making (TDM) is NP-hard.2

References
Basseville, M., & Nikiforov, I. V. (1993). Detection of abrupt changes: theory and application.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Beaver, B. M., & Mendenhall, W. (1983). Introduction to probability and statistics, sixth edition,
William Mendenhall, study guide. Statistics Series. Duxbury Press.
Bilgic, M., & Getoor, L. (2011). Value of information lattice: Exploiting probabilistic independence
for effective feature subset acquisition. Journal of Artificial Intelligence Research (JAIR), 41,
6995.
Boddy, M., & Dean, T. L. (1994). Deliberation scheduling for problem solving in time-constrained
environments. Artificial Intelligence, 67(2), 245285.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming with factored representations. Artificial Intelligence, 121, 49107.
Chen, S. J., Choi, A., & Darwiche, A. (2014). Algorithms and applications for the same-decision
probability. Journal of Artificial Intelligence Research (JAIR), 49, 601633.
Cheng, X., & Riedel, F. (2010). Optimal stopping under ambiguity in continuous time. Working
papers 429, Bielefeld University, Institute of Mathematical Economics.
Cheridito, P., & Stadje, M. (2009). Time-inconsistency of var and time-consistent alternatives.
Finance Research Letters, 6(1), 4046.
Cicirello, V. A., & Smith, S. F. (2005). The max k-armed bandit: A new model of exploration
applied to search heuristic selection. In Veloso, M. M., & Kambhampati, S. (Eds.), AAAI, pp.
13551361.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In STOC 71: Proceedings
of the third annual ACM symposium on Theory of computing, pp. 151158, New York, NY,
USA. ACM.
da Rocha, V. F. M., & Riedel, F. (2010). On equilibrium prices in continuous time. Journal of
Economic Theory, 145(3), 10861112.
Dayanik, S., Poor, H. V., & Sezer, S. O. (2007). Multisource bayesian sequential change detection.
CoRR, abs/0708.0224.
Ferguson, T. S. (1989). Who solved the secretary problem?. Statistical Science, 4(3), 282289.
Finkelstein, L., & Markovitch, S. (2001). Optimal schedules for monitoring anytime algorithms.
Artificial Intelligence, 126, 63108.
273

fiK ALECH & R ECHES

Frazier, P., & Powell, W. (2008). The knowledge-gradient stopping rule for ranking and selection.
In Simulation Conference, 2008. WSC 2008. Winter, pp. 305312.
Gilboa, I., & Schmeidler, D. (1989). Maxmin expected utility with non-unique prior. Journal of
Mathematical Economics, 18(2), 141153.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms for
factored MDPs. Journal of Artificial Intelligence Research (JAIR), 19, 399468.
Horvitz, E. (2001). Principles and applications of continual computation. Artificial Intelligence,
126, 1261.
Horvitz, E. (2013). Reasoning about beliefs and actions under computational resource constraints.
CoRR, abs/1304.2759.
Kalech, M., & Pfeffer, A. (2010). Decision making with dynamically arriving information. In
van der Hoek, W., Kaminka, G. A., Lesperance, Y., Luck, M., & Sen, S. (Eds.), AAMAS, pp.
267274.
Katehakis, M., & Veinott, J. A. (1987). The multi-armed bandit problem: decomposition and computation. Mathematics of Operations Research, 12(2), 262268.
Krause, A., & Guestrin, C. (2009). Optimal value of information in graphical models. Journal of
Artificial Intelligence Research (JAIR), 35, 557591.
Lippman, S. A., & McCall, J. J. (1976). The economics of job search: A survey. Economic Inquiry,
14(3), 155189.
Peskir, G., & Shiryaev, A. (2006). Optimal Stopping and Free-Boundary Problems. Birkhauser
Basel.
Powell, W., & Ryzhov, I. (2012). Optimal Learning. Wiley Series in Probability and Statistics.
Wiley.
Radovilsky, Y., & Shimoni, S. E. (2010). Observation subset selection for optimization under uncertainty. Tech. rep., Lynne and William Frankel Center for Computer Science at Ben Gurion
University of the Negev.
Reches, S., Kalech, M., & Stern, R. (2011). When to stop? that is the question. In Burgard, W., &
Roth, D. (Eds.), AAAI. AAAI Press.
Riedel, F. (2009). Optimal stopping with multiple priors. Econometrica, 77(3), 857908.
Streeter, M. J., & Smith, S. F. (2006). An asymptotically optimal algorithm for the max k-armed
bandit problem. In AAAI, pp. 135142. AAAI Press.
Swisher, J. R., Jacobson, S. H., & Yucesan, E. (2003). Discrete-event simulation optimization
using ranking, selection, and multiple comparison procedures: A survey. ACM Trans. Model.
Comput. Simul., 13(2), 134154.
Tolpin, D., & Shimoni, S. E. (2010). Semi-myopic observation selection for optimization under
uncertainty. Tech. rep. 10-01, Lynne and William Frankel Center for Computer Science at
Ben Gurion University of the Negev.
Wald, A., & Wolfowitz, J. (1948). Optimum Character of the Sequential Probability Ratio Test.
Annals of Mathematical Statistics, 19(3), 326339.
274

fiD ECISION M AKING WITH DYNAMIC U NCERTAIN E VENTS

Williams, D. (1991). Probability with Martingales. Cambridge mathematical textbooks. Cambridge
University Press.
Zhang, W. (2001). Iterative state-space reduction for flexible computation. Artificial Intelligence,
126(1-2), 109138.
Zilberstein, S. (1996). Using anytime algorithms in intelligent systems. AI Magazine, 17(3), 7383.

275

fiJournal of Artificial Intelligence Research 54 (2015) 493-534

Submitted 05/15; published 12/15

Possible and Necessary Winners of Partial Tournaments
Haris Aziz

haris.aziz@nicta.com.au

Data61 and University of New South Wales
Australia

Markus Brill

brill@cs.duke.edu

Computer Science Department
Duke University, USA

Felix Fischer

fischerf@math.tu-berlin.de

Institut fur Mathematik
Technische Universitat Berlin, Germany

Paul Harrenstein

paul.harrenstein@cs.ox.ac.uk

Computer Science Department
University of Oxford, UK

Jerome Lang

lang@lamsade.dauphine.fr

LAMSADE
Universite Paris-Dauphine, France

Hans Georg Seedig

seedigh@in.tum.de

Institut fur Informatik
Technische Universitat Munchen, Germany

Abstract
We study the problem of computing possible and necessary winners for partially specified weighted and unweighted tournaments. This problem arises naturally in elections with
incompletely specified votes, partially completed sports competitions, and more generally
in any scenario where the outcome of some pairwise comparisons is not yet fully known.
We specifically consider a number of well-known solution conceptsincluding the uncovered set, Borda, ranked pairs, and maximinand show that for most of them, possible and
necessary winners can be identified in polynomial time. These positive algorithmic results
stand in sharp contrast to earlier results concerning possible and necessary winners given
partially specified preference profiles.

1. Introduction
Many multi-agent situations can be modeled and analyzed using weighted or unweighted
tournaments. Prime examples are voting scenarios in which pairwise comparisons between
alternatives are decided by majority rule and sports competitions that are organized as
round-robin tournaments. Other application areas include webpage and journal ranking,
biology, psychology, and AI. More generally, tournaments and solution concepts on tournaments are used as a mathematical tool for the analysis of all kinds of situations where
a choice among a set of alternatives has to be made exclusively on the basis of pairwise
comparisons.
When choosing from a tournament, relevant information may only be partly available.
This could be because some preferences are yet to be elicited, some matches yet to be played,
c
2015
AI Access Foundation. All rights reserved.

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

or certain comparisons yet to be made. In such cases, it is natural to speculate which are
the potential and inevitable outcomes on the basis of the information already at hand.
For tournaments, a number of attractive solution concepts have been proposed (Brandt,
Brill, & Harrenstein, 2016; Laslier, 1997). Given any such solution concept S, we define
possible winners of a partial tournament G as alternatives that are selected by S in some
completion of G, and necessary winners are alternatives that are selected in all completions.
By a completion we here understand a (complete) tournament extending G.
In this article we address the computational complexity of identifying the possible and
necessary winners for a number of solution concepts whose winner determination problem for tournaments is tractable. We consider five of the most common solution concepts
for tournamentsnamely, Condorcet winners (COND), Condorcet non-losers (CNL), the
Copeland set (CO), the top cycle (TC ), and the uncovered set (UC )and three common
solutions for weighted tournamentsBorda (BO), maximin (MM ), and ranked pairs (RP ).
For each of these solution concepts, we consider the computational complexity of the following problems: deciding whether a given alternative is a possible winner (PW), deciding
whether a given alternative is a necessary winner (NW), as well as deciding whether a
given subset of alternatives equals the set of winners (the winning set) in some completion (PWS). These problems can be challenging, as even unweighted partial tournaments
may allow for an exponential number of completions. Our results are encouraging, in the
sense that most of the problems can be solved in polynomial time. Table 1 summarizes our
findings.
Similar problems have been considered before. For Condorcet winners, voting trees
and the top cycle, it has been shown that possible and necessary winners are computable
in polynomial time (Konczak & Lang, 2005; Lang et al., 2012). The same holds for the
computation of possible Copeland winners, a problem that has been considered in the
context of sports tournaments (Cook, Cunningham, Pulleyblank, & Schrijver, 1998).
Another more specific setting is also frequently considered within the area of computational social choice and differs from our setting in a subtle but important way that is
worth being pointed out. There, tournaments are assumed to arise from pairwise majority
comparisons on the basis of a profile of individual voters preferences.1
Since a partial preference profile R need not conclusively settle every majority comparison, it may give rise to a partial tournament only. There are then two natural ways to define
possible and necessary winners for a partial preference profile R and solution concept S as
illustrated in Figure 1. The first is to consider the completions of R and the winners under
S in the corresponding tournaments. The secondcovered by our more general settingis
to consider the completions of the partial tournament G(R) corresponding to R and the
winners under S in these. Since every tournament corresponding to a completion of R is
also a completion of G(R) but not necessarily the other way round, the second definition
1. See, e.g., the work of Baumeister and Rothe (2010), Betzler and Dorn (2010), Konczak and Lang (2005),
Walsh (2007), and Xia and Conitzer (2011) for the basic setting, Betzler, Hemmann, and Niedermeier
(2009) for parameterized complexity results, Bachrach, Betzler, and Faliszewski (2010), Hazon, Aumann,
Kraus, and Wooldridge (2012), and Kalech, Kraus, Kaminka, and Goldman (2011) for probabilistic
settings, Chevaleyre, Lang, Maudet, and Monnot (2011) and Chevaleyre, Lang, Maudet, Monnot, and
Xia (2012) for settings with a variable set of alternatives, Baumeister, Faliszewski, Lang, and Rothe
(2012), Kalech et al. (2011), Lu and Boutilier (2011), Oren, Filmus, and Boutilier (2013), Filmus and
Oren (2014) for settings with truncated ballots and Lu and Boutilier (2013) for multiwinner rules.

494

fiPossible and Necessary Winners of Partial Tournaments

S

PWS

COND
CNL
CO
TC
UC

in
in
in
in
in

BO
MM
RP

in P (Thm. 8)a
in P (Thm. 11)a
NP-C (Thm. 14)

a

P
P
P
P
P

NWS
(Konczak & Lang, 2005)
(Thm. 2)
(Cook et al., 1998)a
(Lang et al., 2012)a
(Thm. 5)

in
in
in
in
in

P
P
P
P
P

PWSS
(Konczak & Lang, 2005) in P
(Thm. 2)
in P
(Thm. 3)a
in P
(Lang et al., 2012)
in P
(Thm. 6)
NP-C

in P
(Thm. 10)
in P
(Thm. 13)
coNP-C (Thm. 15)

(Thm.
(Thm.
(Thm.
(Thm.
(Thm.

1)
2)
3)
4)
7)

in P (Thm. 9)
in P (Thm. 12)
NP-C (Cor. 2)

This P-time result contrasts with the intractability of the same problem for partial preference
profiles (Lang et al., 2012; Xia & Conitzer, 2011).

Table 1: Complexity of computing possible winners (PW) and necessary winners (NW) and
of checking whether a given subset of alternatives is a possible winning set (PWS) under
the following solution concepts: Condorcet winners (COND), Condorcet non-losers (CNL),
Copeland (CO), top cycle (TC ), uncovered set (UC ), Borda (BO), maximin (MM ), and
ranked pairs (RP ).
gives rise to a stronger notion of a possible winner and a weaker notion of a necessary winner. Interestingly, and in sharp contrast to our results, determining these stronger possible
and weaker necessary winners is computationally hard for many voting rules (Lang et al.,
2012; Xia & Conitzer, 2011). This contrast has been foreshadowed by the work of Lang et
al. (2012) and Pini, Rossi, Venable, and Walsh (2011), who compared these two ways of
defining possible and necessary winners (both theoretically and experimentally) for three
solution concepts: Condorcet winners, voting trees, and the top cycle.
In the context of this article, we do not assume that tournaments arise from majority
comparisons in voting or from any other specific procedure. This approach has a number
of advantages. Firstly, it matches the diversity of settings to which solution concepts on
tournaments are applicable, which goes well beyond social choice and voting. For instance,
our results also apply to a question commonly encountered in sports competitions, namely,
which teams can still win the cup and which future results this depends on (Cook et al.,
1998; Kern & Paulusma, 2004; B. L. Schwartz, 1966). Secondly, (partial) tournaments
provide an informationally sustainable way of representing the relevant aspects of many
situations while maintaining a workable level of abstraction and conciseness. For instance,
in the social choice setting described above, the partial tournament induced by a partial
preference profile is a much more succinct piece of information, and discloses less information, than the preference profile itself. More generally, it gives a canonical way of extending
tournament solutions to incomplete tournaments (a line that has been pursued by Brandt,
Brill, & Harrenstein, 2014). Finally, specific settings may impose restrictions on the feasible
extensions of partial tournaments. The positive algorithmic results in this article can be
used to efficiently approximate the sets of possible and necessary winners in such settings,
where the corresponding problems may be intractable. The voting setting discussed above
serves to illustrate this point.
We also point out that computing possible outcomes has been considered in other domains in social choice for example in matching and allocations (Aziz, Walsh, & Xia, 2015;
495

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

1 1 1
a

a c
a
b b
b
c a

completions

completions
1 1 1

a

a c a
b b b
c a c
1 1 1
1 1 1
a c a
b b c
c a b

c

b

b
a

a c c
b b a
c a b

b

a

c
b

a
c

b

c

c

Figure 1: This non-commutative diagram illustrates the two approaches to possible and
necessary winners of partial preference profiles for majoritarian social choice functions.
First, the completions of the partial profile to full preference profiles are shown in the
bottom left. The corresponding majority tournaments are in the dashed box on the bottom
right. In this work, we start from the partial majority tournament on the top right which
is induced by the partial preference profile. Then, we consider all possible completions to
tournaments which are depicted in the solid box on the bottom right.

Rastegari, Condon, Immorlica, & Leyton-Brown, 2013) and for knockout tournaments (Aziz
et al., 2014; Vu, Altman, & Shoham, 2009).

2. Preliminaries
A partial tournament is a pair G = (V, E) where V is a nonempty finite set of alternatives
and E  V  V an asymmetric relation on V , i.e., (y, x) 
/ E whenever (x, y)  E. If
(x, y)  E we say that x dominates y. A tournament T is a partial tournament (V, E) for
which E is also complete, i.e., either (x, y)  E or (y, x)  E for all distinct x, y  V . We
denote the set of all tournaments by T .
Let G = (V, E) be a partial tournament. Another partial tournament G0 = (V 0 , E 0 ) is
called an extension of G, denoted G  G0 , if V = V 0 and E  E 0 . If E 0 is complete, G0 is
called a completion of G. We write [G] for the set of completions of G, i.e.,
[G] = {T  T : G  T }.
496

fiPossible and Necessary Winners of Partial Tournaments

We say an alternative x  V is dominated if (y, x)  E for some y  V , and undominated
+
otherwise. We define the dominion of x in G as DG
(x) = {y  V : (x, y)  E}, and the

+
dominators of x in G as DGS
(x) = {y  V : (y, x)  E}. For X  V , we let DG
(X) =
S
+


D
(x)
and
D
(X)
=
D
(x).
A
nonempty
subset
X

V
of
alternatives
in
xX
xX
G
G
G
a partial or complete tournament (V, E) is dominant if every alternative in X dominates
every alternative outside X. For given G = (V, E) and X  V , we further write E X for
the set of edges obtained from E by adding all missing edges from alternatives in X to
alternatives not in X, i.e.,
E X = E  {(x, y)  X  V : y 
/ X and (y, x) 
/ E}.
We use E X as an abbreviation for E V \X , and write E x , E x , GX , and GX for
E {x} , E {x} , (V, E X ), and (V, E X ), respectively. For G = (V, E) and X  V , we
use E|X and G|X to denote the restriction E(XX) of E to X and the restriction (X, E|X )
of G to X, respectively.
Let n be a positive integer. A partial n-weighted tournament is a pair G = (V, w)
consisting of a finite set V of alternatives and a weight function w : V  V  {0, . . . , n}
such that for each pair (x, y)  V  V with x 6= y, w(x, y) + w(y, x)  n. We say that
T = (V, w) is an n-weighted tournament if for all x, y  V with x 6= y, w(x, y) + w(y, x) = n.
We call T a (partial) weighted tournament if it is a (partial) n-weighted tournament for
some n  N. The class of n-weighted tournaments is denoted by Tn . Observe that with
each partial 1-weighted tournament (V, w) we can associate a partial tournament (V, E) by
setting E = {(x, y)  V : w(x, y) = 1}. Thus, (partial) n-weighted tournaments can be seen
to generalize (partial) tournaments, and we may identify T1 with T .
The notations G  G0 and [G] can be extended naturally to partial n-weighted tournaments G = (V, w) and G0 = (V 0 , w0 ) by letting (V, w)  (V 0 , w0 ) if V = V 0 and
w(x, y)  w0 (x, y) for all x, y  V , and [G] = {T  Tn : G  T }.
For given G = (V, w) and X  V , we further define wX such that for all x, y  V ,

w

X

(
n  w(y, x) if x  X and y 
/ X,
(x, y) =
w(x, y)
otherwise,

and set wX = wV \X . Moreover, wx , wx , GX , and GX are defined in the obvious
way.
We use the term solution concept for functions S that associate with each tournament T = (V, E), or with each weighted tournament T = (V, w), a choice set S(T )  V .2 A
solution concept S is called resolute if |S(T )| = 1 for each tournament T . In this article we
will consider the following solution concepts: Condorcet winners (COND), Condorcet nonlosers (CNL), Copeland (CO), top cycle (TC ), and uncovered set (UC ) for tournaments,
and maximin (MM ), Borda (BO), and ranked pairs (RP ) for weighted tournaments. Of
these only ranked pairs is resolute. Formal definitions will be provided later in the article.
2. We avoid the otherwise natural term tournament solution as in its common definition it requires the
choice set to be nonempty (Laslier, 1997). This would exclude COND.

497

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

3. Possible and Necessary Winners
A solution concept selects a unique set of alternatives from each complete tournament.
This holds in particular for the completions of a partial tournament. However, for each
completion of a partial tournament, a solution concept may select another set of alternatives. A similar remark concerns weighted tournaments and their completions. For a given
solution concept S, we can thus define the set of possible winners for a partial (weighted)
tournament G as the set of alternatives selected by S from some completion of G, i.e., as
[
S(T ).
PWS (G) =
T [G]

Analogously, the set of necessary winners of G is the set of alternatives selected by S from
every completion of G, i.e.,
\
NWS (G) =
S(T ).
T [G]

We furthermore write
PWSS (G) = {S(T ) : T  [G]}
for the possible winning sets, i.e., the set of sets of alternatives that S selects for the
different completions of G. For the sake of completeness, we also mention necessary winning
sets. A set X is a necessary winning set of a partial tournament G if X = S(T ) for
all T  [T ]. Accordingly, the conditions for a set to be a necessary winning set are very
strong and are satisfied relatively seldom. Necessary winning sets can also straightforwardly
be characterized by means of the sets of possible and necessary winners: X is a necessary
winning set if and only if X = PW (G) = NW (G). This implies that for the solution
concepts addressed in this article, computational results surrounding necessary winning
sets follow as easy corollaries.3 We will not further consider necessary winning sets.
Note that NWS (G) may be empty even if S selects a nonempty set of alternatives for
each tournament T  [G], and that the number |PWSS (G)| of possible winning sets may
be exponential in the number of alternatives of G.
We have the following lemmas, which relate to some useful structural properties of
the sets of possible and necessary winners. The proofs are straightforward and therefore
omitted.
Lemma 1. Let S be a solution concept and G and G0 partial tournaments. Then,
(i) G  G0 implies PWS (G0 )  PWS (G), and
(ii) G  G0 implies NWS (G)  NWS (G0 ).
We say that a solution concept S refines another solution concept S 0 , denoted S  S 0 , if
S(G)  S 0 (G) for all G. We find that the following monotonicity properties hold.
3. Given our results in Table 1, the fact that X is a necessary winning set if and only if X = PW (G) =
NW (G) immediately implies that for all concepts apart from ranked pairs deciding whether a set is a
necessary winning set can be achieved in polynomial time. Since ranked pairs is resolute, every set in
NWSRP has to be a singleton {x} and {x}  PWSRP if and only if x  NWRP . Consequently, the
problem of deciding whether a set X is contained in NWSRP is coNP-complete.

498

fiPossible and Necessary Winners of Partial Tournaments

Lemma 2. Let S and S 0 be solution concepts and G and G0 partial tournaments. Then,
(i) S  S 0 implies PWS (G)  PWS 0 (G), and
(ii) S  S 0 implies NWS (G)  NWS 0 (G).
The next lemma concerns the way in which the sets of possible and necessary winners can
be defined in terms of one another.
Lemma 3. Let S be a solution concept and G a partial tournament. Then,
S
(i) PWS (G) = GG0 NWS (G0 ), and
T
(ii) NWS (G) = GG0 PWS (G0 ).
Observe that, while S  S 0 does not generally imply PWSS (G)  PWSS 0 (G), the following
does hold:
if S  S 0 then for all X  PWSS (G) there exists X 0  PWSS 0 (G) such that X  X 0 .
Deciding membership in the sets PWS (G), NWS (G), and PWSS (G) for a given solution
concept S and a partial (weighted) tournament G are natural computational problems.
Overloading notation, we refer to these problems as PWS , NWS , and PWSS , respectively.
PWS (Possible Winners)
Input:
A partial tournament G = (V, E) or an n-weighted partial tournament
G = (V, w) along with the positive integer n; an alternative x  V .
Output: Yes, if there exists a completion T  [G] such that x  S(T ).
No, otherwise.

NWS (Necessary Winners)
Input:
A partial tournament G = (V, E) or an n-weighted partial tournament
G = (V, w) along with the positive integer n; an alternative x  V .
Output: Yes, if x  S(T ) for all completions T  [G].
No, otherwise.

PWSS (Possible Winning Set)
Input:
A partial tournament G = (V, E) or an n-weighted partial tournament
G = (V, w) along with the positive integer n; a subset of alternatives
X V.
Output: Yes, if there exists a completion T  [G] such that X = S(T ).
No, otherwise.
Note that if PWSS can be decided in polynomial time, this only means that there is a
polynomial-time algorithm that decides whether a given subset of alternatives is a possible
499

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

winning set. Outputting the set PWSS of possible winning sets may be much more difficult,
because PWSS may be of exponential size.4
For irresolute solution concepts, PWSS may appear a more complex problem than
PWS . We are, however, not aware of a generic polynomial-time reduction from PWS to
PWSS . The relationship between all of these problems may also be of interest for the
classic possible winner setting with partial preference profiles.
For complete tournaments T we have [T ] = {T } and thus PWS (T ) = NWS (T ) = S(T )
and PWSS (T ) = {S(T )}. As a consequence, for solution concepts S with an NP-hard
winner determination problemlike Banks, Slater, and the tournament equilibrium set
the problems PWS , NWS , and PWSS are NP-hard as well.5 We therefore restrict our
attention to solution concepts for which winners can be computed in polynomial time.

4. Unweighted Tournament Solutions
In this section, we consider the following well-known solution concepts for unweighted tournaments: Condorcet winners, Condorcet non-losers, the Copeland set, the top cycle, and
the uncovered set. We will use the partial tournament depicted in Figure 2(i ) as a running
example.
a

b

a

b

a

b

c

d

c

d

c

d

(i ) Partial tournament G

(ii ) Completion T1

(iii ) Completion T2

Figure 2: Example of a partial unweighted tournament G and possible completions T1
and T2 . Initially, the (dotted) edges between the pairs {a, b}, {b, c}, and {c, d} are not yet
specified.

4.1 Condorcet Winners and Condorcet Non-losers
Condorcet winners and Condorcet non-losers are fundamental solution concepts and will
provide a nice warm-up. An alternative x  V is a Condorcet winner of a complete tournament T = (V, E) if it dominates all other alternatives, i.e., if (x, y)  E for all y  V \ {x}.
The set of Condorcet winners of tournament T will be denoted by COND(T ); obviously
this set is always either a singleton or empty. An alternative x is a Condorcet loser in T if it
is dominated by every other alternative, i.e., if (y, x)  E for all y  V \ {x}. Consequently,
x is a Condorcet non-loser in T = (V, E) if x is not a Condorcet loser in T or V = {x}.
The set of Condorcet non-losers of a tournament T will be denoted by CNL(T ); obviously
this set has always cardinality |V | or |V |  1.
4. For instance, if G = (V, ) then PWS TC (G ) = {X  V : |X| 6= 2}, even though PWSTC is in P
(Theorem 4).
5. This does not exclude the possibility that computing some (arbitrary) possible winner or possible winning
set for some of these solution concepts could be done in polynomial time.

500

fiPossible and Necessary Winners of Partial Tournaments

Let G = (V, E) be a partial tournament. If some alternative x is dominant in G, then x
will obviously be the Condorcet winner in all completions of G. On the other hand, if for
some y  V \ {x} it is not the case that (x, y)  E, there is some completion of G in which x
is not a Condorcet winner. Hence,
x  NWCOND (G) if and only if (x, y)  E for all y  V \ {x}
and
x  PWCOND (G) if and only if (y, x)  E for no y  V \ {x}.
Obviously, the criteria on the right-hand side of the equivalences can be checked in polynomial time.
We now turn to the problem PWSCOND . Each of the sets in PWSCOND (G) is either
a singleton or the empty set, and determining membership for a singleton is obviously
tractable. Checking whether   PWSCOND (G) is not quite that simple. The following
result gives an exact characterization of PWSCOND (G), which is interesting in its own
right.
Lemma 4. Let U be the set of undominated alternatives of a partial tournament G = (V, E).
Then,
 for every alternative x  V , {x}  PWSCOND (G) if and only if x  U ;
  6 PWSCOND (G) if and only if 1  |U |  2 and U is dominant.
Proof. Since a complete tournament has either one Condorcet winner or none, any set in
PWSCOND (G) has cardinality 0 or 1. Clearly, {x}  PWSCOND (G) if and only if x  U .
It remains to be shown that PWSCOND (G) contains  if and only if U = , or |U |  3, or
1  |U |  2 and U is not dominant.
If U = , COND(T ) =  for every T  [G]. It follows that   PWSCOND (G). If
|U |  3, consider a directed cycle C  U  U that visits every alternative in U .6 Then,
the set of undominated alternatives in G0 = (V, E  C) is empty. It again follows that
  PWSCOND (G).
If U = {x} and x is dominant, then x is a Condorcet winner in every T  [G]. Therefore,

/ PWSCOND (G).
If U = {x} and {x} is not dominant, then (x, y) 
/ E for some y 6= x. Consider a
completion of G containing (y, x). In this completion, the set of undominated alternatives
is empty. It follows that   PWSCOND (G).
If U = {x, y} and {x, y} is dominant, then for every T  [G], either (x, y)  T and x is
a Condorcet winner in T , or (y, x)  T and y is a Condorcet winner in T . It follows that

/ PWSCOND (G).
Finally, if U = {x, y} and {x, y} is not dominant, then for some z 6= x, y we have
(x, z) 
/ E or (y, z) 
/ E. Without loss of generality, assume (x, z) 
/ E. Consider a
completion of G containing (z, x) and (x, y). Such a completion exists, because (x, z) 
/ E,
and (y, x) 
/ E (since x  U ). In this completion, the set of undominated alternatives is
empty. It follows that   PWSCOND (G).
6. The cycle C is not a subgraph of G. In fact, G|U does not contain any edges.

501

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

We are now in a position to prove the following theorem.
Theorem 1. PWCOND , NWCOND , and PWSCOND can all be solved in polynomial time.
The results for PWCOND and NWCOND also follow from Corollary 2 of Konczak and
Lang (2005).
We further note that Theorem 1 is a corollary of corresponding results for maximin
in Section 5.2. The reason is that a Condorcet winner is the maximin winner of a 1weighted tournament, and a tournament does not admit a Condorcet winner if and only if
all alternatives are maximin winners.
We conclude this section by observing that the problems PWCNL , NWCNL , and
PWSCNL are reducible to NWCOND , PWCOND , and PWSCOND , respectively. It can
straightforwardly be checked that for all partial tournaments G = (V, E) with |V | > 1
and all X  V ,
X  PWSCNL (G) if and only if V \ X  PWSCOND (G1 ),
where G1 = (V, E 1 ) is G with all of its set edges inverted, i.e., E 1 = {(x, y) : (y, x)  E}.
It also follows that,
PWCNL (G) = V \ NWCOND (G1 ), and
NWCNL (G) = V \ PWCOND (G1 ).
Since the complement of a set can be computed in polynomial time and edges can
be reversed in polynomial time as well, we obtain the following result as a corollary of
Theorem 1.
Theorem 2. PWCNL , NWCNL , and PWSCNL can all be solved in polynomial time.
As an example, consider the partial tournament G depicted in Figure 2(i ) in which there
is no dominating alternative while the set of undominated alternatives in G is U = {a, b}.
Therefore,
PWCOND (G) = {a, b} and
NWCOND (G) = .
For PWSCOND (G), note that the set U is not dominant because (b, c) 
/ E. By Lemma 4,
this gives
PWSCOND (G) = {{a}, {b}, }.
For Condorcet non-losers, we observe that G1 = (V, E 1 ) with E 1 = {(c, a), (d, a), (d, b)}.
Now, PWCOND (G1 ) = {c, d}, NWCOND (G1 ) = , and (from Lemma 4)
PWSCOND (G1 ) = {{c}, {d}, }. Therefore,
PWCNL (G) = {a, b, c, d},
NWCNL (G) = {a, b}, and
PWSCNL (G) = {{a, b, d}, {a, b, c}, {a, b, c, d}}.

502

fiPossible and Necessary Winners of Partial Tournaments

4.2 Copeland
Copelands solution selects alternatives based on the number of other alternatives they
dominate. Define the Copeland score of an alternative x in tournament T = (V, E) as
sCO (x, T ) = |DT+ (x)|.
The set CO(T ) then consists of all alternatives that have maximal Copeland score.
For an illustrative example, consider again the partial tournament G shown in Figure 2(i ). In completions of G where a (respectively b) is a Condorcet winner, a (respectively b) is the sole Copeland winner as in the completion shown in Figure 2(ii ). The only
two completions in which neither a nor b is a Condorcet winner are
{(a, c), (a, d), (b, a), (b, d), (c, b), (c, d)},
where the set of Copeland winners is {a, b, c}, and
{(a, c), (a, d), (b, a), (b, d), (c, b), (d, c)},
also depicted in Figure 2(iii ), where the set of Copeland winners is {a, b}. Therefore,
PWCO (G) = {a, b, c},
NWCO (G) = , and
PWSCO (G) = {{a}, {b}, {a, b}, {a, b, c}}.
Since Copeland scores coincide with Borda scores in the case of 1-weighted tournaments,
the following is a direct corollary of the results in Section 5.1.7
Theorem 3. PWCO , NWCO , and PWSCO can all be solved in polynomial time.
From PWSCO being solvable in polynomial time, we get the following corollary, which
may be of independent interest to graph theorists.
Corollary 1. There exists a polynomial-time algorithm to check whether a partial tournament admits a regular completion, i.e., a completion in which every alternative has the
same out-degree.
To see this, merely observe that a completion T = (V, E) of a partial tournament is
regular if and only if CO(T ) = V .
4.3 Top Cycle
The top cycle of a tournament T = (V, E), denoted by TC (T ), is the unique minimal
dominant subset of V .
Lang et al. have shown that possible and necessary winners for TC can be computed
efficiently by greedy algorithms (Lang et al., 2012, Corollaries 1 and 2). Still, we give the
following characterization that will prove useful when we come to consider the possible
7. PWCO can alternatively be solved via a polynomial-time reduction to maximum network flow (Cook et
al., 1998, p. 51).

503

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

winning sets under TC . An alternative is a possible TC -winner if and only if it can
reach every other alternative via existing or unspecified edges. Formally, given a partial
tournament G = (V, E), an alternative x  V is in PWTC (G) if and only if for every other
alternative y  V , there exists a path x0 , x1 , x2 , . . . , xk with x = x0 , y = xk , and such that
(xi+1 , xi ) 
/ E for all i  {0, . . . , k  1}. We call such a path a possible path. If a possible
path from x to y exists, we denote that by x
y.
Observe that for any pair a and b of alternatives in a partial tournament G = (V, E),
if there is no possible path from a to b, then (b, a)  E. The set of alternatives that can
reach every other alternative via a possible path in a partial tournament G = (V, E) is also
known as the Good set (Good, 1971) and is denoted by GO(G).8 It follows from the above
that GO(G) is polynomial-time computable. Moreover, we have the following lemma.
Lemma 5. Let G = (V, E) be a partial tournament with |V |  3, GO(G) = V , and x
and y alternatives such that (x, y), (y, x) 
/ E. Let further Gxy = (V, E  {(x, y)}) and
Gyx = (V, E  {(y, x)}). Then, GO(Gxy ) = V or GO(Gyx ) = V .
Proof. Assume for contradiction that both GO(Gxy ) and GO(Gyx ) are strict subsets
of V . Clearly, we have x  GO(Gxy ) and y  GO(Gyx ). Moreover, we claim that x 
/
GO(Gyx ) and y 
/ GO(Gxy ). To see why x 
/ GO(Gyx ) holds, assume for contradiction
that x  GO(Gyx ). Then, there is a possible path from x to y in Gyx . This path can
be used to replace the edge (x, y) (which is available for possible paths in G, but not
in Gyx ). Therefore, there is a possible path between two alternatives in Gyx whenever
there is one in G. Since GO(G) = V , we have GO(Gyx ) = V as well, contradicting our
assumption. An analogous argument shows that y 
/ GO(Gxy ).
Having established that x  GO(Gxy ) \ GO(Gyx ) and y  GO(Gyx ) \ GO(Gxy ),
we know that there is no possible path from x to y in Gyx , and neither is there a possible
path from y to x in Gxy . Now consider some z  V \ {x, y}. We have that either
(i) (x, z)  E and (y, z)  E, or
(ii) (z, x)  E and (z, y)  E,
as otherwise there would be either a possible path from x to y in Gyx or a possible path
from y to x in Gxy .
If (i), recall that we have assumed that GO(G) = V . Hence, in G there are possible
paths z
x and z
y. Observe that we may assume that either the possible path from z
to x does not contain (y, x), or the possible path from z to y does not contain (x, y). In the
former case, y, z
x is a possible path in Gxy . In the latter case, x, z
y is a possible
yx
path in G
. Thus, either case yields a contradiction.
If (ii), GO(G) = V implies that there are possible paths x
z and y
z in G, and
we may assume that either the possible path from x to z does not contain (y, x) or the
possible path from y to z that does not contain (x, y). In the former case, there is a possible
path x
z, y in Gxy . In the latter case, there is a possible path y
z, x in Gyx . Again,
either case leads to a contradiction. This concludes the proof.
8. Equivalently, the Good set of a partial tournament G = (V, E) is the unique minimal dominant subset
of V . The Good set is also known as the Smith set (Smith, 1973) and GETCHA (T. Schwartz, 1986).

504

fiPossible and Necessary Winners of Partial Tournaments

We are now ready to show that PWSTC can be solved efficiently. Note that we not only
have to check that there exists a completion such that the set in question is dominating,
but also that there is no smaller dominating set.
Theorem 4. PWSTC can be solved in polynomial time.
Proof. Let the set under consideration be X. The set X cannot be empty as T C(T ) 6= 
for every T  [G]. If |X| = 1, then the problem PWSTC is equivalent to PWCOND . If
|X| = 2, then the answer is already no because the top cycle is never of size two. We may
therefore assume that |X|  3.
Consider the graph GX . If X does not dominate V \ X in GX , then X 
/ PWSTC (G)
because an alternative in V \ X beats an alternative in X. Therefore, we now need to check
whether X  PWSTC (G|X ), i.e., whether X is a possible top cycle set in the partial
tournament G restricted to X. In essence, the problem PWSTC is reduced to the restricted
problem PWSTC for the set of all alternatives.
We prove that V  PWSTC (G) if and only if GO(G) = V . Obviously, if V 6= GO(G)
then V 
/ PWSTC (G). For the other direction, we start with a partial tournament G =
(V, E) with GO(G) = V . By iteratively applying Lemma 5, new edges can successively be
added to G while maintaining GO(G) = V until G is a tournament.
As an example, we again consider the partial tournament G depicted in Figure 2(i ), for
which we show that
PWTC (G) = {a, b, c, d},
NWTC (G) = , and
PWSTC (G) = {{a}, {b}, {a, b, c}, {a, b, c, d}}.
The result for PWTC (G) is witnessed by the completion shown in Figure 2(iii ) where every
alternative is in the top cycle. For NWTC (G), the statement follows from the observation that for every alternative, there exists a completion in which another alternative is
a Condorcet winner. Regarding PWSTC (G), we consider each subset separately. Since
PWSCOND  PWSTC , we get that {a} and {b} are in PWSTC (G). For {a, b, c}, we apply
the result shown in the second paragraph of the proof of Theorem 4: a, b, c are undominated
by d, and the Good set of G|{a,b,c} is {a, b, c}. Likewise, the Good set of G is {a, b, c, d}.
It remains to be shown that the other subsets of size three are not in PWSTC (G). To this
end, note that the Good set of G|{a,b,d} is only {a, b} and that neither {a, c, d} nor {b, c, d}
is undominated in G.
4.4 Uncovered Set
Given a tournament T = (V, E), an alternative x  V is said to cover another alternative
y  V if DT+ (y)  DT+ (x), i.e., if every alternative dominated by y is also dominated by x.
The uncovered set of T , denoted UC (T ), then is the set of alternatives that are not covered
by any other alternative. A useful alternative characterization of the uncovered set is via
the two-step principle: an alternative is in the uncovered set if and only if it can reach
every other alternative in at most two steps.9 Formally, x  UC (T ) if and only if for all
9. In graph theory, vertices satisfying this property are often called kings.

505

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

y  V \ {x}, either (x, y)  E or there is some z  V with (x, z), (z, y)  E. We denote the
+
+
++
two-step dominion DE
(DE
(x)) of an alternative x by DE
(x).
We first consider PWUC , for which we check for each alternative whether it can be
reinforced to reach every other alternative in at most two steps.
Theorem 5. PWUC can be solved in polynomial time.
Proof. For a given partial tournament G = (V, E) and an alternative x  V , we check
whether x is in UC (T ) for some completion T  [G].
Consider the graph G0 = (V, E 00 ) where E 00 is derived from E as follows. First, we
let D+ (x) grow as much as possible by letting E 0 = E x . Then, we do the same for its
+
two-step dominion by defining E 00 as E 0DE0 (x) . We claim that x  PWUC (G) if and only
+
++
if V = {x}  DE
00 (x)  DE 00 (x).
() First, let x  PWUC (G). By definition, there is a completion (V, E  ) such that for
+
++
00
all y  V \ {x} we have y  DE
 (x)  DE  (x). But from the definition of E , it follows that
++
+
++
++
+
+
DE  (x)  DE 00 (c) and DE  (x)  DE 00 (x). Consequently, y is also in DE 00 (x)  DE
00 (x).
++
+
() For the other direction, let y  V \ {x}, y  DE 00 (x)  DE 00 (x). In any completion
T of G0 , x is trivially in UC (T ), implying that x  PWUC (G).
A similar argument yields the following.
Theorem 6. NWUC can be solved in polynomial time.
Proof. For a given partial tournament G = (V, E) and an alternative x  V , we check
whether x is in UC (T ) for all completions T  [G].
Consider the graph G0 = (V, E 00 ) with E 00 defined as follows. First, let E 0 = E x . Then,

expand it to E 00 = E 0DE0 (x) . Intuitively, this makes it as hard as possible for x to beat
alternatives outside of its dominion in two steps. We claim that x  NWUC (G) if and only
+
++
if V = {x}  DE
00 (x)  DE 00 (x) or equivalently, if and only if for all y 6= x there is a path
of length one or two from x to y in G.
() First, let x  NWUC (G). Assume for contradiction that there exists a y  V \ {x}
++
+

0
such that y 
/ DE
00 (x)  DE 00 (x). Then, in any completion (V, E ) of G , x cannot reach y

in two steps and consequently x 
/ UC (V, E ), a contradiction.
++
+
In any completion (V, E  ) of G, we
() Now, let V \ {x} = DE
00 (x)  DE 00 (x).
+
+
++
++
have DE 00 (x)  DE  (x) and DE 00 (x)  DE  (x). Consequently, x  UC (V, E  ) and
x  NWUC (G).
+
++
As it can be checked in polynomial time whether V = {x}  DE
00 (x)  DE 00 (x), this
completes the proof.
Consider the partial tournament G from Figure 3(i ) as an example. It can be
checked that NWUC (G) = .10 For PWUC , we consider each alternative separately.
For a, we have E 0 = E a = {(a, b), (a, c), (a, d), (b, d)}, and E 00 = E 0 , therefore
+
DE
Likewise, b  PWUC (G). Now, for c, we
00 (a) = {b, c, d} and a  PWUC (G).
have E 0 = {(a, c), (a, d), (b, d), (c, b), (c, d)} and E 00 = {(a, c), (a, d), (b, d), (c, b), (c, d), (b, a)},
+
++
see also Figure 3(ii ). This gives us DE
00 (c) = {b, d} and DE 00 (c) = {a}, and there0
fore, c  PWUC (G). Lastly, for d, we have E = {(a, c), (a, d), (b, d), (d, c)} and E 00 =
10. This is also a consequence of NWTC (G) =  (Section 4.3) and NWUC  NWTC (Lemma 2).

506

fiPossible and Necessary Winners of Partial Tournaments

a

b

a

b

a

b

c

d

c

d

c

d

(i )

(ii )

(iii )

Figure 3: A partial unweighted tournament G and possible extensions. In the center, the
alternative c and its dominion is maximally reinforced resulting in c reaching every other
alternative in at most two steps. Therefore, c  PWUC (G). On the right, the same is
done for alternative d that cannot reach a in two steps and is therefore not contained in
PWUC (G).
+
{(a, c), (a, d), (b, d), (d, c), (c, b)} as depicted in Figure 3(iii ). This gives us DE
00 (d) = {c}
++
/ PWUC (G). In summary,
and DE 00 (d) = {b}, implying that d 

PWUC [G] = {a, b, c},
NWUC (G) = , and
PWSUC (G) = {{a}, {b}, {a, b, c}},
where PWSUC (G) is obtained by an ad hoc argument.
For all solution concepts considered so farCondorcet winners, Condorcet non-losers,
Copeland, and the top cyclePW and PWS have the same complexity. One might wonder
whether a result like this holds more generally, and whether there could be a polynomialtime reduction from PWS to PW. In the following we find that this is not the case, unless
P=NP, and show that PWSUC , the problem of deciding whether a subset of alternatives of
a partial tournament G is the uncovered set of some completion of G, is NP-complete. The
proof of this result proceeds by a reduction of Sat and involves the construction of partial
tournaments on the basis of formulas in conjunctive normal form. For each propositional
variable p and every clause c, there is a gadget that is based on the partial tournament Gp
depicted in Figure 4(i ).
It is not hard to see that there are exactly two completions of Gp with {p , p+ , 1}
as the uncovered set. The first, T + or positive completion, is depicted in Figure 4(ii )
and the other, T  or negative completion, in Figure 4(iii ). To verify that there are no
other ones, consider an arbitrary completion (V, E 0 ) of Gp . Then, either (p , p+ )  E 0
or (p+ , p )  E 0 . In the former case, observe that p must be covered by 1. Hence,
(1, p )  E 0 and (c, p )  E 0 . It now follows that c is covered by p+ . Therefore, also
(p+ , 1)  E 0 and (p+ , c)  E 0 . This entails that p covers p+ and, with (p+ , p+ )  E 0 we
finally obtain (p , p+ )  E 0 . The resulting tournament is T + . By an analogous argument
it can be seen that T  results if we assume that (p , p+ )  E 0 . In the construction below,
the positive completion T + will correspond to setting propositional variable p to true and
the negative completion T  to setting p to false.
Besides c, the construction also involves an alternative c for each clause. How c is related
to the other alternatives in Gp depends on whether the respective clause contains p or p as
a literal. As we may assume that no clause contains both p and p, three cases remain, which
507

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

p

p+

1

p

p+

1

p

p+

1

p

p+

c

p

p+

c

p

p+

c

(i ) variable gadget for variable p

(ii ) completion T + for p set to true

(iii ) completion T  for p set to false

Figure 4: The partial tournament Gp and its only two completions, T + and T  , for which
the uncovered set is given by {p , p+ , 1}. Dotted edges are missing and omitted edges point
downwards.

p

p+

1

p

p+

c

p

p+

(i ) c contains p but not p

1

p

p+

c

c

p

p+

(ii ) c contains p but not p

1

c

c

p

p+

c

(iii ) c contains neither p nor p

Figure 5: The Gp -gadget with alternative c added. As in Figure 4, dotted edges are missing
and omitted edges point downwards.
are depicted in Figure 5. Some reflection reveals that if a clause contains p as a positive
literal, c will be covered by p+ if the partial tournament is completed positively, but not
by p if it is completed negatively. Similarly, if the clause contains p as a negative literal, c
will be covered by p if Gp is completed negatively, but not by p+ if Gp is completed
positively. If c contains neither p nor p as a literal, c will not be covered by either p+ or p
irrespective of whether Gp is completed positively or negatively.
In the construction below, for every clause, alternative c will be covered if and only if
the clause contains a literal p and the Gp -gadget is completed positively or a literal q and
the Gq -gadget is completed negatively.
Theorem 7. PWSUC is NP-complete.
Proof. Given a partial tournament G = (V, E), a set X  V , and a completion T  [G],
it can be checked in polynomial time whether X = UC (T ). Hence, PWSUC is obviously
in NP.
NP-hardness is shown by a reduction from Sat. Let  be a formula in conjunctive
normal form. Without loss of generality we may assume that no clause contains both a
508

fiPossible and Necessary Winners of Partial Tournaments

literal and its negation, that there are at least two clauses, and that every literal occurs in
at least one clause. We construct a partial tournament G = (V , E ) as follows. For each
propositional variable p we introduce five alternatives denoted p, p , p+ , p , and p+ . For
each clause c, we introduce two alternatives denoted c and c. We also have two auxiliary
alternatives denoted by 1 and 0. Thus,
V = {p, p , p+ , p , p+ : p is a variable}  {c, c : c is a clause}  {1, 0}.
We now give a description of the edge set E . For every propositional variable p and
every clause c the alternatives p , p+ , p , p+ , 1, c, and c are organized as in Figure 5. The
remaining edges are set in such a way as to make the construction work properly. Formally,
define the edge set E such that for every propositional variable p and every clause c:
 p dominates d and d for every clause d as well as q  , q + , q  , q + for every q 6= p;
 p+ dominates 0, p, and p along with q  , q + , and d for all q 6= p and all clauses d.
Moreover, for every clause d, alternative p+ dominates alternative d if and only if p
occurs as a literal in clause d;
 p dominates 0, p, and p+ along with q  , q + , and d for all q 6= p and all clauses d.
Moreover, for every clause d, alternative p dominates alternative d if and only if p
occurs as a literal in clause d;
 p+ dominates 0, p, and p+ ;
 p dominates 0, p, and p ;
 c dominates 0, q  , and q + for every variable q, and d for every clause d 6= c. Moreover,
for very variable q, alternative c dominates q + whenever c does not contain q as a literal,
and q  if c does not contain q as a literal;
 c

dominates 0, c, and 1;

 1 dominates 0 as well as q, q  , and q + for all variables q, and d for all clauses d;
 0 dominates alternative q for every variable q, otherwise 0 is dominated by all other
alternatives.
Moreover, for every variable p, the edges among p , p+ , and 1 are missing as well as those
between p , p+ , and d for every clause d. Finally, any edges not specified in the above
description can be set arbitrarily. For an example of this construction the reader is referred
to Figure 6.
Now let
X = {p, p , p+ : p a propositional variable}  {c : c a clause}  {1}.
Table 2 summarizes which alternatives can reach which other alternatives in at most two
steps in G . We thus find that, for every completion T of G , the set X is contained
in UC (T ) and that 0 is covered by 1. For propositional variables p and clauses c, the alternatives p , p+ , and c can only be covered by alternatives from {p , p+ , 1}, i.e., whether
509

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

p

p

p

q

p+

p+

1

pq

pr

pq

pr

q

q+

q

q+

0

Figure 6: Part of the dominance relation of the partial tournament G associated with the
CNF formula  = (p  q)  (p  r). The alternatives pq and pr represent the two clauses of .
The part involving variable r, i.e., the alternatives r, r , r+ , r , and r+ , has been omitted.
The dashed edges are dependent on the clauses of . Omitted edges point downwards or,
when on the same level, in an arbitrary direction. Dotted edges are missing.
they are covered only depends on how the subtournament on {p , p+ , 1, p , p+ , c} is completed. As we saw in the discussion preceding this theorem, this can be done positively
or negatively, where a positive completion corresponds to setting variable p to true and a
negative completion to setting p to false. We complete the proof by showing that
X = UC (T ) for some T  [G ]

if and only if  is satisfiable.

() First assume that  is satisfiable and let v be the satisfying assignment for . For
each propositional variable p that v sets to true and each clause c, we complete the subtournament on {p , p+ , 1, p , p+ , c} positively, i.e., we add edges (p , p+ ), (p+ , 1), and (1, p )
as well as (p , p+ ), (p+ , c), and (c, p ). Thus, p is covered by 1, p+ by p , and, provided
that p occurs as a literal in c, c also by p+ . Similarly, for each propositional variable q that v
sets to false and each clause c, we complete the subtournament on {p , p+ , 1, p , p+ , c} negatively, i.e., we add edges (1, q + ), (q + , q  ), and (q  , 1) as well as (c, q + ), (q + , q  ), and
(q  , c). Accordingly, q  is covered by q + , q + by 1, and, provided that q occurs as a literal
in c, c also by q  . Observe that this procedure induces a well-defined completion of G ,
which we denote by Tv . As v satisfies , every clause contains a literal p such that v sets p to
true or a literal q such that v sets q to false. It follows that for every clause c, alternative c
is covered in Tv . Observe that p and p+ are covered in Tv irrespective of whether v sets p
510

fiPossible and Necessary Winners of Partial Tournaments

p q

p

p+

1

q q+ c

d p

p+

c

d q q+ 0

p
p
p+
1
c[p]
c[p]





0
0

q
0
0

0
0

c[p]

p
p

p

c[p]
p+

p+
p+


c
c
c

d
d



  c
c


p
p
p p c[p] 


p
p
p p 
c[p] 

q q+   

p
p


 d 

p 


 d 

p+ 

p
p+
c[p]
c[p]
0



0
0


0
0
0
0




c

q




c
q







p
p
c
c
p

p
p
c
c
p

p
p
1
1
p

p
p
1
1
p


p+
1
1
q

p

1
1
q

p
p


p

p
p
c
c
p















c






p
p
1
1
p

p
p
1
1
p







Table 2: Table summarizing which types of alternatives reach which other types of alternatives in one or two steps in (all completions of) the partial tournament G . We assume p
and q to be distinct variables such that neither q nor q occurs as a literal in c. Furthermore, c
and d are assumed to be distinct clauses, where c[p] denotes clause c on the understanding
that p occurs as a literal in c. Similarly, c[p] denotes clause c on the understanding that p
occurs as a literal in c. An alternative x in the entry for row r and column c means that r
can reach c via x. If the entry is a dot (), r can reach c directly, i.e., in one or zero
steps. A box () signifies that it depends on how G is being completed whether and
via which alternative r can reach c. The minus () in the entry for 0 and 1 means that 0
cannot reach 1 in at most two steps, no matter how G is completed. Thus, 0 is covered
by 1 in every completion of G . We may assume that no clause contains both a literal and
its negation, that there are at least two clauses, and that every literal occurs in at least one
clause.
to true or false. Hence, c, p+ , p 
/ UC (Tv ). Recalling that 1 covers 0 and X  UC (T ) for
all completions T of G , we may conclude that UC (Tv ) = X, as desired.
() For the opposite direction, assume that there is some completion T of G such
that for every propositional variable p and for every clause c, alternatives p , p+ , and c
are covered in T , i.e., such that UC (T ) = X. Define assignment vT such that it sets
propositional variable p to true if there is some clause c containing p as a literal such
that p+ covers c in T and sets p to false, otherwise. Observe that vT is a well-defined
assignment.
We now show that vT satisfies every clause in  and hence  itself as well. To this end
consider an arbitrary clause c. By assumption, c is covered by some alternative x. Recall
that c reaches all alternatives in at most two steps except alternatives p+ such that p occurs
as a literal in c and alternatives q  such that q occurs as a literal in c (also see Table 2).
Hence, either x = p+ for some variable p occurring as a literal in c or x = q  for some
variable q such that q occurs as a literal in c.
If the former, vT sets p to true and consequently also satisfies clause c. If the latter, we
have to demonstrate that vT sets q to false and in that way satisfies clause c. It suffices to
511

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

2

4

q

q+

6

1

5
3
1

q

q+

c

d

Figure 7: Illustration of the concluding argument of the proof of Theorem 7. A double
edge from alternative x to alternative y indicates that x covers y. The numbers some of the
edges are labelled with correspond to the order in which their existence is demonstrated in
the proof of Theorem 7.

show that there is no clause d such that q + covers d in T . To this end, consider an arbitrary
clause d. We prove that q + does not cover d and refer to Figure 7 for an illustration of our
reasoning. Let ET denote the edge set of T . As T extends G , obviously E  ET . First
recall that we have assumed that q  covers c in T . As (c, 1)  E , then also (q  , 1)  ET .
Since (q  , q  )  E , it can therefore not be the case that 1 covers q  in T . Reaching every
other alternative in at most two steps in G , alternative q  must therefore be covered by q +
in T . As (q  , q  )  E , it now follows that (q + , q  )  ET . Since, moreover, (q + , q + )  E ,
we also have that q  does not cover q + in T . Rather, q + reaches every alternative except 1
in at most two steps in T . It follows that q + is covered by 1. Moreover, since (q + , q + )  E ,
also (1, q + )  ET . Now consider alternative d and observe that, by construction, (d, 1)  E .
Thus, d can reach q + in two steps and we may conclude that q + does not cover d in T . It
follows that vT sets q to false and thus satisfies c, as desired.

5. Weighted Tournament Solutions
We now turn to weighted tournaments, and in particular consider the solution concepts
Borda, maximin, and ranked pairs.
5.1 Borda
The Borda solution (BO) is typically used in a voting context with a set N of n voters,
where each voter i is equipped with a linear ranking i as its individual preference. Then,
each alternative receives |V |  1 points for each time it is ranked first by a voter, |V |  2
points for each time it is ranked second, and so forth. The total number of points of an
alternative x constitute its Borda score sBO (x, (i )iN ) which can be written as
sBO (x, (i )iN ) =

X
iN

512

|{y  V : x i y}|.

fiPossible and Necessary Winners of Partial Tournaments

More generally, the Borda solution can be extended to n-weighted tournaments where the
Borda scores are defined as
X
sBO (x, (V, w)) =
w(x, y)
yV \{x}

and BO(V, w) again chooses those alternatives with maximum Borda score. This subsumes
the voting setting because
X
sBO (x, (i )iN ) =
|{i  N : x i y}| = sBO (x, (V, w))
yV

if the weight on the edge from x to y is defined to be the number of voters who rank x
higher than y, i.e.,
w(x, y) = |{i  N : x i y}|.
Before we proceed, we define the notion of a b-matching, which will be used in the proofs
of several of our results in this section. Let H = (VH , EH ) be an undirected graph with
vertex capacities b : VH  N0 . Then, a b-matching of H is a function m : EH  N0 such
that for all v  VH ,
X
m(e)  b(v).
e{e0 EH :ve0 }

P
The size of b-matching m is defined as
eEH m(e). It is easy to see that if b(v) = 1
for all v  VH , then a maximum-size b-matching is equivalent to a maximum-cardinality
matching. In a b-matching problem with upper and lower bounds, there further is a function
a : VH  N0 . A feasible b-matching then is a function m : EH  N0 such that
X
a(v) 
m(e)  b(v).
e{e0 EH :ve0 }

If H is bipartite, then the problem of computing a maximum-size feasible b-matching
with lower and upper bounds can be solved in strongly polynomial time (Schrijver, 2003,
ch. 21). We will use this result to show that PWBO and PWSBO can both be solved in
polynomial time. While the following result for PWBO can also be shown using Theorem 6.1
of Kern and Paulusma (2004), we still give a direct proof that will then be extended to
PWSBO .
Theorem 8. PWBO can be solved in polynomial time.
Proof. Observe that BO satisfies the following (weak) monotonicity property: making a
winner x stronger by increasing weight on an edge to another alternative, cannot make x a
losing alternative.
Let G = (V, w) be a partial n-weighted tournament, x  V . By the previous observation,
x  PWBO (G) if and only if x  PWBO (Gx ). Therefore, we can assume w.l.o.g that
G = Gx , i.e., all edges incident to x are completely specified already. Moreover, if there
exists a y  V \ {x} such that sBO (y, Gx ) > sBO (x, Gx ), then we already know that
x
/ PWBO (G). We thus assume that sBO (y, Gx )  sBO (x, Gx ) for all y  V \ {x}.
513

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

We give a polynomial-time algorithm for checking whether x  PWBO (Gx ) via a
reduction to the problem of computing a maximum-size b-matching of a bipartite graph.
Let s = sBO (x, Gx ) be the Borda score of x in Gx . We construct a bipartite graph
H = (VH , EH ) with vertices
VH = V \ {x}  E x , where
E x = {{i, j}  V \ {x} : i 6= j}
and edges
EH = {{i, {i, j}} : {i, j}  V \ {x}, i 6= j}.
We further define vertex capacities b : VH  N0 such that
b({i, j}) = n  w(i, j)  w(j, i) for {i, j}  E x and
b(v) = s  sBO (v, Gx ) for v  V \ {x}.
Now observe that in any completion T = (V, w0 )  [Gx ], w0 (i, j) + w0 (j, i) = n for all
i, j  V with i 6= j. The sum of the Borda scores in T is therefore n|V |(|V |  1)/2. Some
of the weight has already been used up in Gx ; the weight which has not yet been used up
is equal to
X
 = n|V |(|V |  1)/2 
sBO (v, Gx ).
vV

(Gx )

We claim that x  PWBO
if and only if H has a b-matching of size at least .
0
x
() Let T = (V, w )  [G ] be a completion with x  BO(T ). Consider the bmatching m with m(i, {i, j}) = w0 (i, j)  w(i, j). We verify that m is a feasible b-matching.
Let v  VH . If v  V \ {x}, we have that
X

m(e) = sBO (v, T )  sBO (v, Gx )  s  sBO (v, Gx ) = b(v).

e{e0 EH :ve0 }

Otherwise, v = {i, j}  E x and
X

m(e) = m({i, {i, j}}) + m({j, {i, j}}) = n  w(i, j)  w(j, i) = b({i, j}).

e{e0 EH :{i,j}e0 }

As the size of m is
X
X
X X
 X
m(e) =
w0 (i, j) + w0 (j, i)  w(i, j)  w(j, i) =
n
w(i, j) = ,
eEH

i6=j

i6=j

iV jV \{i}

the statement is shown.
() For the other direction, assume that a feasible b-matching of size at least  exists.
We construct a completion T = (V, w0 )  [Gx ] with x  BO(T ). Let
w0 (i, j) = m(i, {i, j}) + w(i, j)
0

for all {i, j}  V \ {x},

0

w (x, i) = w(x, i), and w (i, x) = w(i, x)
514

for i  V \ {x}.

fiPossible and Necessary Winners of Partial Tournaments

As w(i, j)  w0 (i, j) and w0 (i, j)+w0 (j, i)  w(i, j)+w(j, i)+b({i, j}) = n for all {i, j}  V ,
T is an extension of Gx . From
X
X
X
b({i, j}) 
m(e) =
 ,
=
{i,j}E x

eEH

i6=j

we know that the upper capacities b({i, j}) of all {i, j}  E x are exactly met by m (and
that there cannot be a matching with size more than ). This implies that
w0 (i, j) + w0 (j, i) = w(i, j) + w(j, i) + b({i, j}) = n,
showing that T is indeed a completion of Gx .
Since H can be constructed efficiently, and since a maximum-size b-matching can be
computed in strongly polynomial time, our algorithm runs in polynomial time.

1
1

a

1
1

a

b

1

5

1

5

5

4

4

2

c

2
2

2

cap.

3
2

c

d

cap.

a

3

{a, b}

1
4

a

3

b

5

d

2

1

{a, d}

2

5

{b, d}

1

c

b
5

3

3

d

(ii ) The partial tournament Gc .

(i ) A partial 5-weighted tournament G.

0

b

1 4
2

(iii )
The
constructed
bipartite
graph H for target Borda score
s = sBO (c, Gc ) = 8. Capacities
are given next to the vertices. Thick
edges with weights indicate the unique
maximum b-matching.

3
2

d

(iv ) The completion T of G that corresponds to the maximum b-matching. In
this case, BO(T ) = {a, b, c}.

Figure 8: Illustration of the algorithm for checking whether an alternative c is contained in
PWBO (G) for a partial 5-weighted tournament G.
Figure 8 illustrates the described steps for determining whether an alternative is contained in PWBO (G).
This idea can be extended to a polynomial-time algorithm for PWSBO where we use
a similar construction for a given G = (V, w), a candidate set X  V and a target Borda
score s . Binary search can be used to efficiently search the interval of possible target scores.
515

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Theorem 9. PWSBO can be solved in polynomial time.
Proof. Let G = (V, w) be a partial n-weighted tournament, and X  V . We give a
polynomial-time algorithm for checking whether X  PWSBO (G), via a bisection method
and a reduction to the problem of computing a maximum b-matching of a graph with lower
and upper bounds.
Assume that there is a target Borda score s and a completion T  [G] with X 
PWSBO (T ) and sBO (x, T ) = s for all x  X. Then, the maximum possible Borda score of
an alternative not in X is s  1.
For a given target Borda score s , we construct a bipartite graph H = (VH , EH ) with
vertices VH = V  E x , where
E x = {{i, j}  V : i 6= j},
and edges
EH = {{i, {i, j}} : {i, j}  V, i 6= j, w(i, j) + w(j, i) < n}.
Only the lower bounds as : VH  N0 and upper bounds bs : VH  N0 depend on s and
are defined as follows. For vertices x  X, lower and upper bounds coincide and are given
by
as (x) = bs (x) = s  sBO (x, G).
All other vertices v  VH \ X have a lower bound of as (v) = 0. Upper bounds for these
vertices are defined such that
bs (v) = s  sBO (v, G)  1
for v  V \ X, and
bs ({i, j}) = n  w(i, j)  w(j, i)
for {i, j}  E x .
As in the proof of Theorem 8, it holds that a feasible b-matching in H corresponds to
an extension of G. Such an extension is a completion T  [G] if and only if the b-matching
has size
X
 = n|V |(|V |  1)/2 
sBO (v, G),
vV

which equals the weight not yet used up in G. Then, T satisfies X  PWSBO (T ) and
sBO (x, T ) = s for all x  X. If, on the other hand, no s gives rise to a graph that has a
b-matching of size , then X 6 PWSBO (G).
In order to obtain a polynomial-time algorithm, we need to check whether there exists a
target score s for which the corresponding graph H with upper and lower bounds admits a
b-matching of size . It is easily verified that any such s is contained in the integer interval
I = [ max sBO (x, G), n(|V |  1) ].
xX

Observe that |I| depends on n and thus is not polynomially bounded in the size of G.
Checking every integer s  I is therefore not feasible in polynomial time. However, we now
show that we can perform binary search in order to find s efficiently. We need the following
516

fiPossible and Necessary Winners of Partial Tournaments

two observations about the interval I. For s  I, we say that s admits a feasible b-matching
if the corresponding graph H has a feasible b-matching.
First, if an s0  I admits a feasible b-matching, then every s00  I with s00  s0 also
admits a feasible b-matching. This is because removing all weight from edges that exceeds
the (reduced) upper bounds gives a feasible b-matching for s00 .
Second, with s0 as before and 0 the size of the corresponding maximum feasible bmatching m0 , there cannot be an s00  I with s00  s0 such that the size 00 of a maximum
feasible b-matching m00 for s00 is smaller than 0 . This is because either (i ) no such m00 exists
since not all lower bounds can be met, or (ii ) such an m00 exists and its size is at least 0 .
To see the latter, note that a decrease in the size of a maximum feasible matching cannot be
caused by upper bounds as bs00 (v)  bs0 (v) for all v  VH . It remains to be shown that the
increase in as00 (v) for v  X does not result in a smaller maximum b-matching. Since the
weight of all edges incident to a vertex in X in the b-matching is completely determined by
the bounds and increases from m0 to m00 , a total decrease in size can only be due to edges
{j, {i, j}} with i  V \ X, j  V whose weight is bounded by bs00 ({i, j})  m00 (i, {i, j}). But
then,
m00 (i, {i, j}) + m00 (j, {i, j}) = bs00 ({i, j})  bs0 ({i, j})  m0 (i, {i, j}) + m0 (j, {i, j})
and therefore 00  0 .
These two observations show that I can be partitioned into two non-overlapping integer
intervals I1 and I2 . Here, each s  I1 admits a feasible b-matching whose size increases
when s grows, whereas each s  I2 does not admit a feasible b-matchings. Therefore, either
I1 is empty and the desired s does not exist, or s = max(I1 ).
We can check the existence of s with the following binary search algorithm. Let
[Imin , Imax ] be an interval that is initialized to I = [maxxX sBO (x, G), n(|V |  1)]. Consider
the median value s of this interval. If the corresponding graph H has no feasible b-matching,
continue with the interval [Imin , s  1]. Otherwise, if the maximum feasible b-matching has
size at least , return yes. If its size is less than , continue with [s+1, Imax ]. If [Imin , Imax ]
is empty, return no.
The number of queries of this algorithm is bounded by dlog2 |I|e  dlog2 n|V |e and,
therefore, polynomial in the size of G.
To conclude this section, we show that NWBO can be solved in polynomial time as well.
It is worth noting that this result does not follow directly from the polynomial-time result
for NWBO for the case of preference profiles (Xia & Conitzer, 2011).
Theorem 10. NWBO can be solved in polynomial time.
Proof. Let G = (V, w) be a partial weighted tournament, x  V . We give a polynomial-time
algorithm for checking whether x  NWBO (G).
Let G = Gx . We want to check whether some other alternative y  V \ {x} can
achieve a Borda score of more than s = sBO (x, G). This can be done separately for each
y  V \ {x} by reinforcing it as much as possible in G. If for some y, sBO (y, Gy ) > s ,
then x 
/ NWBO (G). If, on the other hand, sBO (y, Gy )  s for all y  V \ {x}, then
x  NWBO (G).
517

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

As an example, consider the partial 5-weighted tournament G in Figure 8(i ). The fact
that {a, b, c}  PWBO (G) follows already from the completion shown in Figure 8(iv ). Also
note that this was the only completion in which c was chosen. Alternative d is not a possible
Borda winner since sBO (d, Gd ) = 7 < 8 = sBO (a, G). To determine PWSBO (G), we still
have to check which subsets of {a, b, c} are possible winning sets. For singletons, it is easy
to see that only {a} and {b} are in PWSBO (G). For {a, b}, we could employ the binary
search method described in Theorem 9. Here, we just argue that moving one unit of weight
from (c, d) to (d, c) in the completion shown in Figure 8(iv ) gives a completion in which
{a, b} is the winning set. For NWBO (G), it is straightforward to check that no alternative
is a necessary Borda winner. Altogether, we have that
PWBO (G) = {a, b, c},
NWBO (G) = , and
PWSBO (G) = {{a}, {b}, {a, b}, {a, b, c}}.
5.2 Maximin
The maximin score sMM (x, T ) of an alternative x in a weighted tournament T = (V, w), is
given by its worst pairwise comparison, i.e., sMM (x, T ) = minyV \{x} w(x, y). The maximin solution, also known as Simpsons method and denoted by MM , returns the set of all
alternatives with the highest maximin score.
As an example, consider the partial 5-weighted tournament depicted in Figure 9(i ). It is
easy to see that a (or b) are the unique maximin winners in all completions of Ga (or Gb ).
Also, c cannot be a possible maximin winner as it will always have a maximin score of 0
whereas a always has at least 1. Similarly, alternative d can never have a higher maximin
score than a. Figure 9(iii ) shows a completion in which {a, d} is the set of maximin winners.
If one unit of weight is shifted from (c, b) to (b, c), the resulting completion has {a, b, d} as
the maximin winners. It is also straightforward to find a completion of G{a,b} with {a, b}
as the set of maximin winners. It is easy to verify that no alternative is a necessary maximin
winner.
Together, this gives
PWMM (G) = {a, b, d},
NWMM (G) = , and
PWSMM (G) = {{a}, {b}, {a, b}, {a, d}, {a, b, d}}.
We first show that PWMM is polynomial-time solvable by reducing it to the problem of
finding a maximum-cardinality matching of a graph.
Theorem 11. PWMM can be solved in polynomial time.
Proof. We show how to check whether x  PWMM (G) for a partial n-weighted tournament
G = (V, w). Consider the graph Gx = (V, wx ). Then, sMM (x, Gx ) is the best possible
maximin score x can get among all completions of G. If sMM (x, Gx )  n2 , then we have
sMM (y, T )  wx (y, x)  n2 for every y  V \ {x} and every completion T  [Gx ], and
therefore x  PWMM (G).
518

fiPossible and Necessary Winners of Partial Tournaments

1
1

a

b

1
5

4

c

2
2
2

c

d

d

(i ) A partial 5-weighted tournament G.

1
4

a

{a, b}
{a, c}
{a, d}
{b, c}
{b, d}
{c, d}

a

b

(ii )
The
constructed
bipartite

graph H s for s = 1 and X = {a, d}
as in the proof of Theorem 12. A
maximum-cardinality matching is given
by the thick edges.

b

a

14

5

5

3
5
3
2

b

3

2

c

3
2

14
2

5

c

d

(iii ) A completion T of G that could
be obtained from the matching. Indeed,
MM (T ) = {a, d} with sMM (T ) = 1.

3
2

d

(iv ) This completion T of G is a witness
for {a, b}  PWSMM (G).

Figure 9: Example of a 5-weighted partial tournament and completions relevant for possible
maximin winners.
Now consider sMM (x, Gx ) < n2 . We will reduce the problem of checking whether x 
PWMM (G) to that of finding a maximum-cardinality matching of an undirected unweighted
graph, which is known to be solvable in polynomial time (Edmonds, 1965). We want to
find a completion T  [Gx ] such that sMM (x, T )  sMM (y, T ) for all y  V \ {x}. In
other words, we want to complete the weights on edges between alternatives in V \ {x} in
such a balanced way so that x is still a winner. If there exists a y  V \ {x} such that
sMM (y, Gx ) > sMM (x, Gx ), then we already know that x 
/ PWMM (G). Otherwise,
each y  V \ {x} derives its maximin score from at least one particular edge (y, z) where
z  V \ {x, y} and w(y, z)  sMM (x, Gx ). Moreover, it is clear that in any completion, y
and z cannot both achieve a maximin score of less than sMM (x, Gx ) from edges (y, z) and
(z, y) at the same time. Let H = (VH , EH ) be an undirected and unweighted graph with
vertices
VH = V \ {x}  {{i, j}  V : i 6= j}
and edges
EH = {{i, {i, j}} : i  V \ {x}, j  V \ {i}, wx (i, j)  sMM (x, Gx )}.
In this way, if i is matched to {i, j} in H, then i derives a maximin score of less than or
equal to sMM (x, Gx ) from his comparison with j. Clearly, the size of H is polynomial
519

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

in the size of G. We show that x  PWMM (G) if and only if there exists a matching of
cardinality |V |  1 in H.
() First, assume that x  PWMM (G). Then there exists a completion T = (V, w0 )
of Gx in which the maximin score of each y  V \ {x} is at most sMM (x, Gx ) < n2 .
If alternative i derives its maximin score from a comparison with j 6= i  V \ {x}, i.e.,
sMM (i, T ) = w0 (i, j), then j cannot derive its maximin score from a comparison with i
because w0 (j, i)  n  sMM (x, Gx ) implies w0 (j, i) > n2 . Therefore, in H, each i  VH  V
can be matched to a vertex {i, j}  VH such that {i, j} is not matched to any other vertex
in VH . The resulting matching in H has cardinality |V |  1.
() Now, assume that there exists a matching M of cardinality |V |  1 in H. Then,
each i  V \ {x} has to be matched to an {i, j} where w(i, j)  sMM (x, Gx ). Consider a
completion T = (V, w0 )  [Gx ] in which for all (i, j)  V  V such that {i, {i, j}}  M , we
set w0 (i, j) = w(i, j) and w0 (j, i) = n  w(i, j). Moreover, the weights of all other edges in T
are set by any arbitrary completion of edges in Gx . Clearly, T is a proper completion of
Gx and therefore of G. In T , the maximin score of each y  V \ {x} is less than or equal
to the maximin score of x. Therefore x  MM (T ) which implies that x  PWMM (G).
Next, we show that PWSMM can be solved in polynomial time. The proof proceeds
by identifying the maximin values that could potentially be achieved simultaneously by
all elements of the set in question, and solving the problem for each of these values using
similar techniques as in the proof of Theorem 11. Only a polynomially bounded number of
problems need to be considered.
Theorem 12. PWSMM can be solved in polynomial time.
Proof. Let G = (V, w) be a partial n-weighted tournament and X  V . We give a
polynomial-time algorithm for checking whether X  PWSMM (G).
If X  PWSMM (G) there must be a completion T  [G] and s  {0, . . . , n} such that
sMM (x, T ) = s for all x  X and sMM (i, T ) < s for all y  V \ X.
First, we note that if s > n  w(j, i) for some i  X, j  V or s  w(i, j) for some
i
/ X, j  V , then X 6 PWSMM (G). Therefore, we assume that
n  w(j, i)  s for all i  X, j  V and
w(i, j) < s for all i 
/ X, j  V.
We treat the cases s > n2 , s = n2 , and s <

n
2

separately.

Case 1: s > n2 . Then, X  PWSMM only if X is a singleton, and for each x  V ,
whether {x}  PWSMM with maximin score s can be checked easily.
Case 2: s = n2 . With the assumptions above, we can define G0 = (V, w0 ) as an extension
of GX with w0 (i, j) = w0 (j, i) = n2 = s for all i, j  X. Note that in every completion T
of G0 , sMM (i, T ) = s for all i  X and that X  PWSMM (G) with maximin score n2 in the
corresponding completion if and only if X  PWSMM (G0 ) with the same maximin score in
the respective completion.
520

fiPossible and Necessary Winners of Partial Tournaments

In addition, we need to check whether alternatives not in X can be forced to have a
strictly smaller maximin score than n2 . To this end, construct an unweighted undirected
bipartite graph H = (VH , EH ) with vertices
VH = V  {{i, j}  V : i 6= j}
and edges
EH = {{i, {i, j}} : i  V \ X, j  V \ {i}, w(i, j) < s }.
We claim that X  PWSMM (G0 ) with a maximin score of s = n2 in the corresponding
completion if and only if there is a maximum-cardinality matching of size |V \ X| in H.
() Let T = (V, w00 ) a completion of G0 (and thereby of G) in which X is the set of
maximin winners with sMM (i, T ) = s = n2 for all i  X. For each i 
/ X, there needs to
be a j 6= i with w00 (i, j) < s . Collecting {i, {i, j}} for each such pair gives a matching of
size |V \ X| in H which is maximum since each vertex on one side of the bipartite graph is
contained in it.
() For the other direction, assume that there is a maximum matching of size |V \ X|.
We construct a completion T = (V, w00 ) of G0 such that X is the set of maximin winners.
Note that every i  (VH  V ) \ X has to be contained in an edge {i, {i, j}} in the matching.
For each such edge, let w00 (i, j) = w0 (i, j) < s and w00 (j, i) = n  w00 (i, j), implying that
sMM (i, T ) < s . Otherwise, T is an arbitrary completion of G.
Together, we have that sMM (i, T ) = s for all i  X and sMM (i, T ) < s for all i 
/ X.
Figure 10 illustrates the procedure for a 2-weighted tournament and the set X = {a}.
Case 3: s < n2 . For a given s , we construct an undirected unweighted bipartite

s ). Let V be as before and
graph H s = (VH , EH
H
[
[
s
EH
=
{{i, {i, j}} : w(i, j)  s  n  w(j, i)} 
{{i, {i, j}} : w(i, j)  s  1}.
iX
j6=i

iV
j6=i

We claim that X is in PWSMM (G) with a maximin score of s in the corresponding

completion if and only if there is a maximum-cardinality matching of size |V | in H s .
() Let T = (V, w0 ) a completion of G in which X is the set of maximin winners with
the maximum maximin score s . For every vertex i  V , there has to be an j 6= i such that
w0 (i, j) accounts for the maximin score of i. Also, since s < n2 , it cannot be the case that j
also derives its maximin score from w0 (j, i). Therefore, the set of all such pairs {i, {i, j}} is
a valid matching of size |V |. It is obviously maximal.
() For the other direction, assume that there is a maximum matching of size |V |. Note
that every i  (VH  V ) is matched and define j(i)  V such that the edge {i, {i, j(i)} is
contained in the matching. We construct a completion T = (V, w0 ) in which X is the set of
maximin winners. To this end, define
w0 (i, j(i)) = s and w0 (j(i), i) = n  s for i  X, and
w0 (i, j(i)) = s  1 and w0 (j(i), i) = n  (s  1) for i  V \ X.
As long as there are unspecified edges (i, j) in the completion, define
w0 (i, j) = max{w(i, j), s } and w0 (j, i) = n  w(i, j) if i  X, j  V , and
w0 (i, j) = max{w(i, j), s  1} and w0 (j, i) = n  w(i, j) otherwise.
521

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Note that T is a proper completion of G. Now, we have sMM (i, T ) = s for all i  X and
sMM (i, T ) < s for all i 
/ X. This completes Case 3.


It remains to be shown that only a limited number of possible s (and thereby H s ) have
to be considered. In contrast to the proof of Theorem 9, we cannot employ a binary search
method since there is no clear cut between a feasible and an infeasible integer interval.
However, we can see that when s is gradually incremented from 0 to n2  1, whether an
s or not changes at most twice due to the definition of
edge {i, {i, j}} is contained in EH

s . This partitions the integer interval I = [0, n  1] of possible s into a finite number
EH
2

of subintervals Ik such that all s within a single Ik induce the same H s . Therefore, it is
sufficient to only consider one s per Ik and for this we choose the minimum. The set S 
of possibly relevant target scores s is then given by
[
[
[
S =
min Ik 
{w(i, j), n  w(j, i) + 1} 
{w(i, j) + 1}.
iX
j6=i

k

iV
j6=i

The size of S  is obviously bounded by 3n2 .
All cases can be handled in polynomial time.
a

a

b
1

1

1

1

1

b

2

11

1

1

1

c

1

c

d

(ii ) An extension G0 reinforcing {a}.

(i ) A partial 2-weighted tournament G.

b
c
d

d

{a, b}
{a, c}
{a, d}
{b, c}
{b, d}
{c, d}

a

b

2
1

1
11

2
1

1

c

2

d

(iv ) A completion of G0 and G with
MM (G0 ) = {a}

(iii ) The constructed undirected bipartite graph H. Thick edges indicate a
maximum-cardinality matching.

Figure 10: Illustration of the algorithm for checking whether a singleton {a} is contained
in PWSMM (G) for a partial 2-weighted tournament G. It is obvious that a cannot have a
maximin score of 2 in any completion or be the sole maximin winner with a maximin score
of 0. Therefore, we check the for the case s = n2 = 1.
Lastly, we consider NWMM , for which we apply a similar technique as for NWBO : to
see whether x  NWMM (G), we start from the graph Gx and check whether some other
alternative can achieve a higher maximin score than x in a completion of Gx .
522

fiPossible and Necessary Winners of Partial Tournaments

Theorem 13. NWMM can be solved in polynomial time.
Proof. We show how to check whether x  NWMM (G) for a partial n-weighted tournament
G = (V, w). The maximin score of x in Gx is the worst case maximin score of x among
all proper completions of G.
For each y  V \ {x}, the maximin score of y in Gy is the best possible maximin score
of y among the completions of G. If the maximin score of each y in the corresponding
Gy is not more than the maximin score of x in Gx , then x  NWMM (G), otherwise
x
/ NWMM (G).
5.3 Ranked Pairs
The method of ranked pairs (RP ) is the only resolute solution concept considered in this
article. Given a weighted tournament T = (V, w), it returns the unique undominated
alternative of a transitive tournament T 0 on V constructed in the following manner. First
order the (directed) edges of T in decreasing order of weight, breaking ties according to
some exogenously given tie-breaking rule. Then start with an empty graph T 0 and consider
the edges one by one according to this ordering. If the current edge can be added to T 0
without creating a cycle, then do so; otherwise discard the edge.11
As an example, consider the partial 5-weighted tournament depicted in Figure 11(i ),
which is a slightly modified version of the tournament considered in Figures 8 and 9. It is
easy to see that a is the ranked pairs winner in all completions of Ga , and likewise b is the
ranked pairs winner in all completions of Gb . On the other hand, there is no completion
that has c as a ranked pairs winner. Whether d is a possible ranked pairs winner depends
on the tie-breaking rule that is used, and in particular on how the tie-breaking rule ranks
edges (d, c) and (b, d): alternative d is a possible ranked pairs winner if and only if (d, c) is
considered before (b, d) (see Figure 11(iv )). Since RP is resolute, we have (assuming that
the tie-breaking rule ranks (d, c) over (b, d))
PWRP (G) = {a, b, d}
NWRP (G) = 
PWSRP (G) = {{a}, {b}, {d}}.
It is readily appreciated that the winner determination problem for RP is computationally tractable. The possible winner problem, on the other hand, turns out to be NP-hard.
This also shows that tractability of the winner determination problem, while necessary for
tractability of PW, is not generally sufficient.
Theorem 14. PWRP is NP-complete.
Proof. We will work with an alternative characterization of ranked pairs winners that was
introduced by Zavist and Tideman (1989). For a given a weighted tournament T = (V, w)
11. The variant of ranked pairs originally proposed by Tideman (1987), which was also used by Xia and
Conitzer (2011), instead chooses a set of alternatives, containing any alternative that is selected by the
above procedure for some way of breaking ties among edges with equal weight. We do not consider
this irresolute version of ranked pairs because winner determination for this variant is NP-hard (Brill
& Fischer, 2012). As mentioned in Section 3, this immediately implies that all problems concerning
possible or necessary winners are NP-hard as well.

523

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

1
1

a

4
1

a

b

1

5

1

5

5

4

5

2
1
2

c

4

1
4

b

a

5

5

4
1

14

4
3
2

b
5

3

5

d

(ii ) A completion of G with ranked pairs
winner a.

1
5

c

3
2

c

d

(i ) A partial 5-weighted tournament G.

a

b

2

c

d

(iii ) A completion of G with ranked
pairs winner b.

1
4

d

(iv ) A completion of G with ranked pairs
winner d. Here, we assume that edge
(d, c) is considered before edge (b, d).

Figure 11: Example of a 5-weighted partial tournament and completions relevant for possible
ranked pairs winners. For each completion, the transitive tournament constructed by the
ranked pairs procedure is indicated by the thick edges.
and a given tie-breaking rule, let  denote the order in which edges are considered in the
ranked pairs procedure. That is, (x, y)  (u, v) if and only if either w(x, y) > w(u, v) or
w(x, y) = w(u, v) and the tie-breaking rule ranks (x, y) higher than (u, v). Given a ranking
L of V , and two alternatives a and b, we say that a attains b through L if there exists
a sequence of distinct alternatives a1 , a2 , . . . , at , where t  2, such that a1 = a, at = b,
ai L ai+1 , and
(ai , ai+1 )  (b, a) for all i with 1  i < t.
In this case, we will say that a attains b via (a1 , a2 , . . . , at ). A ranking L is called a stack if
for any pair of alternatives a and b it holds that a L b implies that a attains b through L.
Zavist and Tideman (1989) have shown that an alternative is the ranked pairs winner if and
only if it is the top element of a stack.12 Intuitively, the defining properties of a stack L
ensure that for all pairs (a, b) of alternatives with a L b, by the point in time the edge (b, a)
is considered, it will be discarded because it would create a cycle.
Membership of PWRP in NP is obvious, as for a given completion and a given tiebreaking rule, the ranked pairs winner can be found efficiently.
12. The characterization by Zavist and Tideman (1989) addresses the irresolute version of ranked pairs
discussed in the previous footnote. Our adaptation to the resolute version of ranked pairs is a straightforward corollary.

524

fiPossible and Necessary Winners of Partial Tournaments

d

p01

p02

p1

p1

p2

p2

p01

p02

c1

c2

c3

Figure 12: The partial 8-weighted tournament G for the Boolean formula  = {p1 , p2 } 
{p1 , p2 }  {p1 , p2 }. Double-shafted arrows represent heavy edges, standard arrows represent
medium edges, dashed arrows represent light edges, and dotted lines represent partial edges.
For all pairs (a, b) that are not connected by an arrow, we have w (x, y) = w (y, x) = 4.

NP-hardness can be shown by a reduction from Sat. Our construction is based on the
proof of Theorem 1 by Brill and Fischer (2012). For a Boolean formula  in conjunctive
normal-form with a set C of clauses and set P of propositional variables, we construct a
partial 8-weighted tournament G = (V , w ) as follows. For each variable p  P , V
contains two literal alternatives p and p and two auxiliary alternatives p0 and p0 . For
each clause c  C, there is an alternative c. Finally, there is an alternative d for which
membership in PWRP (G ) is to be decided.
In order to conveniently describe the weight function w , let us introduce the following
terminology. For two alternatives x, y  V , say that there is a heavy edge from x to y if
w (x, y) = 8 (and therefore w (y, x) = 0). A medium edge from x to y means w (x, y) = 6
and w (y, x) = 2, and a light edge from x to y means w (x, y) = 5 and w (y, x) = 3.
Finally, a partial edge between x and y means w (x, y) = w (y, x) = 1.
We are now ready to define w . For each variable p  P , we have heavy edges from
p to p0 and from p to p0 , and partial edges between p and p0 and between p and p0 . For
each clause c  C, we have a medium edge from c to d and a heavy edge from the literal
alternative ` (with ` = p or ` = p for some p  P ) to c if the corresponding literal ` appears
in clause c. Finally, we have heavy edges from d to all auxiliary alternatives and light edges
from d to all literal alternatives. For all pairs x, y for which no edge has been specified, we
define w (x, y) = w (y, x) = 4. An example is shown in Figure 12. Observe that the only
pairs of alternatives for which w is not fully specified are those pairs that are connected
by a partial edge.
525

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

d

d

`0

`0
`

`

`

`

`0

`0

c

c
0

(ii ) Sc = (d, `0 , `, c)

(i ) Sc = (d, ` , `, `0 , `, c)

Figure 13: Two possibilities for the sequence Sc .
We will now show that alternative d is a possible ranked pairs winner in G if and only
if  is satisfiable. Intuitively, choosing a completion such that (p0 , p)  (p0 , p) corresponds
to setting the variable p to true.
() First assume that d  PWRP and let T  [G ] be a completion of G with
RP (T ) = {d}. Consider a stack L with top element d and an alternative c corresponding to
a clause in . Since L is a stack and d L c, d attains c though L via some sequence Sc . (If
d attains c via several sequences, fix one of them arbitrarily.) Since w (c, d) = 6, all edges
in the sequence Sc must be heavy, medium, or appropriate completions of partial edges.
Therefore, Sc must have one of the following two forms (depicted in Figure 13):
0

Sc = (d, ` , `, `0 , `, c)

or Sc = (d, `0 , `, c),
0

where ` is some literal. The former is in fact not possible because w (`, ` ) = 8 implies
0
that ` does not attain ` through L. Therefore, each Sc is of the form Sc = (d, `0 , `, c) for
some literal `.
Now define assignment  by setting to true all literals that are contained in one of the
sequences Sc , c  C. We claim that  is a satisfying assignment for .
In order to show that  is well-defined, suppose there exists a literal ` such that both `
and ` are set to true under . This implies that there exist c1 and c2 such that d attains c1
0
0
via Sc1 = (d, `0 , `, c1 ) and d attains c2 via Sc2 = (d, ` , `, c2 ). In particular, `0 L ` and ` L `.
0
0
However, it is easily verified that any stack ranks ` higher than ` (because w (`, ` ) = 8)
0
and ` higher than `0 (because w (`, `0 ) = 8). Thus, there is an L-cycle ` L ` L ` L `0 L `,
contradicting the assumption that L is a stack.
In order to show that  satisfies , consider an arbitrary clause c. As d attains c via
Sc = (d, `0 , `, c) and w (c, d) = 6, we have that w (`, yj )  6. By definition of w (, ), this
implies that literal ` appears in clause c. Furthermore, ` is set to true under  because ` is
contained in Sc .
() For the other direction, assume that  is satisfiable and let  be a satisfying
assignment. We use  to construct a completion T = (V , w )  [G ] with RP (T ) = {d}.
526

fiPossible and Necessary Winners of Partial Tournaments

On partial edges, the weight function w is defined as follows. If literal ` is set to true
under , let w (`0 , `) = 7 and w (`, `0 ) = 1. Otherwise, let w (`0 , `) = 1 and w (`, `0 ) = 7.
We now show that RP (T ) = {d} by going through the procedure that constructs the
transitive tournament T 0 , starting with the empty tournament on V .13 First, the set of
all edges with weight 7 or more will be added, because there are no cycles among those
edges. This set consists of the heavy edges and the (previously) partial edges. Next, the
medium edges are considered. All these edges are of the form (c, d) where c is an alternative
corresponding to a clause. Since  is a satisfying assignment, T 0 already contains paths from
d to every clause alternative c. Therefore, all of the edges (c, d) with c  C will be discarded.
In the next step, all light edges (i.e., edges of weight 5) are considered. All of those edges
are of the form (d, `) for a literal `. Therefore, all of those edges can be added to T 0 without
creating a cycle (d has no ingoing edges in T 0 ). After adding the light edges, d has an
outgoing edge to all literal alternatives ` and to all auxiliary alternatives `0 . Furthermore,
all edges from a clause candidate c to d have already been discarded. Thus, d is the unique
undominated alternative in T 0 , i.e., RP (T ) = {d}.
Since the ranked pairs method is resolute, hardness of PWSRP follows immediately.
Corollary 2. PWSRP is NP-complete.
Computing necessary ranked pairs winners turns out to be coNP-complete. This is again
somewhat surprising, as computing necessary winners is often considerably easier than computing possible winners, for both partial tournaments and partial preference profiles (Xia
& Conitzer, 2011).
Theorem 15. NWRP is coNP-complete.
Proof. Membership in coNP is again obvious. For hardness, we give a reduction from
UnSat that is a slight variation of the reduction in the proof of Theorem 14. Let G0 be
the partial 8-weighted tournament that results form G by adding a new alternative d
which has heavy edges to all alternatives in V except d. Furthermore, there is a light edge
from d to d . We show that d is a necessary ranked pairs winner in G0 if and only if  is
unsatisfiable.
() Assume for contradiction that NWRP (G0 ) = {d } and  is satisfiable. Let  be
a satisfying assignment and define the tournament T = (V  {d }, w0 )  [G0 ] such that
w0 coincides with w (as defined in the proof of Theorem 14) for all partial edges. By the
same arguments as in the proof of Theorem 14, it follows that d does not have any ingoing
edges in the tournament T 0 constructed by the ranked pairs procedure. At the point in
time when the edge (d, d ) is considered, it will be added to T 0 . This yields RP (T ) = {d},
contradicting the assumption that NWRP (G0 ) = {d }.
() Assume for contradiction that  is unsatisfiable and there exists a completion
T  [G0 ] with RP (T ) = {x} 6= {d }. It follows that x = d. (All alternatives in V \ {d}
have an incoming heavy edge (from d ), and all heavy edges will be added because there
is no cycle among them.) By the same argument as in the proof of Theorem 14, it follows
that  is satisfiable, contradicting our assumption.
13. The following arguments are independent of the choice of a particular tie-breaking rule.

527

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

6. Possible Winning Subsets
We considered the problem whether a subset of alternatives is a possible winning set (PWS).
In addition, it may be of interest whether a subset of alternatives is among the winners in
some completion, i.e., whether there is a completion for which all the alternatives in the
subset (and possibly other alternatives) are in the choice set. We will refer to the latter
problem as PWSS (possible winning subset). We note that an oracle to solve PWSS can
be used to solve PW. If we want to check whether i  PW (G), we simply check whether
{i}  PWSS (G). We are not aware of any direct algorithmic relation between the problems
PWS and PWSS.
We examined the computational complexity of PWSSS for most of the solution concepts
considered in this article. Since the arguments are often very similar to proofs already given,
we briefly summarize our findings here.
COND As there is never more than one Condorcet winner, every X  PWSSCOND (G) is a
singleton and the problem reduces to computing PWCOND (G).
CNL For PWSSCNL , note that for a nonempty set X  V , X 
/ PWSSCNL (G) if and
1
only if |V | > 1 and every completion of G
has a Condorcet winner which is
furthermore located in X. Therefore,


or
|V | = 1
1
X  PWSSCNL (G) if and only if
  PWSCOND (G )
or


1
PWCOND (G ) \ X 6= .
CO Just as for the other problems, polynomial computability of PWSSCO follows from
the corresponding result for PWSSBO .
TC The problem PWSSTC can be solved in polynomial time. In fact, it can be shown
that for a partial tournament G and a set of alternatives X, it is sufficient to check
whether X  PWTC (G) (with an additional argument if |X| = 2) in order to
determine whether X  PWSSTC (G).
BO The argument and algorithm for checking whether X  PWSSBO (G) is almost
the same as the argument for PWSBO in Theorem 9. The only difference is that
sBO (v, T ) may now be up to s instead of s  1 for v  V \ X in T  [G].
Consequently, we only need to redefine bs (v) to s  sBO (v, G) for all v  V \ X.
MM The proof for efficient computability of checking whether X  PWSMM (G) can be
modified to accommodate PWSSMM . More precisely, the second basic assumption
is now w(i, j)  s for i 
/ X, j  V . For s = n2 , it is sufficient to check whether G0
is an extension of G. For s < n2 , edges {i, {i, j}} with i  X are now contained in
s if w(i, j)  s . The rest of the argument can be adjusted appropriately. For
EH
s > n2 , nothing changes.
RP Since PWRP is NP-complete (Theorem 14), we get NP-hardness of PWSSRP by
the oracle argument above. Since membership in NP is obvious, the problem is
NP-complete.
528

fiPossible and Necessary Winners of Partial Tournaments

The complexity of PWSSUC is left open. Minor modification of our hardness construction for PWSUC will not do the trick. In that argument, the crucial question was whether
there is a completion that excludes certain alternatives from the choice set. This does not
help for PWSSUC .

7. Discussion
The problem of computing possible and necessary winners for partial preference profiles has
recently received a lot of attention. In this article, we have investigated this problem in a
setting where partially specified (weighted or unweighted) tournaments instead of profiles
are given as input. We have summarized our findings in Table 1.
A key conclusion is that computational problems for partial tournaments can be significantly easier than their counterparts for partial profiles. For example, possible Borda or
maximin winners can be found efficiently for partial tournaments, whereas the corresponding problems for partial profiles are NP-complete (Xia & Conitzer, 2011). Furthermore,
computing possible and necessary Copeland winners is NP-hard and coNP-hard respectively for partial preference profiles (Xia & Conitzer, 2011). In contrast, we showed that
even PWSCO can be solved in polynomial time for partial tournaments. As for negative
(hardness) results, they can be tempered by the fact that when some parameters of the
problem are bounded by a constant, some of these hard problems may be solved in polynomial time. In particular, Yang and Guo (2013) have shown that PWSUC is polynomial-time
solvable if the size of the given subset X is bounded by a constant.14
While tractability of the winner determination problem is necessary for tractability of
the possible or necessary winners problems, the results for ranked pairs in Section 5.3 show
that it is not sufficient. We further considered the problem of deciding whether a given
subset of alternatives equals the winning set for some completion of the partial tournament.
The results for the uncovered set in Section 4.4 imply that this problem cannot be reduced
in polynomial time to the computation of possible or necessary winners; whether a reduction
exists in the opposite direction remains an open problem.
Partial tournaments have also been studied in their own right, independent of their
possible completions. For instance, Peris and Subiza (1999) and Dutta and Laslier (1999)
have generalized several solution concepts on tournaments to partial tournaments. The
common point with the approach we follow here is the nature of the input, namely, partial tournaments. However, Peris and Subiza (1999) and Dutta and Laslier (1999) define
solution concepts for partial tournaments by directly generalizing the usual definition on
tournaments. This is in contrast to our definitions, which are based on the completions of
the input partial tournament. The notion of possible winners suggests a canonical way to
generalize any solution concept defined on tournaments to partial tournaments. This way
of extending tournament solutions to partial tournaments is referred to as the conservative
extension and inherits various axiomatic properties which the original tournament solutions satisfies for tournaments (Brandt et al., 2014). The positive computational results in
this article are an indication that this may be a promising approach.
14. Yang and Guo (2013) also give hardness and fixed-parameter tractability results for a generalization of
the Banks set to partial tournaments.

529

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

We also highlight another way of viewing algorithmic results concerning possible and
necessary winners. There is a burgeoning literature in computational social choice which
deals with the problem of manipulation and control in voting (Bartholdi, III, Tovey, & Trick,
1989, 1992; Faliszewski & Procaccia, 2010). If a given alternative is already a necessary winner, then there is no need to invest effort into influencing the remaining comparisons or votes
to make it winning. Moreover, our results also have implications on a partial tournament
version of the coalitional manipulation problem: coalitional tournament manipulation, in
its constructive version, is defined as follows. Given a partial tournament (V, E), a subset X  V , and a distinguished alternative x, is there a way to complete the missing edges
in X  X such that x is a winner? Informally, do the players in X have a way of fixing the
winners of the matches among themselves so as to make x win?
Constructive coalitional tournament manipulation is polynomial-time solvable whenever
PW is. Likewise, the destructive version of coalitional tournament manipulation (is there a
way to complete the edges within X such that candidate x is not winning?) is polynomial
whenever NW is.
Regarding future work, we have not yet examined the complexity of computing possible
and necessary winners for some attractive tournament solutions such as the minimal covering
set and weighted versions of the top cycle and the uncovered set (De Donder, Le Breton, &
Truchon, 2000).15
An interesting related question that goes beyond the computation of possible and necessary winners is the following: when the winners are not yet fully determined, which unknown
comparisons need to be learned, which pairs of candidates do we have to compare, or which
matches should be played? This problem can be seen as a tournament-based version of
the preference elicitation problem (Conitzer & Sandholm, 2002; Ding & Lin, 2013; Walsh,
2008). While the standard version of the problem looks for minimal sets of queries to voters
about pairwise preferences between candidates, in the tournament version a query bears on
a pair of candidates and its output is an edge between these two candidates, in one direction
or the other. Procaccia (2008) considers a similar question for COND. The construction of
a policy tree defining an optimal protocol minimizing the number of questions to be asked
or the number of matches to be played, in the worst case or on average, is an even more
challenging issue that we leave for future research.

Acknowledgments
Previous versions of this paper have been presented at the 11th International Conference
on Autonomous Agents and Multi-Agent Systems (AAMAS 2012) and at the 4th International Workshop on Computational Social Choice (COMSOC 2012). We are grateful to Felix
Brandt for extensive discussions and useful advice. We also thank Gerhard Woeginger for
hints towards improving our previous pseudo-polynomial time algorithms for PWSBO and
PWSMM and various anonymous reviewers, whose comments greatly helped us to improve
our paper. This material is based on work supported by the Deutsche Forschungsgemeinschaft under grants BR 2312/9-1, BR 2312/10-1, and FI 1664/1-1. Haris Aziz is supported
by the Australian Government as represented by the Department of Broadband, Commu15. Brill, Freeman, and Conitzer (2016) have recently shown that computing possible and necessary winners
for the bipartisan set (Laffond, Laslier, & Le Breton, 1993) is intractable.

530

fiPossible and Necessary Winners of Partial Tournaments

nications and the Digital Economy and the Australian Research Council through the ICT
Centre of Excellence program. Markus Brill has been supported by a Feodor Lynen research
fellowship of the Alexander von Humboldt Foundation and by the ERC under Starting Grant
639945 (ACCORD). Jerome Lang has been supported by the ANR project CoCoRICoCoDec. Paul Harrenstein has been supported by the ERC under Advanced Grant 291528
(RACE).

References
Aziz, H., Gaspers, S., Mackenzie, S., Mattei, N., Stursberg, P., & Walsh, T. (2014). Fixing
a balanced knockout tournament. In Proceedings of the 28th AAAI Conference on
Artificial Intelligence (pp. 552558). AAAI Press.
Aziz, H., Walsh, T., & Xia, L. (2015). Possible and necessary allocations via sequential
mechanisms. In Proceedings of the 23rd International Joint Conference on Artificial
Intelligence (pp. 468474).
Bachrach, Y., Betzler, N., & Faliszewski, P. (2010). Probabilistic possible winner determination. In Proceedings of the 24th AAAI Conference on Artificial Intelligence (pp.
697702). AAAI Press.
Bartholdi, III, J., Tovey, C. A., & Trick, M. A. (1989). The computational difficulty of
manipulating an election. Social Choice and Welfare, 6 (3), 227241.
Bartholdi, III, J., Tovey, C. A., & Trick, M. A. (1992). How hard is it to control an election?
Mathematical and Computer Modelling, 16 (89), 2740.
Baumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012). Campaigns for lazy voters:
truncated ballots. In Proceedings of the 11th International Conference on Autonomous
Agents and Multi-Agent Systems (pp. 577584). IFAAMAS.
Baumeister, D., & Rothe, J. (2010). Taking the final step to a full dichotomy of the possible
winner problem in pure scoring rules. In Proceedings of the 19th European Conference
on Artificial Intelligence (pp. 10191020).
Betzler, N., & Dorn, B. (2010). Towards a dichotomy for the possible winner problem in
elections based on scoring rules. Journal of Computer and System Sciences, 76 (8),
812836.
Betzler, N., Hemmann, S., & Niedermeier, R. (2009). A multivariate complexity analysis
of determining possible winners given incomplete votes. In Proceedings of the 21st
International Joint Conference on Artificial Intelligence (pp. 5358). AAAI Press.
Brandt, F., Brill, M., & Harrenstein, P. (2014). Extending tournament solutions. In
Proceedings of the 28th AAAI Conference on Artificial Intelligence (pp. 580586).
AAAI Press.
Brandt, F., Brill, M., & Harrenstein, P. (2016). Tournament solutions. In F. Brandt,
V. Conitzer, U. Endriss, J. Lang, & A. D. Procaccia (Eds.), Handbook of Computational Social Choice (chap. 3). Cambridge University Press. (Forthcoming)
531

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Brill, M., & Fischer, F. (2012). The price of neutrality for the ranked pairs method. In
Proceedings of the 26th AAAI Conference on Artificial Intelligence (pp. 12991305).
AAAI Press.
Brill, M., Freeman, R., & Conitzer, V. (2016). Computing possible and necessary equilibrium actions (and bipartisan set winners). In Proceedings of the 30th AAAI Conference
on Artificial Intelligence. AAAI Press. (Forthcoming)
Chevaleyre, Y., Lang, J., Maudet, N., & Monnot, J. (2011). Compilation and communication protocols for voting rules with a dynamic set of candidates. In Proceedings of the
13h Conference on Theoretical Aspects of Rationality and Knowledge (pp. 153160).
Chevaleyre, Y., Lang, J., Maudet, N., Monnot, J., & Xia, L. (2012). New candidates welcome! Possible winners with respect to the addition of new candidates. Mathematical
Social Sciences, 64 (1), 7488.
Conitzer, V., & Sandholm, T. (2002). Vote elicitation: Complexity and strategy-proofness.
In Proceedings of the 18th National Conference on Artificial Intelligence (pp. 392
397). AAAI Press.
Cook, W. J., Cunningham, W. H., Pulleyblank, W. R., & Schrijver, A. (1998). Combinatorial optimization. Wiley and Sons.
De Donder, P., Le Breton, M., & Truchon, M. (2000). Choosing from a weighted tournament. Mathematical Social Sciences, 40 (1), 85109.
Ding, N., & Lin, F. (2013). Voting with partial information: what questions to ask? In
Proceedings of the 12th International Conference on Autonomous Agents and MultiAgent Systems (pp. 12371238). IFAAMAS.
Dutta, B., & Laslier, J.-F. (1999). Comparison functions and choice correspondences. Social
Choice and Welfare, 16 (4), 513532.
Edmonds, J. (1965). Paths, trees and flowers. Canadian Journal of Mathematics, 17 ,
449467.
Faliszewski, P., & Procaccia, A. D. (2010). AIs war on manipulation: Are we winning? AI
Magazine, 31 (4), 5364.
Filmus, Y., & Oren, J. (2014). Efficient voting via the top-k elicitation scheme: a probabilistic approach. In Proceedings of the 15th ACM Conference on Economics and
Computation (pp. 295312). ACM Press.
Good, I. J. (1971). A note on Condorcet sets. Public Choice, 10 (1), 97101.
Hazon, N., Aumann, Y., Kraus, S., & Wooldridge, M. (2012). On the evaluation of election
outcomes under uncertainty. Artificial Intelligence, 189 , 118.
Kalech, M., Kraus, S., Kaminka, G. A., & Goldman, C. V. (2011). Practical voting rules
with partial information. Journal of Autonomous Agents and Multi-Agent Systems,
22 (1), 151182.

532

fiPossible and Necessary Winners of Partial Tournaments

Kern, W., & Paulusma, D. (2004). The computational complexity of the elimination
problem in generalized sports competitions. Discrete Optimization, 1 (2), 205214.
Konczak, K., & Lang, J. (2005). Voting procedures with incomplete preferences. In
Proceedings of the Multidisciplinary Workshop on Advances in Preference Handling
(pp. 124129).
Laffond, G., Laslier, J.-F., & Le Breton, M. (1993). The bipartisan set of a tournament
game. Games and Economic Behavior , 5 (1), 182201.
Lang, J., Pini, M. S., Rossi, F., Salvagnin, D., Venable, K. B., & Walsh, T. (2012). Winner
determination in voting trees with incomplete preferences and weighted votes. Journal
of Autonomous Agents and Multi-Agent Systems, 25 (1), 130157.
Laslier, J.-F. (1997). Tournament solutions and majority voting. Springer-Verlag.
Lu, T., & Boutilier, C. (2011). Vote elicitation with probabilistic preference models: Empirical estimation and cost tradeoffs. In Proceedings of the 2nd International Conference
on Algorithmic Decision Theory (pp. 135149). Springer-Verlag.
Lu, T., & Boutilier, C. (2013). Multiwinner social choice with incomplete preferences. In
Proceedings of the 23rd International Joint Conference on Artificial Intelligence (pp.
263270). AAAI Press.
Oren, J., Filmus, Y., & Boutilier, C. (2013). Efficient vote elicitation under candidate
uncertainty. In Proceedings of the 23rd International Joint Conference on Artificial
Intelligence (pp. 309316). AAAI Press.
Peris, J. E., & Subiza, B. (1999). Condorcet choice correspondences for weak tournaments.
Social Choice and Welfare, 16 (2), 217231.
Pini, M. S., Rossi, F., Venable, K. B., & Walsh, T. (2011). Possible and necessary winners
in voting trees: Majority graphs vs. profiles. In Proceedings of the 10th International
Conference on Autonomous Agents and Multi-Agent Systems (pp. 311318). IFAAMAS.
Procaccia, A. (2008). A note on the query complexity of the Condorcet winner. Information
Processing Letters, 108 (6), 390393.
Rastegari, B., Condon, A., Immorlica, N., & Leyton-Brown, K. (2013). Two-sided matching
with partial information. In Proceedings of the 14th ACM Conference on Electronic
Commerce (pp. 733750). ACM Press.
Schrijver, A. (2003). Combinatorial optimizationpolyhedra and efficiency. Springer.
Schwartz, B. L. (1966). Possible winners in partially completed tournaments. SIAM Review ,
8 (3), 302308.
Schwartz, T. (1986). The logic of collective choice. Columbia University Press.
Smith, J. H. (1973). Aggregation of preferences with variable electorate. Econometrica,
41 (6), 10271041.

533

fiAziz, Brill, Fischer, Harrenstein, Lang, & Seedig

Tideman, T. N. (1987). Independence of clones as a criterion for voting rules. Social Choice
and Welfare, 4 (3), 185206.
Vu, T., Altman, A., & Shoham, Y. (2009). On the complexity of schedule control problems
for knockout tournaments. In Proceedings of the 8th International Conference on
Autonomous Agents and Multi-Agent Systems (pp. 225232). IFAAMAS.
Walsh, T. (2007). Uncertainty in preference elicitation and aggregation. In Proceedings of
the 22nd AAAI Conference on Artificial Intelligence (pp. 38). AAAI Press.
Walsh, T. (2008). Complexity of terminating preference elicitation. In Proceedings of the
7th International Conference on Autonomous Agents and Multi-Agent Systems (pp.
967974). IFAAMAS.
Xia, L., & Conitzer, V. (2011). Determining possible and necessary winners under common
voting rules given partial orders. Journal of Artificial Intelligence Research, 41 , 25
67.
Yang, Y., & Guo, J. (2013). Possible winner problems on partial tournaments: A parameterized study. In Proceedings of the 3rd International Conference on Algorithmic
Decision Theory (Vol. 8176, pp. 425439). Springer-Verlag.
Zavist, T. M., & Tideman, T. N. (1989). Complete independence of clones in the ranked
pairs rule. Social Choice and Welfare, 6 (2), 167173.

534

fiJournal of Artificial Intelligence Research 54 (2015) 1-57

Submitted 09/14; published 09/15

Knowledge-Based Textual Inference via
Parse-Tree Transformations
Roy Bar-Haim

barhair@gmail.com

Ido Dagan

dagan@cs.biu.ac.il

Computer Science Department, Bar-Ilan University
Ramat-Gan 52900, Israel

Jonathan Berant

yonatan@cs.stanford.edu

Computer Science Department, Stanford University

Abstract
Textual inference is an important component in many applications for understanding
natural language. Classical approaches to textual inference rely on logical representations
for meaning, which may be regarded as external to the natural language itself. However,
practical applications usually adopt shallower lexical or lexical-syntactic representations,
which correspond closely to language structure. In many cases, such approaches lack a principled meaning representation and inference framework. We describe an inference formalism
that operates directly on language-based structures, particularly syntactic parse trees. New
trees are generated by applying inference rules, which provide a unified representation for
varying types of inferences. We use manual and automatic methods to generate these rules,
which cover generic linguistic structures as well as specific lexical-based inferences. We also
present a novel packed data-structure and a corresponding inference algorithm that allows
efficient implementation of this formalism. We proved the correctness of the new algorithm
and established its efficiency analytically and empirically. The utility of our approach was
illustrated on two tasks: unsupervised relation extraction from a large corpus, and the
Recognizing Textual Entailment (RTE) benchmarks.

1. Introduction
Textual inference in Natural Language Processing (NLP) is concerned with deriving target
meanings from texts. In the textual entailment framework (Dagan, Roth, Sammons, &
Zanzotto, 2013), this is reduced to inferring a textual statement (the hypothesis h) from
a source text (t). Traditional approaches in formal semantics perform such inferences over
logical forms derived from the text. By contrast, most practical NLP applications avoid the
complexities of logical interpretation. Instead, they operate over shallower representations
such as parse trees, possibly supplemented with limited semantic information about named
entities, semantic roles, and so forth. This was clearly demonstrated in the recent PASCAL
Recognizing Textual Entailment (RTE) Challenges (Dagan, Glickman, & Magnini, 2006b;
Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, & Szpektor, 2006; Giampiccolo,
Magnini, Dagan, & Dolan, 2007; Giampiccolo, Trang Dang, Magnini, Dagan, & Dolan,
2008; Bentivogli, Dagan, Dang, Giampiccolo, & Magnini, 2009; Bentivogli, Clark, Dagan,
c
2015
AI Access Foundation. All rights reserved.

fiBar-Haim, Dagan & Berant

Dang, & Giampiccolo, 2010), a popular framework for evaluating application-independent
semantic inference.1
Inference over such representations is commonly made by applying transformations or
substitutions to the tree or graph representing the text. These transformations are based on
available knowledge about paraphrases, lexical relations such as synonyms and hyponyms,
syntactic variations, and more (de Salvo Braz, Girju, Punyakanok, Roth, & Sammons,
2005; Haghighi, Ng, & Manning, 2005; Kouylekov & Magnini, 2005; Harmeling, 2009).
Such transformations may be generally viewed as inference rules. Some of the available semantic knowledge bases were composed manually, either by experts, for example WordNet
(Fellbaum, 1998), or by a large community of contributors, such as the Wikipedia-based
DBPedia resource (Lehmann et al., 2009). Other knowledge bases were learned automatically through distributional and pattern-based methods, or by using aligned monolingual
or bilingual parallel texts (Lin & Pantel, 2001; Shinyama, Sekine, Sudo, & Grishman,
2002; Szpektor, Tanev, Dagan, & Coppola, 2004; Chklovski & Pantel, 2004; Bhagat &
Ravichandran, 2008; Ganitkevitch, Van Durme, & Callison-Burch, 2013). Overall, applied
knowledge-based inference is a prominent line of research that has gained much interest. Recent examples include the series of workshops on Knowledge and Reasoning for Answering
Questions (Saint-Dizier & Mehta-Melkar, 2011) and the evaluation of knowledge resources
in the recent Recognizing Textual Entailment challenges (Bentivogli et al., 2010).
While many applied systems use semantic knowledge through such inference rules, their
use is typically limited, application-specific, and somewhat heuristic. Formalizing these
practices is important for textual inference research, analogous to the role of well-formalized
models in parsing and machine translation. We take a step in this direction by introducing
a generic inference formalism over parse trees. Our formalism uses inference rules to capture
a wide variety of inference knowledge in a simple and uniform manner, and specifies a small
set of operations that suffice to broadly utilize such knowledge.
In our formalism, applying an inference rule has a clear, intuitive interpretation of generating a new sentence parse (a consequent), semantically entailed by the source sentence.
The inferred consequent may be subject to further rule applications, and so on. Rule applications may be independent of each other, modifying disjoint parts of the source tree, or
may specify mutually-exclusive alternatives (e.g., different synonyms for the same source
word). Deriving the hypothesis from the text is analogous to proof search in logic, where
the propositions are parse trees and deduction steps correspond to rule applications.
A nave implementation of the formalism would generate each consequent explicitly as
a separate tree. However, as we discuss further in Section 5, such implementation raises
severe efficiency issues, since the number of consequents may grow exponentially in the
number of possible rule applications. Previous work proposed only partial solutions to this
problem (cf. Section 8). In this work we present a novel data-structure, termed compact
forest, for packed representation of entailed consequents, and a corresponding inference
algorithm. We prove that the new algorithm is a valid implementation of the formalism,
and establish its efficiency both analytically, showing typical exponential-to-linear reduction,
and empirically, showing improvement by orders of magnitude. Together, our formalism and
1. See, for instance, the listing of techniques per submission that was provided by the organizers of the first
three challenges (Dagan et al., 2006b; Bar-Haim et al., 2006; Giampiccolo et al., 2007).

2

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

its novel efficient inference algorithm open the way for large-scale rule application within a
well-formalized framework.
Based on our formalism and inference algorithm, we built an inference engine that
incorporates a variety of semantic and syntactic knowledge bases (cf. Section 6). We
evaluated the inference engine on the following tasks:
1. Unsupervised relation extraction from a large corpus. This setting allows evaluation
of knowledge-based inferences over a real-world distribution of texts.
2. Recognizing textual entailment (RTE). To cope with the more complex RTE examples, we complemented our knowledge-based inference engine with a machine-learningbased entailment classifier, which provides necessary approximate matching capabilities.
The inference engine was shown to have a substantial contribution to both tasks, illustrating
the utility of our approach.
Bar-Haim, Dagan, Greental, and Shnarch (2007) and Bar-Haim, Berant, and Dagan
(2009) described earlier versions of the inference framework and the algorithm for its efficient implementation, respectively. The current article includes major enhancements to
both of these contributions. The formalism is presented in more detail, including further
examples and pseudo-code for its algorithms. We present several extensions to the formalism, including treatment of co-reference, traces and long-range dependencies, and enhanced
modeling of polarity. The efficient inference algorithm is also presented in more detail,
including its pseudo-code. In addition, we provide complete proofs for the theorems, which
establish the correctness of this algorithm. Finally, the article contains an extended analysis
of the inference component in our RTE system, in terms of applicability, coverage, and the
correctness of rule applications.

2. Background
In this section, we provide background on textual entailment. We then survey the various
approaches applied to the task of Recognizing Textual Entailment (RTE). In particular, we
focus on the use of semantic knowledge within current RTE systems.
2.1 Textual Entailment
Many semantic applications need to identify that the same meaning is expressed by, or can
be inferred from, various language expressions. For example, Question-Answering systems
need to verify that the retrieved passage text entails the selected answer. Given the question
Who is John Lennons widow?, the text Yoko Ono unveiled a bronze statue of her late
husband, John Lennon, to complete the official renaming of Englands Liverpool Airport as
Liverpool John Lennon Airport. entails the expected answer Yoko Ono is John Lennons
widow 2 . Similarly, Information Extraction systems need to validate that the given text
indeed entails the semantic relation that is expected to hold between the extracted slot fillers
(e.g., X works for Y ). Information Retrieval queries such as Alzheimers drug treatment3
2. The example is taken from the RTE-2 dataset (Bar-Haim et al., 2006).
3. This was one of the topics in the TREC-6 IR benchmark (Voorhees & Harman, 1997).

3

fiBar-Haim, Dagan & Berant

can be rephrased as propositions (e.g., Alzheimers disease is treated using drugs), which
are expected to be entailed from relevant documents. When selecting sentences to be
included in the summary, multi-document summarization systems should verify that the
meaning of the candidate sentence is not entailed by sentences already in the summary, to
avoid redundancy.
This observation led Dagan and Glickman to propose a unifying framework for modeling
language variability, termed Textual Entailment (TE) (Dagan & Glickman, 2004). Dagan
et al. (2006b) define TE as follows:
We say that t entails h if, typically, a human reading t would infer that h is most
likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge.
Dagan et al. (2013) further discuss TE definition and its relation to classical semantic
entailment in linguistics literature. The Recognizing Textual Entailment Challenges (RTE),
which have been held annually since 2004 (Dagan et al., 2006b; Bar-Haim et al., 2006;
Giampiccolo et al., 2007, 2008; Bentivogli et al., 2009, 2010), have formed a growing research
community around this task.
The holy grail of TE research is the development of entailment engines, to be used
as generic modules within different semantic applications, similar to the current use of
syntactic parsers and morphological analyzers. Since textual entailment is defined as a
relation between surface texts, it is not bound to a particular semantic representation.
This allows a black-box view of the entailment engine, where the input/output interface
is independent from the internal implementation, which may employ different types of
semantic representations and inference methods.
2.2 Determining Entailment
Consider the following (t,h) pair4 :
t
h

The oddest thing about the UAE is that only 500,000 of the 2 million
people living in the country are UAE citizens.
The population of the United Arab Emirates is 2 million.

Understanding that t  h involves several inference steps. First, we infer from the
reduced relative clause in 2 million people living in the country the proposition:
(1) 2 million people live in the country.
Next, we observe that the country refers to the UAE, so we can rewrite (1) as
(2) 2 million people live in the UAE.
Knowing that UAE is an acronym for United Arab Emirates, we further obtain:
(3) 2 million people live in the United Arab Emirates.
4. Taken from the RTE1 test set (Dagan et al., 2006b).

4

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

We finally paraphrase this to obtain h:
(4) The population of the United Arab Emirates is 2 million.
In general, textual inference involves diverse linguistic and world knowledge, including
knowledge of relevant syntactic phenomena (e.g., relative clauses), paraphrasing (X people
live in Y  the population of Y is X ), lexical knowledge (UAE  United Arab Emirates),
and so on. It may also require co-reference resolution, for example, substituting the country with UAE. We may think of all these types of knowledge as representing inference
rules that define derivation of new entailed propositions or consequents. In this work we
introduce a formal inference framework based on inference rule application. For the current
discussion, however, an informal notion of inference rules would suffice.
The above example illustrates the derivation of h from t through a sequence of inference
rule applications, a procedure generally known as forward chaining. Finding the sequence
of rule applications that would get us from t to h (or as close as possible) is thus a search
problem, defined over the space of all possible rule application chains.
Ideally, we would like to base our entailment engine solely on trusted knowledge-based
inferences. In practice, however, available knowledge is incomplete, and full derivation of h
from t is often not feasible. Therefore, requiring strict knowledge-based proofs is likely
to yield limited recall. Alternatively, we may back off to a more heuristic approximate
entailment classification.
The next two sections survey these two complementary inference types: knowledgebased inference, which is the focus of this research, and approximate entailment matching
and classification.
2.3 Knowledge-Based Inference
In this section, we describe some of the common resources for inference rules (2.3.1), and
their use in textual entailment systems (2.3.2).
2.3.1 Semantic Knowledge Resources
Lexical Knowledge Lexical-semantic relations between words or phrases play an important role in textual inference. The most prominent lexical resource is WordNet (Fellbaum,
1998), a manually composed wide-coverage lexical-semantic database. The following WordNet relations are typically used for inference: synonyms (buy  purchase), antonyms (win
 lose), hypernyms/hyponyms (is-a relations, violin musical instrument), meronyms
(part-of relations, Provence France) and derivations such as meeting meet.
Many researchers aimed at deriving lexical relations automatically, using diverse methods and sources. Much of this automatically-extracted knowledge is complementary to
WordNet, however, it is typically less accurate. Snow, Jurafsky, and Ng (2006a) presented
a method for automatically expanding WordNet with new synsets, achieving high precision.
Lins thesaurus (Lin, 1998) is based on distributional similarity. Recently, several works
aimed to extract lexical-semantic knowledge from Wikipedia, using its metadata, as well as
textual definitions (Kazama & Torisawa, 2007; Ponzetto & Strube, 2007; Shnarch, Barak,
& Dagan, 2009; Lehmann et al., 2009, and others). For a recent empirical study on the
5

fiBar-Haim, Dagan & Berant

inferential utility of common lexical resources, see the work of Mirkin, Dagan, and Shnarch
(2009).
Paraphrases and Lexical-Syntactic Inference Rules These rules typically represent
entailment or equivalence between predicates, including the correct mapping between their
arguments (e.g., acquisition of Y by X  X purchase Y ). Much work has been dedicated
to unsupervised learning of such relations from comparable corpora (Barzilay & McKeown, 2001; Barzilay & Lee, 2003; Pang, Knight, & Marcu, 2003), by querying the Web
(Ravichandran & Hovy, 2002; Szpektor et al., 2004), or from a local corpus (Lin & Pantel,
2001; Glickman & Dagan, 2003; Bhagat & Ravichandran, 2008; Szpektor & Dagan, 2008;
Yates & Etzioni, 2009). In particular, textual entailment systems have widely used the
DIRT resource of Lin and Pantel. The common idea underlying these algorithms, is that
predicates sharing the same argument instantiations are likely to be semantically related.
NomLex-Plus (Meyers, Reeves, Macleod, Szekeley, Zielinska, & Young, 2004) is a lexicon containing mostly nominalizations of verbs, with allowed argument structures (e.g.,
Xs acquisition of Y/Ys acquisition by X etc.). Argument-mapped WordNet (AmWN)
(Szpektor & Dagan, 2009) is a resource for inference rules between verbal and nominal predicates, including their argument mapping. It is based on WordNet and NomLex-Plus, and
was verified statistically through intersection with the unary-DIRT algorithm (Szpektor &
Dagan, 2008).
Syntactic Transformations Textual entailment often involves inference over generic
syntactic phenomena such as passive/active transformations, appositions, conjunctions, etc.,
as illustrated in the following examples:
 John smiled and laughed  John laughed (conjunction)
 My neighbor, John, came in  John is my neighbor (apposition)
 The paper that Im reading is interesting  Im reading a paper (relative clause).
Syntactic transformations have been addressed to some extent by de Salvo Braz et al.
(2005) and Romano, Kouylekov, Szpektor, Dagan, and Lavelli (2006). We describe a novel
syntactic rule base for entailment, based on a survey of relevant linguistic literature, as well
as on extensive data analysis (Sections 6.16.2).
2.3.2 The Use of Semantic Knowledge in Textual Entailment Systems
Following our description of common knowledge sources for textual inference, we now discuss
the use of such knowledge in textual entailment systems.
Textual entailment systems usually represent t and h as trees or graphs, based on their
syntactic parse, predicate-argument structure, and various semantic relations. Entailment
is then determined by measuring how well h is matched (or embedded ) in t, or by estimating
the distance between t and h, commonly defined as the cost of transforming t into h. In
the next section, we briefly cover various methods that have been proposed for approximate
matching and heuristic transformations of graphs and trees. The role of semantic knowledge
in this general scheme is to bridge the gaps between t and h that stem from language
variability. For example, applying the lexical-semantic rule purchase buy to t allows the
matching of the word buy appearing in h with the word purchase appearing in t.
6

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Most RTE systems restrict both the type of allowed inference rules and the search space.
Systems based on lexical (word-based or phrase-based) matching of h in t (Haghighi et al.,
2005; MacCartney, Galley, & Manning, 2008) or on heuristic transformation of t into h
(Kouylekov & Magnini, 2005; Harmeling, 2009) typically apply only lexical rules (without
variables), where both sides of the rule are matched directly in t and h.
Hickl (2008) derived from a given (t, h) pair a small set of consequents that he terms
discourse commitments. The commitments were generated by several different tools and
techniques, based on syntax (conjunctions, appositions, relative clauses, etc.), co-reference,
predicate-argument structure, the extraction of certain relations, and paraphrase acquisition
from the Web. Pairs of commitments derived from t and h were fed into the next stages
of the RTE system  lexical alignment and entailment classification. Prior to commitment
generation, several linguistic preprocessing modules were applied to the text, including
syntactic dependency parsing, semantic dependency parsing, named entity recognition, and
co-reference resolution. Hickl employed a probabilistic finite-state transducer (FST)-based
extraction framework for commitment generation, and extraction rules were modeled as a
series of weighted regular expressions. The commitments in their textual form were fed
back into the system, until no additional commitments were generated.
De Salvo Braz et al. (2005) were the first to incorporate syntactic and semantic inference
rules in a comprehensive entailment system. In their system, inference rules are applied over
hybrid syntactic-semantic structures called concept graphs. When the left hand side (LHS)
of a rule is matched in the concept graph, the graph is augmented with an instantiation
of the right hand side (RHS) of the rule. After several iterations of rule application, their
system attempts to embed the hypothesis in the augmented graph. Other types of semantic
knowledge, such as verb normalization and lexical substitutions, are applied either before
rule application (at preprocessing time) or after rule application, as part of hypothesis
subsumption (embedding).
Several entailment systems are based on logical inference. Bos and Markert (2005, 2006)
represented t and h as DRS structures used in Discourse Representation Theory (Kamp &
Reyle, 1993), which were then translated into first-order logic. Background knowledge
(BK) was encoded as axioms, and comprised lexical relations from WordNet, geographical
knowledge, and a small set of manually composed axioms encoding generic knowledge.
Bos and Markert used a logic theorem prover to find a proof that t entails h (alone or
together with the background knowledge BK), or that h and t are inconsistent with each
other (implying non-entailment) or with the background knowledge. The logic prover was
complemented by a model builder that aimed to find counter-examples (e.g., a model where
t  h holds). The logical inference system suffered from low coverage, due to the limited
background knowledge available, and was able to find proofs only for a small fraction of the
RTE2 dataset. Therefore, the RTE system of Bos and Markert combined logical inference
with a shallow approximate matching method, based mainly on word overlap.
LCCs logic-based entailment system (Tatu & Moldovan, 2006) was one of the top performers in RTE2 and RTE3 (Tatu, Iles, Slavick, Novischi, & Moldovan, 2006; Tatu &
Moldovan, 2007). It was based on proprietary tools for deriving rich semantic representations, and on extensive knowledge engineering. The syntactic parses of t and h were
transformed into logic forms (Moldovan & Rus, 2001), and this representation was enriched
with a variety of relations extracted by a semantic parser, as well as named entities and
7

fiBar-Haim, Dagan & Berant

temporal relations. Inference knowledge included on-demand axioms based on extended
WordNet lexical chains, WordNet glosses, and NLP rewrite rules. Additional knowledge
types included several hundreds of world knowledge axioms, temporal axioms, and semantic composition axioms (e.g., encoding the transitivity of the kinship relation). Based on
the rich semantic representation and the extensive set of axioms, a theorem prover aimed
to prove by refutation that t entails h. If the proof failed, h was repeatedly simplified until
a proof was found, reducing the proof score with each simplification.
2.4 Approximate Entailment Classification
Semantic knowledge is always incomplete. Therefore, in most cases, knowledge-based inference must be complemented with approximate, heuristic methods for determining entailment. Most RTE systems employ only a limited amount of semantic knowledge, and
focus on methods for approximate entailment classification. A common architecture for
RTE systems (Hickl, Bensley, Williams, Roberts, Rink, & Shi, 2006; Snow, Vanderwende,
& Menezes, 2006b; MacCartney, Grenager, de Marneffe, Cer, & Manning, 2006) comprises
the following stages:
1. Linguistic processing: Includes syntactic (and possibly semantic) parsing, namedentity recognition, co-reference resolution, etc. Often, t and h are represented as trees
or graphs, where nodes correspond to words and edges represent relations between
words.
2. Alignment: Find the best mapping from h nodes to t nodes, taking into account both
node and edge matching.
3. Entailment classification: Based on the alignment found, a set of features is extracted
and passed to a classifier for determining entailment. These features measure the
alignment quality, and also try to detect cues for false entailment. For example, if a
node in h is negated but its aligned node in t is not negated, it may indicate false
entailment.
An alternative approach aims to transform the text into the hypothesis, rather than
aligning them. Kouylekov and Magnini (2005) applied a tree edit distance algorithm for
textual entailment. Each edit operation (node insertion/deletion/substitution) is assigned a
cost. The algorithm aims to find the minimum-cost sequence of operations that transform
t into h. Mehdad and Magnini (2009b) proposed a method for estimating the cost of
each edit operation based on Particle Swarm Optimization. Wang and Manning (2010)
presented a probabilistic tree-edit approach that models edit operations using structured
latent variables. Tree edits are represented as state transitions in a Finite-State Machine
(FSM), and the model is parameterized as a Conditional Random Field (CRF). Harmeling
(2009) developed a probabilistic transformation-based approach. He defined a fixed set of
operations, including syntactic transformations, WordNet-based substitutions, and more
heuristic transformations such as adding/removing a verb or a noun. The probability of
each transformation was estimated from the development set. Similarly, Heilman and Smith
(2010) classify entailment based on the sequence of edits transforming t to h. They employ
more generic edit operations and a greedy search heuristic, which is guided by a cost function
that measures the remaining distance from h using a tree kernel.
8

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Zanzotto, Pennacchiotti, and Moschitti (2009) aimed to classify a given (t, h) pair by
analogy to similar pairs in the training set. Their method is based on finding intra-pair
alignment (i.e., between t and h) for capturing the transformation from t to h, and interpair alignment, capturing the analogy between the new pair (t, h) and a previously seen
pair (t0 , h0 ). A cross-pair similarity kernel is then computed, based on tree kernel similarity
applied to the aligned texts and the aligned hypotheses. Another cross-pair similarity kernel
was proposed by Wang and Neumann (2007). They extracted tree skeletons from t and h,
consisting of left and right spines, defined as unlexicalized paths starting at the root. They
then found sections where t and h spines differ and compared these sections across pairs
using a subsequence kernel.

3. Research Goal
The goal of textual entailment research is to develop entailment engines that can be used as
generic inference components within various text-understanding applications. Logic-based
entailment systems provide a formalized and expressive framework for textual inference.
However, deriving logic representations from text is a complex task, and available tools do
not match the accuracy and robustness of current syntactic parsers (which is often the basis
for semantic parsing). Furthermore, interpretation into logic forms is often unnecessary, as
many of the common inferences can be modeled with shallower representations.
It follows that most textual entailment systems (and text-understanding applications
in general) operate over lexical-syntactic representations, possibly supplemented with some
partial semantic annotation. However, unlike logic-based approaches, most of these systems
lack a clear, unified formalism for knowledge representation and inference; instead they
employ multiple representations and inference mechanisms. A notable exception is the
natural logic framework of MacCartney and Manning (2009), which has a rather different
focus than the current work. We discuss this further in Section 8.
In this work, we develop a well-formalized entailment approach for the lexical-syntactic
level. Our formalism models a wide variety of inference rules and their composition, based
on a unified representation and a small set of inference operations. Moreover, we present
an efficient implementation of this formalism using a novel data structure and algorithm
that allow compact representation of the proof search space.
We see the contribution of this work as both practical and theoretical. From a practical
(or engineering) perspective, our formalism may simplify the development of entailment
systems, as the number of representations and inference mechanisms that need to be dealt
with is minimal. Furthermore, our efficient implementation may allow entailment engines to
explore much larger search spaces. From a theoretical perspective, concise, formal modeling
leads to better insight into the phenomenon under investigation. In particular, having a
formal model of an entailment engine makes it possible to apply formal methods for investigating its properties. This enabled us to prove the correctness of the efficient implementation
of our formalism (cf. Appendix A). We next present our inference formalism.
9

fiBar-Haim, Dagan & Berant

Rule
Type
Syntactic

Sources

Examples

Manually-composed

Lexical

Learned with unsupervised algorithms (DIRT, TEASE), and
derived automatically by integrating information from WordNet and
Nomlex, verified using corpus
statistics (AmWN)
WordNet, Wikipedia

Passive/active, apposition, relative
clause, conjunctions
Xs wife, Y  X is married to Y

Syntactic

Lexical

X bought Y  Y was sold to X

X is a maker of Y  X produces Y
steal take, AlbanianAlbania
Janis Joplinsinger
AmazonSouth America

Table 1: Representing diverse knowledge types as inference rules

4. An Inference Formalism over Parse Trees
The previous sections highlighted the need for a more principled, well-formalized approach
for textual inference at the lexical-syntactic level. In this section, we propose a step towards
filling this gap, by defining a formalism for textual inference over parse-based representations. All semantic knowledge required for inference is represented as inference rules, which
encode parse tree transformations. Each rule application generates a new consequent sentence (represented as a parse tree) from a source tree. Figure 1b shows a sample inference
rule, representing a passive-to-active transformation.
From a knowledge representation and usage perspective, inference rules provide a simple
unifying formalism for representing and applying a very broad range of inference knowledge.
Some examples of this breadth are illustrated in Table 1. From a knowledge acquisition
perspective, representing inference rules at the lexical-syntactic level allows easy incorporation of rules learned by unsupervised methods, which is important for scaling inference
systems. Interpretation into stipulated semantic representations, which is often difficult and
is inherently a supervised semantic task for learning, is circumvented altogether. From a
historical machine translation perspective, our approach is similar to transfer-based translation, as contrasted with semantic interpretation into Interlingua. Our overall research goal
is to explore the reach of such an inference approach, and to identify the scope in which
semantic interpretation may not be needed.
Given a syntactically parsed source text and a set of inference rules, our formalism
defines the set of consequents derivable from the text using the rules. Each consequent is
obtained through a sequence of rule applications, each generating an intermediate parse
tree, similar to a proof process in logic. In addition, new consequents may be inferred based
on co-reference relations and identified traces. Our formalism also includes annotation rules
that add features to existing trees. According to the formalism, a text t entails a hypothesis
h if h is a consequent of t.
In the rest of this section, we define and illustrate each of the formalism components:
sentence representation (Section 4.1), inference rules and their application (Sections 4.2
4.3), inference based on co-reference relations and traces (Section 4.4), and annotation
10

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Input: a source tree s ; a rule E : L  R
Output: a set D of derived trees
M  the set of all matches of L in s
D
for each f  M do
l  the subtree matched by L in s according to match f
// R instantiation
r  a copy of R
for each variable v  r do
Instantiate v with f (v)
for each aligned pair of nodes uL  l and uR  r do
for each daughter m of uL such that m 
/ l do
Copy the subtree of s rooted in m under uR in r, with the same dependency relation
// Derived tree generation
if substitution rule then
d  s copy with l (and the descendants of its nodes) replaced by r
else // introduction rule
dr
add d to D

Algorithm 1: Applying a rule to a tree
rules (Section 4.5). These components form an inference process that specifies the set of
inferable consequents for a given text and a set of rules (Section 4.6). Section 4.7 extends
the hypothesis definition, allowing h to be a template rather than a proposition. Finally,
Section 4.8 discusses limitations and possible extensions of our formalism.
4.1 Sentence Representation
We assume that sentences are represented by some form of parse trees. In this work, we focus
on dependency tree representation, which is often preferred to directly capture predicateargument relations. Two dependency trees are shown in Figure 1a. Nodes represent words
and hold a set of features and their values. These features include the word lemma and
part-of-speech, and additional features that may be added during the inference process.
Edges are annotated with dependency relations.
4.2 Inference Rules
An entailment (or inference) rule L  R is primarily composed of two templates, lefthand-side (LHS) L and right-hand-side (RHS) R. Templates are dependency subtrees,
which may contain POS-tagged variables, matching any lemma. Figure 1 shows a passiveto-active transformation rule, and illustrates its application.
The rule application procedure is given in Algorithm 1. Rule application generates a set
D of derived trees (consequents) from a source tree s through the steps described below.
11

fiBar-Haim, Dagan & Berant

root


i

rain VERB

expletive

r

wha

it OTHER

,

when ADJ
i

r

Mary NOUN
mod

see VERB

obj

q


mod

 bysubj

be

be VERB

by PREP

,

yesterday NOUN

 pcompn


little ADJ

John NOUN

Source: It rained when little Mary was seen by John yesterday.

root
i



rain VERB

r

expletive

wha

it OTHER

,

when ADJ


i
subj

r

John NOUN

see VERB
 obj

mod

,

Mary NOUN yesterday NOUN
mod



little ADJ
Derived: It rained when John saw little Mary yesterday.

(a) Passive-to-active tree transformation


V VERB
obj

L

u

N1 NOUN

V VERB

bysubj
be

subj

obj



)

u

)

be VERB

by PREP

N2 NOUN

N1 NOUN

pcompn

R



N2 NOUN
(b) Passive to active substitution rule.
Figure 1: Application of an inference rule. POS and relation labels are based on Minipar
(Lin, 1998). N 1, N 2 and V are variables, whose instances in L and R are implicitly aligned.
The by-subj dependency relation indicates a passive sentence.

12

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

root

root



i

i



V1 VERB  V2 VERB
L



wha

R

when ADJ
i



V2 VERB
Figure 2: Temporal clausal modifier extraction (introduction rule)

4.2.1 L Matching
First, matches of L in the source tree s are sought. L is matched in s if there exists a
one-to-one node mapping function f from L to s, such that:
1. For each node u in L, f (u) has the same features and feature values as u. Variables
match any lemma value in f (u).
2. For each edge u  v in L, there is an edge f (u)  f (v) in s, with the same dependency
relation.
If matching fails, the rule is not applicable to s. In our example, the variable V is matched
in the verb see, N 1 is matched in Mary and N 2 is matched in John. If matching succeeds,
then the following is performed for each match found.
4.2.2 R Instantiation
a copy of R is generated and its variables are instantiated according to their matching node
in L. In addition, a rule may specify alignments, defined as a partial function from L nodes
to R nodes. An alignment indicates that for each modifier m of the source node that is not
part of the rule structure, the subtree rooted at m should also be copied as a modifier of the
target node. In addition to explicitly defining alignments, each variable in L is implicitly
aligned to its counterpart in R. In our example, the alignment between the V nodes implies
that yesterday (modifying see) should be copied to the generated sentence, and similarly
little (modifying Mary) is copied for N 1.
4.2.3 Derived Tree Generation
Let r be the instantiated R, along with its descendants copied from L through alignment,
and l be the subtree matched by L. The formalism has two methods for generating the
derived tree d: substitution and introduction, as specified by the rule type. Substitution
rules specify modification of a subtree of s, leaving the rest of s unchanged. Thus, d is
formed by copying s while replacing l (and the descendants of ls nodes) with r. This is
the case for the passive rule, as well as for lexical rules such as buy  purchase. By
contrast, introduction rules are used to make inferences from a subtree of s, while the other
parts of s are ignored and do not affect d. A typical example is inferring a proposition
embedded as a relative clause in s. In this case, the derived tree d is simply taken to be
13

fiBar-Haim, Dagan & Berant

root
i

root
i



buy VERB
subj



purchase VERB

obj

subj

obj

v

(

v

(

John NOUN

books NOUN

John NOUN

books NOUN

John bought books.

L

buy VERB

John purchased books.



purchase VERB

R

Figure 3: Application of a lexical substitution rule. The dotted arc represents explicit
alignment.

r. Figure 2 presents such a rule, which enables deriving propositions that are embedded
within temporal modifiers. Note that the derived tree does not depend on the main clause.
Applying this rule to the right part of Figure 1a yields the proposition John saw little
Mary yesterday.
4.3 Further Examples for Rule Application
In this section we further illustrate rule representation and application through additional
examples.
4.3.1 Lexical Substitution Rule with Explicit Alignment
Figure 3 shows the derivation of the consequent John purchased books from the sentence
John bought books using the lexical substitution rule buy  purchase. This example
illustrates the role of explicit alignment: since buy and purchase are not variables, they are
not implicitly aligned. However, they need to be aligned explicitly, otherwise the daughters
of buy would not be copied under purchase.
4.3.2 Lexical-Syntactic Introduction Rule
Figure 4 illustrates the application of a lexical-syntactic rule, which derives the sentence
Her husband died from I knew her late husband. It is defined as introduction rule, since
the resulting tree is derived based solely on the phrase Her late husband, while ignoring
the rest of the source tree. This example illustrates that a leaf variable in L (variable
at a leaf node) may become a non-leaf in R and vice versa. The alignment between the
instances of variable N (matched in husband ) allows copying of its modifier, her (recall that
such alignments are defined implicitly by the formalism). We note here that the correctness
of rule application may depend on the context in which it is applied. For instance, the
rule in our example is correct only if late has the meaning of no longer alive in the given
context. We discuss context-sensitivity of rule application in Section 4.8.
14

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

root

root

i

i



know VERB
subj

die VERB

obj

subj

v

(

I NOUN

husband NOUN
gen




husband NOUN

mod

v

(

her NOUN

late ADJ

gen



her NOUN

I knew her late husband.

Her husband died.

root
i

L



N NOUN

 die VERB



subj

mod

late ADJ



R

N NOUN

Figure 4: Application of a lexical-syntactic introduction rule

4.4 Co-Reference and Trace-Based Inference
Aside from the primary inference mechanism of rule application, our formalism also allows
inference based on co-reference relations and long-distance dependencies. We view coreference as an equivalence relation between complete subtrees, either within the same tree
or in different trees, which are linked by a co-reference chain. In practice, such relations are
obtained from an external co-reference resolution tool, as part of the text pre-processing.
The co-reference substitution operation is similar to the application of a substitution rule.
Given a pair of co-referring subtrees, t1 and t2 , the derived tree is generated by copying
the tree containing t1 , while replacing t1 with t2 ; the same operation is symmetrically
applicable for t2 .5 For example, given the sentences [My brother] is a musician. [He] plays
the drums, we can infer that My brother plays the drums.
Long-distance dependencies are another type of useful relation for inference, as illustrated by the following examples:
(1) Relative clause: The boyi whom [I saw ti ] went home.
( I saw the boy.)
(2) Control verbs: Johni managed to [ti open the door].
( John opened the door.)
5. The view of co-referring expressions as substitutional can also be found in the seminal paper of van
Deemter and Kibble (2000), where noun phrases are shown to be non-substitutable as evidence that
they are not co-referring.

15

fiBar-Haim, Dagan & Berant

(3) Verbal conjunction: [Johni sang] and [ti danced].
( John danced.)
Some parsers including Minipar, which we use in the current work, recognize and annotate
such long distance dependencies. For instance, Minipar generates a node representing the
trace (ti in the examples), which holds a pointer to its antecedent (e.g., Johni in (2)). As
shown in these examples, inference from such sentences may involve resolving long- distance
dependencies, where traces are substituted with their antecedent. Thus, we can generalize
co-reference substitution to operate over trace-antecedent pairs, as well. This mechanism
works together with inference rule application. For instance, after substituting the trace
with its antecedent in (2) we obtain John managed to [John opened the door]. We then
apply the introduction rule N managed to S  S to extract the embedded clause John
opened the door.
4.5 Polarity Annotation Rules
In addition to inference rules, our formalism implementation includes a mechanism for
adding semantic features to parse tree nodes. However, in many cases there is no natural
way to define semantic features or classes. Hence, it is often difficult to agree on the right
set of semantic annotations (a common example is the definition of word senses). With
our approach, we aim to keep semantic annotation to a minimum, while sticking to lexicalsyntactic representation, for which widely-agreed schemes do exist.
Consequently, the only semantic annotation we employ is predicate polarity. This feature
marks the truth of a predicate, and may take one of the following values: positive(+),
negative(-) or unknown(?). Some examples of polarity annotation are shown below:
(4) John called[+] Mary.
(5) John hasnt called[] Mary yet.
(6) John forgot to call[] Mary.
(7) John might have called[?] Mary.
(8) John wanted to call[?] Mary.
Sentences (5) and (6) both entail John didnt call Mary, hence the negative annotation of
call. By contrast, the truth of John called Mary cannot be determined from (7) and (8),
therefore the predicate call is marked as unknown. In general, the polarity of predicates
may be affected by the existence of modals, negation, conditionals, certain verbs, etc.
Technically, annotation rules do not have a right-hand-side R, but rather each node of L
may contain annotation features. If L is matched in a tree, then the annotations it contains
are copied to the matched nodes. Figure 5 shows an example of annotation rule application.
Predicates are assumed to have positive polarity by default. The polarity rules are used
to mark negative or unknown polarity. If more than one rule applies to the same predicate
(as with the sentence John forgot not to call Mary), they may be applied in any order,
and the following simple calculus is employed to combine current polarity with new polarity:
16

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

root
i

V[]
L

be



listen[]
subj

VERB

VERB

be



v

(

be VERB

John NOUN

be VERB

neg

neg



not ADJ



not ADJ
John is not listening[] .

(a) Annotation rule

(b) Annotated sentence

Figure 5: Application of the annotation rule (a), marking the predicate listen with negative
polarity (b)

Current polarity
+

?
+/  /?

New polarity



?

Result

+
?
?

Annotation rules are used for detecting polarity mismatches between the text and the hypothesis. Incompatible polarity would block the hypothesis from being matched in the text.
In the case of approximate entailment classification, polarity mismatches detected by the
annotation rules are used as features for the classifier, as we discuss further in Section 7.3. In
addition, the existence of polarity annotation features may prevent inappropriate inference
rule applications, by blocking their L matching. We discuss this further in Section 6.1.
4.6 The Inference Process
Let T be a set of dependency trees representing the text, along with co-reference and
trace information. Let h be the dependency tree representing the hypothesis, and let R
be a collection of inference rules (including both inference and polarity rules). Based on
the previously defined components of our inference framework, we next give a procedural
definition for the set of trees inferable from T using R, denoted I(T, R). The inference
process comprises the following steps:
1. Initialize I(T, R) with T .
2. Apply all matching polarity rules in R to each of the trees in I(T, R) (cf. Section 4.5).
3. Replace all the trace nodes with a copy of their antecedent subtree (cf. Section 4.4).
4. Add to I(T, R) all the trees derivable by co-reference substitution (cf. Section 4.4).
17

fiBar-Haim, Dagan & Berant

5. Apply all matching inference rules in R to the trees in I(T, R) (cf. Section 4.2), and
add the derived trees to I(T, R). Repeat this step iteratively for the newly added
trees, until no new trees are added.
Steps 2 and 3 are performed for h as well.6 h is inferable from T using R if h  I(T, R).
Since I(T, R) may be infinite or very large, practical implementation of this process must
limit the search space, for example by restricting the number of iterations and the applied
rules at each iteration.
When an inference rule is applied, polarity annotation is propagated from the source
tree s to the derived tree d as follows. First, nodes copied from s to d retain their original
polarity. Second, a node in d gets the polarity of its aligned node in s.
4.7 Template Hypotheses
For many applications it is useful to allow the hypothesis h to be a template rather than a
proposition, that is, to contain variables. The variables in this case are existentially quantified: t entails h if there exists a proposition h0 , obtained from h by variable instantiation,
so that t entails h0 . Each variable X is instantiated (replaced) with a subtree SX . If X
has modifiers in h (i.e., X is not a leaf), they become modifiers of SX s root. The obtained
variable instantiations may stand for answers sought in questions or slots to be filled in relation extraction. For example, applying this framework in a question-answering setting, the
question Who killed Kennedy? may be transformed into the hypothesis X killed Kennedy.
A successful proof of h from the sentence The assassination of Kennedy by Oswald shook
the nation would instantiate X with Oswald, providing the sought answer.
4.8 Limitations and Possible Extensions
We conclude this section by discussing some limitations of the presented inference formalism,
as well as possible extensions to address these limitations. First, our inference rules match
only a single subtree, and therefore are less expressive than the logic axioms used by Bos
and Markert (2005) and Tatu and Moldovan (2006), which may combine several predicates
originating from the text representation as well as from the background knowledge. This
allows logic-based systems to make inferences that combine multiple pieces of information.
For instance, if the text says that a person X lives in a city Y , and the background knowledge
tells us that the city Y is in country Z, we can infer that X lives in country Z, using a
rule such as person(X)  location(Y)  location(Z)  live(X,Y)  in(Y,Z)  live(X,Z) .
Schoenmackers, Etzioni, Weld, and Davis (2010) describe a system that acquires such rules
(first-order horn clauses) from Web text. Allowing our rules to match multiple subtrees in
t, as well as information in the background knowledge, seems a plausible future extension
to our formalism.
Another limitation of the formalism is the lack of context disambiguation. Word sense
mismatch is a potential cause for incorrect rule applications. For example, the rule hit 
score is applied correctly in (9) but not in (10):
6. Step 4 is not applied to h since the hypothesis is typically a short, simple sentence that usually does not
include co-referring NPs. Moreover, in the presented formalism h is a single tree. Applying co-referencebased inference would have resulted in additional trees inferred from h, and thus would have required
extending the formalism accordingly.

18

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

(9) The team hit a home run.  The team scored a home run.
(10) The car hit a tree. ; The car scored a tree.
Several works over the past years addressed the problem of context-dependent rule application (Dagan, Glickman, Gliozzo, Marmorshtein, & Strapparava, 2006a; Pantel, Bhagat,
Coppola, Chklovski, & Hovy, 2007; Connor & Roth, 2007; Szpektor, Dagan, Bar-Haim, &
Goldberger, 2008; Dinu & Lapata, 2010; Ritter, Mausam, & Etzioni, 2010; Berant, Dagan,
& Goldberger, 2011; Melamud, Berant, Dagan, Goldberger, & Szpektor, 2013). Szpektor
et al. (2008) proposed a comprehensive framework for modeling context matching, termed
Contextual Preferences (CP). Given a text t, a hypothesis h (possibly a template hypothesis) and an inference rule r bridging between t and h, each of these objects is annotated
with two context components: (a) global (topical) context, and (b) preferences and constraints on the instantiation of the objects variables (for r and template h). CP requires
that h and r are matched in t, and h is matched in r7 , where each context component
is matched to its counterpart. Szpektor et al. also proposed concrete implementations for
each of these components. In the above example, we could model the global context of t and
r as the sets of their content words, and compute the semantic relatedness between these
two sets, using methods such as Latent Semantic Analysis (LSA) (Deerwester, Dumais,
Furnas, Landauer, & Harshman, 1990), or Explicit Semantic Analysis (ESA) (Gabrilovich
& Markovitch, 2007). We would expect that the semantic relatedness between {score} and
{team, home run} will be much higher than between {score} and {car, tree}, which would
permit inference in (9) but not in (10).
In most RTE systems (including our system in the RTE experiments, described in
Section 7.3) lexicalized rules bridge between t and h directly, so that the rules LHS and
RHS are matched in t and h, respectively. Since in the RTE benchmarks t and h tend to
have the same semantic context, this setting alleviates context matching problems to some
extent. However, our analysis, presented later in this work (Subsection 7.5.2), shows that
context matching remains an issue even in this setting, and is expected to become even more
important when chaining of lexicalized rules is attempted. Adding contextual preferences
to our formalism is an important direction for future work.
The validity of rule application also depends on the monotonicity properties of its application site. For instance, the hypernym rule poodle  dog is applicable only in upward
monotone contexts. Monotonicity may be affected by the presence of quantifiers, negation, and certain verbs such as implicatives and counterfactives (Nairn, Condoravdi, &
Karttunen, 2006). As common with textual entailment systems, we assume upward monotonicity anywhere. While this assumption usually holds true, in some cases it may lead to
incorrect inferences. The following examples show correct applications of the above rule in
upward monotone contexts ((11),(14)), and incorrect applications in downward monotone
contexts ((12),(13),(15)):
(11) She bought a poodle.  She bought a dog.
(12) She didnt buy a poodle ; she didnt buy a dog
(13) Poodles are smart. ; Dogs are smart.
7. Context matching, like textual entailment, is a directional relation.

19

fiBar-Haim, Dagan & Berant

(14) She failed to avoid buying a poodle  She failed to avoid buying a dog.
(15) She did not fail to avoid buying a poodle ; She did not fail to avoid buying a dog.
MacCartney and Manning (2009) address monotonicity as well as other semantic relations
such as exclusion, in a Natural Logic framework based on syntactic representation. We
discuss their work in more detail in Section 8.
Finally, since our polarity annotation rules are applied locally, they may fail in complex
cases, such as computing the polarity of buying in sentences (14) and (15), in which polarity
information need to be propagated along the syntactic structure of the sentence. The
TruthTeller system (Lotan, Stern, & Dagan, 2013), computes predicate polarity (truth
value) by a combination of annotation rules and a global polarity propagation algorithm,
extending previous work by Nairn et al. (2006) and MacCartney and Manning (2009).
4.9 Summary
In this section, we presented a well-formalized approach for textual inference over parsebased representations, which is the core of this paper. In our framework, semantic knowledge
is represented uniformly as inference rules specifying tree transformations. We provided
detailed definitions for the representation of these rules as well as the inference mechanisms
that apply them. Our formalism also models inferences based on co-reference relations and
traces. In addition, it includes annotation rules that are used to detect contexts affecting
the polarity of predicates. In the next section we present an efficient implementation of this
formalism.

5. A Compact Forest for Scalable Inference
According to our formalism, each rule application generates a new sentence parse (a consequent), semantically entailed by the source sentence. Each inferred consequent may be
subject to further rule applications, and so on. A straightforward implementation of this
formalism would generate each consequent as a separate tree. Unfortunately, this nave
approach raises severe efficiency issues, since the number of consequents may grow exponentially in the number of rule applications. Consider, for example, the sentence Children
are fond of candies, and the following rules: childrenkids, candiessweets, and X is
fond of YX likes Y. The number of derivable sentences, including the source sentence,
would be 23 (the power set size), as each rule can either be applied or not, independently. We
found that this exponential explosion leads to poor scalability of the nave implementation
approach in practice.
Intuitively, we would like for each rule application to add just the entailed part of the rule
(e.g., kids) to a packed sentence representation. Yet, we still want the resulting structure
to represent a set of entailed sentences, rather than a mixture of sentence fragments with
unclear semantics. As discussed in Section 8, previous work proposed only partial solutions
to this problem.
In this section, we introduce a novel data structure, termed compact forest, and a corresponding inference algorithm, which efficiently generate and represent all consequents while
preserving the identity of each individual one. This data structure allows compact representation of a large set of inferred trees. Each rule application generates explicitly only the
20

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

nodes of the rules right-hand-side. The rest of the consequent tree is shared with the source
sentence, which also reduces the number of redundant rule applications, as explained later
in this section. We show that this representation is based primarily on disjunction edges,
an extension of dependency edges that specify a set of alternative edges of multiple trees.
Since we follow a well-defined inference formalism, we are able to prove that all inference
operations in our formalism are equivalently applied over the compact forest. We compare
inference cost over compact forests to explicit consequent generation both theoretically,
illustrating an exponential-to-linear complexity ratio, and empirically, showing improvement
by orders of magnitude (empirical results are reported in Section 7.2).
5.1 The Compact Forest Data Structure
A compact forest F represents a set of dependency trees. Figure 6d shows an example of a
compact forest containing trees for the sentences Little Mary was seen by John yesterday
and John saw little Mary yesterday. We first define a more general data structure for
directed graphs, and then narrow the definition to the case of trees.
A Compact Directed Graph (cDG) is a pair G = (V, E) where V is a set of nodes and E
is a set of disjunction edges (d-edges). Let D be a set of dependency relations. A d-edge
d is a triple (Sd , reld , Td ), where Sd and Td are disjoint sets of source nodes and target
nodes; reld : Sd  D is a function specifying the dependency relation that corresponds to
each source node. Graphically, d-edges are shown as point nodes, with incoming edges from
source nodes and outgoing edges to target nodes. For instance, let d be the bottommost
d-edge in Figure 7. Then Sd = {of, like}, Td = {candy, sweet}, rel(of ) = pcomp-n, and
rel(like) = obj .
A d-edge represents, for each si  Sd , a set of alternative directed edges {(si , tj ) : tj 
Td }, all of which are labeled with the same relation given by reld (si ). Each of these edges,
termed embedded edge (e-edge), would correspond to a different graph represented in G.
obj

obj

pcompn

In the previous example, the e-edges are likecandy, likesweet, ofcandy and
pcompn
ofsweet (the definition implies that all source nodes in Sd have the same set of
alternative target nodes Td ). The d-edge d is called an outgoing d-edge of a node v if v  Sd
and an incoming d-edge of v if v  Td . A Compact Directed Acyclic Graph (cDAG) is a
cDG that contains no cycles of e-edges.
A DAG G rooted in a node v  V of a cDAG G is embedded in G if it can be derived
as follows: we initialize G with v alone; then, we expand v by choosing exactly one target
node t  Td from each outgoing d-edge d of v, and adding t and the corresponding e-edge
(v, t) to G. This expansion process is repeated recursively for each new node added to G.
Each such set of choices results in a different DAG with v as its only root. In Figure 6d,
we may choose to connect the root either to the left see, resulting in the source passive
sentence, or to the right see, resulting in the derived active sentence.
A Compact Forest F is a cDAG with a single root r (i.e., r has no incoming d-edges)
where all the embedded DAGs rooted in r are trees. This set of trees, termed embedded
trees, and denoted T (F) comprise the set of trees represented by F.
Figure 7 shows another example of a compact forest efficiently representing the 23 sentences resulting from the three independently applied rules presented at the beginning of
this section.
21

fiBar-Haim, Dagan & Berant

ROOT

ROOT

i

i

see

V

by-subj obj

by

be

Mary

pcomp-n

John

see

mod

be

by-subj obj

yesterday

by

mod

pcomp-n

little

mod

be

yesterday

little

(b) Variable instantiation

ROOT

ROOT

i

i

see
obj

be

mod

John

(a) Right-hand-side generation

by-subj

Mary

see

see

see

be

by-subj be

mod mod

see
mod

mod

obj

obj

subj

by
pcomp-n

John

Mary

be

yesterday

by

mod

be

yesterday
pcomp-n

little

John

(c) Alignment sharing

Mary
mod

little

(d) Dual-leaf variable sharing

Figure 6: Step-by-step construction of the compact forest containing both the source sentence Little Mary was seen by John yesterday and the sentence John saw little Mary
yesterday derived from it via the application of the passive rule of Figure 1b. Parts of
speech are omitted.

5.2 The Inference Process
We next describe the algorithm implementing the inference process described in Section 4.6
over the compact forest (henceforth, compact inference), illustrated by Figures 1b (the
passive-to-active rule) and 6.
22

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

ROOT
i

be
pred

fond

like

mod subj

subj
obj

of

child

kid
pcomp-n

candy

sweet

Figure 7: A compact forest representing the 23 sentences derivable from the sentence Children are fond of candies using the following three rules: childrenkids, candiessweets,
and X is fond of YX likes Y.

5.2.1 Forest Initialization
F is initialized with the set of dependency trees representing the text sentences, with their
roots connected under the forest root as the target nodes of a single d-edge. Dependency
edges are transformed trivially to d-edges with a single source and target. Annotation
rules are applied at this stage to the initial F. Figure 6a, without the node labeled V
and its incoming edge, corresponds to the initial forest (containing a single sentence in our
example).
5.2.2 Inference Rule Application
Inference rule application comprises the steps described below, which are summarized in
Algorithm 2.
L Matching We first find all the matches of the rules LHS L in the forest F (line 1). For
the sake of brevity, we omitted the technical details of the L matching implementation from
the pseudocode of Algorithm 2. The following is a high-level description of the matching
procedure, focusing on the key algorithmic points.
L is matched in F if there exists an embedded tree t in F such that L is matched in
t, as in Section 4.2. We denote by l the subtree of t in which L was matched (line 3).
23

fiBar-Haim, Dagan & Berant

Input: a compact forest F ; an inference rule E : L  R
Output: A modified F, denoted F 0 , such that T (F 0 ) = T (F)  D, where D is the set of trees derived by
applying E for any subset of Ls matches in each of the trees in T (F)
1: M  the set of all matches of L in F
2: for each match f  M do
3:
l  the subtree of F in which L is matched according to f
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

// Right-hand-side generation
SR  copy of R excluding dual leaf variable nodes
Add SR to F
SL  l excluding dual leaf variable nodes
rR  root(SR )
rL  root(l)
if E is a substitution rule then
d  the incoming d-edge of rL // will set SR as an alternative to SL
else // introduction rule
d  the outgoing d-edge of root(F) // will set SR as an alternative to other trees in T (F)
Add rR to Td

15:
16:
17:
18:
19:

// Variable instantiation
for each variable X held in node xR  SR do // Rs variables excluding dual leaves
if X is not a leaf in L then
xL  f (X) // the node in SL matched by X
(xR .lemma, xR .polarity)  (xL .lemma, xL .polarity)

20:
21:
22:
23:
24:
25:
26:

else // X is a leaf in L so it is matched in the whole target node set
(xR .lemma, xR .polarity)  (n.lemma, n.polarity) for some node n  f (X)
for each n0  f (X); n0 6= n do
generate a substitution rule n  n0 where n and n0 are aligned, and apply it to xR
x0R  the instantiation of n0
for each u  SL such that u is aligned to xR do
add alignment from u to x0R

27:
28:
29:
30:
31:
32:

// Alignment sharing
for each aligned pair of nodes nL  SL and nR  SR do
nR .polarity  nL .polarity
for each outgoing d-edge d of nL whose e-edges are not part of SL do
Add nR to Sd
reld (nR )  reld (nL )

33:
34:
35:
36:

// Dual leaf variable sharing
for each dual-leaf variable X matched in a node v  l do
d  the incoming d-edge of v
p  parent node of X in SR

37:
38:
39:
40:
41:

// go over p and alternatives for p generated during variable instantiation
P  set of target nodes of ps incoming d-edge
for each p0  P do
Add p0 to Sd
reld (p0 )  the relation between X and p

Algorithm 2: Applying an inference rule to a compact forest

24

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

This subtree may be shared by multiple trees represented in F, in which case the rule is
applied simultaneously to all of these trees. As in Section 4.2, the match in our example
is (V, N 1, N 2)=(see, Mary, John). This definition does not allow l to be scattered over
multiple embedded trees. Matches are constructed incrementally, aiming to add Ls nodes
one by one to the partial matches constructed so far, while verifying for each candidate node
in F that both node content and the corresponding edge labels match. It is also verified
that the match does not contain more than one e-edge from each d-edge. The nodes in F
are indexed using a hash table to enable fast lookup.
As the target nodes of a d-edge specify alternatives for the same position in the tree, their
parts-of-speech are expected to be substitutable. We further assume that all target nodes of
the same d-edge have the same part-of-speech8 and polarity. Consequently, variables that
are leaves in L and may match a certain target node of a d-edge d are mapped to the whole
set of target nodes Td rather than to a single node. This yields a compact representation of
multiple matches, and prevents redundant rule applications. For instance, given a compact
representation of {Children/kids} are fond of {candies/sweets} (cf. Figure 7), the rule X
is fond of YX likes Y will be matched and applied only once, rather than four times (for
each combination of matching X and Y ).
Right-Hand-Side Generation Given an inference rule L  R, we define a dual-leaf
variable as a variable that is a leaf of both L and R. In our example, both N 1 and N 2
are dual-leaf variables in the passive-to-active rule of Figure 1b. Variables that are the
only node in R (and hence are both the root and a leaf), and variables with additional
alignments (other than the implicit alignment between their occurrences in L and R) are
not considered dual-leaves. As explained below, the instantiations of dual leaf variables are
shared between the source and the target trees.
In the right-hand-side generation step, a template SR (line 5), consisting of R while
excluding dual-leaf variables, is generated and inserted into F (line 6). In our example,
SR only includes the node V out of the passive rules RHS. Similarly, we define SL as l
excluding dual-leaf variables (line 7).
In the case of a substitution rule (as in our example), SR is set as an alternative to SL
by adding SR s root to Td , where d is the incoming d-edge of SL s root (line 11). In case
of an introduction rule, it is set as an alternative to the other trees in the forest by adding
SR s root to the target node set of the forest roots outgoing d-edge (line 13). Figure 6a
illustrates the results of this step for our example. SR is the gray node labeled with the
variable V , and it becomes an additional target node of the d-edge entering the original
(left) see.
Variable Instantiation Each variable in SR (i.e., a non dual-leaf) is instantiated (lines
16-26) according to its match in L (as in Section 4.2). In our example, V is instantiated
with see (Figure 6b, lines 17-19). As specified above, if a variable in SR is a leaf in L (which
is not the case in our example) then it is matched in a set of nodes, and each of them
should be instantiated in SR (lines 20-26). This is decomposed into a sequence of simpler
operations: first, SR is instantiated with a representative from the set (line 21). We then
apply ad-hoc lexical substitution rules for creating a new node for each additional node in
8. This is the case in our current implementation, which is based on the coarse tag-set of Minipar.

25

fiBar-Haim, Dagan & Berant

the set (line 22-26). These nodes, in addition to the usual alignment with their source nodes
in SL (lines 25-26), share the same daughters in SR (due to the alignment between n and
n0 , defined in line 23).
Alignment Sharing Modifiers of aligned nodes are shared (rather than copied) as follows.
Given a node nL in SL aligned to a node nR in SR , and an outgoing d-edge d of nL
which is not part of l, we share d between nL and nR by adding nR to Sd and setting
reld (nR ) = reld (nL ) (lines 28-32). In our example (Figure 6c), the aligned nodes nL and
nR are the left and right see nodes, respectively, and the shared modifier is yesterday. The
dependency relation mod is copied for the right see node. We also copy polarity annotation
from nL to nR (line 29).
We note at this point that the instantiation of variables that are not dual leaves cannot
be shared because they typically have different modifiers at the two sides of the rule. Yet,
their modifiers, which are not part of the rule, are shared through the alignment operation
(recall that common variables are always considered aligned). Dual leaf variables, on the
other hand, might be shared, as described next, since the rule doesnt specify any modifiers
for them.
Dual Leaf Variable Sharing This final step (lines 34-41) is performed similarly to
alignment sharing. Suppose that a dual leaf variable X is matched in a node v in l whose
incoming d-edge is d. Then we simply add the parent p of X in SR to Sd and set reld (p) to
the relation between p and X (in R). Since v itself is shared, its modifiers become shared as
well, implicitly implementing the alignment operation. The subtrees little Mary and John
are shared this way for variables N 1 and N 2 (Figure 6d). If ad-hoc substitution rules were
applied to p at the variable instantiation phase, the generated nodes serve as alternative
parents of X, thus the sharing procedure applied to p should be repeated for each of them.
Applying the rule in our example added only a single node and linked it to four d-edges,
compared to duplicating the whole tree in explicit inference.
5.2.3 Co-reference Substitution
In Section 4.4 we defined co-reference substitution, an inference operation that allows replacing a subtree t1 with a co-referring subtree t2 . This operation is implemented by generating
on-the-fly a substitution rule t1  t2 and applying it to t1 . In our implementation, the
initial compact forest is annotated with co-reference relations obtained from an external
co-reference resolution tool, and all substitutions are performed prior to rule applications.
Substitutions where t2 is a pronoun are ignored, as they are usually not useful.
5.3 Correctness
In this section, we present two theorems proving that the inference process presented is a
valid implementation of the inference formalism. We provide the full proofs in Appendix A.
In Theorem 1, we argue that applying a rule to a compact forest results in a compact
forest. Since we begin with a valid compact forest created by the initialization step, it follows
by induction that for any sequence of rule applications the result of the inference process
is a compact forest. The fact that the embedded DAGs generated during the inference
process are indeed trees is not trivial, since nodes generally have many incoming e-edges
26

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

from many nodes. However, we show that any pair of these parent nodes cannot be part
of the same embedded DAG. For example, in Figure 7, the node candy has an incoming
e-edge from both the node like and the node of . However, the nodes like and of  are
not part of the same embedded DAG. This is because the d-edge emanating from the root
forces us to choose between the node like and the node be. Thus, we see that the reason
for correctness is not local: the two incoming e-edges into the leaf node candies cannot be
in the same embedded DAG because of a rule applied at the root of the tree. We now turn
to the theorem and its proof scheme:
Theorem 1 Applying a rule to a compact forest results in a compact forest.
Proof scheme We prove that if applying a rule to a compact forest creates a cycle or an
embedded DAG that is not a tree, then such a cycle or a non-tree DAG already existed
prior to rule application. This contradicts the assumption that the original structure is a
compact forest. A crucial observation for this proof is that for any directed path from a
node u to a node v that passes through SR , where u and v are outside SR , there is also an
analogous path from u to v that passes through SL instead.
The next theorem is the main result. We argue that the inference process over a compact
forest is complete and sound, that is, it generates exactly the set of consequents derivable
from a text according to the inference formalism.
Theorem 2 Given a rule base R and a set of initial trees T , a tree t is represented by a
compact forest derivable from T by the inference process  t is a consequent of T according
to the inference formalism.
Proof scheme We first show completeness by induction on the number of explicit rule
applications. Let tn+1 be a tree derived from a tree tn using the rule rn according to the
inference formalism. The inductive assumption determines that tn is embedded in some
derivable compact forest F. It is easy to verify that applying rn on F will yield a compact
forest F 0 in which tn+1 is embedded.
Next, we show soundness by induction on the number of rule applications over the
compact forest. Let tn+1 be a tree represented in some derived compact forest Fn+1 (tn+1 
T (F n+1 )). Fn+1 was derived from the compact forest Fn , using the rule rn . The inductive
assertion states that all the trees in T (F n ) are consequents of T according to the formalism.
Hence, if tn+1 is already in T (F n ) then it is a consequent of T . Otherwise, it can be shown
that there exists a tree tn  T (F n ) such that applying rn to tn will yield tn+1 according to
the formalism. tn is a consequent of T according to the inductive assertion and therefore
tn+1 is a consequent of T as well.
These two theorems guarantee that the compact inference process is valid, that is, it
yields a compact forest that represents exactly the set of consequents derivable from a given
text by a given rule set.
27

fiBar-Haim, Dagan & Berant

5.4 Complexity
In this section, we explain why compact inference exponentially reduces the time and space
complexity in typical scenarios.
We consider a set of rule matches in a tree T independent if their matched left-handsides (excluding dual-leaf variables) do not overlap in T , and their application over T can
be chained in any order. For example, the three rule matches presented in Figure 7 are
independent.
Let us consider explicit inference first. Assume we start with a single tree T with k
independent rules matched. Applying k rules will yield 2k trees, since any subset of the
rules might be applied to T . Therefore, the time and space complexity of applying k
independent rule matches is (2k ). Applying more rules on the newly derived consequents
behaves in a similar manner.
Next, we examine compact inference. Applying a rule using compact inference adds
the right-hand-side of the rule and shares with it existing d-edges. Since the size of the
right-hand-side and the number of outgoing d-edges per node are practically bounded by
low constants, applying k rules on a tree T yields a linear increase in the size of the forest.
Thus, the resulting size is O(|T | + k), as we can see from Figure 7.
The time complexity of rule application is composed of matching the rule in the forest
and applying the matched rule. Applying a matched rule is linear in its size. Matching
a rule of size r in a forest F takes O(|F|r ) time even when performing an exhaustive
search for matches in the forest. Since r tends to be quite small and can be bounded by
a low constant9 , this already gives polynomial time complexity. Furthermore, matches are
constructed incrementally, where at each step we aim to extend the partial matches found.
Due to the typical low connectivity of the forest, as well as the various constraints imposed
by the rule (lemma, POS, and dependency relation), the number of candidates for extending
the matches at each step << |F|, and these candidates can be retrieved efficiently using
proper indexing. Thus, the matching procedure is very fast in practice, as illustrated in the
empirical evaluation described in Section 7.2.
5.5 Related Work on Packed Representations
Packed representations in various NLP tasks share common principles, which also underlie
our compact forest: factoring out common substructures and representing choice as local
disjunctions. Applying this general scheme to individual problems typically requires specific representations and algorithms, depending on the type of alternatives that should be
represented and the specified operations for creating them. We create alternatives by rule
application, where a newly derived subtree is set as an alternative to existing subtrees.
Alternatives are specified locally using d-edges.
Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky & Martin, 2008), and were extended in later
work for various purposes (Maxwell III & Kaplan, 1991; Kay, 1996). Alternatives in the
parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases.
9. In our RTE system, the average rule LHS size was found to be 2 nodes, and the maximal size was 7
nodes, for the experimental setting described in Section 7.2.2, applied to the RTE3 test set.

28

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Packed representations have also been utilized in transfer-based machine translation.
Emele and Dorna (1998) translated packed source language representation to packed target
language representation while avoiding unnecessary unpacking during transfer. Unlike our
rule application, in their work transfer rules preserve ambiguity stemming from source
language, rather than generating new alternatives. Mi et al. (2008) applied statistical
machine translation to a source language parse forest, rather than to the 1-best parse.
Their transfer rules are tree-to-string, contrary to our tree-to-tree rules, and chaining is not
attempted (rules are applied in a single top-down pass over the source forest). Thus, their
representation and algorithms are quite different from ours.

6. Incorporated Knowledge Bases
In this section, we describe the various knowledge bases used by our inference engine. We
first describe a novel rule base addressing generic linguistic structures. This rule base was
composed manually, based on our formalism, and includes both inference rules (Section 6.1)
and polarity annotation rules (Section 6.2). In addition, we derived inference rules from
several large scale semantic resources (Section 6.3). Overall, this variety illustrates the
suitability of our formalism for representing diverse types of inference knowledge.
6.1 Inference Rules for Generic Linguistic Phenomena
These rules capture inferences associated with common syntactic structures, which are
summarized in Table 2. The rules have three major functions:
1. Simplification and canonization of the source tree (categories 6 and 7 in Table 2).
2. Extracting embedded propositions (categories 1, 2, 3).
3. Inferring propositions from non-propositional subtrees of the source tree (category 4).
Inference rules that merely extract a subtree out of the source tree without changing its
structure (such as the relative clause rule) are useful for exact inference that aims to generate
the hypothesis, and were used in the evaluation of such inferences (cf. Section 7.1). However, the currently implemented approximate classification features are focused on matching
substructures of the hypothesis in the forest (as described in Section 7.3), hence they do
not take advantage of such extractions. Therefore, these rules were excluded from the rest
of the experiments, reported in Sections 7.27.3.
The rules in categories 1-7 depend solely on syntactic structure and closed-class words,
and are referred to as generic rules. By contrast, verb complement extraction rules (category
8) are considered lexicalized rules, since they are specific to certain verbs: if we replace forced
with advised in the example, the entailment would not hold. We extracted from the PARC
polarity lexicon (Nairn et al., 2006) a list of verbs that allow such inference when appearing
in positive polarity contexts, and generated inference rules for these verbs. The list was
complemented with a few reporting verbs, such as say and announce, since information in
the news domain, in which these rules were applied in our experiments (cf. Section 7.1) is
often given in reported speech, while the speaker is usually considered reliable.
We sidestep the issue of polarity propagation by applying these rules only at the main
clause, which is implemented by including the tree root node in the rule LHS. When the
29

fiBar-Haim, Dagan & Berant

#
1

Category
Conjunctions

2

Clausal extraction from
connectives
Relative
clauses

3

4

Appositives

5

Determiner
Canonization

6

Passive

7

Genitive
modifier

8

Verb complement clause
extraction

Example: source
Helenas very experienced
and has played a long time
on the tour.
But celebrations were muted
as many Iranians observed a
Shiite mourning month.
The assailants fired six bullets at the car, which carried
Vladimir Skobtsov.
Frank Robinson, a onetime manager of the Indians, has the distinction for
the NL.
The plaintiffs filed their lawsuit last year in U.S. District
Court in Miami.
We have been approached
by the investment banker.
Malaysias crude palm oil
output is estimated to have
risen by up to six percent.
Yadav was forced to resign.

Example: derived
 Helena has played a long
time on the tour.
 Many Iranians observed a
Shiite mourning month.
 The car carried Vladimir
Skobtsov.
 Frank Robinson is a onetime manager of the Indians.

 The plaintiffs filed a lawsuit last year in U.S. District
Court in Miami.
 The investment banker approached us.
 The crude palm oil output of Malaysia is estimated
to have risen by up to six percent.
 Yadav resigned.

Table 2: Inference rules for generic linguistic structures

embedded clause is extracted, it becomes the main clause in the derived tree, and these rules
can then extract its own embedded clauses. The polarity of the verb is detected by applying
annotation rules, as described next. If the verb was annotated with negative or unknown
polarity, matching of complement extraction rules fails. For example, if the last sentence in
Table 2 was Yadav was not forced to resign, then forced would be annotated with negative
polarity, and consequently the matching of the corresponding complement extraction rule
would fail, and Yadav resigned would not be entailed. Hence, annotation rules may block
erroneous inference rule applications. While polarity is important for correct application of
such rules, this is not the case for other rule types, such as passive-to-active transformation.
We therefore checked polarity matching for rule application only in the exact inference
experiment (Section 7.1), where the verb complement extraction rules were used. We leave
further analysis of polarity-dependence of our rules to future work.
6.2 Polarity Annotation Rules
We use annotation rules to mark negative and unknown polarity of predicates (cf. Section 4.5). Table 3 summarizes the polarity-inducing contexts that we address. Like inference rules, annotation rules also comprise generic rules (categories 1-4) and lexicalized
30

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

#
1

Category
Explicit Negation

2
3
4

Implied Negation
Modal Auxiliaries
Overt Conditionals

5
6
7

Verb complements
Adjectives
Adverbs

Example
What weve never seen[] is actual costs come
down.
No one stayed[] for the last lecture.
I could eat[?] a whale now!
if Venus wins[?] this game, she will meet[?] Sarena
in the finals.
I pretend that I know[] calculus.
It is impossible that he survived[] such a fall.
She probably danced[?] all night.

Table 3: Polarity annotation rules

rules (categories 5-7). When a verb complement embedded clause has negative or unknown
polarity, it is not extracted, however, its polarity is annotated (category 5; compare with
category 8 in Table 2). This list of verbs that imply negative/unknown polarity for their
clausal complements was taken from the PARC lexicon, as well as from VerbNet (Kipper,
2005).
6.3 Lexical and Lexical-Syntactic Rules
In addition to the manually-composed generic rules, the system integrates inference knowledge from a variety of large-scale semantic resources, introduced in Section 2.3. The information derived from these resources is represented uniformly as inference rules in our
formalism. Some examples for such rules were shown in Table 1. The following resources
were used:
WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on the synonym, hyponym (a word is entailed by its hyponym, e.g., dog animal ), instance
hyponym 10 and derivation relations.
Wikipedia: We used the lexical rulebase of Shnarch et al. (2009), who extracted rules
such as Janis Joplin  singer from Wikipedia based on both its metadata (e.g.,
links and redirects) and text definitions, using patterns such as X is a Y .11
DIRT: The DIRT algorithm (Lin & Pantel, 2001) learns from a corpus inference rules
between binary predicates, for example, X is fond of YX likes Y. We used a
version that learns canonical rule forms (Szpektor & Dagan, 2007).
Argument-Mapped WordNet (AmWN): A resource for inference rules between predicates, covering both verbal and nominal forms (Szpektor & Dagan, 2009), includ10. According to the WordNet glossary, an instance is a proper noun that refers to a particular, unique
referent (as distinguished from nouns that refer to classes). This is a specific form of hyponym. For
example, Ganges is an instance of river.
11. In addition to the extraction methods described by Shnarch et al. (2009), we employed two additional
methods. First, extraction of entailments among terms that are redirected to the same page. Second,
generalization of rules with the same RHS and common LHS head, but different modifiers. For instance,
the rules Ferrari F430  car  and Ferrari Ascari car  are generalized into Ferrari car .

31

fiBar-Haim, Dagan & Berant

ing their argument mapping. It is based on WordNet and NomLex-plus (Meyers
et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor & Dagan, 2008). AmWN rules are defined between unary templates,
for example, kill XX die
These automatically-extracted inference rules lack two attributes defined in our formalism: rule type (substitution/introduction) and explicit alignments (beyond alignments
between Rs variables and their L counterparts, which are defined by default). These attributes are added automatically using the following heuristics:
1. If the roots of L and R have the same part-of-speech, then it is a substitution rule
(e.g., X buy Y  Y was sold to X ). Otherwise (e.g., Ys acquisition by X  Y was
sold to X ), it is an introduction rule.
2. The roots of L and R are assumed to be aligned.
Note that application of some of the above rules, (e.g., WordNet derivations and some
of the rules learned by DIRT), does not result in a valid parse tree. These rules should
not be used when aiming for exact derivation of h from t. However, they may be useful
when the inference engine is used together with an approximate matching component, as
in our RTE system. Our approximate matcher (described in Section 7.3) employs features
such as the coverage of words and subtrees in h by F, and therefore can benefit from such
inferences. These rules should preferably be applied only as the last step of the inference
process, to avoid cascading errors.

7. Evaluation
In this section, we present an empirical evaluation of our entailment system as a whole, as
well as evaluation of its individual components. We evaluate both the quality of the systems
output (in terms of accuracy, precision, and recall) and its computational efficiency (in terms
of running time and space, using various application settings.
We first evaluate the knowledge-based inference engine. In Section 7.1, we describe an
experiment in which the engine aims to prove simple template hypotheses, representing
binary predicates, from texts sampled from a large corpus. Next, in Section 7.2 we evaluate
the efficiency of our engine implementation using the compact forest data structure. We then
evaluate the complete entailment system, including the approximate entailment classifier
(Section 7.3). Finally, in Sections 7.47.5 we provide an in-depth analysis of the performance
of the inference component on RTE data.
7.1 Proof System Evaluation
In this experiment, we evaluate the inference engine on finding strict proofs. That is,
the inference process must derive precisely the target hypothesis (or an instantiation of
it, in the case of template hypotheses, which contain variables as defined in Section 4.7).
Thus, we should evaluate its precision over text-hypothesis pairs for which a complete proof
chain is found, using the available rules. We note that the PASCAL RTE datasets are not
suitable for this purpose. These rather small datasets include many text-hypothesis pairs for
32

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

which available inference rules would not suffice for deriving complete proofs. Furthermore,
since the focus of this research is applied textual inference, the inference engine should
be evaluated in an NLP application setting where the texts represent realistic distribution
of linguistic phenomena. Manually-composed benchmarks such as the FraCas test suite
(Cooper et al., 1996), which contains synthetic examples for specific semantic phenomena,
are clearly not suitable for such an evaluation.
As an alternative, we chose a Relation Extraction (RE) setting, for which complete
proofs can be achieved for a large number of corpus sentences. In this setting, the system
needs to identify pairs of arguments in sentences for a target semantic relation (e.g., X buy
Y ).
7.1.1 System Configuration
In this experiment, which was first reported by Bar-Haim et al. (2007), we used an earlier
version of the engine and the rule bases. The engine in this experiment does not make use
of the compact forest, but rather generates each consequent explicitly. Polarity annotations
are not propagated from source to derived trees. Instead, polarity annotation rules are
applied to the original text t, and to each inferred consequent, prior to application of any
inference rule. The following rule bases were used in this experiment:
Generic Linguistic Rules We used the generic rule base presented in Section 6, including both inference and the polarity annotation rules. This early version did not include the
lexicalized polarity rules derived from VerbNet and from the PARC lexicon (category 5 in
Table 3).
Lexical-Syntactic Rules Nominalization rules: inference rules such as Xs acquisition
of Y  X acquired Y  capture the relations between verbs and their nominalizations.
These rules were derived automatically (Ron, 2006) from Nomlex, a hand-coded database
of English nominalizations (Macleod, Grishman, Meyers, Barrett, & Reeves, 1998), and
from WordNet.
Automatically Learned Rules: we used the DIRT paraphrase collection, as well the
output of TEASE (Szpektor et al., 2004), another unsupervised algorithm for learning
lexical-syntactic rules. TEASE acquires entailment relations from the Web for a given
input template I by identifying characteristic variable instantiations shared by I and other
templates. Both algorithms provide a ranked list of output templates for a given input
template. Some of the learned rules are linguistic paraphrases, (e.g., X confirm Y  X
approve Y ), while others capture world knowledge, (e.g., X buy Y  X own Y ). These
algorithms do not learn the entailment direction of the rule, which reduces their accuracy
when applied in any given direction. For each system, we considered the top 15 bi-directional
rules learned for each template.
Generic Default Rules These rules are used to define default behavior, in situations
where no case-by-case rules are available. We used one default rule that allows removal
of any modifiers from nodes. Ideally, this rule would be replaced in future work by more
specific rules for removing modifiers.
33

fiBar-Haim, Dagan & Berant

7.1.2 Evaluation Process
We use a sample of test template hypotheses that correspond to typical RE relations, such as
X approve Y. We then identify in a large test corpus, sentences from which an instantiation
of the test hypothesis is proved. For example, the sentence the budget was approved by
the parliament is found to prove the instantiated hypothesis parliament approve budget
(via the passive-to-active inference rule). Finally, a sample of such candidate sentenceshypothesis pairs is judged manually for true entailment. We repeated the process to compare
different system configurations.
Since the publicly available sample output of TEASE is much smaller than the other
resources12 we randomly selected from this resource 9 transitive verbs that may correspond
to typical RE predicates13 . We formed test templates by adding subject and object varisubj

able nodes. For example, for the verb accuse we constructed the template XNOUN 
obj

accuse VERB  YNOUN .
For each test template h we identify sentences in the corpus from which the template
can be proved by our system. To efficiently find proof chains that generate h from corpus
sentences we combine forward and backward (Breadth-First) searches over the available
rules. First, we use a backward search over the lexical-syntactic rules, starting with rules
whose right-hand-side is identical to the test template. The process of backward chaining
the DIRT/TEASE and nominalization rules generates a set of templates ti , all of them
proving (deriving) h. For example, for the hypothesis X approve Y we may generate
the template X confirm Y, through backward application of a DIRT/TEASE rule, and
then further generate the template confirmation of Y by X, through a nominalization rule.
Since the templates ti are generated by lexical-syntactic rules, which modify open-class
lexical items, they may be considered lexical expansions of h.
Next, for each specific ti we generate a search engine query composed of the open-class
words in ti . This query fetches candidate sentences from the corpus, from which ti might
be proven using the generic linguistic rules (recall that these rules do not modify openclass words). To that end, we use a forward search that applies the generic rules, starting
from a candidate sentence s and trying to derive ti by a sequence of rule applications. If
successful, the variables in ti are instantiated (cf. Section 4.7). Consequently, we know that
under these variable instantiations, h can be proven from s (since s derives ti which in turn
derives h).
We performed the above search for sentences that prove each test template over the
Reuters RCV1 corpus, CD#2, applying Minipar for parsing. Through random sampling,
we obtained 30 sentences that prove (according to the tested system configuration) each of
the 9 test templates, yielding a total of 270 pairs of a sentence, and an instantiated hypothesis, for each of the four tested configurations, described below (1080 pairs overall). These
pairs were split for entailment judgment between two human annotators (graduate students
at the Bar-Ilan NLP group). The annotators achieved, on a sample of 100 shared exam12. The output of TEASE and DIRT, as well as many other knowledge resources, is available from the RTE
knowledge resources page:
http://aclweb.org/aclwiki/index.php?title=RTE_Knowledge_Resources
13. The verbs are approach, approve, consult, lead, observe, play, seek, sign, strike.

34

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

#
1
2
3
4

Configuration
Baseline (embed h anywhere in s)
Proof (embed h at the root of s)
Proof + Generic
Proof + Generic + Lexical-Syntactic

Precision
67.0%
78.5%
74.8%
23.6%

Yield
2,414
1,426
2,967
18,809

Table 4: Proof system evaluation

ples, an agreement level of 87%, and a Kappa value of 0.71 (corresponding to substantial
agreement).
7.1.3 Results
We tested four configurations of the proof system:
1. Baseline: The baseline configuration follows the prominent approach in graph-based
entailment systems: the system tries to embed the given hypothesis anywhere in the
candidate sentence tree s, while only negative or unknown polarity (detected by the
annotation rules) may block embedding.
2. Proof: In this configuration h has to be strictly generated from the candidate sentence s. The only inference rule available is the default rule for removing modifiers
(polarity annotation rules are active as in Baseline). This configuration is equivalent
to embedding h in s with the root of h matched at the root of s, since modifiers that
are not part of the match can be removed from s by the default rule. However, if
h is embedded elsewhere in s it will not be extracted, as opposed to the Baseline
configuration.
3. Proof + Generic: As Proof, plus generic linguistic rules.
4. Proof + Generic + Lexical-Syntactic: As the previous configuration, plus
lexical-syntactic rules.
For each system configuration we measure precision, the percentage of examples judged
as correct (entailing), and average extrapolated yield, which is the expected number of
truly entailing sentences in the corpus that would be proven as such by the system. The
extrapolated yield for a specific template is calculated as the number of sample sentences
judged as entailing, multiplied by the sampling proportion. The average is calculated over
all test templates. We note that, similar to IR evaluations, it is not possible to compute
true recall in our setting since the total number of entailing sentences in the corpus is not
known (recall is equal to the yield divided by this total). However, it is straightforward to
measure relative recall differences among different configurations based on the yield. Thus,
using these two measures estimated from a large corpus it is possible to conduct robust
comparison between different configurations, and reliably estimate the impact of different
rule types. Such analysis is not possible with the RTE datasets, which are rather small, and
their hand-picked examples do not represent the actual distribution of linguistic phenomena.
35

fiBar-Haim, Dagan & Berant

The results are reported in Table 4. First, comparing the results for Proof with the
results for Baseline, we observe that the requirement for matching h at the root of s (i.e.,
at the main clause of s), rather than allowing it to be matched anywhere in s, improves the
precision considerably over the baseline (by 11.5%), while reducing the yield by nearly 40%.
The Proof configuration avoids errors resulting from improper extraction of embedded
clauses.
Remarkably, using the generic inference rules, our system is able to gain back the lost
yield in Proof and further surpass the yield of the baseline configuration. In addition,
we obtain a higher precision than the baseline (a 7.8% difference), which is statistically
significant at a p < 0.05 level, using z test for proportions. This demonstrates that our
principled proof approach appears to be superior to the more heuristic baseline embedding
approach, and exemplifies the contribution of our generic rule base. Overall, generic rules
were used in 46% of the proofs.
Adding the lexical-syntactic rules increased the yield by a factor of six. This shows
the importance of acquiring lexical-syntactic variability patterns. However, the precision of
DIRT and TEASE is currently quite low, causing overall low precision. Manual filtering of
rules learned by these systems is currently required to obtain reasonable precision.
Error analysis revealed that for the third configuration Proof + Generic rules, a
significant 65% of the errors are due to parsing errors, most notably incorrect dependency
relation assignment, incorrect POS assignment, incorrect argument selection, incorrect analysis of complex verbs (e.g., play down in the text vs. play in the hypothesis) and ungrammatical sentence fragments. Another 30% of the errors represent conditionals, negation,
and modality phenomena, most of which could be handled by additional rules, some making use of more elaborate syntactic information such as verb tense. The remaining, and
rather small, 5% of the errors represent truly ambiguous sentences which would require
considerable world knowledge for successful analysis.
7.2 Compact Forest Efficiency Evaluation
Next, we evaluate the efficiency of compact inference (cf. Section 5) in the setting of recognizing textual entailment, using the RTE-3 and RTE-4 datasets (Giampiccolo et al., 2007,
2008). These datasets consist of (text, hypothesis) pairs, which need to be classified as
entailing/non entailing. Our first experiment, using the generic inference rule set, shows
that compact inference outperforms explicit inference (efficiency-wise) by orders of magnitude (Section 7.2.1). The second experiment shows that compact inference scales well to
a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules
are applied per text (Section 7.2.2).
7.2.1 Compact vs. Explicit Inference
To compare explicit and compact inference we randomly sampled 100 pairs from the RTE-3
development set, and parsed the text in each pair using Minipar (Lin, 1998). To avoid
memory overflow for explicit inference, we applied to these sentences only the subset of
generic inference rules described in Section 6.1. For a fair comparison, we aimed to make the
explicit inference implementation reasonably efficient, for example by preventing multiple
generations of the same tree by different permutations of the same rule applications. Both
36

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Time (msec)
Rule applications
Node count
Edge endpoints

Compact
61
12
69
141

Explicit
24,184
123
5,901
11,552

Ratio
396
10
86
82

Table 5: Compact vs. explicit inference, using generic rules. Results are averaged per
text-hypothesis pair.

configurations perform rule application iteratively, until no new matches are found. In each
iteration, we first find all rule matches and then apply all matching rules. We compare run
time, number of rule applications, and the overall generated size of nodes and edges, where
edge size is represented by the sum of its endpoints (2 for a regular edge, |Sd | + |Td | for a
d-edge).
The results are summarized in Table 5. As expected, the results show that compact
inference is by orders of magnitude more efficient than explicit inference. To avoid memory
overflow, inference was terminated after reaching 100,000 nodes. Three out of the 100 test
texts reached that limit with explicit inference, while the maximal node count for compact
inference was only 268. The number of rule applications is reduced due to the sharing
of common subtrees in the compact forest, by which a single rule application operates
simultaneously over a large number of embedded trees. The results suggest that scaling to
larger rule bases and longer inference chains would be feasible for compact inference, but
prohibitive for explicit inference.
7.2.2 Application to an RTE System
The goal of the second experiment was to test if compact inference scales well for broad
inference rule bases. In this experiment we used the Bar-Ilan RTE system (Bar-Haim et al.,
2008). The system operates in two primary stages:
Inference: inference rules are first applied to the initial compact forest F, aiming to bring
it closer to the hypothesis h. In this experiment, we use all the knowledge bases
described in Section 6. Overall, these rule bases contain millions of rules.
In the current system we implemented a simple search strategy, in the spirit of
(de Salvo Braz et al., 2005): first, we applied three exhaustive iterations of generic
rules. Since these rules have low fan-out (few possible right-hand-sides for a given
left-hand-side), it is affordable to apply and chain them more freely. At each iteration
we first find all rule matches, and then apply all matched rules. To avoid repeated
identical rule applications, we mark newly added nodes at each iteration, and in the
next iteration consider only matches containing new nodes. We then perform a single
iteration of all other lexical and lexical-syntactic rules, applying them only if their L
part was matched in F and their R part was matched in h. Further investigation of
effective search heuristics over our representation is left for future research.
Classification: Following inference, a set of features is extracted from the resulting F and
from h and fed into an SVM classifier, which determines entailment. We describe the
37

fiBar-Haim, Dagan & Berant

Rule applications
Node count
Edge endpoints

RTE3-Dev
Avg. Max.
14
275
71
606
155 1,741

RTE4
Avg. Max.
15
110
80
357
173 1,062

Table 6: Application of compact inference to the RTE-3 Dev. and RTE-4 datasets, using
all rule types

classification stage in more detail in the next section, which discusses the performance
of our RTE system.
Table 6 provides statistics on rule applications using all rule bases, over the RTE-3
development set and the RTE-4 dataset14 . Overall, the primary result is that the compact
forest indeed accommodates well extensive rule applications from large-scale rule bases. The
resulting forest size is kept small, even in the maximal cases which were causing memory
overflow for explicit inference.
7.3 Complete RTE System Evaluation
In the previous sections, we evaluated our knowledge-based inference engine (the proof system) with respect to the quality of its output (precision, recall) as well as its computational
efficiency (time, space). We now evaluate the complete RTE system, which combines the
inference engine with an approximate classification module.
The classification setting and its features are quite typical for the RTE literature. Features can be broadly categorized into two subsets: (a) lexical features that solely depend on
the lexical items in F and h, and (b) lexical-syntactic features that also take into account
the syntactic structures and dependency relations in F and h. Below is a brief description
of the features. A complete description appears in our RTE system report (Bar-Haim et al.,
2008).
Lexical features: Coverage features check if the words in h are present (covered) in F.
We assume that a high degree of lexical coverage correlates with entailment. These
features measure the proportion of uncovered content words, verbs, nouns, adjectives
and adverbs, named entities and numbers. Polarity mismatch features detect cases
where nouns or verbs in h are only matched in F with incompatible polarity. These
features are assumed to indicate non-entailment.
Edge coverage features: We say that an edge in h is matched in F if there is an edge
in F with matching relation, source node and target node. We say an edge in h is
loosely-matched if there is some path in F from a matching source node to a matching
target node. Based on these definitions we extract two features: the proportion of h
edges matched/loosely matched in F.15
14. Running time is not included since most of it was dedicated to rule fetching, which was rather slow for
our available implementation of some resources. The elapsed time was a few seconds per (t, h) pair.
15. We only look at a subset of the edges labeled with relevant dependency relations.

38

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Predicate-argument features: If F entails h, then the predicates in h should be matched
in F along with their arguments. Predicates include verbs (except for the verb be)
or subject complements in copular sentences, For example, smart in Joseph is smart.
Arguments are the daughters of the predicate node in h.16 Four features are computed
for each F, h pair. We categorize every predicate in h that has a match in F to one
or more of four possible categories:
1. complete match - a matching predicate exists in F with matching arguments and
dependency relations.
2. partial match - a matching predicate exists in F with some matching arguments
and dependency relations.
3. opposite match - a matching predicate exists in F with some matching arguments
but incorrect dependency relations.
4. no match - no matching predicate in F has any matching arguments.
If a predicate is categorized as a complete match it will not be in any other category.
Finally, we compute the four features for the F, h pair: the proportion of predicates
in h that have a complete match in F, and three binary features, checking if there
is any predicate in h categorized as a partial match/opposite match/no match. Since
the subject and object arguments are crucial for textual entailment, we compute four
similar features only for the subset of predicates that have these arguments (ignoring
other arguments).
A global lexical-syntactic feature: This feature measures how well the subtrees in h
are covered by F, weighted according to the proximity to the root of h. This feature
is somewhat similar to the dependency tree kernel of Collins and Duffy (2001), which
measures the similarity between two dependency trees by counting their common
subtrees. However, our measure has several distinct properties which makes it suitable
for our needs: (a) Its a directional measure, estimating the coverage of h by F, but
not vice versa (b) It operates on a compact forest and a tree, rather than on a pair of
trees. (c) It takes into account the distance from the root of h, assuming that nodes
closer to the root are more important.
The system was trained on the RTE-3 development set, and was tested on the RTE3 and
RTE-4 test sets (no development set was released for RTE-4). Co-reference substitution was
disabled due to the insufficient accuracy of the co-reference resolution tool we used. We
first report its overall performance, and then provide some analysis of the inference module,
which is our focus in this work.
The accuracies obtained in this experiment are shown in Table 7 (under the inference
column). The results on RTE-3 are quite competitive: compared to our 66.4%, only 3 teams
out of the 26 who participated in RTE-3 scored higher than 67%, and three more systems
scored between 66% and 67%. The results for RTE4 rank 9-10 out of 26, with only 6 teams
scoring higher by more than 1%. Overall, these results show that our system is well-situated
in the state of the art for the RTE task.
Table 8 provides a more detailed view of our systems performance. Precision, recall, and
F1 results are given for both entailing and non-entailing pairs, as well as the overall accuracy.
16. When the dependent is a preposition or a clause we take the complement of the preposition or the head
of the clause respectively as the dependent.

39

fiBar-Haim, Dagan & Berant

The table also shows the results per task (IE, IR, QA and SUM). Overall, our system tends
to predict entailment more often than non-entailment. The recall for entailing pairs is much
higher than the recall for non-entailing pairs, while the precision for non-entailing pairs is
much higher than for entailing pairs. Performance varies considerably among different tasks.
Our RTE3 accuracy results for QA and IR are considerably higher than the average results
achieved by RTE3 submissions, as reported by the organizers (Giampiccolo et al., 2007)
(0.71 and 0.66, respectively), while for IE and SUM, our results are a bit above the average
(0.52 and 0.58). Our RTE4 results are better for IR and SUM, which seem to be the easier
tasks in RTE4 (Giampiccolo et al., 2008).17
7.4 Usage and Contribution of Knowledge Bases
To evaluate the accuracy gain from knowledge-based inference, we ran the system with the
inference module disabled, so that entailment classification is applied directly to the initial
parse tree of the text. The results are shown under the no inference column of Table 7.
Comparing these results to the full system accuracy (inference), we see that applying the
inference module resulted in higher accuracy on both test sets. The contribution was more
prominent for the RTE-4 dataset. These results illustrate a typical contribution of current
knowledge sources for current RTE systems. This contribution is likely to increase with
current and near future research, on topics such as extending and improving knowledge
resources, applying them only in semantically suitable contexts, improved classification
features, and broader search strategies.
Tables 9 and 10 illustrate the usage and contribution of individual rule bases. Table 9
shows the distribution of rule applications over the various rule bases. Table 10 presents
ablation study showing the marginal accuracy gain for each rule base. These results show
that each of the rule bases is applicable for a large portion of the pairs, and contributes
to the overall accuracy. We note that the results are highly dependent on the search
strategy. For instance, chaining of lexical rules is expected to increase the number of lexical
rule applications, but reduce their accuracy. We provide a more detailed analysis of rule
applications in our system in the next section.
7.5 Manual Analysis
We conclude the evaluation with two manual analyses of the inference component within the
RTE system. The first analysis (Subsection 7.5.1) assesses the applicability of our inference
framework to the RTE task as well as the actual coverage of the current system. It also
categorizes the cases in which our formalism falls short. We then (Subsection 7.5.2) assess
the correctness of the applied rules, and analyze the various causes for incorrect applications.
The analyses were done by one of the authors on randomly sampled subsets of the RTE-3
test set.

17. According to the RTE4 organizers, the IE task appeared to be the most difficult task, while SUM and
IR seemed to be the easier tasks. However, they did not report the average accuracy per task.

40

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Test set
RTE3
RTE4

Accuracy
No inference Inference
64.6%
66.4%
57.5%
60.6%


1.8 %
*3.1%

Lexical
Overlap
62.4%
56.6%

Best RTE
Result
80.0%
74.6%

Table 7: Inference contribution to RTE performance. The system was trained on the RTE3 development set. * indicates statistically significant difference (at level p < 0.02, using
McNemars test). The best results achieved in the RTE3 and RTE4 challenges (Hickl &
Bensley, 2007; Bensley & Hickl, 2008), as well as lexical overlap baseline results (Mehdad
& Magnini, 2009a), are also given for reference. Mehdad and Magnini have tested eight
configurations of lexical overlap baselines, and chose the one that performs best on average
over the RTE1-4 test sets.

RTE3

RTE4

Task
IE
IR
QA
SUM
All
IE
IR
QA
SUM
All

Non-Entailing Pairs
Precision Recall
F1
0.500
0.095 0.159
0.764
0.743 0.753
0.822
0.787 0.804
0.545
0.341 0.420
0.722
0.505 0.594
0.596
0.187 0.284
0.721
0.587 0.647
0.636
0.210 0.316
0.685
0.630 0.656
0.680
0.400 0.504

Entailing Pairs
Precision Recall
F1
0.527
0.914 0.669
0.678
0.701 0.689
0.818
0.849 0.833
0.600
0.777 0.677
0.634
0.815 0.713
0.518
0.873 0.650
0.652
0.773 0.707
0.527
0.880 0.659
0.657
0.710 0.683
0.575
0.812 0.673

Accuracy
0.525
0.725
0.820
0.585
0.664
0.530
0.680
0.545
0.670
0.606

Table 8: RTE results breakdown by task and pair type

Rule base
WordNet
AmWN
Wikipedia
DIRT
Generic
Polarity

RTE3-Dev
Rules App
0.6
1.2
0.3
0.4
0.6
1.7
0.5
0.7
4.7 10.4
0.2
0.2

RTE4
Rules App
0.6
1.1
0.3
0.4
0.6
1.3
0.5
1.0
5.4 11.5
0.2
0.2

Table 9: Average number of rule applications per (t, h) pair, for each rule base. App counts
each rule application, while Rules ignores multiple matches of the same rule in the same
iteration.

7.5.1 Applicability and Coverage
This analysis assesses the ability of the inference framework to derive complete proofs for
RTE (t,h) pairs in an idealized setting where perfect knowledge bases and co-reference
resolution are available. This provides an upper bound to the coverage of our inference
41

fiBar-Haim, Dagan & Berant

Rule base
WordNet
AmWN
Wikipedia
DIRT
Generic
Polarity

Accuracy (RTE4)
0.8%
0.7%
1.0%
0.9%
0.4%
0.9%

Table 10: Contribution of various rule bases. Results show accuracy loss on RTE-4, obtained
when removing each rule base (ablation tests).

engine. A similar analysis was previously done by Bar-Haim, Szpektor, and Glickman
(2005) on a subset of the RTE-1 dataset. However, here we go further and (a) assess the
actual coverage of the required inferences by the implemented RTE system, and (b) present
classification of uncovered cases into different categories.
We carried out the analysis as follows: 80 positive (entailing) pairs were randomly
sampled from the RTE-3 test set. For each pair we aimed to manually derive a proof
comprising inference steps that are expressible in our formalism, similar to the example
in Section 2.2. If a complete proof could be derived, the pair was classified as inferable.
Otherwise, it was classified into one of the following categories:
Discourse references: Complete proof requires incorporating pieces of information from
the discourse, including event co-reference and bridging (Mirkin et al., 2010). Nominal co-reference substitution was not included, as it is covered in our formalism.
For instance, in the text The Titanics sinking after hitting an iceberg on April 14,
1912. . . , the year 1912 is not explicitly specified as the time of the Titanics sinking,
and this relation should be derived from the discourse in order to infer the hypothesis
The Titanic sank in 1912.
Non-decomposable: The inference cannot be reasonably decomposed to a sequence of
local rewrites. This is the case, for example, with the text The black plague lasted
four years and killed about one-third of the population of Europe, or approximately 20
million people and the hypothesis Black plague swept Europe.
Other: A few other cases that did not fall into the above categories.
The distribution of these categories is shown in Table 11. We found that 60% of the
pairs could be proven by our formalism given appropriate inference rules and co-reference
information, which demonstrates the utility of our approach. The results are somewhat
higher than the 50% reported by Bar-Haim et al. (2005), which may be attributed to the
fact that RTE1 is considered a more difficult dataset, and entailment systems consistently
perform better on RTE3.
Out of the remaining 40% pairs, our analysis highlights the significance of discourse
references, which occur in 16.3% of the pairs. While previous analysis of discourse references
in textual entailment was applied to the RTE-5 search task, where the text sentences are
interpreted in the context of their full discourse (Mirkin et al., 2010), our analysis shows
42

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Category
Inferable
Non-decomposable
Discourse references
Other

Count
48
14
13
5

%
60.0%
17.5%
16.3%
6.3%

Table 11: Applicability of our inference framework to the RTE task. 80 randomly selected
entailing pairs from the RTE-3 test set were analyzed.

the significance of discourse references even for short, self-contained texts, of which RTE3 is composed. Mirkin et al. show how our framework, and similar methods based on
tree transformations, can be extended to utilize discourse references. Several works over
the last few years targeted implied predicate-argument relationships, the most notable of
which is the SemEval-2010 Task Linking Events and Their Participants in Discourse
(Ruppenhofer, Sporleder, Morante, Baker, & Palmer, 2009). In particular, Stern and Dagan
(2014) recently showed that identifying such relations improves the performance of their
RTE system. Finally, the entailment in 17.5% of the pairs could not be established by a
sequence of local rewrites, thus these cases are likely to require deeper methods for semantic
analysis and inference.
The manually-derived proofs for the 48 inferable pairs included a total of 79 rule applications, an average of 1.65 rule applications per pair.18 The maximal number of rules per
pair was 3. 28 of these rules (35.4%) were applied in our system. 21% of the proofs for
the inferable pairs were fully derived by our RTE system. Partial proofs were derived for
additional 25% of the pairs. For the remaining 54% of the pairs, our system did not apply
any of the rules in the manual proof. The results demonstrate the utility of the inference
mechanisms and rule bases in our system, but on the other hand suggest that there is still
much room for improvement in the coverage of the existing rule bases.
7.5.2 Correctness of Applied Rules
We next assess the correctness of rules applied by the inference engine. We focus on the
four lexical and lexical-syntactic rule bases described in Section 6.3: WordNet, Wikipedia,
DIRT, and Argument-Mapped WordNet (AmWN). Except for WordNet, these rule bases
were generated automatically, therefore their accuracy is more of an issue than the accuracy
of the manually-composed generic inference rules and polarity annotation rules. Furthermore, lexicalized rules are often context sensitive, which is an additional potential source
of incorrect rule applications.
For this evaluation we randomly sampled 75 pairs from the RTE-3 test set, and analyzed
all lexical and lexical-syntactic rule applications performed by the system for these pairs, a
total of 201 rule applications. We define two levels of rule application correctness:

18. As previously mentioned, the RTE system does not apply rules that merely extract a subtree from a
given source tree. Accordingly, such rules were ignored in this analysis as well.

43

fiBar-Haim, Dagan & Berant

Propositional: The derived tree resulting from the rule application is both grammatical
and entailed from the source tree. This is the level of correctness assumed by our
formalism.
Referential: In case the propositional correctness does not hold, we turn to the weaker criterion of Referential Correctness, following the notion of Lexical Reference (Glickman,
Shnarch, & Dagan, 2006; Shnarch et al., 2009), which we extend here to the case of
template-based rules with variables. Let rule E : L  R be an inference rule matched
in a source tree s. Let l and r be the instantiations of L and R respectively, according
to the variable matching of L in s. We say that referential correctness holds if l generates a reference in s to a possible meaning of r. Some examples for such rules found
in our analyzed sample are: popepapal, TurkishTurkey and fishermenfishing.
While these rule applications do not result in a valid entailed tree, they are still useful
in the context of an RTE system that applies approximate matching (as previously
discussed at the end of Section 6).
Incorrect rule applications were classified into one of the following categories:
1. Bad rule: The rule is a-priori incorrect (e.g., Walesyear ).
2. Bad context: The rule is incorrect in the context of the source sentence. For example,
the WordNet rule strikecreate corresponds to the rare sense of strike defined as
produce by ignition or a blow (as in strike fire from the flint stone).
3. Bad match: The rule was applied due to incorrect matching of the left-hand-side,
resulting from incorrect parse of the source tree.
The results are summarized in Table 12. Overall, 52.7% of the rule applications are correct.
Interestingly, there are more referential (29.4%) than propositional (23.4%) rule applications. Unsurprisingly, the most accurate knowledge resource is the manually composed
WordNet (75.9% correct applications), followed by the AmWN (57.9%) and Wikipedia
(57.4%) rule bases, which were derived automatically from human-generated resources. The
least accurate resource is DIRT (21.4%), which makes no use of human knowledge engineering, but rather was learned automatically based on corpus statistics. The accuracy of DIRT
is considerably lower than the accuracy of the other resources, substantially decreasing the
overall accuracy as well. Most of the errors for DIRT and Wikipedia are due to bad rules.
This is also the overall dominant cause for incorrect applications, while for WordNet and
AmWN the a-priori rule quality is very high and most of the errors are due to bad context. Wikipedia rules did not suffer from bad context, which can be explained by the fact
that their left-hand-side was often an unambiguous named entity (Madrid, Antelope Valley
Freeway, Microsoft Office). The analysis highlights the need for improving the accuracy
of automatically-generated rule bases, whose quality is still far below human generated resources. The analysis also shows that context-sensitivity of lexicalized rules is still an issue
even when these rules are applied conservatively as in our experiment (no chaining, both L
and R were matched in F and h). This should be addressed in future research.
44

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

% out of rule applications
Propositional
Referential
Correct
Bad rule
Bad context
Bad matching
Incorrect

DIRT
27.9%
17.9%
3.6%
21.4%
58.9%
7.1%
12.5%
78.6%

AmWN
9.5%
21.1%
36.8%
57.9%
5.3%
31.6%
5.3%
42.1%

Wikipedia
33.8%
19.1%
38.2%
57.4%
42.6%
0.0%
0.0%
42.6%

WordNet
28.9%
34.5%
41.4%
75.9%
0.0%
17.2%
6.9%
24.1%

All
100.0%
23.4%
29.4%
52.7%
31.3%
10.0%
6.0%
47.3%

Table 12: Analysis of lexical and lexical-syntactic rule applications

8. Discussion: Comparison to Related Approaches
In this section, we compare our work to several closely-related inference methods, most of
which were described in Section 2.3.2.
The discourse commitments derived by Hickl (2008) are quite similar to the kind of consequents we generate by applying our syntactic, lexical-syntactic, and co-reference substitution rules. However, our work differs from Hickls in several respects. First and foremost,
Hickls work does not fully describe a knowledge representation and inference framework,
which is the main focus of our work. Hickl briefly mentions that the commitments were
generated using a probabilistic FST-based extraction framework, but no further explanations or examples are given in the paper. Second, our framework allows unified modeling of
a variety of inference types that are addressed by various tools and components in Hickls
system (FST, relation extraction, paraphrase acquisition, etc.). In addition, our system
operates over lexical-syntactic representations, and does not rely on semantic parsing. Finally, the consequents generated in our formalism are packed in an efficient data structure,
whereas Hickls commitments are generated explicitly and he does not discuss commitment
generation efficiency. It should be noted, however, that while explicit generation of commitments restricts the search space, it may simplify approximate matching (e.g., finding
alignment between h and a given consequent vs. aligning h with the whole compact forest).
De Salvo Braz et al. (2005) presented a semantic inference framework that augments
the text representation with only the right-hand-side of an applied rule, and in this respect
is similar to ours. However, in their work, both rule application and the semantics of
the resulting augmented structure were not fully specified. In particular, the distinction
between individual consequents was lost in the augmented graph. By contrast, our compact
inference is fully formalized and is proved to be equivalent to an expressive, well-defined
formalism operating over individual trees, where each inferred consequent can be recovered
from the compact forest.
MacCartney and Manning (2009) proposed a model of natural language inference which,
similar to our framework, operates directly on parse-based representations. Their work extends previous work on natural logic (Valencia, 1991), which focused on semantic containment and monotonicity, by incorporating semantic exclusion and implicativity. They model
the inference of h from t as a sequence of atomic edits; each can be thought of as generating
an intermediate premise. Their calculus computes the semantic relation between the source
45

fiBar-Haim, Dagan & Berant

and the derived premise by propagating the semantic relation from the local edit upward
through the parse tree according to the properties of intermediate nodes. For example, it
can correctly infer that Some first-year students arrived  Some students arrived , but
Every first-year student arrived  Every student arrived . The composition of these semantic relations along the inference chain yields the semantic relation holding between t
and h. Their contribution is complementary to ours. In both approaches, the inference of
h from t is modeled as a sequence of atomic steps (rule applications or edits). The focus
of our framework is the representation and application of diverse types of transformations
needed for textual inference, as well as efficient representation of possible inference chains.
Application of an inference rule is assumed to always generate an entailed consequent, and
polarity rules may be used to detect situations where this assumption does not hold and
block rule application. By comparison, the formalism of MacCartney and Manning assumes
rather simple edit operations, and is focused on precise predication of the semantic relation
between t and h for a given sequence of edits that transform t into h. Thus, combining
these two complementary approaches is a natural direction for future research.

9. Conclusion
The subject of this work was the representation and use of semantic knowledge for textual
inference at the lexical-syntactic level. We defined a novel inference framework over parse
trees, which represents diverse semantic knowledge as inference rules. The proof process
aims to transform the source text into the target hypothesis through a sequence of rule
applications, each generating an intermediate parse tree. A complementary contribution of
this work is a novel data structure and an associated rule application algorithm, which are
proved to be a valid implementation of the inference formalism. We illustrated inference
efficiency both analytically and empirically.
Our approach has several advantageous properties. First, the ability to represent and
apply a wide variety of inferences and combine them through rule chaining makes our framework more expressive than most of the previous RTE architectures. Second, this expressive
power is obtained by a well-formalized and compact framework, based on unified knowledge
representation and inference mechanisms. Finally, as shown by our RTE experiments, the
compact forest data structure allows our approach to scale well to practical settings that
involve very large rule bases and hundreds of rule applications per text-hypothesis pair.
We demonstrated the utility of our approach in two different semantic tasks. Experiments with unsupervised relation extraction showed that our exact proofs outperform the
more heuristic common practice of hypothesis embedding. We also achieved competitive
results on RTE benchmarks, by adding a simple approximate matching module to our
inference engine. The contribution of semantic knowledge was illustrated on both tasks.
Limitations and possible extensions for our formalism were discussed in Section 4.8.
Manual analysis of the inference engines performance on the relation extraction and RTE
tasks further suggested promising directions for future research, as discussed in Subsections
7.1.3 and 7.5. Two additional major areas for further research are the approximate matching
heuristics and the proof search strategy. Stern and Dagan (2011) and Stern, Stern, Dagan,
and Felner (2012) extended our work to address these two aspects, respectively.
46

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Acknowledgments
This article is based on the doctoral dissertation of the first author, which was completed
under the guidance of the second author at Bar-Ilan University (Bar-Haim, 2010). This
work was partially supported by Israel Science Foundation grants 1095/05 and 1112/08,
the IST Programme of the European Community under the PASCAL Network of Excellence IST-2002-506778, the PASCAL-2 Network of Excellence of the European Community
FP7-ICT-2007-1-216886, the Israel Internet Association (ISOC-IL), grant 9022, and the
FBK-irst/Bar-Ilan University collaboration. The third author is grateful to the Azrieli
Foundation for the award of an Azrieli Fellowship. The authors wish to thank Cleo Condoravdi for making the polarity lexicon developed at PARC available for this research. We are
grateful to Eyal Shnarch for his help in implementing the experimental setup described in
Section 7.1. We also thank Iddo Greental for his collaboration on developing the generic rule
base. Finally, we would like to thank Dan Roth, Idan Szpektor, Yonatan Aumann, Marco
Pennacchiotti, Marc Dymetman and the anonymous reviewers for their valuable feedback
on this work.

Appendix A: Compact Forest Complete Proofs
In this section, we provide complete proofs for the correctness of the compact inference
algorithm presented in Section 5. We start with a few definitions.
Definition Let L  R be a rule matched and applied to a compact forest F. As in Section 5.2, let l be a subtree of some represented tree t  T (F), in which L is matched. Recall
that SL was defined as l excluding nodes matched by dual-leaf variables, and similarly SR
was defined as a copy of R without its dual-leaf variables that is generated and inserted into
F as part of rule application. The roots of SL and SR are denoted rL and rR respectively.
We say that a node s  SR is tied to a node s0  SL , if s is set as a source node for one of
the outgoing d-edges of s0 , due to alignment sharing or dual leaf variable sharing.
The graph operations performed when applying a rule L  R to a compact forest F
can be summarized as follows:
1. Adding the subtree SR to F.
2. Setting rR as a target node of a d-edge in F.
3. Setting nodes in SR that are tied to nodes in SL as source nodes for d-edges in F,
according to the rules of variable sharing and dual leaf variable sharing. Recall that
these d-edges are not part of SL .
First, we show a simple property of cDGs generated by the inference process:
Lemma 1 Every node in a cDG generated by the inference process has at most one incoming d-edge.
47

fiBar-Haim, Dagan & Berant

Proof By construction, in the initial forest each node has at most one incoming d-edge.
Each rule application adds a subtree SR , whose nodes have at most one incoming d-edge.
Last, the root rR , which initially has no incoming edges, is set as a target for a single
d-edge during rule application (the incoming d-edge of rL ). Therefore, the lemma follows
by induction on the number of rule applications.
Using the following theorem we show that the inference process generates a compact
forest:
Theorem 1 Applying a rule to a compact forest results in a compact forest.
Proof Let F 0 be the cDG generated by applying the rule L  R to a compact forest F.
We show that F 0 is a compact forest, that is, a cDAG with a single root r where all the
embedded DAGs rooted in r are trees. First, we show that F 0 is a cDAG, (i.e., it does not
contain a cycle of e-edges).
Assume by contradiction that F 0 contains a simple cycle of e-edges C. Applying the
rule L  R did not add any e-edges between nodes in F. Therefore, C must pass through
rR , the root of SR and contain an e-edge (p, rR ). Since SR is a tree, C must also leave SR
through an e-edge (u, v) (u  SR and v 
/ SR ). The cycle can be written as p  rR  ... 
u  v  ...  p. Notice that the path from v to p is fully contained in F since the cycle
C is simple and entering SR is possible only through rR .
L  R must be a substitution rule, otherwise p would be the root of F. This is
impossible, since the root has no incoming d-edges. Therefore, rR and rL have the same
single incoming d-edge, and the e-edge (p, rL ) exists in F. In addition, u was added as a
source node of a d-edge d in F since it is tied to some u0  SL , which is also a source node of d.
Therefore, the path rL  ...  u0  v exists in F. Finally, we know that the path from v to
p is fully contained in F, therefore we can construct a cycle p  rL  ...  u0  v  ...  p
in F, in contradiction to our assumption that F is a compact forest.
We have shown that F 0 is a cDAG. Next, we define a generalization for embedded DAGs,
which will help us show that all embedded DAGs in F 0 rooted in r are trees.
Definition An embedded partial DAG G = (V, E) in a cDAG G rooted in a node v  V is
similar to an embedded DAG and is generated using the following process:
1. Initialize G with v alone
2. Repeat any number of iterations:
(a) choose a node s  V
(b) choose an outgoing d-edge d of s that was not already chosen by s in a previous
iteration. If all d-edges have been chosen - halt.
(c) choose a target node t  Td and add the e-edge (s, t)d to G.
We now show that all embedded partial DAGs in F 0 rooted in any node are trees. Since
an embedded DAG is also an embedded partial DAG, this proves that all embedded DAGs
in F 0 rooted in r are trees. Assume by contradiction that after applying L  R there is an
48

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

embedded partial DAG T 0 rooted at a node n that is not a tree. We can assume that n is
not in SR , otherwise, we can extend T 0 by adding a path p  rR  ...  n, where p is a
node outside SR that is a source node of the incoming edge of rR .
Since T 0 is not a tree, there are two simple paths P1 and P2 from n that reach some
node z from two different e-edges. z cannot be in SR , since any two paths that meet in the
subtree SR , must first meet in its root rR entering its incoming d-edge. However, we could
then construct in F the same two paths, only selecting rL instead of rR , in contradiction
with the assumption that F is a compact forest. Clearly, either P1 or P2 must pass through
the new subtree SR , otherwise the two paths already existed in F.
We first handle the case where, without loss of generality, P1 passes through SR and
P2 does not. P1 passes through SR and contains an e-edge (p, rR ). Since z 
/ SR , it
also contains an e-edge (u, v) such that u  SR and v 
/ SR . So P1 can be written as
n  ...  p  rR  ...  u  v  ...  z. The paths from n to p and from v to z
are in F, because the only way to enter SR is through rR and P1 is simple. We can now
incrementally construct in F the following embedded partial DAG T : First, we construct
P2 and the section in P1 from n to p as in T 0 . Next, we expand p with the e-edge (p, rL )
instead of (p, rR ). We would like to expand T from rL and reach z if possible.
As previously explained, u is tied to a node u0  SL and therefore the e-edge (u0 , v) exists
in F. Therefore, there is a path P 0 in SL , from rL through (u0 , v) to z. However, it is not
guaranteed that the whole P 0 can be added to T . We try to expand T incrementally with
P 0 , at each step adding the next e-edge in the path. If we succeed, then T is an embedded
graph in F with two paths to z, a contradiction. If we fail, this can only be due to an e-edge
(z 0 , t)  P 0 we cannot add. Thus, z 0 must already be in P2 , and is a node for which there
are two distinct paths in the embedded graph T , a contradiction. The path constructed is
indeed different from P2 since it contains the e-edge (p, rL ) that cannot be part of P2 , since
P1 contains the disjoint edge (p, rR ).
In the remaining case, both P1 and P2 pass through SR and reach a node z 
/ SR . P1
can be written as n  ...  u1  v1  ...  z and P2 as n  ...  u2  v2  ...  z,
where u1 , u2  SR , and v1 , v2 
/ SR . Assume first that the e-edges (u1 , v1 ) and (u2 , v2 )
originate from the same d-edge d. Then u1 6= u2 , otherwise (u1 , v1 ) and (u2 , v2 ) could not
be both in the same embedded partial DAG. u1 ,u2 are tied to the nodes u01 , u02  SL .
We show that u01 6= u02 : Assume by contradiction that u01 = u02 = u0 . u0 is tied to u1
and u2 due to alignment sharing or dual leaf variable sharing. u0 cannot be tied to both u1
and u2 due to alignment sharing since alignment is a function from nodes in SL to nodes in
SR . It cannot be tied to both due to dual leaf variable sharing, since any variable appears
only once in R. Finally, if u0 is tied to u1 (without loss of generality) due to dual leaf
variable sharing, then the d-edge d is part of l. Therefore, u2 will not include d as an
aligned modifier, and thus u2 will not be tied to u0 due to alignment.
We can now construct an embedded graph T rooted at rL in F: Because SL is part of
the match of L in F, we can construct an embedded graph rooted at rL with a path to
any node in SL , in particular with paths to u01 and u02 . Since u01 6= u02 , and both u01 and
u02 are source nodes of d, which is not part of SL , we can expand these two paths with the
e-edges (u01 , v1 ) and (u02 , v1 ) of d and get an embedded graph in Gn that is not a tree, a
contradiction.
49

fiBar-Haim, Dagan & Berant

Suppose that the e-edges (u1 , v1 ) and (u2 , v2 ) originate from different d-edges d1 and
d2 respectively. u1 and u2 are tied to u01 and u02 . Therefore, if v1 6= v2 we can construct
the following embedded graph T rooted at rL : as in the previous case, we can expand the
paths in SL from rL to u01 and u02 . Next, we add the e-edges (u01 , v1 ) of d1 and (u02 , v2 ) of
d2 . Recall that d1 and d2 are not in SL and can therefore be used for expansion. We try
to expand this embedded graph to include the paths from v1 and v2 to z. If we succeed,
we have two paths in T leading to z. If we fail we have two paths in Tn meeting at some
other node z 0 , as explained above. Last, if v1 = v2 = v, then v is a node in F that has two
incoming d-edges, contradicting Lemma 1.
The case of an introduction rule is quite similar but simpler. If P1 passes through SR
and P2 does not, then n must be the root of the compact forest (the only node with a path
to rR ). However, in this case n has a single outgoing d-edge, and therefore all its outgoing
e-edges are disjoint (i.e. cannot be part of the same embedded DAG). Thus, P2 must also
pass through rR - a contradiction. If both P1 and P2 pass through SR , the proof is identical
to the case of a substitution rule.
We have shown that F 0 is a cDAG whose embedded DAGs rooted in r are trees. F 0
also has a single root because all new nodes added when applying L  R have an incoming
edge. Hence, F 0 is a compact forest.
Corollary 1 The inference process generates a compact forest.
Proof It is easy to verify that initialization generates a compact forest. Since applying
a rule to a compact forest results in a compact forest, the inference process generates a
compact forest by induction on the number of rule applications.
Theorem 2 Given a rule base R and a set of initial trees T , a tree t is represented by a
compact forest derivable from T by the inference process  t is a consequent of T according
to the inference formalism.
Proof () We first show completeness by induction on the number of rule applications n.
If n = 0 then t is one of the initial trees and is represented by the initial compact forest.
Let tn+1 be a tree derived in the formalism by applying a sequence of n + 1 rules. We show
that tn+1 is represented in a derivable compact forest. tn+1 was derived by applying the
rule L  R to the tree tn . According to the inductive assumption, tn is represented in a
compact forest F derivable by the inference process. Therefore, the rule L  R can be
matched and applied in F. We assume L  R is a substitution rule since the case of an
introduction rule is similar. tn+1 is almost identical to tn except it contains the subtree
R instead of L with instantiated variables and aligned modifiers. It is easy to verify that
after application of L  R on F resulting in F 0 , F 0 will contain an embedded tree t that is
almost identical to tn , except that the root of SR , rR , will be chosen instead of the root of
SL , rL , and the rest of SR can also be chosen with the appropriate instantiated variables
and modifiers. Therefore, tn+1 = t is contained in F 0 as required. t is guaranteed to be a
tree according to Corollary 1.
() Next, we prove soundness by induction on the number of rule applications in the
forest. At initialization, all of the initial trees are consequents. Let Fn+1 be a compact
forest derived by n + 1 rule applications (Corollary 1 guarantees that Fn+1 is indeed a
50

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

compact forest). Given a tree tn+1 represented by Fn+1 , we show that tn+1 is a consequent
in the formalism.
If tn+1 was already represented by the compact forest after n rule applications, then
according to the assumption of the induction it is a consequent in the formalism. If not,
then tn+1 is a new embedded tree created after the application of the rule L  R. Therefore,
tn+1 contains the entire subtree SR . We now incrementally construct an embedded tree tn
represented by Fn such that tn+1 is the result of applying L  R to tn .
For a substitution rule, we first construct the part of tn+1 that does not include the
subtree rooted at rR . For an introduction rule, we take any path from the forests root to
rL . Next, we construct SL through rL instead of SR through rR . This is possible since
according to Corollary 1 all embedded graphs are trees, therefore the nodes of SL are not
already in tn . We then look at the set of e-edges (s, t)  tn+1 such that s  SR and t 
/ SR .
Let (s, z) be such an edge originating from a d-edge d and Sz be the subtree rooted at z in
tn+1 . Notice that Sz was already part of Fn . s is tied to s0  SL and therefore s0 is a source
node of d. We can expand tn to include the edge (s0 , z) and Sz if s0 is not already used
with the d-edge d in tn . This is guaranteed because d is not part of SL (only d-edges that
are not part of SL are shared). Finally, we complete the construction of tn by arbitrarily
expanding any unused outgoing d-edge of tn s nodes, until we obtain a complete embedded
tree.
We constructed an embedded tree tn in Fn . Therefore, according to the inductive
assumption, tn is a consequent in the formalism. tn contains SL and an instantiation of the
dual leaf variables. Therefore, it is matched by L and the rule L  R can be applied. It is
easy to verify that an application of the rule on tn will yield tn+1 , as required. Thus, tn+1
is also a consequent in the formalism.

For the sake of simplicity, the above proofs ignored the case where one or more leaf
variables in L that match multiple target nodes in l appear in R as non-leaves. As described
in Section 5.2, in this case the matched target nodes are inserted to SR as alternatives (with
proper sharing of their modifiers). Consequently, SR becomes a compact forest containing
multiple trees. Similarly, SL is a compact forest, whose represented trees correspond to the
possible choices of matching the leaf variables. The mapping between the nodes matched
by the leaf variables in SL and the nodes generated for them in SR defines a one-to-one
mapping between the trees in SL and SR .
The above proofs can be easily adapted to handle this case, as follows. First, the proof of
Lemma 1 need not change. In Theorem 1, the proof that a rule application does not create
cycles still holds if the underlying graph of SR is a DAG rather than a tree. To prove that
each embedded partial DAG T 0 is a tree, we observe that exactly one of the trees embedded
in SR is part of T 0 . Thus, we can consider only that tree in SR and its corresponding tree
in SL , while ignoring the rest of SR and SL , and proceed with the original proof. Similarly,
to prove completeness in Theorem 2, we refer to the tree represented in SL , which is part of
tn , and the corresponding tree in SR . To prove soundness, we consider each of the subtrees
in SR and their corresponding tree in SL .
51

fiBar-Haim, Dagan & Berant

References
Bar-Haim, R. (2010). Semantic Inference at the Lexical-Syntactic Level. Ph.D. thesis,
Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel.
Bar-Haim, R., Berant, J., & Dagan, I. (2009). A compact forest for scalable inference over
entailment and paraphrase rules. In Proceedings of EMNLP.
Bar-Haim, R., Berant, J., Dagan, I., Greental, I., Mirkin, S., Shnarch, E., & Szpektor, I.
(2008). Efficient semantic deduction and approximate matching over compact parse
forests. In Proceedings of the TAC 2008 Workshop.
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., & Szpektor,
I. (2006). The Second PASCAL Recognising Textual Entailment Challenge. In The
Second PASCAL Challenges Workshop on Recognizing Textual Entailment.
Bar-Haim, R., Dagan, I., Greental, I., & Shnarch, E. (2007). Semantic inference at the
lexical-syntactic level. In Proceedings of AAAI.
Bar-Haim, R., Szpektor, I., & Glickman, O. (2005). Definition and analysis of intermediate
entailment levels. In Proceedings of the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment.
Barzilay, R., & Lee, L. (2003). Learning to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of HLT-NAACL.
Barzilay, R., & McKeown, K. R. (2001). Extracting paraphrases from a parallel corpus. In
Proceedings of ACL.
Bensley, J., & Hickl, A. (2008). Workshop: Application of LCCs GROUNDHOG system
for RTE-4. In Proceedings of the TAC 2008 Workshop.
Bentivogli, L., Clark, P., Dagan, I., Dang, H. T., & Giampiccolo, D. (2010). The Sixth
PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the TAC 2010
Workshop.
Bentivogli, L., Dagan, I., Dang, H. T., Giampiccolo, D., & Magnini, B. (2009). The Fifth
PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the TAC 2009
Workshop.
Berant, J., Dagan, I., & Goldberger, J. (2011). Global learning of typed entailment rules.
In Proceedings of ACL.
Bhagat, R., & Ravichandran, D. (2008). Large scale acquisition of paraphrases for learning
surface patterns. In Proceedings of ACL-08: HLT.
Bos, J., & Markert, K. (2005). Recognising textual entailment with logical inference techniques. In Proceedings of EMNLP.
Bos, J., & Markert, K. (2006). When logical inference helps determining textual entailment
(and when it doesnt). In Proceedings of The Second PASCAL Recognising Textual
Entailment Challenge.
Chklovski, T., & Pantel, P. (2004). VerbOcean: Mining the web for fine-grained semantic
verb relations. In Proceedings of EMNLP.
52

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Collins, M., & Duffy, N. (2001). Convolution kernels for natural language. In Advances in
Neural Information Processing Systems 14.
Connor, M., & Roth, D. (2007). Context sensitive paraphrasing with a single unsupervised
classifier. In ECML.
Cooper, R., Crouch, R., van Eijck, J., Fox, C., van Genabith, J., Jaspars, J., Kamp, H.,
Pinkal, M., Milward, D., Poesio, M., Pulman, S., Briscoe, T., Maier, H., & Konrad, K.
(1996). Using the framework. Tech. rep., FraCaS: A Framework for Computational
Semantics.
Dagan, I., & Glickman, O. (2004). Probabilistic textual entailment: Generic applied modeling of language variability. PASCAL workshop on Text Understanding and Mining.
Dagan, I., Glickman, O., Gliozzo, A., Marmorshtein, E., & Strapparava, C. (2006a). Direct
word sense matching for lexical substitution. In Proceedings of COLING-ACL.
Dagan, I., Glickman, O., & Magnini, B. (2006b). The PASCAL Recognising Textual Entailment Challenge. In Quinonero-Candela, J., Dagan, I., Magnini, B., & dAlche Buc, F.
(Eds.), Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944,
pp. 177190. Springer.
Dagan, I., Roth, D., Sammons, M., & Zanzotto, F. M. (2013). Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
de Salvo Braz, R., Girju, R., Punyakanok, V., Roth, D., & Sammons, M. (2005). An inference
model for semantic entailment in natural language.. In Proceedings of AAAI.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).
Indexing by latent semantic analysis. Journal of the American Society of Information
Science, 41 (6), 391407.
Dinu, G., & Lapata, M. (2010). Topic models for meaning similarity in context. In Proceedings of Coling 2010: Posters.
Emele, M. C., & Dorna, M. (1998). Ambiguity preserving machine translation using packed
representations. In Proceedings of COLING-ACL.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. Language, Speech
and Communication. MIT Press.
Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using Wikipediabased Explicit Semantic Analysis. In Proceedings of IJCAI.
Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2013). PPDB: The paraphrase
database. In Proceedings of HLT-NAACL.
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. (2007). The Third PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
Giampiccolo, D., Trang Dang, H., Magnini, B., Dagan, I., & Dolan, B. (2008). The Fourth
PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the TAC 2008
Workshop.
53

fiBar-Haim, Dagan & Berant

Glickman, O., & Dagan, I. (2003). Identifying lexical paraphrases from a single corpus: a
case study for verbs. In Proceedings of RANLP.
Glickman, O., Shnarch, E., & Dagan, I. (2006). Lexical reference: A semantic matching
subtask. In Proceedings of EMNLP.
Haghighi, A. D., Ng, A. Y., & Manning, C. D. (2005). Robust textual inference via graph
matching. In Proceedings of EMNLP.
Harmeling, S. (2009). Inferring textual entailment with a probabilistically sound calculus.
Natural Language Engineering, 15 (4), 459477.
Heilman, M., & Smith, N. A. (2010). Tree edit models for recognizing textual entailments,
paraphrases, and answers to questions. In Proceedings of HLT-NAACL.
Hickl, A. (2008). Using discourse commitments to recognize textual entailment. In Proceedings of COLING.
Hickl, A., & Bensley, J. (2007). A discourse commitment-based framework for recognizing textual entailment. In Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Hickl, A., Bensley, J., Williams, J., Roberts, K., Rink, B., & Shi, Y. (2006). Recognizing textual entailment with LCCs GROUNDHOG system. In The Second PASCAL
Challenges Workshop on Recognizing Textual Entailment.
Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing: An Introduction
to Natural Language Processing, Computational Linguistics and Speech Recognition
(Second edition). Prentice Hall.
Kamp, H., & Reyle, U. (1993). From Discourse to Logic. Introduction to Modeltheoretic
Semantics of Natural Language, Formal Logic and Discourse Representation Theory.
Kluwer Academic Publishers, Dordrecht.
Kay, M. (1996). Chart generation. In Proceedings of ACL.
Kazama, J., & Torisawa, K. (2007). Exploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-CoNLL.
Kipper, K. (2005). VerbNet: A broad-coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Kouylekov, M., & Magnini, B. (2005). Tree edit distance for textual entailment. In Proceedings of RANLP.
Lehmann, J., Bizer, C., Kobilarov, G., Auer, S., Becker, C., Cyganiak, R., & Hellmann,
S. (2009). DBpedia - a crystallization point for the web of data. Journal of Web
Semantics.
Lin, D. (1998). Dependency-based evaluation of minipar. In Proceedings of the Workshop
on Evaluation of Parsing Systems at LREC.
Lin, D., & Pantel, P. (2001). Discovery of inference rules for question answering. Natural
Language Engineering, 7 (4), 343360.
Lotan, A., Stern, A., & Dagan, I. (2013). TruthTeller: Annotating predicate truth. In
Proceedings of HLT-NAACL.
54

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

MacCartney, B., Galley, M., & Manning, C. D. (2008). A phrase-based alignment model
for natural language inference. In Proceedings of EMNLP.
MacCartney, B., Grenager, T., de Marneffe, M.-C., Cer, D., & Manning, C. D. (2006).
Learning to recognize features of valid textual entailments. In Proceedings of HLTNAACL.
MacCartney, B., & Manning, C. D. (2009). An extended model of natural logic. In Proceedings of IWCS-8.
Macleod, C., Grishman, R., Meyers, A., Barrett, L., & Reeves, R. (1998). Nomlex: A lexicon
of nominalizations. In Proceedings of Euralex98.
Maxwell III, J. T., & Kaplan, R. M. (1991). A method for disjunctive constraint satisfaction. In Tomita, M. (Ed.), Current Issues in Parsing Technology. Kluwer Academic
Publishers.
Mehdad, Y., & Magnini, B. (2009a). A word overlap baseline for the recognizing textual
entailment task. Unpublished manuscript.
Mehdad, Y., & Magnini, B. (2009b). Optimizing textual entailment recognition using particle swarm optimization. In Proceedings of the 2009 Workshop on Applied Textual
Inference.
Melamud, O., Berant, J., Dagan, I., Goldberger, J., & Szpektor, I. (2013). A two level
model for context sensitive inference rules. In Proceedings of ACL.
Meyers, A., Reeves, R., Macleod, C., Szekeley, R., Zielinska, V., & Young, B. (2004). The
cross-breeding of dictionaries. In Proceedings of LREC.
Mi, H., Huang, L., & Liu, Q. (2008). Forest-based translation. In Proceedings of ACL-08:
HLT.
Mirkin, S., Dagan, I., & Pado, S. (2010). Assessing the role of discourse references in
entailment inference. In Proceedings of ACL.
Mirkin, S., Dagan, I., & Shnarch, E. (2009). Evaluating the inferential utility of lexicalsemantic resources. In Proceedings of EACL.
Moldovan, D. I., & Rus, V. (2001). Logic form transformation of WordNet and its applicability to question answering. In Proceedings of ACL.
Nairn, R., Condoravdi, C., & Karttunen, L. (2006). Computing relative polarity for textual
inference. In Proceedings of International workshop on Inference in Computational
Semantics (ICoS-5).
Pang, B., Knight, K., & Marcu, D. (2003). Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sentences. In Proceedings of HLT-NAACL.
Pantel, P., Bhagat, R., Coppola, B., Chklovski, T., & Hovy, E. (2007). ISP: Learning
inferential selectional preferences. In Proceedings of HLT-NAACL.
Ponzetto, S. P., & Strube, M. (2007). Deriving a large-scale taxonomy from wikipedia. In
Proceedings of AAAI.
Ravichandran, D., & Hovy, E. (2002). Learning surface text patterns for a question answering system. In Proceedings of ACL.
55

fiBar-Haim, Dagan & Berant

Ritter, A., Mausam, & Etzioni, O. (2010). A latent dirichlet allocation method for selectional
preferences. In Proceedings of ACL.
Romano, L., Kouylekov, M., Szpektor, I., Dagan, I., & Lavelli, A. (2006). Investigating a
generic paraphrase-based approach for relation extraction. In Proceedings of EACL.
Ron, T. (2006). Generating entailment rules based on online lexical resources. Masters
thesis, Computer Science Department, Bar-Ilan University.
Ruppenhofer, J., Sporleder, C., Morante, R., Baker, C., & Palmer, M. (2009). Semeval2010 task 10: Linking events and their participants in discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent Achievements and Future Directions
(SEW-2009).
Saint-Dizier, P., & Mehta-Melkar, R. (Eds.). (2011). Proceedings of the Joint Workshop FAM-LbR/KRAQ11. Learning by Reading and its Applications in Intelligent
Question-Answering.
Schoenmackers, S., Etzioni, O., Weld, D. S., & Davis, J. (2010). Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Shinyama, Y., Sekine, S., Sudo, K., & Grishman, R. (2002). Automatic paraphrase acquisition from news articles. In Proceedings of HLT.
Shnarch, E., Barak, L., & Dagan, I. (2009). Extracting lexical reference rules from
Wikipedia. In Proceedings of ACL-IJCNLP.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006a). Semantic taxonomy induction from heterogenous evidence. In Proceedings of COLING-ACL.
Snow, R., Vanderwende, L., & Menezes, A. (2006b). Effectively using syntax for recognizing
false entailment. In Proceedings of HLT-NAACL.
Stern, A., & Dagan, I. (2011). A confidence model for syntactically-motivated entailment
proofs. In Proceedings of RANLP.
Stern, A., & Dagan, I. (2014). Recognizing implied predicate-argument relationships in
textual inference. In Proceedings of ACL.
Stern, A., Stern, R., Dagan, I., & Felner, A. (2012). Efficient search for transformation-based
inference. In Proceedings of ACL.
Szpektor, I., & Dagan, I. (2007). Learning canonical forms of entailment rules. In Proceedings
of RANLP.
Szpektor, I., & Dagan, I. (2008). Learning entailment rules for unary templates. In Proceedings of COLING.
Szpektor, I., & Dagan, I. (2009). Augmenting WordNet-based inference with argument
mapping. In Proceedings of ACL-IJCNLP Workshop on Applied Textual Inference
(TextInfer).
Szpektor, I., Dagan, I., Bar-Haim, R., & Goldberger, J. (2008). Contextual preferences. In
Proceedings of ACL-08: HLT.
Szpektor, I., Tanev, H., Dagan, I., & Coppola, B. (2004). Scaling web based acquisition of
entailment patterns. In Proceedings of EMNLP.
56

fiKnowledge-Based Textual Inference via Parse-Tree Transformations

Tatu, M., Iles, B., Slavick, J., Novischi, A., & Moldovan, D. (2006). COGEX at the Second Recognizing Textual Entailment Challenge. In The Second PASCAL Challenges
Workshop on Recognizing Textual Entailment.
Tatu, M., & Moldovan, D. (2006). A logic-based semantic approach to recognizing textual
entailment. In Proceedings of COLING-ACL.
Tatu, M., & Moldovan, D. (2007). COGEX at RTE3. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing.
Valencia, V. S. (1991). Studies on Natural Logic and Categorial Grammar. Ph.D. thesis,
University of Amsterdam.
van Deemter, K., & Kibble, R. (2000). On coreferring: Coreference in MUC and related
annotation schemes. Computational Linguistics, 26 (4), 629637.
Voorhees, E. M., & Harman, D. (1997). Overview of the sixth Text REtrieval Conference
(TREC-6). In Proceedings of TREC.
Wang, M., & Manning, C. (2010). Probabilistic tree-edit models with structured latent
variables for textual entailment and question answering. In Proceedings of COLING.
Wang, R., & Neumann, G. (2007). Recognizing textual entailment using a subsequence
kernel method. In Proceedings of AAAI.
Yates, A., & Etzioni, O. (2009). Unsupervised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelligence Research (JAIR), 34, 255296.
Zanzotto, F. m., Pennacchiotti, M., & Moschitti, A. (2009). A machine learning approach
to textual entailment recognition. Natural Language Engineering, 15 (4), 551582.

57

fiJournal of Artificial Intelligence Research 54 (2015) 631-677

Submitted 7/15; published 12/15

On a Practical, Integer-Linear Programming Model for Delete-Free
Tasks and its Use as a Heuristic for Cost-Optimal Planning
Tatsuya Imai

TATSUYA . IMAI .30100041@ GMAIL . COM

Graduate School of Information Science and Engineering
Tokyo Institute of Technology
Tokyo, Japan

Alex Fukunaga

FUKUNAGA @ IDEA . C . U - TOKYO . AC . JP

Graduate School of Arts and Sciences
The University of Tokyo
Tokyo, Japan

Abstract
We propose a new integer-linear programming model for the delete relaxation in cost-optimal
planning. While a straightforward IP for the delete relaxation is impractical, our enhanced model
incorporates variable reduction techniques based on landmarks, relevance-based constraints, dominated action elimination, immediate action application, and inverse action constraints, resulting in
an IP that can be used to directly solve delete-free planning problems. We show that our IP model
is competitive with previous state-of-the-art solvers for delete-free problems. The LP-relaxation
of the IP model is often a very good approximation to the IP, providing an approach to approximating the optimal value of the delete-free task that is complementary to the well-known LM-cut
heuristic. We also show that constraints that partially consider delete effects can be added to our
IP/LP models. We embed the new IP/LP models into a forward-search based planner, and show
that the performance of the resulting planner on standard IPC benchmarks is comparable with the
state-of-the-art for cost-optimal planning.

1. Introduction
The delete relaxation of a classical planning problem is a relaxation of a planning problem such
that all delete effects are eliminated from its operators. In the delete relaxation, every proposition
that becomes true remains true and never becomes false again. The delete relaxation has been
studied extensively in the classical planning literature because it can be used to estimate the cost
of an optimal plan for the original planning problem and is therefore useful as a basis for heuristic
functions for search-based domain-independent planning algorithms. A solution for the original
planning problem is a solution for its delete relaxation, and the cost of the optimal solution to a
delete-relaxed problem can be lower than the cost of the original problem because in the relaxation,
every proposition only needs to be established once. Thus, the optimal cost of the delete relaxation
of a planning problem (denoted h+ ) is a lower bound on the optimal cost of the original planning
problem. Despite the fact that computing h+ is easier than solving the original planning problem,
computing h+ is itself NP-equivalent (Bylander, 1994) and poses a challenging problem.
In addition to its importance as a basis for heuristic functions for standard classical planning,
the delete relaxation is also interesting in its own right, because there are some problems that can
be naturally modeled as delete-free problems (i.e., problems where there are no actions with delete
effects). For example, the minimal seed set problem, a problem in systems biology which seeks
c
2015
AI Access Foundation. All rights reserved.

fiI MAI & F UKUNAGA

the minimal set of nutrients that are necessary for an organism to fully express its metabolism,
can be mapped to a delete-free planning problem (Gefen & Brafman, 2011). Another application
is in relational database query plan generation (Robinson, McIlraith, & Toman, 2014), where the
problem of determining join orders can be modeled as a delete-free problem.
In this paper, we propose a new, integer programming (IP) approach to computing h+ .1 We show
that this model allows fast computation of h+ , and that the linear programming (LP) relaxation of
this model can be used successfully as the heuristic function for an A* -based planner. The rest of
this paper is structured as follows: We begin with a review of previous work on the delete relaxation
as well as applications of LP to planning. Then we introduce IP(T + ), a basic integer programming
model for a delete-free planning problem (Section 3) and show that it correctly computes h+ . Since
the straightforward IP(T + ) model is often intractable and not useful in practice for computing
h+ , we develop an enhanced model, IPe (T + ), which reduces the number of variables in the IP by
using techniques such as landmark-based constraints, relevance analysis (Section 4). We evaluate
the performance of the basic IP(T + ) and enhanced IPe (T + ) models in Section 5, and show that
IPe (T + ) is competitive with the state-of-the-art methods for computing h+ .
While our objective is to use our IP models as a basis for a heuristic for forward state-space
search based planning, solving an IP at every node in the search algorithm is computationally daunting, so in Section 6, we propose and evaluate two relaxations to our IP(T + )-based IP models. We
consider the LP(T + ) and LPe (T + ), LP-relaxation of IP(T + ) and IPe (T + ), and show that the
LP-relaxations usually closely approximate h+ . We also introduce a time-relaxation of the IP and
LP models (IPetr (T + ) and LPetr (T + ), respectively) which further reduces the number of variables,
at the cost of sometimes underestimating h+ , and show that these time-relaxations usually closely
approximate h+ . We experimentally compare how closely these relaxed, delete-free models approximate h+ with the LM-cut heuristic (Helmert & Domshlak, 2009) and show that these approaches
are complementary.
Next, in Section 7, we evaluate the utility of our IP and LP models as heuristics for forwardsearch based planning by embedding them into an A* -based planner. Our results show that although
LPetr (T + ) is not competitive with the LM-cut heuristic overall, there are some domains where
LPetr (T + ) yields state-of-the-art performance, outperforming LM-cut.
We then turn to strengthening our IP and LP models by partially considering delete effects
(Section 8). We add constraints that enforce lower bounds on the number of times an action must be
used. These correspond to the net change constraints that were recently proposed by Pommerening
et al. (2014), as well as the action order relaxation by van den Briel et al. (2007). This tightened
bound IPc (T ) dominates IP(T + ). Counting constraints can also be added to the LP-relaxation


LPec (T ), as well as the time-relaxed LP-relaxation LPectr (T ). However, the additional counting
constraints makes the IP and LP more difficult, so in a A* -based planner that uses these bounds, there
is a tradeoff between a tighter bound (fewer nodes searched by A* ) and the time spent per node. As a
result, we find that although counting constraints result in enhanced performance on some domains,
it significantly degrades performance on other domains. We experimentally compare our countingconstraint enhanced models with the LMC-SEQ LP model of Pommerening et al. (2014) which
combines landmark and net-change constraints, and show that, like LM-cut vs our delete-free LPs,
these models are complementary.
1. This paper revises and extends the work originally reported by the authors in a paper presented at ECAI2014 (Imai &
Fukunaga, 2014). Formal results and proofs which were not in the ECAI paper are included, and this paper contains
a much more thorough experimental evaluation of our models (all of the experimental data is new).

632

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Table 1 provides an overview of all of the IP/LP models discussed in Sections 3-8, and also
serves as a roadmap of this paper . For each model, we indicate the section in the text where the
model is introduced, the constraints used in the model, and the variable elimination optimizations
used in the model. Figure 1 is a directed graph showing the dominance relationships among the
optimal costs of the IP/LP models.
Finally, because there is no clear dominance relationship among our LP models (with respect
to the performance of A* -based planners that use these LP models as the heuristic function), we
propose and evaluate a simple automatic configuration heuristic which selects the LP to use as the
heuristic for A* (Section 9). This simple automated bound selection significantly boosts performance, resulting in a ensemble-based LP-heuristic that is competitive with state-of-the-art heuristics. Section 10 concludes the paper with a summary and discussion of our results and some directions for future work.

2. Background and Related Work
This section first introduces the notation for planning tasks which will be used in the rest of the paper, then surveys related work on solving delete-free planning tasks as well as previous applications
of IP/LP to domain-independent planning.
2.1 Preliminary Definitions
A STRIPS planning task is defined by a 4-tuple T = hP, A, I, Gi. P is a set of propositions. A
is a set of actions. A state is represented by a subset of P , and applying an action to a state adds
some propositions and removes some propositions in the state. Each action a  A is composed
of three subsets of P , hpre(a), add(a), del(a)i which are called the preconditions, add effects, and
delete effects. An action a is applicable to a state S iff it satisfies pre(a)  S. By applying a to
S, propositions in S change from S to S(a) = ((S \ del(a))  add(a)). For a sequence of actions
 = (a0 ,    , an ), we use S() to denote ((((S \ del(a0 ))  add(a0 )) \ del(a1 ))     )  add(an ).
Let I  P be the initial state and G  P the goal. A solution to a planning task is a sequence
of actions that transform I to a state S that satisfies G  S. Formally, a feasible solution, i.e., a
plan, is a sequence of actions  = (a0 ,    , an ) that satisfies (i) i, pre(ai )  I((a0 ,    , ai1 )),
and (ii) G  I().
The basic STRIPS planning task can be extended to STRIPS planning with action costs, where
each action a  A has an associated (non-negative) cost c(a). The objective of cost-optimal planning in a STRIPS
model with action costs is to find a plan  that minimizes the sum of the costs of
P
its actions i=n
c(a
i ).
i=0
The delete relaxation of a task T , denoted by T + , is a task hP, A+ , I, Gi where A+ is a set of
delete-free actions defined as A+ = {hpre(a), add(a), i | a  A}. We also use T + to denote a
task that is delete-free from the beginning without being relaxed.
2.2 Previous Work on Computing h+ and its Relaxations
The delete relaxation has been used as the basis for planning heuristics since the beginning of the
recent era of interest in forward-state space search based planning (Bonet & Geffner, 2001). Unfortunately, computing h+ is known to be NP-equivalent by reduction from vertex cover (Bylander,
633

fiI MAI & F UKUNAGA

Model
IP(T + ) (Sec. 3)
IPe (T + ) (Sec. 4)

Constraints
C1, C2, C3, C4,
C5, C6,
C1, C2a C3, C4,
C5, C6

Variable Eliminations
None
Landmarks (4.1), relevance (4.2), dominated
action elimination (4.3),
immediate action application (4.4)
None
Same as IPe (T + )
Same as IPe (T + )

LP(T + ) (Sec. 6.1)
LPe (T + ) (Sec. 6.1)
LPetr (T + ) (Sec. 6.2)

Same as IP(T + )
Same as IPe (T + )
C1, C2a C3, C4,

IPc (T ) (Sec. 8)

C1, C2, C3, C4,
C5, C6, C7 C8

None

IPec (T + ) (Sec. 8)

C1, C2a C3, C4,
C5, C6 C7 C8

LPc (T ) (Sec. 8)

LPec (T ) (Sec. 8)

LPectr (T ) (Sec. 8)

Same as IPc (T )

Same as IPec (T )
C1, C2a C3, C4,
C7 C8

Landmarks (4.1), relevance
(4.2), modified dominated
action elimination (Definition 2)
None

Same as IPec (T )

Same as IPec (T )

A* /autoconf (Sec. 9)

Selects among LPe (T + ), LPetr (T + ), LPec (T ),

LPectr (T ).



Basic delete-free task IP
model (computes h+ )
Enhanced IP model (computes h+ )

LP relaxation of IP(T + )
LP relaxation of IPe (T + )
LP-relaxation of timerelaxation of IPe (T + )
Basic delete-free task
IP model enhanced with
counting constraints
Enhanced IP model with
counting constraints

LP relaxation of IPc (T )

LP relaxation of IPec (T )
LP-relaxation of time
relaxation of IPec (T )



Automatic LP Model Selection

Table 1: Overview of delete-relaxation based IP/LP models in this paper

LP(T+)

LPtr(T+)

LPe(T+)

IP(T+) = IPe(T+) =aaa
h+

IPcec(T)

LPec
c(T)

LPtre(T+)

IPtre(T+)

ec(T)
IPctr

e (T)
c
LPctr

IPtr(T+)

Figure 1: Dominance relationships among our IP/LP models. Edge modeli  modelj indicates the
optimal cost of modeli  the optimal cost of modelj . The 4 highlighted LPs are the components
of the A* /autoconf model.

634

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

1994), and therefore, from the beginning, researchers avoided direct computation of h+ , and instead
sought approximations to h+ .
In satisficing planning, where optimal solutions are not required, a successful approach to deriving heuristics has been to approximate the delete relaxation. The additive heuristic (hadd ) assumes
that subgoals are independent and computes the sum of achieving each subgoal in the delete-relaxed
model (Bonet & Geffner, 2001). The FF heuristic (Hoffmann & Nebel, 2001) constructs a planning
graph (Blum & Furst, 1997) for the delete-relaxed problem, extracts a relaxed plan, and computes
the number of actions in the relaxed plan, which is an upper bound on h+ .
In the case of cost-optimal planning, where each action is assigned a cost and the objective is to
find a minimal cost plan, lower bounds on h+ are the basis for several admissible heuristic functions
that have been used in the literature. Bonet and Geffner (2001) proposed the hmax heuristic, which
computes the highest cost associated with achieving the most costly, single proposition. While
hmax is admissible, it is often not very informative (i.e, the gap between hmax and h+ is large)
because it only considers the single most costly goal proposition. The admissible landmark cut
(LM-cut) heuristic (Helmert & Domshlak, 2009), approximates h+ as follows. For state s, the LMcut heuristic first computes hmax (s), and if this is zero or infinite, then h+ is zero or infinite, so
hLM cut (s) = hmax (s). Otherwise, a disjunctive action landmark L (a set of actions at least one of
which must be included in any relaxed plan) is computed, and the cost of all actions in L is reduced
by c(m), the cost of the minimal-cost action m  L, and hLM cut is increased by c(m). This process
is repeated until hmax (s) (for the remaining, reduced problem) becomes 0. Other approximations to
h+ that are more informative than hmax include the set-additive heuristic (Keyder & Geffner, 2008)
and cost-sharing approximations to hmax (Mirkis & Domshlak, 2007).
Previous planners have avoided direct computation of h+ because the extra search efficiency
gained from using h+ is offset by the high cost of computing h+ . As far as we are aware, the first
actual use of h+ inside a cost-optimal planner was by Betz and Helmert (2009), who implemented
domain-specific implementations of h+ for several domains. More recently, Haslum et al. evaluated
the use of a domain-independent algorithm for h+ (Haslum, Slaney, & Thiebaux, 2012) as the
heuristic function for A* -based cost-optimal planning, but found that the performance was relatively
poor (Haslum, 2012).
In recent years, there have been several advances in the computation of h+ . Since, as described
above, the LM-cut heuristic (Helmert & Domshlak, 2009) is a lower bound on h+ , a cost-optimal
planner using the A* search algorithm and the LM-cut heuristic can be directly applied to the delete
relaxation of a classical planning problem in order to compute h+ . It is possible to improve upon
this by developing methods that exploit the delete-free property and are specifically tailored for
solving the delete relaxation. Pommerening and Helmert (2012) developed an approach which uses
IDA* or branch-and-bound with an incrementally computed LM-cut heuristic. Gefen and Brafman
(2012) proposed action pruning for delete-free problems.
A different approach to computing h+ is based on the observation that h+ could be formulated
as the problem of finding a minimal hitting set for sets of disjunctive action landmarks (Bonet &
Helmert, 2010). This led to methods for computing h+ by searching for minimum-cost hitting set
for a complete set of action landmarks for the delete-relaxed planning problem (Bonet & Castillo,
2011; Haslum et al., 2012). While the original implementation of Haslum et al.s hitting-set based
h+ solver used a problem-specific branch-and-bound algorithm (Haslum et al., 2012), an improved
implementation (which we use in our experimental evaluation in Section 5) uses integer programming to solve the hitting set problem (Haslum, 2014a).
635

fiI MAI & F UKUNAGA

2.3 Integer/Linear Programming for Classical Planning
Another related line of research is the modeling of classical planning as integer/linear programs
(ILP). The use of very high-performance, general problem solvers to solve planning problems was
pioneered by Kautz and Selman, who solved planning problems by encoding them as propositional
satisfiability (SAT) and applied state-of-the-art SAT solvers. The basic approach is to instantiate a
SAT formula for which a satisfying assignment implies a t-step plan. SATPLAN starts with a small
value of t (e.g., trivially, 1, or some other lower bound), instantiates a propositional formula F (t)
which is satisfiable if and only if a plan of t parallel steps or less exists. If F (t) is satisfiable, then
a minimal parallel makespan plan has been found. Otherwise, t is incremented, and this process
is repeated until a plan is found. While the initial encodings were modestly successful (Kautz &
Selman, 1992), advances in both SAT solver technology as well as improvements to the encoding
and the integration of planning graphs (Blum & Furst, 1997) led to dramatic performance improvements (Kautz & Selman, 1996, 1999). Recent work on SAT-based planning includes improved
encodings as well as execution strategies for SAT strategies that improve upon simply incrementing
t as above (Rintanen, Heljanko, & Niemela, 2006). In addition, improvements to SAT solvers which
specifically target domain-independent planning have been investigated (Rintanen, 2012)
Since the expressiveness of integer programming (IP) subsumes SAT, SAT encodings can be
straightforwardly translated to IP. However, direct translation of SAT encodings to IP resulted in
poor performance, and a state-change formulation which replaces the original fluents in the SAT
encoding with a set of variables that directly expresses the addition, deletion, and persistence of
fluents was shown to be more successful as the basis for an IP model for planning (Vossen, Ball,
Lotem, & Nau, 1999). This formulation was strengthened with additional mutual exclusion constraints (Dimopoulos, 2001). The Optiplan model (van den Briel & Kambhampati, 2005) combined
the state-change IP formulation with the planning-graph based model refinement strategies and improvements by Dimopoulous (2001). As with the SAT-based approaches described above, IP models
which are feasible if and only if a plan of up to t steps exists are constructed. However, unlike the
SAT formulation, it is easy to directly encode action costs into the objective function for the IP
model, so the IP models can be used to directly solve the cost-optimal planning problems. Another
approach decomposes a planning instance into a set of network flow problems, where each subproblem corresponds to a state variable in the original planning problem (van den Briel, Vossen, &
Kambhampati, 2008).
Instead of modeling and directly solving a classical planning problem as an IP, another approach, which we adopt in this paper, uses ILP models to provide a heuristic function which guides
a state-space search planning algorithms such as A* . An early instance of this approach (which,
to our knowledge, is also the earliest application of LP to classical planning) is LPlan, where an
LP encoding of the classical planning problem is used as a heuristic function for a partial order
planner (Bylander, 1997). Van den Briel et al. (2007) developed an admissible heuristic based on
an LP model which represents a planning problem where the order in which actions are executed
is relaxed, and each variable represents the number of times an action is executed. Delete effects
are considered, in that there are constraints such that the number of actions that delete values can
be incremented only if there are actions that add the value. Although this LP-based heuristic was
not integrated into a planning system, they compared the relaxed problem cost found by their model
with Bylanders LPlan LP model, as well as an LP model for h+ .
636

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

To our knowledge, the h+ implementation by van den Briel et al. (2007) is the first implementation of an IP model of h+ . First, a relaxed planning graph (Blum & Furst, 1997) was expanded
until quiescence, which results in the instantiation of all actions that are relevant for the optimal
delete-free task as well as an upper bound on the number of steps in the optimal delete-free task.
Then, h+ was computed using a delete-relaxed, step-based encoding of the planning problem of
Optiplan (van den Briel, 2015).
Cooper et al. (2011) showed that the optimal solution to the dual of an LP model which relaxes the action ordering corresponds to the best lower bound that can be obtained by applying
transformations to the original planning problem that shift costs among actions that affect the same
fluents.
Bonet proposed hSEQ , an admissible, flow-based LP heuristic based on Petri Net state equations (Bonet, 2013) which was used as the heuristic for an A* -based planner. Bonet and van den
Briel (2014) enhanced Bonets flow-based LP model by adding action landmark constraint and implementing variable merging strategies, resulting in a competitive, admissible heuristic. Karpas and
Domshlak (2009) proposed an LP formulation to compute optimal partitioning for landmarks. Pommerening et al. (2014) proposed an operator counting framework which enabled the unification of a
number of ideas, including the state equation formulation (Bonet, 2013), post-hoc optimization constraints (Pommerening, Roger, & Helmert, 2013), as well as landmarks (the formulation by Bonet
& Helmert, 2010, which is the dual of the formulation in Karpas & Domshlak, 2009) and state
abstraction heuristics (Katz & Domshlak, 2010). They showed that combinations of constraints resulted in strong heuristics which significantly outperformed the LM-cut heuristic. A recent survey
by Roger and Pommerening (2015) presents a survey of LP-based heuristics for planning which
includes an earlier conference version of this paper (Imai & Fukunaga, 2014) and suggests how our
delete-relaxation model could be incorporated into the operator counting framework by associating
a operator-counting variable for each action variable (see below) in the delete-relaxed problem.

3. IP(T + ): The Basic IP Formulation of a Delete-Free Task
We now define the integer program IP(T + ), which is the IP formulation of the delete free task
T + = hP, A+ , I, Gi. Note that for any feasible solution to IP(T + ) (not just the optimal solution),
we can derive a corresponding, feasible and non-redundant (i.e., each action appears only once)
plan for T + that has the same cost as the IP(T + ) solution.
First, we define the variables of IP(T + ). In addition to being able to derive a plan from IP(T + ),
there always exists an injective mapping from a feasible non-redundant plan for an IP(T + ) solution.
Thus, we also show the feasible assignments of variables that can be derived from a feasible plan
for T + , as well as the meanings and roles of the variables. We use  = (a0 ,    , an ) to denote a
plan for T + corresponding to a solution for IP(T + ). We say that a is the first achiever of p in plan
 if p 6 I, and a is the first action that achieves (establishes) p.
proposition: p  P, U (p)  {0, 1}. U (p) = 1 iff p  I(). U (p) indicates whether proposition p
is achieved in a relaxed plan for T + .
action: a  A, U (a)  {0, 1}. U (a) = 1 iff a   holds. U (a) indicates whether the action a is
used in a relaxed plan.
add effect: a  A, p  add(a), E(a, p)  {0, 1}. E(a, p) = 1 iff a   holds and a is the first
achiever of p. E(a, p) = 0 if p is true in I, or p is not achieved.
637

fiI MAI & F UKUNAGA

time (proposition): p  P, T (p)  {0,    , |A|}. T (p) = t when p  I() and p is added by
at1 first. T (p) = 0 if p is a member of I. T (p) indicates the time step where p is first
achieved by its first achiever.
time (action): a  A, T (a)  {0,    , |A|}. T (a) = t when a = at . T (a) = |A| when a 6 .
T (a) indicates the time step where a is first used.
initial proposition: p  P, I(p)  {0, 1}. I(p) = 1 iff p  I.
If p  P is achieved more than once, i.e., p appears in the add effects of multiple actions in ,
we assign T (p) the index of the first such action in . If p is not achieved, i.e., p 6 I() holds, we
can assign an arbitrary value in {0,    , |A|} to T (p). Given a delete-free task T + and its feasible
and non-redundant plan , we call the above assignment a solution derived from .
We use
Pthe following fact in later proofs: a solution derived from a feasible solution satisfies (a) a A s.t.padd(a ) E(a , p)  1 for any proposition p such that U (p) = 1, and (b)
P

a A s.t.padd(a ) E(a , p) = 0 for any proposition p such that U (p) = 0.
Variables I(p) are auxiliary variables for computing h+ . Although they are redundant when
solving a delete-free task only one time, they are useful to avoid reconstructing constraints for each
state when IP(T + ) or LP(T + ) are embedded as a heuristic function in a forward-search planner
and called for each state.
The objective function is defined as follows:
X
minimize:
c(a)U (a).
(1)
aA

Because of this objective function, the cost of a solution to IP(T + ) is equal to the cost of the
corresponding (delete-free) plan.
Finally we define the following six constraints.
(C1) p  G, U (p) = 1. (The goals must be achieved).
(C2) a  A, p  pre(a), U (p)  U (a). (Actions require their preconditions).
(C3) a  A, p  add(a), U (a)  E(a, p). (An action can be the first achiever only if it is used).
P
(C4) p  P, I(p) + a A s.t.padd(a ) E(a , p) = U (p). (If a proposition is achieved, it must be
true in the initial state or is the effect of some action).
(C5) a  A, p  pre(a), T (p)  T (a). (Actions must be preceded by the satisfaction of their
preconditions).
(C6) a  A, p  add(a), T (a) + 1  T (p) + (|A| + 1)(1  E(a, p)). (If a is the first achiever
of p, then a must precede p).
Now we can show that a solution of IP(T + ) derived from a feasible non-redundant plan of
is feasible. For a variable V of IP(T + ), VF describes the assignment of V on a solution F of
IP(T + ).
T+

Proposition 1. Given a delete-free task T + and a feasible, non-redundant plan  for T + , the
solution F to IP(T + ) derived from  is a feasible solution to IP(T + ).
638

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Proof. F clearly satisfies constraint C1 since  satisfies G  I().
Constraint C2 is not satisfied only if there exists an action a  A and a proposition p  pre(a)
such that U (a)F = 1 and U (p)F = 0. However, if U (a)F = 1, then U (p)F = 1 because  is a
delete-free feasible plan and p has to be established at some point. We can show that F satisfies
constraints C3 and C4 by similar arguments. If there exists an action a  A and a proposition
p  add(a) such that E(a, p)F = 1, U (a)F = 1 must hold according to the definition of F . In
addition, if there exists a proposition p such that U (p)F = 1, there exists a first achiever a  A of p
such that E(a, p)F = 1, or p is a member of the initial state I.
If an action a  A is a member of , then all propositions in its precondition must be achieved
before a is used. Hence, according to the definition of F , we have T (p)F  T (a)F for any action a
in the plan . If an action a  A is not a member of , then we have T (a)F = |A|. Thus, constraint
C5 is satisfied for any action a not in the plan , regardless of the values of T (p)F .
Finally F satisfies constraint C6 for any action a  A and for any proposition in its precondition
p  pre(a). If a is not the first achiever of p, i.e., E(a, p) = 0, then constraint C6 is satisfied
regardless of the values of T (p)F and T (a)F . If a is the first achiever of p, then, according to the
definition of F , we have T (p)F = T (a)F + 1 , which satisfies constraint C6.
In addition, there exists a feasible plan only if IP(T + ) has a feasible solution. When IP(T + ) is
solved optimally, an optimal plan for T + is obtained according to the following proposition.
Proposition 2. Given a feasible solution F for IP(T + ), the action sequence  = (a0 ,    , an )
obtained by ordering actions in the set {a | U (a)F = 1} in ascending order of T (a)F is a feasible
plan for T + .
Proof. First we show that  satisfies the condition (ii) of a plan (i.e., G  I()) using a proof by
contradiction. Assume that there exists a proposition g  G that satisfies g 6 I(). Then, there
exists no action achieving g in . Since F is a solution to IP(T + ), U (g)F = 1 due to constraint
C1. Since g 6 I() implies g 6 I, I(g)F = 0. Therefore, to satisfy constraint C4, there must
exist an action a  A such that g  add(a) and E(a, g)F = 1. However, to satisfy constraint C3,
U (a)F = 1 has to hold. This means a  , which contradicts the assumption.
Next we show that  satisfies condition (i) (i.e., i, pre(ai )  I((a0 ,    , ai1 ))). For the base
case of an inductive proof, assume that there exists a proposition p  P satisfying p  pre(a0 )
and p 6 I. Since a0  , U (a0 )F = 1 has to hold, and U (p)F = 1 has to hold according to the
constraint U (p)F  U (a0 )F . Then, similar to the proof of condition (ii), there must exist an action
a  A such that p  add(a), U (a)F = 1, and E(a, p)F = 1. However, to satisfy constraint C5,
T (p)  T (a0 ) must be true, and T (a) + 1  T (p) has to hold to satisfy constraint C6. Therefore
we have U (a)F = 1 and T (a) < T (a0 ), but a0 is the first action of , a contradiction.
Similar to the case of i = 0, when i > 0, if pre(ai )  I((a0 ,    , ai1 )) is not true, there must
exist an action a 6 (a0 ,    , ai1 ) such that U (a)F = 1 and T (a) < T (ai ), contradicting the fact
that ai is the i-th action of the sequence .
Corollary 1. Given an optimal solution F of IP(T + ), a sequence of actions built by ordering
actions in the set {a | U (a)F = 1} by ascending order of T (a)F is an optimal plan for T + .
P
+ ) is 3|P | + 2|A| +
The number of variables
in
IP(T
|add(a)|. The number of constraints
P
P
is
Pless than 2|P
P| + 2 aA |pre(a)| + 2 aA |add(a)|. The number of terms is also O(|P | +
|pre(a)| + |add(a)|).
639

fiI MAI & F UKUNAGA

4. Enhanced IP Model
While IP(T + ) provides an IP model for exactly computing h+ , we shall see in Section 5 that
IP(T + ) by itself is not competitive with previous methods for computing h+ . Thus, in this section,
we introduce some variable elimination techniques and some modifications to constraints in order
to speed up the computation h+ . As we will show in the experimental results, IPe (T + ), which
incorporates these enhancements, computes h+ significantly faster than IP(T + ). Some of the enhancements below are adopted into our IP framework from previous work in planning research. In
particular, a landmark-based variable reduction method plays a key role.
Note that some of the enhancements introduce constraints that render some solutions of IP(T + )
mapped from feasible plans for T + infeasible. However, we show that in such cases, at least one
optimal plan will always remain valid in the enhanced model, so the optimal cost of the enhanced
model still corresponds to h+ .
4.1 Landmark-Based IP Model Reduction
A landmark is an element which needs to be used in every feasible solution (Hoffmann, Porteous,
& Sebastia, 2004). We use two kinds of landmarks, called fact landmarks and action landmarks as
in the work of Gefen and Brafman (2012). A fact landmark of a planning task T is a proposition
that becomes true in some state of every feasible plan, and an action landmark for a planning task
T is an action that is included in every feasible plan. We also say that a fact or action landmark
l is a landmark for a proposition p if l is a landmark for the task hP, A, I, {p}i. Similarly we say
that a landmark l is a landmark for an action a if l is a landmark for the task hP, A, I, pre(a)i. In
the IP model of a delete-free task T + , if a proposition p is a fact landmark for a proposition in the
goal G, then we can substitute U (p) = 1. Similarly, if an action a is an action landmark, then we
can substitute U (a) = 1. Landmark extraction and substitution clearly do not prune any feasible
solutions of IP(T + ).
To actually extract the set of landmarks that satisfy the above intensional definitions, a landmark
extraction algorithm is necessary. It is easy to see that given
a feasible delete-free task,
ff a proposition
add
p  P is a fact landmark if and only if p  I holds or P, A \ Ap , I \ {p}, G is infeasible,
= {a | p  add(a)}. Similarly an action a  A is an action landmark if and only
where Aadd
p
if hP, A \ {a}, I, Gi is infeasible. Hence, for each landmark candidate, we can test whether it is
a landmark by checking the feasibility of the delete-free task which excludes that candidate. The
feasibility of a delete-free task can be checked using the following, straightforward algorithm based
on the delete-relaxed planning graph method by Hoffmann and Nebel (2001): For each fluent, let
e(p)  {0, 1} represent whether p is achievable or not. For each action, let e(a)  {0, 1} represent
whether the preconditions of a are satisfied or not. Initially, e(p) = 1 for all p  I , e(p) = 0
for all other fluents. and e(a) = 0 for all a. At each step of the algorithm, for all actions for
which e(a) = 0 and whose preconditions are satisfied; (1) set e(a) = 1, and (2) set e(p) = 1 for
all e  add(a). The algorithm terminates when it reaches quiescence, i.e., no actions for which
e(a) = 0 and whose preconditions are satisfied can be found. This takes at most |A| steps. By
repeating this feasibility check for all facts and actions, we have an algorithm that collects all fact
landmarks and action landmarks satisfying the definitions above in O(|T + |2 )-time.
If we were only interested in computing h+ once, a straightforward method such as the one
described above would be sufficient. However, since we intend to use our h+ -based models as
heuristic functions for forward state-space search planning, the landmark extraction needs to be
640

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

performed repeatedly during the search, so the efficiency of the extraction procedure is important.
We experimented with several methods, and describe our most effective method below.
Our method for extracting landmarks is based on the method by Zhu and Givan (2003), who
proposed a planning based propagation method for collecting causal landmarks. Their method was
later generalized by Keyder et al. to an AND-OR graph based landmark extraction method (Keyder,
Richter, & Helmert, 2010).
Zhu and Givan (2003) define a proposition p as a causal landmark if hP, A \ Apre
p , I \ {p}, Gi
pre
is infeasible, where Ap = {a | p  pre(a)}. They focus on causal landmarks, ignoring other
(non-causal) landmarks because they are nonessential (even misleading) from the point of view of
guiding a search algorithm that uses a landmark-based heuristic. In contrast, we use landmarks in
order to reduce the number of variables in our IP model of the delete relaxation. Thus, instead of
focusing on causal landmarks and using Zhu and Givans criteria, we seek a larger set of landmarks
by slightly modifying 
the criterion for landmark
detection. If hP, A \ Apre
p , I \ {p}, Gi does not
ff
have a solution, then P, A \ Aadd
,
I
\
{p},
G
must
also
be
infeasible,
and furthermore, using
p
pre
add
Ap instead of Ap can extract a larger set of fact landmarks. In addition, while Zhu and Givan
used a forward propagation algorithm based on the layered planning graph of the delete-free task
T + , we use the following, open-list based propagation algorithm.
For each proposition p, we compute a set of fact landmarks for p, using an iterative method
based on the following update equations characterizing fact landmarks:
 If p is a member of the initial state I, then {p} is the set of fact landmarks to achieve p.
T
 If p is not a member of I, then the set of fact landmarks for p is {p} aA s.t.padd(a) (add(a)
S

p pre(a) (fact landmarks for p )).
The pseudocode for this open list based propagation algorithm is shown in Algorithm 1. In the
initialization phase, the candidate set for each proposition p 6 I is set to P , and the fact landmarks
for each p  I is set to {p} (Lines 1-3). In addition, an action a is inserted into a FIFO queue Q if it
satisfies pre(a)  I (Lines 4-7). The main loop of the iterative method is similar to the straightforward method described above. At each iteration, an action a is retrieved from Q, and the candidate
set of fact landmarks is updated for each p  add(a) based on the second equation (Lines 12-14).
Moreover, the method memorizes the achievability of p (Line 11), and action a is inserted into Q
if all members of pre(a ) are achievable and if the candidate set of p  pre(a ) is changed (Lines
15-17). This process continues until Q becomes empty. For clarity and simplicity, some implementation details/optimizations are omitted from Algorithm 1, e.g., instead of literally inserting every
member of P into L[p] in Line 3, we use a single flag to represent L[p] = P  Updating a candidate set always reduces the number of its elements, so this method always terminates. Unlike the
simpler O|T + |2 algorithm described above, this algorithm is not complete (not all landmarks will
be extracted). However, the soundness of this method is guaranteed by the following proposition.
Proposition 3. Given a delete-free STRIPS planning task hP, A+ , I, Gi, assume all propositions
in P can be achieved. Let L(p) be the set of fact landmark candidates for p computed by some
landmark extracting method. If
(i) L(p) = {p} for p  I, and
S
T
(ii) L(p) = {p}  aA s.t.padd(a) (add(a)  p pre(a) L(p )) for p 6 I
641

fiI MAI & F UKUNAGA

Algorithm 1 Our landmark extracting method
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

// L[p] are sets of candidates of fact landmarks for p  P .
L[p]  P for each p 6 I;
L[p]  {p} for each p  I;
S  I;
for a  A do
insert a into a FIFO queue Q if pre(a)  S;
end for
while Q is not empty do
retrieve an action a from Q.
for p  add(a) do
S  S  {p}.
S
X  L[p]  (add(a)  p pre(a) L[p ]);
if L[p] 6= X then
L[p]  X.
for a  Apre
p do

insert a into Q if pre(a )  S and a 6 Q;
end for
end if
end for
end while
// At this point, L[p] contain sets of fact landmarks for p  P .

are satisfied, then all elements of L(p) are fact landmarks for p.
Proof. Assume that some proposition q satisfies q  L(p) and q is not a fact landmark for p. We
have p 6= q since any proposition is a fact landmark for itself. Then, L(p) has more than one
proposition, and from condition (i) and (ii), p 6 I holds. Since q is not a landmark, there exists a
non-empty feasible plan for the delete-free task hP, A+ , I, {p}i that does not achieve q.
Let  = (a0 ,    , an ) be such a plan, and let ai be the action in  that achieves p first. We have
p 6= q as stated above, andS
we have q 6 add(ai ) since  does not achieve q. Hence, according to
condition (ii), we have q  p pre(ai ) L(p ). Let p be a member of pre(ai ) that satisfies q  L(p ).
Since  is a feasible plan that does not achieve q, p is achieved by , and thus p 6= q holds. Then,
L(p ) has more than one proposition, and again, p 6 I holds. Hence,   = (a0 ,    , ai1 ) is a
non-empty feasible plan for a delete-free task hP, A+ , I, {p }i that does not achieve q.
This argument can be extended ad infinitum, but the length of  is clearly finite, so we have a
contradiction. Thus, all members of L(p) are fact landmarks for p for each proposition p  P .

In addition to the fact landmarks which are extracted using the above procedure, our algorithm
extracts action landmarks using the criterion: if a proposition p is a fact landmark of G, and if only
one action a can achieve p, then a is used as an action landmark of G.
642

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

4.2 Relevance Analysis
Backchaining relevance analysis is widely used to eliminate propositions and actions that are irrelevant to a task. An action a is relevant if (i) add(a)  G 6= , or (ii) there exists a relevant action
a satisfying add(a)  pre(a ) 6= . A proposition p is relevant if (i) p  G, or (ii) there exists a
relevant action a and p  pre(a) holds.
In addition, as noted by Haslum et al. (2012), it is sufficient to consider relevance with respect to only a subset of first achievers of an add effect. Although they defined a first achiever
by achievability of a proposition, it is equivalent to the following definition: an action a is a first
achiever of a proposition p if p  add(a) and p is not a fact landmark for a. Let fadd(a) denote
{p  add(a) | a is a first achiever of p}. It is sufficient to use fadd instead of add in the above
definition of relevance.
If a  A or p  P is not relevant, we can eliminate a variable as U (a) = 0 or U (p) = 0.
In addition to this, if p  add(a) but a is not a first achiever of p, we can eliminate a variable as
E(a, p) = 0. It is possible for a fact landmark fact to be irrelevant, in which case we set U (p) = 1.
While this variable elimination prunes some feasible solutions, it clearly does not prune any optimal
solutions.
4.3 Dominated Action Elimination
In a delete-free task, if two actions have the same add effects, then it is clearly sufficient to use
at most one of these two actions. This idea can be generalized to the following reduction, which
eliminates useless (dominated) actions.
Proposition 4. Given a feasible delete-free task T + , there exists an optimal plan that does not
contain a  A if there exists an action a  A satisfying the following: (i) fadd(a)  fadd(a ), (ii)
for all p  pre(a ), p is a fact landmark for a or p  I, and (iii) c(a)  c(a ).
Proof. For any plan  = (a0 ,    , ai1 , a, ai+1 ,    , an ) of T + , we show that a sequence of actions
  = (a0 ,    , ai1 , a , ai+1 ,    , an ) is also a feasible plan. Each proposition of pre(a ) is a fact
landmark for a, hence, if pre(a)  I((a0 ,    , ai1 )), then pre(a )  I((a0 ,    , ai1 )) also
holds. By the definition of first achievers, add(a) \ fadd(a)  I((a0 ,    , ai1 )), so we also have
I((a0 ,    , ai1 , a))  I((a0 ,    , ai1 , a ). Therefore G  I(  ) (  is a feasible plan).
Finally, c()  c(  ) because c(a)  c(a ). Therefore, if a plan contains a, it is not optimal, or
there exists another optimal plan which does not contain a.
If there exists a dominated action a, we can eliminate a variable by setting U (a) = 0. This
variable elimination prunes some feasible solutions of IP(T + ). Moreover, it sometimes prunes
some optimal solutions if c(a) = c(a ) holds for the condition (iii). However, as shown in the proof
above, at least one optimal solution remains.
This is a slight generalization of a similar set of constraints by Robinson (2012)[Definition
5.3.4, p. 108] for a MaxSAT-based planner. Robinsons dominance condition checks whether (R1)
add(a) \ I  add(a ) \ I, (R2) pre(a ) \ I  pre(a) \ I, and (R3) c(a)  c(a ). While our
condition (iii) and (R3) are equivalent, our condition (i) is less strict than condition (R1) because
instead of checking all add effects, condition (i) only tests whether the propositions for which a is a
first achiever is subsumed by those of a . Furthermore, our condition (ii) subsumes (R2) because if
each proposition of pre(a ) is a fact landmark for a, then if pre(a)  I((a0 ,    , ai1 )), pre(a ) 
I((a0 ,    , ai1 )) also holds, satisfying (R2).
643

fiI MAI & F UKUNAGA

4.4 Immediate Action Application
On a delete-free task T + , some actions can be immediately applied to the initial state without
affecting the optimality of the relaxed plan. We adopt immediate application of zero-cost actions
(Gefen & Brafman, 2011) as well as immediate application of action landmarks (Gefen & Brafman,
2012). For a delete-free task T + , if an action a  A satisfies c(a) = 0 and pre(a)  I, then a
sequence made by placing a before an optimal plan for hP, A \ {a}, I  add(a), Gi is an optimal
plan for T + . Similarly, if an action a is an action landmark for T + and a is applicable to I, a can
be applied to I immediately.
In the IP(T + ) model, variables T (p) for p  I can be eliminated by substituting zero for
their values. Given a sequence of immediately applicable actions (a0 ,    , ak ) (it must be a correct
applicable sequence), we can eliminate some variables as follows: (i) U (ai ) = 1, (ii) T (ai ) = i,
(iii) p  pre(ai ), U (p) = 1, (iv) p  add(ai ) \ I((a0 ,    , ai1 )), U (p) = 1, T (p) = i and
E(ai , p) = 1, and (v) p  add(ai ) \ I((a0 ,    , ai1 )), a  A \ {a0 ,    , ai }, E(a, p) = 0.
4.5 Iterative Application of Variable Eliminations
The variable elimination techniques described above can interact synergistically with each other
resulting in a cascade of eliminations. Therefore, we used an iterative variable elimination algorithm
which applies eliminations until quiescence. The order in which each elimination is applied is shown
in Algorithm 2. A full landmark extraction pass after each variable elimination would be extremely
expensive. Therefore, we perform a landmark extraction only once before the iterative application
of the other eliminations.
Algorithm 2 Iterative Variable Elimination
relevance analysis;
landmark extraction;
While a variable can be eliminated do
immediate action application;
dominated actions elimination;
relevance analysis;

4.6 Inverse Action Constraints
We define the following inverse relationship between a pair of actions for a delete-free task T + .
Definition 1 (inverse action). For two actions a1 , a2  A, a1 is an inverse action of a2 if: (i)
add(a1 )  pre(a2 ), and (ii) add(a2 )  pre(a1 ).
By definition, it is clear that if a1 is an inverse action of a2 , then a2 is an inverse action of a1 .
Inverse actions satisfy the following fact.
Proposition 5. Given a delete-free task T + , let  = (a0 ,    , an ) be a feasible plan. If ai   is
an inverse action of aj  , and if i < j holds, then   = (a0 ,    , aj1 , aj+1 ,    , an ) is also a
feasible plan.
Proof. Since  is a feasible plan for T + , pre(ai )  I((a0 ,    , ai1 ))  I((a0 ,    , aj1 )). By the
definition of inverse actions, add(aj )  pre(ai ) holds, and add(aj )  pre(ai )  I((a0 ,    , aj1 )) =
644

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

I((a0 ,    , aj )). Hence (aj+1 ,    , an ) is applicable to I((a0 ,    , aj1 )), and G  I(  ) =
I().
Corollary 2. For a delete-free task T + , a feasible solution  = (a0 ,    , an ) is not optimal if
ai   is an inverse action of aj   and both of ai and aj have non-zero cost.
There are several possible ways to use the above proposition (e.g., U (a) + U (a )  1, for all
 inv(a), where inv(a) is the set of inverse actions of a). In order to avoid adding a large number
of constraints to the IP(T + ) model (|A/2|2 in the worst case where half of the actions are inverses
of the other), we modify constraint C2 as follows:
P
(C2a) a  A, p  pre(a), U (p) a inv(a,p) E(a , p)  U (a), where inv(a, p) denotes the set
of inverse actions of a which have p as an add effect.
a

Proposition 6. Given a delete-free task T + , if IP(T + ) with constraint C2 has a feasible solution,
then an optimal solution to IP(T + ) with constraint C2 is also feasible for IP(T + ) with constraint
C2a.
Proof. Let F  be an optimal solution to IP(T + ) with constraint C2 derived from an optimal plan
for T + . Since F  satisfies all the constraints of IP(T + ) with constraint C2, it suffices to show that
F  satisfies constraint C2a for any action a  A and proposition p  pre(a).
P
Recall that a feasible solution derived from a feasible plan satisfies a A s.t.padd(a ) E(a , p) 
P
1 for any proposition p such that U (p) = 1, and it also satisfies a A s.t.padd(a ) E(a , p) = 0 for
P
P
any proposition p such that U (p) = 0. Since a A s.t.padd(a ) E(a , p)  a inv(a,p) E(a , p)
for any action a  A and proposition p  pre(a), F  clearly satisfies constraint C2a if U (p)F  = 1


and U (a)F  = 0, or
Pif U (p)F = 0 and U (a)F = 0 hold.

To show that a inv(a,p) E(a , p)F = 0 holds when U (a)F  = U (p)F  = 1, assume there
exists an action a  inv(a, p) such that E(a , p)F  = 1. According to constraint C3, U (a )F  = 1.
However, since F  is derived from an optimal plan for T + , there must exist an optimal plan for T +
that contains both a and a . This contradicts Corollary 2.
Since F  is a feasible solution, there does not exist any action a  A and proposition p  pre(a)
such that U (a)F  = 1 and U (p)F  = 0. Hence F  satisfies constraint C2a for any a  A and
p  pre(a).
4.7 IPe (T + ): The Enhanced IP Model for h+
We define IPe (T + ) as the integer programming model that is the result of first adding the inverse
action constraints described in Section 4.6 to the basic IP(T + ) model and then applying the iterative reduction algorithm in Algorithm 2 (which applies the reductions in Sections 4.1-4.4) until
quiescence. As previously noted, IPe (T + ) computes h+ . As we shall see below, the cumulative
effects of these enhancements is quite significant, resulting in a much more practical IP model for
computing h+ . See Table 1 for a summary of the relationship between IPe (T + ) and IP(T + ).

5. Experimental Evaluation of IP Models for Delete-Free Planning (Exact
Computation of h+ )
In this section, we evaluate the effectiveness of our integer programming model of the delete relaxation as a method for solving delete-free tasks and computing h+ exactly. We evaluate the following
models:
645

fiI MAI & F UKUNAGA

 IP(T + ): our basic IP model (Section 3).
 IP(T + )+LM: IP(T + ) with the landmark-based variable reduction method (Section 4.1).
 IPe (T + ): the enhanced model which includes all of the enhancements described in Sections
4.1-4.6 which are designed to speed up the computation of h+ (landmark-based reduction,
relevance analysis, dominated action elimination, immediate action application, inverse action constraints).
We emphasize that (unlike the other models which will be evaluated in later sections) all of
these IP models compute h+ exactly.
Following previous work on solvers for delete-free problems, our main results are based on
an evaluation using delete-free versions of standard IPC benchmark problems (Section 5.1). In
addition, in Section 5.2, we also present results of a much smaller scale study on a set of natural,
delete-free problems from systems biology (Gefen & Brafman, 2011).
5.1 Evaluation on Delete-Free Versions of IPC Benchmark Instances
Following the methodology for evaluating delete-free planning in previous work (Haslum et al.,
2012; Pommerening & Helmert, 2012; Gefen & Brafman, 2012), we evaluate our IP models by
solving International Planning Contest (IPC) benchmark instances for which the delete effects of
all actions are ignored. Below, all experiments used the CPLEX 12.61 solver to solve integer and
linear programs. All experiments were single-threaded and executed on a Xeon E5-2680, 2.8GHz.
Because previous work on computing h+ has been evaluated using several different sets of
experimental settings (different CPU limits and different problem instances), we present the results
of 4 sets of comparisons. In the first 3 sets of comparisons, we compare benchmark results reported
in previous publications with results obtained by running our solvers on the same problem instances,
while the fourth set of results compares our models with an improved implementation of the minimal
hitting set based approach (Haslum et al., 2012) by one of the the original authors.
 Comparison with the results by Pommerening and Helmert (2012) (experimental setup described in Section 5.1.1, results shown in Table 2).
 Comparison with the results by Gefen and Brafman (2012) (experimental setup described in
Section 5.1.2, results shown in Table 3).
 Comparison with the results by Haslum et al. (2012) (experimental setup described in Section
5.1.2, results shown in Table 4).
 Comparison with HST/CPLEX, an improved implementation of the algorithm in (Haslum
et al., 2012) (experimental setup described in Section 5.1.3, results shown in Table 5 and
Figures 2-3).
The results copied from previous work (Pommerening & Helmert, 2012; Haslum et al., 2012;
Gefen & Brafman, 2012) in Tables 2-4 were obtained using hardware available several years ago
when these original papers were written, while our results for IP(T + ), IPe (T + ), and HST/CPLEX
were obtained with slightly more recent hardware. Since coverage is a coarse metric based on binary results (solved/unsolved), it can be significantly impacted by differences in machine speed,
646

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

e.g., if many problems are at the threshold such that a slightly faster machine (equivalent to running
slightly longer) results in many more instances being solved. In order to eliminate the possibility
that improvements in hardware since 2010 (when the first of the results we compared against were
published) explain the improvements obtained using our approach, we also include results of running our best IP model (IPe (T + )) with a significantly shorter CPU time limit than the previous
experiments, in addition to results that use the same CPU time limit as previous experiments.
5.1.1 C OMPARISON WITH R ESULTS BY P OMMERENING
D ELETE -F REE V ERSIONS OF IPC B ENCHMARKS

AND

H ELMERT (2012)

ON

The first comparison is with the results by Pommerening and Helmert (2012). Table 2 shows the
results of running IP(T + ), IP(T + )+LM, and IPe (T + ) with a 5 minute time limit and 2GB memory
limitation. Coverage (# of problem instances solved) on each domain is shown. The columns where
the solver name contains PH12 in Table 2 are copied from the paper by Pommerening and Helmert
(2012). FD/PH12 is Fast Downward using A* and the LM-cut heuristic applied to the deleterelaxed problems, BC/PH12 is the hitting set based approach by Bonet and Castillo (2011), and
BnB/PH12 and IDA*/PH12 are the best performing strategies using the incremental LM-cut
heuristic for delete-free problems proposed by Pommerening and Helmert (2012). Pommerening
and Helmert obtained their results using a AMD Opteron 2356 processor with a 2GB memory limit
and 5 minute time limit.
Table 2 includes a column IPe (T + )/1min, which shows the results for 1-minute runs of
IPe (T + ). All other columns in Table 4 are for 5 minute runs.
5.1.2 C OMPARISONS WITH R ESULTS BY G EFEN AND B RAFMAN (2012) AND H ASLUM ET AL .
(2012) ON D ELETE -F REE V ERSIONS OF IPC B ENCHMARKS
Next, we evaluated our h+ solvers with previous results that were obtained with a 30-minute time
limit and 2GB memory limit. Table 3 compares IP(T + ), IP(T + )+LM, and IPe (T + ) with some
results from (Gefen & Brafman, 2012, p. 62, Table 2). The LM-cut/GB12 column is A* with the
LM-cut heuristic (Helmert & Domshlak, 2009) applied directly to delete-free instances in order to
compute h+ . The LM-cut+Pruning/GB12 column is A* with LM-cut using the pruning techniques
for delete-free instances proposed by Gefen and Brafman (2012). Table 4 compares IP(T + ) and
IPe (T + ) with some results by Haslum et al. (2012, p. 356, Table 1). The BC/HST12 column is
the method by Bonet and Castillo (2011). The ML/HST12 column is the minimal landmark method
proposed by Haslum et al.. In the original work by Haslum et al. (2012), the minimum-cost hitting
set problem was solved using a specialized branch-and-bound algorithm, and the ML/HST12 column reflects the performance of this original algorithm. However, the Minimal Landmark method
was later significantly improved by replacing the hitting set solver with a CPLEX-based solver
(Haslum, 2014b), so Table 4 also includes the HST/CPLEX column, which shows the results of
Minimal Landmark method using the CPLEX hitting set solver. We obtained these HST/CPLEX
results by running the HST/CPLEX code on the same machine used to run our IP models.
Table 4 includes a column IPe (T + )/5min, which shows the results for 5-minute runs of
IPe (T + ) (all other columns in Table 4 are for 30 minute runs).
Note that in Table 4, the instances from IPC2008 and IPC2011 are from the sequential satisfying track (i.e., -sat08 and -sat11 in the domain names), in accordance with the original paper
(Haslum et al., 2012).
647

fiI MAI & F UKUNAGA

5.1.3 C OMPARISON

WITH

HST/CPLEX

ON

D ELETE -F REE V ERSIONS OF IPC B ENCHMARKS

The most detailed comparison is with an improved implementation of the hitting-set based method
of Haslum et al. (2012). Although the original version of this algorithm used a problem-specific
branch-and-bound method to solve the hitting set problems, we used a more recent version of
Haslums h+ solver (source dated 2014-1-17), configured to use CPLEX 12.61 to solve the hitting set subproblem. This configuration is abbreviated as HST/CPLEX. As shown in Table
4, HST/CPLEX significantly outperforms the original HST implementation described in (Haslum
et al., 2012), and compares favorably vs. other previous methods.
Tables 5-6 and Figures 2-3 compare IP(T + ), IPe (T + ), IP(T + )+LM, and HST/CPLEX on 1376
IPC benchmark instances. All algorithms were run with a 2GB memory limit. Table 5 shows results
with a 30 minute time limit, while Table 6 shows results with a 5 minute time limit. Tables 5 and
6 compares coverage and runtimes per domain, while Figure 2 compares the cumulative number
of instances solved as a function of time, and Figure 3 compares the runtimes of all individual
instances.
In contrast to the previous set of experiments described in Section 5.1.2, we used optimal track
instances (-opt08 and -opt11 in the domain names) when both satisficing and optimal track
instances were available in the benchmark sets. This is because in the subsequent sections, we
focus on applying our models as the basis for heuristics for forward-search, cost-optimal planning.
5.1.4 D ISCUSSION OF R ESULTS ON D ELETE -F REE V ERSIONS OF IPC B ENCHMARKS
Not surprisingly, the basic IP(T + ) model is not competitive with previous state-of-the-art methods that were specifically developed for computing h+ (Haslum et al., 2012; Pommerening &
Helmert, 2012). However, Table 3 shows that the basic IP(T + ) model is at least competitive with
A* with LM-cut enhanced with Gefen and Brafmans pruning methods for delete-free instances
(Prune/GB12). IP(T + ) also significantly outperforms standard A* with LM-cut (Table 3, LMcut/GB12 and Table 2, FD/PH12).
On the other hand, enhancing IP(T + ) with our landmark-based model reduction method results
in significant improvement, and IP(T + )+LM is competitive with all previous methods except for
HST/CPLEX.
The IPe (T + ) model, which includes all of the enhancement described in Section 4 for reducing
the model in order to compute h+ faster, performs very well overall, and is competitive with all
previous methods. For example, in Table 4, IPe (T + ) has the highest coverage (or is tied for highest)
on 19/28 domains. Table 5, Figure 2, and Figure 3 show that while IPe (T + ) and HST/CPLEX
have similar coverage with a 30-minute time limit, IPe (T + ) tends to be somewhat faster overall.
However, there is no clear dominance relationship between IPe (T + ) and HST/CPLEX, since there
are some domains where IPe (T + ) clearly performs better (e.g., rovers, satellite, freecell) , and other
domains where HST/CPLEX performs better (e.g., airport, pegsol, scanalyzer, transport). Thus, the
IP-based approach and minimal landmark approaches seem to have complementary strengths with
respect to solving delete-free problems.
Aside from coverage, Figure 3 shows that many delete-free instances are solved much faster by
IPe (T + ) than HST/CPLEX. The difference between solving an easy delete-free instance in 0.1
vs. 0.5 seconds may not seem very important if we only need to solve the instance once. However,
the speed difference between IPe (T + ) and HST/CPLEX on such easy delete-free instances has a
significant implication when we consider using the h+ solvers as heuristic functions for A* -based
648

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

planners, where we may need to solve delete-free problems many thousands of times in the course
of a single A* search. As a result, we see below in Section 7, A* using IPe (T + ) as a heuristic
significantly outperforms A* using HST/CPLEX as a heuristic.
In order to eliminate the possibility that CPU speed differences account for the qualitative improvements in coverage obtained by our IP models compared to previously published results, Table
2 includes a column IPe (T + )/1min, which is the result for 1-minute runs of IPe (T + ), and Table
4 includes a column IPe (T + )/5min, which is the result for 5-minute runs of IPe (T + ) In effect,
these simulate machines that run at 1/5 and 1/6 (respectively) of the speed of the machine we used
in our experiments in Tables 2 and 4. This more than offsets improvements in single-core CPU
performance between 2010-2015. The coverage achieved by IPe (T + )/1min (753) in Table 2 is
higher than that of all other solvers in Table 2 which were given 5 minutes. Similarly, the coverage
achieved IPe (T + )/5min (847) in Table 4 is higher than that of than all other solvers in Table 4 which
were given 30 minutes.
Therefore, overall, IPe (T + ) is competitive with previous state-of-the-art delete-free solvers, and
our results indicate that direct computation of h+ using integer programming is a viable approach,
at least for computing each delete-free task once.
5.2 Comparison with HST/CPLEX on Minimal Seed Set Problem
To assess the performance of our best IP model, IPe (T + ) on a natural, delete-free task, we also
compared IPe (T + ) with HST/CPLEX on a set of minimal seed set problem instances from systems
biology (Gefen & Brafman, 2011). These consist of 22 instances originally evaluated by Gefen and
Brafman, as well as three additional versions of these 22 instances which were also provided by the
original authors, where each version uses a different set of action costs (Gefen & Brafman, 2011, p.
322), for a total of 22  4 = 88 instances. The solvers were run with a 1 hour CPU time limit per
instance and a 2GB RAM limit.
Figure 4 shows a scatter plot comparing the runtimes on each problem instance. The coverage
of IPe (T + ) was 87 instances, while the coverage of HST/CPLEX was 88 instances. On one hand,
Figure 4 shows that the majority of instances were solved significantly faster by IPe (T + ), and
IPe (T + ) solves 22 instances more than 10 times faster than HST/CPLEX. On the other hand, there
was one instance on which HST/CPLEX was more than 10 times faster than IPe (T + ), and there
was one instance which was solved in 40.7 seconds by HST/CPLEX but was not solved within the
time limit by IPe (T + ) (The dre instance with the type 2 preprocessing by Gefen & Brafman,
2011, p. 322).

6. Relaxations of the h+ Models
Although delete-free planning problems are interesting in their own right, our main motivation for
developing an efficient IP model for delete-free problems is to be able to use it as the basis for a
heuristic function for a forward-state space search based domain-independent planner. So far, we
have presented IP(T + ), a basic IP model which computes h+ , and then proposed IPe (T + ), which
incorporates a number of enhancements which, as shown in the experimental results in Section
5, significantly increase the scalability of the model and provide a new approach to computing h+
which is competitive with the previous state-of-the-art methods. It is possible to simply use IPe (T + )
as the heuristic function for a forward search based planner. However, as shown in Section 5,
computing h+ remains relatively expensive even using IPe (T + ), which is not surprising, given that
649

fiI MAI & F UKUNAGA

(Pommerening & Helmert, 2012, Table 2)
Domain (# problems)
airport(50)
blocks(35)
depot(22)
driverlog(20)
freecell(80)
grid(5)
gripper(20)
logistics00(28)
logistics98(35)
miconic(150)
no-mprime(35)
no-mystery(30)
openstacks-opt08(30)
pathways-noneg(30)
pipes-notankage(50)
pipes-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
tpp(30)
trucks(30)
zenotravel(20)
Total coverage (876)
# Best Domains

IP(T + )

IP(T + )+LM

IPe (T + )

IPe (T + )/1min

FD/PH12

BC/PH12

BnB/PH12

IDA*/PH12

solved
22
35
6
14
11
0
20
24
8
150
15
15
2
30
8
5
50
40
31
11
30
14
541
7

solved
36
35
19
14
17
4
20
28
21
150
20
21
30
30
13
9
50
40
30
24
30
14
655
9

solved
36
35
21
15
80
5
20
28
27
150
31
30
30
30
11
9
50
40
34
30
30
20
762
19

solved
35
35
21
14
80
5
20
28
24
150
30
28
30
30
10
9
50
40
34
30
30
20
753
15

solved
34
35
7
14
6
1
20
23
9
150
27
26
5
5
17
10
50
13
6
13
7
13
491
5

solved
50
35
5
2
1
1
20
26
7
150
14
16
0
4
3
2
50
12
6
12
3
8
427
5

solved
50
35
14
15
2
2
20
28
16
150
27
28
5
5
18
9
50
19
8
23
9
13
546
7

solved
50
35
14
15
3
2
20
28
15
150
26
28
4
5
19
10
50
19
9
24
9
13
548
9

Table 2: Coverage (# of instances solved) for delete-free problems (exact computation of h+ ).
5-minute time limit (except for IPe (T + )/1min which was run with a 1-minute time limit), 2GB
RAM. Comparison with data from Table 2 in the paper by Pommerening and Helmert (2012). #
Best domains is the number of domains for which a each solver achieves the highest coverage
(including ties).
(Gefen & Brafman, 2012, Table 2)
Domain (# problems)
blocks(35)
depot(22)
driverlog(20)
freecell(80)
gripper(20)
logistics00(28)
logistics98(35)
miconic(150)
no-mystery(30)
pipesworld-notankage(50)
pipesworld-tankage(50)
rovers(40)
Total coverage (560)
# Best Domains

IP(T + )

IP(T + )+LM

IPe (T + )

LM-cut/GB12

Prune/GB12

solved
35
8
14
12
20
24
8
150
21
11
7
40
350
4

solved
35
19
14
20
20
28
23
150
23
17
9
40
398
6

solved
35
21
15
80
20
28
28
150
30
17
9
40
473
11

solved
35
7
14
6
20
23
10
150
26
17
10
13
331
5

solved
35
12
15
2
20
28
16
150
26
9
9
23
345
5

Table 3: Coverage (# of instances solved) for delete-free problems (exact computation of h+ ).
30-minute time limit, 2GB RAM. Comparison with data from Table 2 in the paper by Gefen and
Brafman (2012).

650

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

(Haslum et al, 2012,
Table 2)
IP(T + )

Domain (# problems)
airport(50)
barman-sat11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-sat08(30)
floortile-sat11(20)
freecell(80)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
nomystery-sat11(20)
parcprinter-08(30)
pegsol-08(30)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
sokoban-sat08(30)
transport-sat08(30)
trucks(30)
visitall-sat11(20)
woodworking-sat08(30)
zenotravel(20)
Total coverage (1041)
# Best Domains

solved
22
7
35
8
14
1
19
12
20
8
24
150
20
11
30
25
11
7
50
40
31
10
25
2
30
8
29
15
664
7

IP(T + )+LM

solved
40
8
35
19
14
5
20
20
20
23
28
150
23
13
30
24
17
9
50
40
31
10
29
3
30
7
30
15
743
10

IPe (T + )

solved
39
9
35
21
15
30
20
80
20
28
28
150
34
19
30
26
17
9
50
40
34
10
29
7
30
8
30
20
868
19

HST/CPLEX

solved
50
20
35
20
14
30
12
76
20
20
28
150
31
7
30
30
24
10
50
32
14
21
30
15
30
16
29
14
858
16

IPe (T + )

HST/CPLEX

5min

5min

solved
36
6
35
21
15
30
19
80
20
27
28
150
31
19
30
25
11
9
50
40
34
9
29
6
30
7
30
20
847
16

solved
50
20
35
20
14
30
12
48
20
18
28
150
26
4
30
30
17
10
50
31
11
16
30
12
30
10
29
12
793
12

ML/HST12

BC/HST12

solved
50
18
35
18
13
27
12
17
20
15
27
150
28
5
30
30
20
15
50
18
8
15
30
6
30
2
19
13
721
10

solved
50
5
35
12
8
11
9
0
20
6
27
99
17
4
30
30
9
6
50
19
5
4
30
6
30
0
9
10
541
8

Table 4: Coverage (# of instances solved) for delete-free problems (exact computation of h+ ).
30-minute time limit (except for IPe (T + )/5min and HST/CPLEX/5min which were run with a 5minute time limit), 2GB RAM. Comparison with data from Table 2 in the paper by Haslum et al.
(2012).

651

fiI MAI & F UKUNAGA

IP(T + )/30min

Domain (# problems)
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best Domains

solved

22
8
35
8
14
2
1
20
12
0
20
8
24
150
20
21
13
5
3
0
30
20
2
30
25
13
11
7
50
40
31
10
7
29
20
13
4
0
30
20
30
20
15

time
253.97
1616.97
0.08
151.07
19.05
294.94
525.76
4.67
130.82
0
0.02
194.01
12.21
0.08
202.01
187.66
180.88
114.55
506.63
0
0.08
0.06
529.75
1.50
229.13
360.87
370.96
154.58
0.03
11.77
35.88
306.24
442.44
34.12
39.14
256.03
289.63
0
1.94
3.97
2.04
2.40
35.54
843
14

IP(T + )+LM/30min
solved

40
8
35
19
14
20
13
20
20
4
20
23
28
150
23
23
17
25
30
20
30
20
18
30
24
14
17
9
50
40
31
10
7
29
20
24
4
0
30
20
30
20
15

time
173.58
1522.41
0.00
12.75
15.77
201.74
179.16
1.76
259.96
5.59
0.02
89.77
0.03
0.09
221.48
129.89
224.40
82.48
0.08
0.04
0.04
0.03
172.21
1.13
39.01
105.91
198.51
22.87
0.02
0.34
38.40
292.41
439.54
0.61
0.47
55.71
45.00
0
0.70
1.76
0.52
0.47
36.69
1044
17

IPe (T + )/30min
solved

39
20
35
21
15
30
20
20
80
5
20
28
28
150
34
30
20
30
30
20
30
20
20
30
26
15
17
9
50
40
34
10
7
30
20
30
15
16
30
20
30
20
20

time  sd
134.68  452.99
14.29  40.80
0.00  0.00
0.92  1.90
5.47  18.32
0.38  0.46
0.32  0.42
1.08  3.02
0.32  0.21
6.50  11.29
0.00  0.00
39.07  132.25
0.01  0.02
0.01  0.01
53.06  132.87
12.84  44.49
0.11  0.11
0.39  1.09
0.01  0.01
0.01  0.01
0.02  0.01
0.01  0.01
0.30  0.23
0.05  0.03
40.72  126.79
86.91  183.21
221.80  306.90
18.39  44.42
0.01  0.05
0.13  0.22
0.96  1.64
86.52  173.64
129.49  213.41
56.97  305.42
0.23  0.28
4.58  9.54
151.31  421.56
203.80  424.60
0.03  0.02
1.07  2.93
0.02  0.01
0.02  0.01
3.21  9.13
1214
34

HST/CPLEX/30min
solved

50
20
35
20
14
30
20
15
76
5
20
20
28
150
31
30
8
27
30
20
30
20
20
30
30
20
24
10
50
32
14
21
13
30
20
28
27
20
30
20
30
20
14

time  sd
9.99  36.34
0.04  0.08
0.00  0.00
3.50  8.70
17.30  56.93
0.09  0.07
0.07  0.04
54.56  193.72
320.71  433.35
1.41  1.61
0.01  0.01
146.85  339.48
0.03  0.06
0.04  0.05
106.60  242.37
12.43  29.27
0.36  0.49
81.80  258.73
0.04  0.04
0.03  0.02
0.07  0.12
0.04  0.05
15.97  30.89
2.55  3.08
0.01  0.01
0.01  0.01
223.55  358.14
4.32  11.94
0.01  0.05
34.36  123.88
205.10  384.71
242.91  460.55
338.77  536.07
0.07  0.12
0.07  0.13
142.13  272.08
100.16  146.57
18.30  35.03
1.32  2.10
0.21  0.38
0.15  0.27
0.09  0.07
179.65  453.63
1202
31

Table 5: Detailed comparison of IP(T + ), IP(T + )+LM, IPe (T + ), and HST/CPLEX on 1376 deletefree tasks (exact computation of h+ ). 30-minute time limit, 2GB RAM. Coverage and mean 
standard deviation of runtimes (average of successful runs only, excludes unsuccessful runs).

652

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

IP(T + )/5min

Domain (# problems)
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best Domains

solved

22
0
35
6
14
1
0
20
11
0
20
8
24
150
15
15
11
5
2
0
30
20
0
30
22
9
8
5
50
40
31
7
4
28
19
11
3
0
30
20
30
20
14

time  sd
0.82
0
0.08
29.35
17.33
25.40
0
4.74
73.07
0
0.02
20.45
11.64
0.08
28.01
9.35
37.85
66.39
16.89
0
0.07
0.05
0
1.53
74.55
131.78
5.53
31.71
0.03
10.26
29.87
57.26
34.20
27.27
22.22
12.42
8.64
0
1.60
3.80
1.98
2.17
4.02
790
13

IP(T + )+LM/5min

time
0.33
0
0.00
12.85
17.04
41.09
30.13
1.64
43.14
5.39
0.02
19.56
0.03
0.08
30.02
27.74
42.15
31.37
0.08
0.04
0.03
0.03
69.50
1.14
16.60
36.09
37.58
21.57
0.02
0.33
28.81
48.99
15.28
0.58
0.46
49.31
43.16
0
0.67
1.83
0.49
0.46
1.04

solved

36
0
35
19
14
16
11
20
17
4
20
21
28
150
20
21
14
24
30
20
30
20
15
30
23
12
13
9
50
40
30
7
4
29
20
24
4
0
30
20
30
20
14
994
17

IPe (T + )/5min
solved

36
20
35
21
15
30
20
20
80
5
20
27
28
150
31
30
20
30
30
20
30
20
20
30
25
13
11
9
50
40
34
9
6
29
20
30
13
13
30
20
30
20
20

time  sd
4.10  23.83
13.60  38.23
0.00  0.00
0.93  1.95
5.86  19.83
0.39  0.47
0.31  0.41
1.05  2.93
0.30  0.20
6.35  11.05
0.00  0.00
13.94  36.73
0.01  0.02
0.01  0.01
14.91  51.52
12.04  41.80
0.10  0.10
0.37  1.00
0.01  0.01
0.01  0.01
0.01  0.01
0.01  0.01
0.29  0.22
0.04  0.03
16.24  37.01
16.65  20.17
16.55  52.18
14.65  34.93
0.01  0.04
0.13  0.23
1.03  1.79
41.39  56.98
52.22  66.89
0.25  0.33
0.23  0.28
4.60  9.66
11.60  24.25
28.55  42.33
0.03  0.02
1.11  3.08
0.02  0.01
0.02  0.01
3.38  9.70
1190
33

HST/CPLEX/5min
solved

50
20
35
20
14
30
20
14
48
5
20
18
28
150
26
30
8
24
30
20
30
20
20
30
30
20
17
10
50
31
11
16
9
30
20
24
24
20
30
20
30
20
12

time  sd
9.44  34.94
0.04  0.08
0.00  0.00
3.47  8.57
17.03  56.02
0.08  0.07
0.07  0.04
2.80  4.21
60.87  80.81
1.37  1.54
0.01  0.00
34.28  67.68
0.03  0.06
0.04  0.05
11.27  23.42
12.88  30.69
0.34  0.46
12.20  35.90
0.04  0.04
0.03  0.02
0.07  0.11
0.04  0.05
15.07  28.61
2.47  2.92
0.01  0.01
0.01  0.01
21.71  35.68
4.31  11.93
0.01  0.05
12.55  28.76
16.92  40.99
21.91  45.07
23.20  54.45
0.07  0.12
0.07  0.13
46.47  81.70
56.58  87.08
17.45  32.84
1.74  3.16
0.21  0.38
0.14  0.26
0.08  0.07
20.86  65.51
1134
31

Table 6: Detailed comparison of IP(T + ), IP(T + )+LM, IPe (T + ), and HST/CPLEX on 1376 deletefree tasks (exact computation of h+ ). 5-minute time limit, 2GB RAM. Coverage and mean 
standard deviation of runtimes (average of successful runs only, excludes unsuccessful runs).

653

fiI MAI & F UKUNAGA

1400
1200

Instances solved

1000
800
600
400
IPe(T+)
HST/CPLEX

200
0
0.0001

IP(T+)+LM
IP(T+)

0.001

0.01

0.1

1

10

100

1000

Time (seconds)
Figure 2: Comparison of IP(T + ), IP(T + )+LM, IPe (T + ), and HST/CPLEX on delete-free tasks
(exact computation of h+ ). 30-minute time limit, 2GB RAM. The cumulative number of instances
(out of the same 1376 instances as Table 5) solved within T ime seconds is shown.
computing h+ is NP-equivalent (Bylander, 1994). Haslum (2012) reported some previous, baseline
results using a direct computation of h+ using the hitting-set method proposed in his earlier work
(Haslum et al., 2012) as a heuristic for A* , and reported poor results. Although we show in Section
7 that A* using IPe (T + ) performs well on some domains, using h+ directly as a heuristic for A*
continues to pose a significant challenge. Thus, we turn next to relaxations of IP(T + ) and IPe (T + )
that are lower bounds on h+ and can be computed faster, making them more suitable as admissible
heuristics for a forward-search planner than our IP models.
6.1 LP(T + ) and LPe (T + ): LP Relaxations of the Delete-Relaxation (h+ ) Models
The linear programming (LP) relaxations of the IP models are obvious candidates for tractable
alternatives to computing h+ using IP(T + ) and IPe (T + ). The LP-relaxations are trivially derived
from the IP models by eliminating the integer constraints on the variables, and the optimal cost of
the LP-relaxation is a lower bound on the optimal cost of the IP. We denote the LP relaxation of
IP(T + ) as LP(T + ) and the LP relaxation of IPe (T + ) as LPe (T + ) (see Table 1). In the case of
problem domains with integer action costs, the ceiling of the LP costs are used.
Although LPe (T + ) can be solved quickly, tight theoretical bounds on the gap between IP(T + )
and LP(T + ) or the gap between IPe (T + ) and LPe (T + ) are difficult to obtain  it has been proven
by Betz and Helmert (2009) that there exists no constant c > 0 and no polynomial-time algorithm
for computing a lower bound h such that for all states s, h(s)  ch+ , unless P = N P (i.e.,
h+ is not polynomial-time approximable for any constant factor c). Fortunately, the worst-case
654

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

100*x

10*x

x

x/100

x/10

1

10

100

1000
100

IPe(T+)

10
1
0.1
0.01
0.001
0.001

0.01

0.1

1000

HST/CPLEX

Figure 3: Comparison of runtimes of IPe (T + ) and HST/CPLEX on 1376 delete-free instances (exact computation of h+ , same instances as Table 5). 30-minute time limit, 2GB RAM. Each point represents a problem instance. The x-axis represents the runtime of HST/CPLEX, while the y-axis represents the runtime of
IPe (T + ). For example, a point below the diagonal (y = x) indicates that IPe (T + ) solved the problem represented by that point faster than HST/CPLEX, and a point below the y = x/10 line indicates that IPe (T + )
solved the problem represented by that point at least 10 times faster than HST/CPLEX. If an algorithm failed
to solve an instance within the 30-minute time limit, the runtime is shown as 1800 seconds.

theoretical approximation results do not necessarily apply to real-world problem instances. In fact,
our experimental results below show that the LP-relaxations often provide fast, accurate, lower
bounds on h+ for standard planning benchmark problems.
6.2 Time-Relaxation of h+ Models
If our motivation is to embed a computation for h+ (or an approximation thereof) as an admissible
heuristic for A* , we are not necessarily interested in the actual optimal delete-free plan for T + , but
only the cost of that plan (or its approximation). In particular, if the exact order in which actions are
executed in the delete-relaxed plan does not matter, the necessity of all time-related variables can
be brought into question.
The time-relaxation of IP(T + ), which is IP(T + ) without constraints C5 and C6, is denoted
IPtr (T + ). The LP relaxation of IPtr (T + ) is denoted LPtr (T + ). Table 1 summarizes the relationships among these models.
If the propositions and actions of the task satisfy some conditions, eliminating the time-related
variables does not affect the cost of the optimal solution to IP(T + ). For example, if the relaxed
causal AND/OR graph (Gefen & Brafman, 2012) of the task does not have a cycle, then we can
decide the values of T (p) and T (a) such that constraints C5 and C6 of IP(T + ) are satisfied in655

fiI MAI & F UKUNAGA

1000

IPe(T+)

100
10
1

0.1
0.01
0.001
0.001

x
10*x
x/10
0.01

0.1

1

10

100

1000

HST/CPLEX

Figure 4: Runtime comparisons of IPe (T + ) and HST/CPLEX on minimal seed set problem (88 natural,
delete-free instances from Gefen & Brafman, 2011). 60-minute time limit, 2GB RAM. Each point represents
a problem instance. If an algorithm failed to solve an instance within the 60-minute time limit, the runtime
is shown as 3600 seconds. The coverage of IPe (T + ) was 87 instances, while the coverage of HST/CPLEX
was 88 instances.

dependently of the values of the other variables, in which case the optimal costs of IP(T + ) and
LP(T + ) are the same as the optimal costs of IPtr (T + ) and LPtr (T + ), respectively.
Indeed, we shall show experimentally in Section 6.3 that the relaxation is quite tight, i.e.,
IP(T + ) and IPtr (T + ) often have the same cost, and that IPtr (T + ) can be computed significantly faster than IP(T + ). Similarly, LPtr (T + ), LPetr (T + ), and IPetr (T + ), the time-relaxations
of LP(T + ), LPe (T + ), and IPe (T + ), can be computed much faster than their non-time-relaxed
counterparts.
6.3 Experimental Evaluation of LP and Time Relaxation Gaps
We evaluated the quality of the LP(T + ), LPe (T + ), and LPetr (T + ) linear programming bounds described above by comparing optimal costs computed for these bounds to exact h+ values (computed
using IPe (T + )). We used the same set of 1376 instances as in Table 5. Table 7 shows the mean ratio
of the optimal cost of each LP model to h+ , on all instances where h+ could be computed using
IPe (T + ). The perfect columns indicate the fraction of instances where the optimal cost of the
LP model was equal to h+ . Note that we used the ceiling of the LP cost, since the IPC benchmark
instances have integer costs. A stacked histogram representation of the same data (aggregated over
all domains) which classifies the ratios of the optimal costs of the LP relaxations to the value of h+
is shown in Figure 5.
We should expect that the variable-fixing constraints in our enhanced LPe (T + ) model would
tend to increase the value of the optimal solution to LPe (T + ) compared to the optimal value of the
base LP relaxation, LP(T + ). In addition, we would also expect that the optimal value for LPe (T + )
would tend to be greater than the optimal value of its time relaxation, LPetr (T + ). Table 7 shows that
656

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

in general, LPe (T + )  LPetr (T + )  LP(T + ). For 10/43 domains, LPe (T + ) matches h+ perfectly,
i.e., LPe (T + )/h+ = 1. For 20/43 domains, LPe (T + )/h+  0.95. On almost every single domain,
the optimal LP value of the enhanced model LPe (T + ) is significantly better (higher) than the basic
formulation LP(T + ), confirming that variable elimination and the additional constraints serve to
tighten the LP bound. Thus, the enhancements to the basic model described in Section 4 provide a
significant benefit beyond the speedups that were demonstrated in Section 5. The time-relaxation
LPetr (T + ) is usually very close to LPe (T + ), indicating that the time relaxation can potentially
achieve a good tradeoff between computation cost and accuracy (and in fact, as we see later in
Section 7, LPetr (T + ) performs quite well when used as a heuristic for A* ).
For comparison, we also evaluated the ratio of the value of the LM-cut heuristic (Helmert &
Domshlak, 2009) to h+ . Comparing the average ratios of each lower bound to h+ , we see that:
 LP(T + ) is less informative than LM-cut on 31 domains, more informative than LM-cut on 5
domains, and equivalent on 6 domains.
 LPe (T + ) is less informative than LM-cut on 16 domains, more informative than LM-cut on
19 domains, and equivalent on 8 domains.
 LPetr (T + ) is less informative than LM-cut on 17 domains, more informative than LM-cut on
17 domains, and equivalent on 9 domains.
Thus, while LM-cut is a better approximation to h+ than the basic LP-relaxation, LP(T + ),
and LPetr (T + ) are roughly equivalent to LM-cut. Interestingly, the LP-relaxation approach appears to be highly complementary to the cost-partitioning approach of LM-cut, in that the
LP-relaxation and LM-cut are each more informative than the other on roughly half of the cases
compared to each other.

LPe (T + )

1

1.0
[0.8-1.0)
[0.6-0.8)
[0.4-0.6)
[0.2-0.4)
[0.0-0.2)

0.9

Fraction of Instances

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

LP

LP

e

LP

e
tr

Figure 5: Ratio between optimal LP costs and h+ , categorized into buckets. [x:y) = fraction of
instances where the ratio LP/h+ is in the range [x:y). For example, the fraction of instances where
the ratio between the optimal value of LPetr (T + ) and h+ was in the range [0.8,1,0) is approximately
0.24 (this stacked histogram is based on the same data as Table 6.3).
657

fiI MAI & F UKUNAGA

LM-cut
perfect
1.00
.74
.74
0
.99
.97
.64
0
.89
.20
.77
.06
.80
.10
.94
.05
.29
0
.67
.40
1.00
1.00
.98
.40
.99
.92
1.00
1.00
.76
.20
.79
.28
.93
.50
.61
0
1.00
1.00
1.00
1.00
.99
.70
.99
.65
.87
0
.87
.13
.60
.26
.55
.05
.68
.02
.75
0
1.00
1.00
.87
.12
.95
.23
.95
.32
.97
.26
.94
.53
.96
.60
.98
.55
.87
.03
.84
.05
.92
0
.69
.10
.89
.13
.88
.10
.95
.50

LM-cut/h+

airport
barman-opt11
blocks
depot
driverlog
elevators-opt08
elevators-opt11
floortile-opt11
freecell
grid
gripper
logistics98
logistics00
miconic
no-mprime
no-mystery
nomystery-opt11
openstacks
openstacks-opt08
openstacks-opt11
parcprinter-08
parcprinter-opt11
parking-opt11
pathways-noneg
pegsol-08
pegsol-opt11
pipesworld-notankage
pipesworld-tankage
psr-small
rovers
satellite
scanalyzer-08
scanalyzer-opt11
sokoban-opt08
sokoban-opt11
tpp
transport-opt08
transport-opt11
trucks
visitall-opt11
woodworking-opt08
woodworking-opt11
zenotravel

LP(T + )
perfect
.46
.02
.17
0
.92
.20
.50
0
.85
.10
.21
0
.20
0
.95
.10
.12
0
.31
.20
1.00
1.00
.39
.02
.46
.03
1.00
1.00
.42
0
.39
0
.96
.60
.23
.03
1.00
1.00
1.00
1.00
.99
.66
.99
.70
.88
0
.90
.13
.26
.03
.20
0
.52
0
.58
0
.87
.82
.48
0
.82
.13
.94
.30
.96
.25
.33
.13
.28
.15
.28
.13
.08
0
.09
0
.40
0
.98
.65
.81
0
.80
0
.91
.25

LP(T + )/h+

LPe (T + )
perfect
.98
.94
.38
0
1.00
1.00
.92
.22
.87
.21
.65
0
.64
0
.95
.10
.94
.35
.81
.20
1.00
1.00
.89
.11
.99
.85
1.00
1.00
.71
.33
.77
.33
1.00
.95
1.00
.96
1.00
1.00
1.00
1.00
.99
.66
.99
.70
.92
.10
.98
.60
.64
.03
.65
0
.83
.38
.93
.55
1.00
1.00
.65
.35
.82
.21
.94
.75
.96
.71
.95
.73
.97
.80
.85
.26
.35
.08
.41
0
1.00
1.00
.98
.65
1.00
1.00
1.00
1.00
.92
.31

LPe (T + )/h+

LPetr (T + )
+
LPe
(T
)/h+
perfect
tr
.98
.38
1.00
.91
.83
.64
.62
.95
.92
.79
1.00
.88
.99
1.00
.63
.72
1.00
.88
1.00
1.00
.99
.99
.87
.98
.64
.65
.79
.91
1.00
.65
.82
.94
.96
.94
.97
.85
.35
.41
1.00
.97
1.00
1.00
.89

.70
0
1.00
.18
.05
0
0
.10
.23
.20
1.00
.05
.78
1.00
.17
.30
.95
1.00
1.00
1.00
.66
.70
0
.60
.03
0
.08
.36
1.00
.30
.20
.34
.29
.66
.75
.26
.03
0
1.00
.65
1.00
1.00
.30

Table 7: Gaps between LP models and h+ : The mean ratio of each LP model to h+ (on the 1228
instances solved using IPe (T + ) is shown. The perfect columns indicate the fraction of instances
where the optimal cost of the LP model was equal to h+ .

658

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Figure 6 compares the runtimes the CPLEX LP solver on the relaxed h+ models. LPe (T + ) is
significantly faster than LP(T + ), solving many instances 2-10 times faster (and solving some instances more than 10 times faster), demonstrating the benefits of our enhanced model. The comparison of LPetr (T + ) and LPe (T + ) shows that using the time relaxation results in an addition speedup
of up to a factor of 2. While this additional speedup may not seem very significant when solving a
single LP instance that takes a fraction of a second, the cumulative effects when using the LP models as a heuristic for forward-search based planning is significant, and as we show in Section 7, this
results in increased coverage when using LPetr (T + ) as a heuristic for A* , compared to LPe (T + ).

100

100

10

10

LPetr(T+)

1000

LPe(T+)

1000

1

0.1

0.1

x
10*x
2*x
x/2
x/10

0.01
0.001
0.001

1

0.01

0.1

1

LP(T+)

10

100

x
10*x
2*x
x/2
x/10

0.01

1000

0.001
0.001

0.01

0.1

1

LPe(T+)

10

100

1000

Figure 6: Runtime comparisons for relaxed h+ models. on 1376 delete-free instances (exact computation of
h+ , same instances as Table 5). 30-minute time limit, 2GB RAM. Each point represents a problem instance.
The left subfigure compare LP(T + ) vs LPe (T + ), showing the impact of our enhancements to the basic LP
model, and the right subfigure compares LPe (T + ) vs LPetr (T + ), showing the impact of the time relaxation.
If an algorithm failed to solve an instance within the 30-minute time limit, the runtime is shown as 1800
seconds.

7. Cost-Optimal Planners Using Our h+ -Based Heuristics
We embedded the IP and LP models that have been introduced so far into an A* -based, cost-optimal
forward search planner (our own planner implementation, which uses a propositional representation
internally) and evaluated their performance. Note that this particular experiment is limited to admissible heuristics whose value is bounded above by h+ . The later results in Section 8 and 9 include
heuristics that are not necessarily bounded above by h+ . Specifically, we evaluated the following
solver configurations:
 A* /IP(T + ) : A* with the basic delete-free IP model IP(T + ) as a heuristic.
 A* /IPe (T + ) : A* with the enhanced delete-free IP model IPe (T + ) as a heuristic.
 A* /LPe (T + ) : A* with the LP relaxation of the enhanced delete-free IP model IPe (T + ) as a
heuristic.
 A* /LPetr (T + ) : A* with the LP relaxation of the time-relaxed, enhanced delete-free IP model
IPe (T + ) as a heuristic.
659

fiI MAI & F UKUNAGA

 hsp/HST/CPLEX : A* where the heuristic is the hitting-set based h+ solver HST/CPLEX
(Haslum et al., 2012) using CPLEX to solve hitting set instances (hsp planner provided by
Patrik Haslum).
 FD/hmax : Fast Downward using the hmax heuristic (Bonet & Geffner, 2001).
 FD/LM-cut : Fast Downward using the landmark cut heuristic (Helmert & Domshlak, 2009)
(the standard seq-opt-lmcut configuration)
As per standard IPC sequential optimal track settings, all solver configurations were run with a
30 minute time limit per problem and a 2GB RAM limit. A set of 1376 instances from IPC1998IPC-2011 were used. Our planner currently handles the STRIPS subset of PDDL with action costs.
Table 8 compares the coverage of these heuristics. Figure 7a shows the cumulative coverage
(out of 1376) solved as a function of time for the solver configurations compared in Table 8, and
Figure 7b shows cumulative coverage as a function of the number of node evaluations (calls to the
heuristic function by A* ).
While we compare our IP/LP-based A* -heuristics with other planners, note that there are significant implementation-level differences other than the heuristic function that can affect execution
speed. For example, Fast Downward uses a multi-valued SAS+ representation (Backstrom & Nebel,
1995) internally to represent states, while our planner uses a STRIPS propositional representation,
so there are significant differences in internal data structures and implementation details. Thus,
these results should only be used for qualitative comparisons.
Table 8 shows that A* /IP(T + ), which uses the basic IP(T + ) model, had the worst coverage
among our IP models (403), comparable to that of A* /HST/CPLEX(398). As noted by Haslum
(2012), straightforward use of h+ as a heuristic can be unsuccessful (even worse than FD using
hmax , which has a coverage of 540) if the cost of computing h+ at each search node is too high.
However, as shown in Section 5, solving the IPe (T + ) IP model is significantly faster than
IP(T + ) and A* /HST/CPLEX. This makes it much more viable as a heuristic function for A* , and
as a result, A* /IPe (T + ) has a coverage of 635, significantly outperforming both A* /HST/CPLEX as
well as FD/hmax.
As shown in Section 6.3, the LP relaxations of our IP models provide relatively tight lower
bounds for h+ . Since the LP models can be solved much faster than IP, they are quite effective
when used as heuristics for A* . Thus, A* /LPe (T + ), which uses the LP-relaxation of the enhanced
IPe (T + ) model, has a coverage of 696, and A* /LPetr (T + ), which uses the LP-relaxation of the
time-relaxed, enhanced IP model, has a coverage of 705.
In Section 6.3, we showed that the LPe (T + ) and LPetr (T + ) models are complementary to LMcut with respect to informativeness, which suggests that at least with respect to search efficiency,
our LP models should be competitive with LM-cut. Figure 7b shows that in fact, A* /LPe (T + ) and
A* /LPetr (T + ) tend to search quite efficiently, and it can be seen that both of these lines are above
the LM-cut line (i.e., more problems were solved using a given number of evaluations) until between 105  106 node evaluations, at which point they are overtaken by the LM-cut line. While
the informativeness comparison in Section 6.3 showed that the LP models are comparable and complementary to LM-cut with respect to informativeness, FD/LM-cut outperforms A* /LPetr (T + ) and
A* /LPetr (T + ) on most domains. This is because the LM-cut implementation in Fast Downward
is often significantly faster than the current implementation of our LP-based heuristics. Nevertheless, there are several domains (freecell, parcprinter-08, parcprinter-opt11, satellite, trucks, visitall),
660

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Domain (# problems)
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best domains

FD/hmax

FD/LM-cut

hsp/HST/CPLEX

A* /IP(T + )

A* /IPe (T + )

A* /LPe (T + )

solved
21
4
18
4
9
15
13
4
15
2
7
2
10
50
23
17
8
7
19
14
14
10
0
4
27
17
16
7
49
6
6
9
6
27
20
6
11
6
7
9
9
4
8
540
15

solved
28
4
28
7
14
22
18
7
15
2
7
6
20
141
23
16
14
7
19
14
19
14
3
5
27
17
17
8
49
7
7
15
12
30
20
6
11
6
10
11
17
12
13
748
36

solved
24
0
17
1
7
3
1
1
19
1
2
3
10
79
15
15
8
5
7
2
19
14
0
4
17
4
9
6
19
4
5
5
2
6
3
5
7
2
3
15
14
8
7
398
0

solved
14
0
19
2
9
0
0
2
8
0
4
3
16
137
10
5
8
0
2
0
19
14
0
5
1
0
3
2
43
7
8
5
2
3
1
5
2
0
7
9
12
7
9
403
0

solved
24
0
27
7
10
9
7
4
54
2
5
5
19
140
20
15
14
7
10
5
21
16
2
5
10
2
10
8
48
7
10
5
2
17
13
6
7
2
13
10
17
11
9
635
13

solved
25
0
28
7
11
13
10
6
44
2
6
6
20
140
18
13
14
7
11
6
20
16
1
5
26
16
12
7
48
7
10
8
5
23
19
6
9
4
15
16
16
10
10
696
14

+
A* /LPe
tr (T )

solved
25
0
28
7
13
13
10
7
43
2
6
6
20
141
17
12
14
7
11
6
20
16
1
5
26
16
13
7
48
7
10
8
5
25
19
6
10
5
15
16
17
11
11
705
17

Table 8: Comparison of forward search (A* ) planners, part 1: Number of problems solved with 30
minute, 2GB RAM limit using A* and our IP/LP models which are bounded above by h+ (Sections
3-7) as heuristic functions. Comparison with Fast Downward with hmax , Fast Downward with
Landmark Cut, and the hsp planner using HST/CPLEX (Haslum et al., 2012) to compute h+ , as the
heuristic function.

where A* /LPetr (T + ) achieves higher coverage than FD/LM-cut. Thus, A* /LPetr (T + ), our best
model among those which are bounded above by h+ , can be considered a fairly powerful, admissible heuristic function for forward-state search based planning.
661

fiI MAI & F UKUNAGA

800
700

Instances solved

600
500
400
300
FD/LMcut
A*/LPetr(T+)
A*/LPe(T+)
A*/IPe(T+)
FD/hmax

200
100
0
0.1

1

10

100

1000

Time (seconds)
(a) Cumulative number of problems solved (out of 1376) vs time (30 minute time limit).

800
700

Instances solved

600
500
400
300
FD/LMcut
A*/LPetr(T+)
A*/LPe(T+)
A*/IPe(T+)
FD/hmax

200
100
0

1

10

100

1000

10000 100000 1e+06

1e+07

1e+08

Evaluations
(b) Cumulative number of problems solved (out of 1376) vs number of search nodes evaluated (30 minute
time limit).

Figure 7: Comparison of forward search (A* ) planners, part 1 ( heuristics that are bounded above
by h+ ).

662

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

8. Incorporating Counting Constraints
So far, we have concentrated on efficient computation of h+ as well as relaxations of h+ , and all of
our models so far have been bounded above by h+ . However, our IP model can be extended with
constraints that consider delete effects. By adding variables and constraints related to delete effects
of actions, our model can also calculate lower bounds on the number of times each action must be
applied. New variables are defined as follows:
 a  A, N (a)  {0, 1,    } : N (a) = n iff a is used n times.
 p  P, G(p)  {0, 1} : G(p) = 1 iff p  G.
G(p) is an auxiliary variable similar to I(p). Furthermore, in this extended model, the meaning
of U (a)  {0, 1} is slightly modified to mean that action a is used at least once in the optimal
solution (in the basic model proposed in Section 3, which was a pure delete-free model, U (a)
denoted whether a was used exactly once or not at all in the optimal solution).
New constraints are defined as follows:
(C7) a  A, N (a)  U (a).
P
P
(C8) p  P, G(p) + as.t.ppredel(a) N (a)  I(p) + as.t.padd(a) N (a),
where
predel(a) = pre(a)  del(a). Finally, the objective function is modified so as to minimize
P
aA c(a)N (a). Given a planning task T , we use IPc (T ) to denote an IP problem which adds the
and above new variables and constraints to IP(T + )
The idea for these types of constraints have been previously proposed several times (for a SAS+
formulation), and correspond to the action order relaxation by van den Briel et al. (2007), the state
equation heuristic by Bonet (2013), and the net change constraints by Pommerening et al. (2014).
Intuitively, the final constraint states that the number of uses of actions adding p must be greater
than or equal to the number of uses of actions requiring and deleting p at the same time in a feasible
plan for T . Any feasible plan for a STRIPS planning task always satisfies this condition. Hence,
for any task T and any feasible plan  for T , we can clearly derive a feasible solution to IPc (T )
with the same cost as . In addition to this, a stronger proposition can be proved for modifications
of models by the enhancements in Section 4.
Proposition 7. Given a task T , for any feasible plan  = (a0 ,    , an ) of T , there exists a feasible
solution to IPc (T ) that has the same cost as the cost of . In addition to this, there exists a feasible solution to IPc (T ) with any combination of landmark extraction and substitution, relevance
analysis, and inverse action constraints that has the same cost as the cost of .
Proof. Let  + be the delete relaxation of the subsequence of the plan  extracted by Algorithm 3.
First we show that the subsequence  + is a feasible delete-free plan for T + , and then we show that
the assignment derived from  + satisfies the constraints.
+
+
+
+
We use (a+
0 ,    , am ) to denote the elements of  . To show that  is feasible in T , assume
+
+
a+
i is the first infeasible action in  . Let p be a proposition such that p  pre(ai ) and p 6
+
+
I((a0 ,    , ai1 )). Since  is a valid feasible plan for T , the delete-relaxation of the entire sequence
of  is a valid feasible plan for T + . Hence, if a+
i is not feasible, then this is because Algorithm 3
+
skipped all the actions that add p before ai is applied. Since S on line 5 in Algorithm 3 is equal to
+
I((a+
0 ,    , ai1 )) for each i, all the skipped actions that add p satisfy add(ai ) \ S 6= , and thus
663

fiI MAI & F UKUNAGA

Algorithm 3 Extracting a subsequence of  = (a0 ,    , an ) (for the proof of Proposition 7)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:

 +  (); // empty
S  I;
for a = a0 ,    , an do
Let a be the delete-relaxation of a.
if a is relevant to T + and add(a ) \ S 6=  then
append a at the end of  + ;
S  S  add(a );
end if
end for
return  + ;

they are irrelevant to T + . However this contradicts the definition of the relevance analysis and the
+
+
fact that a+
i is relevant. Similar to this argument, we have G  I( ). Hence  is a valid feasible
plan for T + .
Define an assignment F for IPc (T ) as:
 VF := VF + for each variable V that is defined for IP(T + ), where F + is the assignment
derived from  + for IP(T + ), and
 N (a)F := (the number of occurrences of a in ) for each a  A.
The assignment F clearly satisfies constraints C1 through C6. The assignment F also satisfies
constraint C8 since  is a valid plan for T , and F satisfies constraint C7 since U (a)F = 0 if a is not
included in . Hence F is a feasible solution to IPc (T ) which has the same cost as .
In addition, F is also a feasible solution to IPc (T ) with any combination of landmark extraction
and substitution, relevance analysis, and inverse action constraints. We can see this by checking
the feasibility of F with each type of modified constraints independently. If F satisfies each of the
modified constraints, then it satisfies any combination of such constraints.
F satisfies the constraints added by the landmark extraction and substitution (i.e. substituting 1 for variables corresponding to landmarks) since  + is a valid feasible plan for T + . F also
satisfies constraints added by the relevance analysis (i.e. substituting 0 for irrelevant actions and
propositions) since  + contains only relevant actions. Finally, we can show
P that F satisfies inverse
action constraints similarly to the proof of Proposition 6. We have a inv(a,p) E(a , p)F = 0
P
when U (a)F = 0 and U (p)F = 0 hold, and we also have a inv(a,p) E(a , p)F  1 when
P
U (a)F = 0 and U (p)F = 1 hold. In addition, we can show that a inv(a,p) E(a , p)F = 0 for
each U (a)F = U (p)F = 1. Assume that there exists a  inv(a, p) such that E(a , p)F = 1. Then,
by constraint C3, U (a )F = 1, and this means a is also a member of  + . Without loss of generality,
assume a is applied before a is applied in  + . Since add(a )  pre(a) by the definition of inverse
actions, nothing new is added to the state after applying a . S on line 5 in Algorithm 3 is equal to

I((a+
0 ,    , a )), and this contradicts add(ai ) \ S 6= .
Unfortunately, the counting constraints conflict with dominated action elimination (Section 4.3)
and zero cost immediate action application (Section 4.4). When counting constraints are used,
it is necessary to disable zero cost immediate action application and to modify the condition of
dominated actions as follows:
664

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Definition 2 (modified dominated action definition). Given a feasible task T , an action a is dominated by action a if (i) add(a)  add(a ), (ii) for any p  pre(a ), p is a fact landmark of a or
p  I, (iii) c(a)  c(a ), and (iv) pre(a )  del(a )  pre(a)  del(a).
We can no longer use the modified dominated actions to make a feasible plan for T , since fact
landmarks are sometimes deleted after they are achieved. However the following fact can be proved.
Proposition 8. Given a task T , let  = (a0 ,    , an ) be a feasible solution to T . There exists a
feasible solution to IPc (T ) with any combination of landmark extraction and substitution, relevance
analysis, inverse action constraints, and the modified dominated action elimination that has cost
equal to or less than the cost of .
Proof. Recall that the dominated action elimination constraints substitute 0s for U (a) for each dominated action a. If  does not contain any modified dominated actions, then the proposition holds
due to Proposition 7.
Otherwise, we can derive a feasible solution using the sequence of actions made by replacing
modified dominated actions in  with their corresponding dominating actions. Let   be such a
sequence. Note that the sum of the costs of the actions in   is clearly less than or equal to that of .
Let  + be the relaxation of the subsequence of   extracted by Algorithm 3. Since we can
prove that the delete-relaxation of   is a feasible plan for T + by an argument similar to the proof
of Proposition 4, we can prove that  + is also a feasible plan for T + by an argument similar to the
proof of Proposition 7.
If  + is a feasible plan, then we can derive a feasible solution for IPc (T ) with the constraints
from   as in the proof of Proposition 7. The solution satisfies constraints C1 through C6 with
any combination of landmark extraction and substitution, relevance analysis, and inverse action
constraints. It satisfies constraint C7 because U (a) = 0 if a is not included in   , and it satisfies
constraint C8 because replacing dominated actions does not invalidate constraint C8 if  is a feasible
plan for T . It also satisfies dominated action elimination constraints (i.e. U (a) = 0 for each
dominated action a) since   does not contain any modified dominated action.




IPec (T ) and LPec (T ) denote the models constructed by applying all of the valid reductions to
IPc (T ) and LPc (T ) respectively. The LP and time relaxations for IP(T + ) described in Section 6

can be applied to IPc (T ) as well, and LPectr (T ) is the time-relaxed, LP-relaxation of the enhanced

IPec (T ) model. Table 1 summarizes the relationships among these models.
8.1 Experimental Results for Models Enhanced with Counting Constraints


To see the impact of adding counting constraints, we evaluated the informativeness of LPec (T ),

LPectr (T ), LPe (T + ), and LPetr (T + ) by comparing their values with the LM-cut heuristic values


(Helmert & Domshlak, 2009). Table 9 shows the values of LPec (T ), LPectr (T ), LPe (T + ), and
LPetr (T + ) as a multiple of the LM-cut values (means for each domain are shown). Note that in
contrast to Table 7, which was limited to the 1228 instances for which h+ could be computed
exactly, Table 9 includes all 1376 instances (because the LM-cut values could be computed for all
1376 instances).
On the majority of domains, the counting constraints result in a more informative heuristic,

compared to the models without the counting constraints, so in most cases, LPe (T + )  LPec (T )

and LPetr (T + )  LPectr (T ). It is sometimes possible for the optimal value of LPe (T + ) to be larger
665

fiI MAI & F UKUNAGA





than the optimal value of LPec (T ) and for LPetr (T + ) to have a larger optimal value than LPectr (T )
because as explained in Section 8, some of the additional constraints that are part of IPe (T + ) are

incompatible with IPc (T ) and are excluded from IPec (T ), resulting in different LP polytopes for
their LP-relaxations.
Next, to see the impact of adding counting constraints on forward-search planning using these


delete-relaxation LP models, we compare A* /LPec (T ) with A* /LPe (T + ), and A* /LPectr (T ) with
A* /LPetr (T + ). Coverage on the same instances as our previous experiment are shown in Table 10.
There is a tradeoff between the improved search efficiency due to the additional informativeness in
the heuristic provided by the counting constraints, and the additional time required to solve the LPs
(because the additional constraints make the LP more difficult to solve). Table 10 shows that the

overall effects of enhancing our delete-relaxation model are mixed. A* /LPec (T ) attains a coverage

of 672 instances, which is lower than the coverage for A* /LPe (T + ), while A* /LPectr (T ) solves 716
problems compared to the 705 problems solved by A* /LPetr (T + ). There are some domains where
adding the counting constraints significantly improved coverage, including parcprinter, pathwaysnoneg, rovers, woodworking. On the other hand, coverage dropped significantly in elevators, freecell, openstacks as a result of adding the counting constraints. The time relaxation seems to be
advantageous overall, resulting in an increase from 672 instances for A* /LPe (T + ) to 716 problems
for A* /LPetr (T + ).
Table 9 also shows the value for the LMC-SEQ LP value (Pommerening et al., 2014). This combination of the landmark constraints and net change constraints in their operator-counting framework is analogous to the combination of our delete-free model with counting constraints, so it is


interesting to compare their optimal LP values. LPec (T ) and LPectr (T ) have a higher average value
than LMC-SEQ on 16 and 15 domains, respectively, while LMC-SEQ has a higher value than both


LPec (T ) and LPectr (T ) on 17 domains. Thus, as with our previous comparison of LM-cut with
LPe (T + ) and LPetr (T + ) in Section 6.2, our delete-relaxation approach seems to be complementary
to the LMC-SEQ combination in the operator-counting framework. On the other hand, comparing
the results of forward search based optimal planning using these LP models, we see that FD/LMC


SEQ has significantly higher coverage than A* /LPec (T ) and A* /LPectr (T ), as well as A* /LPec (T )

and A* /LPectr (T ).

9. Automatic LP Model Selection
From the definitions of the models, we know that for any STRIPS planning task T with action
costs, the relationships among the IP models are as follows: IPtr (T + )  IPetr (T + )  IP(T + ) =

IPe (T + ) = h+  IPc (T ) = IPec (T ). As for the LP relaxations, we know that LP(T + ) 




LPe (T + ), LPetr (T + )  LPe (T + ), LPectr (T )  LPec (T ), and LPectr (T )  LPec (T ). Note that

LPec (T ) does not always dominate LPe (T + ), because the dominated action elimination and immediate action application eliminate different sets of variables in these two LP models. Figure 1
illustrates the dominance relationships among the bounds.

While the time-relaxed LPetr (T + ) and LPectr (T ) are dominated by the non-time-relaxed models

LPe (T + ) and LPec (T ), respectively, the time-relaxed LPs are significantly cheaper to compute than
their non-relaxed counterparts.

Similarly, although IPec (T ) dominates IPe (T + ), it is possible for LPe (T + ) to be larger than

LPec (T ). Furthermore, if two LPs have the same optimal value, the one that can be solved faster is
clearly preferable because the LPs must be solved at each node in the A* search. Thus, we have a set
666

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

airport
barman-opt11
blocks
depot
driverlog
elevators-opt08
elevators-opt11
floortile-opt11
freecell
grid
gripper
logistics98
logistics00
miconic
no-mprime
no-mystery
nomystery-opt11
openstacks
openstacks-opt08
openstacks-opt11
parcprinter-08
parcprinter-opt11
parking-opt11
pathways-noneg
pegsol-08
pegsol-opt11
pipesworld-notankage
pipesworld-tankage
psr-small
rovers
satellite
scanalyzer-08
scanalyzer-opt11
sokoban-opt08
sokoban-opt11
tpp
transport-opt08
transport-opt11
trucks
visitall-opt11
woodworking-opt08
woodworking-opt11
zenotravel

LMC-SEQ

LPe (T + )

1.00
2.23
1.07
1.10
1.04
1.02
1.01
1.05
2.64
1.09
1.00
1.00
1.00
1.00
1.00
1.01
1.03
1.36
1.00
1.00
1.08
1.05
1.00
1.53
1.34
1.33
1.45
1.32
2.60
1.23
1.00
1.00
1.01
1.15
1.11
1.43
1.11
1.08
1.00
1.50
1.04
1.05
1.00

.85
.51
1.00
1.43
1.01
.84
.80
1.01
3.14
1.20
1.00
.91
.99
1.00
.89
.98
1.07
1.61
1.00
1.00
1.00
1.00
1.04
1.13
1.09
1.10
1.18
1.27
1.00
.72
.83
.98
.98
1.01
1.01
.89
.49
.49
1.08
1.42
1.12
1.13
.96

+
LPe
tr (T )

.85
.51
1.00
1.42
.99
.82
.77
1.01
3.07
1.19
1.00
.90
.99
1.00
.78
.90
1.07
1.43
1.00
1.00
1.00
1.00
.99
1.13
1.05
1.10
1.16
1.26
1.00
.72
.75
.94
.98
1.00
1.01
.89
.49
.49
1.08
1.41
1.12
1.13
.94





LPe
c (T )

LPe
ctr (T )

.98
3.59
1.07
1.54
1.12
.71
.67
1.08
3.08
1.55
1.00
1.01
1.00
1.00
.82
.84
1.10
1.61
1.00
1.00
1.08
1.05
1.06
1.72
1.25
1.22
1.73
1.35
2.61
.81
.85
.97
.97
1.13
1.12
1.42
.18
.18
1.08
1.48
1.18
1.19
.94

.98
3.59
1.07
1.54
1.12
.71
.67
1.08
3.07
1.55
1.00
1.01
1.00
1.00
.82
.81
1.10
1.43
1.00
1.00
1.08
1.05
1.00
1.72
1.23
1.22
1.70
1.20
2.61
.81
.75
.93
.97
1.13
1.12
1.42
.18
.18
1.08
1.47
1.18
1.19
.93

Table 9: Optimal values of LP models relative to LM-cut value for 1376 IPC instances. Means for
each domain are shown. E.g., for barman-opt11, the mean LMC-SEQ value was 2.23 times the LM
cut value, the LPe (T + ) and LPetr (T + ) values were 0.51 times the LM-cut value, and the LPec (T )

and LPectr (T ) values were 3.59 times the LM-cut value.

667

fiI MAI & F UKUNAGA

800
700

Instances solved

600
500
400
300

FD/LMC-SEQ
A*/Autoconf
FD/LMC
A*/LPe
ctr(T)
A*/LPetr(T+)
FD/SEQ

200
100
0
0.1

1

10

100

1000

Time (seconds)
(a) Cumulative number of problems solved (out of 1376) vs time (30 minute time limit).

800
700

Instances solved

600
500
400
300

FD/LMC-SEQ
A*/Autoconf

200

FD/LMC
A*/LPe
ctr(T)

100

A*/LPetr(T+)
FD/SEQ

0

1

10

100

1000

10000 100000 1e+06

1e+07

1e+08

Evaluations
(b) Cumulative number of problems solved (out of 1376) vs number of search nodes evaluated (30 minute
time limit).

Figure 8: Comparison of forward search (A* ) planners, part 2.

668

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

of 4 viable LP heuristics, none of which dominate the others when considering both accuracy and
time. The best choice to optimize this tradeoff between heuristic accuracy and node expansion
rate depends on the problem instance. It is difficult to choose the best heuristic a priori because in
general, we do not know (1) whether it is worthwhile to use the counting constraints or not, and (2)
whether the time-relaxation is tight or not for a particular problem instance.
Thus, we implemented a simple mechanism for automatically selecting the LP to be used for

each problem which works as follows: First, we compute LPe (T + ), LPec (T ), LPetr (T + ), and

LPectr (T ) for the problem instance (i.e., at the root node of the A* search). We then select one
based on the following rule: Choose the heuristic with the highest value, and break ties by choosing the heuristic that is cheapest to compute. Although the cheapest heuristic could be identified
according to the CPU time required to compute each heuristic, for many problems, the computations are too fast for robust timing measurements, so we simply break ties in order of LPetr (T + ),


LPectr (T ), LPe (T + ), LPec (T ), because this ordering usually accurately reflects the timing order.
This mechanism makes the simplistic assumption that ranking and behavior of the LP bounds at the
root node will be similar to the ranking of the LP bounds throughout the search graph. A more sophisticated method for heuristic selection may result in better performance (c.f. Domshlak, Karpas,
& Markovitch, 2012), and is an avenue for future work.
9.1 Experimental Results for Automated Model Selection and Comparison with the
State-of-the-Art
We compared A* using our LP-based heuristics, including A* /autoconf, with state-of-the-art heuristics. Specifically, we compared:
 FD/LM-cut : Fast Downward using the landmark cut heuristic (Helmert & Domshlak, 2009)
(the standard seq-opt-lmcut configuration)
 FD/LMC : Fast Downward using an LP-model for the optimal cost partitioning for landmark
cut constraints (Pommerening et al., 2014)
 FD/SEQ : Fast Downward using the lower-bound net change constraints (Pommerening et al.,
2014), corresponding to the state-equation heuristic by Bonet (2013).
 FD/OPT-SYS1, FD/PHO-SYS1, FD/PHO-SYS2 : Fast Downward using optimal cost partitioning constraints for projections on goal variables (OPT-SYS1), and post-hoc optimization
constraints (PHO-SYS1, PHO-SYS2) (Pommerening et al., 2014).
 FD/LMC-SEQ : Fast Downward using both the landmark cut and net change constraints.
 A* /LPe (T + ) : A* with the LP relaxation of the enhanced delete-free IP model IPe (T + )
(Section 4) as a heuristic.
 A* /LPetr (T + ) : A* with the LP relaxation of the time-relaxed, enhanced delete-free IP model
IPe (T + ) as a heuristic.


 A* /LPec (T ) : A* with the LP relaxation of the enhanced delete-free IP model with counting

constraints IPec (T ) as a heuristic.


 A* /LPectr (T ) : A* with the LP relaxation of the time-relaxed, enhanced delete-free IP model

with counting constraints IPec (T ) as a heuristic.
669

fiI MAI & F UKUNAGA

9.1.1 C OVERAGE R ESULTS
The coverage results (number of problems solved) are shown in Tables 10. The time spent at the
root node by A* /autoconf for LP model selection is included in the runtimes, and also counts against
the 30-minute runtime limit. Figures 8a-8b show the cumulative number of instances solved as a
function of the number time and number of node evaluations, respectively (for legibility, only a
subset of the algorithms are included in Figures 8a-8b). Table 11 shows a summary of total coverage
results for all forward-search configurations that are included in Tables 8 and 10.
Our results indicate that automatic LP model selection significantly boosts the performance of
an A* -based planner compared to relying on a single LP model. A* /autoconf achieved a coverage of 761 out of 1376 instances, which is significantly better than its 4 individual components.
Furthermore, A* /autoconf attained higher coverage than all other solver configurations in Table
10 except for FD/LMC-SEQ (Pommerening et al., 2014), which solved 781 instances. Note that
A* /autoconf has higher coverage than FD/LMC-SEQ on 11/43 domains (floortile-opt11, freecell,
grid, logistics98, nomystery-opt11, pathways-noneg, rovers, satellite, trucks, woodworking-opt08,
woodworking-opt11).
9.1.2 ACCURACY OF A* / AUTOCONF M ODEL S ELECTION
We analyzed the accuracy of the model selection by evaluating the performance of A* /autoconf on
each problem instance vs the performance of each of its four component models. If only coverage
is considered, then in 96.4% of the instances, A* /autoconf made the correct decision with respect to
coverage, where the model selection by A* /autoconf was deemed to be correct if either A* /autoconf
solved the problem instance, or none of the 4 components solved the problem instance. On the other
hand, when runtimes are considered as well as coverage, then in 83.0% of the instances, A* /autoconf
made the correct decision, where the selection was deemed to be correct if A* /autoconf selected the
model that had the best runtime (including ties), or none of the 4 components solved the problem

instance. As a baseline, LPectr (T ), which had the best coverage among all of the component models,
is the correct choice according to this criterion 49.9% of the time. Mistakes in the selections made
by A* /autoconf can be seen in Table 10 coverage results  for example, in the woodworking-opt11

domain, A* /autoconf solved 18 instances compared 20 instances solved by LPectr (T ). Thus, there
is significant room for improvement when runtimes are considered in addition to coverage, and
improving the model selection using machine learning techniques is a direction for future work.

10. Discussion and Conclusion
This paper proposed a new, integer-linear programming formulation of the delete relaxation h+
for cost-optimal, domain-independent planning. We started with a basic IP model IP(T + ), and
showed that an enhanced model IPe (T + ), which incorporates landmark-based variable reduction,
relevance analysis, and action elimination, is competitive with previous methods for solving deletefree versions of the standard IPC planning benchmarks tasks (i.e., exact computation of h+ ).
The results of embedding our IP model as the heuristic function in a A* -based forward search
planner confirmed that the plain IP(T + ) model is not practical (coverage of 403/1367 instances
vs. 540 for Fast Downward using hmax ). However, we showed that the IPe (T + ) model, which
uses variable reduction methods to reduce the size of the IP models and exactly computes h+ ,
performed much better, with a coverage of 635 instances. According to the summary results in
670

fiFD/SEQ

A* /LPe (T + )

+
A* /LPe
tr (T )

22
4
28
7
12
11
9
2
15
1
7
4
16
50
21
15
12
7
19
14
15
11
5
4
27
17
14
8
49
6
6
12
9
24
19
6
11
6
6
16
10
5
9
571
10

28
4
27
7
13
19
16
2
15
2
7
5
21
54
21
15
16
7
19
14
17
13
1
4
27
17
16
8
49
6
6
7
4
29
20
6
11
6
7
16
16
11
11
620
12

22
4
28
7
12
10
8
4
39
1
7
4
16
52
20
15
10
7
17
12
28
20
4
4
28
18
15
8
50
6
6
14
11
20
17
8
11
6
9
17
14
9
9
627
12

25
0
28
7
11
13
10
6
44
2
6
6
20
140
18
13
14
7
11
6
20
16
1
5
26
16
12
7
48
7
10
8
5
23
19
6
9
4
15
16
16
10
10
696
5

25
0
28
7
13
13
10
7
43
2
6
6
20
141
17
12
14
7
11
6
20
16
1
5
26
16
13
7
48
7
10
8
5
25
19
6
10
5
15
16
17
11
11
705
6

A* /autoconf

FD/PHO-SYS2

20
4
26
4
10
8
6
2
8
1
6
2
14
45
19
13
8
7
11
6
11
7
1
4
22
12
13
7
48
6
5
10
7
18
15
6
9
4
3
15
8
3
8
462
2



FD/PHO-SYS1

30
4
29
7
13
19
16
6
33
2
6
6
20
141
22
16
12
7
16
11
29
20
2
5
28
18
14
8
50
7
7
14
11
29
20
8
11
6
10
19
21
16
12
781
18

A* /LPe
ctr (T )

FD/OPT-SYS1

28
4
28
7
13
20
16
6
15
2
6
6
20
141
23
16
14
7
19
14
18
13
2
5
27
17
17
8
49
7
7
14
11
28
20
6
11
6
10
10
16
11
12
730
13



FD/LMC-SEQ

28
4
28
7
14
22
18
7
15
2
7
6
20
141
23
16
14
7
19
14
19
14
3
5
27
17
17
8
49
7
7
15
12
30
20
6
11
6
10
11
17
12
13
748
22

25
0
29
7
12
6
4
6
17
2
6
7
20
139
15
11
8
7
6
2
29
20
1
14
22
12
12
7
50
11
9
7
4
22
19
8
6
1
12
17
30
20
10
672
12

25
3
29
7
13
8
6
7
21
3
6
7
20
140
16
11
11
7
10
5
29
20
1
14
26
16
13
7
50
11
9
8
5
26
19
8
6
1
15
17
30
20
10
716
15

25
2
29
7
13
13
10
7
44
3
6
7
20
141
18
12
14
7
11
6
29
20
1
14
26
16
13
7
50
11
10
8
5
25
19
8
10
5
15
17
28
18
11
761
16

A* /LPe
c (T )

FD/LMC

Domain
airport(50)
barman-opt11(20)
blocks(35)
depot(22)
driverlog(20)
elevators-opt08(30)
elevators-opt11(20)
floortile-opt11(20)
freecell(80)
grid(5)
gripper(20)
logistics98(35)
logistics00(28)
miconic(150)
no-mprime(35)
no-mystery(30)
nomystery-opt11(20)
openstacks(30)
openstacks-opt08(30)
openstacks-opt11(20)
parcprinter-08(30)
parcprinter-opt11(20)
parking-opt11(20)
pathways-noneg(30)
pegsol-08(30)
pegsol-opt11(20)
pipesworld-notankage(50)
pipesworld-tankage(50)
psr-small(50)
rovers(40)
satellite(36)
scanalyzer-08(30)
scanalyzer-opt11(20)
sokoban-opt08(30)
sokoban-opt11(20)
tpp(30)
transport-opt08(30)
transport-opt11(20)
trucks(30)
visitall-opt11(20)
woodworking-opt08(30)
woodworking-opt11(20)
zenotravel(20)
Total coverage (1376)
# Best domains

FD/LM-cut

O N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Table 10: Comparison of forward search (A* ) planners, part 2: Number of problems solved with
30 minute, 2GB RAM limit using A* and our IP/LP models as heuristic functions. Includes LP


models that incorporate counting constraints (LPec (T ), LPectr (T ), Section 8), as well as A* /autoconf
(Section 9). Comparison with Fast Downward using operator-counting LP models (Pommerening
et al., 2014).

671

fiI MAI & F UKUNAGA

Configuration
FD/LM-cut

# solved
748

FD/hmax
FD/SEQ
FD/PHO-SYS1
FD/PHO-SYS2
FD/LMC

540
627
571
620
730

FD/OPT-SYS1
FD/LMC-SEQ
A* /HST/CPLEX

462
781
398

A* /IP(T + )
A* /IPe (T + )
A* /LPe (T + )
A* /LPetr (T + )

A* /LPec (T )
*
e
A /LPctr (T )
A* /autoconf

403
635
696
705
672
716
761

Description
Fast Downward (FD) using standard Landmark Cut heuristic
(seq-opt-lmcut)
FD using hmax heuristic
FD using SEQ LP heuristic (Pommerening et al., 2014)
FD using PHO-SYS1 LP heuristic (Pommerening et al., 2014)
FD using PHO-SYS2 LP heuristic (Pommerening et al., 2014)
FD using LP model of optimal cost partitioning on landmark constraints (Pommerening et al., 2014)
FD using OPT-SYS1 LP heuristic (Pommerening et al., 2014)
FD using LMC+SEQ LP heuristic (Pommerening et al., 2014)
hsp planner using A* and h+ heuristic (Haslum et al., 2012; Haslum,
2012)
basic IP formulation for h+
IP(T + ) with all enhancements in Sections 4.1-4.6
LP relaxation of IPe (T + )
LP relaxation of the time-relaxed model IPetr (T + )

LP relaxation of IPec (T )

LP relaxation of the time-relaxed model IPectr (T )
Automated selection of LP at root node(Section 9)

Table 11: Summary of coverage (# solved) on 1376 IPC benchmark problems instances with 30
minute time limit and 2GB RAM (see Tables 8-10 for detailed results)
Table 11, the aggregate coverage of IPe (T + ) is comparable to the coverage obtained by the LPbased SEQ, OPT-SYS1, PHO-SYS1, and PHO-SYS2 heuristics recently implemented using the
operator-counting framework by Pommerening et al. (2014). However, the aggregate coverage on
the IPC benchmarks is skewed by the miconic domain, where SEQ, OPT-SYS1, PHO-SYS1, and
PHO-SYS2 perform particularly poorly compared to other heuristics. If the miconic domain is not
included, then IPe (T + ) is not competitive with these LP-based models. Note that on the freecell
domain, A* with the IPe (T + ) heuristic solved 54/80 instances, which is significantly higher than all
other methods, so there is at least 1 domain where exact h+ computation using the IPe (T + ) model
performs extremely well compared to other state-of-the-art heuristics.
We then showed that the gap between the optimal value of the LP relaxations of our IP models
and h+ tended to be quite small (the gap was often zero), suggesting that the LP relaxations, which
can be computed much faster than the IP models, could be used as a heuristic for A* -based planning.
A time-relaxation that eliminates all time-related constraints was also proposed as another way to
reduce the model in order to be solvable faster. A comparison of our LP-relaxed delete relaxation
models with the LM-cut (Helmert & Domshlak, 2009) heuristic values showed that these approaches
are complementary with respect to how closely they approximate h+ . Thus, the LP-relaxation of
our delete-free models provides a novel, practical alternative to approximating h+ . We showed
that A* search using LPe (T + ) (LP-relaxation of delete-free task) and LPetr (T + ) (time relaxed,
LP-relaxation of delete-free task) significantly improves upon the IP models, solving 696 and 705
instances, respectively, making them usable as practical heuristics.
A major advantage of LP-based heuristics is the relative ease with which additional constraints
can be added in order to obtain improved heuristics. We showed that the counting constraints,
corresponding to the net change constraints proposed in previous work (van den Briel et al., 2007;

Pommerening et al., 2014), could be added to our LP model. The resulting heuristic, LPectr (T )
had mixed results, improving performance on some domains, but degrading performance on other

domains, i.e., LPetr (T + ) and LPectr (T ) are complementary heuristics.
672

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL



Since there is no dominance relationship among A* /LPe (T + ), A* /LPetr (T + ), A* /LPec (T ) and

*
A /LPectr (T ), we proposed A* /autoconf , a simple method which automatically selects among these
4 heuristics by computing all 4 heuristic values at the root node and using the most accurate heuristic
(breaking ties according to speed). We showed that overall, A* /autoconf significantly improves upon
its 4 components, and is competitive with the landmark-cut heuristic, solving 761/1367 instances
and achieving state-of-the-art performance on several domains.
While A* /autoconf has lower total coverage compared to Fast Downward using the LMC-SEQ
LP-based heuristic (Pommerening et al., 2014), the LP(T + )-based approach outperforms LMCSEQ on several domains including freecell, pathways-noneg, rovers, satellite, trucks, and woodworking. Although A* /autoconf includes LP models with counting constraints that consider some
delete effects, note that A* /LPetr (T + ), which uses a pure delete-free LP, performs quite well, obtaining higher coverage than all of the operator-count based heuristics of Pommerening et al. (2014)
in the floortile, freecell, nomystery-opt11, satellite, and trucks domains, so the counting constraints
are not required in order for A* using the delete-relaxation based LPs to achieve state-of-the-art
performance on some domains.
A comparison of the optimal values of our counting-constraint enhanced delete-relaxation LP


models LPec (T ) and LPectr (T ) with the optimal LP values of the LMC-SEQ model showed that
they are complementary, with each class of models outperforming the other on roughly the same
number of domains (Section 8.1). Thus, integrating these two approaches in a single LP model
is a promising direction for future work. In a recent survey of LP-based heuristics for planning,
Roger and Pommerening (2015) noted that our delete-relaxation model can be incorporated into the
operator counting framework of Pommerening et al. (2014) by adding operator-counting variables
for each operator in the delete-relaxed problem  this is a promising direction for future work. Note
that while both Pommerening et al. (2014) and our approach use landmarks, they are used for very
different purposes. The landmark constraints used by Pommerening et al. (2014) are used directly
as operator counting constraints. In contrast, our approach uses landmarks order to decrease the
size of the IP/LP models for the delete-free task and is used for the purpose of speeding up the
computation of the IP/LP models, i.e., landmark based reduction does not change the optimal value
of IP(T + ).

We showed that adding counting constraints that consider some delete effects (i.e., LPec (T ) and

LPectr (T )) can improve performance on some domains, but in some domains, coverage dropped
significantly. This is because the additional constraints make the LP more difficult to solve, so the
increased search efficiency due to the tighter bound is not enough to overcome the increased cost
of solving the LP at each search node. A* /autoconf attempts to address this by selecting the models
with counting constraints only when they return a higher value than the model without counting constraints at the root node, and otherwise uses a model that does not include the counting constraints
(i.e., LPe (T + ) or LPetr (T + )). On the other hand, strengthening the delete-relaxation by considering
some of the delete effects has been an active area of research, and recently, two frameworks that allow flexible interpolation between the delete relaxation and the original model have been proposed.
Keyder, Hoffmann, and Haslum (2014) propose an approach which adds new fluents that represent
conjunctions of fluents in the original planning task. Red-black planning (Domshlak, Hoffmann, &
Katz, 2015) is a framework which separates state variables into two groups  red variables which are
relaxed, and black variables that are not relaxed. Combining these flexible relaxation frameworks
with our IP approach and developing a more principled approach to deciding when to use counting
constraints is an avenue for future work.
673

fiI MAI & F UKUNAGA

Our current implementation uses the CPLEX solver naively, relying entirely on default control
parameters. Systematically tuning and improving the implementation of our IP/LP models in order
to make better use of incremental IP/LP solving capabilities is a promising direction for future work.
Although we have shown that our LP models often compute h+ exactly, there are some domains
where there are significant gaps between h+ and the optimal cost of the LP models. Improved
modeling techniques may allow tighter LP bounds. For example, Constraint C6 uses straightforward
a big-M encoding, and it may be possible to obtain tighter bounds using other methods.
Furthermore, although solving and IP at each node in a forward-search based planner has previously been considered impractical, we have shown that our IPe (T + ) model, which computes h+
exactly, is almost useful as a practical heuristic, and improving the techniques used to solve the IP
for the IPe (T + ) may result in a balance of accuracy and speed necessary for a practical general
purpose heuristic. For example, significant performance improvements might be obtainable by improving the use of the IP solver. For example, in contrast to LP solvers, where parallel speedups are
often difficult to obtain, IP solvers can often be sped up significantly by parallelization, and current
IP solvers already provide parallel search algorithms (which we did not use in this paper because
we limited our experiments to single threads). As the number of cores per processor continues to increase, it is possible that in some cases, IP-based heuristics may become more useful than LP-based
heuristics.

Acknowledgments
Thanks to Patrik Haslum for assistance with his code for computing h+ and his hsp f planner.
Thanks to Florian Pommerening for assistance with the code for the LP heuristic-based Fast Downward (Pommerening et al., 2014). Thanks to the anonymous reviewers for numerous helpful suggestions which significantly improved the paper. This research was supported by a JSPS Grant-in-Aid
for JSPS Fellows and a JSPS KAKENHI grant.

References
Backstrom, C., & Nebel, B. (1995). Complexity Results for SAS+ Planning. Computational Intelligence, 11(4), 625655.
Betz, C., & Helmert, M. (2009). Planning with h+ in theory and practice. In KI 2009, pp. 916.
Springer.
Blum, A., & Furst, M. (1997). Fast Planning Through Planning Graph Analysis. Artificial Intelligence, 90(1-2), 281300.
Bonet, B. (2013). An admissible heuristic for SAS+ planning obtained from the state equation.
In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp.
22682274.
Bonet, B., & Castillo, J. (2011). A complete algorithm for generating landmarks. In Proceedings of
the International Conference on Automated Planning and Scheduling (ICAPS).
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(1-2),
533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. In Proceedings
of the European Conference on Artificial Intelligence (ECAI), pp. 329334.
674

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

Bonet, B., & van den Briel, M. (2014). Flow-based heuristics for optimal planning: Landmarks
and merges. In Proceedings of the International Conference on Automated Planning and
Scheduling (ICAPS).
Bylander, T. (1994). The Computational Complexity of Propositional STRIPS Planning. Artificial
Intelligence, 69(12), 165204.
Bylander, T. (1997). A linear programming heuristic for optimal planning. In Proceedings of the
National Conference on Artificial Intelligence (AAAI), pp. 694699.
Cooper, M. C., de Roquemaurel, M., & Regnier, P. (2011). Transformation of optimal planning
problems. Journal of Experimental & Theoretical Artificial Intelligence, 23(2), 181199.
Dimopoulos, Y. (2001). Improved integer programming models and heuristic search for ai planning.
In Proceedings of 6th European Conference on Planning (ECP), pp. 5057.
Domshlak, C., Karpas, E., & Markovitch, S. (2012). Online speedup learning for optimal planning.
Journal of Artificial Intelligence Research, 44, 709755.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: A new systematic approach
to partial delete relaxation. Artificial Intelligence, 221, 73114.
Gefen, A., & Brafman, R. (2011). The minimal seed set problem. In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS), pp. 319322.
Gefen, A., & Brafman, R. (2012). Pruning methods for optimal delete-free planning. In Proceedings
of the International Conference on Automated Planning and Scheduling (ICAPS), pp. 5664.
Haslum, P. (2012). Incremental lower bounds for additive cost planning problems. In Proceedings
of the International Conference on Automated Planning and Scheduling (ICAPS), pp. 7482.
Haslum, P. (2014a) Personal communication.
Haslum, P. (2014b). Hsp* code and documentatoin http://users.cecs.anu.edu.au/
patrik/un-hsps.html..
Haslum, P., Slaney, J., & Thiebaux, S. (2012). Minimal landmarks for optimal delete-free planning. In Proceedings of the International Conference on Automated Planning and Scheduling
(ICAPS), pp. 353357.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats the difference anyway?. In Proceedings of the International Conference on Automated Planning and
Scheduling (ICAPS), pp. 162169.
Hoffmann, J., & Nebel, B. (2001). The FF Planning System: Fast Plan Generation Through Heuristic Search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal of
Artificial Intelligence Research, 22, 215278.
Imai, T., & Fukunaga, A. (2014). A practical, integer-linear programming model for the deleterelaxation in cost-optimal planning. In Proceedings of the European Conference on Artificial
Intelligence (ECAI).
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI), pp. 17281733.
675

fiI MAI & F UKUNAGA

Katz, M., & Domshlak, C. (2010). Optimal admissible composition of abstraction heuristics. Artificial Intelligence, 174(12-13), 767798.
Kautz, H., & Selman, B. (1992). Planning as Satisfiability. In Proceedings of the European Conference on Artificial Intelligence (ECAI), pp. 359363.
Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic and stochastic search. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp.
11941201.
Kautz, H. A., & Selman, B. (1999). Unifying sat-based and graph-based planning. In Proceedings
of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 318325.
Keyder, E., Richter, S., & Helmert, M. (2010). Sound and complete landmarks for and/or graphs.
In Proceedings of the European Conference on Artificial Intelligence (ECAI), pp. 335340.
Keyder, E., & Geffner, H. (2008). Heuristics for planning with action costs revisited. In Proceedings
of the European Conference on Artificial Intelligence (ECAI), pp. 588592.
Keyder, E. R., Hoffmann, J., & Haslum, P. (2014). Improving delete relaxation heuristics through
explicitly represented conjunctions. Journal of Artificial Intelligence Research, 50, 487533.
Mirkis, V., & Domshlak, C. (2007). Cost-sharing approximations for h+. In Proceedings of the
International Conference on Automated Planning and Scheduling (ICAPS), pp. 240247.
Pommerening, F., & Helmert, M. (2012). Optimal planning for delete-free tasks with incremental LM-cut. In Proceedings of the International Conference on Automated Planning and
Scheduling (ICAPS), pp. 363367.
Pommerening, F., Roger, G., Helmert, M., & Bonet, B. (2014). LP-based heuristics for costoptimal planning. In Proceedings of the International Conference on Automated Planning
and Scheduling (ICAPS).
Pommerening, F., Roger, G., & Helmert, M. (2013). Getting the most out of pattern databases
for classical planning. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI).
Rintanen, J. (2012). Planning as satisfiability: Heuristics. Artificial Intelligence, 193, 4586.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning as satisfiability: parallel plans and algorithms for plan search. Artificial Intelligence, 170(12-13), 10311080.
Robinson, N. (2012). Advancing Planning-as-Satisfiability. Ph.D. thesis, Griffith University.
Robinson, N., McIlraith, S. A., & Toman, D. (2014). Cost-based query optimization via AI planning.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31,
2014, Quebec City, Quebec, Canada., pp. 23442351.
Roger, G., & Pommerening, F. (2015). Linear programming for heuristics in optimal planning. In
AAAI2015 Workshop on Planning, Search, and Optimization.
van den Briel, M. (2015) Personal communication.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). An LP-based heuristic for
optimal planning. In Proceedings of International Conference on Principles and Practice of
Constraint Programming (CP).
676

fiO N A P RACTICAL , I NTEGER -L INEAR P ROGRAMMING M ODEL

van den Briel, M., & Kambhampati, S. (2005). Optiplan: A planner based on integer programming.
Journal of Artificial Intelligence Research, 24, 919931.
van den Briel, M., Vossen, T., & Kambhampati, S. (2008). Loosely coupled formulation for automated planning: An integer programming perspective. Journal of Artificial Intelligence
Research, 31, 217257.
Vossen, T., Ball, M. O., Lotem, A., & Nau, D. S. (1999). On the use of integer programming models
in AI planning. In Proceedings of the International Joint Conference on Artificial Intelligence
(IJCAI), pp. 304309.
Zhu, L., & Givan, R. (2003). Landmark extraction via planning graph propagation. In Proceedings
of ICAPS Doctoral Consortium, pp. 156160.

677

fiJournal of Artificial Intelligence Research 54 (2015) 309-367

Submitted 03/15; published 11/15

PAGOdA: Pay-As-You-Go Ontology Query Answering
Using a Datalog Reasoner
Yujiao Zhou
Bernardo Cuenca Grau
Yavor Nenov
Mark Kaminski
Ian Horrocks

yujiao.zhou@cs.ox.ac.uk
bernardo.cuenca.grau@cs.ox.ac.uk
yavor.nenov@cs.ox.ac.uk
mark.kaminski@cs.ox.ac.uk
ian.horrocks@cs.ox.ac.uk

Department of Computer Science, University of Oxford
Parks Road, Oxford OX1 3QD, United Kingdom

Abstract
Answering conjunctive queries over ontology-enriched datasets is a core reasoning task
for many applications. Query answering is, however, computationally very expensive, which
has led to the development of query answering procedures that sacrifice either expressive
power of the ontology language, or the completeness of query answers in order to improve
scalability. In this paper, we describe a hybrid approach to query answering over OWL 2
ontologies that combines a datalog reasoner with a fully-fledged OWL 2 reasoner in order
to provide scalable pay-as-you-go performance. The key feature of our approach is that
it delegates the bulk of the computation to the datalog reasoner and resorts to expensive
OWL 2 reasoning only as necessary to fully answer the query. Furthermore, although our
main goal is to efficiently answer queries over OWL 2 ontologies and data, our technical
results are very general and our approach is applicable to first-order knowledge representation languages that can be captured by rules allowing for existential quantification and
disjunction in the head; our only assumption is the availability of a datalog reasoner and a
fully-fledged reasoner for the language of interest, both of which are used as black boxes.
We have implemented our techniques in the PAGOdA system, which combines the datalog
reasoner RDFox and the OWL 2 reasoner HermiT. Our extensive evaluation shows that
PAGOdA succeeds in providing scalable pay-as-you-go query answering for a wide range
of OWL 2 ontologies, datasets and queries.

1. Introduction
Ontologies are increasingly used as rich conceptual schemas in a wide range of application
domains (Staab & Studer, 2004). One of the most widely used ontology languages is OWL, a
description logic based language that was standardised by the World Wide Web Consortium
(W3C) in 2004 and revised (as OWL 2) in 2009 (Baader, Calvanese, McGuinness, Nardi,
& Patel-Schneider, 2003; Horrocks, Patel-Schneider, & van Harmelen, 2003; Cuenca Grau,
Horrocks, Motik, Parsia, Patel-Schneider, & Sattler, 2008). An OWL ontology consists of a
set of axioms, which correspond to first-order sentences containing only unary and binary
predicates (called classes and properties in OWL), with the structure of axioms/sentences
being restricted to ensure the decidability of basic reasoning problems.
In some applications, the main focus is on the conceptual model itself, with class subsumption being a key reasoning problem. In an increasing number of applications, however,
the main focus is on using the conceptual model to access data, often in the form of an RDF
c 2015 AI Access Foundation. All rights reserved.

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

graph (Manola & Miller, 2004). In such data-centric applications a key reasoning problem is
to answer conjunctive queries (CQs)sentences constructed from function-free atoms using
conjunction and existential quantification only (Abiteboul, Hull, & Vianu, 1995)which
constitute the core component of standard query languages such as SQL and SPARQL
(W3C SPARQL Working Group, 2013).
Conjunctive query answering over ontology-enriched datasets is, however, of high worstcase complexity (Glimm, Lutz, Horrocks, & Sattler, 2008; Eiter, Ortiz, & Simkus, 2012),
even when measured only with respect to the size of the data (so called data complexity).
Although heavily optimised, existing systems for query answering with respect to (RDF)
data and an unrestricted OWL 2 ontology can process only small to medium size datasets
(Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2007; Moller, Neuenstadt, Ozcep, &
Wandelt, 2013; Wandelt, Moller, & Wessel, 2010; Kollia & Glimm, 2013). This has led
to the development of query answering procedures that sacrifice expressive power of the
ontology language or the completeness of query answers in order to improve scalability.
In the former case (sacrificing expressive power), query answering procedures have been
developed for various fragments of OWL 2 for which conjunctive query answering is tractable
with respect to data complexity, and three such fragments were standardised as so-called
profiles in OWL 2 (Motik, Cuenca Grau, Horrocks, Wu, Fokoue, & Lutz, 2012). The OWL 2
QL and OWL 2 EL profiles are based on the DL-Lite (Calvanese, De Giacomo, Lembo,
Lenzerini, & Rosati, 2007) and EL (Baader, Brandt, & Lutz, 2005) families of description
logics; the OWL 2 RL profile corresponds to a fragment of the rule-based language datalog
(Grosof, Horrocks, Volz, & Decker, 2003; Dantsin, Eiter, Gottlob, & Voronkov, 2001).
Conjunctive query answering systems for such profiles have been shown to be highly scalable
in practice (Bishop, Kiryakov, Ognyano, Peikov, Tashev, & Velkov, 2011; Wu, Eadon, Das,
Chong, Kolovski, Annamalai, & Srinivasan, 2008; Motik, Nenov, Piro, Horrocks, & Olteanu,
2014; Erling & Mikhailov, 2009; Rodriguez-Muro & Calvanese, 2012; Lutz, Seylan, Toman,
& Wolter, 2013; Stefanoni, Motik, & Horrocks, 2013). The more favourable computational
properties of these fragments make them a natural choice for data-intensive applications,
but they also come at the expense of a loss in expressive power, and many ontologies used
in applications are not captured by any of the profiles.
In the latter case (sacrificing completeness), query answering procedures have been
developed that exploit scalable reasoning techniques, but at the expense of computing only
approximate query answers (Thomas, Pan, & Ren, 2010; Tserendorj, Rudolph, Krotzsch,
& Hitzler, 2008; Wandelt et al., 2010; Bishop et al., 2011). In most cases, the computed
answers are sound (only correct answer tuples are identified) but incomplete (some correct
answer tuples may not be identified). One way to realise such a procedure is to weaken
the ontology until it falls within one of the OWL 2 profiles, and then to use a scalable
procedure for the relevant fragment. The required weakening can be trivially achieved
simply by discarding (parts of) out-of-profile axioms, but more sophisticated techniques may
try to reduce or even minimise information loss (Console, Mora, Rosati, Santarelli, & Savo,
2014). Such an approach is clearly sound (if an answer tuple is entailed by the weakened
ontology, then it is entailed by the original ontology), but incomplete in general, and for
ontologies outside the relevant profile, the answer returned by such systems can therefore
be understood as providing a lower-bound on the correct answer; however, such procedures

310

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

cannot in general provide any complementary upper bound or even any indication as to
how complete the computed answer is (Cuenca Grau, Motik, Stoilos, & Horrocks, 2012).
In this paper, we describe a novel hybrid approach to query answering that combines a
scalable datalog (or OWL 2 RL) reasoner with a fully-fledged OWL 2 reasoner to provide
scalable performance while still guaranteeing sound and complete answers in all cases. Our
procedure uses the datalog reasoner to efficiently compute both lower bound (sound but
possibly incomplete) and upper bound (complete but possibly unsound) answers to the input query. If lower and upper bound answers coincide, they obviously provide a sound and
complete answer. Otherwise, relevant subsets of the ontology and data are computed that
are guaranteed to be sufficient to test the correctness of tuples in the gap between the
lower and upper bounds. These subsets are computed using only the datalog reasoner, and
they are typically much smaller than the input ontology and data. Finally, the fully-fledged
reasoner is used to check gap tuples w.r.t. the relevant subset. As this can still be computationally expensive, the load on the fully-fledged reasoner is further reduced by exploiting
summarisation techniques inspired by the SHER system to quickly identify spurious gap
tuples (Dolby, Fokoue, Kalyanpur, Kershenbaum, Schonberg, Srinivas, & Ma, 2007; Dolby,
Fokoue, Kalyanpur, Schonberg, & Srinivas, 2009), and by analysing dependencies between
remaining gap tuples to reduce the number of checks that need to be performed.
The key feature of our approach is its pay-as-you-go behaviour: the bulk of the computational workload is delegated to the datalog reasoner, and the extent to which the
fully-fledged reasoner is needed does not depend solely on the ontology, but on interactions
between the ontology, the dataset and the query. Thus, even when using a very expressive
ontology, queries can often be fully answered using only the datalog reasoner, and even
when the fully-fledged reasoner is required, relevant subset extraction, summarisation and
dependency analysis greatly reduce the number and size of reasoning problems. Moreover,
our approach has the additional advantage that lower bound answer tuples can be quickly
returned, even in cases where completion of the answer requires more time consuming computations. Finally, although our main goal is to efficiently answer queries over OWL 2
ontologies and datasets, our technical results are very general and our approach is not
restricted to ontology languages based on description logics. More precisely, given a KR
language L that can be captured by first-order rules allowing for existential quantification
and disjunction in the head, and over which we want to answer conjunctive queries, our
only assumption is the availability of a fully-fledged reasoner for L and a datalog reasoner,
both of which are used as a black box.
We have implemented our techniques in the PAGOdA system1 using RDFox as a datalog
reasoner (Motik et al., 2014) and HermiT as a fully-fledged OWL 2 reasoner (Glimm,
Horrocks, Motik, Stoilos, & Wang, 2014),2 and conducted an extensive evaluation using a
wide range of realistic and benchmark datasets and queries. This evaluation suggests that
our techniques are eective at providing scalable pay-as-you-go query answering: in our tests
of more than 4,000 queries over 8 ontologies, none of which is contained within any of the
OWL profiles, more than 99% of queries were fully answered without resorting to the fullyfledged reasoner. Moreover, even when the fully-fledged reasoner was used, relevant subset
1. http://www.cs.ox.ac.uk/isg/tools/PAGOdA/
2. Although our techniques are proved correct for general conjunctive queries, in practice we are limited by
the current query capabilities of OWL 2 reasoners.

311

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

extraction, summarisation and dependency analysis greatly reduced the number and size of
reasoning problems: in our tests, the size of the dataset was typically reduced by an order
of magnitude, and often by several orders of magnitude, and it seldom required more than a
single test to resolve the status of all gap tuples. Taken together, our experiments show that
PAGOdA can provide an efficient conjunctive query answering service in scenarios requiring
both expressive ontologies and datasets containing hundreds of millions of facts, something
that is far beyond the capabilities of pre-existing state-of-the-art ontology reasoners.
The remainder of the paper is organised as follows. In Section 2 we introduce key
concepts and definitions. In Section 3 we present a high-level overview of our approach.
In Section 4 we describe how lower bound answers are computed and prove that they are
sound, and in Section 5 we describe how upper bound answers are computed and prove
that they are complete. In Section 6 we present our technique for reducing the size of
the ontology and dataset to be processed by the fully-fledged reasoner and prove that it
preserves completeness. In Section 7 we present our summarisation and dependency analysis
optimisations and prove that they too preserve completeness. In Section 8 we describe the
implementation of our techniques in the PAGOdA system and discuss some additional
optimisations. Finally, after positioning our work within the state-of-the-art in Section 9,
we present our extensive evaluation in Section 10, and draw our conclusions in Section 11.

2. Preliminaries
In this section we briefly introduce rule-based first-order languages and description logics
(DLs)a family of knowledge representation formalisms underpinning the OWL and OWL 2
ontology languages (Baader et al., 2003).
We use standard notions from first-order logic such as constant, predicate, function,
term, substitution, atom, formula, and sentence. We also adopt standard definitions of
(Herbrand) interpretation and model, as well as of (un)satisfiability and entailment (written
|=) of sets of first-order sentences. We denote with ? the nullary predicate that is false in
all interpretations. Formulas may also contain the special equality predicate . We assume
that each first-order knowledge base F over a function-free signature that uses  axiomatises
its semantics in the usual way; that is, F must contain the following first-order sentences,
where (EQ1) and (EQ4) are instantiated for each n-ary predicate P in F and each 1  i  n:
8x1 , . . . , xn (P (x1 , . . . , xi , . . . , xn ) ! xi  xi )

(EQ1)

8x, y(x  y ! y  x)

8x, y, z(x  y ^ y  z ! x  z)

8x1 , . . . , xn , y(P (x1 , . . . , xi , . . . , xn ) ^ xi  y ! P (x1 , . . . , xi

(EQ2)
(EQ3)
1 , y, xi+1 , . . . , xn ))

(EQ4)

Finally, we will also exploit the following notion of homomorphism applicable to sets
of atoms, formulas and substitutions. Given sets of ground atoms S and T , we define
a homomorphism from S to T as a mapping  from ground terms to ground terms s.t.
 (c) = c for any constant c in S, and P (t1 , . . . , tn  ) 2 T for each atom P (t1 , . . . , tn ) 2 S.
The application of a homomorphism can be naturally extended to ground atoms, ground
formulas and ground substitutions, e.g. for an atom  = P (t1 , . . . , tn ),  = P (t1 , . . . , tn  )
and for a ground substitution ,  is the substitution {x 7! x  | x 2 dom( )}.
312

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

2.1 Rule-Based Knowledge Representation
Rule languages are well-known knowledge representation formalisms which are strongly connected with ontology languages (Dantsin et al., 2001; Cal, Gottlob, Lukasiewicz, Marnette,
& Pieris, 2010; Bry, Eisinger, Eiter, Furche, Gottlob, Ley, Linse, Pichler, & Wei, 2007).
We define a fact as a function-free ground atom and a dataset as a finite set of facts. A
rule r is a function-free first-order sentence of the form
8~x, ~y (
where each

x, ~y )
i (~

x, ~y )
1 (~

^  ^

x, ~y )
n (~

!

m
_

i=1

9~zi 'i (~x, ~zi ))

(1)

is an atom dierent from ? with free variables in ~x [ ~y , and either

 m = 1 and '1 (~x, ~z1 ) = ?, or
 m 1, and for each 1  i  m the formula 'i (~x, ~zj ) is a conjunction of atoms dierent
from ? with free variables in ~x [ ~zj .
The conjunction
of atoms 1 (~x, ~y ) ^    ^ n (~x, ~y ) is the body of r, denoted by body(r). The
W
formula m
9~
z
'
x, ~zi ) is the head of r, denoted by head(r). We assume that rules are
i i (~
i=1
safe; that is, every variable in ~x is mentioned in body(r). For brevity, universal quantifiers
are omitted in rules.
Rules of this form are very general and are able to capture most first-order rule languages
for knowledge representation, including datalog (Abiteboul et al., 1995), existential rules
and datalog (Cal et al., 2010), as well as datalog,_ (Alviano, Faber, Leone, & Manna,
2012b; Bourhis, Morak, & Pieris, 2013).
We say that a rule r is
 disjunctive datalog if head(r) contains no existential quantifiers or conjunction;
 existential if m = 1; and
 datalog if it is disjunctive datalog and m = 1.
A knowledge base K = K [ DK consists of a finite set of rules K and a dataset DK where
each predicate in DK is assumed to occur in K .
In order to simplify the presentation of our technical results, we sometimes restrict
ourselves to knowledge bases in a particular normal form, which we specify next. We say
that a rule r is normalised if it is of one of the following forms, where m
1 and each
x, ~zi ) is a single atom dierent from ?:
i (~
x, ~y )
1 (~
x, ~y )
1 (~
x, ~y )
1 (~

^  ^

x, ~y )
n (~

^  ^

x, ~y )
n (~

^  ^

x, ~y )
n (~

!?

(2)

! 9~z1 1 (~x, ~z1 )
!

x)
1 (~

_  _

(3)
x)
m (~

(4)

A knowledge base K [ DK is normalised if all rules in K are normalised. The restriction
to normalised knowledge bases is w.l.o.g. since every set of rules  of the form (1) can be
transformed in polynomial time into a set of normalised rules norm() that is a conservative
extension of  as given next. For each rule r 2  and each 1  i  m, let ~xi be the tuple of
313

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

free variables in the subformulas 9~zi 'i (~x, ~zi ) of head(r), we then have ~xi  ~x. Furthermore,
let E'i be fresh predicates of arity |~xi | and let C'i be fresh predicates of arity |~xi | + |~zi |
uniquely associated to r and i. Then, norm() consists of the following rules:3

x, ~y )
1 (~

^  ^

x, ~y )
n (~

!

m
_

E'i (~xi ),

(5)

i=1

E'i (~xi ) ! 9~zi C'i (~xi , ~zi ) for each 1  i  m,
C'i (~xi , ~zi ) !

for each 1  i  m and each atom

'i (~x, ~zi ) ! E'i (~xi ) for each 1  i  m,

'i (~x, ~zi ) ! C'i (~xi , ~zi ) for each 1  i  m.

(6)
in 'i (~x, ~zi ),

(7)
(8)
(9)

We frequently use Skolemisation to interpret rules in Herbrand interpretations. For each
rule r of the form (1) and each existentially quantified variable zij , let fijr be a function
symbol globally unique for r and zij of arity ~x. Furthermore, let sk be the substitution
such that sk (zij ) = fijr (~x) for each zij 2 ~zi . The Skolemisation sk(r) of r is the following
first-order sentence, which by slight abuse of notation we refer to as a Skolemised rule:
x, ~y )
1 (~

^  ^

x, ~y )
n (~

!

m
_

'i (~x, ~zi )sk

i=1

The Skolemisation sk() of a set of rules  is obtained by Skolemising each individual rule
in . We extend the definitions of head and body of rules to Skolemised rules naturally. It
is well-known that Skolemisation is an entailment-preserving transformation.
2.2 Description Logics and Ontology Languages
We next present a brief overview of the DLs underpinning the W3C standard ontology
language OWL 2 (Horrocks, Kutz, & Sattler, 2006; Cuenca Grau et al., 2008). Typically, the
predicates in DL signatures are restricted to be unary or binary; the former are called atomic
concepts, whereas the latter are typically referred to as atomic roles. DLs typically provide
two special concepts ? (the bottom concept) and > (the top concept), which are mapped
by every interpretation to the empty set and the interpretation domain, respectively.
Every OWL 2 DL ontology can be normalised as a set of axioms of the form given
on the left-hand-side of Table 1 (Motik, Shearer, & Horrocks, 2009).4 Thus, w.l.o.g., we
define an OWL 2 DL ontology as a finite set of axioms of the form (O1)(O13) in Table 1.
Every OWL 2 DL ontology must satisfy certain additional requirements in order to ensure
decidability of reasoning (Horrocks et al., 2006). These restrictions, however, are immaterial
to our technical results.
Each normalised axiom corresponds to a single rule, as given on the right-hand-side of
Table 1. Concept ? is translated as the special nullary predicate ?, whereas > is translated
3. Although rules (5)(7) are sufficient to express  in normal form, we also introduce rules (8)(9) in order
to facilitate the computation of upper bound query answers (see Sections 5.2 and 5.3).
4. For convenience, we omit axioms of the form A v n R.B as they can be simulated by A v 9R.Bi ,
Bi v B and Bi u Bj v ? for 1  i < j  n where each Bi is a fresh concept.

314

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Axioms
dn
Ai v F
?
di=1
n
m
A
v
i=1 i
j=1 Bj
9R.A v B
A v Self(R)
Self(R) v A
R vS
R vS
R S vT
RuS v?
A v 9R.B
A v  m R.B
A v {a}
> v 8R.A

Rules
Vn
Ai (x) ! W
?
Vi=1
n
m
A
(x)
!
i=1 i
j=1 Bj (x)
R(x, y) ^ A(y) ! B(x)
A(x) ! R(x, x)
R(x, x) ! A(x)
R(x, y) ! S(x, y)
R(x, y) ! S(y, x)
R(x, z) ^ S(z, y) ! T (x, y)
R(x, y) ^ S(x, y) ! ?
A(x) ! 9y(R(x, y) ^ B(y))
V
W
A(x) ^ m+1
i=1 [R(x, yi ) ^ B(yi )] !
1i<jm+1 yi  yj
A(x) ! x  a
R(x, y) ! A(y)

(O1)
(O2)
(O3)
(O4)
(O5)
(O6)
(O7)
(O8)
(O9)
(O10)
(O11)
(O12)
(O13)

Table 1: Normalised DL axioms and their translation into rules where n, m > 0, A and B
are atomic concepts or >, and R, S, T are atomic roles.
as an ordinary unary predicate, the meaning of which is axiomatised. Let  be the function
that maps an OWL 2 axiom  to its corresponding rule as in Table 1, and let O be an
ontology. Then, (O) is the smallest knowledge base containing:
 () for each  2 O;
 a rule A(x) ! >(x) for each atomic concept A in O; and
 rules R(x, y) ! >(x) and R(x, y) ! >(y) for each atomic role R in O.
Note that since (O) is a knowledge base, it must contain the axioms of equality for its
signature whenever  is required to translate an axiom in O.
In recent years, there has been a growing interest in ontology languages with favourable
computational properties, which has led to the standardisation of the RL, QL, and EL
profiles of OWL 2 (Motik et al., 2012). We say that an ontology is Horn if m = 1 in all
axioms (O2) and (O11). Additionally, we say that a Horn ontology is
 RL if it does not contain axioms (O4), (O5), or (O10).
 QL if it does not contain axioms (O4), (O5), (O8), (O9), (O11), and (O12); furthermore, all axioms (O1) and (O2) satisfy n  2 and all axioms (O3) satisfy A = >.
 EL if it does not contain axioms (O7), (O9) or (O11). Additionally, we say that an
EL ontology is ELHOr? if it does not contain axioms (O4), (O5) and (O8).
2.3 Conjunctive Queries
A conjunctive query (CQ) is a formula q(~x) of the form 9~y '(~x, ~y ), where '(~x, ~y ) is a
conjunction of function-free atoms. A query is Boolean if |~x| = 0, and it is atomic if '(~x, ~y )
315

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

consists of a single atom and |~y | = 0. For simplicity, we sometimes omit the free variables
and write q instead of q(~x).
Let K be a knowledge base. A tuple ~a of constants is a possible answer to q(~x) w.r.t. K
if it is of the same arity as ~x and each constant in ~a occurs in K. Furthermore, we say that a
possible answer ~a is a certain answer if K |= q(~a); the set of such certain answers is denoted
by cert(q, K). Note that, if '(~x, ~y ) is Boolean, the set of certain answers is either empty
or it consists of a tuple of length zero. We treat unsatisfiability as a Boolean query where
'(~x, ~y ) is the nullary falsehood symbol ?; this query holds w.r.t. K i K is unsatisfiable.
CQs can be alternatively represented using datalog rules. To this end, each query q(~x)
is uniquely associated with a predicate Pq of arity |~x| (where we take P? = ?) and a set
Rq of rules defined as follows:

;
q=?
Rq =
(10)
{'(~x, ~y ) ! Pq (~x)} otherwise
Then, ~a 2 cert(q, K) i K [ Rq |= Pq (~a). In this way, certain answers can be characterised
by means of entailment of single facts.
Answering CQs w.r.t. knowledge bases can be computationally very hard, and decidability for knowledge bases stemming from OWL 2 DL ontologies remains open. Decidability
can be obtained by ensuring that the ontology stays within one of the standardised profiles
of OWL 2. This restriction also ensures tractability with respect to data complexity, which
makes the profiles a natural choice of ontology language for data-intensive applications.
The standard language SPARQL 1.1 (W3C SPARQL Working Group, 2013) allows users
to formulate CQs over OWL 2 ontologies; however, to ensure decidability and reduce the
complexity of query answering, CQs are interpreted in SPARQL 1.1 under ground semantics.
We say that a possible answer ~a to q(~x) = 9~y '(~x, ~y ) is a ground answer w.r.t. a satisfiable
knowledge base K if there exists a tuple ~e of constants in K such that K |= '(~a, ~e). Clearly,
every ground answer is a certain answer but not vice versa. We denote with ground(q, K)
the set of ground answers to q w.r.t. K.
Many reasoning systems currently support SPARQL 1.1 and hence compute ground(q, K)
when given a CQ q and an OWL 2 DL ontology K as input. Additionally, most systems are
able to compute all certain answers if q is suitably restricted. More precisely, we say that q is
internalisable if Kq = K [ Rq corresponds to an OWL 2 DL knowledge base. Internalisation
amounts to transforming the query into an ontology axiom and it is typically referred to as
rolling-up in the DL literature (Horrocks & Tessaris, 2000).
In this paper, we focus on the general problem of computing all certain answers of a CQ
w.r.t. a knowledge base K, and all our theoretical results are generally applicable regardless
of the rule-based language in which K is expressed.
2.4 Hyperresolution
Reasoning over knowledge bases can be realised by means of the hyperresolution calculus
(Robinson & Voronkov, 2001), which we briefly discuss next. In our treatment of hyperresolution we consider standard basic notions in theorem proving such as (ground) clause
and most general unifier (MGU). Furthermore, we treat disjunctions of ground atoms as
sets and hence we do not allow for duplicated atoms in a disjunction. We assume that ?
316

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

does not occur in clauses and denote with  the empty clause. The Skolemisation sk(r)
of a normalised rule r is logically equivalent to the clause containing each atom dierent
from ? in head(sk(r)) and the negation of each atom in body(sk(r)), so we sometimes abuse
notation and use sk(r) to refer to a Skolemised rule or its corresponding clause.
Let C =  1 _    _  n _ 1 _    _ m be a clause, where each i and j are atoms
(possibly containing functional terms). Furthermore for each 1  i  n, let i = i _ i be
a positive ground clause. Finally, let be a MGU of all pairs i , i , 1  i  n. Then, the
positive ground clause 1 _    _ m _ 1 _    _ n is a hyperresolvent of C and 1 , . . . , n .
The inference is called a hyperresolution step, where the clause C is the main premise.
Let K = K [ DK be a normalised knowledge base and let C be a positive ground clause.
A derivation of C from K is a pair  = (T, ) where T is a tree, is a labeling function
that maps each node in T to a ground clause, and for each v in T :
(1)

(v) = C if v is the root;

(2)

(v) 2 DK if v is a leaf; and

(3) if v has children w1 , . . . , wn , then (v) is a hyperresolvent of sk(r) and (w1 ), . . . , (wn )
for a rule r 2 K .
The support of , written support(), is the set of facts and rules participating in hyperresolution steps in . We write K ` C to denote that there is a hyperresolution derivation of
C from K. Hyperresolution is sound and complete: K is unsatisfiable if and only if K ` ;
furthermore, if K is satisfiable then K `  i K |=  for any ground atom .
2.5 The Skolem Chase
Answering CQs over a knowledge base K = K [ DK where K consists only of existential
rules can be realised using the chase technique (Abiteboul et al., 1995; Cal, Gottlob, &
Kifer, 2013). In this paper, we use the Skolem chase variant (Marnette, 2009; Cuenca Grau,
Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang, 2013).
The Skolem chase sequence of K is the sequence of sets of ground atoms {B i }i 0 , where
0
B = DK , and B i+1 is inductively defined as follows:
B i+1 = B i [ {head(sk(r)) | r 2 K ,

a substitution, and B i |= body(r) }.
S
The Skolem chase of K, written as ChaseK , is defined as i 0 B i .
The key property of the Skolem chase is that it computes a universal Herbrand model
of K, which can be used as a database for answering CQs. Formally, K is satisfiable i
?2
/ ChaseK ; furthermore, if K is satisfiable, then ChaseK is homomorphically embeddable
into every Herbrand model of K (seen as a set of atoms). It follows that for K satisfiable
and q a Boolean CQ we have K |= q i ChaseK |= q.
Note that ChaseK might contain infinitely many atoms. If K is datalog, however,
ChaseK is guaranteed to be finite and it contains precisely all facts logically entailed by K.
In this case, we often refer to ChaseK as the materialisation of K.

317

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

3. Overview
In this section we provide a high-level overview of our approach to conjunctive query answering. We assume the availability of two reasoners:
 a datalog reasoner that is sound and complete for answering conjunctive queries over
datalog knowledge bases; and
 a fully-fledged reasoner that is sound and complete for answering a given class of
conjunctive queries Q (which includes the unsatisfiability query) w.r.t. knowledge
bases in a given ontology language L.
We will describe our approach in its most general form, where we make no assumptions
about the two reasoners, treating them as black-box query answering procedures.
The kind of queries and knowledge bases that can be dealt with using this approach
ultimately depends on the capabilities of the fully-fledged reasoner. For instance, OWL
2 DL reasoners can typically process arbitrary OWL 2 DL knowledge bases; however, the
query language is limited to internalisable queries. In turn, the scalability of our approach
ultimately depends on how much of the reasoning workload can be delegated to the datalog
reasoner; our goal is to delegate the bulk of the computation to the datalog reasoner and
to restrict the (expensive) use of the fully-fledged reasoner to the bare minimum.
Here, and in the rest of this paper, we fix an arbitrary normalised knowledge base
K = K [ DK . Given an arbitrary query q (which may be the special unsatisfiability query)
containing only symbols from K, the core of our approach relies on exploiting the datalog
reasoner for accomplishing the following tasks:
 Lower and Upper Bound Computation, where we exploit the datalog reasoner
to compute both a lower bound Lq and an upper bound U q to the certain answers
to q w.r.t. K. If these bounds match (i.e. Lq = U q ), then the query has been fully
answered by the datalog reasoner; otherwise, the dierence Gq = U q \ Lq provides
a set of gap answers that need to be verified using the fully-fledged reasoner. The
relevant techniques for computing these bounds are described in Sections 4 and 5.
 Knowledge Base Subset Computation, where we exploit the datalog reasoner to
compute a (hopefully small) subset Kq of K that is sufficient to check if answers in Gq
are in cert(q, K); that is, ~a 2 cert(q, K) i ~a 2 cert(q, Kq ) for each ~a 2 Gq . The details
on how to compute such Kq are given in Section 6.
We then proceed according to the following steps when given a query q:
Step 1. Check satisfiability of K.
(a) Compute bounds L? and U ? for the unsatisfiability query ?. If L? 6= ;, then
terminate and report that K is unsatisfiable. If U ? = ;, then proceed to Step 2
(K is satisfiable).
(b) Compute the subset K? of K.

(c) Use the fully-fledged reasoner to check the satisfiability of K? . To minimise the
computational workload of the fully-fledged reasoner, we proceed as follows:
318

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

i. Construct a summary of K? (See Section 7), and use the fully-fledged reasoner to check if it is satisfiable; if it is, proceed to Step 2 (K is satisfiable).
ii. Use the fully-fledged reasoner to check the satisfiability of K? ; if it is unsatisfiable, then terminate and report that K is unsatisfiable. Otherwise,
proceed to Step 2 (K is satisfiable).
Step 2. Compute bounds Lq and U q . If Gq = ;, then terminate and return Lq . Otherwise,
proceed to Step 3.
Step 3. Compute the subset Kq of K.
Step 4. For each ~a 2 Gq , use the fully-fledged reasoner to check whether Kq |= q(~a). To
minimise the computational workload, this step is carried out as follows:
(a) Construct a summary Kq  of Kq (see Section 7). For each ~a 2 Gq , use the
fully-fledged reasoner to check whether ~a is a certain answer to q w.r.t. the
summary Kq , and remove ~a from Gq if it is not the case.
(b) Compute a dependency relation between the remaining answers in Gq s.t. if ~b
depends on ~a and ~a is a spurious answer, then so is ~b. (See Section 7).
(c) Remove any remaining spurious answers from Gq , where an answer is spurious
if it is not entailed by Kq or if it depends on a spurious answer; use the fullyfledged reasoner to check relevant entailments, arranging checks by heuristics
w.r.t. the dependency relation.
Step 5. Return Lq [ Gq .
In the following sections, we describe these steps formally. We will also introduce a
number of improvements and optimisations, some of which rely on the additional assumption
that the datalog reasoner is materialisation-basedthat is, for a datalog knowledge base K0
and query q 0 , it computes query answers cert(q 0 , K0 ) by first computing the materialisation
ChaseK0 and then evaluating q 0 over the resulting materialisation. This is a reasonable
assumption in practice since most datalog reasoners in Semantic Web applications (e.g.,
OWLim, RDFox, Oracles native inference engine) are materialisation-based. In such cases,
we further assume that we have direct access to the materialisation. Our PAGOdA system
combines HermiT with the materialisation-based reasoner RDFox, and hence is able to
exploit all of the improvements and optimisations described below; the realisation of our
approach in PAGOdA is discussed in detail in Section 8.
We will illustrate all our techniques using a running example consisting of the knowledge
base Kex = Kex [ DKex and the query qex (x) given in Table 2. Note that rules (R6) and
(R8) in Kex are not normalised; however, they can be easily brought into normal form by
introducing fresh binary predicates eatsH and eatsL as follows:
MeatEater(x) ! 9y eatsH (x, y) (R6a)

eats(x, y) ^ Herbivore(y) ! eatsH (x, y)
eatsH (x, y) ! eats(x, y)

eatsH (x, y) ! Herbivore(y)

(R6b)
(R6c)

(R6d)

319

Folivore(x) ! 9y eatsL (x, y) (R8a)

eats(x, y) ^ Leaf(y) ! eatsL (x, y)
eatsL (x, y) ! eats(x, y)
eatsL (x, y) ! Leaf(y)

(R8b)

(R8c)
(R8d)

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Mammal(tiger)

(D1)

Mammal(wolf )

(D6)

Mammal(howler)

(D11)

Mammal(lion)

(D2)

MeatEater(wolf )

(D7)

MeatEater(python)
eats(python, rabbit)

(D3)
(D4)

eats(wolf , sheep)
Herbivore(sheep)

(D8)
(D9)

Folivore(howler)
Mammal(a hare)

(D12)
(D13)

Folivore(a hare)

(D14)

Herbivore(rabbit)

(D5)

eats(sheep, grass)

(D10)

eats(a hare, willow)

(D15)

Carnivore(x) ! Mammal(x)

Herbivore(x) ! Mammal(x)

Folivore(x) ^ MeatEater(x) ! ?

Herbivore(x) ^ eats(x, y) ! Plant(y)

(R1)
(R2)
(R3)
(R4)

Mammal(x) ! Herbivore(x) _ MeatEater(x)

(R5)

Mammal(x) ! 9y eats(x, y)

(R7)

MeatEater(x) ! 9y[eats(x, y) ^ Herbivore(y)]
Folivore(x) ! 9y[eats(x, y) ^ Leaf(y)]
Leaf(x) ! Plant(x)

(R6)
(R8)
(R9)

qex (x) = 9y[eats(x, y) ^ Plant(y)]

Table 2: Running example knowledge base Kex and query qex (x). The set Kex consists of
rules (R1)(R9), the dataset DKex consists of the facts (D1)(D15).
Our core techniques described in Sections 4-6 are applicable to any knowledge base
and query. In order to simplify the presentation of our definitions and technical results of
those sections we fix, in addition to the knowledge base K = K [ DK , an arbitrary query
q(~x) = 9~y '(~x, ~y ) (which may be the unsatisfiability query ?).

4. Lower Bound Computation
A straightforward way to compute lower bound answers using the datalog reasoner is to
evaluate q w.r.t. the datalog subset of K consisting of all facts in DK and datalog rules in
K . In the case of OWL 2 ontologies, this amounts to considering the subset of OWL 2
RL axioms in the ontology. By the monotonicity property of first-order logic all certain
answers w.r.t. such subset are also certain answers w.r.t. K. Furthermore, if the subset is
unsatisfiable, then so is K.
Example 4.1. The datalog subset of our example Kex consists of rules (R1)(R4) and
(R9), together with all facts (D1)(D15). The materialisation of the datalog subset of
Kex results in the following dataset: Dex [ {Mammal(rabbit), Mammal(sheep), Plant(grass)}
When evaluating qex (x) against the materialisation we obtain sheep as an answer.
}
This basic lower bound can be rather imprecise in practice since rules featuring disjunction or existential quantification typically abound in OWL 2 DL ontologies. To improve

320

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

this bound, we exploit techniques that allow us to deterministically derive (also via datalog
reasoning) additional consequences from K that do not follow from its datalog subset.
4.1 Dealing with Disjunctive Rules: Program Shifting
To deal with disjunctive rules, we adopt a variant of shiftinga polynomial program transformation commonly used in Answer Set Programming (Eiter, Fink, Tompits, & Woltran,
2004). We next illustrate the intuition behind this transformation with an example.
Example 4.2. Let us consider the information in Kex about Arctic hares (a hare). From
(R3) and (D14), one can deduce that a hare is not a MeatEater, and it further follows by
rule (R5) and fact (D13) that a hare is a Herbivore. Since a hare eats willow, we can
deduce Plant(willow) from (R4) and hence a hare is an answer to qex . Although (R5)
is a disjunctive rule, this reasoning process is fully deterministic and can be captured in
datalog. To this end, we introduce a predicate MeatEater which intuitively stands for the
complement of MeatEater. We can then extend the datalog subset of Kex with rules encoding
the intended meaning of the fresh predicate. In particular, (R3) and (R5) are two such rules,
which are obtained from (R3) and (R5), respectively.
Folivore(x) ! MeatEater(x)

(R3)

Mammal(x) ^ MeatEater(x) ! Herbivore(x)

(R5)

We can exploit these rules to derive MeatEater(a hare) and then Herbivore(a hare).

}

We now define the shifting transformation formally.
Definition 4.3. Let r be a normalised disjunctive datalog rule. For each predicate P in r
let P be a fresh predicate of the same arity. Furthermore, given an atom  = P (~t) let  be
P (~t). The shifting of r, written shift(r), is the following set of rules:
 if r of the form (2), then shift(r) = {r}[{ 1 ^  ^

i 1 ^ i+1 ^  ^ n

!

i

| 1  i  n};

 if r of the form (4), then shift(r) consists of the following rules: (i) the rule (S1);
(ii) all rules (S2) for each 1  j  m; and (iii) all rules (S3) for each 1  i  n s.t.
each variable in i also occurs in some other atom in the rule.
1
1
1

^  ^
^  ^
^  ^

n
n

^
^

i 1

1
1

^

^  ^
^  ^
i+1

m

!?

j 1

^  ^

^
n

(S1)
j+1

^

1

^  ^

^  ^

m
m

!

!

j

(S2)

i

(S3)

Let  be a set of normalised disjunctive datalog rules. Then, the shifting of  is defined
as the following set of datalog rules:
[
shift() =
shift(r)
}
r2

Note that shifting is a polynomial transformation. For r a disjunctive datalog rule with
n atoms in the body and m atoms in the head, shift(r) contains at most m + n + 1 datalog
rules. Furthermore, as shown in the following theorem, it is also sound.
321

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Theorem 4.4. Let DD
be the subset of disjunctive datalog rules in K ; furthermore, let
K
K0 = shift(DD
)
[
D
.
Then,
cert(q, K0 )  cert(q, K).
K
K
0
Proof. Let ChaseK0 = {B i }L
i=1 with L some non-negative integer (recall that K is a datalog
knowledge base and hence its Skolem chase is finite). We show by induction that the
following properties hold for each 0  i  L and each  2 B i :

(a) if  = ?, then K is unsatisfiable;
(b) if  = P (~a), then K |= P (~a); and
(c) if  = P (~a), then K |= P (~a).
Base case: Clearly, B 0 = DK and the properties trivially follow from the fact that DK  K.

Inductive step: Assume that properties (a)(c) hold for every  2 B i . We show that they
also hold for every  2 B i+1 \ B i . There must exist a rule r0 2 K0 and a substitution
such that B i |= body(r0 ) and  = head(r0 ) . Since every atom in body(r0 ) is in B i , then
properties (a)-(c) hold for all these atoms by the induction hypothesis. Furthermore, there
must exist a rule r 2 K of the form 1 ^    ^ n ! 1 _    _ m such that r0 2 shift(r).
(a) If  = ?, we distinguish two cases. (i) head(r) = ?, in which case r = r0 and by
the induction hypothesis, K |= { 1 , . . . , n } and hence K |= ?; (ii) head(r) 6= ?,
in which case r0 is of the form (S1) and 1 , . . . , n and 1 , . . . , m are in B i . By
the induction hypothesis, K entails 1 , . . . , n and  1 , . . . ,  m . But then, rule r
cannot be satisfied by any model of K and since r 2 K, we obtain that K is unsatisfiable.
(b) If  = P (~a), then r0 is of the form (S2) and i = P (~a). Hence, B i contains all atoms
and i+1 , . . . , m . By induction hypothesis, K entails
1 ,..., n , 1 ,... i 1
,
.
.
.
,
,

,
.
.
.
,

a) it
1
n
1
i 1 , and  i+1 , . . . ,  m . Since r 2 K and i = P (~
must be the case that K |= P (~a).
(c) If  = P (~a), we have the following cases. (i) head(r) = ?, in which case by induction
K |= { 1 , . . . , i 1 , i+1 , . . . , n }; but then, since 1 ^    ^ n ! ? is also a
rule in K, we obtain that K |=  i , as required. (ii) head(r) 6= ?, in which case
r0 is of the form (S3) and i = P (~a); then, B i contains all atoms 1 , . . . , i 1 ,
and by the induction hypothesis K entails atoms
i+1 , . . . , n , and 1 , . . . , m
,
.
.
.
,
,
,
.
.
.
,
and

a).
1
i 1
i+1
n
1 , . . . ,  m . Since r 2 K we obtain K |= P (~
V
If q = ?, the theorem follows from property (a). Otherwise, let q(~x) = 9~y ( ni=1 i (~x, ~y ))
and let ~a be a possible answer such that K0 |= q(~a). Since K0 is datalog, there exists a tuple
~e of constants in K0 and a non-negative integer L such that i (~a, ~e) 2 B L for each 0  i  n.
But then, by (b) we have K |= i (~a, ~e), and hence K |= q(~a).
Shifting only captures some of the consequences of the disjunctive datalog rules in K.
Furthermore, note that there is no refinement of shifting that ensures preservation of all
consequences; indeed, it is well-known that disjunctive datalog can express queries (e.g.,
non-3-colorabilility) that cannot be captured by means of a datalog program (Afrati, Cosmadakis, & Yannakakis, 1995).
322

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Example 4.5. Consider the disjunctive datalog knowledge base consisting of the fact
GreenSeaTurtle(turtle), the rules (R1), (R2) and
GreenSeaTurtle(x) ! Herbivore(x) _ Carnivore(x).
Clearly, Mammal(turtle) follows from the knowledge base. The shifting consists of fact
GreenSeaTurtle(turtle) and the following rules, where predicates Carnivore, GreenSeaTurtle,
Herbivore and Mammal have been abbreviated as, respectively, C, G, H and M:
C(x) ^ M(x) ! ?

C(x) ! M(x)

M(x) ! C(x)

G(x) ^ H(x) ^ C(x) ! ?

G(x) ^ H(x) ! C(x)

G(x) ^ C(x) ! H(x)

H(x) ^ M(x) ! ?

H(x) ! M(x)

H(x) ^ C(x) ! G(x)

M(x) ! H(x)

}

It can be checked that fact Mammal(turtle) does not follow from the shifting.
4.2 Dealing with Existential Rules: The Combined Approach for OWL 2 EL

Existentially quantified rules are ubiquitous in large-scale and complex ontologies, especially
in life sciences applications. The EL profile of OWL 2 was specifically designed for such
applications, and many large ontologies used in practice can be seen as consisting of a large
EL backbone extended with a small number of axioms outside the profile.
Given the prevalence of EL axioms in realistic ontologies, it is natural to consider the
OWL 2 EL subset of K for computing lower bound answers. CQ answering for OWL
2 EL is, however, PSpace-complete (Stefanoni, Motik, Krotzsch, & Rudolph, 2014) and
no system currently supports CQ answering for the whole of OWL 2 EL. Complexity,
however, drops to NP in the case of ELHOr? (Stefanoni et al., 2014). In our setting, the
restriction to ELHOr? ontologies has the added practical benefit that we can exploit the socalled combined approach to delegate most of the computational work associated with CQ
answering to a datalog reasoner (Stefanoni et al., 2013; Lutz, Toman, & Wolter, 2009)a
technique currently supported by systems such as KARMA.5 Although datalog-based CQ
answering techniques are also available for richer languages, such as the extension of ELHOr?
with inverse roles (i.e., axioms (O7) in Table 1), the resulting datalog programs can be hard
to compute and are of exponential size in the worst case (Perez-Urbina, Motik, & Horrocks,
2010). This is in contrast to the combined approach to ELHOr? , where the relevant datalog
programs can be straightforwardly constructed without the need for reasoning, and are of
linear size (see Related Work section for further details).
Thus, to compute query answers that depend on existentially quantified rules we consider
r
the subset EL
K of ELHO ? rules in K, which can be syntactically characterised as follows.
Definition 4.6. A rule is ELHOr? if it is of one of the following forms, where '(x) is either
?, or of the form A(x), x  c, or 9yR(x, y):
p
^

i=1

Ai (x) ^

q
^

j=1

[Rj (x, yj ) ^

lj
^

k=1

5. http://www.cs.ox.ac.uk/isg/tools/KARMA/

323

Bjk (yj )] ! '(x),

(EL1)

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

R1 (x, y) ! R2 (x, y),
R(x, y) ! A(y).

(EL2)
(EL3)

The combined approach that we exploit for CQ answering can be conceptualised as a
three-step process.
1. The first step is to compute the materialisation M of a datalog program obtained
from EL
K with respect to DK . If M contains ?, then the knowledge base is unsatisfiable. Otherwise M is a model of the knowledge base. This model, however, is not
universal as it cannot be homomorphically embedded into every other model. Thus,
the evaluation of CQs over M may lead to unsound answers.
2. The second step is to evaluate the query q over M . This step is intractable in query
size, but well-known database techniques can be exploited.
3. In the third step, unsound answers obtained from the second step are discarded using
a polynomial time filtration algorithm.
We next specify the transformation from knowledge bases to datalog used in the first
step, as this transformation will also be exploited later on in Section 5 for computing upper
bound query answers. The computation of the datalog program from a knowledge base in
Step 1 relies on a form of Skolemisation where existentially quantified variables are mapped
to fresh constants (instead of functional terms).
Definition 4.7. For each rule r of the form (1) and each existentially quantified variable
zij , let crij be a constant globally unique for r and zij , and let c-sk be the substitution such
that c-sk (zij ) = crij for each zij 2 ~zi . The c-Skolemisation c-sk(r) of r is given as follows:
x, ~y )
1 (~

^  ^

x, ~y )
n (~

!

m
_

'i (~x, ~zi )c-sk .

i=1

Then, we define c-sk(K) = {c-sk(r) | r 2 K } [ DK .

}

Note that the application of c-Skolemisation to an ELHOr? rule always results in a
datalog rule. Note also that, in contrast to standard Skolemisation, c-Skolemisation is not
a satisfiability or entailment preserving transformation, and there may be query answers
w.r.t. c-sk(K) that are unsound w.r.t. K. It can be shown, however, that c-Skolemisation is
satisfiability-preserving over ELHOr? knowledge bases; thus, c-sk(EL
K ) [ DK is satisfiable
i EL
[
D
is
satisfiable
(Stefanoni
et
al.,
2013).
We
next
sketch
the
filtration step, and
K
K
refer the interested reader to the work of Stefanoni et al. for further details.
The main source of spurious answers when evaluating the query over the materialisation
obtained in Step 1 is the presence of forksconfluent chains of binary atoms involving
Skolem constantsin the image of the query over the materialisation. This is due to the
fact that ELHOr? has the so-called forest-model property, and such forks cannot manifest
themselves in forest-shaped models. We will say that a constant c in EL
K [DK is auxiliary if
no dierent constant b exists such that c-sk(EL
)
[
D
|=
c

b;
that
is,
auxiliary constants
K
K
are those that have been introduced by c-Skolemisation and are not entailed to be equal
324

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

to any constant present in the original ELHOr? knowledge base. Let  be the substitution
mapping the free variables ~x in q to constants ~a from K such that M |= q . Then, relation 
for q and ~a in the smallest reflexive-transitive binary relation over the terms in q satisfying
the following fork rule
(fork)

s 0  t0
st

R(s, s0 ) and P (t, t0 ) occur in q, and
 (s0 ) is an auxiliary constant.

Clearly  is an equivalence relation, which can be computed in polynomial time in the size
of q. For any term t in q, let [t] be the equivalence class of t w.r.t. , and let be a mapping
from each term t in q to an arbitrary but fixed representative of [t]. The auxiliary graph of
q and ~a is the directed graph G = hV, Ei such that
 V contains a vertex (t) for each term t in q such that  (t) is auxiliary; and
 E contains a directed edge h (s), (t)i for each atom of the form R(s, t) in q such that
{ (s), (t)}  V .
Now, we are ready to define filtration. We say that ~a is a spurious answer if either the
auxiliary graph of q and  contains a cycle, or terms s and t occurring in q exist such that
s  t but c-sk(EL
a can be
K ) [ DK 6|=  (s)   (t). Clearly, filtration for a candidate answer ~
done in polynomial time in the size of q.
We will assume the availability of a procedure soundAnswers that solves Steps 2 and 3;
that is, given q and the model computed in Step 1, it returns all answers to q w.r.t. the
input ELHOr? knowledge base. Consequently, given K and q, we can obtain a lower bound
on the query answers as follows:
r
 extract the subset EL
K of all ELHO ? rules in K;

 compute the materialisation M of c-sk(EL
K ) [ DK ; and
 if q = ? then return unsatisfiable i ? 2 M ; otherwise, return soundAnswers(q, M ).
Example 4.8. Consider again our running example. The ELHOr? subset of Kex consists
of all facts (D1)(D15) together with all rules except for (R4) and (R5). From fact (D12)
and rule (R8) we deduce that howler eats a leaf, which must be a plant by rule (R9). Hence
howler is an answer to qex . This answer can be identified using the aforementioned steps.
The c-Skolemisation of (R8a) leads to the datalog rule
Folivore(x) ! eatsL (x, c3 )

(R8aU)

The materialisation of the datalog program consisting of all facts and rule (R8aU) contains
the fact Plant(c3 ) and hence the tuple (howler, c3 ) matches qex to the materialisation. This
match is deemed sound by the filtration procedure.
}
4.3 Aggregated Lower Bound
The techniques in this section can be seamlessly combined to obtain a lower bound Lq which
is hopefully close to the actual set of certain answers. Given K and q, we proceed as follows:
325

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

DD is the subset of
1. Construct the datalog knowledge base shift(DD
K ) [ DK , where K
L
disjunctive datalog rules in K . Compute its materialisation M1 .
L
L
2. Construct the datalog program c-sk(EL
K ) [ M1 and compute its materialisation M2 .

3. If q = ?, then Lq = cert(q, M2L ). Otherwise, Lq = soundAnswers(q, M2L ).
Theorem 4.4 ensures that K |=  for any  2 M1L in the signature of K, and hence M1L
can be used as the initial dataset for the second step. The properties of c-Skolemisation and
filtration discussed in Section 4.2 then ensure that every answer in Lq is indeed a certain
answer of q w.r.t. K. Furthermore, if ? 2 M2L , then K is indeed unsatisfiable. Finally, note
that the materialisation M1L obtained in the first step is pipelined into the second step;
as a result, Lq is a (sometimes strict) superset of the answers we would obtain by simply
EL
computing the answers to q w.r.t. shift(DD
K ) [ DK and c-sk(K ) [ DK independently and
then the union of the results.
Example 4.9. For our running example Kex , the aggregated lower bound Lex consists of
sheep (which follows from the datalog subset of Kex ), a hare (which follows from shift(Kex )),
and howler (which follows from the ELHOr? fragment of Kex ).
}

5. Upper Bound Computation
In many practical cases the lower bound Lq described in Section 4.3 constitutes a rather
precise approximation of the actual set of certain answers. Furthermore, it can also be
computed very efficiently by resorting only to the datalog reasoner. The lower bound
computation, however, gives no indication as to the accuracy of its answers: without a
corresponding upper bound, every other possible answer remains a candidate answer, which
needs to be either confirmed or discarded.
In this section, we describe our approach to efficiently computing an upper bound to
the set of certain answers. If lower and upper bounds coincide, then we have fully answered
the query; otherwise, the gap between lower and upper bounds not only provides a margin
of error for the lower bound, but also narrows down the set of candidate answers whose
verification may require more powerful computational techniques.
5.1 Strengthening the Knowledge Base
Our first step towards computing an upper bound will be to construct a (polynomial size)
datalog knowledge base K0 such that if K is unsatisfiable, then K0 entails a nullary predicate
?s , and cert(q, K)  cert(q, K0 ) otherwise. Roughly speaking, this K0 , which we refer to as
the datalog strengthening of K, is obtained from K by
1. replacing ? by a fresh nullary predicate ?s with no predefined meaning;
2. splitting the disjuncts occurring in head position into dierent datalog rules; and
3. Skolemising existentially quantified variables into constants as in Definition 4.7.
It is convenient for subsequent definitions and proofs to explicitly define the splitting of
K, written split(K), as the intermediate knowledge base resulting from Steps 1 and 2 above,
326

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

which is both satisfiable and disjunction-free. The datalog strengthening of K is then defined
as the result of further applying Step 3 by replacing each existentially quantified rule in
split(K) with its c-Skolemisation.
Definition 5.1. The splitting of a rule r of the form (1) is the following set of rules:
 if head(r) = ?, then split(r) = { 1 ^    ^
predicate with no predefined meaning; and
 otherwise, split(r) = {

! ?s }, where ?s is a fresh nullary

! 9~zj 'j (~x, ~zj ) | 1  j  m }.
S
The splitting of K = K [ DK is defined as split(K) = r2K split(r) [ DK . Finally, the
datalog strengthening of K is defined as str(K) = c-sk(split(K)).
}
1

^  ^

n

n

Example 5.2. Consider our example knowledge base Kex . The splitting of Kex is obtained
by replacing rule (R5) with rules (R5Ua) and (R5Ub), and rule (R3) with (R3U).
Mammal(x) ! Herbivore(x)

Mammal(x) ! MeatEater(x)

Folivore(x) ^ MeatEater(x) ! ?s

(R5Ua)
(R5Ub)
(R3U)

Finally, str(K) is obtained by further replacing the existentially quantified rules (R6a), (R7)
with the following rules (R6aU), (R7U)
MeatEater(x) ! eatsH (x, c1 )
Mammal(x) ! eats(x, c2 )

as well as rule (R8a) with rule (R8aU) given in Example 4.8.

(R6aU)
(R7U)
}

Note that if K does not contain rules with ? in the head, then str(K) logically entails
K: splitting amounts to turning disjunctions in the head of rules into conjunctions, while
c-Skolemisation restricts the possible values of existentially quantified variables to fixed
constants. Thus, cert(q, str(K)) constitutes an upper bound to cert(q, K). This is, however,
no longer the case if ? is replaced with an ordinary predicate ?s without a predefined
meaning. The rationale behind this replacement is to provide a meaningful upper bound
even in cases where splitting disjunctions and c-Skolemising existentially quantified variables
would make the strengthened knowledge base unsatisfiable.
0 = str(K ) of our example knowledge base.
Example 5.3. Consider the strengthening Kex
ex
Since howler is a Mammal, we have by Rule (R5Ub) that it is also a MeatEater. But then,
since Folivore(howler) is a fact in Kex we can derive ?s using Rule (R3U). Note that, had
we not replaced the falsehood predicate ? with ?s , the strengthening of Kex would be
unsatisfiable, in which case no meaningful upper bound could be obtained for any query. }

We next show that str(K) can be exploited to compute a meaningful upper bound for
any input query, despite the fact that ? is stripped of its built-in semantics in first-order
logic. The following lemma establishes the key property of the splitting transformation in
Definition 5.1: if a ground clause ' = 1 _    _ n is derivable from K via hyperresolution,
then the Skolem chase of split(K) contains every atom i for 1  i  n.
327

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Lemma 5.4. Let  = (T, ) be a hyperresolution derivation from K and let H = split(K).
Then, for every node v 2 T and ground atom  occurring in (v), we have that  2 ChaseH .
Proof. We prove the claim by structural induction on .
Base case: If v is a leaf in T , then (v) 2 DK . Since DK  H we have  2 ChaseH .

Inductive step: Assume that the induction hypothesis holds for all children w1 , . . . , wn of
a node v 2 T . There exists a rule r 2 K and a substitution , where sk(r) is of the form
 1 _    _  n _ with a disjunction of atoms, such that (v) =
_ 1 _  _ n
is the hyperresolvent of sk(r) and (wi ) = i _ i for each 1  i  n. By the induction
hypothesis, all the disjuncts in each i are in ChaseH , so we only need to show the claim
for each disjunct in
. We distinguish the following cases depending on the form of the
normalised rule r
 If r is of the form (2),

is empty. So the claim holds vacuously.

 If r is of the form (3), then = 1 sk . By the induction hypothesis, each
ChaseH , and since split(r) = r and hence r 2 H, we obtain 1 sk 2 ChaseH .

i

is in

 If r is of the form (4), then = 1 _    _ m . By induction hypothesis, each i is in
ChaseH , and for each 1  i  m, since the rule 1 ^    ^ n ! i is in H, we obtain
that each atom i is also in ChaseH , as required.
We can now exploit the completeness of hyperresolution to show that split(K) satisfies
the required properties. Furthermore, the fact that str(K) |= split(K) immediately implies
that str(K) satisfies those properties as well and hence it may be exploited to compute upper
bound query answers.
Theorem 5.5. The following properties hold for H = split(K) as well as for H = str(K):
(i) cert(?, K)  cert(?s , H), i.e. if K is unsatisfiable, then H |= ?s ; and (ii) if K is
satisfiable then cert(q, K)  cert(q, H).
Proof. We first show that Properties (i) and (ii) hold for H = split(K). If K is unsatisfiable,
then there is a hyperresolution derivation of the empty clause from K. Thus, there must
exist a rule r of the form (2) in K and a substitution
such that each atom i for
1  i  n is also derivable from K. But then, by Lemma 5.4 we have that i 2 ChaseH .
Since H contains the rule 1 ^    ^ n ! ?s we have ?s 2 ChaseH and H |= ?s , as required.
Assume now that K is satisfiable. If cert(q, K) = ;, cert(q, K)  cert(q, H) holds trivially;
otherwise let ~a be a certain answer to q w.r.t. K. So K |= q(~a) and hence K [ Rq |= Pq (~a).
Since cert(?, K) = ;, we have q 6= ?. Using the completeness of hyperresolution and
Lemma 5.4 we obtain that Pq (~a) is in the chase of K [ Rq . But then, the aforementioned
splitting also entails Pq (~a) and since split(K [ Rq ) = H [ Rq we have ~a 2 cert(q, H), as
required. Finally, Properties (i) and (ii) hold for str(K) as a direct consequence of the fact
that str(K) |= split(K).
Example 5.6. Figure 1 depicts the materialisation of str(Kex ), where edges for predicates
introduced during the normalisation are ignored and all edges in the figure represent the
binary predicate eats. Explicit facts in Kex are depicted in black; implicit facts are depicted
328

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

c1

tiger
Mammal
Herbivore
MeatEater

MeatEater
Mammal
Herbivore
Plant

lion
Mammal
Herbivore
MeatEater

c2

python
MeatEater

Plant

grass Plant

wolf

rabbit
Herbivore
Mammal
MeatEater

Mammal
MeatEater
Herbivore

c3

sheep
Herbivore
Mammal
MeatEater
Plant

Leaf
Plant

willow Plant

howler

a hare

Mammal
Folivore
Herbivore
MeatEater

Mammal
Folivore
Herbivore
MeatEater

Figure 1: Materialisation of Datalog strengthening of Kex
using dierent colours to facilitate the subsequent illustration of further refinements of the
materialisation that will allow us to tighten the upper bound. We obtain the following
upper bound of cert(qex , Kex ) by evaluating qex against the materialisation:
cert(qex , str(Kex )) = {tiger, lion, python, rabbit, wolf , sheep, howler, a hare, c1 }
As already mentioned, str(Kex ) |= ?s ; however, the obtained upper bound is still meaningful
since it does not contain all possible answers in Kex , such as grass or willow. Please note
that c1 is a certain answer to qex w.r.t. str(Kex ); however, constant c1 is not in the signature
of Kex and hence it is not a possible answer to qex w.r.t. K.
}
5.2 Tightening the Upper Bound: Existential Rules
The upper bound obtained from str(K) can be rather coarse-grained in practice: as discussed
in Example 5.6, python, tiger, lion and wolf are contained in the upper bound, where none
of them is a certain answer to qex . In this section, we show how to refine the upper bound
by restricting the application of c-Skolemisation to existential rules. Instead of computing
the upper bound of q by constructing the strengthened knowledge base str(K) and then
evaluating q over (the materialisation of) str(K), we proceed as follows.
1. Apply to K a variant of the Skolem chase, which we refer to as the c-chase by first
splitting the disjuncts occurring in head position into dierent rules and then applying
Skolem chasing on split(K) with the following modifications: (i) similarly to the
restricted chase (Cal et al., 2013), existential rules are applied only when the rule
head is not already satisfied; and (ii) rather than Skolemising the head atom (using a
functional term) whenever an existential rule is applied, we resort to c-Skolemisation
instead. Due to the latter modification, the c-chase does not compute the least
Herbrand Model of split(K), but rather just some model of split(K).
2. Evaluate q over the result of the aforementioned chase, thus obtaining an upper bound
to the certain answers of q w.r.t. split(K), and thus also w.r.t. K.
The following example motivates the practical advantages of this approach.
Example 5.7. Consider again the materialisation of str(Kex ) in Figure 1. As already
mentioned, python is returned as an upper bound answer since qex matches the facts
329

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

eats(python, c1 ) and Plant(c1 ) in the materialisation. The fact eats(python, c1 ) is obtained
from eatsH (python, c1 ), which is included in the materialisation to satisfy the c-Skolemised
rule (R6aU) in str(Kex ), and also the existentially quantified rule (R6a) in Kex . In the case
of python, however, rule (R6a) in Kex is already satisfied by the fact eatsH (python, rabbit),
which is derived from eats(python, rabbit) and Herbivore(rabbit) in the dataset, and rule
(R6b). Please note that rule (R6b) is of the form (9) in the normalisation of (R6).
Rule (R6b) ensures that if (R6) is satisfied for any substitution, then (R6a) is also satisfied for the same substitution. To obtain an upper bound it suffices to construct a model
of Kex (rather than a model of str(Kex )); thus, we can prevent the application of rule (R6aU)
on python during the chase, and dispense with eats(python, c1 ) in the materialisation. }
We are now ready to define the c-chase formally.
Definition 5.8. Let H = split(K), let dH be the subset of datalog rules in H , and
eH = H \ dH . The c-chase sequence of K is the sequence of sets of ground atoms {B i }i 0 ,
where B 0 = DH (i.e. B 0 = DK ), and B i+1 is inductively defined as given next. Let Sdi+1 and
Sei+1 be defined as follows:
Sdi+1 = {head(r) | r 2 dH ,

Sei+1 = {head(c-sk(r)) | r 2 eH ,

a substitution, B i |= body(r) and B i 6|= head(r)}
a substitution, B i |= body(r) and B i 6|= head(r)}

Then, B i+1 = B i [ Sdi+1 if Sdi+1
6 ;, and B i+1 = B i [ Sei+1 otherwise. Finally, we define the
S =
c-chase of K as c-ChaseK = i 0 B i .
}

Note that the c-chase of K is a finite set since the only terms that can occur in it are
constants from c-sk(split(K)).
Example 5.9. The c-chase of Kex is depicted in Figure 2. This materialisation is a strict
subset of that in Figure 1, where the orange-coloured binary facts are no longer derived.
Consequently, python is no longer derived as an answer to qex .
}
The relevant properties of the c-chase are summarised in the following lemma.
Theorem 5.10. The following properties hold: (i) cert(?, K)  cert(?s , c-ChaseK ), i.e. if
K is unsatisfiable, then ?s 2 c-ChaseK ; (ii) if K is satisfiable, cert(q, K)  cert(q, c-ChaseK ).
Proof. We first prove that c-ChaseK is a model of split(K). Since DK  c-ChaseK it is clear
that it satisfies all facts in split(K). Let r 2 split(K); we distinguish two cases:
 The rule r is datalog. If c-ChaseK |= body(r) for some substitution the definition
of c-chase ensures that head(r) 2 c-ChaseK and hence the rule is satisfied.
 Otherwise, r is of the form (3). If c-ChaseK |= body(r) for some substitution
the definition of c-ChaseH ensures that head(c-sk(r)) 2 c-ChaseK ; thus, c-ChaseK |=
head(r) and hence the rule is satisfied.
We now show the contrapositive of the first property. Assume that ?s 62 c-ChaseK .
Because c-ChaseK is a model of split(K), we have split(K) 6|= ?s and hence K is satisfiable by
Theorem 5.5. Finally, assume that K is satisfiable. If cert(q, K) = ;, cert(q, K)  cert(q, H)
holds trivially; otherwise let ~a be a certain answer to q w.r.t. K. By Theorem 5.5, we obtain
~a 2 cert(q, split(K)). Because c-ChaseK |= split(K), we have ~a 2 cert(q, c-ChaseK ).
330

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

c1

tiger
Mammal
Herbivore
MeatEater

MeatEater
Mammal
Herbivore
Plant

lion
Mammal
Herbivore
MeatEater

c2

python

Plant

grass Plant

wolf

rabbit

MeatEater

Herbivore
Mammal
MeatEater

Mammal
MeatEater
Herbivore

c3

sheep
Herbivore
Mammal
MeatEater
Plant

Leaf
Plant

willow Plant

howler

a hare

Mammal
Folivore
Herbivore
MeatEater

Mammal
Folivore
Herbivore
MeatEater

Figure 2: c-chase of Kex
5.3 Tightening the Upper Bound: Disjunctive Rules
Although the technique described in the previous section can be quite eective in practice,
its main limitation is that in split(K) all disjunctions in the heads of rules in K have eectively been turned into conjunctions. In this section, we show how to refine the upper bound
by exploiting an extension of c-chase that uses a similar approach to deal with disjunctive
rules as well as existential rules.
Specifically, we extend c-chase to deal with disjunctive rules r of the form (4) such
that (i) r is applied only when none of the disjuncts in the head of the rule is already
satisfied; and (ii) when r is applied, only one of the disjuncts is included in the chase (rather
than all of them). In order to avoid non-determinism during chase expansion and reduce
the computational cost, disjuncts are selected deterministically by means of an (efficiently
implementable) choice function.
Example 5.11. Consider again our running example. First observe that wolf is an answer
to qex w.r.t. the c-chase of Kex shown in Figure 2. Indeed, Herbivore(wolf ) is derived from
Mammal(wolf ) and the rules in the split of (R5); thus, Plant(sheep) is also derived using
rule (R4). Note, however, that wolf is a spurious answer: given that MeatEater(wolf ) is an
explicit fact in Kex , rule (R5) is already satisfied for wolf and hence we can dispense with
fact Herbivore(wolf ) in the materialisation.
Finally, since our goal is to construct a model of Kex it is reasonable to pick disjuncts
whose predicate is unrelated to ? in Kex . Since ? depends only on MeatEater and Folivore
(by rule (R3)), it makes sense to include a fact Herbivore(b) in the materialisation whenever
the disjunctive rule (R5) is applied to a constant b.
}
For further details we refer the reader to Section 8, where the specific choice function
implemented in PAGOdA is described.
We can now define our extended notion of c-chase, where an efficiently implementable
choice function is given as an additional parameter.
Definition 5.12. Let H be the knowledge base obtained from K by replacing ? with the
nullary predicate ?s , let dH be the set of all datalog rules in H , and let nH = H \ dH .
Furthermore, let f be a polynomially computable choice function that given a ground clause
and a set of ground atoms returns a disjunct in . The c-chase sequence of K w.r.t. f is
331

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

c2

tiger
Mammal
Herbivore

lion
Mammal
Herbivore

python
MeatEater

Plant

grass Plant

wolf

rabbit
Herbivore
Mammal

Mammal
MeatEater

c3

sheep
Herbivore
Mammal

Leaf
Plant

willow Plant

howler

a hare

Mammal
Folivore
Herbivore

Mammal
Folivore
Herbivore

Figure 3: c-chasef of Kex
the sequence of sets of ground atoms {B i }i 0 , where B 0 = DH (i.e., B 0 = DK ), and B i+1 is
defined as given next. Let Sdi+1 and Sni+1 be as follows:
Sdi+1 = {head(r) | r 2 dH ,

Sni+1 = {f (head(c-sk(r)) , B i ) | r 2 nH ,

a substitution, B i |= body(r) and B i 6|= head(r)}

a substitution , B i |= body(r) and B i 6|= head(r)}

Then, B i+1 = B i [ Sdi+1 if Sdi+1 6= ;, and B i+1 = B i [ Sni+1 otherwise. Finally, we define the
S
c-chase of K w.r.t. f as c-ChasefK = i 0 B i .
}
Example 5.13. Consider the aforementioned choice function f that picks Herbivore(b)
whenever rule (R5) is applied to a fact Mammal(b). Figure 3 depicts the facts in c-ChasefKex .
It can be observed that c-ChasefKex is a strict subset of the materialisation in Figure 2, where
the brown-colored facts are no longer derived. We can see that wolf is not an answer to
qex w.r.t. c-ChasefKex and hence it can be identified as spurious. Furthermore, the nullary
predicate ?s has not been derived and hence we can determine that Kex is satisfiable. }
The relevant properties of this variant of the c-chase are as follows.
Theorem 5.14. Let f be a choice function as in Definition 5.12. If ?s 62 c-ChasefK , then
c-ChasefK is a model of K and cert(q, K)  cert(q, c-ChasefK ).
Proof. The dataset DK is contained in c-ChasefK , so it suffices to show that c-ChasefK satisfies
each rule r 2 K. We distinguish the following cases:
 r is of the form (2). Since ?s 2
/ c-ChasefK , there cannot exist a substitution
that c-ChasefK |= body(r) and hence c-ChasefK satisfies r vacuously.

such

 r is of the form (3). Pick such that c-ChasefK |= body(r) . The definition of c-ChasefK
ensures that head(c-sk(r)) 2 c-ChasefK and hence c-ChasefK satisfies r.
 r is of the form (4). Pick such that c-ChasefK |= body(r) . By the definition of
c-ChasefK , we have f (head(c-sk(r)), Sni ) 2 c-ChasefK for some set of atoms Sni in the
chase sequence, and then c-ChasefK satisfies r.

If q = ?, then cert(q, K) = ; and cert(q, K)  cert(q, c-ChasefK ) holds trivially; otherwise,
cert(q, K)  cert(q, c-ChasefK ) follows from the fact that c-ChasefK is a model of K.
332

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

5.4 Combined Upper Bound
We have introduced three dierent techniques for computing an upper bound to cert(q, K).
1. Compute the materialisation M1U of str(K), and evaluate q w.r.t. M1U to obtain a set
of possible answers U1q to q w.r.t. K (c.f. Section 5.1).
2. Compute the c-chase of K, denoted by M2U , and evaluate q w.r.t. M2U to obtain a set
of possible answers U2q to q w.r.t. K (c.f. Section 5.2).
3. Fix a choice function f , compute the c-chase of K w.r.t. f , denoted by M3U , and evaluate q w.r.t. M3U to obtain a set of possible answers U3q to q w.r.t. K (c.f. Section 5.3).

It can be trivially seen that U2q and U3q are more precise than U1q , i.e. U2q  U1q and U3q  U1q .
As shown in the following example, U2q and U3q are, however, incomparable.

Example 5.15. Consider a knowledge base H consisting of facts A(a1 ), R(a1 , b1 ), B(b1 ),
A(a2 ), R(a2 , b2 ), B(b2 ) and rules B(x) ! C(x) _ D(x), R(x, y) ^ C(y) ! S(x, y) and
A(x) ! 9yS(x, y). Let c be the freshly introduced constant for A(x) ! 9yS(x, y), and let
f be a choice function that picks the disjunct D(bi ) in every clause C(bi ) _ D(bi ). Then,
c-ChaseH = DH [ {C(b1 ), D(b1 ), S(a1 , b1 ), C(b2 ), D(b2 ), S(a2 , b2 )}, and
c-ChasefH = DH [ {D(b1 ), S(a1 , c), D(b2 ), S(a2 , c)}.

For q1 (x) = 9y(S(x, y) ^ C(y) ^ D(y)), the upper bound computed using the c-ChaseH
contains two additional answers a1 and a2 compared with that computed using c-ChasefH .
But for q2 (x1 , x2 ) = 9y(S(x1 , y) ^ S(x2 , y)), the upper bound computed using c-ChasefH has
additional answers (a1 , a2 ) and (a2 , a1 ) compared with that computed using c-ChaseH . }

There are, however, tradeos to be considered. Clearly, the upper bound U1q is the most
convenient from an ease of implementation point of view: once str(K) has been constructed,
the bound can be directly computed using an o-the-shelf datalog reasoner without modification. Furthermore, the upper bound U3q has an important shortcoming: it is of no use
whenever ?s is derived, as we will show in the following example.
Example 5.16. Consider a choice function g that picks MeatEater(a) for any disjunction of the form Herbivore(a) _ MeatEater(a). Then the c-chase of Kex w.r.t. g will derive
MeatEater(howler) from the fact Mammal(howler) and the disjunctive rule (R5). Using
fact Folivore(howler) and rule (R3U) it will then derive ?s . Thus we can see that, although
howler is in cert(qex , Kex ), Herbivore(howler) is not in the c-chase of Kex w.r.t. g, and hence
howler is not in the upper bound computed using it; this is in contrast to the other two
upper bounds, where Herbivore(howler) is in the materialisation of str(Kex ) and the c-chase
of Kex , and hence howler is in the upper bound computed w.r.t. them.
}
Therefore, if ?s 62 c-ChasefK , we can combine U2q and U3q to compute a hopefully more
precise upper bound; otherwise, we can use U2q . The combined upper bound query answer
U q to q in K is formally defined as follows:
8 ?s
?
< U2 \ U3 s if q = ?;
q
q
q
U =
(13)
U \ U3
if q 6= ? and ?s 62 c-ChasefK ;
: 2q
U2
otherwise.
333

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Example 5.17. The combined upper bound for qex in Kex gives:
Uex = {tiger, lion, rabbit, sheep, howler, a hare}.
If we compare this upper bound with the aggregated lower bound Lex given in Example 4.9
we can identify a gap Gex = {tiger, lion, rabbit}.
}

6. Reducing the Size of the Knowledge Base
Whenever there is a non-empty gap Gq between lower and upper bound (e.g., our running
example) we need to verify whether each answer in Gq is spurious or not. Accomplishing
this task using a fully-fledged reasoner can be computationally very expensive: verifying
each answer in Gq typically involves a satisfiability test, which can be infeasible in practice
for large-scale knowledge bases.
In this section we propose a technique for identifying a (typically small) subset Kq of
the knowledge base K that is sufficient for verifying all answers in Gq (i.e. ~a 2 cert(q, K) i
~a 2 cert(q, Kq ) for each ~a 2 Gq ). It is essential that these subsets be, on the one hand, as
small as possible and, on the other hand, efficiently computable. These requirements are in
conflict: computing minimal-sized subsets can be as hard as answering the query, whereas
subsets that can be easily computed may be almost as large as the initial knowledge base.
The main idea behind our approach is to construct a datalog knowledge base whose
materialisation identifies all rules and facts in Kq . Such knowledge base is of size polynomial
in the sizes of K and q and it does not include predicates of arity higher than those in K or
q. In this way, subset computation can be fully delegated to the scalable datalog reasoner,
hence addressing the efficiency requirement. The key property of Kq , which ensures that
it contains all the relevant information in K, is the following: for each rule or fact  2
/ Kq
we can show that  does not occur in any hyperresolution proof of  (resp. a gap answer
in Gq ) from K [ Rq for q = ? (resp. q 6= ?). The completeness of hyperresolution then
guarantees that all excluded facts and rules are indeed irrelevant.
6.1 Overview of the Approach
Let us motivate the main ideas behind our approach using our running example. Since ?s
has not been derived in M2U \ M3U , we know that cert(?, Kex ) = ;, and hence that Kex
is satisfiable (see Example 5.13). However, we still need to determine whether answers in
Gex = {tiger, lion, rabbit} from the combined upper bound are in cert(qex , Kex ), i.e., if they
are certain answers to qex .
We now sketch the construction of a datalog knowledge base track(Kex , qex , Gex ) from
which the subset of Kex relevant to the answers in Gex is derived. The key property of
this knowledge base is that its materialisation tracks all the rules and facts that may
participate in a hyperresolution proof of a gap answer and thus encodes the contents of the
subset Kqex . The relevant information is recorded using fresh predicates and constants:
 a fresh predicate P R for each predicate P in Kex , the extension of which in the
materialisation of track(Kex , qex , Gex ) will give us the facts in the subset.

334

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

 a fresh constant dr for each rule r in Kex and a special unary predicate Rel, the
extension of which in the materialisation of track(Kex , qex , Gex ) will give us the rules
in the subset.
The key step in the construction of this knowledge base is to invert each rule r 2 Kex
into a set of datalog rules (r) by (i) moving all head atoms of r into the body while
replacing their predicates with the corresponding fresh ones (e.g., replace P with P R );
(ii) copying all the atoms that were originally in the body of r into the (now empty) head
while replacing predicates with the corresponding fresh ones and adding the special atom
Rel(dr ) as an additional conjunct; and (iii) eliminating the conjunction in the head of r by
splitting r into multiple rules, one for each head conjunct.
Consider as a first example the datalog rule (R4) in Kex , which is inverted into the
following rules:
PlantR (y) ^ Herbivore(x) ^ eats(x, y) ! HerbivoreR (x)
R

R

Plant (y) ^ Herbivore(x) ^ eats(x, y) ! eats (x, y)
R

Plant (y) ^ Herbivore(x) ^ eats(x, y) ! Rel(dR4 )

(14)
(15)
(16)

The head Plant(y) of (R4) has been moved to the body and predicate Plant replaced with
PlantR ; the body Herbivore(x) ^ eats(x, y) has been copied into the head as the conjunction
HerbivoreR (x) ^ eatsR (x, y), and then conjoined with the special atom Rel(dR4 ); and finally
the head conjunction has been eliminated by splitting the rule into three separate rules.
These rules reflect the intuitive meaning of the freshly introduced predicates. If fact
PlantR (c) holds for some constant c, this means that fact Plant(c) may participate in a
hyperresolution proof in Kex of an answer in the gap. Additionally, if Herbivore(b) and
eats(b, c) also hold for some b, then these facts and the rule (R4) could also participate
in one such proof since Plant(c) is a hyperresolvent of facts Herbivore(b) and eats(b, c) and
rule (R4), which is recorded as facts HerbivoreR (b), eatsR (b, c), and Rel(dR4 ). Thus, rules
(14)(16) faithfully invert hyperresolution steps involving rule (R4).
Similarly, the disjunctive rule (R5) is inverted into the following two rules:
HerbivoreR (x) ^ MeatEaterR (x) ^ Mammal(x) ! MammalR (x)
R

R

Herbivore (x) ^ MeatEater (x) ^ Mammal(x) ! Rel(dR5 )

(17)
(18)

In this case, the disjunctive head Herbivore(x)_MeatEater(x) of (R5) has been moved to the
body as the conjunction HerbivoreR (x) ^ MeatEaterR (x) over the fresh predicates HerbivoreR
and MeatEaterR . If facts HerbivoreR (c) and MeatEaterR (c) hold for some c (which means
that facts Herbivore(c) and MeatEater(c) may participate in a relevant proof in Kex ) and
Mammal(c) holds, then we also deem fact Mammal(c) and rule (R5) relevant.
The situation is dierent when it comes to inverting and existentially quantified rules, in
which case we no longer capture relevant hyperresolution steps in Kex faithfully. Consider
rule (R7), which is inverted as follows:
eatsR (x, y) ^ Mammal(x) ! MammalR (x)
R

eats (x, y) ^ Mammal(x) ! Rel(dR7 )
335

(19)
(20)

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

In this case, the existentially quantified head 9y eats(x, y) is moved to the body as the atom
eatsR (x, y). If eatsR (b, c) holds for some b and c (and hence this fact may participate in a
relevant proof), and Mammal(b) also holds, then we record both (R7) and Mammal(b) as
relevant (the latter by means of the fact MammalR (b)). The hyperresolvent of Mammal(b)
and (R7) is an atom eats(b, t), with t a functional term, which may be unrelated to eats(b, c)
and hence irrelevant to proving an answer in the gap.
In addition to inverting the rules in Kex , the construction of track(Kex , qex , Gex ) also
needs to take the query and gap answers into account. For this, we encode the query
eats(x, y) ^ Plant(y) ! Pqex (~x) into the rules
PqRex (x) ^ eats(x, y) ^ Plant(y) ! eatsR (x, y)

(21a)

PqRex (x) ^ eats(x, y) ^ Plant(y) ! PlantR (y)

(21b)

and add a fact PqRex (c) for each c 2 Gex . These query-dependent rules are used to initialise the extension of the fresh predicates, which subsequently makes the other rules in
track(Kex , qex , Gex ) applicable.
The query answers in the gap stem from the upper bound; consequently, in order for
rules (21a) and (21b) to be applicable the data in track(Kex , qex , Gex ) is obtained from the
upper bound materialisation of Kex . In the following section we show that it suffices to
include all facts in the c-chase of Kex in order to ensure that the computed subset will
contain all the necessary facts and rules.
6.2 Subset Definition and Properties
We are now ready to formally define the datalog knowledge base used for subset computation
as well as the corresponding relevant subset.
Definition 6.1. Let G be a set of possible answers to q, let Rel be a fresh unary predicate
and let dr be a fresh constant unique to each r in K [ Rq . Furthermore, for each predicate
P in K [ Rq , let P R be a fresh predicate of the same arity as P and, for an atom  = P (~t),
let R denote P R (~t). For any normalised rule r 2 K [ Rq , let move(r) be the following
conjunction of atoms:
 P?R if r of the form (2);


R x, ~
z1 )
1 (~



R x)
1 (~

Then,

if r of the form (3); and

^  ^

R x)
m (~

if r of the form (4).

(r) is the following set of rules:

(r) = {move(r) ^ body(r) ! Rel(dr )} [ {move(r) ^ body(r) !

R
k

|

k

in body(r)}.

The tracking knowledge base track(K, q, G) is the smallest knowledge base containing
(i) all facts in the c-chase of K;
S
(ii) all rules in r2K[Rq (r);
336

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

(iii) a fact PqR (~a) for each ~a 2 G; and
(iv) a fact P?R if q 6= ?.
The subset of K relevant to q and G, denoted by Kq,G , is the smallest knowledge base
containing
 each rule r 2 K such that track(K, q, G) |= Rel(dr ); and
 each fact  2 DK such that track(K, q, G) |= R .
For brevity, we write Kq for the particular case where G is the set of gap answers Uq \ Lq
as defined in Sections 4.3 and 5.4.
}
Note that K? is a subset of Kq since track(K, ?, G? ) is a subset of track(K, q, Gq ): in
Definition 6.1, point (i) is the same for track(K, ?, G? ) and track(K, q, Gq ); furthermore,
the set of rules from (ii) for track(K, ?, G? ) is a subset of that for track(K, q, Gq ) since
K [ R?  K [ Rq ; finally, the fact P?R , which is included in track(K, ?, G? ) by point (iii),
also belongs to track(K, q, Gq ) by point (iv).
Example 6.2. Consider again our running example, where Gex = {tiger, lion, rabbit}. The
subset of Kex relevant to qex and Gex consists of rules R2, R4, R5, R6, and R7 and facts
D1, D2, D3, D5, D7, D9, and D11.
}
The key properties of the computed subsets are established by the following theorem.
Theorem 6.3. The following properties hold:
(1) Assume that L? = ;. Then, K is unsatisfiable i K? is unsatisfiable.
(2) Let q be dierent from ? and let G be any non-empty set of possible answers to q w.r.t.
K. If K is satisfiable, then ~a 2 cert(q, K) i ~a 2 cert(q, Kq,G ) for every ~a 2 G.
Proof. The if direction of (1) and (2) follows directly from the monotonicity of firstorder logic. The only if direction of both (1) and (2) follows from the completeness of
hyperresolution and the following claim, which establishes that for any q and a non-empty
G, Kq,G contains the support of all hyperresolution derivations of any clause in (q, G)
from K [ Rq where

{}
if q = ?;
(q, G) =
{Pq (~a) | ~a 2 G} otherwise.
Claim (|) If  = ( , T ) is a derivation of  2 (q, G) from K [ Rq , then support()  Kq,G .
To show the only if direction of (1), assume that K is unsatisfiable. By Theorem 5.10,
Theorem 5.14 and (13), we have U ? 6= ; and thus G? 6= ;. There exists a hyperresolution
derivation 1 of  from K. Since (?, G? ) = {}, we know that support(1 )  K? by
(|). So K? is unsatisfiable. To show the only if direction of (2), assume that ~a 2 G and
~a 2 cert(q, K). Then there exists a hyperresolution 2 of Pq (~a) from K [ Rq . Similarly, by
(|), we know that support(2 )  Kq,G and hence ~a 2 cert(q, Kq,G ).
337

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

We next show inductively a statement from which (|) will follow. Let  = ( , T ) be
a derivation of a clause in (q, G) from K [ Rq , and let H = split(K). We have already
established (see proof of Theorem 5.10) that c-ChaseK is a model of H. Since ChaseH is a
universal model of H there exists a homomorphism  from ChaseH to c-ChaseK . We show
the following properties inductively for every node v in T .
a. track(K, q, G) |= R  , for each atom  in (v); and
b. track(K, q, G) |= Rel(dr ), if sk(r) is the main premise used to obtain the parent u of v.
We proceed by induction on the distance of v to the root of T .
Base case: In the base case we have that v is the root of T . Property (b) follows vacuously
since v has no parent in .
 If q = ?, then  is a derivation of the empty clause and (v) is the empty disjunction
and. So property (a) also follows vacuously.
 Otherwise, (v) = Pq (~a) for some ~a 2 G. By the definition of track(K, q, G) (point
(iii)) we have that ( (v))R 2 track(K, q, G) and hence property (a) also holds.
Inductive step: Assuming that properties (a) and (b) hold for a node u, we show that
they also hold for the children v1 , . . . , vn of u. Let r be the rule in K such that sk(r) is
the main premise in the relevant hyperresolution step with MGU , i.e., (u) = 1 _    _
m _ 1 _    _ n is the hyperresolvent of sk(r) =  1 _    _  n _ 1 _    _ m and
(vi ) = i _ i for 1  i  n, using . An easy observation of composition between a
substitution and a homomorphism is used later in the rest of the proof.
(

) = (  ) for an arbitrary function-free atom .

(22)

By Lemma 5.4 in Section 5.1 we have that each i 2 ChaseH for each 1  i  n. Since 
is a homomorphism from ChaseH into c-ChaseK we then have that ( i ) 2 c-ChaseK and
by (22), i (  ) 2 c-ChaseK for 1  i  n. We next show that track(K, q, G) |= move(r)  .
 If m = 0, then move(r) = P?R . We distinguish two cases.
 if q 6= ?, P?R 2 track(K, q, G) by point (iv);

 if q = ?, we have ?s 2 c-ChaseK and hence PqR 2 track(K, q, G) by point (iii).
 Otherwise, by the induction hypothesis, we also have that track(K, q, G) |= (
and again by (22), track(K, q, G) |= jR (  ) for 1  j  m.

j

)R 

Therefore track(K, q, G) |= move(r)  . Then the body of rules in (r) is satisfied by
the substitution  and hence track(K, q, G) |= Rel(dr ), and track(K, q, G) |= iR (  ) for
1  i  n. Again by (22), track(K, q, G) |= ( iR ) for 1  i  n. In addition, by the
induction hypothesis, we have track(K, q, G) |= R
i  , for each 1  i  n. Hence, have shown
that (a), (b) hold for each child vi of u.
It only remains to be shown that (a) and (b) imply (|). Indeed, take any  2 support().
338

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

 If  is a fact in K, then it is a leaf node of ; hence, by property (a) we have that
track(K, q, G) |= R  . But then, since  is a fact in DK the definition of homomorphism ensures that R  = R . By the definition of Kq,G this implies that  2 Kq,G .
 If  is a rule in K, then by Property (b) we have that track(K, q, G) |= Rel(d ). Again,
the definition of Kq,G ensures that  2 Kq,G .
This completes the proof of the theorem.
Note that Claim (|) in the proof of the theorem also establishes an important property of
the computed subsets, namely that they are proof-preserving; that is, the support of every
hyperresolution proof of a relevant gap answer in the original knowledge base K is also
contained in the computed subset. This has two key implications. First, every justification
(i.e., minimal subset of K entailing a gap answer) is contained also in the subset; in this
way, our subsets preserve all the formulas in K that are relevant to the gap answers, and all
formulas that are disregarded can be seen as irrelevant. Second, any fully-fledged reasoner
whose underpinning calculus can be cast in the framework of resolution will be able to
compute over the subset the same derivations of the gap answers as over K. Consequently,
in practice it is reasonable to expect that a fully-fledged reasoner will uniformly display
better performance on the computed subsets than over Kan expectation that was borne
out by our experiments.
We conclude this section with an example illustrating why the dataset in track(K, q, G)
(point 1 in Definition 6.1) is obtained from c-ChaseK  the materialisation underpinning
the upper bound in Section 5.2rather than c-ChasefK in Section 5.3.
Example 6.4. Consider the query q(x) = E(x) and the knowledge base K consisting of
the following rules and facts.
A(x) ! B(x) _ D(x)

D(x) ! E(x)

B(x) ! E(x)

A(a)

Let f be a function always choosing B(a) over D(a), then c-ChasefK = {A(a), B(a), E(a)}
and constant a is an answer to q(x) in the gap between lower and upper bound. Suppose
that we were to define track(K, q, G) as in Definition 6.1 but replacing the facts in point (i)
with those in c-ChasefK . Since D(a) does not hold in c-ChasefK the corresponding subset will
not contain the rule D(x) ! E(x), which is essential to derive E(a).
}
6.3 Optimisations of the Datalog Encoding
To conclude this section, we present two optimisations of the datalog encoding in Definition
6.1 that we will exploit in our system PAGOdA.
The first optimisation aims at reducing the size of the computed subsets. Recall that
the key step in the construction of the tracking knowledge base track(K, q, G) was to invert
the rules in K to capture hyperresolution proofs in a backwards fashion. Consider the
inversion (17) of rule (R5) in our running example. The eect of the inversion is to capture the applicability of hyperresolution: if facts Mammal(rabbit), HerbivoreR (rabbit) and
MeatEaterR (rabbit) hold, then we include rule (R5) in the subset since there may be a proof
339

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

in K involving a step where a ground clause Herbivore(rabbit) _ MeatEater(rabbit) _  is
obtained by resolving (R5) with Mammal(rabbit) _ .
Note, however, that such a step is redundant should Herbivore(rabbit) already be contained in K, in which case (R5) may not be needed in the relevant subset. We can capture
this observation by distinguishing in the tracking knowledge base those facts in the c-chase
of K that were not already present in the original dataset DK . We encode these implied
facts by instantiating fresh predicates P I for each predicate P in K. In our running example,
a fact MeatEaterI (rabbit) in the tracking knowledge base establishes that MeatEater(rabbit)
was not present in the original data. We then use atoms over these predicates as guards in
the inverted rules, e.g. rule (17) would now be written as follows:
HerbivoreI (x) ^ MeatEaterI (x) ^ HerbivoreR (x)

^ MeatEaterR (x) ^ Mammal(x) ! MammalR (x)

Formally, Definition 6.1 can be optimised as given next.
Definition 6.5. Let K, q, G and predicates P R be as in Definition 6.1. For each predicate
P , let P I be a fresh predicate of the same arity as P . We now redefine move(r) for each
rule r as the following conjunction of atoms:
 P?R if r of the form (2);


I x, ~
z1 )
1 (~



I x)
1 (~

^

R x, ~
z1 )
1 (~

^  ^

I x)
m (~

if r of the form (3); and

^

R x)
1 (~

^  ^

R x)
m (~

if r of the form (4).

Then, (r) is as in Definition 6.1, and track(K, q, G) is also as in Definition 6.1, but extended
with the addition of a fact P I (~a) for each fact P (~a) that is in c-ChaseK but not in DK . }
It is easy to see that this optimisation does not aect the correctness of Theorem 6.3:
if a disjunction of atoms is derived via hyperresolution, where one of the atoms is already
present in the data, then the disjunction is subsumed and can be dispensed with.
The second optimisation can be used to obtain a more succinct encoding for datalog
reasoners that support equality reasoning natively (such as RDFox). As already mentioned,
the built-in semantics of the equality predicate can be axiomatised within datalog. However,
axiomatisation can lead to performance issues, and scalability can be improved by a native
treatment of equality where equal objects are merged into a single representative of the
whole equivalence class.
The axiomatisation of equality has a significant eect in our tracking encoding. For
example, the replacement rules r of the form (EQ4) are inverted into the following rules in
(r) for each predicate P :
P R (x1 , . . . , xi

1 , y, xi+1 , . . . , xn )

P R (x1 , . . . , xi

1 , y, xi+1 , . . . , xn )

^ P (x1 , . . . , xn ) ^ xi  y ! P R (x1 , . . . , xn )
^ P (x1 , . . . , xn ) ^ xi  y ! R (xi , y)

(23)
(24)

where (23) is an tautology and can be dispensed with, but rule (24) is required. If the
datalog reasoner has native support for equality, then we do not need to include in the
340

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

tracking knowledge base the inversion of equality axioms (EQ1), (EQ2) or (EQ3), and we
only need to include rules (24) in order to ensure that the computed subset has the required
properties. The result is a more succinct encoding that can be materialised more efficiently.
Example 6.6. Consider a knowledge base K consists of facts {R(a1 , b), R(a2 , b), A(a1 )}
and the following rules.
A(x) ! B(x) _ C(x)

R(x1 , y) ^ R(x2 , y) ! x1  x2

(25)
(26)

B(x) ! D(x)
C(x) ! D(x)

(27)
(28)

Let q = D(x), the gap G between lower and upper bounds to q is {a1 , a2 }. It is easy the
see that rule (26) is essential to derive q(a2 ). To ensure that this rule is in the fragment
Kq,G , we have to track a1  a2 using an instance of rule (24).
}
6.4 Comparison with Magic Sets
The idea of inverting rules for recording relevant information has been heavily exploited in
Logic Programming. In particular, the magic set transformation (Bancilhon, Maier, Sagiv,
& Ullman, 1986) is a technique that, given a program and a query, optimises the materialisation process so as to derive only facts that are relevant to the query. Similarly to our
tracking encoding, the magic sets technique uses auxiliary predicates, called magic predicates, to identify the relevant facts. This technique was originally developed for datalog,
and was subsequently extended to handle also negation as failure (Beeri, Naqvi, Ramakrishnan, Shmueli, & Tsur, 1987; Kemp, Srivastava, & Stuckey, 1995) and disjunctions (Alviano,
Faber, Greco, & Leone, 2012a).
In contrast to magic sets, the goal of our transformation is not to reduce the size of the
materialisation, but rather to compute a relevant fragment of a knowledge base potentially
given in a very expressive (even undecidable) language, and to reduce this computation
to datalog reasoning. In this sense, our technique is orthogonal to magic sets. Indeed,
the benefits of our technique are only relevant for knowledge bases containing existentially
quantified and/or disjunctive rules (if K is datalog, then the query would have been fully
answered by the lower bound).
Furthermore, it is worth noticing that the way we invert (datalog) rules is also dierent
from magic sets and yields a more precise tracking. This is so because our assumption is
that tracking starts with an already computed materialisation (see Point (i) in Definition
6.1). For instance, given an already adorned rule A(x) ^ B(x) ! C(x), magic sets would
produce the following rules for deriving the magic predicates AM for A and B M for B:
C M (x) ! AM (x)

C M (x) ^ A(x) ! B M (x)

These rules can be used to derive a fact AM (a) from C M (a), even if A(a) cannot be used to
derive C(a) because the aforementioned rule is not applicable (e.g., if B(a) does not hold
and C(a) is derived using other rules). Our transformation, in contrast, would yield the
more restrictive rules
C R (x) ^ A(x) ^ B(x) ! AR (x)

C R (x) ^ A(x) ^ B(x) ! B R (x)

which are applicable to a only if both A(a) and B(a) hold in the materialisation.
341

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

7. Summarisation and Analysis of Answer Dependencies
In this section, let q be an input query dierent from the unsatisfiability query ?. Once
K? and Kq have been computed, we still need to check, using the fully-fledged reasoner,
the satisfiability of K? as well as whether Kq entails each candidate answer in Gq . This
can be computationally expensive if these subsets are large and complex, or there are many
candidate answers to verify. We therefore exploit summarisation techniques (Dolby et al.,
2007) in an eort to further reduce the number of candidate answers.
The idea behind summarisation is to shrink the data in the knowledge base by merging
all constants that instantiate the same unary predicates. Since summarisation is equivalent
to extending the knowledge base with equality assertions between constants, the summary
knowledge base entails the original one by the monotonicity of first-order logic. Consequently, we can exploit summarisation as follows:
1. If the satisfiability of K remains undetermined, we construct the summary of K? and
check its satisfiability. If it is satisfiable, then K? (and thus also K) is also satisfiable.
2. Construct the summary of Kq and then use the fully-fledged reasoner to check whether
the summary of ~a is entailed to be a certain answer of q in the summary of Kq ,
discarding any answers that are not so entailed.
Formally, summarisation is defined as follows.
Definition 7.1. A type T is a set of unary predicates; given a constant c in K, we say
that T = {A | A(c) 2 K} is the type for c. For each type T , let aT be a fresh constant
uniquely associated with T . The summary function over K is the substitution mapping
each constant c in K to aT , where T is the type for c. Finally, the summary of K is K . }
The following proposition shows how summarisation can be exploited to detect spurious
answers in our setting. Since summarisation can significantly reduce data size in practice,
and the relevant subsets K? and Kq are already significantly smaller than K, checking the
satisfiability of K? and of each gap answer against Kq becomes feasible in many cases, even
though doing so implies resorting to the fully-fledged reasoner.
Proposition 7.2. Let be the summary function over K. Satisfiability of K? implies the
following: (i) K is satisfiable; and (ii) cert(q, K)  cert(q , Kq ) for every CQ q.
Example 7.3. In the case of our running example, the constants tiger and lion both have
type {Mammal}, and are therefore mapped to a fresh constant, say tMammal , that is uniquely
associated with {Mammal}. Since tMammal is not a certain answer to qex w.r.t. the summary
of Kex , we can determine that both tiger and lion are spurious answers.
}
If summarisation did not succeed in pruning all candidate answers in G, we try in a
last step to further reduce the calls to the fully-fledged reasoner by exploiting dependencies
between the remaining candidate answers such that, if answer ~a depends on answer ~c, and
~a is spurious, then so is ~c.
Consider two tuples ~c and d~ of constants in Gq . Suppose that we can find an endomor~ If we can determine (by calling the fully-fledged
phism  of the dataset DK in which ~c = d.
~
reasoner) that d is a spurious answer, then so must be ~c; as a result, we no longer need to
call the fully-fledged reasoner to verify ~c. Such endomorphisms are defined next.
342

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Definition 7.4. Let ~c = (c1 , . . . , cn ) and d~ = (d1 , . . . , dn ) be n-tuples of constants from K.
An endomorphism from ~c to d~ in K is a mapping  from constants to constants such that
(i) ci  = di for each 1  i  n; (ii) P (t1 , . . . , tm ) 2 DK for each fact P (t1 , . . . , tm ) 2 DK ;
and (iii) r 2 K for each r 2 K .
}
The relevant property of endomorphisms is given in the following proposition.
Proposition 7.5. Let ~c, d~ be possible answers to q and let  be an endomorphism from ~c
to d~ in K. Then, ~c 2 cert(q, K) implies d~ 2 cert(q, K).
Proof. Since ~c 2 cert(q, K), we know that K |= q(~c). So there is a hyperresolution derivation
 = (T, ) of Pq (~c) from K [ Rq . It is easy to check that (T,
) is a hyperresolution
~ from K [ Rq . Then, K |= q(d)
~ and hence d~ 2 cert(q, K).
derivation of Pq (d)
We exploit this idea to compute a dependency graph having candidate answer tuples as
~ whenever an endomorphism in DK exists mapping ~c to d.
~ Since
nodes and an edge (~c, d)
computing endomorphisms is hard we resort in practice to a sound greedy algorithm to
approximate the dependency graph, which we describe in Section 8.

8. Implementation: The PAGOdA System
We have implemented our approach in a system called PAGOdA, which is written in Java
and is available under an academic license. Our system integrates the datalog reasoner
RDFox (Motik et al., 2014) and the fully-fledged OWL 2 reasoner HermiT (Glimm et al.,
2014) as black-boxes, and we also exploit the combined approach for ELHOr? (see Section
4.2) implemented in KARMA (Stefanoni et al., 2014).
PAGOdA accepts as input arbitrary OWL 2 DL ontologies, datasets in turtle format
(PrudHommeaux & Carothers, 2014) and CQs in SPARQL. Queries can be interpreted
under ground or certain answer semantics. In the former case, PAGOdA is sound and
complete. In the latter case, however, PAGOdA is limited by the capabilities of HermiT,
which can only check entailment of ground or DL concept queries; hence, PAGOdA can
guarantee completeness only if the lower and upper bounds match, or if the query can
be transformed into a DL concept query via internalisation (see Section 2.3). Otherwise,
PAGOdA returns a sound (but possibly incomplete) set of answers, along with a bound on
the incompleteness of the computed answer set.
The architecture of PAGOdA is depicted in Figure 4. Each box in Figure 4 represents a
component of PAGOdA, and indicates any external systems that are exploited within that
component. We could, in principle, use any materialisation-based datalog reasoner that
supports CQ evaluation and the incremental addition of facts, and any fully-fledged OWL
2 DL reasoner that supports fact entailment.
PAGOdA uses four instances of RDFox (one in each of the lower bound, c-chase, cchasef and subset extractor components) and two instances of HermiT (one in each of the
summary filter and dependency graph components).
The process of fully answering a query can be divided into several steps. Here, we distinguish between query independent steps and query dependent ones. As we can see in Figure
4, the loading ontology and materialisation steps are query independent. Therefore, both

343

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

cert(q, O [ D)
heuristic planner

G0  Gq

HermiT
q, Gq

summary filter
HermiT
q, Gq

endomorphism
checker

Full reasoning

Kq

Lq

subset extractor

tracking encoder

Extracting subsets

RDFox of D
track(, q, Gq )

, q, Gq

Gq

Lq
F

D

Computing query bounds

soundAnswers(q,  [ D)
certU3 (q,  [ D)

M2L
q
lower store
KARMA
RDFox

certU2 (q,  [ D)

M3U

M2U
q

q
c-chase

f

*

c-chase
RDFox

RDFox



Materialisation

D

shift

Loading ontology & data

profile checker

normaliser
HermiT clausifier
O

Figure 4: The architecture of PAGOdA
of them are counted as pre-processing steps. Computing query bounds, extracting subset
and full reasoning are query dependent, and are called query processing steps.
We next describe each component, following the process flow of PAGOdA.
8.1 Loading Ontology and Data
PAGOdA uses the OWL API to parse the input ontology O. The dataset D is given
separately in turtle format. The normaliser then computes the set of rules corresponding to
the axioms in the ontology. PAGOdAs normaliser is an extension of HermiTs clausification
component (Glimm et al., 2014), which transforms axioms into so-called DL-clauses (Motik
et al., 2009). The dataset is loaded directly into (the four instances of) RDFox.
After normalisation, the ontology is checked to determine if it is inside OWL 2 RL
or ELHOr? . If an input ontology is in OWL 2 RL (resp. ELHOr? ), then RDFox (resp.
KARMA) is already sound and complete, and in such cases PAGOdA simply processes the
344

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

ontology, dataset and queries using the relevant component. Otherwise, PAGOdA uses a
dedicated program shifting component to enrich the deterministic part of the ontology with
additional information from disjunctive rules (see Section 4.1), resulting in a set of rules .
8.2 Materialisation
There are three components involved in this step, namely lower bound, c-chase and cchasef . Each of these takes as input  and D, and each computes a materialisation (shown
in Figure 4 as ellipses). The lower bound component performs Steps 1 and 2 from Section 4.3
in order to compute an aggregated lower bound M2L . The c-chase and c-chasef components
compute the M2U and M3U upper bound materialisations as described in Section 5.4 using a
dedicated implementation of the c-chase algorithm. The chase sequence is stored in RDFox,
and the applicability of existential and disjunctive rules is determined by posing SPARQL
queries to RDFox. When applying a disjunctive rule (while computing M3U ), PAGOdA
uses a choice function to select one of the disjuncts. As discussed in Section 5.4, the choice
function should try to select disjuncts that will not (eventually) lead to a contradiction. To
this end, PAGOdA implements the following heuristics.
 We construct a standard dependency graph containing an edge from predicate P to
Q if there is a rule where P occurs in the body and Q in the head. Then, we compute
a preference ordering on the predicates occurring in a disjunction according to their
distance from ? in the dependency graph, preferring those that are furthest from ?.
 We exploit the result of materialising D using the shifting enriched rules in  (see
Section 4.1). If a fact of the form P (~a) is obtained in the materialisation, then P (~a)
follows from the knowledge base. Hence, if we have obtained P (~a), then we try to
avoid choosing P (~a) from a disjunct P (~a) _ during chase computation.
If M2L contains a contradiction, then the input ontology and dataset is unsatisfiable,
and PAGOdA reports this and terminates. If ?s is derived in M3U , then the computation
is aborted and M3U is no longer used. If M2U contains ?s , then PAGOdA checks the
satisfiability of  [ D; in eect, it computes cert(?,  [ D). If the answer to this query is
non-empty, then the input ontology and dataset is unsatisfiable, and PAGOdA reports this
and terminates; otherwise the input ontology and dataset is satisfiable, and PAGOdA is
able to answer queries.
8.3 Computing Query Bounds
Given a query q, PAGOdA uses the M2L lower bound materialisation to compute the lower
bound answer Lq . In order to do this it exploits KARMAs implementation of the filtration
procedure (algorithm soundAnswers in Section 4.2), but for clarity this step is shown separately (as a circle with an F in it) in Figure 4. If ?s was not derived when computing the
M3U materialisation, U q = cert(q, M2U ) \ cert(q, M3U ); otherwise U q = cert(q, M2U ). In either
case U q is computed directly by using RDFox to answer q w.r.t. the relevant materialisation.
Extracting Subsets The tracking encoder component implements the datalog encoding
based on Definition 6.1 with the optimisations described in Section 6.3. The resulting
datalog knowledge base is added to the rules and data in the c-chase component, and
345

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

RDFox is used to extend the c-chase materialisation accordingly. The freshly derived facts
(over the tracking predicates introduced by the tracking encoder) are then passed to the
subset extractor component, which uses these facts to identify all the facts and rules that
are relevant for checking gap answers, and computes the intersection between relevant facts
and the input dataset D by querying an instance of RDFox containing D only.
8.4 Full Reasoning
PAGOdA uses HermiT to verify gap answers in Gq = U q \ Lq . As HermiT only accepts
queries given either as facts or DL concepts, we have implemented the standard rolling-up
technique to transform internalisable CQs. In the summary filter component, PAGOdA uses
HermiT to filter out gap answers that are not entailed by a summary of Kq (see Section 7).
The remaining gap answers G0  Gq are then passed to the endomorphism checker, which
exploits a greedy algorithm to compute an incomplete dependency graph between answers
in G0 . This graph is used by the heuristic planner to optimise the order in which the answers
in G0 are checked using HermiT (see Section 7). Verified answers from G0 are combined
with the lower bound Lq to give cert(q, O [ D).
The implementation of summarisation is straightforward: PAGOdA essentially merges
all constants having the same (explicit) types in the data.

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10
11
12

Input: a knowledge base K = K [ DK , two tuples (a1 , . . . , an ), and (b1 , . . . , bn ).
Output: return true if an endomorphism from (a1 , . . . , an ) to (b1 , . . . , bn ) in K is found,
otherwise, false.
= ;;
foreach i 2 [1..n] do
if ai is not locally embeddable into bi in K then return false;
else (ai ) = bi ;
end
foreach i 2 [1..n] do
if not check(ai , bi ) then return false;
end
if K 6= K then return false;
else return true;
Subroutine check(a, b)
Oa := {c | P (ai , c) 2 DK }, Ia := {c | P (c, ai ) 2 DK };
Ob := {d | P (bi , d) 2 DK }, Ib := {d | P (d, bi ) 2 DK };
foreach S 2 {O, I} and c 2 Sa do
D := {d 2 Sb | c can be locally embedded into d};
if D is empty then return false;
if is not defined on c then
(c) := d where d is the most similar constant to c in D;
if not check(c, d) then return false;
end
else if (c) 62 D then return false;
end

Algorithm 1: Greedy endomorphism checker.

346

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

We next describe the greedy algorithm implemented in PAGOdA for checking answer
dependencies (see Algorithm 1). Given tuples (a1 , . . . , an ) and (b1 , . . . , bn ), the algorithm
returns True if it is able to find an endomorphism, and False otherwise. The algorithm
considers each constant ai and tries to map it into bi locally, in the sense that only the
immediate neighbourhoods of of ai and bi are considered at this stage. Formally, this is
captured by the following notion of local embedding.
Definition 8.1. Given K and a constant c let Mc be the multiset containing an occurrence
of A for each fact A(c) 2 DK , an occurrence of P for each binary fact P (c, c0 ) 2 DK , and an
occurrence of P for each binary fact P (c0 , c) 2 DK .
Given constants c and d in K, we say that c is locally embeddable into d if each predicate
in Mc occurs (with any cardinality) in Md .
}
The check(a, b) subroutine implements greedy search by looking at the immediate neighbours of a and b. Specifically, the subroutine considers each neighbour c of a and picks a
neighbour d of b such that c can be locally embedded into d. When several choices of d
are available, the algorithm heuristically chooses one according to the Jaccard similarity
between multisets Mc and Md .6 The algorithm terminates with success if it manages to
compute a mapping that is defined on all constants that are reachable from {a1 , . . . , an }
in K. It is immediate to see that the computed is an endomorphism from ~a to ~b in K; thus,
the algorithm is sound. The algorithm works in polynomial time as the choices made in the
construction of are never revisited and local embeddability can be checked efficiently.

9. Related Work
Conjunctive query answering over ontology-enriched datasets has received a great deal of
attention in recent years. Its computational complexity has been thoroughly investigated for
a wide range of KR languages and a number of practicable algorithms have been proposed
in the literature and implemented in reasoning systems.
9.1 Computational Complexity of CQ Answering
The decision problem associated to CQ answering is conjunctive query entailment (CQE),
namely to decide whether K |= q(~a) when given as input a CQ q, a possible answer ~a,
and a knowledge base K expressed in a (fixed) language L. This problem is well-known
to be undecidable in general, even if q is restricted to be atomic and L is the language of
existential rules (Dantsin et al., 2001).
CQE for knowledge bases stemming from OWL DL ontologies is decidable under the
assumption that the query does not mention transitive relations (Rudolph & Glimm, 2010).
Decidability of CQE for unrestricted OWL DL or OWL 2 DL ontologies and CQs remains
an open problem. Even in the cases where CQE is decidable, it is typically of very high
computational complexity. CQE is 2-ExpTime-complete for the expressive DLs SHIQ
and SHOQ (Glimm et al., 2008; Eiter, Lutz, Ortiz, & Simkus, 2009). Hardness results
6. The Jaccard similarity for multisets M and M 0 is defined as |M \ M 0 |/|M [ M 0 |, where |M \ M 0 | counts
the minimum number of occurrences of each common element in M and M 0 , whereas |M [ M 0 | counts
the sum of occurrences of elements in M and M 0 .

347

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

for 2-ExpTime are obtained already for ALCI (Lutz, 2008) as well as for Horn-SROIQ,
which underpins the Horn fragment of OWL 2 DL (Ortiz, Rudolph, & Simkus, 2011). CQE
for ALC and SHQ, which do not involve inverse roles, is ExpTime-complete (Lutz, 2008).
Single exponential time results are also obtained for Horn DLs by disallowing complex role
inclusion axioms: CQE is ExpTime-complete Horn-SHOIQ, which underpins the Horn
fragment of OWL DL (Ortiz et al., 2011).
Given the high complexity of CQE, there has recently been an increasing interest in
lightweight DLs for which CQE is computationally easier. Such lightweight DLs have been
incorporated in the OWL 2 standard as profiles (Motik et al., 2012). CQE in the OWL
2 EL profile is PSpace-complete (Stefanoni et al., 2014). Furthermore, the complexity of
CQE drops to NP if complex role inclusions (with the exception only of transitivity and
reflexivity) are disallowed in OWL 2 EL (Stefanoni & Motik, 2015). The latter complexity
is rather benign since CQE over databases is already NP-hard. Finally, CQE for the OWL
2 QL profile is also NP-complete (Calvanese et al., 2007). Regarding data complexity,
CQE is coNP-complete for non-Horn DLs, such as ALE (Schaerf, 1993). In contrast,
data complexity is PTime-complete for Horn DLs that can encode recursion, such as HornSROIQ and OWL 2 EL (Ortiz et al., 2011; Stefanoni et al., 2014). Finally, data complexity
is known to be in AC0 for the OWL 2 QL profile (Calvanese et al., 2007).
The complexity of CQE is also well understood for rule-based KR languages. For plain
datalog, it is ExpTime-complete in combined complexity and PTime-complete w.r.t. data
complexity. For disjunctive datalog, it is coNExpTime-complete in combined complexity
and coNP-complete w.r.t. data complexity. Datalog refers to a family of decidable KR
languages based on existential rules (Cal, Gottlob, & Lukasiewicz, 2012). This includes
guarded (Cal et al., 2013), sticky (Cal, Gottlob, & Pieris, 2011), and acyclic (Cuenca Grau
et al., 2013) datalog . The extension of datalog languages with disjunctive rules has been
recently studied in (Alviano et al., 2012b; Bourhis et al., 2013).
Finally, we refer to ground query entailment (GCQE) as the problem of checking whether
a tuple ~a is a ground answer to q(~x) = 9~y '(~x, ~y ) w.r.t. K. In KR languages that allow for
existentially quantified rules, the restriction to ground answers typically makes CQE easier:
the definition of ground answers means that GCQE can be trivially reduced to satisfiability
checking. Consequently, GCQE is decidable for OWL 2 DL.
9.2 Practical Query Answering Approaches
Some o-the-shelf DL reasoners, such as Pellet (Sirin et al., 2007) and HermiT (Glimm
et al., 2014) provide support for query answering. Pellet supports SPARQL conjunctive
queries and also implements the rolling-up technique. In contrast, HermiT does not provide
a SPARQL API and it only supports CQs in the form of (complex) DL concepts. Racer
was among the first DL reasoners to implement and optimise CQ answering under ground
semantics (Haarslev, Hidde, Moller, & Wessel, 2012). Finally, there has also been intensive
work on optimising query answering in DL systems, including filter-and-refine techniques
(Wandelt et al., 2010), ordering strategies of query atoms (Kollia & Glimm, 2013), and data
summarisation (Dolby et al., 2009). Optimising CQ answering in DL reasoners is complementary to our approach, as the use of a more optimised DL reasoner could significantly
improve the performance of PAGOdA on queries that require full reasoning.

348

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

RDF triple stores typically implement materialisation-based (a.k.a. forward chaining)
reasoning algorithms, and answer queries by evaluating them over the resulting materialisation. Jena (McBride, 2001) and Sesame (Broekstra, Kampman, & van Harmelen, 2002)
were among the first such systems to provide support for RDF-Schema. Modern triple stores
such as OWLim (Bishop et al., 2011), and Oracles native inference engine (Wu et al., 2008),
provide extended suppport for ontologies in the RL profile. Additionally, RDFox (Motik
et al., 2014) supports arbitrary datalog over unary and binary predicates. Finally, ASP
engines such as DLV (Leone, Pfeifer, Faber, Eiter, Gottlob, Perri, & Scarcello, 2006) implement sound and complete reasoning for (extensions of) disjunctive datalog. Although triple
stores exhibit appealing scalability, they can support only restricted ontology languages;
however, as with DL reasoners, improving the scalability of triple stores is complementary
to our approach, and advances in this area can be directly exploited in PAGOdA.
A technique for CQ answering over lightweight DLs that is receiving increasing attention
is the so-called combined approach (Lutz et al., 2009; Stefanoni et al., 2013; Kontchakov,
Lutz, Toman, Wolter, & Zakharyaschev, 2011). In the combined approach the dataset is
first augmented with new facts in a query-independent way to build (in polynomial time) a
model of the ontology. This model can be exploited for query answering in two equivalent
ways. In the approach by Lutz et al. (2009) and Kontchakov et al. (2011) the query is first
rewritten and then evaluated against the constructed model. Alternatively, in the work of
Stefanoni et al. (2013) and Lutz et al. (2013) the query is first evaluated over the model
and then unsound answers are eliminated by means of a polynomial time filtration process.
Combined approaches have been applied to logics of the EL family (Lutz et al., 2009;
Stefanoni et al., 2013) as well as DL-Lite (Kontchakov et al., 2011), and in PAGOdA, we
use the implementation of (Stefanoni et al., 2013) to compute the aggregated lower bound.
CQ answering over Horn ontologies is often realised by means of query rewriting techniques. A rewriting of a query q w.r.t. an ontology O is another query q 0 that captures all
information from O necessary to answer q over an arbitrary dataset. Unions of CQs and
datalog are common target languages for query rewriting. Query rewriting enables the reuse
of optimised data management system: UCQs can be answered using standard relational
databases, whereas datalog queries can be evaluated using a triple store. Query rewriting
has been successfully applied to OWL 2 QL ontologies, where rewritability into UCQs is
guaranteed. Example systems include QuOnto (Acciarri, Calvanese, De Giacomo, Lembo,
Lenzerini, Palmieri, & Rosati, 2005), Mastro (Calvanese, De Giacomo, Lembo, Lenzerini,
Poggi, Rodriguez-Muro, Rosati, Ruzzi, & Savo, 2011), Rapid (Chortaras, Trivela, & Stamou, 2011), Prexto (Rosati, 2012), and Ontop (Bagosi, Calvanese, Hardi, Komla-Ebri,
Lanti, Rezk, Rodriguez-Muro, Slusnys, & Xiao, 2014). Some of these systems have been
successful in large scale applications; however, they are only applicable to OWL 2 QL and
the size of the rewriting can be exponential in the size of the ontology. Datalog-based query
rewriting has been implemented in systems such as REQUIEM (Perez-Urbina et al., 2010),
which supports the extension of ELHOr? with inverse roles. The introduction of inverse
roles, however, leads to a significant jump in complexity: query answering over ELHOr?
is NP-complete (and tractable for atomic queries), whereas it becomes ExpTime-complete
once inverse roles are introduced (furthermore, ExpTime-hardness holds already for unsatisfiability checking and atomic queries). In practice, restricting ourselves to ELHOr?
allows us to compute a datalog program of linear size in a straightforward way by Skolemis349

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

ing existentially quantified variables into constants. Furthermore, datalog materialisation is
query independent and all queries without existentially quantified variables can be answered
directly over the materialisation, where more complex queries are answered using filtration.
Finally, similarly to PAGOdA, the system Hydrowl (Stoilos, 2014a) combines an OWL
2 RL reasoner with a query rewriting system and a fully-fledged DL reasoner in order to
answer conjunctive queries over an OWL 2 knowledge base. The techniques in Hydrowl are,
however, rather dierent to those in PAGOdA. Hydrowl uses two dierent query answering
strategies. The first one is based on repairing (Stoilos, 2014b) and query rewriting, and is
applicable only to ontologies for which a suitable repair exists. The second strategy exploits
a query base: a set of atomic queries that Hydrowl computes in a pre-processing phase, and
that can be fully answered using the triple store for the given ontology and an arbitrary
dataset. When answering a query q, Hydrowl checks if q is covered by query base (Stoilos
& Stamou, 2014); if it is, then q can be completely evaluated using the OWL 2 RL reasoner;
otherwise, the fully-fledged reasoner is used to answer q. However, the computation of the
query base does not appear to be correct in general,7 and we believe that this accounts for
the apparent incompleteness of Hydrowl in some of our tests (see Section 10.3.1).
9.3 Approximate Reasoning
The idea of transforming the ontology, data and/or query to obtain lower and upper bound
answers has been already explored in previous work. The Screech system (Tserendorj et al.,
2008) uses KAON2 (Hustadt, Motik, & Sattler, 2007) to transform a SHIQ ontology
into a (exponential size) disjunctive datalog program in such a way that ground answers to
queries are preserved. Subsequently, Screech can exploit (unsound or incomplete) techniques
to transform disjunctive datalog into plain datalog. In this way, Screech computes only
an approximation of the answer. TrOWL (Thomas et al., 2010) exploits approximation
techniques to transform an OWL 2 ontology into an ontology in the QL profile (Pan &
Thomas, 2007). The approximation first computes the closure of the input ontology under
entailment of OWL 2 QL axioms, and then disregards all axioms outside OWL 2 QL.
Related approximations into OWL 2 QL have also been proposed, e.g., by Wandelt et al.
(2010) and Console et al. (2014). Efficient approximation strategies for OWL 2 ontologies
are again complementary to our approach, as they can be exploited by PAGOdA in order
to refine lower and upper bound query answers.

10. Evaluation
We have evaluated our query answering system PAGOdA on a range of realistic and benchmark ontologies, datasets and queries, and we have compared its performance with stateof-the-art query answering systems. Our test data and the systems used for comparison
are introduced in Sections 10.1 and 10.2, respectively. Our results are discussed in Section
10.3. Experiments were conducted on a 32 core 2.60GHz Intel Xeon E5-2670 with 250GB of
RAM, and running Fedora 20. All test ontologies, queries, and results are available online.8
7. Stoilos (2014a) mentions a limitation in automatically extracting [the atomic queries].
8. http://www.cs.ox.ac.uk/isg/tools/PAGOdA/2015/jair/

350

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

LUBM(n)
UOBM(n)
FLY
NPD
DBPedia+
ChEMBL
Reactome
Uniprot

]axioms
93
186
14,447
771
1,716
2,593
559
442

]rules
133
234
18,013
778
1,744
2,960
575
459

]9-rules
15
23
8396
128
11
426
13
20

]_-rules
0
6
0
14
5
73
23
43

]facts
n  105
2.6n  105
8  103
3.8  106
2.9  107
2.9  108
1.2  107
1.2  108

Table 3: Statistics for test datasets
10.1 Test Ontologies and Queries
Table 3 summarises our test data. The first two columns in the table indicate the total
number of DL axioms in each test ontology as well as the total number of rules after
normalisation. We are interested in ontologies that are not captured by OWL 2 RL and
hence cannot be fully processed by RDFox; thus, the number of rules containing existential
quantification and disjunction is especially relevant and is given in the third and fourth
columns of the table, respectively. Finally, the rightmost column lists the number of data
facts in each dataset.
LUBM and UOBM are widely-used reasoning benchmarks (Guo, Pan, & Heflin, 2005;
Ma, Yang, Qiu, Xie, Pan, & Liu, 2006). The ontology axioms in these benchmarks have
been manually created and are considered fixed, whereas the data is synthetically generated
according to a parameter n that determines its size. LUBM and UOBM come with 14 and 15
standard queries, respectively. To make the tests on LUBM more challenging, we extended
the benchmark with 10 additional queries for which datalog lower-bound answers are not
guaranteed to be complete (as is the case for the standard queries).
FLY is a realistic ontology that describes the anatomy of the Drosophila and which is
currently integrated in the Virtual Fly Brain tool.9 Although the data is rather small
compared to other test cases (about 8, 000 facts), the ontology is very rich in existentially
quantified rules, which makes query answering especially challenging. We tested 6 realistic
queries that were provided by the developers of the ontology.
NPD FactPages is an ontology describing the petroleum activities in the Norwegian
continental shelf. The ontology comes with a realistic dataset containing 3.8 million facts.
Unfortunately, for NPD we have no realistic queries so we tested all atomic queries over the
signature of the ontology.
DBPedia contains information about Wikipedia entries. Although the dataset is rather
large, the ontology axioms are simple and can be captured by OWL 2 RL. To provide
a more challenging test, we have used the ontology matching system LogMap (JimenezRuiz & Cuenca Grau, 2011) to extend DBPedia with a tourism ontology containing both
9. http://www.virtualflybrain.org/site/vfb site/overview.htm

351

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

existential and disjunctive rules. As in the case of NPD we have no example test queries,
so we focused our evaluation on atomic queries.
ChEMBL, Reactome, and Uniprot are realistic ontologies that have been made publicly available through the European Bioinformatics Institute (EBI) linked data platform.10
These ontologies are especially interesting for testing purposes. On the one hand, both the
ontology axioms and data are realistic and are being used in a number of applications; on
the other hand, the ontologies are rich in both existentially quantified and disjunctive rules,
and the datasets are extremely large. Furthermore, the EBI website provides a number of
example queries for each of these ontologies. In order to test scalability on these datasets as
well as to compare PAGOdA with other systems we implemented a data sampling algorithm
based on random walks (Leskovec & Faloutsos, 2006) and computed subsets of the data of
increasing size. We have used for evaluation those example queries that correspond to CQs
as well as all atomic queries over the relevant signature.
10.2 Comparison Systems
We compared PAGOdA against four ontology reasoners: HermiT (v.1.3.8), Pellet (v.2.3.1),
TrOWL-BGP (v.1.2), and Hydrowl (v.0.2). With the single exception of TrOWL, all these
systems implement sound and complete algorithms for standard reasoning tasks over OWL
2 DL ontologies, including ontology consistency checking and concept instance retrieval.
Additionally, all but HermiT provide support for SPARQL queries.
As pointed out in the Section 9, there are many other systems that can answer queries
over ontologies. However, these systems have generally been designed for specific fragments
of OWL 2, and are incomplete for ontologies outside these fragments. Although TrOWL
is also incomplete for OWL 2, it has been included in the evaluation because it is, on the
one hand, a widely-used system in Semantic Web applications and, on the other hand, it is
similar to PAGOdA in that it exploits ontology approximation techniques. In what follows,
we describe the capabilities of these systems in more detail.
HermiT is a fully-fledged OWL 2 reasoner based on the hypertableau calculus (Motik
et al., 2009; Glimm et al., 2014). HermiT focuses on standard reasoning tasks in DLs.
It does not provide a SPARQL or conjunctive query answering API, but it is capable of
answering atomic queries over unary predicates and checking fact entailment.
Pellet is a tableau-based OWL 2 DL reasoner with support for CQ answering (Sirin et al.,
2007). Pellet provides a SPARQL API, and hence it can compute the set of all ground
answers to arbitrary conjunctive queries expressed in SPARQL. Pellet is also capable of
computing all certain answers to internalisable conjunctive queries using the rolling-up
technique (see Section 2.3).
TrOWL is a system based on approximated reasoning. It accepts as input an arbitrary
OWL 2 DL ontology and a CQ in SPARQL, and aims at computing all ground answers to
the given query (Thomas et al., 2010). TrOWL exploits a technique that approximates the
input ontology into the OWL 2 QL profile, and it does not provide completeness guarantees.
10. http://www.ebi.ac.uk/rdf/platform

352

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

correct#

incomplete#

unsound#

error#

Kmeout#

cannot#handle#

100%#
90%#
80%#
70%#
60%#
50%#

Tr

Tr

Pe He Hy

Pe He Hy

Tr

Pe He Hy

Tr

Tr

Pe He Hy

Pe He Hy

Tr

Pe He Hy

Tr

Pe He Hy

Tr

Pe He Hy

Tr

Pe He Hy

Tr

Pe He Hy

40%#
30%#
20%#
10%#

#1
%
#
Pr
ot
Un
i

EM

Re
ac
to
m
e#
10
%
#

BL
#1
%
#

#
DB
Pe
d

ia
Ch

NP
D#

Fa

ct
Pa
ge
s#

le
dU
p#
ro
l
Y_
FL

1_
ro
l

le
dU
p#

1#
UO
BM

UO
BM

le
dU
p#
1_
ro
l
LU
BM

LU
BM

1#

0%#

Figure 5: Quality of the answers computed by each system. The four bars for each ontology
represent Trowl, Pellet, HermiT and Hydrowl respectively.
Hydrowl (Stoilos, 2014a) is a hybrid reasoning system that is similar in spirit to PAGOdA
(see Section 9.2 for a detailed comparison). Hydrowl integrates the triple store OWLim and
HermiT. It accepts as input an arbitrary OWL 2 ontology and conjunctive queries as rules,
and then computes the ground answers to the query.
10.3 Experiments and Results
We have performed three dierent experiments. In the first experiment, we compared
PAGOdA with the above mentioned systems, with respect to both the quality of their
answers (i.e., the number of correctly answered queries) and their performance relative to
PAGOdA. In the second experiment, we evaluated the scalability by considering datasets
of increasing size. Finally, in the third experiment, we evaluated the eectiveness of each
of the dierent reasoning techniques implemented in PAGOdA.
10.3.1 Comparison with Other Systems
We have compared PAGOdA with the other systems on our test ontologies. We used
LUBM(1) and UOBM(1) since they are already rather hard for some of the systems. Similarly, we used relatively small samples of the EBI platform ontologies (1% of the data for
ChEMBL and UniProt, and 10% for Reactome) that can be processed by the majority
of systems. For each test ontology we computed all ground answers to the corresponding
test queries, and whenever possible we used internalisation (see Section 2.3) to additionally
compute all certain answers. In the case of FLY, all test queries yield an empty set of
ground answers, so in this case we computed only the certain answers (all FLY queries can
be internalised). We set timeouts of 20 minutes for answering each individual query, and 5
hours for answering all the queries over a given ontology.
Figure 5 summarises the quality of the answers computed by each reasoner. Each bar
in the figure represents the performance of a particular reasoner w.r.t. a given ontology and
353

fiPellet"

HermiT"

Hydrowl"

DB
Pe
d

TrOWL"

ct
Pa
ge
s"

Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

1000"

100"

10"

"1
%
"
Pr
ot
Un
i

EM

Re
ac
to
m
e"
10
%
"

BL
"1
%
"

"
ia
Ch

Fa
NP
D"

le
dU
p"
ro
l
Y_
FL

1_
ro
l

le
dU
p"

1"
UO
BM

le
dU
p"
1_
ro
l

UO
BM

0"

LU
BM

LU
BM

1"

1"

Figure 6: Performance comparison with other systems. Each bar depicts the total time to
answer all test queries for the relevant ontology in comparison with PAGOdA.
set of test queries. We use green to indicate the percentage of queries for which the reasoner
computed all the correct answers, where correctness was determined by majority voting,
and blue (resp. purple) to indicate the percentage of queries for which the reasoner was
incomplete (resp. unsound). Red, orange and grey indicate, respectively, the percentage of
queries for which the reasoner reported an exception during execution, did not accept the
input query, or exceeded the timeout. Under our criterion of correctness, PAGOdA was
able to correctly compute all answers for every query and test ontology within the given
timeouts. Consequently, the performance of PAGOdA is not represented in the figure.
Figure 6 summarises the performance of each system relative to PAGOdA, but in this
case we considered only those queries for which the relevant system yields an answer (even
if the computed answer is unsound and/or incomplete). This is not ideal, but we chose
to consider all such queries (rather than only the queries for which the relevant system
yields the correct answer) because (i) the resulting time measurement is obviously closer
to the time that would be required to correctly answer all queries; and (ii) correctness is
only relative as we do not have a gold standard for query answers. For each ontology and
reasoner, the corresponding bar shows t2 /t1 (on a logarithmic scale), where t1 (resp. t2 ) is
the total time required by PAGOdA (resp. the compared system) to compute the answers to
the queries under consideration; a missing bar indicates that the comparison system failed
to answer any queries within the given timeout. Please note that two dierent bars for the
same ontology are not comparable as they may refer to dierent sets of queries, so each bar
needs to be considered in isolation.
We can draw the following conclusions from the results of our experiments.
 TrOWL is faster than PAGOdA on LUBM with rolling up, UOBM with rolling up
and FLY with rolling up, but it is incomplete for 7 out of 14 LUBM queries and 3 out
of 4 UOBM queries. For ChEMBL, TrOWL exceeds the timeout while performing the
satisfiability check. For the remaining ontologies, PAGOdA is more efficient in spite
of the fact that TrOWL is incomplete for some queries, and even unsound for several
UniProt queries.
354

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

 Pellet is one of the most robust systems in our evaluation. Although it times out for
the FLY ontology, it succeeds in computing all answers in the remaining cases. We
can observe, however, that in all cases Pellet is significantly slower than PAGOdA,
sometimes by more than two orders of magnitude.
 HermiT can only answer queries with one distinguished variable, so we could not
evaluate atomic binary queries. We can see that HermiT exceeds the timeout in many
cases. In the tests where HermiT succeeds, it is significantly slower than PAGOdA.
 Although Hydrowl is based on a theoretically sound and complete algorithm, it was
found to be incomplete in some of our tests. It also exceeded the timeout on all queries
for three of the ontologies, ran out of memory on all queries for another two of the
ontologies, and reported an exception for ChEMBL 1%. In the remaining cases, it
was significantly slower than PAGOdA.
10.3.2 Scalability Tests
We tested the scalability of PAGOdA on LUBM, UOBM and the ontologies from the EBI
linked data platform. For LUBM we used datasets of increasing size with a step of n =
100. For UOBM we also used increasingly large datasets with step n = 100 and we also
considered a smaller step of n = 5 for hard queries. Finally, in the case of EBIs datasets,
we implemented a data sampling algorithm based on random walks and computed subsets
of the data of increasing sizes from 1% of the original dataset up to 100% in steps of
10%. We used the test queries described in Section 10.1 for each of these ontologies; as in
Section 10.3.1, we computed ground answers and, whenever possible, used internalisation
to additionally compute certain answers. For each test ontology we measured the following:
 Pre-processing time. This includes all pre-processing steps in Section 8 as well as
satisfiability checking (i.e., query processing for the Boolean unsatisfiability query).
 Query processing time. This is the time to perform the query processing steps for
a query in the given ontology. We organise the test queries into the following three
groups depending on the techniques exploited by PAGOdA to compute their answers:
 G1: queries for which the lower and upper bounds coincide;
 G2: queries with a non-empty gap, but for which summarisation is able to filter
out all remaining candidate answers; and
 G3: queries where the fully-fledged reasoner is called over an ontology subset on
at least one of the test datasets.
In the scalability test, we set a timeout of 5 hours for answering all queries and 2.5 hours
for each individual query. For LUBM and UOBM, we increased the size of the dataset until
PAGOdA exceeded the timeout; for the other ontologies, PAGOdA was able to answer all
queries within the timeout, even with the largest dataset.
Pellet was the only compared system found to be sound and complete for our test
ontologies and queries, so we have also conducted scalability tests on it. The scalability of
Pellet is, however, limited: it already failed on LUBM(100), UOBM(5), as well as ChEMBL
355

fi3.0#

G1(18)"

2.5#

Thousands)seconds)

Thousands)seconds)

Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

2.0#
1.5#
1.0#

Q32"

Q34"

9"
8"
7"
6"
5"
4"
3"
2"

0.5#

1"

0.0#

0"

1#

100#

200#

300#

400#

500#

600#

700#

800#

1"

200"

300"

400"

500"

600"

700"

800"

(b) LUBM query processing

14"

G1(18)"

12"

Thousands)seconds)

Thousands)seconds)

(a) LUBM pre-processing

100"

10"
8"
6"

G2(1)"

Q18"

2.5"
2"
1.5"
1"

4"

0.5"

2"
0"
1"

100"

200"

300"

400"

0"

500"

0"

(c) UOBM pre-processing

100"

200"

300"

400"

500"

(d) UOBM query processing

Figure 7: Scalability tests on benchmarks
10% and Uniprot 10%. The only dataset were Pellet managed to process at least two data
samples was Reactome, where it succeeded on all samples smaller than 40%. The case for
Reactome is discussed in detail later on.
Our results are summarised in Figures 7 and 8. For each ontology, we plot time against
the size of the input dataset, and for query processing we distinguish dierent groups of
queries as discussed above. PAGOdA behaves relatively uniformly for queries in G1 and
G2, so we plot only the average time per query for these groups. In contrast, PAGOdAs
behaviour for queries in G3 is quite variable, so we plot the time for each individual query.
LUBM(n) As shown in Figure 7a, pre-processing is fast, and times appear to scale linearly with increasing dataset size. All LUBM queries belong to either G1 or G3 with the
latter group containing just two queries. Figure 7b illustrates the average query processing
time for queries in G1, which never exceeds 13 seconds, as well as the time for each of the
two queries in G3 (Q32 and Q34), which reaches 8,000 seconds for LUBM(800), most of
which is accounted for by HermiT.
UOBM(n) As shown in Figure 7c, pre-processing times are significantly higher than for
LUBM, reflecting the increased complexity of the ontology, but still appear to scale linearly
with dataset size. As with LUBM, most test queries were contained in G1, and their
processing times never exceeds 8 seconds from UOBM(1) to UOBM(500). We found one
query in G2. Processing times for this query were somewhat longer than for those in G1
and reached 569s for UOBM(500). Finally, we found one query (Q18) that, due to UOBMs
356

fi12"

G1(1896)#

10"

Seconds(

Thousands))seconds)

PAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

8"
6"
4"
2"
0"
1%"

10%" 20%" 30%" 40%" 50%" 60%" 70%" 80%" 90%" 100%"

0.50#
0.45#
0.40#
0.35#
0.30#
0.25#
0.20#
0.15#
0.10#
0.05#
0.00#
1%# 10%# 20%# 30%# 40%# 50%# 60%# 70%# 80%# 90%# 100%#

(a) ChEMBL pre-processing
Pellet"

G1(128)"

14"

Seconds(

Hundreds(seconds(

PAGOdA"

(b) ChEMBL query processing

12"

G2(1)"

Q65"

Pellet_Q65"

1000"
800"

10"
8"

600"

6"

400"

4"
200"

2"
0"

0"
10%" 20%"

30%"

40%"

50%" 60%"

70%"

80%"

90%" 100%"

10%" 20%" 30%" 40%" 50%" 60%" 70%" 80%" 90%" 100%"

(c) Reactome pre-processing
Unsa9sable#

G1(236)"

2.0#

Seconds(

Thousands)seconds)

Satsiable#

(d) Reactome query processing

1.5#

G2(4)"

25"
20"
15"

1.0#
10"

0.5#

5"

0.0#

0"

1%# 10%# 20%# 30%# 40%# 50%# 60%# 70%# 80%# 90%# 100%#

1%"

(e) Uniprot pre-processing

10%"

20%"

30%"

40%"

(f) Uniprot query processing

Figure 8: Scalability tests on EBI linked data platform
randomised data generation, was in dierent groups for dierent datasets: in UOBM(1),
UOBM(10) and UOBM(50) it was in G3, and HermiT was called on the relevant subsets
to fully answer the query; in UOBM(40) it was in G2, and HermiT was called on only the
summary of the relevant subset; and in all the remaining cases shown in Figure 7d it was
in G1, and the lower and upper bounds coincided. This query timed out in UOBM(50),
due to the time taken by HermiT to reason over the relevant subset, but we have shown
the times for the remaining G1 and G2 queries up to UOBM(500).
ChEMBL As shown in Figure 8a, pre-processing times are significant but manageable,
and again appear to scale linearly with dataset size. All test queries were contained in G1.

357

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Total
L1 + U 1
L2 + U 1
L2 + U 2
L2 + U2|3

LUBM
(100)
35
26
33
33
33

UOBM
(1)
20
4
4
12
16

FLY

NPD

DBPedia

6
0
5
5
5

478
442
442
442
473

1247
1240
1241
1241
1246

ChEMBL
1%
1896
1883
1883
1883
1896

Reactome
10%
130
82
82
98
128

Uniprot
1%
240
204
204
204
236

Table 4: ]Queries answered by dierent bounds
Figure 8b illustrates the average processing times for all queries, which was less than 0.5s
for all datasets and increases smoothly with dataset size.
Reactome As shown in Figure 8c, pre-processing times again appear to scale quite
smoothly. Groups G2 and G3 each contained one query, with all the remaining queries
belonging to G1. Query processing times are shown in Figure 8d. Average query processing time for queries in G1 never exceeded 10 seconds. Average processing times for G2
queries appeared to grow linearly to the size of datasets, and average time never exceeded
10 seconds. Finally, it can be seen that the G3 query (Q65) is much more challenging, but
it could still be answered in less than 900 seconds, even for the largest dataset.
As already mentioned, we also tested the scalability of Pellet on Reactome, where Pellet
is able to process the samples of size 10%, 20% and 30%. The pre-processing time of Pellet
on these datasets is comparable with PAGOdA as shown in Figure 8c. Average queryprocessing times for queries in G1 and G2 are slightly higher than those of PAGOdA. In
contrast, times for query Q65 were significantly higher: 445s, 518s and 2, 626s for Reactome
10%, 20% and 30%, respectively (see Figure 8d). Processing times for Q65 in PAGOdA,
however, grow smoothly thanks to the eectiveness of the subset extraction technique, which
is able to keep the input to the fully-fledged reasoner small, even for the largest datasets.
Uniprot In contrast to the other cases, Uniprot as a whole is unsatisfiable; our sampling
technique can, however, produce a satisfiable subset. Figure 8e illustrates pre-processing
times. As can be seen, these drop abruptly for unsatisfiable samples (50% and larger); this
is because unsatisfiability can be efficiently detected in the lower bound. The figure shows
that time to detect inconsistency for 100% is even less than that for 90%; this is because
the time is dominated by loading time, and I/O performance varies from run to run. Query
processing times were only considered for satisfiable samples (see Figure 8f). There were
no queries in G3, and only four in G2. We can observe that average times for all queries
appear to scale linearly with data size for both groups.
10.3.3 Effectiveness of the Implemented Techniques
We have evaluated the eectiveness of the various reasoning techniques implemented in
PAGOdA by comparing the numbers of test queries that can be fully answered using the
relevant technique.
Query bounds In Sections 4 and 5 we described dierent techniques for computing lower
and upper bound query answers. Table 4 illustrates the eectiveness of each of these bounds
358

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Facts
Rules

LUBM
0.5%
3.7%

UOBM
10.4%
10.9%

Fly
7.3%
0.9%

NPD
16.5%
18.4%

DBPedia
9  10 5 %
2.4%

Reactome
5.2%
5.3%

Uniprot
4  10 4 %
1.1%

Table 5: Size of the largest subsets given as percentage over input rules and facts.
in terms of the number of queries for which the bounds coincided on our test ontologies. In
the table, we refer to the lower bound described in Section 4.1 as L1 and to the aggregated
lower bound described in Section 4.3 as L2 . Similarly, we refer to the three upper bound
computation techniques discussed in Section 5.4 as U1 , U2 , U3 and the combined upper
bound U2|3 . We can observe the following from our experiments:
 The basic lower and upper bounds suffice to answer most of the queries in many
test ontologies. In particular, L1 and U1 matched in 26 out of the 35 queries for
LUBM(100), 442 out of 478 for NPD, 240 out of 1247 for DBPedia, 1883 out of 1896
for ChEMBL, and 204 out of 240 for Uniprot.
 The aggregated lower bound L2 was very eective in the case of FLY, where the basic
bounds did not match for any query. It was also useful for LUBM, yielding matching
bounds for 7 more queries.
 The refined treatment of existential rules described in Section 5.2, which yields the
upper bound U2 , was especially eective for UOBM(1) and Reactome, where many
existentially quantified rules were already satisfied by the lower bound materialisation.
 Finally, the refined treatment of disjunctive rules in Section 5.3, which yields the combined upper bound U2|3 , was instrumental in obtaining additional matching bounds
for non-Horn ontologies. We could answer an additional 4 queries for UOBM(1), 31
for NPD, 5 for DBPedia, 13 for ChEMBL, 30 for Reactome, and 32 for Uniprot.
Overall, we obtained matching bounds for most queries in all our test ontologies: we
could answer all queries for ChEMBL, and all but 1 for FLY and DBPedia, all but 2 for
Reactome and LUBM(100), all but 4 for UOBM(1) and Uniprot, and all but 5 for NPD.
Subset extraction Table 5 shows, for each dataset, the maximum percentage of facts
and rules that are included in the relevant subset over all test queries with non-matching
bounds. We can observe that subset extraction is eective in all cases in terms of both facts
and rules. For Uniprot and DBPedia, the reduction in data size was especially dramatic.
It is also interesting to observe the large reduction in the number of rules for FLY, which
is a rather complex ontology. Finally, subset extraction was least eective for NPD and
UOBM, but even in these cases there was a reduction of almost one order of magnitude in
the size of both ontology and dataset.
We now turn our attention to summarisation and dependency analysis. The eectiveness
of these techniques was measured by the number of hard calls to HermiT that were required
to fully answer each query, where a call to HermiT is considered hard if the knowledge base
passed to HermiT is not a summary. The first row of Table 6 shows the number of gap
359

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

L2 + U2|3
+ Sum
+ Dep

LUBM
26 14
26 14
1
1

264
264
1

UOBM
112 1470
0 1444
0
1

264
264
1

FLY
344
344
7

DBPedia
10
0
0

NPD
326
0
0

Reactome
18
52
0
52
0
37

UniProt
168
0
0

Table 6: The number of hard calls to HermiT to fully answer each query
answers for each query where the L2 and U2|3 bounds do not match. Without optimisation,
we would have to call HermiT this number of times to fully answer each query. Row 2
(resp. row 3) shows the number of hard calls to HermiT after applying summarisation (resp.
summarisation plus dependency analysis). As we mentioned above, there are respectively 5
and 4 queries with non-matching bounds for NPD and UniProt. However, for each of these
groups, summarisation and dependency analysis have identical eects on all the queries in
the group, so we present just one representative query for each ontology.
Summarisation As already discussed, summarisation enables PAGOdA to fully answer
a number of test queries with non-empty gaps. It was instrumental in fully answering one
query for each of UOBM(1), DBPedia and Reactome, as well as 5 queries for NPD, and 4
queries for Uniprot. Even in the cases where summarisation did not suffice to fully answer
the query, it was eective in reducing the size of the gap. For instance, for one of the queries
for UOBM(1) we obtained 1,470 gap answers, of which 26 were ruled out by summarisation.
Dependency analysis In LUBM(100) there were two queries with a gap of 26 answers
and 14 answers, respectively; in both cases, all answers were merged into a single group, and
hence a single call to HermiT sufficed to complete the computation. Similarly, in UOBM(1)
a single call to HermiT was again sufficient, even though the three queries with a gap
involved a large number of candidate answers. For FLY, there are 344 answers remaining
to be verified after summarisation, but only 7 hard calls to HermiT were required. Finally,
in the case of Reactome one query had 52 gap answers, but dependency analysis reduced
the number of calls to HermiT to 37.

11. Conclusions
In this paper, we have investigated a novel pay-as-you-go approach to conjunctive query
answering that combines a datalog reasoner with a fully-fledged reasoner. The key feature
of our approach is that it delegates the bulk of the computation to the datalog reasoner
and resorts to the fully-fledged reasoner only as necessary to fully answer the query.
The reasoning techniques we have proposed here are very general and are applicable to
a wide range of knowledge representation languages. Our main goal in practice, however,
has been to realise our approach in a highly scalable and robust query answering system for
OWL 2 DL ontologies, which we have called PAGOdA. Our extensive evaluation has not only
confirmed the feasibility of our approach in practice, but also that our system PAGOdA
significantly ourperforms state-of-the art reasoning systems in terms of both robustness
and scalability. In particular, our experiments using the ontologies in the EBI linked data
platform have shown that PAGOdA is capable of fully answering queries over highly complex
and expressive ontologies and realistic datasets containing hundreds of millions of facts.
360

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Acknowledgments
This is an extended version of our conference publications (Zhou, Nenov, Cuenca Grau, &
Horrocks, 2014; Zhou, Nenov, Grau, & Horrocks, 2013). This work has been supported
by the Royal Society under a Royal Society Research Fellowship, by the EPSRC projects
Score!, MaSI3 , and DBOnto, as well as by the EU FP7 project Optique.

References
Abiteboul, S., Hull, R., & Vianu, V. (Eds.). (1995). Foundations of Databases: The Logical
Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &
Rosati, R. (2005). QuOnto: Querying ontologies. In Veloso, M. M., & Kambhampati,
S. (Eds.), AAAI 2005, Proceedings of the Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications of Artificial Intelligence
Conference, July 9-13, 2005, Pittsburgh, Pennsylvania, USA, pp. 16701671. AAAI
Press / The MIT Press.
Afrati, F. N., Cosmadakis, S. S., & Yannakakis, M. (1995). On datalog vs. polynomial time.
J. Comput. Syst. Sci., 51 (2), 177196.
Alviano, M., Faber, W., Greco, G., & Leone, N. (2012a). Magic sets for disjunctive datalog
programs. Artificial Intelligence, 187188, 156192.
Alviano, M., Faber, W., Leone, N., & Manna, M. (2012b). Disjunctive datalog with existential quantifiers: Semantics, decidability, and complexity issues. Theory and Practice
of Logic Programming, 12 (4-5), 701718.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In IJCAI 2015,
Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,
Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 364369.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (2003).
The Description Logic Handbook: Theory, Implementation, and Applications. Cambridge Univ. Press.
Bagosi, T., Calvanese, D., Hardi, J., Komla-Ebri, S., Lanti, D., Rezk, M., Rodriguez-Muro,
M., Slusnys, M., & Xiao, G. (2014). The Ontop framework for ontology based data access. In Zhao, D., Du, J., Wang, H., Wang, P., Ji, D., & Pan, J. Z. (Eds.), CSWS 2014,
Proceedings of the Semantic Web and Web Science - 8th Chinese Conference, Wuhan,
China, August 8-12, 2014, Revised Selected Papers, Vol. 480 of Communications in
Computer and Information Science, pp. 6777. Springer.
Bancilhon, F., Maier, D., Sagiv, Y., & Ullman, J. D. (1986). Magic sets and other strange
ways to implement logic programs. In Silberschatz, A. (Ed.), Proceedings of the Fifth
ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, March 2426, 1986, Cambridge, Massachusetts, USA, pp. 115. ACM.
Beeri, C., Naqvi, S. A., Ramakrishnan, R., Shmueli, O., & Tsur, S. (1987). Sets and negation
in a logic database language (LDL1). In Vardi, M. Y. (Ed.), Proceedings of the Sixth
361

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,
March 23-25, 1987, San Diego, California, USA, pp. 2137. ACM.
Bishop, B., Kiryakov, A., Ognyano, D., Peikov, I., Tashev, Z., & Velkov, R. (2011).
OWLIM: A family of scalable semantic repositories. Semantic Web, 2 (1), 3342.
Bourhis, P., Morak, M., & Pieris, A. (2013). The impact of disjunction on query answering under guarded-based existential rules. In IJCAI 2013, Proceedings of the 23rd
International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9,
2013, pp. 796802. AAAI Press.
Broekstra, J., Kampman, A., & van Harmelen, F. (2002). Sesame: A generic architecture
for storing and querying RDF and RDF schema. In Horrocks, I., & Hendler, J. A.
(Eds.), ISWC 2002, Proceedings the Semantic Web - First International Semantic
Web Conference, Sardinia, Italy, June 9-12, 2002, Proceedings, Vol. 2342 of Lecture
Notes in Computer Science, pp. 5468. Springer.
Bry, F., Eisinger, N., Eiter, T., Furche, T., Gottlob, G., Ley, C., Linse, B., Pichler, R., & Wei,
F. (2007). Foundations of rule-based query answering. In Antoniou, G., Amann, U.,
Baroglio, C., Decker, S., Henze, N., Patranjan, P., & Tolksdorf, R. (Eds.), Reasoning
Web 2007, Vol. 4636 of Lecture Notes in Computer Science, pp. 1153. Springer.
Cal, A., Gottlob, G., & Kifer, M. (2013). Taming the infinite chase: Query answering
under expressive relational constraints. Journal of Artificial Intelligence Research, 48,
115174.
Cal, A., Gottlob, G., & Lukasiewicz, T. (2012). A general datalog-based framework for
tractable query answering over ontologies. J. Web Sem., 14, 5783.
Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010). Datalog+/-: A
family of logical knowledge representation and query languages for new applications.
In LICS 2010, Proceedings of the 25th Annual IEEE Symposium on Logic in Computer
Science, 11-14 July 2010, Edinburgh, United Kingdom, pp. 228242. IEEE Computer
Society.
Cal, A., Gottlob, G., & Pieris, A. (2011). New expressive languages for ontological query
answering. In Burgard, W., & Roth, D. (Eds.), AAAI 2011, Proceedings of the TwentyFifth AAAI Conference on Artificial Intelligence, San Francisco, California, USA,
August 7-11, 2011, Vol. 2, pp. 15411546. AAAI Press.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M.,
Rosati, R., Ruzzi, M., & Savo, D. F. (2011). The MASTRO system for ontology-based
data access. Semantic Web, 2 (1), 4353.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning and efficient query answering in description logics: The DL-Lite family.
Journal of Automated Reasoning, 39 (3), 385429.
Chortaras, A., Trivela, D., & Stamou, G. B. (2011). Optimized query rewriting for OWL
2 QL. In Bjrner, N., & Sofronie-Stokkermans, V. (Eds.), CADE 23, Proceedings of
the 23rd International Conference on Automated Deduction, Wroclaw, Poland, July
31 - August 5, 2011, Vol. 6803 of Lecture Notes in Computer Science, pp. 192206.
Springer.
362

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Console, M., Mora, J., Rosati, R., Santarelli, V., & Savo, D. F. (2014). Eective computation
of maximal sound approximations of description logic ontologies. In ISWC 2014,
Proceedings of the Semantic Web - 13th International Semantic Web Conference,
Riva del Garda, Italy, October 19-23, 2014. Proceedings, Part II, pp. 164179.
Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.
(2013). Acyclicity notions for existential rules and their application to query answering
in ontologies. Journal of Artificial Intelligence Research, 47, 741808.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.
(2008). OWL 2: The next step for OWL. Journal of Web Semantics, 6 (4), 309322.
Cuenca Grau, B., Motik, B., Stoilos, G., & Horrocks, I. (2012). Completeness guarantees for
incomplete ontology reasoners: Theory and practice. Journal of Artificial Intelligence
Research, 43, 419476.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity and expressive
power of logic programming. ACM Computing Surveys, 33 (3), 374425.
Dolby, J., Fokoue, A., Kalyanpur, A., Kershenbaum, A., Schonberg, E., Srinivas, K., &
Ma, L. (2007). Scalable semantic retrieval through summarization and refinement. In
AAAI 2007, Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 299304. AAAI
Press.
Dolby, J., Fokoue, A., Kalyanpur, A., Schonberg, E., & Srinivas, K. (2009). Scalable highly
expressive reasoner (SHER). Journal of Web Semantics, 7 (4), 357361.
Eiter, T., Fink, M., Tompits, H., & Woltran, S. (2004). Simplifying logic programs under
uniform and strong equivalence. In LPNMR 2004, Proceedings of Logic Programming
and Nonmonotonic Reasoning - 7th International Conference, Fort Lauderdale, FL,
USA, January 6-8, 2004, Proceedings, pp. 8799.
Eiter, T., Lutz, C., Ortiz, M., & Simkus, M. (2009). Query answering in description logics
with transitive roles. In Boutilier, C. (Ed.), IJCAI 2009, Proceedings of the 21st
International Joint Conference on Artificial Intelligence, Pasadena, California, USA,
July 11-17, 2009, pp. 759764.
Eiter, T., Ortiz, M., & Simkus, M. (2012). Conjunctive query answering in the description
logic SH using knots. Journal of Computer and System Sciences, 78 (1), 4785.

Erling, O., & Mikhailov, I. (2009). Virtuoso: RDF support in a native RDBMS. In Virgilio,
R. D., Giunchiglia, F., & Tanca, L. (Eds.), Semantic Web Information Management
- A Model-Based Perspective, pp. 501519. Springer.
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: An OWL 2
reasoner. Journal of Automated Reasoning, 53 (3), 245269.
Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Conjunctive query answering for
the description logic SHIQ. Journal of Artificial Intelligence Research, 31, 157204.

Grosof, B. N., Horrocks, I., Volz, R., & Decker, S. (2003). Description logic programs:
combining logic programs with description logic. In Hencsey, G., White, B., Chen,
Y. R., Kovacs, L., & Lawrence, S. (Eds.), WWW 2003, Proceedings of the Twelfth
363

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

International World Wide Web Conference, Budapest, Hungary, May 20-24, 2003,
pp. 4857. ACM.
Guo, Y., Pan, Z., & Heflin, J. (2005). LUBM: A benchmark for OWL knowledge base
systems. Journal of Web Semantics, 3 (2-3), 158182.
Haarslev, V., Hidde, K., Moller, R., & Wessel, M. (2012). The RacerPro knowledge representation and reasoning system. Semantic Web, 3 (3), 267277.
Horrocks, I., Kutz, O., & Sattler, U. (2006). The even more irresistible SROIQ. In KR
2006, Proceedings of the Tenth International Conference on Principles of Knowledge
Representation and Reasoning, Lake District of the United Kingdom, June 2-5, 2006,
pp. 5767.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). From SHIQ and RDF
to OWL: the making of a web ontology language. Journal of Web Semantics, 1 (1),
726.
Horrocks, I., & Tessaris, S. (2000). A conjunctive query language for description logic
aboxes. In Kautz, H. A., & Porter, B. W. (Eds.), AAAI/IAAI 2000, Proceedings of the
Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on
on Innovative Applications of Artificial Intelligence, July 30 - August 3, 2000, Austin,
Texas, USA., pp. 399404. AAAI Press / The MIT Press.
Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning in description logics by a reduction
to disjunctive datalog. Journal of Automated Reasoning, 39 (3), 351384.
Jimenez-Ruiz, E., & Cuenca Grau, B. (2011). LogMap: Logic-based and scalable ontology
matching. In Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy,
N. F., & Blomqvist, E. (Eds.), ISWC 2011, The Semantic Web - 10th International
Semantic Web Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part
I, Vol. 7031 of Lecture Notes in Computer Science, pp. 273288. Springer.
Kemp, D. B., Srivastava, D., & Stuckey, P. J. (1995). Bottom-up evaluation and query
optimization of well-founded models. Theoretical Computer Science, 146 (12), 145
184.
Kollia, I., & Glimm, B. (2013). Optimizing SPARQL query answering over OWL ontologies.
Journal of Artificial Intelligence Research, 48, 253303.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). The combined approach to ontology-based data access. In Walsh, T. (Ed.), IJCAI 2011,
Proceedings of the 22nd International Joint Conference on Artificial Intelligence,
Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 26562661. IJCAI/AAAI.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
The DLV system for knowledge representation and reasoning. ACM Transactions on
Computational Logic, 7 (3), 499562.
Leskovec, J., & Faloutsos, C. (2006). Sampling from large graphs. In KDD 2006, Proceedings
of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, Philadelphia, PA, USA, August 20-23, 2006, pp. 631636.

364

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Lutz, C. (2008). The complexity of conjunctive query answering in expressive description logics. In Armando, A., Baumgartner, P., & Dowek, G. (Eds.), IJCAR 2008,
Proceedings of the 4th International Joint Conference Automated Reasoning, Sydney,
Australia, August 12-15, 2008, Vol. 5195 of Lecture Notes in Computer Science, pp.
179193. Springer.
Lutz, C., Seylan, I., Toman, D., & Wolter, F. (2013). The combined approach to OBDA:
Taming role hierarchies using filters. In Alani, H., Kagal, L., Fokoue, A., Groth, P. T.,
Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K. (Eds.),
ISWC 2013, Proceedings of the Semantic Web - 12th International Semantic Web
Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part I, Vol.
8218 of Lecture Notes in Computer Science, pp. 314330. Springer.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering in the description logic EL using a relational database system. In Boutilier, C. (Ed.), IJCAI
2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009, pp. 20702075.
Ma, L., Yang, Y., Qiu, Z., Xie, G. T., Pan, Y., & Liu, S. (2006). Towards a complete OWL
ontology benchmark. In Sure, Y., & Domingue, J. (Eds.), ESWC 2006, The Semantic
Web: Research and Applications, 3rd European Semantic Web Conference, Budva,
Montenegro, June 11-14, 2006, Proceedings, Vol. 4011 of Lecture Notes in Computer
Science, pp. 125139. Springer.
Manola, F., & Miller, E. (2004). RDF primer. W3C Recommendation. Available at
http://www.w3.org/TR/rdf-primer/.
Marnette, B. (2009). Generalized schema-mappings: from termination to tractability.
In PODS 2009, Proceedings of the Twenty-Eigth ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems, June 19 - July 1, 2009, Providence,
Rhode Island, USA, pp. 1322.
McBride, B. (2001). Jena: Implementing the RDF model and syntax specification. In
SemWeb 2001, Proceedings of the Second International Workshop on the Semantic
Web.
Moller, R., Neuenstadt, C., Ozcep, O. L., & Wandelt, S. (2013). Advances in accessing
big data with expressive ontologies. In Timm, I. J., & Thimm, M. (Eds.), KI 2013,
Proceedings of Advances in Artificial Intelligence - 36th Annual German Conference
on AI, Koblenz, Germany, September 16-20, 2013, Vol. 8077 of Lecture Notes in
Computer Science, pp. 118129. Springer.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2012). OWL 2
Web Ontology Language Profiles (second edition). W3C Recommendation. Available
at http://www.w3.org/TR/owl2-profiles/.
Motik, B., Nenov, Y., Piro, R., Horrocks, I., & Olteanu, D. (2014). Parallel materialisation
of datalog programs in centralised, main-memory RDF systems. In Brodley, C. E., &
Stone, P. (Eds.), AAAI 2014, Proceedings of the Twenty-Eighth AAAI Conference on
Artificial Intelligence, July 27 -31, 2014, Quebec City, Quebec, Canada., pp. 129137.
AAAI Press.
365

fiZhou, Cuenca Grau, Nenov, Kaminski, & Horrocks

Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau reasoning for description logics.
Journal of Artificial Intelligence Research, 36, 165228.
Ortiz, M., Rudolph, S., & Simkus, M. (2011). Query answering in the horn fragments of
the description logics SHOIQ and SROIQ. In IJCAI 2011, Proceedings of the 22nd
International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain,
July 16-22, 2011, pp. 10391044.
Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. In AAAI 2007,
Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July
22-26, 2007, Vancouver, British Columbia, Canada, pp. 14341439.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering and rewriting
under description logic constraints. Journal of Applied Logic, 8 (2), 186209.
PrudHommeaux, E., & Carothers, G. (2014). RDF 1.1 Turtle. W3C Recommendation.
Available at http://www.w3.org/TR/turtle/.
Robinson, J. A., & Voronkov, A. (Eds.). (2001). Handbook of Automated Reasoning (in 2
volumes). Elsevier and MIT Press.
Rodriguez-Muro, M., & Calvanese, D. (2012). High performance query answering over
DL-Lite ontologies. In Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.), KR 2012,
Proceedings of Principles of Knowledge Representation and Reasoning, the Thirteenth
International Conference, Rome, Italy, June 10-14, 2012, pp. 308318. AAAI Press.
Rosati, R. (2012). Prexto: Query rewriting under extensional constraints in DL - lite.
In Simperl, E., Cimiano, P., Polleres, A., Corcho, O., & Presutti, V. (Eds.), ESWC
2012, Proceedings of the Semantic Web: Research and Applications - 9th Extended
Semantic Web Conference, Heraklion, Crete, Greece, May 27-31, 2012, Vol. 7295 of
Lecture Notes in Computer Science, pp. 360374. Springer.
Rudolph, S., & Glimm, B. (2010). Nominals, inverses, counting, and conjunctive queries or:
Why infinity is your friend!. Journal of Artificial Intelligence Research, 39, 429481.
Schaerf, A. (1993). On the complexity of the instance checking problem in concept languages
with existential quantification. In Komorowski, H. J., & Ras, Z. W. (Eds.), ISMIS
1993, Proceedings of Methodologies for Intelligent Systems, 7th International Symposium, Trondheim, Norway, June 15-18, 1993, Vol. 689 of Lecture Notes in Computer
Science, pp. 508517. Springer.
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2007). Pellet: A practical
OWL-DL reasoner. Journal of Web Semantics, 5 (2), 5153.
Staab, S., & Studer, R. (Eds.). (2004). Handbook on Ontologies. International Handbooks
on Information Systems. Springer.
Stefanoni, G., & Motik, B. (2015). Answering conjunctive queries over EL knowledge bases
with transitive and reflexive roles. In Bonet, B., & Koenig, S. (Eds.), AAAI 2015,
Proceedings of the 29th AAAI Conference on Artificial Intelligence, Austin, TX, USA.
AAAI Press. To appear.
Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals to the combined
query answering approaches for EL. In AAAI 2013, Proceedings of the Twenty-Seventh
AAAI Conference on Artificial Intelligence, pp. 11771183.
366

fiPAGOdA: Pay-As-You-Go Query Answering Using a Datalog Reasoner

Stefanoni, G., Motik, B., Krotzsch, M., & Rudolph, S. (2014). The complexity of answering
conjunctive and navigational queries over OWL 2 EL knowledge bases. Journal of
Artificial Intelligence Research, 51, 645705.
Stoilos, G. (2014a). Hydrowl: A hybrid query answering system for OWL 2 DL ontologies.
In RR 2014, Proceedings of Web Reasoning and Rule Systems - 8th International
Conference, Athens, Greece, September 15-17, 2014, pp. 230238.
Stoilos, G. (2014b). Ontology-based data access using rewriting, OWL 2 RL systems and
repairing. In Presutti, V., dAmato, C., Gandon, F., dAquin, M., Staab, S., & Tordai,
A. (Eds.), The Semantic Web: Trends and Challenges - 11th International Conference,
ESWC 2014, Anissaras, Crete, Greece, May 25-29, 2014. Proceedings, Vol. 8465 of
Lecture Notes in Computer Science, pp. 317332. Springer.
Stoilos, G., & Stamou, G. B. (2014). Hybrid query answering over OWL ontologies. In
Schaub, T., Friedrich, G., & OSullivan, B. (Eds.), ECAI 2014 - 21st European Conference on Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic - Including Prestigious Applications of Intelligent Systems (PAIS 2014), Vol. 263 of Frontiers
in Artificial Intelligence and Applications, pp. 855860. IOS Press.
Thomas, E., Pan, J. Z., & Ren, Y. (2010). Trowl: Tractable OWL 2 reasoning infrastructure.
In ESWC 2010, Proceedings of the Semantic Web: Research and Applications, 7th
Extended Semantic Web Conference, Heraklion, Crete, Greece, May 30 - June 3, 2010,
Part II, pp. 431435.
Tserendorj, T., Rudolph, S., Krotzsch, M., & Hitzler, P. (2008). Approximate OWLreasoning with screech. In Calvanese, D., & Lausen, G. (Eds.), RR 2008, Proceedings
of Web Reasoning and Rule Systems, Second International Conference, Karlsruhe,
Germany, October 31-November 1, 2008, Vol. 5341 of Lecture Notes in Computer
Science, pp. 165180. Springer.
W3C SPARQL Working Group (2013). SPARQL 1.1 Overview. W3C Recommendation.
Available at http://www.w3.org/TR/sparql11-overview/.
Wandelt, S., Moller, R., & Wessel, M. (2010). Towards scalable instance retrieval over
ontologies. International Journal of Software and Informatics, 4 (3), 201218.
Wu, Z., Eadon, G., Das, S., Chong, E. I., Kolovski, V., Annamalai, M., & Srinivasan, J.
(2008). Implementing an inference engine for RDFS/OWL constructs and user-defined
rules in oracle. In Alonso, G., Blakeley, J. A., & Chen, A. L. P. (Eds.), ICDE 2008,
Proceedings of the 24th International Conference on Data Engineering, April 7-12,
2008, Cancun, Mexico, pp. 12391248. IEEE.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL query
answering using a triple store. In Proceedings of the Twenty-Eighth AAAI Conference
on Artificial Intelligence.
Zhou, Y., Nenov, Y., Grau, B. C., & Horrocks, I. (2013). Complete query answering over
horn ontologies using a triple store. In The Semantic Web - ISWC 2013 - 12th
International Semantic Web Conference, Sydney, NSW, Australia, October 21-25,
2013, Proceedings, Part I, pp. 720736.

367

fiJournal of Artificial Intelligence Research 54 (2015) 83-122

Submitted 02/15; published 09/15

Word vs. Class-Based Word Sense Disambiguation
Ruben Izquierdo

RUBEN . IZQUIERDOBEVIA @ VU . NL

VU University of Amsterdam
Amsterdam. The Netherlands

Armando Suarez

ARMANDO @ DLSI . UA . ES

University of Alicante
Alicante. Spain

German Rigau

GERMAN . RIGAU @ EHU . ES

University of the Basque Country
San Sebastian. Spain

Abstract
As empirically demonstrated by the Word Sense Disambiguation (WSD) tasks of the last SensEval/SemEval exercises, assigning the appropriate meaning to words in context has resisted all
attempts to be successfully addressed. Many authors argue that one possible reason could be the
use of inappropriate sets of word meanings. In particular, WordNet has been used as a de-facto
standard repository of word meanings in most of these tasks. Thus, instead of using the word
senses defined in WordNet, some approaches have derived semantic classes representing groups
of word senses. However, the meanings represented by WordNet have been only used for WSD
at a very fine-grained sense level or at a very coarse-grained semantic class level (also called SuperSenses). We suspect that an appropriate level of abstraction could be on between both levels.
The contributions of this paper are manifold. First, we propose a simple method to automatically
derive semantic classes at intermediate levels of abstraction covering all nominal and verbal WordNet meanings. Second, we empirically demonstrate that our automatically derived semantic classes
outperform classical approaches based on word senses and more coarse-grained sense groupings.
Third, we also demonstrate that our supervised WSD system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples.
Finally, we also demonstrate the robustness of our supervised semantic class-based WSD system
when tested on out of domain corpus.

1. Introduction
Word Sense Disambiguation (WSD) is an intermediate Natural Language Processing (NLP) task
that consists in assigning the correct lexical interpretation to ambiguous words depending on the surrounding context (Agirre & Edmonds, 2007; Navigli, 2009). One of the most successful approaches
in the last years is the supervised learning from examples, in which Machine Learning classification
models are induced from semantically annotated corpora (Marquez, Escudero, Martnez, & Rigau,
2006). Quite often, machine learning systems have obtained better results than the knowledge-based
ones, as shown by experimental work and international evaluation exercises such as Senseval or SemEval1 . Nevertheless, lately some weakly supervised or knowledgebased approaches are reaching
a performance close to the supervised techniques on some specific tasks. In all these tasks, the
1. All the information about these competitions can be found at http://www.senseval.org.
c
2015
AI Access Foundation. All rights reserved.

fiI ZQUIERDO , S U AREZ & R IGAU

corpora are usually manually annotated by experts with word senses taken from a particular lexical
semantic resource, most commonly WordNet (Fellbaum, 1998).
However, WordNet has been widely criticized for being a sense repository that often provides
too finegrained sense distinctions for higher level applications like Machine Translation (MT) or
Question & Answering (AQ). In fact, WSD at this low level of semantic granularity has resisted
all attempts of inferring robust broad-coverage models. It seems that many wordsense distinctions
are too subtle to be captured by automatic systems with the current small volumes of wordsense
annotated examples. Using WordNet as a sense repository, the organizers of the English all-words
task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder & Palmer, 2004). Interestingly, this result is difficult to outperform by state-of-the-art sense-based WSD systems.
Moreover, supervised sensebased approaches are too biased towards the most frequent sense or
the predominant sense on the training data. Therefore, the performance of supervised sensebased
systems is strongly punished when applied to domain specific texts where the sense distribution differs considerably with respect to the sense distribution in the training corpora (Escudero, Marquez,
& Rigau., 2000).
In this paper we try to overcome these problems by facing the task of WSD from a Semantic
Class point of view instead of the traditional word sense based approach. A semantic class can be
seen as an abstract concept that groups subconcepts and word senses sharing some semantic properties or features. Examples of semantic classes are VEHICLE, FOOD or ANIMAL. Our hypothesis is
that using an appropriate set of semantic classes instead of word-senses could help WSD in several
aspects:
 A higher level of abstraction could ease the integration of WSD systems into other higher
level NLP applications such as Machine Translation or Question & Answering
 Grouping together semantically coherent sets of training examples could also increase the
robustness of supervised WSD systems
 The socalled bottleneck acquisition problem could also be alleviated
These points will be further explained along the paper. Following this hypothesis we propose to
create classifiers based on semantic classes instead of word sense experts. One semantic classifier
will be trained for each semantic class and the final system will assign the proper semantic class to
each ambiguous word (instead of the sense as in traditional approaches). For example, using our
automatically derived semantic classes (that will be introduced later), the three senses of church in
WordNet 1.6 are subsumed by the semantic classes R ELIGIOUS O RGANIZATION, B UILDING and
R ELIGIOUS C EREMONY. Also note that these semantic classes still discriminate among the three
different senses of the word church. For instance, if we assign the semantic class B UILDING to
an occurrence of church in a context, we still know that it refers to its second sense. Additionally,
the semantic class B UILDING now covers more than six times more training examples than those
covered by the second sense of church.
An example of text from senseval2 automatically annotated with semantic classes can be seen
in Figure 1. It shows the automatic annotations by our classbased classifiers with different semantic classes. BLC stands for Basic Level Concepts2 (Izquierdo, Suarez, & Rigau, 2007), SS
2. We will use the following format throughout this paper to refer to a particular sense: wordnum
pos , where pos is the
part-of-speech: n for nouns, v for verbs, a for adjectives and r for adverbs, and num stands for the sense number.

84

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

for SuperSenses (Ciaramita & Johnson, 2003), WND for WordNet Domains (Magnini & Cavaglia,
2000; L. Bentivogli & Pianta, 2004) and SUMO for Suggested Upper Merged Ontology (Niles &
Pease, 2001). Incorrect assignments are marked in italics. The correct tags are included between
brackets next to the automatic ones. Obviously, these semantic resources relate senses at different
level of abstraction using diverse semantic criteria and properties that could be of interest for subsequent semantic processing. Moreover, their combination could improve the overall results since
they offer different semantic perspectives of the text.
Id
1
2
3
4
6
7
8

Word
An
ancient
stone
church
amid
the
fields

BLC

SS

WND

SUMO

artifact1n
building1n

noun.artifact
noun.artifact

building
building

Mineral
Building

geographic area1n
[physical object1n ]

noun.location
[noun.object]

factotum [geography]

LandArea

9
10
11

,
the
sound

property2n

noun.attribute

factotum [acoustics]

RadiatingSound
[SoundAttribute]

12
13

of
bells

device1n

noun.artifact

MusicalInstrument

14
15
16
17
18

cascading
from
its
tower
calling

move2v

verb.motion

factotum [acoustics]
factotum

construction3n
designate2v
[request2v ]

noun.artifact
factotum
verb.stative
factotum
[verb.communication]

Building
Communication
[Requesting]

19
20

the
faithful

[sogroup1n
cial group1n ]

noun.group

person [religion]

Group

21
22

to
evensong

time of day1n
[writing2n ]

noun.communication

religion

TimeInterval
[Text]

Motion

Table 1: Example of the automatic annotation of a text with several semantic class labels
The main goal of our research is to investigate the performance of alternative Semantic Classes
derived from WordNet on supervised WSD. First, we propose a system to automatically extract sets
of semantically coherent groupings from nominal and verbal senses from WordNet. The system
allows to generate arbitrary sets of semantic classes at distinct levels of abstraction. Second, we
also analyze its impact with respect to alternative Semantic Classes when performing classbased
WSD. Our empirical results show that our automatically generated classes performs better than
those created manually (WNDomains, SUMO, SuperSenses, etc.) while capturing more precise
information. Third, we also demonstrate that our supervised WSD system benefits from using
these new semantic classes as additional semantic features while reducing the amount of training
85

fiI ZQUIERDO , S U AREZ & R IGAU

examples. Finally, we show that our supervised class-based system can be adapted to a particular
domain. Traditional word sense based systems are also included only for comparison purposes.
Summarizing, our research empirically investigates:
 The performance of alternative semantic groupings when used in a supervised class-based
WSD system
 The impact of class-based semantic features in our supervised WSD framework
 The required amount of training examples needed by a class-based WSD in order to obtain
competitive results
 The relative performance of the class-based WSD systems with respect WSD based on word
experts
 The robustness of our class-based WSD system on specific domains
Moreover, when tested on out of domain dataset, our supervised class-based WSD system obtains slightly better results than a state-of-the-art word sense based WSD system, the ItMakesSense
system presented by Zhong and Ng (2010).
After this introduction, we present the work directly related with our research on supervised
WSD based on semantic classes. Then, Section 3 presents the sense-groupings and semantic classes
used in this study. Section 4 explains our method to automatically derive semantic classes from
WordNet at different levels of abstraction. Moreover an analysis of different semantic groupings is
included. Section 5, presents the system that we have developed to perform supervised class-based
WSD. The performance of this system is shown in Section 6, where the system is tested on several
WSD datasets provided by international evaluations. A comparison with other participants on these
competitions is introduced in sections 7 and 8. Some experiments with our system applied to a
specific domain are analyzed in Section 9. Finally, some conclusions and future work are presented
in section 10.

2. Related Work
The field of WSD is very broad. There have been a large amount of publications about WSD over the
last 50 years. This section only revises some relevant WSD approaches dealing with the appropriate
sets of meanings a word should have.
Some research has been focused on deriving different word-sense groupings to overcome the
finegrained distinctions of WordNet (Hearst & Schutze, 1993; Peters, Peters, & Vossen, 1998;
Mihalcea & Moldovan, 2001; Agirre & de Lacalle, 2003; Navigli, 2006; Snow, S., D., & A., 2007).
That is, they provide methods for grouping senses of the same word, thus producing coarser word
sense groupings. For example, for the word church having three senses in WordNet 1.6, the sense
grouping presented by Snow et al. (2007) only produces a unique grouping. That is, according to
this approach church is monosemous.
In the OntoNotes project (Hovy, Marcus, Palmer, Ramshaw, & Weischedel, 2006), the different
meanings of a word are considered as a kind of tree, ranging from coarse concepts on the root to
finegrained meanings on the leaves. The merging was increased from fine to coarse grained until
obtaining an inter annotator agreement of around 90%. This coarse-grained repository was used
86

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

in the WSD lexical sample task of SemEval-2007 (Pradhan, Dligach, & Palmer, 2007), where the
systems scored up to 88.7% Fscore. Note that this merging was created for each word following a
manual and very costly process.
Similarly to the previous approach, another task was organized within SemEval-2007 which
consisted in the traditional WSD all word task using another coarsegrained sense repository derived
from WordNet (Navigli, Litkowski, & Hargraves, 2007). In this case all the WordNet synsets were
automatically linked to the Oxford Dictionary of English (ODE) using a graph algorithm. All the
meanings of a word linked to the same ODE entry were merged into a coarse sense. The systems
achieving the top scores followed supervised approaches taking advantage of different corpora for
the training, reaching a top Fscore of 82.50%.
Both of the previous cases are aimed at solving the granularity problem of the word sense
definitions in WordNet. However, both approaches are still word experts (one classifier is trained
for each word). Obviously, decreasing the average polysemy of a word by using coarsersenses
makes easier the classification choice. As a result, the performance of these systems increase at the
cost of reducing its discriminative power.
Conversely, instead of word experts, our approach creates semantic class experts. Each of these
semantic classifiers can exploit diverse information extracted from all the meanings from different
words that belong to that class.
Wikipedia (Wikipedia, 2015) has been also recently used to overcome some problems of the supervised learning methods: excessively finegrained definition of meanings, lack of annotated data
and strong domain dependence of the existing annotated corpora. In this way, Wikipedia provides
a new source of annotated data, very large and constantly in expansion (Mihalcea, 2007; Gangemi,
Nuzzolese, Presutti, Draicchio, Musetti, & Ciancarini, 2012).
In contrast, some research has been focused on using predefined sets of sense-groupings for
learning classbased classifiers for WSD (Segond, Schiller, Greffenstette, & Chanod, 1997; Ciaramita & Johnson, 2003; Villarejo, Marquez, & Rigau, 2005; Curran, 2005; Ciaramita & Altun,
2006; Izquierdo, Suarez, & Rigau, 2009). That is, grouping senses of different words into the same
explicit and comprehensive semantic class. Also the work presented by Mihalcea, Csomai, and
Ciaramita (2007) makes use of three different sets of semantic classes (WordNet classes and two
Named Entity annotated corpora) to train sequential classifiers. The classifiers are trained using
basic features, collocations and semantic features, and they reach a performance around 60% and
the 14th position in the SemEval-2007 allwords task.
The semantic classes of WordNet (also called SuperSenses) have been widely used in different
works. For instance, Paa and Reichartz (2009a) apply Conditional Random Fields to model the
sequential context of words and their relation to SuperSenses. They also extend the model to include
the potential SuperSenses of each word into the training data. An F1 score of 82.8% is reported (both
nouns and verbs) when only potential labels are used (no training data at all) which is just 1% worse
than when using the training data with right labels. Although interesting, they only evaluate the
system applying a 5-fold cross validation on SemCor.

3. Semantic Classes and Levels of Abstraction
The meanings represented by WordNet have been only used for WSD at a very fine-grained sense
level or at a very coarse-grained semantic class level (also called SuperSenses). We suspect that an
appropriate level of abstraction could be found on between both levels. In this section we propose a
87

fiI ZQUIERDO , S U AREZ & R IGAU

simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal WordNet meanings. First, we introduce WordNet, the semantic resource
and sense repository used by most WSD systems. Also note that all semantic classes used in our
work are also linked to WordNet.
WordNet (Fellbaum, 1998) is an online lexical database of English which contains concepts
represented by synsets, which are sets of synonyms of content words (nouns, verbs, adjectives and
adverbs). One synset groups together several senses of different words which are synonyms.
In WordNet different types of lexical and semantic relations interlink different synsets, creating
in this way a very large structured lexical and semantic network. The most important relation
encoded in WordNet is the subclass relation (for nouns is called hyponymy relation and for verbs
troponymy relation). Table 2 shows some basic figures of different WordNet versions including the
total number of words, polysemous words, synsets, senses (all the possible senses for all the words)
and average polysemy.
Version
WN 1.6
WN 1.7
WN 1.7.1
WN 2.0
WN 2.1
WN 3.0

Words
121,962
144,684
146,350
152,059
155,327
155,287

Polysemous
23,255
24,735
25,944
26,275
27,006
26,896

Synsets
99,642
109,377
111,223
115,424
117,597
120,982

Senses
173,941
192,460
195,817
203,145
207,016
206,941

Avg. Polysemy
2.91
2.93
2.86
2.94
2.89
2.89

Table 2: Statistics of WordNet versions.

3.1 SuperSenses
SuperSenses is the name of the WordNet Lexicographer Files within the framework of WSD3 . More
in detail, WordNet synsets are organized into forty five SuperSenses, based on syntactic categories
(nouns, verbs, adjectives and adverbs) and logical groupings such as PERSON, PHENOMENON,
FEELING , LOCATION , etc. There are 26 basic categories for nouns, 15 for verbs, 3 for adjectives
and 1 for adverbs. In some cases, different senses of a word can be grouped at a high level under
the same SuperSense, reducing the polysemy of the word. This is often the case of very similar
senses of a word. Having just a few classes for adjectives and adverbs, SuperSense taggers have
been usually developed only for nouns and verbs. (Tsvetkov, Schneider, Hovy, Bhatia, Faruqui, &
Dyer, 2014) presents a very interesting study on tagging adjectives with SuperSenses acquired from
GermaNet (Hamp, Feldweg, et al., 1997).
3.2 WordNet Domains
WordNet Domains4 (WND) (Magnini & Cavaglia, 2000; L. Bentivogli & Pianta, 2004) is a hierarchy of 165 domains which have been used to label semi-automatically all WordNet synsets. This
set of labels is organized into a taxonomy following the Dewey Decimal Classification System5 .
3. More information of these SuperSenses can be found at http://wordnet.princeton.edu/wordnet/
man/lexnames.5WN.html.
4. http://wndomains.itc.it
5. http://www.oclc.org/dewey

88

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

For building WND, many labels were assigned to high levels of the WordNet hierarchy and were
automatically inherited across the hypernym and troponym hierarchy. Thus, the semi-automatic
method6 used to develop this resource was not free of errors and inconsistencies (Castillo, Real, &
Rigau, 2004; Gonzalez, Rigau, & Castillo, 2012).
Information brought by domain labels is complementary to what is already in WordNet. WND
present some characteristics that can be interesting for WSD. First of all, a domain label may contain
senses from different WordNet subhierarchies (derived from different SuperSenses). For instance,
the domain RELIGION contains senses such as priest, deriving from NOUN . PERSON and church,
deriving from NOUN . ARTIFACT. Second, a domain label may also include synsets of different
syntactic categories. For instance, the domain RELIGION also contains the verb pray or the adjective
holy.
Furthermore, a single WND label can subsume different senses of the same word, reducing in
this way its polysemy. For instance, the first and third senses of church in WordNet 1.6 have the
domain label RELIGION.
3.3 SUMO Concepts
SUMO7 (Niles & Pease, 2001) was created as part of the IEEE Standard Upper Ontology Working
Group. Their goal was to develop a standard upper ontology to promote data interoperability, information search and retrieval, automated inference, and natural language processing. S UMO consists
of a set of concepts, relations, and axioms that formalize an upper ontology. For these experiments,
we used the complete WordNet 1.6 mapping with 1,019 S UMO labels (Niles & Pease, 2003). In this
case, the three noun senses of church in WordNet 1.6 are classified as R ELIGIOUS O RGANIZATION,
B UILDING and R ELIGIOUS C EREMONY according to the SUMO ontology.
3.4 Example of Semantic Classes
As an example, table 3 presents the three senses and glosses of the word church in WordNet 1.6.
Sense
1

2
3

WordNet 1.6
gloss
1
Christian churchn a group of Christians; any group professing
Christian doctrine or belief: church is a biblical term for assembly
church2n church building1n
for public (especially Christian) worship: the
church was empty
church service1n church3n
a service conducted in a church: dont be late
for church
word senses
church1n
Christianity2n

Table 3: Glosses and examples for the senses of churchn
In Table 4 we show the classes assigned to each sense according to the semantic resources introduced previously. For instance, considering WordNet Domains, it can be observed that the senses
number 1 (group of Christians) and 3 (service conducted in a church) belong to the same domain
6. It was based on several cycles of manual checking over automatically labeled data.
7. http://www.ontologyportal.org

89

fiI ZQUIERDO , S U AREZ & R IGAU

RELIGION . On the contrary, SuperSenses and SUMO represent the three senses of church using
different semantic classes. Also note that the resulting assignment of semantic classes identifies
each word sense individually.

Sense
1
2
3

SuperSense
NOUN . GROUP
NOUN . ARTIFACT
NOUN . ACT

Semantic Class
WND
SUMO
R ELIGION R ELIGIOUS O RGANIZATION
B UILDINGS
B UILDING
R ELIGION
R ELIGIOUS C EREMONY

Table 4: Semantic Classes for the noun churchn
3.5 Levels of Abstraction
Basic Level Concepts (Rosch, 1977) (hereinafter BLC) are the result of a compromise between two
conflicting principles of characterization (general vs. specific):
 Represent as many concepts as possible
 Represent as many features as possible
As a result of this conflicting characterization, BLC typically should occur in the middle levels
of the semantic hierarchies.
The notion of Base Concepts (hereinafter BC) was introduced in EuroWordNet (Vossen, 1998).
BC are supposed to be the most important concepts in several language specific wordnets. This
importance can be measured in terms of two main criteria:
 A high position in the semantic hierarchy
 Having many relations to other concepts
In EuroWordNet a set of 1,024 concepts were selected and called Common Base Concepts.
Common BC are concepts that act as BC in at least two languages. Only local wordnets for English,
Dutch and Spanish were used to select this set of Common BC. In later initiatives, similar sets have
been derived to harmonize the construction of multilingual wordnets.
Considering both definitions, in the next section we present a method to automatically generate
different sets of Basic Level Concepts from WordNet at different levels of abstraction.

4. Automatic Selection of Basic Level Concepts
Several approaches have been developed trying to alleviate the fine granularity problem of WordNet
senses by obtaining word sense groupings (Hearst & Schutze, 1993; Peters et al., 1998; Mihalcea
& Moldovan, 2001; Agirre & de Lacalle, 2003; Navigli, 2006; Snow et al., 2007; Bhagwani, Satapathy, & Karnick, 2013). In most cases the approach consists on grouping different senses of
the same word, resulting in a decrease of the polysemy. Obviously, when the polysemy is reduced
the WSD task as a classification problem becomes easier, and a system using these coarse senses
obtain better results than other systems using word senses. Other works have used predefined sets
of semantic classes, mainly SuperSenses (Segond et al., 1997; Ciaramita & Johnson, 2003; Curran,
90

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

2005; Villarejo et al., 2005; Ciaramita & Altun, 2006; Picca, Gliozzo, & Ciaramita, 2008; Paa &
Reichartz, 2009b; Tsvetkov et al., 2014).
In this section, we describe a simple method to automatically create different sets of Basic Level
Concepts from WordNet. The method exploits the nominal and verbal structure of WordNet. The
basic idea is that synsets in WordNet having a high number of relations are important, and they could
be candidates to be a BLC. To capture the relevance of a synset in WordNet we have considered two
options:
1. All: the total number of relations encoded in WordNet for the synset
2. Hypo: the total number of the hyponymy relations of the synset
Our method follows a bottomup approach exploiting the hypernymy chains of WordNet. For
each synset, the process starts visiting the synsets in the hyperonymy chain and selecting (and
stopping the walk for this synset) as its BLC the ancestor having the first local maximum considering
the total number of relations (either All or Hypo)8 . For synsets having more than one hyperonym, the
method chooses the one with the higher number of relations to continue the process. This process
ends with a preliminary set of candidate synsets selected as potential BLC.
Additionally, each synset selected as a potential BLC candidate must subsume (or represent) at
least a certain number of descendant synsets. Thus, the minimum number of synsets a BLC must
subsume is another parameter of the algorithm, and it is represented by the symbol . Candidate
BLCs that do not reach this threshold are discarded, and their subsumed synsets are reassigned to
other BLC candidate appearing in higher levels of abstraction.
Algorithm 1 presents the pseudocode of the algorithm. The parameters of the algorithm are:
a WordNet resource, the type of relations considered (All or Hypo), and the minimum number
of concepts that must be subsumed by each BLC (). The algorithm has two phases. The first
one selects the candidate BLC, following the bottomup approach. The second phase discards the
candidate BLC that do not satisfy the  threshold.
Figure 1 shows a schema to illustrate the selection process. Each node represents a synset, and
the edges represent the hyperonymy relations (for instance, A is the hyperonym of D, and D is the
hyperonym of F). The number under each synset indicates its number of hyponymy relations.
The schema illustrates the selection process of BLC candidates for synset J using criterion Hypo.
The process starts checking the hyperonym of J, which is F. F has two hyperonyms, B and D. The
next synset visited in the hyperonymy chain of J is D since it has a higher number of hyponymy
relations (three). Again the algorithm compares the number of relations of the hyperonym synset (D
with three relations), with those from the previous synset (F with two). As the number is increasing
the process continues. Now, the next node to visit is A. As the number of relations of A is two and
the number for D is three, the process stops and the synset selected as BLC candidate for J is D.
Table 5 shows a real example of the selection process for the noun church in WordNet 1.6. The
hyperonym chain and the number of relations encoded in WordNet (All criterion) are shown for each
synset. The local maximum in the chain is marked in bold.
8. The algorithm starts by checking the first hyperonym of the synset, not the synset itself.

91

fiI ZQUIERDO , S U AREZ & R IGAU

Figure 1: Example of BLC selection
#rel.
18
19
37
10
12
5
#rel.
14
29
39
63
79
11
19
#rel.
20
69
5
11
7
1

synset
group 1,grouping 1
social group 1
organisation 2,organization 1
establishment 2,institution 1
faith 3,religion 2
Christianity 2,church 1,Christian church 1
synset
entity 1,something 1
object 1,physical object 1
artifact 1,artefact 1
construction 3,structure 1
building 1,edifice 1
place of worship 1, ...
church 2,church building 1
synset
act 2,human action 1,human activity 1
activity 1
ceremony 3
religious ceremony 1,religious ritual 1
service 3,religious service 1,divine service 1
church 3,church service 1

Table 5: BLC selection for the noun church in WordNet 1.6

92

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

Algorithm 1 BLC Extraction
Require: WordNet (WN) , typeOfRelation (T), threshold ()
BlcCandidates = 
for each {synset S  W N } do
cur := S
{Obtaining the hypernym chains for the current synset cur}
H := Hypernyms(W N, cur)
new := SynsetW ithM oreRelations(W N, H, T )
{Iterating while the number of relations is increased}
while N umOf Rels(W N, T, cur) < N umOf Rels(W N, T, new) do
cur := new
H := Hypernyms(W N, cur)
new := SynsetW ithM oreRelations(W N, H, T )
end while{Store cur as a candidate BLC}
BlcCandidates := BlcCandidates  {cur}
end for
{Filtering out the BLC candidates}
BlcF inal = 
for each {blc  BlcCandidates} do
if  < N umberOf Descendants(W N, blc) then
BlcF inal := BlcF inal  {blc}
end if
end for
return BlcF inal

Figure 2: Example of BLC selection for the sense 2 of church
93

fiI ZQUIERDO , S U AREZ & R IGAU

In figure 2 we can see a diagram showing a partial view of the selection process of a candidate
BLC for the sense number 2 of the noun church. The synset dotted is the synset that is being
processed (church2n ). The synsets in bold are those that are visited by the algorithm, and the one
in gray (building1n ) is the one selected as BLC for church2n . The process stops checking the synset
for structure1n as the number of relations is 63, which is lower than the number of relations of the
previous synset (79 relations for edifice1n ).
Obviously, combining different values for the  threshold (for example 0, 10, 20 or 50) and the
criterion considered by the algorithm (All or Hypo), the process ends up in different sets of BLC
extracted automatically from any WordNet version.
Furthermore, instead of the number of relations we can consider the frequency of the synsets
in a corpus as a measure of its importance. Synset frequency can be calculated as the sum of the
frequencies of the word senses contained in the synset, which can be obtained from SemCor (Miller,
Leacock, Tengi, & Bunker, 1993), or WordNet.
To sum up, the algorithm has two main parameters, the  parameter, representing the minimum number of synsets that each BLC must represent, and the criterion used for characterizing the
relevance of the synsets. The values for both parameters can be:
  parameter: any integer value greater or equal to 0
 Synset relevance parameter: the value considered to measure the importance of the synset.
Four possibilities:
 Number of relations of the synset
 All: all relations encoded for the synset
 Hypo: only hyponymy relations
 Frequency of the synset
 FreqWN: frequency obtained using WordNet
 FreqSC: frequency obtained using SemCor
An implementation of this algorithm and the different sets of BLC used in this paper for several
WordNet versions are freely available9 .
4.1 Analysis of Basic Level Concepts
We have selected WordNet 1.6 to generate several sets of BLC, combining the four types of synset
relevance criteria and values 0, 10, 20 and 50 for . These values have been selected since they
represent different levels of abstraction, ranging from  = 0 (no filtering) to  = 50 (each BLC
must subsume at least 50 synsets). Table 6 shows, for combinations of  and synset relevance
parameters, the number of concepts that each set of BLC contains, and the average depth on the
WordNet hierarchy of each group. In gray we highlight the two sets of BLC (BLC-20 and BLC-50
with all relations parameter) that we use through all the experiments described in this paper.
As expected, increasing the  threshold has a direct effect on the number of BLC and on its
average depth in the WordNet hierarchy. In particular, both values are decreased, indicating that
when the  threshold is increased, the concepts selected are more abstract and general. For instance,
9. http://adimen.si.ehu.es/web/BLC

94

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

 Threshold

Synset Relevance

0

10

20

50

All
Hypo
FreqSC
FreqWN
All
Hypo
FreqSC
FreqWN
All
Hypo
FreqSC
FreqWN
All
Hypo
FreqSC
FreqWN

# BLC
Nouns Verbs
3,094 1,256
2,490 1,041
34,865 3,070
34,183 2,615
971
719
993
718
690
731
691
738
558
673
558
672
339
659
340
667
253
633
248
633
94
630
99
631

Depth
Nouns Verbs
7.09
3.32
7.09
3.31
7.44
3.41
7.44
3.30
6.20
1.39
6.23
1.36
5.74
1.38
5.77
1.40
5.81
1.25
5.80
1.21
5.43
1.22
5.47
1.23
5.21
1.13
5.21
1.10
4.35
1.12
4.41
1.12

Table 6: Automatic Base Level Concepts for WN1.6

using (All) in the nominal part of WordNet, the number of concepts selected range from 3,094 with
no filtering ( = 0) to 253 ( = 50). However, on average, its depth reduction is not so acute since
it varies from 7.09 to 5.21. This fact shows the robustness of our method for selecting synsets from
an intermediate level of abstraction.
Also as expected, the verbal part of WordNet behave differently. In this case, since the verbal
hierarchies are less deep, the average depth of the synsets selected ranges from 3.32 to only 1.13
using All relations, and from 3.31 to 1.10 using Hypo relations.
In general, when using the frequency criteria, we can observe a similar behavior than when
using the relation criteria. However, now the effect of the threshold is more dramatic, specially for
nouns. Again, as expected, verbs behave differently than nouns. The number of BLC (for both
SemCor and WordNet frequencies) reaches a plateau of around 600. In fact, this number is very
close to the verbal top beginners of WordNet.
Summing up, we have devised a simple automatic procedure for deriving different sets of BLC
representing at a different level of abstraction the whole set of nominal and verbal synsets of WordNet. In the following section we show and explain the supervised framework developed for WSD
in order to exploit the semantic classes described in this section and the previous one.

5. Supervised Class-Based WSD
We follow a supervised machine learning approach to develop a set of semantic class based WSD
classifiers. Our systems use an implementation of a Support Vector Machine algorithm to train the
classifiers, one per semantic class, on semantic annotated corpora for acquiring both positive and
negative examples of each class. These classifiers are built on the basis of a set of features defined
for representing these examples. Being class-based, the training data must be collected and treated
in a pretty different way than in the usual word-based approach.
95

fiI ZQUIERDO , S U AREZ & R IGAU

First, word-based and class-based approaches selects the training examples very differently. In
the word-based approach, only instances of the same word can be used as training examples. Figure
3 shows the distribution of training examples used to generate a word sense classifier for the noun
house. Following the binary definition of SVM, one classifier is generated for each word sense. For
each of these classifiers, only occurrences of the word sense associated with the classifier can be
used as positive examples, while the rest of word sense occurrences are used as negative examples.
Classifier for HOUSE

Classifier
for sense#1

... house.n#1 ...

Classifier
for sense#2

... house.n#2...

... house.n#1 ...

Classifier
for sense#3

... house.n#2 ...

... house.n#3 ...

Figure 3: Distribution of examples using a word-based approach
In a class-based approach, we can use all the examples from all the words that belong to a
particular semantic class. Figure 4 shows the distribution of examples in the class-based approach.
In this case, one classifier is created for each semantic class. All occurrences of words belonging
to the semantic class associated with the classifier can be used as positive examples, while the rest
of occurrences of word senses associated with a different semantic class are selected as negative
examples.
Obviously, in the class-based approach the number of examples for training is increased. Table
7 shows an example for sense church2n . Following a word-based approach only 58 examples can
be found in Semcor for church2n . Conversely, 371 positive training examples can be used when
building a classifier for the semantic class building, edifice.
We think that this approach has several advantages. First, semantic classes reduce the average
polysemy degree of words (some word senses might be grouped together within the same semantic
class). Moreover, the acquisition bottleneck problem in supervised machine learning algorithms is
attenuated because of the increase in the number of training examples. However, we are mixing
in one classifier examples from very different words. For instance, for the building class we are
grouping together examples from hotel, hospital or church, which could introduce noise in the
learning process when grouping unrelated word senses.
5.1 The Learning Algorithm: SVM
Support Vector Machines (SVM) have been proven to be robust and very competitive in many NLP
tasks, and in WSD in particular (Marquez et al., 2006). In our experiments, we used SVM-Light
96

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

Classifier for ANIMAL

Classifier for BUILDING

...hospital..
(BUILDING)

...house..
(BUILDING)

...dog...
(ANIMAL)

...cat...
(ANIMAL)

...star..
(PERSON)

Figure 4: Distribution of examples using a class-based approach

church2n

Classifier
(word-based approach)

building, edifice
(class approach)

Examples
church2n
church2n
building1n
hotel1n
hospital1n
barn1n
.......

# of positive examples
58
58
48
39
20
17
......
371 examples

Table 7: Number of examples in Semcor: word vs. class-based approaches

97

fiI ZQUIERDO , S U AREZ & R IGAU

implementation (Joachims, 1998). SVM are used to induce a hyperplane that separates the positive
from the negative examples with a maximum margin. It means that the hyperplane is located in an
intermediate position between positive and negative examples, trying to keep the maximum distance
to the closest positive example, and to the closest negative example. In some cases, it is not possible
to get a hyperplane that divides the space linearly, or it is better to allow some errors to obtain a more
efficient hyperplane. This is known as soft-margin SVM, and requires the estimation of a parameter
(C), that represents the trade-off allowed between training errors and the margin. We have set this
value to 0.01, which has been demonstrated as a good value for SVM in WSD tasks.
When classifying an example, we obtain the value of the output function for each SVM classifier
corresponding to each semantic class for the word example, and our system simply selects the class
having the greatest value.
5.2 Corpora
Three semantic annotated corpora have been used for training and testing. Semcor for training, and
SensEval-2 and SensEval-3 English all-words tasks, for testing.
SemCor (Miller et al., 1993) is a subset of the Brown Corpus plus the novel The Red Badge
of Courage, and it has been developed by the same group that created WordNet. It contains 253
texts and around 700,000 running words, and more than 200,000 are also lemmatized and sensetagged according to Princeton WordNet 1.6. The sense annotations from SemCor have been also
automatically ported to other WordNet versions10 .
SensEval-211 English all-words corpus (hereinafter SE2) (Palmer, Fellbaum, Cotton, Delfs, &
Dang, 2001) consists of 5,000 words of text from three Wall Street Journal (WSJ) articles representing different domains from the Penn TreeBank II. The sense inventory used for tagging was
WordNet 1.7.
SensEval-312 English all-words corpus (hereinafter SE3) (Snyder & Palmer, 2004), is made up
of 5,000 words, extracted from two WSJ articles and one excerpt from the Brown Corpus. Sense
repository of WordNet 1.7.1 was used to tag 2,041 words with their proper senses.
We also considered alternative evaluation datasets. For instance, the SemEval-2007 coarse
grained task corpus13 . However, this dataset has been discarded because this corpus is annotated
with a particular set of word sense clusters. Additionally, it does not provide a clear and simple way
to compare orthogonal sets of clusterings. Although there have been more recent SensEval/SemEval
tasks about WSD, we think that for the purpose of this evaluation (different level of abstraction in
WSD), SensEval-2 and SensEval-3 are still the datasets that best fit to our purposes. More recent
SemEval competitions have been designed to address specific topics, such as multilinguality or joint
WSD and Named Entity Recognition. However, we have also make some additional experiments
on domain adaptation with the dataset provided by SemEval-10 task 17 All-words Word Sense
Disambiguation on a Specific Domain (WSD-domain)14 (Agirre, Lopez de Lacalle, Fellbaum,
Hsieh, Tesconi, Monachini, Vossen, & Segers, 2010).
10.
11.
12.
13.
14.

http://web.eecs.umich.edu/mihalcea/downloads.html#semcor
http://www.sle.sharp.co.uk/senseval2
http://www.senseval.org/senseval3
Indeed we participated in this task with a preliminary version of our system
http://semeval2.fbk.eu/semeval2.php?location=tasks#T25

98

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

5.3 Feature Types
Following previous contributions in supervised WSD, we have selected a set of basic features to
represent the training and testing examples. We also include additional features based on semantic
classes.
 Basic features
 Word-forms and lemmas in a window of 10 words around the target word.
 PoS, the concatenation of the preceding/following three and five PoS tags.
 Bigrams and trigrams formed by lemmas and word-forms in a window of 5 words
around the target word; we use all tokens regardless their PoS to build bi/trigrams. We
replace the target word by a character X in these features to increase its generalization.
 Semantic features
 Most frequent semantic class for the target word, calculated over SemCor.
 Monosemous semantic class of monosemous words in a window of size five words
around the target word.
Basic features are those widely used in the literature, as the work presented by Yarowsky (1994).
These features are pieces of information that occur in the context of the target word: local features
including bigrams and trigrams (including the target word) of lemmas, word-forms or partof
speech labels (PoS). In addition, wordforms or lemmas in some larger window around the target
word are considered as features representing the topic of the discourse.
The set of features is extended with semantic information. Several types of semantic classes
have been considered to create these features. In particular, two different sets of BLC (BLC20 and
BLC5015 ), SuperSenses, WordNet Domains (WND) and SUMO.
In order to increase the generalization capabilities of the class-based classifiers we filter out
irrelevant features. We measure the relevance of a feature16 f for a class c in terms of the frequency
of f. For each class c, and for each feature f of that class, we calculate the frequency of the feature
within the class (the number of times that it occurs in examples of the class), and we also obtain
the total frequency of the feature for all the classes. We get the relative frequency by dividing
both values (classFreq / totalFreq) and if the result is lower than a certain threshold t, the feature is
removed from the feature list of the class c17 . In this way, we make sure that the features selected
for a class are more frequently related with that class than with others. We set this threshold t to
0.25, obtained empirically with very preliminary versions of the classifiers when applying a crossvalidation setting on SemCor.
15. We have selected these set since they represent different levels of abstraction. As said in section 4, 20 and 50 refer to
the threshold of minimum number of synsets that a possible BLC must subsume to be considered as a proper BLC.
These sets of BLC were built using all criterion.
16. That is, the value of the feature, for example a feature type can be word-form, and a feature of that type can be
houses.
17. Depending on the experiment, around 30% of the original features are removed by this filter.

99

fiI ZQUIERDO , S U AREZ & R IGAU

6. Semantic ClassBased WSD Experiments
In this section we present the performance of our semantic class-based WSD system in the all
words WSD SensEval-2 (SE2) and SensEval3 (SE3) datasets. We want to analyze the behavior
of our class-based WSD system when working at different levels of abstraction. As we have said
before, the level of abstraction is defined by the semantic class used to build the classifiers.
An experiment is defined by two different parameters each one involving a particular set of
semantic classes.
1. Target class: The semantic classes used to train the classifiers (determining the abstraction
level of the system). In this case, we tested: word-sense18 , BLC20, BLC50, WordNet Domains (WND), SUMO and SuperSenses (SS).
2. Semantic features class: The semantic classes used for building the semantic features. In
this case, we tested: BLC20, BLC50, WND, SUMO and SuperSenses (SS).
The target class is the type of classes that the classifier assigns to a given ambiguous word. For
instance, the target class for the traditional word expert classifiers are word senses. The Semantic
feature class is the one used for building the semantic features, which is independent of the target
class. For instance, we can use WordNet Domains to extract monosemous words from the context
of the target word and use the WND labels of these words as semantic features for building the
classifier.
Combining different semantic classes for target and features, we generated the set of experiments described in the next sections. In that way, we can evaluate independently the impact of
selecting one semantic class or another as target class or as semantic feature class.
Test
SE2
SE3

PoS
N
V
N
V

Sense
4.02
9.82
4.93
10.95

BLC20
3.45
7.11
4.08
8.64

BLC50
3.34
6.94
3.92
8.46

SUMO
3.33
5.94
3.94
7.60

SS
2.73
4.06
3.06
4.08

WND
2.66
2.69
3.05
2.49

Table 8: Average polysemy on SE2 and SE3
Table 8 shows the average polysemy (AP) measured on SE2 and SE3 with respect to the different semantic classes used in our evaluation as target classes. As expected, every corpus behaves
differently and the average polysemy for verbs is higher than for nouns. Also as we could assume
in advance, relevant reductions on the polysemy degree are obtained when increasing the level of
abstraction. This fact is more acute also for verbs. Note the large reduction of polysemy for verbs
when using SuperSenses and also WND. Also note that a priori SE3 seems to be more difficult to
disambiguate than SE2, independently of its abstraction level.
6.1 Baselines
As baselines of these evaluations we define the most frequent classes (MFC) of each word calculated
over SemCor. Ties between classes on a specific word are solved obtaining the global frequency in
18. We included a word-based evaluation for comparison purposes only since the current system have been designed for
class-based evaluation.

100

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

SemCor of each of these tied classes, and selecting the most frequent class over the whole training
corpus. When Semcor has no occurrences of a particular word (that is, we are not able to calculate
the most frequent class of the word), we compute the global frequency for each of its possible
semantic classes (obtained from WordNet) over SemCor, and we select the most frequent one. Table
9 shows the baseline for each semantic class over both testing corpora.
Class
Sense
BLC20
BLC50
SUMO
SuperSense
WND

Pos
N
V
N
V
N
V
N
V
N
V
N
V

SE2
MFC AP
70.02 4.02
44.75 9.82
75.71 3.45
55.13 7.11
76.65 3.34
54.93 6.94
76.09 3.33
60.35 5.94
80.41 2.73
68.47 4.06
86.11 2.66
90.33 2.69

SE3
MFC
AP
72.30
4.93
52.88 10.95
76.29
4.08
58.82
8.64
76.64
3.92
60.05
8.46
79.55
3.94
64.71
7.60
81.50
3.06
79.07
4.08
83.82
3.05
92.20
2.49

Table 9: Most Frequent Class baselines and average polysemy (AP) on SE2 and SE3
As expected, the performances of the MFC baselines are very high. In particular, those corresponding to nouns (ranging from 70% to 80%). While nominal baselines seem to perform similarly
in both SE2 and SE3, verbal baselines appear to be consistently much lower for SE2 than for SE3.
In SE2, verbal baselines range from 44% to 68% while in SE3 verbal baselines range from 52% to
79%. The results of WND are very high due to its low polysemy degree for both nouns and verbs.
Obviously, when increasing the level of abstraction (from senses to WND) the results also increase.
6.2 Results of Our Basic System
In this section we present the performance of our supervised semantic classbased WSD system.
Table 10 shows the results of the system when trained varying the target classes and using only
the basic feature set. Their values correspond to the F1 measures (harmonic mean of recall and
precision) when training our systems on SemCor and testing on SE2 and SE3 test sets. The results
that improve the baselines are shown in italics. Additionally, those results showing a statistically
significant positive difference when compared with its corresponding baseline using McNemars
test are marked in bold.
Interestingly, only the basic system at a word-sense level outperforms the baselines in SE2 and
SE3 for both nouns and verbs. In addition, our systems obtain in some cases significantly better
results for verbs. Also interesting is that on verbs at a word-sense level the baselines and results are
very different, while at a class-level the differences on both datasets are much smaller.
As expected, the results of the systems increase when augmenting the level of abstraction (from
senses to WND), and in most cases, the baseline results are reached or outperformed. This is even
more relevant if we consider that the baseline results are already quite high. However, at a very high
level of abstraction (SuperSenses or WND) our basic systems seem to be unable to outperform the
baselines.
101

fiI ZQUIERDO , S U AREZ & R IGAU

Class
Sense
BLC20
BLC50
SUMO
SuperSense
WND

Pos
N
V
N
V
N
V
N
V
N
V
N
V

SE2
71.20
45.53
75.52
57.06
74.57
58.03
77.60
62.09
79.94
71.95
80.81
90.14

SE3
73.15
57.02
73.82
61.10
75.84
61.97
76.74
66.21
79.48
78.39
77.64
88.92

Table 10: Results of the basic system trained on SemCor with a basic set of features and evaluated
against SE2 and SE3

In general, the results obtained by BLC20 are not so different to those of BLC50. For instance,
if we consider the number of classes within BLC20 (558 classes), BLC50 (253 classes) and SuperSense (24 classes), BLC classifiers obtain high performance rates while maintaining much higher
expressive power than SuperSenses (they are able to classify among much larger number of classes).
In fact, using SuperSenses (40 classes for nouns and verbs) we obtain a very accurate semantic tagger with a performance close to 80%. Even more interesting, we could use BLC20 for tagging
nouns (558 semantic classes and F1 around 75%) and SuperSenses for verbs (14 semantic classes
and F1 around 75%).
6.3 Results Exploiting the Semantic Features
One of our main goals is to prove that simple semantic features added to the training process are
capable of producing significant improvements against the basic systems. The results of the experiments considering also the different types of semantic features are presented in Tables 11 and 12,
respectively for nouns and verbs.
In both tables, the column labeled as Class refers to what we have called the target class, and
the column labeled as SF indicates the type of semantic features included to represent the examples
within our machine learning approach.
Again, the values in the tables correspond to the F1 measures (harmonic mean of recall and
precision) when training our systems on SemCor and testing on SE2 and SE3 test sets. The results
improving the baselines appear in italics. Additionally, those results showing a statistically significant positive difference when compared with its corresponding baseline using the McNemars test
are marked in bold.
Regarding nouns (see Table 11), a very different behavior is observed for SE2 and SE3. Adding
semantic features mainly improves the results on SE2. While for SE3 none of the systems present
a significant improvement over the baselines, for SE2 such improvement is obtained when using
several types of semantic features (in particular, when using WND features on SE2). The use of
semantic class-based features seems to improve the systems using as target classes intermediate
levels of abstraction (specially BLC20 and BLC50). Interestingly, in SE3 only BLC20 and BLC50
102

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

Class

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

Sense

BLC20

BLC50

SE2
70.02
71.20
71.79
71.69
71.59
71.10
71.20
75.75
75.52
77.69
77.79
77.60
75.14
77.88
76.65
74.57
78.45
76.65
79.58
75.52
78.92

SE3
72.30
73.15
73.15
73.04
73.15
72.70
73.15
76.29
73.82
76.52
75.73
73.71
73.82
74.24
76.74
75.84
76.85
76.74
75.51
74.61
74.83

Class

SUMO

SS

WND

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
76.09
77.60
75.52
75.52
77.88
77.50
77.88
80.41
79.94
81.07
80.22
80.51
80.32
82.47
86.11
80.81
81.85
82.33
83.55
83.08
86.01

SE3
79.55
76.74
76.74
77.19
78.76
76.97
77.42
81.50
79.48
81.39
81.73
81.05
76.46
79.82
83.82
77.64
80.79
80.11
81.24
78.31
83.71

Table 11: Results for nouns using the extended system
seem to provide some improvements over the baselines in some of the target classes (for instance,
BLC20, BLC50 and SS), although not significant.
Regarding verbs (see Table 12), also a very different behavior is observed for SE2 and SE3. In
this case, we can observe almost the opposite effect than for nouns. On SE3 most of the semantic
class features improve the results obtained by the baselines. While for SE2 only some of the systems
present a significant improvement over the baselines, for SE3 such improvement is obtained when
using several types of semantic features. However, in this case we also obtain significantly better
results for several semantic features on SE2. The use of semantic class-based features seems to
benefit lower levels of abstraction (specially word-sense, BLC20, BLC50 and also SUMO).
In general, the results show that using semantic features in addition to the basic features helps to
reach a better performance for the class-based WSD systems. Additionally, it also seems that using
these semantic features we are able to obtain very competitive classifiers at a sense level.
6.4 Learning Curves
We also investigate the behavior of the class-based WSD system with respect the number of training
examples. Although the same experiments have been carried out for nouns and verbs, we only
include the results for nouns since in both cases, the trend is very similar.
In these experiment, the Semcor files have been randomly selected and added to the training
corpus in order to generate subsets of 5%, 10%, 15%, etc. of the training corpus19 . Then, we train
19. Each portion contains also the same files than the previous portion. For example, all files in the 25% portion are also
contained in the 30% portion.

103

fiI ZQUIERDO , S U AREZ & R IGAU

Class

Sense

BLC20

BLC50

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
44.75
45.53
45.14
45.53
45.73
45.34
45.53
55.13
57.06
56.87
55.90
57.06
56.29
58.61
54.93
58.03
57.45
56.67
57.06
57.45
59.77

SE3
52.88
57.02
56.61
56.47
57.02
56.75
56.75
58.82
61.10
59.92
60.60
61.15
61.29
60.88
60.05
61.97
61.29
61.01
61.83
61.83
62.38

Class

SUMO

SS

WND

SF
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND
baseline
basicFeat
BLC20
BLC50
SUMO
SS
WND

SE2
60.35
62.09
61.12
62.09
60.74
59.96
61.51
68.47
71.95
69.25
69.25
70.21
69.25
71.76
90.33
90.14
90.14
90.14
90.52
89.75
90.52

SE3
64.71
66.21
66.07
66.48
64.98
64.71
66.35
79.07
78.39
77.70
77.70
77.70
77.84
79.75
92.20
88.92
90.42
90.15
89.88
88.78
92.20

Table 12: Results for verbs using the extended system
the system on each of the training portions and we test the system on SE2 and SE3. Finally, we also
compare the resulting system with the baseline computed over the same training portion.
Figures 5 and 6 present the learning curves over SE2 and SE3, respectively. In this case, we
selected a BLC20 class-based WSD system using WordNet Domains as semantic features20 .
Surprisingly, in SE2 the system only improves the F1 measure around 2% while increasing the
training corpus from 25% to 100% of SemCor. In SE3, the system again only improves the F1
measure around 3% while increasing the training corpus from 30% to 100% of SemCor. That is,
most of the knowledge required for the class-based WSD system seems to be already present on a
small part of SemCor.
Figures 7 and 8 present the learning curves over SE2 and SE3, respectively, of a class-based
WSD system based on SuperSenses using as semantic features those built with WordNet Domains.
In SE2 the system just improves the F1 measure around 2% while increasing the training corpus
from 25% to 100% of SemCor. In SE3, the system again only improves the F1 measure around 2%
while increasing the training corpus from 30% to 100% of SemCor. That is, with only 25% of the
whole corpus, the class-based WSD system reaches a F1 close to the performance using all corpus.
In SE2 ans SE3, when using BLC20 (Figures 5 and 6) or SuperSenses (Figures 7 and 8) as
semantic classes for WSD, the behavior of the system is similar to the MFC baseline. This is very
interesting since the MFC obtains very high results due to the way it is defined: the MFC over the
total corpus is assigned if there are no occurrences of the word in the training corpus. Without this
definition, there would be a large number of words in the test set with no occurrences when using
20. As shown in previous experiments, this combination obtains a very good performance.

104

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

80

System SV2
MFC SV2

78

76

74

72
F1
70

68

66

64

62
5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95 100

% corpus

Figure 5: Learning curve of BLC20 classifier on SE2

78

System SV3
MFC SV3

76

74

72

F1

70

68

66

64

62
5

10

15

20

25

30

35

40

45

50 55
% corpus

60

65

70

75

80

85

90

Figure 6: Learning curve of BLC20 classifier on SE3

105

95 100

fiI ZQUIERDO , S U AREZ & R IGAU

84

System SV2
MFC SV2

82

80

78

F1

76

74

72

70

68
5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95 100

% corpus

Figure 7: Learning curve of SuperSense classifier on SE2

82

System SV3
MFC SV3

80

78

F1

76

74

72

70
5

10

15

20

25

30

35

40

45

50 55
% corpus

60

65

70

75

80

85

90

95 100

Figure 8: Learning curve of SuperSense classifier on SE3

106

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

small training portions. In these cases, the recall of the baselines (and in turn F1) would be much
lower.
This evaluation seems to indicate that the class-based approach to WSD reduces considerably
the required amount of training examples.

7. Comparison with SensEval Systems: Sense Level
The main goal of the experiments included in this section is to verify whether the abstraction level
of our class-based systems maintains its discriminative power when evaluated at a sense level. Additionally, we compare our results against the results of the top participant systems in SE2 and SE3
which provided the best senselevel outputs. Thus, our class-based systems have been adapted following a simple protocol. The output based on semantic classes is converted to sense identifiers:
instead of the semantic class produced by our systems for a particular instance, we select the first
sense of the word according to the WordNet sense ranking belonging to the predicted semantic class.
So, first we obtain the semantic class by means of our classifiers, then we obtain the restricted set of
senses for the word that match the semantic class obtained, and then we choose the most frequent
sense from that restricted subset.
The results of the first experiment on SE2 data are shown in Table 13. All our systems have the
prefix SVM- while the suffix denotes the type of semantic class used to generate the classifier21 .
In all cases in these experiments, WND has been selected as target semantic class to generate the
semantic features. Two baselines marked in Italics have been also included. The first sense in WordNet (base-WordNet) and the most frequent sense in SemCor (base-SemCor). In fact, the developers
of WordNet ranked their word senses using SemCor and other sense-annotated corpora. Thus, the
frequencies and ranks appearing in SemCor and in WordNet are similar, but not equal. We also
include the results of our system when working at a word level (SVM-sense).
In both cases, for nouns and verbs, our systems outperform the most frequent baselines. The
most frequent sense for a word, according to the WordNet sense ranking is very competitive in
WSD tasks, and it is extremely hard to improve upon even slightly (McCarthy, Koeling, Weeds,
& Carroll, 2004). As expected, the behavior of the different semantic features produces slightly
different results. However, independently of the semantic features used, in SE2 at a sense level, the
class-based systems rank at the third position.
Table 14 shows the same experiment but using SE3 dataset. In this case, our class-based systems
clearly outperform the baselines, achieving the best results for nouns and the second place for verbs.
Interestingly, for nouns, the best system at the SE3 did not achieve the SemCor baseline. Also recall
that SE3 seems to be more difficult than SE2.
It is worth to mention that our class-based systems use the same features for both nouns and
verbs. For instance, we do not take profit of complex feature sets encoding syntactic information
that seems to be important for verbs.
These experiments show that class-based classifiers seem to be quite competitive when evaluated at a word sense level. They perform over the most frequent sense according to WordNet and
SemCor, and achieve the higher position for nouns and the second for verbs in SE3, and the third
position for nouns and verbs in SE2. Obviously, this indicates that class-based WSD maintains a
very high discriminative power at a word sense level.
21. For instance, SVM-BLC20 stands for the experiment that creates classifier considering BLC20 semantic classes.

107

fiI ZQUIERDO , S U AREZ & R IGAU

Class  Sense on SE2
Nouns
Verbs
System
F1
System
SMUam
73.80 SMUaw
AVe-Antwerp
74.40 AVe-antwerp
SVM-semBLC20 71.80 SVM-semSUMO
SVM-semBLC50 71.70 SVM-sense
SVM-semSUMO 71.60 SVM-semWND
SVM-semWND
71.20 SVM-semBLC50
SVM-sense
71.20 SVM-semSS
SVM-semSS
71.10 SVM-semBLC20
base-WordNet
70.10 LIA-Sinequa
base-SemCor
70.00 base-SemCor
LIA-Sinequa
70.00 base-WordNet

F1
52.70
47.90
45.70
45.53
45.50
45.50
45.30
45.10
44.80
44.80
43.80

Table 13: Class to Sense results on SE2. Class to word sense transformation.

Class  Sense on SE3
Nouns
System
SVM-semWND
SVM-semBLC20
SVM-semSUMO
SVM.sense
SVM-semBLC50
SVM-semSS
base-SemCor
GAMBL-AW
base-WordNet
kuaw
UNTaw
Meaning-allwords
LCCaw

F1
73.20
73.20
73.20
73.15
73.00
72.70
72.30
70.80
70.70
70.60
69.60
69.40
69.30

Verbs
System
GAMBL-AW
SVM-semSUMO
SVM-semWND
SVM-semSS
SVM-sense
SVM-semBLC20
SVM-semBLC50
UNTaw
Meaning-allwords
kuaw
R2D2
base-SemCor
base-WordNet

F1
59.30
57.00
56.80
56.80
56.75
56.60
56.50
56.40
55.20
54.50
54.40
52.90
52.80

Table 14: Class to Sense results on SE3. Class to word sense transformation.

108

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

8. Comparison with SensEval Systems: Class Level
The experiments presented in this section explore the performance of the word-based classifiers
participating at SE2 and SE3 when are evaluated at a class level. To perform this kind of evaluation,
the word sense output of the participant systems have been mapped to its corresponding semantic
classes. Our class-based systems are not modified. Obviously, we expect different performances
of the systems depending on the semantic class level. Considering the results presented in tables
11 and 12, in order to perform the comparison, we have selected the experiments that use WND to
build the semantic features22 . Thus, our system results using the different target semantic classes
are all represented by SVM-semWND.
Table 15 presents ordered by F1-measure the results of the best performing systems on SE2 data
when evaluated at a different levels of abstraction. As previously, in italics we include the most
frequent senses according to WordNet base-WordNet and SemCor base-SemCor.
On SE2, independently of the abstraction level and PoS, our system (SVM-semWND) scores
at the first positions of the ranking. In one case our system reaches the best position, and twice
the second one. The baselines are outperformed in all experiments, except for nouns using WND,
where baseSemCor is very high.
Table 16 presents ordered by F1-measure the results of the best performing systems on SE3 data
when evaluated at a different levels of abstraction. In italics we include the most frequent senses
according to WordNet base-WordNet and SemCor base-SemCor. Our systems are again represented
by SVM-semWND.
On SE3, we can see that our system performs better than the baselines in most cases, except for
the SemCorbased baseline on nouns, which obtains a very high result. In particular, our system
obtains very good results on verbs, reaching the first or second best positions in all cases, and
outperforming both baselines in all cases.
To sum up, our classbased approach outperforms most SensEval participants (both SE2 and
SE3), at sense level and at semantic class level. This suggests that the good performance of the
semantic classifiers are not only due to the polysemy reduction. Actually, it confirms that our
classbased semantic classifiers are learning from the semantic class training examples at different
abstraction levels.

9. Out of Domain Evaluation
In this section we describe our system at the SemEval-2 Allwords Word Sense Disambiguation
on a Specific Domain task (Izquierdo, Suarez, & Rigau, 2010). The aim of this evaluation is to
show how robust our semantic class approach is when tested on a specific domain, different to the
domain of the training material.
Traditionally, SensEval competitions have been focused on general domain texts. Thus, domain
specific texts present fresh challenges for WSD. For example, specific domains reduce the possible meaning of a word in a given context. Moreover, the distribution of word senses on the data
examples changes when compared to general domains. These problems affect both supervised and
knowledgebased systems. In fact, supervised word-based WSD systems are very sensitive to the
corpora used for training and testing the system (Escudero et al., 2000).
22. Remind that the semantic features are the most frequent class of the target word, and the semantic class of monosemous words in the context around target word.

109

fiI ZQUIERDO , S U AREZ & R IGAU

Nouns

Verbs
F1
System
Sense  BLC20
SMUaw
78.72 SMUaw
SVM-semWND 77.88 SVM-semWND
AVe-antwerp
76.71 LIA-Sinequa
base-SemCor
75.71 AVe-antwerp
base-WordNet
74.29 base-SemCor
LIA-Sinequa
73.39 base-WordNet
Sense  BLC50
SMUaw
79.01 SMUaw
SVM-semWND 78.92 SVM-semWND
AVe-antwerp
77.57 LIA-Sinequa
base-SemCor
76.65 AVe-Antwerp
base-WordNet
75.24 base-SemCor
LIA-Sinequa
74.53 base-WordNet
Sense  SUMO
SMUaw
79.30 SMUaw
SVM-semWND 77.88 LIA-Sinequa
base-SemCor
76.09 AVe-Antwerp
AVe-Antwerp
75.94 SVM-semWND
LIA-Sinequa
74.92 base-SemCor
base-WordNet
71.74 base-WordNet
Sense  SuperSense
SVM-semWND 82.47 SMUaw
SMUaw
81.21 LIA-Sinequa
AVe-Antwerp
80.75 SVM-semWND
base-SemCor
80.41 AVe-Antwerp
LIA-Sinequa
79.58 base-WordNet
base-WordNet
78.16
base-SemCor
Sense  WND
SMUaw
88.80 SMUaw
base-SemCor
86.11 SVM-semWND
SVM-semWND 86.01 base-SemCor
AVe-Antwerp
87.30 LIA-Sinequa
base-WordNet
85.82 base-WordNet
LIA-Sinequa
84.85 AVe-Antwerp
System

F1
61.22
58.61
57.42
57.28
55.13
54.16
61.61
59.77
57.81
57.67
54.93
54.55
68.22
64.79
62.56
61.51
61.33
60.35
73.47
72.74
71.76
69.31
69.05
68.47
91.16
90.52
90.33
89.82
89.75
89.74

Table 15: Results for sense to BLC20, BLC50, SUMO, SuperSense and WND semantic classes on
SE2

110

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

Nouns

Verbs
F1
System
Sense  BLC20
base-SemCor
76.29 GAMBL-AW
GAMBL-AW
74.77 SVM-semWND
kuaw
74.69 kuaw
LCCaw
74.44 R2D2
UNTaw
74.40 UNTaw
SVM-semWND
74.24 Meaning-allwords
base-WordNet
74.16
base-SemCor
Meaning-allwords 73.11 base-WordNet
Sense  BLC50
base-SemCor
76.74 GAMBL-AW
GAMBL-AW
75.56 SVM-semWND
kuaw
75.25 kuaw
SVM-semWND
74.83 R2D2
LCCaw
74.78 UNTaw
UNTaw
74.73 Meaning-allwords
base-WordNet
74.49 base-SemCor
R2D2
73.93 base-WordNet
Sense  SUMO
base-SemCor
79.55 GAMBL-AW
kuaw
78.18 SVM-semWND
LCCaw
77.54 UNTaw
SVM-semWND
77.42 kuaw
UNTaw
77.32 Meaning-allwords
GAMBL-AW
77.14 upv-eaw2
base-WordNet
76.97
base-SemCor
Meaning-allwords 76.75 base-WordNet
Sense  SuperSense
base-SemCor
81.50 SVM-semWND
kuaw
79.89 GAMBL-AW
SVM-semWND
79.82 base-SemCor
UNTaw
79.71 base-WordNet
GAMBL-AW
79.62 Meaning-allwords
upv-eaw2
79.27 Meaning-simple
upv-eaw
78.42 kuaw
base-WordNet
78.25 upv-eaw2
Sense  WND
base-SemCor
83.80 SVM-semWND
SVM-semWND
83.71 base-SemCor
UNTaw
83.62 UNTaw
kuaw
81.78 GAMBL-AW
GAMBL-AW
81.53 base-WordNet
base-WordNet
81.46
R2D2
LCCaw
80.64 Meaning-simple
Meaning-allwords 80.50 kuaw
System

F1
63.56
60.88
60.66
59.79
59.73
59.37
58.82
58.28
64.38
62.38
61.22
60.35
60.27
60.19
60.06
58.82
68.77
66.35
66.03
65.93
65.43
64.92
64.71
64.02
79.75
79.40
79.07
78.25
78.14
77.72
77.53
77.21
92.20
92.20
91.37
91.01
90.83
90.52
90.50
90.44

Table 16: Results for sense to BLC20, BLC50, SUMO, SuperSense and WND semantic classes on
SE3

111

fiI ZQUIERDO , S U AREZ & R IGAU

Therefore, the main challenge is how to develop specific domain WSD systems or how to adapt
a general system to a particular domain. Following this research line, a task was proposed within the
SemEval2 competition: Allwords Word Sense Disambiguation on a Specific Domain (Agirre
et al., 2010). The restricted domain selected for this task was the environmental domain. The test
corpora consist of three texts compiled by the European Center for Nature Conservation23 (ECNC)
and World Wildlife Forum24 (WWF). The task was proposed in several languages: Chinese, Dutch,
English an Italian, although our participation was limited to English. More in detail, there were
a total of 1,032 noun tokens and 366 verb tokens to be tagged. Moreover, a set of background
documents related with the environmental domain were provided. These texts were no sense tagged,
they were just plain text, and they were also provided by ECNC and WWF. They could be used by
the systems to help to the adaptation to the specific domain. For English, there were a total of 113
background documents, containing 2,737,202 words.
We do not apply any kind of specific domain adaptation technique to our supervised classbased
system. In order to adapt our supervised system to the environmental domain we just increase automatically the training data with new training examples from the domain. To acquire these examples,
we use the 113 background documents of the environmental domain provided by the organizers. We
use TreeTagger (Schmid, 1994) to preprocess the documents, performing PoStagging and lemmatization. Since the background documents are not semantically annotated, and our supervised system
needs labeled data, we have selected only the monosemous instances occurring in the documents
according to our BLC20 semantic classes25 . Note that this approach can only be exploited by classbased WSD systems. In this way, we have obtained automatically a large set of examples annotated
with BLC20. This semantic class was selected because it provided very good results in previous
experiments. In order to analyze how the same approach and system would work with other level of
abstraction, we performed the same evaluation a posteriori using BLC50, WordNet Domains and
SuperSenses besides to BLC20, which was the official participation in SemEval-2. Nevertheless,
this section will be focused on BLC20.
Regarding BLC20, Table 17 presents the total number of training examples extracted from SemCor (SC) and from the background documents (BG). As expected, by this method a large number
of monosemous examples can be obtained for nouns and verbs, although, verbs are much less productive than nouns. However, all these background examples correspond to a reduced set of 7,646
monosemous words.
SC
BG
Total

Nouns
87,978
193,536
281,514

Verbs
48,267
10,821
59,088

N+V
136,245
204,357
340,602

Table 17: Number of training examples for BLC20
Table 18 lists the ten most frequent monosemous nouns and verbs occurring in the background
documents. Remember that all these examples are monosemous according to BLC20 semantic
classes.
23. http://www.ecnc.org
24. http://wwf.org
25. BLC20 (see section 4) stands for Basic Level Concepts obtained with all relations criterion and a minimum threshold
of subconcepts subsumed equal to 20.

112

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

1
2
3
4
5
6
7
8
9
10

Nouns
Lemma
biodiversity
habitat
specie
climate
european
ecosystem
river
grassland
datum
directive

# ex.
7,476
7,206
7,067
3,539
2,818
2,669
2,420
2,303
2,276
2,197

Verbs
Lemma # ex.
monitor
788
achieve
784
target
484
select
345
enable
334
seem
287
pine
281
evaluate 246
explore
200
believe
172

Table 18: Most frequent monosemous words in the background documents
SC
BG
Total

Nouns
87,978
116,912
204,890

Verbs
48,267
7,019
55,286

N+V
136,245
123,931
260,176

Table 19: Number of training examples for word senses
Our approach applies the same semantic class architecture shown in the previous sections, but
using examples extracted from the background documents. In this case, the semantic class used to
extract the examples and generate the classifiers is BLC2026 . We select a simple feature set widely
used in many WSD systems. In particular, we use a window of five tokens around the target word
to extract word forms, lemmas; bigrams and trigrams of word forms and lemmas; trigrams of PoS
tags, and also the most frequent BLC20 semantic class of the target word in the training corpus.
To analyze the contribution of the monosemous examples in the performance of the system three
experiments we have defined:
 BLC20SC: only training examples extracted from SemCor
 BLC20BG: only monosemous examples extracted from the background data
 BLC20SCBG: training examples extracted from SemCor and monosemous background data
The first run (BLC20SC) aims to show the behavior of a supervised system trained on a general
corpus, and tested in a specific domain. The second one (BLC20BG) analyzes the contribution of
the monosemous examples extracted from the background data. Finally, the third run (BLC20SCBG) studies the robustness of the approach when combining the training examples from SemCor
and the automatic ones obtained from the background documents.
Table 20 summarizes ordered by recall the official results of the participants in the English
WSD domain specific task of SemEval2. In this table, Type refers to the approach followed by
the corresponding system: Weakly Supervised (WS), Supervised (S) or KB (Knowledge Based,
unsupervised). We only participate with the system using BLC20 as semantic class (the BLC20
SC/BG/SCBG runs). The wordbased classifiers (labeled SenseBG, Sense-SC and SenseSCBG)
26. In this case we use the set of BLCs from WordNet3.0, because also this version of WN was the one used in the
annotation.

113

fiI ZQUIERDO , S U AREZ & R IGAU

have been included after the evaluation campaign. Finally, as we mentioned in the introduction,
we have also included the performance of ItMakesSense system, which is one of the best performing WSD systems, on this same task for comparison purposes (it is the row on the table called
ItMakesSense in Italics).
Rank
1
2
3
4
5
6
7
8
9
10
11
...
25
...
32

System ID
CFILT2
CFILT1
IIITH1-d.1.ppr.05
IIITH2-d.2.ppr.05
BLC20SCBG
ItMakesSense
BLC20SC
Most Frequent Sense
CFILT3
Treematch
Treematch2
SenseSCBG
SenseSC
...
BLC20BG
...
Random baseline
SenseBG

Type
WS
WS
WS
WS
S
S
S
KB
KB
KB
S
S
...
S
...
S

P
0.570
0.554
0.534
0.522
0.513
0.510
0.505
0.505
0.512
0.506
0.504
0.498
0.498
...
0.380
...
0.232
0.045

R
0.555
0.540
0.528
0.516
0.513
0.510
0.505
0.505
0.495
0.493
0.491
0.484
0.484
...
0.380
...
0.232
0.001

Table 20: Precision and Recall of SemEval2 participants. ItMakesSense results are included for
comparison purpose only
In general, the results reported by SemEval for this task were quite low. The best system only
achieved a precision of 0.570, and the most frequent baseline reached a precision of 0.505. This
fact shows that the domain adaptation of WSD systems is a very difficult task.
Analyzing the results of our three runs at SemEval, our worst result is obtained by the system
using only the monosemous background examples (BLC20BG). This system ranks 23rd27 with a
Precision and Recall of 0.380 (0.385 for nouns and 0.366 for verbs). The system using only SemCor
(BLC20SC) ranks 6th with Precision and Recall of 0.505 (0.527 for nouns and 0.443 for verbs).
This is also the performance of the first sense baseline. As expected, the best result of our three
runs is obtained when combining the examples from SemCor and the background (BLC20SCBG).
This supervised system obtains the 5th position with a Precision and Recall of 0.513 (0.534 for
nouns, 0.454 for verbs) which is slightly above the baseline. Actually, this version of the system
obtains slightly better results than the best performing supervised system (ItMakesSense). Also note
that we could include automatically monosemous examples from the background test thanks to the
class-based nature of the WSD system.
Moreover, our system is the only one completely supervised participating in the task. The organizers calculated the recall with a confidence interval of 95% using bootstrap re-sampling procedure
(Noreen, 1989). This method of estimation might be more strict than other pairwise methods. It
reveals that the differences between the four first systems and our system (BLC20SCBG) are not
27. In the table it appears in the 25th position due to we have included the wordbased classifier results.

114

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

statistically significant. As can be seen in Figure 9, there is overlapping between the recall confidence interval of the four first systems and our system (ranking the 5th), which proves that the
differences are not statistically significant28 .

Figure 9: Recall confidence intervals.
Possibly, the reason of low performance of the BCL20BG system is the high correlation between the features of the target word and its semantic class. In this case, these features correspond
to the monosemous word while later are evaluated over polysemous words, with all kind of features. However, it also seems that class-based systems are robust enough to incorporate large sets
of monosemous examples from a domain text. In fact, to our knowledge, this is the first time that
a supervised WSD algorithm has been successfully adapted to a specific domain. Furthermore,
our system trained only on SemCor also achieves a good performance, reaching the most frequent
baseline, showing the robustness of class-based WSD approaches to domain variations.
Comparing to wordbased classifiers, it seems that our BLC20 classes contribute in two main
aspects. First, using the same set of features, the classbased classifiers obtain better results than
wordbased ones. The classifiers built with BLC20 are more robust and domain adaptable than
wordbased approaches. Second, the experiment that uses only examples extracted from background data considering word senses (Sense-BG) obtain an accuracy very close to zero, while the
same experiment but using BLC20 semantic classes (BLC20BG) reaches an accuracy of 0.380.
This fact indicates that BLCs are useful to extract good training examples from unlabeled data. As
mentioned previously, in order to obtain a better insight, after the evaluation campaign we performed
the same evaluation with our system using other semantic classes which represent different levels
of abstractions: BLC50, WordNet Domains and SuperSenses. Table 21 shows the precision (P)
and recall (R)29 of our evaluation considering different training datasets (SemCor only, Background
documents only and both SemCor and Background documents: SC, BG and SC+BG respectively)
and different semantic classes.
As can be seen in Table 21, BLC20 leads to a better performance when using the three different
corpora for training (BG, SC and SCBG). When training only with monosemous examples extracted
from the background documents, BLC20 obtains the best result, which may indicate that its level
of abstraction is more adequate than any other, including WND or SS, which are sets much smaller
and with a much lower polysemy. The same effect can be drawn from the results when training
with SemCor and the monosemous examples from the background (SCBG). the best results are
obtained with BLC20, and together with SuperSenses are the only two semantic classes that seem
28. This figure has been taken directly from the overview paper of the task.
29. These figures have been obtained using the official scorer script and the official gold key, without any modification.

115

fiI ZQUIERDO , S U AREZ & R IGAU

System ID
BLC20SCBG
ItMakesSense
BLC20SC
Most Frequent Sense
WNDSC
SenseSCBG
SenseSC
SS-SCBG
BLC50SCBG
BLC50SC
SSSC
WNSCBG
BLC20BG
WNDBG
SSBG
BLC50BG
Random baseline

Type
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
-

P
0.513
0.510
0.505
0.505
0.495
0.498
0.498
0.484
0.481
0.481
0.472
0.471
0.380
0.362
0.348
0.277
0.232

R
0.513
0.510
0.505
0.505
0.495
0.484
0.484
0.484
0.481
0.481
0.457
0.471
0.380
0.362
0.348
0.277
0.232

Table 21: Results of our experiments according to different semantic classes
to benefit from the background monosemous examples. These results seem to confirm the potential
capabilities of BLC20 to provide an adequate level of abstraction to perform class-based WSD.
Finally, we have proved that our system performs at the same level of one state-of-the-art sys30
tem , the ItMakesSense system (Zhong & Ng, 2010). Considering that the set of features of our
system is quite simple, and that we do not apply any machine learning optimization nor feature
engineering, our results show that the use of Semantic Classes provides a very robust behavior on
specific domains, reaching state-of-the-art results.

10. Concluding Remarks
Word sense disambiguation is a difficult task as empirically has been demonstrated by all SensEval/SemEval exercises. One reason of such difficulties could be the use of inappropriate sets of
word meanings. While WordNet is the de-facto standard repository of meanings, several attempts
have been made grouping its senses in order to achieve higher levels of accuracy. Moreover, this
approach tries to ease the hard task of creating large enough sets of annotated data per domain and
language to train supervised systems. A possible solution would be to use for manual annotation semantic class labels instead of fine-grained word senses (Schneider, Mohit, Oflazer, & Smith, 2012;
Schneider, Mohit, Dyer, Oflazer, & Smith, 2013).
Several attempts have been made to obtain word sense groupings to alleviate the problem of the
too fine granularity of word senses, most widely using WordNet senses. In most cases the approach
consists in grouping different senses of the same word, resulting in a decrease of the polysemy,
while reducing its discriminative capacity. Other works use predefined sets of semantic classes to
be integrated directly in a WSD system, mainly SuperSenses.
30. This has been tested offline, as the ItMakesSense system did not participate in the task. We downloaded the last
version of the software from http://www.comp.nus.edu.sg/nlp/software.html.

116

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

In this work we describe a simple method to automatically select Basic Level Concepts from
WordNet. Based on very simple structural properties of WordNet, our method automatically selects
different sets of BLC representing different levels of abstraction.
The aim of this work is to explore on several allwords WSD tasks the performance of different
levels of abstraction provided by Basic Level Concepts, WordNet Domains, SUMO and SuperSense
labels. Furthermore, our study empirically demonstrates that:
a) these word sense groupings cluster senses into a coherent level of abstraction in order to
perform supervised classbased WSD while not harming its performance,
b) these semantic classes can be successfully used as semantic features to boost the performance
of these classifiers,
c) the classbased approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers,
d) the classbased approach obtains competitive performances compared with word-based systems,
e) the classbased approach outperforms wordbased systems when evaluated at class level,
f) the robustness of our class-based WSD system when performing out of domain evaluation,
g) our system reaches results comparable to a state-of-the-art system (ItMakesSense) when
tested on a specific domain.
In general, classbased disambiguation of nouns and verbs achieves better results than most of
the wordbased systems presented in both SensEval2 and SensEval3. We also showed that the classbased approach reduces considerably the required amount of training examples. In order to prove
that such type of disambiguation is possible and accurate we have ranked the class-based systems
together with the SensEval2 and Senseval3 official results. In order to establish a fair comparison
we mapped when necessary word senses to semantic classes and viceversa.
Some experiments have been designed to use our classbased classifiers to perform wordsense
disambiguation. It has been shown that a very simple approach of selecting the first sense in WordNet that corresponds to the class selected by the classifiers performs as well as the top systems at
SensEval2 and SensEval3.
Additional experiments have been carried out to compare the wordbased systems to perform
classbased disambiguation. In this case we translated the official system outputs to its corresponding semantic classes.
Different experiments have been performed using different levels of abstraction, ranging from
SuperSenses (a very small set) to SUMO (which has over 1,000 labels linked to WordNet1.6 senses),
WordNet Domains (with 163 labels), or Basic Level Concepts (with an arbitrary number of classes
depending on the abstraction level selected).
With some expected differences between SensEval2 and SensEval3 results, most of the class
based systems outperform the baselines both for nouns and verbs. Specially for nouns, class-based
systems outperforms most of the SensEval2 and SensEval3 systems. In general, the results obtained
by SVM-semBLC20 are not very different to the results of SVM-semBLC50. Thus, we can select
117

fiI ZQUIERDO , S U AREZ & R IGAU

a medium level of abstraction, without having a significant decrease of the performance. Considering the number of classes, BLC classifiers obtain high performance rates while maintaining much
higher expressiveness than SuperSenses. However, using SuperSenses (40 classes) we can obtain
a very accurate semantic tagger with performances around 80%. Even better, we can use BLC20
for tagging nouns (558 semantic classes and F1 over 75%) and SuperSenses for verbs (14 semantic
classes and F1 around 75%).
Our systems at SemEval2 All-words Word Sense Disambiguation on a Specific Domain task
proved that simple features exploiting BLC can perform as well as more sophisticated methods.
Comparing with wordbased classifiers, we see that our BLC20 classes contribute in two main
aspects: the classbased classifiers obtain better results than wordbased ones and semantic classes
contribute effectively to those results. This fact indicates that, in particular, BLC20 are useful to
extract monosemous training examples from unlabeled domain data.
Our next goal is to exploit the inconsistencies of the different labeling provided by the different
class-based classifiers in order to obtain a more robust and accurate class-based WSD system. The
main idea is to study why several classifiers, each one based on a different degree of abstraction (e.g.
BLC20, BLC50, WordNet Domains, etc.) label a concrete context or example with incompatible
tags. In this manner, we would be able to predict when to apply the best classifier depending on the
context.

Acknowledgements
This work has been partially supported by the NewsReader project31 (ICT-2011-316404), the Spanish project SKaTer32 (TIN2012-38584-C06-02).

References
Agirre, E., & de Lacalle, O. L. (2003). Clustering wordnet word senses. In Proceedings of
RANLP03, Borovets, Bulgaria.
Agirre, E., & Edmonds, P. (2007). Word Sense Disambiguation: Algorithms and Applications.
Springer.
Agirre, E., Lopez de Lacalle, O., Fellbaum, C., Hsieh, S.-K., Tesconi, M., Monachini, M., Vossen,
P., & Segers, R. (2010). Semeval-2010 task 17: All-words word sense disambiguation on a
specific domain. In Proceedings of the 5th International Workshop on Semantic Evaluation,
pp. 7580, Uppsala, Sweden. Association for Computational Linguistics.
Bhagwani, S., Satapathy, S., & Karnick, H. (2013). Merging word senses. In Proceedings of Workshop on Graph-based Methods for Natural Language Processing (TextGraphs-8), pp. 1119.
Castillo, M., Real, F., & Rigau, G. (2004). Automatic assignment of domain labels to wordnet. In
Proceeding of the 2nd International WordNet Conference, pp. 7582.
Ciaramita, M., & Altun, Y. (2006). Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP06), pp. 594602, Sydney, Australia. ACL.
31. http://www.newsreader-project.eu
32. http://nlp.lsi.upc.edu/skater

118

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

Ciaramita, M., & Johnson, M. (2003). Supersense tagging of unknown nouns in wordnet.
In Proceedings of the Conference on Empirical methods in natural language processing
(EMNLP03), pp. 168175. ACL.
Curran, J. (2005). Supersense tagging of unknown nouns using semantic similarity. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics (ACL05), pp. 26
33. ACL.
Escudero, G., Marquez, L., & Rigau., G. (2000). An Empirical Study of the Domain Dependence
of Supervised Word Sense Disambiguation Systems. In Proceedings of the joint SIGDAT
Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,
EMNLP/VLC, Hong Kong, China.
Fellbaum, C. (Ed.). (1998). WordNet. An Electronic Lexical Database. The MIT Press.
Gangemi, A., Nuzzolese, A. G., Presutti, V., Draicchio, F., Musetti, A., & Ciancarini, P. (2012).
Automatic typing of dbpedia entities. In Proceedings of the 11th International Conference on
The Semantic Web - Volume Part I, ISWC12, pp. 6581, Berlin, Heidelberg. Springer-Verlag.
Gonzalez, A., Rigau, G., & Castillo, M. (2012). A graph-based method to improve wordnet domains.
In Computational Linguistics and Intelligent Text Processing, pp. 1728. Springer.
Hamp, B., Feldweg, H., et al. (1997). Germanet-a lexical-semantic net for german. In Proceedings of
ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources
for NLP Applications, pp. 915. Citeseer.
Hearst, M., & Schutze, H. (1993). Customizing a lexicon to better suit a computational task. In
Proceedingns of the ACL SIGLEX Workshop on Lexical Acquisition, Stuttgart, Germany.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). Ontonotes: The 90
In Proceedings of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, NAACL-Short 06, pp. 5760, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Izquierdo, R., Suarez, A., & Rigau, G. (2007). Exploring the automatic selection of basic level concepts. In et al., G. A. (Ed.), International Conference Recent Advances in Natural Language
Processing, pp. 298302, Borovets, Bulgaria.
Izquierdo, R., Suarez, A., & Rigau, G. (2009). An empirical study on class-based word sense disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association
for Computational Linguistics, EACL 09, pp. 389397, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Izquierdo, R., Suarez, A., & Rigau, G. (2010). Gplsi-ixa: Using semantic classes to acquire monosemous training examples from domain texts. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pp. 402406. Association for Computational Linguistics.
Joachims, T. (1998). Text categorization with support vector machines: learning with many relevant
features. In Nedellec, C., & Rouveirol, C. (Eds.), Proceedings of ECML-98, 10th European
Conference on Machine Learning, No. 1398, pp. 137142, Chemnitz, DE. Springer Verlag,
Heidelberg, DE.
L. Bentivogli, P. Forner, B. M., & Pianta, E. (2004). Revising wordnet domains hierarchy: Semantics, coverage, and balancing. In COLING 2004 Workshop on Multilingual Linguistic
Resources, Geneva, Switzerland.
119

fiI ZQUIERDO , S U AREZ & R IGAU

Magnini, B., & Cavaglia, G. (2000). Integrating subject field codes into wordnet. In Proceedings of
LREC, Athens. Greece.
Marquez, L., Escudero, G., Martnez, D., & Rigau, G. (2006). Supervised corpus-based methods
for wsd. In E. Agirre and P. Edmonds (Eds.) Word Sense Disambiguation: Algorithms and
applications., Vol. 33 of Text, Speech and Language Technology. Springer.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Association for Computational Linguistics,
Barcelona, Spain.
Mihalcea, R. (2007). Using wikipedia for automatic word sense disambiguation. In Proceedings of
NAACL HLT 2007.
Mihalcea, R., Csomai, A., & Ciaramita, M. (2007). Unt-yahoo: Supersenselearner: Combining
senselearner with supersense and other coarse semantic features. In Proceedings of the 4th
International Workshop on Semantic Evaluations, SemEval 07, pp. 406409, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Mihalcea, R., & Moldovan, D. (2001). Automatic generation of coarse grained wordnet. In Proceding of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburg, USA.
Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). A Semantic Concordance. In Proceedings
of the ARPA Workshop on Human Language Technology.
Navigli, R. (2006). Meaningful clustering of senses helps boost word sense disambiguation performance. In ACL-44: Proceedings of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Association for Computational Linguistics,
pp. 105112, Morristown, NJ, USA. Association for Computational Linguistics.
Navigli, R. (2009). Word Sense Disambiguation: a survey. ACM Computing Surveys, 41(2), 169.
Navigli, R., Litkowski, K., & Hargraves, O. (2007). Semeval-2007 task 07: Coarse-grained english
all-words task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pp. 3035, Prague, Czech Republic. Association for Computational
Linguistics.
Niles, I., & Pease, A. (2001). Towards a standard upper ontology. In Proceedings of the 2nd
International Conference on Formal Ontology in Information Systems (FOIS-2001), pp. 17
19. Chris Welty and Barry Smith, eds.
Niles, I., & Pease, A. (2003). Linking lexicons and ontologies: Mapping WordNet to the Suggested
Upper Merged Ontology. In Arabnia, H. R. (Ed.), Proc. of the IEEE Int. Conf. on Inf. and
Knowledge Engin. (IKE 2003), Vol. 2, pp. 412416. CSREA Press.
Noreen, E. (1989). Computer-intensive methods for testing hypotheses: an introduction. A Wiley
Interscience publication. Wiley.
Paa, G., & Reichartz, F. (2009a). Exploiting semantic constraints for estimating supersenses with
crfs.. In SDM, pp. 485496. SIAM.
Paa, G., & Reichartz, F. (2009b). Exploiting semantic constraints for estimating supersenses with
crfs.. In SDM, pp. 485496. SIAM.
120

fiW ORD VS . C LASS -BASED W ORD S ENSE D ISAMBIGUATION

Palmer, M., Fellbaum, C., Cotton, S., Delfs, L., & Dang, H. T. (2001). English tasks: All-words
and verb lexical sample. In Proceedings of the SENSEVAL-2 Workshop. In conjunction with
ACL2001/EACL2001, Toulouse, France.
Peters, W., Peters, I., & Vossen, P. (1998). Automatic sense clustering in eurowordnet. In First International Conference on Language Resources and Evaluation (LREC98), Granada, Spain.
Picca, D., Gliozzo, A. M., & Ciaramita, M. (2008). Supersense tagger for italian.. In LREC. Citeseer.
Pradhan, S., Dligach, E. L. D., & Palmer, M. (2007). Semeval-2007 task 17: English lexical sample,
srl and all words. In SemEval 07: Proceedings of the 4th International Workshop on Semantic
Evaluations, pp. 8792, Morristown, NJ, USA. Association for Computational Linguistics.
Rosch, E. (1977). Human categorisation. Studies in Cross-Cultural Psychology, I(1), 149.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in Language Processing, pp. 4449.
Schneider, N., Mohit, B., Dyer, C., Oflazer, K., & Smith, N. A. (2013). Supersense tagging for
arabic: the mt-in-the-middle attack.. In HLT-NAACL, pp. 661667. Citeseer.
Schneider, N., Mohit, B., Oflazer, K., & Smith, N. A. (2012). Coarse lexical semantic annotation
with supersenses: an arabic case study. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short Papers-Volume 2, pp. 253258. Association
for Computational Linguistics.
Segond, F., Schiller, A., Greffenstette, G., & Chanod, J. (1997). An experiment in semantic tagging
using hidden markov model tagging. In ACL Workshop on Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP Applications, pp. 7881. ACL, New
Brunswick, New Jersey.
Snow, R., S., P., D., J., & A., N. (2007). Learning to merge word senses. In Proceedings of Joint
Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 10051014.
Snyder, B., & Palmer, M. (2004). The english all-words task. In Mihalcea, R., & Edmonds, P.
(Eds.), Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pp. 4143, Barcelona, Spain. Association for Computational Linguistics.
Tsvetkov, Y., Schneider, N., Hovy, D., Bhatia, A., Faruqui, M., & Dyer, C. (2014). Augmenting
english adjective senses with supersenses. In Proc. of LREC, pp. 43594365.
Villarejo, L., Marquez, L., & Rigau, G. (2005). Exploring the construction of semantic class classifiers for wsd. In Proceedings of the 21th Annual Meeting of Sociedad Espaola para el
Procesamiento del Lenguaje Natural SEPLN05, pp. 195202, Granada, Spain. ISSN 11365948.
Vossen, P. (Ed.). (1998). EuroWordNet: A Multilingual Database with Lexical Semantic Networks
. Kluwer Academic Publishers .
Wikipedia (2015). Wikipedia, the free encyclopedia. https://en.wikipedia.org.. [Online;
accessed 21-August-2015].
Yarowsky, D. (1994). Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french. In Proceedings of the 32nd Annual Meeting of the Association for
Computational Linguistics (ACL94).
121

fiI ZQUIERDO , S U AREZ & R IGAU

Zhong, Z., & Ng, H. T. (2010). It makes sense: A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos 10, pp.
7883, Stroudsburg, PA, USA. Association for Computational Linguistics.

122

fiJournal of Artificial Intelligence Research 54 (2015) 193231

Submitted 6/15; published 10/15

Expressiveness of Two-Valued Semantics for
Abstract Dialectical Frameworks
Hannes Strass

strass@informatik.uni-leipzig.de

Computer Science Institute, Leipzig University
Augustusplatz 10, 04109 Leipzig, Germany

Abstract
We analyse the expressiveness of Brewka and Woltrans abstract dialectical frameworks
for two-valued semantics. By expressiveness we mean the ability to encode a desired set
of two-valued interpretations over a given propositional vocabulary A using only atoms
from A. We also compare ADFs expressiveness with that of (the two-valued semantics of)
abstract argumentation frameworks, normal logic programs and propositional logic. While
the computational complexity of the two-valued model existence problem for all these
languages is (almost) the same, we show that the languages form a neat hierarchy with
respect to their expressiveness. We then demonstrate that this hierarchy collapses once we
allow to introduce a linear number of new vocabulary elements. We finally also analyse and
compare the representational succinctness of ADFs (for two-valued model semantics), that
is, their capability to represent two-valued interpretation sets in a space-efficient manner.

1. Introduction
More often than not, different knowledge representation languages have conceptually similar and partially overlapping intended application areas. What are we to do if faced with
an application and a choice of several possible knowledge representation languages which
could be used for the application? One of the first axes along which to compare different
formalisms that comes to mind is computational complexity: if a language is computationally too expensive when considering the problem sizes typically encountered in practice,
then this is a clear criterion for exclusion. But what if the available language candidates
have the same computational complexity? If their expressiveness in the computationalcomplexity sense of What kinds of problems can the formalism solve? is the same, we
need a more fine-grained notion of expressiveness. In this paper, we use such a notion and
study the expressiveness of abstract dialectical frameworks (ADFs) (Brewka & Woltran,
2010; Brewka, Ellmauthaler, Strass, Wallner, & Woltran, 2013), a recent generalisation of
abstract argumentation frameworks (AFs) (Dung, 1995).
Argumentation frameworks are the de-facto standard formalism in abstract argumentation, a field that studies how (abstract) arguments relate to each other in terms of directed
conflicts (attacks), and how these conflicts can be resolved without looking into the
arguments. While AFs are popular and well-studied, it has been noted many times in the
literature that their expressive capabilities are somewhat limited. This has only recently
been made technically precise by Dunne, Dvorak, Linsbichler, and Woltran (2014, 2015),
who basically showed that introducing new, purely technical arguments is sometimes inevitable when using AFs for representation purposes. However, due to their very nature,
the dialectical meaning of such technical arguments might be  ironically  debatable.
c
2015
AI Access Foundation. All rights reserved.

fiStrass

Not surprisingly, quite a number of generalisations of AFs have been proposed (for an
overview we refer to Brewka, Polberg, & Woltran, 2014). As one of the most general AF
alternatives, the aforementioned abstract dialectical frameworks (ADFs) have emerged. In
that formalism, not only arguments (called statements there) are abstract, but also links
between arguments. While in AFs the links are necessarily attacks, in ADFs each statement
has an associated acceptance condition  a Boolean function over its parent statements
 that specifies exactly when the statement can be accepted. In this way, acceptance
conditions ultimately express the meaning of links in an ADF. Even the restricted subclass
of bipolar ADFs  where intuitively all links are supporting or attacking  is a proper
generalisation of AFs, and a quite expressive one as we shall see in this paper.
ADFs could be called the lovechild of AFs and logic programs, since they combine
intuitions and semantics from Dung-style abstract argumentation as well as logic programming (Brewka et al., 2013; Strass, 2013; Alviano & Faber, 2015). While on the abstract
level, ADFs are intended to function as argumentation middleware  a sufficiently expressive target formalism for translations from more concrete (application) formalisms. As
part of the ADF success story, we just mention a reconstruction of the Carneades model
of argument (Brewka & Gordon, 2010), an instantiation of simple defeasible theories into
ADFs (Strass, 2015a), and recent applications of ADFs for legal reasoning and reasoning
with cases by Al-Abdulkarim, Atkinson, and Bench-Capon (2014, 2015).
In this paper, we approach abstract dialectical frameworks as knowledge representation
formalisms, since they are used to represent knowledge about arguments and relationships
between these arguments. We employ this view to analyse the representational capabilities
of ADFs. Due to their roots in AFs and logic programs, we also compare the representational capabilities of those formalisms in the same setting. In this initial study we restrict
ourselves to looking at two-valued semantics, more specifically the ADF (stable) model semantics, which corresponds to AF stable extension semantics, and the supported and stable
model semantics for logic programs. We add propositional logic to have a well-known reference point. Analysing these precise formalisms additionally makes sense to us because the
computational complexity of their respective model existence problems is the same (with
one exception):
 for AFs, deciding stable extension existence is NP-complete (Dimopoulos, Nebel, &
Toni, 2002);
 for normal logic programs, deciding the existence of supported/stable models is NPcomplete (Bidoit & Froidevaux, 1991; Marek & Truszczynski, 1991);
 for ADFs, deciding the existence of (supported) models is NP-complete (Brewka
et al., 2013), deciding the existence of stable models is P2 -complete for general
ADFs (Brewka et al., 2013) and NP-complete for the subclass of bipolar ADFs (Strass
& Wallner, 2015);
 the propositional satisfiability problem is NP-complete.
In view of these almost identical complexities, we use an alternative measure of the
expressiveness of a knowledge representation formalism F: Given a set of two-valued
interpretations, is there a knowledge base in F that has this exact model set? This notion
194

fiExpressiveness of Two-Valued Semantics for ADFs

lends itself straightforwardly to compare different formalisms (Gogic, Kautz, Papadimitriou,
& Selman, 1995):
Formalism F2 is at least as expressive as formalism F1 if and only if every
knowledge base in F1 has an equivalent knowledge base in F2 .
So here expressiveness is understood in terms of realisability, What kinds of model sets
can the formalism express? (In model theory, this is known as definability.)
It is easy to see that propositional logic can express any set of two-valued interpretations,
it is universally expressive. The same is easy (but less easy) to see for normal logic programs
under supported model semantics. For normal logic programs under stable model semantics,
it is clear that not all model sets can be expressed, since two different stable models are
always incomparable with respect to the subset relation.1 In this paper, we study such
expressiveness properties for all the mentioned formalisms under different semantics. It
turns out that the languages form a more or less strict expressiveness hierarchy, with AFs
at the bottom, ADFs and LPs under stable semantics higher up and ADFs and LPs under
supported model semantics at the top together with propositional logic.
To show that a language F2 is at least as expressive as a language F1 we will mainly
use two different techniques. In the best case, we can use a syntactic compact and faithful
translation from knowledge bases of F1 to those of F2 . Compact means that the translation
does not change the vocabulary, that is, does not introduce new atoms. Faithful means that
the translation exactly preserves the models of the knowledge base for respective semantics
of the two languages. In the second best case, we assume the knowledge base of F1 to be
given in the form of a set X of desired models and construct a semantic realisation of X
in F2 , that is, a knowledge base in F2 with model set precisely X. To show that language
F2 is strictly more expressive than F1 , we additionally have to present a knowledge base kb
from F2 of which we prove that F1 cannot express the model set of kb.
Analysing the expressiveness of argumentation formalisms is a quite recent strand of
work. Its ascent can be attributed to Dunne et al. (2014, 2015), who studied realisability
for argumentation frameworks (allowing to introduce new arguments as long as they are
never accepted). Likewise, Dyrkolbotn (2014) analysed AF realisability under projection
(allowing to introduce new arguments) for three-valued semantics. Baumann, Dvorak, Linsbichler, Strass, and Woltran (2014) studied the expressiveness of the subclass of compact
AFs, where each argument is accepted at least once. Finally, and most recently, Puhrer
(2015) analysed the realisability of three-valued semantics for ADFs. Previous more preliminary works include that of Brewka, Dunne, and Woltran (2011), who translated ADFs
into AFs for the ADF model and AF stable extension semantics, however this translation
introduces additional arguments and is therefore not compact; and ours (Strass, 2013),
where we studied the syntactic intertranslatability of ADFs and LPs, but did not look at
expressiveness or realisability.
The gain that is achieved by our analysis in this paper is not only that of increased
clarity about fundamental properties of these knowledge representation languages  What
can these formalisms express, actually?  but has several further applications. As Dunne
et al. (2015) remarked, a major application is in constructing knowledge bases with the aim
1. However, the stable model semantics becomes universally expressive once we allow nested expressions of
the form not not p in rule bodies (Lifschitz, Tang, & Turner, 1999; Lifschitz & Razborov, 2006).

195

fiStrass

of encoding a certain model set. As a necessary prerequisite to this, it must be known that
the intended model set is realisable in the first place. For example, in a recent approach to
revising argumentation frameworks (Coste-Marquis, Konieczny, Mailly, & Marquis, 2014),
the authors avoid this problem by assuming to produce a collection of AFs whose model sets
in union produce the desired model set. While the work of Dunne et al. (2015) showed that
this is indeed necessary in the case of AFs and stable extension semantics, our work shows
that for ADFs under the model semantics, a single knowledge base (ADF) is always enough
to realise any given model set. What is more, if we assume that the intended model set is
given in the form of a propositional formula, then the size of the realising ADF is at most
linear in the size of the formula. This is only one example  we will on several occasions
also consider the sizes of realisations, as is not uncommon in logic-based AI (Darwiche &
Marquis, 2002; Lifschitz & Razborov, 2006; French, van der Hoek, Iliev, & Kooi, 2013; Shen
& Zhao, 2014). Indeed, representation size is a fundamental practical aspect of knowledge
representation languages: universal expressiveness is of little use if the model sets to express
require exponential-size knowledge bases even in the best case!
Of course, the fact that the languages we study have the same computational complexity
means that there in principle exist polynomial intertranslations for the respective decision
problems. But such intertranslations may involve the introduction of a polynomial number
of new atoms. In theory, an increase from n atoms to nk atoms for some k > 1 is of
no consequence. In practice, it has a profound impact: the number n of atoms directly
influences the search space that any implementation potentially has to cover. There, a step
from 2n to
 k1 n
k
k1
2n = 2n n = 2n
amounts to an exponential increase in search space size. Being able to realise a model set
compactly, without new atoms, therefore attests that a formalism F has a certain basic
kind of efficiency property, in the sense that the F-realisation of a model set does not
unnecessarily enlarge the search space of algorithms operating on it.
It might seem that it is a restricting assumption to view formalisms as sets F of knowledge bases kb where F is associated with a two-valued semantics. However, this language
representation model is universal in the sense that it is just another way of expressing languages as sets of words over {0, 1}. Using an n-element vocabulary An = {a1 , . . . , an }, a binary word w = x1 x2    xn of length n is encoded as the set Mw = {ai  An | xi = 1}  An .
For example, using the vocabulary A3 = {a1 , a2 , a3 }, the binary word 101 of length 3 corresponds to the set M101 = {a1 , a3 }. Consequently, a set Ln of words of length n can
be represented by a set XLn  2An of subsets of An : XLn = {Mw | w  Ln }. With the
above example vocabulary, the word set L3 = {101, 110, 011} is represented by the model
set XL3 = {{a1 , a3 } , {a1 , a2 } , {a2 , a3 }}. Conversely, each sequence (Xn )n0 of sets with
S
Xn  2An uniquely determines a language L = n0 Ln over {0, 1}: for each n  N, we
have Ln = {wM | M  Xn } with wM = x1 x2    xn where for each i  {1, . . . , n}, xi = 1 if
ai  M and xi = 0 if ai 
/ M . In this paper we use language to refer to object-level
languages while formalism refers to meta-level languages, such as propositional logic,
argumentation frameworks, abstract dialectical frameworks, and logic programs.
Formally, the syntax of ADFs is defined via Boolean functions. However, we are interested in representations of ADFs. So we have to fix a representation of ADFs via fixing
196

fiExpressiveness of Two-Valued Semantics for ADFs

a representation of Boolean functions. We choose to use (unrestricted) propositional formulas, as is customary in most of the literature (Brewka & Woltran, 2010; Brewka et al.,
2013; Polberg et al., 2013; Polberg, 2014; Gaggl & Strass, 2014; Linsbichler, 2014; Strass &
Wallner, 2015; Puhrer, 2015; Gaggl, Rudolph, & Strass, 2015). Exceptions to this custom
are the works of Brewka et al. (2011), who use Boolean circuits, and one of ours (Strass,
2013) where we used characteristic models (that is, used a representation that is equivalent
to representing the formulas in disjunctive normal form). For the subclass of bipolar ADFs,
yet no uniform representation exists, which is another question we address in this paper.
By propositional formulas over a vocabulary A we mean formulas over the Boolean
basis {, , }, that is, trees whose leaves (sinks) are atoms from A or the logical constants
true > or false , and internal nodes are either unary () or binary (,). We also make
occasional use of Boolean circuits, where trees above is replaced by directed acyclic
graphs; in particular, we allow unbounded fan-in, that is, reusing sub-circuits. As usual,
the depth of a formula (circuit) is the length of the longest path from the root to a leaf
(sink). Figure 1 below shows formula and circuit examples of depth 3.



p



















q

p

q

p

q

Figure 1: Representing (p  q)  (q  p) as a formula tree (left) and a circuit (right).

Analysing the expressive power and representation size of Boolean circuits is an established sub-field of computational complexity (Arora & Barak, 2009). This has led to a
number of language classes whose members can be recognised by Boolean circuits satisfying
S
certain restrictions. We will need the class AC0 , which contains all languages L = n0 Ln
for which there exist d, k  N such that for each n  N, there exists a Boolean circuit Cn of
depth at most d and size at most nk where the models of Cn exactly express Ln .2 In other
words, every language L  AC0 can be recognised by a family of polynomial-size Boolean
circuits of a fixed maximal depth that is independent of word length.
The paper proceeds as follows. We first define the notion of expressiveness (and succinctness) formally and then introduce the formalisms we will study. After reviewing several
intertranslatability results for these languages, we step-wise obtain the results that lead to
the expressiveness hierarchy, while at times also looking at representational efficiency. We
finally show that allowing to linearly expand the vocabulary leads to a collapse of the
hierarchy. The paper concludes with a discussion of possible future work.

2. To be more precise, for each n  N, the models of Cn are exactly XLn , which in turn expresses Ln .

197

fiStrass

2. Background
We presume a finite set A of atoms (statements, arguments), the vocabulary. A knowledge
representation formalism interpreted over A is then some set F; a (two-valued) semantics
A
for F is a mapping  : F  22 that assigns sets of two-valued models to knowledge bases
kb  F. (So A is implicit in .) Strictly speaking, a two-valued interpretation is a mapping
from the set of atoms into the two truth values true and false, but for technical ease we
represent two-valued interpretations by the sets containing the atoms that are true. Below,
we write (F) = {(kb) | kb  F}; intuitively, (F) is the set of interpretation sets that
formalism F can express, with any knowledge base whatsoever. For example, for F = PL
A
propositional logic and  = mod the usual model semantics, we have (PL) = 22 since
obviously any set of models is realisable in propositional logic.3 This leads us to compare
different pairs of languages and semantics with respect to the semantics range of models.
Our concept of formalism concentrates on semantics and decidedly remains abstract. We
first define the expressiveness relation among formalisms.
Definition 1. Let A be a finite vocabulary, F1 , F2 be formalisms that are interpreted over
A
A
A and 1 : F1  22 and 2 : F2  22 be two-valued semantics. We define
F11 e F22

iff

1 (F1 )  2 (F2 )

Intuitively, formalism F2 under semantics 2 is at least as expressive as formalism F1 under
semantics 1 , because all model sets that F1 can express under 1 are also contained in
those that F2 can produce under 2 . (If the semantics are clear from the context we will
omit them; this holds in particular for argumentation frameworks and propositional logic,
where we only look at a single semantics.) As usual,
 F1 <e F2 iff F1 e F2 and F2 6e F1 ;
 F1 
=e F2 iff F1 e F2 and F2 e F1 .
The relation e is reflexive and transitive by definition, but not necessarily antisymmetric.
That is, there might different formalisms F1 6= F2 that are equally expressive: F1 
=e F2 .
We next introduce the succinctness relation as defined by Gogic et al. (1995).
Definition 2. Let A be a finite vocabulary; let F1 and F2 be formalisms that are interpreted over A, have size measures kk1 and kk2 , and two-valued semantics 1 and 2 ,
respectively. Define F11 s F22 if and only if there is a k  N such that for all kb1  F1 with
1 (kb1 )  1 (F1 )  2 (F2 ), there is a kb2  F2 with 1 (kb1 ) = 2 (kb2 ) and kkb2 k2  kkb1 kk1 .
Intuitively, F11 s F22 means that F2 under 2 is at least as succinct as F1 under 1 .
Put another way, for F11 s F22 to hold, any knowledge base from F1 with an equivalent
counterpart in F2 must have an equivalent counterpart that is at most polynomially larger.
Note that succinctness talks only about those model sets that both can express, so it is
most meaningful when comparing languages that are equally expressive, that is, whenever
3. For a set X  2A we can simply define X =
mod (X ) = X.

W

M X

198

M with M =

V

aM

a

V

aA\M

a and clearly

fiExpressiveness of Two-Valued Semantics for ADFs

1 (F1 ) = 2 (F2 ). As usual, we define F1 <s F2 iff F1 s F2 and F2 6s F1 , and F1 
=s F2
iff F1 s F2 and F2 s F1 . The relation s is reflexive, but not necessarily antisymmetric
or transitive.
The final general definition is about formalisms polynomially expressing languages.
Here, we already make use of the previously introduced bijection between interpretations
and binary words and use the term languages to synonymously refer to both.
S
Definition 3. A formalism F can polynomially express a language L = n0 Ln under
A

semantics  : F  22 if and only if there is a k  N such that for each positive n  N there
is a knowledge base kbn  F of that formalism such that (kbn ) = Ln and kkbn k  O(nk ).
We next introduce some specific object-level languages that we will use. First of all,
the language Parity contains all odd-element subsets of the vocabulary. Formally, for
An = {a1 , . . . , an } with n  1 we have
Parityn = {M  An | m  N : |M | = 2m + 1}
S
As explained before, then Parity = nN,n1 Parityn . It is a textbook result that Parity
is expressible by polynomial-size propositional formulas (Jukna, 2012); for example, we can
define Parity
(a1 ) = a1 and for n  2 set
1
Parity
(a1 , . . . , an ) = (Parity
(a1 , . . . , an )  Parity
(an +1 , . . . , an )) 
n
n
n
(Parity
(a1 , . . . , an )  Parity
(an +1 , . . . , an ))
n
n
 
 
with n = n2 and n = n2 . (This construction yields a formula of logarithmic depth and
therefore polynomial size.) It is also a textbook result (although not nearly as easy to
see) that Parity cannot be expressed by depth-bounded polynomial-size circuits, that is,
Parity 
/ AC0 (Jukna, 2012).
As another important class, threshold languages are defined for n, k  N with n  1 and
k  n:
Thresholdn,k = {M  An | k  |M |}
That is, Thresholdn,k contains
 all interpretations over n atoms where at least k atoms
are true. The special case k = n2 leads to the majority languages,
Majorityn = Thresholdn,d n e
2

that contain all interpretations where at least half of the atoms in the vocabulary are true.
We next introduce the particular knowledge representation languages we study in this
paper. All will make use of a vocabulary A; the results of the paper are all considered
parametric in such a given vocabulary.
2.1 Logic Programs
For a vocabulary A we define not A = {not a | a  A} and accordingly the set of literals over
A as A = A  not A. A normal logic program rule over A is then of the form a  B where
a  A and B  A . The set B is called the body of the rule, we abbreviate B + = B  A and
199

fiStrass

B  = {a  A | not a  B}. A logic program (LP) P over A is a set of logic program rules
over A. An interpretation M  A satisfies the body B of a rule a  B  P iff B +  M and
B   M = . M is a supported model for P iff M = {a  A | a  B  P, M satisfies B}.
For a logic program P we denote the set of its supported models by su(P ). The intuition
behind this semantics is that the atoms that are true in a model are all and only those that
have some kind of support.
However, this support might be cyclic self-support. For instance, the logic program
{a  {a}} has two supported models,  and {a}, where the latter is undesired in many
application domains. As an alternative, Gelfond and Lifschitz (1988) proposed the stable
model semantics, that does not allow self-support: A set M  A is a stable model for P iff
M is the -least supported model of P M , where P M is obtained from P by (1) eliminating
each rule whose body contains a literal not a with a  M , and (2) deleting all literals of
the form not a from the bodies of the remaining rules (Gelfond & Lifschitz, 1988). We
write st(P ) for the set of stable models of P . It follows from the definition that st(P )
is a -antichain: for all M1 6= M2  st(P
P) we have M1 6 M2 . As size measure we define
ka  Bk = |B| + 1 for rules and kP k = rP krk for programs.
As an example, consider the vocabulary A = {a, b, c} and over it the logic program
P = {a  {b} , b  {a} , c  {not a}}. We find su(P ) = {{c} , {a, b}} and st(P ) = {{c}}.
2.2 Argumentation Frameworks
Dung (1995) introduced argumentation frameworks as pairs F = (A, R) where A is a set
of (abstract) arguments and R  A  A a relation of attack between the arguments. The
purpose of semantics for argumentation frameworks is to determine sets of arguments (called
extensions) which are acceptable according to various standards. For a given extension
S  A, the arguments in S are considered to be accepted, those that are attacked by
some argument in S are considered to be rejected, and all others are neither, their status is
undecided. We will only be interested in so-called stable extensions, sets S of arguments that
do not attack each other and attack all arguments not in the set. For stable extensions, each
argument is either accepted or rejected by definition, thus the semantics is two-valued. More
formally, a set S  A of arguments is conflict-free iff there are no a, b  S with (a, b)  R.
A set S is a stable extension for (A, R) iff it is conflict-free and for all a  A \ S there is an
argument b  S with (b, a)  R. For an AF F , we denote the set of its stable extensions by
st(F ). Again, it follows from the definition of a stable extension that the set st(F ) is always
a -antichain. The size of an argumentation framework F = (A, R) is kF k = |A| + |R|.
For example, the AF F = ({a, b, c} , {(a, b), (b, a), (b, c)}) can be visualised using the
c and has the set of stable extensions st(F ) = {{a, c} , {b}}.
b
directed graph a
2.3 Abstract Dialectical Frameworks
An abstract dialectical framework is a tuple D = (A, L, C) where A is a set of statements
(representing positions one can take or not take in a debate), L  A  A is a set of links
(representing dependencies between the positions), C = {Ca }aA is a collection of total
functions Ca : 2par (a)  {t, f }, one for each statement a  A. The function Ca is called
acceptance condition of a and expresses whether a can be accepted, given the acceptance
200

fiExpressiveness of Two-Valued Semantics for ADFs

status of its parents par (a). In this paper, we represent each Ca by a propositional formula
a over par (a). As mentioned earlier, propositional formulas are built using negation ,
conjunction  and disjunction ; connectives for material implication , logical equivalence
 and exclusive disjunction = are regarded as abbreviations. To specify an acceptance
condition, then, we take Ca (M  par (a)) = t to hold iff M is a model for a , M |= a .
Brewka and Woltran (2010) introduced a useful subclass of ADFs: an ADF D = (A, L, C)
is bipolar iff all links in L are supporting or attacking (or both). A link (b, a)  L is supporting in D iff for all M  par (a), we have that Ca (M ) = t implies Ca (M  {b}) = t.
Symmetrically, a link (b, a)  L is attacking in D iff for all M  par (a), we have that
Ca (M  {b}) = t implies Ca (M ) = t. If a link (b, a) is both supporting and attacking then
b has no influence on a, the link is redundant (but does not violate bipolarity). We will
sometimes use this circumstance when searching for ADFs; there we simply assume that
L = A  A, then links that are actually not needed can be expressed by acceptance conditions that make them redundant.
There are numerous semantics for ADFs; we will only be interested in two of them,
(supported) models and stable models. A set M  A is a model of D iff for all a  A
we find that a  M iff Ca (M ) = t. The definition of stable models is inspired by logic
programming and slightly more complicated (Brewka et al., 2013). Define an operator by4
D (X, Y ) = (ac(X, Y ), re(X, Y )) for X, Y  A, where
ac(X, Y ) = {a  A | Z  A : X  Z  A \ Y  Ca (Z) = t}
re(X, Y ) = {a  A | Z  A : X  Z  A \ Y  Ca (Z) = f }
The intuition behind the operator is as follows: A pair (X, Y ) represents a partial interpretation of the set of statements where those in X are accepted (true), those in Y are rejected
(false), and those in A \ (X  Y ) are neither. The operator checks for each statement a
whether all total interpretations that can possibly arise from (X, Y ) agree on their truth
value for the acceptance condition of a. That is, if a has to be accepted no matter how the
statements in A \ (X  Y ) are interpreted, then a  acc(X, Y ). The set rej (X, Y ) is defined
symmetrically, so the pair (acc(X, Y ), rej (X, Y )) constitutes a refinement of (X, Y ).
For M  A, the reduced ADF DM = (M, LM , C M ) is defined by LM = L  M  M and
for each a  M setting M
/ M ], that is, replacing all b 
/ M by false in the
a = a [b/ : b 
acceptance formula of a. A model M for D is a stable model of D iff the least fixpoint of the
operator DM is given by (M, ). As usual, su(D) and st(D) denote the respective model
sets; while ADF models can be
P-related, ADF stable models cannot. The size of an ADF
D over A is given by kDk = aA ka k; the size kk of a formula  is the number of its
nodes.
As an example ADF D, consider vocabulary A = {a, b, c} and the acceptance formulas
a = c, b = c, and c = a  b. While D has a single supported model, su(D) = {{a, b, c}},
we find st(D) =  since the atoms in the model support each other circularly.
2.4 Translations Between the Formalisms
We will review all known translations between the mentioned formalisms.
4. This operator is closely related to the ultimate approximation operators of Denecker, Marek, and
Truszczynski (2004), as we observed earlier (Strass, 2013).

201

fiStrass

2.4.1 From AFs to BADFs
Brewka and Woltran (2010) showed how to translate AFs into ADFs: For an AF FV= (A, R),
define the ADF associated to F as DF = (A, R, C) with C = {a }aA and a = (b,a)R b
for a  A. Clearly, the resulting ADF is bipolar: parents are always attacking. Brewka
and Woltran proved that this translation is faithful for the AF stable extension and ADF
model semantics (Proposition 1). Brewka et al. (2013) later proved the same for the AF
stable extension and ADF stable model semantics (Theorem 4). It is easy to see that the
translation can be computed in polynomial time and induces at most a linear blowup.
2.4.2 From ADFs to PL
Brewka and Woltran (2010) also showed that ADFs under supported model semantics
can be faithfully translated into propositional logic: when acceptance conditions of statements a  A are represented by propositional formulas a , then the supported models
of an ADF D over A are given by the classical propositional models of the formula set
D = {a  a | a  A}.
2.4.3 From AFs to PL
In combination, the previous two translations yield
and
n a polynomial
V
 fifaithfulotranslation
fi
chain from AFs into propositional logic: (A,R) = a 
(b,a)R b fi a  A .
2.4.4 From ADFs to LPs
In earlier work (Strass, 2013), we showed that ADFs can be faithfully translated into normal
logic programs. For an ADF D = (A, L, C), its standard LP is
PD = {a  (M  not (par (a) \ M )) | a  A, Ca (M ) = t}
It follows from Lemma 3.14 of Strass (2013) that this translation preserves the supported
model semantics. The translation is size-preserving for the acceptance condition representation of Strass (2013) via characteristic models; when representing acceptance conditions
via propositional formulas, this cannot be guaranteed as we will show later.5
2.4.5 From AFs to LPs
The translation chain from AFs to ADFs to LPs is compact, and faithful for AF stable
semantics and LP stable semantics (Osorio, Zepeda, Nieves, & Cortes, 2005), and AF stable
semantics and LP supported semantics (Strass, 2013). It is size-preserving since the single
rule for each atom contains all attackers once: P(A,R) = {a  {not b | (b, a)  R} | a  A}.
5. Already for complexity reasons, we cannot expect that this translation is also faithful for the stable
semantics. And indeed, the ADF D = ({a} , {(a, a)} , {a = a  a}) has a stable model {a} while its
standard logic program P (D) = {a  {a} , a  {not a}} has no stable model. However, it holds that
st(P (D))  st(D) (Denecker et al., 2004; Strass, 2013).

202

fiExpressiveness of Two-Valued Semantics for ADFs

2.4.6 From LPs to PL
It is well-known that logic programs under supported model semantics can be translated to
propositional logic (Clark, 1978). A logic program P becomes the propositional theory P ,


_
^
^

P = {a  a | a  A} where a =
b
b for a  A.
aBP

bB +

bB 

For the stable model semantics, additional formulas have to be added, but the extended
translation works all the same (Lin & Zhao, 2004).
2.4.7 From LPs to ADFs
The Clark completion of a normal logic program directly yields an equivalent ADF over
the same signature (Brewka & Woltran, 2010). Clearly the translation is computable in
polynomial time and the blowup (with respect to the original logic program) is at most
linear. The resulting translation is faithful for the supported model semantics, which follows
from Lemma 3.16 of Strass (2013).
2.5 Representing Bipolar Boolean Functions
While bipolarity has hitherto predominantly been defined and used in the context of
ADFs (Brewka & Woltran, 2010), it is easy to define the concept for Boolean functions in
general. Let A be a set of atoms and f : 2A  {t, f } be a Boolean function. An atom a  A
is supporting iff for all M  A, f (M ) = t implies f (M  {a}) = t; we then write a  sup(f ).
An atom a  A is attacking iff for all M  A, f (M ) = f implies f (M  {a}) = f ; we then
write a  att(f ). A Boolean function f : 2A  {t, f } is semantically bipolar iff each a  A is
supporting or attacking or both. Throughout the paper, we will sometimes take a Boolean
function to be given by an interpretation set and then say that the set is bipolar.
We will now define bipolar propositional formulas for representing bipolar ADFs. This
is important not only for our study, but also since (for three-valued semantics), bipolarity is
the key to BADFs low complexity in comparison to general ADFs (Strass & Wallner, 2015).
Up to now, we usually assumed that to specify a bipolar ADF, in addition to statements,
links and acceptance conditions, the user specifies for each link whether it is supporting
or attacking (Strass & Wallner, 2015). Here we introduce an arguably simpler way, where
support and attack is represented in the syntax of the propositional formula encoding the
acceptance function.
Formally, the polarity of an atom a  A in a formula is determined by the number of
negations on the path from the root of the formula tree to the atom. The polarity is positive
if the number is even and negative if the number is odd.
Definition 4. A propositional formula  over A is syntactically bipolar if and only if no
atom a  A occurs both positively and negatively in .
Recall that we only use formulas over the basis {, , } and thus there are no hidden
negations, e.g. from material implication. For formulas in negation normal form (that is,
where negation is only applied to atomic formulas), the polarities of the atoms can be read
off the formula directly.
203

fiStrass

We will now address the question how to represent bipolar Boolean functions. Clearly all
Boolean functions can be represented by propositional formulas; we modify this construction
later and thus reproduce it here: for a Boolean function f : 2A  {t, f }, its associated
formula is
^
^
_
a
(1)
M with M =
a
f =
aM

M A,f (M )=t

aA\M

That is, each M has exactly one model M , and f enumerates those models.
So in particular, all bipolar Boolean functions can be represented by propositional formulas as well. However, this only guarantees us the existence of such representations but
gives us no way to actually obtain them. Our first fundamental result shows how we can
construct a syntactically bipolar propositional formula from a given semantically bipolar
Boolean function. The converse is straightforward, and thus the two notions of bipolarity
are closely related. For a formula , its associated Boolean function f returns t if and only
if it gets as input a model of .
Theorem 1. Let A be a set of atoms.
1. For each syntactically bipolar formula  over A, its Boolean function f is semantically
bipolar.
2. For each semantically bipolar Boolean function f : 2A  {t, f }, a syntactically bipolar
formula f with ff = f is given by
f =

_

M

with M =

M A,
f (M )=t

^
aM,
aatt(f
/
)

a

^

a

(2)

aA\M,
asup(f
/
)

Proof.
1. Obvious: every atom occurring only positively is supporting, every atom occurring only negatively is attacking.
2. Let f : 2A  {t, f } be semantically bipolar. Note first that by (2), for any M  A
we have |= M  M . It is easy to see that f is syntactically bipolar: Since f
is semantically bipolar, each a  A is: (1) attacking and not supporting, then it
occurs only negatively in f ; or (2) supporting and not attacking, then it occurs only
positively in f ; or (3) supporting and attacking, then it does not occur in f . It
remains to show that ff = f ; we show |= f  f .
|= f  f : Let v : A  {t, f } with v(f ) = t. Then there is an M  A such that
f (M ) = t and v(M ) = t. (Clearly v = vM .) By |= M  M we get v(M ) = t
and thus v(f ) = t.
|= f  f : For each model v of f , there is an M  A with f (M ) = t such that
v(M ) = t. To show that each model of f is a model of f , we show that for
all M  A with f (M ) = t, each model v of M is a model of f . Let |A| = n.
Then each M contains exactly n literals. For the corresponding M there is
a k  N with 0  k  n such that M contains exactly n  k literals. For two
204

fiExpressiveness of Two-Valued Semantics for ADFs

interpretations v1 : A  {t, f } and v2 : A  {t, f }, define the difference between
them as (v1 , v2 ) = {a  A | v1 (a) 6= v2 (a)}. (Note that for |A| = n we always
have |(v1 , v2 )|  n.) We will use induction on k to show the following: for each
M  A with f (M ) = t, each v : A  {t, f } with v(M ) = t and |(v, vM )| = k
we find that v(f ) = t. This covers all models v of f (since |(v, vM )|  |A|)
and thus establishes the claim.
k = 0: (v, vM ) =  implies v = vM whence v(f ) = vM (f ) = vM (M ) = t by
definition of M and f .
k
k + 1: Let M  A with f (M ) = t, and v : A  {t, f } with v(M ) = t and
|(v, vM )| = k + 1. Since k + 1 > 0, there is some a  (v, vM ), that is, an
a  A with v(a) 6= vM (a).
(a) a is supporting and not attacking. Then necessarily v(a) = t. (If v(a) = f ,
then vM (a) 6= v(a) implies vM (a) = t, that is, a  M whence {M } |= a
and v(M ) = f , contradiction.) Define the interpretation w : A  {t, f }
such that w(a) = f and w(c) = v(c) for c  A \ {a}. Clearly (v, w) = {a}
and |(w, vM )| = k. Hence the induction hypothesis applies to w and
w(f ) = t. Now w(a) = f , v(a) = t and w(f ) = t. Since a is supporting, also v(f ) = t.
(b) a is attacking and not supporting. Symmetric to the opposite case above.
(c) a is both supporting and attacking. Define interpretation w : A  {t, f }
such that w(a) = vM (a) and w(c) = v(c) for c  A \ {a}. It follows
that |(w, vM )| = k, whence the induction hypothesis applies to w and
w(f ) = t. Since a is both supporting and attacking (thus redundant),
we get that v(f ) = w(f ) = t.

This result paves the way for analysing the succinctness of bipolar ADFs, since now we
have a quite natural way of representing them.

3. Relative Expressiveness
We now analyse and compare the relative expressiveness of argumentation frameworks
(AFs), (bipolar) abstract dialectical frameworks ((B)ADFs), normal logic programs (LPs)
and propositional logic (PL). We first look at the different families of semantics  supported
and stable models  in isolation and afterwards combine the results for the two semantics.
For the formalisms F  {ADF, LP} that have both supported and stable semantics, we will
indicate the semantics  via a superscript as in Definition 1. For AFs we only consider the
stable semantics, as this is (to date) the only semantics for AFs where all interpretations are
guaranteed to map all arguments to either true (accepted) or false (rejected, i.e. attacked by
an accepted argument). For propositional logic PL we consider the usual model semantics.
With the syntactic translations we reviewed in the previous section, we currently have
the following expressiveness relationships. For the supported semantics,
AF e BADFsu e ADFsu 
=e LPsu e PL
and for the stable semantics,
205

fiStrass

AF e LPst <e PL and AF e BADFst e ADFst <e PL
Note that LPst <e PL and ADFst <e PL hold since sets of stable models have an antichain
property, in contrast to model sets of propositional logic.
For the succinctness relation, we have
AF s BADFsu s ADFsu s PL and LPsu s ADFsu
3.1 Supported Semantics
As depicted above, we know that expressiveness from AFs to propositional logic does not
decrease. However, it is not yet clear if any of the relationships is strict. In what follows
we will show that two of them are strict, working our way top-down from most to least
expressive.
3.1.1 ADF vs. PL
We first show that ADFs can realise any set of models by showing how a given propositional
formula can be used to construct an equivalent ADF of linear size.6
Theorem 2. PL e ADFsu and PL s ADFsu .
Proof. Let  be a propositional formula over vocabulary A. Define the ADF D over A by
setting, for all a  A,
a = a   = (a  )  (a  )
Thus ka k  O(kk), whence kD k  O(|A|  kk). It remains toVshow su(D ) = mod ().
Recall that for any ADF D over A, su(D) = mod (D ) for D = aA (a  a ). Applying
the definition of a in D yields
V
D = aA (a  (a  ))
Now for any a  A, the formula (a  (a  ))Vis equivalent to . (The proof is by case
distinction on a.) Thus D is equivalent to aA , that is, to , and it follows that
su(D ) = mod (D ) = mod ().

For example, consider the vocabulary A = {a, b} and the propositional formula  = a  b.
The canonical construction above yields ADF D with acceptance formulas a = a  (a  b)
and b = b  (a  b). Now we have:
a = a  (a  b) = (a  (a  b))  ((a  b)  a)  a  (a  b)  a  b
Intuitively, a = a  b expresses that a cannot be false, and is true if b is true. By a
symmetrical argument, the acceptance formula of b is equivalent to b  a. It is readily
checked that su(D ) = {{a, b}} as desired. Since we know from Section 2.4.2 that the
converse translation is also possible (ADFsu s PL), we get the following.
Corollary 3. PL 
=s ADFsu
6. If we consider the vocabulary A to be part of the input, the size increase is quadratic.

206

fiExpressiveness of Two-Valued Semantics for ADFs

When the acceptance conditions are written as propositional formulas, the construction to
realise X  2A in the proof of Theorem 2 defines a space-efficient equivalent of
_
_
M
a =
M 
M X,aM

M A,M X,a
/
M
/

as acceptance formula of a, where M is as in Footnote 3.
3.1.2 ADF vs. LP
Since ADFs under supported semantics can be faithfully translated into logic programs,
which can be likewise further translated to propositional logic, we have the following.
Corollary 4. ADFsu 
=e LPsu 
=e PL
However, this does not extend to the succinctness relation, as logic programs stipulate a
particular syntactic form that is essentially a fixed-depth circuit. More specifically, it is
easy to see that any language that is polynomially expressible by normal logic programs
under supported semantics is in AC0 . For the stable semantics of so-called canonical logic
programs, this has recently been shown by Shen and Zhao (2014) (Proposition 2.1). The
case we are interested in (supported semantics) works similarly, but we still present the
proof for completeness. The main technical result towards proving that is a lemma showing
how to turn a logic program into an equivalent Boolean circuit of a fixed depth.
Lemma 5. For every normal logic program P , there exists a circuit CP over the basis
{, , } such that:
1. CP accepts all and only the supported models of P ,
2. the size of CP is linear the size of P ,
3. CP has depth 4.
Proof. Let A = {a1 , . . . , an } be the vocabulary of P , and its Clark completion be P =
{ai  i | ai  A} where
V the i are DNFs over literals from A. Clearly the circuit for P
must compute CP = ai A (ai  i ) where ai  i can be replaced by (ai i )(ai i )
with i a CNF over literals from A. The construction can be depicted as follows, where
the inner layers are shown for one i only, and dotted lines represent potential edges.

i  ai

...

i


...



a1

a1

...







ai  i
...



ai

ai
207

...

...



an

an

fiStrass

Now (1) follows since su(P ) = mod (P ) and CP accepts all and only the models of P .
For (2), if P contains m = |P | rules, then m  kP k and the total number of inner gates is
bounded by n(2m + 3)  n(2  kP k + 3). (3) is clear.

While the statement of Lemma 5 is actually much stronger and gives a constant upper
bound of the resulting circuit depth for arbitrarily-sized logic programs, it readily follows
that the set of polynomially logic-program expressible languages is a subset of the languages
expressible by alternating Boolean circuits with unbounded fan-in and constant depth.
Proposition 6. If L is polynomially expressible by normal logic programs under supported
semantics, then L  AC0 .
It follows immediately that normal logic programs cannot polynomially express the
language Parity.7 This is the supported-semantics counterpart of Theorem 3.1 in (Shen
& Zhao, 2014).
Corollary 7. Parity has no polynomial size normal logic program representation.
Proof. By Proposition 6 and Parity 
/ AC0 (Jukna, 2012).



It follows that propositional logic is strictly more succinct than normal logic programs
under supported semantics.
Corollary 8. PL 6s LPsu and thus LPsu <s PL.
From our considerations since Theorem 2, it follows that if  has a small conjunctive
normal form (a conjunction of clauses) and disjunctive normal form (disjunction of monomials) representation, then there is also a small normal logic program representation for
mod ().
3.1.3 ADF vs. BADF
It is quite obvious that the canonical ADF constructed in Theorem 2 is not bipolar, since
a as well as every atom mentioned by  occurs both positively and negatively in a . This
raises the question whether the construction can be adapted to bipolar ADFs.
It turns out that the subclass of bipolar ADFs is strictly less expressive. Towards the
proof of this result we start out with a new concept: that of the conjugate of a model set
with respect to an atom. This concept will be used to characterise ADF realisability and
precisely captures the if-and-only-if part of ADFs supported model semantics: From the
translation of an ADF D into propositional logic V
(cf. Section 2.4.2) we can see that the result
is basically a conjunction of equivalences: D = aA (a  a ). While the conjunction part
will be captured by set intersection, the conjugate will capture the equivalence part.
Definition 5. Let A be a vocabulary, X  2A and a  A. The a-conjugate of X is the set
hai(X) = {M  A | M  X, a  M }  {M  A | M 
/ X, a 
/ M}
7. Logic programs under supported models are universally expressive, so they can express Parity, just not
in polynomial size.

208

fiExpressiveness of Two-Valued Semantics for ADFs

Alternatively, we could write hai(X) = {M  A | M  X  a  M }. Intuitively, hai(X)
contains all interpretations M where containment of a in M coincides exactly with containment of M in X. Formulated in terms of propositional formulas, if X is the model set of
formula  over A, then hai(X) is the model set of formula a  . Note that the vocabulary
A is implicit in the conjugate function.
Example 1. Consider the vocabulary A2 = {a, b}. The functions hai() and hbi() operate
A
on the set 22 2 of interpretation sets over A2 and are shown in Table 1.


a  b
a  b
a  b
ab
a
b
a
b
a=b
ab
ab
a  b
ab
ba
>

hai()
a
a  b
a  b
a  b
ab
>
ab

a=b
b
b
ba
a  b
ab
ab
a

hbi()
b
a  b
a  b
a  b
ba
ab
>
a=b

a
a
ab
a  b
ab
ab
b

Table 1: Conjugation functions for A2 = {a, b}. Interpretation sets are represented using
formulas over A2 , and connective = denotes exclusive disjunction XOR.
For two-valued ADF semantics, this conjugation function plays an essential semantical
role, since it provides the bridge between models of the acceptance functions and models of
the ADF. But it is also interesting in itself: We first show some properties of the conjugation
function associated to an atom, since some of them will be used in the proof later on. First
of all, it is an involution, that is, its own inverse (and thus in particular a bijection). Next,
it is compatible with the complement operation (logical negation on the formula level).
Finally, it also preserves the evenness of the cardinality of the input set.
Proposition 9. Let A be a vocabulary, X  2A and a  A.
1. hai(hai(X)) = X.

(involution)

2. 2A \ hai(X) = hai 2A \ X .


(compatible with negation)

3. |X| is even iff |hai(X)| is even.

(preserves evenness)

Proof. Let |A| = n, X  2A and a  A.
209

fiStrass

1. Let M  A. We have
M  hai(hai(X)) iff M  hai(X)  a  M
iff (M  X  a  M )  a  M
iff M  X  (a  M  a  M )
iff M  X
2. Denote
S, = {M  A | M  X, a  M }
S,/ = {M  A | M  X, a 
/ M}
S,
= {M  A | M 
/ X, a  M }
/
S,
/ X, a 
/ M}
/
/ = {M  A | M 
and observe that
2A = S, ] S,/ ] S,
/ ] S,
/
/
X = S, ] S,/
hai(X) = S, ] S,
/
/
where ] denotes disjoint union. Now

2A \ hai(X) = 2A \ S, ] S,
/
/
= S,/ ] S,
/
= {M  A | M  X, a 
/ M } ] {M  A | M 
/ X, a  M }
fi
fi

	 
	
A
= M AfiM 
/ 2 \ X, a 
/ M ] M  A fi M  2A \ X, a  M

= hai 2A \ X
3. We show that |X| + |hai(X)| is even. Firstly,
S,/ ] S,
/ M } = 2A\{a}
/
/ = {M  A | a 
fi
fi fi
fi
n1 . Thus
fi
whence fiS,/ fi + fiS,
/
/ =2
fi
fi
fi
fi
fi
|X| + |hai(X)| = |S, | + fiS,/ fi + |S, | + fiS,
/
/
fi
fi fi
fi
fi
= 2  |S, | + fiS,/ fi + fiS,
/
/
= 2  |S, | + 2n1
is even.



For our current purpose of characterising the expressiveness of bipolar ADFs, we now
use the concept of conjugation to make ADF realisability for the model semantics slightly
more accessible. We show that each ADF realisation of a model set X over an n-element
vocabulary A can equivalently be characterised by an n-tuple (Y1 , . . . , Yn ) of supersets of X
whose intersection is exactly X. The crux of the proof of this result is how the acceptance
conditions of the realising ADF and the Yi are related through the conjugation function.
210

fiExpressiveness of Two-Valued Semantics for ADFs

Proposition 10. Let A = {a1 , . . . , an } be a vocabulary and X  2A be a set of interpretations. Denote an ADF over A by the sequence (1 , . . . , n ) of its acceptance formulas (for
each i  {1, . . . , n}, formula i over A is the acceptance formula of ai ), and further define
CX = {(mod (1 ), . . . , mod (n )) | su(1 , . . . , n ) = X}
fi
(
!
)
n
fi
\
fi
YX = (Y1 , . . . , Yn ) fi Y1 , . . . , Yn  2A ,
Yi = X
fi
i=1

The sets CX and YX are in one-to-one correspondence; in particular |CX | = |YX |.
Proof. We provide a bijection between CX and YX . Consider the function
 A n
 A n
with (B1 , . . . , Bn ) 7 (ha1 i(B1 ) , . . . , han i(Bn ))
 22
f : 22
which is an involution by Proposition 9. Using the results of Section 2.4.2, we get that
(mod (1 ), . . . , mod (n ))  CX iff su(1 , . . . , n ) = X


^
iff mod 
(ai  i ) = X
1in

iff

\

mod (ai  i ) = X

1in

iff

\

hai i(mod (i )) = X

1in

iff (ha1 i(mod (1 )) , . . . , han i(mod (n )))  YX
iff f (mod (1 ), . . . , mod (n ))  YX
Thus f (CX ) = YX whence f (YX ) = f (f (CX )) = CX and f |CX : CX  YX is bijective.



This one-to-one correspondence is important since we will later analyse the precise
number of realisations of given model sets. Furthermore, this result shows the role of the
conjugation function for characterising two-valued model realisability for general ADFs. We
can now adapt this characterisation result to the case of bipolar ADFs. More precisely, we
give several necessary and sufficient conditions when a given model set is bipolarly realisable.
With this characterisation in hand, we can later show that a specific interpretation set fails
the necessary conditions and thus cannot be the model set of any BADF.
 Below, fiwe denote
	
the set of all supersets of a set X of interpretation sets over A by X  = Y  2A fi X  Y .
Proposition 11. Let A = {a1 , . . . , an } be a vocabulary and X  2A be a set of interpretations. The following are equivalent:
1. X is bipolarly realisable.
2. there exist Y1 , . . . , Yn  X  such that:
T
(a) ( ni=1 Yi ) = X, and
211

fiStrass

(b) for each 1  i  n, the set hai i(Yi ) is bipolar.
3. there exist Y1 , . . . , Yn  X  such that
T
(a) ( ni=1 Yi ) = X, and
(b) for each 1  i, j  n, at least one of :
 for all M  A, (M  Yi  ai  M )  (M  {aj }  Yi  ai  M  {aj }); or
 for all N  A, (N  Yi = ai  N )  (N  {aj }  Yi = ai  N  {aj }).
Proof. (1)  (2): If X is bipolarly realisable, then there exists a bipolar ADF D = (A, L, C)
with su(D) = X. In particular, there exist bipolar Boolean functions C1 , . . . , Cn such
that M  X if and only if for all 1  i  n we find ai  M iff Ci (M ) = t. For each
1  i  n define YT
i = hai i(Ci ). By assumption, hai i(Yi ) = hai i(hai i(Ci )) = Ci is bipolar; furthermore ( ni=1 Yi ) = X follows from the above.
(2)  (3): Let i  {1, . . . , n} and assume that hai i(Yi ) is bipolar. This means that for all
aj  A, we find that aj is supporting or attacking (or both) in hai i(Yi ). Now aj is
supporting in haj i(Yi ) iff for all M  A we find:
M  hai i(Yi )  M  {aj }  hai i(Yi ) , that is,
(M  Yi  ai  M )  (M  {aj }  Yi  ai  M  {aj })
Similarly, aj is attacking in hai i(Yi ) iff for all N  A we find:
N
/ hai i(Yi )  N  {aj } 
/ hai i(Yi ) , that is,
(N  Yi  ai  N )  (N  {aj }  Yi  ai  N  {aj })
Thus for all aj  A, we find that at least one of the following:
 for all M  A, (M  Yi  ai  M )  (M  {aj }  Yi  ai  M  {aj }); or
 for all N  A, (N  Yi = ai  N )  (N  {aj }  Yi = ai  N  {aj }).
(3)  (1): We construct an ADF D = (A, L, C) as follows: for each i  {1, . . . , n} we define Ci = hai i(Yi ) and finally set L = A  A. Each Ci is bipolar by the equivalences
established in the previous proof item, and su(D) T
= X follows from the fact that
hai i(Ci ) = hai i(hai i(Yi )) = Yi and the presumption ( ni=1 Yi ) = X.


We now apply this characterisation result to show that there is an interpretation set
over three atoms that cannot be realised by bipolar ADFs under the model semantics. This
is the smallest example in terms of the number of atoms (actually, one of the two smallest
examples)  all interpretation sets over a binary vocabulary are bipolarly realisable.
Proposition 12. For vocabulary A3 = {1, 2, 3}, there is no bipolar ADF that realises
X = Even3 = {, {1, 2} , {1, 3} , {2, 3}}.
212

fiExpressiveness of Two-Valued Semantics for ADFs

Proof. Assume to the contrary that X is bipolarly realisable. Then there exist Y1 , Y2 , Y3  X 
A
from Proposition 11. There are 2|2 ||X| = 284 = 24 = 16 candidates for each Yi , that is,
every Yi must be of the form X ] Z with
Z  {{1} , {2} , {3} , {1, 2, 3}} = 2A \ X
For eleven out of those sixteen model set candidates for each Yi , the set hii(Yi ) is not
bipolar. To show that a model set hii(Yi ) is not bipolar, we provide a statement j  A3
that is neither supporting nor attacking; we say that such a statement is dependent.
1. For Y1 = X, we get h1i(Y1 ) = {{1, 2} , {1, 3} , {2} , {3}}, which is not bipolar since
statement 2 is dependent: If 2 was supporting, then {3}  h1i(Y1 ) would imply
{2, 3}  h1i(Y1 ); if 2 was attacking, then  
/ h1i(Y1 ) would imply {2} 
/ h1i(Y1 ). For
the remaining cases, the justifications for a specific statement being dependent are
equally easy to read off the model set; for brevity we just indicate the statements.
2. For Y1 = X  {{1}}, we get h1i(Y1 ) = {{1, 2} , {1, 3} , {1} , {2} , {3}}, which is not
bipolar since statement 2 is dependent.
3. For Y1 = X  {{2}}, we get h1i(Y1 ) = {{1, 2} , {1, 3} , {3}}, which is not bipolar since
statement 2 is dependent.
4. The case Y1 = X  {{3}} is symmetric to the previous one: we get the model set
h1i(Y1 ) = {{1, 2} , {1, 3} , {2}}, which is not bipolar since statement 3 is dependent.
5. For Y1 = X  {{1, 2, 3}}, we get h1i(Y1 ) = {{1, 2, 3} , {1, 2} , {1, 3} , {2} , {3}}, which
is not bipolar since statement 2 is dependent.
6. For Y1 = X  {{1} , {2}}, we get h1i(Y1 ) = {{1, 2} , {1, 3} , {1} , {3}}, which is not
bipolar since statement 3 is dependent.
7. The case Y1 = X  {{1} , {3}} is again symmetric to the previous one.
8. For Y1 = X  {{2} , {3}}, we get h1i(Y1 ) = {{1, 2} , {1, 3}}, which is not bipolar since
statement 2 is dependent.
9. For Y1 = X  {{1} , {1, 2, 3}}, we get h1i(Y1 ) = {{1, 2, 3} , {1, 2} , {1, 3} , {1} , {2} , {3}},
which is not bipolar since statement 2 is dependent.
10. For Y1 = X  {{2} , {1, 2, 3}}, we get h1i(Y1 ) = {{1, 2, 3} , {1, 2} , {1, 3} , {3}}, which
is not bipolar since statement 2 is dependent.
11. Y1 = X  {{3} , {1, 2, 3}} is again symmetric to the previous case.
There remains a set C of five candidates (due to symmetry they are the same for each i):
C = {X ] {{1} , {2} , {3}} ,
X ] {{1} , {2} , {1, 2, 3}} ,
X ] {{1} , {3} , {1, 2, 3}} ,
X ] {{2} , {3} , {1, 2, 3}} ,
X ] {{1} , {2} , {3} , {1, 2, 3}}}
213

fiStrass

Basically, the candidates are those where at least three out of the four interpretations in
D = {{1} , {2} , {3} , {1, 2, 3}} are contained in addition to those already in X. Now clearly
by the assumption that the Yi realise X we have Y
1 , Y2 , Y3 
 C. But then there is some
T3
M  D with M  Yi for all 1  i  3 and thus M 
i=1 Yi = X. However, D  X = .
Contradiction. Thus such Yi do not exist and X is not bipolarly realisable.

As the only other interpretation set over A3 that is not bipolarly realisable, we found
the complement of Even3 above, the Parity language over three atoms.
Proposition 13. For vocabulary A3 = {1, 2, 3}, there is no bipolar ADF that realises
Parity3 = {{1} , {2} , {3} , {1, 2, 3}}.
Together with the straightforward statement of fact that Even3 can be realised by a
non-bipolar ADF, Proposition 12 leads to the next result.
Theorem 14. BADFsu <e ADFsu
Proof. Model set Even3 from Proposition 12 is realisable under model semantics by ADF
DEven3 with acceptance conditions
1 = (2 = 3),

2 = (1 = 3),

3 = (1 = 2)

However, there is no bipolar ADF realising Even3 , as is witnessed by Proposition 12.



Another consequence of our characterisation of two-valued model realisability in Proposition 10 is that we can get a precise number of distinct realisations of a given model set.
This is significant in that it further illustrates the rather intricate difficulty underlying bipolar non-realisability: we cannot necessarily use the model set Even3 above to determine a
single reason for bipolar non-realisability, that is, a single link (b, a) that is neither supporting nor attacking in all realisations. Rather, the culprit(s) might be different in each
realisation, and to show bipolar non-realisability, we have to prove that for all realisations,
there necessarily exists some reason for non-bipolarity. And the number of different ADF
realisations of a given model set X can be considerable.8
A
Proposition
fi A
fi 15. Let A be a vocabulary with |A| = n, and X  2 an interpretation set
with fi2 \ X fi = m. The number of distinct ADFs D with su(D) = X is

r(n, m) = (2n  1)m
Proof. According to Proposition
10,T each realisation
by a tufi of
fi X can be characterised
n
n such tuples.
ple (Y1 , . . . , Yn )  X  with X = ni=1 Yi . Since fiX  fi = 2m , there are (2m )T
However,Ttowards r(n, m), this wrongly counts all tuples (Y1 , . . . , Yn ) with ( ni=1 Yi ) ) X,
that is, |( ni=1 Yi ) \ X| > 0 (at least once); it remains to subtract
any i  {1, . . . , n},
n them. For T
we can overestimate the number of tuples (Y1 , . . . , Yn )  X  such that |( ni=1 Yi ) \ X|  i
by the expression
 
n
m
q(n, m, i) =
2mi
(3)
i
8. When counting ADFs over A, we do not take into account different link relations, but take L = A  A
and only count different acceptance functions, through which redundant links can be modelled.

214

fiExpressiveness of Two-Valued Semantics for ADFs


This is seen as follows: Let I  2A \ X be a fixed i-element set. (Intuitively, the interpretation-set X  I contains
i interpretations too many.)
There are mi such sets.
fi exactly
fi

n
For each such I, we have fiI fi = 2mi . Thus there are 2mi possible ways to choose n
elements (the Y1 , . . . , Yn ) out of I  . No matter how the Yj are chosen, their intersection
contains I and thus has at least i elements too many. However, all sets that have at least
i + 1 elements too many are counted twice and have to be subtracted. If we subtract
q(n, m, i + 1), then we have not counted the sets that have at least i + 2 elements too many
and have to add q(n, m, i + 2),
by the inclusion-exclusion principle, the number
n etc. Hence
T
of tuples (Y1 , . . . , Yn )  X  with ni=1 Yi = X is given by
r(n, m) = q(n, m, 0)  q(n, m, 1) + q(n, m, 2)  . . .  q(n, m, m)
m
X
=
(1)i q(n, m, i)
i=0
m
X

 
n
m
=
(1)
2mi
i
i=0


m
X m
=
(2n )mi (1)i
i
i

(by (3) above)
(reordering factors)

i=0
n

= (2  1)m

(binomial theorem) 

So the main contributing factor is the number m of interpretations that are excluded from
the desired model set X. For Proposition 12, for instance, there are (23  1)4 = 74 = 2401
ADFs with the model set Even3 . According to Theorem 14, none of them is bipolar. Obvin
ously, the maximal number of realisations is achieved by X =  whence r(n, 2n ) = (2n  1)2 .
On the other hand, the model set X = 2A has exactly one realisation, r(n, 0) = 1. Note
that the number of (syntactically distinct) realisations for the other universally expressive
formalisms, logic programs and propositional logic, is unbounded in general since we can
add an arbitrary number of tautologies.
We finally show a reduction of the problem of bipolar realisability to propositional
satisfiability. This approaches the problem from another angle (a possible implementation
deciding bipolar realisability using a SAT solver), and provides the proof of Theorem 3 by
Strass (2015b), which was not contained in that work.
For a given vocabulary A and set X  2A be a set of interpretations, it is our aim
to construct a propositional formula X that is satisfiable if and only if X is bipolarly
realisable. The propositional signature we use is the following: For each a  A and M  A,
there is a propositional variable pM
a that expresses whether Ca (M ) = t. This allows to
encode all possible acceptance conditions for the statements in A. To enforce bipolarity, we
use additional variables to model supporting and attacking links: for all a, b  A, there is
a,b
a variable pa,b
sup saying that a supports b, and a variable patt saying that a attacks b. So the
vocabulary of X is given by
fi
n
o
a,b fi
a,b
P = pM
,
p
,
p
M

A,
a

A,
b

A
fi
a
sup att
To guarantee the desired set of models, we constrain the acceptance conditions as dictated
by X: For any desired set M and statement a, the containment of a in M must correspond
215

fiStrass

exactly to whether Ca (M ) = t; this is encoded in 
X . Conversely, for any undesired set
/
M and statement a, there must not be any such correspondence, which 
X expresses. To
enforce bipolarity, we state that each link must be supporting or attacking. To model the
meaning of support and attack, we encode all ground instances of their definitions.
Definition 6. Let A be a vocabulary and X  2A be a set of interpretations. Define the
following propositional formulas:

/
BADF
= 
X
X  X  bipolar


^
^
^


pM

pM
a
a 
X =
M X

aM

aA\M




^

/

X =

_


M A,M X
/

bipolar =

^ 

_

pM
a 

aM


pM
a

aA\M



a,b
a,b
a,b
pa,b
sup  patt  sup  att



a,bA

a,b
sup

= pa,b
sup 

a,b
att

pa,b
att

^ 



(a, b  A)



(a, b  A)

M {a}

pM
b  pb

M A

=



^ 

M {a}

pb

 pM
b

M A

The corresponding result shows the reduction to be correct.
Theorem 16. Let A be a vocabulary and X  2A be a set of interpretations. X is bipolarly
realisable if and only if BADF
is satisfiable.
X
Proof. if: Let I  P be a model for X . For each a  A, we define an acceptance condition as follows: for M  A, set Ca (M ) = t iff pM
a  I. It is easy to see that bipolar
guarantees that these acceptance conditions are all bipolar. The ADF is now given
su = (A, A  A, C). It remains to show that any M  A is a model of D su if and
by DX
X
only if M  X.
su . Consider any a  A.
if: Let M  X. We have to show that M is a model of DX
M
1. a  M . Since I is a model of 
X , we have pa  I and thus by definition
Ca (M ) = t.
M / I and thus by definition
2. a  A \ M . Since I is a model of 
X , we have pa 
Ca (M ) = f .
/
only if: Let M 
/ X. Since I is a model of 
X , there is an a  M such that
Ca (M ) = f or an a 
/ M such that Ca (M ) = t. In any case, M is not a model of
su .
DX

only if: Let D be a bipolar ADF with su(D) = X. We use D to define a model I for X .
First, for M  A and a  A, set pM
a  I iff Ca (M ) = t. Since D is bipolar, each link
is supporting or attacking and for all a, b  A we can find a valuation for pa,b
sup and
pa,b
.
It
remains
to
show
that
I
is
a
model
for

.
X
att
216

fiExpressiveness of Two-Valued Semantics for ADFs

1. I is a model for 
X : Since D realises X, each M  X is a model of D and thus
for all a  A we have Ca (M ) = t iff a  M .
/
2. I is a model for 
/ X is not a model
X : Since D realises X, each M  A with M 
of D. Thus for each such M , there is an a  A witnessing that M is not a model
of D: (1) a  M and Ca (M ) = f , or (2) a 
/ M and Ca (M ) = t.

3. I is a model for bipolar : straightforward since D is bipolar by assumption.



Remarkably, the decision procedure does not only give an answer, but in the case of a
positive answer we can read off the BADF realisation from the satisfying evaluation of the
constructed formula. We illustrate the construction with an example seen earlier.
Example 2. Consider A3 = {1, 2, 3} and the model set Even3 = {, {1, 2} , {1, 3} , {2, 3}}.
The construction of Theorem 16 yields these formulas:
{1}





Even3 = p1  p2  p3 
{1,2}

 p2

{1,3}

 p2

p1
p1

{1,2}

{2,3}

p1

/

Even3 = (p1
{1,2}



(p1

{1,3}



(p1

 p3

{1,3}

 p3

{2,3}

 p3

 p2

{2,3}

{1}

 p3 ) 

{2}

 p2

{1}

{2}

 p2

 p3 ) 

{3}

 p2

{3}

 p3 ) 

{1,2,3}

(p1

{2}

{3}

{1,2,3}

 p2

{1,2,3}

 p3

)

The remaining formulas about bipolarity are independent of Even3 , we do not show them
here. We have implemented the translation of the proof of Theorem 16 and used the solver
clasp (Gebser, Kaminski, Kaufmann, Ostrowski, Schaub, & Schneider, 2011) to verify that
Even3 is unsatisfiable.
3.1.4 BADF vs. LP
Earlier, we used the language Parity to show that propositional logic is (and thus by
PL 
=s ADFsu general ADFs are) exponentially more succinct than normal logic programs
(under supported models). However, for bipolar ADFs, by Proposition 13 there is no BADF
D over A3 = {1, 2, 3} with model set su(D) = Parity3 = {{1} , {2} , {3} , {1, 2, 3}}, that is,
BADFs cannot even express Parity. Fortunately, the Majority language does the trick
in this case.
Theorem 17. BADFsu 6s LPsu
Proof. We show that the language Majority can be polynomially expressed by BADFsu ,
but not by LPsu . The latter fact follows from Majority 
/ AC0 (Jukna, 2012) and Proposition 6. We show the first part by constructing a series of BADFs Dn over An = {a1 , . . . , an }
(n  N, n  1) such that su(Dn ) = Majorityn . We use results of (Friedman, 1986; Boppana, 1986), who show that for all positive n  N and k  n, the language Thresholdn,k
has negation-free propositional formulas Threshold
of polynomial size s, where we use the
n,k

 
4.27
bound of Boppana, s  O k n log n . Define D1 by a1 = >, and for n  2 set k = n2
and for 1  i  n,
ai = ai  Threshold
(a1 , . . . , ai1 , ai+1 , . . . , an )
n1,k
217

fiStrass

Intuitively, the formula ai checks whether the remaining variables could achieve a majority
without ai . If so, then ai can be set arbitrarily; otherwise, ai must be set to true. Clearly
the Boolean function computed by ai is bipolar, since ai is supporting and all other parents
are attacking. For the size of Dn , we observe that


kDn k  O n Threshold 
n1,k

whence the overall size is polynomial. It remains to show that su(Dn ) = Majorityn .
: Let M  Majorityn . We have to show M  su(Dn ), that is, a  M iff M |= a
for all a  An . For a  M , it is immediate that M |= a , so let aj 
/ M for some
j  {1, .. . ,n}. We have to show M 6|= aj . Since M  Majorityn , we have |M | = m
for k = n2  m  n  1 and M  Thresholdn1,k , that is, we have
M |= Threshold
(a1 , . . . , aj1 , aj+1 , . . . , an )
n1,k
Together with M 6|= aj , it follows that M 6|= aj .
 
: Let M 
/ Majorityn . Then |M | = m for 0  m < n2 = k. In particular, there is
some aj  An \ M . Now m < k implies that there is no N  Thresholdn1,k with
(a1 , . . . , aj1 , aj+1 , . . . , an ) whence it follows
|N | = m = |M |. Thus M 6|= Threshold
n1,k
that M |= aj . Together with M 6|= aj we conclude that M 
/ su(Dn ).

Since every BADF is an ADF of the same size, we get:
Corollary 18. ADFsu 6s LPsu
In combination with the translation from logic programs to ADFs (implying the relation
LPsu s ADFsu ), this means that also ADFs are strictly more succinct than logic programs.
Corollary 19. LPsu <s ADFsu
3.1.5 BADF vs. AF
It is comparably easy to show that BADF models are strictly more expressive than AFs,
since sets of supported models of bipolar ADFs do not have the antichain property.
Proposition 20. AF <e BADFsu
Proof. Consider vocabulary A = {a} and BADF D = (A, {(a, a)} , {a }) with a = a. It is
straightforward to check that its model set is su(D) = {, {a}}. Since model sets of AFs
under stable extension semantics satisfy the antichain property, there is no equivalent AF
over A.

This yields the following overall relationships:
AF <e BADFsu <e ADFsu 
=e LPsu 
=e PL
For a concise overview of relative succinctness, we present the results and open problems
at a glance in Table 2 below.9
9. We remark that the three open problems in Table 2 are really only two: It is easy to show that ADFs
and propositional logic behave equivalently in relation to bipolar ADFs, since they are equally expressive
and equally succinct; that is, it holds that ADFsu s BADFsu if and only if PL s BADFsu .

218

fiExpressiveness of Two-Valued Semantics for ADFs

su

BADF
ADFsu
LPsu
PL

BADFsu
=
?
?
?

ADFsu
s
=
<s

=s

LPsu
6s
6s
=
6s

PL
s

=s
<s
=

Table 2: Relative succinctness results for (bipolar) ADFs under the model semantics, normal
logic programs under the supported semantics, and classical propositional logic. An entry 
in row F1 and column F2 means F1  F2 .
3.2 Stable Semantics
As before, we recall the current state of knowledge:
AF e BADFst e ADFst <e PL and AF e LPst <e PL
We first show that BADFs are strictly more expressive than AFs.
Proposition 21. AF <e BADFst
Proof. Consider the set X2 = {{a, b} , {a, c} , {b, c}} of desired models. Dunne et al. (2015)
proved that X2 is not realisable with stable AF semantics. However, the model set X2 is
realisable with BADF DX2 under stable semantics:
a = b  c,

b = a  c,

c = a  b

Let us exemplarily show that M = {a, b} is a stable model (the other cases are completely
symmetric): The reduct DM is characterised by the two acceptance formulas a = b  
and b = a  . We then easily find that DM (, ) = (M, ) = DM (M, ).

Intuitively, the argument for AF non-realisability of X2 is as follows: Since a and b occur
in an extension together, there can be no attack between them. The same holds for the
pairs a, c and b, c. But then the set {a, b, c} is conflict-free and thus there must be a stable
extension containing all three arguments, which is not allowed by X2 . The reason is AFs
restriction to individual attack, as set attack (also called joint or collective attack) suffices
to realise X2 as seen above.
The construction that we used in the proof above to realize X2 comes from the work
of Eiter, Fink, Puhrer, Tompits, and Woltran (2013) in logic programming, and can be
generalised to realise any non-empty model set satisfying the antichain property.
st = (A, L, C) where C for a  A
Definition 7. Let X  2A . Define the following BADF DX
a
is given by


_
^

a =
b
M X,aM

bA\M

and thus L = {(b, a) | M  X, a  M, b  A \ M }.
219

fiStrass

The next result shows that the construction indeed works.
st ) = X.
Theorem 22. Let X with  =
6 X  2A be a -antichain. We find that st(DX

Proof. Let M  A.
st )  st(D st ); we use a case distinction.
: Let M 
/ X. We show that M 
/ su(DX
X

1. There is an N  X with M ( N . Then there is an a  N \ M . Consider its
acceptance
V formula a . Since a  N and N  X, the formula a has a disjunct
a,N = bA\N b. Now M  N implies A \ N  A \ M and M is a model for
st ).
a,N . Thus M is a model for a although a 
/ M , hence M 
/ su(DX
2. For all N  X, we have M 6 N . Then X 6=  implies M 6= , so let a  M .
For each
V N  X with a  N , the acceptance formula a contains a disjunct
a,N = bA\N b. By assumption, for each N  X there is a bN  M \ N .
Clearly bN  A \ N and bN is evaluated to true by M . Hence for each N  X
with a  N , the disjunct a,N is evaluated to false by M . Thus a is false under
st ).
M and M 
/ su(DX
st ), that is: for all a  A, we find a  M
: Let M  X. We first show that M  su(DX
iff M is a model for a .
st
1. Let a  M . By
V construction, we have that a in DX contains a disjunct of the
form a,M = bA\M b. According to the interpretation M , all such b  A \ M
are false and thus a,M is true whence a is true.

2. Let a  A \ M and consider its acceptance formula a . Assume to the contrary
that M is a model for V
a . Then there is some N  X with a  N such that M
is a model for a,N = bA\N b, that is, A \ N  A \ M . Hence M  N ; and,
since a  N \ M , even M ( N , whence X is not a -antichain. Contradiction.
Thus M is no model for a .
st with respect to M . There, M contains the
Now consider the reduct DM of DX
a
M
disjunct a,M = a,M [b/ : b 
/ M ] where all b  A \ M have been replaced by false,
M =   . . .   and M is equivalent to true. Thus each a  M is true
whence a,M
a
st ).
in the least fixpoint of DM and thus M  st(DX


The restriction to non-empty model sets is immaterial for relative expressiveness, since we
can use the construction of Theorem 2 and the fact that st(D)  su(D) for any ADF D to
realize the empty model set. As the stable model semantics for ADFs and logic programs
both have the antichain property, we get:
Corollary 23. ADFst e BADFst and LPst e BADFst
This leads to the following overall relationships:
AF <e BADFst 
=e ADFst 
=e LPst <e PL
We remark that the antichain property provides a characterisation of realisability with the
stable semantics; that is, a model set is stable-realisable iff it is a -antichain.
220

fiExpressiveness of Two-Valued Semantics for ADFs

3.3 Supported vs. Stable Semantics
Now we put the supported and stable pictures together. From the proof of Theorem 22,
st of an antichain X, the supported
we can read off that for the canonical realisation DX
st ) = st(D st ) = X. With this observation, also
and stable semantics coincide, that is, su(DX
X
bipolar ADFs under the supported semantics can realize any antichain, and we have this:
Proposition 24. BADFst e BADFsu
As we have seen in Proposition 20, there are bipolar ADFs with supported-model sets
that are not antichains. We get:
Corollary 25. BADFst <e BADFsu
This result allows us to close the last gap and put together the big picture on relative
expressiveness in Figure 2 below.
ADFsu 
=e PL
=e LPsu 
BADFsu
BADFst 
=e LPst
=e ADFst 
AF
Figure 2: The expressiveness hierarchy. Expressiveness strictly increases from bottom to
top. F  denotes formalism F under semantics , where su is the supported and st
the stable model semantics; formalisms are among AFs (argumentation frameworks), ADFs
(abstract dialectical frameworks), BADFs (bipolar ADFs), LPs (normal logic programs) and
PL (propositional logic).

4. Allowing Vocabulary Expansion
Up to here, we only considered compact realisations, that do not introduce new vocabulary elements. In this section, we allow the introduction of a small number of new
atoms/arguments/statements. More precisely, small means the number is linear in the
size of the source knowledge base (representing the model set that we wish to realize in a
target language). For the purpose of realisability, the new vocabulary elements are projected
out of the resulting models.
As it turns out, adding additional arguments already makes AFs universally expressive
(under projection). More technically, we will now show that for each propositional formula
 over vocabulary A, there exists an AF F over an expanded vocabulary A  A such
that the models of  and the stable extensions of F correspond one-to-one. Roughly,
this is possible since AFs can be regarded as a syntactic variant of classical propositional
logic that has as its only connective the logical NOR  (Gabbay, 2011; Brewka et al.,
2011). Using this connective, negation is expressed by  =    and disjunction by
221

fiStrass

   = (  ) = (  )  (  ). These equivalences can be used to translate arbitrary propositional formulas (over , , ) into the syntactical -fragment; to guarantee
that the size increase is at most linear, we introduce names a for subformulas  (Tseitin,
1968). The next definition combines all of these ideas.
Definition 8. Let  be a formula using , ,  over vocabulary A. Define the sets A and
R inductively as follows:
A> = {a> }

R> = 

A = {a }

R = {(a , a )}

Ap = {p, ap } for p  A

Rp = {(p, ap ), (ap , p)} for p  A

A = {a }  A

R = {(a , a )}  R

A = {a , a , a }  A  A R = {(a , a ), (a , a )}  R  R
A = {a , a }  A  A

R = {(a , a ), (a , a ), (a , a )}  R  R

The AF associated to  is given by F = (A  A , R  {(a , a )}  R ).
The argument a> is unattacked and thus part of every stable extension (is true in every
interpretation); the argument a attacks itself and thus cannot be part of any stable extension (is false in every interpretation). The mutually attacking arguments p and ap for
p  A serve to guess a valuation of A, while a and a guarantee that only (and all)
valuations that are models of  can lead to stable extensions of F : intuitively, a must be
attacked, and the only candidate to do so is a . The arguments and attacks for the Boolean
connectives express their usual truth-theoretic semantics, as our first technical result for this
translation shows.
Lemma 26. Let  be a formula over vocabulary A and F its associated AF. For each
stable extension M of F and a , a  A , we have:
 a  M iff a 
/ M;
 a  M iff both a  M and a  M ;
 a  M iff one of a  M or a  M ;
 a  M iff neither a  M nor a  M .
Proof.
 By definition, the only attacker of an argument of the form a is the argument
a . Thus a  M iff a 
/ M.
 The only attackers of a are the arguments a and a . By the case above, we have
a  M iff a 
/ M , and a  M iff a 
/ M . Consequently, a  M and a  M iff
a 
/ M and a 
/ M iff a  M .
 The only attacker of a is the argument a . Similarly to the previous cases, we
can show that a  M iff a 
/ M and a 
/ M , and that a  M iff a 
/ M . In
combination, a  M iff a  M or a  M .
222

fiExpressiveness of Two-Valued Semantics for ADFs

 The only attackers of a are the arguments a and a . It directly follows that
a  M iff neither a  M nor a  M .

These correspondences can be used to show by induction that the newly introduced
arguments capture the semantics of the formulas they encode (for all subformulas  of ).
Lemma 27. Let  be a formula over A and F its associated AF. For each stable extension
M of F and a  A , we have a  M iff M  A is a model of .
Proof. Let M be a stable extension of F . We use structural induction on .
 = >: Trivial: a>  M since it has no attackers.
 = : Trivial: a 
/ M since the set {a } is not conflict-free.
 = p  A: Trivial: p  M iff M |= p by definition.
 = : a  M iff a  M iff a 
/ M iff M 6|=  iff M |=  iff M |= .
 =   : a  M iff a  M iff a  M and a  M iff M |=  and M |=  iff M |=   
iff M |= .
 =   : a  M iff a  M iff a  M or a  M iff M |=  or M |=  iff M |=   
iff M |= .
 =   : a  M iff a  M iff a 
/ M and a 
/ M iff M 6|=  and M 6|=  iff M |=   
iff M |= .

This lets us show the main result of this section, namely that the AF stable extension
semantics is universally expressive under projection.
Theorem 28. Let  be a formula over vocabulary A and F its associated AF.
1. For each model M  A of , there exists a stable extension E of F with M = E  A.
2. For each stable extension E of F , the set E  A is a model of .
Proof.

1. Let M  A be a model of . Define the set
E = {a | a  A , M |= }

Observe that M = E  A. By presumption, a  E. It remains to show that E is a
stable extension, that is, E is conflict-free and attacks all arguments b 
/ E.
E is conflict-free: Assume to the contrary that there is an attack r = (a, b)  R with
a, b  E. By definition, there are only these cases:
 a is arbitrary and b = . But then by definition of E we get M |= ,
contradiction.
 r = (p, ap ) or r = (ap , p) for p  A. But then by definition of E we get
M |= p and M |= p, contradiction.
223

fiStrass

 r = (a , a ). But then by definition of E we get M |=  and M |= ,
contradiction.
 r = (a , a ) or r = (a , a ). Then M |=  , and M |=  or M |= ,
contradiction.
 r = (a , a ). Then M |=   , whence M |= ( ). But also M |=  ,
contradiction.
 r = (a , a ) or r = (a , a ). Then M |=   , and M |=  or M |= .
But then also M |=   , contradiction.
E attacks all arguments not in E: Let b  (A  A  {a }) \ E be an argument. By
definition, there is a formula  such that b = a and M 6|= . We use structural
induction.
 If  =  then a  E attacks a by definition.
 If  = , then M |=  whence a  E attacks a by definition.
 If  =   , then M |=  or M |=  whence a  E or a  E. In any
case, E attacks a by definition.
 If  =   , then M |=    whence a  E attacks a by definition.
 If  =   , then M |=    whence a  E or a  E.
In any case, E attacks a by definition.
2. Let E be a stable extension of F . Since E is conflict-free, a 
/ E. Since E is stable,
E attacks a , which yields a  E. By Lemma 27, E  A is a model of .

In particular, F has no stable extension iff  is unsatisfiable. While this shows that the
construction of Definition 8 works as intended, it remains to show that the number of new
arguments is at most linear in the formula size. We can even show that the total increase
in size is only linear, thus also the number of new arguments is linear.
Proposition 29. For any formula , we find that kF k  O(kk).
Proof. We first note that
kF k = k(A  A , R  {(a , a )}  R )k
= |A  A | + |R  {(a , a )}  R |
= |A | + 1 + |R | + 2
= |A | + |R | + 3
We now use structural induction on  to show that for all formulas , we find |A |  5  kk
and |R |  4  kk. It then follows that kF k  (5 + 4)  kk + 3 = 9  kk + 3  O(kk).
 = >:
|A> | = |{a> }| = 1  5 = 5  k>k
|R> | = || = 0  4 = 4  k>k
224

fiExpressiveness of Two-Valued Semantics for ADFs

 = :
|A | = |{a }| = 1  5 = 5  kk
|R | = |{(a , a )}| = 1  4 = 4  kk
 = a  A:
|Aa | = |{a, aa }| = 2  5 = 5  kak
|Ra | = |{(a, aa ), (aa , a)}| = 2  4 = 4  kak
 = :
|A | = |A  {a }|  |A | + 1  (5  kk) + 1  5  (kk + 1) = 5  kk
|R | = |R  {(a , a )}|  |R | + 1  (4  kk) + 1  4  (kk + 1) = 4  kk
 =   :
|A |  |A | + |A | + 3  (|A | + 1) + (|A | + 1) + 3
 (5  kk + 1) + (5  kk + 1) + 3 = 5  kk + 5  kk + 5
= 5  (kk + kk + 1) = 5  k  k
|R |  |R | + |R | + 2  (|R | + 1) + (|R | + 1) + 2
 (4  kk + 1) + (4  kk + 1) + 2 = 4  kk + 4  kk + 4
= 4  (kk + kk + 1) = 4  kk
 =   :
|A |  |A | + |A | + 2  5  kk + 5  kk + 2
 5  kk + 5  kk + 5 = 5  (kk + kk + 1) = 5  kk
|R |  |R | + |R | + 3  (4  kk) + (4  kk) + 3
 4  kk + 4  kk + 4 = 4  (kk + kk + 1) = 4  kk



Hence under projection, the AF stable extension semantics can realise as much as propositional logic can. With the results of the previous section (AF e PL), this means that
allowing to introduce a linear number of new vocabulary elements (that are later projected
out), all languages considered in this paper are equally (universally) expressive.
However, we must note that equal expressiveness does not mean equal efficiency: When
we assume that a knowledge base of size n leads to a search space of size O(2n ), then a
linear increase in knowledge base size (that is, from n to c  n for some constant c) leads to
a polynomial increase in search space size (that is, from O(2n ) to O(2cn ) = O((2n )c ).
225

fiStrass

5. Discussion
We compared the expressiveness of abstract argumentation frameworks, abstract dialectical
frameworks, normal logic programs and propositional logic. We showed that expressiveness
under different semantics varies for the formalisms and obtained a neat expressiveness hierarchy. These results inform us about the capabilities of these languages to encode sets
of two-valued interpretations, and help us decide which languages to use for specific applications. Furthermore, we have seen that the results are sensitive to the vocabulary one is
permitted to use, as the hierarchy collapses when we allow to introduce even only a linear
number of new atoms.
Concerning succinctness, we have shown that ADFs (under model semantics) are exponentially more succinct than normal logic programs (under supported model semantics),
and that even bipolar ADFs (under model semantics)  although being less expressive 
can succinctly express some model sets where equivalent normal logic programs (under supported model semantics) over the same vocabulary must necessarily blow up exponentially
in size. It is open whether the converse direction also holds, that is, whether BADFs are
exponentially more succinct than logic programs (if LPsu s BADFsu ) or the two are just
mutually incomparable in terms of succinctness (if LPsu 6s BADFsu ). For the stable semantics, relative succinctness of logic programs and BADFs is completely open, partly due
to the technical aspect that the two stable semantics are conceptually different, as ADFs
in fact employ ultimate stable models (Denecker et al., 2004; Brewka et al., 2013; Strass
& Wallner, 2015). Furthermore, for general ADFs, the computational complexity of the
model existence problem of stable semantics is higher than for normal logic programs,10 so
a succinctness comparison with regard to stable models would be of limited significance.
It is easy to see that AFs have a somewhat special role as they are representationally
succinct in any case: for a vocabulary An , there is syntactically no possibility to specify a knowledge base (an AF) of exponential size, since the largest AF over An has size
k(An , An  An )k = n + n2 and is thus polynomially large. So anything that can be expressed with an AF can be expressed in reasonable space by definition. However, this
strength of AFs should be taken with a grain of salt, since they are comparably inexpressive. This can (in addition to the results we presented) already be seen from a simple
counting argument: even if all syntactically different AFs over An were semantically differ2
ent (which they are not), they could express at most 2n different model sets, which is  for
n
increasing n  negligible in relation to the 22 possible model sets over An .
In their original paper, Gogic et al. (1995) also used a relaxed version of succinctness,
where they allowed to introduce a linear number of new variables. It follows from our results
in Section 4 that all formalisms we consider here are equally succinct under this relaxed
notion.
Parts of the expressiveness results for normal logic programs carry over to further LP
classes. For example, canonical logic programs provide a limited form of nesting by allowing
literals of the form not not a in rule bodies (Lifschitz et al., 1999). This makes it quite easy
to see how normal logic programs under supported semantics can be translated to equivalent
canonical logic programs, namely by replacing each positive body atom a by not not a in
10. P
2 -hard for ADFs (Strass & Wallner, 2015) as opposed to in NP for normal LPs (Bidoit & Froidevaux,
1991; Marek & Truszczynski, 1991).

226

fiExpressiveness of Two-Valued Semantics for ADFs

all rule bodies. Recently, Shen and Zhao (2014) showed that canonical logic programs and
propositional logic programs are succinctly incomparable (under an assumption11 ), and also
provide interesting avenues for further succinctness studies. We can also add succinctness
questions of our own: firstly that of comparing disjunctive logic programs under stable
models with general ADFs under stable models, since the two have an equally complex
(P2 -complete) model existence problem (Eiter & Gottlob, 1995; Brewka et al., 2013). What
is more, there have been alternative proposals for stable model semantics for ADFs:
 ours (Strass, 2013) (Definition 3.2, later called approximate stable models by Strass
& Wallner, 2015), for which model existence is NP-complete (Strass & Wallner, 2015)
and thus potentially easier than that of the stable models of Brewka et al. (2013)
(called ultimate stable models by Strass & Wallner, 2015);
 the grounded model semantics by Bogaerts, Vennekens, and Denecker (2015) (Definition 6.8), whose model existence problem is also P2 -complete (Bogaerts et al.,
2015);
 the F-stable model semantics by Alviano and Faber (2015) (Definition 10).
It follows from Theorem 5.9 of Bogaerts et al. (2015) that grounded models and F-stable
models coincide. Still, they are demonstrably different from both approximate and ultimate stable models for ADFs (Alviano & Faber, 2015),12 and their relative succinctness in
comparison to normal/disjunctive logic programs is unanalysed.
There is more potential for further work. First of all, a nice characterisation of bipolar
ADF realisability is still missing; we are unsure whether much improvement over Proposition 11 is possible. Incidentally, for AFs the exact characterisation of compact stable extension realisability constitutes a major open problem (Dunne et al., 2015; Baumann et al.,
2014). Second, there are further semantics for abstract dialectical frameworks whose expressiveness could be studied; Dunne et al. (2015) and Dyrkolbotn (2014) already analyse many
of them for argumentation frameworks. This work is thus only a start and the same can
be done for the remaining semantics. For example the admissible, complete and preferred
semantics are all defined for AFs, (B)ADFs and LPs (Strass, 2013; Brewka et al., 2013),
and Puhrer (2015) has already made a huge step into that direction by characterising realisability. Third, there are further formalisms in abstract argumentation (Brewka et al., 2014)
whose expressiveness is by and large unexplored to the best of our knowledge. Finally, the
representational succinctness of the subclass of bipolar ADFs (using bipolar propositional
formulas to represent them) under supported model semantics is mostly open (cf. Table 2),
with some evidence pointing toward meaningful capabilities.

Acknowledgements
This paper combines, extends and improves results of our previous work (Strass, 2014,
2015b, 2015c). We wish to thank Stefan Woltran for providing a useful pointer to related
1
11. P 6 NC/poly
, the Boolean circuit equivalent of the assumption NP 6 P.
12. In the terminology of Alviano and Faber (2015), approximate stable models (Strass, 2013) are called
S-stable models and ultimate stable models (Brewka et al., 2013) are called B-stable models. Both are
shown to be different from F-stable models.

227

fiStrass

work on realisability in logic programming, Bart Bogaerts for pointing out that grounded
models and F-stable models are the same, Jorg Puhrer for several suggestions for improvement of the manuscript, and Frank Loebe for helpful discussions. This research was partially
supported by Deutsche Forschungsgemeinschaft (DFG, project BR 1817/7-1).

References
Al-Abdulkarim, L., Atkinson, K., & Bench-Capon, T. J. M. (2014). Abstract dialectical
frameworks for legal reasoning. In Hoekstra, R. (Ed.), Proceedings of the TwentySeventh Annual Conference on Legal Knowledge and Information Systems (JURIX),
Vol. 271 of Frontiers in Artificial Intelligence and Applications, pp. 6170. IOS Press.
Al-Abdulkarim, L., Atkinson, K., & Bench-Capon, T. J. M. (2015). Evaluating an approach
to reasoning with cases using abstract dialectical frameworks. In Proceedings of the
Fifteenth International Conference on Artificial Intelligence and Law (ICAIL).
Alviano, M., & Faber, W. (2015). Stable model semantics of abstract dialectical frameworks revisited: A logic programming perspective. In Yang, Q., & Wooldridge, M.
(Eds.), Proceedings of the Twenty-Fourth International Joint Conference on Artificial
Intelligence (IJCAI), pp. 26842690, Buenos Aires, Argentina. IJCAI/AAAI.
Arora, S., & Barak, B. (2009). Computational Complexity: A Modern Approach. Cambridge
University Press.
Baumann, R., Dvorak, W., Linsbichler, T., Strass, H., & Woltran, S. (2014). Compact
argumentation frameworks. In Proceedings of the Twenty-First European Conference
on Artificial Intelligence (ECAI), pp. 6974, Prague, Czech Republic.
Bidoit, N., & Froidevaux, C. (1991). Negation by default and unstratifiable logic programs.
Theoretical Computer Science, 78 (1), 85112.
Bogaerts, B., Vennekens, J., & Denecker, M. (2015). Grounded fixpoints and their applications in knowledge representation. Artificial Intelligence, 224, 5171.
Boppana, R. B. (1986). Threshold functions and bounded depth monotone circuits. Journal
of Computer and System Sciences, 32 (2), 222229.
Brewka, G., Dunne, P. E., & Woltran, S. (2011). Relating the semantics of abstract dialectical frameworks and standard AFs. In Proceedings of the Twenty-Second International
Joint Conference on Artificial Intelligence (IJCAI), pp. 780785. IJCAI/AAAI.
Brewka, G., Ellmauthaler, S., Strass, H., Wallner, J. P., & Woltran, S. (2013). Abstract
dialectical frameworks revisited. In Proceedings of the Twenty-Third International
Joint Conference on Artificial Intelligence (IJCAI), pp. 803809. IJCAI/AAAI.
Brewka, G., & Gordon, T. F. (2010). Carneades and abstract dialectical frameworks: A reconstruction. In Proceedings of the Third International Conference on Computational
Models of Argument (COMMA), Vol. 216 of FAIA, pp. 312. IOS Press.
Brewka, G., Polberg, S., & Woltran, S. (2014). Generalizations of Dung frameworks and
their role in formal argumentation. IEEE Intelligent Systems, 29 (1), 3038. Special
Issue on Representation and Reasoning.
228

fiExpressiveness of Two-Valued Semantics for ADFs

Brewka, G., & Woltran, S. (2010). Abstract dialectical frameworks. In Proceedings of the
Twelfth International Conference on the Principles of Knowledge Representation and
Reasoning (KR), pp. 102111.
Clark, K. L. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and
Data Bases, pp. 293322. Plenum Press.
Coste-Marquis, S., Konieczny, S., Mailly, J.-G., & Marquis, P. (2014). On the revision of
argumentation systems: Minimal change of arguments statuses. In Proceedings of the
Fourteenth International Conference on Principles of Knowledge Representation and
Reasoning (KR), pp. 5261.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial
Intelligence Research, 17, 229264.
Denecker, M., Marek, V. W., & Truszczynski, M. (2004). Ultimate approximation and
its application in nonmonotonic knowledge representation systems. Information and
Computation, 192 (1), 84121.
Dimopoulos, Y., Nebel, B., & Toni, F. (2002). On the computational complexity
of assumption-based argumentation for default reasoning. Artificial Intelligence,
141 (1/2), 5778.
Dung, P. M. (1995). On the Acceptability of Arguments and its Fundamental Role in
Nonmonotonic Reasoning, Logic Programming and n-Person Games. Artificial Intelligence, 77, 321358.
Dunne, P. E., Dvorak, W., Linsbichler, T., & Woltran, S. (2014). Characteristics of multiple
viewpoints in abstract argumentation. In Proceedings of the Fourteenth International
Conference on the Principles of Knowledge Representation and Reasoning (KR), pp.
7281, Vienna, Austria.
Dunne, P. E., Dvorak, W., Linsbichler, T., & Woltran, S. (2015). Characteristics of multiple
viewpoints in abstract argumentation. Artificial Intelligence, 228, 153178.
Dyrkolbotn, S. K. (2014). How to argue for anything: Enforcing arbitrary sets of labellings
using AFs. In Proceedings of the Fourteenth International Conference on the Principles
of Knowledge Representation and Reasoning (KR), pp. 626629, Vienna, Austria.
Eiter, T., Fink, M., Puhrer, J., Tompits, H., & Woltran, S. (2013). Model-based recasting in
answer-set programming. Journal of Applied Non-Classical Logics, 23 (12), 75104.
Eiter, T., & Gottlob, G. (1995). On the computational cost of disjunctive logic programming:
Propositional case. Annals of Mathematics and Artificial Intelligence, 15 (34), 289
323.
French, T., van der Hoek, W., Iliev, P., & Kooi, B. (2013). On the succinctness of some
modal logics. Artificial Intelligence, 197, 5685.
Friedman, J. (1986). Constructing O(n log n) size monotone formulae for the k-th elementary symmetric polynomial of n Boolean variables. SIAM Journal on Computing, 15,
641654.
Gabbay, D. M. (2011). Dungs argumentation is essentially equivalent to classical propositional logic with the Peirce-Quine dagger. Logica Universalis, 5 (2), 255318.
229

fiStrass

Gaggl, S. A., & Strass, H. (2014). Decomposing Abstract Dialectical Frameworks. In Parsons, S., Oren, N., & Reed, C. (Eds.), Proceedings of the Fifth International Conference
on Computational Models of Argument (COMMA), Vol. 266 of FAIA, pp. 281292.
IOS Press.
Gaggl, S. A., Rudolph, S., & Strass, H. (2015). On the computational complexity of naivebased semantics for abstract dialectical frameworks. In Yang, Q., & Wooldridge, M.
(Eds.), Proceedings of the Twenty-Fourth International Joint Conference on Artificial
Intelligence (IJCAI), pp. 29852991, Buenos Aires, Argentina. IJCAI/AAAI.
Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Schneider, M.
(2011). Potassco: The Potsdam Answer Set Solving Collection. AI Communications,
24 (2), 105124. Available at http://potassco.sourceforge.net.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming.
In Proceedings of the International Conference on Logic Programming (ICLP), pp.
10701080. The MIT Press.
Gogic, G., Kautz, H., Papadimitriou, C., & Selman, B. (1995). The comparative linguistics
of knowledge representation. In Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence (IJCAI), pp. 862869. Morgan Kaufmann.
Jukna, S. (2012). Boolean Function Complexity: Advances and Frontiers, Vol. 27 of Algorithms and Combinatorics. Springer.
Lifschitz, V., & Razborov, A. (2006). Why are there so many loop formulas?. ACM Transactions on Computational Logic, 7 (2), 261268.
Lifschitz, V., Tang, L. R., & Turner, H. (1999). Nested expressions in logic programs. Annals
of Mathematics and Artificial Intelligence, 25 (34), 369389.
Lin, F., & Zhao, Y. (2004). ASSAT: Computing answer sets of a logic program by SAT
solvers. Artificial Intelligence, 157 (1-2), 115137.
Linsbichler, T. (2014). Splitting abstract dialectical frameworks. In Parsons, S., Oren, N., &
Reed, C. (Eds.), Proceedings of the Fifth International Conference on Computational
Models of Argument (COMMA), Vol. 266 of FAIA, pp. 357368. IOS Press.
Marek, V. W., & Truszczynski, M. (1991). Autoepistemic logic. Journal of the ACM, 38 (3),
587618.
Osorio, M., Zepeda, C., Nieves, J. C., & Cortes, U. (2005). Inferring acceptable arguments
with answer set programming. In Proceedings of the Sixth Mexican International
Conference on Computer Science (ENC), pp. 198205.
Polberg, S. (2014). Extension-based semantics of abstract dialectical frameworks. In Endriss,
U., & Leite, J. (Eds.), Proceedings of the Seventh European Starting AI Researcher
Symposium (STAIRS), Vol. 264 of FAIA, pp. 240249. IOS Press.
Polberg, S., Wallner, J. P., & Woltran, S. (2013). Admissibility in the abstract dialectical
framework. In Leite, J., Son, T. C., Torroni, P., van der Torre, L., & Woltran, S.
(Eds.), Proceedings of the Fourteenth International Workshop on Computational Logic
in Multi-Agent Systems (CLIMA XIV), Vol. 8143 of LNAI, pp. 102118. Springer.
230

fiExpressiveness of Two-Valued Semantics for ADFs

Puhrer, J. (2015). Realizability of three-valued semantics for abstract dialectical frameworks. In Yang, Q., & Wooldridge, M. (Eds.), Proceedings of the Twenty-Fourth
International Joint Conference on Artificial Intelligence (IJCAI), pp. 31713177. IJCAI/AAAI, Buenos Aires, Argentina.
Shen, Y., & Zhao, X. (2014). Canonical logic programs are succinctly incomparable with
propositional formulas. In Proceedings of the Fourteenth International Conference
on the Principles of Knowledge Representation and Reasoning (KR), pp. 665668,
Vienna, Austria.
Strass, H. (2013). Approximating operators and semantics for abstract dialectical frameworks. Artificial Intelligence, 205, 3970.
Strass, H. (2014). On the relative expressiveness of argumentation frameworks, normal
logic programs and abstract dialectical frameworks. In Konieczny, S., & Tompits,
H. (Eds.), Proceedings of the Fifteenth International Workshop on Non-Monotonic
Reasoning (NMR).
Strass, H. (2015a). Instantiating rule-based defeasible theories in abstract dialectical frameworks and beyond. Journal of Logic and Computation, Advance Access published 11
February 2015, http://dx.doi.org/10.1093/logcom/exv004.
Strass, H. (2015b). The relative expressiveness of abstract argumentation and logic programming. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence
(AAAI), pp. 16251631, Austin, TX, USA.
Strass, H. (2015c). Representational succinctness of abstract dialectical frameworks. In
Black, E., Modgil, S., & Oren, N. (Eds.), Proceedings of the Third International Workshop on Theory and Applications of Formal Argumentation (TAFA).
Strass, H., & Wallner, J. P. (2015). Analyzing the computational complexity of abstract
dialectical frameworks via approximation fixpoint theory. Artificial Intelligence, 226,
3474.
Tseitin, G. S. (1968). On the complexity of derivations in the propositional calculus. Structures in Constructive Mathematics and Mathematical Logic, Part II, Seminars in
Mathematics (translated from Russian), 115125.

231

fiJournal of Artificial Intelligence Research 54 (2015) 471492

Submitted 05/15; published 11/15

Weighted Regret-Based Likelihood: A New Approach to
Describing Uncertainty
Joseph Y. Halpern

halpern@cs.cornell.edu

Computer Science Department
Cornell University
Ithaca, NY 14853, USA

Abstract
Recently, Halpern and Leung suggested representing uncertainty by a set of weighted
probability measures, and suggested a way of making decisions based on this representation
of uncertainty: maximizing weighted regret. Their paper does not answer an apparently
simpler question: what it means, according to this representation of uncertainty, for an
event E to be more likely than an event E 0 . In this paper, a notion of comparative
likelihood when uncertainty is represented by a set of weighted probability measures is
defined. It generalizes the ordering defined by probability (and by lower probability) in
a natural way; a generalization of upper probability can also be defined. A complete
axiomatic characterization of this notion of regret-based likelihood is given.

1. Introduction
Recently, Samantha Leung and I (Halpern & Leung, 2012) suggested representing uncertainty by a set of weighted probability measures, and suggested a way of making decisions
based on this representation of uncertainty: maximizing weighted regret. However, we did
not answer an apparently simpler question: given this representation of uncertainty, what
does it mean for an event E to be more likely than an event E 0 ? This is what I do in this
paper. To explain the issues, I start by reviewing the Halpern-Leung approach.
It has frequently been observed that there are many situations where an agents uncertainty is not adequately described by a single probability measure. Specifically, a single
measure may not be adequate for representing an agents ignorance. For example, there
seems to be a big difference between a coin known to be fair and a coin whose bias an agent
does not know, yet if the agent were to use a single measure to represent her uncertainty,
in both of these cases it would seem that the measure that assigns heads probability 1/2
would be used.
One approach that has been suggested for representing ignorance is to use a set P of
probability measures. This idea is an old one, apparently going back to the work of Boole
(1854, ch. 1621) and Ostrogradsky (1838); some authors (e.g., Campos & Moral, 1995;
Couso, Moral, & Walley, 1999; Gilboa & Schmeidler, 1993; Levi, 1985; Walley, 1991) have
additionally required the set P to be convex (so that if 1 and 2 are in P, then so is
a1 + b2 , where a, b  [0, 1] and a + b = 1). This approach has the benefit of representing
uncertainty in general, not by a single number, but by a range of numbers. This allows us
to distinguish the certainty that a coin is fair (in which case the uncertainty of heads is
represented by a single number, 1/2) from knowing only that the probability of heads could
be anywhere between, say, 1/3 and 2/3.
c
2015
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHalpern

But this approach also has its problems. For example, consider an agent who believes
that a coin may have a slight bias. Thus, although it is unlikely to be completely fair, it
is close to fair. How should we represent this with a set of probability measures? Suppose
that the agent is quite sure that the bias is between 1/3 and 2/3. We could, of course, take
P to consist of all the measures that give heads probability between 1/3 and 2/3. But how
does the agent know that the possible biases are exactly between 1/3 and 2/3. Does she
not consider 2/3 +  possible for some small ? And even if she is confident that the bias is
between 1/3 and 2/3, this representation cannot take into account the possibility that she
views biases closer to 1/2 as more likely than biases further from 1/2.
There is also a second well-known concern: learning. Suppose that the agent initially
considers possible all the measures that gives heads probability between 1/3 and 2/3. She
then starts tossing the coin, and sees that, of the first 20 tosses, 12 are heads. It seems that
the agent should then consider a bias of greater than 1/2 more likely than a bias of less than
1/2. But if we use the standard approach to updating with sets of probability measures
(Halpern, 2003), and condition each of the measures on the observation, since the coin
tosses are viewed as independent, the agent will continue to believe that the probability of
the next coin toss is between 1/3 and 2/3. The observation has no impact as far as learning
to predict better. The set P stays the same, no matter what observation is made.
There is a well-known solution to these problems: putting a measure of uncertainty on
these probability measures in P. This idea too has a long history. One special case is to put
a second-order probability on these probability measures; see (Good, 1980) for discussion
of this approach and further references. For example, an agent can express the fact that
the bias of a coin is more likely to be close to 1/2 than far from 1/2. In addition, the
problem of learning can be dealt with by straightforward conditioning. But this approach
leads to other problems. Essentially, it seems that the ambiguity that an agent might feel
about the outcome of the coin toss seems to have disappeared. For example, suppose that
the agent has no idea what the bias is. The obvious second-order probability to use is the
uniform probability on possible biases. While we cannot talk about the probability that the
coin is heads (there is a set of probabilities, after all, not a single probability), the expected
probability of heads is 1/2. Why should an agent that has no idea of the bias of the coin
know or believe that the expected probability of heads is 1/2? Of course, if one had to use
a single probability measure to describe uncertainty, symmetry considerations dictate that
it should be the one that ascribes equal likelihood to heads and tails; similarly, if one had
to put a single second-order probability on the set of possible biases, uniform probability
seems like the most obvious choice. Moreover, if our interest is in making decisions, then
maximizing the expected utility using the expected probability again does not take the
agents ignorance into account. Kyburg (1988) and Pearl (1987) have even argued that
there is no need for a second-order probability on probabilities; whatever can be done with
a second-order probability can already be done with a basic probability.
Nevertheless, when it comes to decision-making, it does seem useful to use an approach
that represents ambiguity, while still maintaining some of the features of having a secondorder probability on probabilities. This idea goes back to at least Gardenfors and Sahlin
(1982, 1983). Walley (1997) suggested putting a possibility measure (Dubois & Prade, 1998;
Zadeh, 1978) on probability measures; this was also essentially done by Cattaneo (2007),
Chateauneuf and Faro (2009), and de Cooman (2005). All of these authors and others, such
472

fiWeighted Regret-Based Likelihood

as Klibanoff et al. (2005), Maccheroni et al. (2006), and Nau (1992), proposed approaches
to decision making using their representations of uncertainty.
Leung and I similarly suggested putting weights on each probability measure in P. Since
we assumed that the weights are normalized so that the supremum of the weights is 1, these
weights can also be viewed as a possibility measure. If the set P is finite, we can also
normalize so as to view the weights as being second-order probabilities. As with secondorder probabilities, the weights can vary over time, as more information is acquired. For
example, we can start with a state of complete ignorance (modeled by assuming that all
probability measures have weight 1), and update the weights after making an observation
ob, we take the weight of a measure Pr to be the relative likelihood of ob if Pr were the
true measure. (See Section 2 for details.) With this approach, called likelihood updating by
Halpern and Leung (2012), if there is a true underlying measure generating the data, over
time, the weight of the true measure approaches 1, while the weight of all other measures
approaches 0. Thus, this approach allows learning in a natural way. If, for example, the
actual bias of the coin was 5/8 in the example above, no matter what the initial weights, as
long as 5/8 had positive weight, then its weight would almost surely converge to 1 as more
observations were made, while the weight of all other measures would approach 0. This,
of course, is exactly what would happen if we had a second-order probability on P. The
weights can also be used to represent the fact that some probabilities in the set P are more
likely than others.
Like essentially all others who considered a representation of uncertainty based on a set
of probability with weights, Leung and I also suggested a way of using this representation
to make decisions. However, our approach was different than those suggested earlier. We
based our approach on regret, a standard approach to decision-making that was introduced
(independently) by Niehans (1948) and Savage (1951). If uncertainty is represented by
a set P of probability measures, then regret works as follows: for each act a and each
measure Pr  P, we can compute the expected regret of a with respect to Pr; this is
the difference between the expected utility of a and the expected utility of the act that
gives the highest expected utility with respect to Pr. We can then associate with an act
a its worst-case expected regret of a, over all measures Pr  P, and compare acts with
respect to their worst-case expected regret. With weights in the picture, we modify the
procedure by multiplying the expected regret associated with measure Pr by the weight of
Pr, and compare acts according to their worst-case weighted expected regret. This approach
to making decisions is very different from the others mentioned above that incorporate a
likelihood on probabilities. Moreover, using the weights in the way means that we cannot
simply replace a set of weighted probability measures by a single probability measure; the
objections of Kyburg (1988) and Pearl (1987) do not apply.
Leung and I (Halpern & Leung, 2012) show that this approach seems to do reasonable
things in a number of examples of interest, and provide an axiomatization of decision-making
with this approach. Since sets of weighted probabilities are certainly intended to be a way
of representing uncertainty, it seems natural to ask whether they can be used to represent
relative likelihood in a direct way. Surprisingly, this is something largely not considered in
earlier papers using sets of weighted probabilities, since their focus was on decision-making
(although the work of Nau discussed in Section 3 is an exception).
473

fiHalpern

Representing relative likelihood is straightforward if uncertainty is represented by a
single probability measure: E is more likely than E 0 exactly if the probability of E is greater
than the probability of E 0 . When using sets of probability measures, various approaches
have been considered in the literature. The most common takes E to be more likely than
E 0 if the lower probability of E is greater than the lower probability of E 0 , where the lower
probability of E is its worst-case probability, taken over the measures in P (see Section 3).
We could also compare E and E 0 with respect to their upper probabilities (the best-case
probability with respect to the measures in P). Another possibility is to take E to be
more likely than E 0 if Pr(E)  Pr(E 0 ) for all measures Pr  P; this gives a partial order
on likelihood.1 But what should we do if uncertainty is represented by a set of weighted
probability measures?
In this paper, I define a notion of relative likelihood when uncertainty is represented
by a set of weighted probability measures that generalizes the ordering defined by lower
probability in a natural way; I also define a generalization of upper probability. We can then
associate with an event E two numbers that are analogues of lower and upper probability. If
uncertainty is represented by a single measure, then these two numbers coincide; in general,
they do not. The interval can be thought of as representing the degree of ambiguity in
the likelihood of E. Indeed, in the special case when all the weights are 1, the numbers
are essentially just the lower and upper probability (technically, they are 1 minus the lower
and upper probability, respectively). Interestingly, the approach to assigning likelihood is
based on the approach to decision-making. Essentially, what I am doing is the analogue of
defining probability in terms of expected utility, rather than the other way around. The
approach can be viewed as generalizing both probability and lower probability, while at
the same time allowing a natural approach to updating.
Why we should be interested in such a representation? If all that we ever did with probability was to use it to make decisions, then arguably this wouldnt be of much interest; my
work with Leung already shows how sets of weighted probabilities can be used in decisionmaking. The results of this paper add nothing further to that question. However, we often
talk about the likelihood of events quite independent of their use in decision-making. There
are clearly many examples in physics. The issue arises in AI applications as well: a typical
explanation of why we did A rather than B is that we thought some event E was more
likely than F . And computations of expectation, which clearly involve a representation of
uncertainty, arise in many AI applications. Thus, having an analogue of probability seems
important and useful in its own right.
The rest of this paper is organized as follows. After reviewing the relevant material
from (Halpern & Leung, 2012) in Section 2, I define regret-based likelihood in Section 3,
and compare it to lower probability. I provide an axiomatic characterization of regret-based
likelihood in Section 4, and show how it relates to the axiomatic characterization of lower
probability. I conclude in Section 5.

1. There is a long tradition of considering partially ordered notions of likelihood; see (Halpern, 1997) and
the references therein, and the work of Walley (1991).

474

fiWeighted Regret-Based Likelihood

2. Weighted Expected Regret: A Review
Consider the standard setup in decision theory. We have a state space S and an outcome
space O. An act is a function from S to O; it describes an outcome for each state. Suppose
that we have a utility function u on outcomes and a set P + of weighted probability measures.
That is, P + consists of pairs (Pr, Pr ), where Pr is a weight in [0, 1] and Pr is a probability
on S. Let P = {Pr : ((Pr, )  P + )}. For each Pr  P there is assumed to be
exactly one , denoted Pr , such that (Pr, )  P + . It is further assumed that weights
have been normalized so that there is at least one measure Pr  P such that Pr = 1.
Finally, P + is assumed to be weakly closed, so that if (Prn , n )  Pr+ for n = 1, 2, 3, . . .,
(Prn , n )  (Pr, Pr ), and Pr > 0, then (Pr, Pr )  P + . (I discuss below why I require
P + to be just weakly closed, rather than closed.)
The assumption that at least one probability measure has a weight of 1 is convenient
for comparison to other approaches; see below. However, making this assumption has no
impact on the results of this paper; as long as we restrict to sets where the weight is bounded,
all the results hold without change. This assumption is, of course, incompatible with the
weights being probabilities. Note that the assumption that the weights are probabilities
runs into difficulties if we have an infinite number of measures in P; for example, if P
includes all measures on heads from 1/3 to 2/3, as discussed in the Introduction, using a
uniform probability, we would be forced to assign each individual probability measure a
weight of 0, which would not work well for our later definitions.
Where are the weights in P + coming from? In general, they can be viewed as subjective,
just like the probability measures. However, as Leung and I (Halpern & Leung, 2012)
observed, there is an important special case where the weights can be given a natural
interpretation. Suppose that, as in the case of the biased coin in the Introduction, we make
observations in a situation where the probability of making a given observation is determined
by some objective source. Then we can start by giving all probability measures a weight of 1.
Given an observation ob (e.g., sequence of coin tosses in the example in the Introduction),
we can compute Pr(ob) for each measure Pr  P; we can then update the weight of Pr
to be Pr(ob)/ supPr0 P Pr0 (ob). Thus, the more likely the observation is according to Pr,
the higher the updated weight of Pr relative to other probability measures in P.2 (The
denominator is just a normalization to ensure that some measure has weight 1.) With this
approach to updating, if there is a true underlying measure generating the data, then as an
agent makes more observations, almost surely, the weight of the true measure approaches
1, while the weight of all other measures approaches 0.3 In addition, this approach gives
an agent a natural way of determining weights for each probability measure in P. While,
in general, this means that the agent may need to carry around a lot of information (not
2. The idea of putting a possibility on probabilities in P that is determined by likelihood also appears in the
work of Moral (1992), although he does not consider a general approach to dealing with sets of weighted
probability measures.
3. The almost surely is due to the fact that, with probability approaching 0, as more and more observations are made, it is possible that an agent will make misleading observations that are not representative
of the true measure. This also depends on the set of possible observations being rich enough to allow
the agent to ultimately discover the true measure generating the observations; for example, an agent will
never learn the distributions of outcomes of a die she never gets to observe the die when it lands 5 or 6.
Since learning is not a focus of this paper, I do not make this notion of rich enough precise here.

475

fiHalpern

only a possibly infinite set of probabilities, but a weight associated with each one), if the
set P has a reasonable parametric representation, then the weight can often be evaluated
in terms of the parameters, so should admit a compact representation (see Example 3.2).
The weight associated with a probability Pr can be viewed as an upper bound on an
agents confidence that Pr actually describes the situation. That is why an agent who has
no idea of what is going on is modeled as starting by placing weight 1 on all probability
measures. I believe that having the weights will allow agents to express nuances that they
consider important, and that such weights will not be hard to elicit. Whether this is the
case is really an empirical question, one which I believe deserves further exploration, but is
beyond the scope of this paper.
I now review the definition of weighted regret, and introduce the notion of absolute
(weighted) regret. I start with regret. The regret of an act a in a state s  S is the
difference between the utility of the best act at state s and the utility of a at s. Typically,
the act a is not compared to all acts, but to the acts in a set M , called a menu. Thus, the
regret of a in state s relative to menu M , denoted reg M (a, s), is supa0 M u(a0 (s))  u(a(s)).4
There are typically some constraints put on M to ensure that supa0 M u(a0 (s)) is finitethis
is certainly the case if M is finite, or the convex closure of a finite set of acts, or if there is a
best possible outcome in the outcome space O. The latter assumption holds in this paper,
so I assume throughout that supa0 M u(a0 (s)) is finite.
For simplicity, I assume that the state space S is finite. Given a probability measure
Pr on S, the expected regret of an act a with respect to Pr relative to menu M is just
P
M
reg M
sS reg (a, s) Pr(s). The (expected) regret of a with respect to P and a menu
Pr (a) =
M is just the worst-case regret, that is,
M
reg M
P (a) = sup reg Pr (a).
PrP

Similarly, the weighted (expected) regret of a with respect to P + and a menu M is just the
worst-case weighted regret, that is,
M
wr M
P + (a) = sup Pr reg Pr (a).
PrP

Thus, regret is just a special case of weighted regret, where all weights are 1.
Note that, as far weighted regret goes, it does not hurt to augment a set P + of weighted
probability measures by adding pairs of the form (Pr, 0) for Pr 
/ P. But if we start with a set
+
P of unweighted probability measures, the set P = {(Pr, 1) : Pr  P}{(Pr, 0) : Pr 
/ P} is
not closed in general, although it is weakly closed. There may well be a sequence Prn  Pr,
where Prn 
/ P for all n, but Pr  P. But then we would have (Prn , 0)  P + converging to
+
(Pr, 0) 
/ P . This is exactly why I required only weak closedness. Note for future reference
that, since P + is assumed to be weakly closed, if wr M
P + (a) > 0, then there is some element
M
+
M
(Pr, Pr )  P such that wr P + (a) = Pr reg Pr (a).
Weighted regret induces an obvious preference order on acts: act a is at least as good
0
0
M
M
as a0 with respect to P + and M , written a reg
P + ,M a , if wr P + (a)  wr P + (a ). As usual, I
4. Recall that if X is a set of real numbers, sup X, the supremum of X, is the smallest real numbers that
is greater than or equal to all the elements of X. If X is finite, then the sup is the same as the max.
But if X is, say, the interval (0, 1), then sup X = 1. Similarly, inf X is the largest real number that is
less than or equal to all the elements in X.

476

fiWeighted Regret-Based Likelihood

reg
reg
0
0
0
write a reg
P + ,M a if a P + ,M a but it is not the case that a P + ,M a. The standard notion
of regret is the special case of weighted regret where all weights are 1. I sometimes write
0
+
a reg
P,M a to denote the unweighted case (i.e., where all the weights in P are 1).
In this setting, using weighted regret gives an approach that allows an agent to transition
smoothly from regret to expected utility. It is well known that regret generalizes expected
M 0
utility in the sense that if P is a singleton {Pr}, then wr M
P (a)  wr P (a ) iff EUPr (a) 
0
EUPr (a ) (where EUPr (a) denotes the expected utility of act a with respect to probability
Pr); this follows from the observation that, given a menu M , there is a constant cM such
that, for all acts a  M , wr M
{Pr} (a) = cM  EUPr (a). (In particular, this means that if P
is a singleton, regret is menu independent.) If we start with all the weights being 1, then,
as observed above, the weighted regret is just the standard notion of regret. As the agent
makes observations, if there is a measure Pr generating the uncertainty, the weights will
get closer and closer to a situation where Pr gets weight 1, with the weights of all other
measures dropping off quickly to 0, so the ordering of acts will converge to the ordering
given by expected utility with respect to Pr.
There is another approach with some similar properties, which again starts with uncertainty being represented by a set P of (unweighted) probability measures. Define wc P (a) =
inf PrP EUPr (a). Thus wc P (a) is the worst-case expected utility of a, taken over all Pr  P.
Then define a mm
a0 if wc P (a)  wc P (a0 ). This is the maxmin expected utility rule, quite
P
often used in economics (Gilboa & Schmeidler, 1989). There are difficulties in getting a
weighted version of maxmin expected utility (Halpern & Leung, 2012) (discussed further in
Section 3); however, Epstein and Schneider (2007) propose another approach that can be
combined with maxmin expected utility. They fix a parameter   (0, 1), and update P
after an observation ob by retaining only those measures Pr such that Pr(ob)  . For any
choice of  < 1, we again end up converging almost surely to a single measure, so again
this approach converges almost surely to expected utility.
I conclude this section with a discussion of menu dependence. Maxmin expected utility
is not menu dependent; the preference ordering on acts induced by regret can be, as the
following example illustrates.

Example 2.1: Take the outcome space to be {0, 1}, and the utility function to be the
identity, so that u(1) = 1 and u(0) = 0. As usual, if E  S, 1E denotes the indicator
function on E, where, for each state s  S, we have 1E (s) = 1 if s  E, and 1E (s) = 0
if s 
/ E. Let S = {s1 , s2 , s3 , s4 }, E1 = {s1 }, E2 = {s2 }, E3 = {s2 , s3 }, M1 = {1E1 , 1E2 },
M2 = {1E1 , 1E2 , 1E3 }, and P = {Pr1 , Pr2 }, where Pr1 (s1 ) = Pr1 (s3 ) = Pr1 (s4 ) = 1/3,
1
Pr2 (s2 ) = 1/4, and Pr2 (s3 ) = 3/4. A straightforward calculation shows that reg M
Pr1 (1E1 ) =
M1
M1
M1
M2
M2
0, reg Pr1 (1E2 ) = 1/3, reg Pr2 (1E1 ) = 1/4, reg Pr2 (1E2 ) = 0, reg Pr1 (1E1 ) = 1/3, reg Pr1 (1E2 ) =
M1
M1
M2
2
2/3, reg M
Pr2 (1E1 ) = 1, and reg Pr2 (1E2 ) = 3/4. Thus, 1/4 = reg P (1E1 ) < reg P (1E2 ) = 1/3,
M2
2
while 1 = reg M
P (1E1 ) > reg P (1E2 ) = 3/4. The preference on 1E1 and 1E2 depends on
whether we consider the menu M1 or the menu M2 .
Suppose that there is an outcome o  O that gives the maximum utility; that is,
 u(o) for all o  O. If o is the constant act that gives outcomes o in all states,
then o is clearly the best act in all states. If there is such a best act, an absolute,
menu-independent notion of weighted expected regret can be defined by always comparing

u(o )

477

fiHalpern

to o . That is, define
reg(s, a) = u(o )  u(a(s));
P
reg Pr (a) = sS (u(o )  u(a(s)) Pr(s) = u(o )  EUPr (a);
P
reg P (a) = supPrP sS (u(o )  u(a(s)) Pr(s) = u(o )  inf PrP (EUPr (a);
P
wr P + (a) = supPrP Pr sS (u(o )  u(a(s)) Pr(s) = supPrP Pr (u(o )  EUPr (a)).
If there is a best act, then I write a P + a0 if wr P + (a)  wr P + (a0 ); similarly in the
unweighted case, I write a P a0 if wr P (a)  wr P (a0 ).
Conceptually, we can think of the agent as always being aware of the best outcome o ,
and comparing his actual utility with a to u(o ). Equivalently, the absolute notion of regret
is equivalent to a menu-based notion with respect to a menu M that includes o (since if
the menu includes o , it is the best act in every state). As we shall see, in our setting, we
can always reduce menu-dependent regret to this absolute, menu-independent notion, since
there is in fact a best act: 1S .

3. Relative Ordering of Events Using Weighted Regret
In this section, I consider how a notion of comparative likelihood can be defined using sets
of weighted probability measures.
As in Example 2.1, take the outcome space to be {0, 1}, the utility function to be the
identity, and consider indicator functions. It is easy to see that EUPr (1E ) = Pr(E), so that
with this setup, we can recover probability from expected utility. Thus, if uncertainty is
represented by a single probability measure Pr and we make decisions by preferring those
acts that maximize expected utility, then we have 1E  1E 0 iff Pr(E)  Pr(E 0 ).
Consider what happens if we apply this approach to maxmin expected utility. Now we
have that 1E mm
1E 0 iff inf PrP Pr(E)  inf PrP Pr(E 0 ). In the literature, inf PrP Pr(E),
P
denoted P (E), is called the lower probability of E, and is a standard approach to describing likelihood. The dual upper probability, supPrP Pr(E), is denoted P  (E). An easy
calculation shows that
P  (E) = 1  P (E),
where, as usual, E denotes the complement of E. The interval [P (E), P  (E)] can be
thought of as describing the uncertainty of E; the larger the interval, the greater the ambiguity.
What happens if we apply this approach to regret? First consider unweighted regret.
If we restrict to acts of the form 1E , then the best act is clearly 1S , which is just the
constant function 1. Thus, we can (and do) use the absolute notion of regret here, and
for the remainder of this paper. We then get that 1E reg
P 1E 0 iff supPrP (1  Pr(E)) 
0
0
supPrP (1  Pr(E )) iff supPrP Pr(E)  supPrP Pr(E ); that is,
0



1E reg
P 1E 0 iff P (E)  P (E ).

478

fiWeighted Regret-Based Likelihood

Moreover, easy manipulation shows that supPrP (1  Pr(E)) = 1  inf PrP Pr(E) = 1 
P (E). It follows that
1E reg
P 1E 0
iff (1  P (E))  (1  P (E 0 ))
iff P (E)  P (E 0 )
iff 1E mm
1E 0 .
P
That is, both regret and maxmin expected utility put the same ordering on events.
+ (E), the (weighted) regret-based
The extension to weighted regret is immediate. Let Preg
likelihood of E, be defined by taking
+
Preg
(E) = sup Pr Pr(E).
PrP

If P + is unweighted, so that all the weights are 1, I write Preg (E) to denote supPrP Pr(E).
Note that Preg (E) = 1  P (E), so
Preg (E)  Preg (E 0 ) iff P (E)  P (E 0 ).
That is, the ordering induced by Preg is the opposite of that induced by P . So, for example,
Preg () = 1 and Preg (S) = 0; smaller sets have larger regret-based likelihood. However, since
an act with smaller regret is viewed as better, the ordering on acts of the form 1E induced
by regret is the same as that induced by maxmin expected utility.
Regret-based likelihood provides a way of associating a number with each event, just
as probability and lower probability do. Moreover, just as lower probability gives a lower
+ (E) as giving an upper bound on the uncertainty.
bound on uncertainty, we can think of Preg
(It is an upper bound rather than a lower bound because larger regret means less likely,
just as smaller lower probability does.) The naive corresponding lower bound is given by
inf PrP Pr Pr(E). This lower bound is not terribly interesting; if there are probability
measures Pr0  P such that Pr0 is close to 0, then this lower bound will be close to 0,
independent of the agents actual feeling about the likelihood of E. A more reasonable
+
lower bound is given by the expression P +
reg (E) = 1  Preg (E) (recall that the analogous
expression relates upper probability and lower probability). The intuition for this choice
is the following. If nature were conspiring against us, she would try to prove us wrong
by making Pr Pr(E) as large as possiblethat is, make the weighted probability of being
wrong as large as possible. On the other hand, if nature were conspiring with us, she would
try to make Pr Pr(E) as large as possible, or, equivalently, make 1  Pr Pr(E) as small
as possible. Note that this is different from making Pr Pr(E) as large as possible, unless
Pr = 1 for all Pr  P. An easy calculation shows that
+ (E) = 1  sup
1  Preg
PrP Pr Pr(E)
= inf PrP (1  Pr Pr(E)).

This motivates the definition of P +
reg .
The following lemma clarifies the relationship between these expressions, and shows that
+
[P +
reg (E), Preg (E)] really does give an interval of ambiguity.
+ (E)  P + (E).
Lemma 3.1: inf PrP Pr Pr(E)  1  Preg
reg

479

fiHalpern

Proof: Clearly
inf Pr Pr(E) = inf Pr (1  Pr(E)).

PrP

PrP

Since, as observed above,
+
1  Preg
(E) = inf (1  Pr Pr(E)),
PrP

and for all Pr  P, we have
1  Pr Pr(E)  Pr (1  Pr(E)),
+ (E).
it follows that inf PrP Pr Pr(E)  1  Preg
Since, by assumption, there is a probability measure Pr0  P such that Pr0 = 1, it
follows that
+ (E) = 1  sup
1  Preg
PrP Pr Pr(E)
 1  Pr0 (E)
= Pr0 (E)
 supPrP Pr Pr(E)
+ (E).
 Preg

In general, equality does not hold in Lemma 3.1, as shown by the following example. The
example also illustrates how the ambiguity interval can decrease with weighted regret, if
the weights are updated as Leung and I (Halpern & Leung, 2012) suggested.
Example 3.2: Suppose that the state space consists of {h, t} (for heads and tails); let Pr
be the measure that puts probability  on h. Let P0+ = {(Pr , 1) : 1/3    2/3}. That is,
we initially consider all the measures that put probability between 1/3 and 2/3 on heads. We
toss the coin and observe it lands heads. Intuitively, we should now consider it more likely
that the probability of heads is greater than 1/2. Indeed, applying likelihood updating, we
get the set P1+ = {(Pr , 3/2) : 1/3    2/3}; the probability measures that give h higher
probability get higher weight. In particular, the weight of Pr2/3 is still 1, but the weight of
Pr1/3 is only 1/2. (The weight of Pr is the likelihood of observing heads according to Pr ,
which is just , normalized by the likelihood of observing heads according to the measure
that gives heads the highest probability, namely 2/3.) If the coin is tossed again and this
time tails is observed, we update further to get P2+ = {(Pr , 4(1  )) : 1/3    2/3}.
Before going on, it is worth noting here how the simple parametric form of P0+ leads to
simple parametric forms for P1+ and P2+ .
+
+
+
An easy calculation shows that [P +
0,reg (h), P0,reg (h)] = [1/3, 2/3], [P 1,regret (h), P1,reg (h)] =
+
+
[1/3, 3/8], and [P 2,reg (h), P2,reg (h)] = [11/27, 16/27]. In more detail, since Pr (h) =  and
Pr (t) = 1  , so we have the following:
0
 P0,reg
(h) = sup[1/3,2/3] (1  ) = 2/3.

 P 00,reg (h) = inf   [1/3, 2/3](1  ) = 1/3.
0
 P1,reg
(h) = sup[1/3,2/3] (3/2)(1). Taking the derivative shows that (3/2)(1)
0
is maximized when  = 1/2, so P1,reg
(h) = 3/8.

480

fiWeighted Regret-Based Likelihood

 P 01,reg (h) = inf [1/3,2/3] (1  (3/2)). Now 1  (3/2) is minimized, when (3/2)
is maximized; for   [1/3, 2/3], this happens when  = 2/3, so P 01,reg (h) = 1/3.
0
 P2,reg
(h) = sup[1/3,2/3] 4(1)(1). Taking the derivative shows that 4(1)2
is maximized when  = 1/3, in which case it is 16/27.

 P 02,reg (h) = inf [1/3,2/3] (1  4(1  )). Now 1  4 2 (1  ) is minimized when
4 2 (1) is maximized; for   [1/3, 2/3], this happens when  = 2/3, so P 01,reg (h) =
11/27.
It is also easy to see that inf Pr 4(1  ) Pr (t) = inf [1/3,2/3] 4(1  )2 = 8/27, so
+
+
inf 4(1  )Pr (t) < 1  P2,reg
(t) < P2,reg
(h).

PrP2

Thus, for P2+ , we get strict inequalities for the expressions in Lemma 3.1.
+
The width of the interval [P +
reg (E), Preg (E)] can be viewed as a measure of the ambiguity
the agent feels about E, just as the interval [P (E), P  (E)]. Indeed, if all the weights are 1,
+ (E) and P  (E) = 1  P + (E)
the two intervals have the same width, since P (E) = 1  Preg
reg
in this case.
However, weighted regret has a significant advantage over upper and lower probability.
If the true bias of the coin is, say 5/8, then if the set Pk+ represents the uncertainty after
+
k steps, as k increases, almost surely, [P +
k,reg (h), Pk,reg (h)] will be a smaller and smaller
interval containing 1  5/8 = 3/8. More generally, using likelihood updated combined with
weighted regret provides a natural way to model the reduction of ambiguity via learning.
It is worth at this point comparing the approach to representing likelihood taken here
to the work of Nau (1992). Nau starts with a preference order on lotteries (functions from
some finite state space S to the reals) satisfying certain axioms, and derives from that what
he calls confidence-weighted (lower and upper) probabilities. Roughly speaking, rather than
just associating with each event its lower and upper probability, Nau can associate with
 of probabilities
each event E, confidence c  [0, 1], and probability p  [0, 1] the set Pc,p
that give event E lower probability p with confidence at least c. If c0  c, then Pc0 ,p  Pc,p
(every probability measures that gives E lower probability p with the higher confidence
c0 will also give it lower probability p with confidence c, but the converse may not hold).
Similarly, we can consider the probability measures that give E upper probability p with
confidence c. With a set P of unweighted probabilities, an agents uncertainty regarding an
event E can be characterized by a single interval [P (E), P  (E)]. In Naus framework, an
agents uncertainty regarding E can be characterized by a family of intervals [Pc (E), P c (E)],
indexed by the confidence c, where Pc (E) is the largest p such that E has lower probability
with confidence c, and P c (E) is defined similarly. Clearly these intervals are nested; if
0
c0 > c, then [Pc0 (E), P c (E)] contains [Pc (E), P c (E)]. Thus, Naus approach provides a
more fine-grained representation of uncertainty than the single intervals [P (E), P  (E)] or
+
[P +
reg (E), Preg (E)]. To some extent, this distinction is due to the fact that Naus preference
order on lotteries is only a partial order; the preference order induced by max=min expected
+ , and P + all put a
utility regret is total. However, note that even though P , P  , Preg
reg

+ , and P + together,
total order on events, when considering both P and P or both Preg
reg

481

fiHalpern

we can also obtain a partial order on events; in particular, these approaches can express
ambiguity.
One benefit of the regret-based approach is that it provides a natural way of updating.
Nau does not consider updating; it would be interesting to see if an analogue of likelihood
updating could be defined axiomatically in Naus framework, perhaps in the spirit of the
characterization that Leung and I (Halpern & Leung, 2012) gave for likelihood updating in
the context of regret.
One concern with the use of regret has been the dependence of regret on the menu;
Naus approach, and other approaches to decision-making that are not based on regret, do
not require a menu. While there is evidence from the psychology literature suggesting that
people are quite sensitive to menus, it is also worth noting that when dealing with likelihood,
there is a sense in which we can work with the absolute notion of weighted regret without
loss of generality: if we restrict to indicator functions, then a preference relative to a menu
can always be reduced to an absolute preference. Given a menu M consisting of indicator
functions, let EM = {E : 1E  M }; that is, EM is the union of the events for which the
corresponding indicator function is in M . The following property shows that, when restrict
to indicator functions, regret satisfies satisfies an axiom similar in spirit to Naus (1992)
cancellation axiom.
Proposition 3.3: If M is a menu consisting of indicator functions, and 1E1 , 1E2  M ,
reg
then 1E1 reg
P + ,M 1E2 iff 1E1 + 1E M P + 1E2 + 1E M .
Proof: Let M 0 be any menu consisting of indicator functions that includes 1E1 + 1E M ,
reg
1E2 + 1E M , and 1S . Recall that 1E1 + 1E M reg
P + 1E2 + 1E M iff 1E1 + 1E M M 0 ,P + 1E2 + 1E M ;
the absolute notion of regret is equivalent to the menu-based notion, as long as the menu
includes the best act, which in this case is 1S . It clearly suffices to show that, for all states
s  S and all acts 1E  M ,
0

reg M (1E , s) = reg M (1E + 1E M , s).
This is straightforward. There are two cases, depending on whether s  EM .
If s  EM , then, by definition, there is some act 1E 0  M such that s  E 0 , so
supaM u(a(s)) = u(1). Clearly supaM 0 u(a(s)) = u(1), since 1S  M 0 . Moreover,
1E M (s) = 0, so (1E + 1E M )(s) = 1E (s). Thus, for s  EM ,
reg M (1E , s) = supaM u(a(s))  u(1E (s))
= supaM 0 u(a(s))  u((1E + 1E M )(s))
0
= reg M (1E + 1E M , s).
For s 
/ E M , we have a(s) = 0 for all a  M and 1E (s) = 0, so supaM u(a(s))  u(1E (s)) =
0. On the other hand, supaM 0 u(a(s)) = u(1), and u((1E + 1E M )(s)) = u(1), so again
0
supaM 0 u(a(s))  u((1E + 1E M )(s)) = 0. Thus, we again have reg M (1E , s) = reg M (1E +
1E M , s).

482

fiWeighted Regret-Based Likelihood

4. Characterizing Weighted Regret-Based Likelihood
The goal of this section is to characterize weighted regret-based likelihood axiomatically.
In order to do so, it is helpful to review the characterizations of probability and lower
probability. For ease of exposition in this discussion, I assume that the sample space is
finite and all sets are measurable.
A probability measure on a finite set S maps subsets of S to [0, 1] in a way that satisfies
the following three properties:
Pr1. Pr(S) = 1.
Pr2. Pr() = 0.5
Pr3. Pr(E  E 0 ) = Pr(E) + Pr(E 0 ) if E  E 0 = .
These three properties characterize probability in the sense that any function f : 2S  [0, 1]
that satisfies these properties is a probability measure.
Lower probabilities satisfy analogues of these properties:
LP1. P (S) = 1.
LP2. P () = 0.
LP30 . P (E  E 0 )  P (E) + P (E 0 ) if E  E 0 = .
However, these properties do not characterize lower probability. There are functions that
satisfy LP1, LP2, and LP30 that are not the lower probability corresponding to some set
of probability measures. (See (Halpern & Pucella, 2002, Proposition 2.2) for an example
showing that analogous properties do not characterize P  ; the same example also shows
that they do not characterize P .)
Various characterizations of P (and P  ) have been proposed in the literature (Anger &
Lembcke, 1985; Giles, 1982; Huber, 1976, 1981; Lorentz, 1952; Williams, 1976; Wolf, 1977),
all similar in spirit. I discuss one due to Anger and Lembcke (1985) here, since it makes
the contrast between lower probability and regret particularly clear. The characterization
is based on the notion of set cover: a set E is said to be covered n times by a multiset
M if every element of E appears at least n times in M . It is important to note here that
M is a multiset, not a set; its elements are not necessarily distinct. (Of course, a set is a
special case of a multiset.) Let t denote multiset union; thus, if M1 and M2 are multisets,
then M1 t M2 consists of all the elements in M1 or M2 , which appear with multiplicity that
is the sum of the multiplicities in M1 and M2 . For example, using the {{. . .}} notation to
denote a multiset, then {{1, 1, 2}} t {{1, 2, 3}} = {{1, 1, 1, 2, 2, 3}}.
If E  S, then an (n, k)-cover of (E, S) is a multiset M that covers S k times and
covers E n + k times. Multiset M is an n-cover of E if M covers E n times. For example, if
S = {1, 2, 3}, then {{1, 1, 1, 2, 2, 3}} is a (2, 1)-cover of ({1}, S), a (1, 1)-cover of ({1, 2}, S),
and a 3-cover of {1}.
We will be interested in whether a multiset of the form E 1 t . . . t E m is an (n, k)-cover
of (E, S). This is perhaps best thought of in terms of indicator functions. E 1 t . . . t E m
5. This property actually follows from the other two, using the observation that Pr(S  ) = Pr(S) + Pr();
I include it here to ease the comparison to other approaches.

483

fiHalpern

is an (n, k)-cover of (E, S) if and only if 1E1 +    + 1Em  n1E + k1S . The use of equalities and inequalities involving sums of indicator functions in axiomatic characterizations of
uncertainty has a long history; for example, they were used by Scott (1964) to characterize
qualitative probability. Set covers are just a special case of such inequalities. Typically, such
axioms make it possible to apply results from linear programming to prove characterization
results. As we shall see, that will be the case here too.
Consider the following property:
LP3. For all integers m, n, k and all subsets E1 , . . . , Em of S, if E1 t . . . t Em is an (n, k)P
6
cover of (E, S), then k + nP (E)  m
i=1 P (Ei ).
There is an analogous property for upper probability, where  is replaced by . It is easy
to see that LP3 implies LP30 (since E t E 0 is a (1, 0) cover of (E  E 0 , S)). It follows
by a straightforward induction from LP30 that if E1 , . . . , Em are pairwise disjoint, then
P (E1  . . .  Em )  P (E1 ) +    + P (E1 ). LP3 generalizes this property to allow for sets
that are not necessarily disjoint. The soundness of LP3 for lower probability follows using
the same techniques as given below for the soundness of the property REG3. As Anger
and Lembcke (1985) show, LP3 is just the property that is needed to characterize lower
probability.
Theorem 4.1: (Anger & Lembcke, 1985) If f : 2S  [0, 1], then there exists a set P of
probability measures with f = P if and only if f satisfies LP1, LP2, and LP3.
Moving to regret-based likelihood, clearly we have
+ (S) = 0.
REG1. Preg
+ () = 1.
REG2. Preg

The whole space S has the least regret; the empty set has the greatest regret. Again, we see
that regret-based likelihood inverts the standard ordering of probability; larger regret-based
likelihood corresponds to probability.
In the unweighted case, since Preg (E) = P  (E), REG1, REG2, and the following analogue of LP3 (appropriately modified for P  ) clearly characterize Preg :
REG30 . For all integers m, n, k and all subsets E1 , . . . , Em of S, if E 1 t . . . t E m is an
P
(n, k)-cover of (E, S), then k + nPreg (E)  m
i=1 Preg (Ei ).
Note that complements of sets (E 1 , . . . , E m , E) are used here, since regret is minimized if
the probability of the complement is maximized. This need to work with the complement
makes the statement of the properties (and the proofs of the theorems) slightly less elegant,
but seems necessary.
It is not hard to see that REG30 does not hold for weighted regret-based likelihood.
For example, suppose that S = {a, b, c} and P + = ((Pr1 , 2/3), (Pr2 , 2/3), (Pr3 , 1)), where,
identifying the probability Pr with the tuple (Pr(a), Pr(b), Pr(c)), we have
 Pr1 = (2/3, 0, 1/3);
6. Note that LP3 implies LP2, using the fact that  t  is a (1,0)-cover of (, S).

484

fiWeighted Regret-Based Likelihood

 Pr2 = (1/3, 0, 2/3);
 Pr3 = (1/3, 1/3, 1/3).
+ ({a, b}) = P + ({b, c}) = 4/9, while P + ({b}) = 2/3. Since {a, b} t {b, c} is a
Then Preg
reg
reg
(1,1)-cover of ({b}, {a, b, c}), REG30 would require that
+
+
+
Preg
({a, b}) + Preg
({b, c})  1 + Preg
({b}),

which is clearly not the case.
We must thus weaken REG30 to capture weighted regret-based likelihood. It turns out
that the appropriate weakening is the following:
REG3. For all integers m, n and all subsets E1 , . . . , Em of S, if E 1 t . . . t E m is an n-cover
+ (E)  Pm P + (E ).
of E, then nPreg
i
i=1 reg
Although REG3 is weaker than REG30 , it still has some nontrivial consequences. For
+ is anti-monotonic. If E  E 0 , then E is a 1-cover
example, it follows from REG3 that Preg
0
+ (E)  P + (E 0 ). Since E t E 0 is trivially a 1-cover of
of E , so by REG3, we must have Preg
reg
+ (E) + P + (E 0 )  P + (E  E 0 ). REG3 also implies REG1,
E  E 0 , it also follows that Preg
reg
reg
since  (= S) is an n-cover of itself for all n.
I can now state the representation theorem. It says that a representation of uncertainty
satisfies REG1, REG2, and REG3 iff it is the weighted regret-based likelihood determined
by some set P + . The set P + is not unique, but it can be taken to be maximal, in the
sense that if weighted regret-based likelihood with respect to some other set (P 0 )+ gives
the same representation, then for all pairs (Pr, 0 )  (P 0 )+ , there exists   0 such that
(Pr, )  P + . This (unique) maximal set P + can be viewed as the canonical representation
of uncertainty.
Theorem 4.2: If f : 2S  [0, 1], then there exists a weakly closed set P + of weighted
+ if and only if f satisfies REG1, REG2, and REG3;
probability measures with f = Preg
moreover, P + can be taken to be maximal.
Proof: Clearly, given a weakly closed set P + of weighted probability measures, the function
+ satisfies REG1 and REG2. To see that it satisfies REG3, suppose that E t . . . t E is
Preg
1
m
+ (E) = 0, then REG3 trivially holds. If P + (E) > 0, then since P +
an n-cover of E. If Preg
reg
+ (E) =  Pr(E).
is weakly closed, there must be some probability Pr  P such that Preg
Pr
Since E 1 t. . .tE m is an n-cover of E, it is easy to see that Pr(E 1 )+  +Pr(E m ) = n Pr(E),
+ (E), by construction,
so Pr Pr(E 1 ) +    + Pr Pr(E m ) = nPr Pr(E). But Pr Pr(E) = Preg
P
m
+ (E ), i = 1, . . . , n. Thus, nP + (E) 
+
and Pr Pr(E i )  Preg
i
reg
i=1 Preg (Ei ).
S
For the opposite direction, suppose that f : 2  [0, 1] satisfies REG1, REG2, and
REG3. Let P = (S), the set of all probability measures on S, and for Pr  P, define
Pr = sup{ :  Pr(E)  f (E) for all E  S}.
Note that, for all Pr  P, we have 0 Pr(E)  f (E) for all E  S, since f (E)  [0, 1], and
1 Pr() = f () = 1. It follows that Pr  [0, 1] for all Pr  P. Let P + = {(Pr, Pr ) :
485

fiHalpern

Pr  (S)}. It is easy to see that P + is weakly closed. Moreover, if we can show that P +
+ ), it is immediate that P + is maximal among all sets of weighted
represents f (i.e., f = Preg
probability measures that represent f . Thus, it suffices to show that there exists Pr  (S)
such that (1) Pr = 1 (since this is one of the conditions on sets of weighted measures) and
+ (E) for all E  S.
(2) f (E) = Preg
The proof of this result makes critical use of the following variant of Farkas Lemma
(Farkas, 1902) (see also Schrijver, 1986, pg. 89) from linear programming, where A is a
matrix, b is a column vector, and x is a column vector of distinct variables:
Lemma 4.3: If Ax  b is unsatisfiable, then there exists a row vector  such that
1.   0
2. A = 0
3. b > 0.
Intuitively,  is a witness of the fact that Ax  b is unsatisfiable. This is because if there
were a vector x satisfying Ax  b, then 0 = (A)x = (Ax)  b > 0, a contradiction.
To prove the first claim, suppose that S = {s1 , . . . , sN }. I now construct a set of linear
equations in the variables x1 , . . . , xN such that a solution to the equations guarantees the
existence of a probability measure Pr  (S) such that Pr = 1. Intuitively, we want
xi to be Pr(si ). Since we must have Pr(E)  f (E) for all E  S,7 for each E  S, we
P
have the inequality {i:si E}
xi  f (E). Note that since f () = 1, the equation when
/
E =  is x1 +    + xN  1. In addition, we require that xi  0 for i = 1, . . . , N , and that
x1 +  +xN = 1. It suffices to require that x1 +  +xn  1, since, as I observed earlier, the
equation corresponding to E =  already says x1 +    + xn  1. To apply Farkas Lemma
all the inequalities need to involve , so this collection of inequalities must be rewritten as:
 {i:si E}
xi  f (E), for all E  S
/
xi  0, for i = 1, . . . , N
x1 +    + xN  1.
P

This system of inequalities can be expressed in the form Ax  b. Note that A is a matrix all
of whose entries are either 1, 0, or 1, and, in the first 2N  1 rows (the lines corresponding
to equations for each E  S), all the entries are either 0 or 1, while in the final N + 1
rows, all the entries are either 0 or 1.
A solution of this system of inequalities provides the desired Pr. But if this systems has
no solution, then by Farkas Lemma, there exists a nonnegative vector  such that A = 0
and b > 0. Since all the entries of A are either 1, 0, or 1, it follows from standard
observations (cf., Fagin, Halpern, & Megiddo, 1990, Lemma 2.7) we can take  to a vector
of all whose entries are rational.8 Since we can multiply each term in  by the product
7. I use  to denote strict subset.
8. There is a slight subtlety here since  also has to satisfy b > 0, and b may involve irrational numbers
(since f (E) may be irrational for some sets E). However, if there is a nonnegative  that satisfies A = 0
and b > 0, then there is a nonnegative  that satisfies A = 0 and b0 > 0, where b0 consists only of
rational entries and b0  b. Thus, there is a vector  with rational entries such that A = 0 and b0 > 0,
so b > 0.

486

fiWeighted Regret-Based Likelihood

of the denominators of the entries of , we can assume without loss of generality that the
entries of  are natural numbers.
Since A has 2N + N rows,  is a vector of the form (1 , . . . , 2N +N ). Let A1 , . . . , A2N +N
be the rows of A; each of these is a vector of length N . Since A = 0, that means that
1 A1 +    + 2N +N A2N +N = 0. Suppose for now that 2N , . . . , 2N +N 1 (the coefficients
for the rows corresponding to the inequalities xi  0 for i = 1, . . . , N ) are all 0; as I show
below, this assumption can be made without loss of generality.
With this assumption, we can rewrite the equations as 1 A1 +. . . 2N 1 A2N 1 = 2N +N A2N +N .
If E1 , . . . , E2N 1 are the subsets of S that correspond to the equations for A1 , . . . , A2N 1 ,
respectively, this equation says that 1 copies of E 1 , 2 copies of E 2 , . . . , and 2N 1 copies
of E 2N 1 form a 2N +N -cover of S. (Recall that A2N +N is a row of all 1s, so A2N +N corresponds to S.) Thus, by REG3, 1 f (E1 ) +    + 2N 1 f (E2N 1 )  2N +N f () = 2N +N .
But Farkas Lemma requires that b > 0, where, by construction, bi = f (Ei ) for i =
1, . . . , 2N  1, bi = 0 for i = 2N , . . . , 2N + N  1, and b2N +N = 1. Thus, we must have
(1 f (E1 )+  +2N 1 f (E2N 1 )) > 2N +N . Clearly, this gives a contradiction. Thus, we
can conclude, as desired, that the equations are solvable, and that there exists a probability
measure Pr such that Pr = 1.
N
N
It remains to show that we can assume without loss of generality that  2 , . . . ,  2 +N 1
are all 0. Note that since   0, they must all be nonnegative. I prove by induction on
2N +    + 2N +N 1 that if there is a vector   0 such that A = 0 and b > 0, then
there is such a vector with 2N +    + 2N +N 1 = 0.
So suppose that there is a solution  with 2N +    + 2N +N 1 > 0. Suppose without
loss of generality that 2N > 0. Recall that A2N corresponds to the inequality x1  0.
Choose j  {0, . . . , 2N  1} such that j > 0 and s1 
/ Ej . There must be such a j, for
otherwise we would not have A = 0. Let j 0 be such that Ej 0 = Ej  {s1 }. Define a vector
/ {j, j 0 , 2N }.
 0 such that 20 N = 2N  1, j0 = j  1, j0 0 = j + 1, and i0 = i if i 
0
0
0
It is easy to check that  A = 0 and that 2N +    + 2N +N 1 < 2N +    + 2N +N 1 .
It remains to show that  0 b > 0. Since Ej  Ej 0 , we must have f (Ej )  f (Ej 0 ), so
 0 b = b + f (Ej )  f (Ej 0 )  b > 0. This completes the inductive step of the argument.
+ (E) for
Now we must show the second required property holds, namely, that f (E) = Preg
all E  S. By construction, Pr Pr(E)  f (E) for all E  S, so it suffices to show that there
is some Pr  P such that Pr Pr(E) = f (E). For this, it suffices to show that there exists a
measure Pr such that Pr(E) = 1, and for each E 0  S, we have f (E) Pr(E 0 )  f (E 0 ), since
then Pr = f (E), so Pr Pr(E) = f (E), as desired.
To show that such a measure exists, we again construct a set of linear inequalities
much as above, and apply Farkas Lemma. Using the same notation as above, suppose for
simplicity that E = {s1 , . . . , sM }, where M  N . Now the required inequalities just involve
the variables x1 , . . . , xM :
0

 {i:s EE 0 } xi  f (E 0 )/f (E), for all E 0  S such that E  E 6= 
i
xi  0, for i = 1, . . . , M
x1 +    + xM  1.
P

Again, the requirement that x1 +    + xM  1 follows from the equation for E.
If this system of inequalities is satisfiable, then we have the required probability measure,
so suppose that it is not satisfiable. Again, writing this system of equations as Ax  b, by
487

fiHalpern

Farkas Lemma, there exists a nonnegative vector  such that A = 0 and b > 0. We now
proceed much as before. Again, we can assume that  is a vector of natural numbers. If
M
M
we assume for now that  2 , . . . ,  2 +M 1 (the coefficients for the rows corresponding to
the inequalities xi  0 for i = 1, . . . , N ) are all 0, then the fact that A = 0 means that
we have 2M +M cover of E. We get a contradiction to REG3 in an almost identical way to
above. This completes the argument.
As I said earlier, the set P + guaranteed to exist by Theorem 4.2 is not unique, although
it is canonical, in the sense of being the unique maximal set of weighted probability measures
that represents f . We might wonder if we can actually get uniqueness by imposing a few
extra requirements, particularly since Leung and I were able to do so in our representation
theorem. The answer seems to be no. To explain why, it is helpful to review some material
from (Halpern & Leung, 2012).
Define a sub-probability measure p on S to be like a probability measure (i.e., a function
mapping measurable subsets of S to [0, 1] such that p(T  T 0 ) = p(T ) + p(T 0 ) for disjoint
sets T and T 0 ), without the requirement that p(S) = 1. We can identify a weighted
probability distribution (Pr, ) with the sub-probability measure  Pr. Conversely, given a
sub-probability measure p, there is a unique pair (, Pr) such that P =  Pr: we simply
take  = p(S) and Pr = p/. Thus, in the sequel, I identify a set of sub-probability
measures with a set of weighted probability measures.
A set B of sub-probability measures is downward-closed if, whenever p  B and q  p,
then q  B.
One advantage of considering sub-probability measures is that while it is not clear what
it would mean for a set of weighted probabilities to be convex (indeed, it is not obvious what
should count as a convex combination of (Pr, ) and (Pr0 , 0 )), it is quite clear what counts
as a convex combination of sub-probability measures. Moreover, a convex combination of
sub-probability measures is itself a sub-probability measure.
Call a set of subprobability measures regular if it is convex, downward-closed, closed,
and contains at least one proper probability measure. (The latter requirement corresponds
to having Pr = 1 for some Pr  P + .) Leung and I provide a set of axioms for preference
orders, and show that a family of preference orders M indexed by menus satisfies these
axioms iff there is a unique regular set of weighted probability measures P + such that, for
M
all a M b iff wr M
P + (a)  wr P + (b). Thus, we might hope that we can get uniqueness by
imposing a regularity requirement. It is easy to see that the canonical maximal set P +
constructed in the proof of Theorem 4.2 is regular, which lends some credence to this hope.
Unfortunately, as the following example shows, regularity does not suffice for uniqueness.
Example 4.4: Let S = {s1 , s2 }, and let f be defined on 2S by taking f ({s1 }) = 1/4 and
f ({s2 }) = 1 (and f (S) = 0 and f () = 1). A sub-probability measure p on S can be
identified with the pair (p(s1 ), p(s2 )), which makes it easy to think about sub-probability
measures on S geometrically. A set of sub-probability measures is just a region in IR2
contained in the triangle bounded by the lines x = 0, y = 0, and y = 1  x. A set P +
of subprobability measures is downward closed if, whenever it contains a point (x, y), it
contains all (x0 , y 0 ) in the rectangle defined by the points (0, 0), (x, 0), (0, y), and (x, y).
With this intuition, let P0+ be the set of subprobabilities in the quadrilateral bounded
by x = 0, y = 0, y = 1  x, and y = 1/4 (the region marked by vertical lines in Figure 1). It
488

fiWeighted Regret-Based Likelihood

is not hard to show that P0+ is the maximal set of weighted probabilities representing f . It
+
is clearly regular. Since it contains the subprobability (1, 0), it follows that P0,reg
({s2 }) = 1.
+
+
It is also easy to see that, since (0, 1/4)  P0 and p(s2 )  1/4 for all p  P0 , we have that
+
P0,reg
({s1 }) = 1/4.
But now let P1+ consist of all sub-probabilities in the triangle bounded by x = 0, y = 0,
+
and y = 1x
4 (the region marked by horizontal lines in Figure 1). Clearly P1 is a strict
+
subset of of P0 , but it is clear from the figure that it is also regular. Moreover, since it
contains the points ( 41 , 0) and (0, 1), it also represents f . Indeed, it easily follows from
the geometry of the situation that there are uncountably many regular sets of weighted
probabilities representing f ; for all z  [0, 34 ], the regular set bounded by the lines x = 0,
y = 0, y = 14 , and the line from (z, 41 ) to (1, 0).
y
1

( 34 , 14 )

1
4

0

3
4

1

x

Figure 1: Regular sets of weighted probability measures that represent f .

Intuitively, the problem here is that a function on S does not contain enough information
to uniquely determine a regular set of weighted probability measures. It is not clear whether
there are natural further conditions that can be imposed that we lead to uniqueness. It
seems that the closest that we can come to uniqueness is to consider the maximal set.

5. Conclusion
I have defined an approach for associating with an event E a numerical representation of
its likelihood when uncertainty is represented by a set of weighted probability measures.
The representation consists of a pair of a numbers, which can be thought of as upper and
lower bounds on the uncertainty. The difference between these numbers can be viewed
as a measure of ambiguity. The two numbers coincide when uncertainty is represented
by a single probability. Moreover, if each probability measure gets weight 1, then the
two numbers can essentially be viewed as the lower and upper probabilities of E (more
precisely, 1  P (E) and 1  P  (E)). Thus, the approach can be viewed as a generalization
of lower and upper probability to the case of weighted probability measures, with regretbased likelihood corresponding to upper probability. The definitions show that there is
489

fiHalpern

an interesting connection between regret-based approaches and minimization/maximization
approaches when it comes to defining likelihood; this connection breaks down when it comes
to more general utility calculations (Halpern & Leung, 2012).
The main technical result of the paper is a complete characterization of the likelihood
in the case where the state space is finite. The notion of likelihood can easily be extended
to the case of an infinite state space (of course, an integral has to be used instead of a sum
to calculate expected utility). I conjecture that the characterization theorem will still hold
with essentially no change, although I have not checked details carefully.
Of course, it would be useful to get a better understanding of this numerical representation, to see if it really captures an agents feelings about both the ambiguity and the risk
associated with an event, and to understand its technical properties. I leave this to future
work.

Acknowledgments
I thank Samantha Leung, the reviewers of ECSQARU, and the JAIR referees for many useful
comments on the paper. The work was supported in part by NSF grants IIS-0812045, IIS0911036, and CCF-1214844, by AFOSR grants FA9550-08-1-0438, FA9550-09-1-0266, and
FA9550-12-1-0040, and by ARO grant W911NF-09-1-0281.

References
Anger, B., & Lembcke, J. (1985). Infinitely subadditive capacities as upper envelopes of
measures. Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete, 68, 403
414.
Boole, G. (1854). An Investigation into the Laws of Thought on Which Are Founded the
Mathematical Theories of Logic and Probabilities. Macmillan, London.
Campos, L. M. d., & Moral, S. (1995). Independence concepts for sets of probabilities.
In Proc. Eleventh Conference on Uncertainty in Artificial Intelligence (UAI 95), pp.
108115.
Cattaneo, M. E. G. V. (2007). Statistical decisions based directly on the likeihood function.
Ph.D. thesis, ETH.
Chateauneuf, A., & Faro, J. (2009). Ambiguity through confidence functions. Journal of
Mathematical Economics, 45, 535  558.
Couso, I., Moral, S., & Walley, P. (1999). Examples of independence for imprecise probabilities. In Proc. First International Symposium on Imprecise Probabilities and Their
Applications (ISIPTA 99).
de Cooman, G. (2005). A behavioral model for vague probability assessments. Fuzzy Sets
and Systems, 154 (3), 305358.
Dubois, D., & Prade, H. (1998). Possibility measures: qualitative and quantitative aspects.
In Gabbay, D. M., & Smets, P. (Eds.), Quantified Representation of Uncertainty and
490

fiWeighted Regret-Based Likelihood

Imprecision, Vol. 1 of Handbook of Defeasible Reasoning and Uncertainty Management
Systems, pp. 169226. Kluwer, Dordrecht, Netherlands.
Epstein, L., & Schneider, M. (2007). Learning under ambiguity. Review of Economic
Studies, 74 (4), 12751303.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). A logic for reasoning about probabilities.
Information and Computation, 87 (1/2), 78128.
Farkas, J. (1902). Theorie der enfachen ungleichungen. J. Reine und Angewandte Math.,
124, 127.
Gardenfors, P., & Sahlin, N. (1982). Unreliable probabilities, risk taking, and decision
making. Synthese, 53, 361386.
Gardenfors, P., & Sahlin, N. (1983). Decision making with unreliable probabilities. British
Journal of Mathematical and Statistical Psychology, 36, 240251.
Gilboa, I., & Schmeidler, D. (1989). Maxmin expected utility with a non-unique prior.
Journal of Mathematical Economics, 18, 141153.
Gilboa, I., & Schmeidler, D. (1993). Updating ambiguous beliefs. Journal of Economic
Theory, 59, 3349.
Giles, R. (1982). Foundations for a theory of possibility. In Gupta, M. M., & Sanchez, E.
(Eds.), Fuzzy Information and Decision Processes, pp. 183195. North-Holland.
Good, I. J. (1980). Some history of the hierarchical Bayesian methodology. In Bernardo,
J. M., DeGroot, M. H., Lindley, D., & Smith, A. (Eds.), Bayesian Statistic I, pp.
489504. University Press: Valencia.
Halpern, J. Y. (1997). Defining relative likelihood in partially-ordered preferential structures. Journal of A.I. Research, 7, 124.
Halpern, J. Y. (2003). Reasoning About Uncertainty. MIT Press, Cambridge, Mass.
Halpern, J. Y., & Leung, S. (2012). Weighted sets of probabilities and minimax weighted
expected regret: new approaches for representing uncertainty and making decisions. In
Proc. Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI 2012),
pp. 336345. To appear, Theory and Decision.
Halpern, J. Y., & Pucella, R. (2002). A logic for reasoning about upper probabilities.
Journal of A.I. Research, 17, 5781.
Huber, P. J. (1976). Kapazitaten statt Wahrscheinlichkeiten? Gedanken zur Grundlegung
der Statistik. Jahresbericht der Deutschen Mathematiker-Vereinigung, 78, 8192.
Huber, P. J. (1981). Robust Statistics. Wiley, New York.
Klibanoff, P., Marinacci, M., & Mukerji, S. (2005). A smooth model of decision making
under ambiguity. Econometrica, 73 (6), 18491892.
491

fiHalpern

Kyburg, Jr., H. E. (1988). Higher order probabilities and intervals. International Journal
of Approximate Reasoning, 2, 195209.
Levi, I. (1985). Imprecision and uncertainty in probability judgment. Philosophy of Science,
52, 390406.
Lorentz, G. G. (1952). Multiply subadditive functions. Canadian Journal of Mathematics,
4 (4), 455462.
Maccheroni, F., Marinacci, M., & Rustichini, A. (2006). Ambiguity aversion, robustness,
and the variational representation of preferences. Econometrica, 74 (6), 14471498.
Moral, S. (1992). Calculating uncertainty intervals from conditional convex sets of probabilities. In Proc. Eighth Conference on Uncertainty in Artificial Intelligence (UAI
95), pp. 199206.
Nau, R. F. (1992). Indeterminate probabilities on finite sets. Annals of Statistics, 40 (4),
17371767.
Niehans, J. (1948). Zur preisbildung bei ungewissen erwartungen. Schweizerische Zeitschrift
fur Volkswirtschaft und Statistik, 84 (5), 433456.
Ostrogradsky, M. V. (1838). Extrait dun memoire sur la probabilite des erreurs des tribuneaux. Memoires dAcademie St. Petersbourg, Series 6, 3, xixxxv.
Pearl, J. (1987). Do we need higher-order probabilities and, if so, what do they mean?. In
Proc. Third Workshop on Uncertainty in Artificial Intelligence (UAI 87), pp. 4760.
Savage, L. J. (1951). The theory of statistical decision. Journal of the American Statistical
Association, 46, 5567.
Schrijver, A. (1986). Theory of Linear and Integer Programming. Wiley, New York.
Scott, D. (1964). Measurement structures and linear inequalities. Journal of Mathematical
Psychology, 1, 233247.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities, Vol. 42 of Monographs
on Statistics and Applied Probability. Chapman and Hall, London.
Walley, P. (1997). Statistical inferences based on a second-order possibility distribution.
International Journal of General Systems, 26 (4), 337383.
Williams, P. M. (1976). Indeterminate probabilities. In Przelecki, M., Szaniawski, K., &
Wojcicki, R. (Eds.), Formal Methods in the Methodology of Empirical Sciences, pp.
229246. Reidel, Dordrecht, Netherlands.
Wolf, G. (1977). Obere und untere Wahrscheinlichkeiten. Ph.D. thesis, ETH, Zurich.
Zadeh, L. A. (1978). Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems,
1, 328.

492

fiJournal of Artificial Intelligence Research 54 (2015) 277-308

Submitted 12/14; published 10/15

Relations Between Spatial Calculi
About Directions and Orientations
Till Mossakowski

till@iws.cs.uni-magdeburg.de

Otto-von-Guericke-University of Magdeburg,
Faculty of Computer Science
Universittsplatz 2
39106 Magdeburg

Reinhard Moratz

reinhard.moratz@maine.edu

University of Maine,
National Center for Geographic Information and Analysis,
School of Computing and Information Science,
348 Boardman Hall, Orono, 04469 Maine, USA.

Abstract
Qualitative spatial descriptions characterize essential properties of spatial objects or
configurations by relying on relative comparisons rather than measuring. Typically, in
qualitative approaches only relatively coarse distinctions between configurations are made.
Qualitative spatial knowledge can be used to represent incomplete and underdetermined
knowledge in a systematic way. This is especially useful if the task is to describe features
of classes of configurations rather than individual configurations.
Although reasoning with them is generally NP-hard (even IR-complete), relative directions are important because they play a key role in human spatial descriptions and there are
several approaches how to represent them using qualitative methods. In these approaches
directions between spatial locations can be expressed as constraints over infinite domains,
e.g. the Euclidean plane. The theory of relation algebras has been successfully applied
to this field. Viewing relation algebras as universal algebras and applying and modifying
standard tools from universal algebra in this work, we (re)define notions of qualitative constraint calculus, of homomorphism between calculi, and of quotient of calculi.Based on this
method we derive important properties for spatial calculi from corresponding properties of
related calculi. From a conceptual point of view these formal mappings between calculi are
a means to translate between different granularities.

1. Introduction
A qualitative representation of space and/or time provides mechanisms which characterize
the essential properties of objects or configurations. The advantages over quantitative representations can be: (1) a better match with human concepts related to natural language,
and (2) better efficiency for reasoning. The two main trends in qualitative spatial constraint
reasoning (Ligozat, 2011) are topological reasoning about regions (Randell & Cohn, 1989;
Randell, Cui, & Cohn, 1992; Egenhofer & Franzosa, 1991; Renz & Nebel, 1999; Worboys
& Clementini, 2001) and reasoning about directions between points and straight lines and
orientations of straight lines or configurations derived from points (Frank, 1991; Ligozat,
1998; Renz & Mitra, 2004; Freksa, 1992; Clementini, Felice, & Hernandez, 1997; Scivos &
c
2015
AI Access Foundation. All rights reserved.

fiMossakowski & Moratz

Nebel, 2004; Moratz, Lcke, & Mossakowski, 2011; Mossakowski & Moratz, 2012; Dubba,
Bhatt, Dylla, Cohn, & Hogg, 2015).
In constraint-based reasoning about spatial configurations, typically a partial initial
knowledge of a scene is represented in terms of qualitative constraints between spatial objects. Implicit knowledge about spatial relations is then derived by constraint propagation.
Previous research has found that the mathematical notion of a relation algebra and related
notions are well-suited for this kind of reasoning. In particular, in an arbitrary relation algebra, the well-known path consistency algorithm (Montanari, 1974) computes an algebraic
closure of a given constraint network, and this approximates, and in many cases also decides,
consistency of the network in polynomial time. Intelligent backtracking techniques and the
study of maximal tractable subclasses also allow for efficiently deciding networks involving
disjunctions. Starting with Allens temporal interval algebra, this approach has been successfully applied to several qualitative constraint calculi, and is now supported by freely
available toolboxes (Gantner, Westphal, & Wlfl, 2008; Wallgrn, Frommberger, Wolter,
Dylla, & Freksa, 2006). Moreover, people have started to develop benchmark problem libraries (Nebel & Wlfl, 2009) and have shown that this method performs quite well also
when compared to other constraint reasoning techniques (Westphal & Wlfl, 2009).
In this work, we apply universal algebraic tools to qualitative calculi. This connection has
been previous investigated in the literature (Li, Kowalski, Renz, & Li, 2008; Bodirsky, 2008;
Huang, 2012). However, in our paper we deviate from standard universal algebra by using lax
and oplax homomorphisms, which have weaker properties than standard homomorphisms
(and more an order-theoretic than algebraic flavor), but are better suited for transferof
algebraic structure between qualitative calculi such as DRAfp , OPRA1 and CYC b . In this
work, we focus on calculi of binary relations only.

2. Relation Algebras for Spatial Reasoning
Standard methods developed for finite domains generally do not apply to constraint reasoning over infinite domains. The theory of relation algebras (Ladkin & Maddux, 1994;
Maddux, 2006) allows for a purely symbolic treatment of constraint satisfaction problems
involving relations over infinite domains. The corresponding constraint reasoning techniques
were originally introduced by Montanari (1974), applied for temporal reasoning (Allen, 1983)
and later proved to be valuable for spatial reasoning (Renz & Nebel, 1999; Isli & Cohn, 2000).
The central data for a binary calculus is given by:
 a list of (symbolic names for) base-relations, which are interpreted as relations over
some domain, having the crucial JEPD properties of joint exhaustiveness and pairwise disjointness (a general relation is then simply a union of base-relations).
 a table for the computation of the converses of relations.
 a table for the computation of the compositions of relations.
Then, the path consistency algorithm (Montanari, 1974) and backtracking techniques (van
Beek & Manchak, 1996) are the tools used to tackle the problem of consistency of constraint
networks and related problems. These algorithms have been implemented in both generic
reasoning toolboxes GQR (Gantner, Westphal, & Wlfl, 2008) and SparQ (Wallgrn et al.,
278

fiRelations Between Spatial Calculi

2006). To integrate a new calculus into these tools, only a list of base-relations and tables
for compositions and converses (plus a compositional identity, which however is not really
used) need to be provided. Thereby, the qualitative reasoning facilities of these tools become
available for this calculus.1 Since the compositions and converses of general relations can be
reduced to compositions and converses of base-relations, these tables only need to be given
for base-relations. Based on these tables, the tools provide a means to approximate the
consistency of constraint networks, list all their atomic refinements, and more (see Section 4
for some details).
Let b be a base-relation. The converse b` = {(x, y)|(y, x)  b} is often itself a baserelation. Since base-relations generally are not closed under composition, this operation is
approximated by a weak composition:
[
b1  b2 = {b base-relation | (b1  b2 )  b 6= }
where b1  b2 is the usual set theoretic composition
b1  b2 = {(x, z)|y . (x, y)  b1 , (y, z)  b2 }
Composition is said to be strong if b1  b2 = b1  b2 for all base-relations b1 , b2 . Generally,
b1  b2 over-approximates the set-theoretic composition, while a strong composition captures
it exactly.
The mathematical background of composition in table-based reasoning is given by the
theory of relation algebras (Maddux, 2006; Renz & Nebel, 2007). For many calculi, including
the dipole calculus (see Ex. 9 below), a slightly weaker notion is needed, namely that of a
non-associative algebra (Maddux, 2006; Ligozat & Renz, 2004), where associativity has been
dropped. These algebras treat spatial relations as abstract entities (independently of any
domain) that can be combined by certain operations and governed by certain equations.
Definition 1 (Maddux 2006; Ligozat & Renz 2004). A non-associative algebra A is a tuple
A = (A, , , , 0, 1, ,` , ) such that:
1. (A, , , , 0, 1) is a Boolean algebra.  is called join,  meet, 0 bottom, 1 top, and
 relative complement. Note each Boolean algebra carries a partial order defined by
a  b iff a  b = b;
2.  is a constant (called identity relation), ` a unary operation (called converse) and 
a binary operation (called weak composition) such that, for any a, b, c  A:
(a) (a` )` = a
(b)   a = a   = a
(c) a  (b  c) = a  b  a  c
(d) (a  b)` = a`  b` (e) (a  b)` = a`  b` (f ) (a  b)` = b`  a`
(g) (a  b)  c` = 0 if and only if (b  c)  a` = 0

A non-associative algebra is called a relation algebra, if weak composition  is associative.2
1. With more information about a calculus, both of the tools can provide functionality that goes beyond
simple qualitative reasoning for constraint calculi.
2. This terminology is a bit misleading, since relation algebras are associative non-associative algebras.
A more precise name for non-associative algebras would be relation algebras without associativity requirement. Nevertheless, we stick to the terminology established in the literature.

279

fiMossakowski & Moratz

The elements of such an algebra will be called (abstract) relations. We are mainly
interested in finite non-associative algebras that are complete and atomic, which means that
there is a set of pairwise disjoint minimal relations, the atoms, also called base-relations,
and all relations can be obtained as joins of these. Then, the following fact is well-known
and easy to prove:
Proposition 2 (Dntsch, 2005). A complete atomic non-associative algebra is uniquely
determined by its set of base-relations, together with the converses and compositions of baserelations. (Note that the composition of two base-relations is in general not a base-relation.)
When providing examples, it is easier to start with partition schemes:
Definition 3 (Ligozat & Renz, 2004; Mossakowski et al., 2006). Let U be a non-empty set.
A partition scheme on U is defined by a finite (index) set I with a distinguished element
i0  I, a unary operation ` on I, and a family of binary relations (Ri )iI on U such that
1. (Ri )iI is a partition of U  U in the sense that the Ri are pairwise disjoint and jointly
exhaustive.
2. Ri0 is the diagonal relation {(x, x) | x  U }.
3. Ri` is the (set-theoretical) converse of relation Ri , for each i  I.
The relations Ri are referred to as basic relations. In the following we often write
U U =

[

Ri

iI

to denote partition schemes.
Proposition 4 (Ligozat & Renz, 2004; Mossakowski et al., 2006). Given a partition scheme
U U =

[

Ri

iI

we obtain a non-associative algebra as follows: the Boolean algebra component is P(I),
the powerset of I. The converse is given by pointwise application of ` ; the diagonal is i0 .
Composition is given by weak composition as defined above.
We now introduce several qualitative calculi by just giving its domain U and its set of
basic relations; the diagonal and the converse are clear.
Example 5. The most prominent temporal calculus is Allens interval algebra IA (Allen,
1983), which describes possible relations between intervals in linear flows of time3 . An
interval is a pair (s, t) of real numbers such that s < t. The 13 basic relations between such
intervals are depicted in Fig. 1.
3. There is also a spatial interpretation of the Allen calculus in which the intervals are interpreted as
one-dimensional spatial entities

280

fiRelations Between Spatial Calculi

Figure 1: Allens interval relations

Figure 2: CYC b relations. A r B means B is to the right of A.
Example 6. The CYC b calculus (Isli & Cohn, 2000) is based on the domain CYC = { |
 <   } of cyclic orientations. Equivalently, these angles can be represented as
oriented straight lines containing the origin of the 2D Euclidian plane associated with a
reference system. Using this latter representation, Fig. 2 depicts the four base-relations r, l,
o, e (e.g. right, left, opposite, equal) of CYC b .
The converse and composition tables are as follows:
b
e
l
o
r

b`
e
r
o
l


e
l
o
r

e
l
e
l
l {l, o, r}
o
r
r {e, l, r}

281

o
r
o
r
r {e, l, r}
e
l
l {l, o, r}

fiMossakowski & Moratz

Example 7. The OPRAn calculus (Moratz, 2006; Mossakowski & Moratz, 2012) is based
on the domain OP = {(p, ) | p  R2 ,  <   } of oriented points in Euclidean plane.
An oriented point consists of a point and an angle serving as its orientation. The full angle
is divided using n axes, leading to 4n regions, see Fig. 3. If the points of A and B differ, the
0 15 14

1 0

13
12
11

2
3
4
5

13

3

B

A
6

10
9

7 8 9 10

7 8

Figure 3: Two o-points in relation A 4313 B
relation A mji B (i, j  Z4m 4 ) reads like this: given a granularity m, the relative position
of B with respect to A is described by i and the relative position of A with respect to B is
described by j. If the points of A and B coincide, the relation A mi B expresses that the
difference between Bs and As orientations (angles) is in region i.
For the special case of the OPRAn calculus with n = 1 (e.g. OPRA1 ) we have a cognitively motivated symbolic notation in addition to the general notation for OPRAn baserelations introduced above. Fig. 4 depicts an oriented point and the corresponding division
of the plane into the regions Front, Left, Right and Back and Same (the latter stands for the
point itself). The naming schema for the OPRA1 base-relations concatenates the name for
the relative position of the second oriented point w.r.t. the first and then the relative position
of first oriented point w.r.t. the second. Using capitalization of the first part of the relation symbol, the cognitively motivated schema for the relation names leads to these names
for the 16 base-relations of OPRA1 : FRONTfront, FRONTleft, FRONTright, FRONTback, LEFTfront, LEFTleft, LEFTright, LEFTback, RIGHTfront, RIGHTleft, RIGHTright,
RIGHTback, BACKfront, BACKleft, BACKright, and BACKback. Again, if both points
coincide, we compare their orientations. This leads to the relations SAMEfront, SAMEleft,
SAMEright and SAMEback.
SAMEfront is the identity relation. SAMEback is analogous to the opposite relation of
CYC b (see Fig. 2). Also SAMEleft and SAMEright are analogous to the corresponding CYC b
relations.
Example 8. The OPRAm calculus (Dylla, 2008) is similar to OPRAm . Here, we concentrate on OPRA1 . The important extension is a refinement that is applied to the relations
RIGHTright, RIGHTleft, LEFTleft, and LEFTright. These relations are refined by marking
them with letters + or , P or A, according to whether the two orientations of the
oriented points are positive, negative, parallel or anti-parallel, similar as in Fig. 6:
 LEFTleft is refined into LEFTleftA, LEFTleft+ and LEFTleft-.
4. Z4m is the residue ring; for simplicity, we set Z4m = {0, . . . , 4m  1}.

282

fiRelations Between Spatial Calculi

Figure 4: OPRA1 base frame
 LEFT right is refined into LEFTrightP, LEFTright+ and LEFTright-.
 RIGHTright is refined into RIGHTrightA, RIGHTright+ and RIGHTright-.
 RIGHT left is refined into RIGHTleftP, RIGHTleft+ and RIGHTleft-.
The remaining four options LEFTleftP, LEFTrightA, RIGHTrightP and RIGHTleftA are
geometrically impossible. Altogether, we obtain a set of 28 base-relations.
Example 9. A dipole is a pair of distinct points in the Euclidean plane. Before explaining
dipole-dipole relations, we first study dipole-point relations. We distinguish between whether
a point lies to the left, to the right, or at one of five qualitatively different locations on the
straight line that passes through the corresponding dipole (Ligozat, 1993; Scivos & Nebel,
2004). The corresponding regions are shown on the right side of Fig. 5.
Using these seven possible relations between a dipole and a point, the relations between
two dipoles may be specified according to the following conjunction of four relationships:
A R1 sB  A R2 eB  B R3 sA  B R4 eA , 5
where Ri  {l, r, b, s, i, e, f} with 1  i  4. The formal combination gives us 2401 relations,
out of which 72 relations are geometrically possible. These constitute the DRAf calculus
(Moratz, Renz, & Wolter, 2000; Moratz, Lcke, & Mossakowski, 2011). For example, in
Fig. 5, the relation A lrrr B holds.

B
sB

f
eA

eB

e
r

l

A

i
s

sA

b

A l r rr B

Figure 5: Orientation between two dipoles based on four dipole-point relations
Fig. 6 shows a refinement of DRAf , called DRAfp , with additional distinguishing features due to parallelism. For the relations different from rrrr, llrr rrll and llll, a +,
, P or A is already determined by the original base-relation and does not have to be
mentioned explicitly. These base-relations then have the same relation symbol as in DRAf .
This leads to a set of 80 DRAfp base-relations. The relation sese is the identity relation.
We denote the resulting non-associative algebra by DRAfp .
5. Note that e.g. A r sB reads sB is to the right of A.

283

fiMossakowski & Moratz

A rrrr B

rrrrA

rrrr

rrrr+

A rrll B

rrllP

rrll

rrll+

A llll B

llllA

llll

llll+

A llrr B

llrrP

llrr

llrr+

Figure 6: Refined base-relations in DRAfp . The solid arrow denotes A, the dashed arrow
denotes B.

3. Homomorphisms and Weak Representations
The presented calculi offer the possibility to describe scenes on different levels of granularity.
The granularity of a description is the context-dependent selection of an adequate level of
detail in the description (Hobbs, 1985). Granularity plays a key role in human strategies
to deal with the complexity of the spatial features of the real world. This is demonstrated
nicely by an example from Hobbs (1985). In his example he points out that humans conceptualize streets as one-dimensional entities when they plan a trip, they use a two-dimensional
conception when they cross a street. And in contexts where the pavement has to be dug
up the street becomes a three-dimensional volume. The key importance of mechanisms to
flexibly switch and translate between granularities for successful reasoning about the world
is highlighted by the following quote from Hobbs (1985, p. 432):
Our ability to conceptualize the world at different granularities and to switch
among these granularities is fundamental to our intelligence and flexibility. It
enables us to map the complexities of the world around us into simple theories
that are computationally tractable to reason in.
Imagine a scenario involving ships and their relative positions in the open sea (see Fig. 7).
Ships can be modelled as elongated, directed entities neglecting their width or any other
shape property. The resulting DRAfp representation uses a single dipole for each ship to
284

fiRelations Between Spatial Calculi

be represented (see left part of Fig. 7). In the OPRA1 representation in addition even the
lengths of the ships are neglected (see middle part of Fig. 7). The CYC b representation abstracts away the different locations of the ships and only focuses on their relative orientation
(see right part of Fig. 7).

abstraction from shape

abstraction from length

abstraction from location

Figure 7: Modelling relative ship directions on different levels of granularity with DRAfp ,
OPRA1 , and CYC b .

In another example ships are represented with DRAfp in such a way that the start point
corresponds to the position of the ship and the end point represents its current speed. More
specifically, the end point denotes the future position after one minute travel (if speed and
heading were constant). Then longer arrows represent faster ships in a diagram. When we
have an alternative representation in OPRA1 , in this representation we might only focus
on location and heading of the ships and abstract away from the their speed. Then several
DRAfp relations in one representation map onto a single OPRA1 relation in the alternative
representation. For example the three relations {flll, ells, illr} are mapped to FRONTleft
(see Fig. 8).

Figure 8: In a quotient homomorphism between DRAfp and OPRA1 the three relations
{flll, ells, illr} are mapped to FRONTleft.

If different spatial calculi can be used to represent a given spatial situation at different
levels of granularity, the relation between the calculi can typically be formalized as a quotient
homomorphism. Figure 8 exemplifies the action of a quotient homomorphism. Homomorphisms also arise in other contexts, e.g. as embeddings of a smaller calculus into a larger
285

fiMossakowski & Moratz

one (for example, Allens interval algebra can be embedded into DRAfp , see Proposition 25
below).
We now study homomorphisms in general. They are a means for the examination of
relationships among calculi. Often, conceptual relations between different calculi and their
domains can be formalised as homomorphism, and vice versa, if one has found a homomorphism, then often there is also some conceptual relation behind it.
Homomorphisms can also be used to transfer properties (like strength of composition, or
algebraic closure deciding consistency) from one calculus to another one, see Propositions
16, 19, 23, 39, 40, 44, 46, 47 and 48 below. Using homomorphisms, it is also possible to find
errors in composition tables (we discovered errors in 197 entries of the composition table of
OPRA1 , see Example 38 below).
Homomorphisms have been studied by Ligozat and Renz (2004) and Ligozat (2005, 2011)
(mainly under the name of representations). We here introduce a more systematic treatment
of homomorphisms. For non-associative algebras, we recall and refine the weaker notion of
lax homomorphisms, which allow for both the embedding of a calculus into its domain, as
well as relating several calculi to each other.
Definition 10 (Lax homomorphism, Moratz et al., 2009; Lcke, 2012). Given non-associative
algebras A and B, a lax homomorphism is a homomorphism h : A  B on the underlying
Boolean algebras such that:
 h(A )  B
 h(a` ) = h(a)` for all a  A
 h(a  b)  h(a)  h(b) for all a, b  A
A lax homomorphism between complete atomic non-associative algebras is called semistrong (Mossakowski, Schrder, & Wlfl, 2006) if for atoms a, b
ab=

_
{c | (h(a)  h(b))  h(c) 6= 0}

This notion has been inspired by the definition of weak composition and will be used for
representation homomorphisms of qualitative calculi.
Dually to lax homomorphisms, we can define oplax homomorphisms6 , which enable us
to define projections from one calculus to another.
Definition 11 (Oplax homomorphism, Moratz et al., 2009; Lcke, 2012). Given nonassociative algebras A and B, an oplax homomorphism is a homomorphism h : A  B
on the underlying Boolean algebras such that:
 h(A )  B
 h(a` ) = h(a)` for all a  A
 h(a  b)  h(a)  h(b) for all a, b  A
6. The terminology is motivated by that for monoidal functors.

286

fiRelations Between Spatial Calculi

For quotients, we now introduce a strengthening of the notion of oplax homomorphism.
A full 7 homomorphism is an oplax homomorphism for which even
_
h(c  d)
h(a)  h(b) =
h(a)=h(c),h(b)=h(d)

A proper homomorphism (sometimes just called a homomorphism) of non-associative
algebras is a homomorphism that is lax and oplax at the same time; the above inequalities
then turn into equations. Each proper homomorphism is also full. A proper injective
homomorphism is also semi-strong.
A homomorphism between complete atomic non-associative algebras can be given by its
action on base-relations; it is extended to general relations by
_
_
h( bi ) =
h(bi ),
iI

iI

W
where is arbitrary (possibly infinite) join. In the sequel, we will always define homomorphisms in this way.
While semi-strong lax homomorphisms can be used to transfer the composition from the
target to the source algebra, surjective full oplax homomorphisms can be used for a transfer
in the opposite direction. We now study the latter, the former will be treated in Def. 20.
Definition 12. Given a complete atomic non-associative algebra A and an equivalence
relation  on the atoms of A that is a congruence for _` , we define the quotient algebra
A/A to have equivalence classes of A-atoms as atoms. General relations are then sets of
such atoms. We further define for atoms a, b:
A/ = {[a] | a  A }
[a]` = [a` ]
[a]  [b] = {[c] | c  a0  b0 , a0  a, b0  b}
where as usual we treat general relations as sets of atoms; hence general relations in A/A
are sets of equivalence classes of A-atoms.
Unfortunately, in general, A/A will not be a non-associative algebra again:
Example 13. Consider the relation algebra of the CYC b calculus (Example 6) and the
equivalence relation generated by o  e. The quotient algebra fails to satisfy the identity
laws (laws (b) in Def. 1). This can be seen from the quotient composition table:

{e, o}
{l}
{r}

{e, o}
{l}
{r}
{e, o}
{{l}, {r}}
{{l}, {r}}
{{l}, {r}} {{e, o}, {l}, {r}} {{e, o}, {l}, {r}}
{{l}, {r}} {{e, o}, {l}, {r}} {{e, o}, {l}, {r}}

7. This terminology is borrowed from the theory of partial algebras (Burmeister, 1986). Burmeister (2002,
p. 101) puts it as follows: f is full iff f fully induces the structure on its direct image f (A).  which
is exactly what he want here, too.

287

fiMossakowski & Moratz

We will study a method to prove that A/A is a non-associative algebra later under
additional conditions. For now, it is straightforward to prove:
Proposition 14. If the algebra A/A defined in Def. 12 is a non-associative algebra, the
homomorphism q : A  A/A given by a 7 [a] is surjective and full.
This naturally leads to:
Definition 15. An oplax homomorphism of non-associative algebras is said to be a quotient
homomorphism if it is full and surjective.
An easy standard result from universal algebra (Grtzer, 1979) gives us:
Proposition 16. Proper quotient homomorphisms preserve the holding of equations, in
particular, associativity.
However, non-proper quotient homomorphisms in general do not preserve the holding of
equations. See Example 35: DRAfp is associative, but its quotient DRAf is not.
This raises the question why we do not use the standard constructions and results of
universal algebra (Grtzer, 1979; Maddux, 2006), where homomorphisms are always proper
and hence quotients preserve equations (Prop. 16) and thus the quotient of a non-associative
algebra is a non-associative algebra again. The reason is the following:
Example 17. Consider the point algebra induced by the three base-relations <, = and >,
with converse and composition tables:
a
<
=
>

a`
>
=
<


<
=
>

<
<
<
{<, =, >}

=
>
< {<, =, >}
=
>
>
>

Let  be the standard algebraic congruence relation generated by <>. Then < is equal
to <  <, which is congruent to <  >, which is {<, =, >}. Similarly, > is congruent to
{<, =, >}. Since congruence respects meet, we obtain that <  >, which is , is congruent
to {<, =, >}. This means that the congruence is trivial and the standard algebraic quotient
is the trivial one-point relation algebra.
By contrast, with our notion of quotient, we obtain the following relation algebra, which
is the expected one (we denote the equivalence class {<, >} by 6=):
a
6
=
=

a`
6=
=


6
=
=

6
=
=
{6=, =} 6=
6=
=

The corresponding quotient homomorphism is not proper: q(<)  q(<) is 6=  =
6 , which is
{6=, =}, but q(<  <) = q(<), which is 6=. However, by Prop. 14, it is surjective and full.
Proposition 18. In the context of Prop. 14, if q is proper, then A/A is a non-associative
algebra.
288

fiRelations Between Spatial Calculi

Proof. By Prop. 16, we know that equations are preserved by q. The only axiom in Def. 1
not in equational form is (g). Now Tarski has shown (Maddux, 2006) that (g) is equivalent
to
(a`  (1  (a  b)))  (1  b) = 1  b

An important application of quotients and quotient homomorphisms lies in the following
fact:
Proposition 19. Given a quotient homomorphism q : A  B, Bs converse and composition tables can be computed from those for A, using q.
Proof. Use the formulas for converse resp. composition from the definition of full homomorphism. Since q is surjective, the formulas work for all elements of B.
Another important application of homomorphisms is their use in the definition of a
qualitative calculus. Ligozat and Renz (2004) define a qualitative calculus in terms of a
so-called weak representation (Ligozat, 2005, 2011):
Definition 20 (Weak representation). A weak representation  : A  P(U  U)8 is an
identity-preserving (i.e. (A ) = B ) and converse-preserving lax homomorphism  from
a complete atomic non-associative algebra A into the relation algebra of a domain U. The
latter is given by the canonical relation algebra on the powerset P(U  U), where identity,
converse and composition (as well as the Boolean algebra operations) are given by their
set-theoretic interpretations. A weak representation is semi-strong if  is semi-strong. It is
strong, if  is strong.
Example 21. Let D = {(s, e) | s, e  R2 , s 6= e} be the set of all dipoles in R2 . Then the
weak representation of DRAfp is the lax homomorphism f : DRAfp  P(D  D) given by
f (b) = b.
Here, the b on the left hand-side of the equation is an element of the abstract relation
algebra, while the b on the right hand-side is the set-theoretic extension as a relation. Since
we have chosen to use set-theoretic relations themselves as elements of the relation algebra,
here both are the same.
This can be generalized as follows:
Proposition 22. Semi-strong representations and partition schemes are in one-one correspondence.
Proof. Given a partition scheme, bySProp. 4, we obtain a non-associative algebra. Let 
map each general relation R  P(I) to iR Ri . The definition of weak composition ensures
that  is a semi-strong lax homomorphism. Conversely, given a semi-strong representation
 : A  P(U  U), define a partition scheme on the atoms of A by putting Ra := (a).
8. Note that the domain and codomain are part of the weak representation.

289

fiMossakowski & Moratz

Preservation of top, bottom and meet by  ensure the JEPD property. Moreover, by semistrength, composition in A is just weak composition according to the partition scheme. It
is clear that these constructions are inverses of each other.
The following propositions are straightforward.
Proposition 23 (Moratz et al., 2009; Lcke, 2012). A calculus has strong composition if
and only if its weak representation is a proper homomorphism.
Proposition 24 (Ligozat, 2005). A weak representation  is injective if and only if (b) 6= 
for each base-relation b.
A first sample use of homomorphism is the embedding of Allens interval relations (Allen,
1983) into DRAfp via a homomorphism.
Proposition 25 (Moratz et al., 2011). A proper homomorphism from Allens interval algebra
to DRAfp exists and is given by the following mapping of base-relations.
equals
before
meets
overlaps
during
starts
finishes

7
7

7

7

7
7

7


sese
ffbb
efbs
ifbi
bfii
sfsi
beie

before `
meets `
overlaps `
during `
starts `
finishes `

7
7

7

7

7

7


bbff
bsef
biif
iibf
sisf
iebe

When studying quotients of calculi, it is natural to consider homomorphisms of weak
representations. We refine the notion by Moratz et al. (2009) and Lcke (2012) in order to
fit it better to the examples:
Definition 26. Given weak representations  : A  P(U  U) and  : B  P(V  V),
a  {lax, oplax, full, proper} and b  {lax, oplax, proper}, an (a,b)-homomorphism of weak
representations (h, i) :    is given by
 an a-homomorphism of non-associative algebras h : A  B, and
 a map i : U  V, such that the diagram
A



P(U  U)

P(i  i)

h

B

P(V  V)



290

fiRelations Between Spatial Calculi

commutes according to b. Here, lax commutation means that for all R  A, (h(R)) 
P(ii)((R)), oplax commutation means the same with , and proper commutation with =.
Note that P(ii) is the obvious extension of i to a function between relation algebras; further
note that (unless i is bijective) this is not even a homomorphism of Boolean algebras (it
may fail to preserve top, intersections and complements), although it satisfies the oplaxness
property (and the laxness property if i is injective)9 .
Ligozat (2005) defines a more special notion of morphism between weak representations;
it corresponds to our notion of (proper,oplax) homomorphism of weak representations where
the component h is the identity.
Example 27. The homomorphism from Prop. 25 can be extended to a (proper, proper)
homomorphism of weak representations by letting i be the embedding of time intervals to
dipoles on the x-axis.
Definition 28. A quotient homomorphism of weak representations is a (full,oplax) homomorphism of weak representations that is surjective in both components.
We also refine the construction of a weak representation from an equivalence relation on
the domain introduced by Moratz et al. (2009) and Lcke (2012), whose constructions in
typical cases will produce a trivial one-point quotient, cf. Example 17.
Definition 29. Given a weak representation  : A  P(U  U) and an equivalence relation
 on U that is a congruence for _` , we obtain the quotient representation / as follows:
A



P(U  U)

qA

A/A

P(q  q)

P(U/  U/)

/

 Let q : U  U/ be the set-theoretic factorization of U by ;
 q extends to relations: P(q  q) : P(U  U)  P(U/  U/);
 let A be the equivalence relation on the atoms of A generated by
P(q  q)((b1 ))  P(q  q)((b2 )) 6=   b1 A b2
for base-relations b1 , b2  A;
 let qA : A  A/A be the quotient of A by A in the sense of Def. 12;
9. The reader with background in category theory may notice that the categorically more natural formulation would use the contravariant powerset functor, which yields homomorphisms of Boolean algebras
(Mossakowski et al., 2006). However, the present formulation fits better with the examples.

291

fiMossakowski & Moratz

 finally, the function / is defined as
1
/(R) = P(q  q)((qA
(R))).

 is called regular w.r.t.  if A is the kernel of P(q  q)   (i.e. the set of all pairs
made equal by P(q  q)  ). In this case, each base-relation b  A already generates (via
P(q  q)  ) the full relation of the equivalence class [b]  A/A .
Proposition 30. Let a strong representation  : A  P(U  U) of a complete atomic nonassociative algebra A and an equivalence relation  on U be given, such that 
1. is identity-regular, that is ((a)  ) 6=  implies (a)
2. is a congruence for converse, and
3. enjoys the following fill-in property: if u(a)x and u  y, then there exist a0  a and
z  x with
u

(a)


y

x


(a0 )

z

Then A/A as defined in Def. 29 is a non-associative algebra, qA : A  A/A is a quotient
homomorphism, and / is a semi-strong lax homomorphism of non-associative algebras.
Proof. We use the atoms of A/A to define a partition scheme b  At(A/A ) 7 /(b).
Note that we know that A/A is a Boolean algebra (although we do not know yet that it
is a non-associative algebra). It is straightforward to show that / preserves bottom and
joins; since q is surjective, also top is preserved. Concerning meets, since general relations
in A/A can be considered to be sets of base-relations, it suffices to show that b1  b2 = 0
1
1
(b2 ))) = . Assume to the contrary that P(q 
(b1 )))  P(q  q)((qA
implies P(q  q)((qA
1
1
q)((qA (b1 )))P(qq)((qA (b2 ))) 6= . Then already P(qq)((b01 ))P(qq)((b02 )) 6= 
1
for base-relations b0i  qA
(bi ), i = 1, 2. But then b01 A b02 , hence qA (b01 ) = qA (b02 )  b1  b2 ,
contradicting b1  b2 = 0. From these preservation properties, the JEPD property follows.
By identity-regularity and converse being a congruence, the condition of a partition scheme
on identity and converse are fulfilled.
By Prop. 22, we obtain a semi-strong representation / : B  P(U/  U/). In
order to show that A/A is a non-associative algebra, we show that A/A = B. We already
know that they have the same atoms and thus agree as complete atomic Boolean algebras.
We show agreement on the remaining operations:
Identity: Since  is identity-preserving, A is atomic. By identity-regularity, B = [A ] =
A/A .
Converse: Since  is a congruence for converse, for an atomic relation a  A, [a]`B =
[a`A ] = [a]`A/A .
292

fiRelations Between Spatial Calculi

Composition: Given atomic relations a, b  A, we have [c]  [a] B [b] iff (by definition of
weak composition) there exist x, y, z with [x] /(a) [y] /(b) [z] and [x] /(c) [z]
iff (by definition of /) there exist x1 , x2 , y1 , x2 , z1 , z2 and a0  a, b0  b, c0  c with
x1 (a0 ) y1



y2 (b0 ) z1




(c0 )

x2

z2

By the fill-in property, this is equivalent to (implicitly quantifying variables existentially and omitting a0  a, b0  b, c0  c):
x1 (a0 )

y

(b0 ) z1




(c0 )

x2

z2

By strength of , this is equivalent to
x1 (a0  b0 ) z1

x2


(c0 )

z2

which in turn is equivalent to c0  c00  (a0  b0 ) (for some c00 and a0  a, b0  b, c0  c).
Now this is equivalent to [c]  [a] A/A [b].
This completes the proof that A/A is a non-associative algebra. By Prop. 14, qA : A 
A/A is a quotient homomorphism, and by Prop. 22, / is a semi-strong lax homomorphism.
An interesting open question is whether Prop. 30 also holds for semi-strong representations. We conjecture that the answer is positive. Note that / can fail to be strong even
if  is (consider the quotient DRAf of DRAfp introduced in Example 35).
Example 31. CYC b is a quotient of OPRA1 . At the level of domains, it acts as follows:
an oriented point (p, ) is mapped to the orientation  (the point p is forgotten). At the
level of non-associative algebras, the quotient is given by the table in Fig. 9.
Proposition 32. Under the conditions of Prop. 30, (qA , q) :   / is a (full, oplax)
quotient homomorphism of semi-strong representations. If  is regular w.r.t. , then (qA , q)
is (full,proper) and satisfies the following universal property: if (qB : A  B, i : U 
V) :    is another (full,proper) homomorphism of weak representations with  injective
and   ker (i), then there is a unique (full,proper) homomorphism of weak representations
(h, k) : /   with (qB , i) = (h, k)  (qA , q).
293

fiMossakowski & Moratz

{LEFTleftA, FRONTfront, BACKback, RIGHTrightA, SAMEback} 7 o
{LEFTleft+, LEFTback, LEFTright+, RIGHTright+, RIGHTleft+,
RIGHTfront, FRONTleft, BACKright, SAMEleft} 7 l
{LEFTleft, LEFTfront, LEFTright, RIGHTright, RIGHTleft,
RIGHTback, FRONTright, BACKleft, SAMEright} 7 r
{LEFTrightP, RIGHTleftP, FRONTback, BACKfront, SAMEfront} 7 e

Figure 9: Mapping from OPRA1 to CYC b relations
Proof. The (full,_) property10 follows from Prop. 14. The (_,oplax) property for (qA , q)
is P(q  q)    /  qA , which by definition of / amounts to
1
 qA ,
P(q  q)    P(q  q)    qA

which follows from surjectivity of q. Regularity of  w.r.t.  means that A is the kernel of P(q  q)  , which turns the above inequation into an equality; hence we obtain
(_,properness). Concerning the universal property, let (qB , i) :    with the mentioned
properties be given. Since   ker (i), there is a unique function k : U/  V with i = k  q.
The homomorphism h we are looking for is determined uniquely by h(qA (b)) = qB (b); this
also ensures the (full,proper) homomorphism property. All that remains to be shown is
well-definedness. Suppose that b1 A b2 . By regularity, P(q  q)((b1 )) = P(q  q)((b2 )).
Hence also P(i  i)((b1 )) = P(i  i)((b2 )) and (qB (b1 )) = (qB (b2 )). By injectivity of
, we get qB (b1 ) = qB (b2 ).
Example 33. The equivalence relation of the quotient in Ex. 31 is regular, and consequently,
the quotient of weak representations is (full,proper), cf. Fig. 10 illustrating this for the
relation RIGHTright+.
So far, we have studied quotients arising from quotienting the domain. There are also
quotients leaving the domain intact and just identifying certain base-relations.
Proposition 34. Let  : A  P(U U) be a semi-strong representation of a complete atomic
non-associative algebra A and A 11 be an equivalence relation on the atoms (base-relations)
of A that does not relate  with any other relation and that is a congruence for _` . This
leads to a (full, oplax) quotient of the weak representation as follows:
10. We write _ as a placeholder for dont care, i.e. (full,_) only refers to fullness of the first component.
11. Note that in contrast to Def. 29, where A is constructed, here A is a parameter that can be chosen.

294

fiRelations Between Spatial Calculi

Figure 10: The OPRA1 relation RIGHTright+ generates all possible angles for the CYC b
relation r.

A



P(U  U)

qA

A/A

P(id  id)

P(U  U)

/

Proof. Let qA : A  A/A be defined as in Def. 12. Then / can be defined similarly
as in Def. 29 (where q = id). From this, we get a semi-strong representation / : B 
P(U  U) as in Prop. 30. The proof of A/A = B parallels that in Prop. 30 for the
Boolean algebra structure and for converse. For identity, we use the assumption about ,
which implies that A/A = {A }. Concerning composition, we need semi-strength only:
[c]  [a] B [b] iff there exist x, y, z with x /(a) y /(b) z and x/(c)z iff there exist
x, y, z and a0  a, b0  b, c0  c with x a0 y b0 z and x c0 z iff (by semi-strength) there exist
a0  a, b0  b, c0  c with c0  a0  b0 iff [c]  [a] A/A [b].
The (full, oplax)-property follows from Prop. 14 and as in Prop. 32.
Example 35. DRAf (as a semi-strong representation) is a quotient of DRAfp . It is obtained by forgetting the labels +, -, P and A.
Example 36. OPRAn is a quotient of OPRAnm , which is the identity at the domain
level. At the level of non-associative algebras, qA maps region i  m in OPRAnm to region
i in OPRAn (for even i), and regions (i  1)  m + 1 to (i + 1)  m  1 in OPRAnm
to region i in OPRAn (for odd i), see Fig. 11. This is canonically extended the OPRA
relations. The equivalence relation A is the kernel of qA , i.e. it relates i  m only to
itself, while all elements from (i  1)  m + 1 to (i + 1)  m  1 are related to each other.
295

fiMossakowski & Moratz

Note that this yields an oplax homomorphism of non-associative algebra that is not lax. A
counterexample to laxness of OPRA2  OPRA1 is the following: h(200  212 ) = {113 },
but h(200 )  h(212 ) = {113 , 123 , 133 }.
(i-1)m

i-1
i

(i-1)m+1...(i+1)m-1
(i+1)m

i+1

Figure 11: OPRAn is a quotient of OPRAmn
In (Dylla, Mossakowski, Schneider, & Wolter, 2013), we show that OPRA1 to OPRA8
are not associative. By Prop. 16 and Ex. 36, this carries over to any OPRAn .
Example 37. OPRA1 is a quotient of OPRA1 . It is the identity at the domain level. At
the level of non-associative algebras, it forgets the labels +, -, P and A.

296

fiRelations Between Spatial Calculi

{llllA} 7 LEFTleftA
{llll+, lllr, lllb} 7 LEFTleft+
{llll, lrll, lbll} 7 LEFTleft
{ffff, eses, fefe, fifi, ibib, fbii, fsei, ebis, iifb, eifs, iseb} 7 FRONTfront
{bbbb} 7 BACKback
{llbr} 7 LEFTback
{llfl, lril, lsel} 7 LEFTfront
{llrrP} 7 LEFTrightP
{llrr+} 7 LEFTright+
{llrf, llrl, llrr, lfrr, lrrr, lere, lirl, lrri, lrrl} 7 LEFTright
{rrrrA} 7 RIGHTrightA
{rrrr+, rbrr, rlrr} 7 RIGHTright+
{rrrr, rrrl, rrrb} 7 RIGHTright
{rrllP} 7 RIGHTleftP
{rrll+, rrlr, rrlf, rlll, rfll, rllr, rele, rlli, rilr} 7 RIGHTleft+
{rrll} 7 RIGHTleft
{rrbl} 7 RIGHTback
{rrfr, rser, rlir} 7 RIGHTfront
{ffbb, efbs, ifbi, iibf, iebe} 7 FRONTback
{frrr, errs, irrl} 7 FRONTright
{flll, ells, illr} 7 FRONTleft
{blrr} 7 BACKright
{brll} 7 BACKleft
{bbff, bfii, beie, bsef, biif} 7 BACKfront
{slsr} 7 SAMEleft
{sese, sfsi, sisf} 7 SAMEfront
{sbsb} 7 SAMEback
{srsl} 7 SAMEright

Figure 12: Mapping from DRAfp to OPRA1 relations
Example 38 (refined from Moratz et al. 2009; Lcke 2012). OPRA1 is a quotient of DRAfp .
At the level of non-associative algebras, the quotient is given by the table in Fig. 12. At
the level of domains, it acts as follows: Given dipoles d1 , d2  D, the relation d1  d2
expresses that d1 and d2 have the same start point and point in the same direction. (This
is regular w.r.t. f .) Then D/ is the domain OP of oriented points in R2 . See Fig. 14.
297

fiMossakowski & Moratz

The equivalence relation of this quotient is indeed regular: given a base-relation b  DRAfp ,
(b) already generates (via the quotient) the whole of /([b]): for any pair of oriented
points in /([b]), a suitable choice of dipole end points leads to a relation in (b) (also cf.
Fig. 8). Consequently, the quotient of weak representations is (full,proper).
By Prop. 19, the construction of OPRA1 as a quotient allows us the computation of
the converse and composition tables by applying the congruence relations to the tables for
DRAfp . Actually, we have compared the result of this procedure with the composition
table for OPRA1 published by Dylla (2008) and provided with the tool SparQ (Wallgrn,
Frommberger, Dylla, & Wolter, 2009). In the course of checking the full oplaxness property
of the quotient homomorphism from DRAfp to OPRA1 , we discovered errors in 197 entries
of the composition table of OPRA1 as it was shipped with the qualitative reasoner SparQ.12
The table has been corrected accordingly in the meantime.13
For example, the composition of SAMEright= q(srsl) and RIGHTright+= q(rrrr+, rbrr,
rlrr) can be computed as q({blrr, lere, lfrr, lirl, llrf, llrl, llrr+, llrr-, llrrp, lrri,
lrrl, lrrr, rbrr, rlrr, rrrr+}) = {LEFTright-, LEFTright+, LEFTrightP, BACKright,
RIGHTright+}. Now the old table additionally contained RIGHTright-. However, the
configuration a SAMEright b, b RIGHTright+ c and a RIGHTright- c is geometrically
not possible. Consider three oriented points oA , oB and oC with oA SAMEright oB and

o
C

o
A

o
B

Figure 13: OPRA1 configuration
oB RIGHTright+ oC , as depicted in Fig. 13. In the picture, oA RIGHTright+ oC . For the
relation oA RIGHTright- oC to hold, oC would need to be turned counter-clockwise. But
turn would lead to first oB RIGHTrightA oC and then oB RIGHTright- oC , even before
oA RIGHTright- oC is reached.
The next result shows that we also can use quotients to transfer an important property
of calculi.
Proposition 39 (refined from Moratz et al. 2009; Lcke 2012). Quotient homomorphisms
of weak representations that are bijective in the second component preserve strength of composition.
12. This has already been reported (Moratz et al., 2009; Lcke, 2012). While the actual computation of the
table was done with the same congruence relation as here, the quotient construction was wrong, resulting
in a one-point algebra, as stated above.
13. See https://github.com/dwolter/SparQ/commit/89bebfc60a and https://github.com/dwolter/
SparQ/commit/dad260edd9.

298

fiRelations Between Spatial Calculi

DRAfp

OPRA?1

fp

opra 1

P(D  D)

P(OP  OP)



Figure 14: Quotient homomorphism of weak representations from DRAfp to OPRA1
Proof.
Let (h, i) :    with  : A  P(U  U) and  : B  P(V  V) be a quotient
homomorphism of weak representations such that i is bijective. According to Prop. 23, the
strength of the composition is equivalent to  (respectively ) being a proper homomorphism. We assume that  is a proper homomorphism and need to show that  is proper
as well. We also know that h and P(i  i) are proper. Let R2 , S2 be two abstract relations
in B. By surjectivity of h, there are abstract relations R1 , S1  A with h(R1 ) = R2 and
h(S1 ) = S2 . Now (R2  S2 ) = (h(R1 )  h(S1 )) = (h(R1  S1 )) = P(i  i)((R1  S1 )) =
P(i  i)((R1 ))  P(i  i)((S1 )) = (h(R1 ))  (h(S1 )) = (R2 )  (S2 ), hence  is
proper.
Corollary 40 (Moratz et al. 2009; Lcke 2012). Composition in OPRA1 is strong.
Proof.
Composition in DRAfp is known to be strong (Moratz, Lcke, & Mossakowski,
2011). By Example 38 and Prop. 39, the strength of composition carries over to OPRA1 .
Corollary 41. Composition in CYC b is strong.
Example 42. The quotient homomorphism of Example 31 has a one-sided inverse, namely
the embedding (i.e. a proper injective homomorphism) of CYC b into OPRA1 At the level
of non-associative algebras, the quotient is given by the table in Fig. 15. At the level of
domains, it acts as follows: An orientation is mapped to the oriented point at (0, 0) with
that orientation.
Altogether, we get the diagram of calculi (semi-strong representations) and homomorphisms in Fig. 16.

4. Constraint Reasoning
Let us now apply the relation-algebraic method to constraint reasoning. Given a nonassociative algebra A, a constraint network is a map  : N  N  A, where N is a set of
nodes (or variables) (Ligozat & Renz, 2004). Individual constraints (X, Y ) = R are written
as X R Y , where X, Y are variables in N and R is a relation in A. A constraint network
 : N  N  A is atomic or a scenario, if each (X, Y ) is a base-relation.
299

fiMossakowski & Moratz

o 7 SAMEback
l 7 SAMEleft
r 7 SAMEright
e 7 SAMEfront

Figure 15: Mapping from CYC b to OPRA1 relations
IA

(proper,proper)

DRAfp

(full,proper)

(proper,proper)

OPRA?1

CYC b
(proper, oplax)

(full,oplax)

DRAf

(full,oplax)

(full,proper)

OPRA1

(full,oplax)

OPRAnm

(full,oplax)

OPRAn

Figure 16: Homomorphisms among various calculi.
A constraint network  is consistent if there is an assignment of all variables of  with
elements in the domain such that all constraints are satisfied (a solution). This problem is
a Constraint Satisfaction Problem (CSP) (Mackworth, 1977). We rely on relation algebraic
methods to check consistency, namely the above mentioned path consistency algorithm. For
non-associative algebras, the abstract composition of relations need not coincide with the
(associative) set-theoretic composition. Hence, in this case, the standard path-consistency
algorithm does not necessarily lead to path consistent networks, but only to algebraic closure
(Renz & Ligozat, 2005):
Definition 43 (Algebraic Closure). A constraint network over binary relations is called
algebraically closed if for all variables X1 , X2 , X3 and all relations R1 , R2 , R3 the constraint
300

fiRelations Between Spatial Calculi

relations
X1 R1 X2 ,

X2 R2 X3 ,

X1 R3 X3

imply
R 3  R1  R2 .
Algebraic closure can be enforced by successively applying
R3 := R3  (R1  R2 )
for X1 R1 X2 , X2 R2 X3 , X1 R3 X3 until a fixed point is reached. Note that this procedure
leaves the set of solutions of the constraint network invariant. This means that if the
algebraic closure contains the empty relation, the original network is inconsistent.14
However, in general, algebraic closure is only a one-sided approximation of consistency:
if algebraic closure detects an inconsistency, then we are sure that the constraint network
is inconsistent; however, algebraic closure may fail to detect some inconsistencies: an algebraically closed network is not necessarily consistent. For some calculi, like Allens interval
algebra, algebraic closure is known to exactly decide consistency of scenarios, for others it
does not (Renz & Ligozat, 2005). It is also shown that this question is completely orthogonal
to the question whether the composition is strong.
Constraint networks can be translated along homomorphisms of non-associative algebras
as follows: Given h : A  B and  : N  N  A, let h() be the composition h  . It
turns out that oplax homomorphisms preserve algebraic closure.
Proposition 44 (refined from Moratz et al. 2009; Lcke 2012). Given non-associative algebras A and B, an oplax homomorphism h : A  B preserves algebraic closure. An injective
lax homomorphism reflects algebraic closure.
Proof. Since an oplax homomorphism is a homomorphism between Boolean algebras, it
preserves the order. So for any three relations for X1 R1 X2 , X2 R2 X3 , X1 R3 X3 in the
algebraically closed constraint network over A, with
R3  R1  R2
preservation of the order implies:
h(R3 )  h(R1  R2 ).
Applying the oplaxness property yields:
h(R3 )  h(R1 )  h(R2 ).
and hence the image of the constraint network under h is also algebraically closed. If h is
injective and lax, it reflects equations and inequalities, and the converse implication follows
in a similar way.
14. For scenarios, it suffices to check whether the scenario is algebraically closed, because any proper refinement must contain the empty relation.

301

fiMossakowski & Moratz

Given a scenario  : N  N  A, following Renz and Ligozat (2005), we can reorganize
it as a function  : A  P(N  N ) by defining (b) = {(X, Y )  N  N | (X, Y ) = b}
for base-relations b and extending this to all relations using joins as usual. Note that  is a
weak representation iff the scenario is algebraically closed and normalised. Here, a constraint
network is normalised if (X, X) =  and (Y, X) = (X, Y )` .
For atomic homomorphisms (i.e. those mapping atoms to atoms), the translation of
constraint networks can be lifted to scenarios represented as  : A  P(N  N ) using the
above correspondence, we then obtain h() : B  P(N  N ).
Definition 45. Given a scenario  : A  P(N N ), a solution for  in a weak representation
 : A  P(U  U) is a function j : N  U such that for all R  A, P(j  j)((R))  (R),
or P(j  j)     for short:
P(j  j)

P(N  N )





P(U  U)



A
Proposition 46 (refined from Moratz et al. 2009; Lcke 2012). (_,oplax) homomorphisms
of weak representations preserve solutions for scenarios.
Proof.
Let weak representations  : A  P(U  U) and  : B  P(V  V) and an
(_,oplax) homomorphism of weak representations (h, i) :    be given.
A given solution j : N  U for  in  is defined by P(j  j)    . From this and the
oplax commutation property P(i  i)      h we infer P(i  j  i  j)      h, which
implies that i  j is a solution for h().
An important question for a calculus (= weak representation) is whether algebraic closure
decides consistency of scenarios (Renz & Ligozat, 2005). (Note that in general, any consistent
scenario is algebraically closed, but not vice versa.) We will now prove that this property is
preserved under certain homomorphisms.
Proposition 47 (refined from Moratz et al. 2009; Lcke 2012). Atomic (lax,oplax) homomorphisms (h, i) of weak representations with injective h preserve the following property to
the image of h:
Algebraic closure decides scenario-consistency.
Proof. Let weak representations  : A  P(U  U) and  : B  P(V  V) and an atomic
oplax homomorphism of weak representations (h, i) :    be given. Further assume that
for , algebraic closure decides consistency of scenarios.
302

fiRelations Between Spatial Calculi

Any scenario in the image of h can be written as h() : B  P(N  N ). If h() is
algebraically closed, then by Prop. 44, so is . Hence, by the assumption,  is consistent,
i.e. has a solution. By Prop. 46, h() is consistent as well.
The general scenario consistency problem for the DRAfp calculus is NP-hard and even
IR-complete (Wolter & Lee, 2010; Lee, 2014). However, for specific scenarios, we can do
better: We can apply Prop. 47 to the homomorphism from interval algebra to DRAfp (see
Example 27) and obtain:
Proposition 48 (Moratz et al. 2009; Lcke 2012). Algebraic closure decides consistency of
DRAfp scenarios that involve the interval algebra relations only.
Hence, consistency of such scenarios can be decided in polynomial time (in spite of the
NP-hardness of the general scenario consistency problem). A similar remark holds for the
CYC b relations embedded into OPRA1 .
For calculi such as RCC8, the interval algebra etc., (maximal) tractable subsets have
been determined, i.e. sets of relations for which algebraic closure decides consistency also
of non-atomic constraint networks involving these relations. It follows then that algebraic
closure in DRAfp decides consistency of any constraint network involving (the homomorphic
image of) a maximal tractable subset of the interval algebra only.

5. Conclusion
Our study investigated calculi which on the application side represent the same modality
on different levels of granularity. This modality in our case is relative direction. We demonstrated how to model relative directions on different levels of granularity with DRAfp ,
OPRA1 , and CYC b . It turned out that in our case study of relative direction between
oriented objects the formal relation between the calculi could be expressed as quotient homomorphisms.
This result is a step in the application of universal algebraic methods to qualitative
constraint reasoning. Since there has been an explosion of qualitative constraint calculi
in the recent years it becomes important to study the relations between those calculi and
to make automatic mappings between the calculi. This is where we contribute with the
presented work. We also have contributed a new notion of quotient (based on so-called
oplax homomorphisms) between relation algebras that captures existing natural quotients
between spatial calculi. We have published Haskell tools used for finding and checking
homomorphisms between calculi in a public repository.15
As concrete results of our study we demonstrated how to answer questions whether composition is strong or algebraic closure decides consistency for calculi in which this has not
been examined yet. With purely algebraic methods, we can lift the properties of strength of
composition and of algebraic closure deciding consistency along homomorphisms of qualitative calculi. The latter is particularly important, because algebraic closure is a polynomialtime method, whereas qualitative constraint problems in some cases turn out to be NP-hard,
even for scenarios of base-relations.
15. See https://github.com/spatial-reasoning/homer

303

fiMossakowski & Moratz

We derived a chain of calculi and homomorphisms between DRAfp , OPRA1 , CYC b .
Thereby we combined the dipole and opra calculi with the cycord approach. Based on
this new approach we could automatically derive a composition table for OPRA1 based
on the formally verified composition table of DRAfp . We compared this table with the
composition table for OPRA1 described in previous work by other authors (Dylla, 2008).
It turned out that this old composition table as it was shipped with the qualitative reasoner
SparQ contained errors in 197 entries. This emphasizes our point how important it is to
develop a sound mathematical theory as a basis for the computation of composition tables
and to stay as close as possible with the implementation to the theory.

Acknowledgements
The authors would like to thank Dominik Lcke, Andr van Delden, Torsten Hahmann, Jay
Lee, Thomas Schneider and Diedrich Wolter for fruitful discussions and Thomas Schneider
for valuable comments on a draft. Also the anonymous referees provided valuable hints. Our
work was supported by the DFG Transregional Collaborative Research Center SFB/TR 8
Spatial Cognition, projects I4-[SPIN] and R4-[LogoSpace] (TM), and the National Science
Foundation under Grant No. CDI-1028895 (RM).

References
Allen, J. F. (1983). Maintaining knowledge about temporal intervals. Communications of
the ACM, pages 832843, 1983.
Bodirsky, M. (2008). Constraint satisfaction problems with infinite templates. In Creignou,
N., Kolaitis, P. G., & Vollmer, H., editors, Complexity of Constraints - An Overview of
Current Research Themes [Result of a Dagstuhl Seminar]., volume 5250 of Lecture Notes
in Computer Science, pages 196228. Springer, 2008. ISBN 978-3-540-92799-0. doi: 10.
1007/978-3-540-92800-3_8. URL http://dx.doi.org/10.1007/978-3-540-92800-3_8.
Burmeister, P. (1986). A model theoretic approach to partial algebras. Akademie Verlag,
Berlin, 1986.
Burmeister, P. (2002). Lecture notes on universal algebra many-sorted partial algebras
preliminary version.
see http://www.mathematik.tu-darmstadt.de/Math-Net/
Lehrveranstaltungen/Lehrmaterial/SS2002/AllgemeineAlgebra/download/
LNPartAlg.pdf, 2002.
Clementini, E., Felice, P. D., & Hernandez, D. (1997). Qualitative Represenation of Positional Information. Artificial Intelligence, 95:317356, 1997.
Dubba, K. S. R., Bhatt, M., Dylla, F., Cohn, A. G., & Hogg, D. C. (2015). Learning
relational event models from video. J. Artif. Intell. Res. (JAIR), 52, 2015. accepted for
publication.
Dntsch, I. (2005). Relation algebras and their application in temporal and spatial reasoning.
Artif. Intell. Rev., 23(4):315357, 2005.
304

fiRelations Between Spatial Calculi

Dylla, F. (2008). An Agent Control Perspective on Qualitative Spatial Reasoning  Towards
More Intuitive Spatial Agent Development. PhD thesis, University of Bremen, 2008. Published by Akademische Verlagsgesellschaft Aka GmbH.
Dylla, F., Mossakowski, T., Schneider, T., & Wolter, D. (2013). Algebraic properties of
qualitative spatio-temporal calculi. In Tenbrink, T., Stell, J. G., Galton, A., & Wood,
Z., editors, COSIT, volume 8116 of Lecture Notes in Computer Science, pages 516536.
Springer, 2013. ISBN 978-3-319-01789-1. doi: 10.1007/978-3-319-01790-7. URL http:
//dx.doi.org/10.1007/978-3-319-01790-7.
Egenhofer, M. & Franzosa, R. (1991). Point-Set Topological Spatial Relations. International
Journal of Geographical Information Systems, 5(2):161174, 1991.
Frank, A. (1991). Qualitative Spatial Reasoning with Cardinal Directions. In Kaindl, H., editor, Proc. of 7th sterreichische Artificial-Intelligence-Tagung, pages 157167. Springer,
1991.
Freksa, C. (1992). Using orientation information for qualitative spatial reasoning. In Frank,
A. U., Campari, I., & Formentini, U., editors, Theories and methods of spatio-temporal
reasoning in geographic space, volume 639 of Lecture Notes in Comput. Sci., pages 162
178. Springer, 1992.
Gantner, Z., Westphal, M., & Wlfl, S. (2008). GQR - A Fast Reasoner for Binary Qualitative Constraint Calculi. In Proc. of the AAAI-08 Workshop on Spatial and Temporal
Reasoning, 2008.
Grtzer, G. (1979). Universal Algebra. Springer-Verlag, New York, NY, second edition,
1979.
Hobbs, J. R. (1985). Granularity. In In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, 1985.
Huang, J. (2012). Compactness and its implications for qualitative spatial and temporal
reasoning. In Brewka, G., Eiter, T., & McIlraith, S. A., editors, Principles of Knowledge
Representation and Reasoning: Proceedings of the Thirteenth International Conference,
KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press, 2012. ISBN 978-1-57735-560-1.
URL http://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4494.
Isli, A. & Cohn, A. G. (2000). A new approach to cyclic ordering of 2D orientations using
ternary relation algebras. Artificial Intelligence, 122(1-2):137187, 2000.
Ladkin, P. & Maddux, R. (1994). On Binary Constraint Problems. J. ACM, 41(3):435469,
1994.
Lee, J. H. (2014). The complexity of reasoning with relative directions. In Schaub, T.,
Friedrich, G., & OSullivan, B., editors, ECAI 2014 - 21st European Conference on
305

fiMossakowski & Moratz

Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic - Including Prestigious Applications of Intelligent Systems (PAIS 2014), volume 263 of Frontiers in Artificial Intelligence and Applications, pages 507512. IOS Press, 2014. ISBN 978-161499-418-3. doi: 10.3233/978-1-61499-419-0-507. URL http://dx.doi.org/10.3233/
978-1-61499-419-0-507.
Li, J. J., Kowalski, T., Renz, J., & Li, S. (2008). Combining binary constraint networks
in qualitative reasoning. In Ghallab, M., Spyropoulos, C. D., Fakotakis, N., & Avouris,
N. M., editors, ECAI 2008 - 18th European Conference on Artificial Intelligence, Patras,
Greece, July 21-25, 2008, Proceedings, volume 178 of Frontiers in Artificial Intelligence
and Applications, pages 515519. IOS Press, 2008. ISBN 978-1-58603-891-5. doi: 10.3233/
978-1-58603-891-5-515. URL http://dx.doi.org/10.3233/978-1-58603-891-5-515.
Ligozat, G. (1993). Qualitative triangulation for spatial reasoning. In Frank, A. U. & Campari, I., editors, Proc. International Conference on Spatial Information Theory., volume
716 of Lecture Notes in Comput. Sci., pages 5468. Springer, 1993.
Ligozat, G. (1998). Reasoning about Cardinal Directions. J. Vis. Lang. Comput., 9(1):
2344, 1998.
Ligozat, G. (2005). Categorical Methods in Qualitative Reasoning: The Case for Weak
Representations. In Cohn, A. G. & Mark, D. M., editors, Proc. of COSIT, volume 3693
of Lecture Notes in Comput. Sci., pages 265282. Springer, 2005.
Ligozat, G. & Renz, J. (2004). What Is a Qualitative Calculus? A General Framework. In
Zhang, C., Guesgen, H. W., & Yeap, W.-K., editors, Proc. of PRICAI-04, pages 5364,
2004.
Ligozat, G. (2011). Qualitative Spatial and Temporal Reasoning.
9781848212527,9781118601457.

Wiley, 2011.

ISBN

Lcke, D. (2012). Qualitative Spatial Reasoning about Relative Orientation: A Question of
Consistency. PhD thesis, University of Bremen, 2012. http://elib.suub.uni-bremen.
de/edocs/00102632-1.pdf.
Mackworth, A. K. (1977). Consistency in Networks of Relations. Artif. Intell., 8:99118,
1977.
Maddux, R. (2006). Relation Algebras. Stud. Logic Found. Math. Elsevier Science, 2006.
Montanari, U. (1974). Networks of constraints: Fundamental properties and applications to
picture processing. Inf. Sci., 7:95132, 1974.
Moratz, R. (2006). Representing Relative Direction as a Binary Relation of Oriented Points.
In Brewka, G., Coradeschi, S., Perini, A., & Traverso, P., editors, Proc. of ECAI-06,
volume 141 of Frontiers in Artificial Intelligence and Applications, pages 407411. IOS
Press, 2006.
Moratz, R., Renz, J., & Wolter, D. (2000). Qualitative Spatial Reasoning about Line
Segments. In Proc. of ECAI 2000, pages 234238, 2000.
306

fiRelations Between Spatial Calculi

Moratz, R., Lcke, D., & Mossakowski, T. (2009). Oriented straight line segment algebra:
Qualitative spatial reasoning about oriented objects. CoRR, abs/0912.5533, 2009. URL
http://arxiv.org/abs/0912.5533.
Moratz, R., Lcke, D., & Mossakowski, T. (2011). A condensed semantics for qualitative
spatial reasoning about oriented straight line segments. Artif. Intell., 175(16-17):2099
2127, 2011.
Mossakowski, T., Schrder, L., & Wlfl, S. (2006). A categorical perspective on qualitative
constraint calculi. In Wlfl, S. & Mossakowski, T., editors, Qualitative Constraint Calculi
- Application and Integration. Workshop at KI 2006, pages 2839, 2006.
Mossakowski, T. & Moratz, R. (2012). Qualitative reasoning about relative direction of
oriented points. Artificial Intelligence, 180-181(0):34  45, 2012.
Nebel, B. & Wlfl, S., editors (2009). AAAI Spring Symposium on Benchmarking of Qualitative Spatial and Temporal Reasoning Systems. AAAI Technical Report SS-09-02, 2009.
Randell, D. A. & Cohn, A. G. (1989). Modelling topological and metrical properties of
physical processes. In Brachman, R. J., Levesque, H. J., & Reiter, R., editors, Proc. of
KR-89, pages 357368. Morgan Kaufmann, 1989.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). A spatial logic based on regions and
connection. In Nebel, B., Rich, C., & Swartout, W., editors, Proc. of KR-92, pages
165176. Morgan Kaufmann, 1992.
Renz, J. & Ligozat, G. (2005). Weak Composition for Qualitative Spatial and Temporal
Reasoning. In van Beek, P., editor, Proc. of CP-05, volume 3709 of Lecture Notes in
Comput. Sci., pages 534548. Springer, 2005.
Renz, J. & Mitra, D. (2004). Qualitative Direction Calculi with Arbitrary Granularity. In
Zhang, C., Guesgen, H. W., & Yeap, W.-K., editors, Proc. of PRICAI-04, volume 3157
of Lecture Notes in Comput. Sci., pages 6574. Springer, September 2004.
Renz, J. & Nebel, B. (1999). On the Complexity of Qualitative Spatial Reasoning: A
Maximal Tractable Fragment of the Region Connection Calculus. Artificial Intelligence,
108(1-2):69123, 1999.
Renz, J. & Nebel, B. (2007). Qualitative Spatial Reasoning Using Constraint Calculi. In
Aiello, M., Pratt-Hartmann, I., & van Benthem, J., editors, Handbook of Spatial Logics,
pages 161215. Springer, 2007.
Scivos, A. & Nebel, B. (2004). The finest of its class: The natural point-based ternary
calculus for qualitative spatial reasoning. In Freksa, C., Knauff, M., Brckner, B. K.,
Nebel, B., & T.Barkowski, editors, Spatial Cognition, volume 3343 of Lecture Notes in
Comput. Sci., pages 283303. Springer, 2004.
van Beek, P. & Manchak, D. W. (1996). The design and experimental analysis of algorithms
for temporal reasoning. J. Artif. Intell. Res., 4:118, 1996.
307

fiMossakowski & Moratz

Wallgrn, J. O., Frommberger, L., Wolter, D., Dylla, F., & Freksa, C. (2006). Qualitative
Spatial Representation and Reasoning in the SparQ-Toolbox. In Barkowsky, T., Knauff,
M., Ligozat, G., & Montello, D. R., editors, Spatial Cognition, volume 4387 of Lecture
Notes in Comput. Sci., pages 3958. Springer, 2006.
Wallgrn, J. O., Frommberger, L., Dylla, F., & Wolter, D. (2009). SparQ User Manual
V0.7. User manual, University of Bremen, January 2009.
Westphal, M. & Wlfl, S. (2009). Qualitative CSP, finite CSP, and SAT: Comparing methods
for qualitative constraint-based reasoning. In Boutilier, C., editor, IJCAI, pages 628633,
2009.
Wolter, D. & Lee, J. H. (2010). Qualitative reasoning with directional relations. Artificial
Intelligence, 174(18):14981507, 2010. doi: 10.1016/j.artint.2010.09.004.
Worboys, M. F. & Clementini, E. (2001). Integration of Imperfect Spatial Information.
Journal of Visual Languages and Computing, 12:6180, 2001.

308

fiJournal of Artificial Intelligence Research 54 (2015) 123158

Submitted 04/15; published 09/15

Achieving Goals Quickly Using Real-time Search:
Experimental Results in Video Games
Scott Kiesel
Ethan Burns
Wheeler Ruml

skiesel at cs.unh.edu
eaburns at cs.unh.edu
ruml at cs.unh.edu

Department of Computer Science
University of New Hampshire
Durham, NH 03824 USA

Abstract
In real-time domains such as video games, planning happens concurrently with execution and the planning algorithm has a strictly bounded amount of time before it must
return the next action for the agent to execute. We explore the use of real-time heuristic
search in two benchmark domains inspired by video games. Unlike classic benchmarks such
as grid pathfinding and the sliding tile puzzle, these new domains feature exogenous change
and directed state space graphs. We consider the setting in which planning and acting are
concurrent and we use the natural objective of minimizing goal achievement time. Using
both the classic benchmarks and the new domains, we investigate several enhancements
to a leading real-time search algorithm, LSS-LRTA*. We show experimentally that 1) it
is better to plan after each action or to use a dynamically sized lookahead, 2) A*-based
lookahead can cause undesirable actions to be selected, and 3) on-line de-biasing of the
heuristic can lead to improved performance. We hope this work encourages future research
on applying real-time search in dynamic domains.

1. Introduction
In many applications, it is desirable for an agent to achieve an assigned task as quickly
as possible. Consider the common example of navigation in a video game. When a user
selects a destination for a character to move to, they expect the character to begin moving
immediately and to arrive at its destination as soon as possible. This suggests a planning strategy featuring concurrent planning and execution. The area of real-time heuristic
search has been developed to address this problem. Algorithms in this class perform short
planning episodes that are limited by a provided real-time bound, finding partial solutions
and beginning execution before a complete plan to a goal has been found. While solution
quality and search time are traditional heuristic search metrics, real-time heuristic search
algorithms are usually compared by the length of the trajectories they execute.
Most recent work in real-time heuristic search has focused on grid pathfinding problems
because of their simplicity. While important, grid pathfinding has some characteristics that
do not exist in other search problems: the search space is undirected and small enough
to easily fit in memory. We explore the use of real-time heuristic search on two additional
domains that more closely reflect features of dynamic application domains, such as robotics.
One is a platform-based pathfinding domain proposed by Burns, Ruml, and Do (2013b), and
the other is a novel domain that we call the traffic problem, featuring navigation through a
c
2015
AI Access Foundation. All rights reserved.

fiKiesel, Burns, & Ruml

field of moving obstacles. Unlike the traditional grid pathfinding problem used to evaluate
real-time search, both of these benchmarks have dynamics and large state spaces that form
directed graphs.
In addition to evaluating real-time heuristic search on new domains, we introduce three
modifications to LSS-LRTA* (Koenig & Sun, 2008), which is among the state-of-the-art
in real-time heuristic search algorithms. First, we show that LSS-LRTA*, which executes
multiple actions per planning episode, can be improved by executing only a single action
at a time. Second, it has become the standard practice to construct the local search space
of a real-time search using a partial A* search. We show that, if care is not taken to
compare search nodes correctly, the agent may execute unnecessary actions. Third, we show
that applying on-line de-biasing of the heuristic used during search can significantly reduce
the overall goal achievement time. Together, these modifications can be easily applied to
improve the overall performance of an agent being controlled by a real-time heuristic search
algorithm. Videos illustrating our new domains and the discussed algorithms are provided
on-line (Kiesel, Burns, & Ruml, 2015b) as well as described in Appendix B.
Instead of comparing techniques based solely on solution length or convergence time,
we evaluate our new methods by comparing their goal achievement timesthe time from
when the problem is issued until the goal is achieved. This metric follows naturally from our
benchmark domains and allows us to easily compare real-time search algorithms with offline planning techniques such as A*. Our results show that A*, which performs optimally
with respect to the number of expansions required to produce an optimal solution, can
easily be outperformed when one cares about goal achievement time. We hope that this
work and its methodology will encourage future research on applying real-time search in
dynamic domains.

2. Previous Work
There has been much work in the area of real-time search since it was initially proposed by
Korf (1990). In this section we will review those real-time search algorithms most relevant
for our study. (Additional related algorithms are reviewed in Section 7.)
2.1 LRTA*
Many real-time search algorithms are considered agent-centered because the agent performs
a bounded amount of lookahead search rooted at its current state before acting. Since the
size of each lookahead search is bounded, the agent can respect its real-time constraints
by restricting its lookahead to be completed by the time that the real-time limit has been
reached. In his seminal paper, Korf (1990) presents Learning Real-time A* (LRTA*), a
complete, agent-centered, real-time search algorithm. To select the next action to perform,
LRTA* uses the action costs and an estimate of the cost-to-goal, or heuristic value, for the
states resulting from applying each of its current applicable actions; it chooses to execute
the action that has the lowest estimated cost-to-goal.
LRTA* estimates the heuristic value for states in two different ways. First, if a state
has never been visited before, then it uses a depth-bounded, depth-first lookahead search.
The estimated cost of the state is the minimum f value among all leaves of the lookahead
search, where f is the cost-so-far (notated as g) plus the estimated cost-to-goal (notated as
124

fiAchieving Goals Quickly Using Real-time Search

LSS-LRTA*(s, expansion limit)
1. until a goal is reached
2.
perform expansion limit expansions of best-first search on f from s
3.
update heuristic values of nodes in CLOSED
4.
s  state on OP EN with the lowest f
5.
start executing path to s
6.
OP EN  {s}; clear CLOSED
Figure 1: Pseudocode for LSS-LRTA*.

h). The second way that it estimates cost is through learning. Each time LRTA* performs a
search, it learns an updated heuristic value for its current state. If this state is encountered
again, the learned estimate is used instead of searching again. Korf (1990) proved that as
long as the states heuristic estimate is increased after each move by an amount bounded
from below by some , then the agent will never get into an infinite cycle, and the algorithm
will be complete. In the original algorithm, the second best actions heuristic value is used
to update the cost estimate of the current state before the agent moves.
2.2 LSS-LRTA*
Local Search Space Learning Real-time A* (LSS-LRTA*, Koenig & Sun, 2008) is currently
one of the most popular real-time search algorithms. LSS-LRTA* has two big advantages
over the original LRTA*: it has much less variance in lookahead times and it does significantly more learning. LRTA* can have a large variance in its lookahead times because,
even with the same depth limit, different searches can expand very different numbers of
nodes due to pruning. Instead of using bounded depth-first search beneath each successor state, LSS-LRTA* uses a single A* search rooted at the agents current state. The
A* search can be limited by an exact number of nodes to expand, so there is significantly
less variance in lookahead times. The second advantage is that the original LRTA* only
learns updated heuristics for states that the agent has visited; LSS-LRTA* learns updated
heuristics for every state expanded in each lookahead search. This is accomplished using
Dijkstras algorithm to propagate more accurate heuristic values from the fringe of the
lookahead search back to the interior before the agent moves. Koenig and Sun showed that
LSS-LRTA* can find much cheaper solutions than LRTA* and that it is even competitive
with a state-of-the-art incremental search, D*Lite (Koenig & Likhachev, 2002).
Another major difference between LRTA* and LSS-LRTA* is how the agent moves. In
LRTA*, after each lookahead, the agent moves by performing a single action; in LSS-LRTA*
the agent moves all the way to the node on the fringe of its current lookahead search with
the lowest f value. As a result, the agent performs many fewer lookahead searches before
reaching a goal. If one is concerned with minimizing the total number of expansions, this
may be advantageous. However, as we will see below, when search and execution are allowed
to happen in parallel, the movement method of LSS-LRTA* can actually be detrimental to
performance. Pseudocode for LSS-LRTA* is presented in Figure 1.
125

fiKiesel, Burns, & Ruml

3. Evaluating Real-time Search Algorithms
Traditionally, real-time heuristic search algorithms have been evaluated using two criteria:
convergence time and solution length. Convergence time measures the number of repeated
start-to-goal plans that an algorithm must execute before it learns the optimal path between
a given start and goal pair. While it is useful for comparing the rate at which different
algorithms learn more accurate heuristic values, it does not seem to be as useful in practice;
agents rarely need to repeatedly plan between exactly the same start and goal states. Often
just one solution is needed, and an algorithm that finds a better first solution is preferred
even if it takes a long time to converge.
Solution length is the number of actions executed to achieve the goal. In real-time
search, where planning and action execution can happen in parallel, solution length is a
good proxy for the amount of time between when a real-time agent is given a problem
and when the goal is actually achieved. The downside of simply using the solution length,
however, is that it makes comparison with offline techniques unfair. For example, when
comparing algorithms solely by the solution length, no technique can perform better than
an optimal search like A*. But, in practice, A* may not be the best method to solve the
problem. An agent using A* could spend a very long time planning before it finally begins
executing an optimal path, but an agent using a real-time algorithm may start executing a
long path right away, and consequently it can arrive at the goal first.
3.1 Goal Achievement Time
Recently, Hernandez, Baier, Uras, and Koenig (2012) introduced the game time model for
evaluating real-time heuristic search algorithms. In the game time model, time is divided
into uniform intervals. During each interval, an agent has three choices: it can search, it can
execute an action, or it can both search and execute in parallel. The objective of the game
time model is for the agent to move from the initial state to a goal state using the fewest
time intervals. The advantage of the game time model is that it allows for comparisons
between real-time algorithms that search and execute in the same time step and off-line
algorithms, like A*, that search first and execute only after all search has completed. For
our experiments, we compare algorithms directly on their goal achievement time. Goal
achievement time (GAT) is a slight generalization of the game time model that allows for
real-valued times, not just fixed-size discrete time intervals. It is computed as the planning
time plus the execution time minus the time spent planning and executing in parallel:
goal achievement time = time planning + time executing  time both
Since some of the benchmark domains used in our experiments have no natural definition
of execution time (e.g., in the 15-puzzle, exactly how much time is needed to slide a tile?),
we present results using a variety of different execution times. We define execution time as
the number of seconds required to execute a unit-cost action. We call this value the unit
action duration; it effectively converts action costs into units
 of time. For example, on an
8-way grid pathfinding problem where diagonal edges cost 2, we can simply multiply the
edge costs by the unit action duration to convert them to seconds of execution. A large
unit action duration models an agent that moves slowly and a small unit action duration
models an agent that moves quickly, relative to its planning speed.
126

fiAchieving Goals Quickly Using Real-time Search

In the two following sections, we present modifications to the LSS-LRTA* algorithm.
The benefit of each modification is evaluated using goal achievement time.

4. Lookahead Commitment
An important step in a real-time search algorithm is selecting how far to move the agent
before the next phase of planning begins. As mentioned above, in the original LRTA*
algorithm the agent moves a single step, while in LSS-LRTA* the agent moves all of the
way to a frontier node in the local search space. Lustrek and Bulitko (2006) reported
that solution length increased when switching from a single-step to multi-step policy using
the original LRTA* algorithm. It was unclear if this behavior would carry over given the
increased learning performed by LSS-LRTA* and the use of a new goal achievement metric.
4.1 Single-step and Dynamic Lookahead
We implemented a standard LSS-LRTA* as well as a version that executes single actions
like LRTA*. We also implemented LSS-LRTA* using a dynamic lookahead strategy that
executes multiple actions leading from the current state to a selected state on the fringe
of the most recent local search. With a dynamic lookahead, the agent selects the amount
of lookahead search to perform based on the duration of its currently-executing trajectory.
When the agent commits to executing multiple actions, it simply adjusts its lookahead to
fill the entire execution time.
Because of the learning step, any algorithm based on LSS-LRTA* cannot simply search
until the real-time bound expires  it must leave time for learning. To account for this
we use offline training to determine the speed at which the agent searches. With the
fixed lookahead algorithms, it is necessary to know the maximum lookahead size that the
agent can search during the minimum action execution time. This can be found by simply
running the search algorithm with different fixed lookahead settings on a representative
set of training instances and recording the per-step search times. In the case of dynamic
lookahead, the agent must learn a function mapping durations to lookahead sizes. When
the agent commits to a trajectory that requires time t to execute, then it must use this
function to find l(t), the maximum lookahead size that the agent can search in time t. Note
that, because the data structures used during search often have non-linear-time operations,
this function may not be linear. It is possible to create a conservative approximation of l(t)
by running an algorithm on a representative set of training instances with a large variety
of fixed lookahead sizes. The approximation of l(t) selects the largest lookahead size that
always completed within time t.
4.2 Experimental Evaluation
We compare the different techniques on the platform pathfinding benchmark of Burns et al.
(2013b). This domain is inspired by popular platform-based video games like Super Mario
Bros. The agent must find a path, jumping from platform to platform, through a maze. A
screenshot of the domain and an example instance is shown in Figure 2. Videos are also
available on-line (Kiesel et al., 2015b) and described in Appendix B.
127

fiKiesel, Burns, & Ruml

Figure 2: A screenshot of a problem instance in the platform path-finding domain (left),
and a zoomed-out image of the entire instance (right). The knight must find a
path from its starting location, through a maze, to the door (on the right side in
the left image, and just above the center in the right image).

The available actions are different combinations of controller keys that may be pressed
during a single iteration of the games main loop: left, right, and jump. Left and right move
to the knight in the respective directions (holding both at the same time is never considered
by the search domain, as the movements would cancel each other out, leaving the knight in
place), and the jump button makes the knight jump, if applicable. The knight can jump
to different heights by holding the jump button across multiple actions in a row up to a
maximum of 8. The actions are unit cost.
Each state in the state space contains the x, y position of the knight using double
precision floating point values, the velocity in the y direction (x velocity is not stored as
its determined solely by the left and right actions), the number of remaining actions for
which pressing the jump button will add additional height to a jump, and a boolean stating
whether or not the knight is currently falling. The knight moves at a speed of 3.25 units per
frame in the horizontal direction, it jumps at a speed of 7 units per frame, and to simulate
gravity while falling, 0.5 units per frame are added to the knights downward velocity up to
a maximum of 12 units per frame.
This benchmark is a natural fit for real-time search algorithms, since the agent must
decide on an action to execute before it is forced to move due to gravity. The state space
for the platform domain is directed, because while in the air the agents actions are not
reversible. The heuristic is based on visibility navigation (see Burns et al. for details)
and it is quite accurate except that it does not account for the players limited jumping
height. The C++ source code is available on GitHub (Kiesel, Burns, & Ruml, 2015a). All
128

fiAchieving Goals Quickly Using Real-time Search

factor of optimal GAT (log10)

platform
3

multi-step (LSS-LRTA*)
single-step
dynamic

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 3: LSS-LRTA*: multi-step, single-step, and dynamic lookahead.
experiments were run on a Core2 duo E8500 3.16 GHz with 8GB RAM running Ubuntu
10.04.
For our experiments we used 25 test instances created using the level generator described
by Burns et al. (2013b), where the maze for each instance was unique and had a random
start and goal location. We used the offline training techniques described above to learn
the amount of time required to perform different amounts of lookahead search. For the
offline training, we generated an additional 25 training instances. The lookahead values
used were 1, 5, 10, 20, 50, 100, 200, 400, 800, 1000, 1500, 2000, 3000, 4000, 8000 10000,
16000, 32000, 48000, 64000, 128000, 196000, 256000, and 512000. For algorithms that use a
fixed-size lookahead, the lookahead value was selected by choosing the largest lookahead size
for which the mean step time on the training instances was within a single action duration.
If none of the lookahead values were fast enough to fit within a single action time for a given
action duration, then no data is reported. Our implementation used the mean step time
instead of the maximum step time, as the latter was usually too large due to very rare, slow
steps. We attribute these outliers to occasional, unpredictable overhead in system-related
subroutine calls such as memory allocation. We suspect that this issue would go away if a
true real-time operating system were used, where such operations perform more predictable
computations, or if a domain-specific optimized implementation were used. (Unfortunately,
developing such optimized implementations would have made it much more difficult to
perform thorough scientific comparisons.)
Figure 3 shows a comparison of these different techniques for the LSS-LRTA* algorithm.
The y axis shows the goal achievement time as a factor of the optimal goal achievement
time for each instance. The optimal goal achievement time is computed as the GAT of an
optimal solution with no planning time taken into account. One could imagine an oracle
that is able to instantly provide the optimal set of actions to execute. In the plot, the y
129

fiKiesel, Burns, & Ruml

h
3

3

2

2

1

1

0

g=2
f=5

g=1
f=4

g=2
f=4

g=3
f=5

g=4
f=5

g=5
f=6

g=6
f=6

g=1
f=4

S

g=1
f=3

g=2
f=4

g=3
f=4

g=4
f=5

g=2
f=5

g=1
f=4

g=2
f=4

g=3
f=5

g=4
f=5

g=5
f=6

g=6
f=6

Figure 4: Example of heuristic error and f layers.
axis is shown on a log10 scale. We consider a variety of action durations, these are shown on
the x axis, also on a log10 scale. Smaller action durations represent an agent that can move
relatively quickly, so spending a lot of time planning to make small decreases in solution
cost may not be worth the time. For larger values, the agent moves more slowly, and it
may be worth planning more to execute cheaper paths. The unit action duration is used to
limit the number of expansions performed in each iteration.
Each point on the plot shows the mean goal achievement time out of the 25 test instances
solved by all algorithms given as the factor of the optimal goal achievement time at that
action duration. A value of log10 (0) = 1 indicates that the given algorithm had an optimal
time. Error bars show the 95% confidence intervals on the means.
From this plot, it is clear that the multi-step approach (standard LSS-LRTA*) performed
worse than both the single-step and the dynamic lookahead variants. This is likely because
the multi-step technique commits to too many actions with only a little bit of planningthe
same amount of planning that the single-step variant uses to commit to just one action.
As the unit action duration was increased to 1 second, the algorithms started to perform
similarly. However, single-step and dynamic lookahead still appear to perform slightly
better. Note that there were 0.02 seconds per frame in the game from which the platform
domain was derived, so values greater than log10 (0.02)  1.7 represent an agent that
moves at an unusually slow pace.
It is also important to note that for some very small unit action durations, one algorithm may perform better than another, while as the unit action duration increases, this
relationship inverts. At the very small unit action durations, there is not much time for
search to be performed.

5. A*-based Lookahead
In standard LSS-LRTA*, the lookahead search is A*-based, so nodes are expanded in f
order. After searching, the agent moves to the node on the open list with the lowest f
value. While this may seem intuitively reasonable, we will show why this choice can be
problematic, and we will see how it can be remedied.
130

fiAchieving Goals Quickly Using Real-time Search





S

Figure 5: f -layered lookahead.
5.1 Heuristic Error
The crux of the problem is that f -based lookahead doesnt account for heuristic error. The
admissible heuristic used to compute f is, by definition, low-biased, so f will typically
optimistically underestimate the true solution cost through each node. Because of this
heuristic error, not all nodes with the same f value will actually lead toward the goal node.
Figure 4 shows an example using a simple grid pathfinding problem. In the figure, the agent
is located in the cell labeled S and the goal node is denoted by the star. The admissible
h values are the same for each column of the grid; they are listed across the top of the
columns. The g and f values are shown in each cell. Cells with f = 4 are bold, and the rest
are light gray. We can see that nodes with equivalent f values form elliptical rings around
the start node. In the heuristic search literature, these are referred to as f layers. While
some nodes in an f layer are closer to the goal node, there are many nodes in each layer
that are notsome nodes in an f layer will even be exactly away from the goal. In this
simple problem, the optimal solution is to move the agent right until the goal is reached,
however, of the 7 nodes with f = 4, only 2 nodes are along this optimal path; the other
nodes are not, but they have the same f value because of the heuristic error. If the agent
were to move to a random node with f = 4, chances are it will not be following the optimal
path to the goal.
One way to alleviate this problem is to use a second criterion for breaking ties among
nodes with the same f value. A common tie breaker is to favor nodes with lower h values
as, according to the heuristic, these nodes will be closer to the goal. We can see, in Figure 4
that among all nodes in the f = 4 layer, the one with the lowest h value (h = 1) is actually
along the optimal path. In LSS-LRTA* this tie breaking is insufficient, because when LSSLRTA* stops its lookahead, it may not have generated all of the nodes in the largest f layer.
If the node with h = 1 was not generated, then even with tie breaking, the agent can be
led astray.
5.2 Incomplete f Layers
These incomplete f layers cause other problems too. Recall that in LSS-LRTA*, the agent
moves to the node at the front of the open list. If low-h tie breaking is used to order the
expansions of the local search space, then the best nodes in the first f layer on the open list
will actually be expanded first and will not be on the open list when it comes time for the
agent to move. Figure 5 shows the problem diagrammatically. As before, the agent is at
the node labeled S and the goal is denoted by the star. Each ellipse represents a different
f layer, the shaded portions show closed nodes, darker shading denotes nodes with larger
131

fiKiesel, Burns, & Ruml

h(s)

{

S
heuristic error

(a)
g()


h()

{

S
h(s)  g() + h()

heuristic error

(b)
g()


S

{

^h(s)  g() + h() + d()

h()
heuristic error  d()

(c)
Figure 6: (a) A standard heuristic and its error. (b) An updated heuristic and its error.
(c) Using an updated heuristic and accounting for heuristic error.

f values, and the dotted lines surround nodes on the open list. As we can see, the closed
nodes with the largest f values cap the tip of the second-largest f layer. This is caused
by low-h tie breaking where the first open nodes to be expanded and added to the closed
list will be those that have the lowest h. These are the nodes on the portion of the f layer
that are nearest to the goal. If the agent moves to the node on its open list with the lowest
f value, tie breaking on low h, then it will select node  and will not take the best route
toward the goal.
5.3 Improving Lookahead Search
We have demonstrated two problems: 1) because of heuristic error, f layers can contain a
large number of nodes, many of which do not lead toward the goal, and 2) even with good
tie breaking, LSS-LRTA* may miss the good nodes because it only considers partial f layers
when deciding where to move. Next we present two possible solutions to these problems.
The first is quite simple. When choosing where to move, select a node with the lowest
h value on the completely expanded f layer with the largest f value, not the next node
on open. In Figure 5, this corresponds to the node labeled . We call this the complete
technique, as it considers only completely expanded f layers instead of partially expanded,
incomplete layers.
The second technique explicitly accounts for heuristic errorit orders both the search
and the agents action selection not on f , but on a less-biased estimate of solution cost.
Ideally, we would prefer to use an unbiased (and hence inadmissible) estimate that accounts
for and attempts to correct heuristic error. While any inadmissible heuristic could be used,
we note that Thayer, Dionne, and Ruml (2011) investigated the use of inadmissible heuristics
132

fiAchieving Goals Quickly Using Real-time Search

for offline search and here we adopt their simple heuristic correction technique for real-time
search. We call this estimate f. Like f , f attempts to estimate the solution cost through a
node in the search space. Unlike f , f is not explicitly biasedit is not a lower bound. f is
computed similarly to f , however, it attempts to correct for the heuristic error by adding
in an additional term:
f
error
z
}|
{ z }| {

f (n) = g(n) + h(n) + d(n)  
where  is the average single-step error in the heuristic, and the additional term d(n)  
corrects the error by adding  back to the cost estimate for each of the d(n) steps estimated
to remain from n to the goal. Following Thayer et al. (2011), we make the simplifying
assumption that the error in the heuristic is distributed evenly among each of the actions
on the path from a node to the goal.
Distance estimates d are readily available for many domains; they tend to be just as
easy to compute as heuristic estimates (Thayer & Ruml, 2009). To estimate the single-step
heuristic error, we use an average of the difference in the f values between each expanded
node and its best child. This difference accounts for the amount of heuristic error due to
the single step between a parent node and its child. With a perfect heuristic, one with no
error, the f values of a parent node and its best child would be equalsome of the f will
simply have moved from h into g:
f (parent ) = f (child ), in the ideal case, so
h(parent ) = h(child ) + c(parent , child ), and
g(parent ) = g(child )  c(parent , child )
Since g is known exactly, as is the cost of the edge c(parent , child ), with an imperfect
heuristic any difference between f (child ) and f (parent ) must be caused by error in the
heuristic over this step. Averaging these differences gives us our estimate .
Adapting this technique to real-time search requires some subtlety. In real-time search
algorithms like LSS-LRTA*, the heuristic values of nodes that are expanded during a lookahead search are updated each time the agent moves. Figure 6a schematically depicts the
error in the default heuristic value for a node S. Its error is accrued over the distance from
S to the goal. After lookahead (Figure 6b), the updated heuristics are more accurate than
the originals because they are based on the heuristic values of nodes that are closer to the
goal, and thus have less heuristic error. Here, we can see that  is the node on the fringe
from which the start state inherits its updated heuristic value. Since g(), the cost from
S to , is known exactly, the error in the backed up heuristic now comes entirely from the
steps between  and the goal. Since  is closer to the goal, the error is less than the error
of the original heuristic value for S.
When computing f(S) in a real-time search, it is necessary to account for the fact that
error in the updated heuristic comes from the node . To do this, we track a value called
derr , the distance over which heuristic error is accrued, for each node, and we use it to
compute f. Initially, for nodes without updated heuristic values derr (n) = d(n). After
performing a lookahead search, h is updated from backed-up h values as before. If h(n)
receives a backed up value that originated at node , then we set derr (n) = d(), since
the error in the updated heuristic comes from the instance between the fringe node  an
133

fiKiesel, Burns, & Ruml

platform
GAT difference from LSS-LRTA*

factor of optimal GAT (log10)

platform
complete
incomplete (LSS-LRTA*)

3

2

1

0
-4

-3

-2

-1

unit action duration (log10)
(a)

0

0

-40

unit action duration (log10)
(b)

0

Figure 7: LSS-LRTA*: f-based lookahead and f-based lookahead.
the goal, not the distance between n and the goal. The updated heuristic, when accounting
for heuristic error, is h(s) = g() + h() + derr (n)  , where g() + h() is the standard
heuristic backup and derr (n)   is the error correction (cf Figure 6b, derr (n) = d() due
to the update). This is demonstrated by Figure 6c. Our new technique uses f to order
expansions during the lookahead search in LSS-LRTA*, and it moves the agent toward the
node on the open list with the lowest f value.
Figure 7 shows a comparison of the three node selection techniques: the standard incomplete f layer method of LSS-LRTA*, the complete f -layer method, and the approach
that uses f (denoted fhat). To better demonstrate the problem with the standard approach,
the plot shows results for the multi-step movement model that commits to an entire path
from the current state to the fringe of the local search space after each lookahead. The style
of the plot in panel (a) is the same as for Figure 3.
In this figure, we can see that complete performed worse than the standard LSS-LRTA*
algorithm for small action durations where the agent may not have time to expand many
nodes and thus ignoring some expansions has a large effect. For longer action durations,
however, this performance improves and complete becomes the best performer on the right
side of the plot where cheaper solutions are preferred. To clarify the improvement on the
right side of the plot in Figure 7 (a), we have included Figure 7 (b). This plot has a
different y-axis that highlights the improvement by comparing each algorithm directly to
the incomplete version of LSS-LRTA*. This indicates that using the completed f layer
does lead to fewer extraneous actions and gives cheaper solutions. Using f to sort the open
list of lookahead searches performs much better than the other two algorithms on the left
side of the plot, although it begins to perform slightly worse as the unit action duration is
increased. This is likely because the inadmissibility in f hinders its ability to find solutions
that are as cheap as those found by the complete f layer variant.
134

fiAchieving Goals Quickly Using Real-time Search

factor of optimal GAT (log10)

platform
3

LSS-LRTA*
single-step f

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 8: Comparison of the four new real-time techniques.
Dynamic-f(s, expansion limit)
1. until a goal is reached
2.
perform expansion limit expansions of best-first search on f from s
3.
update heuristic values of nodes in CLOSED
4.
s  state on OP EN with the lowest f
5.
6.
7.
8.

start executing path to s
execution time  execution time to reach s
expansion limit  number of expansions possible within execution time
OP EN  {s}; clear CLOSED
Figure 9: Pseudocode for LSS-LRTA* with a dynamic lookahead using f.

Figure 8 shows the results of a comparison between the four combinations of single-step
versus dynamic lookahead and f -based versus f-based node ordering. In this domain, f
with dynamic lookahead tended to give the best goal achievement times in portions of the
plot where all algorithms did not have significant overlap (i.e., everywhere except for the
right-half of the plot).
In both Figure 7 and Figure 8, ordering the lookahead search on f was the common
link to the best performance out of all considered algorithms. Figure 7 demonstrates the
initial intuition that heuristic correction can be used in real-time search. Figure 8 builds
on this idea by adding in the dynamic look ahead proposed in the previous section to yield
the strongest algorithm we have seen so far. We present pseudocode in Figure 9 for this
dynamic-f method.
135

fiKiesel, Burns, & Ruml

factor of optimal GAT (log10)

platform
3

LSS-LRTA*

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 10: Comparison of the four new real-time techniques with a weak heuristic.
5.3.1 Heuristic Accuracy
It could be argued that the effects of dynamic lookahead and heuristic correction are distorted by such a strong heuristic as a visibility graph. In this short experiment, we replace
the visibility graph heuristic in the platform domain with a much weaker euclidean distance
heuristic. To solve instances using this heuristic, we had to decrease the overall size of the
instances from 50x50 to 25x25.
Even with this decreased instance size, only the dynamic algorithms were able to solve
all 25 instances for all unit action durations. The other algorithms, for some unit action
durations, solved as few as 13 out of the full 25 instances. Figure 10 shows results for this
experiment, each data point represents the mean over only those instances that were solved
by all algorithms at that unit action duration (between 13 and 17 instances).
We can see that the ranking of the algorithms remains the same between the algorithms
regardless of the weakening in heuristic. We note that at a unit action duration of 0.001
seconds there is a rise in the data. This can be attributed to solving a larger subset of the
25 instances containing more difficult instances, thus increasing the GAT.
5.3.2 CPU Usage
While this paper does not focus on minimizing CPU time and assumes that any time
allocated to search may be utilized, it is still instructive to compare CPU usage of these
new techniques. As one might imagine, there is a tradeoff between CPU usage and GAT.
This inverse relationship can be seen by examining how each algorithm utilizes its search
time. Single step policies will execute search during every action until the goal is reached.
This behavior can increase the overall demand on the CPU. However, using a single step
policy, the goal can be achieved more quickly than using a multi-step policy. Dynamic
136

fiAchieving Goals Quickly Using Real-time Search

platform

platform
single-step f
LSS-LRTA*
dynamic fhat

100

50

-4

-3

-2

-1

unit action duration (log10)
(a)

single-step f
dynamic fhat
LSS-LRTA*

0.9

fraction of search time used

total raw cpu time (seconds)

150

0.6

0.3

-4

0

-3

-2

-1

unit action duration (log10)
(b)

0

Figure 11: Comparison of LSS-LRTA*, single-step LSS-LRTA* and dynamic f in terms of
cpu usage.

lookahead will greedily use more search time as it becomes available, but will arrive at the
goal more quickly.
In Figure 11, search time is plotted. Figure 11 (a) shows the raw cpu time used by each
algorithm in the platform domain. On the y axis is the raw cpu time in seconds and on
the x axis is the log10 unit action duration. As can be seen from the plot, dynamic f uses
a very small amount of planning time. This can be attributed to dynamic f finding the
goal very quickly. While the single-step policy is able to find the goal more quickly than
LSS-LRTA*(see Figure 3), it requires more cpu time to do so as the unit action duration
increases.
Figure 11 (b) plots the raw cpu time divided by the goal achievement time. This provides
an idea of how active the CPU is during execution for each of the algorithms. It is important
to keep in mind that quick goal achievement times will have a small denominator, causing
utilization to appear higher than for the same raw cpu time with a longer goal achievement
time. The reduced utilization for longer action durations is likely because dynamic f is able
to find goals quickly using a small number of iterations and the remainder of execution has
no overlapping planning time.
5.3.3 Implementation Details
The f technique requires more information that standard LSS-LRTA*, so it has slightly
greater storage requirements. We should note, however, that during our experiments we
did not run into memory issues, so we did not optimize our implementation to reduce
memory requirements.
137

fiKiesel, Burns, & Ruml

Our implementation uses two different types of nodes: persistent and transient. Persistent nodes form the agents memory of all states it has encountered during all past lookahead
searches: their connectivity, learned heuristic values, and the distance estimates over which
their heuristic error is accrued. Transient nodes exist for a single round of lookahead search
and h-cost learning; they are akin to traditional search nodes used in, for example, an A*
search, however they include information required to order the search on f.
In each persistent node, we store information about the connectivity of the search graph.
This includes the set of predecessors and successors of the node and the costs of the associated edges. We store the predecessors because we do not assume an undirected search graph
or that a predecessor function is easily computable. Both the predecessors and successors
are computed lazily. Each predecessor is added only when it is first expanded, while the entire set of successors is populated the first time the node is expanded. For successor nodes,
we also store the cost of reversing the edge (which matters in the case where edge costs
are not symmetric) and the operator used to generate that successor. Persistent nodes also
store the learned heuristic estimate and cached values such as the original h and d estimates
for the node, which would otherwise need to be computed each time they are needed.
Transient nodes hold additional information needed to perform a best first search ordered
on f. First, each transient node has a pointer to the corresponding persistent node. In
addition, each transient node has the g-cost, f -cost, and f-cost as computed during the
single lookahead search for which the node persists. Other information contained in the
transient nodes are: a pointer to the best parent during the current lookahead search; the
nodes index on the open list (which is implemented as an array-based binary heap), needed
for updating the nodes position in the heap if it is encountered via a better path; and two
booleans used during h-cost learning to easily determine if the node has already had its
h value updated and to determine if it is on the closed list. For more detail, we refer the
reader to the source code which is freely available on GitHub (Kiesel et al., 2015a).
The only differences between the information stored by f and our normal LSS-LRTA*
implementation is that the latter does not store d estimates in its persistent node set, and
it does not store f values in its transient nodes. All other information is exactly the same.
5.3.4 Theoretical Evaluation
Here we prove that, under certain conditions, our modifications to LSS-LRTA* retain the
completeness property of the algorithm. This is because learning will cause f to converge
to f  . To begin, we assume that the heuristic is admissible and the state space is finite.
Proposition 1. Following the spirit of Korf s completeness proof for LRTA*, we note that,
if a search algorithm is incomplete in a finite state space, then there must exist nodes that
are visited an infinite number of times. 1
Our goal now is to show that such nodes cannot exist.
Lemma 1. If dynamic f searches only within some finite set of nodes D, the h values of the
nodes that are in the interior of D will reach a fixed point (remain static and unchanging)
at some finite time T1 , for at least as long as the search remains within D.
1. We note that, contrary to the assumptions of some previous work, the algorithm need not actually enter
a loop, as the trajectory may vary in a non-repeating way, just as the digits of  are conjectured to.

138

fiAchieving Goals Quickly Using Real-time Search

Proof.
1. Because h is updated using the same update rule as in LSS-LRTA*, all h
values in the state space are non-decreasing via Koenig and Suns (2008) Theorem 1
regarding LSS-LRTA*.
2. Note that the h values of nodes on the fringe of D will remain static because the
h-value learning only updates the heuristic of nodes in the interior of an LSS. Every
update during a learning step obeys h(p) = max(h(p), c(p, bc) + h(bc)), where bc is
the best child of p  the one with the lowest f value. Thus every h(p) value will
be the sum of numbers drawn from the set C that contains: all edge costs, the set
of all fringe h values, and the set of initial h(p) values. Because there are a finite
number of costs in C, any update to h(p) must be larger than the minimum positive
difference between any two possible sums of costs drawn from C. Thus the increases
are bounded from below by a constant.
3. Similarly to step 1, the h values remain admissible via Theorem 2 of Koenig and Sun
regarding LSS-LRTA*, so they cannot rise above the true cost to go.
4. By steps 1, 2, and 3, there must be a time T1 , after which the h values do not change
for as long as the search remains within the set D.
Lemma 2. If a search visits some finite set of nodes D an infinite number of times, there
exists a time T  after which the search visits only nodes in D.
Proof. Consider the LSSes that are formed in each iteration and the learning step in which
parents inherit h values from their child with the lowest f . Considering each of these pairs
of nodes, there are two cases: a) those two nodes will be in the same LSS an infinite number
of times as the number of search iterations approaches infinity, or b) these nodes will only
be in the same LSS a finite number of times. For those pairs in case (b), note that there
must exist a time, T  , after which they are never in the same LSS, for otherwise the two
nodes would have been covered by case (a) instead.
Lemma 3. If dynamic f searches only within some set D, then the one-step heuristic error
 goes to 0 by some time T2 and stays there for at least as long as the search remains within
D.
Proof.
1. Consider those pairs of nodes in the same LSS after T  . By Lemma 1, there
exists some T1 after which the h values have converged. So after T1 , we know that
h(p) = c(p, bc) + h(bc).
2. Note that  is the average, across all internal nodes in an LSS and their associated
best children, of the differences between the f values of parents and their best children.
By step 1, f (p) = f (bc), so  = 0 holds after some time T2  T1 .
Lemma 4. There cannot exist a set of nodes D that dynamic f visits infinitely often.
Proof.
1. For the sake of contradiction, let D be a set of nodes which are part of an LSS
an infinite number of times.
2. By Lemma 3,  will become 0 by some time T2 .
139

fiKiesel, Burns, & Ruml

3. At time T2 when  = 0, h = h, f = h, and so dynamic f will behave like LSS-LRTA*
(dynamic-sized lookahead makes no difference to LSS-LRTA*s theoretical properties
as they hold without regard to lookahead size).
4. LSS-LRTA* is complete (Koenig & Sun, 2008, Thm. 3), so the search will eventually
reach a goal. This contradicts 1, so dynamic f will not visit a set of states infinitely
often.
Note that, if dynamic f were to escape a potential set D, as described in step 1 of the
proof of Lemma 4, but then circulate within another set of nodes D  for an infinite time,
then D would have equaled D  instead. Also, if it were to oscillate between two or more
sets, then D would be defined as the union of all such sets.
Theorem 1. In a finite search space with admissible h, dynamic f will eventually reach a
goal.
Proof. Proof by contradiction:
1. Assume the search never reaches a goal. Because dynamic f goes to any goal in the
LSS, this means that a goal is never in the LSS.
2. In a finite space, if the search never sees a goal, then it must visit other states an
infinite number of times.
3. By Lemma 4, dynamic f will not exhibit such behavior.
4. Thus, using dynamic f retains the completeness of LSS-LRTA*.

6. Comparison With Off-line Techniques
In the previous sections, we explored modifications to the LSS-LRTA* algorithm to improve
its ability to achieve goals quickly. LSS-LRTA*s performance can be improved by applying
a heuristic correction and either using a single-step movement policy or using dynamicallysized lookahead searches. In this section we evaluate the performance of these algorithms
against standard offline techniques. We have included three extra domains for this final
comparison, the 15-puzzle, grid pathfinding, and a novel domain that we call the traffic
domain. For the 15-puzzle, we used the 94 instances from Korfs 100 instances (Korf, 1985)
that our implementation of A* was able to solve using a 6GB memory limit. For grid
pathfinding, we ran on the orz100d grid map from the video game Dragon Age: Origins
(Sturtevant, 2012). This map, shown in Figure 12, includes a mix of open space and mazelike areas with narrow corridors. We used the 25 start and end locations with the longest
optimal path lengths in the scenarios from Sturtevant (2012). For completeness, results for
the best performing algorithms on a random selection of 10 additional maps are presented
in Appendix A.
140

fiAchieving Goals Quickly Using Real-time Search

Figure 12: Grid path-finding on a video game map.
6.1 The Traffic Domain
The traffic domain is a new domain inspired in part by video games such as Frogger2 .
The goal in the traffic domain is to navigate through a grid to a given goal location while
avoiding obstacles, many of which are in motion. Each state includes the x, y location
of the agent and the x, y location of each obstacle (In our implementation, the locations
of obstacles were not stored in the state; instead, we store each states current time, and
obstacle locations were computed based on their initial location, velocity, and the time).
Time is divided into discrete intervals called ticks, and the agent can move in one of the four
cardinal directions or remain still during each tick. Obstacles each have both horizontal
and vertical velocities that are either -1, 0, or 1 cell per-tick in the respective direction.
Obstacle locations are known to the agent at any time in the future. When an obstacle
hits the edge of the grid it bounces off, reversing its velocity in the direction of the hit.
Obstacles simply pass through each other. The search space is directed, because time is
ticking forward and, once the obstacles move, the agent cannot perform an action to move
the obstacles back to their previous locations. This domain is especially well-suited for
real-time techniques as the agent must have an action ready to execute at each tick when
the world transitions and the obstacles move.
To eliminate dead end states (real-time algorithms are incomplete in the presence of
dead ends), when the result of executing an action by the agent is an intersection with an
obstacle, the agent is teleported back to the start state location (at time t=0). This is
especially important for offline algorithms that expect to begin execution from the initial
2. This domain is similar to the 2011 ICAPS International Probabilistic Planning Competition domain
called crossing traffic which was constructed as an MDP and also a POMDP. Our version of the
domain is deterministic and fully observable.

141

fiKiesel, Burns, & Ruml

15-puzzle
factor of optimal GAT (log10)

factor of optimal GAT (log10)

platform
2.4

1.6

0.8

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

orz100d

0

traffic

3

factor of optimal GAT (log10)

factor of optimal GAT (log10)

-1

unit action duration (log10)

2

1

0

3

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 13: Comparison with off-line techniques.

state despite the passage of time while planning. For our experiments, we generated 25
random solvable instances consisting of 100x100 grids with 5,000 obstacles placed randomly
with random velocities. The start location was in the upper-left corner of the grid and the
goal location was in the lower-right corner. The average solution length was 211.24 moves.
Videos showing the traffic domain are available on-line (Kiesel et al., 2015b) and discussed
in Appendix B.
142

0

fiAchieving Goals Quickly Using Real-time Search

6.2 Results
By allowing planning and execution to take place simultaneously, it should be possible to
improve over offline techniques that delay execution until planning is finished. To assess
this, we compared the best-performing variants of LSS-LRTA* with A* and an algorithm
called Speedy (Thayer & Ruml, 2009). Speedy is a best-first greedy search on d(n), the
estimated number of actions remaining to the goal. It tends to find poor plans very quickly,
providing an informative contrast with A*. Figure 13 shows the results of this comparison,
again with the log10 factor of optimal goal achievement time on the y axis and the unit
action duration, on a log10 scale, on the x axis.
f-based search with a dynamic lookahead gave the best goal achievement times on the
platform, 15-puzzle, and traffic domains. Speedy was a strong performer in both the grid
pathfinding domain and the traffic domain. This is not surprising as these domains are both
based on grid navigation. In the traffic domain Speedy is able to quickly find a collision free
path (avoiding additional cost overhead). On grid pathfinding A* actually had the lowest
goal achievement times for all unit action durations, and f with a dynamic lookahead was
about tied with A* for all except the fastest unit action duration. These results on grid
pathfinding are consistent with the results presented by Hernandez et al. (2012), where
their best performer was about as good as A*. (In their study, the best performer was
TBA*, which we dont compare against as it does not work for directed graphs.) This is
likely because A* can solve these grid pathfinding problems very quickly, thus it has short
planning times, and still finds optimal solutions.
Even though A* performs well, it is not applicable when a real-time constraint is present
and an action needs to be returned within a bound. A*-based real-time algorithms give
similar results with an infinite lookahead, although some would waste time learning.

7. Related Work
There is a large body of work relating to real-time search. In this section we review some of
this work and discuss its relation with the techniques that we have presented in the previous
sections.
7.1 Pruning Dead States
f -LRTA* (Sturtevant & Bulitko, 2011) is an extension of LSS-LRTA* and RIBS (Sturtevant,
Bulitko, & Bjornsson, 2010), combining both h-cost learning and g-cost learning. The g-cost
learning enables the algorithm to label states as dead-ends or redundant. Determining these
types of states using the basic algorithm relies on an underlying undirected graph. This
arises from the requirement to compute the cost from successor to parent. In an undirected
graph this is simply the reverse operator, but in the case of a directed graph, this would
require a call to either a heuristic or a call to an additional search to determine the cost
of that edge. In consultation with Sturtevant, we created a small modification to include
reverse edge costs where they were easily computable. However, in practice this did not
perform well in our directed graph domains. We conclude that further work is needed to
adapt the ideas behind f -LRTA* to directed graphs.
143

fiKiesel, Burns, & Ruml

Sharon, Sturtevant, and Felner (2013) introduce a technique for pruning dead states in
real-time agent-centered search. This work detects two types of dead states: expendable
and swamp. These are determined by considering reachability and shortest paths in the
local neighborhood of a state. While dead state pruning has been shown to lead to speedups in 8-way grid pathfinding, it is only applicable in certain domains. Let us first consider
the undirected domains considered in this paper. In the sliding tile puzzle, for example,
expendable and swamp states do not exist locally. No neighbors of a state s can reach
another neighbor without passing through s when only considering a local neighborhood.
Without reachability, no shortest paths can exist, so no locally expendable or swamp states
would be pruned. We also note that in a grid navigation problem that only considers
movement in the four cardinal directions, no pruning could occur either for the same reason.
In a directed graph, the predecessors of s must also be considered in the local neighborhood. In this case, an expendable state would be a state s whose predecessors can reach all
of ss successors without traversing through s. Similarly, a swamp state s would be a state
whose predecessors have shortest paths to all successors of s that do not pass through s.
In the traffic domain, for example, the state contains time, so all predecessors of a state s
with time t, would have time t  1 and its successors all have a time of t + 1. There is no
way to traverse from a state t  1 to a state with time t + 1 without traversing through a
state with time t. As state s is the only state in the local neighborhood with a time t, no
paths can exist between predecessors and successors of s that do not pass through s. No
expendable or swamp states would be pruned in this domain.
7.2 Minimizing Search Effort
The GAT model assumes that search can occur during action execution, which is appropriate for situations in which separate processor cores are responsible for planning versus
managing execution. When processor resources are scarce and shared among many tasks,
one may want to minimize search effort even during execution. Bulitko, Lustrek, Schaeffer,
Bjornsson, and Sigmundarson discuss methods for dynamically adjusting real-time search
lookahead in order to minimize search effort while still selecting good actions. In contrast,
as we discussed in section 5.3.2, dynamic f attempts to use all available execution time to
perform as much search as possible.
7.3 Time-bounded A*
Time-bounded A* (TBA*, Bjornsson, Bulitko, & Sturtevant, 2009) is a non-agent-centered
real-time search algorithm. Instead of performing a bounded amount of lookahead search
from the agents current state, TBA* maintains a single A* search from the agents initial
starting state to the goal state. During each iteration, a fixed number of expansions are done
on this single search and the agent attempts to move toward the most promising node on the
search frontier. Since the agent may have already moved away from the initial state during
previous iterations, and because A* vacillates between many different paths, the agents
current state may not be along the current best path. If this occurs, the agent backtracks
toward the initial state until it is on the current best path. In their experiments, Bjornsson
et al. showed that, on grid pathfinding benchmarks, TBA* requires fewer iterations to find
the same quality paths as other real-time algorithms such as LRTA*.
144

fiAchieving Goals Quickly Using Real-time Search

As mentioned briefly above, Hernandez et al. (2012) found that TBA* was the best
technique for optimizing goal achievement time on grid pathfinding problems in fully-known
grids. They state that its performance was about the same as that of A*. In our experiments, we do not compare with TBA*, as we consider domains that form directed graphs,
and TBA* only works on undirected graphs, due to the agents need to backtrack. We
suspect, however, that our dynamic lookahead f technique would be quite competitive with
TBA*, as it also matched the performance of A* on grid pathfinding problems and it was
able to greatly outperform A* on all of the other domains.
7.4 Avoiding Depressions in Real-time Heuristic Search
Real-time search algorithms can become temporarily stuck in a heuristic local minimum for
an extended period of search and execution time (Sturtevant & Bulitko, 2014). The agent
will typically wander around in a heuristic minimum until it learns that the heuristic in
that area is inaccurate and corrects it. This behavior results in long solutions and can be
aesthetically undesirable.
daLSS-LRTA* and daRTAA* (Hernandez & Baier, 2012) attempt to actively avoid and
escape heuristic depressions. Instead of selecting the node with the lowest f value, daLSSLRTA* selects the node along the frontier whose heuristic value has changed the least.
daRTAA* is similar but uses a simpler learning phase borrowed from Real-Time Adaptive
A* (Koenig & Likhachev, 2006). RTAA* and daRTAA* update the entire interior of the
local search with the f value of the node on the open list with the best f value.
We implemented both daLSS-LRTA* and daRTAA* and compared them to standard
LSS-LRTA* as well as the multi-step and dynamic lookahead variants of LSS-LRTA* using
f. The results of the comparison are shown in Figure 14. Our results on the grid pathfinding
problem (orz100d) agree with results from Hernandez and Baier (2012) and show that using
depression avoidance techniques can help improve performance (for example, compare LSSLRTA* and daLSS-LRTA*). Also, daLSS-LRTA* and daRTAA* outperform the standard
multi-step variant of LSS-LRTA* using f. Dynamic lookahead f clearly gives the best
performance for all but the fastest unit action duration of 0.0001. In the platform and
traffic domains, however, daLSS-LRTA* appears slightly worse than LSS-LRTA*, and in
the 15-puzzle depression avoidance appears to have little effect. Overall, we found dynamic
f to dominate the other techniques.
Similar to Figure 10, we can see a spike at 0.001 unit action duration in the platform
domain. This can be attributed to a jump in the number of instances that were solved by
all algorithms between 0.0001 and 0.001. The larger set contains more difficult instances
and increases the factor of optimal GAT.
It may be interesting future work to combine depression avoidance with dynamic lookahead, especially for grid pathfinding domains.
7.5 Weighted Real-time Heuristic Search
Weighted A* (wA*, Pohl, 1970) is a popular heuristic search algorithm that proceeds like
A*, but orders nodes on the open list using f  (n) = g(n) + w  h(n), for some w  1. As
w increases, the search becomes more greedy and can often find solutions faster than A*.
While these solutions may not be optimal, they are guaranteed to be within a factor w of the
145

fiKiesel, Burns, & Ruml

15-puzzle
factor of optimal GAT (log10)

factor of optimal GAT (log10)

platform

1.8

1.2

0.6

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

orz100d

0

traffic

3

factor of optimal GAT (log10)

factor of optimal GAT (log10)

-1

unit action duration (log10)

2

1

1.8

1.2

0.6

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 14: Depression avoidance real-time heuristic search.

optimal solution cost. Rivera, Baier, and Hernandez (2012) recently showed a variant of wA*
for real-time search called wLSS-LRTA*. One obvious way to implement a real-time variant
of wA* would be to simply multiply the heuristic value by w  1 during a lookahead search
in LSS-LRTA*, however, wLSS-LRTA* does not do this. Instead, wLSS-LRTA* multiplies
the edge weights by w during the learning phase of LSS-LRTA*. The update rule becomes:
h(n)  minmopen w  g(n, m) + h(m), where g(n, m) is the cost between the node n being
updated and a node m from the open list of the lookahead search.
146

0

fiAchieving Goals Quickly Using Real-time Search

15-puzzle

2.4

factor of optimal GAT (log10)

factor of optimal GAT (log10)

platform

1.6

0.8

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

0

traffic
factor of optimal GAT (log10)

factor of optimal GAT (log10)

orz100d
3

w=16 LSS-LRTA*
w=1 LSS-LRTA*
dynamic fhat
w=2 LSS-LRTA*

2

-1

unit action duration (log10)

1

2.4

1.6

0.8

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 15: Weighted real-time heuristic search.

Rivera et al. (2012) show that using an increased weight in wLSS-LRTA* can lead to
lower-cost solutions. They point out that, because admissible heuristics are lower bounds,
inflating the heuristic by a factor w may make the heuristic more accurate. This is the
same reasoning behind our f technique. The difference is that wLSS-LRTA* uses a weight
to inflate the g portion of the updated heuristic whereas the f technique adds a correction
based on the h portion of the updated heuristic. We would argue that the f approach
makes more sense because the error causing the heuristic to underestimate does not come
from the perfectly-known g portion of the update, but from the estimated h portion.
147

0

fiKiesel, Burns, & Ruml

We implemented wLSS-LRTA* and compared it to the standard multi-step and dynamic
lookahead variants of LSS-LRTA* using f. The results of the comparison are shown in
Figure 15. Our results on the grid pathfinding problem (orz100d) tend to agree with those
of Rivera et al. (2012): using larger weights for wLSS-LRTA* can increase performance.
This trend seems to depend, however, on the unit action duration; it is more noticeable
when actions are fast. Also, for grid pathfinding, wLSS-LRTA* outperforms the standard
multi-step variant of LSS-LRTA* using f, but the dynamic lookahead f clearly gives the
best performance for all but the fastest unit action duration of 0.0001. However, in the
platform, 15-puzzle, and traffic domains we found almost the opposite to be true! Dynamic
f is still the bestit nearly dominates all other techniques. But the fairest comparison is
LSS-LRTA* using f (with statically sized lookahead) which provides better performance
than wLSS-LRTA*, dominating wLSS-LRTA* with a weight greater than 1 on the 15-puzzle
and traffic problems, and increasing the weight in wLSS-LRTA* either has no effect or it
makes the performance worse. For the traffic domain with weights greater than 1, wLSSLRTA* was unable to solve any problems with a lookahead of 1, and all greater lookahead
values tried (including a lookahead of 2) were too slow to meet the real-time deadline for
a unit action duration of 0.0001, thus there is no data point for x=0.0001 for either of
these algorithms. Based on these results, we conclude that using the parameter-free f
technique to explicitly attempt to account for heuristic error is the recommended approach
over weighting the edge costs during learning by a user-specified parameter.
7.6 FRIT
Follow and Reconnect with the Ideal Tree, or FRIT (Rivera, Illanes, Baier, & Hernandez,
2013), takes another approach to dealing with heuristic minima in real-time search. Rather
than applying heuristic learning and updating to escape a local minimum, FRIT instead
tries to follow the ideal tree of the state space. An ideal tree represents a family of paths
that connect states of the search space with the goal state. It also can be thought of as
being implicitly represented by a heuristic.
The ideal tree is explored by following the heuristic greedily as if all operators were
applicable in all states, until the heuristic suggests an operator that is inapplicable. A simple
example is following the Manhattan distance heuristic in a grid pathfinding domain until
encountering an obstacle. When an inapplicable operator is suggested, the tree becomes
disconnected and the agent must reconnect with the tree. This is done by performing a
local search around the agents current state until a state believed to be in the Ideal Tree
is found. The agent then moves to that state and continues on. The resulting behavior in
grid pathfinding domains can appear very similar to wall following.
Some modifications are required to make FRIT into a real-time algorithm. The local
search to find a state in the Ideal Tree is bounded only by the size of the state space,
rather than a time bound or an expansion limit. The authors suggest a few techniques for
bounding the local search but in our experiments, we allowed FRIT to be thought of as
offline and allowed it as much time as needed when looking to reconnect with the Ideal
Tree. We also used breadth first search as the local search algorithm.
We implemented FRIT and compared it against standard LSS-LRTA* and the multistep and dynamic versions of LSS-LRTA* using f. The results are shown in Figure 16. We
148

fiAchieving Goals Quickly Using Real-time Search

factor of optimal GAT (log10)

orz100d
3

LSS-LRTA*
dynamic fhat

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

Figure 16: Comparison with offline FRIT using breadth first search.
only present results for the grid pathfinding problem (orz100d). FRIT was only able to
solve a few of the easier instances in the platform and traffic domains within the five minute
timeout. As for the sliding tile puzzle domain, it was unclear how to adapt this algorithm
for this domain. A naive approach results in a branching factor of 44 and poor results.
On grid pathfinding, even by treating FRIT as an offline algorithm and not penalizing it
for search time in its final goal achievement time, it performs worse than the three variants
of LSS-LRTA* presented. The exception is when the unit action duration is very small, at
which point FRIT is competitive with the other algorithms (ignoring its search time).
7.7 FALCONS
Furcy and Koenig (2000) present two modifications to LRTA* to speed up its convergence
time. They noticed that by breaking ties in favor of successors with smaller f -values LRTA*
would converge more quickly. They also point out that if you also use this tie-breaking criteria to select which successor to move to, convergence occurs even faster. These two modifications yield two new algorithms: Tie Breaking LRTA* (TB-LRA*) and FAst Learning
and CONverging Search (FALCONS).
In Figure 17 we compare against the original LRTA*, TB-LRTA* and FALCONS. In
all domains these three algorithms perform worse than the newer LSS-LRTA* and modified
versions of LSS-LRTA*.
7.8 RTA*
Korf (1990) not only proposed LRTA* in his seminal paper but also another algorithm
simply called Real Time A* (RTA*). RTA*, unlike its counterpart LRTA*, is focused on
solving the problem of getting from the start state to the goal state only once. LRTA* is
149

fiKiesel, Burns, & Ruml

15-puzzle
factor of optimal GAT (log10)

factor of optimal GAT (log10)

platform

2.4

1.6

0.8

0

3

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

0

traffic
factor of optimal GAT (log10)

orz100d
factor of optimal GAT (log10)

-1

unit action duration (log10)

4

3

2

1

4

3

LSS-LRTA*
dynamic fhat
2

1

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 17: Comparison with LRTA*, TBLRTA* and FALCONS.

proven to converge to optimal heuristic values over successive trials. RTA*s learning policy
does not guarantee a convergence of heuristic values but in practice can find solutions more
quickly than LRTA*.
In Figure 18 we compare against RTA*. We include LRTA* in these plots as well to show
the tradeoff between convergence and initial goal achievement time RTA* makes. RTA*s
lookahead is based on a bounded depth first search, so its run time is difficult to predict. For
these experiments we ran RTA* with lookahead depths of {1, 5, 10, 20, 50, 100, 200, 400,
800, 1000, 1500, 2000, 3000, 4000, 8000, 10000, 16000} and chose the largest depth where
150

0

fiAchieving Goals Quickly Using Real-time Search

15-puzzle
factor of optimal GAT (log10)

factor of optimal GAT (log10)

platform

3

2

1

4
3
2
1
0

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

0

traffic
factor of optimal GAT (log10)

factor of optimal GAT (log10)

orz100d
5
4

LSS-LRTA*
dynamic fhat

3

-1

unit action duration (log10)

2
1
0

3

LSS-LRTA*
dynamic fhat

2

1

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 18: Comparison with RTA*.

instances were solved within the timeout. It is interesting to note that for the 15-puzzle
a depth of 1500 was able to be used, while in platform and traffic only a lookahead of 10
could be used. We attribute this to platform and traffic being very graphy domains, while
tiles has fewer cycles. The extreme case is grid pathfinding on the orz100d map where only
a maximum lookahead of 5 could solve instances within the timeout. For supplementary
comparison, we also provide a line for RTA* using an A* lookahead instead of depth first
search in the grid pathfinding domain. An expansion limit of 4000 was the largest lookahead
151

0

fiKiesel, Burns, & Ruml

size that solved all the instances. In the four plots in Figure 18, we can see that the newer
algorithms outperform RTA* in all domains.
7.9 Bugsy
Bugsy (Burns et al., 2013b) is not a real-time search, but it is an off-line algorithm that
explicitly attempts to optimize a utility function given as a linear combination of search
time and solution cost. If solution cost is specified in units of time, then Bugsy can
explicitly attempt to minimize goal achievement time by appropriately weighting search
and execution times so that they are given in the same units. As the only off-line algorithm
that can optimize our goal achievement time objective, it is interesting to see how Bugsy
compares to real-time algorithms. Since it performs a global search, it may be better able
to optimize cost, but it is inherently less efficient, as it cannot plan and execute in parallel.
Figure 19 shows the results. Bugsy tended to have the lowest goal achievement times
in all domains except for the traffic domain, where the dynamic lookahead f method nearly
dominated all other approaches. However, in all domains except the 15-puzzle, the advantage of Bugsy was small. We conclude that, if a full solution can be found upfront,
then off-line methods like Bugsy can often given the best results. When an agent must
respect real-time constraints, however, the dynamic lookahead f technique is the algorithm
of choice.
It may be possible to create a new algorithm that incorporates the ideas of Bugsy into
a real-time search. Bugsy proceeds like A*, but it orders its open list on a utility estimate
u(n) = wf  f (n) + wt  time(n), where time(n) is an estimate of the time the search will
take to reach the best solution beneath node n (for details, see Burns et al., 2013b). The
difficulty in incorporating the ideas of Bugsy into real-time search is that Bugsys utility
estimate assumes that none of the planning time, time(n), will occur in parallel with the
execution time f (n) (recall that cost is in units of time when optimizing goal achievement
time). In real-time search, this is not true. If solution cost is in units of time and all
planning happens during execution, then optimizing cost seems appropriate.

8. Conclusion
In this paper we considered real-time search in the context of minimizing goal achievement
time when concurrent planning and execution is possible. When optimizing goal achievement time, it is important to consider the tradeoff between searching and executing. We
presented three modifications to LSS-LRTA*: 1) taking single steps instead of moving all of
the way to the fringe of the lookahead search, 2) use multiple steps, but dynamically increase
the lookahead size to match the execution time of each trajectory, and 3) using f to correct
the bias in the heuristic. We then evaluated these techniques against plain LSS-LRTA*, A*,
Speedy, daRTAA*, daLSS-LRTA*, wLSS-LRTA*, FRIT, TBLRTA*, FALCONS, LRTA*,
RTA*, and Bugsy on four domains. In addition to the 15-puzzle and grid pathfinding
domains, which are classic heuristic search benchmarks, we used two video-game-inspired
domains: the platform domain and a new traffic domain.
We showed that committing to single actions at a time can give better performance
than using the traditional multiple action approach. Then we demonstrated that using the
multiple action technique can be even better than performing single steps if the amount of
152

fiAchieving Goals Quickly Using Real-time Search

15-puzzle
factor of optimal GAT (log10)

factor of optimal GAT (log10)

platform
2.4

1.6

0.8

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

-3

-2

orz100d

0

traffic

3

factor of optimal GAT (log10)

factor of optimal GAT (log10)

-1

unit action duration (log10)

2

1

1.8

1.2

0.6

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-3

-2

-1

unit action duration (log10)

Figure 19: Comparison with Bugsy.

lookahead search is dynamically adjusted to use all of the time available for the execution
of the currently-executing multi-step trajectory. We pointed out some possible reasons why
using an A*-based lookahead search may lead to poor performance and showed how f could
be used to fix these issues. Overall, the combination of a dynamically sized lookahead and
f gave the best performance when compared to previous real-time techniques. We hope
that this work will spur further research into applying real-time heuristic search to dynamic
domains.
153

0

fiKiesel, Burns, & Ruml

9. Acknowledgments
This work was supported in part by the NSF (grants 0812141 and 1150068), the DARPA
CSSG program (grant D11AP00242), and the University of New Hampshire Dissertation
Year Fellowship. A preliminary version of this work was published by Burns, Kiesel, and
Ruml (2013a).

Appendix A. Grid Pathfinding Results on Additional Maps
The following are a random sample of 10 maps from Sturtevants repository with the top
performing algorithms plotted. These plots are similar to those included in the paper with
the log10 factor of optimal goal achievement time on the y-axis and the log10 unit action
duration on the x-axis.

0.4

factor of optimal GAT (log10)

factor of optimal GAT (log10)

arena2
0.6

single-step f

0.2

0.24

single-step f
0.16

0.08

0

0
-3

-4

-2

-1

unit action duration (log10)

0

-4

-3

-2

-1

unit action duration (log10)

0

factor of optimal GAT (log10)

factor of optimal GAT (log10)

orz201d
0.024

single-step f
daRTAA*

0.016

0.008

0

0.12

single-step f

0.08

0.04

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

154

-3

-2

-1

unit action duration (log10)

0

fi1.2

factor of optimal GAT (log10)

factor of optimal GAT (log10)

Achieving Goals Quickly Using Real-time Search

single-step f
daRTAA*
daLSS-LRTA*

0.8

0.4

0.024

single-step f
0.016

0.008

0

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

factor of optimal GAT (log10)

factor of optimal GAT (log10)

isound1
daLSS-LRTA*
LSS-LRTA*

single-step f

-3

-2

-1

unit action duration (log10)

0

0.15

single-step f

0.1

0.05

0
-4

-3

2.4

-1

0

-4

w=2 LSS-LRTA*
LSS-LRTA*
dynamic fhat

1.6

-3

daRTAA*
daLSS-LRTA*

0.8

-2

-1

unit action duration (log10)

0

rmtst01
LSS-LRTA*
daRTAA*

factor of optimal GAT (log10)

factor of optimal GAT (log10)

-2

unit action duration (log10)

dynamic fhat

0
-4

-3

-2

-1

unit action duration (log10)

0

-4

155

-3

-2

-1

unit action duration (log10)

0

fiKiesel, Burns, & Ruml

Appendix B. Video Descriptions
Here we describe the videos that are available on-line (Kiesel et al., 2015b).
B.1 Platform Videos
The videos numbered 1-10 in the playlist show algorithms solving the Platform domain.
B.1.1 Random Instance
Videos 1-7 provide an example of a random instance of the Platform of the platform domain
being solved by various algorithm configurations.
Video 1 is LSS-LRTA* with a 1,000 node lookahead using a multi-step policy. In this
video you can see the algorithm get stuck in local heuristic minima and actively trying to
update its heuristic estimates for those states in the minimum.
Video 2 is LSS-LRTA* with a 1,000 node lookahead using a single-step policy. In this
video you can see the algorithm traverse the smaller sized local minima much more quickly
but still become stuck for a short while in the larger local minimum at around 9 seconds.
Video 3 is LSS-LRTA* using a dynamically sized lookahead. Initially, it gets stuck
inside of a local minimum, similar to LSS-LRTA* with a static lookahead, but soon is able
to escape through learning and increasing lookahead sizes.
Video 4 is LSS-LRTA* using a dynamically sized lookahead and heuristic correction. It
is very quickly able to escape the various heuristic minima on the way to the goal.
Video 5 is Speedy. In this video you can see that the algorithm is able to find a solution
very quickly as it starts moving almost instantly. The next 2 minutes of the video are spent
executing the highly suboptimal solution.
Video 6 is A*. We do not visualize the planning time for complete solution to be found
before A* begins moving (roughly 1 minute and 15 seconds, see video 7). Its solution is
optimal and very quickly gets the agent from its initial position to its goal.
Video 7 is a comparison of LSS-LRTA* with a 1,000 node lookahead, LSS-LRTA* with
a dynamically sized lookahead and heuristic correction, A* and Speedy. In this video,
planning time is visualized.
B.1.2 Ladder Instance
Videos 8-10 demonstrate an extreme example of a heuristic minimum. The visibility graph
heuristic assumes the agent is able to jump infinitely high, creating a large local minimum
that the agent must learn its way out of.
Video 8 shows LSS-LRTA* with a 1,000 node lookahead and a multi-step policy struggling to climb the platform ladder.
Video 9 shows LSS-LRTA* with a 1,000 node lookahead and a single-step lookahead
policy quickly climbing up the ladder but struggling at the end.
Video 10 shows an optimal example of how to climb the ladder using A*.
156

fiAchieving Goals Quickly Using Real-time Search

B.2 Traffic Videos
The remaining videos illustrate the traffic domain, a highly dynamic domain with many
moving obstacles that the agent must avoid. These videos are provided only as a visualization of the domain that the algorithms are trying to solve.
Video 11 shows an example of an optimal solution found by A* that never collides with
an obstacle.
Video 12 shows LSS-LRTA* with a 1,000 node lookahead solving the same problem. In
this video around 20 seconds, the agent wanders into a situation where a collision occurs
and is transported back to the start state.

References
Bjornsson, Y., Bulitko, V., & Sturtevant, N. (2009). TBA*: time-bounded A*. In Proceedings
of the Twenty-first International Joint Conference on Artificial Intelligence (IJCAI),
pp. 431436.
Bulitko, V., Lustrek, M., Schaeffer, J., Bjornsson, Y., & Sigmundarson, S. (2008). Dynamic
control in real-time heuristic search. Journal of Artificial Intelligence Research, 32,
419452.
Burns, E., Kiesel, S., & Ruml, W. (2013a). Experimental real-time heuristic search results
in a video game. In Proceedings of the Sixth Annual Symposium on Combinatorial
Search (SoCS).
Burns, E., Ruml, W., & Do, M. B. (2013b). Heuristic search when time matters. Journal
of Artificial Intelligence Research (JAIR), 47, 697740.
Furcy, D., & Koenig, S. (2000). Speeding up the convergence of real-time search. In
Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 891
897.
Hernandez, C., & Baier, J. (2012). Avoiding and escaping depressions in real-time heuristic
search. Journal of Artificial Intelligence Research (JAIR), 43, 523570.
Hernandez, C., Baier, J., Uras, T., & Koenig, S. (2012). Time-bounded adaptive A*. In
Proceedings of the Eleventh International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS).
Kiesel, S., Burns, E., & Ruml, W. (2015a).
Research code for heuristic search.
https://github.com/eaburns/search. Accessed September 2, 2015.
Kiesel, S., Burns, E., & Ruml, W. (2015b). Videos for achieving goals quickly using realtime search. http://bit.ly/1bW3Ey8. Accessed September 2, 2015.
Koenig, S., & Likhachev, M. (2002). D lite. In Proceedings of the Eighteenth National
Conference on Artificial Intelligence (AAAI), pp. 476483.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS).
Koenig, S., & Sun, X. (2008). Comparing real-time and incremental heuristic search for
real-time situated agents. In Journal of Autonomous Agents and Multi-Agent Systems,
pp. 18(3):313341.
157

fiKiesel, Burns, & Ruml

Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1990). Real-time heuristic search. Artificial intelligence, 42 (2-3), 189211.
Lustrek, M., & Bulitko, V. (2006). Lookahead pathology in real-time path-finding. In
Proceedings of the National Conference on Artificial Intelligence (AAAI), Workshop
on Learning For Search, pp. 108114.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence,
1, 193204.
Rivera, N., Illanes, L., Baier, J. A., & Hernandez, C. (2013). Reconnecting with the ideal
tree: An alternative to heuristic learning in real-time search. In Proceedings of the
Sixth International Symposium on Combinatorial Search (SoCS).
Rivera, N., Baier, J. A., & Hernandez, C. (2012). Weighted real-time heuristic search.
In Proceedings of the Twelfth International Conference on Autonomous Agents and
Multiagent Systems (AAMAS).
Sharon, G., Sturtevant, N. R., & Felner, A. (2013). Online detection of dead states in
real-time agent-centered search. In Proceedings of the Sixth Annual Symposium on
Combinatorial Search (SoCS).
Sturtevant, N. (2012). Benchmarks for grid-based pathfinding. Transactions on Computational Intelligence and AI in Games (TCIAIG), 4 (2), 144  148.
Sturtevant, N., & Bulitko, V. (2014). Reaching the goal in real-time heuristic search: Scrubbing behavior is unavoidable. In Proceedings of the Seventh Annual Symposium on
Combinatorial Search (SoCS).
Sturtevant, N. R., & Bulitko, V. (2011). Learning where you are going and from whence
you came: h-and g-cost learning in real-time heuristic search. In Proceedings of the
Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI), pp.
365370.
Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). On learning in agent-centered search.
In Proceedings of the Ninth International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 333340. International Foundation for Autonomous
Agents and Multiagent Systems.
Thayer, J. T., Dionne, A., & Ruml, W. (2011). Learning inadmissible heuristics during
search. In Proceedings of the Twenty-first International Conference on Automated
Planning and Scheduling (ICAPS).
Thayer, J. T., & Ruml, W. (2009). Using distance estimates in heuristic search. In Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS).

158

fiJournal of Artificial Intelligence Research 59 (2015) 59-82

Submitted 05/15; published 09/15

Solving #SAT and MaxSAT by Dynamic Programming
Sigve Hortemo Sther
Jan Arne Telle
Martin Vatshelle

sigve.sether@ii.uib.no
telle@ii.uib.no
martin.vatshelle@ii.uib.no

Department of Informatics, University of Bergen
Bergen, Norway

Abstract
We look at dynamic programming algorithms for propositional model counting, also
called #SAT, and MaxSAT. Tools from graph structure theory, in particular treewidth,
have been used to successfully identify tractable cases in many subfields of AI, including
SAT, Constraint Satisfaction Problems (CSP), Bayesian reasoning, and planning. In this
paper we attack #SAT and MaxSAT using similar, but more modern, graph structure
tools. The tractable cases will include formulas whose class of incidence graphs have not only
unbounded treewidth but also unbounded clique-width. We show that our algorithms extend
all previous results for MaxSAT and #SAT achieved by dynamic programming along
structural decompositions of the incidence graph of the input formula. We present some
limited experimental results, comparing implementations of our algorithms to state-of-the-art
#SAT and MaxSAT solvers, as a proof of concept that warrants further research.

1. Introduction
The propositional satisfiability problem (SAT) is a fundamental problem in computer science
and in AI. Many real-world applications such as planning, scheduling, and formal verification
can be encoded into SAT and a SAT solver can be used to decide if there exists a solution.
To decide how many solutions there are, the propositional model counting problem (#SAT),
which finds the number of satisfying assignments, could be useful. If there are no solutions,
it may be interesting to know how close we can get to a solution. When the propositional
formula is encoded in Conjunctive Normal Form (CNF) this may be solved by the maximum
satisfiability problem (MaxSAT), which finds the maximum number of clauses that can
be satisfied by some assignment. In this paper we investigate classes of CNF formulas
where these two problems, #SAT and MaxSAT, can be solved in polynomial time. Tools
from graph structure theory, in particular treewidth, have been used to successfully identify
tractable cases in many subfields of AI, including SAT, Constraint Satisfaction Problems
(CSP), Bayesian reasoning, and planning (Bacchus, Dalmao, & Pitassi, 2003; Darwiche,
2001; Fischer, Makowsky, & Ravve, 2008; Samer & Szeider, 2010). In this paper we
attack #SAT and MaxSAT using similar, but more modern, graph structure tools. The
tractable cases will include formulas whose class of incidence graphs have not only unbounded
treewidth but also unbounded clique-width.
Both #SAT and MaxSAT are significantly harder than simply deciding if a satisfying
assignment exists. #SAT is #P-hard (Garey & Johnson, 1979) even when restricted
to Horn 2-CNF formulas, and to monotone 2-CNF formulas (Roth, 1996). MaxSAT is
NP-hard even when restricted to Horn 2-CNF formulas (Jaumard & Simeone, 1987), and to
c
2015
AI Access Foundation. All rights reserved.

fiSther, Telle & Vatshelle

2-CNF formulas where each variable appears at most 3 times (Raman, Ravikumar, & Rao,
1998). Both problems become tractable under certain structural restrictions obtained by
bounding width parameters of graphs associated with formulas (Fischer, Makowsky, & Ravve,
2008; Ganian, Hlineny, & Obdrzalek, 2013; Samer & Szeider, 2010; Szeider, 2003). The
work we present here is inspired by recent results in the work of Paulusma, Slivovsky, and
Szeider (2013) and also in the work of Slivovsky and Szeider (2013) showing that #SAT
is solvable in polynomial time when the incidence graph1 I(F ) of the input formula F has
bounded modular treewidth, and more strongly, bounded symmetric clique-width.
These tractability results work by dynamic programming along a decomposition of I(F ).
There are two steps involved: (1) find a good decomposition, and (2) perform dynamic
programming along the decomposition. The goal is to have a fast runtime, and this is usually
expressed as a function of some known graph width parameter of the incidence graph I(F )
of the formula F , like its tree-width. Step (1) is solved by a known graph algorithm for
computing a decomposition of low (tree-)width, while step (2) solves #SAT or MaxSAT
by dynamic programming with runtime expressed in terms of the (tree-)width k of the
decomposition.
The algorithms we give in this paper also work by dynamic programming along a
decomposition, but in a slightly different framework. Since we are not solving a graph
theoretic problem, expressing runtime by a graph theoretic parameter may be a limitation.
Therefore, our strategy will be to develop a framework based on the following strategy
(A) consider, for #SAT or MaxSAT, the amount of information needed to combine
solutions to subproblems into global solutions, then
(B) define the notion of good decompositions based on a parameter that minimizes this
information, and then
(C) design a dynamic programming algorithm along such a decomposition with runtime
expressed by this parameter
Both in the work of Paulusma et al. (2013) and in that of Slivovsky and Szeider (2013)
two assignments are considered to be equivalent if they satisfy the same set of clauses.
When carrying out (A) for #SAT and MaxSAT this led us to the concept of ps-value of a
CNF formula. Let us define it and give an intuitive explanation. A subset C of the clauses
of a CNF formula F is called projection satisfiable if there is some complete assignment
satisfying every clause in C but not satisfying any clause not in C. The ps-value of F is
the number of projection satisfiable subsets of clauses. Let us consider its connection to
dynamic programming, which in general applies when an optimal solution can be found
by combining optimal solutions to certain subproblems. For #SAT and MaxSAT these
subproblems, at least in the cases we consider, take the form of a subformula of F induced
by a subset S of clauses and variables, i.e. first remove from F all variables not in S and
then remove all clauses not in S. Consider for simplicity the two subproblems FS and FS
defined by S and its complement S. When combining the solutions to FS and FS , in order
1. I(F ) is the bipartite incidence graph between the clauses of F on the one hand and the variables of F on
the other hand. Information about positive or negative occurrences of variables is not encoded in I(F ) so
sometimes a signed or directed version is used that includes also this information.

60

fiSolving #SAT and MaxSAT by Dynamic Programming

to find solutions to F , it seems clear that we must consider a number of cases at least as
big as the ps-values of the two disjoint subformulas crossing between S and S, i.e. the
subformulas obtained by removing from clauses in S the variables of S, and by removing
from clauses in S the variables of S. See Figure 2 for an example.
We did not find in the literature a study of the ps-value of CNF formulas, so we start by
asking for a characterization of formulas having low ps-value. We were led to the concept of
the mim-value of I(F ), which is the size of a maximum induced matching of I(F ), where
an induced matching is a subset M of edges with the property that any edge of the graph
is incident to at most one edge in M . Note that this value can be much lower than the
size of a maximum matching, e.g. any complete bipartite graph has mim-value 1. We show
that the ps-value of F is upper bounded by the number of clauses of F raised to the power
of the mim-value of I(F ), plus 1. For a CNF formula F where I(F ) has mim-value 1 the
interpretation of this result is straightforward: its clauses can be totally ordered such that for
any two clauses C < C 0 the variables occurring in C are a subset of the variables occurring
in C 0 , and this has the implication that the number of subsets of clauses for which some
complete assignment satisfies exactly this subset is at most the number of clauses plus 1.
Families of CNF formulas having small ps-value are themselves of algorithmic interest,
but in this paper we continue with part (B) of the above strategy, and focus on how to
decompose a CNF formula F based on the concept of ps-value. A common way to decompose
a mathematical object is to recursively partition its ground set into two parts, giving a
binary tree whose root represents the ground set and whose leaves are bijectively mapped
to the elements of the ground set. Taking the ground set of F to be the set containing its
clauses and its variables, this is how we will decompose F , in other words by a binary tree
whose leaves are in 1-1 correspondence with the variables and clauses. A node of the binary
tree represents the subset X of variables and clauses at the leaves of its subtree. Which
decomposition trees are good for efficiently solving #SAT and MaxSAT? In accordance
with the above discussion under part (A) the answer is that the good decomposition trees
are those where all subformulas crossing between X and X, for some X defined by a node
of the tree, have low ps-value. See Figure 2 for an example. To define this informal notion
precisely we use the concept of a branch decomposition over the ground set of a formula with
cut function being the ps-value of the formulas crossing the cut. Branch decompositions are
by now a standard notion in graph and matroid theory, originating in the work of Robertson
and Seymour on graph minors (Robertson & Seymour, 1991). This way we arrive at the
definition of the ps-width of a CNF formula F , and of the decompositions of F that achieve
this ps-width. It is important to note that a formula can have ps-value exponential in
formula size while ps-width is polynomial, and that in general the class of formulas of low
ps-width is much larger than the class of formulas of low ps-value.
To finish the above strategy, we must carry out part (C) and show how to solve #SAT
and MaxSAT by dynamic programming along the branch decomposition of the formula,
and express its runtime as a function of the ps-width. This is not complicated, as dynamic
programming when everything has been defined properly simply becomes an exercise in
brute-force computation of the sufficient and necessary information, but it is technical and
quite tedious. It leads to the following theorem.
Theorem 2. Given a formula F over n variables and m clauses, and a decomposition of F
of ps-width k, we solve #SAT and weighted MaxSAT in time O(k 3 m(m + n)).
61

fiSther, Telle & Vatshelle

Thus, given a decomposition having a ps-width k that is polynomially-bounded in the
number of variables n and clauses m of the formula, we get polynomial-time algorithms.
Let us compare our result to the strongest previous result in this direction, namely the
work of Slivovsky and Szeider (2013) for #SAT. Their algorithm takes as input a branch
decomposition over the vertex set of I(F ), which is the same as the ground set of F , and
evaluates its runtime by the cut function they call index. They show that this cut function is
closely related to the symmetric clique-width scw of the given decomposition, giving runtime
(n + m)O(scw) . Considering the clique-width cw of the given decomposition the runtime of the
cw
work of Slivovsky and Szeider (2013) becomes (n + m)O(2 ) since symmetric clique-width
and clique-width is related by the essentially tight inequalities 0.5cw  scw  2cw (Courcelle,
2004). Their algorithm is thus a polynomial-time algorithm if given a decomposition with
constantly bounded scw. The result of Theorem 2 encompasses this, since our Corollary 1
ties ps-width to mim-width and the work of Vatshelle (2012) shows that mim-width is upper
bounded by clique-width, see also the work of Rao (2008) for symmetric clique-width, so
that a decomposition of I(F ) having constantly bounded (symmetric) clique-width also has
polynomially bounded ps-width. In this way, given the decomposition assumed as input in
the work of Slivovsky and Szeider (2013), the algorithm of Theorem 2 will have runtime
O(m3cw s), for cw the clique-width of the given decomposition.
In the work of Brault-Baron, Capelli, and Mengel (2014), appearing after a preliminary
presentation of our results (Sther, Telle, & Vatshelle, 2014), it is argued that the framework
behind Theorem 2 gives a uniform explanation of all tractability results for #SAT in the
literature, in particular those using dynamic programming based on structural decompositions
of the incidence graph. The work of Brault-Baron et al. (2014) also goes beyond this, giving
a polynomial-time algorithm, not by dynamic programming, to solve #SAT on -acyclic
CNF formulas, being exactly those formulas whose incidence graphs are chordal bipartite.
They show that these formulas do not have bounded ps-width and that their incidence
graphs do not have bounded mim-width. See Figure 1 which gives an overview of the results
in this paper and in other papers.
Using the concept of mim-width of graphs, introduced in the thesis of Vatshelle (2012), and
the connection between ps-value and mim-value alluded to earlier, we show that a rich class of
formulas, including classes of unbounded clique-width, have polynomially bounded ps-width
and are thus covered by Theorem 2. Firstly, this holds for classes of formulas having incidence
graphs that can be represented as intersection graphs of certain objects, like interval graphs
(Belmonte & Vatshelle, 2013). Secondly, it holds also for the much larger class of bipartite
graphs achieved by taking bigraph bipartizations of these intersection graphs, obtained by
imposing a bipartition on the vertex set and keeping only edges between the partition classes.
Some such bigraph bipartizations have been studied previously, in particular the interval
bigraphs. The interval bigraphs contain all bipartite permutation graphs, and these latter
graphs have been shown to have unbounded clique-width (Brandstadt & Lozin, 2003). See
Figure 1.
Let us discuss step (1), finding a good decomposition. Note that Theorem 2 assumes
that the input formula is given along with a decomposition of some ps-width k. The value k
need not be optimal, so any heuristic finding a reasonable branch decomposition could be
used in practice. Computing decompositions of optimal ps-width is probably not doable in
62

fiSolving #SAT and MaxSAT by Dynamic Programming

iden
The inc

F

h of F
ce grap

ps-width mk

#SAT poly. Paper A.
#SAT poly. Paper B.

chordal
bipartite

#SAT and MaxSAT poly.
This paper.

-acyclic

ps-width m2
MIM-width k
clique-width k
symmetric
clique-width k/2
modular
treewidth k/2

ps-width m
k-trapezoid
bigraph

circular arc
bigraph
interval bigraph
bipartite
permutation

Figure 1: We believe, as argued in the work of Brault-Baron et al. (2014), that any dynamic
programming approach working along a structural decomposition to solve #SAT
(or MaxSAT) in polynomial time cannot go beyond the green box. Paper A is by
Brault-Baron et al. (2014) and Paper B is by Slivovsky and Szeider (2013). On
the left of the two dashed lines are 4 classes of graphs with bound k/2 or k on
some structural graph width parameter, and 5 classes of bipartite graphs. On the
right are -acyclic CNF formulas and 3 classes of CNF formulas with ps-width
varying from linear in the number of clauses m, to m2 and mk . There is an arc
from P to Q if any formula F or incidence graph I(F ) having property P also has
property Q. This is a Hasse diagram, so lack of an arc in the transitive closure
means this relation provably does not hold.
polynomial-time, but the complexity of this question is not adressed in this paper. However,
we are able to efficiently decide if a CNF formula has a certain linear structure guaranteeing
low ps-width. By combining an alternative definition of interval bigraphs (Hell & Huang,
2004) with a fast recognition algorithm (Muller, 1997; Rafiey, 2012) we arrive at the
following. Say that a CNF formula F has an interval ordering if there exists a total ordering
of variables and clauses such that for any variable x occurring in clause C, if x appears
before C then any variable between them also occurs in C, and if C appears before x then x
occurs also in any clause between them.
Theorem 6. Given a formula F over n variables and m clauses each of at most t literals.
In time O((m + n)mn) we can decide if F has an interval ordering (yes iff I(F ) is an
interval bigraph), and if yes we solve #SAT and weighted MaxSAT with an additional
runtime of O(min{m2 , 4t }(m + n)m).
Formulas with an interval ordering are precisely those whose incidence graphs are interval
bigraphs, so Theorem 6 encompasses classes of formulas whose incidence graphs have
unbounded clique-width.
Could parts of our algorithms be of interest for practical applications? Answering this
question is beyond the scope of the present paper. However, we have performed some limited
testing, in particular for formulas with a linear structure, as a simple proof of concept. All
our code can be found online (Sther, Telle, & Vatshelle, 2015). We have designed and
implemented a heuristic for step (1) finding a good decomposition, in this case a linear
63

fiSther, Telle & Vatshelle

one where the binary tree describing the decomposition is a path with attached leaves. We
have also implemented step (2) dynamic programming solving #SAT and MaxSAT along
such decompositions. We then run (1) followed by (2) and compare against one of the
best MaxSAT solvers from the Max-SAT-2014 event of the SAT-2014 conference and the
latest version of the #SAT solver called sharpSAT (Thurley, 2006). These solvers beat our
implementation on most inputs, which is not suprising since our code does not include any
techniques beyond our algorithm. Nevertheless, we were able to generate some classes of
CNF formulas having interval orderings where our implementation is by far the better. This
lends support to our belief that methods related to ps-value warrants further research to
investigate if they could be useful in practice.
Our paper is organized as follows. In Section 2 we give formal definitions of ps-value
and ps-width of a CNF formula and show the central combinatorial lemma linking ps-value
of a formula to the size of the maximum induced matching in the incidence graph of the
formula. In Section 3 we present dynamic programming algorithms that given a formula and
a decomposition solves #SAT and weighted MaxSAT, proving Theorem 2. In Section 4
we investigate classes of formulas having decompositions of low ps-width, basically proving
the correctness of the hierarchy presented in Figure 1. In Section 5 we consider formulas
having an interval ordering and prove Theorem 6. In Section 6 we present the results of the
implementations and testing. We end in Section 7 with some open problems.

2. Framework
We consider propositional formulas in Conjunctive Normal Form (CNF). A literal is a
propositional variable or a negated variable, x or x, a clause is a set of literals, and a
formula is a multiset of clauses. For a formula F , cla(F ) denotes the clauses in F . The
incidence graph of a formula F is the bipartite graph I(F ) having a vertex for each clause
and variable, with variable x adjacent to any clause C in which it occurs. We consider
only input formulas where I(F ) is connected, as otherwise we would solve our problems
on the separate components of I(F ). For a clause C, lit(C) denotes the set of literals
in C and var(C) denotes
the variables of the literals in lit(C). For a formula F , var(F )
S
denotes the union Ccla(F ) var(C). For a set X of variables, an assignment of X is a
function  : X  {0, 1}. For a literal `, we define  (`) to be 1   (var(`)) if ` is a negated
variable (` = x for some variable x) and to be  (var) otherwise (` = x for some variable
x). A clause C is said to be satisfied by an assignment  if there exists at least one literal
`  lit(C) so that  (`) = 1. Any clause which an assignment  does not satisfy is said to be
falsified by  . We notice that this means an empty clause will be falsified by all assignments.
A formula is satisfied by an assignment  if  satisfies all clauses in cla(F ).
The problem #SAT, given a formula F , asks how many distinct assignments of var(F )
satisfy F . The optimization problem weighted MaxSAT, given a formula
P F and weight
function w : cla(F )  N, asks what assignment  of var(F ) maximizes C w(C) for all
C  cla(F ) satisfied by  . The problem MaxSAT asks for the maximum number of satisfied
clauses that can be achieved, equivalent to weighted MaxSAT where all clauses have weight
one. For weighted MaxSAT, we assume the sum of all the weights are at most 2O(cla(F )) ,
and thus we can do summation on the weights in time linear in cla(F ).
64

fiSolving #SAT and MaxSAT by Dynamic Programming

For a set A, with elements from a universe U we denote by A the elements in U \ A, as
the universe is usually given by the context.
2.1 Cut of a Formula
In this paper, we will solve MaxSAT and #SAT by the use of dynamic programming.
We will be using a divide and conquer technique where we solve the problem on smaller
subformulas of the original formula F and then combine the solutions to each of these smaller
formulas to form a solution to the entire formula F . Note however, that the solutions found
for a subformula will depend on the interaction between the subformula and the remainder
of the formula. We use the following notation for subformulas.
For a clause C and set X of variables, by C|X we denote the clause {`  C : var(`)  X}.
We say C|X is the clause C induced by X. Unless otherwise specified, all clauses mentioned
in this paper are from the set cla(F ) (e.g., if we write C|X  cla(F 0 ), we still assume C 
cla(F )). For a formula F and subsets C  cla(F ) and X  var(F ), we say the subformula
FC,X of F induced by C and X is the formula consisting of the clauses {Ci |X : Ci  C}. That
is, FC,X is the formula we get by removing all clauses not in C followed by removing each
literal of a variable not in X. For a set C of clauses, we denote by C|X the set {C|X : C  C}.
As with a clause, for an assignment  over a set X of variables, we say the assignment 
induced by X 0  X is the assignment  |X 0 where the domain is restricted to X 0 .
For a formula F and sets C  cla(F ), X  var(F ), and S = C  X, we call S a cut of F
and note that it breaks F into four subformulas FC,X , FC,X , FC,X , and FC,X . See Figure 2.
One important fact we may observe from this definition is that a clause C in F is satisfied
by an assignment  of var(F ), if and only if C (induced by X or X) is satisfied by  in at
least one of the formulas of any cut of F .
2.2 Projection Satisfiable Sets and ps-value of a Formula
For a formula F and assignment  of some of the variables in var(F ), we denote by sat(F,  )
the inclusion maximal set C  cla(F ) so that each clause in C is satisfied by  . If for a
set C  cla(F ) we have sat(F,  ) = C for some  over all the variables in var(F ), then C
is known as a projection (Kaski, Koivisto, & Nederlof, 2012; Slivovsky & Szeider, 2013)
and we say C is projection satisfiable in F . We denote by PS(F ) the family of all projection
satisfiable sets in F . That is,
PS(F ) = {sat(F,  ) :  is an assignment of the entire set var(F )}.
The cardinality of this set, |PS(F )|, is referred to as the ps-value of F .
To get a grasp of the structure of formulas having low ps-value we consider induced
matchings in the incidence graph of a formula. The incidence graph of a formula F is the
bipartite graph I(F ) having a vertex for each clause and variable, with variable x adjacent
to any clause C in which it occurs. An induced matching in a graph is a subset M of edges
with the property that any edge of the graph is incident to at most one edge in M . In other
words, for any 3 vertices a, b, c, if ab is an edge in M and bc is an edge then there does not
exist an edge cd in M . The number of edges in M is called the size of the induced matching.
The following result provides an upper bound on the ps-value of a formula in terms of the
maximum size of an induced matching of its incidence graph.
65

fiSther, Telle & Vatshelle

Lemma 1. Let F be a CNF formula with no clause containing more than t literals, and
let k be the maximum size of an induced matching in I(F ). We then have |PS(F )| 
min{|cla(F )|k + 1, 2tk }.
Proof. We first argue that |PS(F )|  |cla(F )|k + 1. Let C  PS(F ) and Cf = cla(F ) \ C.
Thus, there exists a complete assignment  such that the clauses not satisfied by  are
Cf = cla(F ) \ sat(F,  ). Since every variable in var(F ) appears in some clause of F this
means that  |var(Cf ) is the unique assignment of the variables in var(Cf ) which do not satisfy
0
0
any clause of Cf . Let Cf  Cf be an inclusion minimal set such that var(Cf ) = var(Cf ),
hence  |var(Cf ) is also the unique assignment of the variables in var(Cf ) which do not satisfy
0
0
any clause of Cf . An upper bound on the number of different such minimal Cf , over all
0
C  PS(F ), will give an upper bound on |PS(F )|. For every C  Cf there is a variable vC
0
0
appearing in C and no other clause of Cf , otherwise Cf would not be minimal. Note that we
have an induced matching M of I(F ) containing all such edges vC , C. By assumption, the
0
induced matching M can have at most k edges and hence |Cf |  k. It is easy to show by
induction on k that there are at most |cla(F )|k + 1 sets of at most k clauses and the lemma
follows.
We now argue that |PS(F )|  2tk . As the maximum induced matching has size k there
is some set C of k clauses so that var(C) = var(F ). As each clause C  C has |var(C)|  t,
we have |var(F )| = |var(C)|  tk. As there are no more than 2|var(F )| assignments for F ,
the PS-value of F is upper bounded by 2tk .
2.3 The ps-width of a Formula
We define a branch decomposition of a formula F to be a pair (T, ) where T is a rooted
binary tree and  is a bijective function from the leaves of T to the clauses and variables
of F . If all the non-leaf nodes (also referred to as internal nodes) of T induce a path, we
say that (T, ) is a linear branch decomposition. For a non-leaf node v of T , we denote
by (v) the set {(l) : l is a leaf in the subtree rooted in v}. Based on this, we say that the
decomposition (T, ) of formula F induces certain cuts of F , namely the cuts defined by (v)
for each node v in T .
For a formula F and branch decomposition (T, ), for each node v in T , by Fv we denote
the formula induced by the clauses in cla(F ) \ (v) and the variables in (v), and by Fv
we denote the formula on the complement sets; i.e. the clauses in (v) and the variables
in var(F ) \ (v). In other words, if (v) = C  X with C  cla(F ) and X  var(F ) then
Fv = FC,X and Fv = FC,X . To simplify the notation, we will for a node v in a branch
decomposition and a set C of clauses denote by C|v the set C|var(Fv ) . We define the ps-value
of the cut (v) to be
ps((v)) = max{|P S(Fv )|, |P S(Fv )|}
We define the ps-width of a branch decomposition to be
psw(T, ) = max{ps((v)) : v is a node of T }
We define the ps-width of a formula F to be
psw(F ) = min{psw(T, ) : (T, ) is a branch decomposition of F }
66

fiSolving #SAT and MaxSAT by Dynamic Programming

v
x4
x3 c4 x5 c2
x1

x2

c1

C
c1 = {x1 , x2 }
c3 = {x2 , x4 , x5 }

c3
FC,X = Fv

X

FC,X

FC,X

x1 x2

x3 x4
x5

FC,X = Fv

c2 = {x1 , x2 , x3 }
c4 = {x2 , x3 , x5 }

X

C

Figure 2: On top is a branch decomposition of a formula F with var(F ) = {x1 , x2 , x3 , x4 , x5 }
and the 4 clauses cla(F ) = {c1 , c2 , c3 , c4 } as given in the boxes. The node
v of the tree defines the cut (v) = C  X where C = {c1 , c3 } and X =
{x1 , x2 }. There are 4 subformulas defined by this cut: FC,X , FC,X , FC,X , FC,X .
For example, FC,X = {{x1 , x2 }, {x2 }} and FC,X = {, {x4 , x5 }}. We have
Fv = FC,X and Fv = FC,X with projection satisfiable sets of clauses PS(Fv ) =
{{c2 |v }, {c4 |v }, {c2 |v , c4 |v }} and PS(Fv ) = {, {c3 |v }} and the ps-value of this cut
is ps((v)) = max{|P S(Fv )|, |P S(Fv )|} = 3.

Note that the ps-value of a cut is a symmetric function. That is, the ps-value of cut S
equals the ps-value of the cut S. See Figure 2 for an example.

3. Dynamic Programming for MaxSAT and #SAT
Given a branch decomposition (T, ) of a CNF formula F over n variables and m clauses
and of total size s, we will give algorithms that solve MaxSAT and #SAT on F in time
O(psw(T, )3 m(m + n)). Our algorithms are strongly inspired by the work of Slivovsky and
Szeider (2013), but in order to achieve a runtime polynomial in ps-width, and also to solve
MAXSAT, we must make some crucial changes. In particular, we must index the dynamic
progranming tables by PS-sets rather than the shapes used in the work of Slivovsky and
Szeider (2013).
Let us discuss some special terminology to be used in this section. In this dynamic
programming section, we will combine partial solutions to subformulas into solutions for
the input formula F . To improve readability we introduce notation P S 0 and sat0 that
allows us to refer directly to the clauses of F , also when working on the subformulas.
67

fiSther, Telle & Vatshelle

Thus, for a formula F and branch decomposition (T, ), for each node v in T , and induced
subformula Fv of F , by PS0 (Fv ) we denote the subsets of clauses C from cla(F ) \ (v) so
that PS(Fv ) = C|var(Fv ) . Similarly, for an assignment  over var(Fv ), by sat0 (Fv ,  ) we
denote the set of clauses C from cla(F ) \ (v) so that sat(Fv ,  ) = C|var(Fv ) . Note that
|PS0 (Fv )| = |PS(Fv )| and |sat0 (Fv ,  )| = |sat(Fv ,  )|. We take the liberty to call also these
sets projection satisfiable and refer to them as PS-sets in the text, but it will be clear from
context that we mean clauses of cla(F ) and not cla(Fv ).
Let us discuss some implementation details. We regard PS-sets as boolean vectors of
length |cla(F )|, and assume we can identify clauses and variables by integer numbers. So,
checking if a clause is in a PS-set can be done in constant time, and checking if two PS-sets
are equal can be done in O(|cla(F )|) time. To manage our PS-sets, we use a binary trie
datastructure (Fredkin, 1960). We can add and retrieve a PS-set to and from a trie in
O(|cla(F )|) time. Trying to add a PS-set to a trie already containing an equivalent PS-set
will not alter the content of the trie, so our tries will only contain distinct PS-sets. As
retrieval of an element in our trie takes O(|cla(F )|) time, by assigning a distinct integer
to each PS-set at the time it is added to the trie, we have a O(|cla(F )|)-time mapping
from PS-sets to distinct integers. This will be used implicitly in our algorithms when we
say we index by PS-sets; when implementing the algorithm we will instead index by the
corresponding integer the PS-set is mapped to.
In a pre-processing step we will need the following which, for each node v in T computes
the sets of projection satisfiable subsets of clauses PS0 (Fv ) and PS0 (Fv ) of the two crossing
subformulas Fv and Fv .
Theorem 1. Given a CNF formula F with a branch decomposition (T, ) of ps-width k, we
can in time O(k 2 m(m + n)) compute the sets PS0 (Fv ) and PS0 (Fv ) for each v in T .

Proof. We notice that for a node v in T with children c1 and c2 , we can express PS0 (Fv ) as


C1  PS0 (Fc1 ), and
0
PS (Fv ) = (C1  C2 )  cla(Fv ) :
.
C2  PS0 (Fc2 )

Similarly, for sibling s and parent p of v in T , the set PS0 (Fv ) can be expressed as


Cp  PS0 (Fp ), and
0
PS (Fv ) = (Cp  Cs )  cla(Fv ) :
.
Cs  PS0 (Fs )
By transforming these recursive expressions into a dynamic programming algorithm, as
done in Procedure 1 and Procedure 2 below, we are able to calculate all the desired sets as
long as we can compute the sets for the base cases PS0 (Fl ) when l is a leaf of T , and PS0 (Fr )
for the root r of T . However, these formulas contain at most one variable, and thus we can
easily construct their set of projection satisfiable clauses in linear amount of time for each
of the formulas. For the rest of the formulas, we construct the formulas using Procedure 1
and Procedure 2. As there are at most twice as many nodes in T as there are clauses and
variables in F , the procedures will run at most O(|cla(F )| + |var(F )|) times. In each run
of the algorithms, we iterate through at most k 2 pairs of projection satisfiable sets, and do a
constant number of set operations that might take O(|cla(F )|) time each. This results in a
total runtime of O(k 2 |cla(F )|(|cla(F )| + |var(F )|)) = O(k 2 m(m + n)) for all the nodes of
T combined.
68

fiSolving #SAT and MaxSAT by Dynamic Programming

Procedure 1: Generating PS0 (Fv )
input: PS0 (Fc1 ) and PS0 (Fc2 ) for children c1 and c2 of v
in branch decomposition
output: PS0 (Fv )
L  empty trie of projection satisfiable clause-sets
for each (C1 , C2 )  PS0 (Fc1 )  PS0 (Fc2 ) do
add (C1  C2 )  cla(Fv ) to L
return L
Procedure 2: Generating PS0 (Fv )
input: PS0 (Fs ) and PS0 (Fp ) for sibling s and parent p of v
in branch decomposition
output: PS0 (Fv )
L  empty trie of projection satisfiable clause-sets
for each (Cs , Cp )  PS0 (Fs )  PS0 (Fp ) do
add (Cs  Cp )  cla(Fv ) to L
return L
We now move on to the dynamic programming proper. We first give the algorithm for
MaxSAT and then briefly describe the changes necessary for solving weighted MaxSAT
and #SAT.
Our algorithm uses the technique of expectation introduced in the work of Bui-Xuan,
Telle, and Vatshelle (2010, 2011). Some partial solutions might be good when combined
with certain partial solutions, but bad when combined with others. In the technique of
expectation we categorize how partial solutions can interact, and then optimize our selection
of partial solutions based on the expectation that this interaction occurs. In our dynamic
programming algorithm for MaxSAT, we apply this technique by making expectations on
each cut regarding what set of clauses will be satisfied by variables of the opposide side of
the cut.
For a node v in the decomposition of F and PS-sets C  PS0 (Fv ) and C 0  PS0 (Fv ), we
say that an assignment  of var(F ) meets the expectation C and C 0 if sat0 (Fv ,  |v ) = C
and sat0 (Fv ,  |v ) = C 0 . For each node v of the branch decomposition, our algorithm uses a
table Tabv that for each pair (C, C 0 )  PS0 (Fv )  PS0 (Fv ) stores in Tabv (C, C 0 ) the maximum
number of clauses in (v) that are satisfied, over all assignments meeting the expectation
of C and C 0 . As the variables in var(F ) \ (v) satisfy exactly C 0 , for any assignment that
meets this expectation, an equivalent formulation of the content of Tabv (C, C 0 ) is that it
must satisfy the following constraint:
Over all assignments  of var(F )  (v) such that sat0 (Fv ,  ) = C ,
fi	
fi

Tabv (C, C 0 ) = max fi sat0 (F,  0 )  (v)  C 0 fi

(1)



By bottom-up dynamic programming along the tree T we compute the tables of each
node of T . For a leaf l in T , generating Tabl can be done easily in linear time since the
formula Fv contains at most one variable. For an internal node v of T , with children c1 , c2 ,
69

fiSther, Telle & Vatshelle

we compute Tabv by the algorithm described in Procedure 3. There are 3 tables involved in
this update, one at each child and one at the parent. A pair of entries, one from each child
table, may lead to an update of an entry in the parent table. Each table entry is indexed by
a pair, thus there are 6 indices involved in a single potential update. A trick first introduced
in the work of Bui-Xuan et al. (2011) allows us to loop over triples of indices and for each
triple compute the remaining 3 indices forming the 6-tuple involved in the update, thereby
reducing the runtime.
Procedure 3: Computing Tabv for inner node v with children c1 , c2
input: Tabc1 , Tabc2
output: Tabv
1. initialize Tabv : PS0 (Fv )  PS0 (Fv )  {1}
2. for each (Cc1 , Cc2 , Cv0 ) in PS0 (Fc1 )  PS0 (Fc2 )  PS0 (Fv ) do
3.
Cc0 1  (Cc2  Cv0 )  (c1 )
4.
Cc0 2  (Cc1  Cv0 )  (c2 )
5.
Cv  (Cc1  Cc2 ) \ (v)
6.
t  Tabc1 (Cc1 , Cc0 1 ) + Tabc2 (Cc2 , Cc0 2 )
7.
if Tabv (Cv , Cv0 ) < t then Tabv (Cv , Cv0 )  t
8. return Tabv
Lemma 2. For a CNF formula F of m clauses and an inner node v, of a branch decomposition (T, ) of ps-width k, Procedure 3 computes Tabv satisfying Constraint (1) in time
O(k 3 m).
Proof. We assume Tabc1 and Tabc2 satisfy Constraint (1). Procedure 3 loops over all triples
in PS0 (Fc1 )  PS0 (Fc2 )  PS0 (Fv ). From the definition of ps-width of (T, ) there are at most
k 3 such triples. Each operation inside an iteration of the loop take O(m) time and there is
a constant number of such operations. Thus the runtime is O(k 3 m).
Before we show the correctness of the output, let us look a bit at the workings of
Procedure 3. For any assignment  over var(F ), and cut, the assignment  will only meet
the expectation of a single pair of PS-sets. Let (X1 , X10 ), (X2 , X20 ) and (Xv , Xv0 ) be the pairs
an assignment  meets the expectation for with respect to the cuts induced by c1 , c2 , and v,
respectively. We notice that
Xv = sat0 (Fv ,  |v )

= sat0 (Fv ,  |c1 ]  |c2 )

= sat0 (Fv ,  |c1 )  sat0 (Fv ,  |c2 )

= (sat0 (Fc1 ,  |c1 ) \ (v))  (sat0 (Fc2 ,  |c2 ) \ (v))

(2)

= (X1 \ (v))  (X2 \ (v))
= (X1  X2 ) \ (v).

This can also be seen from Figure 3. By symmetry, we find similar values for X10 and X20 ;
namely X10 = (X2  Xv0 )  (c1 ) and X20 = (X1  Xv0 )  (c2 ). So, these latter three sets
will be implicit based on the three former sets with respect to the cuts induced by v, c1
and c2 . We will therefore, for convenience of this proof, say that an assignment  meets the
70

fiSolving #SAT and MaxSAT by Dynamic Programming

= X1 = sat (Fc1 ,  |c1 )

clauses in cla(F ) \ (v)

= X2 = sat (Fc2 ,  |c2 )
= Xv = sat (Fv ,  |v )

clauses in (c2 )

clauses in (c1 )

Figure 3: As shown by the chain of equalities in (2) in the proof of Lemma 2, the clauses in
sat0 (Fv ,  |v ) are precisely the clauses in (sat0 (Fc1 ,  |c1 )  sat0 (Fc2 ,  |c2 )) \ (v).
expectation of a triple (C1 , C2 , C 0 ) of PS-sets, when  meets the expectation of the implicit
three pairs on each of their respective cuts. We notice that for each choice of triples of
PS-sets (Cc1 , Cc2 , Cv0 ) Procedure 3 computes the implicit three other sets and names them
Cc0 1 , Cc0 2 and Cv accordingly.
We will now show that for all pairs (C, C 0 )  PS0 (Fv )  PS0 (Fv ) the value of Tabv (C, C 0 )
is correct. Let 0 be an assignment over var(F ) that satisfies the maximum number of
clauses, while meeting the expectation of C and C 0 . Thus, the value of Tabv (C, C 0 ) is correct
if and only if it stores exactly the number of clauses from (v) that 0 satisfies.
Let (C1 , C10 ) and (C2 , C20 ) be the pairs of PS-sets that 0 meet the expectation of for the
cut ((c1 ), (c1 )) and ((c2 ), (c2 )), respectively. As 0 meets these expectations, the value
of Tabc1 (C1 , C10 ) and Tabc2 (C2 , C20 ) must be at least as large as the number of clauses 0
satisfies in (c1 ) and (c2 ), respectively. Thus, the number of clauses 0 satisfies in both
(c1 ) and (c2 ) is at most as large as the sum of these two entries. Since Procedure 3, in
the iteration where Cv0 = C 0 , Cc1 = C1 and Cc2 = C2 , ensures that Tabv (C, C 0 ) is at least
the sum of Tabc1 (C1 , C10 ) and Tabc2 (C2 , C20 ), we know Tabv (C, C 0 ) is at least as large as the
correct value.
Now assume for contradiction that the value of the cell Tabv (C, C 0 ) is too large. That
means that at some iteration of Procedure 3 it is being assigned the value Tabc1 (Cc1 , Cc0 1 ) +
Tabc2 (Cc2 , Cc0 2 ) when this sum is too large. Let 1 and 2 be the assignments of var(F )
meeting the expectation of Cc1 and Cc0 1 and meeting the expectation of Cc2 , Cc0 2 , respectively,
where the number of clauses of (c1 ) and (c2 ), respectively, equals the according table
entries of Tabc1 and Tabc2 . If we now take the assignment x = 1 |c1 ] 2 |c2 ] 0 |v , we have
an assignment that meets the expectation of C and C 0 , and who satisfies more clauses in
(v) than 0 , contradicting the choice of 0 . So Tabv (C, C 0 ) can be neither smaller nor larger
than the number of clauses in (v) 0 satisfies, so it is exactly the same.
Theorem 2. Given a formula F over n variables and m clauses, and a branch decomposition
(T, ) of F of ps-width k, we solve MaxSAT, #SAT, and weighted MaxSAT in time
O(k 3 m(m + n)).
Proof. To solve MaxSAT, we first compute Tabr for the root node r of T . This requires
that we first compute PS0 (Fv ) and PS0 (Fv ) for all nodes v of T , and then, in a bottom up
manner, compute Tabv for each of the O(m + n) nodes in T . The former part we can do in
71

fiSther, Telle & Vatshelle

O(k 2 m(m + n)) time by Theorem 1, and the latter part we do in O(k 3 m(m + n)) time by
Lemma 2.
At the root r of T we have (r) = var(F )  cla(F ). Thus Fr =  and Fr does not have
any variables, so that P S(Fr )  P S(Fr ) contains only (, ). As all assignments over var(F )
meet the expectation of  and  on the cut ((r), (r)), and cla(F )  (r) = cla(F ), by
Constraint (1) the value of Tabr (, ) is the maximal number of clauses in F any assignment
of var(F ) satisfies. And hence, this number is the solution to MaxSAT.
For a weight function w : cla(F )  N, by redefining Constraint (1) for Tabv to maximize
w(sat0 (F,  )  (v)) instead of |sat0 (F,  ))(v)|, we are able to solve the more general
problem weighted MaxSAT in the same way.
For the problem #SAT, we care only about assignments satisfying all the clauses of
F , and we want to decide the number of distinct assignments doing so. This requires a
few alterations. Firstly, alter the definition of the contents of Tabv (C, C 0 ) in Constraint
(1) to be the number of assignments  over var(F )  (v) where sat0 (Fv ,  ) = C and all
clauses in (v) is either in C 0 or satisfied by  . Secondly, when computing Tabl for the
leaves l of T , we set each of the entries of Tabl to either zero, one, or two, according to the
definition. Thirdly, we alter the algorithm to compute Tabv (Procedure 3) for inner nodes.
We initialize Tabv (C, C 0 ) to be zero at the start of the algorithm, and substitute lines 6 and
7 of Procedure 3 by the following line which increases the table value by the product of the
table values at the children
Tabv (Cv , Cv )  Tabv (Cv , Cv ) + Tabc1 (Cc1 , Cc1 )  Tabc2 (Cc2 , Cc2 )
This will satisfy our new constraint of Tabv for internal nodes v of T . The value of Tabr (, )
at the root r of T will be exactly the number of distinct assignments satisfying all clauses of
F.
The bottleneck giving the cubic factor k 3 in the runtime of Theorem 2 is the number
triples in PS0 (Fv )  PS0 (Fc1 )  PS0 (Fc2 ) for any node v with children c1 and c2 . When (T, ) is
a linear branch decomposition, it is always the case that either c1 or c2 is a leaf of T . In this
case either |PS0 (Fc1 )| or |PS0 (Fc2 )| is a constant. Therefore, for linear branch decompositions
PS0 (Fv )  PS0 (Fc1 )  PS0 (Fc2 ) will contain no more than O(k 2 ) triples. Thus we can reduce
the runtime of the algorithm by a factor of k.
Theorem 3. Given a formula F over n variables and m clauses, and a linear branch
decomposition (T, ) of F of ps-width k, we solve #SAT, MaxSAT, and weighted MaxSAT
in time O(k 2 m(m + n)).

4. CNF Formulas of Polynomial ps-width
In this section we investigate classes of CNF formulas having decompositions with ps-width
polynomially bounded in the total size s of the formula. In particular, we show that this
holds whenever the incidence graph of the formula has constant mim-width (maximum
induced matching-width, introduced in Vatshelle, 2012). We also show that a large class of
bipartite graphs, using what we call bigraph bipartizations, have constant mim-width.
72

fiSolving #SAT and MaxSAT by Dynamic Programming

In order to lift the upper bound of Lemma 1 on the ps-value of F , i.e |PS(F )|, to the
ps-width of F , we use mim-width of the incidence graph I(F ), which is defined using branch
decompositions of graphs. A branch decomposition of the formula F , as defined in Section
2, can also be seen as a branch decomposition of the incidence graph I(F ). Nevertheless, for
completeness, we formally define branch decompositions of graphs and mim-width.
A branch decomposition of a graph G is a pair (T, ) where T is a rooted binary tree
and  a bijection between the leaf set of T and the vertex set of G. For a node w of T
let the subset of V (G) in bijection  with the leaves of the subtree of T rooted at w be
denoted by Vw . We say the decomposition defines the cut (Vw , Vw ). The mim-value of a cut
(Vw , Vw ) is the size of a maximum induced matching of G[Vw , Vw ]. The mim-width of (T, )
is the maximum mim-value over all cuts (Vw , Vw ) defined by a node w of T . The mim-width
of graph G, denoted mimw(G), is the minimum mim-width over all branch decompositions
(T, ) of G. As before a linear branch decomposition is a branch decomposition where inner
nodes of the underlying tree induces a path.
Since a decomposition of I(F ) of can be seen also as a decomposition of F , we immediately
get from Lemma 1 the following corollary.
Corollary 1. For any CNF formula F over m clauses, with no clause containing more than
t literals, the ps-width of F is at most min{mk + 1, 2tk } for k = mimw(I(F )).
Many classes of graphs have intersection models, meaning that they can be represented
as intersection graphs of certain objects, i.e. each vertex is associated with an object and
two vertices are adjacent iff their objects intersect. The objects used to define intersection
graphs usually consist of geometrical objects such as lines, circles or polygons. Many well
known classes of intersection graphs have constant mim-width, as in the following which
lists only a subset of the classes proven to have such bounds (Belmonte & Vatshelle, 2013;
Vatshelle, 2012).
Theorem 4. (Belmonte & Vatshelle, 2013; Vatshelle, 2012) Let G be a graph. If G is a:
interval graph then mimw(G)  1.
circular arc graph then mimw(G)  2.
k-trapezoid graph then mimw(G)  k.
Moreover there exist linear decompositions satisfying the bound, that can be found in polynomial time (for k-trapezoid assume the intersection model is given).
Let us briefly mention the definition of these graph classes. A graph is an interval graph
if it has an intersection model consisting of intervals of the real line. A graph is a circular arc
graph if it has an intersection model consisting of arcs of a circle. To build a k-trapezoid we
start with k parallel line segments (s1 , e1 ), (s2 , e2 ), ..., (sk , ek ) and add two non-intersecting
paths s and e by joining si to si+1 and ei to ei+1 respectively by straight lines for each
i  {1, ..., k  1}. The polygon defined by s and e and the two line segments (s1 , e1 ), (sk , ek )
forms a k-trapezoid. A graph is a k-trapezoid graph if it has an intersection model consisting
of k-trapezoids. See the work of Brandstadt, Le, and Spinrad (1999) for information about
graph classes and their containment relations.
Combining Corollary 1 and Theorem 4 we get the following
Corollary 2. Let F be a CNF formula containing m clauses with maximum clause-size t.
If I(F ) is a:
73

fiSther, Telle & Vatshelle

interval graph then psw(F )  min{m + 1, 2t }.
circular arc graph then psw(F )  min{m2 + 1, 4t }.
k-trapezoid graph then psw(F )  min{mk + 1, 2tk }.
Moreover there exist linear decompositions satisfying the bound, that can be found in polynomial time (for k-trapezoid assume the intersection model is given).
The incidence graphs of formulas are bipartite graphs, which is not the case for the
majority of graphs in the above-mentioned graph classes. In the following we show how to
extend the results of Corollary 2 to large classes of bipartite graphs. For a graph G and
subset of vertices A  V (G) the bipartite graph G[A, A] is the subgraph of G containing all
edges of G with exactly one endpoint in A. For any graph G and A  V (G) we call G[A, A]
a bigraph bipartization of G, and note that G has a bigraph bipartization for each subset of
vertices. For a graph class X we define the class of X bigraphs as the bipartite graphs H for
which there exists G  X such that H is isomorphic to a bigraph bipartization of G. For
example, a bipartite graph H is an interval bigraph if there is some interval graph G and
some A  V (G) with H isomorphic to G[A, A].
The following result will allow us to lift the results of Corollary 2 from the given graphs
to the bigraph bipartizations of the same graphs.
Theorem 5. Assume that we are given a CNF formula F of m clauses and maximum
clause-size t, a graph G, a subset A  V (G), and (T, G ) a (linear) branch decomposition
of G of mim-width k. If I(F ) is connected and isomorphic to G[A, A] (thus I(F ) a bigraph
bipartization of G) then we can in linear time produce a (linear) branch decomposition (T, F )
of F having ps-width at most min{mk + 1, 2tk }
Proof. Since each variable and clause in F has a corresponding node in I(F ), and each node
in I(F ) has a corresponding node in G, by defining F to be the function mapping each leaf
l of T to the variable or clause in F corresponding to the node G (l), we get that (T, F ) is
a branch decomposition of F . Consider a cut (B, B) induced by a node of (T, F ). Note
that the mim-value of G[B, B] is at most k. I(F ) is connected which means that we have
either A or A corresponding to the set of variables of F . Assume wlog the former. Thus
C = A  B  cla(F ) are the clauses in B, with C = cla(F ) \ C and X = A  B  var(F )
are the variables in B, with X = var(F ) \ X. The mim-values of G[C, X] and G[C, X] are
at most k, since these are induced subgraphs of G[B, B], and taking induced subgraphs
cannot increase the size of the maximum induced matching. Hence by Lemma 1, we have
|PS(FC,X )|  |cla(F )|k + 1, and likewise we have |PS(FC,X )|  |cla(F )|k + 1, with the
maximum of these two being the ps-value of this cut. Since the ps-width of the decomposition
is the maximum ps-value of each cut the theorem follows.
Combining Theorems 5 and 4 we immediately get the following.
Corollary 3. Let F be a CNF formula containing m clauses with maximum clause-size t.
If I(F ) is a:
interval bigraph then psw(F )  min{m + 1, 2t }.
circular arc bigraph then psw(F )  min{m2 + 1, 4t }.
k-trapezoid bigraph then psw(F )  min{mk + 1, 2tk }.
Moreover there exist linear decompositions satisfying the bound.
74

fiSolving #SAT and MaxSAT by Dynamic Programming

In the next section we address the question of finding such linear decompositions in
polynomial time. We succeed in the case of interval bigraphs, but for circular arc bigraphs
and k-trapezoid bigraphs we must leave this as an open problem.

5. Interval Bigraphs and Formulas Having Interval Orders
We will in this section show that for formulas whose incidence graph is an interval bigraph
we can in polynomial time find linear branch decompositions having small ps-width. Let
us recall the definition of interval ordering. A CNF formula F has an interval ordering if
there exists a linear ordering of variables and clauses such that for any variable x occurring
in clause C, if x appears before C then any variable between them also occurs in C, and
if C appears before x then x occurs also in any clause between them. See Figure 4 for an
example.
Order:
x1 c 1 x2 x3 c 2 c 3 x4 x5

Clauses:

Bipartized interval rep.

Incidence graph

x1
c1
x2
x3
c2
c3
x4
x5

c1 = {x1 , x2 }
c2 = {x2 , x3 , x5 }
c3 = {x3 , x4 , x5 }

c1

x1

c2 c3

x2 x3

x4 x5

Figure 4: A CNF formula having an interval ordering. Its incidence graph is an interval
bigraph, since it is isomorphic to the bigraph bipartization, defined by the blue
intervals, of the interval graph with intersection model on the left.
From the work of Hell and Huang (2004) it follows that a formula F has an interval ordering
if and only if I(F ) is a interval bigraph.
Theorem 6. Given a CNF formula F over n variables and m clauses each of at most t
literals. In time O((m + n)mn) we can decide if F has an interval ordering (yes iff I(F ) is
an interval bigraph), and if yes we solve #SAT and weighted MaxSAT with an additional
runtime of O(min{m2 , 4t }(m + n)m).
Proof. Using the characterization in the work of Hell and Huang (2004) and the algorithm
of Rafiey (2012) we can in time O((m + n)mn) decide if F has an interval ordering and if
yes, then we find it. From this interval ordering we build an interval graph G such that I(F )
is a bigraph bipartization of G, and construct a linear branch decomposition of G having
mim-width 1 (Belmonte & Vatshelle, 2013). From such a linear branch decomposition we
75

fiSther, Telle & Vatshelle

get from Theorem 5 that we can construct another linear branch decomposition of F having
ps-width O(m). We then run the algorithm of Theorem 3.

6. Experimental Results
We present some simple experimental results, intended as proof of concept. It is our belief
that some of the ideas behind our algorithms, like the notion of ps-value, are useful in
practice, but it will require a thorough investigation to confirm such a belief. Our results
indicate that the worst-case runtime bounds of the dynamic programming, Theorems 2 and
3, are probably higher than what would commonly be seen in practice.
In the past decade, SAT solvers have become very powerful, and are currently able
to handle very large practical instances. Techniques from these SAT solvers have been
applied to develop relatively powerful MaxSAT and #SAT solvers (Biere, Heule, & van
Maaren, 2009). In our experiments we compare implementations of our algorithms against
state-of-the-art MaxSAT and #SAT solvers. We do not enhance our implementations with
any other techniques, not even simple pre-processing, and on the vast majority of instances
our implementations fall far behind in a comparison. However, when focusing on formulas
with a certain linear order our implementations compare favorably.
As explained in Section 1, there are two steps involved: (1) find a good decomposition
of the input CNF formula F , and (2) perform DP (dynamic programming) along the
decomposition. Let us start by describing a very simple heuristic for step (1). It takes
as input the bipartite graph I(F ) with vertex set cla(F )  var(F ), and outputs a linear
order  on the vertex set. The below heuristic GreedyOrder is a greedy algorithm that
for increasing values of i chooses (i) to be a vertex having the highest number of already
chosen neighbors, and among these choosing one with fewest non-chosen neighbors. This
defines a linear branch decomposition (T, ) of the CNF formula F , with non-leaf nodes
of the binary tree T inducing a path, with T rooted at one end of this path, and with 
mapping the ith leaf encountered by a breadth-first search starting at the root of T to the
clause or variable (i), for all 1  i  |cla(F )  var(F )|.

Algorithm GreedyOrder
input: G = (V, E), a (bipartite) graph
output: , a linear ordering of V
L = , R = V , i = 1
for all v  V set Ldegree(v) = 0
while R is not empty do
choose v: from vertices in R with max Ldegree take one of smallest degree
set (i) = v, increment i, add v to L and remove v from R
for all w  R with vw  E increment Ldegree(w)

76

fiSolving #SAT and MaxSAT by Dynamic Programming

All our implementations can be found online (Sther, Telle, & Vatshelle, 2015). We
have implemented GreedyOrder in Java, together with a straight-forward implementation of
the DP algorithm of Theorem 3.
Given a CNF formula, this allows us to solve MaxSAT and #SAT by first running
GreedyOrder and then the DP. We compare our implementation to the best solvers we could
find online, respectively CCLS-to-akmaxsat (Luo, Cai, Wu, Jie, & Su, 2014) which was
among the best solvers of the Ninth Max-SAT Evaluation (2014), and the latest version of
the #SAT solver called sharpSAT (Thurley, n.d., 2006). These solvers handily beat our
implementation on most inputs. We have therefore generated some CNF formulas having
interval orderings, as in Theorem 6, to check if at least on these instances we do better.
Note that for step (1) we have not implemented the polynomial-time algorithm recognizing
formulas having interval orders, relying instead on the GreedyOrder heuristic.
6.1 Generation of Instances
Before presenting our results, let us describe the generation of the set of instances, which
are of three types. We start with type 1. The generation of these formulas is based on the
definition of interval orderings given by the interval bigraph definition, see e.g. the left side
of Figure 4. To generate a formula of type 1 with n variables and m clauses, we generate
n + m intervals of the real line by iterating through points i from 1 to 2(n + m) as left and
right endpoints of the intervals:
 At step i, check which of the 4 cases below are legal (e.g. 3 is legal if there exists a
live variable, i.e. with left endpoint < i and no right endpoint) and randomly make
one of those legal choices:
1. start interval of new variable with left endpoint i
2. start interval of new clause with left endpoint i
3. end interval of randomly chosen live variable by right endpoint i
4. end interval of randomly chosen live clause by right endpoint i
Towards the end of the process boundary conditions are enforced to reach exactly m
clauses, with n expected to be slightly smaller than m. For each clause interval we randomly
choose each variable having overlapping interval as being either positive or negative in this
clause. The resulting CNF formula will have an interval ordering given by the rightmost
endpoints of intervals. To hide this ordering the clauses and variables are randomly permuted
to make the final CNF formula.
The formulas of type 2 are generated in a very similar fashion as type 1, except we
guarantee that all clauses have the same size t, which by Lemma 1 could be of big help. The
only change is to case 4 above which instead of being a choice becomes enforced for a live
clause that at step i has accumulated exactly t overlapping variable intervals. We also let
each clause interval represent 4 clauses over the same variable set but on randomly chosen
literals, at the aim of increasing the probability of each instance not being satisfiable.
The formulas of type 3 are the CNF-representation of a conjunction of XOR functions
where each XOR has a fixed number t of literals and the variables of the XOR functions
77

fiSther, Telle & Vatshelle

overlap in such a way that the incidence graph will be the bipartization of a circular arc
graph.
A formula of type 3 is generated from three input parameters n, t, s. It has n variables
represented by successive points 1 to n on the circle. The first XOR function has interval
from 1 to t thus containing variables with points 1 to t, the second has interval s + 1 to s + t,
and in general the ith has interval i  s + 1 to i  s + t, with appropriate modulo addition
and some boundary condition at the end to ensure n/s XOR functions. Variables are chosen
randomly to appear positive or negative in each XOR. Each XOR is then transformed in
the standard way to a CNF formula with 2t1 clauses to give us a resulting CNF formula
with n/s  2t1 clauses. Again, variables and clauses are randomly permuted to hide the
ordering giving the circular arc bigraph representation.
Note that all the resulting formulas have a quite simple structure, and that a state-ofthe-art SAT solver, like lingeling (Biere, 2014), handles all generated instances within a
few seconds.
6.2 Results
We are now ready to present our results. We ran all the solvers on a Dell Optiplex 780
running Ubuntu 12.04 64-Bit. The machine has 8GB of memory and an Intel Core 2 Quad
Q9650 processor with OpenJDK java 6 (IcedTea6 1.13.5).
For instances of type 1 the GreedyOrder heuristic fails terribly and becomes a huge
bottleneck. The greedy choice based on degrees of vertices in I(F ) is too simple. However,
when given the correct interval order to our solver(s) they performed better.
Instances of type 2 are generated similar to those of type 1 but all clauses have small size,
which by Lemma 1 could be of help. In this case the number of clauses is approximately

Runtime in seconds

600

400

CCLS
sharpSAT
our MaxSAT
and #SAT
(practically equal)

200

0
0

1000
2000
Number of variables

3000

Figure 5: Runtimes of instances of type 2. Here our MaxSAT solver is clearly faster than
CCLS to akmaxsat. The vertical axis represents time in seconds. Runs taking
more than 600 seconds were stopped before completion and are drawn on the
dotted line.

78

fiSolving #SAT and MaxSAT by Dynamic Programming

four times the number of variables, and as a consequence a great number of the instances
were not satisfiable, making the work of the #SAT-solvers easier than that of the MaxSAT
solvers. All generated instances of type 2 were solved within seconds by sharpSAT, see
Figure 5. As the size of the instances grow, we see a clear tendency for the runtimes of
CCLS to akmaxsat to increase much more rapidly than both our solvers. The runtimes of
our two solvers were almost identical. The GreedyOrder heuristic on these instances seems
to produce decompositions/orders of low PS-width.
The type 3 instances shown in Figure 6 were generated with k = 5 and s = 3. All
instances are satisfiable, which may explain why CCLS to akmaxsat is very fast. Choosing
k = 3 and s = 2 there will be some not satisfiable instances and CCLS to akmaxsat would
then often spend more than 600 seconds and time out. As the size of the instances grow, we
see a clear tendency for the runtimes of sharpSAT to increase much more rapidly than our
solvers. The runtimes of our two solvers were almost identical.

Runtime in seconds

600

400

CCLS
sharpSAT
our MaxSAT
and #SAT
(practically equal)

200

0
60

80
100
120
Number of variables

140

Figure 6: Runtimes of instances of type 3. Here our #SAT solver is clearly faster than
sharpSAT. The vertical axis represents time in seconds. Runs taking more than
600 seconds were stopped before completion and are drawn on the dotted line.

7. Conclusion
In this paper we have proposed a structural parameter of CNF formulas, called ps-width
or projection-satisfiable-width. We showed that weighted MaxSAT and #SAT can be
solved in polynomial time if given a decomposition of the formula of polynomially bounded
ps-width. Using the concept of interval bigraphs we also showed a polynomial time algorithm
that actually finds such a decomposition, for formulas having an interval ordering. Could
one devise such an algorithm also for the larger class of circular arc bigraphs, or maybe
even for the even larger class of k-trapezoid bigraphs? In other words, is the problem of
recognizing if a bipartite input graph is a circular arc bigraph, or a k-trapezoid bigraph,
polynomial-time solvable?
79

fiSther, Telle & Vatshelle

It could be of practical interest to design a heuristic algorithm which given a formula finds
a decomposition of relatively low ps-width, as has been done for boolean-width (Hvidevold,
Sharmin, Telle, & Vatshelle, 2011). One could then check if benchmarks covering real-world
SAT instances have low ps-width, and perform a study on the correlation between low
ps-width and their practical hardness by MaxSAT and #SAT solvers, as has been done
for treewidth and SAT solvers (Mateescu, 2011). We presented some simple experimental
results, but it will require a thorough investigation to check if ideas from our algorithms
could be useful in practice. Finally, we hope the essential combinatorial result enabling the
improvements in this paper, Lemma 1, may have other uses as well.

References
Bacchus, F., Dalmao, S., & Pitassi, T. (2003). Algorithms and complexity results for#SAT
and bayesian inference. In Foundations of computer science, 2003. proceedings. 44th
annual ieee symposium on (pp. 340351).
Belmonte, R., & Vatshelle, M. (2013). Graph classes with structured neighborhoods and
algorithmic applications. Theor. Comput. Sci., 511 , 54-65.
Biere, A. (2014). Yet another local search solver and lingeling and friends entering the SAT
competition 2014. SAT Competition 2014 , 39.
Biere, A., Heule, M., & van Maaren, H. (2009). Handbook of satisfiability. In (Vol. 185,
chap. 20). IOS Press.
Brandstadt, A., Le, V. B., & Spinrad, J. P. (1999). Graph classes: A survey (Vol. 3).
Philadelphia: SIAM Society for Industrial and Applied Mathematics.
Brandstadt, A., & Lozin, V. V. (2003). On the linear structure and clique-width of bipartite
permutation graphs. Ars Comb., 67 .
Brault-Baron, J., Capelli, F., & Mengel, S. (2014). Understanding model counting for
-acyclic CNF-formulas. CoRR, abs/1405.6043 . Retrieved from http://arxiv.org/
abs/1405.6043
Bui-Xuan, B.-M., Telle, J. A., & Vatshelle, M. (2010). H-join decomposable graphs and
algorithms with runtime single exponential in rankwidth. Discrete Applied Mathematics,
158 (7), 809-819.
Bui-Xuan, B.-M., Telle, J. A., & Vatshelle, M. (2011). Boolean-width of graphs. Theoretical
Computer Science, 412 (39), 51875204.
Courcelle, B. (2004). Clique-width of countable graphs: a compactness property. Discrete
Mathematics, 276 (1-3), 127148. Retrieved from http://dx.doi.org/10.1016/S0012
-365X(03)00303-0 doi: 10.1016/S0012-365X(03)00303-0
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1), 541.
Fischer, E., Makowsky, J. A., & Ravve, E. V. (2008). Counting truth assignments of
formulas of bounded tree-width or clique-width. Discrete Applied Mathematics, 156 (4),
511-529.
80

fiSolving #SAT and MaxSAT by Dynamic Programming

Fredkin, E. (1960). Trie memory. Communications of the ACM , 3 (9), 490499.
Ganian, R., Hlineny, P., & Obdrzalek, J. (2013). Better algorithms for satisfiability problems
for formulas of bounded rank-width. Fundam. Inform., 123 (1), 59-76.
Garey, M. R., & Johnson, D. S. (1979). Computers and intractability: A guide to the theory
of NP-completeness. W. H. Freeman.
Hell, P., & Huang, J. (2004). Interval bigraphs and circular arc graphs. Journal of Graph
Theory, 46 (4), 313-327.
Hvidevold, E. M., Sharmin, S., Telle, J. A., & Vatshelle, M. (2011). Finding good
decompositions for dynamic programming on dense graphs. In D. Marx & P. Rossmanith
(Eds.), Ipec (Vol. 7112, p. 219-231). Springer.
Jaumard, B., & Simeone, B. (1987). On the complexity of the maximum satisfiability
problem for Horn formulas. Inf. Process. Lett., 26 (1), 1-4.
Kaski, P., Koivisto, M., & Nederlof, J. (2012). Homomorphic hashing for sparse coefficient
extraction. In Proceedings of the 7th international conference on parameterized and
exact computation (pp. 147158).
Luo, C., Cai, S., Wu, W., Jie, Z., & Su, K. (2014). CCLS: An efficient local search
algorithm for weighted maximum satisfiability. IEEE Transactions on Computers. doi:
10.1109/TC.2014.2346195
Mateescu, R. (2011). Treewidth in industrial SAT benchmarks (Tech. Rep.). Tech. rep.
Cambridge, UK: Microsoft Research. Retrieved from http://research.microsoft
.com/pubs/145390/MSR-TR-2011-22.pdf
Muller, H. (1997). Recognizing interval digraphs and interval bigraphs in polynomial time.
Discrete Applied Mathematics, 78 (1-3), 189-205.
Ninth Max-SAT Evaluation. (2014). Retrieved from http://www.maxsat.udl.cat/14/
(accessed 16-January-2015)
Paulusma, D., Slivovsky, F., & Szeider, S. (2013). Model counting for CNF formulas of
bounded modular treewidth. In N. Portier & T. Wilke (Eds.), Stacs (Vol. 20, p. 55-66).
Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik.
Rafiey, A. (2012).
abs/1211.2662 .

Recognizing interval bigraphs by forbidden patterns.

CoRR,

Raman, V., Ravikumar, B., & Rao, S. S. (1998). A simplified NP-complete MAXSAT
problem. Inf. Process. Lett., 65 (1), 1-6.
Rao, M. (2008). Clique-width of graphs defined by one-vertex extensions. Discrete
Mathematics, 308 (24), 61576165.
Robertson, N., & Seymour, P. D. (1991). Graph minors X. obstructions to tree-decomposition.
J. COMBIN. THEORY SER. B , 52 (2), 153190.
Roth, D. (1996). A connectionist framework for reasoning: Reasoning with examples. In

81

fiSther, Telle & Vatshelle

W. J. Clancey & D. S. Weld (Eds.), Aaai/iaai, vol. 2 (p. 1256-1261). AAAI Press /
The MIT Press.
Sther, S. H., Telle, J. A., & Vatshelle, M. (2014). Solving MaxSAT and #SAT on
structured CNF formulas. In C. Sinz & U. Egly (Eds.), SAT 2014 (Vol. 8561, pp. 16
31). Springer. Retrieved from http://dx.doi.org/10.1007/978-3-319-09284-3 3
doi: 10.1007/978-3-319-09284-3 3
Sther, S. H., Telle, J. A., & Vatshelle, M. (2015). online implementations. Retrieved from
http://people.uib.no/ssa032/pswidth/
Samer, M., & Szeider, S. (2010). Algorithms for propositional model counting. J. Discrete
Algorithms, 8 (1), 50-64.
Slivovsky, F., & Szeider, S. (2013). Model counting for formulas of bounded clique-width. In
L. Cai, S.-W. Cheng, & T. W. Lam (Eds.), Isaac (Vol. 8283, p. 677-687). Springer.
Szeider, S. (2003). On fixed-parameter tractable parameterizations of SAT. In E. Giunchiglia
& A. Tacchella (Eds.), Sat 2003 (Vol. 2919, p. 188-202). Springer.
Thurley, M. (n.d.). sharpSAT. Retrieved from https://sites.google.com/site/
marcthurley/sharpsat (accessed 16-January-2015)
Thurley, M. (2006). sharpSATcounting models with advanced component caching and
implicit BCP. In Theory and applications of satisfiability testing-sat 2006 (pp. 424429).
Springer.
Vatshelle, M. (2012). New width parameters of graphs. Unpublished doctoral dissertation,
The University of Bergen.

82

fi