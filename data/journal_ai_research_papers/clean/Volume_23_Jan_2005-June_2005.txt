Journal of Artificial Intelligence Research 23 (2005) 625-666

Submitted 07/04; published 06/05

An Expressive Language and Efficient Execution System
for Software Agents
gbarish@fetch.com

Greg Barish
Fetch Technologies
2041 Rosecrans Avenue, Suite 245
El Segundo, CA 90245 USA

knoblock@isi.edu

Craig A. Knoblock
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
Software agents can be used to automate many of the tedious, time-consuming
information processing tasks that humans currently have to complete manually.
However, to do so, agent plans must be capable of representing the myriad of actions and
control flows required to perform those tasks. In addition, since these tasks can require
integrating multiple sources of remote information  typically, a slow, I/O-bound process
 it is desirable to make execution as efficient as possible. To address both of these
needs, we present a flexible software agent plan language and a highly parallel execution
system that enable the efficient execution of expressive agent plans. The plan language
allows complex tasks to be more easily expressed by providing a variety of operators for
flexibly processing the data as well as supporting subplans (for modularity) and recursion
(for indeterminate looping). The executor is based on a streaming dataflow model of
execution to maximize the amount of operator and data parallelism possible at runtime.
We have implemented both the language and executor in a system called THESEUS. Our
results from testing THESEUS show that streaming dataflow execution can yield
significant speedups over both traditional serial (von Neumann) as well as non-streaming
dataflow-style execution that existing software and robot agent execution systems
currently support. In addition, we show how plans written in the language we present can
represent certain types of subtasks that cannot be accomplished using the languages
supported by network query engines. Finally, we demonstrate that the increased
expressivity of our plan language does not hamper performance; specifically, we show
how data can be integrated from multiple remote sources just as efficiently using our
architecture as is possible with a state-of-the-art streaming-dataflow network query
engine.

1. Introduction
The goal of software agents is to automate tasks that require interacting with one or more
accessible software systems. Past research has yielded several types of agents and agent
frameworks capable of automating a wide range of tasks, including: processing sequences of
operating system commands (Golden, Etzioni, & Weld, 1994), mediation of heterogeneous data
sources (Wiederhold 1996; Bayardo, Bohrer, Brice, Cichocki, Fowler, Helal, Kashyap, Ksiezyk,
Martin, Nodine, Rashid, Rusinkiewicz, Shea, Unnikrishnan, Unruh, & Woelk 1997; Knoblock,
Minton, Ambite, Ashish, Muslea, & Tejada 2001), online comparison shopping (Doorenbos,
 2005 AI Access Foundation. All Rights Reserved.

fiBARISH & KNOBLOCK

Etzioni, & Weld, 1996), continual financial portfolio analysis (Decker, Sycara, & Zeng, 1996),
and airline ticket monitoring (Etzioni, Tuchinda, Knoblock, & Yates, 2004), to name only a few.
Despite software agent heterogeneity, two recurring characteristics are (i) the wide variety of
tasks that agents are used to automate and (ii) the frequent need to process and route information
during agent execution.
Perhaps no other domain poses as many tantalizing possibilities for software agent automation
as the Web. The ubiquity and practicality of the Web suggests that many potential benefits can
be gained from automating tasks related to sources on the web. Furthermore, the Web is ripe for
such automation  given the sheer number of online applications and the complete lack of
coordination between them, agents could address an endless list of needs and problems to be
solved for people that do use the Web for practical purposes. Furthermore, like other software
agent domains, Web tasks vary widely in complexity and, by definition, involve routing and
processing information as part of the task.
In this paper, we describe a software agent plan language and execution system that enables
one to express a wide range of tasks as a software agent plan and then to have that plan be
efficiently executed. We have implemented both the language and the executor in a system called
THESEUS. Throughout this paper, we will discuss THESEUS in the context of Web information
gathering and processing, since the Web represents a domain where most (if not all) of the
challenges that software agents face can be found.
1.1 Web Information Agents
In recent years, the Web has experienced a rapid rate of growth, with more and more useful
information becoming available online. Today, there exists an enormous amount of online data
that people can not only view, but also use in order to accomplish real tasks. Hundreds of
thousands of people use the Web every day to research airfares, monitor financial portfolios, and
keep up to date with the latest news headlines. In addition to its enormity, what is compelling
about the Internet as a practical tool is its dynamic, up-to-the-minute nature. For example,
although information such as stock quotes and ticket availabilities change frequently, many
sources on the Internet are capable of reporting these updates immediately. For this reason, and
because of the breadth and depth of information it provides, the Web has become  for certain
tasks  a more timely and necessary medium than even the daily newspaper, radio, or television.
The degree of complexity in gathering information from the Web varies significantly. Some
types of tasks can be accomplished manually because the size of the data gathered is small or the
need to query is infrequent. For example, finding the address of a restaurant or a theater in a
particular city using a Yellow Pages type of Web site is easy enough for people to do themselves.
It does not need to be automated, since the query need only be done once and the result returned
is small and easy to manage. However, not all information gathering tasks are as simple. There
are often times when the amount of data involved is large, or the answer requires integrating data
from multiple sites, or the answer requires multiple queries over a period of time. For example,
consider shopping for an expensive product over a period of time using multiple sources that are
each updated daily. Such tasks can become quickly tedious and require a greater amount of
manual work, making them very desirable to automate.
1.1.1. MORE COMPLICATED TASKS
One type of difficult Web information gathering task involves interleaved gathering and
navigation. For the benefit of people that use a Web browser to access online data, many Web
sources display large sets of query results spread over a series of web pages connected through
Next Page links. For example, querying an online classified listings source for automobiles for
sale can generate many results. Instead of displaying the results on a single very long Web page,
many classified listings sites group sets of results over series of hyperlinked pages. In order to
automatically collect this data, a system needs to interleave navigation and gathering an

626

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

indeterminate number of times: that is, it needs to collect results from a given page, navigate to
the next, gather the next set of results, navigate, and so on, until it reaches the end of set of
results. While there has been some work addressing how to theoretically incorporate navigation
into the gathering process (Friedman, Levy, & Millstein, 1999), no attention has been given to the
efficient execution of plans that engage in this type of interleaved retrieval.
A second example has to do with monitoring a Web source. Since the Web does not contain a
built-in trigger facility, one is forced to manually check sources for updated data. When updates
are frequent or the need to identify an update immediately is urgent, it becomes desirable to
automate the monitoring of these updates, notifying the user when one or more conditions are
met. For example, suppose we want to be alerted as soon as a particular type of used car is listed
for sale by one or more online classified ad sources. Repeated manual checking for such changes
is obviously tedious. Mediators and network query engines can automate the query, but
additional software in programming languages such as Java or C must be written to handle the
monitoring process itself, something that requires conditional execution, comparison with past
results, possible notification of the user, and other such actions.
1.1.2. THE NEED FOR FLEXIBILITY
These examples show that automatically querying and processing Web data can involve a
number of subtasks, such as interleaved navigation and gathering and integration with local
databases. Because of these needs, traditional database query languages like SQL are insufficient
for the Web. The root of the problem is lack of flexibility, or expressivity, in these languages -typically, only querying is supported. More complicated types of Web information gathering
tasks, such as those described here and in other articles (Etzioni & Weld, 1994; Doorenbos et al.,
1997; Chalupsky, Gil, Knoblock, Lerman, Oh, Pynadath, Russ, & Tambe, 2001; Ambite, Barish,
Knoblock, Muslea, Oh, & Minton, 2002; Sycara, Paolucci, van Velsen, & Giampapa, 2003;
Graham, Decker, & Mersic, 2003), usually involve actions beyond those needed for merely
querying (i.e., beyond filtering and combining)  they require plans capable of a variety of
actions, such as conditional execution, integration with local databases, and asynchronous
notification to users. In short, Web information gathering tasks require an expressive query or
plan language with which to describe a solution.
XQuery (Boag, Chamberlin, Fernandez, Florescu, Robie, & Simeon, 2002), used for querying
XML documents, is one language that offers more flexibility. For example, XQuery supports
FLWOR expressions that allow one to easily specify how to iterate over data. XQuery also
supports conditional expressions, UDFs, and recursive functions.
Support for expressive agent plans can also be found in a number of software agent and robot
agent frameworks, such as INFOSLEUTH (Bayardo et al., 1997), RETSINA (Sycara et al., 2003),
DECAF (Graham et al., 2003), RAPs (Firby 1994) or PRS-LITE (Myers 1996). These systems
support concurrent execution of operators and the ability to execute more complicated types of
plans, such as those that require conditionals. In addition, unlike database systems, software
agent and robot agent execution plans can contain many different types of operators, not just
those limited to querying and filtering data.
1.1.3. THE NEED FOR EFFICIENCY
Despite support for more expressive plans, existing software agent and robot agent plan execution
systems lack efficiency  a problem that is painfully realized when doing any kind of large scale
data integration or when working with remote sources that operate at less than optimal speeds. In
particular, while systems like RETSINA, DECAF, RAPs and PRS-LITE ensure a high-degree of
operator parallelism (independent operators can execute concurrently), they do not ensure any
type of data parallelism (independent elements of data can be processed concurrently). For
example, it is not possible for one operator in these systems to stream information to another.
This is understandable in the case of robot plan execution systems, which address how a robot

627

fiBARISH & KNOBLOCK

interacts in the physical world, and are typically concerned with communicating effects, such as
has object X or direction north, which are  relatively speaking  small amounts of local
information. Interestingly, while other software agent frameworks like DECAF and INFOSLEUTH
have expressed a desire to support some sort of streaming architecture (Bayardo et al., 1997),
such research has been constrained in part by the use of data transport layers (such as KQML)
that do not contain the infrastructure necessary to support streaming.
However, for a software agent to process large amounts of remote information efficiently,
both types of parallelism are critical. Dataflow-style parallelism is important in order to schedule
independent operations concurrently; streaming is important in order to be able to process remote
information as it becomes available and to make maximum use of local processing resources.
Consider an agent that engages in two types of image processing on image data downloaded from
a surveillance satellite. If it normally takes one minute to download the data and another minute
for each type of processing, streaming dataflow execution can theoretically reduce the overall
execution time by up to two-thirds. Even greater speedups are possible for different information
processing tasks.
Though existing software agent and robot plan execution systems do not support streaming, a
substantial amount of previous work has gone into building such architectures for database
systems (Wilschut & Apers, 1993) and more recently network query engines (Ives, Florescu,
Friedman, Levy, & Weld, 1999; Hellerstein, Franklin, Chandrasekaran., Deshpande, Hildrum,
Madden, Raman, & Shah, 2000; Naughton, DeWitt, Maier, Aboulnaga, Chen, Galanis, Kang,
Krishnamurthy, Luo, Prakash, Ramamurthy, Shanmugasundaram, Tian, Tufte, Viglas, Wang,
Zhang, Jackson, Gupta, & Che, 2001). These systems employ special iterative-style operators
that are aware of the underlying support for streaming1 and exploit that feature to minimize
operator blocking. Examples include the pipelined hash join (Wilschut & Apers, 1993; Ives, et
al., 1999) the eddy data structure (Avnur & Hellerstein, 2000), which efficiently routes streaming
data to operators. Despite support for a streaming dataflow model of execution, network query
engines lack the generality and flexibility of existing agent frameworks and systems. XQuery
does provide a more powerful and flexible language for Web data gathering and manipulation.
However, the network query engines support only a subset of XQuery or related XML query
processing operators and do not support constructs such as conditionals and recursion2, which are
essential for more complex types of information processing tasks.
1.2 Contributions
In summary, while it is desirable to automate the gathering and processing of data on the Web, it
is currently not possible to build an agent that is both flexible and efficient using existing
technologies. Existing agent execution systems are flexible in the types of plans they support, but
they lack the ability to stream information, a critical feature that needs to be built into both the
underlying architecture as well as the individual operators (i.e., operators need to implemented as
iterators). Network query engines contain support for streaming dataflow, but lack the
expressivity provided by existing agent plan languages.
In this paper, we address the need to combine both by presenting an expressive plan language
and an efficient execution system for software agents. More specifically, this paper makes two
contributions. The first is a software agent plan language that extends features of existing agent
plan languages and is more expressive than the query languages of existing information
integration systems and network query engines. The language proposed consists of a rich set of
operators that, beyond gathering and manipulating data, support conditional execution,
management of data in local persistent sources, asynchronous notification of results to users,
1

Note: The term pipelining is common in database literature (e.g., Graefe, 1993), although streaming is
often used in network query engine literature (see recent publications by Niagara and Telegraph).
2
For example, Tukwila supports only a subset of Xquery (Ives et al., 2002).

628

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

integration between relational and XML sources, and extensibility (i.e., user-defined functions).
In addition, the language is modular and encourages re-use: existing plans can be called from
other plans as subplans and plans can be called recursively. Both the operators and constructs
provide the expressivity necessary to address more complicated types of software agent tasks,
such as the monitoring and interleaved navigation and gathering of Web data.
A second contribution of this paper is the design for an executor that efficiently processes
agent plans written in the proposed language. The core of the executor implements a streaming
dataflow architecture, where data is dispatched to consuming operators as it becomes available
and operators execute whenever possible. This design allows plans to realize the maximum
degree of operational (horizontal) and data (vertical) parallelism possible. Our design also
supports recursive streaming, resulting in the efficient execution of plans that require
indeterminate looping, such as agents that interleave navigation and gathering of information on
the Web. In short, the executor supports the highly parallel execution of software agent plans,
leading to significant performance improvement over that provided by expressive, but less
efficient agent executors.
We have implemented both the plan language and executor in a system called THESEUS.
Throughout this paper, we refer to example plans that have been deployed in THESEUS in order to
better illustrate plan expressivity and, later, to validate efficiency claims.
1.3 Organization
The rest of this paper is organized as follows. Section 2 provides background and the basic
terminology of both dataflow computing (Arvind & Nikhil, 1990; Papdopoulos & Culler, 1990;
Gurd & Snelling, 1992), generic information integration (Chawathe, Garcia-Molina, Hammer,
Ireland, Papakonstantinou, Ullman, & Widom, 1994; Arens, Knoblock, & Shen, 1996; Levy,
Rajaraman, & Ordille, 1996; Weiderhold 1996; Genesereth, Keller, & Duschka, 1997) and
automated Web information gathering (Knoblock et al., 2001; Ives et al., 1999; Barish &
Knoblock, 2002; Thakkar, Knoblock, & Ambite, 2003; Tuchinda & Knoblock, 2004). Section 3
describes the details involved in one type of complex software agent information gathering task,
an example that will be used throughout the rest of the paper. In Section 4, we describe the
proposed plan language in detail. Section 5 deals with the design of the streaming dataflow
executor and how it provides high degrees of horizontal and vertical parallelism at runtime. In
Section 6, we present experimental results, describing how the plan language and executor
implemented in THESEUS measure up to those provided by other systems. In Section 7, we
discuss the related work in greater detail. Finally, we present overall conclusions and discuss
future work.

2. Preliminaries
The language and execution system we present in this paper build upon a foundation of prior
research related to dataflow computing (Dennis 1974) and Web information integration
(Wiederhold 1996; Bayardo et al., 1997, Knoblock et al., 2001). Although seemingly orthogonal
disciplines, they are effective complements in that the parallelism and asynchrony provided by
dataflow computing lends itself to the performance problems associated with Web information
gathering.
2.1 Dataflow Computing
The pure dataflow model of computation was first introduced by Dennis (1974) as an alternative
to the standard von Neumann execution model. Its foundations share much in common with past
work on computation graphs (Karp & Miller, 1955), process networks (Kahn 1974), and
communicating sequential processes (Hoare 1978). Dataflow computing has a long theoretical
and experimental history, with the first machines being proposed in the early 1970s and real

629

fiBARISH & KNOBLOCK

physical systems being constructed in the late 1970s and throughout the 1980s and early 1990s
(Arvind & Nikhil, 1990; Papdopoulos & Culler, 1990; Gurd & Snelling, 1992).
The dataflow model of computation describes program execution in terms of data
dependencies between instructions. A dataflow graph is a directed acyclic graph (DAG) of nodes
and edges. The nodes are called actors. They consume and produce data tokens along the edges
that connect them to other actors. All actors run concurrently and each is able to execute, or fire,
at any time after its input tokens arrive. Input tokens can come from initial program input or as a
result of earlier execution (i.e., the output of prior actor firings). The potential overall
concurrency of execution is thus a function of the data dependencies that exist in the program, a
degree of parallelism referred to as the dataflow limit.
The key observation to be made about dataflow computing that its execution is inherently
parallel  actors function independently (asynchronously) and fire as necessary. In contrast, the
von Neumann execution model involves the sequential processing of a pre-ordered set of
instructions. Thus, execution is inherently serial. When comparing dataflow to von Neumann, a
more subtle difference (yet one at the heart of the distinction between the two) to be noted is that
the scheduling of instructions is determined at run-time (i.e., dynamic scheduling), whereas in a
von Neumann system it occurs at compile-time (i.e., static scheduling). Figure 1 illustrates the
difference between the dataflow and von Neumann approaches applied to the execution of a
simple program. The program requires the multiplication of two independent additions. Under
the von Neumann style of execution, the ADD operations must be executed sequentially, even
though they are independent of each other, because an instruction counter schedules one
instruction at a time. In contrast, since the availability of data drives the scheduling of a dataflow
machine, both ADD operations can be executed as soon as their input dependencies are fulfilled.
Dataflow systems have evolved from the classic static (Dennis 1974) model to dynamic
tagged token models (Arvind & Nikhil, 1990) that allowed multiple tokens per arc, to hybrid
models that combine von Neumann and traditional dataflow styles of execution (Iannucci, 1988;
Evripidou & Gaudiot, 1991; Gao, 1993). Other models that have been applied to digital signal
processing include boolean dataflow and synchronous dataflow (Lee & Messerschmitt, 1987),
resulting in architectures known as dataflow networks. The work described in this paper is
most relevant to a specific hybrid dataflow approach, known as threaded dataflow
(Papadopoulos & Traub, 1991), which maintains a data-driven model of execution but associates

Figure 1: Comparing von Neuman and dataflow computing

630

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

instruction streams with individual threads that execute in a von Neumann fashion. It is distinct
from pure von Neumann multithreading in the sense that data, not an instruction counter, remains
the basis for scheduling instructions (operators). But it is also distinct from pure dataflow in the
sense that execution of instruction streams is a statically scheduled sequential task, unlike the
typical dynamic scheduling found in dataflow machines. As a result, threaded dataflow can also
be viewed as data-driven multithreading.
Recent advances in processor architecture, such as the Simultaneous Multithreading (SMT)
project (Tullsen, Eggers, & Levy, 1995) have demonstrated the benefits of data-driven
multithreading. SMT-style processors differ from conventional CPUs (such as the Intel Pentium)
by partitioning on-chip resources so that multiple threads can execute concurrently, making better
use of available functional units on the same amount of chip real estate. The resulting execution
reduces vertical waste (the wasting of cycles) that can occur when a sequence of instructions is
executed using only one thread, as well as horizontal waste (the wasting available functional
units) that can occur when executing multiple threads. To do so, the technique effectively trades
instruction-level parallelism (ILP) benefits for thread-level parallelism (TLP) benefits. Instead of
having a deep processor pipeline (which becomes less useful as its depth increases), SMT
processors contain multiple shorter pipelines, each associated with a single thread. The result
can, for highly parallel applications, substantially improve the scheduling of on-chip resources
that, on conventional CPUs, would normally be starved as a result of both I/O stalls as well as
thread context-switching.
The work described here applies a threaded dataflow design to a higher level of execution 
the information gathering plan level. Instead of executing fine-grained instructions, we are
interested in the execution of coarse-grained operators. Still, we believe that threaded dataflow is
generally an efficient strategy for executing I/O-bound information gathering plans that integrate
multiple remote sources because it allows coarse-grained I/O requests (such as network requests
to multiple Web sources) to be automatically scheduled in parallel. Such plans are similar to
other systems that maintain high degrees of concurrent network connections, such as a Web
server or database system. Prior studies on such Web servers (Redstone, Eggers, & Levy, 2000)
and database systems (Lo, Barroso, Eggers, Gharachorloo, Levy, & Parekh, 1998) have already
shown that such systems run very efficiently on SMT-style processors; we believe the same will
hold true for the execution of dataflow-style information gathering plans.
2.2 Web-based Information Gathering and Integration
Generic information integration systems (Chawathe et al., 1994; Arens et al., 1996; Levy et al.,
1996; Genesereth et al., 1997) are concerned with the problem of allowing multiple distributed
information sources to be queried as a logical whole. These systems typically deal with
heterogeneous sources  in addition to traditional databases, they provide transparent access to
flat files, information agents, and other structured data sources. A high-level domain model maps
domain-level entities and attributes to underlying sources and the information they provide. An
information mediator (Wiederhold 1996) is responsible for query processing, using the domain
model and information about the sources to compile a query plan. In traditional databases, query
processing involves three major phases: (a) parsing the query, (b) query plan generation and
optimization and (c) execution. Query processing for information integration involves the same
phases but builds upon traditional query plan optimization techniques by addressing cases that
involve duplicate, slow, and/or unreliable information sources.
Web-based information integration differs from other types of information integration by
focusing on the specific case where information sources are Web sites (Knoblock et al., 2001).
This adds two additional challenges to the basic integration problem: (1) that of retrieving
structured information (i.e., a relation) from a semi-structured source (Web pages written in
HTML) and (2) querying data that is organized in a manner that facilitates human visual
consumption, not necessarily in a strictly relational manner. To address the first challenge, Web

631

fiBARISH & KNOBLOCK

site wrappers are used to convert semi-structured HTML into structured relations, allowing Web
sites to be queried as if they were databases. Wrappers take queries (such as those expressed in a
query language like SQL) and process them on data extracted from a Web site, thus providing a
transparent way of accessing unstructured information as if it were structured. Wrappers can be
constructed manually or automatically, the latter using machine learning techniques (Knoblock,
Lerman, Minton, & Muslea 2000; Kushmerick, 2000). While wrappers can be used to extract
data from many Web sites, other sites are problematic because of how the data to be extracted is
presented. One common case is where the Web site distributes a single logical relational answer
over multiple physical Web pages, such as in the case of the online classifieds example described
earlier. Automating interleaved navigation with gathering is required in such scenarios, yet it has
received little attention in the literature. One approach is to extend traditional query answering
for information integration systems to incorporate the capability for navigation (Friedman et al.,
1999). However, such solutions mostly address the query processing phase and it remains an
open issue regarding how to execute these types of information gathering plans efficiently.
A more recent technology for querying the Web is the network query engine (Ives et al.,
1999; Hellerstein et al., 2000; Naughton et al., 2001). While these systems are, like mediators,
capable of querying sets of Web sources, there has been a greater focus on the challenges of
efficient query plan execution, robustness in the face of network failure or large data sets, that of
processing XML data. Many network query engines rely on adaptive execution techniques, such
as dynamic reordering of tuples among query plan operators (Avnur & Hellerstein 2000) and the
double pipelined hash join (Ives et al., 1999), to overcome the inherent latency and unpredictable
availability of Web sites.
An important aspect of network query engine research has been its focus on dataflow-style
execution. Research on parallel database systems has long regarded dataflow-style query
execution efficient (Grafe, 1993; Wilschut & Apers, 1993). However, when applied to the Web,
dataflow-style processing can yield even greater speedups because (a) Web sources are remote, so
the base latency of access is much higher than that of accessing local data and (b) Web data
cannot be strategically pre-partitioned, as it can in shared-nothing architectures (DeWitt & Gray,
1992). Thus, because the average latency of Web data access is high, the parallelizing capability
of dataflow-style execution is even more compelling than it is for traditional parallel database
systems because the potential speedups are greater.

3. Motivating Example
As discussed earlier, while mediators and network query engines allow distributed Web data to be
queried efficiently, they cannot handle some of the more complicated types of information
gathering tasks because their query (and thus plan) languages do not support the degree of
expressivity required. To better motivate our discussion, we now describe a detailed example of
an information gathering problem that requires a more complex plan. Throughout the rest of this
paper, we will refer to this example as we describe the details of our proposed agent plan
language and execution system.
Our example involves using the Web to search for a new house to buy. Suppose that we want
to use an online real estate listings site, such as Homeseekers (http://www.homeseekers.com), to
locate houses that meet a certain set of price, location, and room constraints. In doing so, we
want our query to run periodically over a medium duration of time (e.g., a few weeks) and have
any new updates (i.e., new houses that meet our criteria) e-mailed to us as they are found.
To understand how to automate the gathering part of this task, let us first discuss how users
would complete it manually. Figures 2a, 2b, and 2c show the user interface and result pages for

632

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

Figure 2a: Initial query form for Yahoo Real Estate

Figure 2b: Initial results from Yahoo Real Estate

Figure 2c: Detailed result from Yahoo Real Estate

633

fiBARISH & KNOBLOCK

Homeseekers. To query for new homes, users initially fill the criteria shown in Figure 2a 
specifically, they enter information that includes city, state, maximum price, etc. Once they fill in
this form, they submit the query to the site and an initial set of results are returned  these are
shown in Figure 2b. However, notice that this page only contains results 1 through 15 of 22. To
get the remainder of the results, a "Next" link (circled in Figure 2b) must be followed to the page
containing results 16 through 22. Finally, to get the details of each house, users must follow the
URL link associated with each listing. A sample detail screen is shown in Figure 2c. The detail
screen is useful because it often contains pictures and more information, such as the MLS
(multiple listing services) information, about each house. In our example, the detailed page for a
house must be investigated in order to identify houses that contain the number of rooms desired.
Users would then repeat the above process over a period of days, weeks, or even months. The
user must both query the site periodically and keep track of new results by hand. This latter
aspect can require a great deal of work  users must note which houses in each result list are new
entries and identify changes (e.g., selling price updates) for houses that have been previously
viewed.
As we have already discussed, it is possible to accomplish part of our task using existing Web
query techniques, such as those provided by mediators and network query engines. However,
notice that our task requires actions beyond gathering and filtering data. It involves periodic
execution, comparison with past results, conditional execution, and asynchronous notification to
the user. These are not actions that traditional Web query languages support  indeed, these
actions involve more than gathering and filtering. Instead of a query plan language, what is
needed is an agent plan language that supports the operators and constructs necessary to complete
the task.
We can consider how such agent plans generally might look. Figure 3 shows an abstract plan
for monitoring Homeseekers. As the figure shows, search criteria are used as input to generate
one or more pages of house listing results. The URLs for each house from each results page are
extracted and then compared against houses that already existed in a local database. New houses
 those on the web page but not in the database  are subsequently queried for their details and
appended to the database so that future queries can distinguish new results. During the extraction
of houses from a given Homeseekers results page, the "Next" link (if any) on that page is
followed and the houses on that page go through the same process. The next-link processing
cycle stops when the last result page, the page without a Next link, has been reached. Then,
after the details of the last house have been gathered, an update on the set of new houses found is
e-mailed to the user.
LOAD DAT AB ASE
of h ous es
pre viousl y s een
EXTRACT
hous e URLs

search
criteria

GET h ouse
results p ag e

FILTER OUT
thos e h ouses
pre viousl y s een

EXTRACT
"ne xt pag e" lin k

GET h ouse
det ail pag e

UPDATE D ATAB ASE
with ne w ho uses

Figure 3: Abstract plan for monitoring Yahoo Real Estate

634

SEND E-MAIL
to t he user

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

4. An Expressive Plan Language for Software Agents
In this section, we present an agent plan language that makes it possible to construct plans
capable of more complicated tasks. Throughout this section, we focus on information gathering
tasks, such as the Homeseekers example shown in Figure 3.
4.1 Plan Representation
In our language, plans are textual representations of dataflow graphs describing a set of input
data, a series of operations on that data (and the intermediate results it leads to), and a set of
output data. As discussed earlier, dataflow is a naturally efficient paradigm for information
gathering plans. Graphs consist of a set of operator sequences (flows) where data from one
operator in a given flow is iteratively processed and then flows to successive operators in the
flow, eventually being merged with another flow or output from the plan.
For example, Figure 4 illustrates the dataflow graph form of a plan named Example_plan. It
shows that the plan consists of six nodes (operators) connected with a set of edges (variables).
The solid directed edges (labeled a, b, c, d, f, and g) represent a stream of data, while the dashed
directed edge (labeled e) represents a signal used for synchronization purposes.
c

a
b

Op1

d
Op2

f
Op4
g

e

Op3

Op5

Figure 4: Graph form of Example_plan

Figure 5 shows the text form of the same plan. The header consists of the name of the plan
(example_plan), a set of input variables (a and b), and a set of output variables (f). The body
section of the plan contains the set of operators. The set of inputs for each operator appears to the
left of the colon delimiter and the set of outputs appears to the right of the delimiter. One
operator (Op3) has a WAIT clause that is associated with the production of the signal indicated in
Figure 4 by e. The ENABLE clause of a later operator (Op5) describes the consumption of that
signal.
Both the graph and text forms of the example plan describe the following execution.
Variables a and b are plan input variables. Together, they trigger the execution of Op1, which
produces variable c. Op2 fires when c becomes available, and this leads to the output of variable
d. Op3 fires upon the availability of d and produces the signal e. Op4 uses d to compute f (the
PLAN example_plan
{
INPUT: a, b
OUTPUT: f
BODY
{
Op1
Op2
Op3
Op4
Op5
}

(a, b : c)
(c : d)
(c : ) {ENABLE: e}
(d : f, g)
(g : ) {WAIT : e}

}
Figure 5: Text form of Example_plan

635

fiBARISH & KNOBLOCK

plan output variable) and g. Finally, the availability of g and the signal e triggers the execution of
Op5.
Note that although the body part of the text form of the plan lists operators in a linear order,
this ordering does not affect when they are actually executed. Per the dataflow model of
processing, operators fire whenever their individual data dependencies are fulfilled. For example,
although Op3 follows Op2 in the order specified by the plan text, it actually executes at the same
logical time as Op2. Also note that plan output, f, can be produced while the plan is still running
(i.e., while Op5 is still processing).
4.1.1. FORMAL DEFINITIONS
Formally, we define the following:
Definition 1: An information gathering plan P can be represented as a directed graph of
operators Ops as nodes connected through a set of variables Vars that are the edges. Each plan
is associated with a subset of Vars that are plan input variables PlanIn and another subset of
variables that are plan output variables PlanOut. More specifically, let a plan P be represented
as the tuple
P = <Vars, Ops, PlanIn, PlanOut>
where
Vars = {v1, ..., vn}, n > 0
Ops = {Op1, ..., Opm}, m > 0
PlanIn = {va1, ..., vax}, x > 0, s.t. {va1, ..., vax}  Vars
PlanOut = {vb1, ..., vby}, y >= 0, s.t. {vb1, ..., vby}  Vars
Definition 2: A plan operator Op encapsulates a function Func that computes a set of operator
output variables OpOut from a set of operator input variables OpIn. More specifically, let each
operator Opi in P be represented as the tuple
where

Opi = <OpIn, OpOut, Func>
OpIn = {vi1, ..., vic}, c > 0, s.t. {vi1, ..., vic}  Vars
OpOut = {vo1, ..., vog}, g >= 0, s.t. {vo1, ..., vog}  Vars
Func = Function that computes {vo1, ..., vog} from {vi1, ..., vic}

Furthermore, any plan Pa can also be called from another plan Pb as an operator. In this case,
the plan Pa is known as a subplan.
Definition 3: The schedule of execution for any operator instance Opi is described by a firing
rule i that depends on OpIn, an optional second set of input wait variables OpWait, and results
in the generation of OpOut and an optional second set of output enablement variables OpEnable.
The initial firing of an operator is conditional on the availability of at least one of OpIn and all of
OpWait. After the initial firing, any OpEnable variables declared are also produced. All other
OpOut variables are produced in accordance with the semantics of the operator.
More
specifically, let us define:
where

i (Opi) = <OpIn, OpWait, OpOut, OpEnable>
OpWait = {vw1, ..., vwd}, d >= 0, s.t. {vw1, ..., vwd}  Vars
OpEnable = {ve1, ..., veh}, h >= 0, s.t. {ve1, ..., veh}  Vars

Wait and enable variables are synchronization mechanisms that allow operator execution to be
conditional beyond its normal set of input data variables. To understand how, let us first
distinguish between a standard data variable and a synchronization variable. A standard data
variable is one that contains information that is meant to be interpreted, or more specifically,

636

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

processed by the function that an operator encapsulates. For example, PlanIn, PlanOut, OpIn,
and OpOut all consist of normal data variables. A synchronization variable (earlier called a
signal) is one that consists of data not meant to be interpreted  rather, such variables are
merely used as additional conditions to execution. Since control in dataflow systems is driven by
the availability of data, synchronization variables in dataflow style plans are useful because they
provide more control flow flexibility. For example, if a certain static operation should occur each
time a given data flow is active, synchronization variables allow us to declare such behavior.
Definition 3 indicates that, like actors in traditional dataflow programs, operators in
information gathering plans have a firing rule that describes when an operator can process its
input. For example, in the dataflow computer specified by (Dennis 1974), actors can fire when
their incoming arcs contain data. For the plans in the language described in this paper, the firing
rule is slightly different:
An operator may fire upon receipt of any input variable, providing it has received all
of its wait variables.
Note that both plans and operators require at least one input because, as the firing rule implies,
plans or operators without at least one input would fire continuously.
4.2 Data Structures
Operators process and transmit data in terms of relations. Each relation R consists of a set of
attributes (i.e., columns) a1..ac and a set of zero or more tuples (i.e., rows) t1..tr, each tuple ti
containing values vi1..vic. We can express relations with attributes and a set of tuples containing
values for each of those attributes as:
R (a1, ..., ac) = {{v11, ..., v1c}, {v21, ..., v2c}, ..., {vr1, ..., vrc}}
Each attribute of a relation can be one of five types: char, number, date, relation (embedded), or
document (i.e., a DOM object).
Embedded relations (Schek & Scholl, 1986) within a particular relation Rx are treated as
opaque objects vij when processed by an operator. However, when extracted, they become a
separate relation Ry that can be processed by the rest of the system. Embedded relations are
useful in that they allow a set of values (the non-embedded objects) to be associated with an
entire relation. For example, if an operator performs a COUNT function on a relation to
determine the number of tuples contained in that relation, the resulting tuple emitted from the
operator can consist of two attributes: (a) the embedded relation object and (b) the value equal to
the number of rows in that embedded relation. Embedded relations thus allow sets to be
associated with singletons, rather than forcing a join between the two. In this sense, they preserve
the relationship between a particular tuple and a relation without requiring the space for an
additional key or the repeating of data (as a join would require).
XML data is supported through the document attribute type. XML is one type of document
specified by the Document Object Model (DOM). The proposed language here contains specific
operators that allow DOM objects to be converted to relations, for relations to be converted to
DOM objects, and for DOM objects that are XML documents to be queried in their native form
using XQuery. Thus, the language supports the querying of XML documents in their native or
flattened form.
4.3 Plan Operators
The available operators in the plan language represent a rich set of functions that can be used to
address the challenges of more complex information gathering tasks, such as monitoring.
Specifically, the operators support the following classes of actions:


data gathering: retrieval of data from both the network and from traditional relational
databases, such as Oracle or DB2.

637

fiBARISH & KNOBLOCK








data manipulation: including standard relational data manipulation, such as Select and
Join, as well as XML-style manipulations such as XQuery.
data storage: the export and updating of data in traditional relational databases.
conditional execution: routing of data based on its contents at run-time.
asynchronous notification: communication of intermediate/periodic results through
mediums/devices where transmitted data can be queued (e.g., e-mail).
task administration: the dynamic scheduling or unscheduling of plans from an external
task database.
extensibility: the ability to embed any special type of computation (single-row or
aggregate) directly into the streaming dataflow query plan

Though operators differ on their exact semantics, they do share some similarities in how they
process input and generate output. In particular, there are two modes worth noting: the automatic
joining of output to input (a dependent join) and the packing (embedding) and unpacking
(extracting) of relations.
In information gathering plans, it is common to use data collected from one source as a basis
for querying additional sources. Later, it often becomes desirable to associate the input to the
source with the output it produces. However, doing this join as a separate step can be tedious
because it requires the creation of another key on the existing set of data plus the cost of a join.
To simplify plans and improve the efficiency of execution, many of the operators in the language
perform a dependent join of input tuples onto the output tuples that they produce. A dependent
join simply combines the contents of the input tuple with any output tuple(s) it generates,
preserving the parity between the two. For example, the operator ROUND converts a floating
point value in a column to its nearest whole integer value. Thus, if the input data consisted of the
tuples {{Jack, 89.73}, {Jill, 98.21}} then the result after the ROUND operator executes would be
of {{Jack, 89.73, 90}, {Jill, 98.21, 98}}. Without a dependent join, a primary key would need to
be added (if one did not already exist) and then a separate join would have to be done after the
ROUND computation. Thus, dependent joins simplify plans  they reduce the total number of
operators in plan (by reducing the number of decoupled joins) and eliminate the need to ensure
entity integrity prior to processing.
Another processing mode of operators involves the packing and unpacking of relations3.
These operations are relevant in the context of embedded relations. Instead of creating and
managing two distinct results (which often need to be joined later), it is cleaner and more spaceefficient to perform a dependent join on the packed version of an input relation with the result
output by an aggregate-type operator. For example, when using an AVERAGE operator on the
input data above, the result after a dependent join with the packed form of the original relation
would be: {{{Jack, 89.73}, {Jill, 98.21}}, 93.97}. Unpacking would be necessary to get at the
original data. In short, embedded relations make it easy to associate aggregates with the values
that led to their derivation. Packing and unpacking are useful data handling techniques that
facilitate this goal.
Table 1 shows the entire set of operators in the proposed language. Some of these (such as
Select and Join) have well-known semantics (Abiteboul, Hull, & Vianu, 1995) and are used in
other database and information gathering systems. As a result, we will not discuss them here in
any detail. However, many of the operators are new and provide the ability to express more
complicated types of plans. We now focus on the purpose and mechanics of some of these other
operators.

3

Note: These operations are also referred to as NEST and UNNEST in database literature.

638

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

Operator

Purpose

Fetch and extract data from web sites into relations.
Filters data from a relation.
Filters attributes from a relation.
Combines data from two relations, based on a specified condition.
Performs a set union of two relations.
Finds the intersection of two relations.
Subtracts one relation from another.
Returns tuples unique across one or more attributes.
Conditionally routes one of two streams based on existence of tuples in a third
Embeds a relation within a new relation consisting of a single tuple.
Extracts an embedded relation from tuples of an input relation.
Generates a new formatted text attribute based on tuple values.
Converts a relation to an XML document.
Converts an XML document to a relation.
Queries an XML document attribute of tuples of an input relation using language
specified by the Xquery standard, returning an XML document result attribute
Xquery
contained in the tuples of the output relation.
DbImport Scan a table from a local database.
DbQuery Query the schema from a local database using SQL.
DbAppend Appends a relation to an existing table  creates the table if none exists.
DbExport Exports a relation to a single table.
DbUpdate Executes a SQL-style update query; no results returned.
Uses SMTP to communicate an email message to a valid email address.
Email
Sends a text message to a valid cell phone number.
Phone
Faxes data to a recipient at a valid fax number.
Fax
Schedule Adds a task to the task database with scheduling information.
Unschedule Removes a task from the database.
Executes a user-defined function on each tuple of a relation.
Apply
Aggregate Executes a user-defined function on an entire relation.
Wrapper
Select
Project
Join
Union
Intersect
Minus
Distinct
Null
Pack
Unpack
Format
Rel2xml
Xml2rel

Table 1: The complete set of operators

4.3.1. INTERACTING WITH LOCAL DATABASES
There are two major reasons why it is useful to be able to interact with local database systems
during plan execution. One reason is that the local database may contain information that we
wish to integrate with other online information. A second reason has to do with the ability for the
local database to act as memory for plans that run continuously or when a plan run at a later
time needs to use the results of a plan run at an earlier time.
To address both needs, the database operators DbImport, DbQuery, DbExport, and DbAppend
are provided. A common use for these operators is to implement a monitoring-style query. For
example, suppose we wish to gradually collect data over a period of time, such as the collection
of house data in the Homeseekers example. To accomplish this, DbImport or DbQuery can be
used to bring previously queried data into a plan so that it can be compared with newly queried
data (gathered by a Wrapper operator) by using any of the set-theoretic operators, such as Minus)
and the result or difference can be written back to the database through DbAppend or DbExport.
4.3.2. SUPPORTING CONDITIONAL EXECUTION
Conditional execution is important for plans that need to perform different actions for data based
on the run-time value of that data. To analyze and conditionally route data in a plan, the language

639

fiBARISH & KNOBLOCK

supports the Null operator. Null acts as a switch, conditionally routing one set of data based on
the status of another set of data. Null refers to the predicate Is Null, which is a conditional that
executes different actions depending on the result of the evaluation. When the data is null, action
A is performed; when it is not null, action B is performed. To accomplish this in dataflow, the
Null operator publishes different sets of data, one for either case.
For example, suppose it is desirable to have stock quotes automatically communicated to a
user every 30 minutes. Normally, quotes should be retrieved and then e-mailed. However, if the
percentage price change of any stock in the portfolio is greater than 20%, then all quotes should
be sent via cell phone messaging (since such communication can be more immediate). Null
would be useful in such a case because it would allow a Select condition to process the check on
price changes and  if there exist tuples that match the filtering criteria  allow that data to trigger
an operator that communicated those results via cell phone. Otherwise, Null would route the data
to an operator that communicated in the information via e-mail. In short, Null is powerful
because it is a dynamic form of conditional execution in that it can be used with other operators
(like Select) to activate/deactivate flows based on the runtime content of the data.
The input and output to Null is summarized in Figure 6. The input is data to be analyzed d,
data to be forwarded upon true (null) dt, and the data to be forwarded upon false df. If d is null
(i.e., contains zero tuples), then dt is copied as output variable t. Otherwise, df is copied as output
f. For example, if d contains three tuples {x1, x2, x3} and if dt contains five tuples {t1, t2, t3, t4,
t5} and df contains two tuples {f1, f2}, then only a variable t containing {t1, t2, t3, t4, t5} is
output. Consumers of f will never receive any data.

d
dt
df

Null

t
f

Figure 6: The NULL operator

4.3.3. CALLING USER-DEFINED FUNCTIONS
In designing a number of agent plans, we found that there were times when agents needed to
execute some special logic (e.g., business logic) during execution. Usually, this logic did not
involve relational information processing and the plan writer simply wanted to be able to code in
a standard programming language (such as Java or C). For example, in some of the plans written
for the Electric Elves travel agents (Ambite et al., 2002), it was necessary for the agent to send
updates to users via the DARPA CoAbs Agent Grid network. In other plans, we needed to
normalize the formats of date strings produced by different Web sources. Instead of expanding
the operator set for each unique type of logic encountered, we developed two special operators
that allowed plans to make calls to arbitrary functions written in standard programming
languages. The goal of having these operators was to (a) make it easier to write plans that
required special calculations or library calls, (b) encourage non-relational information processing
(which could not benefit from the efficiency of dataflow style processing) to be modularized
outside of the plan, and (c) to simplify plans.
The two operators, Apply and Aggregate, provide extensibility at both the tuple and relation
level. Apply calls user-defined single-row functions on each tuple of relational data and performs
a dependent join on the input tuple with its corresponding result. For example, a user-defined
single-row function called SQRT might return a tuple consisting of two values: the input value
and its square root. The user defined function is written in a standard programming language,
such as a Java, and is executed on a per-tuple basis. Thus, this type of external function is very
similar to the use of stored procedures or UDFs in commercial relational database systems.
The Aggregate operator calls user-defined multi-row functions and performs a dependent join
on the packed form of the input and its result. For example, a COUNT function might return a

640

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

relation consisting of a single tuple with two values: the first being the packed form of the input
and the second being the count of the number of distinct rows in that relation. As with Apply, the
user-defined multi-row function is written in a standard programming language like Java.
However, in contrast to being called on a per-tuple basis, it is executed on a per-relation basis.
4.3.4. XML INTEGRATION
For purposes of efficiency and flexibility, it is often convenient to package or transform data
to/from XML in mid-plan execution. For example, the contents of a large data set can often be
described more compactly by leveraging the hierarchy of an XML document. In addition, some
Web sources (such as Web services) already provide query answers in XML format. To analyze
or process this data, it is often simpler and more efficient to deal with it in its native form rather
than to convert it into relations, process it, and convert it back to XML. However, in other cases,
a relatively small amount XML data might need to be joined with a large set of relational data.
To provide flexible XML manipulation and integration, the language supports the Rel2xml,
Xml2rel, and Xquery operators. The first two convert relations to XML documents and viceversa, using straightforward algorithms. Xml2Rel allows one to specify an iterating element
(which map to tuples in the relation) and attribute elements (which map to attributes of the
relation), and generates tuples that include an index referring to the order in which the original
XML element was parsed. Cross product style flattening for deeper child elements is performed
automatically. Rel2Xml is even more straightforward: it creates parent XML elements for each
tuple and inserts attribute elements as children, in the order they appear in the relation. To allow
XML to be processed in its native form, we support the Xquery operator, based on the XQuery
standard (Boag et al., 2002).
The Xml2Rel, Rel2Xml, and Xquery are complementary in terms of functionality. Xml2Rel
handles the basic conversion of XML to relational data, noting the order of data in the document.
Rel2Xml handles the basic conversion back to XML, without regards to order  note that the
nature of streaming dataflow parallelism is such that order of processing those tuples is
deliberately not guaranteed. However, if the order of the XML document generated by Rel2Xml
is important, Xquery can be used as a post-processing step to address that requirement. In short,
both Xml2Rel and Rel2Xml focus on the simple task of converting from a relation to a document;
any complex processing of XML can be accomplished through the Xquery operator.
4.3.5. ASYNCHRONOUS NOTIFICATION
Many continuously running plans, such as Homeseekers, do not involve interactive sessions with
users. Instead, users request that a plan be run on a given schedule and expect to receive updates
from the periodic execution of that plan. These updates are delivered through asynchronous
means, such as e-mail, cell-phone messaging, or facsimile. To facilitate such notification, the
language includes the Email, Fax, and Phone operators for communicating data via these devices.
Each of these operators works in a similar fashion. Input data received by the operator is reformatted into a form that is suitable for transmission to the target device. The data is then
transmitted: Email sends an e-mail message, Fax contacts a facsimile server with its data, and
Phone routes data to cell phone capable of receiving messages.
4.3.6. AUTOMATIC TASK ADMINISTRATION
The overall system that accompanies the language includes a task database and a daemon process
that periodically reads the task database and executes plans according to their schedule. This
architecture is shown in Figure 7. Task entries consist of a plan name, a set of input to provide to
that plan, and scheduling information. The latter data is represented in a format similar to the
UNIX crontab entry. This format allows the minute, hour, day of the month, month, and year that
a plan is supposed to be run. For example, a task entry of
05 08-17 1,3,5 * * homeseekers.plan

641

fiBARISH & KNOBLOCK

Figure 7: Task administration process

means: run homeseekers.plan at five minutes after every hour between 8am and 5pm on the 1st,
3rd, and 5th days of every month of every year.
While tasks can be scheduled manually, the language we have developed also allows plans to
automatically update the scheduling of other plans, including it. To do so, we support two special
scheduling operators, Schedule and Unschedule. The former allows a plan to register a new plan
to be run. It creates or updates plan schedule data in the task database. The input to Schedule
consists of a plan name and a schedule description, such as the one shown above. The operator
produces a single tuple of output that indicates the assigned task ID of the scheduled task.
Unschedule removes a scheduled plan from the task database. Unschedule can be used by a
plan to remove itself from a monitoring activity and is often used in tandem with a notification
operator. For example, a plan can monitor the set of available houses on the market for the entire
month of September, send an email at the end of that month to the user containing the results,
unschedule itself from execution, and then schedule a new plan (perhaps, for example, to clean up
the database that stored the monitoring data). The input to Unschedule is the task ID of the
scheduled plan and the output is a tuple indicating success or failure of the attempt to remove the
plan from the task database.
4.4 Subplans
To promote reusability, modularity, and the capability for recursion, the plan language supports
the notion of subplans. Recall that all plans are named, consist of a set of input and output
streams, and a set of operators. If we consider that the series of operators amounts to a complex
function on the input data, then plans present the same interface as do operators. In particular,
using our earlier definitions, it is possible that Opi = P in that OpIn = PlanIn, OpOut = PlanOut,
OpWait = , OpEnable = , and Func = {Op1, ..., Opn}. Thus, a plan can be referenced within
another plan as if it were an operator. During execution, a subplan is called just like any other
operator would  as inputs of the subplan arrive, they are executed within the body of the subplan
by the operators of that subplan. For example, consider how the Example_plan, introduced
earlier, can be referenced by another plan called Parent_plan. Figure 8 illustrates how the text
form of Parent_plan treats Example_plan as merely another operator.

642

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

PLAN parent_plan
{
INPUT: w, x
OUTPUT: z
BODY
{
Op6 (w : y)
example_plan (x, y : z)
}
}
Figure 8: Text of parent_plan

Subplans encourage modularity and re-use. Once written, a plan can be used as an operator in
any number of future plans. Complicated manipulations of data can thus be abstracted away,
making plan construction simpler and more efficient.
For example, one could develop a simple subplan called Persistent_diff, shown in Figure 9,
that uses the existing operators DbQuery, Minus, Null, and DbAppend to take any relation,
compare it to a named relation stored in a local database. This plan determines if there was an
update, appends the result, and returns the difference. Many types of monitoring style plans that
operate on updated results can incorporate this subplan into their existing plan. The Homeseekers
plan itself could be a subplan that returns house details given a set of search parameters.
MINUS

relation

diff

DBQUERY

NULL

DBAPPEND

Figure 9: The Persistent_diff subplan

4.4.1. RECURSION
In addition to promoting modularity and re-use, subplans make another form of control flow
possible: recursion. Recursive execution can be useful in a number of scenarios related to Web
query processing. Here we describe two: reformulating data integration queries and iterating over
Next Page links.
One application of recursion in THESEUS involves reformulating data integration queries. For
example, a more efficient version of the Duschkas Inverse Rules algorithm (Duschka 1997) can
be implemented using recursive streaming dataflow execution in THESEUS (Thakkar & Knoblock,
2003). Support for recursion in query reformulation allowed Thakkar and Knoblock to develop a
system that produced more complete answers than other query reformulation algorithms, such as
MiniCon (Pottinger & Levy, 2001), which do not support recursion.
Another practical use of recursion in Web data integration involves iterating over a list that is
described over multiple documents. As described earlier, a number of online information
gathering tasks require some sort of looping-style (repeat until) control flow. Results from a
single query can be spanned across multiple Web pages. Recursion provides an elegant way to
address this type of interleaved information gathering and navigation in a streaming dataflow
environment.
For example, when processing results from a search engine query, an automated information
gathering system needs to collect results from each page, follow the "next page" link, collect
results from the next page, collect the "next page" link on that page, and so on  until it runs out
of "next page" links. If we were to express this in von Neumann style programming language, a
Do...While loop might be used accomplish this task. However, implementing these types of loops

643

fiBARISH & KNOBLOCK

in a dataflow environment is problematic because it requires cycles within a plan. This leads to
data from one loop iteration possibly colliding with data from a different iteration. In practice,
loops in dataflow graphs require a fair amount of synchronization and additional operators.
Instead, this problem can be solved simply with recursion. We can use subplan reference as a
means by which to repeat the same body of functionality and we can use the Null operator as the
test, or exit condition. The resulting simplicity and lack of synchronization complexity makes
recursion an elegant solution for addressing cases where navigation is interleaved with retrieval
and when the number of iterations for looping style information gathering is not known until
runtime. As an example of how recursion is used, consider the abstract plan for processing the
results of a search engine query. A higher level plan called Query_search_engine, shown in
Figure 10a, posts the initial query to the search engine and retrieves the initial results. It then
processes the results with a subplan called Gather_and_follow, shown in Figure 10b. The search
results themselves go to a Union operator and the next link is eventually used to call
Gather_and_follow recursively. The results of this recursive call are combined at the Union
operator with the first flow.
WRAPPER

search term

GATHER_AND_FOLLOW

initial-result s

web pages

Figure 10a: The Query_search_engine plan

false

urls

DISTINCT
ne xt -pag e -link

WRAPPER
ne xt-re sults

GATHER_AND_FOLLOW

true

NULL

PROJECT

UNION

url

Figure 10b: The Gather_and_follow recursive subplan

4.4.2. REVISITING THE EXAMPLE
Let us now revisit the earlier house search example and see how such a plan would be expressed
in the proposed plan language. Figure 11a shows one of the two plans, Get_houses, required to
implement the abstract real estate plan in Figure 3. Get_houses calls the subplan Get_urls shown
in Figure 11b, which is nearly identical to the plan Gather_and_follow, described above. The rest
of Get_houses works as follows:
(a) A Wrapper operator fetches the initial set of houses and link to the next page (if any)
and passes it off to the Get_urls recursive subplan.
(b) A Minus operator determines which houses are distinct from those previously seen;
new houses are appended to the persistent store.
(c) Another Wrapper operator investigates the detail link for each house so that the full
set of criteria (including picture) can be returned.
(d) Using these details, a Select operator filters out those that meet the specified search
criteria.
(e) The result is e-mailed to the user.

644

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

criteria

PROJECT

WRAPPER

price -inf o

house -urls

GET_URLS

WRAPPER

SELECT

raw-ho use -de tails

co nd

FORMAT
"be ds = %s"

Figure 11a: The Get_houses plan
WRAPPER
false
urls

DISTINCT
ne xt -p ag e -link

NULL

GET_URLS

next -pag e -link

true

PROJECT

UNION

ho use -url

Figure 11b: The Get_urls recursive subplan

5. An Efficient Plan Execution Architecture
By definition, Web information gathering involves processing data gathered from remote sources.
During the execution of an information gathering plan, it is often the case that multiple
independent requests are made for different sets of remote data. Those data are then
independently processed by a series of operations and then combined or output. Network
latencies, bandwidth limitations, slow Web sites, and queries that yield large result sets can
dramatically curtail the execution performance of information gathering plans. This is especially
the case when plan operators are executed serially: any one of the issues mentioned can
bottleneck the execution of an entire plan.
From an efficiency standpoint, there are two problems with standard von Neumann execution
of information gathering plans. One is that it does not exploit the independence of data flows in a
common plan in that multiple unrelated requests for remote data cannot be parallelized. The plan
language we have designed addresses this problem somewhat by allowing plans to be expressed
in terms of their minimal data dependencies: still, that does not dictate how those operators are
actually executed.
The second efficiency problem is that von Neumann execution does not exploit the
independence of tuples in a common relation: for example, when a large data set is being
progressively retrieved from a remote source, the tuples that have already been retrieved could
conceivably be operated on by successive operators in the plan. This is often reasonable, since
the CPU on the local system is often under-utilized while remote data is being fetched.
To remedy both problems, we have designed a streaming dataflow execution system for
software agent plans. The system allows the maximum degree of operator and data parallelism to
potentially be realized at runtime, by executing multiple operators concurrently and pipelining
data between operators throughout execution. Other network query engines have implemented
designs that bear some similarity to what we present below. However, our discussion below
extends the existing work in three ways:


We describe the details of execution (i.e., how threads interact and how our firing rules
work). With the exception of (Shah, Madden, Franklin, & Hellerstein, 2001), we have
not been able to locate a similar discussion of the details of execution in these other
systems.

645

fiBARISH & KNOBLOCK




We present a novel thread-pooling approach to execution, where multiple threads are
shared by all operators in a plan. This allows significant parallelism without exhausting
resources.
We describe how recursive streaming dataflow execution is implemented using a data
coloring approach.

5.1 Dataflow Executor
While our plan language allows dataflow-style plans to be coded in text, it does not specify how
the actual execution process works. Thus, to complement the language and to efficiently execute
plans, we developed a true dataflow-style executor. The executor allows plans to realize
parallelization opportunities between independent flows of data, thus enabling greater horizontal
parallelism at runtime.
The executor functions as a virtual threaded dataflow machine. It assigns user-level threads to
execute operators that are ready to fire. This type of execution is said to be virtual dataflow
because thread creation and assignment is not done natively by the CPU, nor even in kernel space
by the operating system, but by an application program (the executor) running in user space. By
using threads to parallelize execution of a plan, the executor can realize better degrees of true
parallelism, even on single CPU machines. This is because the use of threads reduces the impact
of any I/O penalties caused by a currently executing operator. That is, multiple threads reduce the
effect of vertical waste that can occur when single-threaded execution reaches an operation that
blocks on I/O.
For example, consider the case where a plan containing two independent Wrapper operators is
being executed on a machine with a single CPU. Suppose that both Wrapper operators have their
input and can fire. Both operators will be assigned distinct threads. The single CPU will execute
code that issues the network request for the first Wrapper operator, not wait for data to be
returned, and finish issuing the network request for the second Wrapper operator. Thus, in a
matter of microseconds, both operators will have issued their requests (which typically take on
the order of hundreds of milliseconds to complete) and retrieval of the data (on the remote sites)
will have been parallelized. Thus, the overall execution time will be equal to the slower of the
two requests to complete. This contrasts with the execution time required for serial execution,
which is equal to the sum of time required for each request.
5.1.1. PROMOTING AND BOUNDING PARALLELISM WITH THREAD POOLS
While using threaded dataflow has its benefits, past research in dataflow computing and operating
systems has shown that there are cases when parallelism must be throttled or the overhead of
thread management (i.e., the creation and destruction of threads) can be overly taxing. For
example, if threads are created whenever an operator is ready, the cost to create them can add up
to significant overhead. Also, if there is significant parallelism during execution, the number of
threads employed might result in context switching costs that outweigh the parallelism benefits.
To address both issues, we developed a thread pooling architecture that allows the executor to
realize significant parallelism opportunities within fixed bounds.
At the start of plan execution, a finite number of threads are created (this number is easily
adjustable through an external configuration file) and arranged in a thread pool. Once the threads
have been created, execution begins. When data becomes available (either via input or through
operator production), a thread from the pool is assigned to execute a method on the consuming
operator with that data. Each time that operator produces output, it hands off the output to zero or
more threads so that its consumer(s), if any, can process the output. If the pool does not contain
any available threads, the output is queued in a spillover work queue, to be picked up later by
threads as they return to the queue. This same behavior occurs for all operator input events.
Thus, parallelism is both ensured by the existence of multiple threads in the pool and bounded by
it  in the latter case, the degree of true parallelism during execution can never exceed the pool

646

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

size. Demands on parallelism beyond the number of threads in the pool are handled by the work
queue.
Figure 12 illustrates the details of how the thread pool is used by the executor at runtime. The
figure shows that there are four key parts to the executor:
 The thread pool: This is a collection of threads ready to process the input collected in
the queue. There can be a single thread pool or it can be partitioned so that certain
sources have a guaranteed number of threads available to operators that query those
sources. All available threads wait for new objects in the queue. Typically, contention
for the queue on machines with a single CPU is not an issue (even with hundreds of
threads). However, configuration options do exist for multiple work queues to be
created and for the thread pool to be partitioned across queues.


The spillover work queue: All data received externally and transmitted internally (i.e.,
as a result of operator execution) that cannot be immediately assigned to an available
thread is collected in this queue. As threads return to the pool, they check if there are
objects in the queue: if there are, they process them, otherwise the thread waits to be
activated by future input. The queue itself is an asynchronous FIFO queue implemented
as a circular buffer. When the queue is full, it grows incrementally as needed. The initial
size of the queue is configurable. The structure of a queue element is described in detail
below.



The routing table: This data structure describes the dataflow plan graph in terms of
producer/consumer operator method associations. For example, if a Select operator
produces data consumed by a Project operator, the data structure that marshals output
from the Select is associated with the Project input method that should consume this
data. The table is computed once  prior to execution  so that the performance of
operator-to-operator I/O is not impacted at runtime by repetitive lookups of consumers
by producers. Instead, pre-computation allows the data structure associated with a
producing method to immediately route output data to the proper set of consuming input
methods.

Runtime plan 
internal data structure
Operator obje cts

4

3

1
Plan
Input

Routing table

Thre ad
Pool

2a

Thre ads
available ?

Plan
Output

2b

Spillov er
wor k queu e

5

Figure 12: Detailed design of the executor

647

fiBARISH & KNOBLOCK



The set of operator objects: These are the collection of operator classes (including their
input/output methods and state data structures). There exists one operator object per
instance in the plan.

Each queue object consists of a tuple that describes:





the session ID
the iteration ID
the content (i.e., the data)
destination operator interface (i.e., a function pointer).

The session ID is used to distinguish independent sessions and the iteration ID to distinguish
current call-graph level of a session, which ensures safety during concurrent re-entrancy at
runtime. These IDs provide a unique key for indexing operator state information. For example,
during recursive execution, these IDs ensure that the concurrent firing of the same operator at
different levels of the call graph do not co-mingle state information. Finally, the destination
operator interface is the pointer to the code that the thread assigned to a queue object will run.
At runtime, the system works as follows. Initial plan input arrives and is assigned to threads
from the thread pool (#1 in Figure 12), one thread for each input tuple (#2a), or if no threads are
available the data is added to the spillover work queue (#2b). Each assigned thread from the pool
takes its queue object and, based on the description of its target, fetches the appropriate operator
object so that it can execute the proper function on the data (#3). During the execution of the
operator, state information from previous firings may be accessed using the (session ID, iteration
ID) pair as a key. The result of an operator firing may result in output. If it does, the operator
uses the routing table (#4) to determine the set of consumers of that output. It then composes new
data queue objects for each consumer and hands off those objects (#5) to either an available
thread in the thread pool (#2a) or deposits them to the work queue (#2b) if no threads are
available. To reduce memory demands, producers only deep-copy data they produce if there are
multiple consumers. Finally, operators that produce plan output data route that data out of the
plan as it becomes available.
5.2 Data Streaming
At a logical level, each of the variables in the plan language we describe is relations. However, to
provide more parallelism and thus efficiency at runtime, tuples of a common relation are
streamed between operators. Each stream consists of stream elements (the tuples in a relation),
followed by an end of stream (EOS) marker. Thus, when communicating a relation from
producer to consumer, producing operators communicate individual tuples to consumer operators
and follow the final tuple with an EOS token.
Streaming relations between operators increases the degree of vertical parallelism during
plan execution. In revisiting the firing rule described earlier, we can further clarify it to read:
An operator may fire upon receipt of any input tuple, providing it has received the
first tuple of all of its wait variables.
Thus, when an operator receives a single tuple on any of its inputs, it can consume and process
that tuple. Afterwards, it can potentially emit output that, in turn, can be consumed by a
downstream operator or output from the plan. The resulting parallelism is vertical in the sense
that two or more operators (e.g., one producer and one or more consumers) can concurrently
operate on the same relation of data. Remote sources that return significant amounts of data can
be more efficiently processed through streaming, since the operator that receives the network
transmission can pass along data for processing as it becomes available and before the rest of the
data has been received.

648

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

Support for any kind of streaming implies that state must be kept by operators between firings.
This is because the operation being performed is logically on an entire relation, even though it
physically involves each tuple of that relation. If the operator does not maintain state between
firings, it cannot necessarily produce correct results. For example, consider the set-theoretic
Minus operator that takes two inputs, lhs and rhs, and outputs the result of lhs - rhs. This
operator can begin emitting output as soon as it has received the rhs EOS token. However, the
operator must still keep track of rhs data until it receives the EOS from both; if not, it may emit a
result that is later found to be incorrect. To see how this could happen, suppose that the order of
input received by an instance of the Minus operator was:
lhs: (Dell)
lhs: (Gateway)
rhs: (HP)
rhs: (Gateway)
rhs: EOS
lhs: (HP)
lhs: EOS
The correct output, lhs-rhs, should be
lhs-rhs: (Dell)
lhs-rhs: EOS
However, this can only be achieved by waiting for the EOS before emitting any output and also
by keeping track (i.e., maintaining state) of both inputs. For example, if only lhs data is retained,
then the rhs instance of (HP) would not be in memory when the lhs instance of (HP) occurred
and this tuple would be incorrectly emitted.
In summary, streaming is a technique that improves the efficiency of operator I/O by
increasing the degree of vertical parallelism that is possible at runtime. By allowing producers to
emit tuples as soon as possible  and by not forcing those to wait for consumers to receive them 
both producers and consumers can work as fast as they are able. The main tradeoff is increased
memory, for the queue required to facilitate streaming and for the state that needs to be
maintained between firings.
5.2.1. RECURSIVE STREAMING: SIMPLICITY + EFFICIENCY
Streaming can complement the simplicity of many types of recursive plans with highly efficient
execution. Looping in theoretical dataflow systems is non-trivial because of the desire for singleassignment and because of the need for synchronization during loop iterations. Streaming further
complicates this: data from different loop iterations can collide, requiring some mechanism to
color the data for each iteration. As a result, looping becomes an even more difficult challenge.
To address this problem, we use a data coloring approach. Each time that data enters a flow, it
is given a session value and an iteration value (initially 0). Upon re-entrancy, the iteration value
is incremented. When leaving a re-entrant module, the iteration value is decremented. If the new
value is equal to 0, the flow is routed out of the recursive module; otherwise, the data flow
continues to unravel until its iteration value is 0. For tail-recursive situations, the system
optimizes this process and simply decrements the iteration value to 0 immediately and exits the
recursive module. The two pronged data-coloring approach, which is similar to strategies used in
dataflow computing literature, maintains the property of single assignment at each the level of the
call graph. Streaming easily fits into this model without any other changes. As a result, many
levels of the call graph can be active in parallel  effectively parallelizing the loop.

649

fiBARISH & KNOBLOCK

To see how this works, we return to the Get_houses example of Figures 11a and 11b. When
the input tuple arrives, the initial page of houses is fetched. When that happens, the Next link
is followed in parallel with the projecting of the house URLs to the Union operator and then to
the Minus operator. Since the Union operator can emit results immediately, and the Minus
operator will have both of its inputs, data flow continues until the next Wrapper operator, which
queries the URL and extracts the details from the house. Thus, the details of the houses from the
first page are queried in parallel with the following of the Next link, if it exists. Data from the
next page is then extracted in parallel with the following of the Next link from this second page
and so on. Meanwhile, the results from the Get_urls subplan (the house URLs) are streamed back
to the first level of the plan, to the Union operator. They continue on through and their details are
gathered in parallel.
In short, recursive streaming is a powerful capability that is made possible by the combination
of the expressivity of the THESEUS plan language and efficient execution system. The result
allows one to write plans that gather, extract, and process data as soon as possible  even when a
logical set of results is distributed over a collection of pages (a common case on the Internet).

6. Experimental Results
To demonstrate the contributions of this paper, we conducted a set of experiments that highlight
the increased expressivity and efficient execution supported by our plan language and execution
system. Our method consists of verifying three hypotheses that are fundamental to our claims:
Hypothesis 1:

The streaming dataflow plan execution system ensures faster plan
execution times than what is possible under von Neumann or nonstreaming dataflow execution.

Hypothesis 2:

The agent plan language described here supports plans that cannot be
represented by the query languages of network query engines.

Hypothesis 3:

The additional expressivity permitted by the plan language described here
does not result in increased plan execution time.

After a brief introduction about the implemented system used in the experiments, the rest of
this section is divided into three subsections, each of which focuses on verifying each of these
hypotheses.
6.1 The THESEUS Information Gathering System
We implemented the approach described in this paper in a system called THESEUS. THESEUS is
written entirely in Java (approximately 15,000 lines of code) and thus runs on any operating
system to which the Java virtual machine (JVM) has been ported. We ran the experiments
described here on an Intel Pentium III 833MHz machine with 256MB RAM, running Windows
2000 Professional Edition, using the JVM and API provided by Sun Microsystems Java Standard
Edition (JSE), version 1.4. This machine was connected to the Internet via a 10Mbps Ethernet
network card.
6.2 Hypothesis 1: Streaming Dataflow Ensures Fast Information Agents
To support our first hypothesis, we measured the efficiency of the Homeseekers information
agent. Our experiments show that without the parallelism features of the plan language and
execution system, agents such as Homeseekers would take significantly longer to execute.
The graphical plan for Homeseekers is the same as shown in Figures 11a and 11b. Note that
this plan does not monitor Homeseekers (we will get to that in the next section), but simply
gathers data from the Web site. The textual plans required for this are simply translations of

650

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

Figures 11a and 11b using the plan language described in this paper. The textual form of the
Get_houses plan is shown in Figure 13a and the textual form of the Get_urls plan is shown in
Figure 13b.
To demonstrate the efficiency that streaming dataflow provides, we ran the Homeseekers
Get_houses plan under three different configurations of THESEUS. The first configuration (D-)
consisted of a thread pool with one thread  effectively preventing true multi-threaded dataflow
execution and also makes streaming irrelevant. The resulting execution is thus very similar to the
case where the plan had been programmed directly (without threads) using a language like Java
or C++. A second THESEUS configuration (D+S-) used multiple threads for dataflow-style
processing, but did not stream data between operators. Finally, the third configuration (D+S+)
consisted of running THESEUS in its normal streaming dataflow mode, enabling both types of
parallelism. For the D+S- and D+S+ cases, the number of threads was set to 15.
Note that the configurations were only done for purposes of running these experiments. In
practice, THESEUS runs in only one configuration: streaming dataflow with n threads in the thread
pool (n is typically set to 10 or 20). Only if one wants to modify the number of threads in the
pool does he need to alter the configuration file. This is rarely necessary.
We ran each configuration three times (interleaved, to negate any temporary benefits of
network or source availability) and averaged the measurements of the three runs. The search
constraints consisted of finding houses in Irvine, CA that are priced between $500,000 and
$550,000. This query returned 72 results (tuples), spread across 12 pages (6 results per page).
Figure 14 shows the average performance results for these three configurations in terms of the
time it took to obtain the first tuple (beginning of output) and the time it took to obtain the last
tuple (end of output). A series of unpaired t-tests on these measurements indicates that they are
PLAN get_houses
{
INPUT: criteria
OUTPUT: filtered-house-details
BODY
{
project (criteria, price-range, price-info)
format (beds = %s, beds : bed-info)
wrapper (initial, price-info : result-page-info)
get_urls (house_urls : all-house-urls)
wrapper (detail, all-house-urls : all-house-details)
select (raw-house-details, bed-info : filtered-house-details)
}
}
Figure 13a: Text of the Get_houses plan
PLAN get_urls
{
INPUT: result-page-info
OUTPUT: combined-urls
BODY
{
project(result-page-info, house-url : curr-urls)
distinct(result-page-info, next-page-link : next-status)
null (next-status, next-status, next-status : next-page-info, next-urls)
wrapper (result-page, next-page-info : next-urls)
union ( curr-urls, next-urls : combined-urls)
}
}
Figure 13b: Text of the Get_urls plan

651

fiBARISH & KNOBLOCK

statistically significant at the 0.05 level.4
The time to first tuple is important because it shows the earliest time that data becomes
available. Callers of an information agent plan are often interested in how early results come
back, especially if a substantial amount of data is returned or the time between tuples is great,
since it allows processing of results to begin as soon as possible. The time to last tuple is also an
important metric because it is associated with the time at which all of the data has been returned.
Callers of a plan that require the entire set of results, such as a caller that executes an aggregate
function on the data, are thus interested in this measurement.
As Figure 14 shows, the parallelism provided by streaming dataflow has a significant impact.
Typical von Neumann style execution, such as that in (D-), cannot not leverage opportunities for
parallelism and suffers heavily from the cumulative effects of I/O delays. While the D+S- fares
better because concurrent I/O requests can be issued in parallel, the inability to stream data
throughout the plan prevents all result pages from being queried in parallel. Also, because of the
lack of streaming, results obtained early during execution (i.e., the first tuple) cannot be
communicated until the last tuple is ready. Note that the D+S- case reflects the performance
provided if the plan had been executed by robot plan execution systems like RAPs or PRS-LITE,
which support operational (horizontal) parallelism but not data (vertical) parallelism.
Finally, the D+S+ case shows that streaming can alleviate both problems, allowing the first
tuple to be output as soon as possible, while supporting the ability to query all result pages in
parallel (and process the detail pages as soon as possible, in parallel). In short, Figure 14 shows
that streaming dataflow is a very efficient execution paradigm for I/O-bound Web-information
gathering plans that require interleaved navigation and gathering.
We also sought to compare the execution performance of the Get_houses plan against the
performance achieved when using another type of information gathering system, such as a
network query engine. However, since these systems do not support the ability to express loops
or recursive information gathering, it was not possible to simply run the same plan in these other
executors. To address this, we calculated the theoretical performance for a network query engine
that supported streaming dataflow, but did not have the ability to loop over result pages.
To solve the type of challenge that sites like Homeseekers pose, these systems would need to
gather data from one result page at a time. Note that while loops or recursion for these systems is
not possible (i.e., not possible to gather data spread across a set of pages in parallel), given the

80000

Time (ms)

70000
60000
50000

D-

40000

D+S-

30000
20000

D+S+

10000
0
First tuple

Last tuple

Figure 14: Performance benefits of streaming dataflow
4

Two-tailed P results for the D+S vs. D+S- and D+S- vs. D- time-to-first-tuple cases were  0.0001 and
0.0024 respectively. Two-tailed P results for the D+S+ vs D+S- and D+S- vs. D- time-to-last tuple cases
were 0.0001 and 0.0026, respectively:

652

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

type of intermediate plan language they support, they can still be used to drill down on the
details of a particular result (i.e., gather data below a set of pages) in parallel. Thus, a network
query engine could leverage its dataflow and streaming capabilities to process a single page, but
could not be used to parallelize the information gathering from a set of linked result pages. Each
page (and its details) would have to be processed one at a time.
To simulate this behavior, we used THESEUS to extract house URLs and the details one page at
time, for each of the twelve pages of results we obtained in our initial query. The average time
required to gather the details of all six housing results was 3204 ms. Note again that the time to
retrieve the first detailed result was the same as in the THESEUS D+S+ case: 1852ms. If we take
the time to extract all six detailed results and multiply it by the number of pages in our query (12),
we get a time of last tuple equal to (3204 * 12) = 38448ms. Figure 15 shows how these results
compare to the D+S+ case of THESEUS.
Thus, while an ad-hoc solution using a network query engine could allow the first tuple of
results to be returned just as fast as in THESEUS the inability for the Next links to be navigated
to immediately would result in less loop parallelism and, as a result, would lead to slower
production of the last tuple of data. Therefore, while network query engines could be used to
gather results spread across multiple hyperlinked web pages, their inability to natively support a
mechanism for looping negates the potential for streaming to further parallelize the looping
process.
In summary, to verify our first hypothesis, we described how the expressivity of the plan
language presented enables more complex queries (like Homeseekers) to be answered efficiently.
These results apply not just to Homeseekers, but to any type of site that reports a result list as a
series of hyperlinked pages.
6.3 Hypothesis 2: Better Plan Language Expressivity
To support our second hypothesis, we investigated how the more complex task of monitoring
Homeseekers could be accomplished using our approach versus existing Web query systems. We
have previously described why monitoring in cases such as this would be useful  searching for a
house is a process that requires weeks, if not months of executing the same kind of query. Thus,
a corresponding information gathering plan would query Homeseekers once per day and send
newly found matches to the end user over e-mail. Again, this type of problem is general  it is
often desirable to be able to monitor many Internet sites that produce lists of results. However, to
do so requires support for plans that are capable of expressing the monitoring task, the persistence
of monitoring data, and the ability to notify users asynchronously.
The plan to monitor Homeseekers is shown in Figure 16. It is the same plan as shown in
Figure 13a, but with a few additional modifications. In particular, it uses two database operators
(DbImport and DbAppend) to integrate a local commercial database system for the persistence of
45000

Time (ms)

40000
35000

Theoretical netw ork
query engine

30000

Theseus D+S+

25000
20000
15000
10000
5000
0

First tuple

Last tuple

Figure 15: Comparison against hypothetical network query engine
653

fiBARISH & KNOBLOCK

DB-IMPORT

DBAPPEND

ho use s-se e n

criteria

PROJECT

WRAPPER

price -inf o

ho use -u rls

GET_URLS

MINUS

WRAPPER

SELECT

raw-hou se -de t ails

co nd

EMAIL

FORMAT
"b e ds = %s"

Figure 16: Modifying Homeseekers to support monitoring requirements

results. This allows future queries to only return new results and stored all past results. Notice
that initial DbImport is triggered by a synchronization variable. The plan also communicates new
results asynchronously to users via an Email operator.
To measure expressivity, we consider a comparison of the plan in Figure 16 with those
capable of being produced by the TELEGRAPH and NIAGARA network query engines. Our
comparison focuses on TELEGRAPHCQ (Chandrasekaran, Cooper, Deshpande, Franklin,
Hellerstein, Hong, Krishnamurthy, Madden, Raman, Reiss, & Shah, 2003) and NIAGARACQ
(Chen, DeWitt, Tian, & Wang, 2000), both of which are modifications of their original systems to
support continuous queries for the monitoring of streaming data sources.
Since the
TELEGRAPHCQ and NIAGARACQ query languages are very similar, we present a detailed
comparison with the former and a general comparison with the latter.
Both CQ systems allow continuous Select-Project-Join (SPJ) queries to be expressed.
TELEGRAPHCQ provides a SQL-like language with extensions for expressing operations on
windows of streaming data. Specifically, the language allows one to express SPJ style queries
over streaming data and also includes support for for loop constructs to allow the frequency of
querying those streams. For example, to treat Homeseekers as a streaming data source and to
query it once per day (for 10 days) for houses in Manhattan Beach, CA, that are less than
$800,000:
Select street_address, num_rooms, price
From Homeseekers
Where price < 800000 and city = Manhattan Beach and state = CA
for (t=ST; t<ST+10; t++) {
WindowIs(Homeseekers, t-1, t)
}

NIAGARACQ also allows more complicated operations, such as Email, to be accomplished by
calling out to a function declared in a stored procedure language. The format of a NIAGARACQ
query is:
CREATE CQ_name
XML-QL query
DO action
{START s_time} {EVERY time_interval} {EXPIRE e_time}

In our example, query would be the XML-QL equivalent of selecting house information for
those that met our query criteria.
The action part would be something similar to
MailTo:user@example.com.
Generally, both query language have the same limitations when it comes to flexible
monitoring of sources, limitations that THESEUS does not have. First, there is no ability to
interleave gathering of data with navigation (in fact, NIAGARACQ assumes that Homeseekers can
be queried as an XML source that provides a single set of XML data). Second, there is no
support for actions (like e-mail) based on differentials of data monitored over some period of
time. Although both allow one to write a stored procedure that could accomplish this action, it

654

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

requires a separate programming task and its execution is not as efficient as the rest of the query
(this could be an issue for more complicated or more intensive CPU or I/O-bound activities per
tuple). Finally, with both CQ systems, there is no way to terminate a query other than by
temporal constraints.
6.4 Hypothesis 3: Increased Expressivity Does Not Increase Execution Time
Though we have demonstrated that THESEUS performs well on more complex information
gathering tasks, it is useful to assess whether the increased expressivity in THESEUS impacts its
performance on simpler tasks  in particular, ones that network query engines typically process.
To do this, we explored the performance of THESEUS on a more traditional, database style query
plan for online information gathering and compared it to the same type of plan executed by a
network query engine.
We chose a single, common type of SPJ query that involved multiple data sources to serve as
the basis for comparison. This is the canonical data integration query. We claim that
understanding how THESEUS compares to a network query engine with respect to the
performance of an SPJ query is at the heart of the efficiency comparison between the two types of
systems. Since both types of systems execute dataflow-style plans in pipelined fashion,
theoretical performance should be the same  the only expected differences would be due to
implementation or environment biases (e.g., different LAN infrastructures). Nevertheless, to
support our efficiency claim, we felt it was important to use a concrete SPJ query for comparison.
For our experiment, we chose to reproduce a query from the paper of another network query
engine  Telegraph. To measure the performance of their partial results query processing
technique, Raman and Hellerstein ran a query that gathered data from three sources and then
joined them together (Raman & Hellerstein, 2002). The specific query involved gathering
information on contributors to the 2000 U.S. Presidential campaign, and then combined this
information with neighborhood demographic information and crime index information. Table 2
lists the sources and the data they provide. Bulk scannable sources are those where the data to
be extracted can be read directly (i.e., exists on a static Web page or file). Index sources are
those that provide answers based on queries via Web forms. Index sources are thus sources
which require binding patterns. Table 3 shows the query that was used to evaluate the
performance of TELEGRAPH.
It is important to note that Raman and Hellerstein measured the performance of the query in
Table 3 under standard pipelined mode and compared this with their JuggleEddy partial results
approach. We are only interested in the results of the former, as this is a measure of how well an
unoptimized network query engine  what we call the baseline  gathers data when
processing a traditional, database-style query. Any further optimization, such as the JuggleEddy,
is complementary to the system described here. Since both types of systems rely on streaming
dataflow execution consisting of tuples routed through iterative-style query operators, it would
not be difficult to extend THESEUS to support this and other types of adaptive query processing
techniques.
Source
FEC

Yahoo
Real
Estate
Crime

Site
www.fec.gov

Type of data
Bulk scannable source that provides information
(including zip code) on each contributor to a candidate
in the 2000 Presidential campaign.
realestsate.yahoo.com Index source that returns neighborhood demographic
information for a particular zip code.
www.apbnews.com

Index source that returns crime level ratings for a
particular zip code.

Table 2: Sources used in the FEC-Yahoo-Crime query

655

fiBARISH & KNOBLOCK

Query
SELECT F.Name, C.Crime, Y.income
FROM FEC as F, Crime as C, Yahoo as Y
WHERE F.zip = Y.zip and F.zip = C.zip
Table 3: SQL query that associates crime and income statistics
with political campaign contributions

We wrote a simple THESEUS plan that allowed the query in Table 3 to be executed. We used
exactly the same sources, except we found that the latency of the Crime source had increased
substantially, as compared to the times recorded by Raman and Hellerstein. Instead, we used
another source (Yahoo Real Estate) but added an artificial delay to each tuple processed by that
source, so that the new source performed similarly. Raman and Hellersteins results show that
the performance of their pipeline plan was as slow as the Crime source, and about 250ms per
tuple. To match this, we added a 150ms delay to each tuple of processing for our new source,
Yahoo, which was normally fetching data at about 100ms per tuple. Our results are shown in
Figure 17.
The results show that THESEUS was not only able to execute the same plan at least as fast as
the baseline TELEGRAPH plan, the non-optimized result shown in Figure 8 of the paper by
Raman and Hellerstein, but THESEUS execution can be more efficient depending on the number
of threads in the thread pool. For example, THESEUS-3 describes the case where the THESEUS
thread pool contains 3 threads. The result from this run performs slightly worse than the
TELEGRAPH baseline  such minor differences could be due to changes in source behavior or in
different proximities to network sources. However, running THESEUS with more threads in the
thread pool (i.e., THESEUS-6 and THESEUS-10) shows much better performance. This is because
the degree of vertical parallelism demanded during execution can be better accommodated with
more threads. It should be noted that the reason TELEGRAPH does not perform as well as
THESEUS-6 and THESEUS-10 is likely because that system only assigned a single thread to each
operator (Raman 2002). That is, THESEUS-6 and THESEUS-10 execution involves 6 and 10
concurrent threads, respectively, whereas the TELEGRAPH plan uses 3 concurrent threads.

7. Related Work

Theseus-10 Theseus-6

200000
180000
160000
140000
120000
100000
80000
60000
40000
20000
0

Telegraph

0
12
00

0
10
00

80
00

60
00

40
00

Theseus-3

20
00

0

Cell updates

The language and system discussed in this paper are relevant to other efforts that focus on agent
execution and the querying of Web data. To understand the work presented here in the context of
these other approaches, we consider past work in software agent execution, robot agent execution,
and network query engines. The first area is most relevant, as software agent systems have

Time (seconds)

Figure 17: Comparing THESEUS to TELEGRAPH baseline (FEC-Yahoo-Crime)
656

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

historically addressed expressivity issues and, in recent years, have also attempted to address
some of the efficiency issues. Robot plan executors represent a slightly greater contrast that have
less experience with processing large amounts of data. On the other hand, network query engines
have explored large-scale remote data processing, though plan/query expressivity tends to be
quite narrow.
7.1 Software Agent Execution Systems
The Internet Softbot (Eztioni & Weld, 1994) is a software agent that automates various
information processing tasks, including UNIX command processing and Web information
gathering. To support execution with incomplete information about the world, the system
interleaves planning with execution. The XII (Golden et al., 1994) and later Puccini (Golden
1998) planners generate partially-ordered plans in which the effects of an action do not have to be
known before execution  but which can be verified during execution. While the Softbot makes a
clear distinction between information goals and satisfaction goals, it does not specifically address
the need to efficiently nor flexibly handle the information it processed. For example, the system
does not support any kind of parallel processing of information (to capitalize on the I/O-bound
nature of execution). In terms of expressivity, while XII and Puccini do allow universal
quantification to be expressed (i.e. iteration is possible), to do so requires that the set of what is
being iterated over be known in advance. As we pointed out in an earlier example on Next Page
links, this is not always the case  the set of next pages to be processed are only discovered by
iterating through all of them in an indeterminate, do..while fashion. In contrast, although it does
not interleave planning and execution, the system described here does support a more expressive
plan language capable of handling next-link type of processing, as well as a streaming dataflow
model of execution that enables efficient large scale information processing. To a great extent,
contributions of both research efforts can be viewed as complementary.
Other research, such as INFOSLEUTH (Bayardo et al., 1997) has recognized the importance of
concurrent task/action execution, close to the spirit of true dataflow computing. At the same
time, such work has generally not investigated the impact of streaming combined with dataflow.
INFOSLEUTH describes a collection of agents that, when combined and working together, present
a cohesive view of data integration across multiple heterogeneous sources. INFOSLEUTH
centralizes execution in its Task Execution Agent, which coordinates high-level information
gathering subtasks necessary to fulfill user queries by routing appropriate queries to resources
that can accommodate those queries. The Task Execution Agent is data-driven and, thus, task
fulfillment proceeds in a dataflow-style manner. In addition, a multi-threading architecture
supports concurrent, asynchronous communication between agents. However, the streaming
component does not exist  in fact, while INFOSLEUTH intends to do large scale information
processing, it specifically notes that limitations to KQML (the basis of its message transport
between agents) were such that streaming was not feasible at the time of implementation. Both
INFOSLEUTH and THESEUS are similar in their desire to support efficient, large-scale information
processing. However, THESEUS supports streaming between operators, as well as a more
expressive plan language, capable of support for more complex types of plans, including support
for conditionals and recursion.
In contrast to INFOSLEUTH, BIG (Lesser, Horling, Klassner, Raja, Wagner, & Zhang, 2000) is
a more general software agent that separates the components of agent planning, scheduling, and
execution (among other components). BIG agents execute plans based on tasks modeled in the
TMS modeling language. During execution, BIG reasons about resource tradeoffs and attempts
to parallelize non-local requests (such as Web requests), at least in terms of how such actions
are scheduled. In terms of expressivity, TMS does not include support for conditionals or
looping constructs (see DECAF, below), unlike the system described in this paper. In terms of
execution, BIG may perform some operations concurrently, but it does not execute in a pure
dataflow manner: instead, it parallelizes only certain operations, based on whether or not they are

657

fiBARISH & KNOBLOCK

blocking. This significantly reduces additional opportunities for dataflow-style parallelism. For
example, it is not possible to parallelize CPU-bound operations (desirable on hyperthreaded
processors or multi-CPU machines) nor is it possible to leverage additional I/O-bound parallelism
from two different instruction flows. As an example of the latter, consider a plan that uses
common input data to query a set of sources, performing different computations on the input data
(e.g., to form it into a query) before each remote request. Since only I/O-bound operations are
parallelized, there is no way to execute both flows simultaneously, even though both flows
eventually end up I/O-bound A second but larger difference between BIG and THESEUS is that
the latter supports the capability to stream data between operators, maximizing the degree of
vertical parallelism possible, while the former does not. As we have shown, better vertical
parallelism during execution can yield significant performance gains.
RETSINA (Sycara et al., 2003) is a more general, multi-agent system that attempts to automate
a wide range of tasks, including information processing. RETSINA is unique because it attempts
to interleave not only planning and execution (as did XII in the Internet Softbot), but also
information gathering. Each RETSINA agent is composed of four modules: communication,
planning, scheduling, and execution monitoring. As these modules run as separate threads,
communication, planning and scheduling can occur during information gathering (which is often
I/O-bound). In addition, multiple actions in RETSINA can be executed concurrently, in a
dataflow-style manner, through separate threads. During execution, actions communicate
information between one another via provision/outcome links (Williamson, Decker, & Sycara,
1996), which are similar to the notion of operator input and output variables we have described
here. While the dataflow aspect of agent execution in RETSINA is similar to that in THESEUS, its
plan language is less expressive (no support for conditionals or any kind of looping, including
indeterminate) and no execution support for streaming.
DECAF (Graham et al., 2003) is an extension of both the RETSINA and the TMS task language
to support agent plans that contain if-then-else and looping constructs. In addition, DECAF
incorporates a more advanced notion of task scheduling and views its mode of operation as more
analogous to that of an operating system  for example, during execution, it is concerned with the
number of I/O-bound and CPU-bound tasks at any one time, so as to optimize task scheduling.
While DECAF employs a more expressive task language, closer to what is supported by THESEUS,
there is no support for streaming during execution. Again, as we have shown, the benefits of
increased vertical parallelism through streaming can make a significant difference when
processing large amounts of data or when working with slow, remote sources, a case that is
common with environments like the Web. In fact, we have shown that going beyond the dataflow
limit (maximum vertical and horizontal parallelism) though techniques such as speculative
execution (Barish & Knoblock, 2002; Barish & Knoblock, 2003) can yield even greater
performance benefits. Streaming is not a simple feature to add to an execution system; the way
operators execute must change (i.e., they become iterators), end-of-stream ordering must be
managed with care, support for operator state management is needed, in addition to other related
challenges.
7.2 Robot Agent Execution Systems
Our work on THESEUS is also related to past work on robot agent execution systems. The main
similarity is the emphasis on providing both a plan language and execution system for agents.
The main difference, however, is that robot agent execution systems are built primarily for robots,
which act in the physical world, and lack support for some of the critical features that software
agents like Web information agents require. In discussing specifics, we focus on two well-known
robot agent executors: the RAP system (Firby 1994) and PRS-LITE (Myers 1996).
Both RAP and PRS-LITE offer general plan languages and execution systems that support
concurrent execution of actions. Like other expressive plan languages, such as RPL (McDermott
1991), both RAP and PRS-LITE also support additional action synchronization through the WAIT-

658

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

FOR clause, which triggers an action after a particular signal has been received. This is similar to
the use of WAIT and ENABLE in the THESEUS plan language. PRS-LITE supports even greater
expressivity, including the notion of sequencing goals, which enable conditional goals as well as
parallel or sequential goal execution. For example, PRS-LITE supports the SPLIT and AND
modalities as two different ways to specify parallel goal execution, the former decoupled from the
parent task while the latter is more tightly coupled.
Despite the expressivity supported by RAPs and PRS-LITE, it is clear that their plan languages
are primarily meant to handle the needs of robots. For example, operator execution involves the
processing of signals, not streams of tuples, between operators. In contrast, both the THESEUS
language and the executor are built to stream potentially large amounts of relational data. If a
plan like Homeseekers was executed on RAPs or PRS-LITE, the lack of streaming would result in
significantly worse performance and make poor use of available resources. This is not to say that
RAPs nor PRS-LITE contain design flaws: rather, these systems simply better facilitate the needs
of robots  which process small amounts of local data (such as target presence or location
information) and perform actions in the physical world. In contrast, Web information agents do
not act on physical objects, but software objects, such as Web sites, and need to deal with the
problems associated with the unreliable remote I/O of potentially large amounts of data.
Streaming is thus a critical feature for these agents, as it allows for much faster performance and
for local resources, such as the CPU, to be better utilized.
Another significant difference between the language presented here and those of RAPs and
PRS-LITE is the support for recursion. It is understandable that robot agent execution systems
lack this feature because none of their primary tasks require such control flow. In fact, neither
PRS-LITE nor RAP supports any kind of looping mechanism. In contrast, looping is often required
for Web information agents, which frequently need to gather a logical set of data distributed
across an indeterminate number of pages connected through Next page links. Recursive
streaming enables high-performance looping in a dataflow environment without any kind of
complicated synchronization.
It cannot be understated that features like streaming and recursion make a significant
difference in terms of agent performance. For example, execution of Homeseekers without
recursive streaming would fare no better than the D+S- example in Section 6, which performed
much worse than the D+S+ case.
7.3 Network Query Engines
Network query engines such as TUKWILA (Ives et al., 1999), TELEGRAPH (Hellerstein et al.,
2000) and NIAGARA (Naughton et al., 2001) have focused primarily on efficient and adaptive
execution (Avnur & Hellerstein 2000; Ives et al., 2000; Shanmugasundaram et al., 2000; Raman
& Hellerstein 2002), the processing of XML data (Ives et al., 2001), and continuous queries
(Chen et al., 2000; Chandrasekaran et al., 2003). All of these systems take queries from users,
form query plans, and execute those plans on a set of remote data sources or incoming streams.
As with THESEUS, network query engines rely on streaming dataflow for the efficient, parallel
processing of remote data.
The work described here differs from network query engines in two ways. The first, and most
important difference, has to do with the plan language. Plans in network query engines consist
mostly of relational-style operators and those required to do additional adaptive or XML-style
processing. For example, TUKWILA includes a double pipelined hash join and dynamic collector
operators for adaptive execution (Ives et al., 1999), and x-scan and web-join operators to facilitate
the streaming of XML data as binding tuples. TELEGRAPH contains the Eddy operator (Avnur &
Hellerstein 2000) for dynamic tuple routing and the SteMs operator to leverage the benefits of
competing sources and access methods. NIAGARA contains the Nest operator for XML
processing and other operators for managing partial results (Shanmugasundaram et al., 2000).
Outside of these special operators for adaptive execution and XML processing, plans in network

659

fiBARISH & KNOBLOCK

query engines look very similar to database style query plans. These plans are also inaccessible 
users can only alter the queries that generate plans, not the plans themselves.
In contrast, the plan language presented here is more expressive and agent plans are
accessible. Like network query engines, the language we have described includes relational-style
operators and those for processing XML data. However, it also includes operators that support
conditional execution, interaction with local databases, asynchronous notification, and userdefined single-row and aggregate functions. The plan language we developed also supports
subplans for modularity, re-use, and recursive execution for looping-style information gathering.
In contrast, network query engines do not support these kinds of constructs. As a result, these
systems cannot represent the interleaved navigation and gathering required by tasks such as the
Homeseekers example. Consider the Telegraph approach for handling Next Page links. The
logic for iterating over a set of Next Page type links is located in the wrapper itself, separate from
the query plan5. While this simplifies the wrappers somewhat (each wrapper returns all of the
data for a particular site), it limits the flexibility of describing how to gather remote data. For
example, if one develops a Google wrapper in Telegraph that gathers results from a search (over
several pages), there is no easy way to express the requirement stop after 10 pages or stop
when more than 5 links from the same site are extracted. In short, since the logic for dealing
with the Next Page type links has been decoupled from the plan, expressivity is limited. In
addition, to build a wrapper that handles Next Page links in Telegraph, one must write a custom
Java class that is referenced by the engine at runtime. In contrast, the THESEUS language can
handle interleaved navigation and gathering using recursion to loop over the set of Next Page
links, while streaming tuples back to the system as they are extracted, for immediate postprocessing or for conditional checks (i.e., to know when to stop gathering results).
A final difference worth noting has to do with accessibility. In contrast to network query
engines, plans in the language we have described are accessible to the user. Although they can be
generated by query processors (Thakkar et al., 2003) and other types of applications (Tuchinda &
Knoblock, 2004), just like plans produced by network query engines, they can also be constructed
and modified using a text editor. This provides the ability for users to specify more complicated
plans that could not otherwise be represented as a query. While some network query engines,
such as NIAGARACQ (Chen et al., 2000) support some means for specifying more complicated
types of actions to be associated with continuous queries, this support is not native to the system
and thus it is not possible to execute complex actions in the middle of queries (such actions need
to occur at certain times, for example when certain events occur). For example, NIAGARACQ
requires one to specify actions in a stored procedure language, introducing a barrier (query plan to
stored procedure) that does not exist in our system. Furthermore, this logic is separate from the
query plan (i.e., not integrated with other query plan operators) and does not execute until some
condition is met.

8. Conclusion and Future Work
Software agents have the potential to automate many types of tedious and time-consuming tasks
that involve interactions with one or more software systems. To do so, however, requires that
agent systems support plans expressive enough to capture the complexity of these tasks, while at
the same time execute these plans efficiently. What is needed is a way to marry the generality of
existing software agent and robot agent execution systems with the efficiency of network query
engines.
In this paper, we have presented an expressive plan language and efficient approach to
execution that addresses these needs. We have implemented these ideas in THESEUS and applied
5

See the Advanced TESS Wrapper Writing section of the TESS manual,
http://telegraph.cs.berkeley.edu/tess/advanced.html

660

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

the system to automate many types of Web information processing tasks. The Web is a
compelling domain because it is a medium which demands both agent flexibility and efficiency.
While existing software agent and robot agent plan execution systems can support complex plans
consisting of many different types of operators, such systems are not designed to process
information as efficiently as technologies developed in the database research communities. In
this paper, we have presented a plan language and execution system that combines key aspects of
both agent execution systems and state-of-the-art query engines, so that software agents can
efficiently accomplish complex tasks. The plan language we have described makes it possible to
build agents that accomplish more complex tasks than those supported by the network query
engines. Agents written using this language can then be executed as efficiently as the state-of-theart network query engines and more efficiently than the existing agent execution systems. Beyond
the work here, we have also proposed and are continuing to work on a method for speculative
execution for information gathering plans (Barish & Knoblock 2002). The technique leverages
machine learning techniques to analyze data produced early during execution so that accurate
predictions can be made about data that will be needed later in execution (Barish & Knoblock
2003). The result is a new form of dynamic runtime parallelism that can lead to significant
speedups, beyond what the dataflow limit allows.
We are also currently working on an Agent Wizard (Tuchinda & Knoblock, 2004), which
allows the user to define agents for monitoring tasks simply by answering a set of questions about
the task. The Wizard works similar to the Microsoft Excel Chart Wizard, which builds
sophisticated charts by asking the user a set of simple questions. The Wizard will generate
information gathering plans using the language described in this paper and schedule them for
periodic execution.

Acknowledgements
This research is based upon work supported in part by the National Science Foundation under
Award No. IIS-0324955, in part by the Defense Advanced Research Projects Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.
NBCHD030010, in part by the Air Force Office of Scientific Research under grant numbers
F49620-01-1-0053 and FA9550-04-1-0105, in part by the United States Air Force under contract
number F49620-02-C-0103, in part by a gift from the Intel Corporation, and in part by a gift from
the Microsoft Corporation.
The U.S. Government is authorized to reproduce and distribute reports for Governmental
purposes notwithstanding any copyright annotation thereon. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or implied, of any of the above
organizations or any person connected with them.

661

fiBARISH & KNOBLOCK

References
Abiteboul, S., Hull, R. S., & Vianu, V. (1995). Foundations of Databases, Addison-Wesley.
Ambite, J.-L, Barish, G., Knoblock, C. A., Muslea, M., Oh, J. & Minton, S. (2002). Getting from
Here to There: Interactive Planning and Agent Execution for Optimizing Travel. Proceedings of
the 14th Innovative Applications of Artificial Intelligence (IAAI-2002). Edmonton, Alberta,
Canada.
Arens, Y, Knoblock, C. A., & Shen, W-M. (1996). "Query Reformulation for Dynamic
Information Integration." Journal of Intelligent Information Systems - Special Issue on Intelligent
Information Integration 6(2/3): 99-130.
Arvind, Gostelow, K. P., & Plouffe, W. (1978). The Id Report: An Asynchronous Programming
Language and Computing Machine, University of California, 114.
Arvind & Nikhil. R. S. (1990). "Executing a Program on the MIT Tagged-Token Dataflow
Architecture." IEEE Transactions on Computers 39(3): 300-318.
Avnur, R. & Hellerstein, J. M. (2000). Eddies: Continuously Adaptive Query Processing.
Proceedings of the ACM SIGMOD International Conference on Management of Data. Dallas,
TX: 261-272.
Barish, G. & Knoblock, C. A. (2002). Speculative Execution for Information Gathering Plans.
Proceedings of the Sixth International Conference on AI Planning and Scheduling (AIPS 2002).
Tolouse, France: 184-193
Barish, G. & Knoblock, C. A. (2003). Learning Value Predictors for the Speculative Execution of
Information Gathering Plans. Proceedings of the 18th International Joint Conference on Artificial
Intelligence (IJCAI 2003). Acapulco, Mexico: 1-8.
Boag, S., Chamberlin, D., Fernandez, M. F., Florescu, D., Robie, J., & Simeon, J. (2002).
XQuery 1.0: An XML Query Language. World Wide Web Consortium, http://www.w3.org.
Bayardo Jr., R. J., Bohrer, W., Brice, R. S., Cichocki, A., Fowler, J., Helal, A., Kashyap, V.,
Ksiezyk, T., Martin, G., Nodine, M., Rashid, M., Rusinkiewicz, M., Shea, R., Unnikrishnan, C.,
Unruh, A., & Woelk, D. (1997). InfoSleuth: Semantic Integration of Information in Open and
Dynamic Environments. Proceedings of the ACM SIGMOD International Conference on
Management of Data (SIGMOD 1997), Tucson, AZ: 195-206
Chalupsky, H., Gil, Y., Knoblock, C. A., Lerman, K., Oh, J., Pynadath, D., Russ, T. A., &
Tambe, M. (2001). Electric Elves: Applying Agent Technology to Support Human Organizations.
Proceedings of the 13th Innovative Applications of Artificial Intelligence (IAAI-2001). Seattle,
WA.
Chandrasekaran, S., Cooper, O., Deshpande, A., Franklin, M. J., Hellerstein, J. M., Hong, W.,
Krishnamurthy, S., Madden, S., Raman, V., Reiss, F., & Shah, M.A. (2003). TelegraphCQ:
Continuous Dataflow Processing for an Uncertain World. Proceedings of the First Biennial
Conference on Innovative Data Systems Research. Monterey, CA.

662

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

Chawathe, S., Garcia-Molina, H., Hammer, J., Ireland, K., Papakonstantinou, Y., Ullman, J., &
Widom, J. (1994). The Tsimmis Project: Integration of Heterogenous Information Sources.
Proceedings of the 16th Meeting of the Information Processing Society of Japan. Tokyo, Japan:
7-18.
Chen, J., DeWitt, D. J., Tian, F., & Wang, Y. (2000). NiagaraCQ: A Scalable Continuous Query
System for Internet Databases. Proceedings of the ACM SIGMOD International Conference on
Management of Data. Dallas, TX: 379-390.
Decker, K., Sycara, K., & Zeng, D. (1996). Designing a multi-agent portfolio management
system. In Proceedings of the AAAI Workshop on Internet Information Systems.
Dennis, J. B. (1974). "First Version of a Data Flow Procedure Language." Lecture Notes in
Computer Science 19: 362-376.
DeWitt, D. & Gray, J. (1992). "Parallel Database Systems: The Future of High Performance
Database Systems." Communications of the ACM 35(6): 85-98.
Doorenbos, R. B., Etzioni, O., & Weld, D.S. (1997). A Scalable Comparison-Shopping Agent for
the World-Wide Web. In Proceedings of the First International Conference on Autonomous
Agents, Marina del Rey, CA: 39-48.
Duschka, O.M. (1997). Query Planning and Optimization in Information Integration. Ph.D.
Thesis, Stanford University, Computer Science Technical Report STAN-CS-TR-97-1598.
Etzioni, O. & Weld, D. S. (1994) "A softbot-based interface to the internet". Communications of
the ACM, 37(7):72-76.
Etzioni, O., Tuchinda, R., Knoblock, C.A., & Yates, A. (2003). To buy or not to buy: mining
airfare data to minimize ticket purchase price. Proceedings of the Ninth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, 119-128.
Evripidou, P. & Gaudiot, J. L. (1991). Input/Output Operations for Hybrid Data-Flow/ControlFlow Systems. The Fifth International Parallel Processing Symposium. Anaheim, California:
318-323.
Firby, R. J. (1994). Task Networks for Controlling Continuous Processes. Proceedings of the 2nd
International Conference on Artificial Intelligence Planning Systems. Chicago, IL: 49-54.
Friedman, M., Levy A. Y., & Millstein, T. D. (1999). Navigational Plans for Data Integration.
Proceedings of the 16th National Conference on Artificial Intelligence (AAAI-1999). Orlando, FL:
67-73.
Gao, G. R. (1993). "An Efficient Hybrid Dataflow Architecture Model." International Journal of
Parallel and Distributed Computing 19(4): 293-307.
Genesereth, M. R., Keller, A. M., & Duschka, O. M. (1997). Infomaster: An Information
Integration System. Proceedings of the ACM SIGMOD International Conference on Management
of Data (SIGMOD 1997).. Tucson, AZ: 539-542.

663

fiBARISH & KNOBLOCK

Golden, K., Etzioni, O., & Weld, D. S. (1994). Omnipotence Without Omniscience: Efficient
Sensor Management for Planning. Proceedings of the 12th National Conference on Artificial
Intelligence (AAAI-1994). Seattle, WA: 10481054.
Golden, K. (1998). Leap Before You Look: Information Gathering in the PUCCINI Planner.
Proceedings of the 4th International Conference on Planning and Scheduling (AIPS 1998). 70-77
Graham, J. R.., Decker, K., & Mersic M. (2003). "DECAF - A Flexible Multi Agent System
Architecture." Autonomous Agents and Multi-Agent Systems 7(1-2): 7-27. Kluwer Publishers.
Graefe, G. (1993). "Query evaluation techniques for large databases." ACM Computing Surveys
25(2): 73-169.
Gurd, J. R. & Snelling, D. F. (1992). Manchester Data-Flow: A Progress Report. Proceedings of
the 6th International Conference on Supercomputing. Washington, D.C., United States, ACM
Press: 216-225.
Hellerstein, J. M., Franklin, M. J., Chandrasekaran, S., Deshpande, A., Hildrum, K., Madden, S.,
Raman, V., & Shah, M. A. (2000). "Adaptive Query Processing: Technology in Evolution." IEEE
Data Engineering Bulletin 23(2): 7-18.
Hoare, C. A. R. (1978). "Communicating Sequential Processes." Communications of the ACM
21(8): 666-677.
Iannucci, R. A. (1988). Toward a Dataflow/Von Neumann Hybrid Architecture. The 15th Annual
International Symposium on Computer Architecture. Honolulu, Hawaii, IEEE Computer Society
Press: 131-140.
Ives, Z. G., Florescu, D., Friedman, M., Levy, A., & Weld D. S. (1999). An Adaptive Query
Execution System for Data Integration. Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD 1999). Philadelphia, PA: 299-310.
Ives, Z. G., Halevy, A. Y., & Weld, D. S. (2001). "Integrating Network-Bound Xml Data." IEEE
Data Engineering Bulletin 24(2): 20-26.
Ives, Z. G., Levy, A. Y., Weld, D. S., Florescu, D., & Friedman, M. (2000). "Adaptive Query
Processing for Internet Applications." IEEE Data Engineering Bulletin 23(2): 19-26.
Ives, Z. G., Levy, A. Y., & Weld, D. S. (2002). An XML Query Engine for Network-bound
Data. VLDB Journal 11(4): 380-402
Kahn, G. (1974). "The Semantics of a Simple Language for Parallel Programming." Information
Processing Letters 74: 471-475.
Karp, R. M. & Miller, R. E. (1955). "Properties of a Model for Parallel Computations:
Determinancy, Termination, Queuing." SIAM Journal on Applied Mathematics 14: 1390-1411.
Knoblock, C. A., Lerman, K., Minton, S., & Muslea, I. (2000). "Accurately and Reliably
Extracting Data from the Web: A Machine Learning Approach." IEEE Data Engineering Bulletin
23(4): 33-41.

664

fiAN EXPRESSIVE LANGUAGE AND EFFICIENT EXECUTION SYSTEM FOR SOFTWARE AGENTS

Knoblock, C. A., Minton, S., Ambite, J.-L., Ashish, N., Muslea, I., Philpot, A., & Tejada, S.
(2001). "The Ariadne Approach to Web-Based Information Integration." International Journal of
Cooperative Information Systems 10(1-2): 145-169.
Kushmerick, N. (2000). "Wrapper Induction: Efficiency and Expressiveness." Artificial
Intelligence 118(1-2): 15-68.
Lesser, V., Horling, B., Klassner, F., Raja, A., Wagner, T., & Zhang, S. (2000). "BIG: An Agent
for Resource-Bounded Information Gathering and Decision Making." In Artificial Intelligence
Journal, Special Issue on Internet Information Agents. 118(1-2): 197-244.
Levy, A. Y., Rajaraman, A., & Ordille, J. J. (1996). Querying Heterogeneous Information
Sources Using Source Descriptions. Proceedings of the Twenty-Second International Conference
on Very Large Databases. Bombay, India: 251-262.
Lo, J. L., Barroso, L. A., Eggers, S. J., Gharachorloo, K., Levy, H. M., & Parekh, S. S. (1998).
An Analysis of Database Workload Performance on Simultaneous Multithreaded Processors.
Proceedings of the 25th Annual International Symposium on Computer Architecture. Barcelona,
Spain, IEEE Press: 39-50.
McDermott, D. (1991). A Reactive Plan Language, Yale University, CSD-RR-864
Myers, K. L. (1996). A Procedural Knowledge Approach to Task-Level Control. Proceedings of
the 3rd International Conference on Ai Planning and Scheduling. Edinburgh, UK: 158-165.
Naughton, J. F., DeWitt, D. J., Maier, D., Aboulnaga, A., Chen, J., Galanis, L., Kang, J.,
Krishnamurthy, R., Luo, Q., Prakash, N., Ramamurthy, R., Shanmugasundaram, J., Tian, F.,
Tufte, K., Viglas, S., Wang, Y., Zhang, C., Jackson, B., Gupta, A., & Che, R. (2001). "The
NIAGARA Internet Query System." IEEE Data Engineering Bulletin 24(2): 27-33.
Nikhil, R. S. & Arvind (2001). Implicit Parallel Programming in pH, Morgan Kaufmann
Publishers Inc.
Papadopoulos, G. M. & Traub, K. R. (1991). Multithreading: A Revisionist View of Dataflow
Architectures. Proceedings of the 18th International Symposium on Computer Architecture (ISCA
1991). New York, NY: 342-351.
Papdopoulos, G. M. & Culler, D. E. (1990). Monsoon: An Explicit Token Store Architecture.
Proceedings of the 17th International Symposium on Computer Architecture. Seattle,
Washington.
Pottinger, R., & Halevy, A. Y. (2001). MiniCon: A Scalable Algorithm for Answering Queries
Using Views. VLDB Journal 10(2-3): 182-198
Raman, V. (2002). Personal Communication.
Raman, V. & Hellerstein, J.. M. (2002). Partial Results for Online Query Processing. Proceedings
of the ACM Sigmod International Conference on Management of Data. Madison, Wisconsin,
ACM Press: 275-286.

665

fiBARISH & KNOBLOCK

Redstone, J. A., Eggers, S. J., & Levy, H. M. (2000). An Analysis of Operating System Behavior
on a Simultaneous Multithreaded Architecture. Proceedings of the Ninth International
Conference on Architectural Support for Programming Languages and Operating Systems.
Cambridge, Massachusetts, ACM Press: 245-256.
Schek, H. J., & Scholl, M. H., (1986). "The Relational Model with Relation-Valued Attributes."
Information Systems 11(2): 137-147.
Shah, M. A., Madden, S., Franklin, M. J., & Hellerstein, J. M. (2001). "Java Support for DataIntensive Systems: Experiences Building the Telegraph Dataflow System." SIGMOD Record
30(4): 103-114.
Shanmugasundaram, J., Tufte, K., DeWitt, D. J., Naughton, J. F., & Maier, D. (2000).
Architecting a Network Query Engine for Producing Partial Results. Proceedings of the ACM
SIGMOD 3rd International Workshop on Web and Databases (WebDB 2000). Dallas, TX: 17-22.
Sycara, K., Paolucci, M., van Velsen, M. & Giampapa, J. (2003). "The RETSINA MAS
Infrastructure. Autonomous Agents and Multi-Agent Systems."
7(1-2): 29-48. Kluwer
Publishers.
Thakkar, S. & Knoblock, C. A. (2003). Efficient execution of recursive data integration plans.
Workshop on Information Integration on the Web, The 18th International Joint Conference on
Artificial Intelligence (IJCAI 2003). Acapulco, Mexico.
Thakkar, S., Knoblock, C. A., & Ambite, J.-L. (2003). A view integration approach to dynamic
composition of web services. Workshop on Planning for Web Services, The 13th International
Conference on Automated Planning & Scheduling (ICAPS 2003), Trento, Italy.
Tuchinda, R. & Knoblock, C. A. (2004). Agent wizard: building agents by answering questions.
Proceedings of the 2004 International Conference on Intelligent User Interfaces, Funchal,
Madeira, Portugal: 340-342.
Tullsen, D. M., Eggers, S., & Levy, H. M. (1995). Simultaneous Multithreading: Maximizing onChip Parallelism. Proceedings of the 22nd Annual ACM International Symposium on Computer
Architecture. Santa Magherita Ligure, Italy: 392-403.
Wiederhold, G. (1996). "Intelligent Integration of Information." Journal of Intelligent
Information Systems 6(2): 281-291.
Williamson, M., Decker, K., & Sycara, K. (1996). Unified Information and Control Flow in
Hierarchical Task Networks. Theories of Action, Planning, and Robot Control: Bridging the Gap:
Proceedings of the 1996 AAAI Workshop. Menlo Park, California, AAAI Press: 142-150.
Wilschut, A. N. & Apers, P. M. G. (1993). "Dataflow Query Execution in a Parallel MainMemory Environment." Distributed and Parallel Databases 1(1): 103-128.

666

fiJournal of Artificial Intelligence Research 23 (2005) 167-243

Submitted 07/04; published 02/05

Combining Spatial and Temporal Logics:
Expressiveness vs. Complexity
David Gabelaia
Roman Kontchakov
Agi Kurucz

gabelaia@dcs.kcl.ac.uk
romanvk@dcs.kcl.ac.uk
kuag@dcs.kcl.ac.uk

Department of Computer Science, Kings College London
Strand, London WC2R 2LS, U.K.

Frank Wolter

frank@csc.liv.ac.uk

Department of Computer Science, University of Liverpool
Liverpool L69 7ZF, U.K.

Michael Zakharyaschev

mz@dcs.kcl.ac.uk

Department of Computer Science, Kings College London
Strand, London WC2R 2LS, U.K.

Abstract
In this paper, we construct and investigate a hierarchy of spatio-temporal formalisms
that result from various combinations of propositional spatial and temporal logics such as
the propositional temporal logic PT L, the spatial logics RCC-8, BRCC-8, S4u and their
fragments. The obtained results give a clear picture of the trade-off between expressiveness
and computational realisability within the hierarchy. We demonstrate how different combining principles as well as spatial and temporal primitives can produce NP-, PSPACE-,
EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out of
components that are at most NP- or PSPACE-complete.

1. Introduction
Qualitative representation and reasoning has been quite successful in dealing with both
time and space. There exists a wide spectrum of temporal logics (see, e.g., Allen, 1983;
Clarke & Emerson, 1981; Manna & Pnueli, 1992; Gabbay, Hodkinson, & Reynolds, 1994;
van Benthem, 1995). There is a variety of spatial formalisms (e.g., Clarke, 1981; Egenhofer
& Franzosa, 1991; Randell, Cui, & Cohn, 1992; Asher & Vieu, 1995; Lemon & Pratt,
1998). In both cases determining the computational complexity of the respective reasoning
problems has been one of the most important research issues. For example, Renz and Nebel
(1999) analysed the complexity of RCC-8, a fragment of the region connection calculus RCC
with eight jointly exhaustive and pairwise disjoint base relations between spatial regions
introduced by Egenhofer and Franzosa (1991) and Randell and his colleagues (1992); Nebel
and Burckert (1995) investigated the complexity of Allens interval algebra; numerous results
on the computational complexity of the point-based propositional linear temporal logic PT L
over various flows of time were obtained by Sistla and Clarke (1985) and Reynolds (2003,
2004). In many cases these investigations resulted in the development and implementation
of effective reasoning algorithms (see, e.g., Wolper, 1985; Smith & Park, 1992; Egenhofer
& Sharma, 1993; Schwendimann, 1998; Fisher, Dixon, & Peim, 2001; Renz & Nebel, 2001;
Hustadt & Konev, 2003).
c
2005
AI Access Foundation. All rights reserved.

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

space

.

X

X

X

X

X

.
F
-

0

1

2

3

time

Figure 1: Topological temporal model.
The next apparent and natural step is to combine these two kinds of reasoning. Of
course, there have been attempts to construct spatio-temporal hybrids. For example, the
intended interpretation of Clarkes (1981, 1985) region-based calculus was spatio-temporal.
Region connection calculus RCC (Randell et al., 1992) contained a function space(X, t)
for representing the space occupied by object X at moment of time t. Muller (1998a)
developed a first-order theory for reasoning about motion of spatial entities. However, all
of these formalisms turn out to be too expressive from the computational point of view:
they are undecidable. Moreover, as far as we know, no serious attempts to investigate and
implement partial (say, incomplete) algorithms capable of spatio-temporal reasoning with
these logics have been made.
The problem of constructing spatio-temporal logics with better algorithmic properties and analysing their computational complexity was first attacked by Wolter and Zakharyaschev (2000b); see also the popular and extended version (Wolter & Zakharyaschev,
2002) of that conference paper, as well as (Bennett & Cohn, 1999; Bennett, Cohn, Wolter,
& Zakharyschev, 2002; Gerevini & Nebel, 2002).
The main idea underlying all these papers is to consider various combinations of wellbehaved spatial and temporal logics. The intended spatio-temporal structures can be
regarded then as the Cartesian products of the intended time-line and topological (or some
other) spaces that are used to model the spatial dimension. Figure 1 shows such a product
(of the flow of time F = hN, <i and the two-dimensional Euclidean space T) with a moving
spatial object X. The moving object can be viewed either as a 3D spatio-temporal entity
(in this particular case) or as the collection of the snapshots or slices of this entity at each
moment of time; for a discussion see, e.g., (Muller, 1998b) and references therein. In this
paper, we use the snapshot terminology and understand by a moving spatial object (or,
more precisely, interpret such an object as) any set of pairs hX, ti where, for each point t
168

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

of the flow of time, X is a subset of the topological spacethe state of the spatial object at
moment t.
The expressive power (and consequently the computational complexity) of the combined
spatio-temporal formalisms obviously depends on three parameters:
1. the expressivity of the spatial component,
2. the expressivity of the temporal component, and
3. the interaction between the two components allowed in the combined logic.
Regardless of the chosen component languages, the minimal requirement for a spatiotemporal combination to be useful is its ability to
express changes in time of the truth-values of purely spatial propositions.

(PC)

Typical examples of logics meeting this spatial propositions truth change principle are the
combinations of RCC-8 and Allens interval calculus (Bennett et al., 2002; Gerevini & Nebel,
2002) and those combinations of RCC-8 and PT L introduced by Wolter and Zakharyaschev
(2000b) that allow applications of temporal operators to Boolean combinations of RCC-8 relations. Languages satisfying (PC) can capture, for instance, some aspects of the continuity
of change principle (see, e.g., Cohn, 1997) such as
(A) if two images on the computer screen are disconnected now, then they either remain
disconnected or become externally connected in one quantum of the computers time.
Another example is the following statement about the geography of Europe:
(B) Kaliningrad is disconnected from the EU until the moment when Poland becomes
a tangential proper part of the EU, after which Kaliningrad and the EU will be
externally connected forever.
However, languages meeting (PC) do not necessarily satisfy our second fundamental
spatial object change principle according to which we should be able to
express changes or evolutions of spatial objects in time.

(OC)

In logical terms, (PC) refers to the change of truth-values of propositions, while (OC) to
the change of extensions of predicates; see Fig. 2 where X at moment t denotes the state
of X at moment t + 1. Here are some examples motivating (OC):
(C) Continuity of change: the cyclones current position overlaps its position in an hour.
(D) Two physical objects cannot occupy the same space: if tomorrow object X is at the
place where object Y is today, then Y will have to move by tomorrow.
(E) Geographic regions change: the space occupied by Europe never changes.
(F) Geographic regions change: in two years the EU will be extended with Romania and
Bulgaria.
(G) Fairness conditions on regions: it will be raining over every part of England ever and
ever again.
169

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

space

.

X

X
X

X

X
T

T

T
-F

.
t

t+1

t+2

time

Figure 2: Temporal operators on regions.
(H) Mutual exclusion: if Earth consists of water and land, and the space occupied by
water expands, then the space occupied by land shrinks.
It should be clear that to represent these statements we have to refer to the evolution of
spatial objects in time (say, to compare objects X and X)it is not enough to only take
into account the change of the truth-values of propositions speaking about spatial objects.
The main aim of this paper is to investigate the trade-off between the expressive power
and the computational behaviour of spatio-temporal hybrids satisfying the (PC) and (OC)
principles and interpreted in various spatio-temporal structures. Our purpose is to show
what computational obstacles one can expect if the application domain requires this or that
kind of interactions between temporal and spatial operators.
The spatio-temporal logics we consider below are combinations of fragments of PT L
interpreted over different flows of time with fragments of the propositional spatial logic S4u
(equipped with the interior and closure operators, the universal and existential quantifiers
over points in space as well as the Booleans) interpreted in topological spaces. This choice
is motivated by the following reasons:
 The component logics are well understood and established in temporal and spatial
knowledge representation; all of them are supported by reasonably effective reasoning
procedures.
 By definition, implicit or explicit temporal quantification is necessary to capture (OC),
and fragments of PT L are the weakest languages with such quantification we know of.
170

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Allens interval calculus, for example, does not provide means for any quantification
over intervals. It is certainly suitable for spatio-temporal hybrids satisfying (PC) (see
Bennett et al., 2002; Gerevini & Nebel, 2002) but there is no natural conservative
way of combining it with spatial formalisms to meet (OC). On the other hand, it is
embedded in PT L (Blackburn, 1992). A natural alternative to PT L would be the
extension of Allens calculus by means of quantification over intervals introduced by
Halpern and Shoham (1986), but unfortunately this temporal logic turns out to be
highly undecidable.
 Although the logic S4u was originally introduced in the realm of modal logic (see
below for details), the work of Bennett (1994), Nutt (1999), Renz (2002) and Wolter
and Zakharyaschev (2000a) showed that it can be regarded as a unifying language
that contains many spatial formalisms like RCC-8, BRCC-8 or the 9-intersections of
Egenhofer and Herring (1991) as fragments.
Apart from the choice of component languages and the level of their interaction, the expressive power and the computational complexity of spatio-temporal logics strongly depend
on the restrictions we may want to impose on the intended spatio-temporal structures and
the interpretations of spatial objects.
 We can choose among different flows of time (say, discrete or dense, infinite or finite)
 and among different topological spaces (say, arbitrary, Euclidean or Aleksandrov).
 At each time point we can interpret spatial objects as arbitrary subsets of the topological space, as regular closed (or open) ones, as polygons, etc.
 To represent the assumption that everything eventually comes to an end, we only
do not know when, one can restrict the class of intended models by imposing the
finite change assumption which states that no spatial object can change its spatial
configuration infinitely often, or the more liberal finite state assumption according
to which every spatial object can have only finitely many possible states (although it
may change its states infinitely often).
The paper is organised as follows. In Section 2 we introduce in full detail the component
spatial and temporal logics to be combined later on. In particular, besides the standard
spatial logics like RCC-8 or the 9-intersections of Egenhofer and Herring (1991), we consider
their generalisations in the framework of S4u and investigate the computational complexity. For example, we show that the maximal fragment of S4u dealing with regular closed
spatial objects turns out to be PSPACE-complete, while a natural generalisation of the
9-intersections is still in NP. In Section 3 we introduce a hierarchy of spatio-temporal logics
outlined above, provide them with a topological-temporal semantics, and analyse their computational properties. First we show that spatio-temporal logics satisfying only the (PC)
principle are not more complex than their components. Then we consider maximal combinations of S4u with (fragments of) PT L meeting both (PC) and (OC) and see that this
straightforward approach does not work: the resulting logics turn out to be undecidable.
Finally, we systematically investigate the trade-off between expressivity and complexity of
spatio-temporal formalisms and construct a hierarchy of decidable logics satisfying (PC)
171

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

and (OC) whose complexity ranges from PSPACE to 2EXPSPACE. These and other results, possible implementations as well as open problems are discussed in Section 4. For the
readers convenience most important (un)decidability and complexity results obtained in
this paper are summarised in Table 1 on page 193. All technical definitions and detailed
proofs can be found in the appendices.

2. Propositional Logics of Space and Time
We begin by introducing and discussing the spatial and the temporal formalisms we are
going to combine later on in this paper.
2.1 Logics of Space
We will be dealing with a number of logics suitable for qualitative spatial representation
and reasoning: the well-known RCC-8, BRCC-8 and S4u , as well as certain fragments of the
last one. The intended interpretations for all of these logics are topological spaces.
A topological space is a pair T = hU, Ii in which U is a nonempty set, the universe of
the space, and I is the interior operator on U satisfying the standard Kuratowski axioms:
for all X, Y  U ,
I(X  Y ) = IX  IY,

IX  IIX,

IX  X

and IU = U.

The operator dual to I is called the closure operator and denoted by C: for every X  U ,
we have CX = U  I(U  X). Thus, IX is the interior of a set X, while CX is its closure.
X is called open if X = IX and closed if X = CX. The complement of an open set is
closed and vice versa. The boundary of a set X  U is defined as CX  IX. Note that X
and U  X have the same boundary.
2.1.1 S4u
Our most expressive spatial formalism is S4u i.e., the propositional modal logic S4 extended with the universal modalities. The pedigree of this logic is quite unusual. S4 was
introduced independently by Orlov (1928), Lewis (in Lewis & Langford, 1932), and Godel
(1933) without any intention to reason about space. Orlov and Godel understood it as
a logic of provability (in order to provide a classical interpretation for the intuitionistic
logic of Brouwer and Heyting) and Lewis as a logic of necessity and possibility, that is, as
a modal logic. Besides the Boolean connectives and propositional variables, the language of
S4 contains two modal operators: I (it is necessary or provable) and C, the dual of I (it is
possible or consistent). In other words, the formulas of S4 can be defined as follows:


::=

p

| 

| 1 u 2

| I,

(1)

where the p are variables. Set C = I . We denote the modal operators by I and C
(rather than the conventional 2 and 3) because we understand, following an observation
made by several logicians in the late thirties and early forties (Stone, 1937; Tarski, 1938;
Tsao Chen, 1938; McKinsey, 1941), S4 as a logic of topological spaces: if we interpret the
propositional variables as subsets of a topological space, the Booleans as the standard settheoretic operations, and I and C as, respectively, the interior and the closure operators
172

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

on the space, then an S4-formula is modally consistent if and only if it is satisfiable in a
topological spacei.e., its value is not empty under some interpretation.1
More precisely, a topological model is a pair of the form M = hT, Ui, where T = hU, Ii
is a topological space and U, a valuation, is a map associating with every variable p a set
U(p)  U . Then the valuation U is inductively extended to arbitrary S4-formulas by taking:
U( ) = U  U( ),

U(1 u 2 ) = U(1 )  U(2 ),

U(I ) = IU( ).

Expressions  of the form (1) are interpreted as subsets of topological spaces; that is why we
will call them spatial terms. In particular, propositional variables of S4 will be understood
as spatial variables.

The language of S4u extends S4 with the universal and the existential quantifiers 2

and 3, respectively (known in modal logic as the universal modalities). Given a spatial
  to say that the part of space (represented by)  is not empty (there is
term  , we write 3
  means that  occupies the whole space (all points belong to  ).
at least one point in  ); 2
By taking Boolean combinations of such expressions we arrive at what will be called spatial
formulas. A BNF definition looks as follows:2



::= 2

|



| 1  2 ,

  . Spatial formulas can be either true or false in
  = 2
where the  are spatial terms. Set 3
topological models. The truth-relation M |= a spatial formula  is true in a topological
model Mis defined in the standard way:

 M |= 2

iff

U( ) = U ,

 M |= 

iff

M 6|= ,

 M |= 1  2

iff

M |= 1 and M |= 2 .

Say that a spatial formula  is satisfiable if there is a topological model M such that M |= .
The seemingly simple query language S4u can express rather complex relations between
sets in topological spaces. For example, the formula
 (q @ p)  2
 (p @ Cq)  3
 p  3
 Iq
2

says that a set q is dense in a nonempty set p, but has no interior (here 1 @ 2 is an
abbreviation for 1 u 2 ).
The following folklore complexity result has been proved in different settings (see, e.g.,
Nutt, 1999; Areces, Blackburn, & Marx, 2000):
Theorem 2.1. (i) S4u enjoys the exponential finite model property; i.e., every satisfiable
spatial formula  is satisfiable in a topological space whose size is at most exponential in
the size of .
(ii) Satisfiability of spatial formulas in topological models is PSPACE-complete.
1. Moreover, according to McKinsey (1941) and McKinsey and Tarski (1944), any n-dimensional Euclidean
space, for n  1, is enough to satisfy all consistent S4-formulas.
2. Formally, the language of S4u as defined above is weaker than the standard one, say, that of Goranko
and Passy (1992). However, one can easily show that they have precisely the same expressive power:
see, e.g., (Hughes & Cresswell, 1996) or (Aiello & van Benthem, 2002b).

173

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

One way of proving this theorem is first to observe that every satisfiable spatial formula
is satisfied in an Aleksandrov model, i.e., a model based on an Aleksandrov topological
spacealias a standard Kripke frame for S4 (see, e.g., McKinsey & Tarski, 1944; Goranko
& Passy, 1992).
We remind the reader that a topological space is called an Aleksandrov space (Alexandroff, 1937) if arbitrary (not only finite) intersections of open sets are open. A Kripke frame
(or simply a frame) for S4 is a pair the form G = hV, Ri, where V is a nonempty set and
R a transitive and reflexive relation (i.e., a quasi-order ) on V . Every such frame G induces
the interior operator IG on V : for every X  V ,
IG X = {x  X | y  V (xRy  y  X)}.
In other words, the open sets of the topological space TG = hV, IG i are the upward closed
(or R-closed ) subsets of V . The minimal neighbourhood of a point x in TG (that is the
minimal open set to contain x) consists of all those points that are R-accessible from x. It
is well-known (see, e.g., Bourbaki, 1966) that TG is an Aleksandrov space and, conversely,
every Aleksandrov space is induced by a quasi-order.
Now, to complete the proof, it suffices to recall that S4 is PSPACE-hard (Ladner,
1977) and use, say, the standard tableau technique to establish the exponential finite model
property and construct a PSPACE satisfiability checking algorithm for spatial formulas.
Although being of the same computational complexity as S4, the logic S4u is more
expressive. A standard example is that spatial formulas can distinguish between arbitrary
and connected3 topological spaces. Consider, for instance, the formula
 (Cp @ p)  2
 (p @ Ip)  3
p
 p  2
2

(2)

saying that p is both closed and open, nonempty and does not coincide with the whole space.
It can be satisfied only in a model whose underlying topological space is not connected, while
all satisfiable S4-formulas are satisfied in connected (e.g., Euclidean) spaces.
Another example illustrating the expressive power of S4u is the formula
 (p @ Cp)  2
 (p @ Cp)
p  2
3

(3)

defining a nonempty set p such that both p and p have empty interiors. In fact, the second
and the third conjuncts say that p and p consist of boundary points only.
2.1.2 RCC-8 as a Fragment of S4u
In qualitative spatial representation and reasoning, it is quite often assumed that spatial
terms can only be interpreted by regular closed (or open) sets of topological spaces (see,
e.g., Davis, 1990; Asher & Vieu, 1995; Gotts, 1996). One of the reasons for imposing this
restriction is to exclude from consideration such pathological sets as p in (3). Recall that a
set X is regular closed if X = CIX, which clearly does not hold for any set p satisfying (3).
Another reason is to ensure that the space occupied by a physical body is homogeneous
in the sense that it does not contain parts of different dimensionality. For example, the
3. We remind the reader that a topological space is connected if its universe cannot be represented as the
union of two disjoint nonempty open sets.

174

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

.
X

IX

CIX

.
Figure 3: Regular closure.
subset X of the Euclidean plane in Fig. 3 consists of three parts: a 2D ellipse with a hole, a
2D circle, and a 1D curve connecting them. This curve disappears if we form the set CIX,
which is regular closed because CICIX = CIX, for every X and every topological space.
In this paper, we will consider several fragments of S4u dealing with regular closed sets.
From now on we will call such sets regions. Perhaps, the best known language devised
for speaking about regions is RCC-8 which was introduced in the area of Geographical
Information Systems (see Egenhofer & Franzosa, 1991; Smith & Park, 1992) and as a
decidable subset of Region Connection Calculus RCC (Randell et al., 1992). The syntax of
RCC-8 contains eight binary predicates,
 DC(X, Y )  regions X and Y are disconnected,
 EC(X, Y )  X and Y are externally connected,
 EQ(X, Y )  X and Y are equal,
 PO(X, Y )  X and Y partially overlap,
 TPP(X, Y )  X is a tangential proper part of Y ,
 NTPP(X, Y )  X is a nontangential proper part of Y ,
 the inverses of the last twoTPPi(X, Y ) and NTPPi(X, Y ),
which can be combined using the Boolean connectives. For example, given a spatial database
describing the geography of Europe, we can query whether the United Kingdom and the
Republic of Ireland share a common border. The answer can be found by checking whether
the RCC-8 formula EC(UK, RoI) follows from the database.
The arguments of the RCC-8 predicates are called region variables; they are interpreted
by regular closed setsi.e., regionsof topological spaces. The satisfiability problem for
RCC-8 formulas under such interpretations is NP-complete (Renz & Nebel, 1999).
The expressive power of RCC-8 is rather limited. It only operates with simple regions and does not distinguish between connected and disconnected ones, regions with and
without holes, etc. (Egenhofer & Herring, 1991). Nor can RCC-8 represent complex relations between more than two regions. Consider, for example, three countries (say, Russia,
Lithuania and Poland) such that not only each one of them is adjacent to the others, but
there is a point where all the three meet. To express this fact we may need a ternary
predicate like
EC3(Russia, Lithuania, Poland).
(4)
175

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

To analyse possible ways of extending the expressive power of RCC-8, it will be convenient to view it as a fragment of S4u (that RCC-8 can be embedded into S4u was first shown
by Bennett, 1994). Observe first that, for every spatial variable p, the spatial term
CIp

(5)

is interpreted as a regular closed set in every topological model. So, with every region
variable X of RCC-8 we can associate the spatial term %X = CIpX , where pX is a spatial
variable, and then translate the RCC-8 predicates into spatial formulas by taking:
 (%
 (I%
EC(X, Y ) = 3
X u %Y )  3
X u I%Y ),
 (%
DC(X, Y ) = 3
X u %Y ),
 (%
 (%
EQ(X, Y ) = 2
X @ %Y )  2
Y @ %X ),
 (%
 (%
 (I%
PO(X, Y ) = 3
X u I%Y )  2
X @ %Y )  2
Y @ %X ),
 (%
 (%
 (%
TPP(X, Y ) = 2
X @ %Y )  2
Y @ %X )  2
X @ I%Y ),
 (%
 (%
NTPP(X, Y ) = 2
X @ I%Y )  2
Y @ %X )

(TPPi and NTPPi are the mirror images of TPP and NTPP, respectively). The first of these
formulas, for instance, says that two regions are externally connected iff the intersection of
the regions is not empty, whereas the intersection of their interiors is. It should be clear
that an RCC-8 formula is satisfiable in a topological space if and only if its translation into
S4u defined above is satisfiable in a topological model.
This translation also shows that in RCC-8 any two regions can be related in terms of
truth/falsity of atomic spatial formulas of the form
 (% u % ),
2
1
2

 (I% u I% ),
2
1
2

 (%
2
1 @ %2 )

and

 (%
2
1 @ I%2 ),

where %1 and %2 are spatial terms of the form (5). For example, the first of these formulas
says that the intersection of two regions is empty, whereas the last one states that one region
is contained in the interior of another one. In other words, RCC-8 can be regarded as part
of the following fragment of S4u :
%

::= CIp,



::= %1 u %2




::= 2

|

| I%1 u I%2


| %1 @ %2

| %1 @ I%2 ,

| 1   2 .

Here we distinguish between two types of spatial terms. Those of the form % will be
called atomic region termsthey represent the (regular closed) regions we want to compare.
Spatial terms of the form  are used to relate regions to each other (note that their extensions
are not necessarily regular closed).
Actually, the fragment introduced above is a bit more expressive than RCC-8: for example, it contains (appropriately modified) formula (2) which can be satisfied only in disconnected topological spaces, while all satisfiable RCC-8 formulas are satisfiable in any
Euclidean space (Renz, 1998). However, it will be convenient for us not to distinguish
between these two spatial logics. First, it will turn out that the same technical results regarding their computational complexity hold for them even when combined with temporal
176

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

logics. And second, the more intuitive and concise language of RCC-8 is more suitable for
illustrations. For instance, we do not distinguish between the region variable X and the
 (% u % ).
region term %X and use DC(%1 , %2 ) as an abbreviation for 3
1
2
The definition above suggests two ways of increasing the expressive power of RCC-8
(while keeping all regions regular closed):
(i) by allowing more complex region terms %, and
(ii) by allowing more ways of relating them (i.e., more complex terms  ).
2.1.3 BRCC-8 as a Fragment of S4u
The language BRCC-8 of Wolter and Zakharyaschev (2000a) (see also Balbiani, Tinchev, &
Vakarelov, 2004) extends RCC-8 in direction (i). It uses the same eight binary predicates
as RCC-8 and allows not only atomic regions but also their intersections, unions and complements. For instance, in BRCC-8 we can express the fact that a region (say, the Swiss
Alps) is the intersection of two other regions (Switzerland and the Alps in this case):
EQ(SwissAlps, Switzerland u Alps).

(6)

We can embed BRCC-8 to S4u by using almost the same translation as in the case of RCC-8.
The only difference is that now, since Boolean combinations of regular closed sets are not
necessarily regular closed, we should prefix compound spatial terms with CI. This way we
can obtain, for example, the spatial term
CI (Switzerland u Alps)
representing the Swiss Alps. In the same manner we can treat other set-theoretic operations,
which leads us to the following definition of Boolean region terms:
%

::=

CIp

| CI%

| CI(%1 u %2 ).

In other words, Boolean region terms denote precisely the members of the well-known
Boolean algebra of regular closed sets. (The union t is expressible via intersection and
 
complement in the usual way.) To simplify notation, given a spatial term  , we write 
to denote the result of prefixing CI to every subterm of  ; in particular,
 
 
 


   
p = CIp,
 = CI 
and 1 u 2 = CI( 1 u 2 ).
 
Note that  is (equivalent to) a Boolean region term, for every spatial term  . Now the
Swiss Alps from the example above can be represented as Switzerland u Alps .
It is of interest to note that Boolean region terms do not increase the complexity of
reasoning in arbitrary topological models: the satisfiability problem for BRCC-8 formulas is
still NP-complete (however, it becomes PSPACE-complete if all intended models are based
on connected spaces). On the other hand, BRCC-8 allows some restricted comparisons of
more than two regions as, e.g., in (6). Nevertheless, as we shall see below, ternary relations
like (4) are still unavailable in BRCC-8: they require different ways of comparing regions;
cf. (ii).
177

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

2.1.4 RC
Egenhofer and Herring (1991) proposed to relate any two regions in terms of the 9-intersections33-matrix specifying emptiness/nonemptiness of all (nine) possible intersections
of the interiors, boundaries and exteriors of the regions. Recall that, for a region X, these
three disjoint parts of the space hU, Ii can be represented as
IX,

X  (U  IX)

and

U  X,

respectively. By generalising this approach to any finite number of regions, we obtain the
following fragment RC of S4u :
%

::= Boolean region terms,



::= %




::= 2

| I%
|

| 



| 1 u 2 ,

| 1   2 .

In other words, in RC we can define relations between regions in terms of emptiness/nonemptiness of sets formed by using arbitrary set-theoretic operations on regions and their
interiors. However, nested applications of the topological operators are not allowed (an
example where such applications are required can be found in the next section).
Clearly, both RCC-8 and BRCC-8 are fragments of RC. Moreover, unlike BRCC-8, the
language of RC allows us to consider more complex relations between regions. For instance,
the ternary relation required in (4) can now be defined as follows:
 (%
 (I%
 (I%
 (I%
EC3(X, Y, Z) = 3
X u %Y u %Z )  3
X u I%Y )  3
Y u I%Z )  3
Z u I%X ).

Another, more abstract, example is the formula
 % u    u %  I%
3
1
i
i+1 u    u I%j u %j+1 u    u %k u I%k+1 u    u I%n



which says that
regions %1 , . . . , %i meet somewhere inside the region occupied jointly by all
%i+1 , . . . , %j , but outside the regions %j+1 , . . . , %k and not inside %k+1 , . . . , %n .
Although RC is more expressive than both RCC-8 and BRCC-8, reasoning in this language is still of the same computational complexity:
Theorem 2.2. The satisfiability problem for RC-formulas in arbitrary topological models
is NP-complete.
This result will be proved in Appendix A. Lemma A.1 shows that every satisfiable RCformula can be satisfied in a model based on the Aleksandrov space that is induced by a
disjoint union of n-broomsi.e., quasi-orders of the form depicted in Fig. 4. Topological
spaces of this kind have a rather primitive structure satisfying the following property:
(rc) only the roots of n-brooms can be boundary points, and the minimal neighbourhood
of every boundary pointi.e., the n-broom containing this pointmust contain at
least one internal point and at least one external point.
178

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

b

b
b
b
*

HH
Y




J
]
H

 
HHJ
HJ b


depth 0
depth 1

Figure 4: n-broom (for n = 4).
For example, spatial formula (3) cannot be satisfied in a model with this property, and so
it is not in RC.
By Lemma A.2, the size of such a satisfying model is polynomial (in fact, quadratical)
in the length of the input RC-formula, and so we have a nondeterministic polynomial time
algorithm. Actually, the proof is a straightforward generalisation of the complexity proof
for BRCC-8 given by Wolter and Zakharyaschev (2000a): the only difference is that in the
case of BRCC-8 it is sufficient to consider only 2-brooms (which were called forks). This
means, in particular, that ternary relation (4)which is satisfiable only in a model with an
n-broom, for n  3is indeed not expressible in BRCC-8.
Remark 2.3. In topological terms, n-brooms are examples of so-called door spaces where
every subset is either open or closed. However, the modal theory of n-brooms defines a
wider and more interesting topological class known as submaximal spaces in which every
dense subset is open. Submaximal spaces have been around since the early 1960s and have
generated interesting and challenging problems in topology. For a survey and a systematic
study of these spaces see (Arhangelskii & Collins, 1995) and references therein.
2.1.5 RC max
One could go even further in direction (ii) and impose no restrictions whatsoever on the
ways of relating Boolean region terms. This leads us to the maximal fragment RC max of
S4u in which spatial terms are interpreted by regular closed sets. Its syntax is defined as
follows:
%

::= Boolean region terms,



::= %




::= 2

| 
|

| 1 u 2


| I,

| 1  2 ,

To understand the difference between RC and RC max , consider the RC max -formula
  
 
 
 
   
 


3
q1 u I q1  2
q1 u I q1 @ C I q1 u q2 u I q2
.

(7)

 
It says that the boundary of q1 is not empty and that
 every neighbourhood of every
point
  in this boundary contains an internal point of q1 that belongs to the boundary of
q2 (compare with property (rc) above). The simplest Aleksandrov model satisfying this
formula is of depth 2; it is shown in Fig. 5.
The price we have to pay for this expressivity is that the complexity of RC max is the
same as that of full S4u :
179

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

q1 q2

q1 q2

q1 q2

b
b
]
J



J


J b


b

depth 0








q1 q2


]
J


J
J 

q1 q2 b

depth 1
depth 2

Figure 5: Model satisfying formula (7).
Theorem 2.4. The satisfiability problem for RC max -formulas is PSPACE-complete.
The upper bound follows from Theorem 2.1 and the lower bound is proved in Appendix A, where we construct a sequence of RC max -formulas such that each of them is
satisfiable in an Aleksandrov space of cardinality at least exponential in the length of the
formula. The first formula of the sequence is similar to (7) above.
It is of interest to note, however, that RC max is still not expressive enough to define such
pathological sets as p in (3) which is clearly not regular closed.
To conclude this section, we summarise the inclusions between the spatial languages
introduced above:
RCC-8

$

BRCC-8

$

RC

$

RC max

$

S4u .

For more discussions of spatial logics of this kind we refer the reader to the paper (PrattHartmann, 2002).
2.2 Temporal Logics
As was said in the introduction, the temporal components of our spatio-temporal hybrids
are (fragments of) the propositional temporal logic PT L interpreted in various flows of time
which are modelled by strict linear orders F = hW, <i, where W is a nonempty set of time
points and < is a (connected, transitive and irreflexive) precedence relation on W .
The language PT L is based on the following alphabet:
 propositional variables p0 , p1 , . . . ,
 the Booleans  and , and
 the binary temporal operators U (until) and S (since).
The set of PT L-formulas is defined in the standard way:


::=

p

|



| 1   2

| 1 U  2

| 1 S 2 .

PT L-models are pairs of the form M = hF, Vi such that F = hW, <i is a flow of time
and V, a valuation, is a map associating with each variable p a set V(p)  W of time
points (where p is supposed to be true). The truth-relation (M, w) |= , for an arbitrary
PT L-formula  and w  W , is defined inductively as follows, where (u, v) denotes the open
interval {w  W | u < w < v}:
180

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

 (M, w) |= p

w  V(p),

iff

 (M, w) |= 

(M, w) 6|= ,

iff

 (M, w) |= 1  2

(M, w) |= 1 and (M, w) |= 2 ,

iff

 (M, w) |= 1 U 2
all u  (w, v),

iff

there is v > w such that (M, v) |= 2 and (M, u) |= 1 for

 (M, w) |= 1 S 2
all u  (v, w).

iff

there is v < w such that (M, v) |= 2 and (M, u) |= 1 for

A PT L-formula  is satisfied in M if (M, w) |=  for some w  W .
We took the operators U and S as primitive simply because all other important temporal
operators can be defined via them. For example, 3F (sometime in the future) and 2F
(always in the future) are expressible via U as
3F  = > U ,

2F  = 3F ,

(> is the logical constant true) which means that
 (M, w) |= 3F 

iff

there is v > w such that (M, v) |= ,

 (M, w) |= 2F 

iff

(M, v) |=  for all v > w.

As our intended flows of time are strict linear orders, the next-time operator
definable via U by taking
 =  U 



is also

( is the logical constant false) which perfectly reflects our intuition: if F is discrete then
 (M, w) |= 

iff

(M, w + 1) |= ,

where w + 1 is the immediate successor of w in F. The reader should not have problems in
defining the past versions of 3F , 2F and .
The following results are due to Sistla and Clarke (1985) and Reynolds (2003, 2004):
Theorem 2.5. The satisfiability problem for PT L-formulas is PSPACE-complete for each
of the following classes of flows of time: all strict linear orders, all finite strict linear orders,
hN, <i, hZ, <i, hQ, <i, hR, <i.
Note, however, that reasoning becomes somewhat simpler if we take 3F , 2F and their
past counterparts (but no , U and S) as the only temporal primitives. Denote by PT L2
the corresponding fragment of PT L. Then, according to the results of Ono and Nakamura
(1980), Sistla and Clarke (1985), and Wolter (1996), we have:
Theorem 2.6. The satisfiability problem for PT L2 -formulas is NP-complete for each of
the classes of flows of time mentioned in Theorem 2.5.
181

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

3. Combinations of Spatial and Temporal Logics
In this section we introduce and discuss various ways of combining logics of space and time.
First we construct spatio-temporal logics satisfying only the (PC) principle (see the introduction) and show that they inherit good computational properties of their components.
Being encouraged by these results, we then consider maximal combinations of S4u with
(fragments of) PT L meeting both (PC) and (OC) and see that such a straightforward approach does not work: we end up with undecidable logics. This leads us to a systematic
investigation of the trade-off between expressivity and computational complexity of spatiotemporal formalisms. The result is a hierarchy of decidable logics satisfying (PC) and (OC)
whose complexity ranges from PSPACE to 2EXPSPACE.
3.1 Spatio-Temporal Logics with (PC)
We begin our investigation of combinations of the spatial and temporal logics introduced
above by considering the language PT L[S4u ] in which the temporal operators can be applied
to spatial formulas but not to spatial terms (this way of temporalising a logic was first
introduced by Finger and Gabbay, 1992). A precise syntactic definition of PT L[S4u ]-terms
 and PT L[S4u ]-formulas  is as follows:


::= p




::= 2

| 
|

| 1 u 2


| I,

| 1  2

|  1 U 2

| 1 S  2 .

Note that the definition of PT L[S4u ]-terms coincides with the definition of spatial terms
in S4u which reflects the fact that PT L[S4u ] cannot capture the change of spatial objects
in time. We have imposed no restrictions upon the temporal operators in formulasso the
combined language still has the full expressive power of PT L. (Clearly, S4u is a fragment
of PT L[S4u ].)
In a similar way we can introduce spatio-temporal logics based on all other spatial
languages we are dealing with: RCC-8, BRCC-8, RC and RC max . For example, the temporalisation PT L[BRCC-8] of BRCC-8 (denoted by ST 0 in the hierarchy of Wolter and
Zakharyaschev 2002) allows applications of the temporal operators to RCC-8 predicates but
not to Boolean region terms. These languages can be regarded as fragments of PT L[S4u ]
in precisely the same way as their spatial components were treated as fragments of S4u .
We illustrate the expressive power of PT L[RCC-8] by formalising sentences (A) and (B)
from the introduction:
DC(Image1 , Image2 )  DC(Image1 , Image2 )  EC(Image1 , Image2 ),

DC(Kaliningrad, EU) U TPP(Poland, EU) 
2F

(A)
(B)


TPP(Poland, EU)  EC(Kaliningrad, EU) .

Sentences (C)(H) cannot be expressed in this language (or even in PT L[S4u ]): they require
comparisons of states of spatial objects at different time instants.
The intended semantics of PT L[S4u ] (and all other spatio-temporal logics considered
in this paper) is rather straightforward. A topological temporal model (a tt-model, for short)
is a triple of the form M = hF, T, Ui, where F = hW, <i is a flow of time, T = hU, Ii a
182

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

topological space, and U, a valuation, is a map associating with every spatial variable p
and every time point w  W a set U(p, w)  U the space occupied by p at moment
w; see Fig. 1. The valuation U is inductively extended to arbitrary PT L[S4u ]-terms (i.e.,
spatial terms) in precisely the same way as for S4u , we only have to add a time point as a
parameter:
U( , w) = U  U(, w),

U(1 u 2 , w) = U(1 , w)  U(2 , w),

U(I, w) = IU(, w).

The truth-values of PT L[S4u ]-formulas are defined in the same way as for PT L:

 (M, w) |= 2

 (M, w) |= 

iff
iff

U(, w) = U ,
(M, w) 6|= ,

 (M, w) |= 1  2

iff

(M, w) |= 1

 (M, w) |= 1 U 2
all u  (w, v),

iff

there is v > w such that (M, v) |= 2 and (M, u) |= 1 for

 (M, w) |= 1 S 2
all u  (v, w).

iff

there is v < w such that (M, v) |= 2 and (M, u) |= 1 for

and

(M, w) |= 2 ,

And as in the pure temporal case, the operators 2F , 3F ,  as well as their past counterparts
can be defined in terms of U and S.
A PT L[S4u ]-formula  is said to be satisfiable if there exists a tt-model M such that
(M, w) |=  for some time point w.
The following optimal complexity result will be obtained in Appendix B.1:
Theorem 3.1. The satisfiability problem for PT L[S4u ]-formulas in tt-models based on
arbitrary flows of time, (arbitrary) finite flows of time, hN, <i, hZ, <i, hQ, <i, or hR, <i, is
PSPACE-complete.
The proof of this theorem is based on the fact that the interaction between spatial and
temporal components of PT L[S4u ] is very restricted. In fact, for every PT L[S4u ]-formula 
one can construct a PT L-formula  by replacing every occurrence of a (spatial) subformula
  in  with a fresh propositional variable p . Then, given a PT L-model N = hF, Vi for
2

 and a moment of time w, we take the set
  | (N, w) |= p }  {2
  | (N, w) |= p }
w = {2



of spatial formulas. It is not hard to see that if w is satisfiable for every w in F, then there
is a tt-model satisfying  and based on the flow F. Now, to check whether  is satisfiable,
it suffices to use a suitable nondeterministic algorithm (see, e.g., Sistla & Clarke, 1985;
Reynolds, 2003, 2004) which guesses a PT L-model for  and then, for each time point w,
to check satisfiability of w . This can be done using polynomial space in the length of .
Theorem 3.1 (together with Theorem 2.5) shows that all spatio-temporal logics of the
form PT L[L], for L  {RCC-8, BRCC-8, RC, RC max }, are also PSPACE-complete over the
standard flows of time.
183

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Now let us consider temporalisations of spatial logics with the (NP-complete) fragment PT L2 of PT L. By Theorems 2.4 and 3.1, both PT L2 [S4u ] and PT L2 [RC max ] are
PSPACE-complete. However, for simpler (NP-complete) spatial components we obtain a
better result:
Theorem 3.2. The satisfiability problem for PT L2 [RC]-formulas in tt-models based on
each of the classes of flows of time mentioned in Theorem 3.1 is NP-complete.
The proof is essentially the same as that of Theorem 3.1, but now nondeterministic
polynomial-time algorithms for the component logics are available. It follows from Theorem 3.2 that PT L2 [RCC-8] and PT L2 [BRCC-8] are NP-complete as well.
3.2 Maximal Combinations with (PC) and (OC)
As we saw in the previous section, the computational complexity of spatio-temporal logics
without (OC) is the maximum of the complexity of their components, which reflects the
very limited interaction between spatial and temporal operators in languages without any
means of expressing (OC).
A maximalist approach to constructing spatio-temporal logics capable of capturing
both (PC) and (OC) is to allow unrestricted applications of the Booleans, the topological
and the temporal operators to form spatio-temporal terms.
Denote by PT L  S4u the spatio-temporal language given by the following definition:


::= p




::= 2

| 
|

|  1 u 2


| I

| 1  2

| 1 U 2

| 1 U  2

| 1 S 2 ,
| 1 S 2 .

Expressions of the form  will be called spatio-temporal terms. Unlike the previous section,
these terms can be time-dependent. The definition of expressions of the form  is the
same as for PT L[S4u ]; they will be called PT L  S4u -formulas. All of the languages from
Section 3.1, including PT L[S4u ], are clearly fragments of PT L  S4u .
As before, we can introduce the temporal operators 2F , 3F ,  as well as their past
counterparts applicable to formulas. Moreover, these operators can now be used to form
spatio-temporal terms: for example,
3F  = > U ,

2F  = 3F 

and



=  U ,

where  denotes the empty set and > the whole space.
Spatio-temporal formulas are supposed to represent propositions speaking about moving
spatial objects represented by spatio-temporal terms. The truth-values of propositions in
spatio-temporal structures can vary in time, but do not depend on points of spacesthey
are defined in precisely the same way as in the case of PT L[S4u ]. But how to understand
temporalised terms?
The meaning of  should be clear: at moment w, it denotes the space occupied by 
at the next moment w + 1 (see Fig. 2). For example, we can write




 I Cyclone
3
u I Cyclone
184

(C)

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

to say that regions Cyclone and
the introduction). The formula

Cyclone

overlap (thereby formalising sentence (C) from

EQ(EU, EU t Romania t Bulgaria)

(F)

says that in two years the EU (as it is today) will be extended with Romania and Bulgaria.
Note that EQ(EU, EU t Romania t Bulgaria) has a different meaning because the EU
may expand or shrink in a year. It is also not hard to formalise sentences (D), (E) and (H)
from the introduction:
EQ(X, Y )  EQ(Y, Y ),

(D)

2F EQ(Europe, Europe),

(E)

EQ(Earth, W t L)  EC(W, L)  P(W, W )  P(L, L),

(H)

where P(X, Y )X is a part of Y denotes the disjunction of EQ(X, Y ), TPP(X, Y ) and
NTPP(X, Y ).
The intended interpretation of terms of the form 3F  , 2F  (and their past counterparts)
is a bit more sophisticated. It reflects the standard temporal meanings of propositions
3F x    and 2F x   , for all points x in the topological space:
 at moment w, term 3F  is interpreted as the union of all spatial extensions of  at
moments v > w;
 at moment w, term 2F  is interpreted as the intersection of all spatial extensions of
 at moments v > w.
For example, consider Fig. 2 with moving spatial object X depicted on it at three consecutive
moments of time (it does not change after t + 2). Then 3F X at t is the union of X and
X at t and 2F X at t is the intersection of X and X at t (i.e., X).
As another example, take the spatial object Rain. Then
 3F Rain at moment w occupies the space where it will be raining at some time points
v > w (which may be different for different places). 2F Rain at w occupies the space
where it will always be raining after w.
 2F 3F Rain at w is the space where it will be raining ever and ever again after w,
while 3F 2F Rain comprises all places where it will always be raining starting from
some future moments of time.
This interpretation shows how to formalise sentence (G) from the introduction:
P(England, 2F 3F Rain).

(G)

Now, what can be the meaning of Rain U Snow? Similarly to the readings of 2F  and
3F  above, we adopt the following definition:
 at moment w, the spatial extension of 1 U 2 consists of those points x of the topological
space for which there is v > w such that x belongs to 2 at moment v and x is in 1
at all u whenever w < u < v.
185

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

The past counterpart of Ui.e., the operator since Scan be used to say that the part of
Russia that has been remaining Russian since 1917 is not connected to the part of Germany
(Konigsberg) that became Russian after the Second World War (Kaliningrad):
DC(Russia S Russian Empire, Russia S Germany).
The models M = hF, T, Ui for PT L  S4u are precisely the same topological temporal
models we introduced for PT L[S4u ]. However, now we need additional clauses defining
extensions of spatio-temporal terms:

[ 
\
 U(1 U 2 , w) =
U(2 , v) 
U(1 , u) ,
v>w

 U(1 S 2 , w) =

[ 

u(w,v)

U(2 , v) 

v<w

\


U(1 , u) .

u(v,w)

Then we also have:
U(3F , w) =

[

U(, v)

and

U(2F , w) =

v>w

\

U(, v),

v>w

and, for discrete F,
U(, w) = U(, w + 1).
The truth-values of PT L  S4u -formulas are computed in precisely the same way as in the
case of PT L[S4u ]. A PT L  S4u -formula  is called satisfiable if there exists a tt-model
M such that (M, w) |=  for some time point w.
At first sight it may appear that the computational properties of the constructed logic should not be too badafter all, its spatial and temporal components are PSPACEcomplete. It turns out, however, that this is not the case:
Theorem 3.3. The satisfiability problem for PT L  S4u -formulas in tt-models based on
the flows of time hN, <i or hZ, <i is undecidable.
Without going into details of the proof of this theorem, one might immediately conjecture that it is the use of the infinitary operators U, 2F and 3F in the construction of
spatio-temporal terms that makes the logic over-expressive. Moreover, the whole idea of
topological temporal models based on infinite flows of time may look counterintuitive in
the context of spatio-temporal representation and reasoning (unlike, say, models used to
represent the behaviour of reactive computer systems).
There are different approaches to avoid infinity in tt-models. The most radical one is to
allow only finite flows of time. A more cautious approach is to impose the following finite
change assumption on models (based on infinite flows of time):
FCA No term can change its spatial extension infinitely often.
This means that under FCA we consider only those valuations U in tt-models hF, T, Ui
that satisfy the following condition: for every spatio-temporal term  , there are pairwise
disjoint intervals I1 , . . . , In of F = hW, <i such that W = I1      In and the state of 
remains constant on each Ij , i.e., U(, u) = U(, v) for any u, v  Ij . It turns out, however,
186

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

that in the case of discrete flows of time FCA does not give us anything new as compared
to arbitrary finite flows of time. More precisely, one can easily show that the satisfiability
problem for PT L  S4u -formulas in tt-models satisfying FCA and based on hN, <i or
hZ, <i is polynomially reducible to satisfiability in tt-models based on finite flows of time,
and the other way round. Note also that for the flows of time mentioned above, FCA can
be captured by the formulas 3F 2F EQ(, F  ) (and its past counterpart for hZ, <i), for
every spatio-temporal term  .
A more liberal way of reducing infinite unions and intersections to finite ones is to
adopt the finite state assumption:
FSA Every term may have only finitely many possible states (although it may
change its states infinitely often).
Say that a tt-model hF, T, Ui satisfies FSA if, for every spatio-temporal term  , there are
finitely many sets A1 , . . . , Am in the space T such that {U(, w) | w  W } = {A1 , . . . , Am }.
Such models can be used, for instance, to capture periodic fluctuations due to season or
climate changes, say, a daily tide. Similarly to FCA finitising the flow of time, FSA
virtually makes the underlying topological space finite. The following proposition will be
proved in Appendix B:
Proposition 3.4. A PT L  S4u -formula is satisfiable in a tt-model with FSA and based
a flow of time F iff it is satisfiable in a tt-model based on F and a finite (Aleksandrov )
topological space.
Unfortunately, none of these approaches works for PT L  S4u we still have:
Theorem 3.5. (i) The satisfiability problem for PT L  S4u -formulas in tt-models based
on (arbitrary) finite flows of time is undecidable.
(ii) The satisfiability problem for PT L  S4u -formulas in tt-models based on the flows
of time hN, <i or hZ, <i and satisfying FSA is undecidable.
The next-time operator  does not look so harmful as the infinitary U, 2F , 3F , and
still can capture some aspects of (OC) (see formulas (C), (D), (F) and (G) above). So let
us consider the fragment PT L  S4u of PT L  S4u with spatio-temporal terms of the form:


::=

p

| 

| 1 u 2

| I

|

.

In other words, PT L  S4u does not allow applications of temporal operators different from
 to form spatio-temporal terms (but they are still available as formula constructors). This
means that we can compare the states of a spatial object X over a bounded set of time
points only: for any time point t and any natural numbers n, m  0, we can compare at t
the state of X at t + n with its state at t + m.
This fragment is definitely less expressive than full PT L  S4u . For instance, according
to Lemma B.1, PT L  S4u -formulas do not distinguish between arbitrary tt-models and
those based on Aleksandrov topological spaceswe will call them Aleksandrov tt-models.
On the other hand, the set of PT L  S4u -formulas satisfiable in Aleksandrov models is
a proper subset of those satisfiable in arbitrary tt-models. Consider, for example, the
PT L  S4u -formula
 (2 Ip @ I2 p).
2
F
F
187

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

One can readily see that it is true in every Aleksandrov tt-model, but its negation can be
satisfied in a topological model. For it suffices to take the flow F = hN, <i and the topology
T = hR, Ii with the T
standard interior operator I on the real line, select a sequence Xn of
open sets such that nN Xn is not open, e.g., Xn = (1/n, 1/n), and put U(p, n) = Xn .
However, even this seemingly weak interaction between topological and temporal operators turns out to be dangerous:
Theorem 3.6. The satisfiability problem for PT L  S4u -formulas in tt-models based on
the flows of time hN, <i or hZ, <i is undecidable. It is undecidable as well for tt-models
satisfying FSA or based on (arbitrary) finite flows of time.
Theorem 2.6 might suggest considering the fragment PT L2  S4u with 2F and its past
counterpart 2P as the only temporal primitives applicable both to formulas and terms:


::= p




::= 2

| 
|

| 1 u 2


| I

| 1   2

| 2F 

| 2F 

| 2P ,

| 2P .

Yet again the result is negative:
Theorem 3.7. The satisfiability problem for PT L2  S4u -formulas in tt-models (with or
without FSA) based on the flows of time hN, <i or hZ, <i is undecidable. It is undecidable
as well for tt-models based on (arbitrary) finite flows of time.
These undecidability results (the strongest ones, Theorems 3.6 and 3.7, to be more
precise) will be proved in Appendix B.2 by a reduction of Posts correspondence problem
which is known to be undecidable (Post, 1946). As we will see from the proofs, these
theorems actually hold for the future fragments of the corresponding languages.
3.3 Decidable Spatio-Temporal Logics with (PC) and (OC)
An important lesson we learn from (the proofs of) the negative results of Section 3.2 is that
full S4u is too expressive for computationally well-behaved combinations with fragments of
PT L. On the other hand, as was said in Section 2.1.2, qualitative spatial representation and
reasoning often requires extensions of spatial variables to be regular closed (i.e., regions).
This restriction is very important for constructing decidable spatio-temporal logics with
(PC) and (OC). First, the undecidability proofs from Appendix B.2 do not go through
in this case. And second, as will be shown below, decidable combinations of PT L and
some of the fragments of S4u introduced in Section 2.1 do exist. In fact, we will construct
a hierarchy of decidable spatio-temporal logics of different computational complexity by
imposing various restrictions on regions themselves, the ways they can be compared, and
the interactions between spatial and temporal constructors.
We begin by considering the simplest combination of PT L and RCC-8 capturing (PC)
and (OC). This logic called PT LRCC-8 (it was introduced under the name ST 
1 by Wolter
and Zakharyaschev, 2002) operates with spatio-temporal region terms of the form
%

::=

CIp

| CI%.

To relate these terms, we are allowed to use the eight binary predicates of RCC-8; then arbitrary temporal operators and Boolean connectives can be applied to produce PT L  RCC-8
188

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

formulas. Typical examples of such formulas are (A), (B), (D) and (E) above. Note that
(C) can be regarded as a PT L  RCC-8 formula as well (two regions overlap iff they are
neither disconnected nor externally connected). On the other hand, (F), (H) and (G) are
not PT LRCC-8 formulas because the first two use the t operation on region terms and (G)
uses temporal operators 2F and 3F on region terms.
As before, PT L  RCC-8 formulas are interpreted in topological temporal models (or
tt-models). However, only discrete flows of time do make sense for this language. Although
the interaction between topological and temporal operators is similar to that in PT L  S4u
(clearly, PT LRCC-8 is a fragment of PT LS4u ), we have the following rather unexpected
and encouraging result:
Theorem 3.8. The satisfiability problem for PT L  RCC-8 formulas in tt-models based on
hN, <i, hZ, <i or (arbitrary) finite flows of time is PSPACE-complete.
This theorem will be proved in Appendix C.5. The idea of the proof is similar to that of
Theorem 3.1: we consider the spatial and the temporal parts of a given formula separately.
However, to take into account the interaction between these parts, we use the so-called
completion property of RCC-8 (cf. Balbiani & Condotta, 2002) with respect to a certain
class C of models: given a satisfiable set  of RCC-8 formulas and a model in C satisfying
a subset of , one can extend this partial model to a model in C satisfying the whole .
What happens if we extend the expressive power of the spatial component by allowing
Boolean operators on spatio-temporal region terms, i.e., jump from RCC-8 to BRCC-8?
Define spatio-temporal Boolean region terms by taking
%

::=

CIp

| CI%

| CI(%1 u %2 ) | CI%.

Denote by PT L  BRCC-8 the language obtained from PT L  RCC-8 by allowing spatiotemporal Boolean region terms as arguments of the RCC-8 predicates (this language was
called ST 1 by Wolter and Zakharyaschev, 2002). Formulas (A)(F) and (H) belong to
PT L  BRCC-8, but (G) uses the 2F and 3F operators on regions and so is not in PT L 
BRCC-8.
Now, another surprise is that the replacement of RCC-8 with BRCC-8 in our temporal
context results in an exponential jump of the computational complexity (remember that
both RCC-8 and BRCC-8 are NP-complete):
Theorem 3.9. The satisfiability problem for PT L  BRCC-8 formulas in tt-models based
on the flows of time hN, <i or hZ, <i is EXPSPACE-complete. It is EXPSPACE-complete
as well for models satisfying FSA or based on (arbitrary) finite flows of time.
The EXPSPACE upper bound (see Appendix C.3) is proved by a polynomial embedding of PT L  BRCC-8 into the one-variable fragment QT L1 of first-order temporal logic,
which is known to be EXPSPACE-complete (Hodkinson, Kontchakov, Kurucz, Wolter, &
Zakharyaschev, 2003). To construct this embedding, we first show that PT L  BRCC-8 is
complete with respect to Aleksandrov tt-models. In fact, we prove that every satisfiable
formula of the more expressive logic PT L  S4u introduced in Section 3.2 can be satisfied
in an Aleksandrov tt-model (see Lemma B.1 and the discussion above). Lemma C.1 then
shows that to satisfy a PT L  BRCC-8 formula, it suffices to take an Aleksandrov tt-model
189

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

based on a partial order of depth 1. By Lemma C.2, the width of the partial order can be
bounded by 2 (just as in the case of BRCC-8), and therefore unions of forks (or 2-brooms)
are enough to satisfy PT L  BRCC-8 formulas. These Aleksandrov tt-models based on
unions of forks can be encoded by means of unary predicates of QT L1 .
The EXPSPACE lower bound is proved in Appendix C.1 by encoding the corridor
tiling problem. It can also be established by a direct polynomial embedding of QT L1 into
PT LBRCC-8. To illustrate the idea, consider the QT L1 -formula x (P (x) P (x)) saying
that, for every point of the space, either it is in P now or will be there tomorrow. The same
statement can be expressed in PT L  BRCC-8 by the formula EQ(P t P, E)  DC(E, E),
where the last conjunct makes E empty.
Now let us make one more step in space and extend BRCC-8 to RC, thus obtaining the
spatio-temporal language PT L  RC with the following syntax:
%

::= CIp



::= %




::= 2

| CI%

| I%
|

| CI(%1 u %2 ) | CI%,

| 



| 1 u 2 ,

| 1  2

|  1 U 2

| 2 S  2 .

The reader should not be surprised now (although the authors were) that the extra expressivity results in one more exponential gap:
Theorem 3.10. The satisfiability problem for PT L  RC-formulas in tt-models based on
the flows of time hN, <i or hZ, <i is 2EXPSPACE-complete. It is 2EXPSPACE-complete
as well for models satisfying FSA or based on (arbitrary) finite flows of time.
The lower bound is established in Appendix C.1 and the upper bound in Appendix C.2.
Perhaps, it is proper time now to have a closer look at the emerging landscape. What
exactly causes these exponential jumps ? Can we locate more precise borders in the ladder
PSPACEEXPSPACE2EXPSPACE?
By analysing the proof of Theorem 3.8 (see Appendix C.5), we note that not so much
can be added to RCC-8. In fact, the maximal spatio-temporal logic (denoted by PT LRC 2 )
for which this proof goes through is based on spatio-temporal terms of the form
| CI%,

%

::= CIp



::= %



::= 1 u 2 .

| I%

| %

| I%,

On the other hand, even the addition of predicates of the form EQ(X, Y t Z) is enough
to make the logic EXPSPACE-hard (see Remark C.3). Thus, PT L  RCC-8 (or rather
its extension PT L  RC 2 ) is located pretty close to the border between PSPACE and
EXPSPACE spatio-temporal logics.
The following fragment RC  of RC indicates where the border between EXPSPACE
and 2EXPSPACE may lie:
%

::= Boolean region terms,



::= %

| ,



::= I%

| 



::= 1 u    u m

| 1 u 2 ,

190

| u

| .

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Intuitively, the  and the  are spatial terms interpreted by regular closed and regular open4
sets, respectively (the interior of a region is regular open, the complement of a regular closed
set is regular open (and vice versa), regular closed sets are closed under unions and regular
open ones are closed under intersections). Thus,  can be regarded as a generalisation of
region terms and  as a generalisation of the interiors of regions. In other words, RC  is
the fragment of RC in which only the following ways of relating regions are available:
 there is a point where some regions meet;
 a region intersects the interior of another one;
 the interior of a region is not empty.
It is readily checked that BRCC-8 is a fragment of RC  . Moreover, it is a proper fragment
because (4) belongs to the latter but not to the former. The formula

 



 ( N orthKorea
2
u SouthKorea ) @ DmZone
(8)
(saying that the demilitarised zone between the North Korea and the South Korea consists
of the border between them along with some adjacent territories) shows that RC  is a
proper subset of RC:
BRCC-8 $ RC  $ RC.
Although RC  extends BRCC-8, it gives rise to the spatio-temporal logic of the same
computational complexity:
Theorem 3.11. The satisfiability problem for PT L  RC  -formulas in tt-models based on
the flows of time hN, <i or hZ, <i is EXPSPACE-complete. It is EXPSPACE-complete as
well for models satisfying FSA or based on (arbitrary) finite flows of time.
The lower bound follows immediately from Theorem 3.9 and the proof of the upper
bound is similar to that of Theorem 3.9 (see Appendix C.3). Again, due to the restriction
on possible ways of relating regions, we can polynomially bound the width n of n-brooms
that are required to satisfy PT LRC  -formulas (cf. Lemma C.2). In fact, we need formulas
similar to (8) in order to increase complexity to 2EXPSPACE.
The constructed hierarchy of decidable spatio-temporal logics still leaves at least one
important question: do there exist decidable spatio-temporal logics that allow applications
of the temporal operators U, 2F , 3F to region terms and what is their complexity? Consider
the languages PT L  L, for L  {BRCC-8, RC  , RC}, which differ from PT L  L only in
the definition of spatio-temporal region terms:
%

::=

CIp

| CI%

| CI(%1 u %2 ) | CI(%1 U %2 ) | CI(%1 S %2 ).

The following two theorems provide a positive (though partial) answer to this question:
Theorem 3.12. The satisfiability problem for PT L  BRCC-8 and PT L  RC  -formulas
in tt-models based on hN, <i or hZ, <i and satisfying FSA, or based on (arbitrary) finite
flows of time is EXPSPACE-complete.
4. Remember that a set X is regular open if ICX = X.

191

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Theorem 3.13. The satisfiability problem for PT L  RC-formulas in tt-models based
on hN, <i or hZ, <i and satisfying FSA, or based on (arbitrary) finite flows of time is
2EXPSPACE-complete.
The upper bounds mentioned in these two theorems are proved in Appendices C.3 and
C.2, respectively. The lower bounds follow from the results for PT LBRCC-8 (Theorem 3.9)
and PT L  RC  (Theorem 3.10).
To appreciate the following theorem, the reader should recall that both PT L2 and RC 
are NP-complete:
Theorem 3.14. The satisfiability problem for PT L2 BRCC-8 and PT L2 RC  -formulas
in tt-models based on hN, <i or hZ, <i and satisfying FSA, or based on (arbitrary) finite
flows of time is EXPSPACE-complete.
Actually it is a consequence of the EXPSPACE-hardness of QT L1 with the sole temporal
operator 2F (see Hodkinson et al., 2003).
Unfortunately, very little is known about the complexity of our spatio-temporal languages interpreted in tt-models based on dense or arbitrary flows of time. In fact, the only
result we know of can be proved using the recent work (Hodkinson, 2004; Hodkinson et al.,
2003):
Theorem 3.15. The satisfiability problem for PT L  BRCC-8 and PT L  RC  -formulas
in tt-models satisfying FSA and based on hQ, <i, hR, <i or arbitrary flows of time belongs
to 2EXPTIME and is EXPSPACE-hard.

4. Conclusion
We have provided an in-depth analysis of the computational complexity of various spatiotemporal logics interpreted in Cartesian products of flows of time and topological spaces.
Some of these results are collected in Table 1. The design of the languages was driven by the
idea to cover the most basic features of spatio-temporal hybrids combining standard logics
of time and mereotopology, with the aim being to see how complex reasoning with these
hybrids could be. We did not try to fine-tune the languages for real-world applications. On
the contrary, we tried to keep them as pure and representative as possible and determine
computational challenges which any multi-dimensional approach to reasoning about space
and time would face. With this research objective in mind, we discuss now some conclusions
that can be drawn from Table 1.
The conclusion to be drawn from the undecidability results is easy: do not try to
implement a sound, complete and terminating algorithm which is supposed to decide the
satisfiability problem for PT L  S4u , PT L  S4u or PT L2  S4u you will never succeed.
If decision procedures are required, then alternative languages have to be devised.
The interpretation of the complexity results for decidable logics is not so transparent:
it is well-known that such results do not provide us with immediate conclusions regarding
the behaviour of implemented systems. For example, sometimes algorithms running in
exponential time in the worst-case perform better on practical problems than worst-case
optimal algorithms that run in polynomial time. Indeed, the complexity results should be
analysed together with their proofsif significant conclusions are required (cf. Nebel, 1996).
192

filanguage

n/a

spatial component L

flow
RCC-8

PT L[L]

N, Z,Q, R,
finite
or
arbitrary
N, Z

PT L2  L

RC

RC max

NP

PSPACE

PSPACE

(Thm. 2.4)

(Thm. 2.1)

NP

PSPACE

(Thm. 3.2)

(Thm. 3.1)

PSPACE
(Thm. 3.1)

PSPACE
(Thm. 3.8)


finite
 EXPSPACE
or
 PSPACE
N, Z+FSA

EXPSPACE

2EXPSPACE

(Thm. 3.9)

(Thm. 3.10)

?

?


finite
 EXPSPACE EXPSPACE
 2EXPSPACE
or
 NP
 EXPSPACE
(Thm. 3.14)
N, Z+FSA
arbitrary 

or
 2EXPTIME
 2EXPTIME
?
Q, R
 NP
 EXPSPACE
with FSA

undecidable
(Thm. 3.6)

undecidable
(Thm. 3.7)



N, Z

S4u

(Thm. 2.2)

NP

N, Z

PT L  L

BRCC-8

N, Z,Q, R,
finite
or
arbitrary

PT L  L

PT L2 [L]

of time

L

Combining Spatial and Temporal Logics: Expressiveness vs. Complexity

?

?

?

undecidable

?

(Thm. 3.3)


finite
 EXPSPACE EXPSPACE
or
 PSPACE
(Thm. 3.12)
N, Z+FSA

arbitrary 
 2EXPTIME
or
 2EXPTIME

EXPSPACE
Q, R
 PSPACE
(Thm. 3.15)
with FSA

2EXPSPACE

?

(Thm. 3.13)

?

undecidable
(Thm. 3.5)

?

?

Table 1: Complexity of the satisfiability problem for spatial and spatio-temporal logics.

193

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Only the proofs show where the sources of the complexity are and whether they could be
relevant for practical problems and the implementation of algorithms.
In this respect our proofs are actually rather informative. The decidability proof for
PT L[S4u ] immediately provides us with a modular algorithm combining any known procedures for the components. The EXPSPACE-completeness results for PT L  BRCC-8
(with FSA) and PT L  BRCC-8 show an extremely close link between the spatio-temporal
languages and the one-variable fragment of first-order temporal logic. The algorithmic
problems investigated in the context of first-order temporal logic are, therefore, of the same
character as those we deal with in the spatio-temporal context. Thus, the experience of
working with algorithms for (fragments of) first-order temporal logics (Hodkinson, Wolter,
& Zakharyaschev, 2000; Degtyarev, Fisher, & Konev, 2003; Kontchakov, Lutz, Wolter, &
Zakharyaschev, 2004) about which we have a pretty good knowledge by now almost directly
translates to insights into possible algorithms for spatio-temporal logics. The PSPACEcompleteness result for PT L  RCC-8 is obtained by means of a reduction (modulo RCC-8
reasoning) to PT L. So we can conclude from the proof that it will be sufficient to have
good solvers for RCC-8 and PT L to obtain a reasonable prover for PT L  RCC-8. The
interaction between the two components turned out to be rather weak.
In conclusion, the complexity proofs clearly show the algorithmic problems to be solved
when dealing with the spatio-temporal logics presented in this paper. In particular, devising
algorithms for these logics should be conceived as part of the more general enterprise of
developing algorithms for propositional and the one-variable fragment of first-order temporal
logic.
Here are some comments on and explanations of the most important results in Table 1:
1. The undecidability result for PT L  S4u , PT L  S4u and PT L2  S4u solves a major
open problem of Wolter and Zakharyaschev (2002). It shows that, while S4u is a
suitable candidate for efficient pure spatial reasoning (Bennett, 1996; Renz & Nebel,
1998; Aiello & van Benthem, 2002a), its temporal extensions satisfying both (PC)
and (OC) are not suitable for practical spatio-temporal representation and reasoning.
2. Logics like PT L  BRCC-8 may turn out to be undecidable when interpreted in
arbitrary topological temporal models. One of the main origins of their expressive
power is a possibility to form infinite intersections and unions of regions. However, we
can tame the computational behaviour of these logics by imposing natural restrictions
on the classes of admissible models such as FSA.
3. The PSPACE upper bound for PT L  RCC-8 and the EXPSPACE lower bound for
PT L  BRCC-8 solve two other major open problems of Wolter and Zakharyaschev
(2002). It is of interest to note that the spatial fragments of PT L  RCC-8 and
PT L  BRCC-8 have the same computational complexity: both are NP-complete
over arbitrary topological spaces. Thus the additional Boolean connectives on spatial
regions interacting with the next-time operator  can make the logic substantially
more complex.
4. The 2EXPSPACE-completeness result for PT L  RC with FSA and PT L  RC is another example when a seemingly tiny increase of expressiveness results in a significant
jump of complexity.
194

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

5. PSPACE-completeness of PT L  RCC-8 is a particularly good news, since it shows
that this combination of PT L and RCC-8 has the same computational complexity
as PT L itself, for which surprisingly fast systems have been implemented (Schwendimann, 1998; Hustadt & Konev, 2003). This gives us hope that practical algorithms
for PT LRCC-8 can be implemented. Indeed, the proof shows that it may be possible
to encode the satisfiability problem for PT LRCC-8 into the satisfiability problem for
PT L and then use PT L provers. We note that this complexity result has been conjectured by Demri and DSouza (2002) and that our proof uses some ideas of Balbiani
and Condotta (2002).
6. On the other hand, the EXPSPACE lower bounds for PT L  BRCC-8 with FSA and
PT LBRCC-8 do not necessarily mean that reasoning with these logics is hopeless. In
fact, we show that both of them can be regarded as fragments of the one-variable firstorder temporal logic, for which tableau- and resolution-based decision procedures have
been developed and implemented (Degtyarev et al., 2003; Kontchakov et al., 2004).
Of course, there are many directions of further research in spatio-temporal knowledge representation and reasoning. Here we mention only some of them that are closely related to
the logics we have considered above.
 In this paper, we confined ourselves to considering linear flows of time. It may be
of interest, however, to investigate the computational properties of spatio-temporal
logics based on the branching time paradigm (see, e.g., Clarke & Emerson, 1981;
Emerson & Halpern, 1985) in order to model uncertainty about the future. Recent
results by Hodkinson, Wolter and Zakharyaschev (2001, 2002) give hope that such
logics can be decidable.
 We confined ourselves to considering only mereotopological formalisms for the spatial
dimension. It would be also of interest to consider spatial logics of directions (Ligozat,
1998), shape (Galton & Meathrel, 1999), size (Zimmermann, 1995), position (Clementini, Di Felice, & Hernandez, 1997), or even their hybrids (Gerevini & Renz, 2002).
We note that some results in this direction have been recently obtained by Balbiani
and Condotta (2002) and Demri and DSouza (2002).
 Another interesting and important perspective in both spatial and spatio-temporal
representation and reasoning is to move from arbitrary topological spaces to those
induced by metric spaces and introduce explicit and/or implicit numerical parameters.
First encouraging steps in this direction have been made in the work (Kutz, Sturm,
Suzuki, Wolter, & Zakharyaschev, 2003).
We conclude the paper with a number of open problems:
1. What is the precise computational complexity of PT L  BRCC-8 with FSA over
dense flows of time and arbitrary strict linear orders?
2. Are logics of the form PT L  L and PT L2  L, for L  {RC, BRCC-8, RCC-8},
decidable without FSA?
195

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

3. Are combinations of PT L and PT L2 with RC max (satisfying both (PC) and (OC))
decidable?
4. Is PT L  S4u undecidable over dense flows of time and arbitrary strict linear orders?
5. Is PT L  RCC-8 with FSA decidable in PSPACE?

Acknowledgments
The work on this paper was partially supported by U.K. EPSRC grants no. GR/R45369/01,
GR/R42474/01, GR/S61966/01 and GR/S63182/01. The work of the third author was also
partially supported by Hungarian Foundation for Scientific Research grants T30314 and
035192.
Special thanks are due to the referees of the first version of this paper whose remarks,
criticism and constructive suggestions have led to many days of intensive and exciting
research, new results and, hopefully, a better paper.

Appendix A. Complexity of Spatial Logics
In this appendix we prove Theorems 2.2 and 2.4. In these proofs we use the fact that S4u
(as well as its fragments) is complete with respect to (finite) Aleksandrov topological spaces
(McKinsey & Tarski, 1944; Goranko & Passy, 1992). Recall from p. 174 that an Aleksandrov
(topological ) model is a pair of the form M = hG, Vi, where G = hV, Ri is a quasi-order
and V is a map from the set of spatial variables into 2V . It will be more convenient for us
to unify notation for spatial formulas and spatial terms and write (M, x) |=  instead of
x  V( ), for  a spatial term and x a point in V . In particular, by the definition of the
interior and closure operators in Aleksandrov spaces,
(M, x) |= I

iff

(M, x) |= C

iff


y  V xRy  (M, y) |=  ,

y  V xRy  (M, y) |=  .

By the length `() of a formula  we understand the number of subformulas and subterms occurring in .
Proof of Theorem 2.2. The proof follows from Lemmas A.1 and A.2 below which show
together that every satisfiable RC-formula can be satisfied in an Aleksandrov model of size
polynomial (in fact, quadratical) in the length of the input formula (in other words, RC has
the polynomial finite model property). Thus, we have a nondeterministic polynomial time
algorithm for the satisfiability problem.
q
In fact, Lemma A.1 shows that RC is complete with respect to a subclass of Aleksandrov
spaces, namely, finite disjoint unions of finite brooms. Recall from p. 179 that a broom is
a partial order b of the form h{r}  V0 , Ri, where R is the reflexive closure of {r}  V0
(see Fig. 4). We call r the root of b and points in V0 the leaves of b; they are also referred
to as points of depth 1 and 0, respectively. A broom b is said to be a -broom,   , if
|V0 |  . In particular, we call a broom finite if it is an n-broom, for some n < .
196

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Lemma A.1. Every satisfiable RC-formula is satisfied in an Aleksandrov model based on
a finite disjoint union of finite brooms.
Proof. As is well-known, if an RC-formula  is satisfiable then it can be satisfied in a finite
Aleksandrov model M = hG, Vi, G = hV, Ri. Define a new relation R0 on V by taking R0
to be the reflexive closure of R  (V1  V0 ), where
V0 = {x  V | y (xRy   yRx)}

and V1 = V  V0 .

(Without loss of generality we may assume that V1 6=  and no y  V0 has more than one
proper R-predecessor.) Let G0 = hV, R0 i and M0 = hG0 , Vi. Clearly, G0 is a partial order as
required. We prove that, for every RC-formula ,
M |= 

iff

M0 |= .

(9)

First we show that, for every Boolean region term % and every x  V ,
(M0 , x) |= %

iff

(M, x) |= %.

(10)

By definition, (M0 , x) |= p iff (M, x) |= p, for every spatial variable p. It is readily seen that
for every y  V0 and every spatial term  , we have (M0 , y) |=  iff (M, y) |=  . Now, if % is
a Boolean region term then % = CI for some spatial term  , and we clearly have:

(M, x) |= CI iff y  V xRy and z  V (yRz  (M, z) |=  )

iff y  V0 xR0 y and (M, y) |= 

iff y  V0 xR0 y and (M0 , y) |= 

iff y  V0 xR0 y and (M0 , y) |= I
iff

(M0 , x) |= CI.

Next, we extend (10) to spatial terms of the form I% where % is a Boolean region term. If
(M, x) |= I% then (M, y) |= % whenever xRy, and so, by R0  R, we have (M0 , x) |= I%.
Conversely, suppose (M0 , x) |= I%. Take any y with xRy and any z  V0 with yRz. We
claim that (M, z) |= %. Indeed, if x  V1 then this follows by IH from xR0 z. If x  V0
then zRx. Since (M0 , x) |= %, by IH and % = CI , we obtain (M, z) |= %. Now (M, y) |= %
follows by yRz and % = CI . Thus, (M, x) |= I%.
Finally, we can easily extend (10) to arbitrary spatial terms and formulas of RC because
both are constructed from spatial terms of the form % and I%, with % a Boolean region term,
using operators that do not depend on the structure of the underlying partial order. Thus
we have (9).
q
Lemma A.2. Every satisfiable RC-formula  is satisfied in an Aleksandrov model based
on a disjoint union of at most `() many 2`()-brooms.
Proof. Remember that every RC-formula  is (equivalent to) a Boolean combination of
5
 ,...,3

    , the spatial term
spatial formulas from some set  = {3
1
m }. For each 3

5. In the following proof we consider

  as an abbreviation for 3
 as primary and 2
.
3

197

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

 is also a Boolean (or rather set-theoretic) combination of some %1 , . . . , %k , I%01 , . . . , I%0m ,
where the %i and the %0i are Boolean region terms.
It follows from Lemma A.1 that  is satisfied in an Aleksandrov model M = hG, Vi,
  
where G = hV, Ri is a finite disjoint union of finite brooms. For every 3
 with

M |= 3 , fix a point x  V such that (M, x ) |=  . We may assume that the x are
   .
pairwise distinct and that the roots of all brooms are the points of the form x for 3

   .
Therefore, G is a disjoint union of  `() many finite brooms b , for 3

    , and each
Let us construct a new model M0 as follows. For each broom b , 3

%   , we pick
 a leaf y,% of b (if any) such that (M, y,% ) |= %,
 a leaf y,% of b (if any) such that (M, y,% ) |= %
and remove the other leaves of b . Denote by b0 the resulting broom. Clearly, it is a 2`()    , and M0 = hG0 , Vi.
broom. Let G0 = hV 0 , R0 i be the disjoint union of all b0 , for 3

It is easy to see that G0 is as required.
   ,
Now, to show that  is satisfied in M0 , it suffices to prove that, for all 3


M0 |= 3

iff

 .
M |= 3

(11)

By definition of M0 , for all leaves y of G0 and all spatial terms  ,
(M0 , y) |= 

iff

(M, y) |= .

  
Next, for every root x of b , every 3
 and every %   , we have (M, x ) |= % iff
there is a leaf y such that x Ry and (M, y) |= % (simply because % = CI, for some ). It
follows from the construction of M0 that (M, x ) |= % iff (M0 , x ) |= %, for every %   .
It also follows that (M, x ) |= I% implies (M0 , x ) |= I%. Conversely, if (M0 , x ) |= I%, but
(M, x ) 6|= I% then there is a leaf y such that x Ry and (M, y) 6|= % which is a contradiction.
Since intersection and complement do not depend on the structure of the underlying frame,
we have (M0 , x ) |=  iff (M, x ) |=  , for every root x of b , which proves (11).
q

Proof of Theorem 2.4. The PSPACE upper bound follows from Theorem 2.1. The proof
of PSPACE-hardness is by reduction of the validity problem for quantified Boolean formulas
which is known to be PSPACE-complete (Stockmeyer, 1987). We will slightly modify the
proof of Ladner (1977) (that shows the PSPACE-hardness of S4), in order to take into
account that the variables in RC max -formulas are always prefixed by CI.
We may assume that quantified Boolean formulas are of the form
 = Q1 p1 . . . Qn pn 0 ,
where Qi  {, } and 0 is a Boolean formula with variables p1 , . . . , pn . As is well known,
all possible truth assignments to p1 , . . . , pn can be arranged as the leaves of a full binary
tree of depth n. The left subtree of the root contains all truth assignments with p1 true
and the right subtree those with p1 false; then we branch on p2 , then p3 , and so on. We
can determine whether  is valid by pruning this full binary tree: whenever Qi is , then
we keep both subtrees at the ith level, and whenever Qi is  then only one of them. If this
198

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

00
q 3 , p 1 , p2 , p 3

r

I
3@

00
q 3 , p 1 , p2

00
q 3 , p 2 , p3

r
3

00
q 3 , p2

r

r
3

I
3@

@

@
@
q ,p ,p
2@r - r 2 1 2
6
1

@
@
2 r -r
6

r - r q 1 , p1
YH
H
HH
H

1

r -r

q 2 , p2

q1

*





HH

r rq
H

0

0

Figure 6: An Aleksandrov model that may satisfy  , for  = p1 p2 p3 0 .
way we can end up with a tree such that all its leaves evaluate 0 to true, then  is valid,
otherwise not.
We will generate the leaves of this binary tree in Aleksandrov models with the help of
an RC max -formula. More precisely, we will construct an RC max -formula  such that
 its length is polynomial in the length of , and
  is satisfied in an Aleksandrov model iff  is valid.
Take fresh spatial variables q0 , . . . , qn , and put, for i = 0, . . . , n,
   
q0 u q1
if i = 0;



   

i =
qi1 u qi u qi+1 ,
if 0 < i < n;







qn1 u qn ,
if i = n.
Now consider the variables p1 . . . , pn of 
variables, and let 00 be the result of
 as spatial
replacing every occurrence of pi with pi in 0 . Put
^
^




+

+
00
 
 
 

 = 3
2
2
0 
i1 @ (i t i ) 
i1 @ (i u i )  2
n @ ,
Qi =

Qi =

where, for i = 1, . . . , n,
 
i = C i u pi

and

 
i+ = C i u I pi .

Clearly,  is an RC max -formula and its length is polynomial in the length of .
Suppose first that  is valid. Then Fig. 6 shows the structure of a possible Aleksandrov
model satisfying  .
The converse direction is similar to that of Ladners proof (1977). Suppose that  is
satisfied
in an Aleksandrov model M. Then, for each necessary sequence of truth values for
 
p1 , . . . , pn , there is a point in M reflecting this sequence (we do not use the structure
of the spatial terms i here). Since, by the last conjunct of  , 00 holds in M at all these
points, we obtain that the quantified Boolean formula  must be valid.
q
199

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Appendix B. Spatio-Temporal Logics Based on S4u
In this appendix we prove Theorems 3.1, 3.2, 3.6 and 3.7 as well as Proposition 3.4. Then
Theorems 3.3 and 3.5 are immediate corollaries of Theorem 3.6. But first, some general
results are established to be used later on.
We remind the reader that by an Aleksandrov tt-model we mean a tt-model based on an
Aleksandrov (topological) space. Every such model can be regarded as a triple of the form
K = hF, G, Vi, where F = hW, <i is a flow of time, G = hV, Ri a quasi-order, and V is a map
associating with every spatial variable p and every time point w  W a set V(p, w)  V .
As in Appendix A, instead of x  V(, w) we write (K, hw, xi) |=  to unify notation for
spatio-temporal formulas and terms.
Given a spatio-temporal formula , we denote by sub  the set of all its subformulas
and by term  the set of all spatio-temporal terms occurring in .
Lemma B.1. (i) If a PT L  S4u -formula  is satisfied in a tt-model with FSA and based
on a flow of time F, then  is satisfied in an Aleksandrov tt-model with FSA and based
on F.
(ii) If a PT L  S4u -formula  is satisfied in a tt-model based on a flow of time F, then
 is satisfied in an Aleksandrov tt-model based on F as well.
Moreover, in both cases we can choose an Aleksandrov tt-model K = hF, G, Vi satisfying
 (with F = hW, <i and G = hV, Ri) in such a way that for all w  W , x  V and
spatio-temporal terms  , the set
Aw,x, = {y  V | xRy and (K, hw, yi) |=  }
contains an R-maximal point 6 (provided of course that Aw,x, 6= ).
Proof. The proof uses the StoneJonssonTarski representation of topological Boolean
algebras (in particular, topological spaces) in the form of general frames (see, e.g., Goldblatt,
1976 or Chagrov & Zakharyaschev, 1997).
(i) Suppose that  is satisfied in a tt-model M = hF, T, Ui with FSA and based on a
topological space T = hU, Ii. Denote by V the set of all ultrafilters over U . For any two
ultrafilters x1 , x2  V , put x1 Rx2 iff A  U (IA  x1  A  x2 ). It is easy to see that R
is a quasi-order on V . Define an Aleksandrov tt-model K = hF, G, Vi by taking G = hV, Ri
and V(p, w) = {x  V | U(p, w)  x}. We show by induction on the construction of a
spatio-temporal term  that, for all w  W and x  V ,
(K, hw, xi) |= 

iff

U(, w)  x.

(12)

The basis of induction and the case of the Booleans are trivial. The case of  = I 0 is
standard (consult Goldblatt, 1976 or Chagrov & Zakharyaschev, 1997).
Case  = 1 U 2 . Assume that (K, hw, xi) |= 1 U 2 . Then there is v > w such that
(K, hv, xi) |= 2 and (K, hu, xi) |= 1 for all u in the interval (w, v). By IH, U(2 , v)  x and
U(1 , u)  x for all u  (w, v). Since
\
U(1 U 2 , w)  U(2 , v) 
U(1 , u),
u(w,v)

6. A point z is said to be R-maximal in A  V if, for every z 0  A, we have z 0 Rz whenever zRz 0 .

200

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

we shall have U(1 U 2 , w)  x if we show that
\
U(1 , u)  x.
U(2 , v) 

(13)

u(w,v)

In view of FSA, we can find time points u1 , . . . , ul  (w, v) such that
\
U(1 , u),
U(1 , u1 )      U(1 , ul ) =
u(w,v)

which yields (13) because ultrafilters are closed under finite intersections.
Conversely, let U(1 U 2 , w)  x. By FSA, there are time points v1 , . . . vl such that

[ 
\
U(1 , u) .
U(1 U 2 , w) =
U(2 , vi ) 
1il

u(w,vi )

And since x is an ultrafilter,
U(2 , vi ) 

\

U(1 , u)  x,

u(w,vi )

for some i, 1  i  l. Therefore, by IH, (K, hvi , xi) |= 2 and (K, hu, xi) |= 1 for all
u  (w, vi ). Hence (K, hw, xi) |= 1 U 2 .
Case  = 1 S 2 is considered analogously.
Now, we show that, for all w  W and spatio-temporal terms  ,

(K, w) |= 2

iff

U(, w) = U.

  . Then (K, hw, yi) |=  for all y  V , and so, by IH, U(, w)  y
Suppose that (K, w) |= 2
for all y  V . But then U(, w) = U . Conversely, if U(, w) = U then U(, w)  y for all
 .
y  V , from which, by IH, (K, w) |= 2
It follows immediately that  is satisfied in K. It should be also clear that K satisfies
FSA. This proves (i). The existence of R-maximal points in sets of the form Aw,x, (where
w  W , x  V and  is a spatio-temporal term) follows from a result of Fine (1974); see
also (Chagrov & Zakharyaschev, 1997, Theorem 10.36).

(ii) The construction is the same as in (i). First we show by induction that, for every
spatio-temporal term  of PT L  S4u , (K, hw, xi) |=  iff U(, w)  x. This time, however,
instead of U and S we need the inductive step for .
Case  =  0 . We have (K, hw, xi) |=  0 iff there exists an immediate successor w0 of
w such that (K, hw0 , xi) |=  0 iff, by IH, there is an immediate successor w0 of w such that
U( 0 , w0 )  x. It remains to recall that U( 0 , w) = U( 0 , w0 ) whenever w0 is the immediate
successor of w and U( 0 , w) =  whenever w has no immediate successor.
The remaining part of the proof is the same as in (i).
q
Proof of Proposition 3.4. The implication () follows immediately from the definition.
() Suppose that a PT LS4u -formula  is satisfied in a tt-model with FSA and a flow
of time F = hW, <i. Then, by Lemma B.1 (i),  is satisfiable in an Aleksandrov tt-model
M = hF, G, Vi with FSA and based on a quasi-order G = hV, Ri. In view of FSA, for
201

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

every   term , there are finitely many sets A1 , . . . , Ak  V such that {V(, w) | w 
W } = {A1 , . . . , Ak }. Therefore, there are finitely many time points w1 , . . . , wm  W such
that, for every w  W , there is wi , 1  i  m, with V(, w) = V(, wi ) for all   term .
Now we use the Lemmon filtration (see, e.g., Chagrov & Zakharyaschev, 1997) to construct
a tt-model based on a finite Aleksandrov topological space. First, define an equivalence
relation  on V by taking x  y if
(M, hwi , xi) |= 

iff

(M, hwi , yi) |= ,

for all i, 1  i  m, and   term .

Denote by [x] the equivalence class of x  V . The set V / of pairwise distinct equivalence
classes is clearly finite. Define a binary relation S on V / by taking [x]S[y] if
(M, hwi , yi) |= I

whenever (M, hwi , xi) |= I,

for all i, 1  i  m, and   term .

Clearly, S is well-defined, reflexive and transitive, and so G0 = hV / , Si is a finite quasiorder. Let V0 (p, w) = {[x] | x  V(p, w)}, for every spatial variable p and every w  W .
Consider the tt-model M0 = hF, G0 , V0 i. First we show that for all   term , x  V
and w  W ,
(M, hw, xi) |= 
iff
(M0 , hw, [x]i) |= .
The basis of induction follows from the definition of V0 , the cases of intersection and complement are trivial, and those of temporal operators follow by IH.
Suppose that (M, hw, xi) |= I and [x]S[y]. Then there is a moment wi such that
(M, hw, zi) |=  iff (M, hwi , zi) |=  , for all   term  and z  V . By the definition of S,
we have (M, hwi , yi) |=  , and so (M, hw, yi) |=  . Finally, by IH, (M0 , hw, [y]i) |=  , and
since y was arbitrary, we obtain (M0 , hw, [x]i) |= I .
Conversely, let (M0 , hw, [x]i) |= I and xRy. Then [x]S[y], and so (M0 , hw, [y]i) |=  ,
from which, by IH, (M, hw, yi) |=  . Thus, (M, hw, xi) |= I .
Finally, by a straightforward induction on the structure of , one can show that
(M, w) |= 

iff

(M0 , w) |= ,

for all   sub  and w  W . It follows that  is satisfied in M0 .

q

B.1 Temporalisations of S4u
Lemma B.2. Let  be a finite set of S4u -formulas. Then there is a finite quasi-order G
such that every satisfiable subset    is satisfied in some Aleksandrov model based on G.
Proof. For every satisfiable   , fix a model based on a finite quasi-order G = hV , R i
and satisfying . Let n = max{|V | :   ,  is satisfiable} and let G be the disjoint
union of n full n-ary (transitive) trees of depth n whose nodes are clusters of cardinality
n. It is not difficult to see that every G is a p-morphic image of G. Therefore, every
satisfiable    is satisfied in an Aleksandrov model based on G.
q
Proof of Theorem 3.1. PSPACE-hardness follows from Theorem 2.1 or 2.5. We show
the matching upper bound.
Let  be a PT L[S4u ]-formula. Since  is a PT L  S4u -formula, by Lemma B.1 (ii), it
is satisfiable in a tt-model iff it is satisfiable in an Aleksandrov tt-model based on the same
202

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

  of  we associate a fresh propositional
flow of time. With every (spatial) subformula 2

variable p and denote by  the PT L-formula that results from  by replacing all its
  with p .
subformulas of the form 2
We claim that  is satisfiable in an Aleksandrov

tt-model over a flow of time F = hW, <i iff

 there exists a temporal model N = hF, Ui satisfying  and,
  | (N, w) |= p }  {2
  | (N, w) |= p } of spatial
 for every w  W , the set w = {2


formulas is satisfiable.

The implication () is obvious. Conversely,
S suppose that we have a temporal model N
satisfying the conditions above. Let  = wW w . By Lemma B.2, there is a finite
quasi-order G such that, for every w  W , we have hG, Vw i |= w for some valuation
Vw . It should be clear that  is satisfied in the Aleksandrov tt-model hF, G, Vi, where
V(p, w) = Vw (p), for every spatial variable p and every w  W .
Now, to devise a decision procedure for PT L[S4u ] which uses polynomial space in the
length of the input formula, one can take the corresponding nondeterministic PSPACE
algorithm for PT L (Sistla & Clarke, 1985; Reynolds, 2004, 2003) and modify it as follows.
The algorithm constructs a pure temporal model N = hF, Ui for  and every time it
produces a state for a time instant w  W , it additionally checks whether the set w of
spatial formulas is satisfiable. By Theorem 2.1, this extra test can also be performed by a
PSPACE algorithm, which does not increase the complexity of the combined algorithm. q
Proof of Theorem 3.2. The proof is essentially the same as that of Theorem 3.1, but now
nondeterministic polynomial-time algorithms for the component logics are available.
q
B.2 Undecidability of PT L  S4u and PT L2  S4u
Note that although our spatio-temporal languages contain no propositional variables, we
 p can be regarded as a proposition.
still can simulate them: for a spatial variable p, formula 2
 p, for a spatial
Thus, in what follows by a propositional variable p we mean the formula 2
variable p (note the different typefaces used to denote propositional and spatial variables).
Proof of Theorem 3.6. The proof is by reduction of the undecidable Posts (1946) correspondence problem or PCP, for short. It is formulated as follows. Given a finite alphabet
A and a finite set P of pairs hv1 , u1 i , . . . , hvk , uk i of nonempty finite words


ff


ff
vi = bi1 , . . . , bili ,
ui = ci1 , . . . , ciri
(i = 1, . . . , k)
over A, an instance of PCP, decide whether there exist an N  1 and a sequence i1 , . . . , iN
of indices such that
vi1      viN = ui1      uiN ,
(14)
where  is the concatenation operation. We will construct (using only future-time temporal
operators) a PT L  S4u -formula A,P such that
(i) the length of A,P is a polynomial function in the size of both A and P ;
(ii) if A,P is satisfiable in a tt-model based on hN, <i then there exist an N  1 and a
sequence i1 , . . . , iN of indices such that (14) holds;
203

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

(iii) if there exist an N  1 and a sequence i1 , . . . , iN of indices such that (14) holds then
A,P is satisfiable in a tt-model with FSA and based on hN, <i;
(iv) A,P is satisfiable in a tt-model based on hN, <i iff A,P is satisfiable in a tt-model
based on a finite flow of time.
The case of hZ, <i follows immediately. By Lemma B.1 (ii), it suffices to consider only
Aleksandrov tt-models for A,P .
We build A,P using spatial variables lefta and righta (a  A), left, right and stripe, as
well as propositional variables pairi , for every pair hvi , ui i, 1  i  k, and range.
The variable range is required to relativise temporal operators 2F and 3F in order to
ensure that we can construct a model based on a finite flow of time. The variable stripe is
used to introduce a new strict closure operator in Aleksandrov spaces by taking, for every
spatio-temporal term  ,


S = stripe @ C(stripe u C ) u stripe @ C(stripe u C ) .
Denote by Sn a sequence of n operators S. Other abbreviations we need are 1  2 which
stands for (1 @ 2 ) u (2 @ 1 ) and 2+
F  which replaces   2F .
The formula A,P is defined as the conjunction
A,P = range  stripe  pair  eq  left  right ,
where
range = range  3F range  2F (range  2F range),


_
^
3
range

pair

(pair

pair
)
,
pair = 2+
F
i
i
j
F
1ik


 (stripe  stripe) ,
3F range  2


^
 (left
= 3F range 
2

right
)
a
a ,

stripe =
eq

1i<jk

2+
F

aA

left is the conjunction of (15)(21), for all i with 1  i  k,
^
G


+

2+
lefta ,
F 3 lefta u leftb  2F 2 left 
aA

a6=b
a,bA

^

(15)




2+
F pairi  2(lefta @ lefta ) ,

(16)

aA

 left  2+ 2
2
F (left @ Sleft),

2+
F
2+
F

(17)

 (left @
pairi  2
,
^
 ((Sj left u Sj+1 left) @ left i
2
pairi 
b



Sli left)

li j

j<li
left

pairi  3
i ,

(18)

) ,

(19)
(20)

 ((left u Sleft) @
2F pairi  2

204

left 
S
i ) ,

(21)

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity
b

b

b

b

yn4 b
.
..
.
.
.

b

b

b

.
.
.

.
.
.

.
.
.

b

b

b

yn3 +1 b
yn3 b
.
..
.
.
.

yn2 +1 b
yn2 b
.
..
.
.
.

yn1 +1 b
yn1 b 

.
..
.
.
li
.
 1
b
y1
pairi1
0

b

b

.
.
.

.
.
.

b

b

b

trb


li
 3

b
btr


li
 4

trb

bili4
vi4

. 4
.
.

br

bi14

br

bili3

. 3
.
.

vi3
rb

br

bi13

rb

br

bili2

. 2
.
.


.
.
l
.
 i2
b

rb

rb

br

bi12

trb

rb

rb

br

bili1

vi2

. 1
.
.

vi1
rb

rb

pairi2
1

b = left
r = left
t = left u Sleft

rb

pairi3
2
range

pairi4
3

br

bi11

4

...

Figure 7: Model satisfying left , for N = 4.
where
ileft = leftbi u S leftbi u S(leftbi u    u Sleftbi ) . . .
1

2

3



li

(remember that li is the length of the word vi ). The conjunct right is defined by replacing
in left all occurrences of left with right, lefta with righta (for a  A), li with ri and ileft
with iright , which is defined similarly. (Note that pairi occurs in both left and right .)
Let us prove that A,P is as required. Suppose that (M, 0) |= A,P , for an Aleksandrov
tt-model M = hhN, <i , G, Vi with G = hV, Ri. Since (M, 0) |= eq , we can find an N ,
1  N < , such that
^
 (left
(M, N ) |= range 
2
(22)
a  righta ).
aA

In view of range , we have (M, j) |= range for all j, 0  j  N . Let i1 , . . . , iN be the
sequence of indices such that, for 1  j  N , we have (M, j  1) |= pairij (pair ensures
that there is a unique sequence of this sort). We claim that (14) holds for this sequence.
Since stripe holds in M at 0, we have, for every y  V , (M, h0, yi) |= stripe iff
(M, hj, yi) |= stripe for all j, 0  j  N . Denote by Rs the transitive binary relation
on V defined by taking xRs y if there is z  V such that xRzRy and (M, h0, xi) |= stripe
holds iff (M, h0, zi) 6|= stripe. Then we clearly have that, for every j, 0  j  N , and every
xV,
(M, hj, xi) |= S

iff

there is y  V such that xRs y and (M, hj, yi) |= .

Call a sequence hy1 , . . . , yl i of (not necessarily distinct) points from V an Rs -path in
V(left, j) of length l if y1 , . . . , yl  V(left, j) and y1 Rs y2 Rs . . . Rs yl . For every sequence
205

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

z1 , . . . , zl of points from V(left, j) we define
leftwordj (z1 , . . . , zl ) = ha1 , . . . , al i ,
where the ai are the (uniquely determined by (15)) symbols from A with (M, hj, zi i) |= leftai .
We will show now that, for every j, 1  j  N , the following holds:


ff
(a) there exists an Rs -path y1 , . . . , ynj in V(left, j) of length nj = li1 +    + lij such
that
leftwordj (y1 , . . . , ynj ) = vi1  . . . vij ;
(b) every Rs -path in V(left, j) is of length  nj ;


ff
(c) for every Rs -path y1 , . . . , ynj in V(left, j), we have
leftwordj (y1 , . . . , ynj ) = vi1  . . . vij .
Indeed, for j = 1, we have (a) by (M, 0) |= pairi1 and (20), (b) by (17) and (18), and
(c)

 by (19).ff Now assume inductively that (a)(c) hold for some j, 1  j < N . Let
y1 , . . . , ynj be a maximal Rs -path in V(left, j). First, by (16), y1 , . . . , ynj  V(left, j + 1).


ff
Second, since (M, j, ynj ) |= left u Sleft and (M, j) |= pairij+1 , (21) now implies that
ff


there exist ynj +1 , . . . , ynj +lij+1 such that y1 , . . . , ynj +lij+1 is an Rs -path in V(left, j + 1),
as required in 
(a). For (b) and
ff (c), observe first that for every Rs -path hy1 , . . . , yl i in
V(left, j + 1), y1 , . . . , yllij+1 is an Rs -path in V(left, j), by (18). So l  nj+1 must
hold. If l = nj+1 then leftwordj (y1 , . . . , yllij+1 ) = vi1  . . .  vij by the induction hypothesis, and so leftwordj+1 (y1 , . . . , yllij+1 ) = vi1  . . .  vij by (16). On the other hand,
leftwordj+1 (yllij+1 +1 , . . . , yl ) = vij+1 by (19), and so we have leftwordj+1 (y1 , . . . , yl ) =
vi1  . . . vij  vij+1 , as required.
We can repeat the argument above for the right side as well. For every sequence
z1 , . . . , zl of points from V(right, j), define
rightwordj (z1 , . . . , zl ) = ha1 , . . . , al i ,
where the ai are the uniquely determined elements of A such that (M, hj, zi i) |= rightai .
We then have, for every 1  j  N :


ff
(a0 ) there is an Rs -path y1 , . . . , ymj in V(right, j) of length mj = ri1 +    + rij such that
rightwordj (y1 , . . . , ymj ) = ui1  . . . uij ;
(b0 ) every Rs -path in V(right, j) is of length  mj ;


ff
(c0 ) for every Rs -path y1 , . . . , ymj in V(right, j), we have
rightwordj (y1 , . . . , ymj ) = ui1  . . . uij .
206

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Now, by (15) and (22), we have V(left, N ) = V(right, N ). By (a), there exists an Rs path hy1 , . . . , yl i in V(left, N ) such that l = nN and leftwordN (y1 , . . . , yl ) = vi1  . . . viN .
By (b0 ), we have nN  mN . Similarly, using (a0 ) and (b), we obtain mN  nN , from which
nN = mN . Hence, by (c0 ), rightwordN (y1 , . . . , yl ) = ui1  . . . uiN . Since, by (22),
leftwordN (y1 , . . . , yl ) = rightwordN (y1 , . . . , yl ),
we finally obtain vi1  . . . viN = ui1  . . . uiN , as required.
Conversely, suppose there is an N  1 and a sequence i1 , . . . , iN for which (14) holds.
We will show that A,P is satisfiable in an Aleksandrov tt-model M = hhN, <i , hN, i , Vi
with FSA. Let nj = li1 +    + lij and mj = ri1 +    + rij for every j, 1  j  N . By our
assumption, nN = mN and we have
vi1  . . . viN = ha1 , . . . , anN i = ui1  . . . uiN .
Define a valuation V by taking
 V(range, j) is true iff 0  j  N ,
 V(stripe, j) = {2m | m < , 0  j  N },
 V(pairi , j  1) is true iff i = ij and 1  j  N ,
 V(lefta , j) = {k | 1  k  nj , ak = a} for a  A and 1  j  N ,
 V(righta , j) = {k | 1  k  mj , ak = a} for a  A and 1  j  N ,
S
S
V(righta , j).
V(lefta , j) and V(right, j) =
 V(left, j) =
aA

aA

One can easily check that under this valuation we have (M, 0) |= A,P and M satisfies
FSA. It is also readily seen that A,P is satisfiable in a tt-model based on hN, <i iff it is
satisfiable in a tt-model based on a finite flow of time.
q
Proof of Theorem 3.7. We show this by modifying formulas from the proof of Theorem 3.6. First, we replace stripe with
+

2+
F 2(stripe @ 2F stripe)  2F 2(stripe @ 2F stripe).

Then, left is the conjunction of (150 )(210 ), for all i with 1  i  k,
^
G


+

2+
lefta ,
F 3 lefta u leftb  2F 2 left 
aA

a6=b
a,bA

^

(150 )



2+
F pairi  2(lefta @ 2F lefta ) ,

(160 )

aA

 left  2+ 2
2
F (left @ Sleft),

2+
F
2+
F

(170 )


 (left @ 3 Sli left) ,
pairi  2
F
^
 (left u 2 left) @ 2 ((Sj left u Sj+1 left) @ left i
pairi 
2
F
F
b

li j

j<li
left

pairi  2F 3
i ,

2F

(180 )

) ,

(190 )
(200 )

left 
 ((left u Sleft) @ 2 S
pairi  2
F
i ) ,

207

(210 )

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

where the ileft are defined exactly as in the proof of Theorem 3.6. Formula right is modified
in a similar way.
q
Remark B.3. In fact, the set of PCP instances without solutions is not recursively enumerable and therefore, the proofs above show that the sets of PT L  S4u and PT L2  S4u formulas which are true in all models based on hN, <i, hZ, <i or finite flows of time are not
recursively enumerable either. Therefore, these logics are not recursively axiomatisable.

Appendix C. Spatio-Temporal Logics Based on RC
In this appendix we establish lower and upper complexity bounds for a wide range of
decidable spatio-temporal combinations and, in particular, prove Theorems 3.83.15. We
begin with a straightforward generalisation of Lemma A.1 to the spatio-temporal case:
Lemma C.1. (i) If a PT L  RC-formula  is satisfiable in a tt-model with FSA and based
on a flow of time F then  is satisfiable in an Aleksandrov tt-model based on F and a finite
disjoint union of finite brooms.
(ii) If a PT L  RC-formula  is satisfiable in a tt-model based on a flow of time F then 
is satisfiable in an Aleksandrov tt-model based on F and a (possibly infinite) disjoint union
of -brooms.
Proof. (i) By Lemma B.1 (i),  is satisfiable in an Aleksandrov tt-model based on F and a
finite quasi-order G. The rest of the proof is similar to that of Lemma A.1. Further details
are left to the reader.
(ii) By Lemma B.1 (ii),  is satisfiable in an Aleksandrov tt-model based on F and a
quasi-order G = hV, Ri. The rest of the proof again is similar to that of Lemma A.1. We
only note that although G can be infinite, still for every x  V there is a y  V0 such that
xRy. This is guaranteed by the condition that the set Aw,x,> has a maximal point.
q
Observe that Aleksandrov spaces are essentially infinite in case (ii) of Lemma C.1 and a
generalisation of Lemma A.2 does not go through. First, we can easily enforce a topological
space to be infinite using the PT L  RCC-8 formula

2+
F NTTP(p, p).

Moreover, the formula
 
 
   

( p
p )
3
u I p )  2+
F 2( p @


2+
F2



 
  

 
p u I p @ I p u p u p

is satisfied in an Aleksandrov tt-model based on a single -broom, but cannot be satisfied
in an Aleksandrov tt-model based on a union of n-brooms for any finite n.
On the other hand, Aleksandrov tt-models based on disjoint unions of n-brooms, where
n is bounded by the width of the formula, are enough for spatio-temporal logics based on
RC  . Recall that spatial terms  of PT L  RC  (and PT L  RC  ) are defined as follows


::= %

| ,



::= I%

| 



::= 1 u    u m

| 1 u 2 ,

208

| u

| ,

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

where the % are spatio-temporal Boolean region terms of PT L  BRCC-8 (and PT L 
BRCC-8, respectively). It is not hard to see that, for every tt-model M = hF, T, Vi with
F = hW, <i, T = hU, Ii and every w  W , we have
V(, w) = CIV(, w)

and

V(, w) = ICV(, w),

(23)

i.e., the  are always interpreted by regular closed sets, whereas the  by regular open ones.
We define the width w() of a PT L  RC  -formula  as the maximal number m of
 ( u    u 
conjuncts in its subformulas of the form 2
1
m ), if such subformulas exist, and put
w() = 1 otherwise.
Lemma C.2. (i) If a PT L  RC  -formula  is satisfiable in a tt-model with FSA and
based on a flow of time F then  is satisfiable in an Aleksandrov tt-model based on F and a
finite disjoint union of w()-brooms.
(ii) If a PT L  RC  -formula  is satisfiable in a tt-model based on a flow of time F
then  is satisfiable in an Aleksandrov tt-model based on F and a (possibly infinite) disjoint
union of w()-brooms.
Proof. By Lemma C.1, we may assume that  is satisfied in an Aleksandrov tt-model M =
hF, G, Vi, where F = hW, <i and G = hV, Ri is a disjoint union of brooms (in (i), the union
and the brooms are finite). Without loss of generality we may assume that  is composed
 ,...,3
  },7
(using temporal operators and the Booleans) from formulas of the set  = {3
1
n
where every i has one of the following forms
1 u    u m ,

u

or

,

(24)

and the i ,  and  are as defined above.
  
  , we fix a point x
For every 3
 and every w  W with (M, w) |= 3
,w  V such
that (M, hw, x,w i) |=  . We may assume that the x,w are pairwise distinct and that the
   .
roots of all the brooms are the points of the form x,w for some w  W and 3

  
Therefore, G is a disjoint union of brooms b,w , for 3
 and w  W .
Let us construct a model M0 = hF, G0 , V0 i as follows. Given a broom b,w , we delete
some of its leaves depending on the form of  . Three cases are possible:
Case  = 1 u    u m : take m leaves y1 , . . . , ym of b,w such that (M, hw, yi i) |= i and
x,w Ryi for i = 1, . . . , m and remove all leaves different from y1 , . . . , ym .
Case  =  u : take a leaf y of b,w such that (M, hw, yi) |=  and x,w Ry and remove
all other leaves. Note that, by (23), we have (M, hw, yi) |= , and therefore (M, hw, yi) |=  .
Case  = : take a leaf y of b,w such that x,w Ry and remove all other leaves. By (23),
we have (M, hw, yi) |=  .
Denote by b0,w the resulting broom. Clearly, it is a w()-broom. Let G0 = hV 0 , R0 i be
0
  
the disjoint union of all b,w , for 3
 and w  W . It should be clear that G is as
0
required. Finally, we define V by taking for every spatial variable p, every w  W and
every x  V 0 ,
x  V0 (p, w)
7. We treat

iff

there is y  V 0 of depth 0 such that xR0 y and y  V(p, w).

 as an abbreviation.
 as primitive and 2
3

209

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

   ,
To show that  is satisfied in M0 , we first prove that, for all w  W and all 3


(M0 , w) |= 3

iff

 .
(M, w) |= 3

It is readily proved by induction that we have (M0 , hw, xi) |=  iff (M, hw, xi) |=  , for all
points x  V 0 of depth 0, all w  W and all spatio-temporal terms  .
  
Then, by the construction, we also have that, for all formulas 3
 and all w  W ,
0


  implies
(M, w) |= 3 implies (M , w) |= 3 . So it remains to show that (M, w) |= 3
0
0
  for all 3
  
(M , w) |= 3
 and all w  W . Suppose that we have (M , hw, xi) |=  and
  . Consider three possible cases for  :
(M, w) |= 3
Case  = 1 u    u m . Then, for every i, 1  i  m, there is yi  V 0 of depth 0 such
that xR0 yi and (M0 , hw, yi i) |= i . But then (M, hw, yi i) |= i and, by (23), (M, hw, xi) |= i .
.
Therefore, (M, hw, xi) |=  , contrary to (M, w) |= 3
0
Case  =  u . Then there is y  V of depth 0 such that xR0 y, (M0 , hw, yi) |=  and,
by (23), (M0 , hw, yi) |= . Thus (M0 , hw, yi) |=  . But then (M, hw, yi) |=  , contrary to
.
(M, w) |= 3
Case  = . Then there is y  V 0 of depth 0 with xR0 y and, by (23), (M0 , hw, yi) |=  .
.
But then (M, hw, yi) |=  , contrary to (M, w) |= 3
Now, by a straightforward induction we can easily show that, for all w  W and all
formulas  built from  using the temporal operators and the Booleans,
(M0 , w) |= 

iff

(M, w) |= .

It follows that  is satisfied in M0 .

q

C.1 Lower Complexity Bounds (I)
Proof of Theorem 3.10, lower bound. The proof is by reduction of an arbitrary problem in 2EXPSPACE to the satisfiability problem of PT L  RC. Let A be a (single-tape,
deterministic) Turing machine such that A halts on every input (accepting or rejecting it),
f (n)
and A uses  22
cells of the tape on any input of length n, for some polynomial f . Given
any such Turing machine A and an input x for it, we will construct a PT L  RC-formula
A,x (using only future-time temporal operators) such that
(i) the length of A,x is polynomial in the size of A and x;
(ii) if A,x is satisfiable in a tt-model based on hN, <i then A accepts x; and
(iii) if A accepts x then A,x is satisfiable in a tt-model with FSA and based on hN, <i.
The case of hZ, <i as a flow of time (with or without FSA) follows immediately. The case
of finite flows of time can be proved by relativising the temporal operators of A,x (say,
by a propositional variable range as in the proof of Theorem 3.6 in Appendix B.2 and in
the proof of the lower bound of Theorem 3.9 below): we can obtain a formula 0A,x such
that 0A,x is satisfiable in an Aleksandrov tt-model based on hN, <i iff it is satisfiable in an
Aleksandrov tt-model based on the same quasi-order but on a finite flow of time. So this
way all the lower bound results of this theorem follow.
210

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Given a Turing machine A, polynomial f , and input x = hx1 , . . . , xn i as above, let
d = f (n),
exp(1, d) = d  2d and exp(2, d) = exp(1, d)  2exp(1,d) .
Then we have
22

f (n)

 exp(2, d).

(25)

Our plan is as follows. First, we will show that yardsticks of length exp(2, d) (similar to
those used by Stockmeyer, 1974 or Halpern and Vardi, 1989) can be encoded by PT L  RCformulas of length polynomial in d. These yardsticks will be used to define a temporal
operator exp(2,d) . Then, using this operator, we will encode the computation of A on
input x.
By Lemma C.1 (ii), if a PT LRC-formula A,x is satisfied in a tt-model based on a flow
of time hN, <i, then it is satisfied in an Aleksandrov tt-model M = hhN, <i , G, Ui, where
G = hV, Ri is a disjoint union of -brooms. Take such a model M and suppose that the
PT L  RC-formula8

 


aux
(26)
2+
F 2 aux 


is true in M at moment 0. Since region aux does not change over time, we can divide all
points

 in V into three disjoint sets: external, boundary and internal points with respect to
aux i.e., those satisfying








ep(aux) = aux ,
bp(aux) = aux u I aux
and
I aux ,
respectively. Note that every boundary point has a non-boundary R-successor, so boundary
points can only be of depth 1. In what follows

 we simply speak about external and boundary
points not mentioning with respect to aux .
We define the exp(2,d) operator by a PT L  RC-formula of length polynomial in d as
follows:
(a) First, we encode yardsticks of length d. We will use different formulas for yardsticks
on external points and for yardsticks on boundary points.
(b) Then, with the help of d-yardsticks, we encode yardsticks of length exp(1, d). We
will again use different formulas for external and boundary points.
(c) Next, with the help of exp(1, d)-yardsticks on both boundary and external points, we
encode yardsticks of length exp(2, d) on boundary points.
(d) Finally, with the help of exp(2, d)-yardsticks on boundary points, we define a polynomial-length exp(2,d) operator applicable to propositional variables.
Step (a). Suppose that (26) and the following formula hold in M at 0:


+
ext

2+
F 2 bp(aux) @ 0,d  2F 2 ep(aux) @ 0,d
where
0,d =



(27)


l

 

 d1
j delim0
,
delim0  d delim0 u
delim0 @
j=1

8. Recall that 1  2 stands for (1 @ 2 ) u (2 @ 1 ). We assume that u and t bind stronger than .

211

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

ext results from 
and 0,d
0,d by replacing each occurrence of delim0 with ext delim0 .


Take a boundary point z. Suppose that v  N is such that (M, hv, xi) |= delim0 . By
0,d , for every time moment w  v,


(M, hw, zi) |= delim0
iff
w  v (mod d),


that is, on z, delim0 holds once inevery
 d time instants, starting from v.By the second

conjunct of (27), external points of aux behave similarly with respect to ext delim0 .
Step (b). To encode yardsticks of length exp(1, d), recall first that every number a < 2d
can be represented in binary by asequence
a0 . . . ad1 of bits. We will mark the bits of

binary numbers by a region term bit1 as
 follows. Given a boundary point z and a time
moment v such that (M, hv, zi) |= delim0 , we say that an interval [w, w + d  1], for some
w = v + j  d, j  N, encodes a number a < 2d on z, if for every i < d,


(M, hw + i, zi) |= bit1
iff
ai = 1.

Recall that the binary representation b0 . . . bd1 is the successor of a0 . . . ad1 modulo
the following holds: for all i, 0  i < d, we have ai = bi iff aj = 0, for some j, i < j
We will use the d-intervals starting from v to encode < 2d numbers in such a way
consecutive intervals encode consecutive (modulo 2d ) numbers, starting from 0.
So, suppose that (26), (27) and the following formula hold in M at 0:


ext
ext


 2+
2+
F 2 ep(aux) @ 1,d u 1,d ,
F 2 bp(aux) @ 1,d u 1,d

2d if
< d.
that

(28)

where
1,d

=

1,d

=







 

lwr1  delim0 t bit1 u lwr1
u
 



 




  
zr1  bit1 u delim0 t zr1
u delim1  delim0 u zr1 ,



 

lwr1 
bit1  d bit1 ,

ext and  ext result from 
and both 1,d
1,d and 1,d , respectively, by attaching prefix ext to all
1,d
of their spatial variables (save aux).


Take a boundary point z. Suppose that v  N is such
 that (M, hv, zi) |= delim1 .
Then, by the last conjunct of 1,d , we have (M, hv, zi) |= delim0 . Since, by (a), delim0
holds once in every d time instants on z, delim0 marks the starting moment of each
d-interval. Then, by the first conjunct of 1,d , for every i, 0  i < d, we have9




(M, hv + i, zi) |= lwr1
iff
(M, hv + j, zi) |= bit1 , for all j, i < j < d.

Therefore, 1,d says that consecutive < 2d numbers (starting with 0) are encoded by consecutive d-intervals (starting from v). Similarly to the first conjunct of 1,d , its second conjunct
ensures that, for every i, 0  i < d,
 


(M, hv + i, zi) |= zr1
iff
(M, hv + j, zi) |= bit1 , for all j, i  j < d.
9. Since
apply the U operator

we cannot

  to form
 spatio-temporal
 terms,
 auxiliary regions

 are
 used instead:

lwr1  delim0 t bit1 u lwr1 ensures that lwr1 behaves as bit1 U delim0 . This
equality term can indeed be regarded as a fixed point characterisation of the U operator.
Note

 also that


we do not need to require (as we should do for the fixed point characterisation) lwr1 @ 3F delim0 to
be true because the eventuality is already enforced by 0,d .

212

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity



So, by the last conjunct of 1,d , delim1 holds on z once in every exp(1, d) = d  2d time
instants, starting
 from v. By
 the second conjunct of (28), external points behave similarly
with respect to ext delim1 .
Step (c). Now we construct yardsticks of length exp(2, d), using the exp(1, d)-yardsticks
constructed in (b). Suppose that (26)(28) and the following formulas hold in M at 0:

2+
F 2 bp(aux) @






ext delim1
 2+
@ (ep(aux) t bp(aux)) ,
F 2 ext delim1


2+
F 2 ep(aux) @ 1,d (bit2 ) ,


2+
F 2 bp(aux) @ 2,d u 2,d ,


(29)
(30)
(31)

where 2,d is defined similarly to 1,d and
1,d (bit2 )

=



jm1 bit2







 

 

delim1 u bit2
t ext delim1 u jm1 bit2 ,









=
bit2  I bit2
u lwr2 
bit2  J1,d bit2 ,


= I (aux u ext delim1 ) @ jm1 bit2 .
ext

2,d
J1,d bit2



Take a boundary point z. Suppose v is a time moment such that (M, hv, zi) |= delim2 .
Then, by the last conjunct of 2,d , (M, hv, zi) |= delim1 . We know from (b) that delim1
holds on z once in every exp(1, d) time instants starting from v. So, by 2,d and the first
conjunct of 2,d we intend to express that consecutive < 2exp(1,d) numbers (starting with
0) are encoded by consecutive
 exp(1, d)-intervals starting from v. If we could do this then,
by the last conjunct of 2,d , delim2 would hold on z once in every exp(2, d) time instants
starting from v. The only problem (and the only
from step (b)) is that to mark
 difference

exp(1,d)
the bits of < 2
binary numbers by a term bit2 , we need to show that the (polynomial


length) term J1,d bit2 actually defines exp(1,d) bit2  in the sense that, for every w  v,
(M, hw, zi) |= J1,d bit2



ff


(M, w + exp(1, d), z ) |= bit2 .

iff

(32)

Suppose first that (M, hw, zi) |= J1,d bit2 . Then,
by (29),
yw
 there is an external R-successor


of z (of depth 0) such that (M, hw, yw i) |= ext delim1 , and so (M, hw, yw i) |= jm1 bit2 .
On the other hand, it is not hard to see that if (M, hw, zi) 6|= J1,d bit2 , then there
 is
0
0
an external R-successor yw of z (of depth 0) such that (M, hw, yw i) |= ext delim1 but


0 i) |= jm bit .
(M, hw, yw
2
1


In both cases, it is readily checked that if (M, hw, yi) |= ext delim1 , for some external
point y, then, by (30),


(M, hw, yi) |= jm1 bit2



ff


(M, w + exp(1, d), y ) |= bit2 .

iff

Now (32) follows by the first conjunct of 2,d .
Step (d). We are now in a position to define a polynomial-length exp(2,d) operator J2,d
applicable to propositional variables. Recall that a propositional variable p stands for spatial
 p, where p is a spatial variable associated with p. Now, for every propositional
formula 2
213

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

variable p we intend to apply the new operator to, we introduce a fresh spatial variable
jm2 p. Suppose that (26)(31) and the following formulas hold in M at 0:



2+
F 3 bp(aux) u delim2 ,
 
 



2+
 2+
F 2 p  2 p
F 2 2,d (p),

(33)
(34)

where 2,d (p) is obtained by replacing bit2 , jm1 bit2 and ext delim1 in 1,d (bit2 ) with p,
jm2 p and delim2 , respectively. Let




 (bp(aux) u
delim2 ) @ jm2 p .
J2,d p = 2
We claim that, for every time moment w and every propositional variable p,
(M, w) |= J2,d p

(M, w + exp(2, d)) |= p.

iff

(35)

Suppose first that (M,
point z such

 w) |= J2,d p. Then, by (33), there is a boundary
that (M, hw, zi) |= delim2 , and therefore (M, hw, zi) |= jm2 p . On the other hand,

if (M, w) 6|= J2,d p, then there is a boundary point z 0 with (M, hw, z 0 i) |= delim2 but




(M, hw, z 0 i) |= jm2 p . In both cases, it is readily checked that if (M, hw, zi) |= delim2 ,
for some boundary point z, then, by the second conjunct of (34),


(M, hw, zi) |= jm2 p



ff
 
(M, w + exp(2, d), z ) |= p .

iff

Now (35) follows by the first conjunct of (34).
Finally, we are in a position to define the PT L  RC-formula A,x that encodes the
computation of Turing machine A on input x. Let A be the tape alphabet (with the blank
symbol b  A) and S the set of states (with two halt states syes and sno in S) of A. We use
the symbol  
/ A to mark the left end of the tape. We know that the space used by A on
f (n)
input x = hx1 , . . . , xn i is  22 , which is  exp(2, d) by (25). So we can represent each
configuration of the computation of A on x as a finite word
h, a1 , . . . , ai1 , hs, ai i , ai+1 , . . . , am , b, . . . , bi
of length exp(2, d), where a1 , . . . , am  A and hs, ai i  S  A represents the current state
and the active cell. The transition function  of A takes triples of the form hai , hs, aj i , ak i
(for ai  A  {}, aj , ak  A, s  S  {syes , sno }) to similar triples. For instance,
(ai , hs, aj i , ak ) = hai , aj , hs0 , ak ii
means that, when being in state s and reading symbol aj , the new state should be s0 and
the head should move one cell to the right. We also assume that, for all ai  A  {} and
aj , ak  A, we have (ai , hsyes , aj i , ak ) = hai , aj , ak i and (ai , hsno , aj i , ak ) = hai , aj , ak i
meaning that the head is removed after A is being halted.
Now, for every   A  {}  (S  A), we introduce a fresh propositional variable p .
Let A,x be the conjunction of (26)(31), (33) and an instance of (34), for each p , as well
214

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

'

$ '

exp(1, d) many l0j
exp(1,d)
 l0

c
c
r
c
c
c 
H
1
Y
*

HH@



I

6
@


ep(aux)
I aux

HH


@ 
r0 - rd
bp(aux)

&

...

j
exp(1, d) many lexp(2,d)1
exp(1,d)
lexp(2,d)1


c
r
c
c
c
c 
H
1
Y
*

HH@



I

6
@

ep(aux)
I aux

HH


@ rd

-

rexp(2,d)1
% &

|

$

bp(aux)

{z
exp(2, d) many (exp(1, d) + 1)-brooms

%

}

Figure 8: Structure of yardsticks.
as the following formulas:
^

2+
F p  p ,

(36)

,A{}SA
6=

p  (phs0 ,x1 i  (px2  (    (pxn  pb U p )    ))),
2+
F



af

head 

_


phs,ai ,

(37)
(38)

hs,aiSA

^

2+
F



af




head  p  J2,d p0   p  J2,d p 0   p  J2,d p 0 , (39)

(,,)=
h0 , 0 , 0 i


2+
F 

^

af



head  af head  af head  pa  J2,d pa ,

(40)

aA{}

3F

_

phsyes ,ai  3F

aA

_

phsno ,ai .

(41)

aA

Suppose first that A,x holds in M at time moment 0. By (36)(40) and (35), the consecutive
configurations of the computation of A on input x are properly encoded along the time
axis. (For instance, p holds once in every exp(2, d) time moments.) Finally, (41) says that
A accepts input x.
Conversely, suppose that A accepts input x. We will define an Aleksandrov tt-model
M = hhN, <i , G, Ui with FSA that satisfies A,x . Let the partial order G = hV, Ri be a
disjoint union of exp(2, d) many (exp(1, d) + 1)-brooms (see Fig. 8):
V = {ri | i < exp(2, d)}  {lij | i < exp(2, d), j  exp(1, d)},
zRy

iff

z=y

or z = ri , y = lij , for some i, j.

Suppose that the number of steps in the computation of A on x is m. Then M will have
a prefix of length N = m  exp(2, d) after which the final configuration (without a halting
state) repeats to infinity. For w  N, let
exp(1,d)

U(w, aux) = {li

215

| i < exp(2, d)}.

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Then it is easy to see that the boundary points are the ri , and the external points are the
lij , for i < exp(1, d). Now put, for every w  N,
U(w, delim2 ) = {li

exp(1,d)

|iw

(mod exp(2, d))},

exp(1,d)
U(w, delim1 ) = {li
exp(1,d)
U(w, delim0 ) = {li
U(w, ext delim1 ) = {liv |
U(w, ext delim0 ) = {liv |

|iw

(mod exp(1, d))},

|iw

(mod d)},

vw

(mod exp(1, d)), i < exp(2, d)},

vw

(mod d), i < exp(2, d)}.

The valuations for the other variables should be clear. We then have


exp(1,d)
(M, hw, zi) |= delim2
iff z = ri or z = li
for some i  w (mod exp(2, d)),


exp(1,d)
(M, hw, zi) |= delim1
iff z = ri or z = li
for some i  w (mod exp(1, d)),


v
(M, hw, zi) |= ext delim1
iff z = ri or z = li for some v  w (mod exp(2, d))
and i < exp(2, d),
and so on, as required. It is not hard to see that M satisfies FSA and (M, 0) |= A,x .

q

Proof of Theorem 3.9, lower bound. The proof is by reduction of the 2n -corridor tiling
problem which is known to be EXPSPACE-complete (Chlebus, 1986; van Emde Boas, 1997).
The problem can be formulated as follows: given an instance T = hT, t0 , t1 , ni, where T is
a finite set of tile types, t0 , t1  T and n > 0, decide whether there is an m  N such that
T tiles the m  2n -grid (or corridor) in such a way that t0 is placed onto h0, 0i, t1 onto
hm  1, 0i, and the top and bottom sides of the corridor are of some fixed colour, say, white.
Suppose T = hT, t0 , t1 , ni is given. Our aim is to construct (using only future-time
temporal operators) a PT L  BRCC-8 formula T such that
(i) the length of T is a polynomial function of |T | and n;
(ii) if T is satisfiable in a tt-model based on hN, <i then there is m  N such that T tiles
the m  2n -corridor;
(iii) if there is m  N such that T tiles the m  2n -corridor, then T is satisfiable in a
tt-model with FSA and based on hN, <i;
(iv) T is satisfiable in a tt-model based on hN, <i iff it is satisfiable in a tt-model based
on a finite flow of time.
The case of hZ, <i follows immediately.
Recall that, by Lemma C.2 (ii), if T is satisfied in a tt-model then it is satisfied in
an Aleksandrov tt-model M = hhN, <i , G, Vi, where G = hV, Ri is a disjoint union of brooms. To explain the meaning of T s subformulas, we assume that such a model M is
given. Throughout the proof we use only a restricted subset of RCC-8 predicates: for spatiotemporal terms 1 and 2 constructed from spatial variables using only the complement, the
intersection and the next-time operator , we need EQ(1 , 2 ) as well as two abbreviations
P(1 , 2 ) = EQ(1 , 2 )  TPP(1 , 2 )  NTPP(1 , 2 ) and E 1 = DC(1 , 1 ) standing for 1
216

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

m  2n

0

(m + 1)  2n

count
range

0



0

1



1

2



2

3



4

3



0

5

6



1



2

7



3

8

9



0



1

10



2

11



3

12



0

13



1

14



2

15



3

16

...



0

Figure 9: Counting formulas for m = 3 and n = 2.
is a part of 2  and 1 is nonempty, respectively. Clearly, this language forms a subset of
PT L  BRCC-8 (in fact, as we show in Remark C.3 below, the proof goes through for an
even more restricted subset of the langauge).
Our first step in the construction of T (which will contain, among many others, spatial
variables t for all t  T ) is to write down formulas forcing a sequence y0 , y1 , . . . , ym2n 1
of distinct points (of depth 0) from V , for some m  N, such that, for each i < m  2n ,
(M, hi, yi i) |= t for a unique tile type t  T . If i = k  2n + j, for some k < m, j < 2n ,
then we will use yi (at time i) to encode the pair hk, ji of the m  2n -grid. Thus, the up
neighbour hk, j + 1i of hk, ji will be coded by the point yi+1 at time i + 1, while its right
neighbour hk + 1, ji by yi+2n at moment i + 2n (see Fig. 10).
Let q0 , . . . , qn1 be pairwise distinct propositional variables and
d

n1
j = qd00      qn1
,

where dn1 . . . d0 is the binary representation of j < 2n , q0i = qi and q1i = qi , for each i.
Suppose that the formula



+
count  0  count U (0  2+
(42)
F count)  2F count  
is true in M at 0, where count is a fresh propositional variable and  is the following
counting formula (the length of which is polynomial in n)

^

^  ^
^

qi  qk 
=
qi  qk 
(qi  qi )
 2n 1  0 .
k<n

i<k

i<k

k<i<n

Then there is an m  N such that count is true before moment (m + 1)  2n and false starting
from (m+1)2n . The sequence 0 , 1 , . . . , 2n 1 is repeated m+1 times along the time-line,
i.e., while count is true. Let
range = 3F (count  0 ).
Clearly, range is true before moment m  2n and then always false (see Fig. 9).
Let equ, p0 , . . . , pn1 and e0 , . . . , en1 be fresh distinct spatial variables, and
d

n1
j = pd00 u    u pn1
,

where dn1 . . . d0 is the binary representation of j < 2n , p0i = pi and p1i = pi , for each i.
Suppose that (42) and
l 
^
^


+

2+
e

2
2+
EQ
equ,
q

EQ(e
,
p
)

(43)
i
i
i
i
F
F
F EQ pi , pi
i<n

i<n

i<n

217

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

are true in M at 0. Then, by the first two conjuncts of (43), for all i  N and y  V of
depth 0, there is j < 2n such that (M, hi, yi) |= equ iff (M, i) |= j and (M, hi, yi) |= j . By
the last conjunct of (43), we then have


(M, hi, yi) |= equ iff j < 2n (M, i) |= j and (M, hk, yi) |= j , for all k  N . ()
We can generate the required sequence of points yi using the formulas:
range  2+
(range  E tile),
 F

G  l
2+
EQ
tile,
equ
u
t
u
no
t
in
future
,
F
tT

^

2+
F P no t in future,

(44)
(45)

tT
t


u no t in future ,

(46)

tT

where tile and the no t in future (for all t  T ) are fresh spatial variables. Indeed, suppose
the conjunction of (42)(46) holds at time 0 in M. Then, by the first conjunct of (44) and
(42), (M, 0) |= range  0 and, by the second conjunct of (44), (M, h0, y0 i) |= tile for some
y0  V . We may assume that y0 is of depth 0. Then, by (45), we have
(a0 ) (M, h0, y0 i) |= equ, and, by (), (M, hk, y0 i) |= 0 for all k  N;
(b0 ) for all t  T , (M, h0, y0 i) |= no t in future and, by (46), (M, hk, y0 i) |= t for all k > 0.
Next, by (42), we have (M, 1) |= range  1 and, by (44), there is y1  V (again, of depth
0) such that (M, h1, y1 i) |= tile. In particular, we have:
(a1 ) (M, h1, y1 i) |= equ, and, by (), (M, hk, y1 i) |= 1 for all k  N;
(b1 ) for all t  T , (M, h1, y1 i) |= no t in future and, by (46), (M, hk, y1 i) |= t for all k > 1.
By (b0 ), (M, h1, y0 i) |= t, for all t  T , and thus y1 6= y0 . Now we consider y1 at moment 1
and use the same argument to find a point y2  V (which is different from y1 by (b1 )), and
so forth; see Fig. 10. This gives us points y0 , y1 , . . . , ym2n 1 (of depth 0) from V we need.
Our next aim is to write down formulas that could serve as pointers to the up and right
neighbours of a given pair in the corridor (at this moment we do not bother about its top
border). Consider the formulas

tile ,
2+
(47)
F EQ up,

+
2F EQ right, equ u no equ U tile ,
(48)

+
2F EQ no equ U tile, tile t  equ u no equ U tile ,
(49)
where up, right and no equ U tile are fresh spatial variables.
i, j < m  2n ,
 (M, hi, yj i) |= up
 (M, hi, yj i) |= right

iff

j = i + 1;
iff

j = i + 2n .
218

We claim that, for all

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

b = t for all
q q= no equ U

range

V
3 

y11

q

q

q

equ
q

q

q

q

2 

y10

q

q

equ
q

q

q

q

1 

y9

q

equ
q

q

q

q

0 

q
y8 equ

q

q

q

3 

y7

q

q

q

equ
q
right

equ
q
right

equ
q
right

equ
q
right

equ
q
right

2 

y6

q
q

equ
q
right

1 

y5

q

0 

y4

equ
q
right

3 

y3

q

q

2 

y2

q

up
q

1 

y1

up
q

0 

r
y0 equ
tile
0



0

q



equ
r
tile

q

q

q

q

q

up
q



equ
q
right

up
r
q equ
b
tile
up
r
q
q
q equ
b
b
tile
up
r
q
q equ
b
b
b
tile
up
r
q equ
b
b
b equ
b
tile
equ
r
b
b
b equ
b
b
tile
b
b
b equ
b
b
b

q

q

b

b

equ
b

b

b

b

equ
b

b

b

b

equ
b

1

2

3

4

5

6

7

8

9

10

11

12



6
right

6

up
r
q
q
q equ
tile
up
r
q
q
q equ
b
b
b equ
b
b
b
b
tile
up
q
q equ
b
b
b equ
b
b
b
b equ
b
r
tile
up
r
q equ
b
b
b equ
b
b
b
b equ
b
b
tile
equ
r
b
b
b equ
b
b
b
b equ
b
b
b
tile
b
b
b equ
b
b
b
b equ
b
b
b
b

b

1



2



3



0



1



2



3



0

tT
tile



1



2



3



up

6 6
s


up

6
c- right

... |

{z

}

3  22 corridor

0

Figure 10: Satisfying T , n = 2, in a tt-model based on space with 3  22 points.
The former is obvious. Let us prove the latter. To show that (M, hi, yj i) |= right, for
j = i + 2n , we first observe that (M, hj, yj i) |= equ and (M, hi, yj i) |= equ by (). It follows
from (M, hj, yj i) |= tile by (49) that (M, hj  1, yj i) |= no equ U tile. Then, applying
(49) (from right to left) sufficiently many times, we obtain (M, hi, yj i) |= no equ U tile,
(M, hi  1, yj i) 6|= no equ U tile, and so (M, hi, yj i) |= right.
Conversely, suppose that (M, hi, yj i) |= right for some yj . Then (M, hi, yj i) |= equ and,
by () (note that i + 2n < (m + 1)  2n , and so count is still true at i + 2n ),
(M, hi + 2n , yj i) |= equ.

()

We have (M, hi, yj i) |= no equ U tile. Then applying (49) (from left to right) sufficiently
many times we arrive at (M, hi + 2n  1, yj i) |= no equ U tile which together with ()
gives (M, hi + 2n , yj i) |= tile. But then j = i + 2n .
It should be noted that at every time point the extension of no equ U tile coincides with
the extension of the term equ U tile on elements of the sequence y0 , . . . , ym2n , and that (49)
is indeed the fixed point characterisation of this U operator.
Finally, the formulas below ensure that every point of the m  2n -corridor is covered by
at most one tile, h0, 0i is covered by t0 , hm  1, 0i by t1 , the top and bottom sides are white
219

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

and that the colours on adjacent edges of adjacent tiles match:
^
0
2+
F E (t u t ),

(50)

t,t0 T
t6=t0





P(tile, t0 )  2+


range

3


range

P
tile,
t
,
0
0
1
F
F


_

n
2+
P tile, t ,
F 2 1 

(51)
(52)

tT
up(t)=white


_

2+


P
tile,
t
,
0
F

(53)

tT
down(t)=white

^



0
n
t
in
future
,
2+


E
t

P
up,
no
2 1
F

(54)

t,t0 T

up(t)6=down(t0 )

^



0
.
E
t

P
right,
no
t
in
future
2+
F

(55)

t,t0 T
right(t)6=left(t0 )

Let T be the conjunction of (42)(55). Suppose that T holds at 0 in M. Then there
is m  N such that (M, m  2n  1) |= range and, for every i  m  2n , (M, i) |= range.
Then we define a map  : m  2n  T by taking
 (k, j) = t

iff

(M, hi, yi i) |= t and i = k  2n + j.

We leave it to the reader to check that  is a tiling of m  2n , as required.
For the other direction, suppose that there is a tiling  of the m  2n -corridor by T , for
some m > 0. Then T is satisfied in the Aleksandrov tt-model M = hhN, <i , hV, Ri , Vi,
where V = {y0 , . . . , ym2n 1 }, R is the minimal reflexive relation on V ,
V(t, i) = {yi  V |  (k, j) = t and i = k  2n + j},
and the other variables of T are interpreted as shown in Fig. 10. Clearly, M satisfies
FSA. Moreover, T is satisfiable in tt-models over finite flows of time iff it is satisfiable in
tt-models over hN, <i. Details are left to the reader.
q
Remark C.3. It may be of interest to note that the language used in the proof above is
rather limited. In fact, it is enough to extend the PSPACE-complete logic PT L  RCC-8
with predicates of the form EQ(%1 , %2 t %3 ) (where the %i are atomic spatio-temporal region
terms) to make it EXPSPACE-hard. To show this, we transform the PT LBRCC-8 formula
T constructed above in the following way. First, we take a fresh spatial variable u (denoting

the universe) and add to T the conjunct 2+
F EQ(u, u). Next, for every spatio-temporal
Boolean region term % of T , we introduce a spatial variable neg % (the complement of %
+
with respect to u), add to T conjuncts 2+
F EQ(u, % t neg %)  2F DC(%, neg %), and replace
every occurrence of % in the resulting formula with neg %. Finally, for every spatio-temporal
term of the form %1 u %2 , we introduce a fresh spatial variable %1 and %2 , add the conjuncts
+
+
2+
F P(%1 and %2 , %1 )  2F P(%1 and %2 , %2 )  2F P(%1 , neg %2 t %1 and %2 )

220

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

and replace occurrences of %1 u %2 with %1 and %2 . One can readily see that (i) the length of
the resulting formula 0T is linear in the length of T and (ii) 0T is satisfiable in a tt-model
based on hN, <i (with FSA) iff T is satisfiable in a tt-model based on hN, <i (with FSA).
C.2 Upper Complexity Bounds (I): Quasimodels for PT L  RC
In this appendix we define quasimodels for PT L  RC in the spirit of the paper (Hodkinson
et al., 2000) in order to establish the upper complexity bounds of Theorems 3.10 and 3.13.
We remind the reader that spatio-temporal terms of PT L  RC are of the form:
| I%



::= %

%

::= CIp

| 

| CI%

| 1 u 2 ,
| CI(%1 u %2 ) | CI(%1 U %2 ) | CI(%1 S %2 ),

and that PT L  RC forms a sublanguage of PT L  RCit differs from the latter only in
the definition of spatio-temporal region terms:
%

::=

CIp

| CI%

| CI(%1 u %2 ) | CI%.

Let  be a PT L  RC-formula. Recall from p. 200 that by sub  we denote the set of
all subformulas of  and by term  the set of all its spatio-temporal terms including those
of the form  and %. A type t for  is a subset of term  such that
 for every 1 u 2  term ,
 for every   term ,

1 u 2  t

 t

iff

iff

1  t and 2  t;


/ t.

Clearly, the number [() of different types for  is bounded by 2|term | .
A broom type b for  is a pair hhT, i , ti, where hT, i is a broom (with T 0 being its
leaves) and t a labelling function associating with each x  T a type t(x) for  such that
the following conditions hold:
(bt0) t(x) 6= t(y), for each pair of distinct points x, y  T 0 ;
(bt1) for every x  T 0 ,
 for every CI(%1 u%2 )  term , CI(%1 u%2 )  t(x) iff %1  t(x) and %2  t(x),
 and for every CI%  term , CI%  t(x) iff % 
/ t(x);
(bt2) for every I%  term ,
(bt3) for every %  term ,

I%  t(x)
%  t(x)

iff
iff

%  t(y) for every y  T , x  y;
y  T 0 with x  y and %  t(y).

Broom types b1 = hhT1 , 1 i , t1 i and b2 = hhT2 , 2 i , t2 i for  are said to be isomorphic if
 for every x1  T10 , there is x2  T20 such that t1 (x1 ) = t2 (x2 ) and
 for every x2  T20 , there is x1  T10 such that t1 (x1 ) = t2 (x2 ).
Clearly, given two isomorphic broom types b1 and b2 , we also have t1 (r1 ) = t2 (r2 ), where
r1 and r2 are the roots of b1 and b2 , respectively.
A quasistate for  is a pair hs, mi, where s is a Boolean-saturated subset of sub  and m a
disjoint union hhT, i , ti of broom types b1 , . . . , bn for  such that the following conditions
hold:
221

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

(qs0) bi and bj are not isomorphic, for i 6= j;
   sub ,
  s
(qs1) for every 2
2

iff

  t(x) for every x  T .
[()

Clearly, the number ]() of quasistates for  is bounded by 22
 2|sub | .
Fix a flow of time F = hW, <i. A basic structure for  is a pair hF, qi, where q is a
function associating with each w  W a quasistate q(w) = hsw , mw i for  such that, for
each w  W ,
 for every 1 U 2  sub , 1 U 2  sw iff there is v > w such that 2  sv and
1  su for all u  (w, v);
 for every 1 S 2  sub , 1 S 2  sw iff there is v < w such that 2  sv and
1  su for all u  (v, w).
Let hF, qi be a basic structure for , where q(w) = hsw , mw i and mw = hhTw , w i , tw i
for w  W . Denote by Tw0 the set of all leaves in hTw , w i and by Tw1 the set of all roots
of brooms in it. A 1-run through hF, qi is a function r giving for each w  W a point
r(w)  Tw1 ; a coherent and saturated 0-run through hF, qi is a function r giving for each
w  W a point r(w)  Tw0 such that the following conditions hold:
 for every CI(%1 U %2 )  term , CI(%1 U %2 )  tw (r(w)) iff there is v > w such that
%2  tv (r(v)) and %1  tu (r(u)) for all u  (w, v);
 for every CI(%1 S %2 )  term , CI(%1 S %2 )  tw (r(w)) iff there is v < w such that
%2  tv (r(v)) and %1  tu (r(u)) for all u  (v, w).
Say that a quadruple Q = hF, q, R, Ci is a quasimodel for  based on F if hF, qi is a basic
structure for , R = R0  R1 , with R1 being a set of 1-runs and R0 a set of coherent and
saturated 0-runs through hF, qi, and C the reflexive closure of a subset of R1  R0 such
that
(qm2) w0  W   sw0 ;
(qm3) for every w  W and every x  Tw , there is r  R with r(w) = x;
(qm4) for all r, r0  R, if r C r0 then r(w) w r0 (w) for all w  W ;
(qm5) for all r  R, w  W and x  Tw0 , if r(w) w x then there is r0  R0 such that
r0 (w) = x and r C r0 .
A quasimodel Q is said to be finitary if the set R of runs is finite.
Lemma C.4. A PT L  RC-formula  is satisfiable in an Aleksandrov tt-model based on
a flow of time F and a (finite) disjoint union of (finite) brooms iff there is a (finitary)
quasimodel for  based on F.
Proof. () Let  be a PT L  RC-formula and Q = hF, q, R, Ci a quasimodel for , where
F = hW, <i and q(w) = hsw , hhTw , w i , tw ii for w  W . We construct an Aleksandrov
tt-model M = hF, G, Vi by taking G = hR, Ci and, for each spatial variable p and w  W ,
V(p, w) = {r | CIp  tw (r(w))}.
222

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Clearly, if Q is finitary then G is finite. Thus, it remains to prove that  is satisfied in M.
First, we show by induction on the construction of a region term %  term  that, for
every w  W and every r  R,
(M, hw, ri) |= %

iff

%  tw (r(w)).

(56)

The basis of induction: % = CIp. Let (M, hw, ri) |= %. Then there is r0  R such that
r C r0 and (M, hw, r0 i) |= Ip. By (qm4), r(w) w r0 (w). Take any y  Tw0 , r0 (w) w y. By
(qm5), there is a run r00  R0 such that r0 C r00 and r00 (w) = y. Then (M, hw, r00 i) |= p
and, by the definition of V, CIp  tw (r00 (w)) and, by (bt3), %  tw (r(w)).
Conversely, if %  tw (r(w)) then, by (bt3), there is y  Tw0 with r(w) w y and
%  tw (y). By (qm5), there is r00  R0 , r C r00 , such that r00 (w) = y. Then CIp  tw (r00 (w))
and, by the definition of V, (M, hw, r00 i) |= p. Therefore, (M, hw, ri) |= %.
The induction steps for % = CI%1 , CI(%1 u %2 ), CI(%1 U %2 ) and CI(%1 S %2 ) are similar,
but instead of the definition of V, we use (bt1) for the cases of the Booleans and coherence
and saturatedness of r00 for the cases of temporal operators.
Next, we extend (56) to arbitrary spatio-temporal terms   term .
Case  = I%. Suppose that (M, hw, ri) |= I%. Take any y  Tw , r(w) w y. If y  Tw0
then, by (qm5), there is r0  R0 such that r C r0 and r0 (w) = y. If y 
/ Tw0 then clearly
y = r(w) and take r0 = r. We have (M, hw, r0 i) |= %, which, by IH, implies %  tw (r0 (w)).
Therefore, %  tw (y) for every y w r(w) and, by (bt2), I%  tw (r(w)).
Conversely, if I%  tw (r(w)) then, by (bt2), %  tw (y), for every y w r(w). Take any
run r0  R such that r C r0 . By (qm4), r(w) w r0 (w), and so %  tw (r0 (w)), from which,
by IH, (M, hw, r0 i) |= %. Hence, (M, hw, ri) |= I%.
Cases  = 1 u 2 and 1 follow from IH by the definition of type.
Finally, we show by induction on the construction of   sub  that, for every w  W ,
(M, w) |= 

iff

  sw .

(57)

  . Suppose (M, w) |= 2
  . Take any x  T . By (qm3), there is r  R
Case  = 2
w
such that r(w) = x. Then (M, hw, ri) |=  and, by IH,   tw (r(w)). Therefore, by (qs1),
   s . Conversely, let 2
   s . Take any run r  R. By (qs1), we have   t (r(w)),
2
w
w
w
 .
from which, by IH, (M, hw, ri) |=  . Hence, (M, w) |= 2
Cases  = 1  2 and 1 follow from IH by the Boolean-saturatedness of the sw .
It follows from (57) and (qm2) that  is satisfiable in M.

() Let  be a PT L  RC-formula and suppose that  is satisfied in an Aleksandrov
tt-model M = hF, G, Vi, where F = hW, <i and G = h, i is a disjoint union of brooms.
Denote by 0 and 1 the leaves and the roots of brooms in G, respectively. With every
pair hw, xi  W   we associate the type
t(w, x) = {  term  | (M, hw, xi) |=  }.
Fix a w  W and define a binary relation on  as follows. For x, x0  0 , let x w x0 iff
t(w, x) = t(w, x0 ) and, for z, z 0  1 , let z w z 0 iff the brooms generated by z and z 0 are
isomorphic, i.e.,
x  0 (z  x  x0  0 (z 0  x0  x w x0 )) 
x0  0 (z 0  x0  x  0 (z  x  x w x0 )).
223

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Clearly, w is an equivalence relation on . Denote by [x]w the w -equivalence class of x
and define a map fw by taking, for each x  ,
(
[x]w ,
x  1 ,
fw (x) =
h[z]w , [x]w i , x  0 and z  1 such that z  x.
Since G is a disjoint union of brooms, fw is well-defined. Now put
Tw = {fw (x) | x  },
u w v

iff

x, y  

tw (fw (x)) = t(w, x),

such that x  y, u = fw (x) and v = fw (y),

for x  .

By definition of fw , hTw , w i is a union of brooms and tw is well-defined. Consider the
structure hsw , mw i, where
mw = hhTw , w i , tw i

and

sw = {  sub  | (M, w) |= }.

It is readily seen that for each of the brooms of mw we have (bt0) and that mw satisfies
(qs0). Moreover, as fw is a p-morphism from h, i onto hTw , w i, we also have (bt1)
(bt3) and (qs1). So, by taking q(w) = hsw , mw i for each w  W we obtain a basic structure
hF, qi for  satisfying (qm2).
It remains to define appropriate runs through hF, qi. For k = 0, 1, let Rk be the set of
all maps r : w 7 fw (x) for x  k . Clearly, R1 and R0 are sets of 1- and coherent and
saturated 0-runs, respectively. Put R = R0  R1 and for r, r0  R, r C r0 iff r(w) w r0 (w)
for all w  W . Then (qm4) holds by definition. Let v  W and y  Tv . Then there
is x   such that fv (x) = y. Clearly, R contains the run r : w 7 fw (x), which proves
(qm3). Finally, let r  R, v  W and y  Tv0 be such that r(v) v y. There are some
z, x   such that fw (z) = r(w), for every w  W , and fv (x) = y. We clearly have z  x
and x  0 . Then take the run r0 : w 7 fw (x). By definition, r C r0 , which proves (qm5).
Thus, Q = hF, q, R, Ci is a quasimodel for . Note that if G is finite then R is finite as
well and therefore, Q is finitary.
q
We are now in a position to establish the upper complexity bounds of the satisfiability
problem for PT L  RC- and PT L  RC-formulas in tt-models based on hN, <i, hZ, <i or
arbitrary finite flows of time.
Proof of Theorem 3.10, upper bound. We consider the cases of hN, <i and hZ, <i.
The case of arbitrary finite flows of time and that of tt-models with FSA and based on
hN, <i and hZ, <i will follow from Theorem 3.13.
One can readily check that as for the propositional temporal logic PT L, we have the
following polynomial reductions for PT L  RC:
 satisfiability in tt-models based on hZ, <i can be polynomially reduced to satisfiability
in tt-models based on hN, <i;
 satisfiability in tt-models based on hN, <i can be polynomially reduced to satisfiability
of formulas without past-time temporal operators.
224

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

So, in what follows we consider the simplest case of the satisfiability problem, that is for
PT L  RC-formulas without past-time temporal operators in tt-models based on hN, <i.
We present a nondeterministic 2EXPSPACE satisfiability checking algorithm which is
similar to that of Sistla and Clarke (1985). First, one can prove (with the help of Lemmas C.1 (ii) and C.4) an analogue of (Hodkinson et al., 2000, Theorem 24) which states
that a PT L  RC-formula  is satisfiable in tt-model based on hN, <i iff there are l1 , l2  N
such that

2
l1  ](),
0 < l2  |term |  2[()  ]() + ]()
and a balloon-like quasimodel Q = hhN, <i , q, R, Ci for  with q(l1 + n) = q(l1 + l2 + n)
for every n  N. Although Theorem 24 of (Hodkinson et al., 2000) was proved for the
monodic fragment of first-order temporal logic, the basic idea of extracting a balloon-like
quasimodel from an arbitrary one works for PT L  RC as well. The only difference is that
quasistates now are more complex: they can be regarded as sets of sets of types for  (not
just sets of types) and thus, both l1 and l2 are triple exponential in the length `() of .
Then a quasimodel Q can be guessed in 2EXPSPACE by an algorithm which is very similar
to that in the proof of (Hodkinson et al., 2003, Theorem 4.1).
q
Proof of Theorem 3.13, upper bound. The proof is similar to that of Theorem 3.10.
Again, one can show that all the cases are polynomially reducible to the case of satisfiability
of PT L  RC-formulas without past-time temporal operators in tt-models with FSA and
based on hN, <i. To take the FSA into account, we can prove (using Lemmas C.1 (i)
and C.4) analogues of Theorems 29 and 35 of (Hodkinson et al., 2000) which state that a
PT L  RC-formula  is satisfiable in a tt-model with FSA and based on hN, <i iff there
is a finitary balloon-like quasimodel for  based on hN, <i. The condition of finiteness for
the set of runs can also be ensured by an algorithm similar to that of Theorem 3.10.
q
C.3 Upper Complexity Bounds (II):
Embedding into First-Order Temporal Logic
In this appendix we introduce the first-order temporal language QT L and use some known
complexity results for fragments of QT L to obtain upper complexity bounds for spatiotemporal logics based on RC  (and therefore, on BRCC-8).
The alphabet of QT L consists of individual variables x1 , x2 , . . . , predicate symbols
P1 , P2 , . . . , each of which is of some fixed arity, the Booleans, the universal x and existential x quantifiers for each variable x, and the temporal operators U, S (with their
derivatives , 3F , 2F , etc.). Note that our language contains neither constant symbols
nor equality (we simply do not need them to obtain our complexity results).
QT L is interpreted in first-order temporal models of the form M = hF, D, Ii, where
F = hW, <i is a flow of time, D a nonempty set, the domain of M, and I a function
associating with every moment of time w  W a first-order structure
D
E
I(w)
I(w)
I(w) = D, P0 , P1 , . . . ,
I(w)

the state of M at moment w, where each Pi
is a relation on D of the same arity as Pi .
An assignment in D is a function a from the set of individual variables to D. Given such
an assignment and a QT L-formula , we define the truth-relation (M, w) |=a  by taking
225

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

u = CIp

x1b
depth 0
depth 1

e = CIp

'

xn1
b

x2b

$

xnb

u
e
e
u
...
*

YH
H


I
@

H @

HH

@ u
H
b

=

Pj1 [db ]Pj2 [db ] . . .Pjn1 [db ]Pjn [db ]
&

%

x0b

db

Figure 11: Representing n-broom b with region CIpj by a point in a first-order model.
I(w)

 (M, w) |=a Pi (x1 , . . . , xm ) iff ha(x1 ), . . . , a(xm )i  Pi

,

0

 (M, w) |=a x  iff (M, w) |=a , for every assignment a0 in D which differ from a
only on x,
plus the standard clauses for the Booleans and temporal operators. We say that a QT Lformula  is satisfied in M if (M, w) |=a  for some w  W and some assignment a in D.
If all free variables of  are among x1 , . . . , xm , then instead of (M, w) |=a  we often write
(M, w) |= [d1 , . . . , dm ], where di = a(xi ) for all i, 1  i  m.
Denote by QT L1 the one-variable fragment of QT L, i.e., the set of all QT L-formulas
which contain at most one individual variable, say, x. Without loss of generality we may
assume that all predicate symbols of QT L1 are at most unary.
Now we define an embedding of spatio-temporal languages based on RC  into QT L1 .
Recall that, by Lemma C.2 (i), if a PT L  RC  -formula  of width n is satisfied in a
tt-model with FSA then  is also satisfiable in an Aleksandrov tt-model based on the same
flow of time and a finite disjoint union of n-brooms. Similarly, if a PT L  RC  -formula 
of width n is satisfiable then  is also satisfiable in an Aleksandrov tt-model based on the
same flow of time and possibly infinite disjoint union of n-brooms.
To cover both cases, let  be a PT L  RC  -formula of width n. We show how to
construct a QT L1 -formula n of length linear in `() such that every Aleksandrov tt-model
based on a (finite) union of n-brooms satisfying  gives rise to a first-order temporal model
(with finite domain, respectively) satisfying n and vice versa. Thus, we polynomially
reduce the satisfiability problem for spatio-temporal languages to that for QT L1 .
Suppose that  is satisfied in an Aleksandrov tt-model M = hF, G, Vi, where F = hW, <i
and G is a (finite or infinite) disjoint union of n-brooms. With every n-broom b of G we
associate an element db of the first-order domain D. Then, for every spatial variable pj in ,
we fix n different unary predicate symbols Pj1 (x), . . . , Pjn (x) with the following meaning:
Pji (x) is true on db  D at moment w  W iff the i-th leaf of b (xib in Fig. 11) belongs to
i
region CIp at w. Define n distinct translations n , 1  i  n, encoding the truth values
of spatio-temporal region terms of  on leaves of G by taking, for a spatial variable pj and
terms %1 and %2 ,
i

(CIpj )n = Pji (x),
i

i

i

(CI%1 )n = (%1 )n ,
i

i

i

i

(CI(%1 U %2 ))n = (%1 )n U (%2 )n ,

i

i

(CI(%1 u %2 ))n = (%1 )n  (%2 )n ,
i

i

(CI(%1 S %2 ))n = (%1 )n S (%2 )n .
226

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Next we extend these n translations to arbitrary spatio-temporal terms of . First we
0
introduce a translation n to encode the truth value of arbitrary spatio-temporal terms in
the roots of the n-brooms of G: for a region term %, let
0n

(%)

n
_

=

k

(%)n .

k=1
0

The formula above shows, in particular, that n is redundant for region terms since their
0
truth values in the roots can be computed as defined by n . For a spatio-temporal term
of the form I%, where % is a region term, we take
0n

(I%)

n
^

=

k

(%)n

and

i

i

(I%)n = (%)n

for all i, 1  i  n,

k=1

and then, for spatio-temporal terms 1 and 2 ,10
i

i

(1 )n = (1 )n

i

i

i

(1 u 2 )n = (1 )n  (2 )n

and

for all i, 0  i  n.

Finally, we define the translation n of subformulas of : for a spatio-temporal term  ,
n

(2 )


0n

= x ( )



n
^

k

x ( )n

k=1

and, for spatio-temporal formulas 1 and 2 ,
(1 )n = 1n ,

(1  2 )n = 1n  2n ,

(1 U 2 )n = 1n U 2n ,

(1 S 2 )n = 1n S 2n ,

Clearly, the length of n is linear in both n and `().
Lemma C.5. A PT L  RC  -formula  of width n is satisfiable in an Aleksandrov tt-model
based on a (finite) disjoint union of n-brooms iff n is satisfiable in a first-order temporal
model (with a finite domain) based on the same flow of time.
Proof. () Suppose that  is satisfied in an Aleksandrov tt-model M = hF, G, Vi, where
0 1
n
F = hW, <i, G = hV, Ri is a disjoint
 unionffof n-brooms

 0 n ffb = hWb , Rb i, Wb = {xb , xb , . . . , xb }
0
1
and Rb is the reflexive closure of { xb , xb , . . . , xb , xb } (see Fig. 11).
Construct a first-order temporal model N = hF, D, Ii by taking D to be the set of all
db for n-brooms b in G and, for every w  W ,
D
E
I(w)
I(w)
I(w)
I(w)
I(w) = D, P11 , . . . , P1n , P21 , . . . , P2n , . . . ,
where for each spatial variable pj in  and each i, 1  i  n,
I(w)

Pji



ff
= {db  D | (M, w, xib ) |= pj }.

10. For brevity, in this definition we follow the syntax of PT L  RC rather than PT L  RC  .

227

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Note that D is finite whenever G is finite.
Now, by induction on the construction of a spatio-temporal region term % of , it can
easily be shown that for every w  W , every n-broom b in G and every i, 1  i  n,


ff
i
(58)
(N, w) |= (%)n [db ] iff (M, w, xib ) |= %.
Next, (58) can be extended to arbitrary spatio-temporal terms  of  and i, 0  i  n:
i

(N, w) |= ( )n [db ]

ff


iff (M, w, xib ) |= .

(59)

The cases for i, 1  i  n, trivially follow from (58) and the fact that leaves have no
successors but themselves. Consider now i = 0. The case  = % holds simply because region
terms are interpreted by regular closed sets:


ff


ff
(M, w, x0b ) |= %
iff
(M, w, xkb ) |= %, for some k, 1  k  n,
(60)
If  = I% then, on one hand,


ff
(M, w, x0b ) |= I%

iff



ff
(M, w, xkb ) |= %,

for all k, 0  k  n,

0

and on the other, by the definition of n ,
0

(N, w) |= (I%)n [db ]

iff

k

(N, w) |= (%)n [db ],

for all k, 1  k  n,

which together with (60) and IH yields (59). The cases of the Booleans are trivial.
Finally, we show that for every   sub , we have
(N, w) |=  n

iff

(M, w) |= .

 :
Case  = 2
  )n
(N, w) |= (2

k

iff

db  D k  {0, 1, . . . , n} (N, w) |= ( )n [db ]


ff
b in G k  {0, 1, . . . , n} (M, w, xkb ) |= 

iff

 .
(M, w) |= 2

iff

The remaining cases are trivial. It follows that n is satisfied in N.
() Assume that  is satisfied in a first-order temporal model N = hF, D, Ii, where
F = hW, <i and, for every w  W ,
D
E
I(w)
I(w)
I(w)
I(w)
I(w) = D, P11 , . . . , P1n , P21 , . . . , P2n , . . . .
With every point d  D we associate an n-broom bd = hWbd , Rbd i so that the sets Wbd ,
for d  D, are pairwise disjoint and each contains n + 1 distinct elements x0bd , . . . , xnbd .
Construct an Aleksandrov tt-model M = hF, G, Vi by taking
 G to be the disjoint union of n-brooms {bd | d  D},

	
i
 V(pj , w) = xibd | (N, w) |= (CIpj )n [d], 0  i  n and d  D .
228

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Clearly, if D is finite then G is finite as well.
By a straightforward induction one can show that for all w  W , d  D, spatio-temporal
region terms %, spatio-temporal terms  , subformulas  of , and all i, 0  i  n,


ff
i
iff
(M, w, xibd ) |= %
(i > 0),
(N, w) |= (%)n [d]


ff
in
i
(N, w) |= ( ) [d]
iff
(M, w, xbd ) |= ,
(N, w) |=  n

iff

(M, w) |= .

For example,
0

(N, w) |= (I%)n [d]

iff
iff
iff

k

(N, w) |= (%)n [d], for all k, 0  k  n
ff


(M, w, xkbd ) |= %, for all k, 0  k  n
ff


(M, w, x0bd ) |= I%.
q

It follows that  is satisfied in M.
Now we obtain the upper complexity bounds for combinations of PT L and RC  :

Proof of Theorem 3.11, upper bound. Follows from Lemmas C.2 (ii) and C.5 together
with the results on the complexity of the one-variable fragment of QT L (Halpern & Vardi,
1989; Sistla & German, 1987; Hodkinson et al., 2000, 2003).
q
Proof of Theorem 3.12, upper bound. Similar to the proof above.

q

Proof of Theorem 3.15, upper bound. The proof follows from Lemmas C.2 (i) and C.5
together with the upper complexity bound of the guarded monodic (and so the one-variable)
fragment of QT L (Hodkinson, 2004).
q
C.4 Lower Complexity Bounds (II): Embedding First-Order Temporal Logic
We are now in a position to prove Theorem 3.14 and establish the lower complexity bounds
for spatio-temporal logics based on BRCC-8 (and so for those based on RC  as well). Denote
by QT L12 the one-variable fragment of QT L with sole temporal operator 2F . We define a
polynomial embedding of QT L12 into PT L2  BRCC-8. Note that a similar embedding of
the full one-variable fragment QT L1 into PT L  BRCC-8 can be regarded as an alternative
way to prove the lower complexity bound of Theorem 3.12.
A QT L12 -formula is said to be a basic Q-formula if it is of the form x (x), where
(x) is quantifier-free and contains no propositional variables. A QT L12 -sentence  is in Qnormal form if it is built from basic Q-formulas using the Booleans and temporal operator
2F . In other words, sentences in Q-normal form do not contain nested quantifiers and use
only unary predicate symbols. The following observation should not come as a surprise (see,
e.g., Hughes & Cresswell, 1996):
Lemma C.6. For every QT L12 -sentence  one can effectively construct a QT L12 -sentence

b in Q-normal form such that  is satisfiable in a first-order temporal model with a flow of
time F (and having finite domain) iff 
b is satisfiable in a first-order temporal model based
on F (and having finite domain). Moreover, the length of 
b is linear in the length of .
229

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Proof. Without loss of generality we may assume that  contains no occurrences of . To
transform  into its Q-normal form, we first introduce a fresh unary predicate symbol Pi (x)
for every propositional variable pi in  and replace each occurrence of pi with x Pi (x).
Denote the resulting formula by 0 . For every subformula  of 0 define a formula  ] by
taking inductively
(P (x))] = P (x),
()] =  ] ,

(x )] = Px (x),
(1  2 )] = 1]  2] ,

where Px (x) is a fresh unary predicate symbol. Let

^

2+
x Px (x)  x Px (x) 

b = x ]0 
F

(2F )] = 2F  ] ,

x Px (x)  x  ]




.

xsub0

One can readily show by induction that 
b is satisfiable in a first-order temporal model based
on F (and having finite domain) iff  is satisfiable in a first-order temporal model based on
F (and having finite domain). Moreover, 
b is in Q-normal form.
q
Now, given a QT L12 -formula  in Q-normal form, denote by  the result of replacing
all occurrences of basic Q-formulas x (x) in  with EQ( , >), where > is a region term
representing the whole space (for instance, CIu t CIu for a fresh spatial variable u), and
the translation  of quantifier-free formulas (x) is defined by taking:
(P (x)) = CIp,

() = CI   ,

(1  2 ) = CI(1 u 2 ),

(2F ) = CI2F   ,

where P (x) is a unary predicate symbol and p a spatial variable standing for P (x). Clearly,
 belongs to PT L2  BRCC-8.
Lemma C.7. A QT L12 -sentence  in Q-normal form is satisfiable in a first-order temporal
model based on a flow of time F and having finite domain iff  is satisfiable in a tt-model
based on F and satisfying FSA.
Proof. () Suppose that  is in Q-normal form and M = hF, D, Ii is a first-order temporal


ff
I(w)
model, where F = hW, <i and, for all w  W , I(w) = D, P0 , . . . , . Let (M, w0 ) |=  for
some w0  W . Construct an Aleksandrov tt-model M0 = hF, G, Vi by taking G = hD, Ri,

I(w) 	
where R = {hd, di | d  D} and V(pi , w) = hw, di | d  Pi
. Note that the topological
space TG = hD, IG i induced by G is discrete, i.e., for all X  D,
IG X = CG X = X.
It follows by induction that for every quantifier-free QT L12 -formula , every w  W and
every d  D we have
(M, w) |= [d]

iff

(M0 , hw, di) |=  .

Therefore, for every basic Q-formula x (x) and every w  W , (M, w) |= x (x) iff
(M0 , w) |= EQ( , >). It follows by induction that (M0 , w0 ) |=  .
230

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

() Suppose that  is satisfied in a tt-model based on F = hW, <i. By Lemma C.1 (i),
is satisfied in an Aleksandrov tt-model M = hF, G, Vi, where G = hV, Ri is a disjoint
union of brooms. Denote by V0  V the set of leaves of G and define a first-order temporal
model M0 = hF, V0 , Ii by taking, for each w  W ,


ff
I(w)
I(w)
I(w) = V0 , P0 , . . .
and
Pi
= V(pi , w)  V0 .


Clearly, for every X  V , we have IG X  V0 = CG X  V0 = X  V0 , where TG = hV, IG i is
the topological space induced by G. So we obtain by induction that for every quantifier-free
QT L12 -formula , all w  W and all d  V0
(M0 , w) |= [d]

iff

(M, hw, di) |=  .

A regular closed set X  V in TG coincides with V iff it contains V0 . So, for all basic
Q-formulas x (x) and all w  W , (M0 , w) |= x (x) iff (M, w) |= EQ( , >). It follows
by induction that  is satisfied in M0 .
q
Proof of Theorem 3.14, lower bound. By Lemmas C.6 and C.7 the satisfiability problem for QT L12 -formulas in first-order temporal models with finite domains and based on
hN, <i, hZ, <i or arbitrary finite flows of time is polynomially reducible to satisfiability of PT L2  BRCC-8 formulas in tt-models with FSA. Since the former is known to
be EXPSPACE-hard (Hodkinson et al., 2003) for hN, <i and hZ, <i, the latter is also
EXPSPACE-hard in these cases. It should be noted that the result of Hodkinson and
his colleagues (2003) can readily be extended to the case of arbitrary finite flows of time
(by reduction of a finite version of the corridor tiling problem). This gives us the lower
complexity bound for PT L2  BRCC-8 in the case of finite flows of time.
q
C.5 PSPACE-complete Spatio-Temporal Logic
In this appendix we prove Theorem 3.8. In fact, we show that the satisfiability problem for
PT L  RC 2 an extension of PT L  RCC-8is decidable in PSPACE, where RC 2 is the
sublanguage of S4u with spatial terms  restricted to the following:
%

::= CIp,



::= %



::= I%



::= 1 t 2

|

I%,
|

%,
|

1 t 2

|

 t .

As before, we denote by  spatial terms representing regular closed sets (regions) and by
 those representing regular open sets (the interiors of regions). Clearly, this definition is
equivalent to the definition on p. 190 (where we did not make an explicit distinction between
 and ). It is easy to see that RC 2 contains RCC-8, but is less expressive than BRCC-8.
Spatio-temporal terms  of PT L  RC 2 are constructed from region terms of the form
%

::=

CIp

|

CI%

in the same way as spatial terms of RC 2 . Finally, PT L  RC 2 -formulas are composed from
  using the Booleans and the temporal operators.
atomic formulas of the form 2
231

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

We will reduce the satisfiability problem for PT LRC 2 to that for PT L. This reduction
will be done in a number steps.
Let F = hW, <i be a flow of time (as in the formulation of Theorem 3.8) and  a
PT L  RC 2 -formula. We begin by removing the next-time operator from the subterms of .
To this end, let 0 =  and for each variable p from the set
1

=

{p | CICIp  term 0 },

we introduce a fresh spatial variable p0 , and then put
^

+ 
 (CICIp  CIp0 ) ,
1 =  1 
2+
2
>

2
P F
p1

where 1 is the result of replacing each occurrence of CICIp in 0 with CIp0 and
 (% t % ). Next, for each p from
 (%
 (% t % )  2
2
1
2
1
2
1  %2 ) stands for 2
2

=

{p | CICIp  term 1 },

we introduce a fresh spatial variable p0 , and set
^
+
2 =  2 
2+
P 2F

>


 (CICIp  CIp0 ) ,
2

p1 2

where 2 is the result of replacing each occurrence of CICIp in 1 with CIp0 . By repeating
this process sufficiently many times we can obtain a formula
^

+ 
 (CICIp  CIp0 ) ,
2
>

2
(61)

e =  
2+
P F
p

where  is a suitable set of spatial variables, and  contains no  in region terms, that
is,  is a PT L[RC 2 ]-formula. (Note that  is such that if a spatial variable p occurs in
 then either CICIp 
/ term  or p   .) It should be clear that the length of 
e is
linear in the length of , and  is satisfiable in a tt-model based on F iff 
e is satisfiable in
a tt-model based on F.
Thus, it suffices to reduce the satisfiability problem for PT L  RC 2 -formulas of the
form (61) to the satisfiability problem for PT L-formulas. Let us now recall the function
 from Appendix B.1 which maps PT L[S4u ]-formulas (in particular, PT L[RC 2 ]-formulas)
  , let (2
  ) = p , where p
to PT L-formulas. Namely, for every atomic RC 2 -formula 2

 is
a fresh propositional variable. Then, given the PT L[RC 2 ]-formula  , define  to be the
  in it with (2
  ) .
result of replacing every occurrence of 2
As is shown in the proof of
Theorem 3.1,  is satisfiable in a tt-model over F = hW, <i iff
(s1) there exists a temporal model N = hF, Ui satisfying  and,
(s2) for every w  W , the set
  | (N, w) |= p ,   term  }  {2
  | (N, w) |= p ,   term  }
w = {2
(62)





of RC 2 -formulas is satisfiable.
232

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

To preserve satisfiability of not only  but the whole ,
e we have to ensure somehow that
(s3) the points satisfying w do have predecessors and successors satisfying w1 and
w+1 , respectively.
In the remainder of the appendix we first describe an encoding of the satisfiability
problem for sets of RC 2 -formulas of the form (62) in Boolean logic, which will be used as
part of our final reduction. Then we prove a completion property of RC 2 (cf. Balbiani &
Condotta, 2002) in the class of exhaustive models that contain sufficiently many points of
every type. Roughly, the completion property says that, given a set  of the form (62) and
an exhaustive model satisfying some subset of , one can extend the valuation of the model
to satisfy the whole . This property will make it possible to solve problem (s3) above. It is
worth noting that a similar construction works for stronger languages such as BRCC-8, but
then, to enjoy the completion property, sets (62) may need exponentially many formulas (in
the number of spatial variables) and, therefore, the reduction to PT L will be exponential
as well. For RC 2 it suffices to consider sets (62) with a quadratic number of formulas, which
results in a quadratic reduction.
C.5.1 Properties of RC 2 -formulas
For any finite set  = {p1 , . . . , pn } of spatial variables, let

	

AtFm =
2
|  is an RC 2 -term with variables from  .
Clearly, every RC 2 -formula with spatial variables from  is a Boolean combination of spatial
formulas from AtFm . It should be also clear that |AtFm |  16  ||2 .
As the width of RC 2 -formulas is  2 (see p. 209 for the definition), by Lemmas A.1 and
C.2 (ii), an RC 2 -formula is satisfiable iff it is satisfiable in an Aleksandrov topological model
based on a disjoint union of 2-brooms, alias forks. In what follows we will regard every such
model M as a disjoint union of fork models m = hf, vi, where f = hW, Ri, W = {x0 , x1 , x2 },
R is the reflexive closure of {hx0 , x1 i, hx0 , x2 i} and v a valuation of the spatial variables.
Given 0  , we say that fork models m1 = hf, v1 i and m2 = hf, v2 i are 0 -equivalent and
write m1 0 m2 , if v1 (CIp) = v2 (CIp) for every p  0 .
Given some   AtFm and   AtFm , we say that  is an f-consequence of  and
write  |=f  if m |=  implies m |=  for every fork model m based on f.  is said to be
closed (under
f-consequences) if, for
 , we have    whenever  |=f .

	 every   AtFm
c
 | 2
   AtFm
Let c = 2
   . Then    is satisfiable iff  is closed and satisfiable.
This means, in particular, that to check whether the set w in (62) is satisfiable, it is enough
  | (N, w) |= p ,   term }.
to consider only the closure of {2

Now we characterise |=f in terms of the Boolean consequence relation |=. As we know
from Appendix C.3, spatial formulas can be embedded into the one-variable fragment of
first-order logic. More precisely, it can easily be shown that first-order translations of
formulas from AtFm are (equivalent to) formulas of the form (which are actually Krom
formulas; see, e.g., Borger, Gradel, & Gurevich, 1997):
1
1 
2
2 
 ( t  ))2 = x  2   2
(2
 x 12  22 ,
1
2
1
2
1
1 
2
2 
 ( t  ))2 = x  2   2
(2
 x 12  22 ,
1
2
1
2

233

(63)
(64)

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

1

1 

 ( t  ))2 = x  2   2
(2
1
2
1
2

1

2 

 x 12  22

2

1 

 x 12  22

2
2 
 x 12  22 , (65)

where


i2

(
Pji (x),
if  = CIpj ,
=
Pji (x), if  = ICIpj

and 

i2

(
Pji (x),
if  = ICIpj ,
=
Pji (x), if  = CIpj ,

for i = 1, 2.

It follows from the proof of Lemma C.5 that an RC 2 -formula  is satisfied in an Aleksandrov
2 is satisfied in
model M based on a disjoint union of forks iff its first-order

 translation
ff 
0
1
2
0
a first-order model where every fork f = hW, Ri of M, W = x , x , x , x Rx1 and x0 Rx2 ,
is encoded by a domain element df with Pji (x) being true on df iff (M, xi ) |= CIpj , for
i = 1, 2 (see Fig. 11). Since in the definition of closed sets we only consider Aleksandrov
models based on a single fork f, the domains of respective first-order models contain a single
element df . This means that (63)(65) can be encoded by the Boolean formulas
2
2
1  2 ,
1
1
2
2
 ( t  )) =
(2
1  2  1  2 ,
1
2
2
1
1
1
 ( t  )) =
(2
1  2  1  2 
1
2
1

1

 ( t  )) =
(2
1  2
1
2





2

1

1  2



2
2
1  2 ,



where
(
qji ,
if  = CIpj ,
 =
qji , if  = ICIpj
i

(
qji ,
if  = ICIpj ,
 =
qji , if  = CIpj ,
i

and

for i = 1, 2.

Thus, with every   AtFm we can associate the conjunction  of the  -translations of
formulas in  such that the following holds:
Claim C.8. For every   AtFm ,  |=f 

iff

 |=   .

To construct the closure of   AtFm and to check whether  is satisfiable, we can
use the following resolution-like inference rules:
()

()

 ( t %)
2
1

 (I% t  )
2
2

()1

 ( t  )
2
1
2

%
2

%
2



2(1 t )


()

 (I% t  )
2
1
 (% t  )
2
1

2(0


t 2 )

 ( t  )
2
1
2

()2

 (% t  )
2
1
 (I% t  )
2
1


0

 = %,  = I%;
for
 = %, 0 = %;


 = I%, 0 = I%;

together with the equivalences:
% = 2
 I%,
2

% = 2
 I%,
2

 (% t  ) = 2
 (I% t  ),
2
1
1

 (% t  ) = 2
 (I% t  ),
2
1
1

where % = CIp for some p  , 1 and 2 are of the form % or I%, and 1 and 2 of the form
I% or %. It is readily checked that the above rules are sound, and so if  is derivable from ,
then  is not satisfiable. On the other hand, if  is not satisfiable then  can be regarded
234

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

as an unsatisfiable set of binary and unary propositional clauses and, using the standard
resolution procedure, one can construct a derivation of the empty clause from  which,
in turn, can be mimicked by applications of the above rules (and equivalences) to derive
 from . Moreover, since the propositional resolution is subsumption complete (see, e.g.,
Slagle, Chang, & Lee, 1969), we can also derive all consequences of , thereby obtaining its
closure.
Now we encode the above rules and equivalences as Boolean formulas with variables p ,
   AtFm . For instance, () and () are encoded by
for 2



%  2
%
 ( t  )
 ( t %)  2
 (I% t  )  2
2

and
2
,
2
1
2
1
respectively. Denote by  the conjunction of all such formulas for spatial variables from
. Then we have the following:
Claim C.9. For every   AtFm ,  is closed and satisfiable iff the Boolean formula
h ^
i
^
p
p 
(66)
 
  AtFm 
2


  
2

is satisfiable.
Finally, to ensure (s3), we need the following completion property of RC 2 :
Lemma C.10. Let  be a closed subset of AtFm , 0   and 0 =   AtFm0 . Then
(i) 0 is closed and (ii) for every fork model m0 , if m0 |= 0 then there is a fork model m
such that m0 0 m and m |= .
Proof. Claim (i) is clear. To show (ii), we define the characteristic formula  of m0 on 0
by taking:
(
^
qji , if (m0 , xi ) 6|= CIpj ,


 =
lji
and
lji =
qji ,
if (m0 , xi ) |= CIpj .
p  , i=1,2
j

0

If m0 |= 0 then it follows immediately from the definitions that 0   is satisfiable. Our
aim is to show that    is also satisfiable, which would mean that there is a fork model
m as required. Suppose otherwise. Then  |= . We can regard  as a set of unary and
 . According
binary clauses and  as a clause with 2  |0 | literals lji , the negations of the lji
to the subsumption theorem (Slagle et al., 1969), by applying the standard resolution rule
to  , we can derive a clause lj1 i1  lj2 i2 which subsumes  (i.e., its both literals occur in
). Since  is closed, we have lj1 i1  lj2 i2 among the clauses of  and as the ljk ik are the
i
 k -translations of spatial terms for spatial variables from 0 , we conclude that lj1 i1  lj2 i2
is indeed among the clauses of 0 , contrary to 0   being satisfiable.
q
C.5.2 The Polynomial Translation of PT L  RC 2 into PT L
Now we are in a position to define a polynomial (at most quadratic) translation  of
PT L  RC 2 into PT L. Starting with a given formula , we construct the PT L  RC 2 formula 
e of the form (61):
^

+ 
 (CICIp  CIp0 ) ,

e =  
2+
>2
P 2F
p

235

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

where  is a PT L[RC 2 ]-formula. Let 0 = {p0 | p   } and let  denote the smallest
set of spatial variables containing   0 and all spatial variables occurring in  . Given

   AtFm
  0 the formula from AtFm 0
2
 , denote by 2
 obtained from 2 by replacing
0
0
every occurrence of p   with p   . Consider the PT L-formula


=





+
2+
P 2 F 



+
2+
P 2F

>



^


  2
  0 ) .
(2

  AtFm
2


Lemma C.11. For every PT L  RC 2 -formula , 
e is satisfiable in a tt-model based on
F = hW, <i iff  is satisfiable in a temporal model based on F.
Proof. () Let (M, w0 ) |= .
e Construct a temporal model N = hF, Vi by taking, for

2  AtFm ,
  }.
V(p ) = {w  W | (M, w) |= 2
It is easy to see that (N, w0 ) |=  .
() Let (N, w0 ) |=  for some w0  W . For every w  W , set
   AtFm
w = {2
 | (N, w) |= p }.

Let w , for w  W , be a set of all non--equivalent fork models m with m |= w . By
Claim C.9, the w are closed and satisfiable, so the sets w are nonempty. We use the
elements of the w as building blocks for exhaustive states in the tt-model we are going to
construct in order to satisfy .
First we show that each element of w has a successor in w+1 and a predecessor in
w1 (provided that w has a successor and predecessor, respectively). More precisely, we
say that a pair of fork models m = hf, vi and m0 = hf, v0 i is suitable and write m  m0 if
v(CIp0 ) = v0 (CIp), for every p   .
(succ) Let m  w , m = hf, vi, and let w  W have a successor w + 1. By the third
conjunct of  , we have
w  AtFm0

=

  0  AtFm 0
{2
 | (N, w) |= p 0 }

=

  0  AtFm 0
{2
 | (N, w + 1) |= p }.

Therefore,
w+1  AtFm

=

   AtFm
{2
 | (N, w) |= p 0 }.

Now, by m  w , we have m |= w  AtFm0 . So if we define a fork model m0 = hf, v0 i by
taking v0 (p) = v(p0 ), for all p   (and arbitrary otherwise), then m0 |= w+1  AtFm
follows. Since w+1 is closed, by Lemma C.10, we can find a fork model m00 = hf, v00 i such
that m00  m0 and m00 |= w+1 . It follows that m  m00 and m00 is -equivalent to some
fork model in w+1 (i.e., we may assume that m00  w+1 ).
(pred ) Similarly, for every m  m , m = hf, vi, and every w  W with a predecessor
w  1, there is m00 = hf, v00 i such that m00  w1 and m00  m.
It should be clear that for every fork model m  w and every w  W , we can define a
function rm,w that gives for each u  W a fork model rm,w (u)  u such that rm,w (w) = m
236

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

and rm,w (u)  rm,w (u + 1), whenever u + 1 is a successor of u. Let  be the set of all such
functions rm,w , for w  W and m  w .
We are now ready to define an Aleksandrov tt-model M = hF, G, Vi satisfying .
e Let
G = hW, Ri be a disjoint union of ||-many forks fr = hWr , Rr i, Wr = {x0r , x1r , x2r }, x0r Rr x1r
and x0r Rr x2r , for each r  , and let V(p, w) = {xir  W | (r(w), xir ) |= CIp}, for all p  
and w  W . We show by induction on the construction of   sub  that, for every w  W ,
(M, w) |= 

iff

(N, w) |=  .

  . Suppose that (M, w) |= 2
  but (N, w) 6|= p . Then 2
 
Case  = 2
/ w and, since

  . It follows that
w is closed (by Claim C.9 and  being true at w), we have w 6|=f 2
  , and so there is r   such that r(w) = m,
there is a fork model m  w with m |= 2
  . Conversely, if (N, w) |= p

contrary to (M, w) |= 2
 then, by construction, (M, w) |= 2 .
The cases of the Booleans and temporal operators are trivial.
As the second conjunct of 
e is satisfied by construction, we obtain (M, w0 ) |= .
e
q

References
Aiello, M., & van Benthem, J. (2002a). Logical patterns in space. In Barker-Plummer,
D., Beaver, D. I., van Benthem, J., & Scotto di Luzio, P. (Eds.), Words, Proofs and
Diagrams, pp. 525. CSLI Publications, Stanford.
Aiello, M., & van Benthem, J. (2002b). A modal walk through space. Journal of Applied
Non-Classical Logics, 12 (34), 319364.
Alexandroff, P. (1937). Diskrete Raume. Matematicheskii Sbornik, 2 (44), 501518.
Allen, J. (1983). Maintaining knowledge about temporal intervals. Communications of the
ACM, 26, 832843.
Areces, C., Blackburn, P., & Marx, M. (2000). The computational complexity of hybrid
temporal logics. Logic Journal of the IGPL, 8, 653679.
Arhangelskii, A., & Collins, P. (1995). On submaximal spaces. Topology and its Applications, 64, 219241.
Asher, N., & Vieu, L. (1995). Toward a geometry of common sense: A semantics and a
complete axiomatization of mereotopology. In Mellish, C. (Ed.), Proceedings of the
14th International Joint Conference on Artificial Intelligence (IJCAI-95), pp. 846
852. Morgan Kaufmann.
Balbiani, P., & Condotta, J.-F. (2002). Computational complexity of propositional linear
temporal logics based on qualitative spatial or temporal reasoning. In Armando, A.
(Ed.), Proceedings of Frontiers of Combining Systems (FroCoS 2002), Vol. 2309 of
Lecture Notes in Computer Science, pp. 162176. Springer.
Balbiani, P., Tinchev, T., & Vakarelov, D. (2004). Modal logics for region-based theories of
space. Manuscript.
Bennett, B. (1994). Spatial reasoning with propositional logic. In Proceedings of the 4th
International Conference on Knowledge Representation and Reasoning, pp. 5162.
Morgan Kaufmann.
237

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Bennett, B. (1996). Modal logics for qualitative spatial reasoning. Logic Journal of the
IGPL, 4, 2345.
Bennett, B., & Cohn, A. (1999). Multi-dimensional multi-modal logics as a framework
for spatio-temporal reasoning. In Proceedings of the Hot topics in Spatio-temporal
reasoning workshop, IJCAI-99, Stockholm.
Bennett, B., Cohn, A., Wolter, F., & Zakharyschev, M. (2002). Multi-dimensional modal
logic as a framework for spatio-temporal reasoning. Applied Intelligence, 17, 239251.
Blackburn, P. (1992). Fine grained theories of time. In Aurnague, M., Borillo, A., Borillo,
M., & Bras, M. (Eds.), Proceedings of the 4th European Workshop on Semantics of
Time, Space, and Movement and Spatio-Temporal Reasoning, pp. 299320, Chateau
de Bonas, France. Groupe Langue, Raisonnement, Calcul, Toulouse.
Borger, E., Gradel, E., & Gurevich, Y. (1997). The Classical Decision Problem. Perspectives
in Mathematical Logic. Springer.
Bourbaki, N. (1966). General topology, Part 1. Hermann, Paris and Addison-Wesley.
Chagrov, A., & Zakharyaschev, M. (1997). Modal Logic, Vol. 35 of Oxford Logic Guides.
Clarendon Press, Oxford.
Chlebus, B. (1986). Domino-tiling games. Journal of Computer and System Sciences, 32,
374392.
Clarke, B. (1981). A calculus of individuals based on connection. Notre Dame Journal of
Formal Logic, 23, 204218.
Clarke, B. (1985). Individuals and points. Notre Dame Journal of Formal Logic, 26, 6175.
Clarke, E., & Emerson, E. (1981). Design and synthesis of synchronisation skeletons using
branching time temporal logic. In Kozen, D. (Ed.), Logic of Programs, Vol. 131 of
Lecture Notes in Computer Science, pp. 5271. Springer.
Clementini, E., Di Felice, P., & Hernandez, D. (1997). Qualitative representation of positional information. Artificial Intelligence, 95, 317356.
Cohn, A. (1997). Qualitative spatial representation and reasoning techniques. In Brewka,
G., Habel, C., & Nebel, B. (Eds.), KI-97: Advances in Artificial Intelligence, Vol. 1303
of Lecture Notes in Computer Science, pp. 130. Springer.
Davis, E. (1990). Representations of Commonsense Knowledge. Morgan Kaufmann.
Degtyarev, A., Fisher, M., & Konev, B. (2003). Monodic temporal resolution. In Baader,
F. (Ed.), Proceedings of the 19th International Conference on Automated Deduction (CADE-19), Vol. 2741 of Lecture Notes in Artificial Intelligence, pp. 397411.
Springer.
Demri, S., & DSouza, D. (2002). An automata-theoretic approach to constraint LTL. In
Agrawal, M., & Seth, A. (Eds.), Proceedings of the 22nd Conference on Foundations
of Software Technology and Theoretical Computer Science (FST TCS 2002), Vol. 2556
of Lecture Notes in Computer Science, pp. 121132. Springer.
Egenhofer, M., & Franzosa, R. (1991). Point-set topological spatial relations. International
Journal of Geographical Information Systems, 5, 161174.
238

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Egenhofer, M., & Herring, J. (1991). Categorizing topological relationships between regions,
lines and points in geographic databases. Tech. rep., University of Maine.
Egenhofer, M., & Sharma, J. (1993). Assessing the consistency of complete and incomplete
topological information. Geographical Systems, 1, 4768.
Emerson, E., & Halpern, J. (1985). Decision procedures and expressiveness in the temporal
logic of branching time. Journal of Computer and System Sciences, 30, 124.
Fine, K. (1974). Logics containing K4, part I. Journal of Symbolic Logic, 39, 229237.
Finger, M., & Gabbay, D. (1992). Adding a temporal dimension to a logic system. Journal
of Logic, Language and Information, 2, 203233.
Fisher, M., Dixon, C., & Peim, M. (2001). Clausal temporal resolution. ACM Transactions
on Computational Logic (TOCL), 2, 1256.
Gabbay, D., Hodkinson, I., & Reynolds, M. (1994). Temporal Logic: Mathematical Foundations and Computational Aspects, Volume 1. Oxford University Press.
Galton, A., & Meathrel, R. (1999). Qualitative outline theory. In Dean, T. (Ed.), Proceedings
of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99), pp.
10611066. Morgan Kaufmann.
Gerevini, A., & Nebel, B. (2002). Qualitative spatio-temporal reasoning with RCC-8 and Allens interval calculus: Computational complexity. In Proceedings of the 15th European
Conference on Artificial Intelligence (ECAI02), pp. 312316. IOS Press.
Gerevini, A., & Renz, J. (2002). Combining topological and size constraints for spatial
reasoning. Artificial Intelligence, 137, 142.
Godel, K. (1933). Eine Interpretation des intuitionistischen Aussagenkalkuls. Ergebnisse
eines mathematischen Kolloquiums, 4, 3940.
Goldblatt, R. (1976). Metamathematics of modal logic, Part I. Reports on Mathematical
Logic, 6, 4178.
Goranko, V., & Passy, S. (1992). Using the universal modality: gains and questions. Journal
of Logic and Computation, 2, 530.
Gotts, N. (1996). An axiomatic approach to topology for spatial information systems. Tech.
rep. 96.25, School of Computer Studies, University of Leeds.
Halpern, J., & Shoham, Y. (1986). A propositional modal logic of time intervals. In Proceedings of the 1st Annual IEEE Symposium on Logic in Computer Science (LICS86),
pp. 279292. IEEE Computer Society.
Halpern, J., & Vardi, M. (1989). The complexity of reasoning about knowledge and time I:
lower bounds. Journal of Computer and System Sciences, 38, 195237.
Hodkinson, I. (2004). Complexity of monodic packed fragment over linear and real time. To
appear in Annals of Pure and Applied Logic, available at http://www.doc.ic.ac.uk/
/~imh/papers/cxmonlin.pdf.
Hodkinson, I., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). On
the computational complexity of decidable fragments of first-order linear temporal
239

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

logics. In Reynolds, M., & Sattar, A. (Eds.), Proceedings of TIME-ICTL 2003, pp.
9198. IEEE Computer Society.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2000). Decidable fragments of first-order
temporal logics. Annals of Pure and Applied Logic, 106, 85134.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2001). Monodic fragments of first-order
temporal logics: 20002001 A.D. In Nieuwenhuis, R., & Voronkov, A. (Eds.), Logic
for Programming, Artificial Intelligence and Reasoning, Vol. 2250 of Lecture Notes in
Artificial Intelligence, pp. 123. Springer.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2002). Decidable and undecidable fragments of first-order branching temporal logics. In Proceedings of the 17th Annual
IEEE Symposium on Logic in Computer Science (LICS 2002), pp. 393402. IEEE
Computer Society.
Hughes, G., & Cresswell, M. (1996). A New Introduction to Modal Logic. Methuen, London.
Hustadt, U., & Konev, B. (2003). TRP++ 2.0: A temporal resolution prover. In Baader,
F. (Ed.), Proceedings of the 19th International Conference on Automated Deduction
(CADE-19), Vol. 2741 of Lecture Notes in Computer Science, pp. 274278. Springer.
Kontchakov, R., Lutz, C., Wolter, F., & Zakharyaschev, M. (2004). Temporalising tableaux.
Studia Logica, 76, 91134.
Kutz, O., Sturm, H., Suzuki, N.-Y., Wolter, F., & Zakharyaschev, M. (2003). Logics of
metric spaces. ACM Transactions on Computational Logic, 4, 260294.
Ladner, R. (1977). The computational complexity of provability in systems of modal logic.
SIAM Journal on Computing, 6, 467480.
Lemon, O., & Pratt, I. (1998). On the incompleteness of modal logics of space: advancing complete modal logics of place. In Kracht, M., de Rijke, M., Wansing, H., &
Zakharyaschev, M. (Eds.), Advances in Modal Logic, Volume 1, pp. 115132. CSLI
Publications, Stanford.
Lewis, C., & Langford, C. (1932). Symbolic Logic. Appleton-Century-Crofts, New York.
Ligozat, G. (1998). Reasoning about cardinal directions. Journal of Visual Languages and
Computing, 9, 2344.
Manna, Z., & Pnueli, A. (1992). The Temporal Logic of Reactive and Concurrent Systems.
Springer.
McKinsey, J. (1941). A solution of the decision problem for the Lewis systems S2 and S4,
with an application to topology. Journal of Symbolic Logic, 6, 117134.
McKinsey, J., & Tarski, A. (1944). The algebra of topology. Annals of Mathematics, 45,
141191.
Muller, P. (1998a). A qualitative theory of motion based on spatio-temporal primitives.
In Cohn, A., Schubert, L., & Shapiro, S. (Eds.), Proceedings of the 6th International
Conference on Principles of Knowledge Representation and Reasoning (KR98), pp.
131142. Morgan Kaufmann.
240

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

Muller, P. (1998b). Space-time as a primitive for space and motion. In Guarino, N. (Ed.),
Proceedings of the International Conference on Formal Ontology in Information Systems (FOIS98), Vol. 46 of Frontiers in Artificial Intelligence and Applications, pp.
6376. IOS Press.
Nebel, B., & Burckert, H. (1995). Reasoning about relations: A maximal tractable subclass
of Allens interval algebra. Journal of the ACM, 42, 4366.
Nebel, B. (1996). Artificial intelligence: A computational perspective. In Brewka, G. (Ed.),
Principles of Knowledge Representation, pp. 237266. CSLI Publications.
Nutt, W. (1999). On the translation of qualitative spatial reasoning problems into modal
logics. In Burgard, W., Christaller, T., & Cremers, A. (Eds.), Advances in Artificial Intelligence. Proceedings of the 23rd Annual German Conference on Artificial
Intelligence (KI99), Vol. 1701 of Lecture Notes in Computer Science, pp. 113124.
Springer.
Ono, H., & Nakamura, A. (1980). On the size of refutation Kripke models for some linear
modal and tense logics. Studia Logica, 39, 325333.
Orlov, I. (1928). The calculus of compatibility of propositions. Mathematics of the USSR,
Sbornik, 35, 263286. (In Russian).
Post, E. (1946). A variant of a recursively unsolvable problem. Bulletin of the AMS, 52,
264268.
Pratt-Hartmann, I. (2002). A topological constraint language with component counting.
Journal of Applied Non-Classical Logics, 12, 441467.
Randell, D., Cui, Z., & Cohn, A. (1992). A spatial logic based on regions and connection.
In Nebel, B., Rich, C., & Swartout, W. (Eds.), Proceedings of the 3rd International
Conference on Principles of Knowledge Representation and Reasoning (KR92), pp.
165176. Morgan Kaufmann.
Renz, J. (1998). A canonical model of the region connection calculus. In Cohn, A., Schubert,
L., & Shapiro, S. (Eds.), Proceedings of the 6th International Conference on Principles of Knowledge Representation and Reasoning (KR98), pp. 330341. Morgan
Kaufmann.
Renz, J., & Nebel, B. (1998). Spatial reasoning with topological information. In Freksa, C.,
Habel, C., & Wender, K. (Eds.), Spatial CognitionAn Interdisciplinary Approach
to Representation and Processing of Spatial Knowledge, Vol. 1404 of Lecture Notes in
Computer Science, pp. 351372. Springer.
Renz, J., & Nebel, B. (1999). On the complexity of qualitative spatial reasoning. Artificial
Intelligence, 108, 69123.
Renz, J. (2002). A canonical model of the region connection calculus. Journal of Applied
Non-Classical Logics, 12, 469494.
Renz, J., & Nebel, B. (2001). Efficient methods for qualitative spatial reasoning. Journal
of Artificial Intelligence Research, 15, 289318.
Reynolds, M. (2003). The complexity of the temporal logic with until over general linear
time. Journal of Computer and System Science, 66, 393426.
241

fiGabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev

Reynolds, M. (2004). The complexity of the temporal logic over the reals. Submitted,
available at http://www.csse.uwa.edu.au/~mark/research/Online/CORT.htm.
Schwendimann, S. (1998). A new one-pass tableau calculus for PLTL. In de Swart, H. (Ed.),
Proceedings of the International Conference on Automated Reasoning with Analytic
Tableaux and Related Methods (TABLEAUX-98), Vol. 1397 of Lecture Notes in Artificial Intelligence, pp. 277291. Springer.
Sistla, A., & Clarke, E. (1985). The complexity of propositional linear temporal logics.
Journal of the Association for Computing Machinery, 32, 733749.
Sistla, A., & German, S. (1987). Reasoning with many processes. In Proceedings of the
Second IEEE Symposium on Logic in Computer Science (LICS87), pp. 138153. IEEE
Computer Society.
Slagle, J., Chang, C.-L., & Lee, R. (1969). Completeness theorems for semantic resolution
in consequence-finding. In Proceedings of the 1st International Joint Conference on
Artificial Intelligence (IJCAI69), pp. 281286. William Kaufmann.
Smith, T., & Park, K. (1992). An algebraic approach to spatial reasoning. International
Journal of Geographical Information Systems, 6, 177192.
Stockmeyer, L. (1974). The Complexity of Decision Problems in Automata Theory and
Logic. Ph.D. thesis, MIT.
Stockmeyer, L. (1987). Classifying the computational complexity of problems. Journal of
Symbolic Logic, 52, 143.
Stone, M. (1937). Application of the theory of Boolean rings to general topology. Transactions of the AMS, 41, 321364.
Tarski, A. (1938). Der Aussagenkalkul und die Topologie. Fundamenta Mathematicae, 31,
103134.
Tsao Chen, T. (1938). Algebraic postulates and a geometric interpretation of the Lewis
calculus of strict implication. Bulletin of the AMS, 44, 737744.
van Benthem, J. (1995). Temporal logic. In Gabbay, D., Hogger, C., & Robinson, J. (Eds.),
Handbook of Logic in Artificial Intelligence and Logic Programming, Vol. 4, pp. 241
350. Oxford Scientific Publishers.
van Emde Boas, P. (1997). The convenience of tilings. In Sorbi, A. (Ed.), Complexity, Logic
and Recursion Theory, Vol. 187 of Lecture Notes in Pure and Applied Mathematics,
pp. 331363. Marcel Dekker Inc.
Wolper, P. (1985). The tableau method for temporal logic: An overview. Logique et Analyse,
28, 119152.
Wolter, F. (1996). Properties of tense logics. Mathematical Logic Quarterly, 42, 481500.
Wolter, F., & Zakharyaschev, M. (2000a). Spatial reasoning in RCC-8 with Boolean region
terms. In Horn, W. (Ed.), Proceedings of the 14th European Conference on Artificial
Intelligence (ECAI 2000), pp. 244248. IOS Press.
Wolter, F., & Zakharyaschev, M. (2000b). Spatio-temporal representation and reasoning
based on RCC-8. In Cohn, A., Giunchiglia, F., & Seltman, B. (Eds.), Proceedings of
242

fiCombining Spatial and Temporal Logics: Expressiveness vs. Complexity

the 7th Conference on Principles of Knowledge Representation and Reasoning (KR
2000), pp. 314. Morgan Kaufmann.
Wolter, F., & Zakharyaschev, M. (2002). Qualitative spatio-temporal representation and
reasoning: a computational perspective. In Lakemeyer, G., & Nebel, B. (Eds.), Exploring Artificial Intelligence in the New Millenium, pp. 175216. Morgan Kaufmann.
Zimmermann, K. (1995). Measuring without measures: the delta-calculus. In Frank, A., &
Kuhn, W. (Eds.), Proceedings of the 2nd International Conference on Spatial Information Theory (COSIT), Vol. 988 of Lecture Notes in Computer Science, pp. 5967.
Springer.

243

fiJournal of Artificial Intelligence Research 23 (2005) 441-531

Submitted 11/04; published 4/05

Generalizing Boolean Satisfiability III: Implementation
Heidi E. Dixon
Matthew L. Ginsberg

dixon@otsys.com
ginsberg@otsys.com

On Time Systems, Inc.
1850 Millrace, Suite 1
Eugene, OR 97403 USA

David Hofer
Eugene M. Luks

hofer@cs.uoregon.edu
luks@cs.uoregon.edu

Computer and Information Science
University of Oregon
Eugene, OR 97403 USA

Andrew J. Parkes

parkes@cirl.uoregon.edu

CIRL
1269 University of Oregon
Eugene, OR 97403 USA

Abstract
This is the third of three papers describing zap, a satisfiability engine that substantially
generalizes existing tools while retaining the performance characteristics of modern highperformance solvers. The fundamental idea underlying zap is that many problems passed to
such engines contain rich internal structure that is obscured by the Boolean representation
used; our goal has been to define a representation in which this structure is apparent and
can be exploited to improve computational performance. The first paper surveyed existing
work that (knowingly or not) exploited problem structure to improve the performance of
satisfiability engines, and the second paper showed that this structure could be understood
in terms of groups of permutations acting on individual clauses in any particular Boolean
theory. We conclude the series by discussing the techniques needed to implement our ideas,
and by reporting on their performance on a variety of problem instances.

1. Introduction
This is the third of a series of three papers describing zap, a satisfiability engine that
substantially generalizes existing tools while retaining the performance characteristics of
modern high-performance solvers such as zChaff (Moskewicz, Madigan, Zhao, Zhang, &
Malik, 2001). In the first two papers in this series, we made arguments to the effect that:
 Many Boolean satisfiability problems incorporate a rich structure that reflects properties of the domain from which the problems arise, and recent improvements in the
performance of satisfiability engines can be understood in terms of their ability to
exploit this structure (Dixon, Ginsberg, & Parkes, 2004b, to which we will refer as
zap1).
 The structure itself can be understood in terms of groups (in the algebraic sense) of
permutations acting on individual clauses (Dixon, Ginsberg, Luks, & Parkes, 2004a,
to which we will refer as zap2).
c
2005
AI Access Foundation. All rights reserved.

fiDixon, Ginsberg, Hofer, Luks & Parkes

We showed that an implementation based on these ideas could be expected to combine
the attractive computational properties of a variety of recent ideas, including efficient implementations of unit propagation (Zhang & Stickel, 2000) and extensions of the Boolean language to include cardinality or pseudo-Boolean constraints (Barth, 1995; Dixon & Ginsberg,
2000; Hooker, 1988), parity problems (Tseitin, 1970), or a limited form of quantification
known as qprop (Ginsberg & Parkes, 2000). In this paper, we discuss the implementation
of a prover based on these ideas, and describe its performance on pigeonhole, parity and
clique coloring problems. These classes of problems are known to be exponentially difficult
for conventional Boolean satisfiability engines, and their formalization also highlights the
group-based nature of the reasoning involved.
From a technical point of view, this is the most difficult of the three zap papers; we need
to draw on the algorithms and theoretical constructions from zap2 and on results from computational group theory (GAP Group, 2004; Seress, 2003) regarding their implementation.
Our overall plan for describing the implementation is as follows:
1. Section 2 is a review of material from zap2. We begin in Section 2.1 by presenting both
the Boolean satisfiability algorithms that we hope to generalize and the basic algebraic
ideas underlying zap. Section 2.2 describes the group-theoretic computations required
by the zap implementation.
2. Section 3 gives a brief  and necessarily incomplete  introduction to some of the ideas
in computational group theory that we use.
3. Sections 4 and 5 describe the implementations of the computations discussed in Section 2. For each basic construction, we describe the algorithm used and give an
example of the computation in action. If there is an existing implementation of something in the public domain system gap (2004), we only provide a pointer to that
implementation; for concepts that we needed to implement from scratch, additional
detail is provided.
4. Section 6 extends the basic algorithms of Section 5 to deal with unit propagation,
where we want to compute not a single unit clause instance, but a list of all of the
unit consequences of an augmented clause.
5. Section 7 discusses the implementation of Zhang and Stickels (2000) watched literal
idea in our setting.
6. Section 8 describes a technique that can be used to select among the possible resolvents
of two augmented clauses. This is functionality with no analog in a conventional
prover, where there is only a single ground reason for the truth or falsity of any given
variable. If the reasons are augmented clauses, there may be a variety of ways in
which ground instances of those clauses can be combined.
7. After describing the algorithms, we present experimental results regarding performance in Sections 9 and 10. Section 9 reports on the performance of zaps individual
algorithmic components, while Section 10 contrasts zaps overall performance to that
of its cnf-based predecessors.1 Since our focus in this paper is on the algorithms
1. A description of zaps input language is contained in Appendix B.

442

fiZAP 3: Implementation

needed by zap, we report performance only for relatively theoretical examples that
clearly involve group-based reasoning. Performance on a wider range of problem
classes will be reported elsewhere.
8. Concluding remarks appear in Section 11.
Except for Section 3, proofs are generally deferred to Appendix A in the interests of maintaining the continuity of our exposition. Given the importance of computational group
theory to the ideas that we will be presenting, we strongly suggest that the reader work
through the proofs in Section 3 of the paper.
This is a long and complex paper; we make no apologies. Zap is an attempt to synthesize
two very different fields, each complex in its own right: computational group theory and
implementations of Boolean satisfiability engines. Computational group theory, in addition
to its inherent complexity, is likely to be foreign to an AI audience. Work on complete
algorithms for Boolean satisfiability has also become increasingly sophisticated over the
past decade or so, with the introduction of substantial and nonintuitive modifications to the
original dpll algorithm such as relevance-bounded learning (Bayardo & Miranker, 1996;
Bayardo & Schrag, 1997; Ginsberg, 1993) and watched literals (Zhang & Stickel, 2000).
As we bring these two fields together, we will see that a wide range of techniques from
computational group theory is relevant to the problems of interest to us; our goal is also
not simply to translate dpll to the new setting, but to show that all of the recent work
on Boolean satisfiability can be moved across. In at least one case (Lemma 5.26), we also
need to extend existing computational group theory results. And finally, there are new
satisfiability techniques and possibilities that arise only because of the synthesis that we are
proposing (Section 8), and we will describe some of those as well.
This paper is not intended to be self-contained. We assume throughout that the reader
is familiar with the material that we presented in zap2; some of the results from that paper
are repeated here for convenience, but the accompanying text is not intended to stand alone.
Finally  and in spite of the disclaimers of the previous two paragraphs  this paper is
not intended to be complete. Our goal is to present a practical minimum of what is required
to implement an effective group-based reasoning system. The results that we have obtained,
both theoretical as described in zap2 and practical as described here, excite us. But we are
just as excited by the number of issues that we have not yet explored. Our primary goal is
to present the foundation needed if other interested researchers are to explore these ideas
with us.

2. ZAP Fundamentals and Basic Structure
Our overview of zap involves summarizing work from two distinct areas: existing Boolean
satisfiability engines, and the group-theoretic elements underlying zap.
2.1 Boolean Satisfiability
We begin with a description of the architecture of modern Boolean satisfiability engines.
We start with the unit propagation procedure, which we describe as follows:

443

fiDixon, Ginsberg, Hofer, Luks & Parkes

Definition 2.1 Given a Boolean satisfiability problem described in terms of a set C of
clauses, a partial assignment is an assignment of values (true or false) to some subset
of the variables appearing in C. We represent a partial assignment P as a sequence of
consistent literals P = hli i where the appearance of vi in the sequence means that vi has
been set to true, and the appearance of vi means that vi has been set to false.
An annotated partial assignment is a sequence P = h(li , ci )i where ci is the reason for
the associated choice li . If ci = true, it means that the variable was set as the result of a
branching decision; otherwise, ci is a clause that entails li by virtue of the choices of the
previous lj for j < i. An annotated partial assignment will be called sound with respect to
a set of constraints C if C |= ci for each reason ci . (See zap2 for additional details.)
Given a (possibly annotated) partial assignment P , we denote by S(P ) the literals that
are satisfied by P , and by U (P ) the set of literals that are unvalued by P .
Procedure 2.2 (Unit propagation) To compute Unit-Propagate(C, P ) for a set C of
clauses and an annotated partial assignment P = h(l1 , c1 ), . . . , (ln , cn )i:
1 while there is a c  C with c  S(P ) =  and |c  U (P )|  1
2
do if c  U (P ) = 
3
then li  the literal in c with the highest index in P
4
return htrue, resolve(c, ci )i
5
else l  the literal in c unassigned by P
6
P  hP, (l, c)i
7 return hfalse, P i
The result returned depends on whether or not a contradiction was encountered during
the propagation, with the first result returned being true if a contradiction was found and
false if none was found. In the former case, where the clause c has no unvalued literals
(line 2), li is the last literal set in c, and ci is the reason that li was set in a way that caused
c to be unsatisfiable. We resolve c with ci and return the result as a new nogood for the
problem in question. Otherwise, we eventually return the partial assignment, augmented
to include the variables that were set during the propagation process.
Given unit propagation, the overall inference procedure is the following:
Procedure 2.3 (Relevance-bounded learning, rbl) Given a sat problem C, a set of
learned nogoods D and an annotated partial assignment P , to compute rbl(C, D, P ):

444

fiZAP 3: Implementation

1 hx, yi  unit-propagate(C  D, P )
2 if x = true
3
then c  y
4
if c is empty
5
then return failure
6
else remove successive elements from P so that c is unit
7
D  learn(D, P, c)
8
return rbl(C, D, P )
9
else P  y
10
if P is a solution to C
11
then return P
12
else l  a literal not assigned a value by P
13
return rbl(C, D, hP, (l, true)i)
As might be expected, the procedure is recursive. If at any point unit propagation produces a contradiction c, we use the (currently unspecified) learn procedure to incorporate c
into the solvers current state, and then recurse. If c is empty, it means that we have derived
a contradiction and the procedure fails. In the backtracking step (line 6), we backtrack not
just until c is satisfiable, but until it enables a unit propagation. This technique is used in
zChaff (Moskewicz et al., 2001). It leads to increased flexibility in the choice of variable
to be assigned after the backtrack is complete, and generally improves performance.
If unit propagation does not indicate the presence of a contradiction or produce a solution
to the problem in question, we pick an unvalued literal, set it to true, and recurse again.
Note that we dont need to set the literal l to true or false; if we eventually need to backtrack
and set l to false, that will be handled by the modification to P in line 6.
Finally, we need to present the procedure used to incorporate a new nogood into the
clausal database C. In order to do that, we make the following definition:
Definition 2.4 Let i li be a clause, which we will denote by c, and let P be a partial
assignment. We will say that the possible value of c under P is given by
poss(c, P ) = |{i|li 6 P }|  1
If no ambiguity is possible, we will write simply poss(c) instead of poss(c, P ). In other
words, poss(c) is the number of literals that are either already satisfied or not valued by P ,
reduced by one (since the clause requires at least one true literal).
Note that poss(c, P ) = |c  [U (P )  S(P )]|  1, since each expression is one less than the
number of potentially satisfied literals in c.
The possible value of a clause is essentially a measure of what other authors have called
its irrelevance (Bayardo & Miranker, 1996; Bayardo & Schrag, 1997; Ginsberg, 1993). An
unsatisfied clause c with poss(c, P ) = 0 can be used for unit propagation; we will say that
such a clause is unit. If poss(c, P ) = 1, it means that a change to a single variable can
lead to a unit propagation, and so on. The notion of learning used in relevance-bounded
inference is now captured by:
445

fiDixon, Ginsberg, Hofer, Luks & Parkes

Procedure 2.5 Given a set of clauses C and an annotated partial assignment P , to compute learn(C, P, c), the result of adding to C a clause c and removing irrelevant clauses:
1
2

remove from C any d  C with poss(d, P ) > k
return C  {c}

We hope that all of this is familiar; if not, please refer to zap2 or to the other papers
that we have cited for fuller explanations.
In zap, we continue to work with these procedures in approximately their current form,
but replace the idea of a clause (a disjunction of literals) with that of an augmented clause:
Definition 2.6 An augmented clause in an n-variable Boolean satisfiability problem is a
pair (c, G) where c is a Boolean clause and G is a group such that G  Wn . A (nonaugmented) clause c0 is an instance of an augmented clause (c, G) if there is some g  G such
that c0 = cg .2 The clause c itself will be called the base instance of (c, G).
Roughly speaking, an augmented clause consists of a conventional clause and a group G
of permutations of the literals in the theory; the intent is that we can act on the clause with
any element of the group and still get a clause that is part of the original theory. The
group G is required to be a subgroup of the group of permutations and complementations
(Harrison, 1989) Wn = S2 o Sn , where each permutation g  G can permute the variables
in the problem and flip the signs of an arbitrary subset as well. We showed in zap2 that
suitably chosen groups correspond to cardinality constraints, parity constraints (the group
flips the signs of any even number of variables), and universal quantification over finite
domains.
We must now lift the previous three procedures to an augmented setting. In unit
propagation, for example, instead of checking to see if any clause c  C is unit given the
assignments in P , we now check to see if any augmented clause (c, G) has a unit instance.
Other than that, the procedure is essentially unchanged from Procedure 2.2:
Procedure 2.7 (Unit propagation) To compute Unit-Propagate(C, P ) for a set of
clauses C and an annotated partial assignment P = h(l1 , c1 ), . . . , (ln , cn )i:
1
2
3
4
5
6
7

while there is a (c, G)  C and g  G with cg  S(P ) =  and |cg  U (P )|  1
do if cg  U (P ) = 
then li  the literal in cg with the highest index in P
return htrue, resolve((cg , G), ci )i
else l  the literal in cg unassigned by P
P  hP, (l, (cg , G))i
return hfalse, P i
The basic inference procedure itself is also virtually unchanged:

2. As in zap2 and as used by the computational group theory community, we denote the image of a clause
c under a group element g by cg instead of the possibly more familiar g(c). As explained in zap2, this
reflects the fact that the composition f g of two permutations acts with f first and with g second.

446

fiZAP 3: Implementation

Procedure 2.8 (Relevance-bounded learning, rbl) Given a sat problem C, a set of
learned clauses D, and an annotated partial assignment P , to compute rbl(C, D, P ):
1 hx, yi  unit-propagate(C  D, P )
2 if x = true
3
then (c, G)  y
4
if c is empty
5
then return failure
6
else remove successive elements from P so that c is unit
7
D  learn(D, P, (c, G))
8
return rbl(C, D, P )
9
else P  y
10
if P is a solution to C
11
then return P
12
else l  a literal not assigned a value by P
13
return rbl(C, D, hP, (l, true)i)
In line 3, although unit propagation returns an augmented clause (c, G), the base instance
c is still the reason for the backtrack by virtue of line 6 of Procedure 2.7. It follows that
line 6 of Procedure 2.8 is unchanged from the Boolean version.
To lift Procedure 2.5 to our setting, we need an augmented version of Definition 2.4:
Definition 2.9 Let (c, G) be an augmented clause, and P a partial assignment. Then by
poss((c, G), P ) we will mean the minimum possible value of an instance of (c, G), so that
poss((c, G), P ) = min poss(cg , P )
gG

Procedure 2.5 can now be used unchanged, with d being an augmented clause instead of a
simple one. The effect of Definition 2.9 is to cause us to remove only augmented clauses for
which every instance is irrelevant. Presumably, it will be useful to retain the clause as long
as it has some relevant instance.
In zap2, we showed that a proof engine built around the above three procedures would
have the following properties:
 Since the number of generators of a group can be made logarithmic in the group size,
it would achieve exponential improvements in basic representational efficiency.
 Since only k-relevant nogoods are retained as the search proceeds, the memory requirements remain polynomial in the size of the problem being solved.
 It can produce polynomially sized proofs of the pigeonhole and clique coloring problems, and any parity problem.
 It generalizes first-order inference provided that all quantifiers are universal and all
domains of quantification are finite.
We stated without proof (and will show in this paper) that the unit propagation procedure 2.7 can be implemented in a way that generalizes both subsearch (Ginsberg & Parkes,
2000) and Zhang and Stickels (2000) watched literal idea.
447

fiDixon, Ginsberg, Hofer, Luks & Parkes

2.2 Group-Theoretic Elements
Examining the above three procedures, the elements that are new relative to Boolean engines
are the following:
1. In line 1 of the unit propagation procedure 2.7, we need to find unit instances of an
augmented clause (c, G).
2. In line 4 of the same procedure 2.7, we need to compute the resolvent of two augmented
clauses.
3. In line 1 of the learning procedure 2.5, we need to determine if an augmented clause
has any relevant instances.
The first and third of these needs are different from the second. For resolution, we need
the following definitions:
Definition 2.10 For a permutation p and set S with S p = S, by p|S we will mean the
restriction of p to the given set, and we will say that p is a lifting of p|S back to the original
set on which p acts.
Definition 2.11 For a set , we will denote by Sym() the group of permutations of .
If G  Sym() is a subgroup of this group and S  , we will say that G acts on S.3
Definition 2.12 Suppose that G acts on a set S. Then for any x  S, the orbit of x in G,
to be denoted by xG , is given by xG = {xg |g  G}. If T  S, then the G-closure of T , to be
denoted T G , is the set
T G = {tg |t  T and g  G}

Definition 2.13 For K1 , . . . , Kn   and G1 , . . . , Gn  Sym(), we will say that a permutation   Sym() is a stable extension of G1 , . . . , Gn for K1 , . . . , Kn if there are gi  Gi
such that for all i, |K Gi = gi |K Gi . We will denote the set of stable extensions of G1 , . . . , Gn
i

i

for K1 , . . . , Kn by stab(Ki , Gi ).
The set of stable extensions stab(Ki , Gi ) is closed under composition, and is therefore a
subgroup of Sym().
Definition 2.14 Suppose that (c1 , G1 ) and (c2 , G2 ) are augmented clauses. Then the result of resolving (c1 , G1 ) and (c2 , G2 ), to be denoted by resolve((c1 , G1 ), (c2 , G2 )), is the
augmented clause (resolve(c1 , c2 ), stab(ci , Gi )  Wn ).
It follows from the above definitions that computing the resolvent of two augmented
clauses as required by Procedure 2.7 is essentially a matter of computing the set of stable
extensions of the groups in question. We will return to this problem in Section 4.
The other two problems can both be viewed as instances of the following:
3. For convenience, we depart from standard usage and permit G to map points in S to images outside
of S.

448

fiZAP 3: Implementation

Definition 2.15 Let c be a clause, viewed as a set of literals, and G a group of permutations
acting on c. Now fix sets of literals S and U , and an integer k. We will say that the ktransporter problem is that of finding a g  G such that cg  S =  and |cg  U |  k, or
reporting that no such g exists.
To find a unit instance of (c, G), we set S to be the set of satisfied literals and U the
set of unvalued literals. Taking k = 1 implies that we are searching for an instance with no
satisfied and at most one unvalued literal.
To find a relevant instance, we set S =  and U to be the set of all satisfied or unvalued
literals. Taking k to be the relevance bound corresponds to a search for a relevant instance.
The remainder of the theoretical material in this paper is therefore focused on these two
problems: computing the stable extensions of a pair of groups, and solving the k-transporter
problem. Before we discuss the techniques used to solve these two problems, we present a
brief overview of computational group theory generally.

3. Computational Group Theory
Both group theory at large and computational group theory specifically (the study of effective computational algorithms that solve group-theoretic problems) are far too broad
to allow detailed presentations in a single journal paper. We ourselves generally refer to
Rotmans An Introduction to the Theory of Groups (1994) for general information, and
to Seress Permutation Group Algorithms (2003) for computational group theory specifically, although there are many excellent texts in both areas. There is also an abbreviated
introduction to group theory in zap2.
If we cannot substitute for these other references, our goal here is to provide enough
general understanding of computational group theory that it will be possible to work through
some examples in what follows. With that in mind, there are three basic ideas that we hope
to convey:
1. Stabilizer chains. These underlie the fundamental technique whereby large groups are
represented efficiently. They also underlie many of the subsequent computations done
using those groups.
2. Group decompositions. Given a group G and a subgroup H < G, H can be used in
a natural way to partition G. Each of the partitions can itself be partitioned using a
subgroup of H, and so on; this gradual refinement underpins many of the search-based
group algorithms that have been developed.
3. Lex-leader search. In general, it is possible to establish a lexicographic ordering on the
elements of a permutation group; if we are searching for an element of the group having
a particular property (as in the k-transporter problem), we can assume without loss
of generality that we are looking for an element that is minimal under this ordering.
This often allows the search to be pruned, since any portion of the search that can be
shown not to contain such a minimal element can be eliminated.

449

fiDixon, Ginsberg, Hofer, Luks & Parkes

3.1 Stabilizer Chains
While the fact that a group G can be described in terms of an exponentially smaller number
of generators is attractive from a representational point of view, there are many issues that
arise if a large set of clauses is represented in this way. Perhaps the most fundamental
is that of simple membership: How can we tell if a fixed clause c0 is an instance of the
augmented clause (c, G)?
In general, this is an instance of the 0-transporter problem; we need some g  G for
which cg , the image of c under g, does not intersect the complement of c0 . A simpler but
clearly related problem assumes that we have a fixed permutation g such that cg = c0 ; is
g  G or not? Given a representation of G in terms simply of its generators, it is not
obvious how this can be determined quickly.
Of course, if G is represented via a list of all of its elements, we could sort the elements
lexicographically and use a binary search to determine if g were included. Virtually any
problem of interest to us can be solved in time polynomial in the size of the groups involved,
but we would like to do better, solving the problems in time polynomial in the total size
of the generators, and therefore generally polynomial in the logarithm of the size of the
groups (and so polylog in the size of the original clausal database). We will call a procedure
polynomial only if it is indeed polytime in the number of generators of G and in the size of
the set of literals on which G acts. It is only for such polynomial procedures that we can
be assured that zaps representational efficiencies will mature into computational gains.4
For the membership problem, that of determining if g  G given a representation of G
in terms of its generators, we need to have a coherent way of understanding the structure
of the group G itself. We suppose that G is a subgroup of the group Sym() of symmetries
of some set , and we enumerate the elements of  as  = {l1 , . . . , ln }.
There will now be some subset G[2]  G that fixes l1 in that for any h  G[2] , we have
h
l1 = l1 . It is easy to see that G[2] is closed under composition, since if any two elements fix
l1 , then so does their composition. It follows that G[2] is a subgroup of G. In fact, we have:
Definition 3.1 Given a group G acting on a set  and a subset L  , the point stabilizer
of L is the subgroup GL  G of all g  G such that lg = l for every l  L. The set stabilizer
of L is that subgroup G{L}  G of all g  G such that Lg = L.
Having defined G[2] as the point stabilizer of l1 , we can go on to define G[3] as the
point stabilizer of l2 within G[2] , so that G[3] is in fact the point stabilizer of {l1 , l2 } in G.
Similarly, we define G[i+1] to be the point stabilizer of li in G[i] and thereby construct a
chain of stabilizers
G = G[1]  G[2]      G[n] = 1
where the last group is necessarily trivial because once n  1 points of  are stabilized, the
last point must be also.
If we want to describe G in terms of its generators, we will now assume that we describe
all of the G[i] in terms of generators, and furthermore, that the generators for G[i] are a
superset of the generators for G[i+1] . We can do this because G[i+1] is a subgroup of G[i] .
4. The development of computationally efficient procedures for solving permutation group problems appears
to have begun with Sims (1970) pioneering work on stabilizer chains.

450

fiZAP 3: Implementation

Definition 3.2 A strong generating set S for a group G  Sym(l1 , . . . , ln ) is a set of
generators for G with the property that
hS  G[i] i = G[i]
for i = 1, . . . , n.
As usual, hgi i denotes the group generated by the gi .
It is easy to see that a generating set is strong just in case it has the property discussed
above, in that each G[i] can be generated incrementally from G[i+1] and the generators that
are in fact elements of G[i]  G[i+1] .
As an example, suppose that G = S4 , the symmetric group on 4 elements (which we
denote 1, 2, 3, 4). Now it is not hard to see that S4 is generated by the 4-cycle (1, 2, 3, 4)
and the transposition (3, 4), but this is not a strong generating set. G[2] is the subgroup of
S4 that stabilizes 1 (and is therefore isomorphic to S3 , since it can randomly permute the
remaining three points) but
hS  G[2] i = h(3, 4)i = G[3] 6= G[2]

(1)

If we want a strong generating set, we need to add (2, 3, 4) or a similar permutation to the
generating set, so that (1) becomes
hS  G[2] i = h(2, 3, 4), (3, 4)i = G[2]
Here is a slightly more interesting example. Given a permutation, it is always possible
to write that permutation as a composition of transpositions. One possible construction
maps 1 where it is supposed to go, then ignores it for the rest of the construction, and so
on. Thus we have for example
(1, 2, 3, 4) = (1, 2)(1, 3)(1, 4)

(2)

where the order of composition is from left to right, so that 1 maps to 2 by virtue of the
first transposition and is then left unaffected by the other two, and so on.
While the representation of a permutation in terms of transpositions is not unique, the
parity of the number of transpositions is; a permutation can always be represented as a
product of an even or an odd number of transpositions, but not both. Furthermore, the
product of two transposition products of lengths l1 and l2 can obviously be represented as
a product of length l1 + l2 , and it follows that the product of two even permutations is
itself even, and we have:
Definition 3.3 The alternating group of order n, to be denoted by An , is the subgroup of
even permutations of Sn .
What about a strong generating set for An ? If we fix the first n  2 points, then the
[n1]
transposition (n  1, n) is obviously odd, so we must have An
= 1, the trivial group.
[i]
For any smaller i, we can get a subset of An by taking the generators for Sn and operating
on each as necessary with the transposition (n  1, n) to make it even. It is not hard to
451

fiDixon, Ginsberg, Hofer, Luks & Parkes

see that an n-cycle is odd if and only if n is even (consider (2) above), so given the strong
generating set
{(n  1, n), (n  2, n  1, n), . . . , (2, 3, . . . , n), (1, 2, . . . , n)}
for Sn , a strong generating set for An if n is odd is
{(n  2, n  1, n), (n  1, n)(n  3, n  2, n  1, n), . . . , (n  1, n)(2, 3, . . . , n), (1, 2, . . . , n)}
and if n is even is
{(n  2, n  1, n), (n  1, n)(n  3, n  2, n  1, n), . . . , (2, 3, . . . , n), (n  1, n)(1, 2, . . . , n)}
We can simplify these expressions slightly to get
{(n  2, n  1, n), (n  3, n  2, n  1), . . . , (2, 3, . . . , n  1), (1, 2, . . . , n)}
if n is odd and
{(n  2, n  1, n), (n  3, n  2, n  1), . . . , (2, 3, . . . , n), (1, 2, . . . , n  1)}
if n is even.
Given a strong generating set, it is easy to compute the size of the original group G. To
do this, we need the following well known definition and result:
Definition 3.4 Given groups H  G and g  G, we define Hg to be the set of all hg for
h  H. For any such g, we will say that Hg is a (right) coset of H in G.
Proposition 3.5 Let Hg1 and Hg2 be two cosets of H in G. Then |Hg1 | = |Hg2 | and the
cosets are either identical or disjoint.
In other words, given a subgroup H of a group G, the cosets of H partition G. This
leads to:
Definition 3.6 For groups H  G, the index of H in G, denoted [G : H], is the number
of distinct cosets of H in G.
Corollary 3.7 For a finite group G, [G : H] =

|G|
|H| .

Given that the cosets partition the original group G, it is natural to think of them as
defining an equivalence relation on G, where x  y if and only if x and y belong to the
same coset of H. We have:
Proposition 3.8 x  y if and only if xy 1  H.
Proof. If xy 1 = h  H and x is in a coset Hg so that x = h0 g for some h0  H, then
y = h1 x = h1 h0 g is in the same coset. Conversely, if x = hg and y = h0 g are in the same
coset, then xy 1 = hgg 1 h01 = hh01  H.
Many equivalence relations on groups are of this form. Indeed, if  is any right invariant
equivalence relation on the elements of a group G (so that if x  y, then xz  yz for any
z  G), then there is some H  G such that the cosets of H define the equivalence relation.
[i]
Returning to stabilizer chains, recall that we denote by liG the orbit of li under G[i]
(i.e, the set of all points to which G[i] maps li ). We now have:
452

fiZAP 3: Implementation

Proposition 3.9 Given a group G acting on a set {l1 , . . . , ln } and associated stabilizer
chain G[1]      G[n] ,
Y [i]
|G| =
|liG |
(3)
i

Proof. We know that
|G| =

|G|
|G[2] | = [G : G[2] ]|G[2] |
|G[2] |

or inductively that
|G| =

Y
[G[i] : G[i+1] ]
i

But it is easy to see that the distinct cosets of G[i+1] in G[i] correspond exactly to the points
to which G[i] maps li , so that
[i]
[G[i] : G[i+1] ] = |liG |
and the result follows.
Note that the expression in (3) is easy to compute given a strong generating set. As an
example, given the strong generating set {(1, 2, 3, 4), (2, 3, 4), (3, 4)} for S4 , it is clear that
[3]
[2]
S4 = h(3, 4)i and the orbit of 3 is of size 2. The orbit of 2 in S4 = h(2, 3, 4), (3, 4)i is of
[1]
size 3, and the orbit of 1 in S4 is of size 4. So the total size of the group is 4! = 24, hardly
a surprise.
For A4 , a strong generating set is {(3, 4)(1, 2, 3, 4), (2, 3, 4)} = {(1, 2, 3), (2, 3, 4)}. The
[2]
[1]
orbit of 2 in A4 = h(2, 3, 4)i is clearly of size 3, and the orbit of 1 in A4 = A4 is of
size 4. So |A4 | = 12. In general, of course, there are exactly two cosets of the alternating
group because all of the odd permutations can be constructed by multiplying the even
permutations in An by a fixed transposition t. Thus |An | = n!/2.
We can evaluate the size of An using strong generators by realizing that the orbit of 1
is of size n, that of 2 is of size n  1, and so on, until the orbit of n  2 is of size 3. The
orbit of n  1 is of size 1, however, since the transposition (n  1, n) is not in An . Thus
|An | = n!/2 as before.
We can also use the strong generating set to test membership in the following way.
Suppose that we have a group G described in terms of its strong generating set (and therefore
its stabilizer chain G[1]      G[n] ), and a specific permutation . Now if (1) = k, there
are two possibilities:
1. If k is not in the orbit of 1 in G = G[1] , then clearly  6 G.
2. If k is in the orbit of 1 in G[1] , select g1  G[1] with 1g1 = g1 (1) = k. Now we construct
1 = g11 , which fixes 1, and we determine recursively if 1  G[2] .
At the end of the process, we will have stabilized all of the elements moved by G, and
should have n+1 = 1. If so, the original   G; if not,  6 G. This procedure is known as
sifting.
Continuing with our example, let us see if the 4-cycle  = (1, 2, 3, 4) is in S4 and in A4 .
[1]
For the former, we see that (1) = 2 and (1, 2, 3, 4)  S4 . This produces 1 = 1, and we
can stop and conclude that   S4 (once again, hardly a surprise).
453

fiDixon, Ginsberg, Hofer, Luks & Parkes

[1]

For the second, we know that (1, 2, 3)  A4 and we get 1 = (1, 2, 3, 4)(1, 2, 3)1 =
(3, 4). Now we could actually stop, since (3, 4) is obviously odd, but let us continue with
the procedure. Since 2 is fixed by 1 , we have 2 = 1 . Now 3 is moved to 4 by 2 , but
[3]
A4 is the trivial group, so we conclude correctly that (1, 2, 3, 4) 6 A4 .
3.2 Coset Decomposition
Some of the group problems that we will be considering (e.g., the k-transporter problem)
subsume what was described in zap1 as subsearch (Dixon et al., 2004b; Ginsberg & Parkes,
2000). Subsearch is known to be NP-hard, so it follows that k-transporter must be as well.
That suggests that the group-theoretic methods for solving it will involve search in some
way.
The search involves a potential examination of all of the instances of some augmented
clause (c, G), or, in group theoretic terms, a potential examination of each member of the
group G. The computational group theory community often approaches such a search
problem by gradually decomposing G into smaller and smaller cosets. What we will call a
coset decomposition tree is produced, where the root of the tree is the entire group G and
the leaf nodes are individual elements of G:
Definition 3.10 Let G be a group, and G[1]      G[n] a stabilizer chain for it. A coset
decomposition tree for G is a tree whose vertices at the ith level are the cosets of G[i] and
for which the parent of a particular G[i] g is that coset of G[i1] that contains it.
At any particular level i, the cosets correspond to the points to which the sequence hl1 , . . . , li i
can be mapped, with the points in the image of li identifying the children of any particular
node at level i  1.
As an example, suppose that we consider the augmented clause
(a  b, Sym(a, b, c, d))

(4)

corresponding to the collection of ground clauses
ab
ac
ad
bc
bd
cd
Suppose also that we are working with an assignment for which a and b are true and c
and d are false, and are trying to determine if any instance of (4) is unsatisfied. Assuming
that we take l1 to be a through l4 = d, the coset decomposition tree associated with S4 is
the following:

454

fiZAP 3: Implementation

b, c, d)
sPSym(a,
PP

@

PP

@
PP


PP
@

PP


@
PP


PP (ad)

Sym(b, c, d)
(ab)s
@
P
@s(ac)
Ps
s 
A
A
A






A
 A
 A
 A
 A
A
A
A
A




A
A
A
A




Sym(c, d)s
(bc)
(bd)
(bc)
(bd)
(bc)
(bd)
(bc)
AAs(bd)
A
A
A
1
1
1
As
As
As
s
s
s
s
s
s
s
B
B
B
B




 E
 E
 E
 E
B
B
B
B
E
E
E
E








B
B
B
B




 E
 E
 E
 E
B
B
B
B
E
E
E
E








s s s EEs s BBs s s s EEs s BBs s s s EEs s BBs s s s EEs s BBs
1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd)
* *
An explanation of the notation here is surely in order. The nodes on the lefthand
edge are labeled by the associated groups; for example, the node at level 2 is labeled with
Sym(b, c, d) because this is the point at which we have fixed a but b, c and d are still allowed
to vary.
As we move across the row, we find representatives of the cosets that are being considered. So moving across the second row, the first entry (ab) means that we are taking the
coset of the basic group Sym(b, c, d) that is obtained by multiplying each element by (ab)
on the right. This is the coset that maps a uniformly to b.
On the lower rows, we multiply the coset representatives associated with the nodes
leading to the root. So the third node in the third row, labeled with (bd), corresponds to
the coset Sym(c, d)  (bd).5 The two elements of this coset are (bd) and (cd)(bd) = (bdc).
The point b is uniformly mapped to d, a is fixed, and c can either be fixed or mapped to b.
The fourth point on this row corresponds to the coset
Sym(c, d)  (ab) = {(ab), (cd)(ab)}
The point a is uniformly mapped to b, and b is uniformly mapped to a. c and d can be
swapped or not.
The fifth point is the coset
Sym(c, d)  (bc)(ab) = Sym(c, d)  (abc) = {(abc), (abcd)}

(5)

a is still uniformly mapped to b, and b is now uniformly mapped to c. c can be mapped
either to a or to d.
For the fourth line, the basic group is trivial and the single member of the coset can be
obtained by multiplying the coset representatives on the path to the root. Thus the ninth
and tenth nodes (marked with asterisks in the tree) correspond to the permutations (abc)
and (abcd) respectively, and do indeed partition the coset of (5).
5. As here, we will occasionally denote the group multiplication operator explicitly by  to improve the
clarity of the typesetting.

455

fiDixon, Ginsberg, Hofer, Luks & Parkes

Understanding how this structure is used in search is straightforward. At the root, the
original augmented clause (4) may indeed have unsatisfiable instances. But when we move
to the first child, we know that the image of a is a, so that the instance of the clause in
question is a  x for some x. Since a is true for the assignment in question, it follows that
the clause must be satisfied. In a similar way, mapping a to b also must produce a satisfied
clause. The search space is already reduced to:
b, c, d)
sPSym(a,

@PPP
PP
@
PP
@
PP
PP
@
PP
@
PP
@s(ac)
Ps(ad)
A


A
A

 A
A
A


A
A


1s
1s
s(bc) AAs(bd)
s(bc) AAs(bd)
B
B
E
E


B
B
 E
 E


E
E
B
B




B
B
 E
 E


E
E
B
s s s Es s Bs s s s Es s BBs











Sym(b, c, d)s

(ab)s

1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd) 1 (cd)
If we map a to c, then the first point on the next row corresponds to mapping b to b,
producing a satisfiable clause. If we map b to a (the next node; b is mapped to c at this
node but then c is mapped to a by the permutation (ac) labeling the parent), we also get a
satisfiable clause. If we map b to d, we will eventually get an unsatisfiable clause, although
it is not clear how to recognize that without expanding the two children. The case where a
is mapped to d is similar, and the final search tree is:
s Sym(a, b, c, d)
P

@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)
A
A
 A
 A
A
A


A
A


(bc)
(bd)
(bc)
A
AAs(bd)
1s
1
As
s
s
s
B
E
B
 E
B
 E
B
 E
s BBs
s EEs









Sym(b, c, d)
s 




(ab)s

1 (cd)

456

1 (cd)

fiZAP 3: Implementation

Instead of the six clauses that might need to be examined as instances of the original
(4), only four leaf nodes need to be considered. The internal nodes that were pruned above
can be pruned without generation, since the only values that need to be considered for a are
necessarily c and d (the unsatisfied literals in the theory). At some level, then, the above
search space becomes:
s Sym(a, b, c, d)
P
@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)
A
A
A
A
AAs(bd)
s(bc)
B
E
B
 E
B
 E
B
 E
B
s Bs
s EEs

1 (cd)

1 (cd)

3.3 Lex Leaders
Although the remaining search space in this example already examines fewer leaf nodes
than the original, there still appears to be some redundancy. To understand one possible
simplification, recall that we are searching for a group element g for which cg is unsatisfied
given the current assignment. Since any such group element suffices, we can (if we wish)
search for that group element that is smallest under the lexicographic ordering of the group
itself:
Definition 3.11 Let G  Sym() be a group, and  = 1 , . . . , n an ordering of the
elements of . For g1 , g2  G, we will write g1 < g2 if there is some i with jg1 = jg2 for
all j < i and ig1 < ig2 .
Since the ordering defined by Definition 3.11 is a total order, we immediately have:
Lemma 3.12 If S  Sym() for some ordered set , then S has a unique minimal
element.
The minimal element of S is typically called a lexicographic leader or lex leader of S.
In our example, imagine that there were a solution (i.e., a group element corresponding
to an unsatisfied instance) under the right hand node at depth three. Now there would
necessarily also have been an analogous solution under the preceding node at depth three,
since the two search spaces are in some sense identical. The two hypothetical group elements
would be identical except the images of a and b would be swapped. Since the group elements
under the left hand node precede those under the right hand node in the lexicographic
457

fiDixon, Ginsberg, Hofer, Luks & Parkes

ordering, it follows that the lexicographically least element (which is all that were looking
for) is not under the right hand node, which can therefore be pruned. The search space
becomes:
sPSym(a, b, c, d)
@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)
A
A
A
A
AAs(bd)
B
B
B
B
s BBs

1 (cd)
This particular technique is quite general: whenever we are searching for a group element with a particular property, we can restrict our search to lex leaders of the set of all
such elements and prune the search space on that basis. Seress (2003) provides a more
complete discussion in the context of the problems typically considered by computational
group theory; an example in the context of the k-transporter problem specifically can be
found in Section 5.5.
Finally, we note that the two remaining leaf nodes are equivalent, since they refer to the
same instance  once we know the images of a and of b, the overall instance is fixed and no
further choices are relevant. So assuming that the variables in the problem are ordered so
that those in the clause are considered first, we can finally prune the search below depth
three to get:
sPSym(a, b, c, d)
@PPP
PP
@
PP
PP
@
PP
@
PP
@
PP
Ps(ad)
@s(ac)
A
A
A
A
AAs(bd)

Only a single leaf node need be considered.
Before we return to the application of these ideas in zap, we should stress that we have
only scratched the surface of computational group theory as a whole. The field is broad
458

fiZAP 3: Implementation

and developing rapidly, and the implementation in zap is based on ideas that appear in
Seress and in the gap code. Indeed, the name was chosen to reflect zaps heritage as an
outgrowth of both zChaff and Gap.6

4. Augmented Resolution
We now turn to our zap-specific requirements. First, we have the definition of augmented
resolution, which involves computing the group of stable extensions of the groups appearing
in the resolvents. Specifically, we have augmented clauses (c1 , G1 ) and (c2 , G2 ) and need to
compute the group G of stable extensions of G1 and G2 . Recalling Definition 2.13, this is
the group of all permutations  with the property that there is some g1  G1 such that
|cG1 = g1 |cG1
1

1

i
and similarly for g2 , G2 and c2 . We are viewing the clauses ci as sets, with cG
i being the
closure of ci under Gi (recall Definition 2.12).
As an example, consider the two clauses

(c1 , G1 ) = (a  b, h(ad), (be), (bf )i)
and
(c2 , G2 ) = (c  b, h(be), (bg)i)
2
The closure of c1 under G1 is {a, b, d, e, f } and cG
= {b, c, e, g}. We therefore need to
2
find a permutation  such that when  is restricted to {a, b, d, e, f }, it is an element of
h(ad), (be), (bf )i, and when restricted to {b, c, e, g} is an element of h(be), (bg)i.
From the second condition, we know that c cannot be moved by , and any permutation
of b, e and g is acceptable because (be) and (bg) generate the symmetric group Sym(b, e, g).
This second restriction does not impact the image of a, d or f under .
From the first condition, we know that a and d can be swapped or left unchanged, and
any permutation of b, e and f is acceptable. But recall from the second condition that we
must also permute b, e and g. These conditions combine to imply that we cannot move f
or g, since to move either would break the condition on the other. We can swap b and e
or not, so the group of stable extensions is h(ad), (be)i, and that is what our construction
should return.

Procedure 4.1 Given augmented clauses (c1 , G1 ) and (c2 , G2 ), to compute stab(ci , Gi ):
6. The authors of zChaff are Moskewicz, Madigan, Zhao, Zhang and Malik; our selection of only Z to
include in our acronym is surely unfair to Moskewicz, Madigan and Malik. Zmap didnt have quite the
same ring to it, however, and we hope that the implicitly excluded authors will accept our apologies for
our choice.

459

fiDixon, Ginsberg, Hofer, Luks & Parkes

1
2
3
4
5
6
7
8
9

G2
1
c closure1  cG
1 , c closure2  c2
g restrict1  G1 |c closure1 , g restrict2  G2 |c closure2
C  c closure1  c closure2
g stab1  g restrict1{C } , g stab2  g restrict2{C }
g int  g stab1 |C  g stab2 |C
{gi }  {generators of g int}
{l1i }  {gi , lifted to g stab1 }, {l2i }  {gi , lifted to g stab2 }
0 }  {l |
{l2i
2i c closure2 C }
0 }i
return hg restrict1C , g restrict2C , {l1i  l2i

Proposition 4.2 The result returned by Procedure 4.1 is stab(ci , Gi ).
The proof is in Appendix A; here, we present an example of the computation in use and
discuss the computational issues surrounding Procedure 4.1. The example we will use is
that with which we began this section, but we modify G1 to be h(ad), (be), (bf ), (xy)i instead
of the earlier h(ad), (be), (bf )i. The new points x and y dont affect the set of instances in
any way, and thus should not affect the resolution computation, either.
1
1. c closurei  cG
1 . This amounts to computing the closures of the ci under the Gi ; as
described earlier, we have c closure1 = {a, b, d, e, f } and c closure2 = {b, c, e, g}.

2. g restricti  Gi |c closurei . Here, we restrict each group to act only on the corresponding c closurei . In this example, g restrict2 = G2 but g restrict1 =
h(ad), (be), (bf )i as the irrelevant points x and y are removed.
Note that it is not always possible to restrict a group to an arbitrary set; one cannot
restrict the permutation (xy) to the set {x} because you need to add y as well. But
in this case, it is possible to restrict Gi to c closurei , since this latter set is closed
under the action of the group.
3. C  c closure1  c closure2 . The construction itself works by considering three
separate sets  the intersection of the closures of the two original clauses (where
the computation is interesting because the various  must agree), and the points in
only the closure of c1 or only the closure of c2 . The analysis on these latter sets is
straightforward; we just need  to agree with any element of G1 or G2 on the set in
question.
In this step, we compute the intersection region C . In our example, C = {b, e}.
4. g stabi  g restricti{C } . We find the subgroup of g restricti that set stabilizes
C , in this case the subgroup that set stabilizes the pair {b, e}. For g restrict1 =
h(ad), (be), (bf )i, this is h(ad), (be)i because we can no longer swap b and f , while for
g restrict2 = h(be), (bg)i, we get g stab2 = h(be)i.
5. g int  g stab1 |C  g stab2 |C . Since  must simultaneously agree with both
G1 and G2 when restricted to C (and thus with g restrict1 and g restrict2 as
well), the restriction of  to C must lie within this intersection. In our example,
g int = h(be)i.
460

fiZAP 3: Implementation

6. {gi }  {generators of g int}. Any element of g int will lead to an element of the
group of stable extensions provided that we extend it appropriately from C back to
G2
1
the full set cG
1  c2 ; this step begins the process of building up these extensions. It
suffices to work with just the generators of g int, and we construct those generators
here. We have {gi } = {(be)}.
7. {lki }  {gi , lifted to g stabk }. Our goal is now to build up a permutation on
c closure1  c closure2 that, when restricted to C , matches the generator gi . We
do this by lifting gi separately to c closure1 and to c closure2 . Any such lifting
suffices, so we can take (for example)
l11 = (be)(ad)
and
l21 = (be)
In the first case, the inclusion of the swap of a and d is neither precluded nor required;
we could just as well have used l11 = (be).
0 }  {l |
8. {l2i
2i c closure2 C }. We cannot simply compose l11 and l21 to get the desired
permutation on c closure1  c closure2 because the part of the permutations acting
on the intersection c closure1  c closure2 will have acted twice. In this case, we
would get l11  l21 = (ad) which no longer captures our freedom to exchange b and e.

We deal with this by restricting l21 away from C and only then combining with l11 . In
the example, restricting (be) away from C = {b, e} produces the trivial permutation
0 = ( ).
l21
0 }i. We now compute the final answer
9. Return hg restrict1C , g restrict2C , {l1i l2i
0
from three sources: The combined l1i l2i that we have been working to construct, along
with elements of g restrict1 that fix every point in the closure of c2 and elements of
g restrict2 that fix every point in the closure of c1 . These latter two sets obviously
consist of stable extensions. An element of g restrict1 point stabilizes the closure
of c2 if and only if it point stabilizes the points that are in both the closure of c1 (to
which g restrict1 has been restricted) and the closure of c2 ; in other words, if and
only if it point stabilizes C .

In our example, we have
g restrict1C

= h(ad)i

g restrict2C

= 1

{l1i 

0
l2i
}

= {(be)(ad)}

so that the final group returned is
h(ad), (be)(ad)i
This group is identical to the obvious
h(ad), (be)i
461

fiDixon, Ginsberg, Hofer, Luks & Parkes

We can swap either the (a, d) pair or the (b, e) pair, as we see fit. The first swap (ad)
is sanctioned for the first resolvent (c1 , G1 ) = (a  b, h(ad), (be), (bf )i) and does not
mention any relevant variable in the second (c2 , G2 ) = (c  b, h(be), (bg)i). The second
swap (be) is sanctioned in both cases.
Computational issues We conclude this section by discussing some of the computational
issues that arise when we implement Procedure 4.1, including the complexity of the various
operations required.
i
1. c closurei  cG
i . Efficient algorithms exist for computing the closure of a set under
a group. The basic method is to use a flood-fill like approach, adding and marking the
result of acting on the set with a single generator, and recurring until no new points
are added.

2. g restricti  Gi |c closurei . A group can be restricted to a set that it stabilizes by
restricting the generating permutations individually.
3. C  c closure1  c closure2 . Set intersection is straightforward.
4. g stabi  g restricti{C } . Set stabilizer is not straightforward, and is not known
to be polynomial in the total size of the generators of the group being considered
(Seress, 2003).7 The most effective implementations work with a coset decomposition
as described in Section 3.2; in computing G{S} for some set S, a node can be pruned
when it maps a point inside of S out of S or vice versa. Gap implements this (but
see our comments at the end of Section 10.2).
5. g int  g stab1 |C  g stab2 |C . Group intersection is also not known to be polynomial in the total size of the generators; once again, a coset decomposition is used.
Coset decompositions are constructed for each of the groups being combined, and the
search spaces are pruned appropriately. Gap implements this as well.
6. {gi }  {generators of g int}. Groups are typically represented in terms of their
generators, so reconstructing a list of those generators is trivial. Even if the generators
are not known, constructing a strong generating set is known to be polynomial in the
number of generators constructed.
7. {lki }  {gi , lifted to g stabk }. Suppose that we have a group G acting on a set T ,
a subset V  T and a permutation h acting on V such that we know that h is the
restriction to V of some g  G, so that h = g|V . To find such a g, we first construct
a stabilizer chain for G using an ordering that puts the elements of T  V first. Now
we are basically looking for a g  G such that the sifting procedure of Section 3.1
produces h at the point that the points in T  V have all been fixed. We can find
such a g in polynomial time by inverting the sifting procedure itself.
0 }  {l |
8. {l2i
2i c closure2 C }. As in line 2, restriction is still easy.

7. Unlike the k-transporter problem, which was mentioned at the beginning of Section 3.2 to be NP-hard,
neither set stabilizer nor group intersection (see step 5) is likely to be NP-hard (Babai & Moran, 1988).

462

fiZAP 3: Implementation

0 }i. Since groups are typically rep9. Return hg restrict1C , g restrict2C , {l1i  l2i
resented by their generators, we need simply take the union of the generators for the
three arguments. Point stabilizers (needed for the first two arguments) are straightforward to compute using stabilizer chains.

5. Unit Propagation and the (Ir)relevance Test
As we have remarked, the other main computational requirement of an augmented satisfiability engine is the ability to solve the k-transporter problem: Given an augmented clause
(c, G) where c is once again viewed as a set of literals, and sets S and U of literals and an
integer k, we want to find a g  G such that cg  S =  and |cg  U |  k, if such a g exists.
5.1 A Warmup
We begin with a somewhat simpler problem, assuming that U =  so we are simply looking
for a g such that cg  S = .
We need the following definitions:
Definition 5.1 Let H  G be groups. A transversal of H in G is any subset of G that
contains one element of each coset of H. We will denote such a transversal by (G : H).
Note that since H itself is one of the cosets, the transversal must contain a (unique) element
of H. We will generally assume that the identity is this unique element.
Definition 5.2 Suppose that G acts on a set  and that c  . By cG we will denote the
elements of c that are fixed by G.
As the search proceeds, we will gradually fix more and more points of the clause in question.
The notation of Definition 5.2 will let us refer easily to the points that have been fixed thus
far.
Procedure 5.3 Given groups H  G, an element t  G, sets c and S, to find a group
element g = map(G, H, t, c, S) with g  H and cgt  S = :
1
2
3
4
5
6
7
8
9
10

if ctH  S 6= 
then return failure
if c = cH
then return 1
  an element of c  cH
for each t0 in (H : H )
do r  map(G, H , t0 t, c, S)
if r 6= failure
then return rt0
return failure

463

fiDixon, Ginsberg, Hofer, Luks & Parkes

This is essentially a codification of the example that was presented in Section 3.2. We
terminate the search when the clause is fixed by the remaining group H, but have not
yet included any analog to the lex-leader pruning that we discussed in Section 3.3. In the
recursive call in line 7, we retain the original group, for which we will have use in subsequent
versions of the procedure.
A more precise description of the procedure would state explicitly that G acts on c and
S, so that G  Sym() with c, S  . Here and elsewhere, we believe that these conditions
are obvious from context and have elected not to clutter the procedural descriptions with
them.
Proposition 5.4 map(G, G, 1, c, S) returns an element g  G for which cg  S = , if such
an element exists, and returns failure otherwise.
Proof. The proof in the Appendix A shows the slightly stronger result that map(G, H, t, c, S)
returns an element g  H for which cgt  S =  if such an element exists.
Given that the procedure terminates the search when all elements of c are stabilized
by G but does not include lex-leader considerations, the search space examined in the
example from Section 3.2 is the following, where we have replaced the variables a, b, c, d
with x1 , x2 , x3 , x4 to avoid confusion with our current use of c to represent the clause in
question.
s Sym(x1 , x2 , x3 , x4 )
P
@PPP
PP
@
PP
@
PP
PP
@
PP
@
PP
@s(x1 x3 )
Ps(x1 x4 )
A
A
A
A
AAs(x2 x4 )
s(x2 x3 )

It is still important to prune the node in the lower right, since for a larger problem, this node
may be expanded into a significant search subtree. We discuss this pruning in Section 5.5.
In the interests of clarity, let us go through the example explicitly. Recall that the clause
c = x1  x2 , G = Sym(x1 , x2 , x3 , x4 ) permutes the xi arbitrarily, and that S = {x1 , x2 }.
On the initial pass through the procedure, cH = ; suppose that we select x1 to stabilize
first. Line 6 now selects the point to which x1 should be mapped; if we select x1 or x2 , then
x1 itself will be mapped into S and the recursive call will fail on line 2. So suppose we pick
x3 as the image of x1 .
Now cH = {x1 }, and we need to fix the image of another point; x2 is all thats left in
the original clause c. As before, selecting x1 or x2 as the image of x2 leads to failure. x3
is already taken (its the image of x1 ), so we have to map x2 into x4 . Now every element
of c is fixed, and the next recursive call returns the trivial permutation on line 4. This is
combined with (x2 x4 ) on line 9 in the caller as we fix x4 as the image of x2 . The original
invocation then combines with (x1 x3 ) to produce the final answer of (x2 x4 )(x1 x3 ).
464

fiZAP 3: Implementation

5.2 The k-Transporter Problem
Extending the above algorithm to solve the k-transporter problem is straightforward; in
addition to requiring that ctH  S =  in line 2, we also need to keep track of the number
of points that have been (or will be) mapped into the set U and make sure that we wont
be forced to exceed the limit k.
To understand this, suppose that we are examining a node in the coset decomposition
tree labeled with a permutation t, so that the node corresponds to permutations gt for
various g in the subgroup being considered at this level. We want to ensure that there is
some g for which |cgt  U |  k. Since cgt is assumed to avoid the set S completely, we can
replace this with the slightly stronger
|cgt  (S  U )|  k
This is in turn equivalent to

1

|cg  (S  U )t |  k

(6)

(7)

since the set in (7) is simply the result of operating on the set in (6) with the permutation
t1 .
We will present a variety of ways in which the bound of (7) can be approximated; for
the moment, we simply introduce an auxiliary function overlap(H, c, V ), which we assume
computes a lower bound on |ch  V | for all h  H. Procedure 5.3 becomes:
Procedure 5.5 Given groups H  G, an element t  G, sets c, S and U and an integer
k, to find a group element g = transport(G, H, t, c, S, U, k) with g  H, cgt  S =  and
|cgt  U |  k:
1
2
3
4
5
6
7
8
9
10
11
12

if ctH  S 6= 
then return failure
1
if overlap(H, c, (S  U )t ) > k
then return failure
if c = cH
then return 1
  an element of c  cH
for each t0 in (H : H )
do r  transport(G, H , t0 t, c, S, U, k)
if r 6= failure
then return rt0
return failure

For convenience, we will denote transport(G, G, 1, c, S, U, k) by transport(G, c, S, U, k).
This is the top level function corresponding to the original invocation of Procedure 5.5.
Proposition 5.6 Provided that |ch  V |  overlap(H, c, V )  |cH  V | for all h  H,
transport(G, c, S, U, k) as computed by Procedure 5.5 returns an element g  G for which
cg  S =  and |cg  U |  k, if such an element exists, and returns failure otherwise.
465

fiDixon, Ginsberg, Hofer, Luks & Parkes

The second condition on overlap (that overlap(H, c, V )  |cH  V |) is needed to ensure
that the procedure terminates on line 4 once the overlap limit is reached, rather than
succeeding on line 6.
Procedure 5.5 is simplified significantly by the fact that we only need to return a single g
with the desired properties, as opposed to all such g. In the examples arising in (ir)relevance
calculations, a single answer suffices. But if we want to compute the unit consequences of
a given literal, we need all of the unit instances of the clause in question. There are other
considerations at work in this case, however, and we defer discussion of this topic until
Section 6.
Our initial version of overlap is:
Procedure 5.7 Given a group H, and two sets c, V , to compute overlap(H, c, V ), a lower
bound on the overlap of ch and V for any h  H:
1

return |cH  V |

Having defined overlap, we may as well use it to replace the test in line 1 of Proce1
dure 5.5 with a check to see if overlap(H, c, S t ) > 0, indicating that for any h  H,
1
|ch  S t | > 0 or, equivalently, that cht  S 6=  . For the simple version of overlap defined
above, there is no difference between the two procedures. But as overlap matures, this
change will lead to additional pruning in some cases.
5.3 Orbit Pruning
There are two general ways in which nodes can be pruned in the k-transporter problem.
Lexicographic pruning is a bit more difficult, so we defer it until Section 5.5. To understand
the other, we begin with the following example.
Consider the clause c = x1  x2  x3 and the group G that permutes the variables
{x1 , x2 , x3 , x4 , x5 , x6 } arbitrarily. If S = {x1 , x2 , x3 , x4 }, is there a g  G with cg  S = ?
Clearly not; there isnt enough room because the image of c will be of size three,
and there is no way that this 3-element set can avoid the 4-element set S in the 6-element
universe {x1 , x2 , x3 , x4 , x5 , x6 }.
We can do a bit better in many cases. Suppose that our group G is h(x1 x4 ), (x2 x5 ), (x3 x6 )i
so that we can swap x1 with x4 (or not), x2 with x5 , or x3 with x6 . Now if S = {x1 , x4 },
can we find a g  G with cg  S = ?
Once again, the answer is clearly no. The orbit of x1 in G is {x1 , x4 } and since {x1 , x4 } 
S, x1 s image cannot avoid the set S.
In the general case appearing in Procedure 5.5, consider the initial call, where t is the
identity permutation. Given the group G, consider the orbits of the points in c. If there is
any such orbit W for which |W  c| > |W  S|, we can prune the search. The reason is that
each of the points in W  c must remain in W when acted on by any element of G; that
is what the definition of an orbit requires. But there are too many points in W  c to stay
away from S, so we will not manage to have cg  S = .
What about the more general case, where t 6= 1 necessarily? For a fixed  in our clause c,
we will construct the image gt , acting on  first with g and then with t. We are interested

466

fiZAP 3: Implementation

1

in whether gt  S or, equivalently, if g  S t . Now g is necessarily in the same orbit
as , so we can prune if
1
|W  c| > |W  S t |
For similar reasons, we can also prune if
1

|W  c| > |W  U t | + k
In fact, we can prune if

1

|W  c| > |W  (S  U )t | + k
because there still is not enough space to fit the image without either intersecting S or
putting at least k points into U .
We can do better still. As we have seen, for any particular orbit, the number of points
that will eventually be mapped into U is at least
1

|W  c|  |W  (S  U )t |
In some cases, this expression will be negative; the number of points that will be mapped
into U is therefore at least
1

max(|W  c|  |W  (S  U )t |, 0)
and we can prune any node for which
X
1
max(|W  c|  |W  (S  U )t |, 0) > k

(8)

W

where the sum is over the orbits of the group.
It will be somewhat more convenient to rewrite this using the fact that
1

1

|W  c| + |W  c| = |W | = |W  (S  U )t | + |W  (S  U )t |
so that (8) becomes
X

1

max(|W  (S  U )t |  |W  c|, 0) > k

(9)

W

Incorporating this type of analysis into Procedure 5.7 gives:
Procedure 5.8 Given a group H, and two sets c, V , to compute overlap(H, c, V ), a lower
bound on the overlap of ch and V for any h  H:
1
2
3
4

m0
for each orbit W of H
do m  m + max(|W  V |  |W  c|, 0)
return m

Proposition 5.9 Let H be a group and c, V sets acted on by H. Then for any h  H,
|ch  V |  overlap(H, c, V )  |cH  V | where overlap is computed by Procedure 5.8.
467

fiDixon, Ginsberg, Hofer, Luks & Parkes

5.4 Block Pruning
The pruning described in the previous section can be improved further. To see why, consider
the following example, which might arise in solving an instance of the pigeonhole problem.
We have the two cardinality constraints:
x1 + x2 + x3 + x4  2

(10)

x5 + x6 + x7 + x8  2

(11)

presumably saying that at least two of four pigeons are not in hole m and at least two
are not in hole n for some m and n.8 Rewriting the individual cardinality constraints as
augmented clauses produces
(x1  x2  x3 , Sym(x1 , x2 , x3 , x4 ))
(x5  x6  x7 , Sym(x5 , x6 , x7 , x8 ))
or, in terms of generators,
(x1  x2  x3 , h(x1 x2 ), (x2 x3 x4 )i)

(12)

(x5  x6  x7 , h(x5 x6 ), (x6 x7 x8 )i)

(13)

What we would really like to do, however, is to capture the full symmetry in a single axiom.
We can do this by realizing that we can obtain (13) from (12) by switching x1 and x5 ,
x2 and x6 , and x3 and x7 (in which case we want to switch x4 and x8 as well). So we add
the generator (x1 x5 )(x2 x6 )(x3 x7 )(x4 x8 ) to the overall group, and modify the permutations
(x1 x2 ) and (x2 x3 x4 ) (which generate Sym(x1 , x2 , x3 , x4 )) so that they permute x5 , x6 , x7 , x8
appropriately as well. The single augmented clause that we obtain is
(x1  x2  x3 , h(x1 x2 )(x5 x6 ), (x2 x3 x4 )(x6 x7 x8 ), (x1 x5 )(x2 x6 )(x3 x7 )(x4 x8 )i)

(14)

and it is not hard to see that this does indeed capture both (12) and (13).
Now suppose that x1 and x5 are false, and the other variables are unvalued. Does (14)
have a unit instance?
With regard to the pruning condition in the previous section, the group has a single
orbit, and the condition (with t = 1) is
|W  (S  U )|  |W  c| > 1

(15)

But
W

= {x1 , x2 , x3 , x4 , x5 , x6 , x7 , x8 }

S = 
U

= {x2 , x3 , x4 , x6 , x7 , x8 }

c = {x1 , x2 , x3 }
8. In an actual pigeonhole instance, all of the variables would be negated. We have dropped the negations
for convenience.

468

fiZAP 3: Implementation

so that |W  (S  U )| = 6, |W  c| = 5 and (15) fails.
But it should be possible to conclude immediately that there are no unit instances
of (14). After all, there are no unit instances of (10) or (11) because only one variable in
each clause has been set, and three unvalued variables remain. Equivalently, there is no
unit instance of (12) because only one of {x1 , x2 , x3 , x4 } has been valued, and two need to
be valued to make x1  x2  x3 or another instance unit. Similarly, there is no unit instance
of (13). What went wrong?
What went wrong is that the pruning heuristic thinks that both x1 and x5 can be
mapped to the same clause instance, in which case it is indeed possible that the instance
in question be unit. The heuristic doesnt realize that x1 and x5 are in separate blocks
under the action of the group in question.
To formalize this, let us first make the following definition:
Definition 5.10 Suppose G acts on a set T . We will say that G acts transitively on T if
T is an orbit of G.
Put somewhat differently, G acts transitively on T just in case for any x, y  T there is
some g  G such that xg = y.
Definition 5.11 Suppose that a group G acts transitively on a set T . Then a block system
for G is a partitioning of T into sets B1 , . . . , Bn such that G permutes the Bi .
In other words, for each g  G and each block Bi , Big = Bj for some j. If j = i, then
the image of Bi under g is Bi itself. If j 6= i, then the image of Bi under g is disjoint from
Bi , since the blocks partition T .
Every group acting transitively and nontrivially on a set T has at least two block systems:
Definition 5.12 For a group G acting transitively on a set T , a block system B1 , . . . , Bn
will be called trivial if either n = 1 or n = |T |.
In the former case, there is a single block consisting of the entire set T (which obviously
is a block system). If n = |T |, each point is in its own block; since G permutes the points,
it obviously permutes the blocks.
Lemma 5.13 All of the blocks in a block system are of identical size.
In the example we have been considering, B1 = {x1 , x2 , x3 , x4 } and B2 = {x5 , x6 , x7 , x8 }
is also a block system for the action of the group on the set T = {x1 , x2 , x3 , x4 , x5 , x6 , x7 , x8 }.
And while it is conceivable that a clause image is unit within the overall set T , it is impossible
for it to have fewer than two unvalued literals within each particular block. Instead of
looking at the overall expression
|W  (S  U )|  |W  c| > 1
we can work with individual blocks.

469

(16)

fiDixon, Ginsberg, Hofer, Luks & Parkes

The clause x1  x2  x3 is in a single block in this block system, and will therefore remain
in a single block after being acted on with any g  G. If the clause winds up in block Bi ,
then the condition (16) can be replaced with
|Bi  (S  U )|  |Bi  c| > 1
or, in this case,
|Bi  (S  U )| > |Bi  c| + 1 = 2
so that we can prune if there are more than two unvalued literals in the block in question.
After all, if there are three or more unvalued literals, there must be at least two in the
clause instance being considered, and it cannot be unit.
Of course, we dont know exactly which block will eventually contain the image of c,
but we can still prune if
min(|Bi  (S  U )|) > 2
since in this case any target block will generate a prune. And in the example that we have
been considering,
|Bi  (S  U )| = 3
for each block in the block system.
Generalizing this idea is straightforward. For notational convenience, we introduce:
Definition 5.14 Let T = {T1 , . . . , Tk } be sets, and
Ti1 , . . . , Tin are the n
Pnsuppose that min
elements of T of smallest size. Then we will denote j=1 |Tij | by in Ti .
Proposition 5.15 Let G be a group acting transitively on a set T , and let c, V  T .
Suppose also that {B1 , . . . , Bk } is a block system for G and that c  Bi 6=  for n of the
blocks in {B1 , . . . , Bk }. Then if b is the size of an individual block Bi and g  G,
|cg  V |  |c| + min
in (Bi  V )  nb

(17)

Proposition 5.16 If the block system is trivial (in either sense), (17) is equivalent to
|cg  V |  |T  V |  |T  c|

(18)

Proposition 5.17 Let {B1 , . . . , Bk } be a block system for a group G acting transitively on
a set T . Then (17) is never weaker than (18).
In any event, we have shown that we can strengthen Procedure 5.8 to:
Procedure 5.18 Given a group H, and two sets c, V , to compute overlap(H, c, V ), a
lower bound on the overlap of ch and V for any h  H:

470

fiZAP 3: Implementation

1
2
3
4
5
6

m0
for each orbit W of H
do {B1 , . . . , Bk }  a block system for W under H
n = |{i|Bi  c 6= }|
m  m + max(|c  W | + min
in (Bi  V )  n|B1 |, 0)
return m

Which block system should we use in line 3 of the procedure? There seems to be no
general best answer to this question, although we have seen from Proposition 5.17 that any
block system is better than one of the trivial ones. In practice, the best choice appears to be
a minimal block system (i.e., one with blocks of the smallest size) for which c is contained
within a single block. Now Procedure 5.18 becomes:
Procedure 5.19 Given a group H, and two sets c, V , to compute overlap(H, c, V ), a
lower bound on the overlap of ch and V for any h  H:
1
2
3
4
5

m0
for each orbit W of H
do {B1 , . . . , Bk }  a minimal block system for W under H for which
c  W  Bi for some i
m  m + max(|c  W | + min(Bi  V )  |B1 |, 0)
return m

Proposition 5.20 Let H be a group and c, V sets acted on by H. Then for any h  H,
|ch  V |  overlap(H, c, V )  |cH  V | where overlap is computed by Procedure 5.19.
Note that the block system being used depends only on the group H and the original
clause c. This means that in an implementation it is possible to compute these block
systems once and then use them even if there are changes in the sets S and U of satisfied
and unvalued literals respectively.
Gap includes algorithms for finding minimal block systems for which a given set of
elements (called a seed in gap) is contained within a single block. The basic idea is to
form an initial block system where the points in the seed are in one block and each point
outside of the seed is in a block of its own. The algorithm then repeatedly runs through
the generators of the group, seeing if any generator g maps elements x, y in one block to
xg and y g that are in different blocks. If this happens, the blocks containing xg and y g are
merged. This continues until every generator respects the candidate block system, at which
point the procedure is complete.9
5.5 Lexicographic Pruning
Block pruning will not help us with the example at the end of Section 5.1. The final space
being searched is:
9. A faster implementation makes use of the procedure designed for testing equivalence of finite automata (Aho, Hopcroft, & Ullman, 1974, chapter 4) and takes O(snA(n)) time, where s is the size
of the generating set and A(n) is the inverse Ackerman function.

471

fiDixon, Ginsberg, Hofer, Luks & Parkes

sPSym(a, b, c, d)
@PPP
PP
@
PP
@
PP
PP
@
PP
(ac)
@
PP
@s
Ps(ad)
A
A
A
A
AAs(bd)
s(bc)

As we have remarked, the first leaf node (where a is mapped to c and b to d) is essentially
identical to the second (where a is mapped to d and b to c). It is important not to expand
both since more complicated examples may involve a substantial amount of search below
the nodes that are leaf nodes in the above figure.
This is the sort of situation in which lexicographic pruning can generally be applied.
We want to identify the two leaf nodes as equivalent in some way, and then expand only
the lexicographically least member of each equivalence class. For any particular node n,
we need a computationally effective way of determining if n is the lexicographically least
member of its equivalence class.
We begin by identifying conditions under which two nodes are equivalent. To understand
this, recall that we are interested in the image of the clause c under a particular group
element g. That means that we dont care about where any particular literal l is mapped,
because we care only about the image of the entire clause c. We also dont care about the
image of any literal that isnt in c.
From a formal point of view, we begin by extending our set stabilizer notation somewhat:
Definition 5.21 For a permutation group G and sets S1 , . . . , Sk acted on by G, by G{S1 ,...,Sk }
we will mean that subgroup of G that simultaneously set stabilizes each of the Si ; equivalently, G{S1 ,...,Sk } = i G{Si } .
In computing a multiset stabilizer G{S1 ,...,Sk } = i G{Si } , we need not compute the individual set stabilizers and then take their intersection. Instead, recall that the set stabilizers
themselves are computed using coset decomposition; if any stabilized point is moved either
into or out of the set in question, the given node can be pruned in the set stabilizer computation. It is straightforward to modify the set stabilizer algorithm so that if any stabilized
point is moved into or out of any of the Si , the node in question is pruned. This allows
G{S1 ,...,Sk } to be computed in a single traversal of Gs decomposition tree.
Now suppose that j is a permutation in G that stabilizes the set c. If cg satisfies the
conditions of the transporter problem, then so will cjg . After all, acting with j first doesnt
affect the set corresponding to c, and the image of the clause under jg is therefore identical
to its image under g. This means that two permutations g and h are equivalent if h = jg
for some j  G{c} , the set stabilizer of c in G. Alternatively, the permutation g is equivalent
to any element of the coset Jg, where J = G{c} .
On the other hand, suppose that k is a permutation that simultaneously stabilizes the
sets S and U of satisfied and unvalued literals respectively. Now it is possible to show that
472

fiZAP 3: Implementation

if we operate with k after operating successfully with g, we also dont impact the question
of whether or not cg is a solution to the transporter problem. The upshot of this is the
following:
Definition 5.22 Let G be a group with J  G and K  G, and let g  G. Then the double
coset JgK is the set of all elements of G of the form jgk for j  J and k  K.
Proposition 5.23 Let G be a group of permutations, and c a set acted on by G. Suppose
also that S and U are sets acted on by G. Then for any instance I of the k-transporter
problem and any g G, either every element of G{c} gG{S,U } is a solution of I, or none is.
To understand why this is important, imagine that we prune the overall search tree so
that the only permutations g remaining are ones that are minimal in their double cosets
JgK, where J = G{c} and K = G{S,U } as above. Will this impact the solubility of any
instance of the k-transporter problem?
It will not. If a particular instance has no solutions, pruning the tree obviously will not
introduce any. If the particular instance has a solution g, then every element of JgK is
also a solution, so specifically the minimal element of JgK is a solution, and this minimal
element will not be pruned under our assumptions.
We see, then, that we can prune any node n for which we can show that every permutation g underneath n is not minimal in its double coset JgK. To state precise conditions
under which this lets us prune the node n, suppose that we have some coset decomposition
of a group G, and that xj is the point fixed at depth j of the tree. Now if n is a node at
depth i in the tree, we know that n corresponds to a coset Ht of G, where H stabilizes each
xj for j  i. We will denote the image of xj under t by zj . If there is no g  Ht that is
minimal in its double coset JgK for J = G{c} and K = G{S,U } as in Proposition 5.23, then
the node n corresponding to Ht can be pruned.
Jx

,...,xk1

Lemma 5.24 (Leon, 1991) If xl  xk 1
then no g  Ht is the first element of JgK.

Kz1 ,z2 ,...,zk1

for some k  l and zk > min(zl

Jx

),

,...,x

Lemma 5.25 (reported by Seress, 2003) Let s be the length of the orbit xl 1 l1 . If
zl is among the last s  1 elements of its orbit in Gz1 ,z2 ,...,zl1 , then no g  Ht is the first
element of JgK.
Both of these results give conditions under which a node in the coset decomposition can
be pruned when searching for a solution to an instance of the k-transporter problem. Let
us consider an example of each.
We begin with Lemma 5.24. If we return to our example from the end of Section 5.1,
we have G = Sym(a, b, c, d), c = {a, b} = S, and U = . Thus J = K = G{a,b} =
Sym(a, b)  Sym(c, d) = h(ab), (cd)i.
Consider the node that we have repeatedly remarked can be pruned at depth 1, where
we fix the image of a to be d. In this case, x1 = a and z1 = d. If we take k = l in the
Jx ,...,x
statement of the lemma, xl  xl 1 l1 since 1  Jx1 ,...,xl1 . Thus we can prune if
Kz1 ,z2 ,...,zl1

zl > min(zl

473

)

fiDixon, Ginsberg, Hofer, Luks & Parkes

Further restricting to l = 1 gives us
z1 > min(z1K )

(19)

In this example, z1 = d, so z1K = {c, d} and (19) holds (assuming that d > c in our ordering).
The node can be pruned, and we finally get the reduced search space:
sPSym(a, b, c, d)
@PPP
PP
@
PP
@
PP
PP
@
PP
@
PP
@s(ac)
Ps(ad)
A
A
A
A
AAs(bd)

as desired.
This node can be pruned by Lemma 5.25 as well. The conditions of the lemma require
that we take s to be the length of the orbit of a under J (since l = 1 here), so s = |{a, b}| = 2.
Thus the image of a cannot be among the last 2  1 = 1 points in as orbit under G. Since
the orbit of a under G is {a, b, c, d}, we can once again prune this node. (The previous
node, which maps a to c, cannot be pruned, of course.)
This particular example is simple. The nodes being examined are at depth one, and
there is significant overlap in the groups in question. While the same node is pruned by
either lemma here, the lemmas prune different nodes in more complex cases. Note also that
the groups J = G{c} and K = G{S,U } can be computed at the root of the tree, and the
group J is independent of the sets S and U and can therefore be cached with the augmented
clause (c, G).
Lemmas 5.24 and 5.25 are both well known results in the computational group theory
community. We will also have use of the following:
Lemma 5.26 Suppose that t is the permutation labeling some node Ht of a coset decomgroup at
position tree at depth k, so that xti = zi for i  k and H = Gx1 ,...,xk is the residual


JM,x1 ,...,xi1 t

this level. Let M be the set of points moved by Gx1 ,...,xk . Now if zi > min xi
for any i  k, then no g  Ht is the first element of JgK.
As an example, consider the cardinality constraint
x1 +    + xm  n
corresponding to the augmented clause (c, G) with
c = x1      xmn+1
and G = Sym(X), where X is the set of all of the xi .
474

fiZAP 3: Implementation

Suppose that we fix the images of the xi in order, and that we are considering a node
where the image of x1 is fixed to z1 and the image of x2 is fixed to z2 , with z2 < z1 . Now
J = G{c} = Sym(x1 , . . . , xmn+1 )  Sym(xmn+2 , . . . , xm ), so taking i = 1 and k = 2 in
Lemma 5.26 gives us Jxk+1 ,...,xm = Sym(x1 , x2 ) since we need to fix all of the xj after x2 .
Jx

,...,xm t

= {z1 , z2 }, and since z1 is not the smallest element of this set, this is
But x1 k+1
enough to prune this node. See the proof of Proposition 6.9 for another example.
We will refer to Lemmas 5.245.26 as the pruning lemmas.
Adding lexicographic pruning to our k-transporter procedure gives us:
Procedure 5.27 Given groups H  G, an element t  G, sets c, S and U and an integer
k, to find a group element g = transport(G, H, t, c, S, U, k) with g  H, cgt  S =  and
|cgt  U |  k:
1
2
3
4
5
6
7
8
9
10
11
12
13
14

1

if overlap(H, c, S t ) > 0
then return failure
1
if overlap(H, c, (S  U )t ) > k
then return failure
if c = cH
then return 1
if a pruning lemma can be applied
then return failure
  an element of c  cH
for each t0 in (H : H )
do r  transport(G, H , t0 t, c, S, U, k)
if r 6= failure
then return rt0
return failure

Note that the test in line 7 requires access to the groups J and K, and therefore to the
original group G with which the procedure was called. This is why we retain a copy of this
group in the recursive call on line 11.
It might seem that we have brought too much mathematical power to bear on the
k-transporter problem specifically, but we disagree; recall Figure 1, repeated from zap1.
High-performance satisfiability engines, running on difficult problems, spend in excess of
90% of their CPU time in unit propagation, which we have seen to be an instance of the
k-transporter problem. Effort spent on improving the efficiency of Procedure 5.27 (and
its predecessors) can be expected to lead to substantial performance improvements in any
practical application. See also Figure 8 and the experimental results in Section 9.2.
We do, however, note that while lexicographic pruning is important, it is also expensive.
This is why we defer it to line 7 of Procedure 5.27. An earlier lexicographic prune would
be independent of the S and U sets, but the count-based pruning is so much faster that we
defer the lexicographic check to the extent possible.

475

fiDixon, Ginsberg, Hofer, Luks & Parkes

100

ZCHAFF data

% of time spent in UP

95

90

85

80

75

70

0

10

20

30

40

50

60

70

80

90

total CPU time (sec)

Figure 1: Fraction of CPU time spent in unit propagation

6. Unit Propagation
Procedure 5.27 was designed around the need to find a single permutation g  G satisfying
the conditions of the k-transporter problem, and this technically suffices for zaps needs.
In unit propagation, however, it is useful to collect all of the unit consequences of an
augmented clause (c, G) at once, as opposed to collecting them via repeated traversals of
Gs coset decomposition tree.
As we work through the consequences of this observation, it will help to have an example
that illustrates the points we are going to be making. To this end, we will consider the
augmented clause
(a  b  e, Sym(a, b, c, d)  Sym(e, f ))
(20)
in a situation where a, b and c are false and d, e and f are unvalued. The group in (20)
allows arbitrary permutations of {a, b, c, d} and of {e, f }, so that both e and f are unit
consequences of instances of the given augmented clause.
Note that we cannot simply collect all the group elements associated with each unit
instance, since many group elements may correspond to the same clause instance cg or to
the same unit literal cg  U . In the above example, both ( ) and (ab) correspond to the
identical clause a  b  e, and both this clause and a  c  e lead to the same conclusion e
given the current partial assignment.
Our goal will therefore be to compute not a set of permutations, but the associated set
of all unit conclusions:
Definition 6.1 Let (c, G) be an augmented clause, and P a partial assignment. The unit
consequences of (c, G) given P is the set of all literals l such that there is a g  G with
cg  S(P ) =  and cg  U (P ) = {l}. For a fixed literal w, the unit w-consequences of (c, G)
given P is the set of all literals l such that there is a g  G with w  cg , cg  S(P ) =  and
cg  U (P ) = {l}.
476

fiZAP 3: Implementation

The unit w-consequences involve an additional requirement that the literal w appear in the
clause instance in question. This will be useful when we discuss watched literals in the next
section.
In our example, the unit consequences of (20) are e and f . The unit c-consequences are
the same, although we can no longer use the identity permutation ( ), since the needed c is
not in the base instance of (20). There are no unit d-consequences of (20).
If the partial assignment is to be annotated, we will need not just the unit consequences,
but the reasons as well:
Definition 6.2 Let X be a set of pairs hl, gi, where g  G and l is a literal for each pair.
If X = {hl1 , g1 i, . . . , hln , gn i}, we will denote {l1 , . . . , ln } by L(X).
If (c, G) is an augmented clause and P a partial assignment, X will be called an annotated set of unit consequences of (c, G) given P if:
1. cg  S(P ) =  and cg  U (P ) = {l} for every hl, gi  X and
2. L(X) is the set of unit consequences of (c, G) given P .
Once again returning to our example, he, ( )i is an annotated consequence, as is he, (abc)i.
So are hf, (ef )i and hf, (abc)(ef )i. The set {he, (abc)i, hf, (ef )i} is an annotated set of unit
consequences, as is {he, (abc)i, hf, (ef )i, hf, (abc)(ef )i}. But {hf, (ef )i, hf, (abc)(ef )i} is not
an annotated set of unit consequences, since e does not appear as a consequence.
We now modify our k-transporter procedure so that we search the entire tree while
accumulating an annotated set of unit consequences. We need to be careful, however,
because the pruning lemmas may prune a node because it includes a permutation g that is
not minimal in its double coset JgK. This is a problem because g and the minimal element
of JgK may correspond to distinct unit consequences. In our running example, it may well
be that none of the minimal elements of JgK supports f as a conclusion; if we accumulate
only all of the minimal elements, we will not get a full set of unit consequences as a result.
Given a successful g that is minimal in its double coset, reconstructing the relevant
orbits under J and K is easy, so we begin by introducing some definitions that cater to this.
The basic idea is that we want the minimal g to entail, in some sense, the conclusions
that can be drawn from other permutations in the double coset JgK.
In our example, the subgroup of G that simultaneously stabilizes S and U is G{S,U } =
Sym(a, b, c)  Sym(e, f ). Once we have a permutation g1 that allows us to conclude e, we
can operate with g1  (ef )  g1 G{S,U } to conclude f as well. We formalize this as follows:
Definition 6.3 Given a group G, we will say that hl1 , g1 i G-entails hl2 , g2 i, to be denoted
hl1 , g1 i |=G hl2 , g2 i, if there is some g  G such that l2 = l1g and g2 = g1 g. We will say that a
set of pairs X G-entails a set of pairs Y , writing X |=G Y , if every pair in Y is G-entailed
by some pair in X.
A skeletal set of unit consequences of (c, G) given P is any set X of unit consequences
that G{S(P ),U (P )} -entails an annotated set of unit consequences of (c, G) given P .
In our running example, we have l1 = e and g = (ef ) in the first paragraph, allowing
(for example) he, ( )i to G{S,U } -entail hf, (ef )i. Thus we see that {he, ( )i} is a skeletal set
of unit consequences of (20) given the partial assignment {a, b, c}.
477

fiDixon, Ginsberg, Hofer, Luks & Parkes

Lemma 6.4 If X |=G Y , then L(Y )  L(X)G .
Proof. Every pair in Y is of the form hl1g , g1 gi for hl1 , g1 i  X and g  G. Thus the
associated literal is in L(X)G .
To construct a full set of unit consequences from a skeletal set, we repeatedly find new
unit conclusions until no more are possible:
Procedure 6.5 Given a set X of pairs hl, gi and a group G, to compute complete(X, G),
where X |=G complete(X, G) and L(complete(X, G)) = L(X)G :
1
2
3
4
5
6

Y 
for each hl, gi  X
do for each l0  lG  L(Y )
do select h  G such that lh = l0
Y  Y  hl0 , ghi
return Y

Proposition 6.6 X |=G complete(X, G) and L(complete(X, G)) = L(X)G .
Now we can apply the pruning lemmas as the search proceeds, eventually returning a
skeletal set of unit consequences for the clause in question. In addition, if there is a unit
instance that is in fact unsatisfiable, we should return a failure marker of some sort. We
handle this by returning two values. The first indicates whether or not a contradiction was
found, and the second is the skeletal set of unit consequences.
Procedure 6.7 Given groups H  G, an element t  G, sets c, S and U , to find
Transport(G, H, t, c, S, U ), a skeletal set of unit consequences for (c, G) given P :
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

1

if overlap(H, c, S t ) > 0
then return hfalse, i
1
if overlap(H, c, (S  U )t ) > 1
then return hfalse, i
if c = cH
then if ct  U = 
then return htrue, 1i
else return hfalse, hct  U, 1ii
if a pruning lemma can be applied
then return hfalse, i
Y 
  an element of c  cH
for each t0 in (H : H )
do hu, V i  Transport(G, H , t0 t, c, S, U )
if u = true
then return htrue, V t0 i
else Y  Y  {hl, gt0 i|hl, gi  V }
return hfalse, Y i
478

fiZAP 3: Implementation

Proposition 6.8 Assume that |ch  V |  overlap(H, c, V )  |cH  V | for all h  H, and
let Transport(G, c, S, U ) be computed by Procedure 6.7. Then if there is a g  G such that
cg  S = cg  U = , Transport(G, c, S, U ) = htrue, gi for such a g. If there is no such
g, Transport(G, c, S, U ) = hfalse, Zi, where Z is a skeletal set of unit consequences for
(c, G) given P .
As an application of the pruning lemmas, we have:
Proposition 6.9 Let (c, G) be an augmented clause corresponding to a cardinality constraint. Then for any sets S and U , Procedure 6.7 will expand at most a linear number of
nodes in finding a skeletal set of unit consequences of (c, G).
In the original formulation of cardinality constraints (as in zap1), determining if a
particular constraint is unit (and finding the implied literals if so) takes time linear in
the length of the constraint, since it involves a simple walk along the constraint itself. It
therefore seems appropriate for a linear number of nodes to be expanded in this case.

7. Watched Literals
There is one pruning technique that we have not yet considered, and that is the possibility
of finding an analog in our setting to Zhang and Stickels (2000) watched literal idea.
To understand the basic idea, suppose that we are checking to see if the clause a  b  c
is unit in a situation where a and b are unvalued. It follows that the clause cannot be unit,
independent of the value assigned to c.
At this point, we can watch the literals a and b; as long as they remain unvalued, the
clause cannot be unit. In practice, the data structures representing a and b include a pointer
to the clause in question, and the unit test needs only be performed for clauses pointed to
by literals that are changing value.
As we continue to discuss these ideas, it will be useful to distinguish among three different
types of clauses: those that are satisfied given the current partial assignment, those that
are unit, and those that are neither:
Definition 7.1 Let C be a clause, and P a (possibly annotated) partial assignment. We
will say that C is settled by P if it is either satisfied or unit; otherwise it is unsettled.
We now have:
Definition 7.2 Let C be a clause, and P a (possibly annotated) partial assignment. If C
is unsettled by P , then a watching set for C under P is any set of literals W such that
|W  C  U (P )| > 1.
In other words, W contains at least two unvalued literals in C if C is unsettled by the
current partial assignment.
What about if C is satisfied or unit? What should the watching set be in this case?
In some sense, it doesnt matter. Assuming that we notice when a clause changes from
unsettled to unit (so that we can either unit propagate or detect a potential contradiction),
479

fiDixon, Ginsberg, Hofer, Luks & Parkes

settled clauses are uninteresting from this perspective, since they can never generate a
second unit propagation. So we can watch a settled clause or not, as we see fit.
In another sense, however, it does matter. One of the properties that we would like the
watching sets to have is that they remain valid during a backtrack. That means that if
a settled clause C becomes unsettled during a backtrack, there must be two watched and
unvalued variables after that backtrack.
In order to discuss backtracking in a formal way, we introduce:
Definition 7.3 Let P be a partial assignment for a set T of (possibly augmented) clauses.
We will say that P is T -closed if no clause C  T has a unit consequence given P . A
T -closure of P is any minimal, sound and T -closed extension of P , and will be denoted by
either PT or by simply P if T is clear from context.
The definition of closure makes sense because the intersection of two closed partial
assignments is closed as well. To compute the closure, we simply add unit consequences
one at a time until no more are available. Note that there is still some ambiguity; if there
is more than one unit consequence that can be added at some point, we can add the unit
consequences in any order.
Definition 7.4 Let P = hl1 , . . . , ln i be a partial assignment. A subassignment of P is any
initial subsequence hl1 , . . . , lj i for j  n. We will say that a subassignment P 0 of P is a
backtrack point for P if either P 0 = P or P 0 = P 0 . We will denote by P the largest
backtrack point for P that is not P itself.
If C is a clause, we will say that the P -retraction of C, to be denoted PC , is the largest
backtrack point for P for which C is unsettled.
Note that we require a backtrack to the point that C is unsettled, as opposed to simply
unsatisfied. If P is closed, there is no difference because Definition 7.4 does not permit a
backtrack to a point where C is unit. But if C is unit under P , we can only retract C
by reverting to a point before C became unit. Otherwise, C will simply be reasserted when
unit propagation computes P .
Since P itself is a backtrack point for P , we immediately have:
Lemma 7.5 If C is unsettled by P , then PC = P .
As an example, suppose that we have the following annotated partial assignment P :
literal
a
b
c
d
e

reason
true
true
a  b  c
true
b  d  e

If our clause C is b  e  f , the P -retraction of C is ha, b, ci. Removing e is sufficient to
make C unsettled, but ha, b, c, di is not closed and is therefore not a legal backtrack point.
If b  e is in our theory, the retraction is in fact hai because ha, b, ci is not a backtrack
point because the unit conclusion e has not been drawn.
We can now generalize Definition 7.2 to include settled clauses:
480

fiZAP 3: Implementation

Definition 7.6 Let C be a clause, and P an annotated partial assignment. A watching set
for C under P is any set of literals W such that |W  C  U (PC )| > 1.
In other words, W will contain at least two unvalued literals in C if we replace P with the
P -retraction of C. As discussed earlier, this is the first point to which we could backtrack so
that C was no longer satisfied or unit. Continuing our earlier example, {e, f } is a watching
set for b  e  f , and {b, e} is a watching set for b  e. A watching set for b  e is {b, e};
recall that the definition forces us to backtrack all the way to hai.
Lemma 7.7 If W is a watching set for C under P , then so is any superset of W .
In order for watching sets to be useful, of course, we must maintain them as the search
proceeds. Ideally, this maintenance would involve modifying the watching sets as infrequently as possible, so that we could adjust them only as required when variables take new
values, and not during backtracking at all. Recall the example at the beginning of this
section, where a and b are unvalued and constitute a watching set for the clause a  b  c. If
a or b becomes satisfied, we need do nothing since the clause is now satisfied and {a, b} is
still a watching set. Note that if a (for example) becomes satisfied, we cant remove b from
the watching set, since we would then need to replace it if we backtrack to the point that
a is unvalued once again. Leaving b in the watching set is required to satisfy Definition 7.6
and needed to ensure that the sets need not be adjusted after a backtrack.
On the other hand, if a (for example) becomes unsatisfied, we need to check the clause
to see whether or not it has become unit. If the clause is unit, then b should be set to true
by unit propagation, so no maintenance is required. If the clause is unsettled, then c must
be unvalued, so we can replace a with c in the set of literals watching the clause. Finally,
if the clause is already satisfied, then a will be unvalued in the P -retraction of the clause
and the watching set need not be modified.
In general, we have:
Proposition 7.8 Suppose that W is a watching set for C under P and l is a literal. Then:
1. W is a watching set for C under any backtrack point for P .
2. If C is settled by hP, li, then W is a watching set for C under hP, li.
3. If C is settled by hP, li, and |(W  {l})  C  U (PC )| > 1, then W  {l} is a
watching set for C under hP, li.
4. If l 6 W  C, then W is a watching set for C under hP, li.
The proposition tells us how to modify the watching sets as the search proceeds. No
modification is required during a backtrack (claim 1). No modification is required if the
clause is satisfied or unit (claim 2), and we can also remove a newly valued literal from a
watching set if enough other unvalued variables are present (claim 3). No modification is
required unless we add the negation of an already watched literal (claim 4).
In sum, modification to the watching sets is only required when we add the negation of a
watched literal to our partial assignment and the watched clause is not settled; in this case,
481

fiDixon, Ginsberg, Hofer, Luks & Parkes

we have to add one of the remaining unvalued literals to the watching set. In addition, we
can remove literals from the watching set if enough unvalued literals are already in it. Since
this last possibility is not used in zChaff or other ground systems, here is an example of
it.
Suppose that we are, as usual, watching a and b in a  b  c. At some point, a becomes
true. We can either leave the watching set alone by virtue of condition 4, or we can extend
the watching set to include c (extending a watching set is always admissible, by virtue of
Lemma 7.7), and then remove a from the watching set. This change is unneeded in a ground
prover, but will be useful in the augmented version 7.10 of the proposition below.
To lift these ideas to an augmented setting, we begin by modifying Definition 7.6 in the
obvious way to get:
Definition 7.9 Let (c, G) be an augmented clause, and P an annotated partial assignment.
A watching set for (c, G) under P is any set of literals W that is a watching set for every
instance cg of (c, G) under P .
This leads to the following augmented analog of Proposition 7.8. (Although there are
four clauses in Proposition 7.8 and four in the following proposition, there is no clause-forclause correspondence between the two results.)
Proposition 7.10 Suppose that W is a watching set for (c, G) under P and l is a literal.
Then:
1. W is a watching set for (c, G) under any backtrack point for P .
2. If l 6 W  cG , then W is a watching set for (c, G) under hP, li.
3. If |(W  V )  cg  U (hP, li)| > 1 for every g  G such that cg is unsettled by hP, li,
then W  V is a watching set for (c, G) under hP, li.
4. If |(W  V )  cg  [U (hP, li)  (S(P )  S(P ))]| > 1 for every g  G, then W  V  {l}
is a watching set for (c, G) under hP, li.
As an example, suppose that we return to the augmented clause we considered in the
previous section, (abe, Sym(a, b, c, d)Sym(e, f )). Suppose that we are initially watching
a, b, c and d, and that e is false, and now imagine that a becomes false as well.
We need to augment W so that |W  cg  U (P )| > 1 for every unsettled instance cg of
(c, G) that contains a. Those instances are a  b  f , a  c  f and a  d  f . Since b, c and
d are already in W , we need to add f . If f had been in the watching set but not b, c and
d, we would have had to add those three points instead.
In this case, since the clause has a unit instance (a  b  e, for example), we cannot
remove a from the watching set. The reason is that if we do so and later backtrack past
this point, we are in danger of watching only b for this unsatisfied clause.
Suppose, however, that e had been unvalued when a became false. Now we would have
to add both e and f to the watching set and we would be free to remove a. This is sanctioned
by Proposition 7.10, since (c, G) has no settled instances and if cg  S(P ) =  for all g  G
as well, the conditions of claims three and four are equivalent.
482

fiZAP 3: Implementation

What if e, instead of being false or unvalued, had been true? Now we add f to the
watching set, but can we remove a from the new watching set {a, b, c, d, f }? We cannot:
the instance a  b  e would have only one watched literal if we did.
In some cases, however, we can remove the literal that just became false from the
watching set. We can surely do so if every clause instance still has two unvalued literals in
the watching set. This would correspond to the requirement that
|(W  V )  cg  U (hP, li)| > 1
for every instance. The stronger condition in claim four of the proposition allows us to do
slightly better in cases where the satisfied literal in the clause became satisfied sufficiently
recently that we know that any backtrack will unvalue it.
The fourth conclusion in Proposition 7.10 is essential to the effective functioning of our
overall prover; when we replace a watched literal l that has become false with a new and
unvalued literal, it is important that we stop watching the original watched literal l. It
is the last claim in the proposition that allows us to do this in most (although not all)
practical cases. Without this fourth conclusion, the watching sets would only get larger as
the search proceeded. Eventually, every literal in every clause would be watched and the
computational power of the idea would be lost.
We can now use the watching sets to reduce the number of clauses that must be examined in line 1 of the unit propagation procedure 2.7. Each augmented clause needs to
be associated with a watching set that is initialized and updated as sanctioned by Proposition 7.10.
Initialization is straightforward; for any clause (c, G) with c of length at least two, we
need to define an associated watching set W with the property that |W  cg | > 1 for every
g  G. In fact, we take W to be simply cG , the union of all of the instances cg , and
rely on subsequent unit tests to gradually reduce the size of W . (Once again, using the
fourth clause of Proposition 7.10.) The challenge is to modify Procedure 6.7 in a way that
facilitates the maintenance of the watching sets.
Before doing this, let us understand in a bit more detail how the watching sets are used
in searching for unit instances of a particular augmented clause. Consider the augmented
clause corresponding to the quantified clause
xy . [q(x)  r(y)  s]
If Q is the set of instances of q(x) and R the set of instances of r(y), this becomes the
augmented clause
(q(0)  r(0)  s, Sym(Q)  Sym(R))
(21)
where q(0) and r(0) are elements of Q and R respectively.
Now suppose that r(y) is true for all y, but q(x) is unvalued, as is s, so that the clause
(21) has no unit instances. Suppose also that we search for unit instances of (21) by first
stabilizing the image of r and then of q (s is stabilized by the group Sym(Q)  Sym(R)
itself). If there are four possible bindings for y (which we will denote 0, 1, 2, 3) and three
for x (0, 1, 2), the search space looks like this:

483

fiDixon, Ginsberg, Hofer, Luks & Parkes

 Sym(R)
sPSym(Q)
PP

@

PP

@
PP


PP
@

PP


@
PP


PP (r0 r3 )
Sym(Q) 
(r0 r1 )
@
P
@s(r0 r2 )
Ps

s
s
A
A
A






A
 A
 A
 A
 A
A
A
A
A




A
A
A
A




(q0 q2 )
(q0 q2 )
(q0 q2 )
1
1
1
1
AAs(q0 q2 )
A
A
A
As
As
As
s
s
s
s
s
s
s
s
(q0 q1 )

(q0 q1 )

(q0 q1 )

(q0 q1 )

In the interests of conserving space, we have written qi instead of q(i) and similarly for rj .
Each of the leaf nodes fails because both s and the relevant instance of q(x) are unvalued,
and we now construct a new watching set for the entire clause (21) that watches s and all
of the q(x).
Note that this causes us to lose significant amounts of information regarding portions
of the search space that need not be reexamined. In this example, the responsible literals
at each leaf node are as follows:
s
P
PP

@

PP

@
PP


PP

@

PP

@
PP


PP

@

P
@s
Ps
s
s

A
A
A






A
A
A
A



 A
A
A
A
A




A
A
A
A
q0 , s 
q2 , s
q0 , s 
q2 , s
q0 , s 
q2 , s
q0 , s 
A
A
A
AAsq2 , s
As
As
As
s
s
s
s
s
s
s
s
q1 , s

q1 , s

q1 , s

q1 , s

When we simply accumulate these literals at the root of the search tree, we conclude that
the reason for the failure is the watching set {q0 , q1 , q2 , s}. If any of these watched literals
changes value, we potentially have to reexamine the entire search tree.
We can address this by changing the order of variable stabilization, replacing the search
space depicted above with the following one:
s Sym(Q)  Sym(R)
P
PP


PP

PP


PP


PP

P




Sym(R) 
s

E
A
  EA
  E A
  E A
1
s s EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

(q0 q1 )

s
AE
  EA
  E A
  E A
1s s EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

484

PP
P

P
Ps(q0 q2 )
EA
  EA
  E A
  E A
1s s
EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

fiZAP 3: Implementation

Now only the center node needs reexpansion if the value of q1 changes, since it is only
at this node that q1 appears. The search space becomes simply:
s Sym(Q)  Sym(R)

(q0 q1 )

s

E
 A
  EA
  E A
  E A
1
s s EEs AAs(r0 r3 )
(r0 r1 ) (r0 r2 )

which is what one would expect if q1 changes value.
The upshot of this is that while we collect a new watching set for the original augmented
clause corresponding to (21), we also need to modify our unit propagation procedure so that
we first stabilize points that can be mapped to a specific watched literal that has become
unsatisfied.
To see how to keep the watching set updated, consider Proposition 7.10. When searching
for the unit instances of an augmented clause (c, G), we need to compute some set W such
that |W  cg  U (P )| > 1 for every unsettled instance cg of (c, G) that contains a fixed literal
w. How are we to do this?
The solution lies in Procedure 6.7, which describes our search for unit instances. If all
of the remaining clause instances below some particular search node are determined to be
nonunit in the test on line 3, instead of simply recognizing that every instance under this
node is nonunit, we need to be able to identify a set of unvalued literals that meets every
unsettled instance of cg at least twice. We modify the overlap procedure 5.19 to become:
Procedure 7.11 Given a group H, two sets c, V acted on by H, and a bound k  0, to
compute overlap(H, c, V, k), a collection of elements of V sufficient to guarantee that for
any h  H, |ch  V | > k, or  if no such collection exists:
1 m0
2 W 
3 for each orbit X of H
4
do {B1 , . . . , Bk }  a minimal block system for W under H for which
c  W  Bi for some i
5
 = |c  X| + min(Bi  V )  |B1 |
6
if  > 0
7
then m  m + 
8
W  W  (X  V )
9
if m > k
10
then return W
11 return 

485

fiDixon, Ginsberg, Hofer, Luks & Parkes

Proposition 7.12 Procedure 7.11 returns a nonempty set W if and only if Procedure 5.19
returns a value in excess of k. In this case, |ch  W | > k for every h  H.
We are finally in a position to replace Procedure 6.7 with a version that uses watched
literals:
Procedure 7.13 Given groups H  G, an element t  G, sets c, S and U , and optionally a watched element w, to find Transport(G, H, t, c, S, U, w), a skeletal set of unit
w-consequences for (c, G) given P :
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

1

if w is supplied and wt 6 cH
then return hfalse, , i
1
V  overlap(H, c, S t , 0)
if V 6= 
then return hfalse, , i
1
V  overlap(H, c, (S  U )t , 1)
if V 6= 
then return hfalse, , V t i
if c = cH
then if ct  U = 
then return htrue, 1, i
else return hfalse, hct  U, 1i, i
if a pruning lemma can be applied
then return hfalse, , i
1
  an element of c  cH . If w is supplied and w 6 ctH , choose  so that wt  H .
Y 
W 
for each t0 in (H : H )
do hu, V, Xi  Transport(G, H , t0 t, c, S, U, w)
if u = true
then return htrue, V t0 , i
else W  W  X
Y  Y  {hl, gt0 i|hl, gi  V }
return hfalse, Y, W i

In the application of the pruning lemmas in line 13, we need to use the restricted group
G{S,U,{w}} , so that we do not prune a group element g with w  cg on the basis of another
group element jgk for which w 6 cjgk , since jgk might itself then be pruned on line 2.
Proposition 7.14 Suppose that overlap(H, c, V, k) is computed using Procedure 7.11, or
otherwise satisfies the conclusion of Proposition 7.12. Then if there is a g  G such that
w  cg and cg  S = cg  U = , Transport(G, c, S, U, w) as computed by Procedure 7.13
returns htrue, g, i for such a g. If there is no such g, Procedure 7.13 returns hfalse, Z, W i,
where Z is a skeletal set of unit w-consequences of (c, G) given P , and W is such that
|W G{S,U,{w}}  ch  U | > 1 for every h  H such that w  ch and ch is unsettled by P .
486

fiZAP 3: Implementation

Note that the pruning lemmas are applied relatively late in the procedure (line 13) even
though a successful application prunes the space without increasing the size of the watching
set. It might seem that the pruning lemmas should be applied earlier.
This appears not to be the case. As discussed at the end of Section 5, the pruning lemmas
are relatively complex to check; moving the test earlier (to precede line 6, presumably)
actually slows the unit propagation procedure by a factor of approximately two, primarily
due to the need to compute the set stabilizer G{S,U } even in cases where a simple counting
argument suffices. In addition, the absolute impact on the watching sets can be expected
to be quite small.
To understand why, suppose that we are executing the procedure for an instance where
it will eventually fail. Now if n is a node that can be pruned either by a counting argument
(with the new contribution Wn to the set of watched literals) or by a lexicographic argument
using another node n0 , then since the node n0 will eventually fail, it will contribute its own
watching set Wn0 to the eventually returned value. While it is possible that Wn 6= Wn0
(different elements can be selected by the overlap function in line 6, for example), we
expect that in the vast majority of cases we will have Wn = Wn0 and the non-lexicographic
prune will have no impact on the eventual watching set computed.
Proposition 7.14 implies that the watching set returned by Procedure 7.13 can be used
to update the watching set as in the third claim of Proposition 7.10. For the fourth claim,
where we hope to remove l from the new watching set, we need to check to see if
|W  cg  [U (hP, li)  (S(P )  S(P ))]| > 1
for each g  G, where W is the new watching set. This can be determined by a single call
to transport; if there is no g  G for which
|cg  [W  (U (hP, li)  (S(P )  S(P )))]|  1

(22)

we can remove l from W . In some cases, we can save the call to transport by exploiting
the fact (as shown in the proof of Proposition 7.10) that (22) cannot be satisfied if hP, li
has a unit consequence.
We are finally in a position to describe watched literals in an augmented setting. As a
start, we have:
Definition 7.15 A watched augmented clause is a pair h(c, G), W i where (c, G) is an augmented clause and W is a watching set for (c, G).
Procedure 7.16 (Unit propagation) To compute Unit-Propagate(C, P, L) where C
is a set of watched augmented clauses, P is an annotated partial assignment, and L is a set
of pairs hl, ri of literals l and reasons r:

487

fiDixon, Ginsberg, Hofer, Luks & Parkes

1 while L 6= 
2
do hl, ri  an element of L
3
L  L  hl, ri
4
P  hP, hl, rii
5
for each h(c, G), W i  C
6
do if l  W
7
then hr, H, V i  Transport(G, c, S(P ), U (P ), l)
8
if r = true
9
then li  the literal in cH with the highest index in P
10
return htrue, resolve((cH , G), ci )i
11
H 0  complete(H, G{S(P ),U (P ),{l}} )
12
for each h  H 0
13
do z  the literal in ch unassigned by P
14
if there is no hz, r0 i in L
15
then L  L  hz, ch i
16
W  W  (U (P )  V G{S(P ),U (P ),{l}} )
17
U  U (P )  (S(P )  S(P ))
18
if H =   transport(G, c, , W  U, 1, l) = failure
19
then W  W  {l}
20 return hfalse, P i
On line 18, we invoke a version of the transport function that accepts as an additional argument a literal that is required to be included in the clause instance being sought.
This modification is similar to the introduction of such a literal w in the Transport procedure 7.13.
Proposition 7.17 Let P be an annotated partial assignment, and C a set of watched augmented clauses, where for every h(c, G), W i  C, W is a watching set for (c, G) under P .
Let L be the set of unit consequences of clauses in C. If Unit-Propagate(C, P, L) returns
htrue, ci for an augmented clause c, then c is a nogood for P , and any modified watching
sets in C are still watching sets under P . Otherwise, the value returned is hfalse, P i and
the watching sets in C will all have been replaced with watching sets under P .
Procedure 7.16 can be modified and incorporated in a fairly obvious way into Procedure 2.8, where the literal most recently added to the partial assignment is added to L and
thereby passed into the unit propagation procedure.

8. Resolution Revisited
There is one additional theoretical point that we need to discuss before turning our attention
to experimental matters.
The goal in augmented resolution is to produce many (if not all) of the resolvents
sanctioned by instances of the augmented clauses being resolved. As we showed in zap2,
however, it is not always possible to produce all such resolvents. Here is another example
of that phenomenon.
488

fiZAP 3: Implementation

Suppose that we are resolving the two clauses
(a  c, (ab))

(23)

(b  c, (ab))

(24)

(a  b, (ab))

(25)

and
The result is10
But consider the example. The instances of (23) are ac and bc; those of (24) are bc
and a  c. Surely it is better to have the resolvent be (a, (ab)) instead of (25). In general,
we never want to conclude (c, G) when it is possible to conclude (c0 , G) for c0  c where the
set inclusion is proper. The resolvent with c0 is properly stronger than that with c.
There is an additional consideration as well. Suppose that we are resolving two augmented clauses, and can choose instances of the resolving clauses so that the resolvent is
(a  c, G) or (b  c, G), where a and b are literals and the two possible resolvents are distinct
because (ab) 6 G. Which should we select?
We know of no general answer, but a reasonable heuristic is to make the choice based on
the order in which literals were added to the current partial assignment. Assuming that the
resolvent is a nogood, presumably a and b are both false for the current partial assignment
P . We should select the resolvent that allows a larger backjump; in this case, the resolvent
involving the literal that was added to P first.
All of these considerations have no direct analog in a conventional Boolean satisfiability
engine. For any particular literal l, the resolvent of the reasons for l and for l is just that;
there is no flexibility possible.11
Definition 8.1 Let (, G) and (, H) be two augmented clauses resolving on a literal l, so
that l   and l  . An l-resolvent for (, G) and (, H) will be any clause obtained by
resolving g and  h for g  G and h  H such that l  g and l   h .
Note that the group Z in the resolvent clause (resolve(g ,  h ), Z) is independent of
the resolvent selected, so we can focus our attention strictly on the syntactic properties of
the resolvent.
We next formalize the fact that the partial assignment P induces a natural lexicographic
ordering on the set of nogoods for a given theory:
Definition 8.2 Let P be a partial assignment, and c a ground clause. If l is the literal in
c whose negation has maximal index in P , we will say that the falsification depth of c is the
position in P of the literal l. The falsification depth is zero if there is no such literal in c;
in any event, the falsification depth of c will be denoted by c?P .
If c1 and c2 are two nogoods, we will say that c1 is falsified earlier than c2 by P , writing
?P
?P
?P
c1 <P c2 , if either c?P
1 < c2 , or c1 = c2 and c1  lc?P <P c2  lc?P .
1

2

10. The result can be obtained by direct computation or by applying the resolution stability property discussed in zap2, since the groups are identical.
11. A weak analog is present in zChaff, which can replace one nogood n with another n0 if n0 leads to a
greater backjump than n does. This functionality is part of the zChaff code but does not appear to
have been documented.

489

fiDixon, Ginsberg, Hofer, Luks & Parkes

As an example, suppose that P is ha, b, c, d, ei. The falsification depth of a  c is
three, since c is the third variable assigned in P . The falsification depth of b  d is four.
Thus a  c <P b  d; we would rather learn a  c because it allows us to backjump
to c instead of to d. Similarly a  c  e <P b  d  e; once the common element
e is eliminated, we would still rather backtrack to c than to d. In general, our goal when
resolving two augmented clauses is to select a resolvent that is minimal under <P . Note
that we have:
Lemma 8.3 If c1  c2 are two nogoods for P , then c1 <P c2 .
Procedure 8.4 Suppose we are given two augmented clauses (, G) and (, H) that are
unit for a partial assignment P = hl1 , . . . , ln i, with l   and l  . To find a <P -minimal
l-resolvent of (, G) and (, H):
1
2
3
4
5
6
7
8
9
10
11
12
13

U  {l, l}
 literals you cant avoid
f  
f  
p  [(  )  U ]?P
while p > 0
do g  transport(G, , {lp , . . . , ln }  U, , 0, l)
h  transport(H, , {lp , . . . , ln }  U, , 0, l)
if g = failure  h = failure
then U  U  {lp }
else f  g
f   h
p  [(f  f )  U ]?P
return resolve(f , f )

The basic idea is that we gradually force the two clause instances away from the end of
the partial assignment; as we back up, we keep track of literals that are unavoidable because
an associated call to transport failed. The unavoidable literals are accumulated in the set
U above, and as we continue to call the transporter function, we have no objection if one or
both of the clause instances includes elements of U . At each point, we refocus our attention
on the deepest variable that is not yet known to be either avoidable or unavoidable; when
we reach the root of the partial assignment, we return the instances found.
Here is an example. Suppose that P = ha, b, c, d, ei as before, and that (, G) has
instances c  d  l and a  e  l. The second clause (, H) has the single instance
b  e  l.
If we resolve the <P -minimal instances of the two augmented clauses, we will resolve
c  d  l with b  e  l to get b  c  d  e. We do better if we resolve a  e  l
and b  e  l instead to get a  b  e. The literals b and e appear in any case,
but were better off with a than with c  d.
Suppose that we follow this example through the procedure, with U initially set to
{l, l} and (say)  and therefore f set to c  d  l. Both  and f are set to b  e  l,
since this is the only instance of (, H). The initial value for p is five, since the last literal
in     U = {b, c, d, e} is e.
490

fiZAP 3: Implementation

We now try to find a way to avoid having e appear in the final resolvent. We do this
by looking for an instance of (, G) that includes l (the literal on which were resolving)
and avoids e (and any subsequent literal, but there arent any). Such an instance is given
by  itself. But there is no instance of (, H) that avoids e, so the call in line 7 fails. We
therefore add e to U and leave the clauses f and f unchanged. We decrement p to four,
since e is no longer in (f  f )  U .
On the next pass through the loop, we are looking for clause instances that avoid
{d, e}  U = {d}. We know that well be forced to include e in the final result, so we
dont worry about it. All we hope to do at this point is to exclude d.
Here, we are successful in finding such instances. The existing instance  suffices, as
does the other instance a  e  l of (, G). This becomes the new f and p gets reduced
to two, since we now have (f  f )  U = {a, b}.
The next pass through the loop tries to avoid b while continuing to avoid c and d
(which we know we can avoid because the current f and f do so). This turns out to be
impossible, so b is added to U and p is decremented to one. Avoiding a is impossible as
well, so p is decremented to zero and the procedure correctly returns a  b  e.
Proposition 8.5 Suppose that we are given two augmented clauses (, G) and (, H) such
that  and  are unit for a partial assignment P , with l   and l  . Then the value
returned by Procedure 8.4 is a <P -minimal l-resolvent of (, G) and (, H).
The procedure can be implemented somewhat more efficiently than described above; if
f , for example, already satisfies the condition implicit in line 6, there is no need to reinvoke
the transport function for g.
More important than this relatively slender improvement, however, is the fact that
resolution now involves repeated calls to the transport function. In general, Boolean
satisfiability engines need not worry about the time used by the resolution function, since
unit propagation dominates the running time. A naive implementation of Procedure 8.4,
however, involves more calls to transport than does the unit propagation procedure, so
that resolution comes to dominate zaps overall runtime.
To correct this, remember the point of Procedure 8.4. The procedure is not needed for
correctness; it is only needed to find improved resolution instances. The amount of time
spent looking for such instances should be less than the computational savings achieved by
having them. Put slightly differently, there is no requirement that we produce a resolvent
that is absolutely minimal under the <P ordering. A resolvent that is nearly minimal will
suffice, especially if producing the truly minimal instance involves large computational cost.
We achieve this goal by working with a modified transport function on lines 6 and 7
of Procedure 8.4. Instead of expanding the coset decomposition tree completely, a limited
number of nodes are examined. Zaps current implementation prunes the transporter search
after 100 nodes have been examined; in solving the pigeonhole problem, for example, this
turns out to be sufficient to ensure that the resulting proof length is the same as it would
have been had strictly <P -minimal resolvents been found. We also modify the pruning
computation, pruning with K = GSU instead of the more difficult to compute G{S,U } .
Since GSU  G{S,U } (stabilizing every element of a set surely stabilizes the set itself), this
approximation saves time but reduces the amount of possible pruning. This is appropriate

491

fiDixon, Ginsberg, Hofer, Luks & Parkes

10
CPU time
1.65e-06 n**4.6

1

secs

0.1

0.01

0.001

1e-04
5

10

15

20

pigeons

Figure 2: CPU time for a resolution in the pigeonhole problem
given the artificially reduced size of the overall search tree and the need to produce an
answer quickly.

9. Experimental Results: Components
We are finally in a position to describe the experimental performance of the algorithms that
we have presented. As remarked in the introduction, we begin by describing the performance
of zaps algorithmic components, its resolution and unit propagation algorithms. Performance results for a complete inference tool build using our ideas are in the next section.
All experiments were performed on a 2GHz Pentium-M with 1GB of main memory.
9.1 Resolution
We have implemented the resolution procedure described in Section 4, and the results for
the pigeonhole problem are shown in Figure 2. This particular example involves resolving
the two basic axioms in a pigeonhole problem containing n pigeons and n  1 holes:
(p11      p1,n1 , G)
(p11  p12 , G)

492

fiZAP 3: Implementation

The first axiom says that pigeon 1 must be in some hole; the second, that the first two
pigeons cannot both be in the first hole. The group G corresponds to a global symmetry
group where pigeons and holes can be interchanged freely.
The resolvent of the above two axioms can in fact be computed without any grouptheoretic computation at all, using the result from zap2 that the group of stable extensions
of (c1 , G) and (c2 , G) is always a superset of the group G. The algorithm in Section 4 for
computing augmented resolvents does not include a check to see if the groups are identical,
but the implementation does include such a check. This test was disabled to produce the
data in Figure 2.
We plot the observed time (in seconds) for the resolution as a function of the number
of pigeons involved, with time plotted on a log scale. Memory usage was typically approximately 5MB; the CPU usage was dominated by the need to compute stabilizer chains for
the groups in question. The algorithms used for doing so take time O(d5 ) where d is the
size of the domain on which the group is operating (Furst, Hopcroft, & Luks, 1980; Knuth,
1991). In this case, the symmetries over pigeons and over holes can be stabilized independently and we therefore expect the stabilizer chain computation to take time O(n5 ), where
n is the number of pigeons. We fit the data to the curve axb , with the best fit occurring for
b  4.6. This is consistent with the stabilizer chain computation dominating the runtime.
If we reinsert the check to see if the groups are the same, the running times are reduced
uniformly by approximately 35%. Testing group equality involves checking to see if each
generator of G1 is a member of G2 and vice versa, and therefore still involves computing
stabilizer chains for the groups in question. Once again, the need to compute the stabilizer
chains dominates the computation.
9.2 Unit Propagation
In Figure 3 we give data showing the average time needed for a unit test in the pigeonhole
problem. These are the naturally occurring unit tests that arise in a run of the prover
on the problem in question. The memory used by the program remained far less than the
1GB available; as an example, maximum usage was approximately 20MB for 13 pigeons.12
Since the unit test is NP-complete, it is customary to give both mean and median
running times; we present only means in Figure 3 because the mean running times appear
to be growing polynomially (compare the two lines of best fit in the figure), and because
the medians appear to be only modestly smaller than the means. This is shown in Figure 4,
where it appears that the ratio of the mean to median running times is growing only linearly
with problem size.
The earlier figure 3 also shows the average CPU time for failed tests (where the clause
in question has no unit instances) and successful tests (where unit instances exist); as
can be seen, failed unit tests generally complete far more quickly than their successful
counterparts of similar size as the various pruning heuristics come into play. In both cases,
however, the scaling continues to appear to be polynomial in the problem size.
12. Accurately measuring peak memory usage is difficult because the group operations regularly allocate
and free relatively large blocks of memory. We measured the usage by simply starting a system monitor
and observing it, which was not practical for problem instances that took extended amounts of time to
complete. This is the reason that we report memory usage only approximately, and only for one problem
instance.

493

fiDixon, Ginsberg, Hofer, Luks & Parkes

10
average
fail
succeed
polynomial fit
exponential fit
1

secs

0.1

0.01

0.001

1e-04
4

6

8

10

12
pigeons

14

16

18

20

Figure 3: CPU time for a unit test in the pigeonhole problem

10. Experimental Results: ZAP
We conclude our discussion of zaps experimental performance with results on problem
instances in their entirety, as opposed to the performance of individual algorithmic components. Before presenting the results, however, let us describe both the domains being
considered and our expectations with regard to performance of both zap and of existing
systems in these areas.
We will be examining performance in three domains:
1. In a pigeonhole problem, the goal is to show that you cannot put n + 1 pigeons into n
holes if each pigeon is to get its own hole.
P
P
2. In a parity problem, the goal is to show that iI xi + iJ xi cannot be odd if the
sets I and J are equal (Tseitin, 1970).
3. In a clique-coloring problem, the goal is to show that a map containing an m-clique
cannot be colored in n colors if n < m.
The reasons that we have chosen these particular problem classes are as follows:
1. They all should
be easy.
P
P Its obvious that you cant put n + 1 pigeons into n holes,
and that iI xi + iJ xi is even if each xi appears exactly twice. Its also obvious
that you cant color a graph containing an m-clique user fewer than m colors.
494

fiZAP 3: Implementation

2.1
ratio
2

1.9

mean/median ratio

1.8

1.7

1.6

1.5

1.4

1.3

1.2
4

6

8

10

12
pigeons

14

16

18

20

Figure 4: Mean vs. median CPU time for a unit test in the pigeonhole problem
In this last case especially, note that we are solving an easy problem. It is not the
case that we are trying to color a specific graph containing an m-clique; the goal is
to show that any graph containing an m-clique anywhere cannot be colored. This is
very different from graph coloring generally.
Put somewhat differently, all of the problems that we will be examining are in P.
Given suitable representations, they should all be easy.
2. All of the problems are known to be exponentially difficult for resolution-based methods. This was shown for pigeonhole problems by Haken (1985) and for parity problems
by Tseitin (1970). Clique-coloring problems are known to be exponentially difficult
not only for resolution, but for linear programming methods as well (Pudlak, 1997).
In fact, we know of no implemented system that scales polynomially on this class of
problem.
3. Finally, all of these problems involve structure that can be captured in a group-based
setting.
The data that we will present compares zaps performance to that of zChaff; Section 10.4 discusses the performance of some other Boolean tools on the problem classes
that we will be discussing. We chose zChaff for comparison partly because it has been
discussed throughout this series of papers, and partly because it appears to have the best
495

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+06
zap
zchaff
10000

secs

100

1

0.01

1e-04

1e-06
4

6

8

10

12
pigeons

14

16

18

20

Figure 5: CPU time for pigeonhole instances, zap and zChaff
overall performance on the three problem classes that we will be considering. (Once again,
see Section 10.4 for additional details.)
ZAP expectations Before proceeding, let us point out that on a theoretical basis, it is
known that short group-based proofs exist for all of these problems. We showed in zap2 that
group-based pigeonhole proofs can be expected to be short, and also that all parity problems
have short group-based proofs that mimic Gaussian elimination. We also showed that short
group-based proofs existed for clique coloring, although the proof was fairly intricate. Our
goal here is to determine whether an implementation of our ideas can discover these short
proofs in practice, or whether the control of group-based inference will require additional
theoretical ideas that we do not yet understand.
Please understand that our goal at this point is not to test zap on standard NP-complete
search problems in Boolean form, such as graph coloring or quasigroup completion problems (Gomes & Selman, 1997). Doing so involves a significant effort in ensuring that zaps
constant factors and data structures are comparable to those of other systems; while preliminary indications are that this will be possible with only modest impact on performance
(approximately a factor of two), the work is not yet complete and will be reported elsewhere.

496

fiZAP 3: Implementation

100000
zap
exponential fit
polynomial fit
10000

1000

secs

100

10

1

0.1

0.01

0.001
4

6

8

10

12
pigeons

14

16

18

20

Figure 6: zap scaling for pigeonhole instances
10.1 Pigeonhole Results
Figure 5 shows running times for both zap and for zChaff on pigeonhole instances. Figure 6 repeats the zap data, also including best exponential and polynomial fits for the time
spent. The overall running time appears to be polynomial, varying as approximately n8.1
where n is the number of pigeons. In very rough terms, there is a factor of O(n5 ) needed for
the stabilizer chain constructions. If we branch only on positive literals, we know (see zap2)
that there will be O(n) resolutions needed to solve the problem, and each resolution will
lead to O(n2 ) unit propagations. The total time can thus be expected to be approximately
O(n8 ), assuming that each unit propagation involves only stabilizer chain computations and
no actual search. Our observed performance is close to this theoretical value.
In practice, zap branches not on positive literals, but on negative ones. The reason is
that the negative literals appear in far more clauses than the positive ones (O(n) clauses
for each negative literal as opposed to a single clause for a positive literal), and the usual
branching heuristic in the Boolean satisfiability community initially assigns to a variable
the value that satisfies as many clauses as possible.
The number of nodes expanded by zap in solving any particular instance of the pigeonhole problem is shown in Figure 7, which also presents similar data for zChaff. The
number of nodes expanded by zap is in fact exactly n2  3n + 1; curiously, this is also the
depth of the zChaff search for the next smaller instance with n  1 pigeons. We do not
497

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+07
zap
zchaff
1e+06

100000

nodes

10000

1000

100

10

1
4

6

8

10

12
pigeons

14

16

18

20

Figure 7: Nodes expanded in the pigeonhole problem
know if the small size of the pigeonhole proofs found by zap is the result of the effectiveness
of the use of <P -optimal resolvents, or if some fundamental argument can be made that all
zap proofs of the pigeonhole problem will be short.
Before moving on to parity problems, allow us to comment on the importance of the
various algorithmic techniques that we have described. We recognize that many of the
algorithms we have presented are quite involved, and it is important to demonstrate that
the associated algorithmic complexity leads to legitimate computational gains.
Figure 8 shows the time needed to solve pigeonhole instances if we either abandon the
pruning lemmas or avoid the search for <P -optimal resolvents. As should be clear from the
data, both of these techniques are essential to obtaining the overall performance exhibited
by the system.
If we abandon the search for <P -optimal resolvents, the proof lengths increase significantly but appear to remain polynomial in n. The length increase in the learned axioms
leads to increased running times for unit propagation, and this appears to be the primary
reason for the performance degradation in the figure. The overall running times scale exponentially.
Abandoning the pruning lemmas also leads to exponential running times. This is to be
expected at some level; there are still exponentially many learned ground axioms and if we
cannot prune the search for unit instances, exponential behavior is to be expected.

498

fiZAP 3: Implementation

1e+06
100000
10000
1000

secs

100
10
1
0.1
0.01
0.001

zap
no pruning
no resolution instances

1e-04
4

6

8

10

12
pigeons

14

16

18

20

Figure 8: Improvement due to pruning lemmas and <P -optimal resolution instances. The
circles mark zaps performance. The xs indicate performance with the pruning
lemmas disabled during unit propagation, and the boxes give performance if resolutions use the original base instances of the clauses being resolved, as opposed
to searching for and resolving <P -optimal instances.

There were other ways that we could have reduced zaps algorithmic complexity as
well. We could, for example, have removed watched literals and the computational machinery needed to maintain them. As it turns out, this change has virtually no impact on
zaps pigeonhole performance because the provers behavior here is typically backtrack-free
(Dixon et al., 2004a). In general, however, watched literals can be expected to play as
important a role in zap as they do in any other dpll-style prover. Our overall focus in this
series of papers has been to show that group-based augmentations could be implemented
without sacrificing the ability to use any of the recent techniques that have made Boolean
satisfiability engines so effective in practice, and watched literals can certainly be numbered
amongst those techniques.
We also did not evaluate the possibility of not learning augmented clauses at all, perhaps
learning instead only their ground versions. This would avoid the need to implement Procedure 4.1, but would also avoid all of the computational gains to which zap theoretically
has access. It is only by learning augmented clauses that theoretical reductions in proof

499

fiDixon, Ginsberg, Hofer, Luks & Parkes

size can be obtained; otherwise, the proof itself would necessarily be unchanged from any
other dpll-style approach.
10.2 Tseitin Results
The next problem class for which we present experimental data is one due to Tseitin (1970)
that was shown by Urquhart (1987) to require resolution proofs of exponential length. Each
problem is based on a graph G. We associate a Boolean variable with each edge in G, and
every vertex v in G has an associated charge of 0 or 1 that is equal to the sum mod 2 of the
variables adjacent to v. The charge of the entire graph G is the sum mod 2 of the charges
of its vertices. If we require that a connected graph G have a charge of one, then the set
of constraints associated with its vertices is unsatisfiable (Tseitin, 1970). Here is the graph
for a problem of size four, together with its associated constraints:
1

0

a

u
@
b@

u

e
@

c

d

@
@
@
u

0

f

@
@u

0

a+b+c1
d+e+a0
f +b+d0
c+e+f 0
In the language of zap (see Appendix B), we have
a
d
f
c

b
e
b
e

c
a
d
f

%2=
%2=
%2=
%2=

1
0
0
0

;
;
;
;

The axiom set is unsatisfiable because adding all of the above axioms gives us
2a + 2b + 2c + 2d + 2e + 2f  1
These problems are known to be exponentially difficult for resolution-based methods (Urquhart,
1987).
Times to solution for zap and zChaff are shown in Figure 9. ZChaff is clearly scaling
exponentially; the best fit for the zap times is 0.00043n1.60 log(n) , where n is the problem
size.

500

fiZAP 3: Implementation

1e+06
zap
zchaff
10000

secs

100

1

0.01

1e-04

1e-06
5

10

15

20

size

Figure 9: CPU time for Tseitin instances, zap and zChaff. ZChaff is scaling exponentially; zap is scaling as O(n1.6 log(n) ).

Figure 10 shows the number of nodes expanded by the two systems. The number
of search nodes expanded by zap appears to be growing polynomially with the size of the
problem (O(n2.6 ), give or take), in keeping with a result from zap2 showing that zap proofs
of polynomial length always exist for parity problems. As with the pigeonhole instances,
we see that short proofs exist not only in theory, but apparently in practice as well.
Given that a polynomial number of nodes are expanded but a super-polynomial amount
of time is consumed, it seems likely that the unit propagation procedure is the culprit,
taking a super-polynomial amount of time per unit propagation. As shown in Figure 11,
this is in fact the case. But the unit test should be easy here  after all, the groups are all
simply those that flip an even number of the variables in question. If we want to know if an
augmented clause has a unit instance, we find the unvalued variables it contains. If more
than one, the clause is not unit. If exactly one, the clause is always unit  the variable must
be valued so as the make the parity of the sum take the desired value. So there seems to
be no reason for the unit tests to be scaling as nlog(n) .
The nlog(n) scaling itself appears to be a consequence of the multiset stabilizer computation that underlies the k-transporter pruning. Here, too, the scaling should be polynomial,
since we can show that polytime (O(n3 )) methods exist for set stabilizer for the groups

501

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+07
zap
zchaff
1e+06

100000

nodes

10000

1000

100

10

1
5

10

15

20

size

Figure 10: Nodes expanded in the Tseitin problems. ZChaff is scaling exponentially; zap
is scaling polynomially as O(n2.6 ).

in question.13 The general methods implemented by gap and by zap do not exploit the
Abelian nature of the parity groups, however, and the scaling is as shown. An obvious extension of the existing implementation would include more efficient set stabilizer algorithms
for these groups.
10.3 Clique Coloring
The final problem class for which we present experimental data is that of clique coloring.
This class of problems is related to the pigeonhole problem but far more difficult.
As mentioned previously, the domain is that of graph coloring, where two nodes connected by an edge must be assigned different colors. If the graph is a clique of size m, then
it is obvious that the graph cannot be colored in m  1 colors. This is equivalent to an
instance of the pigeonhole problem. But in the clique coloring problem, we are not told
that the graph is a clique of size m, only that it contains a clique of size m. The fact that
we do not know the exact location of the clique widens the search considerably.
13. The argument can be made either from the fact that the groups are Abelian, or from the fact that the
group orbits are all of size two, and the set stabilizer problem can thus be converted to one of linear
algebra over Z2 .

502

fiZAP 3: Implementation

1
average
3.21e-05 x**log x

secs

0.1

0.01

0.001

1e-04
5

10

15

20

size

Figure 11: CPU time for a unit test in the Tseitin problems. Zap is scaling as approximately
O(nlog(n) ).

We know of no (other) automated proof system that scales polynomially on problems
in this class; both resolution and linear programming methods inevitably scale exponentially (Pudlak, 1997). We showed in zap2 that zap could produce polynomial-length proofs
in theory, but no suggestions were made that such proofs would be easy to find in practice.
Before we present the details of zaps performance on this problem class, let us reiterate
our observation that clique-coloring problems should not be thought of as unsatisfiable
instances of graph-coloring problems generally. A particular instance of this problem class
does not describe a specific graph that needs to be colored; it says only that the graph
contains an m-clique and needs to be colored in m  1 colors.
An axiomatization of this problem is as follows. We use eij to describe the graph, cij to
describe the coloring of the graph, and qij to describe the embedding of the clique into the
graph. The graph has m nodes, the clique is of size n + 1, and there are n colors available.
ci1      cin

for i = 1, . . . , m

(26)

qi1      qim

for i = 1, . . . , n + 1

(27)

for 1  i < j  m, l = 1, . . . , n

(28)

for 1  i < k  n + 1, j = 1, . . . , m

(29)

eij  cil  cjl
qij  qkj

503

fiDixon, Ginsberg, Hofer, Luks & Parkes

eij  qki  qlj

for 1  i < j  m, 1  k 6= l  n + 1

(30)

Here eij means that there is an edge between graph nodes i and j, cij means that graph
node i is colored with the jth color, and qij means that the ith element of the clique is
mapped to graph node j. Thus the first axiom (26) says that every graph node has a color.
(27) says that every element of the clique appears in the graph. (28) says that two of the m
nodes in the graph cannot be the same color (of the n colors available) if they are connected
by an edge. (29) says that no two elements of the clique map to the same node in the graph.
Finally, (30) says that the clique is indeed a clique  no two clique elements can map to
disconnected nodes in the graph.
The encoding passed to zap was group-based, as follows:
SORT color 2 ;
SORT node 4 ;
SORT clique 3 ;
PREDICATE edge( node node ) ;
PREDICATE color( node color ) ;
PREDICATE clique( clique node ) ;
GROUP COLOR <
(( color[1 1] color[1 2])
( color[2 1] color[2 2])
( color[3 1] color[3 2])
( color[4 1] color[4 2]))
> ;
GROUP CLIQUE <
(( clique[1 1] clique[2 1])
( clique[1 2] clique[2 2])
( clique[1 3] clique[2 3])
( clique[1 4] clique[2 4]))
(( clique[2 1] clique[3 1])
( clique[2 2] clique[3 2])
( clique[2 3] clique[3 3])
( clique[2 4] clique[3 4]))
> ;
GROUP NODES <
(( edge[1 3] edge[2 3])
( edge[1 4] edge[2 4])
( color[1 1] color[2 1])
( color[1 2] color[2 2])
( clique[1 1] clique[1 2])
( clique[2 1] clique[2 2])
( clique[3 1] clique[3 2]))
(( color[2 1] color[3 1] color[4 1])
( color[2 2] color[3 2] color[4 2])
( edge[1 2] edge[1 3] edge[1 4])
( edge[2 3] edge[3 4] edge[2 4])
( clique[1 2] clique[1 3] clique[1 4])
( clique[2 2] clique[2 3] clique[2 4])
( clique[3 2] clique[3 3] clique[3 4]))
504

fiZAP 3: Implementation

1e+06
zap
zchaff
10000

secs

100

1

0.01

1e-04

1e-06
4

6

8
10
12
graph size (clique size one less)

14

16

18

Figure 12: CPU time for clique instances, zap and zChaff
> ;
color[1 1] color[1 2] GROUP NODES ;
clique[1 1] clique[1 2] clique[1 3] GROUP CLIQUE ;
-edge[1 2] -color[1 1] -color[2 1] GROUP NODES COLOR ;
-clique[1 1] -clique[2 1] GROUP NODES CLIQUE ;
-clique[1 1] -clique[2 2] edge[1 2] GROUP NODES CLIQUE ;

This is the version where there is a 3-clique in a graph of size four, and we are trying to
use just two colors. The first group is the symmetry over colors alone, the second that over
the elements of the clique, and the third the symmetry over nodes. The axiomatization is
identical to that presented earlier. Note that although there is a common symmetry in this
problem, the axiomatization obscures that in some sense, since we have only included the
relevant symmetry or symmetries in any particular axiom.
Times to solution for zap and zChaff are shown in Figure 12. As might be expected,
zChaff is scaling exponentially; zap appears to be scaling as n8.5 . In order to allow the
data to be presented along a single axis, these problem instances were selected so that the
clique size was one smaller than the graph size.
Figure 13 shows the number of nodes expanded by the two systems. Once again, the
number of nodes expanded by zChaff is growing exponentially with problem size, while
the number expanded by zap is growing polynomially. As with the pigeonhole problem,
505

fiDixon, Ginsberg, Hofer, Luks & Parkes

1e+07
zap
zchaff
1e+06

100000

nodes

10000

1000

100

10

1
4

6

8
10
12
graph size (clique size one less)

14

16

18

Figure 13: Nodes expanded in the clique problems
we see that the short proofs whose existence is guaranteed by the theory can be found in
practice.
Figures 14 and 15 display zaps performance on a somewhat wider range of problem
instances where the clique and graph sizes are allowed to vary independently. The number
of nodes expanded was in general
(c + g)2  13c  g + 14
2
where c is the size of the clique and g the size of the graph. There were a handful of outliers,
most notably the c = 11, g = 13 instance which expanded a larger number of nodes. The
other exceptions all expanded fewer nodes.
With regard to total CPU time (Figure 15), the time appears to be scaling as (cg)3.89 .
Once again, c = 11, g = 13 is an outlier but polynomial performance is observed generally.
To the best of our knowledge, zap is the first system to exhibit polynomial performance on
this problem class; as we have remarked, most other approaches have been proven to scale
exponentially.
10.4 Related Work
Finally, we compare our experimental results to those obtained using other systems that
attempt to exploit problem structure to improve the performance of satisfiability solvers.
506

fiZAP 3: Implementation

zap
0.075 (x+y)**2.52

1000

100
nodes

10
12
graph size
4

6

8

10

4
12

clique size

Figure 14: Nodes expanded in the clique problems
This section provides a high-level summary of experimental results for a number of these
efforts and compares these results with zap on the benchmark problems described in the
previous sections.
Recall that our benchmark problems are all highly structured, but each has a very different type of structure. Theoretically, these problems all allow polynomial-time solutions,
but they are provably hard for conventional solvers. A solver that solves all of these problems efficiently has the ability to exploit a range of different types of problem structure
and automates a strong proof system. Of course, to be interesting, a solver must also be a
practical general purpose solver. For example, Tseitin problems can be solved in polynomial
time by a form of Gaussian elimination (Schaefer, 1978), and pigeonhole problems can be
solved in polynomial time by a linear programming method such as the simplex method.
However, neither of these solutions constitutes a practical general purpose solver.
We ran a number of solvers on the benchmark problems, obtaining the following results:

507

fiDixon, Ginsberg, Hofer, Luks & Parkes

zap
3.54e-06 (xy)**3.89

10000
1000
100
10
1
secs

0.1
0.01

12
graph size
4

6

4
8

10

12

clique size

Figure 15: CPU time expended in the clique problems

zap
zChaff
pbchaff
eqsatz
march eq
resolution
cutting-planes or
integer programming

pigeonhole
P
E
P
E
E
E

Tseitin
nlog n
E
E
E
E (P)
E (?)

clique coloring
P
E
E
E
E
E

P

?

E

Rather than presenting numerous graphs, we summarize our results above, simply reporting the overall scaling of each solver on each problem class. Polynomial-time scaling is
indicated with a P and exponential-time scaling with an E. Scaling is shown for the three
problem classes we have discussed, with two separate encodings considered for the Tseitin
problems. The first encoding is the Booleanization of the encoding of Section 10.2; the
second involves the introduction of new variables to reduce clause length and is described
below. If the performance is improved by this introduction, the new scaling is given parenthetically. The final two rows give known proof complexity results for the resolution
and cutting-planes proof systems and thus provide lower bounds on the corresponding rows
above them.
508

fiZAP 3: Implementation

Reducing performance results to exponential or polynomial scaling omits valuable information. Clearly the difference between n100 and n2 scaling is something we care about,
although both are polynomial. The details of specific scaling factors will be included in the
discussion that follows; our goal in the table is merely to summarize the strength of each
solvers underlying proof system.
Details of the solvers appearing in the table are as follows:
 pbchaff is a pseudo-Boolean version of the dpll algorithm. It represents problems
in pseudo-Boolean form and automates a cutting-planes proof system. The cuttingplanes proof system allows polynomial-length proofs of the pigeonhole problem and
pbchaff is able to solve these problems efficiently. Scaling for pbchaff on pigeonhole
instances was as n4.8 , where n is the number of pigeons. This is an improvement over
the n8.1 scaling seen for zap. However, the performance of pbchaff on Tseitin and
clique coloring problems is exponential, since cutting-planes inference is not able to
capture and exploit the structure of these problems.
 eqsatz (Li, 2000) and march eq (Heule & van Maaren, 2004) are dpll-based solvers
that have been modified to incorporate equivalence reasoning, which should enable
them to solve parity problems efficiently. As expected, both eqsatz and march eq
exhibited exponential scaling on pigeonhole and clique coloring problems, since these
solvers are not designed to recognize the structure of these problems. More surprising
was the exponential scaling observed for both eqsatz and march eq on our initial
encoding of the Tseitin problems.
Eqsatz scales exponentially because it does not recognize the structure present in
our encoding of the parity problems.14 This performance can be improved by modifying the cnf encoding to reduce its size and make the structure more apparent to
the solver. Doing so involves the introduction of a significant number of new auxiliary variables, and experimental results for this new encoding are discussed below.
March eq does recognize the structure in our original encoding, and solves it during
a preprocessing phase. The exponential scaling here is due simply to the fact that the
size of the Boolean encoding is growing exponentially as a function of graph size (see
Section 10.2).
Any parity constraint can be rewritten as a set of parity constraints, each of length at
most three (Li, 2000). A parity constraint of the form
x1 + x2 + . . . + xn  k
is equivalent to the set of parity constraints
x1 + A1  k
A1 + x2 + A2  0
A2 + x3 + A3  0
..
.
An2 + xn1 + An1  0
An1 + xn  0
14. Li, personal communication (2005).

509

(31)

fiDixon, Ginsberg, Hofer, Luks & Parkes

Summing over this set of parity constraints gives
2A1 + 2A2 +    + 2An1 + x1 +    + xn  k
which is equivalent to (31). If the Tseitin encoding from Section 10.2 is translated into
parity constraints in this way and then converted to cnf, the exponential blowup in the
size of our existing cnf encoding can be avoided. (It is not clear, however, if resolution can
then produce a polynomially sized proof of the unsatisfiability of the resulting theory.)
Eqsatz, march eq and zap all exhibit improved performance if this new encoding is
used; these results are shown parenthetically in the Tseitin column of the table. March eq
solves this encoding of the Tseitin problems virtually instantaneously. Eqsatz now substantially outperforms zChaff, as reported by Li (2003). The running times for eqsatz,
however, remain exponential and the system is unable to solve the instance of size ten
within 10,000 seconds. The performance of zap is improved as well, but the overall scaling
is unchanged.
The introduction of new variables is accepted practice for reducing the size of cnf
encodings, and also has the potential to reduce the length of proofs constructed by solvers.
Indeed, there are no classes of problems known to be hard for extended resolution, a version
of resolution in which the introduction of new variables is permitted. In general, however,
introducing new variables in order to reduce proof length is considered cheating from a
proof complexity perspective; once new variables are introduced, most proof systems are
essentially equivalent. In addition, no general method for introducing variables is known and
we know of no implemented system that does so. One advantage of zap is that group-based
annotations avoid the need for syntactic reworkings of this sort.
Another approach to solving highly symmetric problems is seen in the solver sSatz (Li,
Jurkowiak, & Purdom, Jr., 2002). This solver is also based on the dpll algorithm, and
accepts as input both a problem in cnf and a set of matrices describing a global symmetry on
the variables. The global symmetry is then used to partition the set of variable assignments
into equivalence classes. In addition to the normal pruning techniques used in dpll, search
can now also be pruned by eliminating any partial assignment that is not minimal under the
equivalence corresponding to the global symmetry. sSatz scales polynomially on pigeonhole
problems; however, the class of input symmetry groups allowed by sSatz is currently too
limited to be applied to Tseitin or clique coloring problems. It is not clear whether this is
a limitation that can be overcome as the work matures, which is why we have not included
sSatz in our table.
Of all the solvers tested, zap is the only solver to provide efficient solutions on all the test
problems, and it is the only solver that scales polynomially on clique coloring. Pbchaff
has better scaling on pigeonhole problems, and march eq has better scaling on Tseitin
problems; however, both solvers exploit a narrowly defined type of problem structure and
therefore perform poorly in the other domains. The performance of zap is also likely to
improve as the basic group primitives underlying zaps procedures are optimized.

11. Conclusion and Future Work
Zap represents what appears to be a new synthesis between two very distant fields: computational group theory and Boolean satisfiability. From an algorithmic point of view, each of
510

fiZAP 3: Implementation

these fields is fairly mature and complex, and our synthesis inherits significant algorithmic
complexity as a result. Our goal in this paper has been to present initial versions of the
algorithms that a group-based theorem prover will need, and to describe the performance
of a prototype implementation of these ideas. As we have seen, zap easily outperforms
its conventional counterparts on difficult problem instances where there is group structure
concealed by the Boolean axiomatization.
That said, it is important to realize that our results only scratch the surface of what
zaps underlying representational shift allows. On the Tseitin problems, for example, it
seems likely that incorporation of more sophisticated set stabilizer algorithms will allow us
to improve zaps performance; the fact that only polynomially many nodes are expanded
in solving these problems bodes well for the eventual performance of the system.
Other improvements are also possible. In the pigeonhole and clique coloring problems,
computational performance is dominated by the O(n5 ) stabilizer chain computations on the
groups in question; these groups are products of full symmetry groups. It is well known
that full symmetry groups are extremely difficult for the usual stabilizer chain algorithms,
but in cases such as these it is possible to produce the stabilizer chains directly, taking time
O(n3 ) or even O(n2 ) if the stabilizer chain data structure is modified (Jerrum, 1986). Such
modifications can be expected to improve zaps performance significantly in this domain.
There is simply too much to do. The above extensions are only the beginning; we also
obviously need to experiment with zap on a wide range of other problem instances. There
are also two general points that we would like to make regarding future work in this area.
First, we have left unmentioned the problem of discovering group structure in existing
clausal databases. The practical impact here would be substantial, for several reasons. It
would make it possible to apply zap directly to problems that have already been encoded
using Boolean axioms, and it would also make it possible to discover emergent group
structure that only appears after search has begun. As an example, perhaps a symmetry
exists for a particular problem but is hidden by the existing axiomatization; after a few
inferences, the symmetry may become apparent but still needs to be noticed.
Second, and perhaps most important, zap provides us with a very broad stage on which
to work. Progress in computational group theory can be expected to lead to performance
improvements in inference; dually, applying zap to a wide range of reasoning problems
should provide a new set of examples that the computational group theorists can use to
test their ideas. Lifting heuristics from one area of AI to a group-based setting may make
analogs of those heuristics available in other, more practical domains. As with all new
syntheses, it seems reasonable to hope that zap will allow ideas from Boolean satisfiability,
computational group theory and search-based AI to be combined, leading to new insights
and levels of performance in all of these areas.

Acknowledgments
We would like to thank the members of cirl and the technical staff of On Time Systems
for their assistance with the ideas in this series of papers. We would also like to thank the
implementers and maintainers of gap; many elements of the zap implementation are based
directly on either the implementations that appear in gap or the descriptions in Seress
book (2003). Finally, we would especially like to thank the anonymous reviewers of all of

511

fiDixon, Ginsberg, Hofer, Luks & Parkes

the zap papers for the care and effort they put into reviewing a series of papers that span
some 200 journal pages in their entirety. All three of the papers were substantially improved
through their efforts.
This work was sponsored in part by grants from the Air Force Office of Scientific Research (afosr) number F49620-92-J-0384, the Air Force Research Laboratory (afrl) number F30602-97-0294, Small Business Technology Transfer Research, Advanced Technology
Institute (sttr-ati) number 20000766, the Office of Naval Research (onr) number N0001400-C-0233, the Defense Advanced Research Projects Agency (darpa) and the Air Force Research Laboratory, Rome, NY, under agreements numbered F30602-95-1-0023, F30602-971-0294, F30602-98-2-0181, F30602-00-2-0534, and F33615-02-C-4032, and by darpa under
agreement number HR0011-05-C-0039. The views expressed are those of the authors.

Appendix A. Proofs
Procedure 4.1 Given augmented clauses (c1 , G1 ) and (c2 , G2 ), to compute stab(ci , Gi ):
1
2
3
4
5
6
7
8
9

G2
1
c closure1  cG
1 , c closure2  c2
g restrict1  G1 |c closure1 , g restrict2  G2 |c closure2
C  c closure1  c closure2
g stab1  g restrict1{C } , g stab2  g restrict2{C }
g int  g stab1 |C  g stab2 |C
{gi }  {generators of g int}
{l1i }  {gi , lifted to g stab1 }, {l2i }  {gi , lifted to g stab2 }
0 }  {l |
{l2i
2i c closure2 C }
0 }i
return hg restrict1C , g restrict2C , {l1i  l2i

Proposition 4.2 The result returned by Procedure 4.1 is stab(ci , Gi ).
Proof. We show that every element of the group returned is a stable extension by showing
that the generators in line 9 are all stable extensions; recall that the set of stable extensions
is a subgroup. We show that every stable extension is returned by showing that they can
all be constructed via the above procedure.
For the first claim, we argued in the main text that the elements of g restrictiC are
0 } are as well. For such an element
stable; we must only show that the elements of {l1i  l2i
, however, note that |cG1 = l1i |cG1 = gi and similarly for |cG2 , since  agrees with
1

1

2

0 is stable.
l1i = l2i = gi on C and with l2i outside of C . Thus l1i  l2i
For the second claim, suppose that we have a stable extension ; consider its restriction
G2
G1
G2
1
to cG
1  c2 . Now on the intersection c1  c2 ,  must agree with elements of both G1
and G2 ; call the elements with which it agrees l1 and l2 . Restricting l2 away from the
intersection to get l20 , we see that there will be some element l of the group generated by
0 } that matches  on cG1  cG2 .
{l1i  l2i
1
2
G2
G1
G2
1
Now consider   l1 . This is the identity on cG
1  c2 . Restricting to either c1 or c2
G2
1
we get an element of G1 or G2 that point stabilizes cG
1  c2 , and all such elements are
included directly in line 9 of the resolution procedure. It follows that   l1 is an element
of hg restrict1C , g restrict2C i, so that
0
  hg restrict1C , g restrict2C , {l1i  l2i
}i

512

fiZAP 3: Implementation

Procedure 5.3 Given groups H  G, an element t  G, sets c and S, to find a group
element g = map(G, H, t, c, S) with g  H and cgt  S = :
1
2
3
4
5
6
7
8
9
10

if ctH  S 6= 
then return failure
if c = cH
then return 1
  an element of c  cH
for each t0 in (H : H )
do r  map(G, H , t0 t, c, S)
if r 6= failure
then return rt0
return failure

Proposition 5.4 map(G, G, 1, c, S) returns an element g  G for which cg  S = , if such
an element exists, and returns failure otherwise.
Proof. As we remarked in the main text, we will prove the slightly stronger result that
map(G, H, t, c, S) returns an element g  H for which cgt  S =  if such an element exists.
The proposition as stated is then the special case t = 1.
The proof proceeds by induction on the number of elements of c that are moved by
H. If none are, then either ct  S 6=  and the procedure will return failure on line 2, or
ct  S =  and it will return 1 on line 4.
For the inductive step, assume that H moves at least one point in c. Lines 14 dont
affect the correctness of the procedure at this point, other than to allow an early termination
if some already fixed point is moved inside of S by t. In the interesting case, we form a
transversal at line 6. Every element of H can be represented as gt0 for some g  H and
t0 in the transversal. If some such gt0 should be returned as a solution, we know by the
inductive hypothesis that g will be found by the recursive call in line 7.
Procedure 5.5 Given groups H  G, an element t  G, sets c, S and U and an integer
k, to find a group element g = transport(G, H, t, c, S, U, k) with g  H, cgt  S =  and
|cgt  U |  k:
1
2
3
4
5
6
7
8
9
10
11
12

if ctH  S 6= 
then return failure
1
if overlap(H, c, (S  U )t ) > k
then return failure
if c = cH
then return 1
  an element of c  cH
for each t0 in (H : H )
do r  transport(G, H , t0 t, c, S, U, k)
if r 6= failure
then return rt0
return failure

513

fiDixon, Ginsberg, Hofer, Luks & Parkes

Proposition 5.6 Provided that |ch  V |  overlap(H, c, V )  |cH  V | for all h  H,
transport(G, c, S, U, k) as computed by Procedure 5.5 returns an element g  G for which
cg  S =  and |cg  U |  k, if such an element exists, and returns failure otherwise.
1
Proof. As remarked in the main text, |c  (S  U )t | = |ct  (S  U )|. But since ct  S is
required to be empty, |ct (S U )| = |ct U |. The proof now proceeds essentially unchanged
from that of Proposition 5.4.
The two conditions on the overlap function are both necessary. We need to know that
h
|c  V |  overlap(H, c, V ) in order to avoid terminating the search early on line 3. We
need overlap(H, c, V )  |cH  V | to ensure that once we have fixed every element of c,
line 3 will identify a failure if |ct  U | > k so that we dont return successfully on line 6 in
this case.
Procedure 5.8 Given a group H, and two sets c, V , to compute overlap(H, c, V ), a lower
bound on the overlap of ch and V for any h  H:
1
2
3
4

m0
for each orbit W of H
do m  m + max(|W  V |  |W  c|, 0)
return m

Proposition 5.9 Let H be a group and c, V sets acted on by H. Then for any h  H,
|ch  V |  overlap(H, c, V )  |cH  V | where overlap is computed by Procedure 5.8.
Proof. The only subtlety involves the contribution that the fixed points in the clause make
to the sum. But since each fixed point is in its own orbit, the fixed points contribute either
1 or 0 to the sum depending on whether or not they are already in V .
Proposition 5.15 Let G be a group acting transitively on a set T , and let c, V  T .
Suppose also that {B1 , . . . , Bk } is a block system for G and that c  Bi 6=  for n of the
blocks in {B1 , . . . , Bk }. Then if b is the size of an individual block Bi and g  G,
|cg  V |  |c| + min
in (Bi  V )  nb

(32)

Proof. For any g  G, there will be a set of n blocks that collectively contain the image cg .
We can therefore use the usual counting argument. Within those n blocks, c will contain
|c| points, and the set V will contain at least min
in (Bi  V ) points. But there are only nb
points available, so the result follows.
Proposition 5.16 If the block system is trivial (in either sense), (32) is equivalent to
|cg  V |  |T  V |  |T  c|

(33)

Proof. Suppose first that there is a single block. Now n = 1, b = |T | and there is only one
set over which to take the minimum in (32), which therefore becomes
|cg  V |  |c| + |T  V |  |T |
= |T  V |  |T  c|

514

fiZAP 3: Implementation

If, on the other hand, the block system is trivial in that each point is in its own block,
then n = |c|, b = 1 and
min
in (Bi  V )
is the smallest number of points in V that must be in a set of size n, so
min
in (Bi  V ) = n + |T  V |  |T |
Now (32) becomes
|cg  V |  |c| + |c| + |T  V |  |T |  |c|
= |c| + |T  V |  |T |
= |T  V |  |T  c|
Proposition 5.17 Let {B1 , . . . , Bk } be a block system for a group G acting transitively on
a set T . Then (32) is never weaker than (33).
Proof. Comparing (32) and (33), we see that we are trying to show that
|c| + min
in (Bi  V )  nb  |T  V |  |T  c|
= |c| + |T  V |  |T |
or
min
in (Bi  V )  nb  |T  V |  |T |
If there are q blocks in the block system, then this is equivalent to
min
min
in (Bi  V )  nb  iq (Bi  V )  bq

or
min
bq  nb  min
iq (Bi  V )  in (Bi  V )

(34)

But the lefthand side of (34) is the total amount of space in the q  b blocks not included in
min
in (Bi  V ), and the righthand side is the amount of space used by V within these q  b
blocks. Thus (34) follows and the result is proved.
Lemma A.1 Let G be a group of permutations, and c a set acted on by G. Suppose also
that S and U are sets acted on by G. Now if j  G{c} and g  G is any permutation in G,
then
|cg  S| = |cjg  S|
and
|cg  U | = |cjg  U |

Proof. This is immediate, since cj = c.

515

fiDixon, Ginsberg, Hofer, Luks & Parkes

Lemma A.2 Let G be a group of permutations, and c a set acted on by G. Suppose also
that S and U are sets acted on by G. Now if k  G{S,U } and g  G is any permutation in
G, then
|cg  S| = |cgk  S|
and
|cg  U | = |cgk  U |

Proof. It clearly suffices to show the result for S; U is equivalent. But
1

|cgk  S| = |cg  S k |
= |cg  S|
1

where S k = S because k is in the set stabilizer of S and therefore k 1 is as well (because
the set stabilizer of S is a group).
Proposition 5.23 Let G be a group of permutations, and c a set acted on by G. Suppose
also that S and U are sets acted on by G. Then for any instance I of the k-transporter
problem and any g G, either every element of G{c} gG{S,U } is a solution of I, or none is.
Proof. Combine lemmas A.1 and A.2.
Lemma A.3 Let G, J  Sym() where  is the (ordered) set {x1 , . . . , xn } and suppose
t  Sym() satisfies xtl = zl for 1  l  k where k  n. Suppose that we have fixed i with
i  k and set Z = J{xi ,...,xk } . Suppose finally that

 Z
t
x ,...,x
zi > min xi 1 i1
Then no h  Gx1 ,...,xk t is the first element of Jh.
Proof. We are given the existence of j  Zx1 ,...,xi1 such that zi > xjt
i . Consider any h = gt
with g  Gx1 ,...,xk . Since j  Z, j stabilizes the set {xi , . . . , xk }. Since g stabilizes every
jgt
jt
t
point in this set, it fixes both xi and xji . Thus xgt
i = xi and xi = xi , and
jt
jgt
t
xgt
i = xi = zi > xi = xi
jgt
On the other hand, for l < i, both g and j fix xl , so that xgt
l = xl . Since jgt thus precedes
gt, gt is not minimal in Jgt.
Lemma 5.26 Suppose that t is the permutation labeling some node Ht of a coset decomposition tree at depth k, so that xti = zi for i  k and H = Gx1 ,...,xk is the residual
group at


JM,x

,...,x

t

this level. Let M be the set of points moved by Gx1 ,...,xk . Now if zi > min xi 1 i1
for any i  k, then no g  Ht is the first element of JgK.
Proof. This is a direct consequence of Lemma A.3. Let  be a permutation in JM,x1 ,...,xi1 .
Since  fixes every point moved by Gx1 ,...,xk , and  also fixes x1 , . . . , xi1 , it follows that 
must only permute the remaining points xi , . . . , xk . Thus JM,x1 ,...,xi1  Zx1 ,...,xi1 where
Z is the set stabilizer in the statement of Lemma A.3, and therefore no g  T is the first
element of Jg. Since Jg  JgK, the result follows.
Procedure 6.5 Given a set X of pairs hl, gi and a group G, to compute complete(X, G),
where X |=G complete(X, G) and L(complete(X, G)) = L(X)G :
516

fiZAP 3: Implementation

1
2
3
4
5
6

Y 
for each hl, gi  X
do for each l0  lG  L(Y )
do select h  G such that lh = l0
Y  Y  hl0 , ghi
return Y

Proposition 6.6 X |=G complete(X, G) and L(complete(X, G)) = L(X)G .
Proof. X |=G complete(X, G) because every entry added to Y is clearly G-entailed by
X. L(complete(X, G)) = L(X)G because the entire image of L(X) under G is eventually
added.
Procedure 6.7 Given groups H  G, an element t  G, sets c, S and U , to find
Transport(G, H, t, c, S, U ), a skeletal set of unit consequences for (c, G) given P :
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

1

if overlap(H, c, S t ) > 0
then return hfalse, i
1
if overlap(H, c, (S  U )t ) > 1
then return hfalse, i
if c = cH
then if ct  U = 
then return htrue, 1i
else return hfalse, hct  U, 1ii
if a pruning lemma can be applied
then return hfalse, i
Y 
  an element of c  cH
for each t0 in (H : H )
do hu, V i  Transport(G, H , t0 t, c, S, U )
if u = true
then return htrue, V t0 i
else Y  Y  {hl, gt0 i|hl, gi  V }
return hfalse, Y i

Proposition 6.8 Assume that |ch  V |  overlap(H, c, V )  |cH  V | for all h  H, and
let Transport(G, c, S, U ) be computed by Procedure 6.7. Then if there is a g  G such that
cg  S = cg  U = , Transport(G, c, S, U ) = htrue, gi for such a g. If there is no such
g, Transport(G, c, S, U ) = hfalse, Zi, where Z is a skeletal set of unit consequences for
(c, G) given P .
Proof. Procedure 6.7 is identical to Procedure 5.27 with k = 1 except for the value returned. If there is a g with cg  S =  and cg  U =  as well, htrue, gi will be returned
on line 7, and this will cause htrue, gt0 i to be returned from the recursive call(s) on line 16
also.
If there is no g with cg  S = cg  U = , then the argument proceeds as usual by
induction on the number of points of c moved by H. If none, we know that the correct
answer is returned on line 8 for the usual reasons; it remains to consider the recursive case
517

fiDixon, Ginsberg, Hofer, Luks & Parkes

on line 18. We know that for every g such that cg is unit, we will accumulate a result from
that g 0 that is minimal in JgK where J = G{c} and K = G{S,U } as usual. We only need to
show that the set of hl, gi collected is indeed a skeletal set of unit consequences.
To see this, suppose that hl, gi is any annotated unit consequence. Then there is some
minimal jgk that will be accumulated when the set of pairs is accumulated on line 17, with
the associated literal l0 = cjgk  U . But since j  G{c} set stabilizes the clause c, cj = c
and l0 = cgk  U . Thus taking the given k  G{S,U } produces the given unit consequence
from the element of the proposed skeleton, and Y as returned by Procedure 6.7 is indeed a
skeletal set of unit consequences.
Proposition 6.9 Let (c, G) be an augmented clause corresponding to a cardinality constraint. Then for any sets S and U , Procedure 6.7 will expand at most a linear number of
nodes in finding a skeletal set of unit consequences of (c, G).
Proof. If the original cardinality constraint was
x1 +    + xm  n
then G will be Sym(X) where X is the set of xi and c will be
x1      xmn+1
We will first show that Leons pruning lemma 5.24 suffices to reduce the search to
quadratic size. The basic idea of this part of the proof is as follows.
Suppose that we are expanding a particular node, corresponding to the selection of
an image for a point xi in c. If the image of xi is selected to be in S, we can prune
immediately. If the image is selected to be in either U or X  S  U , the image will have to
be the smallest available point in the set in question for lexicographic reasons. In addition,
the original symmetry on the literals in c can be used to require that the literals that are
neither satisfied nor unvalued are selected in order during the expansion.
To make this argument formally, note first that J = G{c} = Sym(c)  Sym(X  c) and
K = G{S,U } = Sym(S)  Sym(U )  Sym(X  S  U ). We assume without loss of generality
that the points fixed in the coset decomposition tree are the xi in order for i  m  n + 1,
and will continue to denote the fixed image of xi at any particular search node by zi . We
denote by  the sequence of all zi for i less than the depth of the node in question, so  is
the fixed part of the image of the clause c. We also set l = |X  S  U |, the total number
of points that are valued but unsatisfied.
We can now prune any node for which:
1.   S 6= . These nodes can be pruned because the image of c meets the set S of
satisfied literals.
2. |  U| > 1. As above, these nodes will be pruned because the image of c includes two
or more unsatisfied literals.
3.  = hy1 , ..., yj , ui, where each yi  X  U  S, and u  U is not minimal in U.
Leons lemma 5.24 with k = l requires that u = zj+1  min(uKy1 ,...,yj ). But since all of
the yi are outside of U , Ky1 ,...,yj  Sym(U ) and uKy1 ,...,yj is all of U . Since u is assumed
nonminimal in U , the node can be pruned.
518

fiZAP 3: Implementation

4.   (X  S  U) = hy1 , . . . , yj i, where y1 , . . . , yj1 are the first j  1 elements
of X  U  S and are in order, but yj  X  U  S is not the next element of
X  U  S. An argument identical to that in the previous paragraph can be used, since
Ky1 ,...,yj1 includes the full symmetry group on the remaining elements of X  U  S.
It follows from this that the only unpruned nodes in the search are those for which either
 = hy1 , . . . , yk i for k  min(l, m  n + 1), or
 = hy1 , . . . , yj , u, yj+1 , . . . , yk i

(35)

for k  min(l, m  n), u the minimal element of U , and the yi the smallest elements of
X  U  S in order. We need k  l because there are only that many possible y values,
and k  m  n + 1 or k  m  n because that is the depth of the tree when the clause c
has been completely instantiated. There is a linear number of nodes of the first type but a
quadratic number of nodes of the second.
To reduce the total number of nodes being searched to linear, we repeat the argument
used in the discussion of the example following Lemma 5.26. There, we considered a node
where the image of x1 was z1 and that of x2 was z2 , with z1 > z2 . Here, we consider a
slightly more general case, where  = hz1 , . . . , zk1 , zk i, with all of the zi in sequence except
zk1 > zk .
In Lemma 5.26, Gx1 ,...,xk will be the full symmetry group on the remaining xi , so that
M = {xk+1 , . . . , xm }. We also have J = Sym(x1 , . . . , xmn+1 )  Sym(xmn+2 , . . . , xm ).
Now since k  m  n + 1, taking i = k  1 in the statement of the lemma gives us
JM,x1 ,...,xi1 = JM,x1 ,...,xk2  Sym(xk1 , xk )
As a result,
(x

k1
zk1 > xk1

xk )t

= xtk = zk

and the node can be pruned.
This fixes us position in the list to be at the point where it is in sequence among the
yi and thus reduces the number of search nodes to linear.
Proposition 7.8 Suppose that W is a watching set for C under P and l is a literal. Then:
1. W is a watching set for C under any backtrack point for P .
2. If C is settled by hP, li, then W is a watching set for C under hP, li.
3. If C is settled by hP, li, and |(W  {l})  C  U (PC )| > 1, then W  {l} is a
watching set for C under hP, li.
4. If l 6 W  C, then W is a watching set for C under hP, li.
0
Proof. None of these is hard. First, note that if P 0 is a backtrack point for P , then Pc
will be a subassignment of Pc , so a watching set for C under P will also be a watching set
for C under P 0 .
For the second claim, if C is settled by hP, li, there are two possibilities:

519

fiDixon, Ginsberg, Hofer, Luks & Parkes

1. If C is unsettled by P (so that the addition of l to P caused C to be settled), then
hP, liC is a subassignment of P (the subassignment will be proper if P 6= P ). Since
C is unsettled by P , PC = P . Thus U (hP, liC )  U (PC ), and W is still a watching
set.
2. If C is settled by P , then hP, liC = PC , and W is once again still a watching set.
The third claim follows from the second, since W  {l} is assumed to be a watching
set for C under P .
For the fourth claim, suppose that l 6 C and l 6 C. Now C  U (P ) = C  U (hP, li),
and W remains a watching set. If l  C, then C will be satisfied (and therefore settled)
after l is added to P . So W continues to be a watching set by virtue of the second claim.
In the remaining case, l  C and C  U is potentially smaller because l is removed
after l is adjoined to P . But this can only impact the intersection with W if l is itself in
W ; otherwise, W will still be a watching set. So W is still a watching set unless l is in
both C and W , which proves the final claim.
Proposition 7.10 Suppose that W is a watching set for (c, G) under P and l is a literal.
Then:
1. W is a watching set for (c, G) under any backtrack point for P .
2. If l 6 W  cG , then W is a watching set for (c, G) under hP, li.
3. If |(W  V )  cg  U (hP, li)| > 1 for every g  G such that cg is unsettled by hP, li,
then W  V is a watching set for (c, G) under hP, li.
4. If |(W  V )  cg  [U (hP, li)  (S(P )  S(P ))]| > 1 for every g  G, then W  V  {l}
is a watching set for (c, G) under hP, li.
Proof. We know that W is a watching set for every instance of (c, G) under P , and use
Proposition 7.8 to show that each of the above claims follows.
First, Proposition 7.8 states directly that W is a watching set for every instance of (c, G)
under a backtrack point for P .
Second, if l 6 W  cG , then for any g  G, l 6 W  cg . The second claim here thus
follows from the fourth claim in Proposition 7.8.
The remaining two claims are more interesting. For the third, suppose that cg is some
instance of (c, G). Now if cg is settled by hP, li, then we know that W will still be a watching
set for it under hP, li. Therefore W  V will also be a watching set for cg under hP, li. If cg
is unsettled by hP, li, the condition of this claim says that |(W  V )  cg  U (hP, li)| > 1,
so that W  V is a watching set for cg under hP, li. This completes the proof of the third
claim.
For the fourth and final claim, there are three cases.
1. If cg is unsettled by hP, li, note first that cg  S(P ) = , so that
(W  V )  cg  [U (hP, li)  (S(P )  S(P ))] = (W  V )  cg  U (hP, li)
and W  V is a watching set for cg under hP, li. Since l 6 U (hP, li),
(W  V )  cg  U (hP, li) = (W  V  {l})  cg  U (hP, li)
520

fiZAP 3: Implementation

and W  V  {l} is a watching set as well.
2. If cg is unit under hP, li, consider:
(a) If l 6 cg , then we know from the fourth claim of Proposition 7.8 that W is a
watching set for cg under hP, li. It follows that W  {l} must be as well, since
l 6 cg . Thus so is W  V  {l}.
(b) If l  cg , cg must be of the form
cg = x1      xk  l  u
for the new unit consequence u, where no xi  S(P ). Note also that l cannot
be in either U (hP, li) or S(P ). Thus
cg  [U (hP, li)  (S(P )  S(P ))] = {u}
in violation of the premise of the claim.
3. Finally, if cg is satisfied by hP, li, we know that W (and therefore W V ) is a watching
set for cg under hP, li; the trick is to show that we can remove l from W  V safely.
If l 6 cg , then we can obviously do so.
If l  cg , we need to show that the third claim of Proposition 7.8 can be applied, so
we need to show that
|(W  V  {l})  cg  U (Pcg )| > 1

(36)

Given the assumption that
|(W  V )  cg  [U (hP, li)  (S(P )  S(P ))]| > 1

(37)

note first that since l 6 U (hP, li)  (S(P )  S(P )), l is not in the intersection of
(37), which is therefore equivalent to
|(W  V  {l})  cg  [U (hP, li)  (S(P )  S(P ))]| > 1
It follows that (36) will follow if we can show that
U (Pcg )  U (hP, li)  (S(P )  S(P ))

(38)

U (Pcg )  U (hP, li)

(39)

But
because Pcg is a (proper) subassignment of hP, li. And we also have
U (Pcg )  U (P )  S(P )  S(P )

(40)

The first inclusion holds because since l  cg and cg is satisfied by hP, li, cg must have
been satisfied by P as well. Thus Pc involves a backtrack from P , and since P is the
last backtrack point before P , Pcg is a subassignment of P and U (Pcg )  U (P ).
The second inclusion in (40) holds because the literals that are satisfied in P but not
in P must necessarily have been unvalued in P . Combining (39) and (40) gives us
(38), and the result is proved.
521

fiDixon, Ginsberg, Hofer, Luks & Parkes

Procedure 7.11 Given a group H, two sets c, V acted on by H, and a bound k  0, to
compute overlap(H, c, V, k), a collection of elements of V sufficient to guarantee that for
any h  H, |ch  V | > k, or  if no such collection exists:
1 m0
2 W 
3 for each orbit X of H
4
do {B1 , . . . , Bk }  a minimal block system for W under H for which
c  W  Bi for some i
5
 = |c  X| + min(Bi  V )  |B1 |
6
if  > 0
7
then m  m + 
8
W  W  (X  V )
9
if m > k
10
then return W
11 return 
Proposition 7.12 Procedure 7.11 returns a nonempty set W if and only if Procedure 5.19
returns a value in excess of k. In this case, |ch  W | > k for every h  H.
Proof. For the first claim, we examine the two procedures. It is clear that Procedure 7.11
returns as soon as Procedure 5.19 concludes that the minimum overlap is greater than k;
we need simply argue that W will be nonempty. But each increment to W in line 8 must
be nonempty, since if X  V = ,  will be zero on line 6.
For the second part, imagine replacing V in the procedure with the set W returned.
The computation will be unchanged at every step, so the conclusion follows.
Procedure 7.13 Given groups H  G, an element t  G, sets c, S and U , and optionally a watched element w, to find Transport(G, H, t, c, S, U, w), a skeletal set of unit
w-consequences for (c, G) given P :

522

fiZAP 3: Implementation

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

1

if w is supplied and wt 6 cH
then return hfalse, , i
1
V  overlap(H, c, S t , 0)
if V 6= 
then return hfalse, , i
1
V  overlap(H, c, (S  U )t , 1)
if V 6= 
then return hfalse, , V t i
if c = cH
then if ct  U = 
then return htrue, 1, i
else return hfalse, hct  U, 1i, i
if a pruning lemma can be applied
then return hfalse, , i
1
  an element of c  cH . If w is supplied and w 6 ctH , choose  so that wt  H .
Y 
W 
for each t0 in (H : H )
do hu, V, Xi  Transport(G, H , t0 t, c, S, U, w)
if u = true
then return htrue, V t0 , i
else W  W  X
Y  Y  {hl, gt0 i|hl, gi  V }
return hfalse, Y, W i

Proposition 7.14 Suppose that overlap(H, c, V, k) is computed using Procedure 7.11, or
otherwise satisfies the conclusion of Proposition 7.12. Then if there is a g  G such that
w  cg and cg  S = cg  U = , Transport(G, c, S, U, w) as computed by Procedure 7.13
returns htrue, g, i for such a g. If there is no such g, Procedure 7.13 returns hfalse, Z, W i,
where Z is a skeletal set of unit w-consequences of (c, G) given P , and W is such that
|W G{S,U,{w}}  ch  U | > 1 for every h  H such that w  ch and ch is unsettled by P .
Proof. The restriction to permutations g for which w  cg is enforced by the first two
lines of the procedure; note that if a contradiction is found on line 11, all of the points
in c will have been fixed, so w  cg for certain. Note that we can prune on this basis
without affecting the overall correctness of the procedure, since the pruning lemmas have
been restricted to the group K = G{S,U,{w}} , which leaves the watched literal w intact.
The only other difference between Procedure 7.14 and Procedure 6.7 involves the computation of the set W of watched literals. When this set is produced on line 8, we know
from Proposition 7.12 that the set W is sufficient to guarantee that |W  cht  U | > 1 for
every cht in the current residual search tree. We must therefore show than any h satisfying
the conditions in the proposition is covered by W G{S,U,{w}} . To see this, we consider every
point at which a node is pruned in the procedure, and show that all such points are covered
by the exclusions in the statement of the proposition:
1. A prune at line 2 will only occur if w 6 cht for any h  H.
523

fiDixon, Ginsberg, Hofer, Luks & Parkes

2. A prune at line 5 will only occur if cht  S 6=  for every h  H, so that cht is settled
by P .
3. If a pruning lemma is applied, it must be because an eventual solution g can be shown
not to be minimal in the usual double coset G{c} gG{S,U,{w}} . But in this case, the
watching set itself is operated on with G{S,U,{w}} in the statement of the proposition
itself.
Procedure 7.16 (Unit propagation) To compute Unit-Propagate(C, P, L) where C
is a set of watched augmented clauses, P is an annotated partial assignment, and L is a set
of pairs hl, ri of literals l and reasons r:
1 while L 6= 
2
do hl, ri  an element of L
3
L  L  hl, ri
4
P  hP, hl, rii
5
for each h(c, G), W i  C
6
do if l  W
7
then hr, H, V i  Transport(G, c, S(P ), U (P ), l)
8
if r = true
9
then li  the literal in cH with the highest index in P
10
return htrue, resolve((cH , G), ci )i
0
11
H  complete(H, G{S(P ),U (P ),{l}} )
12
for each h  H 0
13
do z  the literal in ch unassigned by P
14
if there is no hz, r0 i in L
15
then L  L  hz, ch i
16
W  W  (U (P )  V G{S(P ),U (P ),{l}} )
17
U  U (P )  (S(P )  S(P ))
18
if H =   transport(G, c, , W  U, 1, l) = failure
19
then W  W  {l}
20 return hfalse, P i
Proposition 7.17 Let P be an annotated partial assignment, and C a set of watched
augmented clauses, where for every h(c, G), W i  C, W is a watching set for (c, G) under P .
Let L be the set of unit consequences of clauses in C. If Unit-Propagate(C, P, L) returns
htrue, ci for an augmented clause c, then c is a nogood for P , and any modified watching
sets in C are still watching sets under P . Otherwise, the value returned is hfalse, P i and
the watching sets in C will all have been replaced with watching sets under P .
Proof. This is really just a matter of assembling the pieces. Procedure 7.16 is essentially
a loop through the literals in L, much like the original procedure 2.7. For each such literal
l, we find all the unit clauses that contain l by an appropriate call to Transport for each
clause where l is watched. If the Transport call reveals the presence of a contradiction, we
return the resolvent of the reasons for l and for l as usual. If no contradiction is found,
we adjust the partial assignment as in Procedure 2.7, add the new unit consequences to

524

fiZAP 3: Implementation

the list of what remains to be analyzed, and update the watching set in accordance with
Propositions 7.10 and 7.14.
The only remaining issue is the removal of l from the watching set W in line 19 of
Procedure 7.16. We do this precisely when the fourth claim in Proposition 7.10 can be
applied. Note that in the call to transport, we use U (P ) instead of U (hP, li), since l has
already been added to P in line 4. We also require that l be in the instance cg , since
otherwise the intersection with cg will obviously be unaffected by the removal of l.
Lemma 8.3 If c1  c2 are two nogoods for P , then c1 <P c2 .
Proof. This is immediate. As soon as the last literal in c2 is not in c1 (which must happen
eventually as literals are removed in Definition 8.2), the falsification depth of c2 will exceed
that of c1 .
Procedure 8.4 Suppose we are given two augmented clauses (, G) and (, H) that are
unit for a partial assignment P = hl1 , . . . , ln i, with l   and l  . To find a <P -minimal
l-resolvent of (, G) and (, H):
1
2
3
4
5
6
7
8
9
10
11
12
13

U  {l, l}
 literals you cant avoid
f  
f  
p  [(  )  U ]?P
while p > 0
do g  transport(G, , {lp , . . . , ln }  U, , 0, l)
h  transport(H, , {lp , . . . , ln }  U, , 0, l)
if g = failure  h = failure
then U  U  {lp }
else f  g
f   h
p  [(f  f )  U ]?P
return resolve(f , f )

Proposition 8.5 Suppose that we are given two augmented clauses (, G) and (, H) such
that  and  are unit for a partial assignment P , with l   and l  . Then the value
returned by Procedure 8.4 is a <P -minimal l-resolvent of (, G) and (, H).
Proof. We need to show that the procedure terminates, that it returns an l-resolvent, and
that the result is <P -minimal.
To show that Procedure 8.4 terminates, we show that p is reduced on every iteration of
the main loop. At the beginning of each iteration, we know that
lp  (f  f )  U

(41)

At the end of the iteration, if line 9 is selected, then f and f are unchanged but lp is
added to U . This means that (41) will no longer be satisfied, but will still be satisfied for
li with i > p. Thus p is reduced on line 12.
If, on the other hand, lines 10 and 11 are selected, we know from the definition of g and
h on lines 6 and 7 that for any literal with l  {lp , . . . , ln }  U , we have l 6 (f  f ).
In other words, if l  {lp , . . . , ln }, then l 6 (f  f )  U . Thus p is once again
reduced, and the procedure therefore terminates. That it returns a resolvent is clear.
525

fiDixon, Ginsberg, Hofer, Luks & Parkes

To see that the value returned is <P -minimal, suppose that gm and hm are such that
gm   hm <P f  f . We will show that f and f cannot be the permutations returned
on line 13.
Set z = [(f  f )  (gm   hm )]?P ; this is the last point included in the f images
produced by the procedure but not in the m images provided by the hypothetical counterexample. Since gm   hm <P f  f , the two sets agree for literals with index greater
than z.
Since lz  (f  f ), the initial value for p set in line 4 will be at least z; since the
procedure terminates when p < 0, the final value will be less than z.
Consider the point in the procedure at which p changes from a value no less than z to one
that is less than z. If the change is because lp is added to U , then one of the transport calls
must have failed, so that it is impossible (say) for the image of  to avoid {lp , . . . , ln }U .
But we know that f avoids {lp+1 , . . . , ln }  U . Thus gm avoids {lp+1 , . . . , ln }  U ,
but we are assuming that lp 6 gm , so transport(Gl,l , , {lp , . . . , ln }  U, , 0) should
have succeeded after all.
It follows that the change in p must have been in lines 10 and 11. But this is also
impossible, since the fact that we have successfully managed to avoid lz contradicts the
assumption that z = [(f  f )  (gm   hm )]?P so that lz  f  f .

Appendix B. ZAP Problem Format
Historically, Boolean satisfiability problems are typically in a format where variables correspond to positive integers, literals are nonzero integers (negative integers are negated
literals), and clauses are terminated with zeroes. The dimacs format precedes the actual
clauses in the problem with a single line such as p cnf 220 1122 indicating that there are
220 variables appearing in 1,122 clauses in this problem.
This numerical format makes it impossible to exploit any existing understanding that
the user might have of the problem in question; this may not be a problem for a conventional
Boolean tool (since the problem structure will have been obscured by the Boolean encoding
in any event), but was felt to be inappropriate when building an augmented solver. We felt
that it was important for the user to be able to:
1. Specify numerical constraints such as appear in cardinality or parity constraints,
2. Quantify axioms over finite domains, and
3. Provide group augmentations explicitly if the above mechanisms were insufficient.
Before discussing the specific provisions zap makes in each of these areas, we remark
that each zap input file begins with a list of domain specifications, giving the names and
sizes of each domain used in the theory. This is followed by predicate specifications, giving
the arity of each predicate and the domain type of each argument. After the predicates
and domains have been defined, it is possible to refer to predicate instances directly (e.g.,
in[1 3] indicating that the first pigeon is in the third hole) or in a nonground fashion (e.g.,
in[x y]).

526

fiZAP 3: Implementation

Group definition When a group is specified directly, it is assigned a symbolic designator
that can then be used in an augmented clause. The group syntax is the conventional one,
with a group being described in terms of generators, each of which is a permutation. Each
permutation is a list of cycles, and each cycle is a space-separated list of literals.
An augmented clause that uses a previously defined group is of the form
clause

GROUP

group1



groupn

where the (ground) clause is essentially a sequence of literals and each groupi is the
designator for a group to be used. The group in the augmented clause is then the group
collectively generated by the groupi s.
As an example, here is the group-based encoding of the pigeonhole instance involving
four pigeons and three holes:
// domain specs
SORT pigeon 4 ;
SORT hole 3 ;
// predicate specs
PREDICATE in(pigeon hole) ;
// group specs
GROUP G < ((in[1
((in[1
(in[1
((in[1
(in[4
((in[1
(in[4

1]
1]
3]
1]
1]
1]
1]

in[2
in[3
in[3
in[1
in[4
in[1
in[4

1]) (in[1 2] in[2
1] in[4 1]) (in[1
3] in [4 3]))
2]) (in[2 1] in[2
2]))
3]) (in[2 1] in[2
3])) > ;

2]) (in[1 3] in[2 3]))
2] in[3 2] in [4 2])
// permute pigeons
2]) (in[3 1] in[3 2])
// permute holes
3]) (in[3 1] in[3 3])

// group-based encoding
-in[1 1] -in[2 1] GROUP G ;
in[1 1] in[1 2] in[1 3] GROUP G ;

There are two types of domain variables, pigeons (of which there are four) and holes (of
which there are three). There is a single predicate indicating that a given pigeon is in a
particular hole. There is a single group, which corresponds to symmetries over both holes
and pigeons.
To generate the group, we use four generators. The first two correspond to the symmetry
over pigeons, with the first generator swapping the first two pigeons and the second generator
rotating pigeons one, three and four. (Recall that the permutations (1, 2) and (1, 3, 4)
generate the symmetry group S4 over the pigeons.)
The second pair of generators generate the symmetry over holes similarly, with the first
generator swapping the first two holes and the second generator swapping holes one and
three. (Once again, (1, 2) and (1, 3) generate S3 .)
Turning to the axioms, the first says that the first hole cannot contain both of the first
two pigeons, and therefore that no hole can contain two distinct pigeons by virtue of the
group action. The second axiom says that the first pigeon has to be in some hole, and
therefore that every pigeon does.
527

fiDixon, Ginsberg, Hofer, Luks & Parkes

Cardinality and parity constraints If the group is not specified directly, the general
form of a zap axiom is
quantifiers clause result
where the quantifiers are described below. The result includes information about the
desired right hand side of the axiom, and can be any of the following:
 A simple terminator, indicating that the clause is Boolean,
 A comparison operator (>, <=, =, etc.) followed by an integer, indicating that the
clause is a cardinality constraint, or
 A mod-2 operator (%2=) followed by an integer m, indicating that the sum of the
values of the literals is required to be congruent to m mod 2.
Quantification

The quantifiers are of the form
(x1 , . . . , xk )

or
(x1 , . . . , xk )
where each of the xi are variables that can then appear in future predicate instances. In
addition to the two classical quantifiers above, we also introduce
(x1 , . . . , xk )
where the  quantifier means that the variables can take any values that do not cause any of
the quantified predicates instances to become identical. As an example, the axiom saying
that only one pigeon can be in each hole now becomes
(p1 , p2 , h) . in(p1 , h)  in(p2 , h)

(42)

Contrast this with the conventional
(p1 , p2 , h) . in(p1 , h)  in(p2 , h)

(43)

For any specific pigeon p and hole h,
in(p, h)  in(p, h)

(44)

is an instance of (43) but not of (42). Since (44) is equivalent to in(p, h), it is inappropriate
for inclusion in a pigeonhole formulation.
The introduction of the new quantifier should be understood in the light of the discussion
of Section 6.1 of zap2, where we argued that in many cases, the quantification given by 
is in fact more natural than that provided by . The  quantification is also far easier to
represent using augmented clauses, and avoids in many cases the need to introduce or to
reason about equality. In any event, zap supports both forms of universal quantification.
Here is the quantifier-based encoding of the pigeonhole problem:

528

fiZAP 3: Implementation

SORT pigeon 9;
SORT hole 8;
PREDICATE in(pigeon hole);
// quantification-based encoding
NOTEQ (x y z) -in[x z] -in[y z] ;
FORALL(z) EXISTS(h) in[z h] ;

This is the nine pigeon instance. The two axioms say directly that no hole z can contain
two distinct pigeons x and y (note the use of the NOTEQ or ), and every pigeon z has to be
in some hole h. This encoding is presumably more intuitive than the previous one.

References
Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). The Design and Analysis of Computer
Algorithms. Addison-Wesley.
Babai, L., & Moran, S. (1988). Arthur-Merlin games: A randomized proof system, and a
hierarchy of complexity classes. J. Comput. System Sci., 36, 254276.
Barth, P. (1995). A Davis-Putnam based enumeration algorithm for linear pseudoboolean optimization. Tech. rep. MPI-I-95-2-003, Max Planck Institut fur Informatik,
Saarbrucken, Germany.
Bayardo, R. J., & Miranker, D. P. (1996). A complexity analysis of space-bounded learning
algorithms for the constraint satisfaction problem. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 298304.
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques to solve real-world
SAT instances. In Proceedings of the Fourteenth National Conference on Artificial
Intelligence, pp. 203208.
Dixon, H. E., & Ginsberg, M. L. (2000). Combining satisfiability techniques from AI and
OR. Knowledge Engrg. Rev., 15, 3145.
Dixon, H. E., Ginsberg, M. L., Luks, E. M., & Parkes, A. J. (2004a). Generalizing Boolean
satisfiability II: Theory. Journal of Artificial Intelligence Research, 22, 481534.
Dixon, H. E., Ginsberg, M. L., & Parkes, A. J. (2004b). Generalizing Boolean satisfiability
I: Background and survey of existing work. Journal of Artificial Intelligence Research,
21, 193243.
Furst, M., Hopcroft, J., & Luks, E. (1980). Polynomial time algorithsm for permutation
groups. In Proceedings 21st Annual IEEE Symp. on Foundations of Computer Science
(FOCS-80), pp. 3641. IEEE.
GAP Group (2004).
GAP  Groups, Algorithms and Programming, Version 4.4.
http://www.gap-system.org.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal of Artificial Intelligence Research,
1, 2546.
Ginsberg, M. L., & Parkes, A. J. (2000). Search, subsearch and QPROP. In Proceedings of
the Seventh International Conference on Principles of Knowledge Representation and
Reasoning, Breckenridge, Colorado.
529

fiDixon, Ginsberg, Hofer, Luks & Parkes

Gomes, C. P., & Selman, B. (1997). Problem structure in the presence of perturbations.
In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp.
221226, Providence, RI.
Haken, A. (1985). The intractability of resolution. Theoretical Computer Science, 39, 297
308.
Harrison, M. A. (1989). Introduction to Switching and Automata Theory. McGraw-Hill.
Heule, M., & van Maaren, H. (2004). Aligning CNF- and equivalence-reasoning. In The
Seventh International Conference on Theory and Applications of Satisfiability Testing,
pp. 174180. Also available as http://www.satisfiability.org/SAT04/programme/72.pdf.
Hooker, J. N. (1988). Generalized resolution and cutting planes. Annals of Operations
Research, 12, 217239.
Jerrum, M. (1986). A compact representation for permutation groups. J. Algorithms, 7,
6078.
Knuth, D. E. (1991). Notes on efficient representation of permutation groups. Combinatorica, 11, 5768.
Leon, J. (1991). Permutation group algorithms based on partitions I: Theory and algorithms.
J. Symbolic Comput., 12, 533583.
Li, C. M. (2000). Integrating equivalency reasoning into Davis-Putnam procedure. In
Proceedings of the Seventeenth National Conference on Artificial Intelligence, pp. 291
296.
Li, C. M. (2003). Equivalent literal propagation in Davis-Putnam procedure. Discrete App.
Math., 130 (2), 251276.
Li, C. M., Jurkowiak, B., & Purdom, Jr., P. W. (2002). Integrating symmetry breaking into
a DLL procedure. In Fifth International Symposium on the Theory and Applications
of Satisfiability Testing (SAT2002), pp. 149155.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
an efficient SAT solver. In 39th Design Automation Conference.
Pudlak, P. (1997). Lower bounds for resolution and cutting planes proofs and monotone
computations. J. Symbolic Logic, 62 (3), 981998.
Rotman, J. J. (1994). An Introduction to the Theory of Groups. Springer.
Schaefer, T. J. (1978). The complexity of satisfiability problems. In Proceedings of the
Tenth Annual ACM Symposium on the Theory of Computing, pp. 216226.
Seress, A. (2003). Permutation Group Algorithms, Vol. 152 of Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, UK.
Sims, C. C. (1970). Computational methods in the study of permutation groups. In Leech, J.
(Ed.), Computational Problems in Abstract Algebra, Proc. Conf. Oxford, 1967. Pergamon Press.
Tseitin, G. (1970). On the complexity of derivation in propositional calculus. In Slisenko,
A. (Ed.), Studies in Constructive Mathematics and Mathematical Logic, Part 2, pp.
466483. Consultants Bureau.
530

fiZAP 3: Implementation

Urquhart, A. (1987). Hard examples for resolution. JACM, 34, 209219.
Zhang, H., & Stickel, M. E. (2000). Implementing the Davis-Putnam method. Journal of
Automated Reasoning, 24 (1/2), 277296.

531

fiJournal of Artificial Intelligence Research 23 (2005) 667-726

Submitted 07/04; published 06/05

Keys, Nominals, and Concrete Domains
Carsten Lutz

lutz@tcs.inf.tu-dresden.de

Theoretical Computer Science, TU Dresden
D-01062 Dresden, Germany

Carlos Areces

areces@loria.fr

INRIA Lorraine, Nancy
54602 Villers les Nancy Cedex, France

Ian Horrocks

horrocks@cs.man.ac.uk

Department of Computer Science
University of Manchester
Oxford Road, Manchester M13 9PL, UK

Ulrike Sattler

sattler@cs.man.ac.uk

Department of Computer Science
University of Manchester
Oxford Road, Manchester M13 9PL, UK

Abstract
Many description logics (DLs) combine knowledge representation on an abstract, logical
level with an interface to concrete domains like numbers and strings with built-in predicates such as <, +, and prefix-of. These hybrid DLs have turned out to be useful in several
application areas, such as reasoning about conceptual database models. We propose to
further extend such DLs with key constraints that allow the expression of statements like
US citizens are uniquely identified by their social security number. Based on this idea,
we introduce a number of natural description logics and perform a detailed analysis of
their decidability and computational complexity. It turns out that naive extensions with
key constraints easily lead to undecidability, whereas more careful extensions yield NExpTime-complete DLs for a variety of useful concrete domains.

1. Motivation
Description logics (DLs) are a family of formalisms that allow the representation of and
reasoning about conceptual knowledge in a structured and semantically well-understood
manner (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). The central
entities for representing such knowledge are concepts, which are constructed from atomic
concept names (unary predicates) and role names (binary relations) by means of the concept
and role constructors offered by a particular DL. For example, in the basic propositionally
closed description logic ALC (Schmidt-Schau & Smolka, 1991), we can describe a company
that has part-time employees but only full-time managers using the concept
Company u employee.Parttime u employee.(Manager t Parttime).
In this example, all words beginning with uppercase letters denote concept names while
employee denotes a role name.
c
2005
AI Access Foundation. All rights reserved.

fiLutz, Areces, Horrocks, & Sattler

Rather than being viewed only as conceptual entities in a knowledge base, concepts
can, more generally, be understood as the central notion in various kinds of class-centered
formalisms. In the last decade, this observation has given rise to various new and challenging applications of description logics such as reasoning about database conceptual models
expressed in entity-relationship diagrams or object-oriented schemas (Calvanese, Lenzerini,
& Nardi, 1998; Calvanese, De Giacomo, & Lenzerini, 1998) and reasoning about ontologies
for use in the semantic web (Baader, Horrocks, & Sattler, 2002a; Horrocks, 2002; Horrocks,
Patel-Schneider, & van Harmelen, 2002). These new applications have, in turn, stimulated
research in description logics since the expressive power of existing DLs was insufficient
for the new tasks. One important extension is by providing expressive means that allow
the integration of numbers and other datatypes: suppose, for example, that we want to
extend our earlier descriptions of companies and employees to include the founding year of
a company and the hiring year of an employee. Then, we may want to describe companies
that were founded before 1970 and state that the hiring year of employees is not prior to
the founding year of the employing company. To do this, we obviously need a way to talk
about natural numbers (such as 1970) and comparisons between natural numbers.
Nowadays, the standard approach to integrate datatypes into description logics is to
extend DLs with concrete domains, as first proposed by Baader and Hanschke (1991a) and
recently surveyed by Lutz (2003). More precisely, a concrete domain D consists of a set
(such as the natural numbers) and predicates which are associated with a fixed extension
over this set1 (such as the unary =0 , the binary <, and the ternary +). The integration of
concrete domains into, say, the description logic ALC is achieved by adding
1. abstract features, i.e. functional roles;
2. concrete features, i.e. (partial) functions associating values from the concrete domain
(e.g., natural numbers) with logical objects;
3. a concrete domain-based concept constructor.
The DL obtained by extending ALC in this way is called ALC(D), where D denotes a
concrete domain that can be viewed as a parameter to the logic. For example, using a
suitable concrete domain D we can describe the constraints formulated above: the concept
Employee u employer.foundingyear.<1970 u hiringyear, (employer foundingyear).
describes the set of employees who are employed by a company founded before 1970 and
who have a hiring year not prior to the companys founding year. In this example, the term
foundingyear.<1970  is an instance of the concrete domain concept constructor (not to be
confused with the existential value restriction as in employee.Parttime), and so is the third
conjunct. While <1970 is a unary predicate and thus the former instance only takes one
concrete feature foundingyear as argument, the second instance uses the binary predicate
 requiring two arguments: the concrete feature hiringyear and the sequence of features
(employer foundingyear) consisting of the abstract feature employer and the concrete feature
foundingyear.
Concrete domains are rather important in many applications of DLs, including the two
mentioned above:
1. This fixed extension is why these predicates are often called built-in.

668

fiKeys, Nominals and Concrete Domains

 The standard way of using description logics for reasoning about conceptual database
models is to translate a given model into a DL representation and then use a DL reasoner such as FaCT (Horrocks, 1998) or RACER (Haarslev & Moller, 2001) to compute
the consequences of the information provided explicitly in the model. This includes
detecting inconsistencies and inferring additional, implicit containments between entities/classes (Calvanese et al., 1998). Since most databases store concrete data
like numbers and strings, constraints concerning such data are usually part of the
conceptual model and should thus also be captured by the description logic used for
reasoning. Indeed, the above example concepts can be viewed as the DL encoding
of constraints from a database about companies and their employees. As discussed
by Lutz (2002c), description logics with concrete domains are well-suited for conceptual modeling applications involving concrete datatypes.
 So-called concrete datatypes play a prominent role in the construction of ontologies (Horrocks et al., 2002). Say, for example, that we want to construct an ontology
which can be used for describing car dealers web pages and web services. In such an
ontology, concrete datatypes such as prices, manufacturing years, and names of car
models will doubtlessly be very important. To formulate this ontology using a DL, we
need a way to represent these concrete datatypes. Consequently, almost all DLs that
have been proposed as an ontology language are equipped with some form of concrete
domain (Fensel, van Harmelen, Horrocks, McGuinness, & Patel-Schneider, 2001; Horrocks et al., 2002; Dean, Connolly, van Harmelen, Hendler, Horrocks, McGuinness,
Patel-Schneider, & Stein, 2002). Furthermore, since these ontology languages provide
inverse abstract roles and functional restrictions, the users and ontology designers
were quite surprised to find that they do not provide inverse concrete functional
featureswhich is due to the fact that such features correspond to concrete key
constraints, for which no reasoning algorithms were known and whose effect on the
decidability/complexity was not yet investigated.
In this paper, we propose to further enhance the expressive power of description logics with
concrete domains by extending them with concrete key constraints. This extension is
useful both for knowledge representation and for the two applications sketched above. The
following three examples describe the basic idea.
1. Suppose that, in a knowledge representation application, we represent nationalities
by concept names such as US and German and, for US citizens, we store the social
security number using a concrete feature ssn. Then it would be natural to state that
US citizens are uniquely identified by their social security number, i.e. any two distinct
instances of
Human u nationality.US
must have different values for the ssn feature. In our extension of DLs with concrete
domains, this can be expressed by using the key assertion 2
(ssn keyfor Human u nationality.US).
2. Readers familiar with the
Vrelationship between DLs and first order logic will notice that this key assertion
is equivalent to x1 x2 .(( i{1,2} (Human(xi )z.(nationality(xi , z)US(z)))(x1 = x2 ))  (ssn(x1 ) =
ssn(x2 ))).

669

fiLutz, Areces, Horrocks, & Sattler

2. Returning to our database about companies and employees, it could be useful to
equip every employee with (i) a concrete feature branch storing the branch-ID in
which she is working and (ii) a concrete feature id storing her personnel-ID. It would
then be natural to enforce that the branch-ID together with the personnel-ID uniquely
identifies employees, even though personnel-IDs are not unique. We can do this by
using the composite key assertion
(branch, id keyfor Employee).
3. In the car dealers ontology, we may assume that cars as well as manufacturers are
equipped with identification numbers and that every car is uniquely identified by the
combination of its own identification number and its manufacturers one. To express
this, we could employ a composite key assertion referring to sequences of features, in
this case (manufacturer id):
(id, (manufacturer id) keyfor Car).
More formally, we propose to extend DLs to provide for concrete domains with key boxes,
which are sets of key assertions of the form
(u1 , . . . , un keyfor C),
where each ui is a sequence f1    fn g of abstract features fj followed by a single concrete
feature g, and C is a concept. As the above examples illustrate, the idea of key constraints is
very natural. Since, moreover, keys play an important role in databases and, as mentioned
above, reasoning about database conceptual models is an important, challenging application
of description logics, several approaches to extend description logics with keys have already
been investigated (Borgida & Weddell, 1997; Calvanese, De Giacomo, & Lenzerini, 2000;
Khizder, Toman, & Weddell, 2001). What distinguishes our approach from existing ones,
however, is the idea of using concrete domains for constructing key constraints, rather than
defining keys on an abstract, logical level.
The goal of this paper is to provide a comprehensive analysis of the effects on decidability and computational complexity of adding key boxes to description logics with concrete
domains. To this end, we extend the two description logics ALC(D) and SHOQ(D) with key
boxes, in this way obtaining ALCK(D) and SHOQK(D), respectively. While the basic DL
with concrete domains ALC(D) has already been discussed above, SHOQ(D) was proposed
as an ontology language in (Horrocks & Sattler, 2001). It provides a wealth of expressive
possibilities such as general concept inclusion axioms (GCIs), transitive roles, role hierarchies, nominals, and qualifying number restrictions. Moreover, it offers a restricted variant
of the concrete domain constructor that disallows the use of sequences of features in order
to avoid undecidability of reasoning. The main outcome of our investigations is that key
constraints can have a dramatic impact on the decidability and complexity of reasoning: for
example, whereas satisfiability of ALC(D)-concepts is known to be PSpace-complete (Lutz,
2002b), we will show that satisfiability of ALCK(D)-concepts w.r.t. key boxes is, in general,
undecidable. Decidability can be regained if we restrict the concepts used in key boxes
670

fiKeys, Nominals and Concrete Domains

to Boolean combinations of concept names (Boolean key boxes). Interestingly, satisfiability of ALCK(D)-concepts w.r.t. Boolean key boxes is still NExpTime-complete even for
very simple concrete domains. In the case of SHOQ(D) and SHOQK(D), the leap in
complexity is somewhat less dramatic since SHOQ(D)-concept satisfiability is already ExpTime-complete: again, the addition of key boxes results in NExpTime-complete reasoning
problems.
It is interesting to note that there exists a close connection between key assertions
and so-called nominals, i.e. concept names that can have at most one instance, such as
Pope. Nominals are a standard means of expressivity in description logics and sometimes
appear in disguise as the one-of operator (Borgida & Patel-Schneider, 1994; Horrocks
et al., 2002). It is not hard to see that key boxes can simulate nominals: if, for example,
we use a concrete domain based on the natural numbers and providing unary predicates
=n for equality with n  , then the key assertion (g keyfor >), where > stands for
logical truth, obviously makes the concept g.=n behave like a nominal, for each n 
. For this reason, we also consider ALCO(D), the extension of ALC(D) with nominals,
and ALCOK(D), the extension of ALCK(D) with nominals.3 Our main result concerning
nominals is that, although in general being of lower expressive power than key boxes, they
already make reasoning NExpTime-hard if combined with concrete domains: there exist
concrete domains D such that ALCO(D)-concept satisfiability is NExpTime-complete. We
should like to stress that this and the other NExpTime-hardness results obtained in this
paper are in accordance with the observation made in (Lutz, 2004) that the PSpace-upper
bound for reasoning with ALC(D) is not robust w.r.t. extensions of the logic: there exist
several seemingly harmless extensions of ALC(D) (for example with acyclic TBoxes and
with inverse roles) which make the complexity of reasoning leap from PSpace-completeness
to NExpTime-completeness for many natural concrete domains.

N

N

The remainder of this paper is organized as follows: in Section 2, we formally introduce
concrete domains, key boxes, and the DL ALCOK(D) together with its fragments ALCK(D)
and ALCO(D). Moreover, we define Boolean key boxes, which only allow Boolean combinations of concept names to appear in key definitions. Additionally, we introduce some
other important properties of key boxes: path-free key boxes prohibit the use of sequences
of features in key assertions; in unary key boxes, each key assertion involves exactly one
sequence of features; and composite key boxes are simply non-unary ones.
Section 3 is devoted to establishing lower bounds for extensions of ALC(D) with key
boxes or nominals. In Section 3.1, we use a reduction of the Post Correspondence Problem
to prove that ALCK(D)-concept satisfiability w.r.t. (non-Boolean) key boxes is undecidable
for a large class of concrete domains. We then shift our attention to Boolean key boxes
since, in Section 4, we show that this restriction restores decidability. In Section 3.2,
we introduce a NExpTime-complete variant of the domino problem and three concrete
domains that are useful for the reduction of this problem to concept satisfiability in DLs
with Boolean key boxes or nominals. In Section 3.3, we use these concrete domains to
prove that ALCK(D)-concept satisfiability w.r.t. Boolean, path-free and unary key boxes
is NExpTime-hard for some natural concrete domains. In Section 3.4, we prove that there
exist concrete domains D such that ALCO(D)-concept satisfiability without reference to
3. Note that the logic SHOQ(D) already provides for nominals.

671

fiLutz, Areces, Horrocks, & Sattler

key boxes is already NExpTime-hard; we show that this is true even for some concrete
domains that are computationally very simple (PTime) when considered in isolation.
The purpose of Section 4 is to develop reasoning procedures for description logics with
key boxes and to prove upper complexity bounds matching the NExpTime lower bounds
established in the previous section. We start in Section 4.1 with a tableau algorithm that
decides ALCOK(D)-concept satisfiability w.r.t. Boolean key boxes, provided that the concrete domain D is key-admissible. Intuitively, a concrete domain D is key-admissible if there
exists an algorithm that takes a finite conjunction c of predicates from D over some set of
variables, decides whether this conjunction is satisfiable and, if so, chooses a solution of c
and returns the information on which variables take the same values in it. We call such
an algorithm a D-tester. We have chosen a tableau algorithm since this type of reasoning
procedure has the potential to be implemented in efficient reasoners and has been shown to
behave well in practice (Horrocks, Sattler, & Tobies, 2000; Haarslev & Moller, 2001). Our
algorithm implies the following upper complexity bound: if D is a key-admissible concrete
domain for which a non-deterministic polynomial time D-tester exists, then ALCO(D)concept satisfiability w.r.t. Boolean key boxes is in NExpTime.
In Section 4.2, we devise a tableau algorithm for SHOQK(D)-concept satisfiability
w.r.t. path-free key boxes which might involve non-Boolean concepts. For the decidability
of ALCOK(D), we restricted key boxes to Boolean ones. For SHOQK(D), such a restriction is not possible since SHOQ(D) provides TBoxes, and we can thus no longer distinguish
between Boolean and non-Boolean concepts. On the other hand, it follows from an undecidability proof by Baader and Hanschke (1992) that SHOQ(D) is undecidable if we allow
for sequences of features in concrete domain constructors. Thus we restrict key assertions
analogously to path-free ones, and show that this yields indeed a decidable logic. Its expressive power is orthogonal to the one of ALCOK(D), and our previous undecidability
results imply that the combination of ALCOK(D) and SHOQK(D) is undecidable. As a
by-product of the correctness proof of the algorithm, we obtain a bounded model property for SHOQK(D), which implies that SHOQK(D)-concept satisfiability w.r.t. path-free
key boxes is in NExpTime if D is a key-admissible concrete domain for which a nondeterministic polynomial time D-tester exists.
In Section 5, we summarize the results obtained and give an outlook to possible future
research.

2. Description Logics with Concrete Domains
In the following, we introduce the description logic ALCOK(D). Let us start by defining
concrete domains:
Definition 2.1 (Concrete Domain). A concrete domain D is a pair (D , D ), where D
is a set and D a set of predicate names. Each predicate name P  D is associated with
an arity n and an n-ary predicate P D  nD .
Based on concrete domains, we can now define ALCOK(D)-concepts and key boxes.
Definition 2.2 (ALCOK(D) Syntax). Let NC , NO , NR , NcF be pairwise disjoint and countably infinite sets of concept names, nominals, role names, and concrete features. Furthermore, we assume that NR contains a countably infinite subset NaF of abstract features. A
672

fiKeys, Nominals and Concrete Domains

path u is a composition f1    fn g of n abstract features f1 , . . . , fn (n  0) and a concrete
feature g. Let D be a concrete domain. The set of ALCOK(D)-concepts is the smallest set
such that
 every concept name and every nominal is a concept, and
 if C and D are concepts, R is a role name, g is a concrete feature, u1 , . . . , un are paths,
and P  D is a predicate of arity n, then the following expressions are also concepts:
C, C u D, C t D, R.C, R.C, u1 , . . . , un .P, and g.
A key assertion is an expression
(u1 , . . . , uk keyfor C),
where u1 , . . . , uk (k  1) are paths and C is a concept. A finite set of key assertions is
called a key box.
As usual, we use > as an abbreviation for an arbitrary propositional tautology,  as an
abbreviation for >, C  D as an abbreviation for C t D, and C  D as an abbreviation
for (C  D) u (D  C). Throughout this paper, we will also consider several fragments
of the description logic ALCOK(D). The DL ALCO(D) is obtained from ALCOK(D) by
admitting only empty key boxes. In particular, the set of ALCO(D)-concepts is just the set
of ALCOK(D)-concepts. Furthermore, by disallowing the use of nominals, we obtain the
fragment ALC(D) of ALCO(D) and ALCK(D) of ALCOK(D).
The description logic ALCOK(D) is equipped with a Tarski-style set-theoretic semantics.
Along with the semantics, we introduce the two standard inference problems: concept
satisfiability and concept subsumption.
Definition 2.3 (ALCOK(D) Semantics). An interpretation I is a pair (I , I ), where I is
a non-empty set, called the domain, and I is the interpretation function. The interpretation
function maps
 each concept name C to a subset C I of I ,
 each nominal N to a singleton subset N I of I ,
 each role name R to a subset RI of I  I ,
 each abstract feature f to a partial function f I from I to I , and
 each concrete feature g to a partial function g I from I to D .
673

fiLutz, Areces, Horrocks, & Sattler

If u = f1    fn g is a path, then uI (d) is defined as g I (fnI    (f1I (d))    ). The interpretation
function is extended to arbitrary concepts as follows:
(C)I := I \ C I
(C u D)I := C I  DI
(C t D)I := C I  DI
(R.C)I := {d  I | There is e  I with (d, e)  RI and e  C I }
(R.C)I := {d  I | For all e  I , if (d, e)  RI , then e  C I }
(u1 , . . . , un .P )I := {d  I | x1 , . . . , xn  D : uIi (d) = xi and (x1 , . . . , xn )  P D }
(g)I := {d  I | g I (d) undefined}.
Let I be an interpretation. Then I is a model of a concept C iff C I 6= . Moreover, I
satisfies a key assertion (u1 , . . . , un keyfor C) if, for any a, b  C I ,
uI1 (a) = uI1 (b), . . . , uIn (a) = uIn (b) implies that a = b.
I is a model of a key box K iff I satisfies all key assertions in K. A concept C is satisfiable
w.r.t. a key box K iff C and K have a common model. C is subsumed by a concept D w.r.t.
a key box K (written C vK D) iff C I  DI for all models I of K.
It is well-known that, in description logics providing for all Boolean operators, subsumption
can be reduced to (un)satisfiability and vice versa: C vK D iff C u D is unsatisfiable
w.r.t. K and C is satisfiable w.r.t. K iff C 6vK . This allows us to concentrate on concept
satisfiability when devising complexity bounds for reasoning with description logics: lower
and upper complexity bounds for concept satisfiability imply corresponding bounds for
concept subsumptiononly for the complementary complexity class.
If decision procedures for description logics with concrete domains are to be devised
without committing to a particular concrete domain, then a well-defined interface between
the decision procedure and a concrete domain reasoner is needed. Usually, this interface
is based on the assumption that the concrete domain is admissible (Baader & Hanschke,
1991a; Lutz, 2002a, 2003):
Definition 2.4 (D-conjunction, Admissibility). Let D be a concrete domain and V a set
of variables. A D-conjunction is a (finite) predicate conjunction of the form
^ (i)
c=
(x0 , . . . , x(i)
ni ) : Pi ,
i<k
(i)

where Pi is an ni -ary predicate for i < k and the xj are variables from V. A D-conjunction
c is satisfiable iff there exists a function  mapping the variables in c to elements of D such
(i)
(i)
that ((x0 ), . . . , (xni ))  PiD for each i < k. Such a function is called a solution for c.
We say that the concrete domain D is admissible iff
1. D contains a unary predicate >D such that >D
D = D ;
2. D is closed under negation, i.e., for each n-ary predicate P  D , there is a predicate
D
P  D of arity n such that P = nD \ P D ;
674

fiKeys, Nominals and Concrete Domains

3. satisfiability of D-conjunctions is decidable.
We refer to the satisfiability of D-conjunctions as D-satisfiability.
As we shall see, it sometimes makes a considerable difference w.r.t. complexity and decidability to restrict key boxes in various ways, for example to disallow paths of length greater
than one. Therefore, we introduce some useful notions.
Definition 2.5 (Boolean, Path-free, Simple). A key box K is called
 Boolean if all concepts appearing in (key assertions in) K are Boolean combinations
of concept names;
 path-free if, for all key assertions (u1 , . . . , un keyfor C) in K, u1 , . . . , un  NcF ;
 simple if it is both path-free and Boolean;
 unary if all key assertions in K are unary key assertions, i.e. of the form (u keyfor C).
A concept C is called path-free if, in all its subconcepts of the form u1 , . . . , un .P , u1 , . . . , un
are concrete features.
To emphasize that a key box might not necessarily be Boolean or path-free, we sometimes
call such a key box general. Similarly, to emphasize that a key box is not necessarily a
unary key box, we sometimes call such a key box composite.

3. Lower Bounds
In this section, we prove lower complexity bounds for description logics with concrete domains and key boxes and/or nominals. In Section 3.1, we start by showing that the satisfiability of ALCK(D)-concepts w.r.t. (general) key boxes is undecidable for many interesting
concrete domains. The discouraging picture painted by this result is mitigated by the fact
that, in Section 4.1, we shall prove that the restriction to Boolean key boxes restores decidability. It is thus interesting to look for lower complexity bounds that apply under this
restriction. In preparation for this, we introduce in Section 3.2 a NExpTime-complete variant of the domino problem and three concrete domains that are well-suited for reductions
of this problem.
In Section 3.3, we then prove that satisfiability of path-free ALCK(D)-concepts w.r.t.
simple key boxes is NExpTime-hard for a large class of concrete domains D and that, for
many concrete domains, this holds even if we restrict key boxes to unary ones. Finally,
we consider the description logic ALCO(D) in Section 3.4 and identify several concrete
domains such that ALCO(D)-concept satisfiability (without key boxes!) is NExpTimehard. As we already mentioned, key boxes and nominals are closely related: key boxes can
express nominals, but are in general more powerful.
3.1 Undecidability of ALCK(D) with General Key Boxes
We prove that satisfiability of ALCK(D)-concepts w.r.t. key boxes is undecidable for a
large class of concrete domains if we allow complex ALCK(D)-concepts to occur in key
assertions. The proof is by a reduction of the well-known undecidable Post Correspondence
Problem (Post, 1946; Hopcroft & Ullman, 1979).
675

fiLutz, Areces, Horrocks, & Sattler

Definition 3.1 (PCP). An instance P of the Post Correspondence Problem (PCP) is given
by a finite, non-empty list (`1 , r1 ), . . . , (`k , rk ) of pairs of words over some alphabet . A
sequence of integers i1 , . . . , im , with m  1, is called a solution for P if `i1    `im = ri1    rim .
The PCP is to decide whether a given instance P has a solution.
For reducing the PCP to the satisfiability of our DLs, we need an appropriate concrete
domain. It is obviously natural to use a concrete domain based on words and concatenation.
We will later see that the results obtained for this concrete domain carry over to other
concrete domains based on numbers and arithmetics. The following concrete domain was
introduced by Lutz (2004). Its definition presupposes a fixed alphabet  that is at least
binary.
Definition 3.2 (Concrete domain W). The concrete domain W is defined by setting W :=
 and defining W as the smallest set containing the following predicates:
 unary predicates word and nword with wordW = W and nwordW = ,
W
+
 unary predicates = and 6= with =W
 = {} and 6= =  ,

 a binary equality predicate = and a binary inequality predicate 6= with the obvious
interpretation, and
 for each w  + , two binary predicates concw and nconcw with
W
concW
w = {(u, v) | v = uw} and nconcw = {(u, v) | v 6= uw}.

It is readily checked that W satisfies properties 1 and 2 of admissibility (see Definition 2.4).
Moreover, W-satisfiability is decidable:
Theorem 3.1 (Lutz, 2004). W-satisfiability is in PTime.
Thus, W is admissible and even of low complexity. This is important since our aim is
to demonstrate that the undecidability of ALCK(W)-concept satisfiability is due to the
presence of keys, and not due to the high complexity of W-satisfiability.
We can now discuss the reduction of the PCP. A given instance (`1 , r1 ), . . . , (`k , rk ) is
translated into an ALCK(D)-concept CP and key box KP as defined in Figure 1 such that
P has a solution iff CP is unsatisfiable w.r.t. KP . The idea behind the reduction is that a
common model of CP and KP encodes all potential solutions for P (i.e., sequences i1 , . . . , in
of integers ij between 1 and k) and makes sure that none of them is in fact a solution. In
Figure 1, f1 , . . . , fk denote abstract features while g, `, and r denote concrete features. The
definition of the concept Step just serves as an abbreviation and should not be confused
with so-called TBoxes (see Section 4.2 for the definition of TBoxes). Models of CP and KP ,
such as the one displayed in Figure 2, have the form of an infinite k-ary tree whose root is
connected to an extra node x via the role R. Intuitively, each node of the tree represents
one partial solution i1 , . . . , in , its `-successor represents the corresponding left concatenation
`i1    `in , and its r-successor the corresponding right concatenation ri1    rin .
To enforce the existence of the infinite tree, we employ the key box KP : consider for
example the root nodes f1 -successor in Figure 2let us call this node y. Due to Line 3
676

fiKeys, Nominals and Concrete Domains

Step :=

u f .(A u g.= u `, r.6=)
u u (`, f `.conc u r, f r.conc
i

1ik



i

1ik

i

`i

ri )

CP := `.= u r.=
u R.(A u g.= u Step)
u Step
KP := {g keyfor Step}
Figure 1: The ALCK(W) reduction concept CP and key box KP .
R
=

conc`1

`

r

=

x

fk

concrk

f1

A
g.=

concr1 conc`k
`
f1



r


fk

f1



r

`


fk



Figure 2: An example model of CP and KP .
of CP and Line 1 of Step, we have y  (g.= )I . Due to Line 2 of CP , we also have
x  (g.= )I and x  (Step)I , where x is the extra node mentioned above. In view of
the key box KP , this implies that either (i) x = y or (ii) y  StepI . It is easy to see that
(i) is impossible since Line 2 of CP and Line 1 of Step imply that x  AI and y  (A)I .
Hence y  StepI and, by Line 2 of Step, y has the appropriate fi -successors for 1  i  n.
In the same way, the construction of the tree can be continued ad infinitum. The second
line in the definition of Step enforces that `I (z) = `i1    `in and rI (z) = ri1    rin for z an
fi1    fin -successor of the root node. Finally, the concept `, r.6= in Line 1 of Step implies
that `I (z) 6= rI (z) holds at all nodes z of the tree (except for the root), which implies that
no potential solution is a solution.
Since the size of CP and KP is clearly polynomial in k and the key box KP is a unary
key box, we obtain the following proposition.
Proposition 3.2. The satisfiability of ALCK(W)-concepts w.r.t. (non-Boolean) path-free
unary key boxes is undecidable.
677

fiLutz, Areces, Horrocks, & Sattler

To emphasize that this undecidability result was obtained using a very simple concrete
domain, let us combine Theorem 3.1 with Proposition 3.2.
Theorem 3.3. There exists a concrete domain D such that D-satisfiability is in PTime
and satisfiability of ALCK(D)-concepts w.r.t. (non-Boolean) path-free unary key boxes is
undecidable.
At first sight, the concrete domain W might look artificial and one may question the relevance of lower bounds that have been obtained using W. However, it is straightforward
to encode words as natural numbers and to define concatenation of words as rather simple operations on the natural numbers (Baader & Hanschke, 1992): a word w 6=  over
the alphabet  of cardinality # can be interpreted as a number written at base # + 1
in which the symbol that is the 0 digit does not occur. Hence, we can use the corresponding natural number (e.g., in base 10) to represent a word w, and the number 0 to
represent the empty word. The concatenation of two words v and w can then be expressed
as vw = v (#+1)|w| +w, where |w| denotes the length of the word w. Moreover, exponentiation can be expressed as multiple multiplications, multiplication as multiple additions,
and addition as multiple incrementation: this is shown in Section 5.6 of (Lutz, 2004) for
the case of ALC(D) extended with TBoxes (c.f. Section 4.2) and can easily be adapted to
ALC(D) with non-Boolean key boxes. This observation gives rise to the following theorem:

N

Theorem 3.4. Let D be a concrete domain such that  D , D contains a unary predicate =0 with (=0 )D = {0}, binary equality and inequality predicates, and a binary predicate
incr with incrD  {(n, x) | n  and x  D } = {(k, k + 1) | k  }. Then satisfiability of
ALCK(D)-concepts w.r.t. (non-Boolean) path-free unary key boxes is undecidable.

N

N

3.2 Domino Problems and Concrete Domains
In this section, we introduce a NExpTime-complete variant of the well-known, undecidable
domino problem (Berger, 1966; Knuth, 1968), and then define three concrete domains D1 ,
D2 , and D3 . These concrete domains will be used in Sections 3.3 and 3.4 to establish
lower bounds for reasoning with ALCK(D) and Boolean key boxes, and for reasoning with
ALCO(D).
In general, a domino problem is given by a finite set of tile types. Intuitively, all tile
types are of the same size, each type having a square shape and colored edges. An unlimited
number of tiles of each type is available. In the NExpTime-hard variant of the domino
problem that we use, the task is to tile a 2n+1  2n+1 -torus (i.e., a 2n+1  2n+1 -rectangle
whose borders are glued together) where neighboring edges have the same color.

N

Definition 3.3 (Domino System). A domino system D is a triple (T, H, V ), where T (
is a finite set of tile types and H, V  T  T represent the horizontal and vertical matching
conditions. Let D be a domino system and a = a0 , . . . , an1 an initial condition, i.e. an
n-tuple of tiles. A mapping  : {0, . . . , 2n+1  1}  {0, . . . , 2n+1  1}  T is a solution for
D and a iff, for all x, y < 2n+1 , the following holds:
 if  (x, y) = t and  (x 2n+1 1, y) = t0 , then (t, t0 )  H
 if  (x, y) = t and  (x, y 2n+1 1) = t0 , then (t, t0 )  V
678

fiKeys, Nominals and Concrete Domains

  (i, 0) = ai for i < n.
where i denotes addition modulo i.
It follows from results in (Borger, Gradel, & Gurevich, 1997) that the above variant of the
domino problem is NExpTime-complete.
We define the concrete domain D1 to be used in the reduction of the NExpTimecomplete domino problem to ALCK(D1 )-concept satisfiability w.r.t. Boolean key boxes.
Definition 3.4 (Concrete Domain D1 ). The concrete domain D1 is defined by setting
D1 := {0, 1} and D1 to the (smallest) set containing the following predicates:
 unary predicates >D1 with (>D1 )D1 = D1 and D1 with (D1 )D1 = ;
 unary predicates =0 and =1 with (=i )D1 = {i}, i  {0, 1}.
The second concrete domain D2 will be used for a reduction of the NExpTime-complete
domino problem to ALCK(D2 )-concept satisfiability w.r.t. Boolean unary key boxes. For
this reduction we need to store vectors of bits in single concrete domain elements.

N

Definition 3.5 (Concrete Domain D2 ). For every n  , a function v : {0, . . . , n  1} 
{0, 1} is called a bit vector of dimension n. We use BVn to denote theSset of all bit vectors
of dimension n. The concrete domain D2 is defined by setting D2 := i>0 BVi and D2 to
the (smallest) set containing the following predicates:
 unary predicates >D2 with (>D2 )D2 = D2 and D2 with (D2 )D2 = ;
 for every k, i 

N with i < k, unary predicates bit0ik and bit1ik with
(bitnik )D2 = {v  D2 | v  BVk and v(i) = n},

and unary predicates bit0ik and bit1ik with (bitnik )D2 = D2 \ (bitnik )D2 .
The last concrete domain D3 is used in the reduction of the NExpTime-complete domino
problem to ALCO(D3 )-concept satisfiability. In this reduction, the concrete domain D3
contains two kinds of elements: firstly, there are elements of D3 that can represent the
whole 2n+1  2n+1 -torus, so-called domino arrays. Secondly, there are elements of D3 that
represent positions in the torus. For technical reasons to be discussed later, these elements
are vectors of natural numbers rather than bit vectors, and in the following we shall just call
them vectors. A domino array is then a function mapping each pair of vectors (of certain
length) to a natural number which represents a tile type.

N

N

Definition 3.6 (Concrete Domain D3 ). For every k  , a function v : {0, . . . , k  1} 
is called a vector of dimension k. We use VEk to denote the set of all vectors of dimension k.
For every k  , a function k : VEk  VEk 
is called a domino array of dimension k.
We use DAk to denote the setSof all domino
arrays
of dimension k. The concrete domain D3
S
is defined by setting D3 := i>0 VEi  i>0 DAi and D3 to the (smallest) set containing
the following predicates:

N

N

 unary predicates >D3 with (>D3 )D3 = D3 and D3 with (D3 )D3 = ;
679

fiLutz, Areces, Horrocks, & Sattler

 for every k, i 

N with i < k, unary predicates pos0ik and pos1ik with
(posnik )D3 = {v  D3 | v  VEk and v(i) = n}

and unary predicates pos0ik and pos1ik with (posnik )D3 = D3 \ (posnik )D3 ;
 for every k, i 

N, a predicate tileik of arity 3 with

(tileik )D3 = {(vx , vy , d) | vx , vy  VEk , d  DAk , and d(vx , vy ) = i}
and a predicate tileik of arity 3 with (tileik )D3 = (D3 )3 \ (tileik )D3 .
The reason for using vectors of natural numbers rather than bit vectors in the definition of
D3 is that we want D3 -satisfiability to be of low complexity, preferably in PTime: consider
the D3 -conjunction
pos002 (x)  pos002 (y)  pos002 (z) 
tile72 (x, v, d)  tile82 (y, v, d)  tile92 (z, v, d).
If we use bit vectors rather than vectors of natural numbers, then the upper line enforces
that at least two out of the three variables x, y, and z must take the same value. Since
the value of v is fixed, the lower line makes the conjunction unsatisfiable: it tries to assign
the three different values 7, 8, 9 to two different positions in the domino array. It seems
unlikely that this kind of inconsistency can be detected in polynomial time. This problem
is circumvented by using vectors of natural numbers in the definition of D3 (but enforcing
them to be bit vectors in the reduction): in this case, the above conjunction is clearly
satisfiable.
Proposition 3.5. For each i  {1, 2, 3}, the concrete domain Di is admissible and satisfiability of Di -conjunctions is in PTime.
For D1 , this is trivial. For D2 , a proof can be found in Appendix A. And for D3 , a proof
can be found in (Lutz, Areces, Horrocks, & Sattler, 2002).
3.3 NExpTime-hardness of ALCK(D) with Boolean Key Boxes
In this section, we prove two NExpTime-lower bounds for ALCK(D)-concept satisfiability
w.r.t. Boolean key boxes by reducing the NExpTime-complete domino problem introduced
in the previous section. The first reduction uses the very simple concrete domain D1 , but
depends on composite key assertions. The second reduction uses the slightly more complex
concrete domain D2 , but only needs unary key assertions. As we will see, the two reductions
yield different, incomparable results.
We first reduce the NExpTime-complete domino problem to ALCK(D1 )-concept satisfiability w.r.t. Boolean composite key boxes. Each domino system D = (T, H, V ) with initial
condition a = a0 , . . . , an1 is translated into an ALCK(D1 )-concept CD,a as displayed in
Figure 3. Names such as TreeX and TreeY are used as abbreviations only. We use Ri .C as
an abbreviation for the n-fold nesting R.    R.C. The names xposi and yposi used in the
figure denote concrete features. In the definition of the Init concept, for each n  , biti (n)

N

680

fiKeys, Nominals and Concrete Domains

TreeX := R.X0 u R.X0 u

u R .(DistX
i

u R.Xi u R.Xi )

i1

i=1..n

TreeY := DistXn u R.Y0 u R.Y0 u

u R .(DistY u DistX u R.Y u R.Y )
DistX := u ((X  R.X ) u (X  R.X ))
DistY := u ((Y  R.Y ) u (Y  R.Y ))
TransXPos := u (X  xpos . = ) u (X  xpos . = )
TransYPos := u (Y  ypos . = ) u (Y  ypos . = )
i

i1

i=1..n

k

k

i

i=0..k

i=0..n

i

i

i

i

i

i=0..k

i

i=0..n

n

i

i

i

i

i

i

1

i

1

i

0

i

i

0

i

Succs := Rx .(TransXPos u TransYPos) u Ry .(TransXPos u TransYPos)

XSuccOk :=
(Yi  Rx .Yi ) u (Yi  Rx .Yi )
i=0..n


Xj  (Xk  Rx .Xk ) u (Xk  Rx .Xk )

u
u u
u t X   (X  R .X ) u (X  R .X )

YSuccOk := u (X  R .X ) u (X  R .X )
u u Y   (Y  R .Y ) u (Y  R .Y )
u t Y   (Y  R .Y ) u (Y  R .Y )
Label := t D u u (D u D )
CheckMatch := t (D u R .D ) u t (D u R .D )

u X u u X u u Y   D
Init := u
k=0..n

j=0..k

k=0..n

j=0..k
i

i=0..n

j=0..k

k=0..n

j=0..k

i

(i,j)H

i=0..n1

i

j

y

k

i

j

j=0..n,bitj (i)=0

x

k

y

y

k

x

k

i

j

i,jT,i6=j

i

x

k

y

k=0..n

iT

j

k

i

k

k

y

k

k

y

k

k

j

(i,j)V
j

i

y

j

j=0..n,bitj (i)=1

j

j=0..n

j



ai

CD,a := TreeX u Rn+1 .TreeY
u R2(n+1) .(TransXPos u TransYPos u Succs u XSuccOk u YSuccOk)
u R2(n+1) .(Label u CheckMatch u Init)
Figure 3: The ALCK(D1 ) reduction concept CD,a .
is supposed to denote the ith bit of the binary representation of n. We claim that CD,a is
satisfiable w.r.t. the key box
{(xpos0 , . . . , xposn , ypos0 , . . . , yposn keyfor >)}
iff there exists a solution for D and a. To substantiate this claim, let us go through the
reduction and explain the various parts of the concept CD,a . The first step towards under681

fiLutz, Areces, Horrocks, & Sattler

standing the structure of models of CD,a (which is the key to understanding the reduction
itself) is to note that the purpose of the first line of CD,a is to enforce a tree structure
of depth 2(n + 1), whose leaves correspond to positions in the 2n+1  2n+1 -torus. More
precisely, the TreeX concept guarantees that, in every model I of CD,a , there exists a binary
tree of depth n + 1. Moreover, the DistXk concepts (there exists one for each k  {0, . . . , n})
ensure that the leaves of this tree are binarily numbered (from 0 to 2n+1  1) by the concept
names X0 , . . . , Xn . More precisely, for a domain object d  I , set

1 if d  XiI
n
i
xpsn(d) = i=0 i (d)  2 where i (d) =
0 otherwise.
The TreeX and DistX concepts ensure that there exist nodes d0 , . . . , d2n+1 1 at level n + 1
of the tree such that xpsn(di ) = i. Intuitively, this numbering represents the horizontal
positions in the 2n+1  2n+1 -torus. The vertical positions are coded in a similar way by the
Y0 , . . . , Yn concept names. More specifically, the concepts TreeY, DistX, and DistY ensure
that every di (i  2n+1  1) is the root of another tree, in which (i) every node has the
same X0 , . . . , Xn -configuration as its root node, and (ii) the leaves are numbered binarily
using the concept names Y0 , . . . , Yn (note that the TreeY concept appears in CD,a inside a
Rn+1 value restriction). Define

1 if d  YiI
n
i
ypsn(d) = i=0 i (d)  2 where i (d) =
0 otherwise.
In the set of leaf nodes of all the trees enforced by the TreeY concept, there exists, for each
i, j < 2n+1 , an object4 ei,j  I such that xpsn(ei,j ) = i and ypsn(ei,j ) = j, i.e., each ei,j
represents the position (i, j) in the 2n+1  2n+1 -torus.
The next step is to translate the individual bits of the numbering of the ei,j -objects,
which are up to now represented by concept names, into concrete domain values. This is
done by the TransXPos and TransYPos concepts which ensure that, for all `  n, we have
xposI` (ei,j ) = 0 if ei,j  X` , xposI` (ei,j ) = 1 if ei,j  X` , and similarly for ypos` and Y` .
Since I is a model for the key box
{(xpos0 , . . . , xposn , ypos0 , . . . , yposn keyfor >)},
grid positions are uniquely represented by domain elements from (TransXPos u TransYPos)I ,
i.e., if d, e  (TransXPos u TransYPos)I such that xpsn(d) = xpsn(e) and ypsn(d) = yxpsn(e),
then d = e. This fact is used in the concepts Succs, XSuccOk, and YSuccOk to enforce that,
for the two roles Rx and Ry and each i, j  n, the following holds:
RxI  ({ei,j }  I ) = {(ei,j , e(i2n+1 1),j }
RyI  ({ei,j }  I ) = {(ei,j , ei,(j2n+1 1) }.

()

The Succs concept ensures that, for each ei,j , there exists an Rx -successor and an Ry successor, and that both are in (TransXPos u TransYPos)I . Let d be an Rx -successor of ei,j .
Then the XSuccOk concept ensures that xpsn(d) = i 2n+1 1 and ypsn(d) = j. Before we
4. It does not matter if there is more than one such object.

682

fiKeys, Nominals and Concrete Domains

explain how it does this, let us note that, since all ei,j are in (TransXPos u TransYPos)I and
the grid positions are uniquely represented by elements of (TransXPos u TransYPos)I , this
implies d = e(i2n+1 1),j which shows that the upper line of () does indeed hold.
Let us now consider the XSuccOk concept in some more detail. It is essentially the
DL-formulation of the well-known propositional formula
n k1
n k1
^
^
^
_
(
xj = 1)  (xk = 1  x0k = 0) 
(
xj = 0)  (xk = x0k )
k=0 j=0

k=0 j=0

which encodes incrementation modulo 2n+1 , i.e., if t is the number (binarily) encoded by
the propositional variables x0 , . . . , xn and t0 is the number encoded by the propositional
variables x00 , . . . , x0n , then we have t0 = t + 1 modulo 2n+1 (see Borger et al., 1997). Taking
into account the Rx quantifiers in XSuccOk, it is readily checked that this concept has
just the desired effect: to ensure that, for every Rx -successor d of ei,j , we have xpsn(d) =
xpsn(e(i2n+1 1),j ) = i 2n+1 1. The explanation of YSuccOk and how it enforces the lower
line of () is analogous to the XSuccOk case.
It remains to ensure that every grid position is labeled with precisely one tile and that
the initial condition as well as the horizontal and vertical matching conditions are satisfied.
The tiles are represented by concept names Di (where i is from the set of tiles T ) and
the described tasks are accomplished in the standard way by the concepts Label, Init, and
CheckMatch.
It is worth noting that the reduction concept is path-free and the key box is simple,
i.e., path-free and Boolean. Path-freeness of concepts is often used to tame the complexity
of description logics with concrete domains, although it largely sacrifices their expressive
power (Lutz, 2003; Baader, Lutz, Sturm, & Wolter, 2002b; Haarslev, Moller, & Wessel, 2001;
Horrocks & Sattler, 2001). For example, if ALC(D) is augmented with general TBoxes, then
reasoning with arbitrary concepts is undecidable while reasoning with path-free concepts
is ExpTime-complete if D is admissible and D-satisfiability is in ExpTime (Lutz, 2002a).
This taming approach does not work in the presence of key boxes since, as we have just
seen, satisfiability of ALC(D)-concepts w.r.t. key boxes is (under some natural assumptions)
NExpTime-hard, even if both concept and key box are path-free.
Since the size of CD,a and of the used key box is clearly polynomial in n, we obtain the
following proposition.
Proposition 3.6. The satisfiability of path-free ALCK(D1 )-concepts w.r.t. simple key boxes
is NExpTime-hard.
It has been shown that (non path-free) ALC(D)-concept satisfiability is PSpace-complete
if D-satisfiability is in PSpace (Lutz, 2002b). Hence, it follows from Proposition 3.5 that
ALC(D1 )-concept satisfiability is PSpace-complete. Thus, there is a rather dramatic increase of complexity if key boxes are added to ALC(D1 ). To stress that this increase is due
to the key boxes themselves and not to the complexity of D1 -satisfiability, we reformulate
Proposition 3.6:
Theorem 3.7. There exists a concrete domain D such that D-satisfiability is in PTime and
satisfiability of path-free ALCK(D)-concepts w.r.t. simple key boxes is NExpTime-hard.
683

fiLutz, Areces, Horrocks, & Sattler

Succs2 := Rx .TransPos u Ry .TransPos
TransPos :=

u
u

i=0..n
i=0..n


(Xi  bv.bit1i2(n+1) ) u Xi  bv.bit0i2(n+1) ) u

n+i+1
n+i+1
(Yi  bv.bit12(n+1)
) u Yi  bv.bit02(n+1)
)

CD,a := TreeX u Rn+1 .TreeY
u R2(n+1) .(TransPos u Succs2 u XSuccOk u YSuccOk)
u R2(n+1) .(Label u CheckMatch u Init)
Figure 4: The ALCK(D2 ) reduction concept CD,a .
Although, due to its very low expressivity, the concrete domain D1 itself is not very natural
for knowledge representation, it is a fragment of many concrete domains that have been
proposed in the literature (Baader & Hanschke, 1992; Haarslev & Moller, 2001; Lutz, 2003,
2002b). Indeed, the presented reduction strategy can be adapted to several standard
concrete domains. Let us formulate a (very weak) condition that a concrete domain must
satisfy in order for the presented reduction strategy to be applicable.
Theorem 3.8. Let D be a concrete domain. If there exist a, b  D with a 6= b and P1 , P2 
D such that P1D = {a} and P2D = {b}, then the satisfiability of path-free ALCK(D)-concepts
w.r.t. simple key boxes is NExpTime-hard.
We now present the second NExpTime-hardness result for ALCK(D)-concept satisfiability.
This time, we reduce the NExpTime-complete domino problem to the satisfiability of pathfree ALCK(D2 )-concepts w.r.t. simple unary key boxes. The reduction is very similar to
the previous one and we only discuss the differences.
In the first reduction, we represented the individual bits of grid positions by individual
concrete features xposi and yposi and used a composite key box to ensure that each point
in the torus is represented by at most one element. In the second reduction, we use a single
concrete feature bv and represent an entire position (i, j) in the torus using a bit vector
from the concrete domain D2 . This allows us to enforce the above mentioned uniqueness of
representations using a unary key box.
The modified reduction concept CD,a can be found in Figure 4, where the concepts
TreeX, TreeY, DistXk , DistYk , XSuccOk, YSuccOk, Label, CheckMatch, and Init are defined
as in Figure 3. The translation of the position in the torus encoded by X0 , . . . , Xn , Y0 , . . . , Yn
into a bit vector is done by the TransPos concept in a straightforward manner. Given what
was said about the first reduction, it is not hard to see that CD,a is satisfiable w.r.t. the key
box {(bv keyfor >)} iff there exists a solution for D and a. We thus obtain the following
proposition.
Proposition 3.9. The satisfiability of path-free ALCK(D2 )-concepts w.r.t. simple unary
key boxes is NExpTime-hard.
Again, we relate the NExpTime lower bound to the complexity of D2 -satisfiability, which
is determined in Proposition 3.5.
684

fiKeys, Nominals and Concrete Domains

Theorem 3.10. There exists a concrete domain D such that D-satisfiability is in PTime
and the satisfiability of path-free ALCK(D)-concepts w.r.t. simple unary key boxes is NExpTime-hard.
Since the elements of D2 are bit vectors, the concrete domain D2 cannot be considered a
natural choice for many application areas. But, in the reduction, D2 can be replaced by
several natural concrete domains.
The central observation is that we use bit vectors only to injectively translate sequences
of bits into values of the concrete domain, i.e., we translate sequences of 2(n + 1) bits
(represented by the concept names X0 , . . . , Xn and Y0 , . . . , Yn ) into elements of D2 such
that, for distinct sequences, the results of the translation are also distinct. Due to this
restricted use of bit vectors, there are several ways to replace them by natural numbers.
For example, we can replace TransPos with the following concept TransPos0 which ensures
that, for each d  TransPos0I , sI2n+1 (d) = xpsn(d) + 2n+1  ypsn(d):

u


TransPos0 := zero.=0 u
ti .=2i u (X0  s0 .=0 ) u (X0  s0 .=1 ) u
 i=1...2n+1


Xi  (si1 , zero, si ).+ u Xi  (si1 , ti , si ).+ u
i=1..n



Yi(n+1)  (si1 , zero, si ).+ u (Yi(n+1)  (si1 , ti , si ).+

u
u

i=n+1..2n+1

N

where zero, si , and ti are concrete features, =k (with k  ) denotes a unary predicate with
the obvious extension, and + denotes a ternary addition predicate such that, intuitively,
the first two arguments are the addends and the third one is the sum.
I
It is easy to check that, whenever two objects d, e  TransPos0 do not agree on the
interpretation of the X0 , . . . , Xn , Y0 , . . . , Yn , then sI2n+1 (d) 6= sI2n+1 (e), and thus the key
box {(s2n+1 keyfor >)} can be used for the reduction. The size of TransPos0 is obviously
polynomial in n if the numbers k appearing in =k predicates are coded in binary. We thus
obtain the following theorem:
Theorem 3.11. Let D be a concrete domain such that
1.

N  D ,
N

2. D contains, for each k  , a predicate =k with (=k )D = {k} where the size of (the
representation of ) =k is logarithmic in k, and
3. D contains a predicate + with (+)D  {(k1 , k2 , x) | k1 , k2 
{(k1 , k2 , k1 + k2 ) | k1 , k2  }.

N

N and x

 D } =

Then the satisfiability of path-free ALCK(D)-concepts w.r.t. simple unary key boxes is
NExpTime-hard.
For example, this theorem yields NExpTime-lower bounds for ALCK(D) instantiated with
the concrete domains proposed in (Baader & Hanschke, 1992; Haarslev & Moller, 2001; Lutz,
2003, 2002b). An alternative to the addition predicate is to use multiplication to injectively
685

fiLutz, Areces, Horrocks, & Sattler

translate sequences of bits into natural numbers. More precisely, let p1 , . . . , p2n+1 be the
first 2n + 1 prime numbers and define another version of TransPos as follows:
TransPos00 := one.=1 u

u
u

i=1..n

u


ti .=pi u (X0  s0 .=0 ) u (X0  s0 .=1 ) u
i=1...2n+1



Xi  (si1 , one, si ). u Xi  (si1 , ti , si ). u



Yi(n+1)  (si1 , one, si ). u Yi(n+1)  (si1 , ti , si ).

i=n+1..2n+1

where  is a ternary multiplication predicate.
Since the factorization of natural numbers into prime numbers is unique, we can again
use the key box {(s2n+1 keyfor >)} for the reduction. Moreover, it is well-known that the
kth prime is polynomial in k (Graham, Knuth, & Patashnik, 1990), and thus the size of
the concept TransPos00 is polynomial in n even if the numbers k in =k predicates are coded
unarily. We thus obtain another theorem concerning quite natural concrete domains:
Theorem 3.12. Let D be a concrete domain such that
1.

N  D ,

N, a predicate =k with (=k )D = {k}, and
3. D contains a predicate  with ()D  {(k1 , k2 , x) | k1 , k2  N and x
{(k1 , k2 , k1  k2 ) | k1 , k2  N}.
2. D contains, for each k 

 D } =

Then the satisfiability of path-free ALCK(D)-concepts w.r.t. simple unary key boxes is
NExpTime-hard.
3.4 NExpTime-hardness of ALCO(D)
As we already pointed out in Section 1, the relationship between key boxes and nominals
is rather close: the latter can be simulated by the former if the concrete domain provides
predicates that can be used to uniquely describe elements of D . For example, in ALCK(D1 )
the concept g.=0 behaves as a nominal if we use the key assertion (g keyfor >). We can
even define n nominals using n single concrete features in unary-key assertions. In the
logics ALCK(D2 ) and ALCK(D3 ), a single concrete feature and unary key assertions are
sufficient to simulate an arbitrary number of nominals: for example, in ALCK(D2 ) the
concept C = g.bit002 u g.bit112 uniquely describes the bit vector (0, 1)  BV2  D2 , i.e.,
a  C I implies g I (a) = (0, 1). Obviously, any other bit vector (of any length!) can be
described in a similar way.
This illustrates that, for most non-trivial concrete domains D, the logic ALCK(D) is
(at least) as expressive as ALCO(D). Although the converse does not hold, the expressive
power of ALCO(D) is still sufficient to prove NExpTime-hardness of concept satisfiability,
provided that a suitable concrete domain D is used. Since ALCO concept satisfiability is
PSpace-complete (Areces, Blackburn, & Marx, 1999), this is yet another example of a DL
where an even seemingly harmless extension with concrete domains has a dramatic effect
on the computational complexity (Lutz, 2003).
686

fiKeys, Nominals and Concrete Domains

Nominal := f.N
XSucc :=
YSucc :=

u u X   (X  X ) u u t X   (X  X )
u u Y   (Y  Y ) u u t Y   (Y  Y )

k=0..n

j=0..k

k=0..n

j=0..k

j

0
k

k

j

0
k

k

k=0..n

k=0..n

j

j=0..k

j=0..k

j

i=0..n

i
n+1

0
i

i
n+1

0
i

i
n+1

0
i

i
n+1

i,jV

u

i=0..n1

i
n+1

i

0
i

i,jH

Init2 :=

i
n+1

i

i
n+1

i

i=0..n

i=0..n

i
n+1

i

i=0..n



u

0
k

k

u (X  bvx.pos1 ) u (X  bvx.pos0 )

TransYPos := u (Y  bvy.pos1
) u (Y  bvy.pos0
)

TransXSucc := u (X  bvxs.pos1
) u (X  bvxs.pos0
)

TransYSucc := u (Y  bvys.pos1
) u (Y  bvys.pos0
)
CheckHMatch := t ((bvx, bvy, f  darr).tile
u (bvxs, bvy, f  darr).tile
CheckVMatch := t ((bvx, bvy, f  darr).tile
u (bvx, bvys, f  darr).tile
TransXPos :=

0
k

k

i
n+1

j
n+1 )

i
n+1

j
n+1 )

Xj u

u

u

Xj u
Yj
j=0..n
j=0..n,bitj (i)=0
j=0..n,bitj (i)=1

i
 (bvx, bvy, f  darr).tilean+1



CD,a := TreeX u Rn+1 .TreeY u R2(n+1) .Nominal u
R2(n+1) .(TransXPos u TransYPos u
XSucc u YSucc u TransXSucc u TransYSucc u
Init2 u CheckHMatch u CheckVMatch)
Figure 5: The ALCO(D3 ) reduction concept CD,a .

In this section, we reduce the NExpTime-complete domino-problem to ALCO(D3 )concept satisfiability. Again, let D = (T, H, V ) be a domino system and a = a0 , . . . , an1
an initial condition. The modified reduction concept CD,a is defined in Figure 5, where bvx,
bvy, bvxs, bvys, and darr denote concrete features, N denotes a nominal, and the concepts
TreeX, TreeY, DistXk , and DistYk are defined as in Figure 3. As in the previous reductions,
we now give a detailed explanation of the reduction strategy to show that CD,a is satisfiable
iff there exists a solution for D and a. Formal details can then easily be worked out by the
interested reader.
Let I be a model for CD,a . To explain the structure of I, it is convenient to start
with the first line of CD,a . As in the previous reductions, the TreeX and TreeY concepts
are used to ensure that I contains a tree-shaped substructure of depth n + 1 whose leaf
nodes are the roots of additional trees of depth n + 1 such that the set of the leafs of the
687

fiLutz, Areces, Horrocks, & Sattler

TreeX

TreeY

...

TreeY

...

...

f

f

TreeY

...f

N
darr

Figure 6: The structure of models of CD,a .
latter trees correspond to the positions in the 2n+1  2n+1 -torus, i.e., for each position,
there is a leaf node representing it. The torus positions are binarily encoded by the concept
names X0 , . . . , Xn and Y0 , . . . , Yn and we use ei,j to refer to the leaf with xpsn(ei,j ) = i and
ypsn(ei,j ) = j (see Section 3.3).
As in the previous reductions, the numbers coded by X0 , . . . , Xn and Y0 , . . . , Yn are
translated into concrete domain values, which is done by the TransXPos and TransYPos
concepts. Note that, in contrast to the ALCK(D2 )-reduction, the x-position and the yposition are not stored in the same bit vector, but rather in the two distinct ones bvx
and bvy. Also in contrast to the previous reduction, the actual tiling of the torus is not
represented by the leaf nodes ei,j , but rather by a domino array: the last conjunct in the
first line of CD,a ensures that every leaf ei,j is connected via the abstract feature f to the
(unique) element w  N I .
The domain element w is associated with a domino array via the concrete feature darr (as
we shall see later, this is guaranteed by the CheckHMatch and CheckVMatch concepts). This
domino array represents the tiling of the 2n+1  2n+1 -torus. Summing up, the structure of
I is roughly as shown in Figure 6.
Since the tiling is stored in a domino array, we need to explain the purpose of the leaf
nodes ei,j : these nodes are used to enforce the initial condition and the horizontal and
vertical matching condition. Let us discuss the horizontal matching condition (the vertical
matching condition is enforced analogously): the XSucc concept is the DL reformulation
of the propositional logic formula for incrementation modulo 2n+1 and ensures that, for
each ei,j , the concept names X00 , . . . , Xn0 encode the number i 2n+1 1, i.e., the horizontal
position of ei,j s horizontal neighbor. In addition to the storage of the horizontal and vertical
position of ei,j in bvx(ei,j ) and bvy(ei,j ), we also store the horizontal position i2n+1 1 of ei,j s
horizontal successor in bvxs(ei,j ). Finally, CheckHMatch verifies that the tiles at positions
688

fiKeys, Nominals and Concrete Domains

(i, j) and (i 2n+1 1, j), which are both stored in the domino array, are compatible with the
horizontal matching condition.
Note that CheckHMatch also ensures that the domain element w (with {w} = N I ) has
a domino array attached via the concrete feature darr and that, for each position (i, j), the
(unique!) tile stored in the domino array is from the set T . The initial condition is ensured
via the Init2 concept in a similar way. We (again) use bitj (i) to denote the jth bit of the
binary encoding of the natural number i.
Using the above considerations, the correctness of the reduction is readily checked.
Moreover, the size of CD,a is at most polynomial in n. Note that CD,a is not path-free:
paths of length two appear in the concepts CheckHMatch, CheckVMatch, and Init2. Summing
up, the reduction described yields the following result:
Proposition 3.13. The satisfiability of ALCO(D3 )-concepts is NExpTime-hard.
Again, we relate the NExpTime lower bound to the complexity of D3 -satisfiability, which
is determined in Proposition 3.5.
Theorem 3.14. There exists a concrete domain D such that D-satisfiability is in PTime
and the satisfiability of ALCO(D)-concepts is NExpTime-hard.
Note that the reduction uses only a single nominal N . This is a dramatic increase of complexity since it has been shown that satisfiability of ALC(D)-concepts (i.e., without nominals
and key boxes) is PSpace-complete provided that D is admissible and D-satisfiability is in
PSpace (Lutz, 2002b).
As in previous sections, we note that D3 can be replaced by more natural concrete
domains in the NExpTime-hardness proof presented. The idea is to represent the whole
domino array by a single natural number and then to use arithmetic operations to access the
individual positions: a natural number k can be viewed as a domino array by partitioning
its binary representation into 2n+1  2n+1 = 22(n+1) sections of length dlog(#T )e, where
#T denotes the cardinality of the set of tile types T . Each such section describes the tile
of a single position in the torus. The sections can be accessed by using integer division
and reminder operations: if k is the natural number representing the torus, then the tile of
posisition i is computed by
(k div 2idlog(#T )e ) mod 2dlog(#T )e + 1.
Thus, we introduce ternary predicates div for integer division and mod for computing the
remainder of a division, and a binary predicate 2x expressing exponentiation with basis 2.
Then we modify the reduction as follows: we replace TransXPos and TransYPos by the
TransPos0 concept from Section 3.3 to translate the two numbers encoded by X1 , . . . , Xn
and Y1 , . . . , Yn into a single natural number that is stored in the concrete feature s2n+1 . We
then devise a new concept Tile[i] (for each i  T ) enforcing that the position identified by
the feature s2n+1 is labeled with tile i:
Tile[i] := r.=dlog(#T )e u s2n+1 , r, r0 . u r0 , r00 .2x u one.=1 u r, one, t.+ u t, t0 .2x
u f torus, r00 , u.div u u, t00 , tile.mod u tile.=i .
689

fiLutz, Areces, Horrocks, & Sattler

Here, r, r0 , r00 , t, t0 , u, one, torus, and tile are concrete features. The torus feature is the counterpart of the darr feature in the original reduction, i.e., it stores the natural number that
represents the tiling array. We can use the Tile[i] concept in the obvious way inside the
CheckHMatch, CheckVMatch, and Init2 concepts. The size of the resulting reduction concept
is polynomial in n if the numbers k appearing in =k predicates are coded in binary. We
thus obtain the following theorem:
Theorem 3.15. Let D be a concrete domain such that
1.

N  D ,

2. D contains the predicates a predicate =k (for each k 
the following extensions
(2x )D

 {(k, x) | k
(+)D  {(k1 , k2 , x) | k1 , k2
()D  {(k1 , k2 , x) | k1 , k2
(div)D  {(k1 , k2 , x) | k1 , k2
(mod)D  {(k1 , k2 , x) | k1 , k2







N and
N and
N and
N and
N and

(=k )D
x  D }
x  D }
x  D }
x  D }
x  D }

=
=
=
=
=
=

N), 2x, +, , div, mod with

{k}
{(k, 2k ) | k  }
{(k1 , k2 , k1 + k2 ) | k1 , k2  }
{(k1 , k2 , k1  k2 ) | k1 , k2  }
{(k1 , k2 , k1 div k2 ) | k1 , k2  }
{(k1 , k2 , k1 mod k2 ) | k1 , k2  }

N

N
N
N
N

Then the satisfiability of ALCO(D)-concepts is NExpTime-hard.

4. Reasoning Procedures
This section is devoted to developing reasoning procedures for DLs with concrete domains,
nominals, and keys. We start with devising a tableau algorithm that decides the satisfiability
of ALCOK(D)-concepts w.r.t. Boolean key boxes. This algorithm yields a NExpTime upper
complexity bound matching the lower bounds established in Section 3.3.
Then we consider the rather powerful description logic SHOQK(D). This DL, which is
an extension of SHOQ(D) (Horrocks & Sattler, 2001; Pan & Horrocks, 2002), provides a
wealth of expressive means such as transitive roles, role hierarchies, nominals, and qualifying
number restrictions. Moreover, SHOQK(D) is equipped with a restricted variant of the
concrete domain constructor and with key boxes. We develop a tableau algorithm for
deciding the satisfiability of SHOQK(D)-concepts w.r.t. path-free key boxes. Due to the
restrictedness of SHOQK(D)s concrete domain constructor, we can even admit general
rather than only Boolean key boxes. Again, the algorithm yields a tight NExpTime upper
complexity bound.
4.1 A Tableau Algorithm for ALCOK(D) with Boolean Key Boxes
Tableau algorithms decide the satisfiability of the input concept (in our case w.r.t. the input
key box) by attempting to construct a model for it. More precisely, a tableau algorithm
starts with an initial data structure induced by the input concept and then repeatedly applies so-called completion rules to it. This rule application can be thought of as attempting
to construct a model for the input concept. Finally, either the algorithm will find an obvious contradiction or it will encounter a situation that is contradiction-free and in which no
690

fiKeys, Nominals and Concrete Domains

more completion rules are applicable. In the former case, the input concept is unsatisfiable,
while it is satisfiable in the latter.
When devising a tableau algorithm for a description logic with concrete domains but
without committing to a particular concrete domain, it is commonly assumed that the concrete domain is admissible, which implies decidability of the satisfiability of D-conjunctions.
In the presence of keys, however, this is not enough: if a D-conjunction is satisfiable, we also
want to know which of its variables take the same values in an arbitrary but fixed solution.
As an example, consider the concrete domain N = ( , {<n | n  }) and the N-conjunction

N

N

c = <2 (v1 )  <2 (v2 )  <2 (v3 ).
Obviously, one solution  for c satisfies (v1 ) = (v2 ), another satisfies (v1 ) = (v3 ), and so
on. Our tableau algorithm uses such identity information passed from the concrete domain
reasoner since, in the presence of key boxes, it can have an impact on the structure of the
constructed model. For example, this information reveals the unsatisfiability of
R.A u R.(A u B) u R.(A u B) u R.g.<2 w.r.t. (g keyfor >).
To formalize this requirement, we strengthen the notion of admissibility into key-admissibility.
Since the tableau algorithm developed in this section is non-deterministic, we formulate keyadmissibility in a non-deterministic way.
Definition 4.1 (Key-admissible). A concrete domain D is key-admissible iff it satisfies the
following properties:
1. D contains a name >D for D ;
2. D is closed under negation;
3. there exists an algorithm that takes as input a D-conjunction c, returns clash if c is
unsatisfiable, and otherwise non-deterministically outputs an equivalence relation 
on the set of variables V used in c such that there exists a solution  for c with the
following property: for all v, v 0  V
(v) = (v 0 ) iff v  v 0 .
An algorithm showing the behaviour described in item 3 above is called a D-tester, and
the equivalence relations  are called concrete equivalences. We say that extended Dsatisfiability is in NP if there exists a D-tester running in polynomial time.
Please note that key-admissibility is less esoteric than it might seem: any concrete domain
that is admissible and provides for an equality predicate is also key-admissible. Due to
admissibility, the presence of an equality predicate implies that an inequality predicate is
also available. We can thus construct a D-tester from an algorithm for D-satisfiability:
when presented with a predicate conjunction c, we simply guess an equivalence relation
 on the set of variables
used in c. VThen we decide the (non-extended) satisfiability of
V
the conjunction c  vv0 =(v, v 0 )  v6v0 6=(v, v 0 ), return clash if it is unsatisfiable and
 otherwise. The rather weak condition that an equality predicate should be present is
691

fiLutz, Areces, Horrocks, & Sattler

(C u D)
(R.C)

C t D (C t D)
R.C
(R.C)

(u1 , . . . , un .P )
(g)

C u D
R.C

C

C

u1 , . . . , un .P t u1  t    t un 
g.>D

Figure 7: The NNF rewrite rules.
satisfied by almost all concrete domains proposed in the literature (see, e.g. (Lutz, 2003;
Baader & Hanschke, 1991b; Kamp & Wache, 1996; Haarslev, Lutz, & Moller, 1998; Baader
& Sattler, 1998)).
Throughout this chapter, we assume that any concrete domain is equipped with an
equality predicate. This assumption is w.l.o.g. since any D-conjunction using equality can
be translated into an equivalent one without equality by identifying variables according to
the stated equalities. This assumption must not be confused with what was discussed in the
previous paragraph: even if the concrete domain D is admissible and its set of predicates is
thus closed under negation, this assumption does not imply the presence of an inequality
predicate.
We need some more prerequisites before we can start the presentation of the tableau
algorithm: a concept is in negation normal form (NNF) if negation occurs only in front of
concept names and nominals. It is easily seen that, if the concrete domain D is admissible,
then every ALCOK(D)-concept can be converted into an equivalent one in NNF by exhaustively applying the rewrite rules displayed in Figure 7. We use  C to denote the result of
converting C to NNF. A key box is in NNF if all concepts occurring in key assertions are
in NNF. In what follows, we generally assume input concepts and key boxes to be in NNF.
Let C be an ALCOK(D)-concept and K a key box. We use sub(C) to denote the set of
subconcepts of C (including C itself) and con(K) to denote the set of concepts appearing
on the
S right-hand side of key assertions in K. For a set of concepts , sub() denotes the
set C sub(C). Moreover, we write cl(C, K) as abbreviation for the set
sub(C)  sub(con(K))  {D
 | D  sub(con(K))}.
We now start the presentation of the tableau algorithm by introducing the underlying data
structure.
Definition 4.2 (Completion System). Let Oa and Oc be disjoint and countably infinite
sets of abstract and concrete nodes. A completion tree for an ALCOK(D)-concept C and a
key box K is a finite, labeled tree T = (Va , Vc , E, L) with nodes Va  Vc such that Va  Oa ,
Vc  Oc , and all nodes from Vc are leaves. The tree is labeled as follows:
 each node a  Va is labeled with a subset L(a) of cl(C, K);
 each edge (a, b)  E with a, b  Va is labeled with a role name L(a, b) occurring in C
or K;
 each edge (a, x)  E with a  Va and x  Vc is labeled with a concrete feature L(a, x)
occurring in C or K.
692

fiKeys, Nominals and Concrete Domains

For a  Va , we use levT (a) to denote the depth at which a occurs in T (starting with the
root node on depth 0). A completion system for an ALCOK(D)-concept C and a key box
K is a tuple (T, P, , ), where
 T = (Va , Vc , E, L) is a completion tree for C and K,
 P is a function mapping each P  D of arity n in C to a subset of Vcn ,
  is a linear ordering of Va such that levT (a)  levT (b) implies a  b, and
  is an equivalence relation on Vc .
Let (Va , Vc , E, L) be a completion tree. A node b  Va is an R-successor of a node a  Va
if (a, b)  E and L(a, b) = R, while a node x  Vc is a g-successor of a if (a, x)  E and
L(a, x) = g. For a path u, the notion of u-successor is defined in the obvious way.
Intuitively, the relation  records equalities between concrete nodes found during the
(non-deterministic) model construction process. The recording is necessary since equalities between concrete nodes can induce equalities between abstract nodes which, in turn,
can imply more equalities between concrete nodes. This can be seen in the following example: assume the completion tree contains, for i  {1, 2}, an abstract node ai with a
concrete g-successor xi and a concrete g 0 -successor yi . Now assume that the key box contains (g keyfor >), and that the D-tester returns x1  x2 . As a consequence, a1 and a2
represent the same element and thus functionality of g 0 implies that also y1 and y2 represent
the same (concrete) element. To deal with such effects, we define an equivalence relation
a on abstract nodes and a second equivalence relation c on concrete nodes.
Definition 4.3 (a and c Relations). Let S = (T, P, , ) be a completion system for
a concept C and a key box K with T = (Va , Vc , E, L), and let  be an equivalence relation
on Va . For each R  NR , a node b  Va is an R/-neighbor of a node a  Va if there exists
a node c  Va such that a  c and b is an R-successor of c. Similarly, for each g  NcF , a
node x  Vc is a g/-neighbor of a if there exists a node c  Va such that a  c and x is a
g-successor of c. For paths u, the notion of u/-neighbor is defined in the obvious way.
We define a sequence of equivalence relations 0a  1a     on Va as follows:
0a = {(a, a)  Va2 } 
{(a, b)  Va2 | there is an N  NO such that N  L(a)  L(b)}
i+1
= ia 
a
{(a, b)  Va2 | there is a c  Va and an f  NaF such that
a and b are f /ia -neighbors of c} 
{(a, b)  Va2 | there is a (u1 , . . . , un keyfor C)  K,
ui /ia -neighbors xi of a for 1  i  n, and
ui /ia -neighbors yi of b for 1  i  n
such that C  L(a)  L(b) and xi  yi for 1  i  n}.
Finally, set a =

S

i
i0 a .

Then define

c =   {(x, y)  Vc2 | there is an a  Va and a g  NcF such that
x and y are g/a -neighbors of a}.
693

fiLutz, Areces, Horrocks, & Sattler

This definition reflects the above mentioned tight coupling between the concrete and abstract equalities: if the D-tester finds (or guesses) that two concrete nodes are equal, the
tableau algorithm may use this to deduce (via the computation of a and c ) even more
equalities between concrete nodes.
Let D be a key-admissible concrete domain. To decide the satisfiability of an ALCOK(D)concept C0 w.r.t. a Boolean key box K (both in NNF), the tableau algorithm is started with
the initial completion tree
TC0 = ({a0 }, , , {a0 7 {C0 }})
in the initial completion system
SC0 = (TC0 , P , , ),
where P maps each P  D occurring in C0 to . We now introduce an operation that is
used by the completion rules to add new nodes to completion trees.
Definition 4.4 (+ Operation). An abstract or concrete node is called fresh in a completion tree T if it does not appear in T. Let S = (T, P, , ) be a completion system with
T = (Va , Vc , E, L). We use the following notions:
 Let a  Va , b  Oa fresh in T, and R  NR . We write S +aRb to denote the completion
system S 0 that can be obtained from S by adding b to Va and (a, b) to E and setting
L(a, b) = R and L(b) = . Moreover, b is inserted into  such that b  c implies
levT (b)  levT (c).
 Let a  Va , x  Oc fresh in T and g  NcF . We write S +agx to denote the completion
system S 0 that can be obtained from S by adding x to Vc and (a, x) to E and setting
L(a, x) = g.
When nesting the + operation, we omit brackets, writing, for example, S + aR1 b + bR2 c for
(S + aR1 b) + bR2 c. Let u = f1    fn g be a path. When a  Va and x  Oc is fresh in T,
we use S + aux to denote the completion system that is obtained from S by taking distinct
nodes b1 , . . . , bn  Oa which are fresh in T and setting
S + aux := S + af1 b1 +    + bn1 fn bn + bn gx.
Strictly speaking, the S + aRb operation is non-deterministic since we did not specify how
precisely the node b is inserted into . However, since this is dont care non-determinism,
we will view the + operation as being deterministic.
The completion rules can be found in Figure 8. Note that the Rt and Rch rules are
non-deterministic, i.e., they have more than one possible outcome (this is true dont know
non-determinism). Some further remarks on the completion rules are in order: the upper
five rules are well-known from existing tableau algorithms for ALC(D)-concept satisfiability
(see, e.g., Lutz, 2002a). Only the use of R/ a -neighbors and u/ a -neighbors in the rules
R, R, and Rc deserves a comment. Take for example R: intuitively, if we have a a b
for two abstract nodes a and b of the completion tree, then a and b describe the same
domain element of the constructed model (and similarly for the c relation on concrete
694

fiKeys, Nominals and Concrete Domains

Ru

if C1 u C2  L(a) and {C1 , C2 } 6 L(a)
then L(a) := L(a)  {C1 , C2 }

Rt

if C1 t C2  L(a) and {C1 , C2 }  L(a) = 
then L(a) := L(a)  {C} for some C  {C1 , C2 }

R

if R.C  L(a) and there is no R/a -neighbor b of a such that C  L(b),
then set S := S + aRb for a fresh b  Oa and L(b) := {C}

R

if R.C  L(a), b is an R/a -neighbor of a, and C 
/ L(b)
then set L(b) := L(b)  {C}

Rc

if u1 , . . . , un .P  L(a) and there exist no x1 , . . . , xn  Vc such that
xi is ui /a -neighbor of a for 1  i  n and (x1 , . . . , xn )  P(P )
then set S := (S + au1 x1 +    + aun xn ) with x1 , . . . , xn  Oc fresh
and P(P ) := P(P )  {(x1 , . . . , xn )}

Rch

if (u1 , . . . , un keyfor C)  K and there exist x1 , . . . , xn  Vc such that
xi is ui /a -neighbor of a for 1  i  n and {C, C}

 L(a) = 
then set L(a) := L(a)  {D} for some D  {C, C}


Rp

if L(b) 6 L(a) and a  Va is minimal w.r.t.  such that a a b
then set L(a) := L(a)  L(b)

Figure 8: Completion rules for ALCOK(D).
nodes). Thus if a a b and c is an R-successor of a, then c should also be an R-successor
of b. However, since we want the completion tree to be a tree, we do not make the latter
successorship explicit. To compensate for this, the R rule talks about R/a -neighbors
rather than about R-successors.
The lower two rules are necessary for dealing with key boxes. The Rch rule is a
so-called choose rule (Hollunder & Baader, 1991; Horrocks et al., 2000): intuitively,
it guesses whether or not an abstract node a satisfies C if there exists a key assertion
(u1 , . . . , un keyfor C)  K such that there are neighbors of a for all the paths ui . This is
necessary since both possibilities may have ramifications: if a satisfies C, then it must be
taken into account in the construction of the relation a ; if a does not satisfy C, then we
must deal with the consequences of it satisfying C
 (e.g. in case that C is >).
The Rp rule deals with equalities between abstract nodes as recorded by the a relation:
since a a b means that a and b describe the same node in the constructed model, their
node labels should be identical. It suffices, however, to choose one representative for each
equivalence class of a and make sure that this representatives node label contains the
labels of all its a -equivalent nodes. As the representative, we use the node that is minimal
w.r.t. the ordering , which has been introduced solely for this reason. The Rp rule does
the appropriate copying of node labels.
Let us now formalize what it means for a completion system to contain a contradiction.
Definition 4.5 (Clash). Let S = (T, P, , ) be a completion system for a concept C and
a key box K with T = (Va , Vc , , ). We say that the completion system S is concrete
695

fiLutz, Areces, Horrocks, & Sattler

define procedure sat(S)
do
if S contains a clash then
return unsatisfiable
 := test(S )
compute a
compute c
while  =
6 c
if S contains a clash then
return unsatisfiable
if S is complete then
return satisfiable
0
S := the application of a completion rule to S
return sat(S 0 )
Figure 9: The ALCOK(D) tableau algorithm.
domain satisfiable iff the conjunction
^
S =

^

P (x1 , . . . , xn ) 

P used in C (x1 ,...,xn )P(P )

^

=(x, y)

xc y

is satisfiable. S is said to contain a clash iff
1. there is an a  Va and an A  NC such that {A, A}  L(a),
2. there are a  Va and x  Vc such that g  L(a) and x is g/a -neighbor of a,
3. S is not concrete domain satisfiable.
If S does not contain a clash, S is called clash-free. S is called complete iff no completion
rule is applicable to S.
The tableau algorithm is described in Figure 9 in pseudo-code notation. In this figure, test
calls a D-tester as specified in Definition 4.1. Let us say a few words about the while loop.
There obviously exist close relationships between the relations  and c and the predicate
conjunction S :
   c (note that both a and c depend on  and are thus recomputed in each
step of the while loop);
 by definition of S and D-tester, the result of test(S ) yields a relation containing c
(and thus also ).
Using these facts, one may check that, in each step of the while loop, new tuples are added
to the  relation, but none are deleted (see the proof of Lemma B.2 in the appendix).
The while loop is needed because (i) a is defined using , (ii), c is defined using a ,
696

fiKeys, Nominals and Concrete Domains

and (iii) new concrete equalities in c may then imply even more concrete and/or abstract
equalities, and so on.
A similar concrete-abstract interplay takes place in the course of several recursion steps:
equalities between concrete nodes provided by the D-tester may make new rules applicable
(for example Rp and Rc) which changes P and thus also S . This may subsequently lead to
the detection of more equalities between concrete nodes by the D-tester, and so on. These
considerations show that, in the presence of keys, there exists a close interplay between the
concrete domain reasoner and the tableau algorithm, which is not needed if keys are not
present: without keys, it suffices to apply the concrete domain satisfiability check only once
after the completion rules have been exhaustively applied (Baader & Hanschke, 1991a).
The detailed proof of termination, soundness, and completeness together with a complexity analysis of the tableau algorithm defined in this section is given in Appendix B.
Theorem 4.1. Let D be a key-admissible concrete domain. If extended D-satisfiability is
in NP, then ALCOK(D)-concept satisfiability w.r.t. Boolean key boxes is in NExpTime.
We should note that, in the way it is presented here, the algorithm leaves considerable
room for optimizations. One possible optimization concerns the re-use of f -successors
(for abstract features f ): for example, when applying the R rule to a concept f.C  L(a),
where a already has an f -successor b, we could simply add C to L(b) instead of adding a
new f -successor c and recording that b a c.
Another candidate for optimizations is the test function. Recall that this function takes a
predicate conjunction c with set of variables V and non-deterministically returns a concrete
equivalence, i.e., a relation  such that there exists a solution  for c with vi  vj iff
(vi ) = (vj ) (see Definition 4.1). It is not hard to devise an ALC(D)-concept that forces
completion systems to have exponentially many concrete nodes by slightly adapting wellknown ALC-concepts that require models of exponential size (Halpern & Moses, 1992).
Hence, the size of input conjunctions c to test can be exponential in the size of the input
concept. Even for trivial D-conjunctions
c = >D (v1 )      >D (vk )
we have an exponential number of distinct concrete equivalences . Thus, the number of
possible outcomes of a call to the test function may be double exponential in the size of
the input concept. Considering the above example, a natural response to this problem is
to require test to return only minimal concrete equivalences: intuitively, an equivalence is
minimal if only those variables are equivalent whose equality is enforced by the conjunction.
More precisely,  is called minimal if there exists no concrete equivalence 0 such that
{(x, y) | x 0 y}  {(x, y) | x  y}. We conjecture that restricting test in this way does
not destroy the soundness and completeness of the tableau algorithm. However, although
this definitely is a worthwhile optimization, it does not help to overcome the existence of
doubly exponentially many outcomes of test in the worst caseat least not for all concrete
domains D: consider the concrete domain N from Page 691 and conjunctions of the form
ci = <i (v1 )      <i (v2i ).
It is readily checked that, for each i  1, the number of minimal concrete equivalences for
ci is exponential in i. Moreover, it is not hard to devise a concept Ci of size logarithmic
697

fiLutz, Areces, Horrocks, & Sattler

in i that leads to completion systems S such that S = ci . Hence, there are still doubly
exponentially many possible outcomes of the test function.
In the example just discussed, the exponential branching of test is clearly due to the
discreteness of the natural numbers. Indeed, if we use a dense structure for defining concrete
domains, it seems that the restriction to minimal concrete equivalences can have the desired
effect, namely that the number of tests possible outcomes becomes polynomial in the size
of its input and thus exponential in the size of the input concept. For example, consider
the concrete domain Q, which is defined as follows:
 Q is the set

Q of rational numbers;

 Q contains unary predicates >Q and its negation Q , unary predicates =q and 6=q
for each q  , binary comparison predicates {<, , =, 6=, , >}, a ternary addition
predicate +, and its negation + (all with the obvious semantics).

Q

It is readily checked that Q is key-admissible (note that it provides a binary equality predicate) and thus falls into our framework. We conjecture that there exists only one minimal
concrete equivalence for every Q-predicate conjunction c: intuitively, it seems possible to
(inductively) determine a relation  on the set of variables V used in c such that (i) x  y
implies that (x) = (y) for every solution  for c and (ii) there exists a solution  for c such
that v 6 v 0 implies (v) 6= (v 0 ). Clearly,  is a minimal concrete equivalence. Moreover,
due to (i) it is the only one.
4.2 A Tableau Algorithm for SHOQK(D)
Although ALCOK(D) is a quite powerful DL, it lacks several expressive means that can be
found in most state-of-the-art description logic systems such as FaCT and RACER (Horrocks,
1998; Horrocks et al., 2000; Haarslev & Moller, 2001). In this section, we consider the
very expressive description logic SHOQK(D) which provides for concrete domains, key
boxes, and nominals, but also for many other means of expressivity such as transitive
roles, role hierarchies, qualifying number restrictions, and general TBoxes. Modulo some
details, SHOQK(D) can be viewed as the extension of the DL SHOQ(D) with key boxes.
SHOQ(D) was proposed by Horrocks and Sattler (2001) (see also Pan & Horrocks, 2002)
as a tool for ontology reasoning in the context of the semantic web (Berners-Lee, Hendler,
& Lassila, 2001; Baader et al., 2002a).
One very important feature of SHOQK(D) are so-called TBoxes, i.e. concept equations5
.
of the form C = D that are used as a background theory in reasoning. Since it is wellknown that combining general TBoxes and the concrete domain constructor easily leads
to undecidability (Baader & Hanschke, 1992; Lutz, 2004), SHOQK(D) only offers a pathfree variant of the concrete domain constructori.e. only concrete features are admitted
inside this constructor rather than paths of arbitrary length. This restriction indeed regains
decidability (Haarslev et al., 2001; Horrocks & Sattler, 2001). Path-freeness of the concrete
domain constructor obviously renders abstract features unnecessary, and thus this syntactic
type is not available in SHOQK(D).
5. Some TBox formalisms also allow for concept inclusions C v D, but these can be re-written into
equivalent equations, see Section 2.2.2.5 of (Baader et al., 2003).

698

fiKeys, Nominals and Concrete Domains

4.2.1 The Description Logic SHOQK(D)
Let us now define SHOQK(D) in a formal way, starting with the syntax.
Definition 4.6 (SHOQK(D) Syntax). A role axiom is either a role inclusion, which is of
the form R v S with R, S  NR , or a transitivity axiom Trans(R) where R  NR . A role
box R is a finite set of role axioms. Let v
* be the reflexive-transitive closure of the role
inclusions in R. A role name R is called simple if S v
* R implies Trans(S) 
/ R for all role
names S. Let D be a concrete domain. The set of SHOQK(D)-concepts is the smallest set
such that
 every concept name and every nominal is a concept, and
 if C and D are concepts, R is a role name, S a simple role name, n and k are natural
numbers, g1 , . . . , gn are concrete features, and P  D is a predicate of arity n, then
the following expressions are also concepts:
C, C u D, C t D, R.C, R.C, (> k S C), (6 k S C), g1 , . . . , gn .P, and g1 .
.
A concept equation is an expression C = D with C and D concepts. A TBox is a finite set
of concept equations.
For SHOQK(D), we consider key boxes that differ in two aspects from the ones we considered for ALCOK(D): in the following, we assume key boxes to be path-free, but we admit
complex concepts to occur in key assertions. Note that abstract features and paths do
not occur in the syntax of SHOQK(D)as will become clear after the semantics has been
defined, the former can be simulated by the more general number restrictions (6 n R C).
As usual in description logics of the SHIQ/SHOQ family, we require role names in
number restrictions to be simple since admitting arbitrary roles yields undecidability of
reasoning (Horrocks et al., 2000; Horrocks & Sattler, 2001). If the role box R is clear from
the context, we will usually write Trans(R) instead of Trans(R)  R. We now introduce the
semantics of SHOQK(D) and the relevant reasoning problems.
Definition 4.7 (SHOQK(D) Semantics). Interpretations I = (I , I ) are defined as in
Definition 2.3, where the function I is extended to the novel SHOQK(D)-concepts as
follows:
(6 k R C)I := {d  I | ]{e | (d, e)  RI }  k} and
(> k R C)I := {d  I | ]{e | (d, e)  RI }  k}.
.
Let I be an interpretation. Then I satisfies a concept equation C = D if C I = DI . I is
a model of a TBox T if I satisfies all concept equations in T . Similarly, I satisfies a role
inclusion R v S if RI  S I and a transitivity axiom Trans(R) if RI is a transitive relation.
I is a model of a role box R if I satisfies all role inclusions and transitivity axioms in R.
Let T be a TBox, R a role box, and K a key box. A concept C is satisfiable w.r.t. T ,
R, and K iff C, T , R, and K have a common model. C is subsumed by a concept D w.r.t.
T , R, and K (written C vT ,R,K D) iff C I  DI for all common models I of T , R, and K.
699

fiLutz, Areces, Horrocks, & Sattler

Note that, due to the requirement that role names used inside number restrictions should be
simple, existential and universal value restrictions are not just syntactic sugar: in contrast
to number restrictions, they can be used on all roles.
It is well-known that, in many expressive description logics, reasoning with TBoxes
can be reduced to reasoning without them (Schild, 1991; Horrocks & Sattler, 2001): for
SHOQK(D), a concept C is satisfiable w.r.t. T , R, and K iff the concept
R.C u R.

u


DE u
.
D=ET

u

R.N



nominal N used
in C, T , or K

is satisfiable w.r.t. to R0 , K, and the empty TBox, where R is a fresh role not appearing in
C, R, and T , and
[
{S v R}.
R0 := R  {Trans(R)} 
role name S used
in C, T , R, or K

Since subsumption can be reduced to satisfiability as described in Section 2, in the following
we will only consider satisfiability of concepts w.r.t. role boxes and key boxes, but without
TBoxes. We will also generally assume role boxes R to be acyclic, i.e. to satisfy the following
condition: for each role name R, there are no role names R1 , . . . , Rk such that R = R1 = Rk
and Ri v Ri+1  R for 1  i < k. It is not hard to see that this is not a restriction since
cycles can be eliminated: if R1 , . . . , Rk is a cycle in R, then we have R1I =    = RkI
for all interpretations I. Thus we can simply remove the cycle from R and replace every
occurrence of R2 , . . . , Rk in C, R, and K with R1 , and add Trans(R1 ) if, before the cycle
elimination, we had Trans(Ri ) for some i with 1  i  n.
Before we turn our attention to the construction of a tableau algorithm for SHOQK(D),
let us comment on a few minor differences between SHOQK(D) as introduced here and
the original version of SHOQ(D) as described in (Horrocks & Sattler, 2001). The main
difference is that our logic, like the extensions investigated in (Haarslev et al., 2001; Pan &
Horrocks, 2002), allows n-ary predicates while Horrocks and Sattler restrict themselves to
unary predicates. Moreover, SHOQ(D) as introduced in (Horrocks & Sattler, 2001) uses
concrete roles rather than concrete features, the difference being that concrete roles are not
necessarily functional. Due to this non-functionality, the original SHOQ(D) admits two
variants T.P and T.P of the concrete domain constructor (where T is a concrete role and
P a unary predicate). In SHOQK(D), we can simulate the universal variant by writing
g.P t g since concrete features g are interpreted as partial functions and, in contrast to
Horrocks and Sattler, we have the undefinedness constructor g available. Except for the
n-ary predicates which provide important additional expressivity, we view these deviations
as minor ones since it is easy to see that they do not affect decidability and complexity of
reasoning.
4.2.2 A Tableau Algorithm for SHOQK(D)
The basic intuitions of the SHOQK(D) tableau algorithm are similar to the ALCOK(D)
algorithm, with one exception: to deal with the various expressive means of SHOQK(D),
700

fiKeys, Nominals and Concrete Domains

(> n R C)
(> 0 R C)
(6 n R C)

(6 (n  1) R C) if n  1

(> (n + 1) R C)

Figure 10: The SHOQK(D) NNF rewrite rules.
it is convenient to introduce a certain abstraction of models, so-called tableaux. The main
difference between tableaux and models is that, in tableaux, roles declared to be transitive
are not necessarily described by transitive relations. We show that there exists a tableau
for a given concept and key box if and only if they have a common model. The aim of the
SHOQK(D) algorithm is then to construct a tableau for its input rather than trying to
construct a model. To do this, the algorithm employs completion forests as its underlying
data structure.
We first introduce tableaux. Let us start by discussing some preliminaries. As for
ALCOK(D), we assume all concepts and key boxes to be in NNF, i.e. negation occurs only
in front of concept names and nominals. We again use  C to denote the NNF of C. The
additional NNF rewrite rules for SHOQK(D) can be found in Figure 10 and complete those
given for ALCOK(D) in Figure 7.
For a concept D, role box R, and key box K, we define
cl(D, K) := sub(D)  sub(con(K))  {C
 | C  sub(D)  sub(con(K))}
cl(D, R, K) := cl(D, K)  {R.C | R v
* S and S.C  cl(D, K)}.
Obviously, the cardinality of cl(D, R, K) is linear in the size of D, R, and K. In what
D,K
to denote the set of role names occurring in D, R, or K, and NcF
follows, we write ND,R,K
R
to denote the sets of concrete features occurring in D or K. We are now ready to define
tableaux.
Definition 4.8 (Tableau). Let D be a SHOQK(D)-concept in NNF, R a role box, and K
a path-free key box in NNF. A tableau T for D w.r.t. R and K is a tuple (Sa , Sc , L, E, e, P)
such that
 Sa , Sc are sets of abstract and concrete individuals,
 L : Sa  2cl(D,R,K) maps each abstract individual to a subset of cl(D, R, K),
D,R,K

 E : Sa  Sa  2NR

maps pairs of abstract individuals to sets of roles,

 e : Sa ND,K
cF  Sc maps pairs of abstract individuals and concrete features to concrete
individuals,
 P maps each n-ary concrete predicate in cl(D, R, K) to a set of n-tuples over Sc ,
 there is an abstract individual s0  Sa such that D  L(s0 ), and
for all s, t  Sa , C, C1 , C2  cl(D, R, K), R, S  ND,R,K
, and for
R
S T (s, C) := {t  Sa | S  E(s, t) and C  L(t)},
it is the case that:
701

fiLutz, Areces, Horrocks, & Sattler

(T1) if C  L(s), then  C 
/ L(s),
(T2) if C1 u C2  L(s), then C1  L(s) and C2  L(s),
(T3) if C1 t C2  L(s), then C1  L(s) or C2  L(s),
(T4) if R  E(s, t) and R v
* S, then S  E(s, t),
(T5) if R.C  L(s) and R  E(s, t), then C  L(t),
(T6) if R.C  L(s), then there is some t  Sa such that R  E(s, t) and C  L(t),
(T7) if S.C  L(s) and R  E(s, t) for some R v
* S with Trans(R), then R.C  L(t),
(T8) if (> n S C)  L(s), then ]S T (s, C) > n,
(T9) if (6 n S C)  L(s), then ]S T (s, C) 6 n,
(T10) if either (6 n S C)  L(s) and S  E(s, t) or (g1 , . . . , gn keyfor C)  K and e(t, gi )
is defined for all 1  i  n, then {C,  C}  L(t) 6= ,
(T11) if N  L(s)  L(t), then s = t,
(T12) if g1 , . . . , gn .P  L(s), then there are x1 , . . . , xn  Sc with e(s, gi ) = xi and
(x1 , . . . , xn )  P(P ),
V
V
V
(T13) P used in D,K (x1 ,...,xn )P(P ) P (x1 , . . . , xn )  x6=y x 6= y is satisfiable,
(T14) if (g1 , . . . , gn keyfor C)  K, C  L(s)  L(t), and e(s, gi ) = e(t, gi ) for all 1  i  n,
then s = t,
(T15) if g  L(s), then e(s, g) is undefined.
Note that the predicate conjunction in (T13) uses a binary inequality predicate. In general,
we do not require the concrete domain D to be equipped with such a predicate and thus this
predicate conjunction is not necessarily a D-conjunction. However, it is nevertheless safe
to use (T13) in the given form since tableaux are only used in proofs and we do not need
a concrete domain reasoner that is capable of deciding the satisfiability of this conjunction.
The following lemma, whose proof is provided in Appendix C, shows that our definition of
tableaux provides an adequate abstraction of models.
Lemma 4.2. Let D be a SHOQK(D)-concept in NNF, R a role box, and K a key box in
NNF. Then D is satisfiable w.r.t. R and K iff D has a tableau w.r.t. R and K.
Given Lemma 4.2, in order to decide satisfiability of SHOQK(D)-concepts w.r.t. role and
key boxes, we may use a (tableau) algorithm that tries to construct a tableau for the input.
In the following, we will describe such an algorithm in detail.
As in the previous section, the algorithm works on completion systems. However, in
the case of SHOQK(D) the core component of completion systems is a completion forest
rather than a completion tree. The reason for this is that some completion rules remove
nodes and edges from the completion system and in this way can disconnect one tree into
two subtrees.
702

fiKeys, Nominals and Concrete Domains

Definition 4.9 (Completion System). Let D be a SHOQK(D)-concept in NNF, R a role
box, and K a path-free key box in NNF. For each concept (> n R C)  cl(D, R, K) and
1  i  n, we reserve a concept name AnRC
not appearing in cl(D, R, K) and define an
i
extended closure
nRc
cl+ (D, R, K) := cl(D, R, K)  {AnRc
| (> n R C)  cl(D, R, K)}.
1 , . . . , An

Let Oa and Oc be disjoint and countably infinite sets of abstract and concrete nodes. A
completion forest for D, R, and K is a finite forest F = (Va , Vc , E, L) with nodes Va  Vc
such that Va  Oa , Vc  Oc , and all nodes from Vc are leaves. The forest is labelled as
follows:
 each node a  Va is labelled with a subset L(a) of cl+ (D, R, K),
 each edge (a, b)  E with a, b  Va is labeled with a non-empty set of role names
L(a, b) occurring in D, R, or K, and
 each edge (a, x)  E with a  Va and x  Vc is labeled with a concrete feature L(a, x)
occurring in D, R, or K.
A completion system for D, R, and K is a tuple S = (F, P, c , ) such that
 F = (Va , Vc , E, L) is a completion forest for D, R, and K,
 P maps each n-ary concrete predicate in cl(D, R, K) to a set of n-tuples in Vc ,
 c is an equivalence relation on Vc , and
  is a linear ordering on Va .
A node t  Va is called an R-successor of a node s  Va if, for some R0 with R0 v
* R, we have
R0  L(s, t). A node x  Vc is called a g-successor of a node s  Va if L(s, x) = g. Finally,
.
we write s =
6 t if s and t are R-successors of the same node and there is some AnRC
 L(s)
i
nRC
and Aj
 L(t) with i 6= j.
Some remarks are in order here. Firstly, in contrast to the ALCOK(D) case, the relation
 is no longer required to respect the level of a node. This is due to the fact that (a)
we have to enforce termination artificially and the mentioned property of  was used to
ensure automatic termination, and (b) the level of a node might change since a node
might become a root node because some completion rules will remove nodes and edges.
Secondly, the relation c will be returned by a D-tester, and is used to compute a
relation a which is then used by the tableau algorithm. However, we do not need to
compute the relation c from a as in the ALCOK(D) case since all concepts and key
boxes are assumed to be path-free.
Thirdly, the new concept names AnRC
will be used to ensure that successors of a node
i
x generated for some (> n R C)  L(x) will not be merged later due to a concept (6
n0 R C 0 )  L(x): each generated successor is labelled with a different concept AnRC
;
i
since merging two nodes means unifying their node labels, it then suffices to disallow the
703

fiLutz, Areces, Horrocks, & Sattler

occurrence of distinct concepts AnRC
in the same node label through a suitable definition
i
of clash.
Since SHOQK(D) provides for transitive roles, we need some cycle-detection mechanism
in order to guarantee termination of our algorithm: roughly speaking, if we encounter a
node which is similar to an already existing one, then this node does not need to be
further explored. Speaking in terms of (Horrocks et al., 2000; Baader & Sattler, 2000), we
employ a mechanism called subset blocking.
Definition 4.10 (Blocked). Let  be the reflexive closure of . A node t  Va is blocked
by a node s  Va if L(t)  L(s), and s  s0 , for all s0 with L(t)  L(s0 ).
Note that, unlike to what is done, e.g., in (Horrocks et al., 2000), the blocking node is not
necessarily an ancestor of the blocked node, but can be anywhere in the forest. It may even
be that blocked nodes have unblocked successors. This modification is used later to obtain
a NExpTime upper bound.
To decide the satisfiability of an ALCOK(D)-concept D w.r.t. a role box R and a pathfree key box K (where D and K are in NNF), the tableau algorithm is started with the
initial completion system
SD = (FD , P , , ), where
FD = ({s0 }, , , {s0 7 {D}}) and
P
maps each P  D occurring in D and K to .
Then the algorithm repeatedly applies completion rules. Before the actual rules are given,
we introduce some new notions: firstly, we define the equivalence relation a over Va as
follows: s a t if one of the following conditions is satisfied:
 N  L(s)  L(t) for some nominal N or
 (g1 . . . , gn keyfor C)  K, C  L(s)L(t), and there are xi , yi such that gi  E(s, xi )
E(t, yi ) and xi c yi for 1  i  n.
Intuitively, two abstract nodes related via the a relation describe the same individual in
a tableau.
Secondly, we use the following abbreviations in the formulation of the rules (written in
italic):
 To remove an abstract node s and all its incoming and outgoing edges, remove s from
Va and each (s, t) and (t, s) from E for all t  Va  Vc .
 Adding a g-successor of an abstract node s means doing nothing if there exists a
g-successor x  Vc of s and, otherwise, adding E(s, x) = g for some x  Oc that does
not yet occur in the completion forest.
 To update the relation c , a D-tester is asked to decide the satisfiability of the Dconjunction
^
^
S :=
P (x1 , . . . , xn ) 
x=y
P used in D,K
(x1 ,...,xn )P(P )

xc y

and returns, in case that this conjunction is satisfiable, an updated concrete equivalence c as defined in Definition 4.1.
704

fiKeys, Nominals and Concrete Domains

Concerning the predicate conjunction used in updates, recall that we can w.l.o.g. assume
the concrete domain to contain an equality predicate as discussed after Definition 4.1.
The completion rules are given in Figure 11. We generally assume that new nodes x
are introduced into the completion forest such that y  x for all already existing nodes y.
Before further describing the tableau algorithm, we comment on the completion rules. The
rules Rt, R6, Rc, and Rch are non-deterministic, i.e., their application has more than one
possible outcome. For the Rc rule, this is due to the update operation performed on c
using the D-tester: as discussed at the end of Section 4.1, computing a concrete equivalence
for a given D-conjunction may result in a high degree of non-determinism. Please note that,
in contrast to ALCOK(D), we now only need to call the D-tester in this ruleand not after
each rule application.
Next, the Ra rule takes care of abstract nodes related via a . Since all the nodes from
a a -equivalence class denote the same individual, we choose only one representative whose
node label contains the labels of all other nodes in the class. This representative simply
is the -minimal node of the equivalence class and the Ra rule performs the appropriate
copying of node labels.
The R6 rule is the only rule to remove nodes and edges: it removes a surplus R-successor
t of a node s with (6 n R C)  L(s). Since the subtree below t is not removed, ts successors
are new, additional root nodes. This behavior is the reason why we work on a completion
forest.
As in the ALCOK(D) case, the tableau algorithm stops applying rules if it finds an
obvious contradiction, a clash, or if no more completion rules are applicable.
Definition 4.11 (Clash). Let S = (F, P, c , ) be a completion system for D, R and K,
and F = (Va , Vc , E, L). Then S is said to contain a clash if one of the following conditions
applies:
(C1) for some concept name A  NC and some node s  Va , {A, A}  L(s);
(C2) the D-conjunction S defined above is not satisfiable;
.
(C3) s =
6 s for some s  Va ;
(C4) for some s  Va and g  NcF , we have g  L(s) and s has a g-successor.
A completion system not containing a clash is called clash-free. The completion system is
complete if none of the completion rules is applicable.
Due to the simplicity of the algorithm, we refrain from describing it in pseudo-code notation: the algorithm starts with the initial completion system and then repeatedly applies
the completion rules, checking for clashes after each rule application. If a clash is detected, it returns unsatisfiable. If a complete and clash-free completion system is found,
then the algorithm returns satisfiable. Note that, since some of the completion rules are
non-deterministic, the algorithm is also non-deterministic.
Details of the proof of termination, soundness, and completeness are given in Appendix C. Unfortunately, we have to leave the complexity of the algorithm as an open
problem: it is not hard to prove that it runs in double exponential time, but it is not clear
whether exponential time also suffices. However, we can still use the algorithm to obtain a
705

fiLutz, Areces, Horrocks, & Sattler

Ru

if C1 u C2  L(s), s is not blocked, and {C1 , C2 } 6 L(s),
then L(s) := L(s)  {C1 , C2 }

Rt

if C1 t C2  L(s), s is not blocked, and {C1 , C2 }  L(s) = ,
then L(s) := L(s)  {C} for some C  {C1 , C2 }

R

if R.C  L(s), s is not blocked, and s has no R-successor t with C  L(t)
then create a new node t such that t0  t for all t0  Va
and set E(s, t) := {R} and L(t) := {C}

R>

if (> n S C)  L(s), s is not blocked, and there are no n S-successors
.
t1 , . . . , tn of s with C  L(ti ) and ti =
6 tj for 1  i < j  n,
then create n new nodes t1 , . . . , tn s.t. t0  ti for 1  i  n and all t0  Va ,
and set E(s, ti ) := {S} and L(ti ) := {C, AnSC
} for 1  i  n
i

R6

if (6 n S C)  L(s), s is not blocked, s has n + 1 S-successors t0 , . . . , tn
with C  L(ti ) for 0  i  n,
then choose i, j such that ti  tj , set L(ti ) := L(ti )  L(tj ),
L(s, ti ) := L(s, ti )  L(s, tj ), and remove tj and all its incoming
and outgoing edges

Rc

if g1 , . . . , gn .P  L(s), s is not blocked, and
there are no gi -successors xi with (x1 , . . . , xn )  P(P )
then add a gi -successor of s for each 1  i  n,
for yi the gi -successor of s, add (y1 , . . . , yn ) to P(P ), and
update c

R

if R.C  L(s), s is not blocked, and
there is an R-successor t of s with C 
/ L(t),
then L(t) := L(t)  {C}

R+

if S.C  L(s), s is not blocked, there is some R with
Trans(R) and R v
* S, and an R-successor t of s with R.C 
/ L(t),
then L(t) := L(t)  {R.C}

Rch

if s is an S-successor of s0 and (6 n S C)  L(s0 ) or
s has gi -successors xi for all 1  i  n and (g1 , . . . gn keyfor C)  K and
s is not blocked and {C,  C}  L(s) = ,
then L(s) := L(s)  {E} for some E  {C,  C}

Ra

if s a t, L(t) 6 L(s), s  t, and s is not blocked,
then set L(s) := L(s)  L(t)
Figure 11: The completion rules for SHOQK(D).

706

fiKeys, Nominals and Concrete Domains

tight complexity bound for SHOQK(D): the following corollary is an easy by-product of
the correctness proofs (for a proof see again Appendix C).
Corollary 4.3. If a SHOQK(D)-concept D is satisfiable w.r.t. a role box R and a path-free
key box K, then D is satisfiable w.r.t. R and K in a model of size at most |I |  2m for
m = # cl+ (D, R, K).
Thus the following is an alternative algorithm for deciding satisfiability of a SHOQK(D)concept D w.r.t. a role box R and a path-free key box K: first, guess an interpretation
I with cardinality of I bounded by 2m , using placeholder variables from Oc instead of
concrete values in the interpretation of concrete features. Let Vc be the set of variables
from Oc occuring in I. Additionally guess an interpretation P for the concrete domain
predicates: just as in completion forests, P maps each n-ary concrete predicate used in D or
K to an n-ary relation on Vc . Then perform standard (polynomial-time) model checking to
ensure that I is a model of D. In doing this, treat concepts of the form g1 , . . . , gn .P using
the interpretation of predicates P. It is easily checked in polynomial time that I is also a
model of R and Kfor the latter, assume that all placeholder variables stand for different
values. Finally, use the concrete domain D-tester to check whether the conjunction
^
P (x1 , . . . , xn )
P used inD,K
(x1 ,...,xn )P(P )

is satisfiable. Answer yes if it is and no otherwise. Since this algorithm can clearly be
implemented in NExpTime provided that there is a D-tester running in non-deterministic
polynomial time, we obtain the following:
Theorem 4.4. Let D be a key-admissible concrete domain such that extended D-satisfiability
is in NP, then SHOQK(D)-concept satisfiability w.r.t. TBoxes, role boxes, and path-free
key boxes is in NExpTime.

5. Conclusion
In this paper, we have identified key constraints as an interesting extension of description
logics with concrete domains. Starting from this observation, we introduced a number of
natural description logics and provided a comprehensive analysis of the decidability and
complexity of reasoning. The main observation of our investigations is that key boxes can
have dramatic consequences on the complexity of reasoning: for example, the PSpacecomplete DL ALC(D) becomes NExpTime-complete if extended with path-free, unary,
Boolean key boxes and undecidable if extended with path-free, unary, non-Boolean key
boxes. Thus the effect of our key boxes on the complexity are quite different from the effect
of key assertions where only abstract features are allowed (Calvanese et al., 2000): these
abstract key assertions can be said to be for free since they do not increase the complexity
for expressive description logics.
We show that the restriction to Boolean key boxes (in the ALCOK(D) case) and to
path-free key boxes (in the SHOQK(D) case) yield decidabile and NExpTime-complete
reasoning problems. We selected ALC(D) and SHOQ(D) as the basis for our analysis since,
707

fiLutz, Areces, Horrocks, & Sattler

in our opinion, these are the most fundamental description logics with concrete domains.
Going one step further, it would be interesting to combine key boxes with other extensions
of concrete domains, such as the ones presented by Lutz (2003, 2004). To name only one
possibility, the extension of both ALCOK(D) and SHOQ(D) with inverse roles seems to
be a natural idea. Note that inverse roles interact with several of the available means of
expressivity: while ALC with inverse roles is PSpace complete (Horrocks, Sattler, & Tobies,
1999), ALCO with inverse roles is ExpTime-complete (Areces et al., 1999) and ALC(D)
with inverse roles even NExpTime-complete (Lutz, 2004).
Other options for future research are more closely related to the material presented in
this paper. For example, is SHOQK(D)-concept satisfiability still decidable if we drop
the requirement of key boxes to be path-free? Moreover, we had to leave the exact time
requirements of our tableau algorithm as an open problem. If this algorithm runs in (nondeterministic) exponential time, it directly yields Theorem 4.4 rather than via a bounded
model property.

Acknowledgments
We would like to thank the anonymous reviewers for valuable comments. This paper is an
extended version of (Lutz, Areces, Horrocks, & Sattler, 2003).

Appendix A. Proofs of Section 3.2
We prove that D2 -satisfiability can be decided in PTime.
Proposition A.1. D2 -satisfiability is in PTime.
Proof. Let c be a D2 -conjunction. We show that c is satisfiable iff none of the following
conditions applies:
1. c contains a conjunct D2 (x);
2. c contains conjuncts bit0ik (x) and bit1ik (x);
3. c contains conjuncts bitnik (x) and bitmj` (x) with k 6= `;
4. c contains conjuncts bitnik (x) and bitnik (x);
5. c contains conjuncts bitnik (x), bit0jk (x), and bit1jk (x).
It is easily seen that c is unsatisfiable if one of the conditions applies. Assume now that
Conditions 1 to 5 do not apply to c and let X be the set of variables used in c. For each
x  X, set t(x) = k if bitnik (x)  c for some n, i  .6 If bitnik (x) 
/ c for all n, i, k  , then
set t(x) = r for some r not appearing as an index r to a predicate in c. The mapping t is
well-defined since c is finite, Condition 3 does not apply, and the only predicates available
are bitnik (), D2 (), and >D2 (). We define a solution  for c as follows: for each x  X, set
(x) to the bit vector v  BVt(x) in which the ith bit is 1 if bit1it(x) (x)  c or bit0it(x) (x)  c,
and 0 otherwise. It remains to prove that  is indeed a solution for c:

N

6. We use P (x)  c as an abbreviation for P (x) is a conjunct in c.

708

N

fiKeys, Nominals and Concrete Domains

 Let bit0ik (x)  c. Then t(x) = k and thus (x)  BVk . Since Condition 2 does
/ c. Moreover, non-applicability of Condition 4 implies
not apply, we have bit1ik (x) 
i
/ c. By definition of , the ith bit of (x) is thus 0.
bit0k (x) 
 Let bit1ik (x)  c. Then t(x) = k and (x)  BVk . By definition of , the ith bit of
(x) is 1.
 Let bit0ik (x)  c. If t(x) 6= k, then (x) 
/ BVk . Thus (x)  (bit0ik )D2 and we are
done. If t(x) = k, then the ith bit of (x) is 1 by definition of  and thus again
(x)  (bit0ik )D2 .
/ BVk and we are done. If t(x) = k, then
 Let bit1ik (x)  c. If t(x) 6= k, then (x) 
j
bitnk (x)  c for some n, j  . Since Condition 5 does not apply, we thus have

N

/ c. Thus, by

/ c. Moreover, non-applicability of Condition 4 yields bit1ik (x) 
definition of , the ith bit of (x) is 0.

bit0ik (x)

It is obvious that the listed properties can be checked in polynomial time.

Appendix B. Proofs of Section 4.1
We prove termination, soundness, and completeness of the ALCOK(D) tableau algorithm
presented in Section 4.1, starting with termination. We start with establishing a few notions
and technical lemmas.
Let C be a concept and K a key box. We use |C| to denote
P the length of C, i.e. the
number of symbols used to write it down, and |K| to denote (u1 ,...,uk keyfor C)K |C|. For
a path u = f1    fk g, we use |u| to denote k + 1. The role depth of concepts is defined
inductively as follows:
rd(A) = rd(N ) = rd(g) = 0
rd(u1 , . . . , un .P ) = max{|ui | | 1  i  n}  1
rd(C u D) = rd(C t D) = max{rd(C), rd(D)}
rd(R.C) = rd(R.C) = rd(C) + 1.
The following series of lemmas will eventually allow us to prove termination.
Lemma B.1. There is a constant k such that, if the tableau algorithm is started on input
C0 , K and T = (Va , Vc , E, L) is a completion tree constructed during the run of the algorithm,
k
k
then #Va  2|C0 | and #Vc  2|C0 | .
Proof. Using induction on the number of rule applications and a case distinction according
to the applied rule, it is straightforward to show that
C  L(a) implies rd(C)  |C0 |  levT (a)

()

for all constructed completion trees T. We omit the details but note that, (1) for treating
the Rch rule, one needs to employ the fact that K is Boolean and thus only adds concepts
of role depth 0 to node labels, and (2) for treating the Rp rule, we use that a  b implies
levT (a)  levT (b).
709

fiLutz, Areces, Horrocks, & Sattler

This implies an upper bound on the depth of constructed completion trees: first, only
the R and Rc rules generate new nodes, and an application of either rule to a node
a  Va implies L(a) 6=  and thus levT (a)  |C0 | by (). Second, each new (abstract or
concrete) node b generated by an application of these rules to a node a  Va clearly satisfies
levT (b)  levT (a) + max(1, mpl(C0 )), where mpl(C0 ) denotes the maximum length of paths
in C0 (note that concepts in K may not contain any paths since it is Boolean). Since
mpl(C0 )  |C0 |, the above observations imply that the depth of constructed completion
trees is bounded by 2  |C0 |.
Now for the out-degree. If a node a is generated, then this is due to the application of
a rule R or Rc and, initially, a has at most one successor. Let us analyze the number
of successors generated by later applications of the rules R and Rc to a: these rules
can be applied at most once for each concept of the form R.C or u1 , . . . , un .P in L(a).
By definition of cl(C0 , K) and since K is Boolean, the number of such concepts per node
label is bounded by #sub(C0 )  |C0 |. Moreover, each rule application creates at most |C0 |
successors. Hence, the out-degree of constructed completion trees is bounded by |C0 |2 + 1.

Lemma B.2. There is a constant k such that, if the tableau algorithm is started with C0 , K,
k
then, in every recursion step, the while loop terminates after at most 2|C0 | steps.
Proof. Fix an argument S = (T, P, , ) with T = (Va , Vc , E, L) passed to the sat function,
let 1 , 2 , . . . be the sequence of concrete equivalences computed in the while loop, and let
1c , 2c , . . . be the corresponding c relations. Since test(S ) calls a D-tester, each of these
calls indeed terminates.
We show that
1 ( 2 (    ,
()
k

which implies Lemma B.2: by Lemma B.1, there exists a constant k such that #Vc  2|C0 | .
k
Hence, we have #i  22|C0 | which, together with (), implies that the number of steps
k
performed by the while loop is also bounded by 22|C0 | .
Now for the proof of (). If the while loop reaches the i-th step, then we had i1 6= i1
c
after step i  1. Since i1  i1
by definition, this implies i1 ( ci1 . By definition
c
of S , it is easy to see that i1
 i for i  1. Hence i1 ( i .
c
Lemma B.3. There is a constant k such that, if the tableau algorithm is started with C0 , K,
k
then the number of recursive calls is bounded by 2(|C0 |+|K|) .
Proof. It obviously suffices to establish an appropriate upper bound on the number of rule
applications. The Ru, Rt, R, and Rc rules can be applied at most once for each concept
in a node label. By Lemma B.1, the number of nodes is at most exponential in |C0 | + |K|.
Since neither nodes nor concepts in node labels are ever deleted, the fact that node labels
are subsets of cl(C0 , K) thus implies that the number of applications of these rules is at most
exponential in |C0 | + |K|. The same holds for the rules R and Rp, which can be applied
at most once for every concept C  cl(C0 , K) and every pair of (abstract) nodes. Finally,
the number of Rch applications is at most exponential in |C0 | + |K| since this rule can be
applied at most once for every abstract node and every key assertion in K.
710

fiKeys, Nominals and Concrete Domains

Termination is now an obvious consequence of Lemmas B.2 and B.3.
Corollary B.4 (Termination). The tableau algorithm terminates on any input.
Let us now prove soundness of the algorithm.
Lemma B.5 (Soundness). If the tableau algorithm returns satisfiable, then the input concept
C0 is satisfiable w.r.t. the input key box K.
Proof. If the tableau algorithm returns satisfiable, then there exists a complete and clashfree completion system S = (T, P, , ) for C0 . Let T = (Va , Vc , E, L). By definition of
the tableau algorithm, there is a completion system S 0 = (T, P, , 0 ) such that a call to
test(S 0 ) returned . Moreover, we have  = c in S. Thus, there exists a solution  for
S 0 such that
(x) = (y) iff x c y.
()
0
Clearly,  is also a solution for S : since
^ the second
^ component P is the same in S and S ,
 is a solution for the first part
P (x1 , . . . , xn ) of S . Moreover, for
P used in C (x1 ,...,xn )P(P )

each conjunct =(x, y) from the second part of S , we have x c y by definition of S and
thus (x) = (y) by ().
We now use S and  to construct an interpretation I by setting
I

= {a  Va | there is no b  Va such that a a b and b  a}  {w}

AI

= {a  I | A  L(a)}

{a  I | N  L(a)} if there is an a  I such that N  L(a)
=
{w}
otherwise

NI
RI

= {(a, b)  I  I | there are a0 , b0  Va such that a a a0 , b a b0 , and
b0 is an R-successor of a0 }

gI

= {(a, (x))  I  D | x is a g/a -neighbor of a}

for all A  NC , N  NO , R  NR , and g  NcF . We first show that I is well-defined:
 N I is a singleton for each N  NO . Assume that there exist a, b  I such that a 6= b
and N  L(a)  L(b). By definition of a (Definition 4.3), N  L(a)  L(b) implies
a a b. This, together with a, b  I , yields a  b and b  a, contradicting  being
a linear ordering.
 f I is functional for each f  NaF . Assume that there exist a, b, c  I such that
{(a, b), (a, c)}  f I and b 6= c. Then there exist a1 , a2 , b0 , c0  Va such that a a a1 a
a2 , b a b0 , c a c0 , b0 is an f -successor of a1 , and c0 is an f -successor of a2 . By
definition of a , we thus have b0 a c0 implying b a c. Since b, c  I , this yields
b  c and c  b, a contradiction.
 g I is functional for each g  NcF . Assume that there exist an a  I and x, y  Vc
such that {(a, (x)), (a, (y))}  f I and (x) 6= (y). Then x and y are both g/a neighbors of a. By definition of c , we thus have x c y implying (x) = (y) by (),
a contradiction.
711

fiLutz, Areces, Horrocks, & Sattler

Now we show the following claim. In the proof, we use the notion of f1    fk /a -neighbors
(with f1 , . . . fk abstract features), which is defined analogously to u/a -neighbors for paths u.
Claim 1: For all a  I and all paths u, we have uI (a) =  iff there is a ui / a -neighbor
x of a with (x) = .
Proof: Let u = f1    fk g. Using induction on i it is easily proved that, for all i and all
b  I , we have fiI (   (f1I (a))    ) = b iff there is a f1    fi /a -neighbor b0 of a with
b a b0 . Thus we have in particular that fkI (   (f1I (a))    ) = b iff there is an f1    fk /a neighbor b0 of a with b a b0 . To prove the claim, it hence remains to use the definition
of g I together with ().
The following claim is central for showing that I is a model for C0 and K.
Claim 2: For all a  I and C  cl(C0 , K), if C  L(a), then a  C I .
Since C0 is in the label of the root node, Claim 2 clearly implies that I is a model for C0 .
Moreover, we can use it to prove that I satisfies all key assertions (u1 , . . . , un keyfor C) in
K: fix a, b  C I such that uIi (a) = uIi (b) for 1  i  n. Non-applicability of Rch yields
{C,  C}  L(a) 6= . If  C  L(a), then Claim 2 implies a  ( C)I in contradiction to
a  C I . Thus we obtain C  L(a). In an analogous way, we can argue that C  L(b). Since
uIi (a) and uIi (b) are defined for 1  i  n, Claim 1 yields that a has a ui /a -neighbor xi
with (xi ) = uIi (a) and b a ui /a -neighbor yi with (yi ) = uIi (b) for 1  i  n. Thus the
fact that uIi (a) = uIi (b) yields (xi ) = (yi ) for 1  i  n. By () we obtain xi c yi and
thus xi  yi for 1  i  n. By definition of a , we thus get a a b. Since a, b  I , we
obtain a 6 b and b 6 a by definition of I and thus a = b.
It remains to prove Claim 2, which we do using structural induction:
 C is a concept name or a nominal. Easy by construction of I.
 C = D. Since C  cl(C0 , K), C is in NNF and D is a concept name. Since S is
clash-free, C  L(a) implies D 
/ L(a). Thus, a 
/ DI by construction of I, which
I
yields a  (D) .
 C = u1 , . . . , un .P . Since the Rc rule is not applicable, there exist x1 , . . . , xn  Vc
such that xi is a ui /a -neighbor of a for 1  i  n and (x1 , . . . , xn )  P(P ). Claim 1
yields uIi (a) = (xi ) for 1  i  n. Since (x1 , . . . , xn )  P(P ) and  is a solution for
S , we have ((x1 ), . . . , (xn ))  P D and thus a  C I .
 C = g. Since S is clash-free, there exists no x  Vc such that x is g/a -neighbor of
a. Thus by Claim 1 there is no  such that (a, )  g I .
 C = D u E or C = D t E. Straightforward using completeness and the induction
hypothesis.
 C = R.D. Since the R rule is not applicable, a has an R/a -neighbor b such that
D  L(b). Let b0 be minimal w.r.t.  such that b a b0 . By definition of I, we have
(a, b0 )  RI . Non-applicability of the Rp rule yields D  L(b0 ). By induction, we get
b0  DI and thus a  C I .
712

fiKeys, Nominals and Concrete Domains

 C = R.D. Let (a, b)  RI . By definition of I, this implies that there exist a0 , b0  Va
such that a is minimal w.r.t.  and a a a0 , b is minimal w.r.t.  and b a b0 , and b0
is an R-successor of a0 . Since b0 is clearly an R/a -neighbor of a, non-applicability of
R yields D  L(b0 ), which implies D  L(b) due to the non-applicability of Rp. By
induction, we get b  DI . Since this holds independently of the choice of b, we obtain
a  (R.D)I .

Lemma B.6 (Completeness). If the input concept C0 is satisfiable w.r.t. the input key box
K, then the tableau algorithm returns satisfiable.
Proof. Let I be a model of C0 and K. We use I to guide (the non-deterministic parts
of) the algorithm such that it constructs a complete and clash-free completion system. A
completion system S = (T, P, , ) with T = (Va , Vc , E, L) is called I-compatible if there
exist mappings  : Va  I and  : Vc  D such that
(Ca) C  L(a)  (a)  C I
(Cb) b is an R-successor of a  ((a), (b))  RI
(Cc) x is a g-successor of a  g I ((a)) =  (x)
(Cd) (x1 , . . . , xn )  P(P )  ( (x1 ), . . . ,  (xn ))  P D
(Ce) x  y   (x) =  (y).
We first establish the following claim:
Claim 1: If a completion system S is I-compatible, then (i) a a b implies (a) = (b)
and (ii) x c y implies  (x) =  (y).
Proof: We show by induction on i that a ia b implies (a) = (b) (see Definition 4.3),
which yields (i).
 Start. If a 0a b, then there exists a nominal N such that N  L(a)  L(b). By (Ca)
we obtain (a)  N I and (b)  N I , which yields (a) = (b) by definition of the
semantics.
 Step. For a ia b, we distinguish three cases:
1. If a i1
b, then (a) = (b) by induction.
a
2. There is a c  Va and an f  NaF such that both a and b are f /i1
a -neighbors of
i1 c , a is an f -successor
c. Hence, there exist c1 , c2  Va such that c i1
c

1
2
a
a
of c1 , and b is an f -successor of c2 . By induction, we have (c) = (c1 ) = (c2 ).
Thus (Cb) yields {((c), (a)), ((c), (b))}  f I , which implies (a) = (b) by
definition of the semantics.
3. There exist (u1 , . . . , un keyfor C)  K, ui /ai1 -neighbors xi of a and ui /ai1 neighbors yi of b for 1  i  n such that C  L(a)L(b) and xi  yi for 1  i  n.
(Ca) yields a, b  C I . Using induction, (Cb), and (Cc), it is straightforward to
713

fiLutz, Areces, Horrocks, & Sattler

show that uIi ((a)) =  (xi ) and uIi ((b)) =  (yi ) for 1  i  n. By (Ce), this
implies uIi ((a)) = uIi ((b)) for 1  i  k. Since I is a model of the key box K,
this yields (a) = (b) by definition of the semantics.
Now for Part (ii) of Claim 1. If x c y, then either x  y or there is an a  Va and a g  NcF
such that both x and y are g/a -neighbors of a. In the former case, (Ce) yields  (x) =  (y).
In the latter case, Part (i) of the claim and (Cc) yields {((a),  (x)), ((a),  (y))}  g I which
implies  (x) =  (y). This finishes the proof of Claim 1.
We can now show that the completion rules can be applied such that I-compatibility is
preserved.
Claim 2: If a completion system S is I-compatible and a rule R is applicable to S, then R
can be applied such that an I-compatible completion system S 0 is obtained.
Proof: Let S be an I-compatible completion system, let  and  be functions satisfying
(Ca) to (Ce), and let R be a completion rule applicable to S. We make a case distinction
according to the type of R.
Ru The rule is applied to a concept C1 u C2  L(a). By (Ca), C1 u C2  L(a) implies
(a)  (C1 u C2 )I and hence (a)  C1I and (a)  C2I . Since the rule adds C1 and
C2 to L(a), it yields a completion system that is I-compatible via the same  and  .
Rt The rule is applied to C1 tC2  L(a). C1 tC2  L(a) implies (a)  C1I or (a)  C2I .
Since the rule adds either C1 or C2 to L(a), it can be applied such that it yields a
completion system that is I-compatible via the same  and  .
R The rule is applied to a concept R.C  L(a), generates a new R-successor b of a and
sets L(b) = {C}. By (Ca), we have (a)  (R.C)I and, hence, there exists a d  I
such that ((a), d)  RI and d  C I . Set  0 :=   {b 7 d}. It is readily checked
that the resulting completion system is I-compatible via  0 and  .
R The rule is applied to a concept R.C  L(a) and adds C to L(b) of an existing
R/a -neighbor b of a. Hence, there exists an a0 such that a a a0 and b is an Rsuccessor of a0 . By Part (i) of Claim 1, we have (a) = (a0 ). Thus, by (Ca) we have
(a0 )  (R.C)I while (Cb) yields (((a0 ), (b))  RI . By definition of the semantics,
(b)  C I and thus the resulting completion system is I-compatible via  and  .
(i)

(i)

Rc The rule is applied to a concept u1 , . . . , un .P  L(a) with ui = f1    fki gi for
(i)

1  i  n. The rule application generates new abstract nodes bj and xj for 1  i  n
and 1  j  ki such that
(i)

(i)

(i)

(i)

 b1 is an f1 -successor of a for 1  i  n,
(i)

 bj is an fj -successor of bj1 for 1  i  n and 2  j  ki ,
(i)

 xi is gi -successor of bki for 1  i  n, and
 (x1 , . . . , xn )  P(P ).
714

fiKeys, Nominals and Concrete Domains

(i)

By (Ca), we have (a)  (u1 , . . . , un .P )I . Hence, there exist dj  I for 1  i  n
and 1  j  ki and 1 , . . . , n  D such that
(i)

(i)

(i)

(i)

 ((a), d1 )  (f1 )I for 1  i  n,
(i)

 (dj1 , dj )  (fj )I for 1  i  n and 2  j  ki ,
(i)

 giI (dki ) = i for 1  i  n, and
 (1 , . . . , n )  P D .
S
S
(i)
(i)
Set  0 :=   1in and 1jki {bj 7 dj } and  0 :=   1in {xi 7 i }.
The resulting completion system is I-compatible via  0 and  0 .
Rch The rule is applied to an abstract node a and a key assertion (u1 , . . . , un keyfor C)
 K and non-deterministically adds either C or  C. By definition of the semantics,
(a)  C I or (a)  ( C)I . Hence, Rch can be applied such that the resulting
completion system is I-compatible via  and  .
Rp The rule is applied to a concept C  L(a) and adds C to the label L(b) of a node b
with a a b. By (Ca), we have (a)  C I . Since Claim 1 yields (a) = (b), it follows
that the resulting completion system is I-compatible via  and  .
Finally, we show that I-compatibility implies clash-freeness.
Claim 3: Every I-compatible completion system is clash-free.
Proof: Let S = (T, P, , ) be an I-compatible completion system. Consider the three
kinds of a clash:
 Due to (Ca), a clash of the form {A, A}  L(a) clearly contradicts the semantics.
 Assume that there are a  Va and x  Vc such that g  L(a) and x is g/a -neighbor
of a. Then there exists b  Va such that a a b and x is g-successor of b. By Claim 1,
a a b implies (a) = (b). Thus, g  L(a) and (Ca) give (b)  (g)I . We obtain a
contradiction since (Cc) yields ((b),  (x))  g I .
 Properties (Cd) and (Ce) and Part (ii) of Claim 1 imply that  is a solution for S .
Thus, S is concrete domain satisfiable.
We can now describe the guidance of the tableau algorithm by the model I in detail:
we ensure that, at all times, the considered completion systems are I-compatible. This
obviously holds for the initial completion system
SC0 = (TC0 , P , , ) with TC0 = ({a0 }, , , {a0 7 {C}}).
We guide the non-deterministic test function such that, when given a predicate conjunction
S with set of variables Vc  Oc as input, it returns the relation  defined by setting x  y
iff  (x) =  (y) for all x, y  V . The relation  is a concrete equivalence since  is a solution
for S (see above). With this guidance, (Ce) is obviously satisfied after each call to test, and
the other properties are not affected by such a call. According to Claim 2, we can apply the
715

fiLutz, Areces, Horrocks, & Sattler

completion rules such that I-compatibility is preserved. By Corollary B.4, the algorithm
always terminates, hence also when guided in this way. Since, by Claim 3, we will not find
a clash, the algorithm returns satisfiable.
The tableau algorithm yields decidability and a tight upper complexity bound for ALCOK(D)concept satisfiability w.r.t. key boxes.
Theorem B.7 (Theorem 4.1 of Section 4.1). Let D be a key-admissible concrete domain.
If extended D-satisfiability is in NP, then ALCOK(D)-concept satisfiability w.r.t. Boolean
key boxes is in NExpTime.
Proof. Corollary B.4 and Lemmas B.5 and B.6 yield decidability of ALCOK(D)-concept
satisfiability w.r.t. Boolean key boxes. For complexity, Lemma B.3 provides an exponential
bound on the number of recursive calls. Hence, it remains to show that each single recursion
step needs at most exponential time. By Lemma B.2, the while loop terminates after at most
exponentially many steps. In each such step, we compute the relations a and c , which
are used in the construction of the predicate conjunction S and for checking termination
of the while loop. Since, by Lemma B.1, there exists an exponential bound on the number
of abstract and concrete nodes in the completion system S, this can obviously be done in
exponential time. Moreover, Lemma B.1 implies that the size of S is at most exponential.
This together with the fact that extended D-satisfiability is in NP implies that the call to
test needs at most exponential time. All remaining tasks (checking for clashes, completeness,
and rule applicability) can clearly also be performed in exponential time.

Appendix C. Proofs of Section 4.2
We first provide the proof of Lemma 4.2 which shows that the notion of tableaux introduced
in Section 4.2 is an adequate abstraction of models.
Lemma C.1 (Lemma 4.2 in Section 4.2). Let D be a SHOQK(D)-concept in NNF, R a
role box, and K a path-free key box in NNF. Then D is satisfiable w.r.t. R and K iff D has
a tableau w.r.t. R and K.
Proof. For the only-if direction, we construct a tableau T from a common model I of D,
R, and K as follows:
Sa := I
Sc := {x  D | g I (s) = x for some s  Sa }
L(s) := {C  cl(D, R, K) | s  C I }
E(s, t) := {S  ND,R,K
| (s, t)  S I }
R
e(s, g) := g I (s) if g I (s) is defined
P(P ) := {(x1 , . . . , xn )  Snc | (x1 , . . . , xn )  P D }.
It can be easily verified that T is a tableau for D w.r.t. R and K: the proof that T satisfies
(T1)  (T9) is identical to the corresponding cases in (Horrocks et al., 2000; Horrocks &
Sattler, 2001); (T10) holds by definition of L; (T11) by definition of L and the fact that
nominals are interpreted as singleton sets; (T12) by definition of L, e, and P together with
716

fiKeys, Nominals and Concrete Domains

the semantics of concepts g1 , . . . , gn .P ; (T13) since the identity function on Sc is clearly a
solution for the listed predicate conjunction; (T14) by definition of L and e together with
the semantics of key constraints; and finally (T15) by definition of L and e together with
the semantics of concepts g.
For the if direction, let T = (Sa , Sc , L, E, e, P) be a tableau for D w.r.t. R and K and
let  be a solution for the predicate conjunction in (T13). We construct a model I for D
as follows:
I

:= Sa

AI

:= {s  I | A  L(s)}

for concept names A

NI

:= {s  I | N  L(s)} for nominals N
( S
I
for R  NR \ NcF with not Trans(R)
S v
* R S  {(s, t) | R  E(s, t)}
I
S6=R
R :=
{(s, t) | R  E(s, t)}+
for R  NR \ NcF with Trans(R)

(x)
if e(s, g) = x
g I (s) :=
for g  NcF .
undefined if e(s, g) is undefined
Due to (T11), the interpretation of nominals is a singleton. Moreover, the interpretation of
roles is well-defined since role boxes are acyclic. The following claim is central for proving
that I is indeed a model for D, R, and K:
Claim: For each C  cl(D, R, K), C  L(s) implies s  C I .
Proof: We proceed by induction on the norm ||C|| of C, which is defined as follows:
||A||
||g||
||C1 u C2 ||
||(> n R C)||

:=
:=
:=
:=

||A||
||u1 , . . . , un .P ||
||C1 t C2 ||
||(6 n R C)||

:=
:=
:=
:=

0 for A concept name
0
1 + ||C1 || + ||C2 ||
1 + ||C||

For concept names A and nominals N , the claim follows by definition of AI and N I . For the
negation of concept names A and nominals N (note that C is in NNF), the claim follows by
definition of AI and N I together with (T1). Concepts C of the form C1 u C2 and C1 t C2
can be treated using (T2) and (T3) together with the induction hypothesis. For existential,
universal, and number restrictions, the proof is analogous to the one for SHIQ in (Horrocks
et al., 2000). For concepts of the form C = g1 , . . . gn .P  L(s), s  C I is an immediate
consequence of (T12), the definition of giI , and the fact that (x1 , . . . , xn )  P(P ) implies
((x1 ), . . . , (xn ))  P D by (T13). Finally, for concepts C = g, s  C I is an immediate
consequence of the definition of g I together with (T15). This finishes the proof of the
claim.
By definition of tableaux, there exists an s0  Sa such that C  L(s0 ). By the claim,
s0  C I and thus I is a model of C.
Next, we show that I is a model of R. By definition of RI , it is obvious that Trans(R) 
R implies that RI is a transitive relation. Now let S v R  R. If Trans(R) 
/ R, then we
have S I  RI by definition of RI . Now let Trans(R)  R and (s, t)  S I . If S  E(s, t),
then (T4) implies R  E(s, t), and thus (s, t)  RI . Otherwise, there is an S 0 v
* S with
Trans(S 0 )  R and (s, t)  {(u, v) | S 0  E(u, v)}+ . Now (T4) together with S 0 v
* R implies
717

fiLutz, Areces, Horrocks, & Sattler

that {(u, v) | S 0  E(u, v)}  {(u, v) | R  E(u, v)}, and thus Trans(R)  R implies that
(s, t)  RI .
It remains to show that I is a model of K. To this end, let (g1 , . . . , gn keyfor C)  K
and s, t  C I such that giI (s) = giI (t) for 1  i  n. Since the predicate conjunction in
(T13) contains explicit inequalities for all distinct concrete individuals, this implies that
e(s, gi ) = e(t, gi ) for 1  i  n. (T10) implies {C,  C}  L(s) 6=  and {C,  C}  L(t) 6= .
If  C  L(s), then the claim yields s  ( C)I contradicting s  C I . Thus we obtain
C  L(s) and, in a similar way, C  L(t). Finally, (T14) implies that s = t, and thus I
satisfies K.
We now proceed to prove termination, soundness, and completeness of the tableau algorithm
presented in Section 4.2, starting with termination. In the following, we use |D, R, K| to
denote | cl+ (D, R, K)|. Recall that this number is polynomial in the size of D, R, K.
Lemma C.2 (Termination). Let D be a key-admissible concrete domain. When started
with a SHOQK(D) concept D in NNF, a role box R, and a path-free key box K in NNF,
the tableau algorithm terminates.
Proof. Assume that there are D, R, and K such that the tableau algorithm does not terminate. Since D is key-admissible, this means that there is an infinite sequence S0 , S1 , . . .
of completion systems such that (a) S0 is the initial completion system SD and (b) Si+1 is
the result of applying a completion rule to Si .
This is only possible if the R or the R> rules are applied infinitely often: it is easily seen
that the rules Ru, Rt, R6, Rc, R, R+ , Rch, and Ra can only be applied finitely often
to completion systems whose set of abstract nodes Va does not increase since they either
add concepts into node labels (whose size is bounded), or they add concrete nodes (whose
number is bounded linearly by the number of abstract nodes), or they remove abstract
nodes from the forest. Hence there is a sub-sequence Si1 , Si2 , . . . of S0 , S1 , . . . such that
Sij is the result of applying the R or the R> rule to Sij 1 . Let si` be the abstract node
to which the R or the R> rule was applied in Si` 1 . Since s  t implies that t was not
generated before s, the linear ordering  is well-founded. Thus, we find an infinite subsequence Sj1 , Sj2 , . . . of Si1 , Si2 , . . . such that either sj` = sj`+1 for each `  1 or sj`  sj`+1
for each `  1. The former, however, is not possible since the R and the R> rules are
applied at most once per node and concept in cl(D, R, K): even if a node is removed, the
label copying performed by the R6 rule together with clashes of type (C3) ensures that
the R> rule is not re-applied to the same concept and node. Thus only the second option
remains: there is a subsequence Sj1 , Sj2 , . . . of Si1 , Si2 , . . . such that sj`  sj`+1 for each
`  1. Let Lj be the labeling function in Sj . Since each abstract node is labeled with a
subset Lj of cl+ (D, R, K), there are nodes sjk  sj` with k fi ` and Ljk (sjk ) = Lj` (sj` ).
Now node labels can only increase and, if a node t is removed, its label is conjoined to the
label of a node t with t  t. Thus there is a node t in the completion system Sj` with
t  sj` and Lj` (sj` )  Lj` (t). By definition, sj` is thus blocked in Sj` , contradicting the
assumption that the R or the R> rule is applied to sj` in Sj` .
Lemma C.3 (Soundness). If the expansion rules can be applied to a SHOQK(D) concept
D in NNF, a role box R, and a path-free key box K such that they yield a complete and
clash-free completion forest, then D has a tableau w.r.t. R and K.
718

fiKeys, Nominals and Concrete Domains

Proof. Let S = ((Va , Vc , E, L), P, c , ) be a complete and clash-free completion system.
We can find a solution  for S such that (x) = (y) iff x c y: only the Rc rule updates
the predicate conjunction S , and after each rule application the c relation is updated
using the concrete equivalence that a D-tester returns for S (note that S is satisfiable due
to clash-freeness). According to Definition 4.1, we can thus find a solution  as required.
From S and , we define a finite tableau T = (Sa , Sc , E, L, P) as follows:
Sa := {s  Va | s occurs in S and is not blocked}
Sc := {(x) | (s, x)  E(g) for some s  Sa and some g}
L(s) := L(s)  cl(D, R, K) (the intersection is due to the auxiliary concepts AnRC
),
i
E(s, t) := {R | t is an R-successor of s or t blocks an R-successor t0 of s}

(x)
if x is a g-successor of s
e(s, g) :=
undefined if x has no g-successor
P := the restriction of P to Sc .
Note that the function e is well-defined due to the definition of adding g-successors. It
remains to show that T satisfies (T1)(T14), which is basically a consequence of S being
clash-free and complete.
 (T1) is satisfied since S does not contain a clash (C1).
 (T2) is satisfied since the Ru rule cannot be applied, and thus C1 u C2  L(s) implies
C1 , C2  L(s).
 (T3) is satisfied since the Rt rule cannot be applied, and thus C1 t C2  L(s) implies
{C1 , C2 }  L(s) 6= .
 For (T4), consider s, t  Sa with R  E(s, t) and R v
* R0 . Then R  E(s, t) implies
that t is or blocks an R-successor of s. By definition of successor, t is or blocks an
R0 -successor of s, and thus R0  E(s, t).
 For (T5), let R.C  L(s) and R  E(s, t). If t is an R-successor of s, then s not
being blocked implies C  L(t) since the R rule cannot be applied. If t blocks an
R-successor t0 of s, then s not being blocked and the fact that the R rule cannot be
applied yields C  L(t0 ), and the blocking condition implies C  L(t).
In both cases, we thus have C  L(t).
 (T6) and (T7) are satisfied for the same reasons as (T5) with R replaced with R
and R+ .
 For (T8), consider s with (> n R C)  L(s). Hence (> n R C)  L(s) and
completeness of S implies the existence of R-successors t1 , . . . , tn of s with C  L(ti )
.
and ti =
6 tj for i 6= j. The latter implies, for each i 6= j, the existence of integers k, `
such that k 6= `, AnRC
 L(ti ), and AnRC
 L(tj ). For (T8) to be satisfied, it remains
k
`
to verify that
 no ti can block a tj : if this was the case, the blocking condition would imply that
{AnRC
, AnRC
}  L(ti ).
k
`
719

fiLutz, Areces, Horrocks, & Sattler

 no t can block both ti and tj with i 6= j: similarly, this would imply that
{AnRC
, AnRC
}  L(t).
k
`
In each case, we would have a clash (C3), in contradiction to S being clash-free.
 For (T9), consider s with (6 n R C)  L(s). Hence (6 n R C)  L(s) and, since
the R6 rule cannot be applied, there are at most n R-successors ti of s. Since each
ti is either not blocked or blocked by exactly one other node (due to  being a linear
ordering), there are at most n ui  Sa with R  E(s, ui ) and C  L(ui ).
 For (T10), let (6 n R C)  L(s) and R  E(s, t). Hence (6 n R C)  L(s) and
t either is an R-successor of s or blocks an R-successor of s. In the first case, nonapplicability of the Rch rule implies that {C,  C}  L(t) 6= . In the second case,
{C,  C}  L(t0 ) 6=  for t0 the R-successor of s blocked by t, and thus the blocking
condition yields {C,  C}  L(t) 6= . In both cases, this implies {C,  C}  L(t) 6= .
Next, consider (g1 , . . . , gn keyfor C)  K and s such that e(s, gi ) is defined for each
i. Hence s has a gi -successor for each i, and thus s not being blocked and the nonapplicability of the Rch rule imply that {C,  C}  L(t) 6= .
 For (T11), consider N  L(s)  L(t). By definition, N  L(s)  L(t) and thus s a t.
Moreover, totality of  implies that we can assume without loss of generality that
s  t or s = t. Thus non-applicability of the Ra rule implies that L(t)  L(s), and
thus t not being blocked implies s = t.
 (T12) is satisfied since the rule Rc cannot be applied.
 For (T13), clash-freeness implies the satisfiability of
^

^

P (x1 , . . . , xn ).

P used in D,K (x1 ,...,xn )P(P )

By choice of , (x) = (y) iff x c y, and thus (T13) is satisfied.
 For (T14), let (g1 , . . . , gn keyfor C)  K, C  L(s)  L(t), and e(s, gi ) = e(t, gi ), for
all 1  i  n. Thus C  L(s)  L(t) and, by choice of e and , we have xi c yi
for gi  E(s, xi )  E(t, yi ). Hence s a t. Without loss of generality, we assume that
s  t or s = t. Thus non-applicability of the Ra rule implies that L(t)  L(s), and
thus t not being blocked implies s = t.
 (T15) is satisfied by definition of T and since S does not contain a clash (C4).

Lemma C.4 (Completeness). If a SHOQK(D)-concept D in NNF has a tableau w.r.t. a
role box R and a path-free key box K, then the expansion rules can be applied to D, R, and
K such that they yield a complete and clash-free completion forest.
720

fiKeys, Nominals and Concrete Domains

Proof. Given a tableau T = (Sa , Sc , L, E, e, P) for D w.r.t. R and K, we can guide the
non-deterministic rules Rt, Rch, and Ra in such a way that each rule application preserves
clash-freeness. This together with termination from Lemma C.2 finishes the proof.
Along with rule application, we perform a stepwise construction of a total mapping 
that takes abstract nodes of the completion forest to elements of Sa and concrete nodes of
the completion forest to elements of Sc .
 L(s)  cl(D, R, K)  L((s)) for each s  Va ,
 if t is an R-successor of s, then R  E((s), (t)),
 if x is a g-successor of s, then e((s), g) = (x),
 x c y iff (x) = (y), and
.
 if s =
6 t, then (s) 6= (t).
A mapping satisfying these four conditions is called correct in the following. Note that a
completion system for which there exists a correct mapping does not contain a clash: due to
(T1) and the first property, we do not encounter a clash (C1). A clash (C3) cannot occur
due to the last property. The first and the third property together with (T15) ensure that
a clash (C4) does not occur. Finally, a clash (C2) cannot occur for the following reason:
by construction of P and since edges labelled with abstract features are never removed, for
each tuple (x1 , . . . , xn )  P(P ), we find an abstract node s and paths u1 , . . . , un such that
u1 , . . . , un .P  L(s) and xi is a ui -successor of s for 1  i  n. Thus, the first, second,
and third property together with (T12) and (T13) ensure that the conjunction
^
P ((x1 ), . . . , (xn ))
P used inD,K
(x1 ,...,xn )P(P )

has a solution  with ((x)) 6= ((x)) iff (x) 6= (y). By the fourth property, setting
 0 (x) := ((x)) for all x  Vc thus yields a solution  0 for S .
The total mapping  is inductively defined as follows: let  be a solution for the equation
in (T13). Choose a node s0 with D  L(s0 ), and set (s0 ) := s0 for s0 the (only) node of the
initial completion forest. Obviously,  is correct. We will now show that each completion
rule can be applied in such a way that  either is still correct or that  can be extended to
a correct mapping.
 An application of the rule Ru preserves correctness of  due to (T2).
 Due to (T3), the rule Rt can be applied such that correctness is preserved.
 If the rule R adds a new node t for R.C  L(s), then correctness implies R.C 
L((s)), and thus (T6) implies the existence of some t  Sa with R  E((s), t) and
C  L(t). Thus extending  with (t) := t obviously yields a correct mapping.
 If the rule R> adds n nodes ti for (> n R C)  L(s), then correctness implies
(> n R C)  L((s)), and thus (T8) implies the existence of t1 , . . . , tn  Sa with
ti 6= tj for i 6= j, R  E((s), ti ), and C  L(ti ). Thus extending  with (ti ) := ti
obviously yields a correct mapping.
721

fiLutz, Areces, Horrocks, & Sattler

 Assume that the R6 rule is applicable to a node s with (6 n R C)  L(s) and more
than n R-successors ti with C  L(ti ). Then correctness implies that (6 n R C) 
L((s)), R  E((s), (ti )), and C  L(ti ). Thus, by (T9), there are i 6= j with
.
(ti ) = (tj ). Again, correctness implies that not ti =
6 tj and, without loss of generality, we can assume that ti  tj . Hence applying the rule and thereby merging L(tj )
into L(ti ) preserves correctness.
 For the rule Rc,  can be extended in a similar way as for R: if a new gi -successor xi
of s is added, then extending  with (xi ) := e((s), gi ) yields a correct . Moreover,
(T13) ensures that c can be updated in such a way that the fourth condition is
preserved.
 For the R rule,  does not need to be extended, and (T5), (T4), and the definition
of R-successors imply that correctness is preserved.
 The R+ rule is similar, with the only difference that (T7) takes the place of (T5).
 Due to (T10), the rule Rch can be applied without violating correctness.
 For Ra , we consider two reasons for Ra to be applicable:
 N  L(s)  L(t). Then correctness of  and (T11) imply that (s) = (t).
 (g1 , . . . , gn keyfor C)  K, C  L(s)  L(t), and gi  E(s, xi )  E(t, yi ) and
xi c yi for 1  i  n. Then correctness implies that e((s), gi ) = e((t), gi ), and
thus (T14) together with the first property of correctness imply that (s) = (t).
In both cases, applying Ra to s and t preserves correctness.

As an immediate consequence of Lemmas 4.2, C.2, C.3, and C.4, the tableau algorithm
always terminates and answers D is satisfiable w.r.t. R and K if and only if the input
concept D is satisfiable w.r.t. the input role box R and the input key box K. Since concept
satisfiability w.r.t. TBoxes can be reduced to concept satisfiability without TBoxes, we
obtain the following result:
Proposition C.5. Let D be a key-admissible concrete domain. The tableau algorithm
decides satisfiability of SHOQK(D) concepts w.r.t. TBoxes, role boxes, and path-free key
boxes.
It is not hard to verify that the proof of Lemma C.4 together with Lemmas 4.2 and C.2
yield a bounded model property for SHOQK(D), where the bound is exponential.
Corollary C.6. If a SHOQK(D)-concept D is satisfiable w.r.t. a role box R and a pathfree key box K, then D is satisfiable w.r.t. R and K in a model of size at most |I |  2m
for m = # cl+ (D, R, K).
722

fiKeys, Nominals and Concrete Domains

Proof. If a SHOQK(D)-concept D is satisfiable w.r.t. a role box R and a path-free key
box K, Lemma C.4 implies that the tableau algorithm constructs a complete and clash-free
completion forest for D, R, and K. By the definition of blocking, the number of abstract
nodes in a completion forest that are not blocked is bounded by 2m : if s 6= t  Va are
abstract nodes in a completion forest and L(s) = L(t), then either s blocks t, t blocks s, or
they are both blocked by another node u. Moreover, it is easily seen that the number of
concrete successors per abstract node is bounded by the number of concrete features in C, R,
and K. Now, in the proof of Lemma C.4, the abstract nodes in the tableau constructed from
a complete and clash-free completion forest coincide with the nodes that are not blocked
in the completion forest. Finally, in the proof of Lemma 4.2 the interpretation domain
of a model constructed from a tableau coincides with the abstract nodes in the tableau.
Summing up, a SHOQK(D)-concept that is satisfiable w.r.t. R and K has a model of size
|I |  2m .

References
Areces, C., Blackburn, P., & Marx, M. (1999). A road-map on complexity for hybrid logics.
In Flum, J., & Rodrguez-Artalejo, M. (Eds.), Computer Science Logic, No. 1683 in
Lecture Notes in Computer Science, pp. 307321. Springer-Verlag.
Baader, F., Horrocks, I., & Sattler, U. (2002a). Description logics for the semantic web. KI
 Kunstliche Intelligenz, 16 (4), 5759.
Baader, F., Lutz, C., Sturm, H., & Wolter, F. (2002b). Fusions of description logics and
abstract description systems. Journal of Artificial Intelligence Research (JAIR), 16,
158.
Baader, F., & Sattler, U. (1998). Description logics with concrete domains and aggregation. In Prade, H. (Ed.), Proceedings of the 13th European Conference on Artificial
Intelligence (ECAI98), pp. 336340. John Wiley & Sons.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (2003).
The Description Logic Handbook: Theory, implementation and applications. Cambridge University Press, Cambridge, MA, USA.
Baader, F., & Hanschke, P. (1991a). A scheme for integrating concrete domains into concept
languages. In Proceedings of the 12th International Joint Conference on Artificial
Intelligence (IJCAI-91), pp. 452457, Sydney, Australia.
Baader, F., & Hanschke, P. (1991b). A scheme for integrating concrete domains into concept
languages. DFKI research report RR-91-10, German Research Center for Artificial
Intelligence (DFKI).
Baader, F., & Hanschke, P. (1992). Extensions of concept languages for a mechanical
engineering application. In Proceedings of the 16th German AI-Conference (GWAI92), Vol. 671 of Lecture Notes in Computer Science, pp. 132143. Springer-Verlag.
Baader, F., & Sattler, U. (2000). Tableau algorithms for description logics. In Dyckhoff,
R. (Ed.), Proceedings of the International Conference on Automated Reasoning with
Tableaux and Related Methods (Tableaux 2000), Vol. 1847 of Lecture Notes in Artificial
Intelligence, pp. 118. Springer-Verlag.
723

fiLutz, Areces, Horrocks, & Sattler

Berger, R. (1966). The undecidability of the domino problem. Memoirs of the American
Mathematical Society, 66, 172.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientific American,
284 (5), 3443.
Borger, E., Gradel, E., & Gurevich, Y. (1997). The Classical Decision Problem. Perspectives
in Mathematical Logic. Springer-Verlag.
Borgida, A., & Patel-Schneider, P. F. (1994). A semantics and complete algorithm for
subsumption in the CLASSIC description logic. Journal of Artificial Intelligence Research, 1, 277308.
Borgida, A., & Weddell, G. E. (1997). Adding uniqueness constraints to description logics
(preliminary report). In Bry, F., Ramakrishnan, R., & Ramamohanarao, K. (Eds.),
Proceedings of the 5th International Conference on Deductive and Object-Oriented
Databases (DOOD97), Vol. 1341 of LNCS, pp. 85102. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). On the decidability of query
containment under constraints. In Proceedings of the 17th ACM SIGACT-SIGMODSIGART Symposium on Principles of Database Systems (PODS98), pp. 149158.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2000). Keys for free in description logics.
In Baader, F., & Sattler, U. (Eds.), Proceedings of the 2000 International Workshop in
Description Logics (DL2000), No. 33 in CEUR-WS (http://ceur-ws.org/), pp. 7988.
Calvanese, D., Lenzerini, M., & Nardi, D. (1998). Description logics for conceptual data
modeling. In Chomicki, J., & Saake, G. (Eds.), Logics for Databases and Information
Systems, pp. 229263. Kluwer Academic Publisher.
Dean, M., Connolly, D., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L.,
Patel-Schneider, P. F., & Stein, L. A. (2002). Web ontology language (OWL) reference
version 1.0. W3C Working Draft.
Fensel, D., van Harmelen, F., Horrocks, I., McGuinness, D. L., & Patel-Schneider, P. F.
(2001). OIL: An ontology infrastructure for the semantic web. IEEE Intelligent
Systems, 16 (2), 3845.
Graham, R. L., Knuth, D. E., & Patashnik, O. (1990). Concrete Mathematics. Addison
Wesley Publ. Co., Reading, Massachussetts.
Haarslev, V., Lutz, C., & Moller, R. (1998). Foundations of spatioterminological reasoning
with description logics. In Cohn, A., Schubert, L., & S.C.Shapiro (Eds.), Proceedings
of the 6th International Conference on Principles of Knowledge Representation and
Reasoning (KR98), pp. 112124. Morgan Kaufman.
Haarslev, V., & Moller, R. (2001). RACER system description. In Gore, R., Leitsch,
A., & Nipkow, T. (Eds.), Proceedings of the 1st International Joint Conference on
Automated Reasoning (IJCAR01), No. 2083 in Lecture Notes in Artificial Intelligence,
pp. 701705. Springer-Verlag.
Haarslev, V., Moller, R., & Wessel, M. (2001). The description logic ALCN HR+ extended
with concrete domains: A practically motivated approach. In Gore, R., Leitsch, A.,
724

fiKeys, Nominals and Concrete Domains

& Nipkow, T. (Eds.), Proceedings of the 1st International Joint Conference on Automated Reasoning IJCAR01, No. 2083 in Lecture Notes in Artificial Intelligence, pp.
2944. Springer-Verlag.
Halpern, J. Y., & Moses, Y. (1992). A guide to completeness and complexity for modal
logics of knowledge and belief. Artificial Intelligence, 54 (3), 319380.
Hollunder, B., & Baader, F. (1991). Qualifying number restrictions in concept languages.
In Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning (KR91), pp. 335346, Boston, MA, USA.
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction to Automata Theory, Languages and
Computation. Addison-Wesley.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning for very expressive description logics. Logic Journal of the IGPL, 8 (3), 239264.
Horrocks, I. (1998). Using an expressive description logic: FaCT or fiction?. In Proceedings
of the 6th International Conference on the Principles of Knowledge Representation
and Reasoning (KR98), pp. 636647.
Horrocks, I. (2002). Reasoning with expressive description logics: Theory and practice. In
Voronkov, A. (Ed.), Proceedings of the 18th International Conference on Automated
Deduction (CADE 2002), No. 2392 in Lecture Notes in Artificial Intelligence, pp. 115.
Springer.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2002). Reviewing the design of
DAML+OIL: An ontology language for the semantic web. In Proceedings of the 18th
National Conference on Artificial Intelligence (AAAI 2002), pp. 792797.
Horrocks, I., & Sattler, U. (2001). Ontology reasoning in the SHOQ(D) description logic. In
Nebel, B. (Ed.), Proceedings of the 17th International Joint Conference on Artificial
Intelligence (IJCAI01), pp. 199204. Morgan-Kaufmann.
Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning for expressive description
logics. In Ganzinger, H., McAllester, D., & Voronkov, A. (Eds.), Proceedings of the
6th International Conference on Logic for Programming and Automated Reasoning
(LPAR99), No. 1705 in Lecture Notes in Artificial Intelligence, pp. 161180. SpringerVerlag.
Kamp, G., & Wache, H. (1996). CTL - a description logic with expressive concrete domains.
Tech. rep. LKI-M-96/01, Laboratory for Artificial Intelligence (LKI), Universitity of
Hamburg, Germany.
Khizder, V. L., Toman, D., & Weddell, G. E. (2001). On decidability and complexity of description logics with uniqueness constraints. In den Bussche, J. V., & Vianu, V. (Eds.),
Proceedings of the 8th International Conference on Database Theory (ICDT2001), Vol.
1973 of LNCS, pp. 5467. Springer.
Knuth, D. (1968). The Art of Computer Programming, Vol. 1. Addison-Wesley.
Lutz, C. (2003). Description logics with concrete domainsa survey. In Advances in Modal
Logics Volume 4, pp. 265296. World Scientific Publishing Co. Pte. LTd.
725

fiLutz, Areces, Horrocks, & Sattler

Lutz, C. (2002a). The Complexity of Reasoning with Concrete Domains. Ph.D. thesis,
LuFG Theoretical Computer Science, RWTH Aachen, Germany.
Lutz, C. (2002b). PSpace reasoning with the description logic ALCF(D). Logic Journal
of the IGPL, 10 (5), 535568.
Lutz, C. (2002c). Reasoning about entity relationship diagrams with complex attribute
dependencies. In Horrocks, I., & Tessaris, S. (Eds.), Proceedings of the International
Workshop in Description Logics 2002 (DL2002), No. 53 in CEUR-WS (http://ceurws.org/), pp. 185194.
Lutz, C. (2004). NExpTime-complete description logics with concrete domains. ACM
Transactions on Computational Logic, 5 (4), 669705.
Lutz, C., Areces, C., Horrocks, I., & Sattler, U. (2002). Keys, nominals, and concrete
domains. LTCS-report 02-04, Technical University Dresden. See http://lat.inf.tudresden.de/research/reports.html.
Lutz, C., Areces, C., Horrocks, I., & Sattler, U. (2003). Keys, nominals, and concrete
domains. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI03), pp. 349354. Morgan-Kaufmann Publishers.
Pan, J. Z., & Horrocks, I. (2002). Reasoning in the SHOQ(Dn ) description logic. In Horrocks, I., & Tessaris, S. (Eds.), Proceedings of the International Workshop in Description Logics 2002 (DL2002), No. 53 in CEUR-WS (http://ceur-ws.org/), pp. 5362.
Post, E. M. (1946). A variant of a recursively unsolvable problem. Bulletin of the American
Mathematical Society, 52, 264268.
Schild, K. D. (1991). A correspondence theory for terminological logics: Preliminary report.
In Mylopoulos, J., & Reiter, R. (Eds.), Proceedings of the 12th International Joint
Conference on Artificial Intelligence (IJCAI-91), pp. 466471. Morgan Kaufmann.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Artificial Intelligence, 48 (1), 126.

726

fiJournal of Artificial Intelligence Research 23 (2005) 421-440

Submitted 07/04; published 04/05

On the Practical use of Variable Elimination in Constraint
Optimization Problems: Still-life as a Case Study
Javier Larrosa
Enric Morancho
David Niso

larrosa@lsi.upc.edu
enricm@ac.upc.edu
niso57@casal.upc.edu

Universitat Politecnica de Catalunya
Jordi Girona 1-3, 08034 Barcelona, Spain

Abstract
Variable elimination is a general technique for constraint processing. It is often discarded because of its high space complexity. However, it can be extremely useful when
combined with other techniques. In this paper we study the applicability of variable elimination to the challenging problem of finding still-lifes. We illustrate several alternatives:
variable elimination as a stand-alone algorithm, interleaved with search, and as a source of
good quality lower bounds. We show that these techniques are the best known option both
theoretically and empirically. In our experiments we have been able to solve the n = 20
instance, which is far beyond reach with alternative approaches.

1. Introduction
Many problems arising in domains such as resource allocation (Cabon, de Givry, Lobjois,
Schiex, & Warners, 1999), combinatorial auctions (Sandholm, 1999), bioinformatics and
probabilistic reasoning (Pearl, 1988) can be naturally modeled as constraint satisfaction
and optimization problems. The two main solving schemas are search and inference. Search
algorithms constitute the usual solving approach. They transform a problem into a set of
subproblems by selecting one variable and instantiating it with its different alternatives.
Subproblems are solved applying recursively the same transformation rule. The recursion
defines a search tree that is normally traversed in a depth-first manner, which has the
benefit of requiring only polynomial space. The practical efficiency of search algorithms
greatly depends on their ability to detect and prune redundant subtrees. In the worst-case,
search algorithms need to explore the whole search tree. Nevertheless, pruning techniques
make them much more effective.
Inference algorithms (also known as decomposition methods) solve a problem by a sequence of transformations that reduce the problem size, while preserving its optimal cost. A
well known example is bucket elimination (BE, also known as variable elimination) (Bertele
& Brioschi, 1972; Dechter, 1999). The algorithm proceeds by selecting one variable at a
time and replacing it by a new constraint which summarizes the effect of the chosen variable. The main drawback of BE is that new constraints may have large arities which require
exponentially time and space to process and store. However, a nice property of BE is that
its worst-case time and space complexities can be tightly bounded by a structural parameter called induced width. The exponential space complexity limits severely the algorithms
c
2005
AI Access Foundation. All rights reserved.

fiLarrosa, Morancho & Niso

practical usefulness. Thus, in the constraint satisfaction community variable elimination is
often disregarded.
In this paper we consider the challenging problem of finding still-lifes which are stable
patterns of maximum density in the game of life. This academic problem has been recently
included in the CSPlib repository1 and a dedicated web page2 has been set to maintain
up-to-date results. In Bosch and Trick (2002), the still-life problem is solved using two
different approaches: integer programming and constraint programming, both of them based
on search. None of them could solve up to the n = 8 problem within reasonable time. Their
best results were obtained with a hybrid approach which combines the two techniques and
exploits the problem symmetries in order to reduce the search space. With their algorithm,
they solved the n = 15 case in about 8 days of cpu. Smith (2002) proposed an interesting
alternative using pure constraint programming techniques, and solving the problem in its
dual form. In her work, Smith could not improve the n = 15 limit. Although not explicitly
2
mentioned, these two works use algorithms with worst-case time complexity O(2(n ) ).
In this paper we show the usefulness of variable elimination techniques. First we apply
plain BE. Against what could be expected, we observe that BE is competitive with stateof-the-art alternatives. Next, we introduce a more sophisticated algorithm that combines
search and variable elimination (following the ideas of Larrosa & Dechter, 2003) and uses
a lower bound based on mini-buckets (following the ideas of Kask & Dechter, 2001). With
our algorithm, we solve in one minute the n = 15 instance. We have been able to solve up to
the n = 20 instance, which was far beyond reach with previous techniques. For readability
reasons, we only describe the main ideas and omit algorithmic details.3
The structure of the paper is the following: In the next Section we give some preliminary
definitions. In Section 3 we solve the problem with plain BE. In Section 4 we introduce the
hybrid algorithm with which we obtained the results reported in Section 5. In Section 6 we
discuss how the ideas explored in this article can be extended to other domains. Besides,
we report additional experimental results. Finally, Section 7 gives some conclusions and
lines of future work.

2. Preliminaries
In this Section we first define the still-life problem. Next, we define the weighted CSP
framework and formulate the still-life as a weighted CSP. Finally, we review the main
solving techniques for weighted CSPS.
2.1 Life and Still-Life
The game of life (Gardner, 1970) is played over an infinite checkerboard, where each square
is called a cell. Each cell has eight neighbors: the eight cells that share one or two corners
with it. The only player places checkers on some cells. If there is a checker on it, the cell
is alive, else it is dead. The state of the board evolves iteratively according to the following
three rules: (1) if a cell has exactly two living neighbors then its state remains the same
1. www.csplib.org
2. www.ai.sri.com/~ nysmith/life
3. The interested reader can find an extended version, along with the source code of our implementation
in www.lsi.upc.edu/~ larrosa/publications

422

fiOn the practical use of variable elimination

3

1

3

1

1

3

1

Xc

4

2

A

2

2

B

2

D

C

E

Figure 1: A: A 3  3 still-life. B: constraint graph of a simple WCSP instance with four
variables and three cost functions. C: the constraint graph after assigning variable
x4 . D: the constraint graph after clustering variables x3 and x4 . E: the constraint
graph after eliminating variable x4 .

in the next iteration, (2) if a cell has exactly three living neighbors then it is alive in the
next iteration and (3) if a cell has fewer than two or more than three living neighbors,
then it is dead in the next iteration. Although defined in terms of extremely simple rules,
the game of life has proven mathematically rich and it has attracted the interest of both
mathematicians and computer scientists.
The still-life problem SL(n) consist on finding a nn stable pattern of maximum density
in the game of life. All cells outside the pattern are assumed to be dead. Considering the
rules of the game, it is easy to see that each cell (i, j) must satisfy the following three
conditions: (1) if the cell is alive, it must have exactly two or three living neighbors, (2) if
the cell is dead, it must not have three living neighbors, and (3) if the cell is at the grid
boundary (i.e, i = 1 or i = n or j = 1 or j = n), it cannot be part of a sequence of three
consecutive living cells along the boundary. The last condition is needed because three
consecutive living cells at a boundary would produce living cells outside the grid.
Example 1 Figure 1.A shows a solution to SL(3). It is easy to verify that all its cells
satisfy the previous conditions, hence it is stable. The pattern is optimal because it has 6
living cells and no 3  3 stable pattern with more that 6 living cells exists.

2.2 Weighted CSP
A weighted constraint satisfaction problem (WCSP) (Bistarelli, Montanari, & Rossi, 1997)
is defined by a tuple (X, D, F), where X = {x1 , . . . , xn } is a set of variables taking values
from their finite domains Di  D. F is a set of weighted constraints (i.e., cost functions).
Each f  F is defined over a subset of variables, var(f ), called its scope. The objective
function is the sum of all functions in F,
F =

X

f

f F

and the goal is to find the instantiation of variables that minimizes the objective function.
Example 2 Consider a WCSP with four variables X = {xi }4i=1 with domains Di = {0, 1}
and three cost functions: f1 (x1 , x4 ) = x1 + x4 , f2 (x2 , x3 ) = x2 x3 and f3 (x2 , x4 ) = x2 + x4 .
423

fiLarrosa, Morancho & Niso

The objective function is F (x1 , x2 , x3 , x4 ) = x1 + x4 + x2 x3 + x2 + x4 . Clearly, the optimal
cost is 0, which is obtained with every variable taking value 0.
Constraints can be given explicitly by means of tables, or implicitly as mathematical
expressions or computing procedures. Infeasible partial assignments are specified by constraints that assign cost  to them. The assignment of value a to variable xi is noted
xi = a. A partial assignment is a tuple t = (xi1 = v1 , xi2 = v2 ,    , xij = vj ). The extension
of t to xi = a is noted t  (xi = a). WCSPs instances are graphically depicted by means
of their interaction or constraint graph, which has one node per variable and one edge connecting any two nodes that appear in the same scope of some cost function. For instance,
Figure 1.B shows the constraint graph of the problem in the previous example.
2.3 Overview of Some Solving Techniques
In this Subsection we review some solving techniques widely used when reasoning with
constraints.
2.3.1 search
WCSPs are typically solved with depth-first search. Search algorithms can be defined in
terms of instantiating functions,
Definition 1 Let P = (X, D, F) a WCSP instance, f a function in F, xi a variable in
var(f ), and v a value in Di . Instantiating f with xi = v is a new function with scope
var(f )  {xi } which returns for each tuple t, f (t  (xi = v)). Instantiating P with xi = v is
a new problem P |xi =v = (X  {xi }, D  {Di }, F 0 ), where F 0 is obtained by instantiating all
the functions in F that mention xi with xi = v.
For instance, instantiating the problem of Example 2 with x4 = 1, produces a new
problem with three variables {xi }3i=1 and three cost functions: f1 (x1 , x4 = 1) = x1 + 1,
f2 (x2 , x3 ) = x2 x3 and f3 (x2 , x4 = 1) = x2 + 1. Figure 1.C shows the corresponding
constraint graph, obtained from the original graph by removing the instantiated variable x4
and all adjacent edges. Observe that the new graph depends on the instantiated variable,
but does not depend on the value assigned to it.
Search algorithms transform the current problem P into a set of subproblems. Usually
it is done by selecting one variable xi which is instantiated with its different domain values
(P |xi =v1 , P |xi =v2 ,    , P |xi =vd ). This transformation is called branching. In each subproblem the same process is recursively applied, which defines a tree of subproblems. Search
algorithms expand subproblems until a trivial case is achieved: there is no variable left, or
a pruning condition is detected. In optimization problems, pruning conditions are usually
defined in terms of lower and upper bounds. Search keeps the cost of the best solution so
far, which is an upper bound of the optimal cost. At each node, a lower bound of the best
cost obtainable underneath is computed. If the lower bound is greater than or equal to the
upper bound, it is safe to backtrack.
The size of the search tree is O(dn ) (being d the size of the largest domain) which bounds
the time complexity. If the tree is traversed depth-first, the space complexity is polynomial.
424

fiOn the practical use of variable elimination

2.3.2 clustering
A well-known technique for constraint processing is clustering (Dechter & Pearl, 1989). It
merges several variables into one meta-variable, while preserving the problem semantics.
Clustering variables xi and xj produces meta-variable xk , whose domain is Di  Dj . Cost
functions must be accordingly clustered. For instance, in the problem of Example 2, clustering variables x3 and x4 produces variable xc with domain Dc = {(0, 0), (0, 1), (1, 0), (1, 1)}.
Cost functions f2 and f3 are clustered into fc (x2 , xc ) = f2 + f3 . With the new variable
notation fc = x2 xc [1] + x2 + xc [2], where xc [i] denotes the i-th component of xc . Function
f1 needs to be reformulated as f1 (x1 , xc ) = x1 + xc [2]. The constraint graph of the resulting
problem is obtained by merging the clustered variables and connecting the meta-node with
all nodes that were adjacent to some of the clustered variables. Figure 1.D shows the constraint graph after the clustering of x3 and x4 . The typical use of clustering is to transform
a cyclic constraint graph into an acyclic one, which can be solved efficiently thereafter.
2.3.3 variable elimination
Variable elimination is based on the following two operations,
Definition 2 The sum of two functions f and g, noted (f + g), is a new function with
scope var(f )  var(g) which returns for each tuple the sum of costs of f and g,
(f + g)(t) = f (t) + g(t)
Definition 3 The elimination of variable xi from f , noted f  xi , is a new function with
scope var(f )  {xi } which returns for each tuple t the cost of the best extension of t to xi ,
(f  xi )(t) = min {f (t  (xi = a))}
aDi

Observe that when f is a unary function (i.e., arity one), eliminating the only variable
in its scope produces a constant.
Definition 4 Let P = (X, D, F) be a WCSP instance. Let xi  X be an arbitrary variable
and let Bi be the set of all cost functions having xi in their scope (Bi is called the bucket of
xi ). We define gi as
X
gi = (
f )  xi
f Bi

The elimination of xi transforms P into a new problem P xi = {X  {xi }, D  {Di }, (F 
Bi )  {gi }}. In words, P xi is obtained by replacing xi and all the functions in its bucket
by gi .
P and P xi have the same optimal cost because, by construction, gi compensates the
absence of xi . The constraint graph of P xi is obtained by forming a clique with all the
nodes adjacent to node xi and then removing xi and all its adjacent edges. For example,
eliminating x4 in the problem of Example 2 produces a new problem with three variables
{xi }3i=1 and two cost functions: f2 and g4 . The scope of g4 is {x1 , x2 } and it is defined as,
425

fiLarrosa, Morancho & Niso

g4 = (f1 + f3 )  x4 = (x1 + x4 + x2 + x4 )  x4 = x1 + x2 . Figure 1.D shows the constraint
graph after the elimination.
In the previous example, the new function g4 could be expressed as a mathematical expression. Unfortunately, in general, the result of summing functions or eliminating variables
cannot be expressed intensionally, and new cost functions must be stored extensionally in
tables. Consequently, the space complexity of computing P xi is proportional to the numQ
ber of entries of gi , which is: ( xj var(gi ) |Dj |). Since xj  var(gi ) iff xj is adjacent to xi
Q
in the constraint graph, the previous expression can be rewritten as ( xj N (i,GP ) |Dj |),
where GP is the constraint graph of P and N (i, GP ) is the set of neighbors of xi in GP .
The time complexity of computing P xi is its space complexity multiplied by the cost of
computing each entry of gi .
Bucket elimination (BE) works in two phases. In the first phase, it eliminates variables
one at a time in reverse order. In the elimination of xi , the new gi function is computed
and added to the corresponding bucket. The elimination of x1 produces an empty-scope
function (i.e., a constant) which is the optimal cost of the problem. In the second phase, BE
considers variables in increasing order and generates the optimal assignment of variables.
The time and space complexity of BE is exponential on a structural parameter from the
constraint graph, called induced width, which captures the maximum arity among all the
gi functions. Without any additional overhead BE can also compute the number of optimal
solutions (see Dechter, 1999, for details).
2.3.4 super-buckets
In some cases, it may be convenient to eliminate a set of variables simultaneously (Dechter
& Fatah, 2001). The elimination of the set of variables Y is performed by collecting in BY
the set of functions mentioning at least one variable of Y . Variables in Y and functions in
BY are replaced by a new function gY defined as,
gY = (

X

f)  Y

f BY

The set BY is called a super-bucket. Note that the elimination of Y can be seen as the
clustering of its variables into a meta-variable xY followed by its elimination.
2.3.5 mini-buckets
When the space complexity of BE is too high, an approximation, called mini buckets
(Dechter & Rish, 2003), can be used. Consider the elimination of xi , with its associated
bucket Bi = {fi1 , . . . , fik }. BE would compute,
gi = (

X

f )  xi

f Bi

The time and space complexity of this computation depends on the arity of gi . If it is beyond
our available resources, we can partition bucket Bi into so-called mini-buckets Bi1 , . . . , Bik
where the number of variables in the scopes of each mini-bucket is bounded by a parameter.
Then we can compute,
X
gij = (
f )  xi , j = 1..k
f Bij

426

fiOn the practical use of variable elimination

6

7

6

1

2

2
2
8

9
3

5

1

8

2

8

9

9
3

1

3

1

8

9

5

5

5

4

4
4

4

A

B

C

D

Figure 2: A constraint graph and its evolution over a sequence of variable eliminations and
instantiations.

where each gij has a bounded arity. Since,
gij

gi

zX }|

(

f Bi

{

f )  xi 

k z X }|
X

(

{

f )  xi

j=1 f Bij

the elimination of variables using mini-buckets yields a lower bound of the actual optimal
cost.
2.3.6 combining search and variable elimination
When plain BE is too costly in space, we can combine it with search (Larrosa & Dechter,
2003). Consider a WCSP whose constraint graph is depicted in Figure 2.A. Suppose that
we want to eliminate a variable but we do not want to compute and store constraints with
arity higher than two. Then we can only take into consideration variables connected to at
most two variables. In the example, variable x7 is the only one that can be selected. Its
elimination transforms the problem into another one whose constraint graph is depicted in
Figure 2.B. Now x6 has its degree decreased to two, so it can also be eliminated. The
new constraint graph is depicted in Figure 2.C. At this point, every variable has degree
greater than two, so we switch to a search schema which selects a variable, say x3 , branches
over its values and produces a set of subproblems, one for each value in its domain. All of
them have the same constraint graph, depicted in Figure 2.D. For each subproblem, it is
possible to eliminate variable x8 and x4 . After their elimination it is possible to eliminate
x2 and x9 , and subsequently x5 and x1 . Eliminations after branching have to be done at
every subproblem since the new constraints with which the eliminated variables are replaced
differ from one subproblem to another. In the example, only one branching has been made.
Therefore, the elimination of variables has reduced the search tree size from d9 to d, where
d is the size of the domains. In the example, we bounded the arity of the new constraints
to two, but it can be generalized to an arbitrary value.

3. Solving Still-life with Variable Elimination
SL(n) can be easily formulated as a WCSP. The most natural formulation associates one
variable xij with each cell (i, j). Each variable has two domain values. If xij = 0 the cell is
427

fiLarrosa, Morancho & Niso

X1
j 2

j 1

j

i 2

j+1

j+2

X2

i 1

X3

i

X4

i+1

X5

i +2

X6

B

A

Figure 3: A: Structure of the constraint graph of SL(n). The node in the center, associated
to cell (i, j), is linked to all cells it interacts with. The shadowed area indicates
the scope of fij . B (left): Constraint graph of SL(6) after clustering cells into
row variables. B (from left to right: Evolution of the constraint graph during the
execution of BE.

dead, if xij = 1 it is alive. There is a cost function fij for each variable xij . The scope of
fij is xij and all its neighbors. It evaluates the stability of xij : if xij is unstable given its
neighbors, fij returns ; else fij returns 1  xij .4 The objective function to be minimized
is,
F =

n X
n
X

fij

i=1 j=1

If the instantiation X represents an unstable pattern, F (X) returns ; else it returns the
number of dead cells. fij can be stored as a table with 29 entries and evaluated in constant
time.
Figure 3.A illustrates the structure of the constraint graph of SL(n). The picture shows
an arbitrary node xij linked to all the nodes it interacts with. For instance, there is an edge
between xij and xi,j+1 because xi,j+1 is a neighbor of xij in the grid and, consequently,
both variables are in the scope of fij . There is an edge between xij and xi1,j2 because
both cells are neighbors of xi1,j1 in the grid and, therefore, both appear in the scope of
fi1,j1 . The shadowed area represents the scope of fij (namely, xij and all its neighbors).
The complete graph is obtained by extending this connectivity pattern to all nodes in the
graph.
For the sake of clarity, we use an equivalent but more compact SL(n) formulation
that makes BE easier to describe and implement: we cluster all variables of each row
into a single meta-variable. Thus, xi denotes the state of cells in the i-th row (namely,
xi = (xi1 , xi2 , . . . , xin ) with xij  {0, 1}). Accordingly, it takes values over the sequences of
n bits or, equivalently, over the natural numbers in the interval [0..2n  1]. Cost functions
are accordingly clustered: there is a cost function fi associated with each row i, defined as,
fi =

n
X

fij

j=1

4. Recall that, as a WCSP, the task is to minimize the number of dead cells. Therefore, we give cost 1 to
dead cells and cost 0 to living cells.

428

fiOn the practical use of variable elimination

For internal rows, the scope of fi is {xi1 , xi , xi+1 }. The cost function of the top row, f1 ,
has scope {x1 , x2 }. The cost function of the bottom row, fn , has scope {xn1 , xn }. If there
is some unstable cell in xi , fi (xi1 , xi , xi+1 ) = . Else, it returns the number of dead cells
in xi . Evaluating fi is (n) because all the bits of the arguments need to be checked. The
new, equivalent, objective function is,
F =

n
X

fi

i=1

Figure 3.B (left) shows the constraint graph of SL(6) with this formulation. An arbitrary
variable xi is connected with the two variables above and the two variables below. The
sequential structure of the constraint graph makes BE very intuitive. It eliminates variables
in decreasing orders. The elimination of xi produces a new function gi = (fi1 + gi+1 )  xi
with scope {xi2 , xi1 }. Figure 3.B (from left to right) shows the evolution of the constraint
graph along the elimination of its variables. Formally, BE applies a recursion that transforms
subproblem P into P xi , where xi is the variable in P with the highest index. It satisfies
the following property,
Property 1 Let gi be the function added by BE to replace xi . Then gi (a, b) is the cost of
the best extension of (xi2 = a, xi1 = b) to the eliminated variables (xi , . . . , xn ). Formally,
gi (a, b) =

min

vi Di ,...,vn Dn

{fi1 (a, b, vi ) + fi (b, vi , vi+1 ) +

+fi+1 (vi , vi+1 , vi+2 ) + . . .
+fn1 (vn2 , vn1 , vn ) + fn (vn1 , vn )}
If gi (a, b) = , it means that the pattern a, b cannot be extended to the inferior rows
with a stable pattern. If gi (a, b) = k (with k 6= ), it means that a, b can be extended and
the optimal extension has k dead cells from xi1 to xn .
The space complexity of BE (n  22n ), due to the space required to store n functions
gi extensionally (2n  2n entries each). Regarding time, computing each entry of gi has
cost (n  2n ) (finding the minimum of 2n alternatives, the computation of each one is
(n)). Since each gi has 22n entries, the total time complexity is (n2  23n ). Observe that
solving SL(n) with BE is an exponential improvement over search algorithms, which have
2
time complexity O(2n ).
Table 4 reports some empirical results. They were obtained with a 2 Ghz Pentium IV
machine with 2 Gb of memory. The first columns reports the problem size, the second
reports the optimal cost as the number of dead cells (in parenthesis, the number of living
cells), the third column reports the number of optimal solutions. We count as different
two solutions even if one can be transformed to the other through a problem symmetry.
The fourth column reports the CPU time of BE in seconds. The fifth, sixth and seventh
columns report the results obtained with the three approaches tried by Bosch and Trick
(2002):5 constraint programming (CP), integer programming (IP), and a more sophisticated
algorithm (CP/IP) which combines CP and IP, and exploits the problem symmetries.
5. The corresponding OPL code is available at http://mat.gsia.cmu.edu/LIFE.

429

fiLarrosa, Morancho & Niso

n
5
6
7
8
9
10
11
12
13
14
15

opt
9(16)
18(18)
21(28)
28(36)
38(43)
46(54)
57(64)
68(76)
79(90)
92(104)
106(119)

n. sol.
1
48
2
1
76
3590
73
129126
1682
11
?

BE
0
0
0
0
4
27
210
1638
13788
105
*

CP
0
0
4
76
> 600
*
*
*
*
*
*

IP
0
1
3
26
> 600
*
*
*
*
*
*

CP/IP
0
0
0
2
20
60
153
11536
12050
5  105
7  105

Figure 4: Experimental results of four different algorithms on the still-life problem. Times
are in seconds.

It can be observed that BE clearly outperforms CP and IP by orders of magnitude.
The n = 14 case is the largest instance that we could solve due to exhausting the available
space. Comparing BE with CP/IP, we observe that there is no clear winner. An additional
observation is that BE scales up very regularly, each execution requiring roughly eight times
more time and four times more space than the previous, which is in clear accordance with
the algorithm complexity.

4. Combining Search and Variable Elimination
One way to overcome the high space complexity of BE is to combine search and variable
elimination in a hybrid approach HYB (Larrosa & Schiex, 2003). The idea is to use search
(i.e, instantiations) in order to break the problem into independent smaller parts where
variable elimination can be efficiently performed.
Let us reformulate the problem in a more convenient way for the hybrid algorithm. For
the sake of simplicity and without loss of generality consider that n is even. We cluster
R
row variables into three meta-variables: xC
i denotes the two central cells of row i, xi and
n
L
xi denote the 2  1 remaining cells on the right and left, respectively (see Figure 5.A).
L
R
Consequently, xC
i takes values in the range [0..3], xi and xi take values in the range
n
1
[0..2 2  1]. Cost functions are accordingly clustered,
n

fiL

=

2
X

fiR =

fij ,

n
X
j= n
+1
2

j=1

The new, equivalent, objective function is,
F =

n
X

(fiL + fiR )

i=1

430

fij

fiOn the practical use of variable elimination

Left

X

L
1

Center

X

C
1

Right

X

Left

R
1

Center

Right

X1

X2

X3
X

L
i

X

C
i

X

R
i

X4

X

L
n

X

C
n

X

X5

R
n

X6

B

A
Left

Center

Right

Left

Center

Right

X1
X1
X2
X2
X3
X3
X4
X4
X5
X5
X6
X6
C

D

Figure 5: Formulation of SL(n) used by the hybrid algorithm. A: Each row is clustered
into three variables. B: Constraint graph of SL(6). C: Constraint graph after
C
C
the assignment of xC
n , xn1 and xn2 . D: Constraint graph after the elimination
L
R
of xn and xn .

431

fiLarrosa, Morancho & Niso

C
L C
L
C
The scopes of internal row functions, fiL and fiR , are {xL
i1 , xi1 , xi , xi , xi+1 , xi+1 } and
C
R
C
R
C
R
L
R
L
C
C
{xi1 , xi1 , xi , xi , xi+1 , xi+1 }. Top functions f1 and f1 have scopes {x1 , x1 , xL
2 , x2 }
R C
R
L
R
L
C
L C
and {xC
1 , x1 , x2 , x2 }. Bottom functions fn and fn have scopes {xn1 , xn1 , xn , xn } and
R
C
R
C
{xn1 , xn1 , xn , xn }. Figure 5.B shows the corresponding constraint graph. The imporR
tance of this formulation is that xL
i and xi are independent (i.e, there is no edge in the
constraint graph connecting left and right variables).

The hybrid algorithm HYB searches over the central variables and eliminates the lateral
variables. Variables are considered in decreasing order of their index. Thus, the algorithm
C
C
starts instantiating xC
n , xn1 and xn2 , which produces a subproblem with the constraint
R
graph shown in Figure 5.C. Observe that variable xL
n (respectively, xn ) is only connected
L
L
R
R
with variables xn1 and xn2 (respectively, xn1 and xn2 ). Then it is eliminated producing
L
R
R
R
a new function gnL with scope {xL
n2 , xn1 } (respectively, gn with scope {xn2 , xn1 }).
Figure 5.D shows the resulting constraint graph. Lateral variables have domains of size
n
n
2 2 1 . Hence, their elimination is space (2n ) and time (23 2 ). It is important to note that
C
C
these eliminations are subject to the current assignment of xC
n , xn1 and xn2 . Therefore,
they have to be recomputed when their value change. After the elimination of xL
n and
C
xR
,
the
algorithm
would
assign
variable
x
which
will
make
possible
the
elimination
of
n
n3
L
R
C
xn1 and xn1 , and so on. At an arbitrary level of search, the algorithm assigns xi , which
R
makes xL
i+2 and xi+2 independent of the central columns and only related to their two
L and
variables above. Then, it eliminates them by replacing the variables by functions gi+2
R with scopes {xL , xL } and {xR , xR }, respectively. Formally, HYB applies a recursion
gi+2
i
i+1
i
i+1
that transforms subproblem P into 4 simpler subproblems {((P |xC =v ) xL ) xR }3v=0 . It
i
i+2
i+2
satisfies the following property,

Property 2 Let giL be the function computed by HYB used to replace variable xL
i . Then
L
giL (a, b) is the cost of the best extension of (xL
=
a,
x
=
b)
to
eliminated
variables
i2
i1
L
R
(xL
i , . . . , xn ), conditioned to the current assignment. Similarly, for the right side, gi (a, b)
R
R
R
is the cost of the best extension of (xi2 = a, xi1 = b) to eliminated variables (xi , . . . , xR
n ),
conditioned to the current assignment.

L (a, b) among all comA consequence of the previous Property is that the minimum gi+2
binations of a and b is a lower bound of the best cost that can be obtained in the left
L (a, b)} +
part of the grid if we continue the current line of search. Therefore, mina,b {gi+2
R
mina,b {gi+2 (a, b)} is a valid lower bound of the current node and can be used for pruning
purposes.

The space complexity of the algorithm is (n  2n ), due to the giL and giR functions
which need to be explicitly stored. The time complexity is O(n  23.5n ), because O(4n )
nodes may be visited (n variables with domains of size 4) and the cost of processing each
n
node is (n  23 2 ) due to the variable eliminations.
Thus, comparing with BE, the time complexity increases from (n2 23n ) to O(n23.5n ).
This is the prize HYB pays for the space decrement from (n  22n ) to (n  2n ).
432

fiOn the practical use of variable elimination

4.1 Refining the Lower Bound
It is well-known that the average-case efficiency of search algorithms depends greatly on
the lower bound that they use. Our algorithm is using a poor lower bound based on the giL
and giR functions, only.
Kask and Dechter (2001) proposed a general method to incorporate information from
yet-unprocessed variables into the lower bound. Roughly, the idea is to run mini buckets
(MB) prior search and save intermediate functions for future use. MB is executed using the
reverse order in which search will instantiate the variables. When the execution of MB is
completed, the search algorithm is executed. At each node, it uses mini-bucket functions
as compiled look-ahead information. In this Subsection, we show how we have adapted this
idea to SL(n) and how we have integrated it into HYB.
C
R
Consider SL(n) formulated in terms of left, central and right variables (xL
i , xi , xi ).
L
C
R
The exact elimination of the first row variables (x1 , x1 , x1 ) can be done using super-bucket
B1 = {f1L , f1R , f2L , f2R } and computing the function,
C
R
h1 = (f1L + f1R + f2L + f2R )  {xL
1 , x1 , x1 }
C
R L C
R
The scope of h1 is {xL
2 , x2 , x2 , x3 , x3 , x3 }. Using the mini-buckets idea, we partition the
L
L
L
R
bucket into B1 = {f1 , f2 } and B1 = {f1R , f2R }. Then, we approximate h1 by two smaller
R
functions hL
1 and h1 ,
L
L
L C
hL
1 = (f1 + f2 )  {x1 , x1 }
R
R
C
R
hR
1 = (f1 + f2 )  {x1 , x1 }
R
L C
L C
C
R C
R
The scopes of hL
1 and h1 are {x2 , x2 , x3 , x3 } and {x2 , x2 , x3 , x3 }, respectively. The same
idea is repeated row by row in increasing order. In general, processing row i, yields two
functions,
L
L
L C
hL
i = (hi1 + fi+1 )  {xi , xi }
R
R
C
R
hR
i = (hi1 + fi+1 )  {xi , xi }
R
L
C
L
C
C
R
C
R
The scopes of hL
i and hi are {xi+1 , xi+1 , xi+2 , xi+2 } and {xi+1 , xi+1 , xi+2 , xi+2 }, respecL
0
0
tively. By construction, hi (a, a , b, b ) contains the cost of the best extension of a, a0 , b, b0
C
L C
to processed variables xL
i , xi , . . . , x1 , x1 considering left functions only. We have the same
0
0
property for hR
i (a , a, b , b) and right functions.
The complexity of MB is space (n2n ) and time (n2 21.5n ). Since these complexities
are smaller than the complexity of HYB, running this pre-process does not affect its overall
complexity.
R
After MB is executed, HYB can use the information recorded in the hL
i and hi functions.
L
R
Consider an arbitrary node in which HYB assigns xC
i and eliminates xi+2 and xi+2 . Let a
L
L
L (a, b)
and b be domain values of variables xi and xi+1 . From Property 2 we have that gi+2
contains the best extension of a, b that can be attained in the left part of rows i + 1
to n as long as the current assignment X C is maintained. Additionally, we have that
C
C
hL
i1 (a, xi , b, xi+1 ) contains the best extension of a, b that can be attained in the left part
L (a, b) + hL (a, xC , b, xC ) is a lower bound for a, b and X C
of rows i to 1. Therefore, gi+2
i1
i
i+1
of the left part of the grid. Consequently,
L
C
C
mina,b[0..2 n2 1 1] {gi+2
(a, b) + hL
i1 (a, xi , b, xi+1 )}

433

fiLarrosa, Morancho & Niso

is a lower bound of the left part of the grid for the current assignment. With the same
reasoning on the right part we have that,
L
C
C
mina,b[0..2 n2 1 1] {gi+2
(a, b) + hL
i1 (a, xi , b, xi+1 )} +
R
C
C
+mina,b[0..2 n2 1 1] {gi+2
(a, b) + hR
i1 (xi , a, xi+1 , b)}

is a lower bound of the current assignment.
4.2 Refining the Upper Bound
The efficiency of the algorithm also depends on the initial value of the upper bound. A
good upper bound facilitates pruning earlier in the search tree. Bosch and Trick (2002)
suggested to modify SL(n) by adding the additional constraint of considering symmetric
patterns, only. Since the space of solutions becomes considerably smaller, the problem is
presumably simpler. Clearly, the cost of an optimal symmetric stable pattern is an upper
bound of the optimal cost of SL(n). It has been observed that such upper bounds are very
tight.
Since the motivation of our work is to use variable elimination techniques, we have
considered still-lifes which are symmetric over a vertical reflection, because they can be
efficiently solved using BE. The symmetric still-life problem SSL(n) consists on finding a
n  n stable pattern of maximum density in the game of life subject to a vertical reflection
symmetry (namely, the state of cells (i, j) and (i, n  j + 1) must be the same.6
Adapting BE to solve SSL(n) is extremely simple: we only need to remove symmetrical
values from the domains. Let us assume that n is an even number (the odd case is similar).
We represent a symmetric sequences of bits of length n by considering the left side of the
sequence (i.e, the first n/2 bits). The right part is implicit in the left part. Thus, we
n
represent symmetrical sequences of n bits as integers in the interval [0..2 2  1]. Reversing a
sequence of bits a is noted a. Hence, if a is a sequence of n/2 bits, a  a is the corresponding
symmetrical sequence of n bits.
The complexity of BE, when applied to SSL(n) is time (n2 21.5n ) and space (n2n ).
Therefore, executing it prior HYB and setting the upper bound with its optimal cost does
not affect the overall complexity of the hybrid.
4.3 Further Exploitation of Symmetries
SL(n) is a highly symmetric problem. For any stable pattern, it is possible to create an
equivalent pattern by: (i) rotating the board by 90, 180 or 270 degrees, (ii) reflecting
the board horizontally, vertically or along one diagonal or (iii) doing any combination of
rotations and reflections.
Symmetries can be exploited at very different algorithmic levels. In general, we can
save any computation whose outcome is equivalent to a previous computation due to a
symmetry if we have kept its outcome. For instance, in MB it is not necessary to compute
0
0
L
0
0
hR
i (a , a, b , b) because it is equal to hi (a, a , b, b ) due to the vertical reflection symmetry.
C
C
Another example occurs in HYB. Let xn = vn , xC
n1 = vn1 , . . . , xi = vi be the current
6. Unlike Smiths (2002) work we cannot easily exploit a larger variety of symmetries such as rotations and
diagonal reflections.

434

fiOn the practical use of variable elimination

n
13
14
15
16
17
18
19
20
22
24
26
28

opt
79(90)
92(104)
106(119)
120(136)
137(152)
153(171)
171(190)
190(210)
?
?
?
?

opt-SSL
79
92
106
120
137
154
172
192
232
276
326
378

CP/IP
12050
5  105
7  105
*
*
*
*
*
*
*
*
*

BE
13788
105
*
*
*
*
*
*
*
*
*
*

HYB
2
2
58
7
1091
2029
56027
2  105
*
*
*
*

HYB no LB
2750
7400
2  105
6  105
*
*
*
*
*
*
*
*

HYB no UB
2
3
61
49
2612
2311
56865
2  105
*
*
*
*

Figure 6: Experimental results of three different algorithms on the still-life problem. Times
are in seconds.

C
C
assignment. The reversed assignment xC
n = vn , xn1 = vn1 , . . . , xi = vi is equivalent due
to the vertical reflection symmetry. Thus, if it has already been considered, the algorithm
can backtrack. Our implementation uses these tricks and some others which we do not
report because it would require a much lower level description of the algorithms.

5. Experimental Results
Figure 6 shows the empirical performance of our hybrid algorithm. The first column contains
the problem size. The second column contains the optimal value as the number of dead
cells (in parenthesis the corresponding number of living cells). The third column contains
the optimal value of the symmetrical problem SSL(n), obtained by executing BE. It can
be observed that SSL(n) provides very tight upper bounds to SL(n). The fourth column
reports the time obtained with the CP/IP algorithm (Bosch & Trick, 2002). The fifth
column reports times obtained with BE. The sixth column contains times obtained with
our hybrid algorithm HYB. As it can be seen, the performance of HYB is spectacular. The
n = 14 and n = 15 instances, which require several days of CPU, are solved by HYB in a few
seconds. Instances up to n = 18 are solved in less than one hour. The largest instance that
we can solve is n = 20, which requires about two days of CPU (Figure 7 shows the optimal
n = 19 and n = 20 still-lifes). Regarding space, our computer can handle executions of
HYB up to n = 22. However, neither the n = 21 nor the n = 22 instance could be solved
within a week of CPU. It may seem that solving the n = 20 instance is a petty progress with
respect previous results on the problem. This is clearly not the case. The search space of
2
2
the n = 15 and n = 20 instances have size 215 = 2225 and 220 = 2400 , respectively. Thus,
we have been able to solve a problem with a search space 2175 times larger than before.
Since BE scales up very regularly, we can accurately predict that it would require 4000 Gb
of memory and about 7 centuries to solve the n = 20 instance.
435

fiLarrosa, Morancho & Niso

Figure 7: Maximum density still-lifes for n = 19 and n = 20.

Since HYB combines several techniques, it is interesting to assess the impact of each
one. The seventh column reports times obtained with HYB without using mini-buckets
information in the lower bound. As can be seen, the algorithm is still better than plain BE,
but it performance is dramatically affected. The information gathered during the preprocess
improves the quality of the lower bound and anticipates pruning. Finally, the eighth column
reports times obtained with HYB without having the upper bound initialized to SSL(n).
In this case we see that the importance of this technique is quite limited. The reason is
that HYB, even with a bad initial upper bound, finds the optimum very rapidly and, after
that moment, the quality of the initial upper bound becomes irrelevant.

6. Extension to Other Domains
The SL(n) problem has a very well defined structure, and the hybrid algorithm that we
have proposed makes an ad hoc exploitation of it. It is easy to find the right variables to
instantiate and eliminate. It is also easy to find a variable order for which mini buckets
produces good quality lower bounds. A natural question is whether it is possible to apply
similar ideas to not so well structured problems. The answer is that it is often possible,
although we need to rely on more naive and consequently less efficient exploitation of the
problems structure. In this Section we support our claim by reporting additional experimental results on different benchmarks. In particular, we consider spot5 and DIMACS
instances. Spot5 instances are optimization problems taken from the scheduling of an earth
observation satellite (Bensana, Lemaitre, & Verfaillie, 1999). The DIMACS benchmark contains SAT instances from several domain. Since we are concerned with optimization tasks,
we have selected some unsatisfiable instances and solved the Max-SAT task (i.e, given an
unsatisfiable SAT instance, find the maximum number of clauses that can be simultaneously
satisfied), which can be modeled as a WCSP (de Givry, Larrosa, Meseguer, & Schiex, 2003).
We consider aim instances (artificially generated random 3-SAT), pret (graph coloring), ssa
and bf (circuit fault analysis).
Figure 8 shows the constraint graph of one instance of each domain, as visualized by
LEDA graph editor. It can be observed that these graphs do not have an obvious pattern
436

fiOn the practical use of variable elimination

Figure 8: Constraint graph of four WCSP instances. From the top-left corner, clockwise,
aim-100-1-6-no-1, pret60-25, ssa0432-003 and Spot5-404.

to be exploited. Thus, we have to use variable elimination techniques in a more naive way.
We solve the problems with the generic WCSP solver toolbar7 (TB). It performs a depthfirst branch-and-bound search and it is enhanced with general-purpose dynamic variable and
value ordering heuristics. We modified toolbar to combine search and variable elimination
as follows: at an arbitrary subproblem, every variable with degree less than 3 is eliminated.
Only when all the variables have degree larger than or equal to 3, an unassigned variable is
heuristically selected and each of its domain values are heuristically ordered and sequentially
instantiated. The process is recursively applied to each of the subproblems. Note that
this is a generic version of the HYB algorithm where the decision of which variables are
instantiated and which variables are eliminated is left to a heuristic, instead of establishing
7. Available at http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/SoftCSP.

437

fiLarrosa, Morancho & Niso

it by hand. We will refer to this implementation as TBHY B . Toolbar offers a variety
of lower bounds based on different forms of local consistency (Larrosa & Schiex, 2003).
One of them, directional arc consistency (DAC*), is essentially equivalent to mini-buckets
of size 2 and, therefore, similar in spirit to the lower bound computed by HYB. However,
unlike HYB where mini-buckets are executed only once as a pre-process, toolbar executes
DAC* at every search state, subject to the current subproblem. It has been shown by Kask
(2000) that this approach is generally more efficient. The other main difference with respect
HYB, is that toolbar executes DAC* subject to an arbitrary variable ordering (in HYB
a good order was identified from the problem structure). Other lower bounds available
in toolbar are node consistency (NC*) which is weaker than DAC*, and full directional
arc consistency (FDAC*) which can be seen as a (stronger) refinement of DAC*. We have
F DAC
B
experimented with four algorithms: TBN C , TBDAC , TBDAC
HY B and TBHY B , where A
denotes algorithm A with lower bound B.
Most spot5 instances are too difficult for toolbar. Therefore, we decreased their size
by letting toolbar make a sequence of k greedy assignments driven by its default variable
and value ordering heuristics. The result is a subproblem with k less variables. In the
following, Ik denotes instance I where k variables have been greedily assigned by toolbar
with default parameters.
Table 9 reports the result of these experiments. The first column indicates the instances
and subsequent columns indicate the CPU time (in seconds) required by the different algorithms. A time limit of 3600 seconds was set up for each execution. It can be observed that
toolbar with the weakest lower bound (TBN C ) is usually the most inefficient alternative.
It cannot solve any of the spot5 instances and also fails with several aim and ssa instances.
When toolbar is enhanced with a mini buckets lower bound (TBDAC ) all spot5 problems
are solved. In the other domains, the new lower bound does not produce a significant effect. When we further add variable elimination (TBDAC
HY B ) all the problems are solved. In
general, there is a clear speed-up. The worst improvements are in the pret instances where
the time is divided by a factor of 2 and the best ones are obtained in the spot5 50340 and
ssa7552-158 instances which are solved instantly. Typical speed-ups range from 5 to 10.
DAC ) has a limited
Finally, we observe that the addition of the stronger lower bound (TBFHY
B
effect in these problems. Only the execution of instance ssa7552-038 is clearly accelerated.
Therefore, from these experiments we can conclude that the main techniques that we used
to solve the still-life problem can also be successfully applied to other domains.

7. Conclusions
In this paper we have studied the applicability of variable elimination to the problem of
finding still-lifes. Finding still-lifes is a challenging problem and developing new solving
techniques is an interesting task per se. Thus, the first contribution of this paper is the
observation that plain variable elimination (i.e, BE) is competitive in practice and provides
time complexity exponentially better than search-based approaches. Besides, we have developed an algorithm with which we have been able to solve up to the n = 20 instance,
with which we clearly improved previous results. The second contribution of the paper
has a deeper insight. Our algorithm uses recent techniques based on variable elimination.
Since these techniques are little known and rarely applied in the constraints community,
438

fiOn the practical use of variable elimination

Problem
Spot5 4040
Spot5 408100
Spot5 412200
Spot5 414260
Spot5 50340
Spot5 505120
Spot5 507200
Spot5 509240
aim-100-1-6-no-1
aim-100-1-6-no-2
aim-100-1-6-no-3
aim-100-1-6-no-4
aim-100-2-0-no-1
aim-100-2-0-no-2
aim-100-2-0-no-3
aim-100-2-0-no-4
bf0432-007
pret60-25
pret60-40
ssa0432-003
ssa2670-141
ssa7552-038
ssa7552-158

T BN C
2516
1191
1222
2162
110
110
22
-

T BDAC
242
314
223
1533
546
3353
204
684
2007
931
850
1599
120
120
22
-

DAC
T BHY
B
40
48
47
221
0
84
58
166
1665
707
1960
2716
830
479
319
738
1206
49
48
5
749
20
0

F DAC
T BHY
B
40
43
42
139
0
84
42
121
1427
571
1627
2375
583
285
278
600
1312
56
56
5
767
2
1

Figure 9: Experimental results in some WCSP instances with four different algorithms.
Each column reports CPU time in seconds. Symbol - indicates that a time limit
of 3600 seconds has been reached.

the results presented in this paper add new evidence of their potential. We have also shown
that variable elimination can be used beyond the academic still-life problem by providing
experimental results in some unstructured realistic problems from different domains.

Acknowledgments
The authors are grateful to Barbara Smith, Neil Yorke-Smith and the anonymous reviewers
for their useful comments at different stages of the work reported in this article. Marti
Sanchez kindly made the plots in Figure 8. This research has been funded by the Spanish
CICYT under project TIC2002-04470-C03-01.

References
Bensana, E., Lemaitre, M., & Verfaillie, G. (1999). Earth observation satellite management.
Constraints, 4(3), 293299.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press.
439

fiLarrosa, Morancho & Niso

Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfaction and
optimization. Journal of the ACM, 44 (2), 201236.
Bosch, R., & Trick, M. (2002). Constraint programming and hybrid formulations for three
life designs. In Proceedings of the International Workshop on Integration of AI and
OR Techniques in Constraint Programming for Combinatorial Optimization Problems,
CP-AI-OR02, pp. 7791.
Cabon, B., de Givry, S., Lobjois, L., Schiex, T., & Warners, J. (1999). Radio link frequency
assignment. Constraints, 4, 7989.
de Givry, S., Larrosa, J., Meseguer, P., & Schiex, T. (2003). Solving max-sat as weighted
csp. In Proc. of the 9th CP, pp. 363376, Kinsale, Ireland. LNCS 2833. Springer
Verlag.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113, 4185.
Dechter, R., & Pearl, J. (1989). Tree clustering for constraint networks. Artificial Intelligence, 38, 353366.
Dechter, R., & Fatah, Y. E. (2001). Topological parameters for time-space tradeoff. Artificial
Intelligence, 125 (12), 93118.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general scheme for bounded inference.
Journal of the ACM, 50 (2), 107153.
Gardner, M. (1970). The fantastic combinations of john conways new solitary game. Scientific American, 223, 120123.
Kask, K. (2000). New search heuristics for max-csp. In Proc. of the 6th CP, pp. 262277,
Singapore. LNCS 1894. Springer Verlag.
Kask, K., & Dechter, R. (2001). A general scheme for automatic generation of search
heuristics from specification dependencies. Artificial Intelligence, 129, 91131.
Larrosa, J., & Dechter, R. (2003). Boosting search with variable elimination in constraint
optimization and constraint satisfaction problems. Constraints, 8 (3), 303326.
Larrosa, J., & Schiex, T. (2003). In the quest of the best form of local consistency for
weighted csp. In Proc. of the 18th IJCAI, Acapulco, Mexico.
Pearl, J. (1988). Probabilistic Inference in Intelligent Systems. Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Sandholm, T. (1999). An algorithm for optimal winner determination in combinatorial
auctions. In IJCAI-99, pp. 542547.
Smith, B. (2002). A dual graph translation of a problem in life. In Proc. of CP-2002, pp.
01, Ithaca, USA. LNCS. Springer Verlag.

440

fiJournal of Artificial Intelligence Research 23 (2005) 299-330

Submitted 07/04; published 03/05

Combining Knowledge- and Corpus-based
Word-Sense-Disambiguation Methods
Andres Montoyo

montoyo@dlsi.ua.es

Dept. of Software and Computing Systems
University of Alicante, Spain

Armando Suarez

armando@dlsi.ua.es

Dept. of Software and Computing Systems
University of Alicante, Spain

German Rigau

rigau@si.ehu.es

IXA Research Group
Computer Science Department
Basque Country University, Donostia

Manuel Palomar

mpalomar@dlsi.ua.es

Dept. of Software and Computing Systems
University of Alicante, Spain

Abstract
In this paper we concentrate on the resolution of the lexical ambiguity that arises when
a given word has several different meanings. This specific task is commonly referred to
as word sense disambiguation (WSD). The task of WSD consists of assigning the correct
sense to words using an electronic dictionary as the source of word definitions. We present
two WSD methods based on two main methodological approaches in this research area: a
knowledge-based method and a corpus-based method. Our hypothesis is that word-sense
disambiguation requires several knowledge sources in order to solve the semantic ambiguity
of the words. These sources can be of different kinds for example, syntagmatic, paradigmatic or statistical information. Our approach combines various sources of knowledge,
through combinations of the two WSD methods mentioned above. Mainly, the paper concentrates on how to combine these methods and sources of information in order to achieve
good results in the disambiguation. Finally, this paper presents a comprehensive study and
experimental work on evaluation of the methods and their combinations.

1. Introduction
Knowledge technologies aim to provide meaning to the petabytes of information content
that our multilingual societies will generate in the near future. Specifically, a wide range of
advanced techniques are required to progressively automate the knowledge lifecycle. These
include analyzing, and then automatically representing and managing, high-level meanings
from large collections of content data. However, to be able to build the next generation
of intelligent open-domain knowledge application systems, we need to deal with concepts
rather than words.
c
2005
AI Access Foundation. All rights reserved.

fiMontoyo, Suarez, Rigau, & Palomar

1.1 Dealing with Word Senses
In natural language processing (NLP), word sense disambiguation (WSD) is defined as the
task of assigning the appropriate meaning (sense) to a given word in a text or discourse.
As an example, consider the following three sentences:
1. Many cruise missiles have fallen on Baghdad.
2. Music sales will fall by up to 15% this year.
3. U.S. officials expected Basra to fall early.
Any system that tries to determine the meanings of the three sentences will need to
represent somehow three different senses for the verb fall. In the first sentence, the missiles
have been launched on Baghdad. In the second sentence, sales will decrease, and in the
third the city will surrender early. WordNet 2.0 (Miller, 1995; Fellbaum, 1998)1 contains
thirty-two different senses for the verb fall as well as twelve different senses for the noun
fall. Note also that the first and third sentence belong to the same, military domain, but
use the verb fall with two different meanings.
Thus, a WSD system must be able to assign the correct sense of a given word, in
these examples, fall, depending on the context in which the word occurs. In the example
sentences, these are, respectively, senses 1, 2 and 9, as listed below.
 1. falldescend in free fall under the influence of gravity (The branch fell from
the tree; The unfortunate hiker fell into a crevasse).
 2. descend, fall, go down, come downmove downward but not necessarily all the
way (The temperature is going down; The barometer is falling; Real estate
prices are coming down).
 9. fallbe captured (The cities fell to the enemy).
Providing innovative technology to solve this problem will be one of the main challenges
in language engineering to access advanced knowledge technology systems.
1.2 Word-Sense Disambiguation
Word sense ambiguity is a central problem for many established Human Language Technology applications (e.g., machine translation, information extraction, question answering,
information retrieval, text classification, and text summarization) (Ide & Veronis, 1998).
This is also the case for associated subtasks (e.g., reference resolution, acquisition of subcategorization patterns, parsing, and, obviously, semantic interpretation). For this reason,
many international research groups are working on WSD, using a wide range of approaches.
However, to date, no large-scale, broad-coverage, accurate WSD system has been built (Snyder & Palmer, 2004). With current state-of-the-art accuracy in the range 6070%, WSD is
one of the most important open problems in NLP.
1. http://www.cogsci.princeton.edu/wn/

300

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Even though most of the techniques for WSD usually are presented as stand-alone
techniques, it is our belief, following McRoy (1992), that full-fledged lexical ambiguity
resolution will require to integrate several information sources and techniques.
In this paper, we present two complementary WSD methods based on two different
methodological approaches, a knowledge-based and a corpus-based methods, as well as several
methods that combine both into hybrid approaches.
The knowledge-based method disambiguates nouns by matching context with information from a prescribed knowledge source. WordNet is used because it combines the characteristics of both a dictionary and a structured semantic network, providing definitions
for the different senses of the English words and defining groups of synonymous words by
means of synsets, which represent distinct lexical concepts. WordNet also organizes words
into a conceptual structure by representing a number of semantic relationships (hyponymy,
hypernymy, meronymy, etc.) among synsets.
The corpus-based method implements a supervised machine-learning (ML) algorithm
that learns from annotated sense examples. The corpus-based system usually represents
linguistic information for the context of each sentence (e.g., usage of an ambiguous word) in
the form of feature vectors. These features may be of a distinct nature: word collocations,
part-of-speech labels, keywords, topic and domain information, grammatical relationships,
etc. Based on these two approaches, the main objectives of the work presented in this paper
are:
 To study the performance of different mechanisms of combining information sources
by using knowledge-based and corpus-based WSD methods together.
 To show that a knowledge-based method can help a corpus-based method to better
perform the disambiguation process and vice versa.
 To show that the combination of both approaches outperforms each of the methods
taken individually, demonstrating that the two approaches can play complementary
roles.
 Finally, to show that both approaches can be applied in several languages. In particular, we will perform several experiments in Spanish and English.
In the following section a summary of the background of word sense disambiguation is
presented. Sections 2.1 and 2.2 describe the knowledge-based and corpus-based systems
used in this work. Section 3 describes two WSD methods: the specification marks method
and the maximum entropy-based method. Section 4 presents an evaluation of our results
using different system combinations. Finally, some conclusions are presented, along with a
brief discussion of work in progress.

2. Some Background on WSD
Since the 1950s, many approaches have been proposed for assigning senses to words in
context, although early attempts only served as models for toy systems. Currently, there
are two main methodological approaches in this area: knowledge-based and corpus-based
methods. Knowledge-based methods use external knowledge resources, which define explicit
301

fiMontoyo, Suarez, Rigau, & Palomar

sense distinctions for assigning the correct sense of a word in context. Corpus-based methods
use machine-learning techniques to induce models of word usages from large collections of
text examples. Both knowledge-based and corpus-based methods present different benefits
and drawbacks.
2.1 Knowledge-based WSD
Work on WSD reached a turning point in the 1980s and 1990s when large-scale lexical
resources such as dictionaries, thesauri, and corpora became widely available. The work
done earlier on WSD was theoretically interesting but practical only in extremely limited
domains. Since Lesk (1986), many researchers have used machine-readable dictionaries
(MRDs) as a structured source of lexical knowledge to deal with WSD. These approaches,
by exploiting the knowledge contained in the dictionaries, mainly seek to avoid the need for
large amounts of training material. Agirre and Martinez (2001b) distinguish ten different
types of information that can be useful for WSD. Most of them can be located in MRDs, and
include part of speech, semantic word associations, syntactic cues, selectional preferences,
and frequency of senses, among others.
In general, WSD techniques using pre-existing structured lexical knowledge resources
differ in:
 the lexical resource used (monolingual and/or bilingual MRDs, thesauri, lexical knowledge base, etc.);
 the information contained in this resource, exploited by the method; and
 the property used to relate words and senses.
Lesk (1986) proposes a method for guessing the correct word sense by counting word
overlaps between dictionary definitions of the words in the context of the ambiguous word.
Cowie et al. (1992) uses the simulated annealing technique for overcoming the combinatorial
explosion of the Lesk method. Wilks et al. (1993) use co-occurrence data extracted from an
MRD to construct word-context vectors, and thus word-sense vectors, to perform a large set
of experiments to test relatedness functions between words and vector-similarity functions.
Other approaches measure the relatedness between words, taking as a reference a structured semantic net. Thus, Sussna (1993) employs the notion of conceptual distance between
network nodes in order to improve precision during document indexing. Agirre and Rigau
(1996) present a method for the resolution of the lexical ambiguity of nouns using the WordNet noun taxonomy and the notion of conceptual density. Rigau et al. (1997) combine a
set of knowledge-based algorithms to accurately disambiguate definitions of MRDs. Mihalcea and Moldovan (1999) suggest a method that attempts to disambiguate all the nouns,
verbs, adverbs, and adjectives in a given text by referring to the senses provided by WordNet. Magnini et al. (2002) explore the role of domain information in WSD using WordNet
domains (Magnini & Strapparava, 2000); in this case, the underlying hypothesis is that
information provided by domain labels offers a natural way to establish semantic relations
among word senses, which can be profitably used during the disambiguation process.
Although knowledge-based systems have been proven to be ready-to-use and scalable
tools for all-words WSD because they do not require sense-annotated data (Montoyo et al.,
302

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

2001), in general, supervised, corpus-based algorithms have obtained better precision than
knowledge-based ones.
2.2 Corpus-based WSD
In the last fifteen years, empirical and statistical approaches have had a significantly increased impact on NLP. Of increasing interest are algorithms and techniques that come from
the machine-learning (ML) community since these have been applied to a large variety of
NLP tasks with remarkable success. The reader can find an excellent introduction to ML,
and its relation to NLP, in the articles by Mitchell (1997), Manning and Schutze (1999), and
Cardie and Mooney (1999), respectively. The types of NLP problems initially addressed by
statistical and machine-learning techniques are those of language- ambiguity resolution, in
which the correct interpretation should be selected from among a set of alternatives in a
particular context (e.g., word-choice selection in speech recognition or machine translation,
part-of-speech tagging, word-sense disambiguation, co-reference resolution, etc.). These
techniques are particularly adequate for NLP because they can be regarded as classification
problems, which have been studied extensively in the ML community. Regarding automatic
WSD, one of the most successful approaches in the last ten years is supervised learning
from examples, in which statistical or ML classification models are induced from semantically annotated corpora. Generally, supervised systems have obtained better results than
unsupervised ones, a conclusion that is based on experimental work and international competitions2 . This approach uses semantically annotated corpora to train machinelearning
(ML) algorithms to decide which word sense to choose in which contexts. The words in
such annotated corpora are tagged manually using semantic classes taken from a particular
lexical semantic resource (most commonly WordNet). Many standard ML techniques have
been tried, including Bayesian learning (Bruce & Wiebe, 1994), Maximum Entropy (Suarez
& Palomar, 2002a), exemplar-based learning (Ng, 1997; Hoste et al., 2002), decision lists
(Yarowsky, 1994; Agirre & Martinez, 2001a), neural networks (Towell & Voorhees, 1998),
and, recently, margin-based classifiers like boosting (Escudero et al., 2000) and support
vector machines (Cabezas et al., 2001).
Corpus-based methods are called supervised when they learn from previously senseannotated data, and therefore they usually require a large amount of human intervention
to annotate the training data (Ng, 1997). Although several attempts have been made (e.g.,
Leackock et al., 1998; Mihalcea & Moldovan, 1999; Cuadros et al., 2004), the knowledge
acquisition bottleneck (too many languages, too many words, too many senses, too many
examples per sense) is still an open problem that poses serious challenges to the supervised
learning approach for WSD.

3. WSD Methods
In this section we present two WSD methods based, respectively, on the two main methodological approaches outlined above: a specification marks method (SM) (Montoyo & Palomar, 2001) as a knowledge-based method, and a maximum entropy-based method (ME)
(Suarez & Palomar, 2002b) as a corpus-based method. The selected methods can be seen
2. http://www.senseval.org

303

fiMontoyo, Suarez, Rigau, & Palomar

as representatives of both methodological approaches. The specification marks method
is inspired by the conceptual density method (Agirre & Rigau, 1996) and the maximum
entropy method has been also used in other WSD systems (Dang et al., 2002).
3.1 Specification Marks Method
The underlying hypothesis of this knowledge base method is that the higher the similarity
between two words, the larger the amount of information shared by two of its concepts. In
this case, the information commonly shared by several concepts is indicated by the most
specific concept that subsumes them in the taxonomy.
The input for this WSD module is a group of nouns W = {w1 , w2 , ..., wn } in a context. Each word wi is sought in WordNet, each having an associated set of possible senses
Si = {Si1 , Si2 , ..., Sin }, and each sense having a set of concepts in the IS-A taxonomy (hypernymy/hyponymy relations). First, this method obtains the common concept to all the
senses of the words that form the context. This concept is marked by the initial specification mark (ISM). If this initial specification mark does not resolve the ambiguity of the
word, we then descend through the WordNet hierarchy, from one level to another, assigning
new specification marks. For each specification mark, the number of concepts contained
within the subhierarchy is then counted. The sense that corresponds to the specification
mark with the highest number of words is the one chosen to be sense disambiguated within
the given context. Figure 1 illustrates graphically how the word plant, having four different
senses, is disambiguated in a context that also has the words tree, perennial, and leaf. It
can be seen that the initial specification mark does not resolve the lexical ambiguity, since
the word plant appears in two subhierarchies with different senses. The specification mark
identified by {plant#2, flora#2}, however, contains the highest number of words (three)
from the context and will therefore be the one chosen to resolve the sense two of the word
plant. The words tree and perennial are also disambiguated, choosing for both the sense
one. The word leaf does not appear in the subhierarchy of the specification mark {plant#2,
flora#2}, and therefore this word has not been disambiguated. These words are beyond
the scope of the disambiguation algorithm. They will be left aside to be processed by a
complementary set of heuristics (see section 3.1.2).
3.1.1 Disambiguation Algorithm
In this section, we formally describe the SM algorithm which consists of the following five
steps:
Step 1:
All nouns are extracted from a given context. These nouns constitute the input context,
Context = {w1 , w2 , ..., wn }. For example, Context = {plant, tree, perennial, leaf }.
Step 2:
For each noun wi in the context, all its possible senses Si = {Si1 , Si2 , ..., Sin } are
obtained from WordNet. For each sense Sij , the hypernym chain is obtained and stored
in order into stacks. For example, Table 1 shows all the hypernyms synsets for each
sense of the word Plant.
Step 3:
To each sense appearing in the stacks, the method associates the list of subsumed senses
304

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Inicial
Specification
Mark (ISM)

{entity#1}

{object#1}

SM

{life form#1}
SM
{natural object#1}
{plant#2, flora#2} (*)

{substance#1}

{part#4}

{artifact#1}
{person#1}
{material#1}

{section#4}

{plant part#1}
{perennial#1}

{vascular plant#1}

{Structure#1}
{paper#1}

{entertainer#1}
{woody plant#1}

{plant organ#1}

{building complex#1}

{performer#1}

{leaf#3}
{sheet#2}

{leaf#1}
{leaf#2}

{tree#1}

{plant#1}
{actor#1}
{plant#4}

Figure 1: Specification Marks
plant#1
building complex#1
structure#1
artifact#1
object#1
entity#1

plant#2
life form#1
entity#1

plant#3
contrivance#3
scheme#1
plan of action#1
plan#1
idea#1
content#5
cognition#1
psychological feature#1

plant#4
actor#1
performer#1
entertainer#1
person#1
life form#1
entity#1

Table 1: Hypernyms synsets of plant
from the context (see Figure 2, which illustrates the list of subsumed senses for plant#1
and plant#2 ).
Step 4:
Beginning from the initial specification marks (the top synsets), the program descends
recursively through the hierarchy, from one level to another, assigning to each specification mark the number of context words subsumed.
Figure 3 shows the word counts for plant#1 through plant#4 located within the specification mark entity#1, ..., life form#1, flora#2. For the entity#1 specification mark,
senses #1, #2, and #4 have the same maximal word counts (4). Therefore, it is not
possible to disambiguate the word plant using the entity#1 specification mark, and
it will be necessary to go down one level of the hyponym hierarchy by changing the
specification mark. Choosing the specification mark life form#1, senses #2 and #4 of
plant have the same maximal word counts (3). Finally, it is possible to disambiguate
the word plant with the sense #2 using the {plant#2, flora#2} specification mark,
because of this sense has the higher word density (in this case, 3).
305

fiMontoyo, Suarez, Rigau, & Palomar

For PLANT:
For PLANT#1:
plant#1  plant#1
building complex#1  plant#1
structure#1  plant#1
artifact#1  plant#1
object#1  plant#1, leaf#1, leaf#2, leaf#3
entity#1  plant#1, plant#2, plant#4, tree#1, perennial#1, leaf#1, leaf#2, leaf#3
For PLANT#2:
plant#2  plant#2, tree#1, perennial#1
life form#1  plant#2, plant#4, tree#1, perennial#1
entity#1  plant#1, plant#2, plant#4, tree#1, perennial#1, leaf#1, leaf#2, leaf#3

Figure 2: Data Structure for Senses of the Word Plant
For PLANT
located within the specification mark {entity#1}
For PLANT#1 : 4 (plant, tree, perennial, leaf)
For PLANT#2 : 4 (plant, tree, perennial, leaf)
For PLANT#3 : 1 (plant)
For PLANT#4 : 4 (plant, tree, perennial, leaf)

located within the specification mark {life form#1}
For PLANT#1 : 1 (plant)
For PLANT#2 : 3 (plant, tree, perennial)
For PLANT#3 : 1 (plant)
For PLANT#4 : 3 (plant, tree, perennial)
located within the specification mark {plant #2, flora#2}
For PLANT#1 : 1 (plant)
For PLANT#2 : 3 (plant, tree, perennial)
For PLANT#3 : 1 (plant)
For PLANT#4 : 1 (plant)

Figure 3: Word Counts for Four Senses of the Word Plant
Step 5:
In this step, the method selects the word sense(s) having the greatest number of words
counted in Step 4. If there is only one sense, then that is the one that is obviously chosen.
If there is more than one sense, we repeat Step 4, moving down each level within the
taxonomy until a single sense is obtained or the program reach a leaf specification
mark. Figure 3 shows the word counts for each sense of plant (#1 through #4) located
within the specification mark entity#1, ..., life form#1, flora#2. If the word cannot
be disambiguated in this way, then it will be necessary to continue the disambiguation
process applying a complementary set of heuristics.

3.1.2 Heuristics
The specification marks method is combined with a set of five knowledge-based heuristics:
hypernym/hyponym, definition, gloss hypernym/hyponym, common specification mark,
and domain heuristics. A short description of each of these methods is provided below.
306

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

3.1.3 Hypernym/Hyponym Heuristic
This heuristic solves the ambiguity of those words that are not explicitly related in WordNet
(i.e., leaf is not directly related to plant, but rather follows a hypernym chain plus a PARTOF relation). All the hypernyms/hyponyms of the ambiguous word are checked, looking
for synsets that have compounds that match with some word from the context. Each
synset in the hypernym/hyponym chain is weighted in accordance with its depth within the
subhierarchy. The sense then having the greatest weight is chosen. Figure 4 shows that,
leaf#1 being a hyponym of plant organ#1 is disambiguated (obtain the greatest weight,
P
4
5
level
weight(leaf #1) = depth
i=1 ( total levels ) = ( 6 ) + ( 6 ) = 1.5) because plant is contained within
the context of leaf.
Context: plant, tree, leaf, perennial
Word non disambiguated: leaf.
Senses: leaf#1, leaf#2, leaf#3.
For leaf#1
=> entity, something
=> object, physical object
=> natural object
=> plant part
=> plant organ
=> leaf#1, leafage, foliage

Level 1
Level 2
Level 3
Level 4
Level 5
Level 6

Figure 4: Application of the Hypernym Heuristic

3.1.4 Definition Heuristic
In this case, all the glosses from the synsets of an ambiguous word are checked looking for
those that contain words from the context. Each match increases the synset count by one.
The sense having the greatest count is then chosen. Figure 5 shows an example of this
heuristic. The sense sister#1 is chosen, because it has the greatest weight.
Context: person, sister, musician.
Words non disambiguated: sister,musician.
Senses: sister#1, sister#2, sister#3 sister#4.
For sister#1  Weight = 2
1. sister, sis -- (a female person who has the same parents as another person; "my sister married a
musician")
For sister#3  Weight = 1
3. sister -- (a female person who is a fellow member (of a sorority or labor union or other group);
"none of her sisters would betray her")

Figure 5: Application of the Definition Heuristic

307

fiMontoyo, Suarez, Rigau, & Palomar

3.1.5 Gloss Hypernym/Hyponym Heuristic
This method extends the previously defined hypernym/hyponym heuristic by using glosses
of the hypernym/hyponym synsets of the ambiguous word. To disambiguate a given word,
all the glosses of the hypernym/hyponym synsets are checked looking for words occurring
in the context. Coincidences are counted. As before, the synset having the greatest count is
chosen. Figure 6 shows a example of this heuristic. The sense plane#1 is chosen, because
it has the greatest weight.
Context: plane, air
Words non disambiguated: plane
Senses: plane#1, plane#2, plane#3, plane#4, plane#5.
For Plane#1:  Weight = 1
airplane, aeroplane, plane -- (an aircraft that has fixed a wing and is powered by propellers or jets; "the
flight was delayed due to trouble with the airplane")
=> aircraft -- (a vehicle that can fly)
=> craft -- (a vehicle designed for navigation in or on water or air or through outer space)
=> vehicle -- (a conveyance that transports people or objects)
=> conveyance, transport -- (something that serves as a means of transportation)
=> instrumentality, instrumentation -- (an artifact (or system of artifacts) that is
instrumental in accomplishing some end)
=> artifact, artefact -- (a man-made object)
=> object, physical object -- (a physical (tangible and visible) entity; "it was full of
rackets, balls and other objects")
=> entity, something -- (anything having existence (living or nonliving))

Figure 6: Application of the Gloss Hypernym Heuristic

3.1.6 Common Specification Mark Heuristic
In most cases, the senses of the words to be disambiguated are very close to each other and
only differ in subtle differences in nuances. The Common Specification Mark heuristic reduce
the ambiguity of a word without trying to provide a full disambiguation. Thus, we select
the specification mark that is common to all senses of the context words, reporting all senses
instead of choosing a single sense from among them. To illustrate this heuristic, consider
Figure 7. In this example, the word month is not able to discriminate completely among
four senses of the word year. However, in this case, the presence of the word month can
help to select two possible senses of the word year when selecting the time period, period as
a common specification mark. This specification mark represents the most specific common
synset of a particular set of words. Therefore, this heuristic selects the sense month#1 and
senses year#1 and year#2 instead of attempting to choose a single sense or leaving them
completely ambiguous.
3.1.7 Domain WSD Heuristic
This heuristic uses a derived resource, relevant domains (Montoyo et al., 2003), which is
obtained combining both the WordNet glosses and WordNet Domains (Magnini & Strap308

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Context: year, month. Words non disambiguated: year. Senses: year#1, year#2, year#3, year#4.
For year#1:

For year#2:

For month#1:

=> abstraction
=> abstraction
=> measure, quantity
=> measure, quantity
=> time period, period
=> time period, period
=> year#1, twelvemonth
=> year#2

=> abstraction
=> measure, quantity
=> time period, period
=> month#1

Figure 7: Example of Common Specification Mark Heuristic
parava, 2000)3 . WordNet Domains establish a semantic relation between word senses by
grouping them into the same semantic domain (Sports, Medicine, etc.). The word bank,
for example, has ten senses in WordNet 2.0, but three of them, bank#1, bank#3
and bank#6 are grouped into the same domain label, Economy, whereas bank#2 and
bank#7 are grouped into the domain labels Geography and Geology. These domain labels are selected from a set of 165 labels hierarchically organized. In that way, a domain
connects words that belong to different subhierarchies and partof-speech.
Relevant domains is a lexicon derived from the WordNet glosses using WordNet Domains. In fact, we use WordNet as a corpus categorized with domain labels. For each
English word appearing in the gloses of WordNet, we obtain a list of their most representative domain labels. The relevance is obtained weighting each possible label with the
Association Ratio formula (AR), where w is a word and D is a domain.
AR(w|D) = P (w|D)  log

P (w|D)
P (w)

(1)

This list can also be considered as a weighted vector (or point in a multidimensional
space). Using such word vectors of Relevant domains, we can derive new vectors to
represent sets of wordsfor instance, for contexts or glosses. We can then compare the
similarity between a given context and each of the possible senses of a polysemous word
by using for instance the cosine function.
Figure 8 shows an example for disambiguating the word genotype in the following text:
There are a number of ways in which the chromosome structure can change, which will
detrimentally change the genotype and phenotype of the organism. First, the glosses of the
word to be disambiguated and the context are postagged and analyzed morphologically.
Second, we build the context vector (CV) which combines in one structure the most relevant and representative domains related to the words from the text to be disambiguated.
Third, in the same way, we build the sense vectors (SV) which group the most relevant and
representative domains of the gloss that is associated with each one of the word senses. In
this example, genotype#1  (a group of organisms sharing a specific genetic constitution)
and genotype#2  (the particular alleles at specified loci present in an organism). Finally,
in order to select the appropriate sense, we made a comparison between all sense vectors
and the context vector, and we select the senses more approximate to the context vector. In
3. http://wndomains.itc.it/

309

fiMontoyo, Suarez, Rigau, & Palomar

this example, we show the sense vector for sense genotype#1 and we select the genotype#1
sense, because its cosine is higher.



 Bio log y
 Ecology

 Botany
 Zoology

CV =  Anatomy
 Physiology

 Chemistry
 Geology

 Meteorology

...




0.00102837 
0.00402855 

3.20408e - 06 
1.77959e - 05 

1.29592e - 05 
0.000226531 

0.000179857 
1.66327e - 05 
0.00371308 

...

AR



 Ecology
 Biology

 Bowling
SV =  Archaeology

 Sociology
 Alimentation

 Linguistics

...


AR 

0.084778 
0.047627 

0.019687 
0.016451 

0.014251 
0.006510 

0.005297 

...


Sense vector genotype#1.
Context vector

Selected sense
genotype#1 = 0.00804111
genotype#2 = 0.00340548

Figure 8: Example of Domain WSD Heuristic
Defining this heuristic as knowledge-based or corpus-based can be seen controversial because this heuristic uses WordNet gloses (and WordNet Domains) as a corpus to
derive the relevant domains. That is, using corpus techniques on WordNet. However,
WordNet Domains was constructed semi-automatically (prescribed) following the hierarchy
of WordNet.
3.1.8 Evaluation of Specification Marks Method
Obviously, we can also use different strategies to combine a set of knowledge-based heuristics. For instance, all the heuristics described in the previous section can be applied in
order passing to the next heuristic only the remaining ambiguity that previous heuristics
were not able to solve.
In order to evaluate the performance of the knowledge-based heuristics previously defined, we used the SemCor collection (Miller et al., 1993), in which all content words are
annotated with the most appropriate WordNet sense.In this case, we used a window of
fifteen nouns (seven context nouns before and after the target noun).
The results obtained for the specification marks method using the heuristics when applied one by one are shown in Table 2. This table shows the results for polysemous nouns
only, and for polysemous and monosemous nouns combined.
310

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Heuristics
Precision Recall Coverage
Polysemic and monosemic nouns
0.553
0.522
0.943
Only polysemic nouns
0.377
0.311
0.943

Table 2: Results Using Heuristics Applied in Order on SemCor

The results obtained for the heuristics applied independently are shown in Table 3. As
shown, all the heuristics perform differently, providing different precision/recall figures.
Heuristics

Precision
Recall
Coverage
Mono+Poly Polysemic Mono+Poly Polysemic Mono+Poly Polysemic
Spec. Mark Method
0.383
0.300
0.341
0.292
0.975
0.948
Hypernym
0.563
0.420
0.447
0.313
0.795
0.745
Definition
0.480
0.300
0.363
0.209
0.758
0.699
Hyponym
0.556
0.393
0.436
0.285
0.784
0.726
Gloss hypernym
0.555
0.412
0.450
0.316
0.811
0.764
0.617
0.481
0.494
0.358
0.798
0.745
Gloss hyponym
Common specification
0.565
0.423
0.443
0.310
0.784
0.732
Domain WSD
0.585
0.453
0.483
0.330
0.894
0.832

Table 3: Results Using Heuristics Applied Independently
Another possibility is to combine all the heuristics using a majority voting schema (Rigau
et al., 1997). In this simple schema, each heuristic provides a vote, and the method selects
the synset that obtains more votes. The results shown in Table 4 illustrate that when the
heuristics are working independently, the method achieves a 39.1% recall for polysemous
nouns (with full coverage), which represents an improvement of 8 percentual points over
the method in which heuristics are applied in order (one by one).
Precision
Recall
Mono+Poly Polysemic Mono+Poly Polysemic
Voting heuristics
0.567
0.436
0.546
0.391

Table 4: Results using majority voting on SemCor
We also show in Table 5 the results of our domain heuristic when applied on the English
all-words task from Senseval-2. In the table, the polysemy reduction caused by domain
clustering can profitably help WSD. Since domains are coarser than synsets, word domain
disambiguation (WDD) (Magnini & Strapparava, 2000) can obtain better results than WSD.
Our goal is to perform a preliminary domain disambiguation in order to provide an informed
searchspace reduction.
3.1.9 Comparison with Knowledge-based Methods
In this section we compare three different knowledge-based methods: conceptual density
(Agirre & Rigau, 1996), a variant of the conceptual density algorithm (Fernandez-Amoros
et al., 2001); the Lesk method (Lesk, 1986) ; and the specification marks method.
311

fiMontoyo, Suarez, Rigau, & Palomar

Level WSD Precision Recall
Sense
0.44
0.32
Domain
0.54
0.43

Table 5: Results of Use of Domain WSD Heuristic

Table 6 shows recall results for the three methods when applied to the entire SemCor
collection. Our best result achieved 39.1% recall. This is an important improvement with
respect to other methods, but the results are still far below the most frequent sense heuristic. Obviously, none of the knowledge-based techniques and heuristics presented above are
sufficient, in isolation, to perform accurate WSD. However, we have empirically demonstrated that a simple combination of knowledge-based heuristics can lead to improvements
in the WSD process.
WSD Method
Recall
SM and Voting Heuristics
0.391
UNED Method
0.313
SM with Cascade Heuristics 0.311
Lesk
0.274
Conceptual Density
0.220

Table 6: Recall results using three different knowledgebased WSD methods

3.2 Maximum Entropy-based Method
Maximum Entropy modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning & Schutze, 1999; Berger
et al., 1996). ME probability models have been successfully applied to some NLP tasks,
such as POS tagging or sentence-boundary detection (Ratnaparkhi, 1998).
The WSD method used in this work is based on conditional ME models. It has been
implemented using a supervised learning method that consists of building word-sense classifiers using a semantically annotated corpus. A classifier obtained by means of an ME
technique consists of a set of parameters or coefficients which are estimated using an optimization procedure. Each coefficient is associated with one feature observed in the training
data. The goal is to obtain the probability distribution that maximizes the entropythat
is, maximum ignorance is assumed and nothing apart from the training data is considered.
One advantage of using the ME framework is that even knowledge-poor features may be applied accurately; the ME framework thus allows a virtually unrestricted ability to represent
problem-specific knowledge in the form of features (Ratnaparkhi, 1998).
Let us assume a set of contexts X and a set of classes C. The function cl : X  C chooses
the class c with the highest conditional probability in the context x: cl(x) = arg maxc p(c|x).
Each feature is calculated by a function that is associated with a specific class c0 , and it
takes the form of equation (2), where cp(x) represents some observable characteristic in
312

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

the context4 . The conditional probability p(c|x) is defined by equation (3), where i is
the parameter or weight of the feature i, K is the number of features defined, and Z(x)
is a normalization factor that ensures that the sum of all conditional probabilities for this
context is equal to 1.
f (x, c) =

(

1 if c0 = c and cp(x) = true
0 otherwise

K
1 Y
f (x,c)
p(c|x) =
i
Z(x) i=1 i

(2)

(3)

The learning module produces the classifiers for each word using a corpus that is syntactically and semantically annotated. The module processes the learning corpus in order
to define the functions that will apprise the linguistic features of each context.
For example, consider that we want to build a classifier for the noun interest using
the POS label of the previous word as a feature and we also have the the following three
examples from the training corpus:
... the widespread interest#1 in the ...
... the best interest#5 of both ...
... persons expressing interest#1 in the ...

The learning module performs a sequential processing of this corpus, looking for the
pairs <POS-label, sense>. Then, the following pairs are used to define three functions
(each context has a vector composed of three features).
<adjective,#1>
<adjective,#5>
<verb,#1>
We can define another type of feature by merging the POS occurrences by sense:
< {adjective,verb},#1>
<adjective,#5>
This form of defining the pairs means a reduction of feature space because all information
(of some kind of linguistic data, e.g., POS label at position -1) about a sense is contained in
just one feature. Obviously, the form of the feature function 2 must be adapted to Equation
4. Thus,

f(c0 ,i) (x, c) =

(

W(c0 ) = {data of sense c0 }

(4)

1 if c0 = c and CP (x)  W(c0 )
0 otherwise

4. The ME approach is not limited to binary features, but the optimization procedure used for the estimation
of the parameters, the Generalized Iterative Scaling procedure, uses this kind of features.

313

fiMontoyo, Suarez, Rigau, & Palomar

We will refer to the feature function expressed by Equation 4 as collapsed features.
The previous Equation 2 we call non-collapsed features. These two feature definitions are
complementary and can be used together in the learning phase.
Due to the nature of the disambiguation task, the number of times that a feature
generated by the first type of function (non-collapsed) is activated is very low, and the
feature vectors have a large number of null values. The new function drastically reduces
the number of features, with a minimal degradation in the evaluation results. In this way,
more and new features can be incorporated into the learning process, compensating the loss
of accuracy.
Therefore, the classification module carries out the disambiguation of new contexts using
the previously stored classification functions. When ME does not have enough information
about a specific context, several senses may achieve the same maximum probability and
thus the classification cannot be done properly. In these cases, the most frequent sense in
the corpus is assigned. However, this heuristic is only necessary for a minimum number of
contexts or when the set of linguistic attributes processed is very small.
3.2.1 Description of Features
The set of features defined for the training of the system is described in Figure 9 and is
based on the features described by Ng and Lee (1996) and Escudero et al. (2000). These
features represent words, collocations, and POS tags in the local context. Both collapsed
and non-collapsed functions are used.
 0: word form of the target word
 s: words at positions 1, 2, 3

 p: POS-tags of words at positions 1, 2, 3
 b: lemmas of collocations at positions (2, 1), (1, +1), (+1, +2)
 c: collocations at positions (2, 1), (1, +1), (+1, +2)
 k m: lemmas of nouns at any position in context, occurring at least m% times with a sense
 r : grammatical relation to the ambiguous word
 d : the word that the ambiguous word depends on

 m: multi-word if identified by the parser
 L: lemmas of content-words at positions 1, 2, 3 (collapsed definition)
 W : content-words at positions 1, 2, 3 (collapsed definition)
 S, B, C, P, D and M : collapsed versions (see Equation 4)

Figure 9: Features Used for the Training of the System
Actually, each item in Figure 9 groups several sets of features. The majority of them
depend on the nearest words (e.g., s comprises all possible features defined by the words
occurring in each sample at positions w3 , w2 , w1 , w+1 , w+2 , w+3 related to the ambiguous word). Types nominated with capital letters are based on the collapsed function
form; that is, these features simply recognize an attribute belonging to the training data.
Keyword features (km) are inspired by Ng and Lee work. Noun filtering is done using
frequency information for nouns co-occurring with a particular sense. For example, let us
314

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

suppose m = 10 for a set of 100 examples of interest#4 : if the noun bank is found 10 times
or more at any position, then a feature is defined.
Moreover, new features have also been defined using other grammatical properties: relationship features (r) that refer to the grammatical relationship of the ambiguous word
(subject, object, complement, ...) and dependency features (d and D) that extract the word
related to the ambiguous one through the dependency parse tree.
3.2.2 Evaluation of the Maximum Entropy Method
In this subsection we present the results of our evaluation over the training and testing data
of the Senseval-2 Spanish lexicalsample task. This corpus was parsed using Conexor
Functional Dependency Grammar parser for Spanish (Tapanainen & Jarvinen, 1997).
The classifiers were built from the training data and evaluated over the test data. Table
7 shows which combination of groups of features works better for every POS and which
work better for all words together.
Nouns
Verbs
Adjectives
ALL

Accuracy Feature selection
0.683
LWSBCk5
0.595
sk5
0.783
LWsBCp
0.671
0LWSBCk5

Table 7: Baseline: Accuracy Results of Applying ME on Senseval-2 Spanish Data
This work entails an exhaustive search looking for the most accurate combination of
features. The values presented here are merely informative and indicate the maximum
accuracy that the system can achieve with a particular set of features.
3.3 Improving ME accuracy
Our main goal is to find a method that will automatically obtain the best feature selection
(Veenstra et al., 2000; Mihalcea, 2002; Suarez & Palomar, 2002b) from the training data.
We performed an 3-fold cross-validation process. Data is divided in 3 folds; then, 3 tests
are done, each one with 2 folds as training data and the remaining one as testing data. The
final result is the average accuracy. We decided on just three tests because of the small
size of the training data. Then, we tested several combinations of features over the training
data of the Senseval-2 Spanish lexicalsample and analyzed the results obtained for each
word.
In order to perform the 3-fold cross-validation process on each word, some preprocessing
of the corpus was done. For each word, all senses were uniformly distributed into the three
folds (each fold contains one-third of the examples of each sense). Those senses that had
fewer than three examples in the original corpus file were rejected and not processed.
Table 8 shows the best results obtained using three-fold cross-validation on the training
data. Several feature combinations were tested in order to find the best set for each selected
word. The purpose was to obtain the most relevant information for each word from the
corpus rather than applying the same combination of features to all of them. Therefore,
the information in the column Features lists only the feature selection with the best result.
315

fiMontoyo, Suarez, Rigau, & Palomar

Word
autoridad,N
bomba,N
canal,N
circuito,N
corazon,N
corona,N
gracia,N
grano,N
hermano,N
masa,N
naturaleza,N
operacion,N
organo,N
partido,N
pasaje,N
programa,N
tabla,N
actuar,V
apoyar,V
apuntar,V

Features
Accur
sbcp
0.589
0LWSBCk5
0.762
sbcprdk3
0.579
0LWSBCk5
0.536
0Sbcpk5
0.781
sbcp
0.722
0sk5
0.634
0LWSBCr
0.681
0Sprd
0.731
LWSBCk5
0.756
sbcprdk3
0.527
0LWSBCk5
0.543
0LWSBCPDk5 0.715
0LWSBCk5
0.839
sk5
0.685
0LWSBCr
0.587
sk5
0.663
sk5
0.514
0sbcprdk3
0.730
0LWsBCPDk5 0.661

MFS
0.503
0.707
0.307
0.392
0.607
0.489
0.295
0.483
0.602
0.455
0.424
0.377
0.515
0.524
0.451
0.486
0.488
0.293
0.635
0.478

Word
clavar,V
conducir,V
copiar,V
coronar,V
explotar,V
saltar,V
tocar,V
tratar,V
usar,V
vencer,V
brillante,A
ciego,A
claro,A
local,A
natural,A
popular,A
simple,A
verde,A
vital,A

Features Accur
sbcprdk3 0.561
LWsBCPD 0.534
0sbcprdk3 0.457
sk5
0.698
0LWSBCk5 0.593
LWsBC
0.403
0sbcprdk3 0.583
sbcpk5
0.527
0Sprd
0.732
sbcprdk3 0.696
sbcprdk3 0.756
0spdk5
0.812
0Sprd
0.919
0LWSBCr 0.798
sbcprdk10 0.471
sbcprdk10 0.865
LWsBCPD 0.776
LWSBCk5 0.601
Sbcp
0.774

MFS
0.449
0.358
0.338
0.327
0.318
0.132
0.313
0.208
0.669
0.618
0.512
0.565
0.854
0.750
0.267
0.632
0.621
0.317
0.441

Table 8: Three-fold Cross-Validation Results on Senseval-2 Spanish Training Data: Best Averaged Accuracies per Word

Strings in each row represent the entire set of features used when training each classifier. For
example, autoridad obtains its best result using nearest words, collocations of two lemmas,
collocations of two words, and POS information that is, s, b, c, and p features, respectively
(see Figure 9). The column Accur (for accuracy) shows the number of correctly classified
contexts divided by the total number of contexts (because ME always classifies precision as
equal to recall). Column MFS shows the accuracy obtained when the most frequent sense
is selected.
The data summarized in Table 8 reveal that using collapsed features in the ME method
is useful; both collapsed and non-collapsed functions are used, even for the same word.
For example, the adjective vital obtains the best result with Sbcp (the collapsed version
of words in a window (3.. + 3), collocations of two lemmas and two words in a window
(2.. + 2), and POS labels, in a window (3.. + 3) too); we can here infer that single-word
information is less important than collocations in order to disambiguate vital correctly.
The target word (feature 0) is useful for nouns, verbs, and adjectives, but many of the
words do not use it for their best feature selection. In general, these words do not have a
relevant relationship between shape and senses. On the other hand, POS information (p
and P features) is selected less often. When comparing lemma features with word features
(e.g., L versus W , and B versus C), they are complementary in the majority of cases.
Grammatical relationships (r features) and wordword dependencies (d and D features)
seem very useful, too, if combined with other types of attributes. Moreover, keywords (km
316

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

features) are used very often, possibly due to the source and size of contexts of Senseval-2
Spanish lexicalsample data.
Table 9 shows the best feature selections for each part-of-speech and for all words. The
data presented in Tables 8 and 9 were used to build four different sets of classifiers in order
to compare their accuracy: MEfix uses the overall best feature selection for all words;
MEbfs trains each word with its best selection of features (in Table 8); MEbfs.pos uses
the best selection per POS for all nouns, verbs and adjectives, respectively (in Table 9); and,
finally, vME is a majority voting system that has as input the answers of the preceding
systems.
POS
Nouns
Verbs
Adjectives
ALL

Acc
0.620
0.559
0.726
0.615

Features
System
LWSBCk5
sbcprdk3 MEbfs.pos
0spdk5
sbcprdk3
MEfix

Table 9: Three-fold Cross-Validation Results on Senseval-2 Spanish Training Data: Best Averaged Accuracies per POS

Table 10 shows a comparison of the four systems. MEfix has the lower results. This
classifier applies the same set of types of features to all words. However, the best feature
selection per word (MEbfs) is not the best, probably because more training examples are
necessary. The best choice seems to select a fixed set of types of features for each POS
(MEbfs.pos).
ALL
Nouns
MEbfs.pos 0.683 MEbfs.pos
vME
0.678 vME
0.661 MEbfs
MEbfs
MEfix
0.646 MEfix
Verbs
Adjectives
0.583 vME
0.774 vME
0.583 MEbfs.pos 0.772 MEbfs.pos
0.583 MEfix
0.771 MEbfs
0.580 MEbfs
0.756 MEfix
MEfix: sbcprdk3 for all words
MEbfs: each word with its
best feature selection
MEbfs.pos: LWSBCk5 for nouns,
sbcprdk3 for verbs,
and 0spdk5 for adjectives
vME: majority voting between MEfix,
MEbfs.pos, and MEbfs
0.677
0.676
0.667
0.658

Table 10: Evaluation of ME Systems
317

fiMontoyo, Suarez, Rigau, & Palomar

While MEbfs predicts, for each word over the training data, which individually selected
features could be the best ones when evaluated on the testing data, MEbfs.pos is an
averaged prediction, a selection of features that, over the training data, performed a good
enough disambiguation of the majority of words belonging to a particular POS. When this
averaged prediction is applied to the real testing data, MEbfs.pos performs better than
MEbfs.
Another important issue is that MEbfs.pos obtains an accuracy slightly better than
the best possible evaluation result achieved with ME (see Table 7)that is, a best-featureselection per POS strategy from training data guarantees an improvement on ME-based
WSD.
In general, verbs are difficult to learn and the accuracy of the method for them is lower
than for other POS; in our opinion, more information (knowledge-based, perhaps) is needed
to build their classifiers. In this case, the voting system (vME) based on the agreement
between the other three systems, does not improve accuracy.
Finally in Table 11, the results of the ME method are compared with those systems that
competed at Senseval-2 in the Spanish lexicalsample task5 . The results obtained by ME
systems are excellent for nouns and adjectives, but not for verbs. However, when comparing
ALL POS, the ME systems seem to perform comparable to the best Senseval-2 systems.

0.713
0.682
0.677
0.676
0.670
0.667
0.658
0.627
0.617
0.610
0.595
0.595
0.582
0.578
0.560
0.548
0.524

ALL
jhu(R)
jhu
MEbfs.pos
vME
css244
MEbfs
MEfix
umd-sst
duluth 8
duluth 10
duluth Z
duluth 7
duluth 6
duluth X
duluth 9
ua
duluth Y

0.702
0.683
0.681
0.678
0.661
0.652
0.646
0.621
0.612
0.611
0.603
0.592
0.590
0.586
0.557
0.514
0.464

Nouns
jhu(R)
MEbfs.pos
jhu
vME
MEbfs
css244
MEfix
duluth 8
duluth Z
duluth 10
umd-sst
duluth 6
duluth 7
duluth X
duluth 9
duluth Y
ua

0.643
0.609
0.595
0.584
0.583
0.583
0.583
0.580
0.515
0.513
0.511
0.498
0.490
0.478
0.477
0.474
0.431

Verbs
jhu(R)
jhu
css244
umd-sst
vME
MEbfs.pos
MEfix
MEbfs
duluth 10
duluth 8
ua
duluth 7
duluth Z
duluth X
duluth 9
duluth 6
duluth Y

0.802
0.774
0.772
0.772
0.771
0.764
0.756
0.725
0.712
0.706
0.703
0.689
0.689
0.687
0.678
0.655
0.637

Adjectives
jhu(R)
vME
MEbfs.pos
css244
MEbfs
jhu
MEfix
duluth 8
duluth 10
duluth 7
umd-sst
duluth 6
duluth Z
ua
duluth X
duluth 9
duluth Y

Table 11: Comparison with the Spanish Senseval-2 systems

5. JHU(R) by Johns Hopkins University; CSS244 by Stanford University; UMD-SST by the University of
Maryland; Duluth systems by the University of Minnesota - Duluth; UA by the University of Alicante.

318

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

3.4 Comparing Specification Marks Method with Maximum Entropy-based
Method
The main goal of this section is to evaluate the Specification Marks Method (Montoyo &
Suarez, 2001) and the Maximum Entropy-based Method (in particular, MEfix System)
on a common data set, to allow for direct comparisons. The individual evaluation of each
method has been carried out on the noun set (17 nouns) of the Spanish lexical-sample task
(Rigau et al., 2001) from Senseval-26 . Table 12 shows precision, recall and coverage of
both methods.
Precision Recall Coverage
SM
0.464
0.464
0.941
ME
0.646
0.646
1

Table 12: Comparison of ME and SM for nouns in Senseval-2 Spanish lexical sample
In order to study a possible cooperation between both methods, we count those cases
that: the two methods return the correct sense for the same occurrence, at least one of the
methods provides the correct sense and finally, none of both provides the correct sense. A
summary of the obtained results is shown in the Table 13. These results clearly show that
there is a large room for improvement when combining both system outcomes. In fact, they
provide also a possible upper bound precision for this technology, which can be set to 0.798
(more than 15 percentual points higher than the current best system). Table 14 presents
a complementary view: wins, ties and loses between ME and SM when each context is
examined. Although ME performs better than SM, there are 122 cases (15 %) solved only
by the SM method.
Contexts Percentage
Both OK
240
0.300
398
0.498
One OK
Zero OK
161
0.202

Table 13: Correct classifications of ME and SM for nouns in Senseval-2 Spanish lexical sample

Wins Ties Loses
ME  SM 267 240 122

Table 14: Wins, ties and loses of ME and SM systems for nouns in Senseval-2 Spanish lexical
sample

6. For both Spanish and English Senseval-2 corpora, when applying the Specification Marks method we
used the whole example as the context window for the target noun

319

fiMontoyo, Suarez, Rigau, & Palomar

4. Experimental Work
In this section we attempt to confirm our hypothesis that both corpus-based and knowledgebased methods can improve the accuracy of each other. The first subsection shows the
results of preprocessing the test data with the maximum entropy method (ME) in order to
help the specification marks method (SM). Next, we test the opposite, if preprocessing the
test data with the domain heuristic can help the maximum entropy method to disambiguate
more accurately.
The last experiment combines the vME system (the majority voting system) and SM
method. Actually, it relies on simply adding the SM as one more heuristic to the voting
scheme.
4.1 Integrating a Corpus-based WSD System into a Knowledge-based WSD
System
This experiment was designed to study and evaluate whether the integration of corpus-based
system within a knowledge-based helps improve word-sense disambiguation of nouns.
Therefore, ME can help to SM by labelling some nouns of the context of the target word.
That is, reducing the number of possible senses of some nouns of the context. In fact, we
reduce the search space of the SM method. This ensures that the sense of the target word
will be the one more related to the noun senses labelled by ME.
In this case, we used the noun words from the English lexical-sample task from Senseval2. ME helps SM by labelling some words from the context of the target word. These words
were sense tagged using the SemCor collection as a learning corpus. We performed a three
fold cross-validation for all nouns having 10 or more occurrences. We selected those nouns
that were disambiguated by ME with high precision, that is, nouns that had percentage
rates of accuracy of 90% or more. The classifiers for these nouns were used to disambiguate
the testing data. The total number of different noun classifiers (noun) activated for each
target word across the testing corpus is shown in Table 15.
Next, SM was applied, using all the heuristics for disambiguating the target words of
the testing data, but with the advantage of knowing the senses of some nouns that formed
the context of these targets words.
Table 15 shows the results of precision and recall when SM is applied with and without
first applying ME, that is, with and without fixing the sense of the nouns that form the
context. A very small but consistent improvement was obtained through the complete
test set (3.56% precision and 3.61% recall). Although the improvement is very low, this
experiment empirically demonstrates that a corpus-based method such as maximum entropy
can be integrated to help a knowledge-based system such as the specification marks method.
4.2 Integrating a Knowledge-based WSD system into a Corpus-based WSD
system
In this case, we used only the domain heuristic to improve ME because this information can
be added directly as domain features. The problem of data sparseness from which a WSD
system based on features suffers could be increased by the fine-grained sense distinctions
provided by WordNet. On the contrary, the domain information significantly reduces the
320

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Target words
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Total

Without fixed senses With fixed senses
noun classifiers Precision
Recall
Precision Recall
63
80
104
37
59
32
59
50
49
136
22
15
14
38
48
38
29
23
40
58
51
25
37
41
31
37
17
37
24

0.475
0.137
0.222
0.421
0.206
0.500
0.500
0.509
0.356
0.038
0.454
0.933
0.875
0.236
0.306
0.184
0.321
0.818
0.375
0.343
0.094
0.269
0.263
0.312
0.200
0.260
0.823
0.228
0.480

0.475
0.123
0.203
0.216
0.190
0.343
0.200
0.509
0.346
0.035
0.454
0.933
0.875
0.230
0.300
0.179
0.310
0.346
0.136
0.338
0.094
0.269
0.263
0.306
0.193
0.240
0.823
0.216
0.480

0.524
0.144
0.232
0.421
0.316
0.521
0.518
0.540
0.369
0.054
0.476
0.933
1
0.297
0.346
0.216
0.321
0.833
0.615
0.359
0.132
0.307
0.289
0.354
0.206
0.282
0.941
0.257
0.541

0.524
0.135
0.220
0.216
0.301
0.375
0.233
0.529
0.360
0.049
0.454
0.933
1
0.282
0.340
0.205
0.310
0.384
0.363
0.353
0.132
0.307
0.289
0.346
0.193
0.260
0.941
0.243
0.520

1294

0.300

0.267

0.336

0.303

Table 15: Precision and Recall Results Using SM to Disambiguate Words, With and Without Fixing of Noun Sense

word polysemy (i.e., the number of categories for a word is generally lower than the number
of senses for the word) and the results obtained by this heuristic have better precision than
those obtained by the whole SM method, which in turn obtain better recall.
As shown in subsection 3.1.7, the domain heuristic can annotate word senses characterized by their domains. Thus, these domains will be used as an additional type of features
for ME in a context window of 1, 2, and 3 from the target word. In addition, the
three more relevant domains were calculated also for each context and incorporated to the
training in the form of features.
This experiment was also carried out on the English lexical-sample task data from
Senseval-2, and ME was used to generate two groups of classifiers from the training data.
The first group of classifiers used the corpus without information of domains; the second,
having previously been domain disambiguated with SM, incorporating the domain label of
adjacent nouns, and the three more relevant domains to the context. That is, providing to
the classifier a richer set of features (adding the domain features). However, in this case,
we did not perform any feature selection.
The test data was disambiguated by ME twice, with and without SM domain labelling,
using 0lW sbcpdm (see Figure 9) as the common set of features in order to perform the
comparison. The results of the experiment are shown in Table 16.
The table shows that 7 out of 29 nouns obtained worse results when using the domains,
whereas 13 obtained better results. Although, in this case, we only obtained a very small
improvement in terms of precision (2%)7 .
7. This difference proves to be statistically significant when applying the test of the corrected difference of
two proportions (Dietterich, 1998; Snedecor & Cochran, 1989)

321

fiMontoyo, Suarez, Rigau, & Palomar

Target words

Without domains With domains Improvement

art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew

0.667
0.600
0.625
0.865
0.898
0.567
0.661
0.560
0.408
0.676
0.909
0.800
0.429
0.850
0.708
0.540
0.759
1.000
0.900
0.534
0.569
0.720
0.459
0.463
0.516
0.676
0.765
0.378
0.792

0.778
0.700
0.615
0.919
0.898
0.597
0.695
0.600
0.388
0.669
0.909
0.800
0.500
0.850
0.688
0.620
0.793
0.957
0.900
0.552
0.588
0.720
0.459
0.512
0.452
0.622
0.882
0.378
0.792

0.111
0.100
-0.010
0.054

All

0.649

0.669

0.020

0.030
0.034
0.040
-0.020
-0.007
0.071
-0.021
0.080
0.034
-0.043
0.017
0.020
0.049
-0.065
-0.054
0.118

Table 16: Precision Results Using ME to Disambiguate Words, With and Without Domains
(recall and precision values are equal)

We obtained important conclusions about the relevance of domain information for each
word. In general, the larger improvements appear for those words having well-differentiated
domains (spade, authority). Conversely, the word stress with most senses belonging to the
FACTOTUM domain do not improves at all. For example, for spade, art and authority
(with an accuracy improvement over 10%) domain data seems to be an important source of
knowledge with information that is not captured by other types of features. For those words
for which precision decrease up to 6.5%, domain information is confusing. Three reasons
can be exposed in order to explain this behavior: there is not a clear domain in the examples
or they do not represent correctly the context, domains do not differentiate appropriately
the senses, or the number of training examples is too low to perform a valid assessment. A
cross-validation testing, if more examples were available, could be appropriate to perform
a domain tuning for each word in order to determine which words must use this preprocess
and which not.
Nevertheless, the experiment empirically demonstrates that a knowledge-based method,
such as the domain heuristic, can be integrated successfully into a corpus-based system,
such as maximum entropy, to obtain a small improvement.
4.3 Combining Results with(in) a Voting System
In previous sections, we have demonstrated that it is possible to integrate two different WSD
approaches. In this section we evaluate the performance when combining a knowledge-based
system, such as specification marks, and a corpus-based system, such as maximum entropy,
in a simple voting schema.
322

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

In the two previous experiments we attempted to provide more information by predisambiguating the data. Here, we use both methods in parallel and then we combine their
classifications in a voting system, both for Senseval-2 Spanish and English lexicalsample
tasks.
4.3.1 Senseval-2 Spanish lexicalsample task
vME+SM is an enrichment of vME: we added the SM classifier to the combination of the
three ME systems in vME (see Section 3.3). The results on the Spanish lexicalsample task
from Senseval-2 are shown in Table 17. Because it only works with nouns, vME+SM
improves accuracy for them only, but obtains the same score as JHU(R) while the overall
score reaches the second place.
0.713
0.684
0.682
0.677
0.676
0.670
0.667
0.658
0.627
0.617
0.610
0.595
0.595
0.582
0.578
0.560
0.548
0.524

ALL
jhu(R)
vME+SM
jhu
MEbfs.pos
vME
css244
MEbfs
MEfix
umd-sst
duluth 8
duluth 10
duluth Z
duluth 7
duluth 6
duluth X
duluth 9
ua
duluth Y

0.702
0.702
0.683
0.681
0.678
0.661
0.652
0.646
0.621
0.612
0.611
0.603
0.592
0.590
0.586
0.557
0.514
0.464

Nouns
jhu(R)
vME+SM
MEbfs.pos
jhu
vME
MEbfs
css244
MEfix
duluth 8
duluth Z
duluth 10
umd-sst
duluth 6
duluth 7
duluth X
duluth 9
duluth Y
ua

Table 17: vME+SM in the Spanish lexicalsample task of Senseval-2
These results show that methods like SM and ME can be combined in order to achieve
good disambiguation results. Our results are in line with those of Pedersen (2002), which
also presents a comparative evaluation between the systems that participated in the Spanish
and English lexical-sample tasks of Senseval-2. Their focus is on pair comparisons between
systems to assess the degree to which they agree, and on measuring the difficulty of the test
instances included in these tasks. If several systems are largely in agreement, then there
is little benefit in combining them since they are redundant and they will simply reinforce
each other. However, if some systems disambiguate instances that others do not, then the
systems are complementary and it may be possible to combine them to take advantage of
the different strengths of each system to improve overall accuracy.
The results for nouns (only applying SM), shown in Table 18, indicate that SM has
a low level of agreement with all the other methods. However, the measure of optimal
combination is quite high, reaching 89% (1.000.11) for the pairing of SM and JHU. In
323

fiMontoyo, Suarez, Rigau, & Palomar

fact, all seven of the other methods achieved their highest optimal combination value when
paired with the SM method.
System pair for nouns
SM and JHU
SM and Duluth7
SM and DuluthY
SM and Duluth8
SM and Cs224
SM and Umcp
SM and Duluth9

Both OKa One OK
0.29
0.32
0.27
0.34
0.25
0.35
0.28
0.32
0.28
0.32
0.26
0.33
0.26
0.31

b

Zero OK
0.11
0.12
0.12
0.13
0.13
0.14
0.16

c

Kappa
0.06
0.03
0.01
0.08
0.09
0.06
0.14

d

Table 18: Optimal combination between the systems that participated in the Spanish
lexicalsample tasks of Senseval-2
a.
b.
c.
d.

Percentage of instances where both systems answers were correct.
Percentage of instances where only one answer is correct.
Percentage of instances where none of both answers is correct.
The kappa statistic (Cohen, 1960) is a measure of agreement between multiple systems (or judges) that
is scaled by the agreement that would be expected just by chance. A value of 1.00 suggests complete
agreement, while 0.00 indicates pure chance agreement.

This combination of circumstances suggests that SM, being a knowledge-based method,
is fundamentally different from the others (i.e., corpus-based) methods, and is able to
disambiguate a certain set of instances where the other methods fail. In fact, SM is different
in that it is the only method that uses the structure of WordNet.
4.3.2 Senseval-2 English lexicalsample task
The same experiment was done on Senseval-2 English lexicalsample task data and the
results are shown in Table 19. The details of how the different systems were built can be
consulted in Section 3.2
Again, we can see in Table 19 that BFS per POS is better than per word, mainly because
the same reasons explained in Section 3.3.
Nevertheless, the improvement on nouns by using the vME+SM system is not as high
as for the Spanish data. The differences between both corpora have a significant relevance
about the precision values that can be obtained. For example, the English data includes
multi-words and the sense inventory is extracted from WordNet, while the Spanish data
is smaller and a dictionary was built for the task specifically, having a smaller polysemy
degree.
The results of vME+SM are comparable to the systems presented at Senseval-2
where the best system (Johns Hopkins University) reported 64.2% precision (68.2%, 58.5%
and 73.9% for nouns, verbs and adjectives, respectively).
Comparing these results with those obtained in section 4.2, we also see that using a
voting system with the best feature selection for ME and Specification Marks vME+SM,
and using a nonoptimized ME with the relevant domain heuristic, we obtain very similar
performance. That is, it seems that we obtain comparable performance combining different
324

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

All Nouns Verbs Adjectives
MEfix
0.601 0.656 0.519
0.696
MEbfs 0.606 0.658 0.519
0.696
MEbfs.pos 0.609 0.664 0.519
0.696
vME+SM 0.619 0.667 0.535
0.707
MEfix: 0mcbWsdrvK3 for all words
MEbfs: each word with its
best feature selection
MEbfs.pos: 0Wsrdm for nouns,
0sbcprdmK10 for verbs,
and 0mcbWsdrvK3 for adjectives
vME+SM: majority voting between MEfix,
MEbfs.pos, MEbfs, and Specification Marks

Table 19: Precision Results Using Best Feature Selection for ME and Specification Marks on
Senseval-2 English lexicalsample task data

classifiers resulting from a feature selection process or using a richer set of features (adding
the domain features) with much less computational overhead.
This analysis of the results from the Senseval-2 English and Spanish lexicalsample
tasks demonstrates that knowledge-based and corpus-based WSD systems can cooperate
and can be combined to obtain improved WSD systems. The results empirically demonstrate that the combination of both approaches outperforms each of them individually,
demonstrating that both approaches could be considered complementary.

5. Conclusions
The main hypothesis of this work is that WSD requires different kinds of knowledge sources
(linguistic information, statistical information, structural information, etc.) and techniques.
The aim of this paper was to explore some methods of collaboration between complementary
knowledge-based and corpus-based WSD methods. Two complementary methods have been
presented: specification marks (SM) and maximum entropy (ME). Individually, both have
benefits and drawbacks. We have shown that both methods can collaborate to obtain better
results on WSD.
In order to demonstrate our hypothesis, three different schemes for combining both
approaches have been presented. We have presented different mechanisms of combining
information sources around knowledge-based and corpus-based WSD methods. We have
also shown that the combination of both approaches outperforms each of the methods individually, demonstrating that both approaches could be considered complementary. Finally,
we have shown that a knowledge-based method can help a corpus-based method to better
perform the disambiguation process, and vice versa.
In order to help the specification marks method, ME disambiguates some nouns in the
context of the target word. ME selects these nouns by means of a previous analysis of
training data in order to identify which ones seem to be highly accurately disambiguated.
325

fiMontoyo, Suarez, Rigau, & Palomar

This preprocess fixes some nouns reducing the search space of the knowledge-based method.
In turn, ME is helped by SM by providing domain information of nouns in the contexts.
This information is incorporated into the learning process in the form of features.
By comparing the accuracy of both methods, with and without the contribution of the
other, it was demonstrated that such combining schemes of WSD methods are possible and
successful.
Finally, we presented a voting system for nouns that included four classifiers, three of
them based on ME, and one of them based on SM. This cooperation scheme obtained the
best score for nouns when compared with the systems submitted to the Senseval-2 Spanish
lexicalsample task and comparable results to those submitted to the Senseval-2 English
lexicalsample task.
We are presently studying possible improvements in the collaboration between these
methods, both by extending the information that the two methods provide to each other
and by taking advantage of the merits of each one.

Acknowledgments
The authors wish to thank the anonymous reviewers of the Journal of Artificial Intelligence Research and COLING 2002, the 19th International Conference on Computational
Linguistics, for helpful comments on earlier drafts of the paper. An earlier paper (Suarez &
Palomar, 2002b) about the corpus-based method (subsection 3.2) was presented at COLING
2002.
This research has been partially funded by the Spanish Government under project CICyT number TIC2000-0664-C02-02 and PROFIT number FIT-340100-2004-14 and the Valencia Government under project number GV04B-276 and the EU funded project MEANING (IST-2001-34460).

References
Agirre, E., & Martinez, D. (2001a). Decision lists for english and basque. In Proceedings of
the SENSEVAL-2 Workshop. In conjunction with ACL2001/EACL2001 Toulouse,
France.
Agirre, E., & Martinez, D. (2001b). Knowledge sources for word sense disambiguation. In
Proceedings of International Conference on Text, Speech and Dialogue (TSD2001)
Selezna Ruda, Czech Republic.
Agirre, E., & Rigau, G. (1996). Word Sense Disambiguation using Conceptual Density. In
Proceedings of the 16th International Conference on Computational Linguistic (COLING96 Copenhagen, Denmark.
Berger, A. L., Pietra, S. A. D., & Pietra, V. J. D. (1996). A maximum entropy approach
to natural language processing. Computational Linguistics, 22 (1), 3971.
326

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Bruce, R., & Wiebe, J. (1994). Word sense disambiguation using decomposable models. In
Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL1994), pp. 139145 Las Cruces, US.
Cabezas, C., Resnik, P., & Stevens, J. (2001). Supervised Sense Tagging using Support
Vector Machines. In Proceedings of the Second International Workshop on Evaluating
Word Sense Disambiguation Systems (SENSEVAL-2) Toulouse, France.
Cardie, C., & Mooney, R. J. (1999). Guest editors introduction: Machine learning and
natural language. Machine Learning, 34 (1-3), 59.
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educ. Psychol. Meas., 20,
3746.
Cowie, J., Guthrie, J., & Guthrie, L. (1992). Lexical disambiguation using simulated annealing. In Proceedings of the 14th International Conference on Computational Linguistic,
COLING92, pp. 359365 Nantes, France.
Cuadros, M., Atserias, J., Castillo, M., & Rigau, G. (2004). Automatic acquisition of sense
examples using exretriever. In IBERAMIA Workshop on Lexical Resources and The
Web for Word Sense Disambiguation. Puebla, Mexico.
Dang, H. T., yi Chia, C., Palmer, M., & Chiou, F.-D. (2002). Simple features for chinese
word sense disambiguation. In Chen, H.-H., & Lin, C.-Y. (Eds.), Proceedings of the
19th International Conference on Computational Linguistics (COLING2002).
Dietterich, T. G. (1998). Approximate statistical test for comparing supervised classification
learning algorithms. Neural Computation, 10 (7), 18951923.
Escudero, G., Marquez, L., & Rigau, G. (2000). Boosting applied to word sense disambiguation. In Proceedings of the 12th Conference on Machine Learning ECML2000
Barcelona, Spain.
Fellbaum, C. (Ed.). (1998). WordNet. An Electronic Lexical Database. The MIT Press.
Fernandez-Amoros, D., Gonzalo, J., & Verdejo, F. (2001). The Role of Conceptual Relations in Word Sense Disambiguation. In Proceedings 6th International Conference on
Application of Natural Language to Information Systems (NLDB2001)., pp. 8798
Madrid, Spain.
Hoste, V., Daelemans, W., Hendrickx, I., & van den Bosch, A. (2002). Evaluating the results
of a memory-based word-expert approach to unrestricted word sense disambiguation.
In Proceedings of the ACL2002 Workshop on Word Sense Disambiguation: Recent
Successes and Future Directions, pp. 95101 PA, USA.
Ide, N., & Veronis, J. (1998). Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art. Computational Linguistics, 24 (1), 140.
Leackock, C., Chodorow, M., & Miller, G. (1998). Using corpus statistics and wordnet
relations for sense identification. Computational Linguistics. Special Issue on WSD,
24 (1).
327

fiMontoyo, Suarez, Rigau, & Palomar

Lesk, M. (1986). Automated sense disambiguation using machine-readable dictionaries:
How to tell a pine cone from an ice cream cone. In Proceedings of the 1986 SIGDOC
Conference, Association for Computing Machinery, pp. 2426 Toronto, Canada.
Magnini, B., & Strapparava, C. (2000). Experiments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Workshop on Word Senses and Multilinguality Hong Kong, China.
Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A. (2002). The Role of Domain
Information in Word Sense Disambiguation. Natural Language Engineering, 8 (4),
359373.
Manning, C., & Schutze, H. (Eds.). (1999). Foundations of Statistical Natural Language
Processing. The MIT Press.
Manning, C. D., & Schutze, H. (1999). Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.
McRoy, S. W. (1992). Using multiple knowledge sources for word sense discrimination.
Computational Linguistics, 18 (1), 130.
Mihalcea, R. (2002). Instance based learning with automatic feature selection applied to
word sense disambiguation. In Chen, H.-H., & Lin, C.-Y. (Eds.), Proceedings of the
19th International Conference on Computational Linguistics (COLING2002).
Mihalcea, R., & Moldovan, D. (1999). A Method for word sense disambiguation of unrestricted text. In Proceedings of the 37th Annual Meeting of the Association for
Computational Linguistic, ACL99, pp. 152158 Maryland, Usa.
Miller, G. A. (1995). Wordnet: a lexical database for english. Communications of the ACM,
38 (11), 3941.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, T. (1993). A Semantic Concordance.
In Proceedings of ARPA Workshop on Human Language Technology, pp. 303308
Plainsboro, New Jersey.
Mitchell, T. M. (Ed.). (1997). Machine Learning. McGraw Hill.
Montoyo, A., & Palomar, M. (2001). Specification Marks for Word Sense Disambiguation:
New Development. In Gelbukh, A. F. (Ed.), CICLing, Vol. 2004 of Lecture Notes in
Computer Science, pp. 182191. Springer.
Montoyo, A., & Suarez, A. (2001). The University of Alicante word sense disambiguation
system.. In Preiss, & Yarowsky (Preiss & Yarowsky, 2001), pp. 131134.
Montoyo, A., Palomar, M., & Rigau, G. (2001). WordNet Enrichment with Classification
Systems.. In Proceedings of WordNet and Other Lexical Resources: Applications,
Extensions and Customisations Workshop. (NAACL-01) The Second Meeting of the
North American Chapter of the Association for Computational Linguistics, pp. 101
106 Carnegie Mellon University. Pittsburgh, PA, USA.
328

fiCombining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods

Montoyo, A., Vazquez, S., & Rigau, G. (2003). Metodo de desambiguacion lexica basada
en el recurso lexico Dominios Relevantes. Procesamiento del Lenguaje Natural, 31,
141148.
Ng, H. (1997). Exemplar-Base Word Sense Disambiguation: Some Recent Improvements.
In Proceedings of the 2nd Conference on Empirical Methods in Natural Language
Processing, EMNLP.
Ng, H. T., & Lee, H. B. (1996). Integrating multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In Joshi, A., & Palmer, M. (Eds.), Proceedings
of the 34th Annual Meeting of the Association for Computational Linguistics San
Francisco. Morgan Kaufmann Publishers.
Pedersen, T. (2002). Assessing System Agreement and Instance Difficulty in the Lexical
Sample Tasks of Senseval-2. In Proceedings of the Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, ACL2002 Philadelphia, USA.
Preiss, J., & Yarowsky, D. (Eds.). (2001). Proceedings of SENSEVAL-2, Toulouse, France.
ACL-SIGLEX.
Ratnaparkhi, A. (1998). Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.
Rigau, G., Agirre, E., & Atserias, J. (1997). Combining unsupervised lexical knowledge
methods for word sense disambiguation. In Proceedings of joint 35th Annual Meeting
of the Association for Computational Linguistics and 8th Conference of the European
Chapter of the Association for Computational Linguistics ACL/EACL97 Madrid,
Spain.
Rigau, G., Taule, M., Fernandez, A., & Gonzalo, J. (2001). Framework and results for the
spanish senseval.. In Preiss, & Yarowsky (Preiss & Yarowsky, 2001), pp. 4144.
Snedecor, G. W., & Cochran, W. G. (1989). Statistical Methods (8 edition). Iowa State
University Press, Ames, IA.
Snyder, B., & Palmer, M. (2004). The english all-words task. In Proceedings of the
3rd ACL workshop on the Evaluation of Systems for the Semantic Analysis of Text
(SENSEVAL-3). Barcelona, Spain.
Suarez, A., & Palomar, M. (2002a). Feature selection analysis for maximum entropy-based
wsd. In Gelbukh, A. F. (Ed.), CICLing, Vol. 2276 of Lecture Notes in Computer
Science, pp. 146155. Springer.
Suarez, A., & Palomar, M. (2002b). A maximum entropy-based word sense disambiguation
system. In Chen, H.-H., & Lin, C.-Y. (Eds.), Proceedings of the 19th International
Conference on Computational Linguistics (COLING2002), pp. 960966.
Sussna, M. (1993). Word sense disamiguation for free-text indexing using a massive semantic
network. . In Proceedings of the Second International Conference on Information and
Knowledge Base Management, CIKM93, pp. 6774 Arlington, VA.
329

fiMontoyo, Suarez, Rigau, & Palomar

Tapanainen, P., & Jarvinen, T. (1997). A non-projective dependency parser. In Proceedings
of the Fifth Conference on Applied Natural Language Processing, pp. 6471.
Towell, G. G., & Voorhees, E. M. (1998). Disambiguating highly ambiguous words. Computational Linguistics, 24 (1), 125145.
Veenstra, J., den Bosch, A. V., Buchholz, S., Daelemans, W., & Zavrel, J. (2000). Memorybased word sense disambiguation. Computers and the Humanities, Special Issue on
SENSEVAL, 34 (12), 171177.
Wilks, Y., Fass, D., Guo, C.-M., McDonald, J., Plate, T., & Slator, B. (1993). Providing machine tractable dictionary tools. In Pustejovsky, J. (Ed.), Semantics and the
lexicon, pp. 341401. Kluwer Academic Publishers.
Yarowsky, D. (1994). Decision lists for lexical ambiguity resolution: Application to accent
restoration in spanish and french.. In In Proceedings of the 32nd Annual Meeting of
the Association for Computational Linguistics (ACL1994) Las Cruces, NM,.

330

fiJournal of Artificial Intelligence Research 23 (2005) 587-623

Submitted 7/04; published 5/05

An Improved Search Algorithm
for Optimal Multiple-Sequence Alignment
Stefan Schroedl

stefan.schroedl@gmx.de

848 14th St
San Francisco CA 94114
+1 (415) 522-1148

Abstract
Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology.
Although it is NP -hard to find an optimal solution for an arbitrary number of sequences,
due to the importance of this problem researchers are trying to push the limits of exact
algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a
challenge with actual practical relevance.
In this paper, we first review two previous, complementary lines of research. Based on
Hirschbergs algorithm, Dynamic Programming needs O(kN k1 ) space to store both the
search frontier and the nodes needed to reconstruct the solution path, for k sequences of
length N . Best first search, on the other hand, has the advantage of bounding the search
space that has to be explored using a heuristic. However, it is necessary to maintain all
explored nodes up to the final solution in order to prevent the search from re-expanding
them at higher cost. Earlier approaches to reduce the Closed list are either incompatible
with pruning methods for the Open list, or must retain at least the boundary of the Closed
list.
In this article, we present an algorithm that attempts at combining the respective
advantages; like A it uses a heuristic for pruning the search space, but reduces both
the maximum Open and Closed size to O(kN k1 ), as in Dynamic Programming. The
underlying idea is to conduct a series of searches with successively increasing upper bounds,
but using the DP ordering as the key for the Open priority queue. With a suitable choice
of thresholds, in practice, a running time below four times that of A can be expected.
In our experiments we show that our algorithm outperforms one of the currently most
successful algorithms for optimal multiple sequence alignments, Partial Expansion A , both
in time and memory. Moreover, we apply a refined heuristic based on optimal alignments
not only of pairs of sequences, but of larger subsets. This idea is not new; however, to
make it practically relevant we show that it is equally important to bound the heuristic
computation appropriately, or the overhead can obliterate any possible gain.
Furthermore, we discuss a number of improvements in time and space efficiency with
regard to practical implementations.
Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1
of the benchmark database BAliBASE .

1. Introduction: Multiple Sequence Alignment
The multiple sequence alignment problem (MSA) in computational biology consists in aligning several sequences, e.g. related genes from different organisms, in order to reveal simic
2005
AI Access Foundation. All rights reserved.

fiSchroedl

larities and differences across the group. Either DNA can be directly compared, and the
underlying alphabet  consists of the set {C,G,A,T} for the four standard nucleotide bases
cytosine, guanine, adenine and thymine; or we can compare proteins, in which case 
comprises the twenty amino acids.
Roughly speaking, we try to write the sequences one above the other such that the
columns with matching letters are maximized; thereby gaps (denoted here by an additional
letter  ) may be inserted into either of them in order to shift the remaining characters into
better corresponding positions. Different letters in the same column can be interpreted as
being caused by point mutations during the course of evolution that substituted one amino
acid by another one; gaps can be seen as insertions or deletions (since the direction of
change is often not known, they are also collectively referred to as indels). Presumably, the
alignment with the fewest mismatches or indels constitutes the biologically most plausible
explanation.
There is a host of applications of MSA within computational biology; e.g., for determining the evolutionary relationship between species, for detecting functionally active sites
which tend to be preserved best across homologous sequences, and for predicting threedimensional protein structure.
Formally, one associates a cost with an alignment and tries to find the (mathematically)
optimal alignment, i.e., that one with minimum cost. When designing a cost function,
computational efficiency and biological meaning have to be taken into account. The most
widely-used definition is the sum-of-pairs cost function. First, we are given a symmetric
(|| + 1)2 matrix containing penalties (scores) for substituting a letter with another one
(or a gap). In the simplest case, this could be one for a mismatch and zero for a match,
but more biologically relevant scores have been developed. Dayhoff, Schwartz, and Orcutt
(1978) have proposed a model of molecular evolution where they estimate the exchange
probabilities of amino acids for different amounts of evolutionary divergence; this gives rise
to the so-called PAM matrices, where PAM250 is generally the most widely used; Jones,
Taylor, and Thornton (1992) refined the statistics based on a larger body of experimental
data. Based on such a substitution matrix, the sum-of-pairs cost of an alignment is defined
as the sum of penalties between all letter pairs in corresponding column positions.
A pairwise alignment can be conveniently depicted as a path between two opposite
corners in a two-dimensional grid (Needleman and Wunsch, 1981): one sequence is placed
on the horizontal axis from left to right, the other one on the vertical axis from top to
bottom. If there is no gap in either string, the path moves diagonally down and right; a gap
in the vertical (horizontal) string is represented as a horizontal (vertical) move right (down),
since a letter is consumed in only one of the strings. The alignment graph is directed and
acyclic, where a (non-border) vertex has incoming edges from the left, top, and top-left
adjacent vertices, and outgoing edges to the right, bottom, and bottom-right vertices.
Pairwise alignment can be readily generalized to the simultaneous alignment of multiple
sequences, by considering higher-dimensional lattices. For example, an alignment of three
sequences can be visualized as a path in a cube. Fig. 1 illustrates an example for the strings
ABCB, BCD, and DB. It also shows the computation of the sum-of-pairs cost, for a hypothetical
substitution matrix. A real example (problem 2trx of BAliBASE , see Sec. 7.3) is given in
Fig. 2.
588

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

Alignment:

Substitution matrix:

A B C _ B
_ B C D _
_ _ _ D B

A B C D _
A 0 2 4 2 3
B
1 3 3 3
C
2 2 3
D
1 3
_
0

Cost: 6+7+8+7+7 = 35

end

D

C

B

B

D

B

A

C

B

start

Figure 1: Fictitious alignment problem: Column representation, cost matrix, threedimensional visualization of the alignment path through the cube.

A number of improvements can be integrated into the sum-of-pairs cost, like associating
weights with sequences, and using different substitution matrices for sequences of varying
evolutionary distance. A major issue in multiple sequence alignment algorithms is their
ability to handle gaps. Gap penalties can be made dependent on the neighbor letters.
Moreover, it has been found (Altschul, 1989) that assigning a fixed score for each indel
sometimes does not produce the biologically most plausible alignment. Since the insertion
of a sequence of x letters is more likely than x separate insertions of a single letter, gap cost
functions have been introduced that depend on the length of a gap. A useful approximation
are affine gap costs, which distinguish between opening and extension of a gap and charge
a + b  x for a gap of length x, for appropriate a and b. Another frequently used modification
is to waive the penalties for gaps at the beginning or end of a sequence.
Technically, in order to deal with affine gap costs we can no longer identify nodes in the
search graph with lattice vertices, since the cost associated with an edge depends on the
preceding edge in the path. Therefore, it is more suitable to store lattice edges in the priority
589

fiSchroedl

1thx
1grx
1erv
2trcP

_aeqpvlvyfwaswcgpcqlmsplinlaantysdrlkvvkleidpnpttvkkyk______vegvpal
__mqtvi__fgrsgcpysvrakdlaeklsnerdd_fqyqyvdiraegitkedlqqkagkpvetvp__
agdklvvvdfsatwcgpckmikpffhslsekysn_viflevdvddcqdvasece______vksmptf
_kvttivvniyedgvrgcdalnssleclaaeypm_vkfckira_sntgagdrfs______sdvlptl

1thx
1grx
1erv
2trcP

rlvkgeqildstegvis__kdkllsf_ldthln_________
qifvdqqhiggytdfaawvken_____lda____________
qffkkgqkvgefsgan___kek_____leatine__lv____
lvykggelisnfisvaeqfaedffaadvesflneygllper_

Figure 2: Alignment of problem 2trx of BAliBASE , computed with algorithm settings as
described in Sec. 7.3.

A

B

C

D

A
g = 53

cost(_,C)=3
gap penalty = 4
g = 60

B

A
cost(A,_)=3
gap penalty = 4
g = 60

cost(A,C)=4
gap penalty = 0
g = 57

D

Figure 3: Example of computing path costs with affine gap function; the substitution matrix
of Fig. 1 and a gap opening penalty of 4 is used.

queue, and let the transition costs for u  v, v  w be the sum-of-pairs substitution costs
for using one character from each sequence or a gap, plus the incurred gap penalties for
v  w followed by u  v. This representation was adopted in the program MSA (Gupta,
Kececioglu, & Schaeffer, 1995). Note that the state space in this representation grows by
a factor of 2k . An example of how successor costs are calculated, with the cost matrix of
Fig. 1 and a gap opening penalty of 4, is shown in Fig. 3.
For convenience of terminology in the sequel we will still refer to nodes when dealing
with the search algorithm.
590

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

2. Overview
Wang and Jiang (1994) have shown that the optimal multiple sequence alignment problem is
NP -hard; therefore, we cannot hope to achieve an efficient algorithm for an arbitrary number
of sequences. As a consequence, alignment tools most widely used in practice sacrifice the
sound theoretical basis of exact algorithms, and are heuristic in nature (Chan, Wong, &
Chiu, 1992). A wide variety of techniques has been developed. Progressive methods build
up the alignment gradually, starting with the closest sequences and successively adding
more distant ones. Iterative strategies refine an initial alignment through a sequence of
improvement steps.
Despite their limitation to moderate number of sequences, however, the research into
exact algorithms is still going on, trying to push the practical boundaries further. They still
form the building block of heuristic techniques, and incorporating them into existing tools
could improve them. For example, an algorithm iteratively aligning two groups of sequences
at a time could do this with three or more, to better avoid local minima. Moreover, it is
theoretically important to have the gold standard available for evaluation and comparison,
even if not for all problems.
Since MSA can be cast as a minimum-cost path finding problem, it turns out that it is
amenable to heuristic search algorithms developed in the AI community; these are actually
among the currently best approaches. Therefore, while many researchers in this area have
often used puzzles and games in the past to study heuristic search algorithms, recently there
has been a rising interest in MSA as a testbed with practical relevance, e.g., (Korf, 1999;
Korf & Zhang, 2000; Yoshizumi, Miura, & Ishida, 2000; Zhou & Hansen, 2003b); its study
has also led to major improvements of general search techniques.
It should be pointed out that the definition of the MSA problem as given above is not the
only one; it competes with other attempts at formalizing biological meaning, which is often
imprecise or depends on the type of question the biologist investigator is pursuing. E.g., in
this paper we are only concerned with global alignment methods, which find an alignment of
entire sequences. Local methods, in contrast, are geared towards finding maximally similar
partial sequences, possibly ignoring the remainder.
In the next section, we briefly review previous approaches, based on dynamic programming and incorporating lower and upper bounds. In Sec. 4, we describe a new algorithm
that combines and extends some of these ideas, and allows to reduce the storage of Closed
nodes by partially recomputing the solution path at the end (Sec. 5). Moreover, it turns out
that our algorithms iterative deepening strategy can be transferred to find a good balance
between the computation of improved heuristics and the main search (Sec. 6), an issue that
has previously been a major obstacle for their practical application. Sec. 7 presents an
experimental comparison with Partial Expansion A (Yoshizumi, Miura, & Ishida, 2000),
one of the currently most successful approaches. We also solve all but two problems of
Reference 1 of the widely used benchmark database BAliBASE (Thompson, Plewniak, &
Poch, 1999). To the best of our knowledge, this has not been achieved previously with an
exact algorithm.
591

fiSchroedl

3. Previous Work
A number of exact algorithms have been developed previously that can compute alignments
of a moderate number of sequences. Some of them are mostly constrained by available
memory, some by the required computation time, and some on both. We can roughly
group them into two categories: those based on the dynamic programming paradigm, which
proceed primarily in breadth-first fashion; and best-first search, utilizing lower and upper
bounds to prune the search space. Some recent research, including our new algorithm
introduced in Sec. 4, attempts to beneficially combine these approaches.
3.1 Dijkstras Algorithm and Dynamic Programming
Dijkstra (1959) presented a general algorithm for finding the shortest (resp. minimum cost)
path in a directed graph. It uses a priority queue (heap) to store nodes v together with
the shortest found distance from the start node s (i.e., the top-left corner of the grid) to v
(also called the g-value of v). Starting with only s in the priority queue, in each step, an
edge with the minimum g-value is removed from the priority queue; its expansion consists
in generating all of its successors (vertices to the right and/or below) reachable in one step,
computing their respective g-value by adding the edge cost to the previous g-value, and
inserting them in turn into the priority queue in case this newly found distance is smaller
than their previous g-value. By the time a node is expanded, the g-value is guaranteed to
be the minimal path cost from the start node, g  (v) = d(s, v). The procedure runs until the
priority queue becomes empty, or the target node t (the bottom-right corner of the grid)
has been reached; its g-value then constitutes the optimal solution cost g  (t) = d(s, t) of
the alignment problem. In order to trace back the path corresponding to this cost, we move
backwards to the start node choosing predecessors with minimum cost. The nodes can either
be stored in a fixed matrix structure corresponding to the grid, or they can be dynamically
generated; in the latter case, we can explicitly store at each node a backtrack-pointer to
this optimal parent.
For integer edge costs, the priority queue can be implemented as a bucket array pointing
to doubly linked lists (Dial, 1969), so that all operations can be performed in constant time
(To be precise, the DeleteMin-operation also needs a pointer that runs through all different
g-values once; however, we can neglect this in comparison to the number of expansions).
To expand a vertex, at most 2k  1 successor vertices have to be generated, since we have
the choice of introducing a gap in each sequence. Thus, Dijkstras algorithm can solve the
multiple sequence alignment problem in O(2k N k ) time and O(N k ) space for k sequences of
length  N .
A means to reduce the number of nodes that have to be stored for path reconstruction
is by associating a counter with each node that maintains the number of children whose
backtrack-pointer refers to them (Gupta et al., 1995). Since each node can be expanded at
most once, after this the number of referring backtrack-pointers can only decrease, namely,
whenever a cheaper path to one of its children is found. If a nodes reference count goes
to zero, whether immediately after its expansion or when it later loses a child, it can
be deleted for good. This way, we only keep nodes in memory that have at least one
descendant currently in the priority queue. Moreover, auxiliary data structures for vertices
592

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

and coordinates are most efficiently stored in tries (prefix trees); they can be equipped with
reference counters as well and be freed accordingly when no longer used by any edge.
The same complexity as for Dijkstras algorithm holds for dynamic programming (DP);
it differs from the former one in that it scans the nodes in a fixed order that is known
beforehand (hence, contrary to the name the exploration scheme is actually static). The
exact order of the scan can vary (e.g., row-wise or column-wise), as long as it is compatible
with the topological ordering of the graph (e.g., for two sequences that the cells left, top,
and diagonally top-left have been explored prior to a cell). One particular such ordering is
that of antidiagonals, diagonals running from upper-right to lower-left. The calculation of
the antidiagonal of a node merely amounts to summing up its k coordinates.
Hirschberg (1975) noticed that in order to determine only the cost of the optimal alignment g  (t), it would not be necessary to store the whole matrix; instead, when proceeding
e.g. by rows it suffices to keep track of only k of them at a time, deleting each row as soon
as the next one is completed. This reduces the space requirement by one dimension from
O(N k ) to O(kN k1 ). In order to recover the solution path at the end, re-computation of
the lost cell values is needed. A Divide-and-conquer -strategy applies the algorithm twice
to half the grid each, once in forward and once in backward direction, meeting at a fixed
middle row. By adding the corresponding forward and backward distances in this middle
row and finding the minimum, one cell lying on an optimal path can be recovered. This
cell essentially splits the problem into two smaller subproblems, one from the upper left
corner to it, and the other one to the lower right corner; they can be recursively solved
using the same method. In two dimensions, the computation time is at most doubled, and
the overhead reduces even more in higher dimensions.
The FastLSA algorithm (Davidson, 2001) further refines Hirschbergs algorithm by exploiting additionally available memory to store more than one node on an optimal path,
thereby reducing the number of re-computations.
3.2 Algorithms Utilizing Bounds
While Dijkstras algorithm and dynamic programming can be viewed as variants of breadthfirst search, we achieve best first search if we expand nodes v in the order of an estimate
(lower bound) of the total cost of a path from s to the t passing through v. Rather than using
the g-value as in Dijkstras algorithm, we use f (v) := g(v) + h(v) as the heap key, where
h(v) is a lower bound on the cost of an optimal path from v to t. If h is indeed admissible,
then the first solution found is guaranteed to be optimal (Hart, Nilsson, & Raphael, 1968).
This is the classical best-first search algorithm, the A algorithm, well known in the artificial
intelligence community. In this context, the priority queue maintaining the generated nodes
is often also called the Open list, while the nodes that have already been expanded and
removed from it constitute the Closed list. Fig. 4 schematically depicts a snapshot during a
two-dimensional alignment problem, where all nodes with f -value no larger than the current
fmin have been expanded. Since the accuracy of the heuristic decreases with the distance to
the goal, the typical onion-shaped distribution results, with the bulk being located closer
to the start node, and tapering out towards higher levels.
The A algorithm can significantly reduce the total number of expanded and generated
nodes; therefore, in higher dimensions it is clearly superior to dynamic programming. How593

fiSchroedl

M
i
ax
um
m
m
ia

D
er
et

Closed
Open
Possible back leak

Le
ve
ls

(a
n

tid

ia
go

na
ls)

X

Start
X X
X
X
X
X

X
X
X
X
X
End

Figure 4: Snapshot during best-first search in pairwise alignment (schematically).

ever, in contrast to the Hirschberg algorithm, it still stores all of the explored nodes in the
Closed list. Apart from keeping track of the solution path, this is necessary to prevent the
search from leaking back, in the following sense.
A heuristic h is called consistent if h(x)  h(x0 )+c(x, x0 ), for any node x and its child x0 .
A consistent heuristic ensures that (as in the case of Dijkstras algorithm) at the time a node
is expanded, its g-value is optimal, and hence it is never expanded again. However, if we try
to delete the Closed nodes, then there can be topologically smaller nodes in Open with a
higher f -value; when those are expanded at a later stage, they can lead to the re-generation
of the node at a non-optimal g-value, since the first instantiation is no longer available for
duplicate checking. In Fig. 4, nodes that might be subject to spurious re-expansion are
marked X.
Researchers have tried to avoid these leaks, while retaining the basic A search scheme.
Korf proposed to store a list of forbidden operators with each node, or to place the parents
of a deleted node on Open with f -value infinity (Korf, 1999; Korf & Zhang, 2000). However,
as Zhou and Hansen (2003a) remark, it is hard to combine this algorithm with techniques
for reduction of the Open list, and moreover the storage of operators lets the size of the
nodes grow exponentially with the number of sequences. In their algorithm, they keep
track of the kernel of the Closed list, which is defined as the set of nodes that have only
Closed nodes as parents; otherwise a Closed node is said to be in the boundary. The key
idea is that only the boundary nodes have to be maintained, since they shield the kernel
from re-expansions. Only when the algorithm gets close to the memory limit nodes from
the kernel are deleted; the backtrack pointer of the children is changed to the parents of
594

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

the deleted nodes, which become relay nodes for them. For the final reconstruction of the
optimal solution path, the algorithm is called recursively for each relay node to bridge the
gap of missing edges.
In addition to the Closed list, also the Open list can grow rapidly in sequence alignment
problems. Particularly, since in the original A algorithm the expansion of a node generates
all of its children at once, those whose f -value is larger than the optimal cost g  (t) are kept
in the heap up to the end, and waste much of the available space.
If an upper bound U on the optimal solution cost g  (t) is known, then nodes v with
f (v) > U can be pruned right away; this idea is used in several articles (Spouge, 1989; Gupta
et al., 1995). One of the most successful approaches is Yoshizumi et al.s (2000) Partial
Expansion A (PEA ). Each node stores an additional value F , which is the minimum
f -value of all of its yet ungenerated children. In each step, only a node with minimum
F -value is expanded, and only those children with f = F are generated. This algorithm
clearly only generates nodes with f value no larger than the optimal cost, which cannot
be avoided altogether. However, the overhead in computation time is considerable: in the
straightforward implementation, if we want to maintain nodes of constant size, generating
one edge requires determining the f -values of all successors, such that for an interior node
which eventually will be fully expanded the computation time is of the order of the square
of the number of successors, which grows as O(2k ) with the number of sequences k. As a
remedy, in the paper it is proposed to relax the condition by generating all children with
f  F + C, for some small C.
An alternative general search strategy to A that uses only linear space is iterative
deepening A (IDA ) (Korf, 1985). The basic algorithm conducts a depth-first search up to
a pre-determined threshold for the f -value. During the search, it keeps track of the smallest
f -value of a generated successor that is larger than the threshold. If no solution is found,
this provides an increased threshold to be used in the next search iteration.
Wah and Shang (1995) suggested more liberal schemes for determining the next threshold dynamically in order to minimize the number of recomputations. IDA is most efficient
in tree structured search spaces. However, it is difficult to detect duplicate expansions without additional memory; Therefore, unfortunately it is not applicable in lattice-structured
graphs like in the sequence alignment problem due to the combinatorially explosive number
of paths between any two given nodes.
A different line of research tries to restrict the search space of the breadth-first approaches by incorporating bounds. Ukkonen (1985) presented an algorithm for the pairwise
alignment problem which is particularly efficient for similar sequences; its computation time
scales as O(dm), where d is the optimal solution cost. First consider the problem of deciding
whether a solution exists whose cost is less than some upper threshold U . We can restrict
the evaluation of the DP matrix to a band of diagonals where the minimum number of
indels required to reach the diagonal, times the minimum indel cost, does not exceed U .
In general, starting with a minimum U value, we can successively double G until the test
returns a solution; the increase of computation time due to the recomputations is then also
bounded by a factor of 2.
Another approach for multiple sequence alignment is to make use of the lower bounds h
from A . The key idea is the following: Since all nodes with an f -value lower than g  (t) have
to be expanded anyway in order to guarantee optimality, we might as well explore them in
595

fiSchroedl

any reasonable order, like that of Dijkstras algorithm or DP, if we only knew the optimal
cost. Even slightly higher upper bounds will still help pruning. Spouge (1989) proposed to
bound DP to vertices v where g(v) + h(v) is smaller than an upper bound for g  (t).
Linear Bounded Diagonal Alignment (LBD-Align) (Davidson, 2001) uses an upper
bound in order to reduce the computation time and memory in solving a pairwise alignment
problem by dynamic programming. The algorithm calculates the DP matrix one antidiagonal at a time, starting in the top left corner, and working down towards bottom-right.
While A would have to check the bound in every expansion, LBD-Align only checks the
top and bottom cell of each diagonal. If e.g. the top cell of a diagonal has been pruned, all
the remaining cells in that row can be pruned as well, since they are only reachable through
it; this means that the pruning frontier on the next row can be shifted down by one. Thus,
the pruning overhead can be reduced from a quadratic to a linear amount in terms of the
sequence length.
3.3 Obtaining Heuristic Bounds
Up to now we have assumed lower and upper bounds, without specifying how to derive them.
Obtaining an inaccurate upper bound on g  (t) is fairly easy, since we can use the cost of any
valid path through the lattice. Better estimates are e.g. available from heuristic linear-time
alignment programs such as FASTA and BLAST (Altschul, Gish, Miller, Myers, & Lipman,
1990), which are a standard method for database searches. Davidson (2001) employed a
local beam search scheme.
Gusfield (1993) proposed an approximation called the star-alignment. Out of all the
sequences to be aligned, one consensus sequence is chosen such that the sum of its pairwise
alignment costs to the rest of the sequences is minimal. Using this best sequence as
the center, the other ones are aligned using the once a gap, always a gap rule. Gusfield
showed that the cost of the optimal alignment is greater or equal to the cost of this star
alignment, divided by (2  2/k).
For use in heuristic estimates, lower bounds on the k-alignment are often based on
optimal alignments of subsets of m < k sequences. In general, for a vertex v in k-space, we
are looking for a lower bound for a path from v to the target corner t. Consider first the
case m = 2. The cost of such a path is, by definition, the sum of its edge costs, where each
edge cost in turn is the sum of all pairwise (replacement or gap) penalties. Each multiple
sequence alignment induces a pairwise alignment for sequences i and j, by simply copying
rows i and j and ignoring columns with a   in both rows. These pairwise alignments can
be visualized as the projection of an alignment onto its faces, cf. Fig. 1.
By interchange of the summation order, the sum-of-pairs cost is the sum of all pairwise
alignment costs of the respective paths projected on a face, each of which cannot be smaller
than the optimal pairwise path cost. Thus, we can construct an admissible heuristic hpair
by computing, for each pairwise alignment and for each cell in a pairwise problem, the
cheapest path cost to the goal node.
The optimal solutions to all pairwise alignment problems needed for the lower bound h
values are usually computed prior to the main search in a preprocessing step (Ikeda & Imai,
1994). To this end, it suffices to apply the ordinary DP procedure; however, since this time
we are interested in the lowest cost of a path from v to t, it runs in backward direction,
596

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

proceeding from the lower right corner to the upper left, expanding all possible parents of
a vertex in each step.
Let U be an upper bound on the cost of an optimal multiple sequence alignment G.
The sum of all optimal alignment costs Lij = d(sij , tij ) for pairwise subproblems i, j 
{1, . . . , k}, i < j, call it L, is a lower bound on G. Carrillo and Lipman (1988) pointed out
that by the additivity of the sum-of-pairs cost function, any pairwise alignment induced
by the optimal multiple sequence alignment can at most be  = U  L larger than the
respective optimal pairwise alignment. This bound can be used to restrict the number of
values that have to be computed in the preprocessing stage and have to be stored for the
calculation of the heuristic: for the pair of sequences i, j, only those nodes v are feasible
such that a path from the start node si,j to the goal node ti,j exists with total cost no more
than Li,j + . To optimize the storage requirements, we can combine the results of two
searches. First, a forward pass determines for each relevant node v the minimum distance
d(sij , v) from the start node. The subsequent backward pass uses this distance like an exact
heuristic and stores the distance d(v, tij ) from the target node only for those nodes with
d(sij , v) + d(v, tij )  d(s, t) +  1 .
Still, for larger alignment problems the required storage size can be extensive. The
program MSA (Gupta et al., 1995) allows the user to adjust  to values below the CarrilloLipman bound individually for each pair of sequences. This makes it possible to generate
at least heuristic alignments if time or memory doesnt allow for the complete solution;
moreover, it can be recorded during the search if the -bound was actually reached. In the
negative case, optimality of the found solution is still guaranteed; otherwise, the user can
try to run the program again with slightly increased bounds.
The general idea of precomputing simplified problems and storing the solutions for use as
a heuristic has been explored under the name of pattern databases (Culberson & Schaeffer,
1998). However, these approaches implicitly assume that the computational cost can be
amortized over many search instances to the same target. In contrast, in the case of MSA,
the heuristics are instance-specific, so that we have to strike a balance. We will discuss this
in greater depth in Sec. 6.2.

4. Iterative-Deepening Dynamic Programming
As we have seen, a fixed search order as in dynamic programming can have several advantages over pure best-first selection.
 Since Closed nodes can never be reached more than once during the search, it is safe to
delete useless ones (those that are not part of any shortest path to the current Open
1. A slight technical complication arises for affine gap costs: recall that DP implementations usually charge
the gap opening penalty to the g-value of the edge e starting the gap, while the edge e0 ending the gap
carries no extra penalty at all. However, since the sum of pairs heuristics h is computed in backward
direction, using the same algorithm we would assign the penalty for the same path instead to e0 . This
means that the heuristic f = g + h would no longer be guaranteed to be a lower bound, since it
contains the penalty twice. As a remedy, it is necessary to make the computation symmetric by charging
both the beginning and end of a gap with half the cost each. The case of the beginning and end of
the sequences can be handled most conveniently by starting the search from a dummy diagonal edge
((1, . . . , 1), (0, . . . , 0)), and defining the target edge to be the dummy diagonal edge ((N, . . . , N ), (N +
1, . . . , N + 1)), similar to the arrows shown in Fig. 1.

597

fiSchroedl

nodes) and to apply path compression schemes, such as the Hirschberg algorithm.
No sophisticated schemes for avoiding back leaks are required, such as the abovementioned methods of core set maintenance and dummy node insertion into Open.
 Besides the size of the Closed list, the memory requirement of the Open list is determined by the maximum number of nodes that are open simultaneously at any
time while the algorithm is running. When the f -value is used as the key for the
priority queue, the Open list usually contains all nodes with f -values in some range
(fmin , fmin + ); this set of nodes is generally spread across all over the search space,
since g (and accordingly h = (f  g)) can vary arbitrarily between 0 and fmin + . As
opposed to that, if DP proceeds along levels of antidiagonals or rows, at any iteration
at most k levels have to be maintained at the same time, and hence the size of the
Open list can be controlled more effectively. In Fig. 4, the pairwise alignment is partitioned into antidiagonals: the maximum number of open nodes in any two adjacent
levels is four, while the total amounts to seventeen2 .
 For practical purposes, the running time should not only be measured in terms of the
number of node expansions, but one should also take into account the execution time
needed for an expansion. By arranging the exploration order such that edges with
the same head node (or more generally, those sharing a common coordinate prefix)
are dealt with one after the other, much of the computation can be cached, and edge
generation can be sped up significantly. We will come back to this point in Sec. 6.
The remaining issue of a static exploration scheme consists in adequately bounding the
search space using the h-values. A is known to be minimal in terms of the number of node
expansions. If we knew the cost g  (t) of a cheapest solution path beforehand, we could
simply proceed level by level of the grid, however only immediately prune generated edges
e whenever f (e) > g  (t). This would ensure that we only generate those edges that would
have been generated by algorithm A , as well. An upper threshold would additionally help
reduce the size of the Closed list, since a node can be pruned if all of its children lie beyond
the threshold; additionally, if this node is the only child of its parent, this can give rise to
a propagating chain of ancestor deletions.
We propose to apply a search scheme that carries out a series of searches with successively larger thresholds, until a solution is found (or we run out of memory or patience).
The use of such an upper bound parallels that in the IDA algorithm.
The resulting algorithm, which we will refer to as Iterative-Deepening Dynamic Programming (IDDP), is sketched in Fig. 5. The outer loop initializes the threshold with a
lower bound (e.g., h(s)), and, unless a solution is found, increases it up to an upper bound.
In the same manner as in the IDA algorithm, in order to make sure that at least one additional edge is explored in each iteration the threshold has to be increased correspondingly
at least to the minimum cost of a fringe edge that exceeded the previous threshold. This
fringe increment is maintained in the variable minNextThresh, initially estimated as the
upper bound, and repeatedly decreased in the course of the following expansions.
2. Contrary to what the figure might suggest, A can open more than two nodes per level in pairwise
alignments, if the set of nodes no worse than some fmin contains holes.

598

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

procedure IDDP(Edge startEdge, Edge targetEdge, int lowerBound, int upperBound)
int thresh = lowerBound
{Outer loop: Iterative deepening phases}
while (thresh  upperBound) do
Heap h = {(startEdge, 0)}
int minNextThresh = upperBound
{Inner loop: Bounded dynamic programming}
while (not h.IsEmpty()) do
Edge e = h.DeleteMin() {Find and remove an edge with minimum level}
if (e == targetEdge) then
{Optimal alignment found}
return TraceBackPath(startEdge, targetEdge)
end if
Expand(e, thresh, minNextThresh)
end while
int threshIncr = ComputeThreshIncr() {Compute search threshold for next iteration, see text}
thresh = max(thresh + threshIncr, minNextThresh)
end while
print(No alignment with cost at most upperBound found)

Figure 5: Algorithm Iterative-Deepening Dynamic Programming.
In each step of the inner loop, we select and remove a node from the priority queue
whose level is minimal. As explained later in Sec. 6, it is favorable to break ties according
to the lexicographic order of target nodes. Since the total number of possible levels is
comparatively small and known in advance, the priority queue can be implemented using
an array of linked lists (Dial, 1969); this provides constant time operations for insertion and
deletion.
The expansion of an edge e is partial (Fig. 6). A child edge might already exist from an
earlier expansion of an edge with the same head vertex; we have to test if we can decrease
the g-value. Otherwise, we generate a new edge, if only temporarily for the sake of calculating its f -value; that is, if its f -value exceeds the search threshold of the current iteration,
its memory is immediately reclaimed. Moreover, in this case the fringe threshold minNextThresh is updated. In a practical implementation, we can prune unnecessary accesses to
partial alignments inside the calculation of the heuristic e.GetH() as soon as as the search
threshold has already been reached.
The relaxation of a child edge within the threshold is performed by the subprocedure
UpdateEdge (cf. Fig. 7). This is similar to the corresponding relaxation step in A , updating
the childs g- and f values, its parent pointers, and inserting it into Open, if not already
contained. However, in contrast to best-first search, it is inserted into the heap according to
the antidiagonal level of its head vertex. Note that in the event that the former parent loses
its last child, propagation of deletions (Fig. 8) can ensure that only those Closed nodes
continue to be stored that belong to some solution path. Edge deletions can also ensue
deletion of dependent vertex and coordinate data structures (not shown in the pseudocode).
The other situation that gives rise to deletions is if immediately after the expansion of a
node no children are pointing back to it (the children might either be reachable more cheaply
from different nodes, or their f -value might exceed the threshold).
599

fiSchroedl

procedure Expand(Edge e, int thresh, int minNextThresh)
for all Edge child  Succ(e) do
{Retrieve child or tentatively generate it if not yet existing, set boolean variable created
accordingly}
int newG = e.GetG() + GapCost(e, child)
+ child.GetCost()
int newF = newG + child.GetH()
if (newF  thresh and newG < child.GetG()) then
{Shorter path than current best found, estimate within threshold}
child.SetG(newG)
UpdateEdge(e, child, h) {Update search structures}
else if (newF > thresh) then
minNextThresh =
min(minNextThresh, newF)
{Record minimum of pruned edges}
if (created) then
Delete(child) {Make sure only promising edges are stored}
end if
end if
end for
if (e.ref == 0) then
DeleteRec(e) {No promising children could be inserted into the heap}
end if

Figure 6: Edge expansion in IDDP.
procedure UpdateEdge(Edge parent, Edge child, Heap h)
parent.ref++
child.GetBacktrack().ref
if (child.GetBacktrack().ref == 0) then
DeleteRec(child.GetBacktrack()) {The former parent has lost its last child and becomes useless}
end if
child.SetBacktrack(parent)
if (not h.Contains(child)) then
h.Insert(child, child.GetHead().GetLevel())
end if

Figure 7: Edge relaxation in IDDP.

The correctness of the algorithm can be shown analogously to the soundness proof of A .
If the threshold is smaller than g  (t), the DP search will terminate without encountering
a solution; otherwise, only nodes are pruned that cannot be part of an optimal path. The
invariant holds that there is always a node in each level which lies on an optimal path and
is in the Open list. Therefore, if the algorithm terminates only when the heap runs empty,
the best found solution will indeed be optimal.
The iterative deepening strategy results in an overhead computation time due to reexpansions, and we are trying to restrict this overhead as much as possible. More precisely,
600

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

procedure DeleteRec(Edge e)
if (e.GetBacktrack() 6= nil) then
e.GetBacktrack().ref
if (e.GetBacktrack().ref == 0) then
DeleteRec(e.GetBacktrack())
end if
end if
Delete(e)

Figure 8: Recursive deletion of edges that are no longer part of any solution path.
procedure TraceBack(Edge startEdge, Edge e)
if (e == startEdge) then
return {End of recursion}
end if
if (e.GetBackTrack().GetTarget() 6= e.GetSource()) then
{Relay node: recursive path reconstruction}
IDDP( e.GetBackTrack(), e, e.GetF(), e.GetF())
end if
OutputEdge(e)
TraceBack(startEdge, e.GetBackTrack())

Figure 9: Divide-and-Conquer solution reconstruction in reverse order.
we want to minimize the ratio
=

nIDDP
,
nA

where nIDDP and nA denote the number of expansions in IDDP and A , respectively. One
way to do so (Wah & Shang, 1995) is to choose a threshold sequence 1 , 2 , . . . such that
the number of expansions ni in stage i satisfies
ni = rni1 ,
for some fixed ratio r. If we choose r too small, the number of re-expansions and hence
the computation time will grow rapidly, if we choose it too big, then the threshold of the
last iteration can exceed the optimal solution cost significantly, and we will explore many
irrelevant edges. Suppose that n0 rp < nA  n0 rp+1 . Then the algorithm performs p + 1
iterations. In the worst case, the overshoot will be maximal if A finds the optimal solution
just above the previous threshold, nA = n0 rp + 1. The total number of expansions is
P
r(r p+1 1)
r2
i
n0 p+1
, and the ratio  becomes approximately r1
. By setting the
i=0 r = n0
r1
derivative of this expression to zero, we find that the optimal value for r is 2; the number
of expansions should double from one search stage to the next. If we achieve doubling, we
will expand at most four times as many nodes as A .
Like in Wah and Shangs (1995) scheme, we dynamically adjust the threshold using runtime information. Procedure ComputeThreshIncr stores the sequence of expansion numbers
and thresholds from the previous search stages, and then uses curve fitting for extrapolation
(in the first few iterations without sufficient data available, a very small default threshold
is applied). We found that the distribution of nodes n() with f -value smaller or equal to
601

fiSchroedl

threshold  can be modeled very accurately according to the exponential approach
n() = A  B  .
Consequently, in order to attempt to double the number of expansions, we choose the next
threshold according to
1
i+1 = i +
.
log2 B

5. Sparse Representation of Solution Paths
When the search progresses along antidiagonals, we do not have to fear back leaks, and
are free to prune Closed nodes. Similarly as in Zhou and Hansens (2003a) work, however,
we only want to delete them lazily and incrementally when being forced by the algorithm
approaching the computers memory limit.
When deleting an edge e, the backtrack-pointers of its child edges that refer to it are
redirected to the respective predecessor of e, whose reference count is increased accordingly.
In the resulting sparse solution path representation, backtrack pointers can point to any
optimal ancestors.
After termination of the main search, we trace back the pointers starting with the goal
edge; this is outlined in Procedure TraceBack (Fig. 9), which prints out the solution path
in reverse order. Whenever an edge e points back to an ancestor e0 which is not its direct
parent, we apply an auxiliary search from start edge e0 to goal edge e in order to reconstruct
the missing links of the optimal solution path. The search threshold can now be fixed at the
known solution cost; moreover, the auxiliary search can prune those edges that cannot be
ancestors of e because they have some coordinate greater than the corresponding coordinate
in e. Since also the shortest distance between e and e0 is known, we can stop at the first path
that is found at this cost. To improve the efficiency of the auxiliary search even further,
the heuristic could be recomputed to suit the new target. Therefore, the cost of restoring
the solution path is usually marginal compared to that of the main search.
Which edges are we going to prune, in which order? For simplicity, assume for the
moment that the Closed list consists of a single solution path. According to the Hirschberg
approach, we would keep only one edge, preferably lying near the center of the search
space (e.g., on the longest anti-diagonal), in order to minimize the complexity of the two
auxiliary searches. With additional available space allowing to store three relay edges, we
would divide the search space into four subspaces of about equal size (e.g., additionally
storing the antidiagonals half-way between the middle antidiagonal and the start node resp.
the target node). By extension, in order to incrementally save space under diminishing
resources we would first keep only every other level, then every fourth, and so on, until only
the start edge, the target edge, and one edge half-way on the path would be left.
Since in general the Closed list contains multiple solution paths (more precisely, a tree
of solution paths), we would like to have about the same density of relay edges on each of
them. For the case of k sequences, an edge reaching level l with its head node can originate
with its tail node from level l  1, . . . , l  k. Thus, not every solution path passes through
each level, and deleting every other level could result in leaving one path completely intact,
while extinguishing another totally. Thus, it is better to consider contiguous bands of k
602

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

procedure SparsifyClosed()
for (int sparse = 1 to blog2 N c) do
while (UsedMemory() > maxMemory and exists {Edge e  Open | e.GetLastSparse() <
sparse}) do
Edge pred = e.GetBacktrack()
{Trace back solution path}
while (pred 6= nil and e.GetLastSparse() < sparse) do
e.SetLastSparse(sparse) {Mark to avoid repeated trace-back}
if (bpred.GetHead().GetLevel() / kc mod 2sparse 6= 0) then
{pred lies in prunable band: redirect pointer}
e.SetBacktrack(pred.GetBacktrack())
e.GetBacktrack().ref++
pred.ref
if (pred.ref == 0) then
{e is the last remaining edge referring to pred}
DeleteRec(pred)
end if
else
{Not in prunable band: continue traversal}
e = e.GetBacktrack()
end if
pred = e.GetBacktrack()
end while
end while
end for

Figure 10: Sparsification of Closed list under restricted memory.

levels each, instead of individual levels. Bands of this size cannot be skipped by any path.
The total number of antidiagonals in an alignment problem of k sequences of length N is
k  N  1; thus, we can decrease the density in blog2 N c steps.
A technical implementation issue concerns the ability to enumerate all edges that reference some given prunable edge, without explicitly storing them in a list. However, the
reference counting method described above ensures that any Closed edge can be reached by
following a path bottom-up from some edge in Open. The procedure is sketched in Fig. 10.
The variable sparse denotes the interval between level bands that are to be maintained in
memory. In the inner loop, all paths to Open nodes are traversed in backward direction;
for each edge e0 that falls into a prunable band, the pointer of the successor e on the path
is redirected to its respective backtrack pointer. If e was the last edge referencing e0 , the
latter one is deleted, and the path traversal continues up to the start edge. When all Open
nodes have been visited and the memory bound is still exceeded, the outer loop tries to
double the number of prunable bands by increasing sparse.
Procedure SparsifyClosed is called regularly during the search, e.g., after each expansion.
However, a naive version as described above would incur a huge overhead in computation
time, particularly when the algorithms memory consumption is close to the limit. Therefore, some optimizations are necessary. First, we avoid tracing back the same solution path
at the same (or lower) sparse interval by recording for each edge the interval when it was
603

fiSchroedl

traversed the last time (initially zero); only for an increased variable sparse there can be
anything left for further pruning. In the worst case, each edge will be inspected blog2 N c
times. Secondly, it would be very inefficient to actually inspect each Open node in the inner
loop, just to find that its solution path has been traversed previously, at the same or higher
sparse value; however, with an appropriate bookkeeping strategy it is possible to reduce the
time for this search overhead to O(k).

6. Use of Improved Heuristics
As we have seen, the estimator hpair , the sum of optimal pairwise goal distances, gives
a lower bound on the actual path length. However, more powerful heuristics are also
conceivable. While their computation will require more resources, the trade-off can prove
itself worthwhile; the tighter the estimator is, the smaller is the space that the main search
needs to explore.
6.1 Beyond Pairwise Alignments
Kobayashi and Imai (1998) suggested to generalize hpair by considering optimal solutions
for subproblems of size m > 2. They proved that the following heuristics are admissible
and more informed than the pairwise estimate.
 hall,m is the sum of all m-dimensional optimal costs, divided by

k2 
m2 .

 hone,m splits the sequences into two sets of sizes m and k  m; the heuristic is the sum
of the optimal cost of the first subset, plus that of the second one, plus the sum of all
2-dimensional optimal costs of all pairs of sequences in different subsets. Usually, m
is chosen close to k/2.
These improved heuristics can reduce the main search effort by orders of magnitudes.
However, in contrast to pairwise sub-alignments, time and space resources devoted to compute and store higher-dimensional heuristics are in general no longer negligible compared
to the main search. Kobayashi and Imai (1998) noticed that even for the case m = 3 of
triples of sequences, it can be impractical to compute the entire subheuristic hall,m . As one
reduction, they show that it suffices to restrict oneself to nodes where the path cost does
not exceed the optimal path cost of the subproblem by more than
!

=

X
k2
U
d(si1 ,...,im , ti1 ,...,im );
m2
i ,...,i
1

m

this threshold can be seen as a generalization of the Carrillo-Lipman bound. However,
it can still
incur excessive overhead in space and computation time for the computation of
k
the m
lower-dimensional subproblems. A drawback is that it requires an upper bound
U , on whose accuracy also the algorithms efficiency hinges. We could improve this bound
by applying more sophisticated heuristic methods, but it seems counterintuitive to spend
more time doing so which we would rather use to calculate the exact solution. In spite of
its advantages for the main search, the expensiveness of the heuristic calculation appears
as a major obstacle.
604

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

McNaughton, Lu, Schaeffer, and Szafron (2002) suggested to partition the heuristic
into (hyper-) cubes using a hierarchical oct-tree data structure; in contrast to full cells,
empty cells only retain the values at their surface. When the main search tries to use one
of them, its interior values are recomputed on demand. Still, this work assumes that each
node in the entire heuristic is calculated at least once using dynamic programming.
We see one cause of the dilemma in the implicit assumption that a complete computation
is necessary. The bound  above refers to the worst-case, and can generally include many
more nodes than actually required in the main search. However, since we are only dealing
with the heuristic, we can actually afford to miss some values occasionally; while this might
slow down the main search, it cannot compromise the optimality of the final solution.
Therefore, we propose to generate the heuristics with a much smaller bound . Whenever
the attempt to retrieve a value of the m-dimensional subheuristic
fails during the main
m
search, we simply revert to replacing it by the sum of the 2 optimal pairwise goal distances
it covers.
We believe that the IDDP algorithm lends itself well to make productive use of higherdimensional heuristics. Firstly and most importantly, the strategy of searching to adaptively
increasing thresholds can be transferred to the -bound as well; this will be addressed in
more detail in the next section.
Secondly, as far as a practical implementation is concerned, it is important to take into
account not only how a higher-dimensional heuristic affects the number of node expansions,
but also their time complexity. This time is dominated by the number of accesses to subalignments. With k sequences, in the worst case an edge has 2k  1 successors, leading to
a total of
!
k
k
(2  1)
m
evaluations for hall,m . One possible improvement is to enumerate all edges emerging from a
given vertex in lexicographic order, and to store partial sums of heuristics of prefix subsets
of sequences for later re-use. In this way, if we allow for a cache of linear size, the number
of accesses is reduced to
!
i=k
X
i i1
;
2
m1
i=m
correspondingly, for a quadratic cache we only need
i=k
X
i=m

!

2

i

i2
m2

evaluations. For instance, in aligning 12 sequences using hall,3 , a linear cache reduces the
evaluations to about 37 percent within one expansion.
As mentioned above, in contrast to A , IDDP gives us the freedom to choose any
particular expansion order of the edges within a given level. Therefore, when we sort edges
lexicographically according to the target nodes, much of the cached prefix information can
be shared additionally across consecutively expanded edges. The higher the dimension of
the subalignments, the larger are the savings. In our experiments, we experienced speedups
of up to eighty percent in the heuristic evaluation.
605

fiSchroedl

Execution time [s]

100

Main search
Heuristic
Total time

10

1

0.1
0

10

20

30 40 50 60 70
Heuristic miss ratio r [%]

80

90

100

Figure 11: Trade-off between heuristic and main search: Execution times for problem 1tvxA
as a function of heuristic miss ratio.

6.2 Trade-Off between Computation of Heuristic and Main Search
As we have seen, we can control the size of the precomputed sub-alignments by choosing
the bound  up to which f -values of edges are generated beyond the respective optimal
solution cost. There is obviously a trade-off between the auxiliary and main searches. It
is instructive to consider the heuristic miss ratio r, i.e., the fraction of calculations of
the heuristic h during the main search when a requested entry in a partial MSA has not
been precomputed. The optimum for the main search is achieved if the heuristic has been
computed for every requested edge (r = 0). Going beyond that point will generate an
unnecessarily large heuristic containing many entries that will never be actually used. On
the other hand, we are free to allocate less effort to the heuristic, resulting in r > 0 and
consequently decreasing performance of the main search. Generally, the dependence has
an S-shaped form, as exemplified in Fig. 11 for the case of problem 1tvxA of BAliBASE
(cf. next section). Here, the execution time of one iteration of the main search at a fixed
threshold of 45 above the lower bound is shown, which includes the optimal solution.
Fig. 11 illustrates the overall time trade-off between auxiliary and main search, if we fix
 at different levels. The minimum total execution time, which is the sum of auxiliary and
main search, is attained at about r = 0.15 (5.86 seconds). The plot for the corresponding
memory usage trade-off has a very similar shape.
Unfortunately, in general we do not know in advance the right amount of auxiliary search.
As mentioned above, choosing  according to the Carrillo-Lipman bound will ensure that
606

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

Execution time [s]

100

10

1

0.1
0

10

20

30

40
50
60
70
Heuristic miss ratio r [%]

80

90

100

Figure 12: Time of the last iteration in the main search for problem 1tvxA as a function of
heuristic miss ratio.

every requested sub-alignment cost will have been precomputed; however, in general we will
considerably overestimate the necessary size of the heuristic.
As a remedy, our algorithm IDDP gives us the opportunity to recompute the heuristic in
each threshold iteration in the main search. In this way, we can adaptively strike a balance
between the two.
When the currently experienced miss rate r rises above some threshold, we can suspend
the current search, recompute the pairwise alignments with an increased threshold , and
resume the main search with the improved heuristics.
Like for the main search, we can accurately predict the auxiliary computation time
and space at threshold  using exponential fitting. Due to the lower dimensionality, it
will generally increase less steeply; however, the constant factor might be higher for the
k
heuristic, due to the combinatorial number of m
alignment problems to be solved.
A doubling scheme as explained above can bound the overhead to within a constant
factor of the effort in the last iteration. In this way, when also limiting the heuristic
computation time by a fixed fraction of the main search, we can ensure as an expected
upper bound that the overall execution time stays within a constant factor of the search
time that would be required using only the pairwise heuristic.
If we knew the exact relation between , r, and the speedup of the main search, an ideal
strategy would double the heuristic whenever the expected computation time is smaller than
the time saved in the main search. However, as illustrated in Fig. 12, this dependence is
more complex than simple exponential growth, it varies with the search depth and specifics
of the problem. Either we would need a more elaborate model of the search space, or the
607

fiSchroedl

algorithm would have to conduct exploratory searches in order to estimate the relation.
We leave this issue to future work, and restrict ourselves here to a simplified, conservative
heuristic: We hypothesize that the main search can be made twice as fast by a heuristic
doubling if the miss rate r rises above 25 percent; in our experiments, we found that this
assumption is almost always true. In this event, since the effective branching factor of the
main search is reduced by the improved heuristic, we also ignore the history of main search
times in the exponential extrapolation procedure for subsequent iterations.

7. Experimental Results
In the following, we compare IDDP to one of the currently most successful approaches,
Partial Expansion A . We empirically explore the benefit of higher-dimensional heuristics;
finally, we show its feasibility by means of the benchmark database BAliBASE .
7.1 Comparison to Partial Expansion A
For the first series of evaluations, we ran IDDP on the same set of sequences as chosen by
Yoshizumi et al. (2000) (elongation factors EF-TU and EF-1 from various species, with a
high degree of similarity). As in this work, substitution costs were chosen according to the
PAM-250 matrix. The applied heuristic was the sum of optimal pairwise goal distances. The
expansion numbers do not completely match with their results, however, since we applied
the biologically more realistic affine gap costs: gaps of length x were charged 8+8x, except
at the beginning and end of a sequence, where the penalty was 8  x.
All of the following experiments were run under RedHat Linux 7.3 on an Intel XeonT M
CPU with 3.06 GHz, and main memory of 2 Gigabytes; we used the gcc 2.96 compiler.
The total space consumption of a search algorithm is determined by the peak number of
Open and Closed edges over the entire running time. Table 1 and Fig. 13 give these values
for the series of successively larger sets of input sequences (with the sequences numbered as
defined in Yoshizumi et al., 2000) 1  4, 1  5, . . ., 1  12.
With our implementation, the basic A algorithm could be carried out only up to 9
sequences, before exhausting our computers main memory.
Confirming the results of Yoshizumi et al. (2000), Partial Expansion requires only about
one percent of this space. Interestingly, during the iteration with the peak in total numbers
of nodes held in memory, no nodes are actually closed except in problem 6. This might
be explained with the high degree of similarity between sequences in this example. Recall
that PEA only closes a node if all of its successors have an f -value of no more than the
optimal solution cost; if the span to the lower bound is small, each node can have at least
one bad successor that exceeds this difference.
IDDP reduces the memory requirements further by a factor of about 6. The diagram
also shows the maximum size of the Open list alone. For few sequences, the difference
between the two is dominated by the linear length to store the solution path. As the
problem size increases, however, the proportion of the Closed list of the total memory drops
to about only 12 percent for 12 sequences. The total number of expansions (including all
search stages) is slightly higher than in PEA ; however, due to optimizations made possible
by the control of the expansion order, the execution time at 12 sequences is reduced by
about a third.
608

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

Num
Exp

Time
[sec]

Max
Open

Max
Open +
Closed

626
1599
3267
10781
116261
246955

A
0.01
0.05
0.25
1.94
49.32
318.58

7805
32178
124541
666098
9314734
35869671

8432
33778
127809
676880
9430996
36116627

3
4
5
6
7
8
9
10
11
12

448
716
2610
6304
23270
330946
780399
5453418
20887627
36078736

PEA
0.01
0.01
0.05
0.33
2.63
87.24
457.98
7203.17
62173.78
237640.14

442
626
1598
3328
10874
118277
249279
1569815
5620926
9265949

442
626
1598
3331
10874
118277
249279
1569815
5620926
9265949

3
4
5
6
7
8
9
10
11
12

496
1367
6776
12770
26026
362779
570898
4419297
21774869
36202456

IDDP
0.01
0.02
0.14
0.59
2.46
73.62
250.48
4101.96
43708.14
158987.80

4
9
171
414
889
13620
21506
160240
860880
1417151

434
443
501
972
1749
19512
30009
192395
997163
1616480

4
5
6
7
8
9

Table 1: Algorithm comparison for varying number of input sequences (elongation factors
EF-TU and EF-1).

Since PEA does not prune edges, its maximum space usage is always the total number
of edges with f -value smaller than g  (t) (call these edges the relevant edges, since they have
to be inspected by each admissible algorithm). In IDDP, on the other hand, the Open list
can only comprise k adjacent levels out of those edges (not counting the possible threshold
overshoot, which would contribute a factor of at most 2). Thus, the improvement of IDDP
over PEA will tend to increase with the overall number of levels (which is the sum of
609

fiSchroedl

1e+08
1e+07

Edges in memory

1e+06
100000
10000
1000
100
A* max open+closed
PEA* max open+closed
IDDP max open+closed
IDDP max open

10
1
3

4

5

6

7
8
9
Number of sequences

10

11

12

Figure 13: Memory requirements for A , IDDP, and PEA (elongation factors EF-TU and
EF-1).

all string lengths), divided by the number of sequences; in other words, with the average
sequence length.
Moreover, the ratio depends on how well the heuristic suits the particular problem.
Fig. 14 shows the distribution of all edges with f value smaller or equal to g  (t), for the
case of 9 of the example sequences. This problem is quite extreme as the bulk of these edges
is concentrated in a small level band between 1050 and 1150. As an example with a more
even distribution, Fig. 15 depicts the situation for problem 1cpt from Reference 1 in the
benchmark set BAliBASE (Thompson et al., 1999) with heuristic hall,3 . In this case, the
proportion of the overall 19492675 relevant edges that are maximal among all 4 adjacent
levels amounts to only 0.2 percent. The maximum Open size in IDDP is 7196, while the
total number of edges generated by PEA is 327259, an improvement by about a factor of
45.
7.2 Multidimensional Heuristics
On the same set of sequences, we compared different improved heuristics in order to get an
impression for their respective potential. Specifically, we ran IDDP with heuristics hpair ,
hall,3 , hall,4 , and hone,k/2 at various thresholds . Fig. 16 shows the total execution time
for computing the heuristics, and performing the main search. In each case, we manually
selected a value for  which minimized this time. It can be seen that the times for hone,k/2
lie only a little bit below hpair ; For few sequences (less than six), the computation of the
heuristics hall,3 and hall,4 dominates their overall time. With increasing dimensions, how610

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

0.016

Open edges / sum open edges [%]

0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
0

500

1000

1500

2000
Level

2500

3000

3500

4000

Figure 14: Distribution of relevant edges over levels (elongation factors EF-TU and EF-1);
compare to the schematic projection in Fig. 4.

ever, this investment starts to yield growing returns, with hall,3 being the fastest algorithm,
requiring only 5 percent of the time of hpair at 12 sequences.
As far as memory is concerned, Fig. 17 reveals that the maximum size of the Open and
Closed list, for the chosen  values, is very similar for hpair and hone,k/2 on the one hand,
and hall,3 and hall,4 on the other hand.
At 12 sequences, hone,6 saves only about 60 percent of edges, while hall,3 only needs 2.6
percent and hall,4 only 0.4 percent of the space required by the pairwise heuristic. Using
IDDP, we never ran out of main memory; even larger test sets could be aligned, the range
of the shown diagrams was limited by our patience to wait for the results for more than two
days.
Based on the experienced burden of computing the heuristic, Kobayashi and Imai (1998)
concluded that hone,m should be preferred to hall,m . We do not quite agree with this judgment. We see that the heuristic hall,m is able to reduce the search space of the main search
considerably stronger than hone,m , so that it can be more beneficial with an appropriate
amount of heuristic computation.
7.3 The Benchmark Database BAliBASE
BAliBASE (Thompson et al., 1999) is a widely used database of manually-refined multiple
sequence alignments specifically designed for the evaluation and comparison of multiple sequence alignment programs. The alignments are classified into 8 reference sets. Reference 1
contains alignments of up to six about equidistant sequences. All the sequences are of sim611

fiSchroedl

Open edges / sum open edges [%]

0.00012

0.0001

8e-05

6e-05

4e-05

2e-05

0
0

200

400

600

800
Level

1000

1200

1400

1600

Figure 15: Distribution of relevant edges over levels, problem 1cpt from BAliBASE .
ilar length; they are grouped into 9 classes, indexed by sequence length and the percentage
of identical amino acids in the same columns. Note that many of these problems are indeed much harder than the elongation factor examples from the previous section; despite
consisting of fewer sequences, their dissimilarities are much more pronounced.
We applied our algorithm to Reference 1, with substitution costs according to the PET91
matrix (Jones et al., 1992) and affine gap costs of 9x+8, except for leading and trailing gaps,
where no gap opening penalty was charged. For all instances, we precomputed the pairwise
sub-alignments up to a fixed bound of 300 above the optimal solution; the optimal solution
was found within this bound in all cases, and the effort is generally marginal compared to
the overall computation. For all problems involving more than three sequences, the heuristic
hall,3 was applied.
Out of the 82 alignment problems in Reference 1, our algorithm could solve all but 2
problems (namely, 1pamA and gal4 ) on our computer. Detailed results are listed in Tables 2
through 10.
Thompson, Plewniak, and Poch (1999) compared a number of widely used heuristic
alignment tools using the so-called SP -score; their software calculates the percentage of
correctly aligned pairs within the biologically significant motifs. They found that all programs perform about equally well for the sequences with medium and high amino acid
identity; differences only occurred for the case of the more distant sequences with less
than 25 percent identity, the so-called twilight zone. Particularly challenging was the
group of short sequences. In this subgroup, the three highest scoring programs are PRRP,
CLUSTALX, and SAGA, with respective median scores of 0.560, 0.687, and 0.529. The
medium score for the alignments found in our experiments amounts to 0.558; hence, it is
about as good as PRRP, and only beaten by CLUSTALX. While we focused in our exper612

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

1e+06
100000
10000

Total time [sec]

1000
100
10
1
0.1
2-fold heuristic
div-conq heuristic
3-fold heuristic
4-fold heuristic

0.01
0.001
2

4

6

8
10
Number of sequences

12

14

Figure 16: Comparison of execution times (including calculation of heuristics), elongation
factors EF-TU and EF-1.

iments on algorithmic feasibility rather than on solution quality, it would be worthwhile
to attempt to improve the alignments found by these program using their more refined
penalty functions. CLUSTALX, for example, uses different PAM matrices depending on
the evolutionary distance of sequences; moreover, it assigns weights to sequences (based on
a phylogenetic tree), and gap penalties are made position-specific. All of these improvements can be easily integrated into the basic sum-of-pairs cost function, so that we could
attempt to compute an optimal alignment with respect to these metrics. We leave this line
of research for future work.
Fig. 18 shows the maximum number of edges that have to be stored in Open during the
search, in dependence of the search threshold in the final iteration. For better comparability,
we only included those problems in the diagram that consist of 5 sequences. The logarithmic
scale emphasizes that the growth fits an exponential curve quite well. Roughly speaking, an
increase of the cost threshold by 50 leads to a ten-fold increase in the space requirements.
This relation is similarly applicable to the number of expansions (Fig. 19).
Fig. 20 depicts the proportion between the maximum Open list size and the combined
maximum size of Open and Closed. It is clearly visible that due to the pruning of edges
outside of possible solution paths, the Closed list contributes less and less to the overall
space requirements the more difficult the problems become.
Finally, we estimate the reduction in the size of the Open list compared to all relevant
edges by the ratio of the maximum Open size in the last iteration of IDDP to the total
number of expansions in this stage, which is equal to the number of edges with f -value
less or equal to the threshold. Considering possible overshoot of IDDP, algorithm PEA
613

fiSchroedl

Maximum size of open + closed

1e+07

1e+06

100000

10000

1000

2-fold heuristic
div-conq heuristic
3-fold heuristic
4-fold heuristic

100
2

4

6

8
10
Number of sequences

12

14

Figure 17: Combined maximum size of Open and Closed, for different heuristics (elongation
factors EF-TU and EF-1).

1e+07
1e+06

Max open

100000
10000
1000
100
10

Short
Medium length
Long

1
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 18: Maximum size of Open list, dependent on the final search threshold (BAliBASE ).

614

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

1e+08
1e+07

Expansions

1e+06
100000
10000
1000
100

Short
Medium length
Long

10
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 19: Number of expansions in the final search iteration (BAliBASE ).

80

Max open/ Max open + closed [%]

70
60
50
40
30
20
10

Short
Medium length
Long

0
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 20: Maximum number of Open edges, divided by combined maximum of Open and
Closed (BAliBASE ).

615

fiSchroedl

5

Short
Medium length
Long

Max Open / Expansions [%]

4

3

2

1

0
0

50

100
150
200
Threshold - Lower bound

250

300

Figure 21: Percentage of reduction in Open size (BAliBASE ).
would expand at least half of these nodes. The proportion ranges between 0.5 to 5 percent
(cf. Fig. 21). Its considerable scatter indicates the dependence on individual problem properties; however, a slight average decrease can be noticed for the more difficult problems.

616

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

8. Conclusion and Discussion
We have presented a new search algorithm for optimal multiple sequence alignment that
combines the effective use of a heuristic bound as in best-first search with the ability of the
dynamic programming approach to reduce the maximum size of the Open and Closed lists
by up to one order of magnitude of the sequence length. The algorithm performs a series
of searches with successively increasing bounds that explore the search space in DP order;
the thresholds are chosen adaptively so that the expected overhead in recomputations is
bounded by a constant factor.
We have demonstrated that the algorithm can outperform one of the currently most
successful algorithms for optimal multiple sequence alignments, Partial Expansion A , both
in terms of computation time and memory consumption. Moreover, the iterative-deepening
strategy alleviates the use of partially computed higher-dimensional heuristics. To the best
of our knowledge, the algorithm is the first one that is able to solve standard benchmark
alignment problems in BAliBASE with a biologically realistic cost function including affine
gap costs without end gap penalties. The quality of the alignment is in the range of the
best heuristic programs; while we have concentrated on algorithmic feasibility, we deem it
worthwhile to incorporate their refined cost metrics for better results; we will study this
question in future work.
Recently, we learned about related approaches developed simultaneously and independently by Zhou and Hansen (2003b, 2004). SweepA explores a search graph according
to layers in a partial order, but still uses the f -value for selecting nodes within one layer.
Breadth-First Heuristic Search implicitly defines the layers in a graph with uniform costs
according to the breadth-first traversal. Both algorithms incorporate upper bounds on the
optimal solution cost for pruning; however, the idea of adaptive threshold determination to
limit re-expansion overhead to a constant factor is not described. Moreover, they do not
consider the flexible use of additional memory to minimize the divide-and-conquer solution
reconstruction phase.
Although we described our algorithm entirely within the framework of the MSA problem,
it is straightforward to transfer it to any domain in which the state space graph is directed
and acyclic. Natural candidates include applications where such an ordering is imposed by
time or space coordinates, e.g., finding the most likely path in a Markov model.
Two of the BAliBASE benchmark problems could still not be solved by our algorithm
within the computers main memory limit. Future work will include the integration of
techniques exploiting secondary memory. We expect that the level-wise exploration scheme
of our algorithm lends itself naturally to external search algorithms, another currently very
active research topic in Artificial Intelligence and theoretical computer science.

Acknowledgments
The author would like to thank the reviewers of this article whose comments have helped
in significantly improving it.

617

fiSchroedl

Appendix A

Table 2: Results for BAliBASE Reference 1, group of short sequences with low amino acid
identity. The columns denote: S  number of aligned sequences;   upper
bound for precomputing optimal solutions for partial problems in last iteration of
main search; g  (t)  optimal solution cost; h(s)  lower bound for solution cost,
using heuristics; #Exp  total number of expansions in all iterations of the main
search; #Op  peak number of edges in Open list over the course of the search;
#Op+Cl  peak combined number of edges in either Open or Closed list during
search; #Heu  peak number of subalignment edge costs stored as heuristic;
Time: total running time including auxiliary and main search, in seconds; Mem
 peak total memory usage for face alignments, heuristic, and main search, in
KB.
1aboA
1idy
1r69
1tvxA
1ubi
1wit
2trx

S
5
5
4
4
4
5
4


57
50
20
44
30
69
20

g  (t)
9006
8165
6215
5532
7395
14287
7918

h(s)
8898
8075
6183
5488
7357
14176
7899

#Exp
3413786
1732008
634844
1263849
1614286
6231378
63692

#Op
104613
74865
19938
24226
26315
209061
3502

#Op+Cl
176126
121404
41719
48633
54059
351582
5790

#Heu
1654547
970933
88802
476622
289599
2442098
127490

Time
331.029
167.867
22.517
52.860
62.133
578.907
4.572

Mem
15568
10893
3568
5278
5448
27273
1861

Table 3: Short sequences, medium similarity.
1aab
1fjlA
1hfh
1hpi
1csy
1pfc
1tgxA
1ycc
3cyr
451c

S
4
6
5
4
5
5
4
4
4
5


20
20
30
20
30
30
20
20
48
49

g  (t)
6002
13673
16556
5858
14077
15341
4891
8926
8480
11440

h(s)
5984
13625
16504
5835
14026
15277
4856
8903
8431
11333

#Exp
263
900
137914
1560
52718
118543
18987
54049
583260
1213162

#Op
12
106
4852
83
3872
6477
543
1118
13422
38004

618

#Op+Cl
83
155
8465
164
5613
8905
1080
2010
25806
54115

#Heu
4404
19573
70471
5269
56191
55887
5507
77156
193690
583363

Time
0.572
0.985
14.077
0.679
6.165
11.850
1.196
3.780
22.592
111.675

Mem
691
1589
2882
656
2252
2478
649
1644
3076
6529

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

Table 4: Short sequences, high similarity.
S
5
5
4
5
4
5
5
5
5
5

1aho
1csp
1dox
1fkj
1fmb
1krn
1plc
2fxb
2mhr
9rnt


20
20
20
20
20
20
20
20
20
20

g  (t)
8251
8434
7416
13554
7571
9752
12177
6950
14317
12382

h(s)
8187
8427
7405
13515
7568
9747
12152
6950
14306
12367

#Exp
30200
90
782
2621
172
101
454
88
256
350

#Op
2255
2
50
140
4
1
25
2
4
19

#Op+Cl
3074
78
186
222
108
87
103
71
121
108

#Heu
10971
3528
8406
10925
1804
6244
10641
1432
7853
6100

Time
3.175
0.569
0.652
0.945
0.540
0.623
0.728
0.534
0.668
0.695

Mem
1042
784
823
1511
788
1035
1415
617
1558
1250

Table 5: Medium-length sequences, low similarity.

1bbt3
1sbp
1havA
1uky
2hsdA
2pia
3grs
kinase

S
5
5
5
4
4
4
4
5


160
200
200
94
96
161
126
200

g  (t)
30598
42925
31600
18046
21707
22755
20222
45985

h(s)
30277
42512
31234
17915
21604
22616
20061
45520

#Exp
902725789
2144000052
2488806444
179802791
65580608
97669470
107682032
2446667393

#Op
11134608
6839269
10891271
659435
293357
789446
640391
13931051

#Op+Cl
15739188
11882990
16321376
1281339
668926
1673807
1396982
19688961

#Heu
23821767
65341855
58639851
15233338
12497761
25718770
24104710
32422084

Time
43860.175
106907.000
132576.000
7006.560
2646.880
4310.030
4267.880
125170.460

Mem
927735
735053
927735
106184
67788
142318
130425
927734

Table 6: Medium-length sequences, medium similarity.

1ad2
1aym3
1gdoA
1ldg
1mrj
1pgtA
1pii
1ton
2cba

S
4
4
4
4
4
4
4
5
5


20
20
58
20
20
50
20
102
160

g  (t)
16852
19007
20696
25764
20790
17442
20837
32564
40196

h(s)
16843
18978
20613
25736
20751
17398
20825
32428
39914

#Exp
379
466536
10795040
446123
252601
1870204
25256
13571887
60545205

#Op
16
4801
57110
4981
4067
19200
584
351174
1037828

619

#Op+Cl
221
8914
102615
9052
7380
32476
1414
526102
1595955

#Heu
27887
83634
1265777
169038
33942
485947
116670
11549908
19186631

Time
0.959
15.386
363.549
16.115
8.694
73.066
3.089
1373.180
2904.651

Mem
2186
3163
12028
4484
2905
5869
3338
58704
140712

fiSchroedl

Table 7: Medium-length sequences, high similarity.
S
5
4
5
4
5
4
4
5
4
5

1amk
1ar5A
1ezm
1led
1ppn
1pysA
1thm
1tis
1zin
5ptp


20
20
20
20
20
20
20
20
20
20

g  (t)
31473
15209
37396
18795
27203
19242
21470
35444
16562
29776

h(s)
31453
15186
37381
18760
27159
19215
21460
35395
16546
29735

#Exp
447
3985
613
93220
18517
10810
361
31996
771
6558

#Op
7
128
4
2956
489
190
2
448
23
309

#Op+Cl
259
356
324
4951
864
801
293
915
225
539

#Heu
13120
22220
15751
39962
20209
14344
8090
42716
6619
37883

Time
0.825
1.066
0.836
3.761
2.545
1.200
0.682
4.409
0.654
1.767

Mem
3366
1755
3900
2564
2991
2224
2469
4122
1767
3600

Table 8: Long sequences, low similarity.

1ajsA
1cpt
1lvl
1ped
2myr
4enl

S
4
4
4
3
4
3


160
160
160
50
200
50

g  (t)
38382
39745
43997
15351
43414
16146

h(s)
38173
39628
43775
15207
43084
16011

#Exp
318460012
873548
537914936
2566052
3740017645
5169296

#Op
1126697
5260
1335670
7986
7596730
9650

#Op+Cl
2310632
12954
2706940
27718
45488908
30991

#Heu
27102589
10494564
37491416
0
118747184
0

Time
9827.233
223.926
16473.420
20.035
136874.980
41.716

#Heu
18464119
96176
101816849
12801019
1476154
6040375
31318364
5962640
3585721
75819994
38368530
22622910

Time
6815.760
7.829
8795.000
843.402
334.475
348.134
2251.190
505.778
463.962
32965.522
15972.000
733.202

Mem
208951
32119
255123
4447
927735
5589

Table 9: Long sequences, medium similarity.

1ac5
1adj
1bgl
1dlc
1eft
1fieA
1gowA
1pkm
1sesA
2ack
arp
glg

S
4
4
4
4
4
4
4
4
5
5
5
5


92
20
243
106
56
86
166
89
58
250
143
160

g  (t)
37147
32815
78366
47430
31377
53321
38784
36356
57670
76937
54939
74282

h(s)
37020
32785
78215
47337
31301
53241
38632
36256
57557
76466
54696
74059

#Exp
169779871
207072
188429118
14993317
9379999
6905957
45590739
11197890
4755983
994225856
182635167
9251905

#Op
732333
3106
857008
65288
42620
46779
275256
75144
96014
8077412
1291185
87916

620

#Op+Cl
1513853
5145
1744149
126608
72502
90937
544800
140472
136677
12436928
2160263
120180

Mem
124877
4595
291618
43158
13115
26884
99537
27244
27452
765715
193364
72148

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

Table 10: Long sequences, high similarity.

1ad3
1gpb
1gtr
1lcf
1rthA
1taq
3pmg
actin

S
4
5
5
6
5
5
4
5


20
54
60
160
128
250
51
53

g  (t)
33641
101296
55242
149249
69296
133723
42193
48924

h(s)
33604
101231
55133
148854
69133
133321
42133
48826

#Exp
104627
1232707
2037633
181810148
14891538
1693501628
1036943
824295

621

#Op
2218
62184
54496
3235312
71081
9384718
8511
35283

#Op+Cl
3461
98476
91656
3824010
105082
17298456
15540
53009

#Heu
34539
2702949
1916127
28614215
24587882
145223167
777639
777058

Time
4.196
178.610
226.791
15363.051
1721.070
5713.240
50.796
96.147

Mem
3968
25698
18050
294688
70569
1170673
8133
11198

fiSchroedl

References
Altschul, S., Gish, W., Miller, W., Myers, E., & Lipman, D. (1990). Basic local alignment
search tool. Journal of Molecular Biology, 215, 403410.
Altschul, S. F. (1989). Gap costs for multiple sequence alignment. Journal of Theoretical
Biology, 138, 297309.
Carrillo, H., & Lipman, D. (1988). The multiple sequence alignment problem in biology.
SIAM Journal of Applied Mathematics, 5 (48), 10731082.
Chan, S. C., Wong, A. K. C., & Chiu, D. K. Y. (1992). A survey of multiple sequence
comparison techniques. Bulletin of Mathematical Biology, 54 (4), 563598.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14 (4), 318334.
Davidson, A. (2001). A fast pruning algorithm for optimal sequence alignment. In Proceedings of The 2nd IEEE International Symposium on Bioinformatics and Bioengineering
(BIBE2001), pp. 4956.
Dayhoff, M. O., Schwartz, R. M., & Orcutt, B. C. (1978). A model of evolutionary change
in proteins. In Dayhoff, M. O. (Ed.), Atlas of Protein Sequence and Structure, pp.
345352, Washington, D.C. National Biomedical Research Foundation.
Dial, R. B. (1969). Shortest-path forest with topological ordering. Comm. ACM, 12 (11),
632633.
Dijkstra, E. W. (1959). A note on two problems in connection with graphs.. Numerische
Mathematik, 1, 269271.
Gupta, S., Kececioglu, J., & Schaeffer, A. (1995). Improving the practical space and time
efficiency of the shortest-paths approach to sum-of-pairs multiple sequence alignment.
J. Computational Biology, 2 (3), 459472.
Gusfield, D. (1993). Efficient methods for multiple sequence alignment with guaranteed
error bounds. Bull. of Math. Biol., 55 (1), 141154.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for heuristic determination
of minimum path cost. IEEE Trans. on Systems Science and Cybernetics, 4, 100107.
Hirschberg, D. S. (1975). A linear space algorithm for computing maximal common subsequences. Comm. ACM, 6 (18), 341343.
Ikeda, T., & Imai, H. (1994). Fast A* algorithms for multiple sequence alignment. In
Proceedings of the Genome Informatics Workshop, pp. 9099.
Jones, D. T., Taylor, W. R., & Thornton, J. M. (1992). The rapid generation of mutation
data matrices from protein sequences. CABIOS, 3, 275282.
Kobayashi, H., & Imai, H. (1998). Improvement of the A* algorithm for multiple sequence
alignment. In Miyano, S., & Takagi, T. (Eds.), Genome Informatics, pp. 120130,
Tokyo. Universal Academy Press.
Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
622

fiAn Improved Search Algorithm for Optimal Multiple-Sequence Alignment

Korf, R. E. (1999). Divide-and-conquer bidirectional search: First results. In Proceedings
of the Sixteenth International Conference on Artificial Intelligence (IJCAI-99), pp.
11811189, Stockholm, Sweden.
Korf, R. E., & Zhang, W. (2000). Divide-and-conquer frontier search applied to optimal
sequence alignment. In Proceedings of the Eighteenth National Conference on Artificial
Intelligence (AAAI-00), pp. 210216.
McNaughton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory-efficient A* heuristics
for multiple sequence alignment. In Proceedings of the Eighteenth National Conference
on Artificial Intelligence (AAAI-02), Edmonton, Alberta, Canada.
Spouge, J. L. (1989). Speeding up dynamic programming algorithms for finding optimal
lattice paths. SIAM J. Applied Mathematics, 49 (5), 15521566.
Thompson, J. D., Plewniak, F., & Poch, O. (1999). A comprehensive comparison of multiple
sequence alignment programs. Nucleic Acids Res., 13 (27), 26822690.
Ukkonen, E. (1985). Algorithms for approximate string matching. Information and Control,
64, 110118.
Wah, B. W., & Shang, Y. (1995). A comparison of a class of IDA* search algorithms.
International Journal of Tools with Artificial Intelligence, 3 (4), 493523.
Wang, L., & Jiang, T. (1994). On the complexity of multiple sequence alignment. Journal
of Computational Biology, 1, 337348.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* with partial expansion for large branching
factor problems. In AAAI/IAAI, pp. 923929.
Zhou, R., & Hansen, E. A. (2003a). Sparse-memory graph search. In 18th International
Joint Conference on Artificial Intelligence (IJCAI-03), Acapulco, Mexico.
Zhou, R., & Hansen, E. A. (2003b). Sweep A*: Space-efficient heuristic search in partiallyordered graphs. In 15th IEEE International Conference on Tools with Artificial Intelligence, Sacramento, CA.
Zhou, R., & Hansen, E. A. (2004). Breadth-first heuristic search. In Fourteenth International Conference on Automated Planning and Scheduling (ICAPS-04), Whistler, BC,
Canada.

623

fiJournal of Artificial Intelligence Research 23 (2005) 1-40

Submitted 05/04; published 01/05

Finding Approximate POMDP Solutions Through Belief
Compression
Nicholas Roy

nickroy@mit.edu

Massachusetts Institute of Technology,
Computer Science and Artificial Intelligence Laboratory
Cambridge, MA

Geoffrey Gordon

ggordon@cs.cmu.edu

Carnegie Mellon University, School of Computer Science
Pittsburgh, PA

Sebastian Thrun

thrun@stanford.edu

Stanford University, Computer Science Department
Stanford, CA

Abstract
Standard value function approaches to finding policies for Partially Observable Markov
Decision Processes (POMDPs) are generally considered to be intractable for large models.
The intractability of these algorithms is to a large extent a consequence of computing
an exact, optimal policy over the entire belief space. However, in real-world POMDP
problems, computing the optimal policy for the full belief space is often unnecessary for
good control even for problems with complicated policy classes. The beliefs experienced
by the controller often lie near a structured, low-dimensional subspace embedded in the
high-dimensional belief space. Finding a good approximation to the optimal value function
for only this subspace can be much easier than computing the full value function.
We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the belief space. We use Exponential family Principal Components Analysis (Collins, Dasgupta, & Schapire, 2002) to represent sparse, high-dimensional belief spaces
using small sets of learned features of the belief state. We then plan only in terms of the
low-dimensional belief features. By planning in this low-dimensional space, we can find
policies for POMDP models that are orders of magnitude larger than models that can be
handled by conventional techniques.
We demonstrate the use of this algorithm on a synthetic problem and on mobile robot
navigation tasks.

1. Introduction
Decision making is one of the central problems of artificial intelligence and robotics. Most
robots are deployed into the world to accomplish specific tasks, but the real world is a
difficult place in which to actactions can have serious consequences. Figure 1(a) depicts
a mobile robot, Pearl, designed to operate in the environment shown in Figure 1(b), the
Longwood retirement facility in Pittsburgh. Real world environments such as Longwood
are characterized by uncertainty; sensors such as cameras and range finders are noisy and
the entire world is not always observable. A large number of state estimation techniques
explicitly recognize the impossibility of correctly identifying the true state of the world
(Gutmann, Burgard, Fox, & Konolige, 1998; Olson, 2000; Gutmann & Fox, 2002; Kanazawa,
c
2005
AI Access Foundation. All rights reserved.

fiRoy, Gordon, & Thrun

Koller, & Russell, 1995; Isard & Blake, 1998) by using probabilistic techniques to track the
location of the robot. Such state estimators as the Kalman filter (Leonard & DurrantWhyte, 1991) or Markov localization (Fox, Burgard, & Thrun, 1999; Thrun, Fox, Burgard,
& Dellaert, 2000) provide a (possibly factored, Boyen & Koller, 1998) distribution over
possible states of the world instead of a single (possibly incorrect) state estimate.

(a)

Figure 1:

(b)

A planner for the mobile robot Pearl, shown in (a), must be able to navigate
reliably in such real environments as the Longwood at Oakmont retirement facility,
shown in (b). The white areas of the map are free space, the black pixels are
obstacles, and the grey areas again are regions of map uncertainty. Notice the
large open spaces, and many symmetries that can lead to ambiguity in the robots
position. The map is 53.6m  37.9m, with a resolution of 0.1m  0.1m per pixel.

In contrast, controllers such as motion planners, dialogue systems, etc. rarely model the
same notions of uncertainty. If the state estimate is a full probability distribution, then the
controller often uses a heuristic to extract a single best state, such as the distributions
mean or mode. Some planners compensate for the inevitable estimation errors through robust control (Chen, 2000; Bagnell & Schneider, 2001), but few deployed systems incorporate
a full probabilistic state estimate into planning. Although the most-likely-state method is
simple and has been used successfully by some real applications (Nourbakhsh, Powers, &
Birchfield, 1995), substantial control errors can result when the distribution over possible
states is very uncertain. If the single state estimate is wrong, the planner is likely to choose
an unreasonable action.
Figure 2 illustrates the difference between conventional controllers and those that model
uncertainty. In this figure, the robot must navigate from the bottom right corner to the top
left, but has limited range sensing (up to 2m) and noisy dead reckoning.1 The impoverished
1. For the purposes of this example the sensing and dead reckoning were artificially poor, but the same
phenomenon would occur naturally in larger-scale environments.

2

fiFinding Approximate POMDP Solutions Through Belief Compression

sensor data can cause the robots state estimate to become quite uncertain if it strays too
far from environmental structures that it can use to localize itself. On the left (Figure 2a)
is an example trajectory from a motion planner that has no knowledge of the uncertainty in
the state estimate and no mechanism for taking this uncertainty into account. The robots
trajectory diverges from the desired path, and the robot incorrectly believes it has arrived
at the goal. Not shown here are the state estimates that reflect the high uncertainty in the
robot position. On the right (Figure 2b) is an example trajectory from a controller that can
model the positional uncertainty, take action to keep the uncertainty small by following the
walls, and arrive reliably at the goal.

Goal

Goal

Measured Path

Measured Path
True Path
True Path

Start

Start

(a) Conventional controller

Figure 2:

(b) Robust controller

Two possible trajectories for navigation in the Longwood at Oakmont environment. The robot has limited range sensing (up to 2m) and poor dead-reckoning
from odometry. (a) The trajectory from a conventional motion planner that uses
a single state estimate, and minimizes travel distance. (b) The trajectory from
a more robust controller that models the state uncertainty to minimize travel
distance and uncertainty.

The controller in Figure 2(b) was derived from a representation called the partially observable Markov decision process (POMDP). POMDPs are a technique for making decisions
based on probabilistic estimates of the state of the world, rather than on absolute knowledge
of the true state. A POMDP uses an a priori model of the world together with the history
of actions taken and observations received in order to infer a probability distribution, or
belief, over the possible states of the world. The controller chooses actions, based upon
the current belief, to maximize the reward it expects to receive over time.
The advantage to using POMDPs for decision making is that the resulting policies
handle uncertainty well. The POMDP planning process can take advantage of actions that
implicitly reduce uncertainty, even if the problem specification (e.g., the reward function)
does not explicitly reward such actions. The disadvantage to POMDPs is that finding the
optimal policy is computationally intractable. Existing techniques for finding exact optimal
3

fiRoy, Gordon, & Thrun

plans for POMDPs typically cannot handle problems with more than a few hundred states
(Hauskrecht, 2000; Zhang & Zhang, 2001). Most planning problems involving real, physical
systems cannot be expressed so compactly; we would like to deploy robots that plan over
thousands of possible states of the world (e.g., map grid cells), with thousands of possible
observations (e.g., laser range measurements) and actions (e.g., velocities).
In this paper, we will describe an algorithm for finding approximate solutions to realworld POMDPs. The algorithm arises from the insight that exact POMDP policies use
unnecessarily complex, high-dimensional representations of the beliefs that the controller
can expect to experience. By finding low-dimensional representations, the planning process
becomes much more tractable.
We will first describe how to find low-dimensional representations of beliefs for realworld POMDPs; we will use a variant of a common dimensionality-reduction technique
called Principal Components Analysis. The particular variant we use modifies the loss
function of PCA in order to better model the data as probability distributions. Using these
low-dimensional representations, we will describe how to plan in the low-dimensional space,
and conclude with experimental results on robot control tasks.

2. Partially Observable Markov Decision Processes
A partially observable Markov decision process (POMDP) is a model for deciding how to
act in an accessible, stochastic environment with a known transition model (Russell and
Norvig (1995), pg. 500). A POMDP is described by the following:









a set of states S = {s1 , s2 , . . . s|S| }
a set of actions A = {a1 , a2 , . . . , a|A| }
a set of observations Z = {z1 , z2 , . . . , z|Z| }
a set of transition probabilities T (si , a, sj ) = p(sj |si , a)
a set of observation probabilities O(zi , a, sj ) = p(zi |sj , a)
a set of rewards R : S  A 7 R
a discount factor   [0, 1]
an initial belief p0 (s)

The transition probabilities describe how the state evolves with actions, and also represent the Markov assumption: the next state depends only on the current (unobservable)
state and action and is independent of the preceding (unobserved) states and actions. The
reward function describes the objective of the control, and the discount factor is used to
ensure reasonable behaviour in the face of unlimited time. An optimal policy is known
to always exist in the discounted ( < 1) case with bounded immediate reward (Howard,
1960).
POMDP policies are often computed using a value function over the belief space. The
value function V (b) for a given policy  is defined as the long-term expected reward the
controller will receive starting at belief b and executing the policy  up to some horizon time,
which may be infinite. The optimal POMDP policy maximizes this value function. The
value function for a POMDP policy under a finite horizon can be described using a piecewise linear function over the space of beliefs. Many algorithms compute the value function
iteratively, evaluating and refining the current value function estimate until no further
4

fiFinding Approximate POMDP Solutions Through Belief Compression

refinements can improve the expected reward of the policy from any belief. Figure 3(a)
shows the belief space for a three-state problem. The belief space is the two-dimensional,
shaded simplex. Each point on the simplex corresponds to a particular belief (a threedimensional vector), and the corners of the simplex represent beliefs where the state is
known with 100% certainty. The value function shown in Figure 3(b) gives the long-term
expected reward of a policy, starting at any belief in the simplex.

1

10
9

0.8

8
7

0.6

6
5

0.4

4
3

0.2

2
0
0

1
0
0.2

0.2
0.4
0.6
0.8
1

0

0.2

0.4

0.6

0.8

0.4

1

0.6
0.8
1

(a) The belief space
Figure 3:

0

0.2

0.4

0.6

0.8

1

(b) The value function

(a) The belief space for a three-state problem is the two-dimensional, shaded
simplex. (b) A value function defined over the belief space. For the purposes
of visualization, the set of beliefs that constitutes the belief space shown in (a)
has been projected onto the XY plane in (b); the value function then rises along
the positive Z axis. Each point in the belief space corresponds to a specific
distribution, and the value function at that point gives the expected reward of
the policy starting from this belief. The belief space (and therefore the value
function) will have one fewer dimension than the total number of states in the
problem.

The process of evaluating and refining the value function is at the core of why solving
POMDPs is considered to be intractable. The value function is defined over the space
of beliefs, which is continuous and high-dimensional; the belief space will have one fewer
dimension than the number of states in the model. For a navigation problem in a map of
thousands of possible states, computing the value function is an optimization problem over
a continuous space with many thousands of dimensions, which is not feasible with existing
algorithms.
However, careful consideration of some real-world problems suggests a possible approach
for finding approximate value functions. If we examine the beliefs that a navigating mobile
robot encounters, these beliefs share common attributes. The beliefs typically have a very
small number of modes, and the particular shape of the modes is fairly generic. The modes
move about and change in variance, but the ways in which the modes change is relatively
constrained. In fact, even for real world navigation problems with very large belief spaces,
the beliefs have very few degrees of freedom.
Figure 4(a) illustrates this idea: it shows a typical belief that a mobile robot might
experience while navigating in the nursing home environment of Figure 1(b). To visualize
the distribution we sample a set of poses (also called particles) according to the distribution
5

fiRoy, Gordon, & Thrun

and plot the particles on the map. The distribution is unimodal and the probability mass
is mostly concentrated in a small area. Figure 4(b) shows a very different kind of belief:
probability mass is spread over a wide area, there are multiple modes, and the locations
of particles bear little relationship to the map. It would be difficult to find a sequence of
actions and observations that would result in such a belief.

Most particles
are here

(a) A common belief

Figure 4:

(b) An unlikely belief

Two example probability distributions over robot pose. The small black dots are
particles drawn from the distribution over discrete grid positions. On the left is a
distribution where the robots location is relatively certain; this kind of compact,
unimodal distribution is very common in robot navigation. On the right is a
very different, implausible distribution. The right hand distribution is sufficiently
unlikely that we can afford to ignore it; even if we are unable to distinguish this
belief from some other belief and as a result fail to identify its optimal action, the
quality of our controller will be unaffected.

If real-world beliefs have few degrees of freedom, then they should be concentrated near a
low-dimensional subset of the high-dimensional belief spacethat is, the beliefs experienced
by the controller should lie near a structured, low-dimensional surface embedded in the belief
space. If we can find this surface, we will have a representation of the belief state in terms
of a small set of bases or features. One benefit of such a representation is that we will need
to plan only in terms of the small set of features: finding value functions in low-dimensional
spaces is typically easier than finding value functions in high-dimensional spaces.
There are two potential disadvantages to this sort of representation. The first is that it
contains an approximation: we are no longer finding the complete, optimal POMDP policy.
Instead (as suggested in Figure 5) we are trying to find representations of the belief which
are rich enough to allow good control but which are also sufficiently parsimonious to make
6

fiFinding Approximate POMDP Solutions Through Belief Compression

the planning problem tractable. The second disadvantage is a technical one: because we are
making a nonlinear transformation of the belief space, POMDP planning algorithms which
assume a convex value function will no longer work. We discuss this problem in more detail
in Section 6.
Conventional
Path Planner

POMDP

Tractable
Not Robust

Figure 5:

Intractable
Robust

The most useful planner lies somewhere on a continuum between the MDP-style
approximations and the full POMDP solution.

3. Dimensionality Reduction
In order to find a low-dimensional representation of our beliefs, we will use statistical dimensionality reduction algorithms (Cox & Cox, 1994). These algorithms search for a projection
from our original high-dimensional representation of our beliefs to a lower-dimensional compact representation. That is, they search for a low-dimensional surface, embedded in the
high-dimensional belief space, which passes near all of the sample beliefs. If we consider
the evolution of beliefs from a POMDP as a trajectory inside the belief space, then our assumption is that trajectories for most large, real world POMDPs lie near a low-dimensional
surface embedded in the belief space. Figure 6 depicts an example low-dimensional surface
embedded in the belief space of the three-state POMDP described in the previous section.
1

0.8

0.6

0.4

0.2

0
0
0.2
0.4
0.6
0.8
1

Figure 6:

0

0.2

0.4

0.6

0.8

1

A one-dimensional surface (black line) embedded in a two-dimensional belief space
(gray triangle). Each black dot represents a single belief probability distribution
experienced by the controller. The beliefs all lie near the low-dimensional surface.

Ideally, dimensionality reduction involves no information lossall aspects of the data
can be recovered equally well from the low-dimensional representation as from the highdimensional one. In practice, though, we will see that we can use lossy representations
of the belief (that is, representations that may not allow the original data or beliefs to
be recovered without error) and still get good control. But, we will also see that finding
such representations of probability distributions will require a careful trade-off between
7

fiRoy, Gordon, & Thrun

preserving important aspects of the distributions and using as few dimensions as possible.
We can measure the quality of our representation by penalizing reconstruction errors with
a loss function (Collins et al., 2002). The loss function provides a quantitative way to
measure errors in representing the data, and different loss functions will result in different
low-dimensional representations.
Principal Components Analysis
One of the most common forms of dimensionality reduction is Principal Components Analysis (Joliffe, 1986). Given a set of data, PCA finds the linear lower-dimensional representation of the data such that the variance of the reconstructed data is preserved. Intuitively,
PCA finds a low-dimensional hyperplane such that, when we project our data onto the
hyperplane, the variance of our data is changed as little as possible. A transformation that
preserves variance seems appealing because it will maximally preserve our ability to distinguish between beliefs that are far apart in Euclidean norm. As we will see below, however,
Euclidean norm is not the most appropriate way to measure distance between beliefs when
our goal is to preserve the ability to choose good actions.
We first assume we have a data set of n beliefs {b1 , . . . , bn }  B, where each belief bi
is in B, the high-dimensional belief space. We write these beliefs as column vectors in a
matrix B = [b1 | . . . |bn ], where B  R|S|n . We use PCA to compute a low-dimensional
representation of the beliefs by factoring B into the matrices U and B,
B = U B T .

(1)

In equation (1), U  R|S|l corresponds to a matrix of bases that span the low-dimensional
space of l < |S| dimensions. B  Rnl represents the data in the low-dimensional space.2
From a geometric perspective, U comprises a set of bases that span a hyperplane B in the
high-dimensional space of B; B are the co-ordinates of the data on that hyperplane. If no
hyperplane of dimensionality l exists that contains the data exactly, PCA will find the surface of the given dimensionality that best preserves the variance of the data, after projecting
the data onto that hyperplane and then reconstructing it. Minimizing the change in variance between the original data B and its reconstruction U B T is equivalent to minimizing
the sum of squared error loss:
L(B, U, B) = kB  U B T k2F .

(2)

PCA Performance
Figure 7 shows a toy problem that we can use to evaluate the success of PCA at finding
low-dimensional representations. The abstract model has a two-dimensional state space:
one dimension of position along one of two circular corridors, and one binary variable that
determines which corridor we are in. States s1 . . . s100 inclusive correspond to one corridor,
and states s101 . . . s200 correspond to the other. The reward is at a known position that is
different in each corridor; therefore, the agent needs to discover its corridor, move to the
2. Many descriptions of PCA are based on a factorization U SV T , with U and V column-orthonormal and
S diagonal. We could enforce a similar constraint by identifying B = V S; in this case the columns of U
would have to be orthonormal while those of B would have to be orthogonal.

8

fiFinding Approximate POMDP Solutions Through Belief Compression

appropriate position, and declare it has arrived at the goal. When the goal is declared
the system resets (regardless of whether the agent is actually at the goal). The agent
has 4 actions: left, right, sense_corridor, and declare_goal. The observation and
transition probabilities are given by discretized von Mises distributions (Mardia & Jupp,
2000; Shatkay & Kaelbling, 2002), an exponential family distribution defined over [ : ).
The von Mises distribution is a wrapped analog of a Gaussian; it accounts for the fact
that the two ends of the corridor are connected. Because the sum of two von Mises variates
is another von Mises variate, and because the product of two von Mises likelihoods is a
scaled von Mises likelihood, we can guarantee that the true belief distribution is always a
von Mises distribution over each corridor after each action and observation.
This instance of the problem consists of 200 states, with 4 actions and 102 observations.
Actions 1 and 2 move the controller left and right (with some von Mises noise) and action
3 returns an observation that uniquely and correctly identifies which half of the maze the
agent is in (the top half or the bottom half). Observations returned after actions 1 and 2
identify the current state modulo 100: the probability of each observation is a von Mises
distribution with mean equal to the true state (modulo 100). That is, these observations
indicate approximately where the agent is horizontally.
Max Prob.
Obs. = #1
1

Max Prob.
Obs. = #3
2

Max Prob.
Obs. = #5

3

4

103

104

5

6

105

106

7

Reward
101

102

107

...
...

Max Prob.
Obs. = #100
100

Observation "top" after
action #3 has prob. 1

Reward

Observation "bottom" after
action #3 has prob. 1

200

Figure 7: The toy maze of 200 states.

This maze is interesting because it is relatively large by POMDP standards (200 states)
and contains a particular kind of uncertaintythe agent must use action 3 at some point to
uniquely identify which half of the maze it is in; the remaining actions result in observations
that contain no information about which corridor the agent is in. This problem is too large
to be solved by conventional POMDP value iteration, but structured such that heuristic
policies will also perform poorly.
We collected a data set of 500 beliefs and assessed the performance of PCA on beliefs
from this problem. The data were collected using a hand-coded controller, alternating at
random between exploration actions and the MDP solution, taking as the current state the
maximum-likelihood state of the belief. Figure 8 shows 4 sample beliefs from this data
set. Notice that each of these beliefs is essentially two discretized von Mises distributions
with different weights, one for each half of the maze. The starting belief state is the
left-most distribution in Figure 8: equal probability on the top and bottom corridors, and
position along the corridor following a discretized von Mises distribution with concentration
parameter 1.0 (meaning that p(state) falls to 1/e of its maximum value when we move 1/4
of the way around the corridor from the most likely state).
9

fiRoy, Gordon, & Thrun

A Sample Belief

A Sample Belief

0.04

0.04

0.035

0.035

0.03

0.03

0.025

0.025

0.02
0.015
0.01

0.02
0.015
0.01

0.005

0.005

0

0

-0.005

0

20

40

60

-0.005

80 100 120 140 160 180 200
State

Probability

0.03
0.025

Probability

Probability

A Sample Belief
0.04
0.035

0.02
0.015
0.01
0.005
0

0

20

40

60

-0.005

80 100 120 140 160 180 200
State

0

20

40

60

80 100 120 140 160 180 200
State

A Sample Belief
0.14
0.12

Probability

0.1
0.08
0.06
0.04
0.02
0

Figure 8:

0

20

40

60

80

100 120 140 160 180 200
State

Sample beliefs for the toy problem, from a sample set of 500, at different (noncontiguous) points in time. The left-most belief is the initial belief state.

Figure 9 examines the performance of PCA on representing the beliefs in this data set by
computing the average error between the original beliefs B and their reconstructions U B.3
In Figure 9(a) we see the average squared error (squared L2 ) compared to the number
of bases, and in Figure 9(b), we see the average Kullbach-Leibler (KL) divergence. The
KL divergence between a belief b and its reconstruction r = U b from the low-dimensional
representation b is given by
KL(b k r) =

|S|
X

b(si ) ln

i=1



b(si )
r(si )



(3)

Minimizing the squared L2 error is the explicit objective of PCA, but the KL divergence is
a more appropriate measure of how much two probability distributions differ. 4
Unfortunately, PCA performs poorly at representing probability distributions. Despite
the fact that probability distributions in the collected data set have only 3 degrees of
freedom, the reconstruction error remains relatively high until somewhere between 10 and
15 basis functions. If we examine the reconstruction of a sample belief, we can see the
kinds of errors that PCA is making. Figure 10 shows a sample belief (the solid line) and
its reconstruction (the dotted line). Notice that the reconstructed belief has some strange
artifacts: it contains ringing (multiple small modes), and also is negative in some regions.
PCA is a purely geometric process; it has no notion of the original data as probability
distributions, and is therefore free to generate reconstructions of the data that contain
negative numbers or do not sum to 1.
3. We use a popular implementation of PCA based on the Golub-Reinsche algorithm (Golub & Reinsch,
1970) available through the GNU Scientific Library (Galassi, Davies, Theiler, Gough, Jungman, Booth,
& Rossi, 2002).
4. Note that before computing the KL divergence between the reconstruction and the original belief, we
shift the reconstruction to be non-negative, and rescale it to sum to 1.

10

fiFinding Approximate POMDP Solutions Through Belief Compression

Average L2 Error vs. Number of Bases

Average KL Divergence vs. Number of Bases

0.02

1.6
Average KL Divergence

1.4
Average L2 Error

0.015
0.01
0.005
0

1.2
1
0.8
0.6
0.4
0.2
0

0

5

10
15
20
Number of Bases

25

30

0

(a) Average Squared L-2 Error
Figure 9:

5

10

15
20
Number of Bases

25

30

(b) Average KL Divergence

The average error between the original sample set B and the reconstructions U B.
(a) Squared L2 error, explicitly minimized by PCA, and (b) the KL divergence.
The error bars represent the standard deviation from the mean of the error over
the 500 beliefs.
An Example Belief and Reconstruction
0.045

Original Belief
Reconstructed Belief

0.04

Probability

0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
-0.005

0

20

40

60

80

100 120 140 160 180 200
State

Figure 10: An example belief and its reconstruction, using 10 bases.

Notice also that the PCA process is making its most significant errors in the lowprobability regions of the belief. This is particularly unfortunate because real-world probability distributions tend to be characterized by compact masses of probability, surrounded
by large regions of zero probability (e.g., Figure 4a). What we therefore need to do is modify
PCA to ensure the reconstructions are probability distributions, and improve the representation of sparse probability distributions by reducing the errors made on low-probability
events.
The question to be answered is what loss functions are available instead of the sum of
squared errors, as in equation (2). We would like a loss function that better reflects the
need to represent probability distributions.
11

fiRoy, Gordon, & Thrun

4. Exponential Family PCA
The conventional view of PCA is a geometric one, finding a low-dimensional projection
that minimizes the squared-error loss. An alternate view is a probabilistic one: if the
data consist of samples drawn from a probability distribution, then PCA is an algorithm
for finding the parameters of the generative distribution that maximize the likelihood of
the data. The squared-error loss function corresponds to an assumption that the data is
generated from a Gaussian distribution. Collins et al. (2002) demonstrated that PCA can
be generalized to a range of loss functions by modeling the data with different exponential
families of probability distributions such as Gaussian, binomial, or Poisson. Each such
exponential family distribution corresponds to a different loss function for a variant of
PCA, and Collins et al. (2002) refer to the generalization of PCA to arbitrary exponential
family data-likelihood models as Exponential family PCA or E-PCA.
An E-PCA model represents the reconstructed data using a low-dimensional weight
vector b, a basis matrix U , and a link function f :
b  f (U b)

(4)

Each E-PCA model uses a different link function, which can be derived from its data
likelihood model (and its corresponding error distribution and loss function). The link
function is a mapping from the data space to another space in which the data can be
linearly represented.
The link function f is the mechanism through which E-PCA generalizes dimensionality reduction to non-linear models. For example, the identity link function corresponds
to Gaussian errors and reduces E-PCA to regular PCA, while the sigmoid link function
corresponds to Bernoulli errors and produces a kind of logistic PCA for 0-1 valued data.
Other nonlinear link functions correspond to other non-Gaussian exponential families of
distributions.
We can find the parameters of an E-PCA model by maximizing the log-likelihood of
the data under the model, which has been shown (Collins et al., 2002) to be equivalent to
minimizing a generalized Bregman divergence
BF  (b k U b) = F (U b)  b  U b + F  (b)

(5)

between the low-dimensional and high-dimensional representations, which we can solve using
convex optimization techniques. (Here F is a convex function whose derivative is f , while
F  is the convex dual of F . We can ignore F  for the purpose of minimizing equation 5
since the value of b is fixed.) The relationship between PCA and E-PCA through link
functions is reminiscent of the relationship between linear regression and Generalized Linear
Models (McCullagh & Nelder, 1983).
To apply E-PCA to belief compression, we need to choose a link function which accurately reflects the fact that our beliefs are probability distributions. If we choose the link
function
f (U b) = eU b
(6)
P U b
P
then it is not hard to verify that F (U b) =
e and F  (b) = b  ln b  b. So, equation 5
becomes
X
X
BF  (b k U b) =
eU b  b  U b + b  ln b 
b
(7)
12

fiFinding Approximate POMDP Solutions Through Belief Compression

If we write b = f (U b), then equation 7 becomes
BF  (b k U b) = b  ln b  b  ln b +

X

b 

X

b = U KL(b k b)

where U KL is the unnormalized KL divergence. Thus, choosing the exponential link function (6) corresponds to minimizing the unnormalized KL divergence between the original
belief and its reconstruction. This loss function is an intuitively reasonable choice for measuring the error in reconstructing a probability distribution.5 The exponential link function
corresponds to a Poisson error model for each component of the reconstructed belief.
Our choice of loss and link functions has two advantages: first, the exponential link
function constrains our low-dimensional representation eU b to be positive. Second, our error
model predicts that the variance of each belief component is proportional to its expected
value. Since PCA makes significant errors close to 0, we wish to increase the penalty for
errors in small probabilities, and this error model accomplishes that.
If we compute the loss for all bi , ignoring terms that depend only on the data b, then6

L(B, U, B) =

|B| 
X
i=1


eU bi  bi  U bi .

(8)

The introduction of the link function raises a question: instead of using the complex
machinery of E-PCA, could we just choose some non-linear function to project the data
into a space where it is linear, and then use conventional PCA? The difficulty with this
approach is of course identifying that function; in general, good link functions for E-PCA
are not related to good nonlinear functions for application before regular PCA. So, while
it might appear reasonable to use PCA to find a low-dimensional representation of the log
beliefs, rather than use E-PCA with an exponential link function to find a representation
of the beliefs directly, this approach performs poorly because the surface is only locally
well-approximated by a log projection. E-PCA can be viewed as minimizing a weighted
least-squares that chooses the distance metric to be appropriately local. Using conventional
PCA over log beliefs also performs poorly in situations where the beliefs contain extremely
small or zero probability entries.
P
5. If we had chosen the link function eU b / eU b we would have arrived at the normalized KL divergence,
which is perhaps an even more intuitively reasonable way to measure the error in reconstructing a
probability distribution. This more-complicated link function would have made it more difficult to
derive the Newton equations in the following pages, but not impossible; we have experimented with the
resulting algorithm and found that it produces qualitatively similar results to the algorithm described
here. Using the normalized KL divergence does have one advantage: it can allow us to get away with
one fewer basis function during planning, since for unnormalized KL divergence the E-PCA optimization
must learn a basis which can explicitly represent the normalization constant.
6. E-PCA is related to Lee and Seungs (1999) non-negative matrix factorization. One of the NMF loss
functions presented by Lee and Seung (1999) penalizes the KL-divergence between a matrix and its
reconstruction, as we do in equation 8; but, the NMF loss does not incorporate a link function and so is
not an E-PCA loss. Another NMF loss function presented by Lee and Seung (1999) penalizes squared
error but constrains the factors to be nonnegative; the resulting model is an example of a (GL) 2 M, a
generalization of E-PCA described by Gordon (2003).

13

fiRoy, Gordon, & Thrun

Finding the E-PCA Parameters
Algorithms for conventional PCA are guaranteed to converge to a unique answer independent of initialization. In general, E-PCA does not have this property: the loss function (8)
may have multiple distinct local minima. However, the problem of finding the best B given
B and U is convex; convex optimization problems are well studied and have unique global
solutions (Rockafellar, 1970). Similarly, the problem of finding the best U given B and B is
convex. So, the possible local minima in the joint space of U and B are highly constrained,
and finding U and B does not require solving a general non-convex optimization problem.
Gordon (2003) describes a fast, Newtons Method approach for computing U and B
which we summarize here. This algorithm is related to Iteratively Reweighted Least Squares,
a popular algorithm for generalized linear regression (McCullagh & Nelder, 1983). In order
to use Newtons Method to minimize equation (8), we need its derivative with respect to U
and B:

 (U B)

L(B, U, B) =
e

B  U B
(9)
U
U
U
= e(U B) B T  B B T
(10)
= (e(U B)  B)B T

(11)

and
 (U B)


L(B, U, B) =
e

B  U B
 B
 B
 B
= U T e(U B)  U T B
T

= U (e

(U B)

 B).

(12)
(13)
(14)

If we set the right hand side of equation (14) to zero, we can iteratively compute Bj , the
column of B, by Newtons method. Let us set q(Bj ) = U T (e(U Bj )  Bj ), and linearize
about Bj to find roots of q(). This gives
j th

Bjnew = Bj 
Bjnew  Bj

= 

q(Bj )
q 0 (Bj )

U T (e(U Bj )  Bj )
q 0 (Bj )

(15)
(16)

Note that equation 15 is a formulation of Newtons method for finding roots of q, typically
written as
f (xn )
xn+1 = xn  0
.
(17)
f (xn )
We need an expression for q 0 :
q
 Bj

=
=


U T (e(U Bj )  Bj )
 Bj

U T e(U Bj )
 Bj

= U T Dj U
14

(18)
(19)
(20)

fiFinding Approximate POMDP Solutions Through Belief Compression

We define Dj in terms of the diag operator that returns a diagonal matrix:

where

Dj = diag(eU Bj ),

(21)


b(s0 ) . . .
0

..  .
..
diag(b) =  ...
.
. 
0
. . . b(s|S| )

(22)



Combining equation (15) and equation (20), we get
(U T Dj U )(Bjnew  Bj ) = U T (Bj  eU Bj )

(23)

U T Dj U Bjnew = (U T Dj U )Bj + U T Dj Dj1 (Bj  eU Bj )
= U T Dj (U Bj + Dj1 (Bj  eU Bj ),

(24)
(25)

which is a weighted least-squares problem that can be solved with standard linear algebra
techniques. In order to ensure that the solution is numerically well-conditioned, we typically
add a regularizer to the divisor, as in
Bjnew =

U T Dj (U Bj + Dj1 (Bj  eU Bj )
(U T Dj U + 105 Il )

.

(26)

where Il is the l  l identity matrix. Similarly, we can compute a new U by computing Ui ,
the ith row of U , as
(Ui B + (Bi  eUi B )Di1 )Di B T
.
(27)
Uinew =
(BDi B T + 105 Il )
The E-PCA Algorithm
We now have an algorithm for automatically finding a good low-dimensional representation
B for the high-dimensional belief set B. This algorithm is given in Table 1; the optimization
is iterated until some termination condition is reached, such as a finite number of iterations,
or when some minimum error  is achieved.
The steps 7 and 9 raise one issue. Although solving for each row of U or column of B
separately is a convex optimization problem, solving for the two matrices simultaneously
is not. We are therefore subject to potential local minima; in our experiments we did not
find this to be a problem, but we expect that we will need to find ways to address the local
minimum problem in order to scale to even more complicated domains.
Once the bases U are found, finding the low-dimensional representation of a highdimensional belief is a convex problem; we can compute the best answer by iterating equation (26). Recovering a full-dimensional belief b from the low-dimensional representation b
is also very straightforward:
x = eU b .
(28)
Our definition of PCA does not explicitly factor the data into U , S and B as many
presentations do. In this three-part representation of PCA, S contains the singular values
15

fiRoy, Gordon, & Thrun

1. Collect a set of sample beliefs from the high-dimensional belief space
2. Assemble the samples into the data matrix B = [b1 | . . . |b|B| ]
3. Choose an appropriate loss function, L(B, U, B)
4. Fix an initial estimate for B and U randomly
5. do
6.

For each column Bj  B,
Compute Bjnew using current U estimate from equation (26)

7.
8.

For each row Ui  U ,

9.

Compute Uinew using new B estimate from equation (27)

10. while L(B, U, B) > 

Table 1:

The E-PCA Algorithm for finding a low-dimensional representation of a POMDP,
including Gordons Newtons method (2003).

of the decomposition, and U and B are orthonormal. We use the two-part representation
B  f (U B) because there is no quantity in the E-PCA decomposition which corresponds
to the singular values in PCA. As a result, U and B will not in general be orthonormal. If
desired, though, it is possible to orthonormalize U as an additional step after optimization
using conventional PCA and adjust B accordingly.

5. E-PCA Performance
Using the loss function from equation (8) with the iterative optimization procedure described
by equation (26) and equation (27) to find the low-dimensional factorization, we can look
at how well this dimensionality-reduction procedure performs on some POMDP examples.
Toy Problem
Recall from Figure 9 that we were unable to find good representations of the data with
fewer than 10 or 15 bases, even though our domain knowledge indicated that the data had
3 degrees of freedom (horizontal position of the mode along the corridor, concentration
about the mode, and probability of being in the top or bottom corridor). Examining one
of the sample beliefs in Figure 10, we saw that the representation was worst in the lowprobability regions. We can now take the same data set from the toy example, use E-PCA
to find a low-dimensional representation and compare the performance of PCA and E-PCA.
Figure 11(a) shows that E-PCA is substantially more efficient at representing the data, as
we see the KL divergence falling very close to 0 after 4 bases. Additionally, the squared L 2
error at 4 bases is 4.64  104 . (We need 4 bases for perfect reconstruction, rather than
3, since we must include a constant basis function. The small amount of reconstruction
16

fiFinding Approximate POMDP Solutions Through Belief Compression

error with 4 bases remains because we stopped the optimization procedure before it fully
converged.)

Average KL Divergence vs. Number of Bases (E-PCA)

An Example Belief and Reconstruction Using 3 Bases

1.6

0.045
0.04
0.035
0.03

1.2
1

Probability

Average KL Divergence

1.4

0.8
0.6
0.4
0
0

5

10

15
20
Number of Bases

25

30

(a) Reconstruction Performance

Probability

Probability

0.036
0.034
0.032
0.03
0.028

40

60

80

100 120 140 160 180 200
State

2e-09
1.5e-09
1e-09
5e-10

0.026

0
146

148

150
State

152

154

90

(c) The Belief and Reconstruction Near the
Peak

Figure 11:

20

An Example Belief and Reconstruction Using 3 Bases
3e-09
Original Belief
Reconstructed Belief
2.5e-09

Original Belief
Reconstructed Belief

0.038

0

(b) An Example Belief and Reconstruction

An Example Belief and Reconstruction Using 3 Bases
0.04

0.025
0.02
0.015
0.01
0.005
0
-0.005

0.2

Original Belief
Reconstructed Belief

95

100
State

105

110

(d) The Belief and Reconstruction In LowProbability Region

(a) The average KL divergence between the original sample set and the reconstructions. The KL divergence is 0.018 after 4 bases. The error bars represent
the standard deviation from the mean over the 500 beliefs. (b) The same example
belief from Figure 10 and its reconstruction using 3 bases. The reconstruction
shows small errors at the peak of each mode. Not shown is the reconstruction
using 4 bases, in which the original belief and its reconstruction are indistinguishable to the naked eye. (c) and (d) show fine detail of the original belief and
the reconstruction in two parts of the state space. Although the reconstruction
is not perfect, in the low-probability area, we see that the error is approximately
2  109 .

17

fiRoy, Gordon, & Thrun

Figure 11(b) shows the E-PCA reconstruction of the same example belief as in Figure 10.
We see that many of the artifacts present in the PCA reconstruction are absent. Using only
3 bases, we see that the E-PCA reconstruction is already substantially better than PCA
using 10 bases, although there are some small errors at the peaks (e.g., Figure 11c) of the
two modes. (Using 4 bases, the E-PCA reconstruction is indistinguishable to the naked eye
from the original belief.) This kind of accuracy for both 3 and 4 bases is typical for this
data set.
Robot Beliefs
Although the performance of E-PCA on finding good representations of the abstract problem
is compelling, we would ideally like to be able to use this algorithm on real-world problems,
such as the robot navigation problem in Figure 2. Figures 12 and 13 show results from two
such robot navigation problems, performed using a physically-realistic simulation (although
with artificially limited sensing and dead-reckoning). We collected a sample set of 500 beliefs
by moving the robot around the environment using a heuristic controller, and computed the
low-dimensional belief space B according to the algorithm in Table 1. The full state space
is 47.7m  17m, discretized to a resolution of 1m  1m per pixel, for a total of 799 states.
Figure 12(a) shows a sample belief, and Figure 12(b) the reconstruction using 5 bases.
In Figure 12(c) we see the average reconstruction performance of the E-PCA approach,
measured as average KL-divergence between the sample belief and its reconstruction. For
comparison, the performance of both PCA and E-PCA are plotted. The E-PCA error falls
to 0.02 at 5 bases, suggesting that 5 bases are sufficient for good reconstruction. This is a
very substantial reduction, allowing us to represent the beliefs in this problem using only
5 parameters, rather than 799 parameters. Notice that many of the states lie in regions
that are outside the map; that is, states that can never receive probability mass were not
removed. While removing these states would be a trivial operation, the E-PCA is correctly
able to do so automatically.
In Figure 13, similar results are shown for a different environment. A sample set of 500
beliefs was again collected using a heuristic controller, and the low-dimensional belief space
B was computed using the E-PCA. The full state space is 53.6m  37.9m, with a resolution
of .5m  .5m per pixel. An example belief is shown in Figure 13(a), and its reconstruction
using 6 bases is shown Figure 13(b). The reconstruction performance as measured by the
average KL divergence is shown in Figure 13(c); the error falls very close to 0 around 6
bases, with minimal improvement thereafter.

6. Computing POMDP policies
The Exponential-family Principal Components Analysis model gives us a way to find a
low-dimensional representation of the beliefs that occur in any particular problem. For the
two real-world navigation problems we have tried, the algorithm proved to be effective at
finding very low-dimensional representations, showing reductions from  800 states and
 2, 000 states down to 5 or 6 bases. A 5 or 6 dimensional belief space will allow much
more tractable computation of the value function, and so we will be able to solve much
larger POMDPs than we could have solved previously.
18

fiFinding Approximate POMDP Solutions Through Belief Compression

KL Divergence between Sampled Beliefs and Reconstructions

Particles form
a bimodal distribution

45

E-PCA
PCA

40

KL Divergence

35

(a) Original Belief

30
25
20
15
10
5
0
1

2

3

4

5

6

7

8

9

Number of Bases

(c) Reconstruction performance

(b) Reconstruction

Figure 12:

(a) A sample belief for the robot navigation task. (b) The reconstruction of this
belief from the learned E-PCA representation using 5 bases. (c) The average
KL divergence between the sample beliefs and their reconstructions against the
number of bases used. Notice that the E-PCA error falls close to 0 for 5 bases,
whereas conventional PCA has much worse reconstruction error even for 9 bases,
and is not improving rapidly.

KL Divergence between Sampled Beliefs and Reconstructions
4
3.5
KL Divergence

3
2.5
2
1.5
1
0.5
0

(a) A sample belief

Figure 13:

(b) The reconstruction

0

2

4

6
8
10
Number of Bases

(c) Average
performance

12

14

16

reconstruction

(a) A sample belief for the navigation problem in Longwood, cf. Figure 2. (b)
The reconstruction from the learned E-PCA representation using 6 bases. (c)
The average KL divergence between the sample beliefs and their reconstructions
against the number of bases used.

Unfortunately, we can no longer use conventional POMDP value iteration to find the
optimal policy given the low-dimensional set of belief space features. POMDP value iteration depends on the fact that the value function is convex over the belief space. When
19

fiRoy, Gordon, & Thrun

we compute a non-linear transformation of our beliefs to recover their coordinates on the
low-dimensional belief surface, we lose the convexity of the value function (compare Figure 3 and Figure 6 to see why). As a result, the value function cannot be expressed as the
supremum of a set of hyperplanes over the low-dimensional belief space.
So, instead of using POMDP value iteration, we will build a low-dimensional discrete
belief space MDP and use MDP value iteration. Since we do not know the form of the
value function, we will turn to function approximation. Gordon (1995) proved that the
fitted value iteration algorithm is guaranteed to find a bounded-error approximation to a
(possibly discounted) MDPs value function, so long as we use it in combination with a
function approximator that is an averager. Averagers are function approximators which are
non-expansions in max-norm; that is, they do not exaggerate errors in their training data.
In our experiments below, we use regular grids as well as irregular, variable-resolution grids
based on 1-nearest-neighbour discretization, represented by a set of low-dimensional beliefs
B  ,
B  = {b1 , b2 , . . . , b|B  | }.
(29)
Both of these approximations are averagers; other averagers include linear interpolation, knearest-neighbours, and local weighted averaging. We will not focus in detail on the exact
mechanism for discretizing the low-dimensional space, as this is outside the scope of this
paper. The resolution of the regular grid in all cases was chosen empirically; in section 7 we
describe a specific variable resolution discretization scheme that worked well empirically.
The reader can consult Munos and Moore (2002) or Zhou and Hansen (2001) for more
sophisticated representations.
The fitted value iteration algorithm uses the following update rule to compute a t-step
lookahead value function V t from a (t  1)-step lookahead value function V t1 :


|B  |
X
V t (bi ) = max R (bi , a) + 
T  (bi , a, bj )  V t1 (bj )
(30)
a

j=1

Here R and T  are approximate reward and transition functions based on the dynamics of
our POMDP, the result of our E-PCA, and the finite set of low-dimensional belief samples
B  that we are using as our function approximator. Note that in all problems described in
this paper, the problem did not require discounting ( = 1). The following sections describe
how to compute the model parameters R and T  .
Computing the Reward Function
The original reward function R(s, a) represents the immediate reward of taking action a
at state s. We cannot know, given either a low-dimensional or high-dimensional belief,
what the immediate reward will be, but we can compute the expected reward. We therefore
represent the reward as the expected value of the immediate reward of the full model, under
the current belief:
R (b , a) = Eb (R(s, a))

(31)

|S|

=

X
i=1

20

R(si , a)b(si ).

(32)

fiFinding Approximate POMDP Solutions Through Belief Compression

Equation (32) requires us to recover the high-dimensional belief b from the low-dimensional
representation b , as shown in equation (28).
For many problems, the reward function R will have the effect of giving a low immediate
reward for belief states with high entropy. That is, for many problems the planner will be
driven towards beliefs that are centred on high-reward states and have low uncertainty. This
property is intuitively desirable: in such beliefs the robot does not have to worry about an
immediate bad outcome.
Computing the Transition Function
Computing the low-dimensional transition function T  = p(bj |a, bi ) is not as simple as
computing the low-dimensional reward function R : we need to consider pairs of lowdimensional beliefs, bi and bj . In the original high-dimensional belief space, the transition
from a prior belief bi to a posterior belief bj is described by the Bayes filter equation:
bj (s) =  O(s, a, z)

|S|
X

T (sk , a, s)bi (sk )

(33)

k=1

Here a is the action we selected and z is the observation we saw; T is the original POMDP
transition probability distribution, and O is the original POMDP observation probability
distribution.
Equation (33) describes a deterministic transition conditioned upon a prior belief, an
action and an observation. The transition to the posterior bj is stochastic when the observation is not known; that is, the transition from bi to bj occurs only when a specific z is
generated, and the probability of this transition is the probability of generating observation
z. So, we can separate the full transition process into a deterministic transition to b a , the
belief after acting but before sensing, and a stochastic transition to b j , the full posterior:
ba (s) =

|S|
X

T (sj , a, s)bi (sj )

(34)

j=1

bj (s) =  O(s, a, z)ba (s).

(35)

Equations 34 and 35 describe the transitions of the high-dimensional beliefs for the
original POMDP. Based on these high-dimensional transitions, we can compute the transitions in our low-dimensional approximate belief space MDP. Figure 14 depicts the process.
As the figure shows, we start with a low-dimensional belief bi . From bi we reconstruct
a high-dimensional belief b according to equation (28). Then we apply an action a and
an observation z as described in equation (34) and equation (35) to find the new belief
b0 . Once we have b0 we can compress it to a low-dimensional representation b0 by iterating
equation (26). Finally, since b0 may not be a member of our sample B  of low-dimensional
belief states, we map b0 to a nearby bj  B  according to our function approximator.
If our function approximator is a grid, the last step above means replacing b0 by a
prototypical bj which shares its grid cell. More generally, our function approximator may
represent b0 as a combination of several states, putting weight w(bj , b0 ) on each bj . (For
example, if our approximator is k-nearest-neighbour, w(bj , b0 ) = k1 for each of the closest k
21

fiRoy, Gordon, & Thrun

~*
bi

~
b

~*
bj
Lowdimensional

action
a

b

ba

b

Highdimensional

observation
z

Figure 14: The process of computing a single transition probability.

samples in B  .) In this case we replace the transition from bi to b0 with several transitions,
each from bi to some bj , and scale the probability of each one by w(bj , b0 ).
For each transition bi  b  ba  b0  b0  bj we can assign a probability
p(z, j|i, a) =

p(z|ba ) w(bj , b0 )

=

w(bj , b0 )

|S|
X

p(z|sl )ba (sl )

(36)

l=1

The total transition probability T  (bi , a, bj ) is the sum, over all observations z, of p(z, j|i, a).
Step 3 in Table 2 performs this computation, but shares work between the computation of
T  (bi , a, bj ) for different posterior beliefs bj which are reachable from the same prior belief
bi under action a.
Computing the Value Function
With the reward and transition functions computed in the previous sections, we can use
value iteration to compute the value function for our belief space MDP. The full algorithm
is given in Table 2.

7. Solving Large POMDPs
In this section, we present the application of our algorithm to finding policies for large
POMDPs.
Toy problem
We first tested the E-PCA belief features using a regular grid representation on a version
of the toy problem described earlier. To ensure that we only needed a small set of belief
samples bi , we made the goal region larger. We also used a coarser discretization of the
underlying state space (40 states instead of 200) to allow us to compute the low-dimensional
model more quickly.
Figure 15 shows a comparison of the policies from the different algorithms. The E-PCA
does approximately twice as well as the Maximum-Likelihood heuristic; this heuristic guesses
its corridor, and is correct only about half the time. The AMDP Heuristic algorithm is the
Augmented MDP algorithm reported by Roy and Thrun (1999). This controller attempts
22

fiFinding Approximate POMDP Solutions Through Belief Compression

1. Generate the discrete low-dimensional belief space B  using E-PCA (cf. Table 1)
2. Compute the low-dimensional reward function R :
For each b  B  , a  A
(a) Recover b from b
(b) Compute R (b, a) =

P|S|

i=1 R(si , a)b(si ).

3. Compute the low-dimensional transition function T  :
For each bi  B  , a  A
(a) For each bj : T  (bi , a, bj ) = 0
(b) Recover bi from bi
(c) For each observation z
(d)

Compute bj from the Bayes filter equation (33) and b.

(e)

Compute b0 from bj by iterating equation (26).

(f)

For each bj with w(bj , b0 ) > 0
Add p(z, j|i, a) from equation (36) to T  (bi , a, bj )

(g)

4. Compute the value function for B 
(a) t = 0
(b) For each bi  B  : V 0 (bi ) = 0
(c) do
(d)

change = 0

(e)

For each bi  B  :


P|B  |
V t (bi ) = maxa R (bi , a) +  j=1 T  (bi , a, bj )  V t1 (bj )

change = change + V t (bi )  V t1 (bi )
(f) while change > 0

Table 2: Value Iteration for an E-PCA POMDP

to find the policy that will result in the lowest-entropy belief in reaching the goal. This
controller does very poorly because it is unable to distinguish between a unimodal belief
that knows which corridor it is in but not its position within the corridor, and a bimodal
belief that knows its position but not which corridor. The results in Figure 15 are averaged
over 10,000 trials.
It should be noted that this problem is sufficiently small that conventional PCA fares
reasonably well. In the next sections, we will see problems where the PCA representation
does poorly compared to E-PCA.
23

fiRoy, Gordon, & Thrun

Average reward vs. Number of Bases
120000

E-PCA
PCA

Average Reward

100000
80000
60000
40000

MDP Heuristic

20000
0
-20000

AMDP Heuristic
1

2

3

4

Number of Bases

Figure 15:

A comparison of policy performance using different numbers of bases, for 10,000
trials, with regular grid discretization. Policy performance was given by total
reward accumulated over trials.

Robot Navigation
We tested the E-PCA POMDP algorithm on simulated robot navigation problems in two
example environments, the Wean Hall corridor shown in Figure 16 and the Longwood retirement facility shown in Figure 1(b). The model parameters are given by robot navigation
models (see Fox et al., 1999).
We evaluated the policy for the relatively simple problem depicted in Figure 16. We set
the robots initial belief such that it may have been at one of two locations in the corridor,
with the objective to get to within 0.1m of the goal state (each grid cell is 0.2m0.2m). The
controller received a reward of +1000 for arriving at the goal state and taking an at_goal
action; a reward of 1000 was given for (incorrectly) taking this action at a non-goal state.
There was a reward of 1 for each motion. The states used for planning in this example
were the 500 states along the corridor, and the actions were forward and backward motion.
Figure 16 shows a sample robot trajectory using the E-PCA policy and 5 basis functions.
Notice that the robot drives past the goal to the lab door in order to verify its orientation
before returning to the goal; the robot does not know its true position, and cannot know
that it is in fact passing the goal. If the robot had started at the other end of the corridor,
its orientation would have become apparent on its way to the goal.
Figure 17 shows the average policy performance for three different techniques. The
Maximum-Likelihood heuristic could not distinguish orientations, and therefore approximately 50% of the time declared the goal in the wrong place. We also evaluated a policy
learned using the best 5 bases from conventional PCA. This policy performed substantially
better than the maximum-likelihood heuristic in that the controller did not incorrectly declare that the robot had arrived at the goal. However, this representation could not detect
when the robot was at the goal, and also chose sub-optimal (with respect to the E-PCA
policy) motion actions regularly. The E-PCA outperformed the other techniques in this example because it was able to model its belief accurately, in contrast to the result in Figure 15
where PCA had sufficient representation to perform as well or better than E-PCA.
24

fiFinding Approximate POMDP Solutions Through Belief Compression

True Position

Goal State
Final Estimated
Position

True Start State

Figure 16:

Goal Position

Start Position

An example robot trajectory, using the policy learned using 5 basis functions.
On the left are the start conditions and the goal. On the right is the robot
trajectory. Notice that the robot drives past the goal to the lab door to localize
itself, before returning to the goal.

Policy perfomance on Mobile Robot Navigation
400000

Average Reward

300000
200000
100000
0

-268500.0
-1000.0

33233.0

-100000
-200000
-300000

Figure 17:

ML Heuristic

PCA

E-PCA

A comparison of policy performance using E-PCA, conventional PCA and the
Maximum Likelihood heuristic, for 1,000 trials.

Figure 18(a) shows a second example of navigation in simulation. Notice that the initial
belief for this problem is bi-modal; a good policy will take actions to disambiguate the
modes before proceeding to the goal. Using a sample set of 500 beliefs, we computed the
low-dimensional belief space B. Figure 18(b) shows the average KL divergence between the
original and reconstructed beliefs. The improvement in the KL divergence error measure
slowed down substantially around 6 bases; we therefore used 6 bases to represent the belief
space.
Figure 18(c) shows an example execution of the policy computed using the E-PCA.
The reward parameters were the same as in the previous navigation example. The robot
parameters were maximum laser range of 2m, and high motion model variance. The first
action the policy chose was to turn the robot around and move it closer to the nearest wall.
This had the effect of eliminating the second distribution mode on the right. The robot then
followed essentially a coastal trajectory up the left-hand wall in order to stay localized,
although the uncertainty in the y direction became relatively pronounced. We see that as
the uncertainty eventually resolved itself at the top of the image, the robot moved to the
goal.
25

fiRoy, Gordon, & Thrun

KL Divergence between Sampled Beliefs and Reconstructions
4
3.5

True (hidden) start

3
KL Divergence

Goal

2.5
2
1.5
1
0.5
0

Start distribution modes

(a) Initial Distribution

0

2

4

6
8
10
Number of Bases

12

14

16

(b) Reconstruction Performance

Positional Accuracy at Goal

8
Distance from goal in metres

7

5

4.544

4
3
2
1
0

(c) The Complete Trajectory

Figure 18:

6.030

6

1.075
E-PCA

AMDP

MDP

(d) Policy Performance

(a) The sample navigation problem in Longwood, cf. Figure 2. This problem
involves multi-modal distributions. (c) The average KL divergence between the
sample beliefs and their reconstructions against the number of bases used, for 500
samples beliefs for a navigating mobile robot in this environment. (d) A comparison of policy performance using E-PCA, conventional MDP and the AMDP
heuristic.

It is interesting to note that this policy contains a similar coastal attribute as some
heuristic policies (e.g., the Entropy heuristic and the AMDP, Cassandra, Kaelbling, &
Kurien, 1996; Roy & Thrun, 1999). However, unlike these heuristics, the E-PCA representation was able to reach the goal more accurately (that is, get closer to the goal). This
representation was successful because it was able more accurately to represent the beliefs
and the effects of actions on the beliefs.
26

fiFinding Approximate POMDP Solutions Through Belief Compression

Finding People

(a) Original Belief

(b) Reconstruction with PCA

(c) Reconstruction with E-PCA

Figure 19:

The performance of PCA and E-PCA on a sample belief. The map is 238  85
grid cells, at a 0.2m resolution. (a) A sample belief. (b) The PCA reconstruction,
using 40 bases. (c) The E-PCA reconstruction, using 6 bases.

In addition to the synthetic problem and the robot navigation problems described in the
previous sections, we also tested our algorithm on a more complicated POMDP problem,
that of finding a person or object moving around in an environment. This problem is
motivated from the Nursebot domain, where residents experiencing cognitive decline can
sometimes become disoriented and start to wander. In order to make better use of the
health-care providers time, we would like to use a robot such as Pearl (Figure 1a) to find
the residents quickly. We assume that the person is not adversarial.
The state space in this problem is much larger than the previous robot navigation problems: it is the cross-product of the persons position and the robots position. However,
we assume for simplicity that the robots position is known, and therefore the belief distribution is only over the persons position. The transitions of the person state feature
are modelled by Brownian motion with a fixed, known velocity, which models the persons
motion as random, independent of the robot position. (If the person was moving to avoid
being captured by the robot, a different transition model would be required.) We assume
that the position of the person is unobservable until the robot is close enough to see the
person (when the robot has line-of-sight to the person, up to some maximum range, usually
3 metres); the observation model has 1% false negatives and no false positives. The reward
function is maximal when the person and the robot are in the same location.
27

fiRoy, Gordon, & Thrun

Figure 19(a) shows an example probability distribution that can occur in this problem
(not shown is the robots position). The grey dots are particles drawn from the distribution
of where the person could be in the environment. The distribution is initially uniform
over the reachable areas (inside the black walls). After the robot receives sensor data,
the probability mass is extinguished within the sensor range of the robot. As the robot
moves around, more of the probability mass is extinguished, focusing the distribution on
the remaining places the person can be. However, the probability distribution starts to
recover mass in places the robot visits but then leaves. In the particle filter, this can be
visualized as particles leaking into areas that were previously emptied out.
We collected a set of 500 belief samples using a heuristic controller given by driving the
robot to the maximum likelihood location of the person, and used E-PCA to find a good
low-dimensional representation of the beliefs. Figure 19(b) shows the reconstruction of the
example belief in Figure 19(a), using conventional PCA and 40 bases. This figure should
reinforce the idea that PCA performs poorly at representing probability distributions. Figure 19(c) shows the reconstruction using E-PCA and 6 bases, which is a qualitatively better
representation of the original belief.
Recall from section 6 that we use a function approximator for representing the value
function. In the preceding examples we used a regular grid over the low-dimensional surface
which performed well for finding good policies. However, the problem of finding people empirically requires a finer resolution representation than would be computationally tractable
with a regular grid. We therefore turn to a different function approximator, the 1-nearestneighbour variable resolution representation. We add new low-dimensional belief states to
the model by periodically re-evaluating the model at each grid cell, and splitting the gridcell into smaller discrete cells where a statistic predicted from the model disagrees with the
statistic computed from experience. A number of different statistics have been suggested
for testing the model against data from the real world (Munos & Moore, 1999), such as
reduction in reward variance, or value function disagreement. We have opted instead for a
simpler criterion of transition probability disagreement. We examine the policy computed
using a fixed representation, and also the policy computed using an incrementally refined
representation. Note that we have not fully explored the effect of different variable resolution representations of the value function, e.g., using k-nearest-neighbour interpolations
such as described by Hauskrecht (2000). These experiments are beyond the scope of this
paper, as our focus is on the utility of the E-PCA decomposition. No variable resolution
representation of a value function has been shown to scale effectively beyond a few tens of
dimensions at best (Munos & Moore, 2002).
This problem shares many attributes with the robot navigation problem, but we see in
Figure 19 and figures 20 and 21 that this problem generates spatial distributions of higher
complexity. It is somewhat surprising that E-PCA is able to find a good representation of
these beliefs using only 6 bases, and indeed the average KL divergence is generally higher
than for the robot navigation task. Regardless, we are able to find good controllers, and
this is an example of a problem where PCA performs very poorly even with a large number
of bases.
Figure 20 shows an example trajectory from the heuristic control strategy, driving the
robot to the maximum likelihood location of the person at each time step. The open circle
is the robot position, starting at the far right. The solid black circle is the position of the
28

fiFinding Approximate POMDP Solutions Through Belief Compression

Figure 20:

(a)

(b)

(c)

(d)

(e)

(f)

An example of a suboptimal person finding policy. The grey particles are drawn
from the distribution of where the person might be, initially uniformly distributed in (a). The black dot is the true (unobservable) position of the person.
The open circle is the observable position of the robot. Through the robots poor
action selection, the person is able to escape into previously explored areas.

person, which is unobservable by the robot until within a 3m range. The person starts in
the room above the corridor (a), and then moves down into the corridor once the robot has
moved to the far end of the corridor (b). As the robot returns to search inside the room (c)
and (d), the person moves unobserved into the previously searched corridor (e). Although
we have deliberately chosen an example where the heuristic performs poorly, the person is
not following an unlikely or adversarial trajectory: at all times the solid black circle remains
in regions of high probability. The robots belief accurately reflects the possibility that the
person will slip past, but the heuristic control algorithm has no way to take this possibility
into account.
Using the policy found for the low-dimensional belief space as described in previous
sections, we are able to find a much better controller. A sample trajectory for this controller
29

fiRoy, Gordon, & Thrun

Figure 21:

(a)

(b)

(c)

(d)

(e)

(f)

The policy computed using the E-PCA representation. The initial conditions in
panel (a) are the same as in Figure 20. Notice that, unlike the previous figure,
this strategy ensures that the probability mass is located in one place, allowing
the robot to find the person with significantly higher probability.

is shown in Figure 21. The robot travels from the right-most position in the corridor (a)
to only part-way down the corridor (b), and then returns to explore the room (c) and
(d). In this example, the persons starting position was different from the one given in the
previous examplethe E-PCA policy would find the person at this point, starting from the
same initial conditions as the previous example). After exploring the room and eliminating
the possibility that the person is inside the room (e), the policy has reduced the possible
locations of the person down to the left-hand end of the corridor, and is able to find the
person reliably at that location.
Note that figures 20 and 21 have the target person in the worst-case start position for
each planner. If the person were in the same start position in Figure 21 as in Figure 20,
the policy would have found the person by panel (d). Similarly, if the person had started
30

fiFinding Approximate POMDP Solutions Through Belief Compression

at the end of corridor as in Figure 21, the policy shown in Figure 20 would have found the
person by panel (b).
Performance for Different Policies

Average # of Actions to Find Person

250

200

150

100
Fully Observable Policy
50

0

Figure 22:

Closest

Densest

MDP

PCA

E-PCA Refined E-PCA

A comparison of 6 policies for person finding in a simple environment. The
baseline is the fully-observable, i.e., cheating, solution (the solid line). The EPCA policy is for a fixed (variable resolution) discretization. The Refined E-PCA
is for a discretization where additional belief samples have been added. The PCA
policy was approximately 6 times worse than the best E-PCA policy.

Figure 22 shows a quantitative comparison of the performance of the E-PCA against
a number of other heuristic controllers in simulation, comparing the average time to find
the person for these different controllers. The solid line depicts the baseline performance,
using a controller that has access to the true state of the person at all times (i.e., a fully
observable lower bound on the best possible performance). The travel time in this case is
solely a function of the distance to the person; no searching is necessary or performed. Of
course, this is not a realizable controller in reality. The other controllers are:
 Closest: The robot is driven to the nearest state of non-zero probability.
 Densest: The robot is driven to the location from which the most probability mass is
visible.
 MDP: The robot is driven to the maximum-likelihood state.
 PCA: A controller found using the PCA representation and a fixed discretization of the
low-dimensional surface.
 E-PCA: The E-PCA controller using a fixed discretization of the low-dimensional surface
to compute the value function.
 Refined E-PCA: The E-PCA controller using an incrementally refined variable resolution
discretization of the surface for computing the value function.
The performance of the best E-PCA controller is surprisingly close to the theoretical best
performance, in terms of time to find the person, but this result also demonstrates the need
for careful choice of discretization of the belief space for computing the value function. The
31

fiRoy, Gordon, & Thrun

initial variable resolution representation proved to be a poor function approximator, however, using the iteratively-refined variable resolution discretization, we are able to improve
the performance substantially. The controller using the conventional PCA representation
case was computed over a fixed discretization of the low-dimensional representation using
40 bases and 500 grid points. The quality of belief representation under PCA was so poor
we did not investigate more complex policy approximators.

8. Discussion
These experiments demonstrate that the E-PCA algorithm can scale to finding low-dimensional surfaces embedded in very high-dimensional spaces.
Time Complexity
The algorithm is iterative and therefore no simple expression for the total running time is
available. For a data set of |B| samples of dimensionality n, computing a surface of size
l, each iteration of the algorithm is O(|B|nl 2 + |B|l3 + nl3 ). Each step of the Newtons
algorithm is dominated by a set of matrix multiplies and the final step of inverting an l  l
matrix, which is O(l3 ). The U step consists of |B| iterations, where each iteration has O(nl)
multiplies and the O(l3 ) inversion. The V step consists of n iterations, where each iteration
has O(|B|l) multiplies and the O(l 3 ) inversion, leading to the total complexity given above.
Figure 23 shows the time to compute the E-PCA bases for 500 sample beliefs, for
20,230 states. This implementation used Java 1.4.0 and Colt 1.0.2, on a 1 GHz Athlon
CPU with 900M of RAM. Also shown are the computation times for conventional PCA
decomposition. For small state space problems, the E-PCA decomposition can be faster
than PCA for a small number of bases, if the implementation of PCA always computes
the full decomposition (l = n, where l is the reduced dimensionality and n is the full
dimensionality).
Exponential Family PCA Running Time
35000
30000
Time in secs

25000
20000
15000
10000
Conventional PCA = 4151sec

5000
0

Figure 23:

0

2

4

6
8
Number of Bases

10

12

The time to compute the E-PCA representations for different discretizations of
the state space.

32

fiFinding Approximate POMDP Solutions Through Belief Compression

By far the dominant term in the running time of our algorithm is the time to compute
the E-PCA bases. Once the bases have been found and the low-dimensional space has been
discretized, the running time required by value iteration to converge to a policy for the
problems we have described was on the order of 50 to 100ms.
Sample Belief Collection
In all example problems we have addressed, we have used a standard sample size of 500
sample beliefs. Additionally, we have used hand-coded heuristic controllers to sample beliefs
from the model. In practice, we found 500 sample beliefs collected using a semi-random controller sufficient for our example problems. However, we may be able to improve the overall
performance of our algorithm on future problems by iterating between phases of building
the belief space representation (i.e., collecting beliefs and generating the low-dimensional
representation) and computing a good controller. Once an initial set of beliefs have been
collected and used to build an initial set of bases and a corresponding policy, we can continue
to evaluate the error of the representation (e.g., K-L divergence between the current belief
and its low-dimensional representation). If the initial representation has been learned with
too few beliefs, then the representation may over-fit the beliefs; we can detect this situation
by noticing that our representation does a poor job at representing new beliefs. Validation
techniques such as cross-validation may also be useful in determining when enough beliefs
have been acquired.
Model Selection
One of the open questions we have not addressed so far is that of choosing the appropriate
number of bases for our representation. Unless we have problem-specific information, such
as the true number of degrees of freedom in the belief space (as in the toy example of
section 3), it is difficult to identify the appropriate dimensionality of the underlying surface
for control. One common approach is to examine the eigenvalues of the decomposition,
which can be recovered using the orthonormalization step of the algorithm in Table 1.
(This assumes our particular link function is capable of expressing the surface that our
data lies on.) The eigenvalues from conventional PCA are often used to determine the
appropriate dimensionality of the underlying surface; certainly the reconstruction will be
lossless if we use as many bases as there are non-zero eigenvalues.
Unfortunately, recall from the description of E-PCA in section 4 that we do not generate
a set of singular values, or eigenvalues. The non-linear projection introduced by the link
function causes the eigenvalues of the U matrix to be uninformative about the contribution
of each basis to the representation. Instead of using eigenvalues to choose the appropriate
surface dimensionality, we use reconstruction quality, as in Figure 11. Using reconstruction
quality to estimate the appropriate dimensionality is a common choice for both PCA and
other dimensionality reduction techniques (Tenenbaum, de Silva, & Langford, 2000). One
alternate choice would be to evaluate the reward for policies computed for different dimensionalities and choose the most compact representation that achieves the highest reward,
essentially using control error rather than reconstruction quality to determine dimensionality.
33

fiRoy, Gordon, & Thrun

Recall from our discussion in section 2 that we are using dimensionality reduction to
represent beliefs from POMDPs with a specific kind of structure. In particular, the E-PCA
representation will be most useful in representing beliefs that are relatively sparse and have
a small number of degrees of freedom. However, E-PCA will be unable to find good lowdimensional representations for POMDP models that do not exhibit this kind of structure 
that is, if the beliefs cannot be represented as lying on low-dimensional hyperplane linked to
the full belief space via the appropriate link function. One additional problem then is how to
know a priori whether or not a specific POMDP has the appropriate structure. It is unlikely
that there is a general technique that can determine the usefulness of E-PCA, but we can
take advantage of model selection techniques also to determine whether or not E-PCA will
find a usefully low dimensional representation for a specific POMDP. For example, if the
KL divergence between a set of sample beliefs and their reconstructions is large even using
a large number of bases, then the problem may not have the right structure.

9. Related Work
Many attempts have been made to use reachability analysis to constrain the set of beliefs
for planning (Washington, 1997; Hauskrecht, 2000; Zhou & Hansen, 2001; Pineau, Gordon,
& Thrun, 2003a). If the reachable set of beliefs is relatively small, then forward search
to find this set is a perfectly reasonable approach. The policy computed over these beliefs is of course optimal, although it is relatively rare in real world problems to be able
to enumerate the reachable beliefs. Reachability analysis has also been used with some
success as a heuristic in guiding search methods, especially for focusing computation on
finding function approximators (Washington, 1997; Hansen, 1998). In this approach, the
problem still remains of how to compute the low-dimensional representation given the finite
set of representative beliefs. Discretization of the belief space itself has been explored a
number of times, both regular grid-based discretization (Lovejoy, 1991), regular variable
resolution approaches (Zhou & Hansen, 2001) and non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000). In the same vein, state abstraction (Boutilier &
Poole, 1996) has been explored to take advantage of factored state spaces, and of particular
interest is the algorithm of Hansen and Feng (2000) which can perform state abstraction
in the absence of a prior factorization. So far, however, all of these approaches have fallen
victim to the curse of dimensionality and have failed to scale to more than a few dozen
states at most.
The value-directed POMDP compression algorithm of Poupart and Boutilier (2002) is a
dimensionality-reduction technique that is closer in spirit to ours, if not in technique. This
algorithm computes a low-dimensional representation of a POMDP directly from the model
parameters R, T , and O by finding the Krylov subspace for the reward function under belief
propagation. The Krylov subspace for a vector and a matrix is the smallest subspace that
contains the vector and is closed under multiplication by the matrix. For POMDPs, the
authors use the smallest subspace that contains the immediate reward vector and is closed
under a set of linear functions defined by the state transitions and observation model. The
major advantage of this approach is that it optimizes the correct criterion: the value-directed
compression will only distinguish between beliefs that have different value. The major
disadvantage of this approach is that the Krylov subspace is constrained to be linear. Using
34

fiFinding Approximate POMDP Solutions Through Belief Compression

our algorithm with PCA instead of E-PCA, we can realize much of the same compression
as the Poupart and Boutilier (2002) method: we can take advantage of regularities in the
same transition matrices T a,z but not in the reward function R. Unfortunately, as we have
seen, beliefs are unlikely to lie on a low-dimensional hyperplane, and our results reported
in section 3 indicate that linear compression will not scale to the size of problems we wish
to address.
Possibly the most promising approaches for finding approximate value-functions are
the point-based methods, which instead of optimizing the value function over the entire
belief space, do so only for specific beliefs. Cheng (1988) described a method for backing
up the value function at specific belief points in a procedure called point-based dynamic
programming (PB-DP). These PB-DP steps are interleaved with standard backups as in
full value iteration. Zhang and Zhang (2001) improved this method by choosing the Witness
points as the backup belief points, iteratively increasing the number of such points. The
essential idea is that point-based backups are significantly cheaper than full backup steps.
Indeed, the algorithm described by Zhang and Zhang (2001) out-performs Hansens exact
policy-search method by an order of magnitude for small problems. However, the need for
periodic backups across the full belief space still limits the applicability of these algorithms
to small abstract problems.
More recently, Pineau et al. (2003a) have abandoned full value function backups in
favour of only point-based backups in the point-based value iteration (PBVI) algorithm.
By backing up only at discrete belief points, the backup operator is polynomial instead
of exponential (as in value iteration), and, even more importantly, the complexity of the
value function remains constant. PBVI uses a fundamentally different approach to finding
POMDP policies, and still remains constrained by the curse of dimensionality in large state
spaces. However, it has been applied successfully to problems at least an order of magnitude
larger than its predecessors, and is another example of algorithms that can be used to make
large POMDPs tractable.
E-PCA is not the only possible technique for non-linear dimensionality reduction; there
exists a large body of work containing different techniques such as Self-Organizing Maps (Kohonen, 1982), Generative Topographic Mapping (Bishop, Svensen, & Williams, 1998),
Stochastic Neighbour Embedding (Hinton & Roweis, 2003). Two of the most successful
algorithms to emerge recently have are Isomap (Tenenbaum et al., 2000) and Locally Linear Embedding (Roweis & Saul, 2000). Isomap extends PCA-like methods to non-linear
surfaces using geodesic distances as the distance metric between data samples, rather than
Euclidean distances. Locally Linear Embedding (LLE) can be considered a local alternative
to the global reduction of Isomap in that it represents each point as the weighted combination of its neighbours and operates in two phases: computing the weights of the k nearest
neighbours for each high-dimensional point, and then reconstructing the data in the lowdimensional co-ordinate frame from the weights. However, these algorithms do not contain
explicit models of the kind of data (e.g., probability distributions) that they are attempting
model. One interesting line of research, however, may be to extend these algorithms using
different loss functions in the same manner that PCA was extended to E-PCA.
35

fiRoy, Gordon, & Thrun

10. Conclusion
Partially Observable Markov Decision Processes have been considered intractable for finding
good controllers in real world domains. In particular, the best algorithms to date for
finding an approximate value function over the full belief space have not scaled beyond a
few hundred states (Pineau et al., 2003a). However, we have demonstrated that real world
POMDPs can contain structured belief spaces; by finding and using this structure, we have
been able to solve POMDPs an order of magnitude larger than those solved by conventional
value iteration techniques. Additionally, we were able to solve different kinds of POMDPs,
from a simple highly-structured synthetic problem to a robot navigation problem to a
problem with a factored belief space and relatively complicated probability distributions.
The algorithm we used to find this structure is related to Principal Components Analysis
with a loss function specifically chosen for representing probability distributions. The real
world POMDPs we have been able to solve are characterized by sparse distributions, and
the Exponential family PCA algorithm is particularly effective for compressing this data.
There do exist POMDP problems which do not have this structure, and for which this
dimensionality reduction technique will not work well; however, it is a question for further
investigation if other, related dimensionality-reduction techniques (e.g., Isomap or LocallyLinear Embedding, Tenenbaum et al., 2000; Roweis, Saul, & Hinton, 2002) can be applied.
There are a number of interesting possibilities for extending this algorithm in order to
improve its efficiency or increase the domain of applicability. The loss function that we
chose for dimensionality reduction was based on reconstruction error, as in
L(B, U, B) = e(U B)  B  U B,

(37)

(cf. equation 8). Minimizing the reconstruction error should allow near-optimal policies to
be learned. However, we would ideally like to find the most compact representation that
minimizes control errors. This could possibly be better approximated by taking advantage
of transition probability structure. For example, dimensionality reduction that minimizes
prediction errors would correspond to the loss function:
L(B, U, B, T ) = e(U b)  B  U b + kB,2...n  T B,1...n1 k2

(38)

where B,1...n1 is the l  n  1 matrix of the first n  1 column vectors in B, and B,2...n is
the l  n  1 matrix of the n  1 column vectors in V starting from the second vector. This
has the effect of finding a representation that allows bt+1 to be predicted from T bt , with the
caveat that the B must be arranged all for the same action. We plan to address this issue
in future work.
Another shortcoming of the approach described in this work is that it contains the
assumption that all beliefs can be described using the same low-dimensional representation.
However, it is relatively easy to construct an example problem which generates beliefs that
lie on two distinct low-dimensional surfaces, which in the current formulation would make
the apparent dimensionality of the beliefs appear much higher than a set of beliefs sampled
from one surface alone.
While this work has largely been motivated by finding better representations of beliefs,
it is not the only approach to solving large POMDPs. Policy search methods (Meuleau,
36

fiFinding Approximate POMDP Solutions Through Belief Compression

Peshkin, Kim, & Kaelbling, 1999) and hierarchical methods (Pineau, Gordon, & Thrun,
2003b) have also been able to solve large POMDPs. It is interesting to note that controllers
based on the E-PCA representations are often essentially independent of policy complexity
but strongly dependent on belief complexity, whereas the policy search and hierarchical
methods are strongly dependent on policy complexity but largely independent of belief
space complexity. It seems likely that progress in solving large POMDPs in general will lie
in a combination of both approaches.
The E-PCA algorithm finds a low-dimensional representation B of the full belief space B
from sampled data. We demonstrated that the reliance on sampled data is not an obstacle
for some real world problems. Furthermore, using only sampled beliefs could be an asset
for large problems where generating and tracking beliefs can be considerably easier than
planning. It may however be preferable to try to compute a low-dimensional representation
directly from the model parameters. Poupart and Boutilier (2002) use the notion of a Krylov
subspace to do this. The subspace computed by their algorithm may correspond exactly
with a conventional PCA and we have seen instances where PCA does a poor job of finding
low-dimensional representations. The most likely explanation is that real-world beliefs do
not lie on low-dimensional planes for most problems, but instead on curved surfaces. An
extremely useful algorithm would be one that finds a subset of belief space closed under the
transition and observation function, but which is not constrained to find only planes.

Acknowledgements
Thanks to Tom Mitchell, Leslie Kaelbling, Reid Simmons, Drew Bagnell, Aaron Courville,
Mike Montemerlo and Joelle Pineau for useful comments and insight into this work. Nicholas
Roy was funded by the National Science Foundation under ITR grant # IIS-0121426. Geoffrey Gordon was funded by AFRL contract F3060201C0219, DARPAs MICA program,
and by AFRL contract F306029820137, DARPAs CoABS program.

References
Bagnell, J. A., & Schneider, J. (2001). Autonomous helicopter control using reinforcement
learning policy search methods. In Proceedings of the IEEE International Conference
on Robotics and Automation (ICRA), pp. 16151620, Seoul, South Korea. IEEE Press.
Bishop, C., Svensen, M., & Williams, C. (1998). GTM: the generative topographic mapping.
Neural Computation, 10 (1), 215234.
Boutilier, C., & Poole, D. (1996). Computing optimal policies for partially observable
Markov decision processes using compact representations. In Proceedings of the 13th
National Conference on Artificial Intelligence (AAAI-96), pp. 11681175.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
Proceedings of the 14th Annual Conference on Uncertainty in AI (UAI), pp. 3342,
Madison, Wisconsin.
Brafman, R. I. (1997). A heuristic variable grid solution method for POMDPs. In Kuipers,
B. K., & Webber, B. (Eds.), Proceedings of the 14th National Conference on Artificial
Intelligence (AAAI), pp. 727733, Providence, RI.
37

fiRoy, Gordon, & Thrun

Cassandra, A. R., Kaelbling, L., & Kurien, J. A. (1996). Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation. In Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and Systems.
Chen, B. M. (2000). Robust and H- Control. Springer-Verlag.
Cheng, H.-T. (1988). Algorithms for Partially Observable Markov Decision Processes. Ph.D.
thesis, University of British Columbia, Vancouver, Canada.
Collins, M., Dasgupta, S., & Schapire, R. (2002). A generalization of principal components
analysis to the exponential family. In Dietterich, T. G., Becker, S., & Ghahramani, Z.
(Eds.), Advances in Neural Information Processing Systems 14 (NIPS), Cambridge,
MA. MIT Press.
Cox, T., & Cox, M. (1994). Multidimensional Scaling. Chapman & Hall, London.
Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization for mobile robots in dynamic
environments. Journal of Artificial Intelligence Research, 11, 391427.
Galassi, M., Davies, J., Theiler, J., Gough, B., Jungman, G., Booth, M., & Rossi,
F. (2002).
GNU Scientific Library Reference Manual (3rd Edition edition).
http://www.gnu.org/software/gsl/.
Golub, G., & Reinsch, C. (1970). Singular value decomposition and least squares solutions.
Numerische Mathematik, pp. 403420.
Gordon, G. (1995). Stable function approximation in dynamic programming. In Prieditis,
A., & Russell, S. (Eds.), Proceedings of the 12 International Conference on Machine
Learning (ICML), pp. 261268, San Francisco, CA. Morgan Kaufmann.
Gordon, G. (2003). Generalized2 linear2 models. In Becker, S., Thrun, S., & Obermayer, K.
(Eds.), Advances in Neural Information Processing Systems 15 (NIPS). MIT Press.
Gutmann, J.-S., Burgard, W., Fox, D., & Konolige, K. (1998). An experimental comparison
of localization methods. In Proceedings of the IEEE/RSJ International Conference
on Intelligent Robots and Systems, Victoria, Canada.
Gutmann, J.-S., & Fox, D. (2002). An experimental comparison of localization methods
continued. In Proceedings of the IEEE/RSJ International Conference on Intelligent
Robots and Systems, Lausanne, Switzerland.
Hansen, E., & Feng, Z. (2000). Dynamic programming for POMDPs using a factored state
representation. In Proceedings of the Fifth International Conference on Artificial
Intelligence Planning and Scheduling (AIPS-00), Breckenridge, CO.
Hansen, E. (1998). Solving POMDPs by searching in policy space. In Proceedings of the
14th Conference on Uncertainty in Artifical Intelligence (UAI), pp. 211219, Madison,
WI.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov
decision processes. Journal of Artificial Intelligence Research, 13, 3394.
Hinton, G., & Roweis, S. (2003). Stochastic neighbor embedding. In Becker, S., Thrun,
S., & Obermayer, K. (Eds.), Advances in Neural Information Processing Systems 15
(NIPS). MIT Press.
38

fiFinding Approximate POMDP Solutions Through Belief Compression

Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT.
Isard, M., & Blake, A. (1998). CONDENSATION  conditional density propagation for
visual tracking. International Journal of Computer Vision, 29 (1), 528.
Joliffe, I. T. (1986). Principal Component Analysis. Springer-Verlag.
Kanazawa, K., Koller, D., & Russell, S. (1995). Stochastic simulation algorithms for dynamic
probabilistic networks. In Proceedings of the 11th Annual Conference on Uncertainty
in AI (UAI), pp. 346351, Montreal, Canada.
Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological Cybernetics, 48, 5969.
Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by non-negative matrix
factorization. Nature, 401, 788791.
Leonard, J., & Durrant-Whyte, H. (1991). Mobile robot localization by tracking geometric
beacons. IEEE Transactions on Robotics and Automation, 7 (3), 376382.
Lovejoy, W. S. (1991). Computationally feasible bounds for partially observable Markov
decison processes. Operations Research, 39, 192175.
Mardia, K. V., & Jupp, P. E. (2000). Directional Statistics (2nd edition). Wiley, Chichester,
NY.
McCullagh, P., & Nelder, J. A. (1983). Generalized Linear Models (2nd edition). Chapman
and Hall, London.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers for partially observable environments. In Laskey, K. B., & Prade, H. (Eds.),
Proceedings of the Fifteenth International Conference on Uncertainty in Artificial Intelligence, pp. 427436, Stockholm, Sweden. Morgan Kaufmann.
Munos, R., & Moore, A. (1999). Variable resolution discretization for high-accuracy solutions of optimal control problems. In Dean, T. (Ed.), Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI), pp. 13481355, Stockholm
Sweden. Morgan Kaufmann.
Munos, R., & Moore, A. (2002). Variable resolution discretization in optimal control. Machine Learning, 49 (2-3), 291323.
Nourbakhsh, I., Powers, R., & Birchfield, S. (1995). DERVISH an office-navigating robot.
AI Magazine, 16 (2), 5360.
Olson, C. F. (2000). Probabilistic self-localization for mobile robots. IEEE Transactions
on Robotics and Automation, 16 (1), 5566.
Pineau, J., Gordon, G., & Thrun, S. (2003a). Point-based value iteration: An anytime
algorithm for POMDPs. In Proceedings of the 18th International Joint Conference on
Artificial Intelligence (IJCAI 2003), Acapulco, Mexico.
Pineau, J., Gordon, G., & Thrun, S. (2003b). Policy-contingent abstraction for robust robot
control. In Meek, C., & Kjlruff, U. (Eds.), Proceedings of the 19th Annual Conference
on Uncertainty in Artificial Intelligence (UAI), Acapulco, Mexico.
39

fiRoy, Gordon, & Thrun

Poupart, P., & Boutilier, C. (2002). Value-directed compression of POMDPs. In Becker,
S., Thrun, S., & Obermayer, K. (Eds.), Advances in Neural Information Processing
Systems 15 (NIPS), Vancouver, Canada. MIT Press.
Rockafellar, R. T. (1970). Convex Analysis. Princeton University Press, New Jersey.
Roweis, S., & Saul, L. (2000). Nonlinear dimensionality reduction by locally linear embedding.. Science, 290 (5500), 23232326.
Roweis, S. T., Saul, L. K., & Hinton, G. E. (2002). Global coordination of local linear
models. In Dietterich, T. G., Becker, S., & Ghahramani, Z. (Eds.), Advances in
Neural Information Processing Systems, Vol. 14, Cambridge, MA. MIT Press.
Roy, N., & Thrun, S. (1999). Coastal navigation with mobile robots. In Solla, S. A., todd
K. Leen, & Muller, K. R. (Eds.), Advances in Neural Processing Systems 12 (NIPS),
pp. 10431049, Denver, CO. MIT Press.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A Modern Approach. Prentice Hall.
Shatkay, H., & Kaelbling, L. P. (2002). Learning geometrically-constrained hidden markov
models for robot navigation: Bridging the geometrical-topological gap. Journal of AI
Research.
Tenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). A global geometric framework
for nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2000). Robust Monte Carlo localization
for mobile robots. Artificial Intelligence, 128 (1-2), 99141.
Washington, R. (1997). BI-POMDP: Bounded, incremental partially-observable Markovmodel planning. In Proceedings of the 4th European Conference on Planning (ECP).
Zhang, N. L., & Zhang, W. (2001). Speeding up the convergence of value iteration in partially observable Markov decision processes. Journal of Artificial Intelligence Research,
14, 128.
Zhou, R., & Hansen, E. (2001). An improved grid-based approximation algorithm for
POMDPs. In Nebel, B. (Ed.), Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI), pp. 707716, Seattle, Washington. Morgan
Kaufmann.

40

fiJournal of Artificial Intelligence Research 23 (2005) 367-420

Submitted 07/04; published 04/05

Hybrid BDI-POMDP Framework for Multiagent Teaming
Ranjit Nair

ranjit.nair@honeywell.com

Automation and Control Solutions
Honeywell Laboratories, Minneapolis, MN 55416

Milind Tambe

tambe@usc.edu

Department of Computer Science
University of Southern California, Los Angeles, CA 90089

Abstract
Many current large-scale multiagent team implementations can be characterized as
following the belief-desire-intention (BDI) paradigm, with explicit representation of team
plans. Despite their promise, current BDI team approaches lack tools for quantitative
performance analysis under uncertainty. Distributed partially observable Markov decision
problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal
policies in such models is highly intractable. The key contribution of this article is a
hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP
tractability and POMDP analysis improves BDI team plan performance.
Concretely, we focus on role allocation, a fundamental problem in BDI teams: which
agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future
uncertainties in the domain; prior work in multiagent role allocation has failed to address
such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our
technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP policies, and the RMTDP policy search
fills the gaps in such incomplete policies by searching for the best role allocation. Our
second key contribution is a novel decomposition technique to further improve RMTDP
policy search efficiency. Even though limited to searching role allocations, there are still
combinatorially many role allocations, and evaluating each in RMTDP to identify the best
is extremely difficult. Our decomposition technique exploits the structure in the BDI team
plans to significantly prune the search space of role allocations. Our third key contribution
is a significantly faster policy evaluation algorithm suited for our BDI-POMDP hybrid approach. Finally, we also present experimental results from two domains: mission rehearsal
simulation and RoboCupRescue disaster rescue simulation.

1. Introduction
Teamwork, whether among software agents, or robots (and people) is a critical capability
in a large number of multiagent domains ranging from mission rehearsal simulations, to
RoboCup soccer and disaster rescue, to personal assistant teams. Already a large number of multiagent teams have been developed for a range of domains (Pynadath & Tambe,
2003; Yen, Yin, Ioerger, Miller, Xu, & Volz, 2001; Stone & Veloso, 1999; Jennings, 1995;
Grosz, Hunsberger, & Kraus, 1999; Decker & Lesser, 1993; Tambe, Pynadath, & Chauvat,
2000; da Silva & Demazeau, 2002). These existing practical approaches can be characterized as situated within the general belief-desire-intention (BDI) approach, a paradigm
c
2005
AI Access Foundation. All rights reserved.

fiNair & Tambe

for designing multiagent systems, made increasingly popular due to programming frameworks (Tambe et al., 2000; Decker & Lesser, 1993; Tidhar, 1993b) that facilitate the design
of large-scale teams. Within this approach, inspired explicitly or implicitly by BDI logics,
agents explicitly represent and reason with their team goals and plans (Wooldridge, 2002).
This article focuses on analysis of such BDI teams, to provide feedback to aid human
developers and possibly to agents participating in a team, on how the team performance
in complex dynamic domains can be improved. In particular, it focuses on the critical
challenge of role allocation in building teams (Tidhar, Rao, & Sonenberg, 1996; Hunsberger
& Grosz, 2000), i.e. which agents to allocate to the various roles in the team. For instance,
in mission rehearsal simulations (Tambe et al., 2000), we need to select the numbers and
types of helicopter agents to allocate to different roles in the team. Similarly, in disaster
rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjoh, & Shimada, 1999), role
allocation refers to allocating fire engines and ambulances to fires and it can greatly impact
team performance. In both these and other such domains, the performance of the team is
linked to important metrics such as loss of human life and property and thus it is critical
to analyze team performance and suggest improvements.
While BDI frameworks facilitate human design of large scale teams, the key difficulty
in analyzing role allocation in these teams is due to the uncertainty that arises in complex
domains. For example, actions may fail and the world state may be only partially observable
to the agents owing to physical properties of the environment or imperfect sensing. Role
allocation demands such future uncertainties be taken into account, e.g. the fact that an
agent may fail during execution and may or may not be replaced by another must be taken
into account when determining the role allocation. Yet most current role allocation algorithms do not address such uncertainty (see Section 7.4). Indeed, such uncertainty requires
quantitative comparison of different role allocations. However, tools for such quantitative
evaluations of BDI teams are currently absent. Thus, given these uncertainties, we may be
required to experimentally recreate a large number of possible scenarios (in a real domain
or in simulations) to evaluate and compare different role allocations.
Fortunately, the emergence of distributed Partially Observable Markov Decision Problems (POMDPs) provides models (Bernstein, Zilberstein, & Immerman, 2000; Boutilier,
1996; Pynadath & Tambe, 2002; Xuan, Lesser, & Zilberstein, 2001) that can be used for
quantitative analysis of agent teams in uncertain domains. Distributed POMDPs represent a class of formal models that are powerful enough to express the uncertainty in these
dynamic domains arising as a result of non-determinism and partial observability and in
principle, can be used to generate and evaluate complete policies for the multiagent team.
However, there are two shortcomings in these models that prevents their application in
the analysis of role allocation. First, previous work on analysis has focused on communication (Pynadath & Tambe, 2002; Xuan et al., 2001), rather than role allocation or any
other coordination decisions. Second, as shown by Bernstein et al. (2000), the problem
of deriving the optimal policy is generally computationally intractable (the corresponding
decision problem is NEXP-complete). Thus, applying optimal policies for analysis is highly
intractable.
To address the first difficulty, we derive RMTDP (Role-based Multiagent Team Decision
Problem), a distributed POMDP framework for quantitatively analyzing role allocations.
Using this framework, we show that, in general, the problem of finding the optimal role
368

fiHybrid BDI-POMDP Framework for Multiagent Teaming

completed policy =
additions to BDI team plan

BDI team plan
RMTDP
Search Policy Space

Incomplete policy
BDI Interpreter

Domain
RMTDP model

Figure 1: Integration of BDI and POMDP.

allocation policy is computationally intractable (the corresponding decision problem is still
NEXP-complete). This shows that improving the tractability of analysis techniques for role
allocation is a critically important issue.
Therefore, in order to make the quantitative analysis of multiagent teams using RMTDP
more tractable, our second contribution provides a hybrid BDI-POMDP approach that
combines the native strengths of the BDI and POMDP approaches, i.e., the ability in BDI
frameworks to encode large-scale team plans and the POMDP ability to quantitatively
evaluate such plans. This hybrid approach is based on three key interactions that improve
the tractability of RMTDP and the optimality of BDI agent teams. The first interaction is
shown in Figure 1. In particular, suppose we wish to analyze a BDI agent team (each agent
consisting of a BDI team plan and a domain independent interpreter that helps coordinate
such plans) acting in a domain. Then as shown in Figure 1, we model the domain via an
RMTDP, and rely on the BDI team plan and interpreter for providing an incomplete policy
for this RMTDP. The RMTDP model evaluates different completions of this incomplete
policy and provides an optimally completed policy as feedback to the BDI system. Thus,
the RMTDP fills in the gaps in an incompletely specified BDI team plan optimally. Here
the gaps we concentrate on are the role allocations, but the method can be applied to other
key coordination decisions. By restricting the optimization to only role allocation decisions
and fixing the policy at all other points, we are able to come up with a restricted policy
space. We then use RMTDPs to effectively search this restricted space in order to find the
optimal role allocation.
While the restricted policy search is one key positive interaction in our hybrid approach,
the second interaction consists of a more efficient policy representation used for converting
a BDI team plan and interpreter into a corresponding policy (see Figure 1) and a new
algorithm for policy evaluation. In general, each agents policy in a distributed POMDP
is indexed by its observation history (Bernstein et al., 2000; Pynadath & Tambe, 2002).
369

fiNair & Tambe

However, in a BDI system, each agent performs its action selection based on its set of
privately held beliefs which is obtained from the agents observations after applying a belief
revision function. In order to evaluate the teams performance, it is sufficient in RMTDP to
index the agents policies by their belief state (represented here by their privately held beliefs)
instead of their observation histories. This shift in representation results in considerable
savings in the amount of time needed to evaluate a policy and in the space required to
represent a policy.
The third key interaction in our hybrid approach further exploits BDI team plan structure for increasing the efficiency of our RMTDP-based analysis. Even though RMTDP
policy space is restricted to filling in gaps in incomplete policies, many policies may result
given the large number of possible role allocations. Thus enumerating and evaluating each
possible policy for a given domain is difficult. Instead, we provide a branch-and-bound algorithm that exploits task decomposition among sub-teams of a team to significantly prune
the search space and provide a correctness proof and worst-case analysis of this algorithm.
In order to empirically validate our approach, we have applied RMTDP for allocation
in BDI teams in two concrete domains: mission rehearsal simulations (Tambe et al., 2000)
and RoboCupRescue (Kitano et al., 1999). We first present the (significant) speed-up
gained by our three interactions mentioned above. Next, in both domains, we compared
the role allocations found by our approach with state-of-the-art techniques that allocate
roles without uncertainty reasoning. This comparison shows the importance of reasoning
about uncertainty when determining the role allocation for complex multiagent domains. In
the RoboCupRescue domain, we also compared the allocations found with allocations chosen
by humans in the actual RoboCupRescue simulation environment. The results showed that
the role allocation technique presented in this article is capable of performing at human
expert levels in the RoboCupRescue domain.
The article is organized as follows: Section 2 presents background and motivation. In
Section 3, we introduce the RMTDP model and present key complexity results. Section
4 explains how a BDI team plan can be evaluated using RMTDP. Section 5 describes the
analysis methodology for finding the optimal role allocation, while Section 6 presents an
empirical evaluation of this methodology. In Section 7, we present related work and in
Section 8, we list our conclusions.

2. Background
This section first describes the two domains that we consider in this article: an abstract
mission rehearsal domain (Tambe et al., 2000) and the RoboCupRescue domain (Kitano
et al., 1999). Each domain requires us to allocate roles to agents in a team. Next, teamoriented programming (TOP), a framework for describing team plans is described in the
context of these two domains. While we focus on TOP, as discussed further in Section 7.1,
our techniques would be applicable in other frameworks for tasking teams (Stone & Veloso,
1999; Decker & Lesser, 1993).
2.1 Domains
The first domain that we consider is based on mission rehearsal simulations (Tambe et al.,
2000). For expository purposes, this has been intentionally simplified. The scenario is as
370

fiHybrid BDI-POMDP Framework for Multiagent Teaming

follows: A helicopter team is executing a mission of transporting valuable cargo from point
X to point Y through enemy terrain (see Figure 2). There are three paths from X to Y of
different lengths and different risk due to enemy fire. One or more scouting sub-teams must
be sent out (one for each path from X to Y), and the larger the size of a scouting sub-team
the safer it is. When scouts clear up any one path from X to Y, the transports can then
move more safely along that path. However, the scouts may fail along a path, and may
need to be replaced by a transport at the cost of not transporting cargo. Owing to partial
observability, the transports may not receive an observation that a scout has failed or that
a route has been cleared. We wish to transport the most amount of cargo in the quickest
possible manner within the mission deadline.
The key role allocation decision here is given a fixed number of helicopters, how should
they be allocated to scouting and transport roles? Allocating more scouts means that the
scouting task is more likely to succeed, but there will be fewer helicopters left that can
be used to transport the cargo and consequently less reward. However, allocating too few
scouts could result in the mission failing altogether. Also, in allocating the scouts, which
routes should the scouts be sent on? The shortest route would be preferable but it is more
risky. Sending all the scouts on the same route decreases the likelihood of failure of an
individual scout; however, it might be more beneficial to send them on different routes, e.g.
some scouts on a risky but short route and others on a safe but longer route.
Thus there are many role allocations to consider. Evaluating each is difficult because
role allocation must look-ahead to consider future implications of uncertainty, e.g. scout
helicopters can fail during scouting and may need to be replaced by a transport. Furthermore, failure or success of a scout may not be visible to the transport helicopters and hence
a transport may not replace a scout or transports may never fly to the destination.
scout
transports

X

route 1

route 2

Y

enemy gun
route 3

Figure 2: Mission rehearsal domain.

The second example scenario (see Figure 3), set up in the RoboCupRescue disaster
simulation environment (Kitano et al., 1999), consists of five fire engines at three different
fire stations (two each at stations 1 & 3 and the last at station 2) and five ambulances
stationed at the ambulance center. Two fires (in top left and bottom right corners of the
map) start that need to be extinguished by the fire engines. After a fire is extinguished,
ambulance agents need to save the surviving civilians. The number of civilians at each
371

fiNair & Tambe

location is not known ahead of time, although the total number of civilians in known. As
time passes, there is a high likelihood that the health of civilians will deteriorate and fires
will increase in intensity. Yet the agents need to rescue as many civilians as possible with
minimal damage to the buildings. The first part of the goal in this scenario is therefore to
first determine which fire engines to assign to each fire. Once the fire engines have gathered
information about the number of civilians at each fire, this is transmitted to the ambulances.
The next part of the goal is then to allocate the ambulances to a particular fire to rescue
the civilians trapped there. However, ambulances cannot rescue civilians until fires are fully
extinguished. Here, partial observability (each agent can only view objects within its visual
range), and uncertainty related to fire intensity, as well as location of civilians and their
health add significantly to the difficulty.

C1
F3
F2
F1
C2
A

Figure 3: RoboCupRescue Scenario: C1 and C2 denote the two fire locations, F1, F2 and
F3 denote fire stations 1, 2 and 3 respectively and A denotes the ambulance
center.

2.2 Team-Oriented Programming
The aim of the team-oriented programming (TOP) (Pynadath & Tambe, 2003; Tambe et al.,
2000; Tidhar, 1993b) framework is to provide human developers (or automated symbolic
planners) with a useful abstraction for tasking teams. For domains such as those described
in Section 2.1, it consists of three key aspects of a team: (i) a team organization hierarchy
consisting of roles; (ii) a team (reactive) plan hierarchy; and (iii) an assignment of roles
to sub-plans in the plan hierarchy. The developer need not specify low-level coordination
details. Instead, the TOP interpreter (the underlying coordination infrastructure) automatically enables agents to decide when and with whom to communicate and how to reallocate
372

fiHybrid BDI-POMDP Framework for Multiagent Teaming

roles upon failure. The TOP abstraction enables humans to rapidly provide team plans for
large-scale teams, but unfortunately, only a qualitative assessment of team performance is
feasible. Thus, a key TOP weakness is the inability to quantitatively evaluate and optimize
team performance, e.g., in allocating roles to agents only a qualitative matching of capabilities may be feasible. As discussed later, our hybrid BDI-POMDP model addresses this
weakness by providing techniques for quantitative evaluation.
As a concrete example, consider the TOP for the mission rehearsal domain. We first
specify the team organization hierarchy (see Figure 4(a)). Task Force is the highest level
team in this organization and consists of two roles Scouting and Transport, where the
Scouting sub-team has roles for each of the three scouting sub-sub-teams. Next we specify
a hierarchy of reactive team plans (Figure 4(b)). Reactive team plans explicitly express
joint activities of the relevant team and consist of: (i) pre-conditions under which the plan
is to be proposed; (ii) termination conditions under which the plan is to be ended; and (iii)
team-level actions to be executed as part of the plan (an example plan will be discussed
shortly). In Figure 4(b), the highest level plan Execute Mission has three sub-plans:
DoScouting to make one path from X to Y safe for the transports, DoTransport to
move the transports along a scouted path, and RemainingScouts for the scouts which
have not reached the destination yet to get there.
Execute Mission [Task Force]
DoScouting
[Task Force]

RemainingScouts
DoTransport
[Scouting Team] [Transport Team]

Task Force
ScoutRoutes
WaitAtBase
[Transport Team] [Scouting Team]

Scouting Team

Transport Team

SctTeamA SctTeamB SctTeamC

ScoutRoute1 ScoutRoute2 ScoutRoute3
[SctTeamA] [SctTeamB] [SctTeamC]

(a)

(b)

Figure 4: TOP for mission rehearsal domain a: Organization hierarchy; b: Plan hierarchy.

Figure 4(b) also shows coordination relationships: An AND relationship is indicated
with a solid arc, while an OR relationship is indicated with a dashed arc. Thus, WaitAtBase and ScoutRoutes must both be done while at least one of ScoutRoute1,
ScoutRoute2 or ScoutRoute3 need be performed. There is also a temporal dependence relationship among the sub-plans, which implies that sub-teams assigned to perform
DoTransport or RemainingScouts cannot do so until the DoScouting plan has completed. However, DoTransport and RemainingScouts execute in parallel. Finally, we
assign roles to plans  Figure 4(b) shows the assignment in brackets adjacent to the plans.
For instance, Task Force team is assigned to jointly perform Execute Mission while SctTeamA is assigned to ScoutRoute1.
The team plan corresponding to Execute Mission is shown in Figure 5. As can be
seen, each team plan consists of a context, pre-conditions, post-conditions, body and constraints. The context describes the conditions that must be fulfilled in the parent plan while
the pre-conditions are the particular conditions that will cause this sub-plan to begin exe373

fiNair & Tambe

cution. Thus, for Execute Mission, the pre-condition is that the team mutually believes
(MB)1 that they are the start location. The post-conditions are divided into Achieved,
Unachievable and Irrelevant conditions under which this sub-plan will be terminated. The
body consists of sub-plans that exist within this team plan. Lastly, constraints describe any
temporal constraints that exist between sub-plans in the body. The description of all the
plans in the plan hierarchy of Figure 4(b) is given in Appendix A.
ExecuteMission:
Context:
Pre-conditions: (MB <TaskForce> location(TaskForce) = START)
Achieved: (MB <TaskForce> (Achieved(DoScouting)  Achieved(DoTransport)))  (time
> T  (MB <TaskForce>
Achieved(RemainingScouts)  ( helo  ScoutingTeam, alive(helo) 
location(helo) 6= END)))
Unachievable: (MB <TaskForce> Unachievable(DoScouting))  (MB <TaskForce>
Unachievable(DoTransport) 
(Achieved(RemainingScouts) ( helo  ScoutingTeam, alive(helo) 
location(helo) 6= END)))
Irrelevant: 
Body:
DoScouting
DoTransport
RemainingScouts
Constraints: DoScouting  DoTransport, DoScouting  RemainingScouts

Figure 5: Example team plan. MB refers to mutual belief.
Just as in HTN (Dix, Muoz-Avila, Nau, & Zhang, 2003; Erol, Hendler, & Nau, 1994), the
plan hierarchy of a TOP gives a decomposition of the task into smaller tasks. However, the
language of TOPs is richer than the language of early HTN planning (Erol et al., 1994) which
contained just simple ordering constraints. As seen in the above example, the plan hierarchy
in TOPs can also contain relationships like AND and OR. In addition, just like more recent
work in HTN planning (Dix et al., 2003), sub-plans in TOPs can contain pre-conditions and
post-conditions, thus allowing for conditional plan execution. The main differences between
TOPs and HTN planning are: (i) TOPs contain an organization hierarchy in addition to a
plan hierarchy, (ii) the TOP interpreter ensures that the team executes its plans coherently.
As seen later, TOPs will be analyzed with all of this expressiveness including conditional
execution; however, since our analysis will focus on a fixed time horizon, any loops in the
task description will be unrolled up to the time horizon.
1. Mutual Belief (Wooldridge, 2002), shown as (MB hteami x) in Figure 5, refers to a private belief held
by each agent in the team that they each believe that a fact x is true, and that each of the other agents
in the team believe that x is true, and that every agent believes that every other agent believes that x
is true and so on. Such infinite levels of nesting are difficult to realize in practice. Thus, as in practical
BDI implementations, for the purposes of this article, a mutual belief is approximated to be a private
belief held by an agent that all the agents in the team believe that x is true.

374

fiHybrid BDI-POMDP Framework for Multiagent Teaming

new observation for
agent i

Belief Update
function

Private beliefs of
agent i

Figure 6: Mapping of observations to beliefs.

During execution, each agent has a copy of the TOP. The agent also maintains a set
of private beliefs, which are a set of propositions that the agent believes to be true (see
Figure 6). When an agent receives new beliefs, i.e. observations (including communication),
the belief update function is used to update its set of privately held beliefs. For instance,
upon seeing the last scout crashed, a transport may update its privately held beliefs to
include the belief CriticalFailure(DoScouting). In practical BDI systems, such belief
update computation is of low complexity (e.g. constant or linear time). Once beliefs
are updated, an agent selects which plan to execute by matching its beliefs with the preconditions in the plans. The basic execution cycle is similar to standard reactive planning
systems such as PRS (Georgeff & Lansky, 1986).
During team plan execution, observations in the form of communications often arise
because of the coordination actions executed by the TOP interpreter. For instance, TOP
interpreters have exploited BDI theories of teamwork, such as Levesque et al.s theory
of joint intentions (Levesque, Cohen, & Nunes, 1990) which require that when an agent
comes to privately believe a fact that terminates the current team plan (i.e. matches the
achievement or unachievability conditions of a team plan), then it communicates this fact
to the rest of the team. By performing such coordination actions automatically, the TOP
interpreter enables coherence at the initiation and termination of team plans within a TOP.
Some further details and examples of TOPs can be seen in the work of Pynadath and
Tambe (2003), Tambe et al. (2000) and Tidhar (1993b).
We can now more concretely illustrate the key challenges in role allocation mentioned
earlier. First, a human developer must allocate available agents to the organization hierarchy (Figure 4(a)), to find the best role allocation. However, there are combinatorially many
allocations to choose from (Hunsberger & Grosz, 2000; Tambe et al., 2000). For instance,
starting with just 6 homogeneous helicopters results in 84 different ways of deciding how
many agents to assign to each scouting and transport sub-team. This problem is exacerbated by the fact that the best allocation varies significantly based on domain variations.
For example, Figure 7 shows three different assignments of agents to the team organization hierarchy, each found in our analysis to be the best for a given setting of failure and
observation probabilities (details in Section 6). For example, increasing the probability of
failures on all routes resulted in the number of transports in the best allocation changing
from four (see Figure 7(b)) to three (see Figure 7(a)), where an additional scout was added
to SctTeamB. If failures were not possible at all, the number of transports increased to
five (see Figure 7(c)). Our analysis takes a step towards selecting the best among such
allocations.
375

fiNair & Tambe

Task Force
Scouting Team

Task Force

Transport Team=3

Scouting Team

SctTeamA=2 SctTeamB=1 SctTeamC=0

Transport Team=4

SctTeamA=2 SctTeamB=0 SctTeamC=0

(a) Medium probability

(b) Low probability
Task Force

Scouting Team

Transport Team=5

SctTeamA=0 SctTeamB=0 SctTeamC=1

(c) Zero probability

Figure 7: Best role allocations for different probabilities of scout failure.

Figure 8 shows the TOP for the RoboCupRescue scenario. As can be seen, the plan hierarchy for this scenario consists of a pair of ExtinguishFire and RescueCivilians plans
done in parallel, each of which further decompose into individual plans. (These individual plans get the fire engines and ambulances to move through the streets using specific
search algorithms, however, these individual plans are not relevant for our discussions in
this article; interested readers should refer to the description of our RoboCupRescue team
entered into the RoboCup competitions of 2001 (Nair, Ito, Tambe, & Marsella, 2002).) The
organizational hierarchy consists of Task Force comprising of two Engine sub-teams, one
for each fire and an Ambulance Team, where the engine teams are assigned to extinguishing
the fires while the ambulance team is assigned to rescuing civilians. In this particular TOP,
the assignment of ambulances to AmbulanceTeamA and AmbulanceTeamB is conditioned
on the communication c, indicated by AmbulanceTeamA|c and AmbulanceTeamB|c.
c is not described in detail in this figure, but it refers to the communication that is received from the fire engines that describes the number of civilians present at each fire. The
problem is which engines to assign to each Engine Team and for each possible value of c,
which ambulances to assign to each Ambulance Team. Note that engines have differing
capabilities owing to differing distances from fires while all the ambulances have identical
capabilities.
Task Force
EngineTeamA

EngineTeamB

AmbulanceTeam

AmbulanceTeamA |c

AmbulanceTeamB |c

(a)
ExecuteMission
[Task Force]
ExtinguishFire1
[EngineTeamA]

RescueCivilians1
[AmbulanceTeamA]

ExtinguishFire2
[EngineTeamB]

RescueCivilians2
[AmbulanceTeamB]

(b)

Figure 8: TOP for RoboCupRescue scenario a: Organization hierarchy; b: Plan hierarchy.

376

fiHybrid BDI-POMDP Framework for Multiagent Teaming

3. Role-based Multiagent Team Decision Problem
Multiagent Team Decision Problem (MTDP) (Pynadath & Tambe, 2002) is inspired by
the economic theory of teams (Marschak & Radner, 1972; Ho, 1980; Yoshikawa, 1978).
In order to do quantitative analysis of key coordination decisions in multiagent teams, we
extend MTDP for the analysis of the coordination actions of interest. For example, the
COM-MTDP (Pynadath & Tambe, 2002) is an extension of MTDP for the analysis of communication. In this article, we illustrate a general methodology for analysis of other aspects
of coordination and present the RMTDP model for quantitative analysis of role allocation
and reallocation as a concrete example. In contrast to BDI systems introduced in the previous section, RMTDP enables explicit quantitative optimization of team performance. Note
that, while we use MTDP, other possible distributed POMDP models could potentially also
serve as a basis (Bernstein et al., 2000; Xuan et al., 2001).
3.1 Multiagent Team Decision Problem
Given a team of n agents, an MTDP (Pynadath & Tambe, 2002) is defined as a tuple:
hS, A, P, , O, Ri. It consists of a finite set of states S = 1      m where each j ,
1  j  m, is a feature of the world state. Each agent i can perform an action from its
set of actions Ai , where 1in Ai = A. P (s, < a1 , . . . , an >, s ) gives the probability of
transitioning from state s to state s given that the agents perform the actions < a1 , . . . , an >
jointly. Each agent i receives an observation i  i (1in i = ) based on the function
O(s, < a1 , . . . , an >, 1 , . . . , n ), which gives the probability that the agents receive the
observations, 1 , . . . , n given that the world state is s and they perform < a1 , . . . , an >
jointly. The agents receive a single joint reward R(s, < a1 , . . . , an >) based on the state s
and their joint action < a1 , . . . , an >. This joint reward is shared equally by all members
and there is no other private reward that individual agents receive for their actions. Thus,
the agents are motivated to behave as a team, taking the actions that jointly yield the
maximum expected reward.
Each agent i in an MTDP chooses its actions based on its local policy, i , which is a
mapping of its observation history to actions. Thus, at time t, agent i will perform action
i (i0 , . . . , it ). This contrasts with a single-agent POMDP, where we can index an agents
policy by its belief state  a probability distribution over the world state (Kaelbling, Littman,
& Cassandra, 1998), which is shown to be a sufficient statistic in order to compute the
optimal policy (Sondik, 1971). Unfortunately, we cannot directly use single-agent POMDP
techniques (Kaelbling et al., 1998) for maintaining or updating belief states (Kaelbling et al.,
1998) in a MTDP  unlike in a single agent POMDP, in MTDP, an agents observation
depends not only on its own actions, but also on unknown actions of other agents. Thus,
as with other distributed POMDP models (Bernstein et al., 2000; Xuan et al., 2001), in
MTDP, local policies i are indexed by observation histories.  =< 1 , . . . , n > refers to
the joint policy of the team of agents.
3.2 Extension for Explicit Coordination
Beginning with MTDP, the next step in our methodology is to make an explicit separation
between domain-level actions and the coordination actions of interest. Earlier work intro377

fiNair & Tambe

duced the COM-MTDP model (Pynadath & Tambe, 2002), where the coordination action
was fixed to be the communication action, and got separated out. However, other coordination actions could also be separated from domain-level actions in order to investigate their
impact. Thus, to investigate role allocation and reallocations, actions for allocating agents
to roles and to reallocate such roles are separated out. To that end, we define RMTDP
(Role-based Multiagent Team Decision Problem) as a tuple hS, A, P, , O, R, RLi with a
new component, RL. In particular, RL = {r1 , . . . , rs } is a set of all roles that the agents
can undertake. Each instance of role rj may be assigned some agent i to fulfill it. The
actions of each agent are now distinguishable into two types:
Role-Taking actions: i = {irj } contains the role-taking actions for agent i. irj  i
means that agent i takes on the role rj  RL.
S
Role-Execution Actions: i = rj RL irj contains the execution actions for agent i
where irj is the set of agent is actions for executing role rj  RL
In addition we define the set of states as S = 1      m  roles, where the feature roles (a vector) gives the current role that each agent has taken on. The reason for
introducing this new feature is to assist us in the mapping from a BDI team plan to an
RMTDP. Thus each time an agent performs a new role-taking action successfully, the value
of the feature roles will be updated to reflect this change. The key here is that we not only
model an agents initial role-taking action but also subsequent role reallocation. Modeling
both allocation and reallocation is important for an accurate analysis of BDI teams. Note
that an agent can observe the part of this feature pertaining to its own current role but it
may not observe the parts pertaining to other agents roles.
The introduction of roles allows us to represent the specialized behaviors associated
with each role, e.g. a transport vs. a scout role. While filling a particular role, rj , agent
i can perform only role-execution actions,   irj , which may be different from the roleexecution actions irl for role rl . Thus, the feature roles is used to filter actions such that
only those role-execution actions that correspond to the agents current role are permitted.
In the worst case, this filtering does not affect the computational complexity (see Theorem 1
below) but in practice, it can significantly improve performance when trying to find the
optimal policy for the team, since the number of domain actions that each agent can choose
from is restricted by the role that the agent has taken on. Also, these different roles can
produce varied effects on the world state (modeled via transition probabilities, P ) and the
teams reward. Thus, the policies must ensure that agents for each role have the capabilities
that benefit the team the most.
Just as in MTDP, each agent chooses which action to perform by indexing its local policy
i by its observation history. In the same epoch some agents could be doing role-taking
actions while others are doing role-execution actions. Thus, each agents local policy i can
be divided into local role-taking and role-execution policies such that for all observation
histories, i0 , . . . , it , either i (i0 , . . . , it ) = null or i (i0 , . . . , it ) = null.  =<
1 , . . . , n > refers to the joint role-taking policy of the team of agents while  =<
1 , . . . , n > refers to the joint role-execution policy.
378

fiHybrid BDI-POMDP Framework for Multiagent Teaming

In this article we do not explicitly model communicative actions as a special action.
Thus communication is treated like any other role-execution action and the communication
received from other agents are treated as observations.2
3.3 Complexity Results with RMTDP
While Section 2.2 qualitatively emphasized the difficulty of role allocation, RMTDP helps
us in understanding the complexity more precisely. The goal in RMTDP is to come up with
joint policies  and  that will maximize the total expected reward over a finite horizon
T . Note that agents can change their roles according to their local role-taking policies. The
agents role-execution policy subsequent to this change would contain actions pertaining to
this new role. The following theorem illustrates the complexity of finding such optimal joint
policies.
Theorem 1 The decision problem of determining if there exist policies,  and  , for an
RMTDP, that yield an expected reward of at least K over some finite horizon T is NEXPcomplete.
Proof sketch: Proof follows from the reduction of MTDP (Pynadath & Tambe, 2002)
to/from RMTDP. To reduce MTDP to RMTDP, we set RMTDPs role taking actions,  ,
to null and set the RMTDPs role-execution actions,  , to the MTDPs set of actions, A.
To reduce RMTDP
to MTDP, we generate a new MTDP such that its set of actions, A
S
is equal to  . Finding the required policy in MTDP is NEXP-complete (Pynadath &
Tambe, 2002).
As this theorem shows us, solving the RMTDP for the optimal joint role-taking and roleexecution policies over even a finite horizon is highly intractable. Hence, we focus on the
complexity of just determining the optimal role-taking policy, given a fixed role-execution
policy. By fixed role-execution policy, we mean that the action selection of an agent is
predetermined by the role it is executing.
Theorem 2 The decision problem of determining if there exists a role-taking policy,  , for
an RMTDP, that yields an expected reward of at least K together with a fixed role-execution
policy  , over some finite horizon T is NEXP-complete.
Proof sketch: We reduce an MTDP to an RMTDP with a different role-taking and a
role-execution action corresponding to each action in the MTDP. Hence, in the RMTDP we
have a role-taking action irj for agent i to take on role rj created for each action aj  Ai in
the MTDP and each such role rj contains a single role-execution action, i.e. |irj | = 1. For
the RMTDP, construct the transition function to be such that a role-taking action always
succeeds and the only affected state feature is roles . For the role-execution action   irj ,
the transition probability is the same as that of the MTDP action, aj  Ai corresponding
to the last role-taking action irj . The fixed role-execution policy is to simply perform
the action,   irj , corresponding to the last successful role-taking action, irj . Thus,
the decision problem for an RMTDP with a fixed role-execution policy is at least as hard
2. For a more explicit analysis of communication please refer to work done by Pynadath and Tambe (2002)
and Goldman et al. (2003).

379

fiNair & Tambe

as the decision problem for an MTDP. Furthermore, given Theorem 1, we can conclude
NEXP-Completeness.
This result suggests that even by fixing the role-execution policy, solving the RMTDP for
the optimal role-taking policy is still intractable. Note that Theorem 2 refers to a completely
general globally optimal role-taking policy, where any number of agents can change roles at
any point in time. Given the above result, in general the globally optimal role-taking policy
will likely be of doubly exponential complexity, and so we may be left no choice but to run
a brute-force policy search, i.e. to enumerate all the role-taking policies and then evaluate
them, which together determinethe run-time of finding the globally optimal policy. The
number of policies is

||

||T 1
||1

n

, i.e. doubly exponential in the number of observation

histories and the number of agents. Thus, while RMTDP enables quantitative evaluation of
teams policies, computing optimal policies is intractable; furthermore, given its low level of
abstraction, in contrast to TOP, it is difficult for a human to understand the optimal policy.
This contrast between RMTDP and TOP is at the root of our hybrid model described in
the following section.

4. Hybrid BDI-POMDP Approach
Having explained TOP and RMTDP, we can now present a more detailed view of our
hybrid methodology to quantitatively evaluate a TOP. We first provide a more detailed
interpretation of Figure 1. BDI team plans are essentially TOP plans, while the BDI
interpreter is the TOP coordination layer. As shown in Figure 1, an RMTDP model is
constructed corresponding to the domain and the TOP and its interpreter are converted
into a corresponding (incomplete) RMTDP policy. We can then analyze the TOP using
analysis techniques that rely on evaluating the RMTDP policy using the RMTDP model
of the domain.
Thus, our hybrid approach combines the strengths of the TOPs (enabling humans to
specify TOPs to coordinate large-scale teams) with the strengths of RMTDP (enabling
quantitative evaluation of different role allocations). On the one hand, this synergistic
interaction enables RMTDPs to improve the performance of TOP-based BDI teams. On
the other hand, we have identified at least six specific ways in which TOPs make it easier
to build RMTDPs and to efficiently search RMTDP policies: two of which are discussed in
this section, and four in the next section. In particular, the six ways are:
1. TOPs are exploited in constructing RMTDP models of the domain (Section 4.1);
2. TOPs are exploited to present incomplete policies to RMTDPs, restricting the RMTDP
policy search (Section 5.1);
3. TOP belief representation is exploited in enabling faster RMTDP policy evaluation
(Section 4.2);
4. TOP organization hierarchy is exploited in hierarchically grouping RMTDP policies
(Section 5.1);
5. TOP plan hierarchy is exploited in decomposing RMTDPs (Section 5.3);
380

fiHybrid BDI-POMDP Framework for Multiagent Teaming

6. TOP plan hierarchies are also exploited in cutting down the observation or belief
histories in RMTDPs (Section 5.3).
The end result of this efficient policy search is a completed RMTDP policy that improves
TOP performance. While we exploit the TOP framework, other frameworks for tasking
teams, e.g. Decker and Lesser (1993) and Stone and Veloso (1999) could benefit from a
similar synergistic interaction.
4.1 Guidelines for Constructing an RMTDP
As shown in Figure 1, our analysis approach uses as input an RMTDP model of the domain,
as well as an incomplete RMTDP policy. Fortunately, not only does the TOP serve as a
direct mapping to the RMTDP policy, but it can also be utilized in actually constructing
the RMTDP model of the domain. In particular, the TOP can be used to determine which
domain features are important to model. In addition, the structure in the TOP can be
exploited in decomposing the construction of the RMTDP.
The elements of the RMTDP tuple, hS, A, P, , O, R, RLi, can be defined using a procedure that relies on both the TOP as well as the underlying domain. While this procedure
is not automated, our key contribution is recognizing the exploitation of TOP structures
in constructing the RMTDP model. First, in order to determine the set of states, S, it
is critical to model the variables tested in the pre-conditions, termination conditions and
context of all the components (i.e. sub-plans) in the TOP. Note that a state only needs
to model the features tested in the TOP; if a TOP pre-condition expresses a complex test
on the feature, that test is not modeled in the state, but instead gets used in defining the
incomplete policy input to RMTDP. Next we define the set of roles, RL, as the leaf-level
roles in the organization hierarchy of the TOP. Furthermore, as specified in Section 3.2, we
define a state feature roles as a vector containing the current role for each agent. Having
defined RL and roles , we now define the actions, A as follows. For each role rj  RL, we
define a corresponding role-taking action, irj which will succeed or fail depending on the
agent i that performs the action and the state s that the action was performed in. The
role-execution actions, irj for agent i in role rj , are those allowed for that role according
to the TOP.
Thus, we have defined S, A and RL based on the TOP. To illustrate these steps, consider
the plans in Figure 4(b). The pre-conditions of the leaf-level plan ScoutRoute1 (See
Appendix A), for instance, tests start location of the helicopters to be at start location X,
while the termination conditions test that scouts are at end location Y. Thus, the locations
of the helicopters are modeled as features in the set of states in the RMTDP. Using the
organization hierarchy, we define the set of roles RL with a role corresponding to each of the
four different kinds of leaf-level roles, i.e. RL = {memberSctT eamA, memberSctT eamB,
memberSctT eamC, memberT ransportT eam}. The role-taking and role-execution actions
can be defined as follows:
 A role-taking action is defined corresponding to each of the four roles in RL, i.e.
becoming a member of one of the three scouting teams or of the transport team. The
domain specifies that only a transport can change to a scout and thus the role-taking
action, jointTransportTeam, will fail for agent i, if the current role of agent i is a scout.
381

fiNair & Tambe

 Role-execution actions are obtained from the TOP plans corresponding to the agents
role. In the mission rehearsal scenario, an agent, fulfilling a scout role (members
of SctTeamA, SctTeamB or SctTeamC), always goes forward, making the current
position safe, until it reaches the destination and so the only execution action we will
consider is move-making-safe. An agent in a transport role (members of Transport
Team) waits at X until it obtains observation of a signal that one scouting sub-team
has reached Y and hence the role-execution actions are wait and move-forward.
We must now define , P, O, R. We obtain the set of observations i for each agent i
directly from the domain. For instance, the transport helos may observe the status of scout
helos (normal or destroyed), as well as a signal that a path is safe. Finally, determining
the functions, P, O, R requires some combination of human domain expertise and empirical
data on the domain behavior. However, as shown later in Section 6, even an approximate
model of transitional and observational uncertainty is sufficient to deliver significant benefits. Defining the reward and transition function may sometimes require additional state
variables to be modeled, if they were only implicitly modeled in the TOP. In the mission
rehearsal domain, the time at which the scouting and transport mission were completed
determined the amount of reward. Thus, time was only implicitly modeled in the TOP and
needed to be explicitly modeled in the RMTDP.
Since we are interested in analyzing a particular TOP with respect to uncertainty, the
procedure for constructing an RMTDP model can be simplified by exploiting the hierarchical decomposition of the TOP in order to decompose the construction of the RMTDP
model. The high-level components of a TOP often represent plans executed by different
sub-teams, which may only loosely interact with each other. Within a component, the
sub-team members may exhibit a tight interaction, but our focus is on the loose coupling
across components, where only the end results of one component feed into another, or the
components independently contribute to the team goal. Thus, our procedure for constructing an RMTDP exploits this loose coupling between components of the plan hierarchy in
order to build an RMTDP model represented as a combination of smaller RMTDPs (factors). Note that if such decomposition is infeasible, our approach still applies except that
the benefits of the hierarchical decomposition will be unavailable.
We classify sibling components as being either parallel or sequentially executed (contains a temporal constraint). Components executed in parallel could be either independent
or dependent. For independent components, we can define RMTDPs for each of these
components such that the sub-team executing one component cannot affect the transitions, observations and reward obtained by the sub-teams executing the other components. The procedure for determining the elements of the RMTDP tuple for component k,
hSk , Ak , Pk , k , Ok , Rk , RLk i, is identical to the procedure described earlier for constructing
the overall RMTDP. However, each such component has a smaller set of relevant variables
and roles and hence specifying the elements of its corresponding RMTDP is easier.
We can now combine the RMTDPs of the independent components to obtain the
RMTDP corresponding to the higher-level component. For a higher level component l,
whose child
S components are independent, the set of states, Sl = x FSl x such that
FSl = k s.t. Child(k,l)=true FSk where FSl and FSk are the sets of features for the set
of states Sl and set of states Sk . A state sl  Sl is said to correspond to the state
sk  Sk if x  FSk , sl [x ] = sk [x ], i.e. the state sl has the same value as state sk
382

fiHybrid BDI-POMDP Framework for Multiagent Teaming

for
is defined as follows, Pl (sl , al , sl ) =
Q all features of state sk . The transition function

k s.t. Child(k,l)=true Pk (sk , ak , sk ), where sl and sl of component l corresponds to states
sk and sk of component k and ak is the joint action performed by the sub-team assigned to component k corresponding to the joint action al performed by the sub-team
assigned
to component l. The observation function is defined similarly as Ol (sl , al , l ) =
Q
Ok (sk , ak , k ). The reward function for component l is defined as
k s.t. Child(k,l)=true
P
Rl (sl , al ) = k s.t. Child(k,l)=true Rk (sk , ak ).

In the case of sequentially executed components (those connected by a temporal constraint), the components are loosely coupled since the end states of the preceding component
specify the start states of the succeeding component. Thus, since only one component is
active at a time, the transition function is defined as follows, Pl (sl , al , sl ) = Pk (sk , ak , sk ),
where component k is the only active child component, sk and sk represent the states of
component k corresponding to states sl and sl of component l and ak is the joint action
performed by the sub-team assigned to component k corresponding to the joint action
al performed by the sub-team corresponding to component l. Similarly, we can define
Ol (sl , al , l ) = Ok (sk , ak , k ) and Rl (sl , al ) = Rk (sk , ak ), where k is the only active child
component.

Consider the following example from the mission rehearsal domain where components
exhibit both sequential dependence and parallel independence. Concretely, the component
DoScouting is executed first followed by DoTransport and RemainingScouts, which
are parallel and independent and hence, either DoScouting is active or DoTransport and
RemainingScouts are active at any point in the execution. Hence, the transition, observation and reward functions of their parent Execute Mission is given by the corresponding
functions of either DoScouting or by the combination of the corresponding functions of
DoTransport and RemainingScouts.
We use a top-down approach in order to determine how to construct a factored RMTDP
from the plan hierarchy. As shown in Algorithm 1, we replace a particular sub-plan by its
constituent sub-plans if they are either independent or sequentially executed. If not, then
the RMTDP is defined using that particular sub-plan. This process is applied recursively
starting at the root component of the plan hierarchy. As a concrete example, consider
again our mission rehearsal simulation domain and the hierarchy illustrated in Figure 4(b).
Given the temporal constraints between DoScouting and DoTransport, and DoScouting and RemainingScouts, we exploited sequential decomposition, while DoTransport
and RemainingScouts were parallel and independent components. Hence, we can replace
ExecuteMission by DoScouting, DoTransport and RemainingScouts. We then apply the same process to DoScouting. The constituent components of DoScouting are
neither independent nor sequentially executed and thus DoScouting cannot be replaced by
its constituent components. Thus, RMTDP for the mission rehearsal domain is comprised
of smaller RMTDPs for DoScouting, DoTransport and RemainingScouts.
Thus, using the TOP to identify relevant variables and building a factored RMTDP
utilizing the structure of TOP to decompose the construction procedure, reduce the load
on the domain expert for model construction. Furthermore, as shown in Section 5.3, this
factored model greatly improves the performance of the search for the best role allocation.
383

fiNair & Tambe

Algorithm 1 Build-RMTDP(TOP top, Sub-plan subplan)
1: children  subplanchildren() {subplanchildren() returns the sub-plans within subplan}
2: if children = null or children are not (loosely coupled or independent) then
3:
rmtdp  Define-RMTDP(subplan) {not automated}
4:
return rmtdp
5: else
6:
for all child in children do
7:
factors[child]  Build-RMTDP(top,child)
8:
rmtdp  ConstructFromFactors(factors)
9:
return rmtdp

4.2 Exploiting TOP Beliefs in Evaluation of RMTDP Policies
We now present a technique for exploiting TOPs in speeding up evaluation of RMTDP
policies. Before we explain our improvement, we first describe the original algorithm for
determining the expected reward of a joint policy, where the local policies of each agent
are indexed by its entire observation histories (Pynadath & Tambe, 2002; Nair, Pynadath,
Yokoo, Tambe, & Marsella, 2003a). Here, we obtain an RMTDP policy from a TOP as
follows. We obtain i (~
it ), i.e. the action performed by agent i for each observation history
t
~i , as the action a performed by the agent i following the TOP when it has a set of privately
held beliefs corresponding to the observation history, ~it . We compute the expected reward
for the RMTDP policy by projecting the teams execution over all possible branches on
different world states and different observations. At each time step, we can compute the
expected value of a joint policy,  =< 1 , . . . , n >, for a team starting in a given state, st ,
with a given set of past observations, 
~ 1t , . . . , 
~ nt , as follows:
X


ff
ff





ff

~ nt ) = R(st , 1 (~
Vt (st , ~1t , . . . , 
1t ), . . . , n (~nt ) ) +
P s t , 1 
~ 1t , . . . , n 
~ nt , st+1
st+1 S

X



ff 

ff


ff
 O st+1, 1 
~ 1t , . . . , n 
~ nt , 1t+1, . . . , nt+1  Vt+1 st+1 , ~1t+1 , . . . , 
~ nt+1

(1)

t+1 

The expected reward of a joint policy  is given by V0 (s0 , < null, . . . , null >) where s0
is the start state. At each time step t, the computation of Vt performs a summation over all
possible world states and agent observations and so has a time complexity of O (|S|  ||).
This computation
is repeated for all states and all observation histories of length t, i.e.

O |S|  ||t times. Therefore,
given a time horizon T , the overall complexity of this algo
2
T
+1
rithm is O |S|  ||
.
As discussed in Section 2.2, in a team-oriented program, each agents action selection
is based on just its currently held private beliefs (note that mutual beliefs are modeled
as privately held beliefs about all agents as per footnote 2). A similar technique can be
exploited when mapping TOP to an RMTDP policy. Indeed, the evaluation of a RMTDP
policy that corresponds to a TOP can be speeded up if each agents local policy is indexed
by its private beliefs, i t . We refer to i t , as the TOP-congruent belief state of agent i
384

fiHybrid BDI-POMDP Framework for Multiagent Teaming

in the RMTDP. Note that this belief state is not a probability distribution over the world
states as in a single agent POMDP, but rather the privately held beliefs (from the BDI
program) of agent i at time t. This is similar to the idea of representing a policy by a
finite-state controller (Hansen & Zhou, 2003; Poupart & Boutilier, 2003). In this case, the
private beliefs would map to the states of the finite-state controller.
Belief-based RMTDP policy evaluation leads to speedup because multiple observation
histories map to the same belief state, i t . This speedup is a key illustration of exploitation
of synergistic interactions of TOP and RMTDP. In this instance, belief representation techniques used in TOP are reflected in RMTDP, and the resulting faster policy evaluation can
help us optimize TOP performance. A detailed example of belief state is presented later
after a brief explanation of how such belief-based RMTDP policies can be evaluated.
Just as with evaluation using observation histories, we compute the expected reward
of a belief-based policy by projecting the teams execution over all possible branches on
different world states and different observations. At each time step, we can compute the
expected value of a joint policy,  =< 1 , . . . , n >, for a team starting in a given state, st ,
with a given team belief state, < 1t , . . . , nt > as follows:


ff


ff X

ff



Vt (st , 1t . . . nt ) = R(st , 1 (1t ), . . . , n (nt ) ) + P st , 1 1t , . . . , n nt , st+1
st+1 S

X
ff 


ff




ff
 O st+1, 1 1t , . . . , n nt , 1t+1, . . . , nt+1  Vt+1 st+1 , 1t+1 , . . . , nt+1

t+1 

(2)
where i

t+1

= BeliefUpdateFunction

t

i , it+1



The complexity of computing this function (expression 2) is O (|S|  ||)  BF , where BF
represents the complexity of the belief update function, BeliefUpdateFunction. At each
time step the computation of the value function is done for every state and for all possible
reachable belief states. Let |i | = max1tT (|it |) represent the maximum number of
possible belief states that agent i can be in at any point in time, where |it | is the number
of belief states that agent i can be in at t. Therefore the complexity of this algorithm is
given by O(|S|2  ||  (|1 |  . . .  |n |)  T )  BF . Note that, in this algorithm T is not in
the exponent unlike in the algorithm in expression 1. Thus, this evaluation method will
give large time savings if: (i) the quantity (|1 |  . . .  |n |)  T is much less than ||T and
(ii) the belief update cost is low. In practical BDI systems, multiple observation histories
map often onto the same belief state, and thus usually, (|1 |  . . .  |n |)  T is much less
than ||T . Furthermore, since the belief update function mirrors practical BDI systems,
its complexity is also a low polynomial or a constant. Indeed, our experimental results
show that significant speedups result from switching to our TOP-congruent belief states
i t . However, in the absolute worst case, the belief update function may simply append
the new observation to the history of past observations (i.e., TOP-congruent beliefs will
be equivalent to keeping entire observation histories) and thus belief-based evaluation will
have the same complexity as the observation history-based evaluation.
We now turn to an example of belief-based policy evaluation from the mission rehearsal
domain. At each time step, the transport helicopters may receive an observation about
385

fiNair & Tambe

whether a scout has failed based on some observation function. If we use the observationhistory representation of the policy, then each transport agent would maintain a complete
history of the observations that it could receive at each time step. For example, in a setting
with two scout helicopters, one on route 1 and the other on route 2, a particular transport
helicopter may have several different observation histories of length two. At every time step,
the transports may receive an observation about each scout being alive or having failed.
Thus, at time t = 2, a transport helicopter might have one of the following observation histories of length two, < {sct1OnRoute1Alive, sct2OnRoute2Alive}1 , {sct1OnRoute1F ailed,
sct2OnRoute2F ailed}2 >, < {sct1OnRoute1Alive, sct2OnRoute2F ailed}1 , {sct1OnRoute1
F ailed}2 >, < {sct1OnRoute1F ailed, sct2OnRoute2Alive}1 , {sct2OnRoute2F ailed}2 >,
etc. However, the action selection of the transport helicopters depends on only whether
a critical failure (i.e. the last remaining scout has crashed) has taken place to change its
role. Whether a failure is critical can be determined by passing each observation through
a belief-update function. The exact order in which the observations are received or the
precise times at which the failure or non-failure observations are received are not relevant
to determining if a critical failure has taken place and consequently whether a transport
should change its role to a scout. Thus, many observation histories map onto the same
belief states. For example, the above three observation histories all map to the same belief
CriticalF ailure(DoScouting) i.e. a critical failure has taken place. This results in significant speedups using belief-based evaluation, as Equation 2 needs to be executed over a
smaller number of belief states, linear in T in our domains, as opposed to the observation
history-based evaluation, where Equation 1 is executed over an exponential number of observation histories (||T ). The actual speedup obtained in the mission rehearsal domain is
demonstrated empirically in Section 6.

5. Optimizing Role Allocation
While Section 4 focused on mapping a domain of interest onto RMTDP and algorithms for
policy evaluation, this section focuses on efficient techniques for RMTDP policy search, in
service of improving BDI/TOP team plans. The TOP in essence provides an incomplete,
fixed policy, and the policy search optimizes decisions left open in the incomplete policy; the
policy thus completed optimizes the original TOP (see Figure 1). By enabling the RMTDP
to focus its search on incomplete policies, and by providing ready-made decompositions,
TOPs assist RMTDPs in quickly searching through the policy space, as illustrated in this
section. We focus, in particular, on the problem of role allocation (Hunsberger & Grosz,
2000; Modi, Shen, Tambe, & Yokoo, 2003; Tidhar et al., 1996; Fatima & Wooldridge, 2001),
a critical problem in teams. While the TOP provides an incomplete policy, keeping open
the role allocation decision for each agent, the RMTDP policy search provides the optimal
role-taking action at each of the role allocation decision points. In contrast to previous
role allocation approaches, our approach determines the best role allocation, taking into
consideration the uncertainty in the domain and future costs. Although demonstrated for
solving the role allocation problem, the methodology is general enough to apply to other
coordination decisions.
386

fiHybrid BDI-POMDP Framework for Multiagent Teaming

5.1 Hierarchical Grouping of RMTDP Policies
As mentioned earlier, to address role allocation, the TOP provides a policy that is complete,
except for the role allocation decisions. RMTDP policy search then optimally fills in the
role allocation decisions. To understand the RMTDP policy search, it is useful to gain an
understanding of the role allocation search space. First, note that role allocation focuses on
deciding how many and what types of agents to allocate to different roles in the organization
hierarchy. This role allocation decision may be made at time t = 0 or it may be made at a
later time conditioned on available observations. Figure 9 shows a partially expanded role
allocation space defined by the TOP organization hierarchy in Figure 4(a) for six helicopters.
Each node of the role allocation space completely specifies the allocation of agents to roles
at the corresponding level of the organization hierarchy (ignore for now, the number to the
right of each node). For instance, the root node of the role allocation space specifies that
six helicopters are assigned to the Task Force (level one) of the organization hierarchy while
the leftmost leaf node (at level three) in Figure 9 specifies that one helicopter is assigned
to SctTeamA, zero to SctTeamB, zero to SctTeamC and five helicopters to Transport Team.
Thus, as we can see, each leaf node in the role allocation space is a complete, valid role
allocation of agents to roles in the organization hierarchy.
In order to determine if one leaf node (role allocation) is superior to another we evaluate
each using the RMTDP by constructing an RMTDP policy for each. In this particular
example, the role allocation specified by the leaf node corresponds to the role-taking actions
that each agent will execute at time t = 0. For example, in the case of the leftmost leaf in
Figure 9, at time t = 0, one agent (recall from Section 2.2 that this is a homogeneous team
and hence which specific agent does not matter) will become a member of SctTeamA while
all other agents will become members of Transport Team. Thus, for one agent i, the roletaking policy will include i (null) = joinSctT eamA and for all other agents, j, j 6= i, it
will include j (null) = joinT ransportT eam. In this case, we assume that the rest of the
role-taking policy, i.e. how roles will be reallocated if a scout fails, is obtained from the role
reallocation algorithm in the BDI/TOP interpreter, such as the STEAM algorithm (Tambe
et al., 2000). Thus for example, if the role reallocation is indeed performed by the STEAM
algorithm, then STEAMs reallocation policy is included into the incomplete policy that
the RMTDP is initially provided. Thus, the best role allocation is computed keeping in
mind STEAMs reallocation policy. In STEAM, given a failure of an agent playing RoleF ,
an agent playing RoleR will replace it if:
Criticality (RoleF )  Criticality (RoleR ) > 0
Criticality (x) = 1 if x is critical; = 0 otherwise

Thus, if based on the agents observations, a critical failure has taken place, then the
replacing agents decision to replace or not will be computed using the above expression
and then included in the incomplete policy input to the RMTDP. Since such an incomplete
policy is completed by the role allocation at each leaf node using the technique above, we
have been able to construct a policy for the RMTDP that corresponds to the role allocation.
In some domains like RoboCupRescue, not all allocation decisions are made at time
t = 0. In such domains, it is possible for the role allocation to be conditioned on observations
(or communication) that are obtained during the course of the execution. For instance, as
shown in Figure 8(a), in the RoboCupRescue scenario, the ambulances are allocated to the
sub-team AmbulanceTeamA or AmbulanceTeamB only after information about the location
387

fiNair & Tambe

6

0

6

[0]
6

1

6

[4167]
5

2

6

[3420]
4

3

6

[2773]
3

4

6

[1926]
2

5

6

[1179]
1

6

6

[432]
0

6 1359.57
6 2926.08
6 1500.12
6 613.81
2 4
2 4
1 5
1 5
1 1 0
0 0 2
0 0 1
1 0 0

Figure 9: Partially expanded role allocation space for mission rehearsal domain(six helos).

of civilians is conveyed to them by the fire engines. The allocation of the ambulances is
then conditioned on this communication, i.e. on the number of civilians at each location.
Figure 10 shows the partially expanded role allocation for a scaled-down rescue scenario
with three civilians, two ambulances and two fire engines (one at station 1 and the other at
station 2). In the Figure, 1;1;2 depicts the fact that there are two ambulances, while there
is one fire engine at each station. As shown, there is a level for the allocation of fire engines
to EngineTeamA and EngineTeamB which gives the number of engines assigned to each
EngineTeam from each station. The next level (leaf level) has different leaf nodes for each
possible assignment of ambulances to AmbulanceTeamA and AmbulanceTeamB depending
upon the value of communication c. Since there are three civilians and we exclude the
case where no civilians are present at a particular fire, there are two possible messages i.e.
one civilian at fire 1 or two civilians at fire 1 (c = 1 or 2).
TaskForce=1;1;2
1;1;2

1;1;2

EngineTeamA=0;1 EngineTeamB=1;0 AmbTeam=2

c=1

EngineTeamA=1;0 EngineTeamB=0;1 AmbTeam=2

1;1;2

1;1;2

0;1 1;0 2

0;1 1;0 2

c=2

c=1

AmbTeamA=2 AmbTeamB=0 AmbTeamA=1 AmbTeamB=1

c=2

AmbTeamA=1 AmbTeamB=1 AmbTeamA=1 AmbTeamB=1

Figure 10: Partially expanded role allocation space for Rescue domain (one fire engine at
station 1, one fire engine at station 2, two ambulances, three civilians).
We are thus able to exploit the TOP organization hierarchy to create a hierarchical
grouping of RMTDP policies. In particular, while the leaf node represents a complete
RMTDP policy (with the role allocation as specified by the leaf node), a parent node
represents a group of policies. Evaluating a policy specified by a leaf node is equivalent to
evaluating a specific role allocation while taking future uncertainties into account. We could
388

fiHybrid BDI-POMDP Framework for Multiagent Teaming

do a brute force search through all role allocations, evaluating each in order to determine
the best role allocation. However, the number of possible role allocations is exponential in
the leaf roles in the organization hierarchy. Thus, we must prune the search space.
5.2 Pruning the Role Allocation Space
We prune the space of valid role allocations using upper bounds (MaxEstimates) for the
parents of the leaves of the role allocation space as admissible heuristics (Section 5.3). Each
leaf in the role allocation space represents a completely specified policy and the MaxEstimate is an upper bound of maximum value of all the policies under the same parent node
evaluated using the RMTDP. Once we obtain MaxEstimates for all the parent nodes (shown
in brackets to the right of each parent node in Figure 9), we use branch-and-bound style
pruning (see Algorithm 2). While we discuss Algorithm 2 below, we note that in essence
it performs branch-and-bound style pruning; the key novelty is step 2 which we discuss in
Section 5.3.
The branch-and-bound algorithm works as follows: First, we sort the parent nodes by
their estimates and then start evaluating children of the parent with the highest MaxEstimate (Algorithm 2: steps 3-13). Evaluate(RMTDP, child) refers to the evaluation of the
leaf-level policy, child, using the RMTDP model. This evaluation of leaf-level policies (step
13) can be done using either of the methods described in Section 4. In the case of the
role allocation space in Figure 9, we would start with evaluating the leaves of the parent
node that has one helicopter in Scouting Team and five in Transport Team. The value of
evaluating each leaf node is shown to the right of the leaf node. Once we have obtained
the value of the best leaf node (Algorithm 2: steps 14,15), in this case 1500.12, we compare
this with the MaxEstimates of the other parents of the role allocation space (Algorithm 2:
steps 16-18). As we can see from Figure 9 this would result in pruning of three parent nodes
(leftmost parent and right two parents) and avoid the evaluation of 65 of the 84 leaf-level
policies. Next, we would then proceed to evaluate all the leaf nodes under the parent with
two helos in Scouting Team and four in Transport Team. This would result in pruning of all
the remaining unexpanded parent nodes and we will return the leaf with the highest value,
which in this case is the node corresponding to two helos allocated to SctTeamA and four
to Transport Team. Although demonstrated for a 3-level hierarchy, the methodology for
applying to deeper hierarchies is straightforward.
5.3 Exploiting TOP to Calculate Upper Bounds for Parents
We will now discuss how the upper bounds of parents, called MaxEstimates, can be calculated for each parent. The MaxEstimate of a parent is defined as a strict upper bound of
the maximum of the expected reward of all the leaf nodes under it. It is necessary that the
MaxEstimate be an upper bound or else we might end up pruning potentially useful role
allocations. In order to calculate the MaxEstimate of each parent we could evaluate each of
the leaf nodes below it using the RMTDP, but this would nullify the benefit of any subsequent pruning. We, therefore, turn to the TOP plan hierarchy (see Figure 4(b)) to break up
this evaluation of the parent node into components, which can be evaluated separately thus
decomposing the problem. In other words, our approach exploits the structure of the BDI
program to construct small-scale RMTDPs unlike other decomposition techniques which
389

fiNair & Tambe

Algorithm 2 Branch-and-bound algorithm for policy search.
1: Parents  list of parent nodes
2: Compute MAXEXP(Parents) {Algorithm 3}
3: Sort Parents in decreasing order of MAXEXP
4: bestVal  
5: for all parent  Parents do
6:
done[parent]  false; pruned[parent]  false
7: for all parent  Parents do
8:
if done[parent] = false and pruned[parent] = false then
9:
child  parentnextChild() {child is a leaf-level policy under parent}
10:
if child = null then
11:
done[parent]  true
12:
else
13:
childVal  Evaluate(RMTDP,child)
14:
if childVal > bestVal then
15:
bestVal  childVal;best  child
16:
for all parent1 in Parents do
17:
if MAXEXP[parent1] < bestVal then
18:
pruned[parent1]  true
19: return best

just assume decomposition or ultimately rely on domain experts to identify interactions in
the agents reward and transition functions (Dean & Lin, 1995; Guestrin, Venkataraman,
& Koller, 2002).
For each parent in the role allocation space, we use these small-scale RMTDPs to evaluate the values for each TOP component. Fortunately, as discussed in Section 4.1, we
exploited small-scale RMTDPs corresponding to TOP components in constructing larger
scale RMTDPs. We put these small-scale RMTDPs to use again, evaluating policies within
each component to obtain upper bounds. Note that just like in evaluation of leaf-level
policies, the evaluation of components for the parent node can be done using either the
observation histories (see Equation 1) or belief states (see Equation 2). We will describe
this section using the observation history-based evaluation method for computing the values
of the components of each parent, which can be summed up to obtain its MaxEstimate (an
upper bound on its childrens values). Thus, whereas a parent in the role allocation space
represents a group of policies, the TOP components (sub-plans) allow a component-wise
evaluation of such a group to obtain an upper bound on the expected reward of any policy
within this group.
Algorithm 3 exploits the smaller-scale RMTDP components, discussed in Section 4.1,
to obtain upper bounds of parents. First, in order to evaluate the MaxEstimate for each
parent node in the role allocation space, we identify the start states for each component from
which to evaluate the RMTDPs. We explain this step using a parent node from Figure 9 
Scouting Team = two helos, Transport Team = four helos (see Figure 11). For the very first
component which does not have any preceding components, the start states corresponds
to the start states of the policy that the TOP was mapped onto. For each of the next
390

fiHybrid BDI-POMDP Framework for Multiagent Teaming

components  where the next component is one linked by a sequential dependence  the
start states are the end states of the preceding component. However, as explained later in
this section, we can significantly reduce this list of start states from which each component
can be evaluated.
Algorithm 3 MAXEXP method for calculating upper bounds for parents in the role allocation space.
1: for all parent in search space do
2:
MAXEXP[parent]  0
3:
for all component i corresponding to factors in the RMTDP from Section 4.1 do
4:
if component i has a preceding component j then
5:
Obtain start states, states[i]  endStates[j]
6:
states[i]  removeIrrelevantFeatures(states[i]) {discard features not present
in Si }
7:
Obtain corresponding observation histories at start OHistories[i] 
endOHistories[j]
8:
OHistories[i]  removeIrrelevantObservations(OHistories[i])
9:
else
10:
Obtain start states, states[i]
11:
Observation histories at start OHistories[i]  null
12:
maxEval[i]  0
13:
for all leaf-level policies  under parent do
14:
maxEval[i]  max(maxEval[i], maxsi states[i],ohi OHistories[i](Evaluate(RM T DPi ,
si , ohi , )))
+
15:
MAXEXP[parent]  maxEval[i]
Similarly, the starting observation histories for a component are the observation histories on completing the preceding component (no observation history for the very first
component). BDI plans do not normally refer to entire observation histories but rely only
on key beliefs which are typically referred to in the pre-conditions of the component. Each
starting observation history can be shortened to include only these relevant observations,
thus obtaining a reduced list of starting observation sequences. Divergence of private observations is not problematic, e.g. will not cause agents to trigger different team plans.
This is because as indicated earlier in Section 2.2, TOP interpreters guarantee coherence
in key aspects of observation histories. For instance, as discussed earlier, TOP interpreter
ensures coherence in key beliefs when initiating and terminating team plans in a TOP; thus
avoiding such divergence of observation histories.
In order to compute the maximum value for a particular component, we evaluate all
possible leaf-level policies within that component over all possible start states and observation histories and obtain the maximum (Algorithm 3:steps 13-14). During this evaluation,
we store all the end states and ending observation histories so that they can be used in
the evaluation of subsequent components. As shown in Figure 11, for the evaluation of
DoScouting component for the parent node where there are two helicopters assigned to
Scouting Team and four helos to Transport Team, the leaf-level policies correspond to all
possible ways these helicopters could be assigned to the teams SctTeamA, SctTeamB, Sct391

fiNair & Tambe

TeamC and Transport Team, e.g. one helo to SctTeamB, one helo to SctTeamC and four
helos to Transport Team, or two helos to SctTeamA and four helos to Transport Team, etc.
The role allocation tells the agents what role to take in the first step. The remainder of the
role-taking policy is specified by the role replacement policy in the TOP infrastructure and
role-execution policy is specified by the DoScouting component of the TOP.
To obtain the MaxEstimate for a parent node of the role allocation space, we simply
sum up the maximum values obtained for each component (Algorithm 3:steps 15), e.g.
the maximum values of each component (see right of each component in Figure 11) were
summed to obtain the MaxEstimate (84 + 3330 + 36 = 3420). As seen in Figure 9, third
node from the left indeed has an upper bound of 3420.
The calculation of the MaxEstimate for a parent nodes should be much faster than
evaluating the leaf nodes below it in most cases for two reasons. Firstly, parent nodes are
evaluated component-wise. Thus, if multiple leaf-level policies within one component result
in the same end state, we can remove duplicates to get the start states of the next component. Since each component only contains the state features relevant to it, the number of
duplicates is greatly increased. Such duplication of the evaluation effort cannot be avoided
for leaf nodes, where each policy is evaluated independently from start to finish. For instance, in the DoScouting component, the role allocations, SctTeamA=1, SctTeamB=1,
SctTeamC=0, TransportTeam=4 and SctTeamA=1, SctTeamB=0, SctTeamC=1, TransportTeam=4, will have end states in common after eliminating irrelevant features when the
scout in SctTeamB for the former allocation and the scout in SctTeamC for the latter allocation fail. This is because through feature elimination (Algorithm 3:steps 6), the only
state features retained for DoTransport are the scouted route and number of transports
(some transports may have replaced failed scouts) as shown in Figure 11.
The second reason computation of MaxEstimates for parents is much faster is that the
number of starting observation sequences will be much less than the number of ending observation histories of the preceding components. This is because not all the observations in
the observation histories of a component are relevant to its succeeding components (Algorithm 3:steps 8). Thus, the function removeIrrelevantObservations reduces the number
of starting observation histories from the observation histories of the preceding component.
We refer to this methodology of obtaining the MaxEstimates of each parent as MAXEXP. A variation of this, the maximum expected reward with no failures (NOFAIL), is
obtained in a similar fashion except that we assume that the probability of any agent failing is 0. We are able to make such an assumption in evaluating the parent node, since we
focus on obtaining upper bounds of parents, and not on obtaining their exact value. This
will result in less branching and hence evaluation of each component will proceed much
quicker. The NOFAIL heuristic only works if the evaluation of any policy without failures
occurring is higher than the evaluation of the same policy with failures possible. This should
normally be the case in most domains. The evaluation of the NOFAIL heuristics for the
role allocation space for six helicopters is shown in square brackets in Figure 9.
The following theorem shows that the MAXEXP method for finding the upper bounds
indeed finds an upper bound and thus yields an admissible search heuristic for the branchand-bound search of the role allocation space.
Theorem 3 The MAXEXP method will always yield an upper bound.
392

fiHybrid BDI-POMDP Framework for Multiagent Teaming

[84]
DoScouting
[ScoutingTeam=2,TransportTeam=4]

Alloc:
SctTeamA=2
SctTeamB=0
SctTeamC=0
TransportTeam=4

Alloc:
SctTeamA=0
SctTeamB=1
SctTeamC=1
TransportTeam=4

[3300]
DoTransport
[TransportTeam=4]

StartState:
RouteScouted=1
Transports=4

[36]
RemainingScouts
[ScoutTeam=2]

StartState:
RouteScouted=1
Transports=3

StartState:
RouteScouted=1
Transports=0

Figure 11: Component-wise decomposition of a parent by exploiting TOP.

Proof: See Appendix C.
From Theorem 3, we can conclude that our branch-and-bound policy search algorithm
will always find the best role allocation, since the MaxEstimates of the parents are true
upper bounds. Also, with the help of Theorem 4, we show that in the worst case, our
branch-and-bound policy search has the same complexity as doing a brute force search.
Theorem 4 Worst-case complexity for evaluating a single parent node using MAXEXP is
the same as that of evaluating every leaf node below it within a constant factor.
Proof sketch:
 The worst case complexity for MAXEXP arises when:
1. Let ESj be the end states of component j executing policy  after removing
features that are irrelevant to the succeeding component k. Similarly, let ESj
be the end states of component j executing policy   after
Tremoving features that
are irrelevant to the succeeding component k. If ESj ESj = null then no
duplication in the end states will occur.
2. Let OHj be the ending observation histories of component j executing policy
 after removing observations that are irrelevant to the succeeding component
k. Similarly, let OHj be the ending observation histories of component j executing policy   after removing observation
histories that are irrelevant to the
T
succeeding component k. If OHj OHj = null then no duplication in the
observation histories will occur. Note that if the belief-based evaluation was used
then we would replace observation histories by the TOP congruent belief states
(see Sect 4).
 In such a case, there is no computational advantage to evaluating each components
MaxEstimate separately. Thus, it is equivalent to evaluating each child node of the
parent. Thus, in the worst case, MAXEXP computation for the parent is the same as
the evaluating all its children within a constant factor. 
In addition, in the worst case, no pruning will result using MAXEXP and each and every
leaf node will need to be evaluated. This is equivalent to evaluating each leaf node twice.
393

fiNair & Tambe

Thus, the worst case complexity of doing the branch-and-bound search using MAXEXP is
the same as that of finding the best role allocation by evaluating every leaf node. We refer
to this brute-force approach as NOPRUNE. Thus, the worst case complexity of MAXEXP
is the same as NOPRUNE. However, owing to pruning and the savings through decomposition in the computation of MaxEstimates, significant savings are likely in the average
case. Section 6 highlights these savings for the mission rehearsal and the RoboCupRescue
domains.

6. Experimental Results
This section presents four sets of results in the context of the two domains introduced
in Section 2.1, viz. mission rehearsal and RoboCupRescue (Kitano et al., 1999). First,
we investigated empirically the speedups that result from using the TOP-congruent belief
states i (belief-based evaluation) over observation history-based evaluation and from using
the algorithm from Section 5 over a brute-force search. Here we focus on determining
the best assignment of agents to roles; but assume a fixed TOP and TOP infrastructure.
Second, we conducted experiments to investigate the benefits of considering uncertainty in
determining role allocations. For this, we compared the allocations found by the RMTDP
role allocation algorithm with (i) allocations which do not consider any kind of uncertainty,
and (ii) allocations which do not consider observational uncertainty but consider action
uncertainty. Third, we conducted experiments in both domains to determine the sensitivity
of the results to changes in the model. Fourth, we compare the performance of allocations
found by the RMTDP role allocation algorithm with allocations of human subjects in the
more complex of our domains  RoboCupRescue simulations.
6.1 Results in Mission Rehearsal Domain
For the mission rehearsal domain, the TOP is the one discussed in Section 2.2. As can be
seen in Figure 4(a), the organization hierarchy requires determining the number of agents
to be allocated to the three scouting sub-teams and the remaining helos must be allocated
to the transport sub-team. Different numbers of initial helicopters were attempted, varying
from three to ten. The details on how the RMTDP is constructed for this domain are given
Appendix B. The probability of failure of a scout at each time step on routes 1, 2 and 3
are 0.1, 0.15 and 0.2, respectively. The probability of a transport observing an alive scout
on routes 1, 2 and 3 are 0.95, 0.94 and 0.93, respectively. False positives are not possible,
i.e. a transport will not observe a scout as being alive if it has failed. The probability of a
transport observing a scout failure on routes 1, 2 and 3 are 0.98, 0.97 and 0.96, respectively.
Here too, false positives are not possible and hence a transport will not observe a failure
unless it has actually taken place.
Figure 12 shows the results of comparing the different methods for searching the role
allocation space. We show four methods. Each method adds new speedup techniques to
the previous:
1. NOPRUNE-OBS: A brute force evaluation of every role allocation to determine the
best. Here, each agent maintains its complete observation history and the evaluation
algorithm in Equation 1 is used. For ten agents, the RMTDP is projected to have in
394

fiHybrid BDI-POMDP Framework for Multiagent Teaming

the order of 10,000 reachable states and in the order of 100,000 observation histories
per role allocation evaluated (thus the largest experiment in this category was limited
to seven agents).
2. NOPRUNE-BEL: A brute force evaluation of every role allocation. The only difference
between this method and NOPRUNE-OBS is the use of the belief-based evaluation
algorithm (see Equation 2).
3. MAXEXP: The branch-and-bound search algorithm described in Section 5.2 that
uses upper bounds of the evaluation of the parent nodes to find the best allocation.
Evaluation of the parent and leaf nodes uses the belief-based evaluation.
4. NOFAIL: The modification to branch-and-bound heuristic mentioned in Section 5.3.
In essence it is same as MAXEXP, except that the upper bounds are computed making
the assumption that agents do not fail. This heuristic is correct in those domains where
the total expected reward with failures is always less than if no failures were present
and will give significant speedups if agent failures is one of the primary sources of
stochasticity. In this method, too, the evaluation of the parent and leaf nodes uses
the belief-based evaluation. (Note that only upper bounds are computed using the
no-failure assumption  no changes are assumed in the actual domains.)
In Figure 12(a), the Y-axis is the number of nodes in the role allocation space evaluated
(includes leaf nodes as well as parent nodes), while in Figure 12(b) the Y-axis represents
the runtime in seconds on a logarithmic scale. In both figures, we vary the number of agents
on the X-axis. Experimental results in previous work using distributed POMDPs are often
restricted to just two agents; by exploiting hybrid models, we are able to vary the number of
agents from three to ten as shown in Figure 12(a). As clearly seen in Figure 12(a), because
of pruning, significant reductions are obtained by MAXEXP and NOFAIL over NOPRUNEBEL in terms of the numbers of nodes evaluated. This reduction grows quadratically to
about 10-fold at ten agents.3 NOPRUNE-OBS is identical to NOPRUNE-BEL in terms of
number of nodes evaluated, since in both methods all the leaf-level policies are evaluated,
only the method of evaluation differs. It is important to note that although NOFAIL and
MAXEXP result in the same number of nodes being evaluated for this domains, this is
not necessarily true always. In general, NOFAIL will evaluate at least as many nodes as
MAXEXP since its estimate is at least as high as the MAXEXP estimate. However, the
upper bounds are computed quicker for NOFAIL.
Figure 12(b) shows that the NOPRUNE-BEL method provides a significant speedup
over NOPRUNE-OBS in actual run-time. For instance, there was a 12-fold speedup using
NOPRUNE-BEL instead of NOPRUNE-OBS for the seven agent case (NOPRUNE-OBS
could not be executed within a day for problem settings with greater than seven agents).
This empirically demonstrates the computational savings possible using belief-based evaluation instead of observation history-based evaluation (see Section 4). For this reason, we
use only belief-based evaluation for the MAXEXP and NOFAIL approaches and also for all
3. The number of nodes for NOPRUNE up to eight agents were obtained from experiments, the rest can
be calculated using the formula [m]n /n! = (m + n  1)  . . .  m/n!, where m represents the number of
heterogeneous role types and n is the number of homogeneous agents. [m]n = (m + n  1)  . . .  m is
referred to as a rising factorial.

395

fiNair & Tambe

the remaining experiments in this paper. MAXEXP heuristic results in a 16-fold speedup
over NOPRUNE-BEL in the eight agent case.
The NOFAIL heuristic which is very quick to compute the upper bounds far outperforms
the MAXEXP heuristic (47-fold speedup over MAXEXP for ten agents). Speedups of
MAXEXP and NOFAIL continually increase with increasing number of agents. The speedup
of the NOFAIL method over MAXEXP is so marked because, in this domain, ignoring
failures results in much less branching.
350

NOFAIL, MAXEXP

Number of nodes

300

NOPRUNE-OBS,
NOPRUNE-BEL

250
200
150
100
50
0

3

4

5

6

7

8

9

10

Number of agents
100000

MAXEXP
NOFAIL
NOPRUNE-BEL
NOPRUNE-OBS

Time in secs (log scale)

10000
1000
100
10
1
0.1
0.01

3

4

5

6

7

8

9

10

Number of agents

Figure 12: Performance of role allocation space search in mission rehearsal domain, a) (left)
Number of nodes evaluated, b) (right)Run-time in seconds on a log scale.

Next, we conducted experiments illustrating the importance of RMTDPs reasoning
about action and observation uncertainties on role allocations. For this, we compared the
allocations found by the RMTDP role allocation algorithm with allocations found using two
different methods (see Figure 13):
1. Role allocation via constraint optimization (COP) (Modi et al., 2003; Mailler & Lesser,
2004) allocation approach: In the COP approach4 , leaf-level sub-teams from the or4. Modi et al.s work (2003) focused on decentralized COP, but in this investigation our emphasis is on the
resulting role allocation generated by the COP, and not on the decentralization per se.

396

fiHybrid BDI-POMDP Framework for Multiagent Teaming

ganization hierarchy are treated as variables and the number of helicopters as the
domain of each such variable (thus, the domain may be 1, 2, 3,..helicopters). The
reward for allocating agents to sub-teams is expressed in terms of constraints:
 Allocating a helicopter to scout a route was assigned a reward corresponding to
the routes distance but ignoring the possibility of failure (i.e. ignoring transition
probability). Allocating more helicopters to this subteam obtained proportionally higher reward.
 Allocating a helicopter a transport role was assigned a large reward for transporting cargo to the destination. Allocating more helicopters to this subteam
obtained proportionally higher reward.
 Not allocating at least one scout role was assigned a reward of negative infinity
 Exceeding the total number of agents was assigned a reward of negative infinity
2. RMTDP with complete observability: In this approach, we consider the transition
probability, but ignore partial observability; achieved by assuming complete observability in the RMTDP. An MTDP with complete observability is equivalent to a
Markov Decision Problem (MDP) (Pynadath & Tambe, 2002) where the actions are
joint actions. We, thus, refer to this allocation method as the MDP method.
Figure 13(a) shows a comparison of the RMTDP-based allocation with the MDP allocation and the COP allocation for increasing number of helicopters (X-axis). We compare
using the expected number of transports that get to the destination (Y-axis) as the metric
for comparison since this was the primary objective of this domain. As can be seen, considering both forms of uncertainty (RMTDP) performs better than just considering transition
uncertainty (MDP) which in turn performs better than not considering uncertainty (COP).
Figure 13(b) shows the actual allocations found by the three methods with four helicopters
and with six helicopters. In the case of four helicopters (first three bars), RMTDP and MDP
are identical, two helicopters scouting route 2 and two helicopters taking on transport role.
The COP allocation however consists of one scout on route 3 and three transports. This
allocation proves to be too myopic and results in fewer transports getting to the destination
safely. In the case of six helicopters, COP chooses just one scout helicopter on route 3,
the shortest route. The MDP approach results in two scouts both on route 1, which was
longest route albeit the safest. The RMTDP approach, which also considers observational
uncertainty chooses an additional scout on route 2, in order to take care of the cases where
failures of scouts go undetected by the transports.
It should be noted that the performance of the RMTDP-based allocation will depend
on the values of the elements of the RMTDP model. However, as our next experiment
revealed, getting the values exactly correct is not necessary. In order to test the sensitivity
of the performance of the allocations to the actual model values, we introduced error in the
various parameters of the model to see how the allocations found using the incorrect model
would perform in the original model (without any errors). This emulates the situation where
the model does not correctly represent the domain. Figure 14 shows the expected number
of transports that reach the destination (Y-axis) in the mission rehearsal scenario with six
helicopters as error (X-axis) is introduced to various parameters in the model. For instance,
397

fiNair & Tambe

7

Number of transports

6
5

RMTDP
COP
MDP

4
3
2
1
0

4

5

6

7

8

Number of agents
7
6 helos

RM

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

Rt3
Rt2
xxx
xxxRt1
xxx
Transports
xxxx
xxxx
xxxx
xxxx
xxxx

xxxx
xxxx
xxxx
xxxx
xxxx

DP

TD

P

0

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxx

M

1

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

TD
P

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx

RM

2

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

M
DP

3

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

P

4

CO

Number of helos

5

P

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

CO

4 helos

6

Figure 13: a) Comparison of performance of different allocation methods, b)Allocations
found using different allocation methods.

398

fiHybrid BDI-POMDP Framework for Multiagent Teaming

when the percentage error in failure rate on route 1 (route1-failure-rate) was between -15%
(i.e. erroneous failure rate is 85% of actual failure rate) and 10%, there was no difference
in the number of transports that reached their destination (3.498). However when the
percentage error was greater than 10%, the allocation found was too conservative resulting
in fewer transports getting to the destination. Similarly, when the percentage error was less
than -15%, the allocation found was too risky, with too few scouts assigned, resulting in
more failures. In general, Figure 14 shows that the model is insensitive to errors of 5 to 10%
in the model parameters for the mission rehearsal domain, but if the model parameters were
outside this range, non-optimal allocations would result. In comparing these non-optimal
allocations with COP, we find that they always perform better than COP for the range of
errors tested (+/-25%) for both failure rate as well as observability of routes. For instance,
at an error of 25% in failure rate on route 1, RMTDP managed to have 2.554 transports
safely reach the destination, and COP only managed to get 1.997 transports reach safely. In
comparing the non-optimal allocations with MDP, we also find that they performed better
than MDP within the range of +/- 25% for error in the observability of the routes. Thus,
although the allocations found using an incorrect model were non-optimal they performed
better than COP and MDP for large ranges of errors in the model. This shows that getting
the model exactly correct is not necessary to find good allocations. We are thus able to
obtain benefits from RMTDP even without insisting on an accurate model.
4

route1-failure-rate
route-2-failure-rate
route3-failure-rate
route1-observability

Number of Transports

3.5
3
2.5
2
1.5
1
0.5
0
-25 -20 -15 -10 -5

0

5

10

15

20

25

Percentage error

Figure 14: Model sensitivity in mission rehearsal domain.

6.2 Results in RoboCupRescue Domain
6.2.1 Speedups in RoboCupRescue Domain
In our next set of experiments, we highlight the computational savings obtained in the
RoboCupRescue domain. The scenario for this experiment consisted of two fires at different
locations in the city. Each of these fires has a different initially unknown number of civilians
in it, however the total number of civilians and the distribution from which the locations
of the civilians is chosen is known ahead of time. For this experiment, we fix the number of
civilians at five and set the distribution used to choose the civilians locations to be uniform.
The number of fire engines is set at five, located in three different fire stations as described
399

fiNair & Tambe

in Section 2.1 and vary the number of ambulances, all co-located at an ambulance center,
from two to seven. The reason we chose to change only the number of ambulances is because
small number of fire engines are unable to extinguish fires, changing the problem completely.
The goal is to determine which fire engines to allocate to which fire and once information
about civilians is transmitted, how many ambulances to send to each fire location.
Figure 15 highlights the savings in terms of the number of nodes evaluated and the actual
runtime as we increase the number of agents. We show results only from NOPRUNE-BEL
and MAXEXP. NOPRUNE-OBS could not be run because of slowness. Here the NOFAIL
heuristic is identical to MAXEXP since agents cannot fail in this scenario. The RMTDP
in this case had about 30,000 reachable states.
In both Figures 15(a) and 15(b), we increase the number of ambulances along the Xaxis. In Figure 15(a), we show the number of nodes evaluated (parent nodes + leaf nodes)5
on a logarithmic scale. As can be seen, the MAXEXP method results in about a 89-fold
decrease in the number of nodes evaluated when compared to NOPRUNE-BEL for seven
ambulances, and this decrease becomes more pronounced as the number of ambulances is
increased. Figure 15(b) shows the time in seconds on a logarithmic scale on the Y-axis and
compares the run-times of the MAXEXP and NOPRUNE-BEL methods for finding the best
role allocation. The NOPRUNE-BEL method could not find the best allocation within a
day when the number of ambulances was increased beyond four. For four ambulances (and
five fire engines), MAXEXP resulted in about a 29-fold speedup over NOPRUNE-BEL.
6.2.2 Allocation in RoboCupRescue
Our next set of experiments shows the practical utility of our role allocation analysis in
complex domains. We are able to show significant performance improvements in the actual
RoboCupRescue domain using the role allocations generated by our analysis. First, we
construct an RMTDP for the rescue scenario, described in Section 2.1, by taking guidance
from the TOP and the underlying domain (as described in Section 4.1). We then use
the MAXEXP heuristic to determine the best role allocation. We compared the RMTDP
allocation with the allocations chosen by human subjects. Our goal in comparing RMTDP
allocations with human subjects was mainly to show that RMTDP is capable at performing
at or near human expert levels for this domain. In addition, in order to determine that
reasoning about uncertainty actually impacts the allocations, we compared the RMTDP
allocations with allocations determined by two additional allocation methods:
1. RescueISI: Allocations used by the our RoboCupRescue agents that were entered in
the RoboCupRescue competitions of 2001(RescueISI) (Nair et al., 2002), where they
finished in third place. These agents used local reasoning for their decision making,
ignoring transitional as well and observational uncertainty.
2. RMTDP with complete observability: As discussed earlier, complete observability in
RMTDP leads to an MDP, and we refer to this method as the MDP method.
5. The number of nodes evaluated using NOPRUNE-BEL can be computed as (f1 + 1)  (f2 + 1)  (f3 + 1) 
(a + 1)c+1 , where f1 , f2 and f3 are the number of fire engines are station 1, 2 and 3, respectively, a is
the number of ambulances and c is the number of civilians. Each node provides a complete conditional
role allocation, assuming different numbers of civilians at each fire station.

400

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Number of nodes (log scale)

10000000

MAXEXP
NOPRUNE

1000000
100000
10000
1000
100
10
1

2

3

4

5

6

7

Number of ambulances

Run time in secs (log scale)

100000

MAXEXP
NOPRUNE

10000

1000

100

10

1

2

3

4

5

Number of ambulances

6

7

Figure 15: Performance of role allocation space search in RoboCupRescue, a: (left) Number
of nodes evaluated on a log scale, and b: (right) Run-time in seconds on a log
scale.

401

fiNair & Tambe

Note that these comparisons were performed using the RoboCupRescue simulator with
multiple runs to deal with stochasticity6 . The scenario is as described in Section 6.2.1. We
fix the number of fire engines, ambulances and civilians at five each. For this experiment,
we consider two settings, where the location of civilians is drawn from:
 Uniform distribution  25% of the cases have four civilians at fire 1 and one civilian
at fire 2, 25% with three civilians at fire 1 and two at fire 2, 25% with two civilians
at fire 1 and three at fire 2 and the remaining 25% with one civilian at fire 1 and
four civilians at fire 2. The speedup results of Section 6.2.1 were obtained using this
distribution.
 Skewed distribution  80% of the cases have four civilians at fire 1 and one civilian at
fire 2 and the remaining 20% with one civilian at fire 1 and four civilians at fire 2.
Note that we do not consider the case where all civilians are located at the same fire as
the optimal ambulance allocation is simply to assign all ambulances to the fire where the
civilians are located. A skewed distribution was chosen to highlight the cases where it
becomes difficult for humans to reason about what allocation to choose.
The three human subjects used in this experiment were researchers at USC. All three
were familiar with RoboCupRescue. They were given time to study the setup and were not
given any time limit to provide their allocations. Each subject was told that the allocations
were going to be judged first on the basis of the number of civilian lives lost and next on the
damage sustained due to fire. These are exactly the criteria used in RoboCupRescue (Kitano
et al., 1999).
We then compared RMTDP allocation with those of the human subjects in the
RoboCupRescue simulator and with RescueISI and MDP. In Figure 16, we compared the
performance of the allocations on the basis of the number of civilians who died and the
average damage to the two buildings (lower values are better for both criteria). These two
criteria are the main two criteria used in RoboCupRescue (Kitano et al., 1999). The values shown in Figure 16 were obtained by averaging forty simulator runs for the uniform
distribution and twenty runs for the skewed distribution for each allocation. The average
values were plotted to account for the stochasticity in the domain. Error bars are provided
to show the standard error for each allocation method.
As can be seen in Figure 16(a), the RMTDP allocation did better than the other five
allocations in terms of a lower number of civilians dead (although human3 was quite close).
For example, averaging forty runs, the RMTDP allocation resulted in 1.95 civilian deaths
while human2s allocation resulted in 2.55 civilian deaths. In terms of the average building
damage, the six allocations were almost indifferentiable, with the humans actually performing marginally better. Using the skewed distribution, the difference between the allocations
was much more perceptible (see Figure 16(b)). In particular, we notice how the RMTDP
allocation does much better than the humans in terms of the number of civilians dead. Here,
human3 did particularly badly because of a bad allocation for fire engines. This resulted in
more damage to the buildings and consequently to the number of civilians dead.
6. For the mission rehearsal domain, we could run on the actual mission rehearsal simulator since that
simulator is not public domain and no longer accessible, and hence the difference in how we tested role
allocations in the mission rehearsal and the RoboCupRescue domains.

402

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Comparing RMTDP with RescueISI and the MDP approach showed that reasoning
about transitional uncertainty (MDP) does better than a static reactive allocation method
(RescueISI) but not as well as reasoning about both transitional and observational uncertainty. In the uniform distribution case, we found that RMTDP does better than both MDP
and RescueISI, with the MDP method performing better than RescueISI. In the skewed distribution case, the improvement in allocations using RMTDP is greater. Averaging twenty
simulation runs, RMTDP allocations resulted in 1.54 civilians deaths while MDP resulted
in 1.98 and RescueISI in 3.52. The allocation method used by RescueISI often resulted
in one of the fires being allocated too few fire engines. The allocations determined by the
MDP approach turned out to be the same as human1.
A two-tailed t-test was performed in order to test the statistical significance of the means
for the allocations in Figure 16. The means of number of civilians dead for the RMTDP
allocation and the human allocations were found to be statistically different (confidence
> 96%) for both the uniform as well as the skewed distributions. The difference in the fire
damage was not statistically significant in the uniform case, however, the difference between
the RMTDP allocation and human3 for fire damage was statistically significant (> 96%) in
the skewed case.
6

Civilians casualties
Building damage

5
4
3
2
1

DP

ue

M

IS

I

3
sc

m

an

2

Re

hu

hu

m

an

1
an
m
hu

RM

TD

P

0

6

Civilians casualties
Building damage

5
4
3
2
1

DP
M

I
Re

sc
ue

IS

3
an
m
hu

2
an
m
hu

1
an
m
hu

RM
TD

P

0

Figure 16: Comparison of performance in RoboCupRescue, a: (left) uniform, and b: (right)
skewed.

403

fiNair & Tambe

Considering just the average performance of these different allocations does not highlight
the individual cases where marked differences were seen in the performance. In Figure 17, we
present the comparison of particular settings where the other allocation methods showed a
bigger difference from RMTDP in terms of their allocations. The standard error is shown in
error bars for each allocation. Figures 17(a) and 17(b) compare the allocations for uniform
civilian distributions in the setting where there was one civilian at fire 1 and four civilians at
fire 2 (1-4 civilian setting) and four civilians at fire 1 and one at fire 2 (4-1 civilian setting)
respectively. As can be seen in these figure, the RMTDP allocation results in fewer civilian
casualties but in slightly more damage to the buildings due to fire (difference in fire damage
was not statistically significant because the damage values were very close). Figures 17(c)
and 17(d) compare the allocations for the skewed civilian distribution. The key difference
arises for human3. As can be seen, human3 results in more damage due to fire. This is
because human3 allocated too few fire engines to one of the buildings, which in turn resulted
in that building being burnt down completely. Consequently, civilians located at this fire
location could not be rescued by the ambulances. Thus, we see specific instances where
the allocation done using the RMTDP-based allocation algorithm is superior to allocations
that a human comes up with.

3.5

Civilians casualties
Building damage

3

4.5

Civilians casualties
Building damage

4
3.5

2.5

3

2

2.5
2

1.5

1.5

1

3.5

DP

I
IS
ue

M

3
sc

2

an
m

Re

hu

an
m
hu

P

an
m

TD

Civilians casualties
Building damage

3

hu

RM

I

DP
M

IS

3

ue

an
Re

hu

sc

m

an
hu

m

an
m
hu

TD
RM

2

0
1

0

P

0.5
1

1
0.5

4.5

Civilians casualties
Building damage

4
3.5

2.5

3
2

2.5
2

1.5

1.5

1

1
0.5

0.5
DP
M

IS
I
sc
ue

m
an
3

Re

hu

m
an
2
hu

hu

P
RM
TD

DP
M

I
ue
IS

3
an
m

Re
sc

hu

2
an
m
hu

1
an
m
hu

TD
P
RM

m
an
1

0

0

Figure 17: Comparison of performance in RoboCupRescue for particular settings, a: (topleft) uniform 1-4 civilian setting b:(top-right) uniform 4-1 civilian setting, c:
(bottom-left) skewed 1-4 civilian setting d:(bottom-right) skewed 4-1 civilian
setting.

404

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Table 1 shows the allocations to fire 1 (agents not assigned to fire 1 are allocated to fire
2) found by the RMTDP role allocation algorithm and those used by the human subjects for
the skewed 4-1 civilian setting (we consider this case since it shows the most difference). In
particular, this table highlights the differences between the various allocators for the skewed
4-1 civilian setting and helps account for the differences seen in their performance in the
actual simulator. As can be seen from Figure 17(d), the main difference in performance
was in terms of the number of civilians saved. Recall that in this scenario, there are four
civilians at fire 1, and one at fire 2. Here all the human subjects and MDP chose to
send only one ambulance to fire 2 (number of ambulances allocated to f ire 2 = 5 
number of ambulances allocated to f ire 1). This lone ambulance was unable to rescue the
civilian at fire 1, resulting in the humans and MDP saving fewer civilians. RescueISI chose to
send all the ambulances to fire 2 using a greedy selection method based on proximity to the
civilians resulting in all the civilians at fire 1 dying7 . In terms of the fire engine allocation,
human3 sent in four fire engines to fire 1 where more civilians were likely to be located
(number of engines allocated to f ire 2 = 5  number of engines allocated to f ire 1).
Unfortunately, this backfired since the lone fire engine at fire 2 was not able to extinguish
the fire there, causing the fire to spread to other parts of the city.
Distribution
Skewed 4-1

Engines from station 1
Engines from station 2
Engines from station 3
Ambulances

RMTDP
0
1
1
3

human1
2
1
0
4

human2
2
1
0
4

human3
1
1
2
4

RescueISI
2
1
0
0

MDP
2
1
0
4

Table 1: Allocations of ambulances and fire engines to fire 1.
These experiments show that the allocations found by the RMTDP role allocation algorithm performs significantly better than allocations chosen by human subjects and RescueISI
and MDP in most cases (and does not do significantly worse in any case). In particular
when the distribution of civilians is not uniform, it is more difficult for humans to come up
with an allocation and the difference between human allocations and the RMTDP allocation
becomes more significant. From this we can conclude that the RMTDP allocation performs
at near-human expertise.
In our last experiment done using the RoboCupRescue simulator, we introduced error
in the RMTDP model in order to determine how sensitive the model was to errors in the
parameters of the model. Figure 18 compares the allocations found, when there were five
ambulances, 5 fire engines and 5 civilians, in terms of the number of civilian casualties (Yaxis) when error (X-axis) was introduced to the probability of fire spread and the probability
of civilian health deterioration. As can be seen increasing the error in the probability of fire
spread to 20% and higher results in allocations that save fewer civilians as the fire brigades
choose to concentrate their effort on only one of the fires. The resulting allocation was
found to have the same value in terms of the number of civilians casualties as that used by
RescueISI, which did not consider any uncertainty. Reducing the error in the probability of
fire did not have an impact on the allocations found. Increasing the error in probability of
7. This strategy of ambulances going to the closest civilian worked fairly well because the ambulances were
usually well spread out

405

fiNair & Tambe

civilian health deterioration to 15% and higher caused some civilians to be sacrificed. This
allocation was found to have the same value in terms of the number of civilians casualties as
that used by RescueISI. Decreasing the error in probability of civilian health deterioration
-5% and lower (more negative) caused the number of ambulances to be allocated to a fire
to be the same as the number of civilians at that fire (same as human1).
3

Civilian casualties

2.5
2

fire-rate
civilian-health

1.5
1
0.5
0

-25 -20 -15 -10 -5

0

5

10

15

20

25

Percentage error

Figure 18: Model sensitivity in the RoboCupRescue scenario.

7. Related Work
There are four related areas of research, that we wish to highlight. First, there has been
a considerable amount of work done in the field of multiagent teamwork (Section 7.1).
The second related area of research is the use of decision theoretic models, in particular
distributed POMDPs (Section 7.2). The third area of related work we describe (Section 7.3)
are hybrid systems that used Markov Decision Process and BDI approaches. Finally, in
Section 7.4, the related work in role allocation and reallocation in multiagent teams is
described.
7.1 BDI-based Teamwork
Several formal teamwork theories such as Joint Intentions (Cohen & Levesque, 1991),
SharedPlans (Grosz & Kraus, 1996) were proposed that tried to capture the essence of
multiagent teamwork in the logic of Beliefs-Desires-Intentions. Next, practical models
of teamwork such as COLLAGEN (Rich & Sidner, 1997), GRATE* (Jennings, 1995),
STEAM (Tambe, 1997) built on these teamwork theories (Cohen & Levesque, 1991; Grosz
& Kraus, 1996) and attempted to capture the aspects of teamwork that were reusable
across domains. In addition, to complement the practical teamwork models, the teamoriented programming approach (Pynadath & Tambe, 2003; Tidhar, 1993a, 1993b) was
introduced to allow large number of agents to be programmed as teams. This approach
was then expanded on and applied to a variety of domains (Pynadath & Tambe, 2003; Yen
et al., 2001; da Silva & Demazeau, 2002). Other approaches for building practical multia406

fiHybrid BDI-POMDP Framework for Multiagent Teaming

gent systems (Stone & Veloso, 1999; Decker & Lesser, 1993), while not explicitly based on
team-oriented programming, could be considered in the same family.
The research reported in this article complements this research on teamwork by introducing hybrid BDI-POMDP models that exploit the synergy between BDI and POMDP
approaches. In particular, TOP and teamwork models have traditionally not addressed
uncertainty and cost. Our hybrid model provides this capability, and we have illustrated
the benefits of this reasoning via detailed experiments.
While this article uses team-oriented programming (Tambe et al., 2000; da Silva &
Demazeau, 2002; Tidhar, 1993a, 1993b) as an example BDI approach, it is relevant to
other similar techniques of modeling and tasking collectives of agents, such as Decker and
Lessers (1993) TAEMS approach. In particular, the TAEMS language provides an abstraction for tasking collaborative groups of agents similar to TOP, while the GPGP infrastructure used in executing TAEMS-based tasks is analogous to the TOP interpreter
infrastructure shown in Figure 1. While Lesser et al. have explored the use of distributed
MDPs in analyses of GPGP coordination (Xuan & Lesser, 2002), they have not exploited
the use of TAEMS structures in decomposition or abstraction for searching optimal policies
in distributed MDPs, as suggested in this article. Thus, this article complements Lesser
et al.s work in illustrating a significant avenue for further efficiency improvements in such
analyses.
7.2 Distributed POMDP Models
Distributed POMDP models represent a collection of formal models that are expressive
enough to capture the uncertainty in the domain and the costs and rewards associated
with states and actions. Given a group of agents, the problem of deriving separate policies for them that maximize some joint reward can be modeled using distributed POMDP
models. In particular, the DEC-POMDP (Decentralized POMDP) (Bernstein et al., 2000)
and MTDP (Multiagent Team Decision Problem) (Pynadath & Tambe, 2002) are generalizations of POMDPs to the case where there are multiple, distributed agents, basing
their actions on their separate observations. These frameworks allow us to formulate what
constitutes an optimal policy for a multiagent team and in principle derive that policy.
However, with a few exceptions, effective algorithms for deriving policies for distributed
POMDPs have not been developed. Significant progress has been achieved in efficient
single-agent POMDP policy generation algorithms (Monahan, 1982; Cassandra, Littman,
& Zhang, 1997; Kaelbling et al., 1998). However, it is unlikely such research can be directly
carried over to the distributed case. Finding optimal policies for distributed POMDPs is
NEXP-complete (Bernstein et al., 2000). In contrast, finding an optimal policy for a single
agent POMDP is PSPACE-complete (Papadimitriou & Tsitsiklis, 1987). As Bernstein et
al. note (Bernstein et al., 2000), this suggests a fundamental difference in the nature of the
problems. The distributed problem cannot be treated as one of separate POMDPs in which
individual policies can be generated for individual agents because of possible cross-agent
interactions in the reward, transition or observation functions. (For any one action of one
agent, there may be many different rewards possible, based on the actions that other agents
may take.)
407

fiNair & Tambe

Three approaches have been used to solve distributed POMDPs. One approach that
is typically taken is to make simplifying assumptions about the domain. For instance, in
Guestrin et al. (2002), it is assumed that each agent can completely observe the world state.
In addition, it is assumed that the reward function (and transition function) for the team
can be expressed as the sum (product) of the reward (transition) functions of the agents
in the team. Becker et al. (2003) assume that the domain is factored such that each agent
has a completely observable local state and also that the domain is transition-independent
(one agent cannot affect another agents local state).
The second approach taken is to simplify the nature of the policies considered for each
of the agents. For example, Chades et al. (2002) restrict the agent policies to be memoryless
(reactive) policies, thereby simplifying the problem to solving multiple MDPs. Peshkin et
al. (2000) take a different approach by using gradient descent search to find local optimum
finite-controllers with bounded memory. Nair et al. (2003a) present an algorithm for finding
a locally optimal policy from a space of unrestricted finite-horizon policies. The third
approach, taken by Hansen et al. (2004), involves trying to determine the globally optimal
solution without making any simplifying assumptions about the domain. In this approach,
they attempt to prune the space of possible complete policies by eliminating dominated
policies. Although a brave frontal assault on the problem, this method is expected to
face significant difficulties in scaling up due to the fundamental complexity of obtaining a
globally optimal solution.
The key difference with our work is that our research is focused on hybrid systems where
we leverage the advantages of BDI team plans, which are used in practical systems, and
distributed POMDPs that quantitatively reason about uncertainty and cost. In particular,
we use TOPs to specify large-scale team plans in complex domains and use RMTDPs for
finding the best role allocation for these teams.
7.3 Hybrid BDI-POMDP Approaches
POMDP models have been used in the context of analysis of both single agent (Schut,
Wooldridge, & Parsons, 2001) and multiagent (Pynadath & Tambe, 2002; Xuan et al., 2001)
behavior. Schut et al. compare various strategies for intention reconsideration (deciding
when to deliberate about its intentions) by modeling a BDI system using a POMDP. The
key differences with this work and our approach are that they apply their analysis to a single
agent case and do not consider the issues of exploiting BDI system structure in improving
POMDP efficiency.
Xuan and Lesser (2001) and Pynadath and Tambe (2002), both analyze multiagent
communication. While Xuan and Lesser dealt with finding and evaluating various communication policies, Pynadath and Tambe used the COM-MTDP model to deal with the problem of comparing various communication strategies both empirically and analytically. Our
approach is more general in that we explain an approach for analyzing any coordination actions including communication. We concretely demonstrate our approach for analysis of role
allocation. Additional key differences from the earlier work by Pynadath and Tambe (2002)
are as follows: (i) In RMTDP, we illustrate techniques to exploit team plan decomposition
in speeding up policy search, absent in COM-MTDP, (ii) We also introduce techniques for
belief-based evaluation absent from previous work. Nonetheless, combining RMTDP with
408

fiHybrid BDI-POMDP Framework for Multiagent Teaming

COM-MTDP is an interesting avenue for further research and some preliminary steps in
this direction are presented in Nair, Tambe and Marsella (2003b).
Among other hybrid systems not focused on analysis, Scerri et al. (2002) employ Markov
Decision Processes within team-oriented programs for adjustable autonomy. The key difference between that work and ours is that the MDPs were used to execute a particular
sub-plan within the TOPs plan hierarchy and not for making improvements to the TOP.
DTGolog (Boutilier, Reiter, Soutchanski, & Thrun, 2000) provides a first-order language
that limits MDP policy search via logical constraints on actions. Although it shares with
our work the key idea of synergistic interactions in MDPs and Golog, it differs from our
work in that it focuses on single agent MDPs in fully observable domains, and does not
exploit plan structure in improving MDP performance. ISAAC (Nair, Tambe, Marsella,
& Raines, 2004), a system for analyzing multiagent teams, also employs decision theoretic
methods for analyzing multiagent teams. In that work, a probabilistic finite automaton
(PFA) that represents the probability distribution of key patterns in the teams behavior
are learned from logs of the teams behaviors. The key difference with that work is that the
analysis is performed without having access to the actual team plans that the agents are
executing and hence the advice provided cannot directly be applied to improving the team,
but will need a human developer to change the team behavior as per the advice generated.

7.4 Role Allocation and Reallocation
There are several different approaches to the problem of role allocation and reallocation.
For example, Tidhar et al. (1996) and Tambe et al. (2000) performed role allocation based
on matching of capabilities, while Hunsberger and Grosz (2000) proposed the use of combinatorial auctions to decide on how roles should be assigned. Modi et al. (2003) showed
how role allocation can be modeled as a distributed constraint optimization problem and
applied it to the problem of tracking multiple moving targets using distributed sensors.
Shehory and Kraus (1998) suggested the use of coalition formation algorithms for deciding
quickly which agent took on which role. Fatima and Wooldridge (2001) use auctions to
decide on task allocation. It is important to note that these competing techniques are not
free of the problem of how to model the problem, even though they do not have to model
transition probabilities. Other approaches to reforming a team are reconfiguration methods due to Dunin-Keplicz and Verbrugge (2001), self-adapting organizations by Horling
and Lesser (2001) and dynamic re-organizing groups (Barber & Martin, 2001). Scerri et
al. (2003) present a role (re)allocation algorithm that allows autonomy of role reallocation
to shift between a human supervisor and the agents.
The key difference with all this prior work is our use of stochastic models (RMTDPs)
to evaluate allocations: this enables us to compute the benefits of role allocation, taking
into account uncertainty and costs of reallocation upon failure. For example, in the mission
rehearsal domain, if uncertainties were not considered, just one scout would have been
allocated, leading to costly future reallocations or even in mission failure. Instead, with
lookahead, depending on the probability of failure, multiple scouts were sent out on one or
more routes, resulting in fewer future reallocations and higher expected reward.
409

fiNair & Tambe

8. Conclusion
While the BDI approach to agent teamwork has provided successful applications, tools and
techniques that provide quantitative analyses of team coordination and other team behaviors under uncertainty are lacking. The emerging field of distributed POMDPs provides
a decision theoretic method for quantitatively obtaining the optimal policy for a team of
agents, but faces a serious intractability challenge. Therefore, this article leverages the
benefits of both the BDI and POMDP approaches to analyze and improve key coordination
decisions within BDI-based team plans using POMDP-based methods. In order to demonstrate these analysis methods, we concentrated on role allocation  a fundamental aspect
of agent teamwork  and provided three key contributions. First, we introduced RMTDP,
a distributed POMDP based framework, for analysis of role allocation. Second, this article
presented an RMTDP-based methodology for optimizing key coordination decisions within
a BDI team plan for a given domain. Concretely, the article described a methodology for
finding the best role allocation for a fixed team plan. Given the combinatorially many
role allocations, we introduced methods to exploit task decompositions among sub-teams
to significantly prune the search space of role allocations.
Third, our hybrid BDI-POMDP approach uncovered several synergistic interactions
between BDI team plans and distributed POMDPs:
1. TOPs were useful in constructing the RMTDP model for the domain, in identifying
the features that need to be modeled as well as in decomposing the model construction
according to the structure of the TOP. The RMTDP model could then be used to
evaluate the TOP.
2. TOPs restricted the policy search by providing RMTDPs with incomplete policies
with a limited number of open decisions.
3. The BDI approach helped in coming up with a novel efficient belief-based representation of policies suited for this hybrid BDI-POMDP approach and a corresponding
algorithm for evaluating such policies. This resulted in faster evaluation and also a
more compact policy representation.
4. The structure in the TOP was exploited to decompose the problem of evaluating
abstract policies, resulting in significant pruning in the search for the optimal role
allocations.
We constructed RMTDPs for two domains  RoboCupRescue and mission rehearsal
simulation  and determined the best role allocation in these domains. Furthermore, we
illustrated significant speedups in RMTDP policy search due to the techniques introduced
in this article. Detailed experiments revealed the advantages of our approach over state-ofthe-art role allocation approaches that failed to reason with uncertainty.
Our key agenda for future work is to continue scale-up of RMTDPs to even larger
scale agent teams. Such scale-up will require further efficiency improvements. We propose
to continue to exploit the interaction in the BDI and POMDP approaches in achieving
such scale-up. For instance, besides disaster rescue, distributed sensor nets and large area
monitoring applications could benefit from such a scale-up.
410

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Acknowledgments
This research was supported by NSF grant #0208580. We would like to thank Jim Blythe,
Anthony Cassandra, Hyuckchul Jung, Spiros Kapetanakis, Sven Koenig, Michael Littman,
Stacy Marsella, David Pynadath and Paul Scerri for discussions related to this article.
We would also like to thank the reviewers of this article whose comments have helped in
significantly improving this article.

Appendix A. TOP details
In this section, we will describe the TOP for the helicopter scenario. The details of each
subplan in Figure 4(b) are shown below:
ExecuteMission:
Context:
Pre-conditions: (MB <TaskForce> location(TaskForce) = START)
Achieved: (MB <TaskForce> (Achieved(DoScouting)  Achieved(DoTransport)))
 (time > T  (MB <TaskForce> Achieved(RemainingScouts) 
( helo  ScoutingTeam, alive(helo)  location(helo) 6= END)))
Unachievable: (MB <TaskForce> Unachievable(DoScouting))
 (MB <TaskForce> (Unachievable(DoTransport)
 (Achieved(RemainingScouts)
( helo  ScoutingTeam, alive(helo)  location(helo) 6= END))))
Irrelevant: 
Body:
DoScouting
DoTransport
RemainingScouts
Constraints:
DoScouting  DoTransport
DoScouting  RemainingScouts
DoScouting:
Context:ExecuteMission <TaskForce>
Pre-conditions: 
Achieved: 
Unachievable: 
Irrelevant:
Body:
WaitAtBase
ScoutRoutes
Constraints:
WaitAtBase AND ScoutRoutes
WaitAtBase:
Context: DoScouting <TaskForce>
Pre-conditions: 
Achieved: 
Unachievable: (MB <TransportTeam>  helo  TransportTeam, alive(helo))
411

fiNair & Tambe

Irrelevant:
Body:
no-op



ScoutRoutes:
Context: DoScouting <TaskForce>
Achieved: 
Unachievable: 
Irrelevant:(MB <ScoutingTeam>  helo  TransportTeam, alive(helo))
Body:
ScoutRoute1
ScoutRoute2
ScoutRoute3
Constraints:
ScoutRoute1 OR ScoutRoute2 OR ScoutRoute3
ScoutRoute1:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions: 
Achieved: (MB <SctTeamA>  helo  SctTeamA, location(helo) = END)
Unachievable: time > T  (MB <SctTeamA>  helo  SctTeamA, alive(helo))
Irrelevant: 
Body:
if (location(SctTeamA) = START) then route(SctTeamA)  1
if (location(SctTeamA) 6= END) then move-forward
ScoutRoute2:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions: 
Achieved: (MB <SctTeamB>  helo  SctTeamB, location(helo) = END)
Unachievable: time > T  (MB <SctTeamB>  helo  SctTeamB, alive(helo))
Irrelevant: 
Body:
if (location(SctTeamB) = START) then route(SctTeamB)  2
if (location(SctTeamB) 6= END) then move-forward
ScoutRoute2:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions: 
Achieved: (MB <SctTeamA>  helo  SctTeamA, location(helo) = END)
Unachievable: time > T  (MB <SctTeamA>  helo  SctTeamA, alive(helo))
Irrelevant: 
Body:
if (location(SctTeamA) = START) then route(SctTeamA)  1
if (location(SctTeamA) 6= END) then move-forward
DoTransport:
Context: ExecuteMission <TaskForce>
Pre-conditions: 
412

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Achieved: (MB <TransportTeam> location(TransportTeam) = END)
Unachievable: time > T  (MB <TransportTeam>  helo  TransportTeam, alive(helo))
Irrelevant: 
Body:
if (location(TransportTeam) = start) then
if (MB <TransportTeam> Achieved(ScoutRoute1)) then
route(TransportTeam)  1
elseif (MB <TransportTeam> Achieved(ScoutRoute2)) then
route(TransportTeam)  2
elseif (MB <TransportTeam> Achieved(ScoutRoute3)) then
route(TransportTeam)  3
if (route(TransportTeam) 6= null) and (location(TransportTeam) 6= END) then
move-forward
RemainingScouts:
Context: ExecuteMission <TaskForce>
Pre-conditions: 
Achieved: (MB <ScoutingTeam> location(ScoutingTeam) = END)
Unachievable: time > T  (MB <ScoutingTeam> ( helo  ScoutingTeam
alive(helo)  location(helo) 6= END))
Irrelevant: 
Body:
if (location(ScoutingTeam) 6= END) then move-forward

The predicate Achieved(tplan) is true if the Achieved conditions of tplan are true. Similarly, the predicates Unachievable(tplan) and Irrelevant(tplan) are true if the the Unachievable conditions and the Irrelevant conditions of tplan are true, respectively. The predicate
(location(team) = END) is true if all members of team are at END.
Figure 4(b) also shows coordination relationships: An AND relationship is indicated
with a solid arc, while an OR relationship is indicated with a dotted arc. These coordination relationships indicate unachievability, achievability and irrelevance conditions that
are enforced by the TOP infrastructure. An AND relationship between team sub-plans
means that if any of the team sub-plans fail, then the parent team plan will fail. Also, for
the parent team plan to be achieved, all the child sub-plans must be achieved. Thus, for
DoScouting, WaitAtBase and ScoutRoutes must both be done:
Achieved: (MB <TaskForce> Achieved(WaitAtBase)  Achieved(ScoutRoutes))
Unachievable: (MB <TaskForce> Unachievable(WaitAtBase)
 Unachievable(ScoutRoutes))

An OR relationship means that all the subplans must fail for the parent to fail and success of
any of the subplans means that the parent plan has succeeded. Thus, for ScoutingRoutes,
at least one of ScoutRoute1, ScoutRoute2 or ScoutRoute3 need be performed:
(MB <ScoutingTeam> Achieved(ScoutRoute1) 
Achieved(ScoutRoute2) Achieved(ScoutRoute3))
Unachievable: (MB <TaskForce> Unachievable(ScoutRoute1) 
Unachievable(ScoutRoute2)  Unachievable(ScoutRoute3))
Achieved:

413

fiNair & Tambe

Also an AND relationship affects the irrelevance conditions of the subplans that it joins. If
the parent is unachievable then all its subplans that are still executing become irrelevant.
Thus, for WaitAtBase:
Irrelevant:

(MB <TaskForce> Unachievable(ScoutRoutes))

Similarly for ScoutingRoutes:
Irrelevant:

(MB <TaskForce> Unachievable(ScoutRoutes))

.
Finally, we assign roles to plans  Figure 4(b) shows the assignment in brackets adjacent to the plans. For instance, Task Force team is assigned to jointly perform Execute
Mission.

Appendix B. RMTDP details
In this section, we present details of the RMTDP constructed for the TOP in Figure 4.
 S: We get the features of the state from the attributes tested in the preconditions
and achieved, unachievable and irrelevant conditions and the body of the team plans
and individual agent plans. Thus the relevant state variables are:location of each
helicopter, role of each helicopter,route of each helicopter, status of each helicopter
(alive or not) and time. For a team of n helicopters, the state is given by the tuple
< time, role1 , . . . , rolen , loc1 , . . . , locn , route1 , . . . , routen , status1 , . . . , statusn >.
 A: We consider actions to be the primitive actions that each agent can perform
within its individual plans. The TOP infrastructure enforces mutual belief through
communication actions. Since analyzing the cost of these is not the focus of this
research we consider communication to be implicit and we model the effect of this
communication directly in the observation function.
We consider 2 kinds of actions role-taking and role-execution actions. We assume
that the initial allocation will specify roles for all agents. This specifies whether the
agent is a scout or a transport and if a scout which scout team it is assigned to. A
scout cannot become a transport or change its team after its initial allocation while a
transport can change its role by taking one of the role-taking actions.The role-taking
and role-execution actions for each agent i are given by:
i,memberT ransportT eam = {joinSctT eamA, joinSctT eamB, joinSctT eamC}
i,memberSctT eamA = i,memberSctT eamB = i,memberSctT eamCx = 
i,memberT ransportT eam = {chooseRoute, moveF orward}
i,memberSctT eamA = i,memberSctT eamB = i,memberSctT eamC = {moveF orward}
 P : We obtain the transition function with the help of a human expert or through
simulations if a simulator is available. In this domain, helicopters can crash (be shot
down) if they are not at START, END or an already scouted location. The probability
that scouts will get shot down depends on which route they are on, i.e. probability
of crash on route1 is p1 , probability of crash on route2 is p2 and probability of crash
on route3 is p3 and how many scouts are on the same spot. We assume that the
414

fiHybrid BDI-POMDP Framework for Multiagent Teaming

probability of a transport being shot down in an unscouted location to be 1 and in
a scouted location to be 0. The probability of multiple crashes can be obtained by
multiplying the probabilities of individual crashes.
The action, moveForward, will have no effect if routei = null or loci = END or if
statusi = dead. In all other cases, the location of the agent gets incremented. We
assume that the role-taking actions scoutRoutex will always succeed if the role of the
performing agent is transport and it has not been assigned a route already.
 : Each transport at START can observe the status of the other agents with some
probability depending on their positions. Each helicopter on a particular route can
observe all the helicopters on that route completely and cannot observe helicopters
on other routes.
 O: The observation function gives the probability for a group of agents to receive a
particular joint observation. In this domain we assume that observations of one agent
are independent of the observations of other agents, given the current state and the
previous joint action. Thus the probability of a joint observation can be computed by
multiplying the probabilities of each individual agents observations.
The probability of a transport at START observing the status of an alive scout on
route 1 is 0.95. The probability of a transport at START observing nothing about
that alive scout is 0.05 since we dont have false negatives. Similarly if a scout on
route 1 crashes, the probability that this is visible to a transport at START is 0.98
and the probability that the transport doesnt see this failure is 0.02. Similarly the
probabilities for observing an alive scout on route 2 and route 3 and 0.94 and 0.93
respectively and the probabilities for observing a crash on route 2 and route 3 and
0.97 and 0.96 respectively.
 R: The reward function is obtained with the help of a human expert who helps
assign value to the various states and the cost of performing various actions. For
this analysis, we assume that actions moveForward and chooseRoute have no cost.
We consider the negative reward (cost) for the replacement action, scoutRoutex, to
be R , the negative reward for a failure of a helicopter to be RF , the reward for a
scout reaching END to be Rscout and the reward for a transport reaching END to be
Rtransport . E.g. R = 10, RF = 50, Rscout = 5, Rtransport = 75.
 RL: These are the roles that individual agents can take in TOP organization hierarchy.
RL = {transport, scoutOnRoute1, scoutOnRoute2, scoutOnRoute3}.

Appendix C. Theorems
Theorem 3 The MAXEXP method will always yield an upper bound.
Proof sketch:
 Let policy   be the leaf-level policy with the highest expected reward under a particular parent node, i, in the restricted policy space.
V = maxChildren(i) V
415

(3)

fiNair & Tambe

 Since the reward function is specified separately for each component, we can separate the expected reward V into the rewards from the constituent components given
the starting states and starting observation histories of these components. Let the
team plan be divided into m components such that the components are parallel and
independent or sequentially executed.
X
V 
maxstates[j],oHistories[j]Vj
1jm

 The expected value obtained for any component j, 1  j  m for   cannot be greater
than that of the highest value obtained for j using any policy.
maxstates[j],oHistories[j]Vj  maxChildren(i) maxstates[j],oHistories[j](Vj )

(4)

 Hence,
V  

X

maxChildren(i) maxstates[j],oHistories[j](Vj )

1jm

V  MaxEstimate(i)

(5)



References
Barber, S., & Martin, C. (2001). Dynamic reorganization of decision-making groups. In
Proceedings of the Fifth International Conference on Autonomous Agents (Agents-01),
pp. 513520.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. In Proceedings of the Second International
Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS-03), pp.
4148.
Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). The complexity of decentralized control of MDPs. In Proceedings of the Sixteenth Conference on Uncertainty in
Artificial Intelligence(UAI-00), pp. 3237.
Boutilier, C. (1996). Planning, learning & coordination in multiagent decision processes. In
Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge (TARK-96), pp. 195210.
Boutilier, C., Reiter, R., Soutchanski, M., & Thrun, S. (2000). Decision-theoretic, highlevel agent programming in the situation calculus. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence (AAAI-00), pp. 355362.
Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: A simple, fast,
exact method for partially observable Markov decision processes. In Proceedings of
the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97),
pp. 5461.
416

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Chades, I., Scherrer, B., & Charpillet, F. (2002). A heuristic approach for solving
decentralized-pomdp: Assessment on the pursuit problem. In Proceedings of the 2002
ACM Symposium on Applied Computing (SAC-02), pp. 5762.
Cohen, P. R., & Levesque, H. J. (1991). Teamwork. Nous, 25 (4), 487512.
da Silva, J. L. T., & Demazeau, Y. (2002). Vowels co-ordination model. In Proceedings
of the First International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS-2002), pp. 11291136.
Dean, T., & Lin, S. H. (1995). Decomposition techniques for planning in stochastic domains. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence (IJCAI-95), pp. 11211129.
Decker, K., & Lesser, V. (1993). Quantitative modeling of complex computational task
environments. In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93), pp. 217224.
Dix, J., Muoz-Avila, H., Nau, D. S., & Zhang, L. (2003). Impacting shop: Putting an
ai planner into a multi-agent environment. Annals of Mathematics and Artificial
Intelligence, 37 (4), 381407.
Dunin-Keplicz, B., & Verbrugge, R. (2001). A reconfiguration algorithm for distributed
problem solving. Engineering Simulation, 18, 227246.
Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity and expressivity. In
Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94),
pp. 11231128.
Fatima, S. S., & Wooldridge, M. (2001). Adaptive task and resource allocation in multiagent systems. In Proceedings of the Fifth International Conference on Autonomous
Agents (Agents-01), pp. 537544.
Georgeff, M. P., & Lansky, A. L. (1986). Procedural knowledge. Proceedings of the IEEE
special issue on knowledge representation, 74, 13831398.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange in cooperative
multi-agent systems. In Proceedings of the Second International Joint Conference on
Autonomous Agents and Multi Agent Systems (AAMAS-03), pp. 137144.
Grosz, B., Hunsberger, L., & Kraus, S. (1999). Planning and acting together. AI Magazine,
20 (4), 2334.
Grosz, B., & Kraus, S. (1996). Collaborative plans for complex group action. Artificial
Intelligence, 86 (2), 269357.
Guestrin, C., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination and planning with factored MDPs. In Proceedings of the Eighteenth National
Conference on Artificial Intelligence (AAAI-02), pp. 253259.
Hansen, E., & Zhou, R. (2003). Synthesis of hierarchical finite-state controllers for pomdps.
In Proceedings of the Thirteenth International Conference on Automated Planning and
Scheduling (ICAPS-03), pp. 113122.
417

fiNair & Tambe

Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming for partially
observable stochastic games. In Proceedings of the Nineteenth National Conference
on Artificial Intelligence (AAAI-04), pp. 709715.
Ho, Y.-C. (1980). Team decision theory and information structures. Proceedings of the
IEEE, 68 (6), 644654.
Horling, B., Benyo, B., & Lesser, V. (2001). Using self-diagnosis to adapt organizational
structures. In Proceedings of the Fifth International Conference on Autonomous
Agents (Agents-01), pp. 529536.
Hunsberger, L., & Grosz, B. (2000). A combinatorial auction for collaborative planning. In
Proceedings of the Fourth International Conference on Multiagent Systems (ICMAS2000), pp. 151158.
Jennings, N. (1995). Controlling cooperative problem solving in industrial multi-agent
systems using joint intentions. Artificial Intelligence, 75 (2), 195240.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101 (2), 99134.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,
S. (1999). RoboCup-Rescue: Search and rescue for large scale disasters as a domain
for multiagent research. In Proceedings of IEEE Conference on Systems, Men, and
Cybernetics (SMC-99), pp. 739743.
Levesque, H. J., Cohen, P. R., & Nunes, J. (1990). On acting together. In Proceedings of the
National Conference on Artificial Intelligence, pp. 9499. Menlo Park, Calif.: AAAI
press.
Mailler, R. T., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. In Proceedings of the Third International Joint Conference
on Agents and Multiagent Systems (AAMAS-04), pp. 438445.
Marschak, J., & Radner, R. (1972). The Economic Theory of Teams. Cowles Foundation
and Yale University Press, New Haven, CT.
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2003). An asynchronous complete
method for distributed constraint optimization. In Proceedings of the Second International Joint Conference on Agents and Multiagent Systems (AAMAS-03), pp.
161168.
Monahan, G. (1982). A survey of partially observable Markov decision processes: Theory,
models and algorithms. Management Science, 101 (1), 116.
Nair, R., Ito, T., Tambe, M., & Marsella, S. (2002). Task allocation in the rescue simulation
domain. In RoboCup 2001: Robot Soccer World Cup V, Vol. 2377 of Lecture Notes in
Computer Science, pp. 751754. Springer-Verlag, Heidelberg, Germany.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003a). Taming decentralized
POMDPs: Towards efficient policy computation for multiagent settings. In Proceedings
of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03),
pp. 705711.
418

fiHybrid BDI-POMDP Framework for Multiagent Teaming

Nair, R., Tambe, M., & Marsella, S. (2003b). Team formation for reformation in multiagent domains like RoboCupRescue. In Kaminka, G., Lima, P., & Roja, R. (Eds.),
Proceedings of RoboCup-2002 International Symposium, pp. 150161. Lecture Notes
in Computer Science, Springer Verlag.
Nair, R., Tambe, M., Marsella, S., & Raines, T. (2004). Automated assistants to analyze
team behavior. Journal of Autonomous Agents and Multi-Agent Systems, 8 (1), 69
111.
Papadimitriou, C., & Tsitsiklis, J. (1987). Complexity of Markov decision processes. Mathematics of Operations Research, 12 (3), 441450.
Peshkin, L., Meuleau, N., Kim, K.-E., & Kaelbling, L. (2000). Learning to cooperate via
policy search. In Proceedings of the Sixteenth Conference in Uncertainty in Artificial
Intelligence (UAI-00), pp. 489496.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. In Proceedings of
Advances in Neural Information Processing Systems 16 (NIPS).
Pynadath, D. V., & Tambe, M. (2002). The communicative multiagent team decision
problem: Analyzing teamwork theories and models. Journal of Artificial Intelligence
Research, 16, 389423.
Pynadath, D. V., & Tambe, M. (2003). Automated teamwork among heterogeneous software agents and humans. Journal of Autonomous Agents and Multi-Agent Systems
(JAAMAS), 7, 71100.
Rich, C., & Sidner, C. (1997). COLLAGEN: When agents collaborate with people. In
Proceedings of the First International Conference on Autonomous Agents (Agents97), pp. 284291.
Scerri, P., Johnson, L., Pynadath, D., Rosenbloom, P., Si, M., Schurr, N., & Tambe, M.
(2003). A prototype infrastructure for distributed robot, agent, person teams. In
Proceedings of the Second International Joint Conference on Agents and Multiagent
Systems (AAMAS-03), pp. 433440.
Scerri, P., Pynadath, D. V., & Tambe, M. (2002). Towards adjustable autonomy for the
real-world. Journal of Artificial Intelligence (JAIR), 17, 171228.
Schut, M. C., Wooldridge, M., & Parsons, S. (2001). Reasoning about intentions in uncertain domains. In Proceedings of the Sixth European Conference on Symbolic and
Quantitative Approaches to Reasoning with Uncertainty (ECSQARU-2001), pp. 84
95.
Shehory, O., & Kraus, S. (1998). Methods for task allocation via agent coalition formation.
Artificial Intelligence, 101 (1-2), 165200.
Sondik, E. J. (1971). The optimal control of partially observable Markov processes. Ph.D.
Thesis, Stanford.
Stone, P., & Veloso, M. (1999). Task decomposition, dynamic role assignment, and lowbandwidth communication for real-time strategic teamwork. Artificial Intelligence,
110 (2), 241273.
419

fiNair & Tambe

Tambe, M. (1997). Towards flexible teamwork. Journal of Artificial Intelligence Research,
7, 83124.
Tambe, M., Pynadath, D., & Chauvat, N. (2000). Building dynamic agent organizations in
cyberspace. IEEE Internet Computing, 4 (2), 6573.
Tidhar, G. (1993a). Team-oriented programming: Preliminary report. Tech. rep. 41, Australian Artificial Intelligence Institute.
Tidhar, G. (1993b). Team-oriented programming: Social structures. Tech. rep. 47, Australian Artificial Intelligence Institute.
Tidhar, G., Rao, A., & Sonenberg, E. (1996). Guided team selection. In Proceedings of the
Second International Conference on Multi-agent Systems (ICMAS-96), pp. 369376.
Wooldridge, M. (2002). An Introduction to Multiagent Systems. John Wiley & Sons.
Xuan, P., & Lesser, V. (2002). Multi-agent policies: from centralized ones to decentralized ones. In Proceedings of the First International Joint Conference on Agents and
Multiagent Systems (AAMAS-02), pp. 10981105.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions in multiagent
cooperation. In Proceedings of the Fifth International Conference on Autonomous
Agents (Agents-01), pp. 616623.
Yen, J., Yin, J., Ioerger, T. R., Miller, M. S., Xu, D., & Volz, R. A. (2001). Cast: Collaborative agents for simulating teamwork. In Proceedings of the Seventeenth International
Joint Conference on Artificial Intelligence (IJCAI-01), pp. 11351144.
Yoshikawa, T. (1978). Decomposition of dynamic team decision problems. IEEE Transactions on Automatic Control, AC-23 (4), 627632.

420

fiJournal of Artificial Intelligence Research 23 (2005) 79-122

Submitted 2/04; published 2/05

Reinforcement Learning for Agents with Many Sensors and
Actuators Acting in Categorizable Environments
Josep M Porta

porta@science.uva.nl

IAS Group, Informatics Institute
University of Amsterdam
Kruislaan 403, 1098SJ, Amsterdam, The Netherlands

Enric Celaya

celaya@iri.upc.edu

Institut de Robotica i Informatica Industrial
Spanish Council of Scientific Research (CSIC)
Llorens i Artigas 4-6, 08028, Barcelona, Spain

Abstract
In this paper, we confront the problem of applying reinforcement learning to agents that
perceive the environment through many sensors and that can perform parallel actions using
many actuators as is the case in complex autonomous robots. We argue that reinforcement
learning can only be successfully applied to this case if strong assumptions are made on
the characteristics of the environment in which the learning is performed, so that the
relevant sensor readings and motor commands can be readily identified. The introduction
of such assumptions leads to strongly-biased learning systems that can eventually lose the
generality of traditional reinforcement-learning algorithms.
In this line, we observe that, in realistic situations, the reward received by the robot
depends only on a reduced subset of all the executed actions and that only a reduced subset
of the sensor inputs (possibly different in each situation and for each action) are relevant to
predict the reward. We formalize this property in the so called categorizability assumption
and we present an algorithm that takes advantage of the categorizability of the environment,
allowing a decrease in the learning time with respect to existing reinforcement-learning
algorithms. Results of the application of the algorithm to a couple of simulated realisticrobotic problems (landmark-based navigation and the six-legged robot gait generation) are
reported to validate our approach and to compare it to existing flat and generalizationbased reinforcement-learning approaches.

1. Introduction
The division between knowledge-based and behavior-based artificial intelligence has been
fundamental to achieving successful applications within the field of autonomous robots (Arkin,
1998). However, up to now, this division has had few repercussions for reinforcement learning. Within artificial intelligence, reinforcement learning has been formalized in a very
general way borrowing ideas from the dynamic programming and decision-theory fields.
Within this formalization, the objective of reinforcement-learning methods is to establish
a correct mapping from a set of abstract observations (formalized as states) to a set of
high level actions, without being worried about how these sets of states and actions are
defined (for an introduction to reinforcement learning you can check Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998, among many others). Algorithms developed within
this general framework can be used in different fields without any modification. For each
c
2005
AI Access Foundation. All rights reserved.

fiPorta & Celaya

particular application, the definition of the sets of states and actions is the responsibility
of the programmer and is not supposed to be part of the reinforcement-learning problem.
However, as clearly pointed by Brooks (1991), in autonomous robots the major hurdles
are those related with perception and action representations. For this reason, in a robotic
task, what traditional reinforcement-learning research assumes to be the major problem
(connecting states and actions) is simpler than what it assumes as given (the definition
of states and actions). The consequence is that existing reinforcement-learning methods
are best suited for problems that fall into the symbolic artificial intelligence domain than
for those that belong to robotics. Due to the generality of existing reinforcement-learning
algorithms, a robotic problem can be analyzed and re-formulated so that it can be tackled
with the available reinforcement-learning tools but, in many cases, this re-formulation is
too awkward introducing unnecessary complexity in the learning process. The alternative
we explore in this paper is a new reinforcement-learning algorithm that can be applied to
robotic problems as they are, without any re-formulation.
As Brooks (1991) remarked, dealing with a real environment is not necessarily a problem
since real environments have properties that can be exploited to reduce the complexity of the
robots controller. In Brooks works, we can find simple robot controllers that achieve very
good performance in particular environments. This is clearly in contrast with the generality
pursued within reinforcement learning. Following an idea parallel to that of Brooks, in this
paper, we present a new reinforcement-learning algorithm that takes advantage of a specific
environment-related property (that we call categorizability) to efficiently learn to achieve
a given task. We formalize the categorizability property and we present a representation
system (partial rules) to exploit this property. A remarkable feature of this representation
system is that it allows generalization in both the spaces of sensors and actions, using
a uniform mechanism. This ability to generalize in both the state and action spaces is
fundamental to successfully apply reinforcement learning to autonomous robots.
This paper is organized as follows. First, in Section 2, we formalize reinforcement learning from the point of view of its use in the field of autonomous robotics and we describe
the problems that make flat (and, in most cases, also generalization-based) reinforcementlearning algorithms not adequate for this case. Section 3 presents the categorizability assumption which is plausible in most robotics environments. Then, in Section 4, we describe
an alternative reinforcement-learning algorithm that exploits the categorizability assumption to circumvent the problems present in existing approaches. In Section 5, we analyze
the points of contact between our proposal and already existing work. Next, in Section 6,
we present experiments that validate our approach. The experiments are performed in
simulations that mimic realistic robotic applications where the categorizability assumption
is likely to be valid. Finally, in Section 7, we conclude by analyzing the strengths and
weaknesses of the proposed learning system.
Additionally, Appendix A provides a detailed description of the partial-rule learning
algorithm introduced in this paper, Appendix B is devoted to an enhancement on this
algorithm to make its execution more efficient, and Appendix C summarizes the notation
we use throughout the paper.
80

fiReinforcement Learning in Categorizable Environments

2. Problem Formalization
For simplicity, we assume that the robot perceives its environment through a set of binary
feature detectors1 F D = {fdi | i = 1..nf }. A feature detector can be devised as a process
that identifies specific combinations of present (and possibly past) sensor readings. The
use of feature detectors is very common in robotics. In this field, feature detectors are
defined by the programmer attending to the special characteristics of the environment, the
robot sensors, and the task to be executed in order to extract potentially useful information
(presence of landmarks or obstacles, . . . ) from raw sensor readings.
In a similar way, instead of working directly with the space of actions provided by
the robot motors (that define a too low-level way of controlling the robot), it is a common
practice to define a set of elementary actions EA = {eai |i = 1..ne }. An elementary action is
a specific sequence/combination of motor commands defined by the programmer attending
to the characteristics of the robot and the task to be achieved. To simplify, we can assume
that elementary actions are of the form (mi  k) (i  [1..nm ]) where mi is a motor and k
a value in the range of valid inputs for the motor mi . This framework is quite flexible since
a motor mi can be either one of the physical motors of the robot or a high-level, abstract
motor that combines movements of the actual motors. With this formalization, at a given
moment, the robot can execute in parallel as many elementary actions as available motors.
A robot controller can be seen as a procedure that executes (combinations of elementary)
actions in response to specific situations (i.e., activation of specific feature detectors) with
the objective of achieving a given task. Reinforcement-learning approaches automatically
define such a controller using the information provided by the reward signal. In the context
of reinforcement learning, the controller is called the policy of the learner.
The objective of the value-function-based reinforcement-learning algorithms (the most
common reinforcement-learning algorithms) is to predict the reward that can be directly or
indirectly obtained from the execution of each action (i.e., of each combination of elementary
actions) in each possible situation, described as a combination of active and inactive feature
detectors. If this prediction is available, the action to be executed in each situation is the
one from which maximum reward is expected.
To predict the reward, classic reinforcement-learning algorithms rely on the Markov
assumption, that requires a state signal to carry enough information to determine the effects
of all actions in a given situation.2 Additionally, non-generalizing reinforcement-learning
algorithms assume that the states of the system must be learned about independently. So,
the information gathered about the effects of an action a in a given state s, denoted Q(s, a),
cannot be safely transferred to similar states or actions. With this assumption, the cost of
a reinforcement-learning algorithm in a general problem is
(ns na ),
where ns is the number of states and na is the number of actions. This is because each
action has to be tried as least once in each state. Since the state is defined as the observed
1. Non-binary feature detectors providing a discrete range of values can be readily binarized.
2. Non-Markovian problems, when confronted, should be converted into Markovian ones. How to do that
is out of the scope of this paper, although it is one of the most relevant points to achieve a successful
real-world reinforcement-learning application.

81

fiPorta & Celaya

combination of feature detectors, we have that the potential number of states is
ns = 2 nf ,
with nf the number of feature detectors. Consequently, we have that
(ns na ) = (2nf na ),
which is exponential in the number of feature detectors. Since the number of feature detectors used in robotic applications tends to be high, non-generalizing reinforcement learning
becomes impractical for realistic problems. This is the well known curse of dimensionality
introduced by Bellman (1957), whose research presaged some of the work in reinforcement
learning.
Although the size of the action set (na ) is as important as the size of the state set (ns )
in the curse of dimensionality, less attention is paid to actions in the reinforcement-learning
literature. However, a robot with many degrees of freedom can execute many elementary
actions simultaneously and this makes the cost of the learning algorithms also increase
exponentially with the number of motors of the robot (nm ).
Suppose we address the same task but with two different sets of feature detectors F D 1
and F D2 such that F D1  F D2 . Using a plain reinforcement-learning algorithm, the cost
of finding a proper policy would be larger using the larger set of features (F D 2 ). And this
is so even if one of the features in F D2  F D1 has a stronger correlation with the reward
than any of the features in F D1 . Non-generalizing reinforcement-learning algorithms are
not able to take advantage of this situation, and, even having better input information,
their performance decreases. A similar argument can be made for actions in addition to
feature detectors.
Generalizing reinforcement-learning algorithms such as those using gradient-descent
techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart,
1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) or decision
trees (Chapman & Kaelbling, 1991; McCallum, 1995) can partially palliate this problem
since they can deal with large state spaces. However, as we approach complex realistic
problems, the number of dimensions of the state-space grows to the point of making the use
of some of these generalization techniques impractical and other function approximation
techniques must be used (Sutton & Barto, 1998, page 209).
Adding relevant inputs or actions to a task should make this task easier or at least
not more difficult. Only methods whose complexity depends on the relevance of the available inputs and actions and not on their number would scale well to real domain problems.
Examples of systems fulfilling this property are, for instance, the Kanerva coding system presented by Kanerva (1988) and the random representation method by Sutton and Whitehead
(1993). While those systems rely on large collections of fixed prototypes (i.e., combinations
of feature detectors) selected at random, our proposal is to search for the appropriate prototypes, but using a strong bias so that the search can be performed in a reasonable time.
This strong bias is based on the categorizability assumption that is a plausible assumption
for the case of autonomous robots, which allows a large speed up in the learning process.
Additionally, existing systems do not address the problem of determining the relevance of
actions, since they assume the learning agent has a single actuator (that is, obviously, the
82

fiReinforcement Learning in Categorizable Environments

only relevant one). This simple set up is not adequate for robotics. In our approach (presented below), combinations of both feature detectors and elementary actions are considered
using a unified framework.

3. The Categorizability Assumption
From our experience developing controllers for autonomous robots, we observe that, in many
realistic situations, the reward received by the robot depends only on a reduced subset of
all the actions executed by the robot and that most of the sensor inputs are irrelevant to
predict that reward. Thus, for example, the value resulting from the action of grasping
the object in front of the robot will depend on what the object is: the object the robot
should bring to the user, an electrified cable, or an unimportant object. However, the result
will probably be the same whether or not the robot is moving its cameras while grasping
the object, if it is day or night, if the robot is, at the same time, checking the distance to
the nearest wall, or if it can see a red light nearby or not (aspects, all of them, that may
become important in other circumstances).
If an agent observes and acts in an environment where a reduced fraction of the available inputs and actuators have to be considered at a time, we say that the agent is in a
categorizable environment.
Categorizability is not a binary predicate but a graded property. In the completely
categorizable case, it would be necessary to pay attention to only one sensor/motor in
each situation. On the other extreme of the spectrum, if all motors have to be carefully
coordinated to achieve the task and the effect of each action could only be predicted by
taking into account the value of all feature detectors, we would say that the environment is
not categorizable at all.
Since robots have large collection of sensors providing a heterogeneous collection of
inputs and many actuators affecting quite different degrees of freedom, our hypothesis is
that, in robotic problems, environments are highly categorizable and, in those cases, an
algorithm biased by the categorizability assumption would result advantageous.

4. Reinforcement Learning in Categorizable Environments: the Partial
Rule Approach
To implement an algorithm able to exploit the potential categorizability of the environment,
we need a representation system able to transfer information between similar situations and
also between similar actions.
Clustering techniques or successive subdivisions of the state space (as, for instance, that
presented by McCallum, 1995) focus on the perception side of the problem and aim at
determining the reward that can be expected in a given state s considering only some of
the feature detectors perceived in that state. This subset of relevant feature detectors is
used to compute the expected reward in this state for any possible action a (the Q(s, a)
function). However, with this way of posing the problem the curse of dimensionality problem
is not completely avoided since some of the features can be relevant for one action but
not for another and this produces an unnecessary (from the point of view of each action)
differentiation between equivalent situations, decreasing the learning speed. This problem
83

fiPorta & Celaya

can be avoided by finding the specific set of relevant feature detectors for each action. In
this case, the Q function is computed as Q(fs (a), a), with a state definition that is function
of the action under consideration. This technique is used, for instance, by Mahadevan and
Connell (1992). Unfortunately, in the problem we are confronting, this is not enough since,
in our case, actions are composed by combinations of elementary actions and we also want to
transfer reward information between similar combinations of actions. Therefore, we have to
estimate Q(fs (a), a) only taking into account some of the elementary actions that compose
a. However, in principle, the relevance of elementary actions is function of the situation (or,
equivalently, of the state): a given elementary action can be relevant in some situations but
not in others. For this reason, the function to approximate becomes Q(f s (a), fa (s)) where
there is a cross-dependency between the state defined as a function of the action, f s (a), and
the action defined as a function of the state, fa (s). The proposal we detail next solves this
cross-dependency by working in the Cartesian product of the spaces of feature detectors
and elementary actions combinations.
To formalize our proposal, we introduce some definitions.
We say that the agent perceives (or observes) a partial view of order k, v(fd i1 , . . . , fdik ),
k  nf whenever the predicate fdi1  ...  fdik holds.3 Obviously, many partial views can be
perceived at the same time.
At a given moment, the agent executes an action a that issues a different command for
each one of the agents motors a = {ea1 , . . . , eanm }, with nm the number of motors.
A partial command of order k, noted as c(eai1 , . . . , eaik ), k  nm , is executed whenever
the elementary actions {eai1 , . . . , eaik } are executed simultaneously. We say that a partial
command c and an action a are in accordance if c is a subset of a. Note that the execution
of a given action a supposes the execution of all the partial commands in accordance with
it.
A partial rule w is defined as a pair w = (v, c), where v is a partial view and c is a
partial command. We say that a partial rule w = (v, c) is active if v is observed, and that w
is used whenever the partial view v is perceived and the partial command c is executed. A
partial rule covers a sub-area of the Cartesian product of feature detectors and elementary
actions and, thus, it defines a situation-action rule that can be used to partially determine
the actions of the robot in many situations (all those where the partial view of the rule is
active). The order of a partial rule is defined as the sum of the order of the partial view
and the order of the partial command that compose the rule.
We associate a quantiy qw to each partial rule. qw is an estimation of the value (i.e., the
discounted cumulative reward) that can be obtained after executing c when v is observed
at time t:

X
qw =
 t+i rt+i ,
i=0

with rt+i the reward received by the learner at time step t + i after rule w is used at time
t. So, a partial rule can be interpreted as: if partial view v is observed then the execution
of partial command c results in value qw .
3. A partial view can also include negations of feature detectors since the non-detection of a feature can be
as relevant as its detection.

84

fiReinforcement Learning in Categorizable Environments

The objective of the learning process is that of deriving a set of partial rules and adjusting
the corresponding qw values so that the desired task can be properly achieved.
The apparent drawback of the partial-rule representation is that the number of possible
partial rules is much larger than the number of state and action pairs: The number of
partial rules that can be defined on a set of nf binary feature detectors and nm binary
motors is 3nf +nm , while the number of different states and action pairs is only 2nf +nm .
If arbitrary problems have to be confronted (as is the case in synthetic learning situations),
the partial-rule approach could not be useful. However, problems confronted by robots
are not arbitrary since, as mentioned, environments present regularities or properties (as
categorizability) that can be exploited to reduce the complexity of the controller necessary
to achieve a given task.
Using the partial-rule framework, the categorizability assumption can be formally defined
as:
Definition 1 We say that an environment/task is highly categorizable if there exists a set
of low-order partial rules that allows us to predict the reward with the same accuracy as if
statistics for each possible state-action combination were considered. The lower the order
of the rules in the controller the higher the categorizability of the environment/task.
To the extent the categorizability assumption is fulfilled, the number of partial rules
necessary to control the robot becomes much smaller than the number of state-action pairs
that can be defined using the same sets of feature detectors and elementary actions in which
the partial views and partial commands are based. Additionally, categorizability implies
that the rules necessary in the controller are mostly those with lower order and this can
be easily exploited to bias the search in the space of partial rules. So, if the environment
is categorizable, the use of the partial-rule approach can suppose an important increase
in the learning speed and a reduction in the use of memory with respect to traditional
non-generalizing reinforcement-learning algorithms.
In the following sections, we describe how it is possible to estimate the effect of an
action given a fixed set of partial rules. This evaluation, repeated for all actions, is used
to determine the best action to be executed at a given moment. Next, we detail how it is
possible to adjust the value predictions of a fixed set of partial rules. Finally, we describe how
the categorizability assumption allows us to use an incremental strategy in the generation
of new partial rules. This strategy results in faster learning than existing generalizing and
non-generalizing reinforcement-learning algorithms. All procedures are described in highlevel form to make the explanation more clear. Details of their implementation can be found
in Appendix A.
4.1 Value Prediction using Partial Rules
In a given situation, many partial views are simultaneously active triggering a subset of the
partial rules of the controller C. We call this subset the active partial rules and we denote
it as C 0 . To evaluate a given action a we only take into account the rules in C 0 with a
partial command in accordance with a. We denote this subset as C 0 (a). Note that, in our
approach, when we refer to an action, we mean the corresponding set of elementary actions
(one per motor) and not a single element, as it is the general case in reinforcement learning.
85

fiPorta & Celaya

Every rule w = (v, c) in C 0 (a) provides a value prediction for a: the qw associated with
the partial rule. This is an averaged value that provides no information about the accuracy
of this prediction. As also pointed by Wilson (1995), we should favor the use of the partial
rules with a high accuracy in value prediction or, as we say it, rules with a high relevance.
It seems clear that the relevance of a rule (w ) depends on the distribution of values
around qw . Distributions with low dispersion are indicative of coherent value predictions
and, so, of a highly relevant rule. To measure this dispersion we maintain an error estimation
ew on the approximation of qw . Another factor (not used by Wilson, 1995) to be taken into
account in the relevance determination is the confidence on the qw and ew statistics: low
confidence (i.e., insufficiently sampled) measures of qw and ew should reduce the relevance
of the rule. The confidence on the value prediction for a given rule (cw ) is a number in
the interval [0, 1], initialized as 0, and increasing as the partial rule is used (i.e., the rule
is active and its partial command is executed). The confidence would only decrease if the
value model for a given partial rule is consistently wrong.
Using the confidence, we approximate the real error in the value prediction for a partial
rule w as
w = ew cw + e (1  cw ),
where value e is the average error on the value prediction. Observe that the importance of
e is reduced as the confidence increases and, consequently, w converges to ew .
With the above definitions, the relevance of a partial rule can be defined as
w =

1
.
1 + w

Note that the exact formula for the relevance is not that important as far as  w1  w2 
w1  w2 . The above formula provides a value in the range [0, 1] that could be directly
used as a scale factor, if necessary.
The problem is then, how can we derive a single value prediction using the qw statistics
of all the rules in C 0 (a) and its corresponding relevance value, w ? Two possible solutions
come to mind: using a weighted sum of the values predicted by all these partial rules using
the relevance as a weighting factor, or using a competitive approach, in which only the most
relevant partial rule is used to determine the predicted value. The weighted sum assumes
a linear relation between the inputs (the value prediction provided by each individual rule)
and the output (the value prediction for a). This assumption has proved powerful in many
systems but, in general, it is not compatible with the categorizability assumption since,
although each one of the partial rules involved in the sum can be of low order, taking all
of them into account means using a large set of different feature detectors and elementary
actions to predict the effect of a given action. For this reason, our learning system uses a
winner-take-all solution where only the value prediction of the most relevant partial rule is
taken into account to predict the value of an action. So, for each action we determine the
winner rule
w =winner (C 0 , a) =arg

max {w0 },

w0 C 0 (a)

and we use the range of likely value for this rule, Iw = [qw  2w , qw + 2w ], to randomly
determine the value prediction for action a. The probability distribution inside this interval
depends on the distribution we assume for the value.
86

fiReinforcement Learning in Categorizable Environments

The procedure just outlined can be used at each time step to obtain a value prediction
for each action. The action with the maximal value is the one we want the robot to execute
next.
Observe that we obtain a probabilistic value prediction: in the same situation with the
same statistics, we can get different value predictions for the same action. In this way,
the action that obtains the maximal evaluation is not always the one with maximal q w
and, consequently, we favor the exploration of promising actions. This probabilistic action selection provides an exploratory mechanism that uses more information than typical
reinforcement-learning exploration mechanisms (the error and confidence of value predictions is not available in most reinforcement-learning algorithms) and the result is a more
sophisticated exploration schema (see Wilson, 1996, for a survey of different exploration
mechanisms in reinforcement learning).
4.2 Partial Rules Value Adjustment
We adjust the value predictions for all the rules in C 0 (a) where a is the last executed action.
For each rule to be adjusted, we have to update its qw , ew , and cw statistics.
The effect of any action a in accordance with the partial command c attending to a
partial rule w = (v, c) can be defined (using a Bellman-like equation) as

qw
= rw + 

X

p(w, C 0 ) v  (C 0 ),

C 0

where r w is the average reward obtained immediately after executing c when v is observed,
 is the discount factor used to balance the importance of immediate with respect to delayed
reward, v  (C 0 ) represents the goodness (or value) of the situation where rules C 0 are active,
and p(w, C 0 ) is the probability of reaching that situation after the execution of c when v
is observed. The value of a situation is assessed using the best action executable in that
situation

v  (C 0 ) = max
{qw
|w = winner(C 0 , a0 )},
0
a

since this gives us information about how well the robot can perform (at most) from that
situation.
As in many of the existing reinforcement-learning approaches, the values of q w and ew
for the rules to be adjusted are modified using a temporal difference rule so that they
 and the error on this measure. Rules that have a direct relation
progressively approach qw
with the received reward would provide a value prediction (qw ) coherent with the actually
obtained one and, consequently, after the statistics adjustment, their prediction error will
be decreased. Contrariwise, rules not related to the observed reward would predict a value
different from the obtained one and their error statistics will be increased. In this way, if a
rule is really important for the generation of the received reward, its relevance is increased
and if not it is decreased. Rules with low relevance have few chances of being used to drive
the robot and, in extreme cases, they could be removed from the controller.
The confidence cw should also be adjusted. This adjustment depends on how the confidence is measured. If it is only related to the number of samples used in the qw and ew
statistics, then cw should be simply slightly incremented every time the statistics of rule w
87

fiPorta & Celaya

are updated. However, we also decrease the confidence if the value model for a given partial
rule is consistently wrong (i.e., the value observed is systematically out of the interval I w ).
Observe that our learning rule is equivalent to those used in state-based reinforcementlearning methods. For instance, in Q-learning (Watkins & Dayan, 1992), Q  (s, a), with s a
state and a an action, is defined as
X
p(s, a, s0 ) V  (s0 ),
Q (s, a) = r w + 
s0

with p(s, a, s0 ) the probability of a transition from s to s0 when a is executed and
V  (s0 ) = max
{Q (s0 , a0 )}
0
a

In our approach, the set of rules active in a given situation C 0 plays the role of a state
 instead
and, thus, v  (C 0 ) and V  (s0 ) are equivalent. On the other hand, we estimate qw

of Q (s, a), but the rule w includes information about both (partial) state and actions
 and Q (s, a) to play a similar role. The value prediction for a given rule, q ,
making qw
w
corresponds to the average of value predictions for the cells of the Cartesian product of
feature detectors and elementary actions covered by that rule. In the case of complete
rules (i.e., rules involving all the feature detectors and actions for all motors), the sub-area
covered by the rule includes only one cell of the Cartesian product and, therefore, if the
controller only includes complete rules, the just described learning rule is exactly the same
as that used in Q-learning. In this particular case, C 0 (a) is just one rule that, consequently,
is the winner rule. The statistics for this rule are the same (and are updated in the same
way) as those for the Q(s, a) entry of the table used in Q-learning. Thus, our learning rule
is a generalization of the learning rule normally used in reinforcement learning.
4.3 Controller Initialization and Partial Rule Creation/Elimination
Since we assume we are working in a categorizable environment, we can use an incremental
strategy to learn an adequate set of partial rules: we initialize the controller with rules of
the lowest order and we generate new partial rules only when necessary (i.e., for cases not
correctly categorized using the available set of rules). So, the initial controller can contain,
for instance, all the rules of order two that include one feature detector and one elementary
action ((v(fdi ), c(aej )), (v(fdi ), c(aej )) i, j). In any case, it is sensible to include the
empty rule (the rule of order 0, w ) in the initial controller. This rule is always active and it
provides the average value and the average error in the value prediction. Additionally, any
knowledge the user has about the task to be achieved can be easily introduced in the initial
controller in the form of partial rules. If available, an estimation of the value predictions
for the user-defined rules can also be included. If the hand-crafted rules (and their value
predictions) are correct the learning process will be accelerated. If they are not correct, the
learning algorithm would take care of correcting them.
We create a new rule when a large error in the value prediction is detected. The new
rule is defined as a combination of two of the rules in C 0 (a), that are the rules that forecast
the effects of the last executed action, a, in the current situation. When selecting a couple
of rules to be combined, we favor the selection of those with a value prediction close to
88

fiReinforcement Learning in Categorizable Environments

the actually observed one, since they are likely to involve features and elementary actions
(partially) relevant for the value prediction we try to refine.
The problem is that it is not possible to determine a priori whether an incorrectly
predicted value would be correctly predicted after some rule adjustments or if it is really
necessary to create a new partial rule to account for the received reward. So, if we create new
rules when there is a large error in the value prediction, it is possible to create unnecessary
rules. The existence of (almost) redundant rules is not necessarily negative, since they
provide robustness to the controller, the so called degeneracy effect introduced by Edelman
(1989). What must be avoided is to generate the same rule twice, since this is not useful at
all. Two rules can be identical with respect to lexicographic criteria (they contain the same
feature detectors and elementary actions) but also with respect to semantic ones (they
get active in the same situations and propose equivalent actions). If identical rules are
created, then they have to be detected and removed as soon as possible. Preserving only
the rules that proved to be useful avoids the number of rules in the controller growing above
a reasonable limit.
Since we create new rules while there is a significant error in the value prediction,
if necessary, we could end up generating complete rules (provided we do not limit the
number of rules in our controller). In this case, and assuming that the more specific the
rule the more accurate the value prediction, our system would behave as a normal tablebased reinforcement-learning algorithm: Only the most specific rules (i.e., the most relevant
ones) would be used to evaluate actions and, as explained before, the statistics for these
rules would be exactly the same as those in table-based reinforcement-learning algorithms.
Thus, in the limit, our system can deal with the same type of problems as non-generalizing
reinforcement-learning algorithms. However, we regard this limit situation as very improbable and we impose limits to the number of rules in our controllers. Observe that this
asymptotic convergence to a table-based reinforcement learning is only possible because we
use a winner-takes-all strategy in the action evaluation. With a weighted-sum strategy,
the value estimation for the non-complete rules possibly present in the controller would
be added to that of complete rules leading to an action evaluation different from that of
table-based reinforcement-learning algorithms.

5. The Partial Rule Approach in Context
The categorizability assumption is closely related with complexity theory principles such as
the Minimum Description Length (MDL) that has been used by authors such as Schmidhuber (2002) to bias learning algorithms. All these complexity results try to formalize the
well-known Occams Razor principle that enforces choosing the simplest model from a
set of otherwise equivalent models.
Boutilier, Dean, and Hanks (1999) presents a good review on representation methods
to reduce the computational complexity of planning algorithms by exploiting the particular
characteristics of a given environment. The representation based on partial rules can be seen
as another of these representation systems. However, the partial rule is just a representation
formalism that, without the bias introduced by the categorizability assumption, would not
be efficient enough to be applied to realistic applications.
89

fiPorta & Celaya

The partial-rule formalism can be seen as a generalization of that of the XCS classifier
systems described by Wilson (1995). This XCS learning system aims at determining a set
of classifiers (that are combinations of features with an associated action) with their associated value and relevance predictions. The main difference between this approach and ours
is that Wilsons work pursues a generic learner and we bias the learning process using the
categorizability assumption. This allows us to use an incremental rule-generation strategy
that is likely to be more efficient in robotic problems. Additionally, the categorizability assumption also modifies the way in which the value for a given action is evaluated: Wilsons
approach uses a weighted sum of the predictions of the classifier advocating for each action
to determine the expected effect of that action, while, to fulfill with the categorizability assumption (i.e., to minimize the number of feature detectors and elementary actions involved
in a given evaluation), we propose to use a winner-takes-all strategy. This is a critical point
since the winner-takes-all strategy takes full advantage of the categorizability assumption
and because it allows the partial-rule system to asymptotically converge to a table-based
reinforcement-learning system. This is not the case when a weighted sum strategy is used.
Furthermore, in the XCS formalism there is no generalization in the action space and, as
already commented, this is a requirement in robotic-like applications.
In general, reinforcement learning does not pay attention to the necessity of generalizing
in the space of actions, although some exceptions exists. For instance, the work of Maes and
Brooks (1990) includes the possible execution of elementary actions in parallel. However
this system does not include any mechanism detecting interactions between actions and,
thus, the coordination of actions relies on sensory conditions. For instance, this system has
difficulties detecting that the execution of two actions results always (i.e., independently of
the active/inactive feature detectors) in positive/negative reward.
The CASCADE algorithm by Kaelbling (1993) learns each bit of a complex action
separately. This algorithm presents a clear sequential structure where the learning of a
given action bit depends on all the previously learned ones. In our approach there is not
a predefined order in the learning of the outputs and the result is a more flexible learning
schema.
In multiagent learning (Claus & Boutilier, 1998; Sen, 1994; Tan, 1997) the objective
is to learn an optimal behavior for a group of agents trying to cooperatively solve a given
task. Thus, in this field, as in our case, multiple actions issued in parallel have to be considered. However, one of the main issues in multiagent learning, the coordination between
the different learners is irrelevant in our case since we only have one learner.
Finally, the way in which we define complex actions from elementary actions has some
points in common with the works in reinforcement learning where macro-actions are defined
as the learner confronts different tasks (Sutton, Precup, & Singh, 1999; Drummond, 2002).
However, the useful combinations of elementary actions detected by our algorithm are only
guaranteed to be relevant for the task at hand (although they are likely to be also relevant
for related tasks).

6. Experiments
We show the results of applying our learning algorithm to two robotics-like simulated problems: robot landmark-based navigation and legged robot walking. The first problem is
90

fiReinforcement Learning in Categorizable Environments

Flowers

Bushes

Boat

Tree

Lake

Goal

     
     
     
     
A7
A6
     
                                                 
                                             
A5     
                                                
                                                
                                                
                                      
                                      

A4
                                      

			 			 			 			 			 			 			 			 			 			 			
ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 
		 		 		 		 		 		 		 		 		 		 		
			 			 			 			 			 			 			 			 			 			 			
ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff

	 	 	 	 	 	 	 	 	 	 	
ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff

fififififififififififififififififififififififififififififififififififififififififififififi
A1
fifififi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi
A2


 A3
fifififififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
fifififififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi








fifififi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi

    


Rock

Bushes

Start

North

Bush

Figure 1: Landscape for the simple landmark-based navigation task. The landscape is divided in areas (the dashed ovals) where the same subsets of landmarks are visible.

simpler (although it includes more delayed reward) and we use it to clearly describe the
workings of the algorithm. The second problem approaches a realistic robotic application,
our objective in the long term. We use the two examples to compare the performance of
our learning system with that of generalizing and non-generalizing reinforcement-learning
algorithms. The confronted problems are different enough to show the generality of the
proposed learning system.
6.1 Simulated Landmark-Based Navigation
We confront a simple simulated landmark-based navigation task in the forest-like environment shown in Figure 1. The objective for the learner is to go from the start position
(marked with a cross at the bottom of the figure) to the goal position where there is the
food (marked with a cross at the top right corner of the environment). The agent can
neither walk into the lake nor escape from the depicted terrain.
The agent can make use of some binary landmark (i.e., feature) detectors to identify its
position in the environment and to decide which action to execute next. In the example,
the landmark detectors of the agent are:
1. Rock detector: Active when the rock is seen.
2. Boat detector: Active when the boat is seen.
3. Flower detector: Active when the bunch of flowers is seen.
91

fiPorta & Celaya

4. Tree detector: Active when the tree is seen.
5. Bush detector: Active whenever a bush is seen.
6. Water detector: Active when there is water nearby.
7. Bird detector: Active when there is a bird flying over the agent.
8. Cow detector: Active when there is a cow nearby.
9. Sun detector: Active when the sun is shining.
10. Cloud detector: Active when it is cloudy.
Of these detectors, only the first 5 are relevant for the task. The water detector is always
active, and the rest of landmark detectors become active at random. These 10 landmark
detectors can differentiate between 210 = 1024 situations.
We simplify the problem by clustering the possible positions of the learner in the environment in 7 areas (shown in Figure 1): each area includes the positions from which the
same set of relevant landmarks can be seen.
As far as actions is concerned, we use three actions for the West-East movement of the
robot: move to the West (denoted as W ), stay in the same place (), move to the East (E).
The other three indicate movement along the North-South dimension (move to the North
N , stay in the same latitude , move to the South S). These two independent groups of
three actions can be combined giving rise to 9 different actions (move North-West, North,
North-East, etc.). We assume that when the agent executes one of these actions, it does
not stop until the nearest area of the terrain in the direction of the movement is reached.
When the agent tries to move into the lake or out of the terrain, it remains in the same
position it was. Figure 1 shows all the possible transitions between contiguous areas in the
environment.
With the just described landmark detectors and elementary actions the maximum possible order of a given rule is 12, and we can define up to 944784 (310 42 ) syntactically different
partial rules. Only taking into account all the rules with one feature detector and one elementary action (that are the ones initially included in the controller) we have 90 different
partial rules.
The agent only receives reward (with value 100) when it reaches the goal. Consequently,
this is a problem with delayed reward since the agent must transmit the information provided by the reward signal to those actions and situations not directly related with the
observation of reward.
The parameters of the partial-rule learning algorithm we used for this task were  = 0.9,
 = 0.99,  = 5,  = 0.1,  = 5,  = 200 and,  = 0.95 (see Appendix A for a detailed
description of the parameters). Observe that, with a maximum number of partial rules
 = 200 and an initial controller containing 90 rules, little room is left for the generation
of rules with order higher that 2.
The learning is organized in a sequence of trials. Each trial consists in placing the
learner in the starting position and letting it move until the goal is reached, allowing the
execution of at most 150 actions to reach the goal. When performing optimally, only three
actions are required to reach the objective from the starting position.
92

fiReinforcement Learning in Categorizable Environments

180
160

Steps to Goal

140
120
100
80
60
40
20
0

0

50

100

150

200

250

Trial
PR Algorithm

XCS

Figure 2: Performance on the landmark-based navigation task. Results shown are the average over 10 runs.

Figure 2 shows that, after 40 learning trials, the agent approaches the optimal behavior
(represented by the flat dashed line at y = 3).
The dashed line in Figure 2 is the performance of a XCS in this problem. To perform
this test, we used the implementation of Wilsons XCS developed by Butz (1999). To
make XCS work in the same search space as the partial-rule algorithm, we modified the
XCS implementation to be able to deal with non-binary actions. No other modification,
but parameter adjustment, were introduced in the original code. The results presented
here corresponds to the average of 10 runs using the set of parameters that gave a better
result. Nominally, these parameters were: learning rate  = 0.1, decay rate  = 0.9,
maximum number of classifiers  = 200 (however, the initial set is empty), the genetic
algorithm is applied in average every 5 time steps, the deletion experience is 5, the subsume
experience is 15, the fall off rate is 0.1, the minimum error 0.01, a prediction threshold
of 0.5, the crossover probability is 0.8, the mutation probability 0.04 and the initial dont
care probability 1/3. The prediction and the fitness of new classifiers are initialized to 10
and the error to 0. A detailed explanation of the meaning of those parameters is provided
by Wilson (1995) and also by the comments in the code of Butz (1999).
We can see that the XCS reaches the same performance of the partial-rule approach, but
using about four times more trials. This difference in performance is partially explained by
XCSs lack of generalization in the action space. However this factor is not that relevant in
this case since the action space has only two dimensions. The main factor that explains the
better performance of the partial-rule approach is the bias introduced by the categorizability
93

fiPorta & Celaya

t
1

Position
A2

V
81

2

A4

90

3

A6

100

Action
(W, N )
(W, )
(, N )
(E, N )
(E, )
(E, )

Winner Rule
w1 = (v(Rock, Boat), c(W, N ))
w2 = (v(Rock, W ater), c(W ))
w3 = (v(Boat, T ree), c(N ))
w4 = (v(T ree), c(E, N ))
w5 = (v(Rock, Boat), c(E))
w6 = (v(Bush), c(E, ))

qw
80.63
79.73
89.61
90.0
86.71
100.0

ew
1.16
2.19
2.04
0.0
4.58
0.0

Guess
79.87
77.65
88.88
89.86
79.56
99.87

Table 1: Partial execution trace for the landmark-based navigation task. Elementary action  means no movement along the corresponding dimension. At each time step
t, the action with the highest guess is executed. After time step 3, the goal is
reached.

assumption that is not present in the XCS system and that, in this case, allows a more
efficient learning process. XCS in more powerful than our partial-rule approach in the sense
that XCS makes no assumption about the categorizability of the environment, while we
assume it as high. The result of the XCS learning process includes the identification of
the degree of categorizability of the environment and in our case this is, in some sense,
pre-defined. The generality of the XCS, however, produces a slower learning process.
If we initialize the classifiers in a XCS with a high dont care probability and we initialize
the rules in the partial-rule algorithm so that no generalization is used in the action space
(i.e., if all rules include a command for each motor), then the two systems become closer.
In this case, the main (but not the only) difference between the two approaches is the
assumption on the relation between the inputs and the value: While XCS assumes a linear
relation, we assume the environment to be categorizable, or, what is the same, we assume
the value to depend on only few of the inputs. Due to this difference, when confronted to
the same problem, the two systems would learn the same policy and the same values for
each action, but the values would be computed using different rules with different associated
values, and this is so independently of the parameter/rule initialization used in each case.
The system with a smaller learning time would be that with an assumption closer to the
reality. The results obtained in the particular example presented above show that the
categorizability assumption is more valid and our hypothesis is that this would be the case
in most robotics-like applications.
Table 1 shows the evaluation of some actions in the different situations the agent encounters on its path from the start to the goal after 50 learning trials. Analyzing this trace,
we can extract some insight about how the partial-rule learning algorithm works.
For instance, at time step 1, we see that rule w2 = (v(Rock, W ater), c(W )) is used to
determine the value of action (W, ). Since the landmark detector Water is always active,
this rule is equivalent to w = (v(Rock), c(W )), which is one of the rules used to generate w 2 .
If we examine the statistics for w we find that qw = 74.70 and ew = 15.02. Obviously, the
value distributions of qw and qw2 look different (74.70 vs. 79.73 and 15.02 vs. 2.19). This
is because w2 has been generated on later stages of the learning and, thus, its statistics
have been updated using a subsample of the values used to adjusts the statistics of w. In
94

fiReinforcement Learning in Categorizable Environments

this particular case, qw has been updated 250 times while qw2 has been updated only 27
times. As learning continues, both distributions will become more similar and rule w 2 will
be eventually eliminated.
In Table 1, we can see that sometimes there are some non-optimal actions that get
an evaluation close to the optimal ones. For this reason, the agent executes, some times,
non-optimal actions and this increases the number of steps necessary to reach the goal.
In general, the adjustment of the statistics of the rules can solve this problem but, in this
particular case, we need to create new rules to fix the situation. For instance, at time step 2,
when the value for rule w4 is increased towards 90, the value of rules active at this time step
and proposing actions in accordance with the action of rule w4 also converge toward 90.
So, in the long term, a rule proposing just action (N ) can get a value close to 90. In the
absence of more specific rules, this rule can be used to estimate the value of an action
such as (, N ) and, due to the probabilistic nature of the action selection procedure, this
action can, eventually, be executed delaying the agent from reaching the goal by 1 time
step. However, the execution of (, N ) results in an error in the value prediction and, thus,
in the creation of new rules to better characterize the situation. As soon as a specific rule
for action (, N ) is generated, the error is no longer repeated.
At time step 3, we see that rule w6 = (v(Bush), c(E, )) has a value of 100 with error
0 but the guess for this rule is 99.87. This is because the maximum confidence () is lower
than 1.0 (0.99 in this case) and this makes the agent to keep always a certain degree of
exploration.
If the agent only receives reward when the task is totally achieved, the function value
for each situation can be computed as V (s) =  n1 r with n the distance (in actions) from
situation s to the target one and r the reward finally obtained. In Table 1, we can see that
situations get the correct evaluation: 80.63( 81 = 100  0.9 2 ) for A2, 90(= 100  0.9) for
A4, and 100 for A6.
Observe that this problem can be solved using only 200 partial rules out of the 9216
possible situation-action combinations in this domain. So, we can say that the problem
is certainly categorizable. The main conclusion we can extract from this toy example is
that, in a particular case in which the confronted problem was categorizable, the presented
algorithm has been able to determine the relevant rules and to adjust their values (including
the effect of the delayed reward) so that the optimal action can be determined for each
situation.
6.2 Gait Generation for a Six-Legged Robot
We also applied our algorithm to the task of learning to generate an appropriate gait (i.e., the
sequence of steps) for a six-legged robot (Figure 3). To apply the learning algorithm to the
real robot would be possible, but dangerous: in the initial phases of the learning the robot
would fall down many times damaging the motors. For this reason we used a simulator
during the learning and, afterward, we applied the learned policy to the real robot.
The problem of learning to walk with a six legged robot has been chosen by many authors
before as a paradigmatic robotic-learning problem. For instance, Maes and Brooks (1990)
implemented a specific method based on immediate reward to derive the preconditions for
each leg to perform the step. Pendrith and Ryan (1996) used a simplified version of the
95

fiPorta & Celaya

six-legged walking problem to test an algorithm able to deal with Non-Markovian spaces
of states and Kirchner (1998) presented a hierarchical version of Q-learning to learn the
low-level movements of each leg, as well as a coordination scheme between the low-level
learned behaviors. Ilg, Muhlfriedel, and Berns (1997) introduced a learning architecture
based on self-organizing neural networks, and Kodjabachia and Meyer (1998) proposed an
evolutionary strategy to develop a neural network to control the gait of the robot. Vallejo
and Ramos (2000) used a parallel genetic algorithm architecture and Parker (2000) described
an evolutionary computation where the robot executes the best controller found up to a
given moment while a new optimal controller is computed in an off-line simulation. All these
algorithms are usually tested on flat terrain with the aim of generating periodic gaits (i.e.,
gaits where the sequence of steps is repeated cyclically). However, for general locomotion
(turns, irregular terrain, etc) the problem of free gait generation needs to be considered.

Figure 3: The Genghis II walking robot and its 2D simulation environment.
Our simulator (see Figure 3) allows the controller to command each leg of the robot in
two independent degrees of freedom (horizontal and vertical) and it is able to detect when
the robot is in an unstable position (in our robot this happens when two neighboring legs are
in the air simultaneously). Using this simulator, we implemented the behaviors described
by Celaya and Porta (1996) except those in charge of the gait generation. Therefore, the
task to be learned consists in deciding at every moment which legs must step (that is, leave
the ground and move to an advanced position), and which must descend or stay on the
ground to support and propel the body.
We defined a set of 12 feature detectors that, due to our experience on legged robots,
we knew could be useful in different situations for the gait-generation task:
 In the air(x): Active if the leg x is in the air.
 Advanced(x): Active if leg x is more advanced than its neighboring leg in a clockwise
circuit around the robot.
Attending to the activation and non-activation of these 12 feature detectors, we can
differentiate between 4096 different situations.
On the action side, we work with two different elementary actions per leg: one that
issues the step of the leg and another that descends the leg until it touches the ground.
96

fiReinforcement Learning in Categorizable Environments

Thus, the cardinality of the set of elementary actions is 12 and, at each time step, the robot
issues an action containing 6 elementary elements (one per leg). Thus, we can think of each
leg as a virtual motor that accepts two possible values, 0 to remain in contact with the
ground and 1 to perform the step.
The reward signal includes two aspects:
 Stability: If an action causes the robot to fall down, a reward of 50 is given.
 Efficiency: When the robot does not fall down, a reward equal to the distance
advanced by the robot is given. Observe that when legs descend to recover contact
with the ground no advance of the robot is obtained but this movement is necessary
to be able to get reward in next time steps. So, we have again a problem with delayed
reward.
The most efficient stable gait is the tripod gait in which two sets of three non-adjacent
legs step alternately. Using this gait, the robot would obtain a reward of 0 (when one group
of three legs are lifted and advanced) followed by a reward of 50 (when the legs in contact
with the ground move backward as a reaction to the advance of legs moved in the previous
time step). Thus, the optimal average reward is 25.
In the experiments, the robot is set in an initial posture with all the legs in contact with
the ground but in a random advance position.
Figure 4 shows results of applying the partial-rule algorithm compared with those obtained using standard Q-learning with 4096 distinct states and 64 different actions.
For the partial-rule algorithm, we used the following set of parameters:  = 0.2,  =
0.99,  = 22,  = 0.1,  = 150,  = 10000 and,  = 0.95 (see Appendix A for a description
of these parameters). For Q-learning, the learning rate is set to  = 0.5 and we use an
action selection rule that performs exploratory actions with probability 0.1.
In Figure 4, we can see that the stability subproblem (i.e., not falling down, which
corresponds to getting a reward greater than zero) is learned very quickly. This is because,
in the stability subproblem, we can take advantage of the generalization provided by using
separate elementary actions and, with a single rule, we can avoid executing several dangerous
actions. However, the advance subproblem (i.e., getting a reward close to 25) is learned
slowly. This is because little generalization is possible and the learning system must generate
very specific rules. In other words, this sub-problem is less categorizable than the stability
one.
As in the landmark-based navigation example discussed in the previous section, we
observe that the controller contains some (slightly) overly general rules that are responsible
for the non optimal performance of the robot. However, we dont regard this as a problem
since we are more interested in efficiently learning a correct enough policy for the most
frequent situations than in finding optimal behaviors for all particular cases.
Figure 5 shows the performance of Q-learning over a longer run using different exploration rates. This shows that Q-learning can eventually converge to an optimal policy but
with many more iterations than our approach (about a factor of 10). Observe that a lower
exploration rate allows the algorithm to achieve higher performance (around 19 with a
learning rate of 0.1 and around 24 with learning rate 0.01) but using a longer period. With
a careful adjustment of the exploration rate we can combine an initial faster learning with
97

fiPorta & Celaya

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

QLearning

Figure 4: Performance of the partial-rule approach compared with standard Q-learning.
Results are the smoothed average of 10 experiments.

a better convergence in the long term. Experiments with Q-learning using learning rates
other than 0.5 showed insignificant differences compared to the results shown here.
The advantage of our algorithm over non-generalizing ones is increased in problems in
which some of the sensors provide information not related to the task. To test this point,
we set up an experiment in which 6 feature detectors that become active randomly were
added to the 12 initial ones. With these new features, the number of possible combinations
of feature activations increases, and so does the number of states considered by Q-learning.
Figure 6 shows the comparison between our algorithm and Q-learning for this problem.
Q-learning is not able to learn a reasonable gait strategy in the 5000 time steps shown
in the figure, while the performance of the partial-rule algorithm is almost the same as
before. This means that the partial-rule algorithm is able to detect which sets of features
are relevant and use them effectively to determine the robots behavior. It is remarkable
that, in this case, the ratio of memory used by our algorithm with respect to that used by
non-generalizing algorithms is below 0.2%. This exemplifies how the performance of the
non-generalizing algorithms degrades as the number of features increases, while this is not
necessarily the case using the partial-rule approach.
The importance of the generation of partial rules in the improvement of the categorization can be seen comparing the results obtained for the same problem with and without
this mechanism (Figure 7). The results show that the task cannot be learned using only
partial rules of order 2. The only aspect of the gait-generation problem that can be learned
with rules of order 2 is to avoid lifting a leg if one of its neighboring legs is already in the
98

fiReinforcement Learning in Categorizable Environments

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

50000

Exploration 0.1

100000
150000
Time Slice

200000

Exploration 0.01

250000

References

Figure 5: Performance of the Q-learning algorithm with different exploration rates. The
reference values 19 and 24 are the upper bound of the performance attainable
when using exploration rate 0.1 and 0.01.

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

QLearning

Figure 6: Performance of our algorithm compared with Q-learning when there are irrelevant
features.

99

fiPorta & Celaya

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

Without Generation

4000

5000

With Generation

Figure 7: Performance with and without the partial-rule generation procedure.
air. For instance, a rule such as
v(In the air(1))  c(Step(2)),
forecasts a highly relevant negative reward and this prevents leg 2 from being raised when
leg 1 is in the air.
Rules with order higher than 2 (i.e., not provided to the robot in the initial controller)
are necessary, for instance, to avoid raising two neighboring legs simultaneously. A rule like
v(In the air(1))  c(Step(1), Step(2))
becomes active when the robot evaluates any action that implies raising leg 1 and leg 2
at the same time. Since the value prediction of this rule is very negative and its relevance
is high, the action under evaluation would be discarded, preventing the robot from falling
down. Similar rules have to be generated for each pair of neighboring legs. To make the
robot advance, we need to generate rules with even higher order.
In Figure 8, we can see the performance of the algorithm when we start the learning
process from a correct rule set (i.e., a rule set learned in a previous experiment), but with
all the statistics initialized to 0. In this experiment, we can compare the complexity of
learning only the values for the rules compared with the complexity of learning the rules
and their value at the same time. We can see that when only the values for the rules need
to be learned the learning process is about two times faster than in the normal application
of the algorithm.
In a final experiment, we issue frequent changes in the heading direction of the robot
(generated randomly every 10 time steps). In this way, periodic gaits become suboptimal
100

fiReinforcement Learning in Categorizable Environments

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

500

1000

1500

2000
2500
Time Slice

PR from a correct rule set

3000

3500

4000

PR Algorithm

Figure 8: Performance of the partial-rule approach when learning is started from a correct
rule set compared with the standard approach where rules are also learned.

and the controller should produce a free gait, i.e., a gait that includes a sequence of steps
without any periodic repetition.
In this case, we focus on the advance subproblem and, thus, we introduced some handcrafted rules to the initial controller to prevent the robot from falling down. These rules
are of the form:
if leg i is lifted then execution of action a results in value 50 with confidence 1,
where a is any of the actions that lift one of the two legs that are contiguous to i.
The set of parameters we used in this case was:  = 0.2,  = 0.99,  = 5,  = 0.1,
 = 150,  = 10000 and,  = 0.95.
Figure 9 shows the average results obtained using the partial-rule learning algorithm
compared with those obtained with our best hand-coded gait-generation strategy. In the figure, the horizontal dashed line shows the average performance using the best gait-generation
strategy we have implemented (Celaya & Porta, 1998). It can be seen that the learned gaitgeneration strategy (the increasing continuous line) produces a performance similar to that
of our best hand-coded strategy and that, in some cases, it even outperforms it. Figure 10
shows a situation where a learned controller produces a better behavior than our hand
coded one. Using the hand-coded strategy, the robot starts to walk raising two legs (3 and
6) and, in few time steps it reaches a state from which the tripod gait is generated. Initially,
leg 2 is more advanced than legs 1 and 4 and, in general, it is suboptimal to execute a step
with a leg when its neighboring legs are less advances that itself. In this particular case
however, this general rule does not hold. The learned strategy detects this exception and
101

fiPorta & Celaya

25

Average Reward

20
15
10
5
0

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

Hand Coded

Figure 9: Performance of the partial-rule approach when learning a free gait.
generates the tripod gait from the very beginning resulting in a larger advance of the robot
in the initial stages of the movement.

7. Conclusions
In this paper, we have introduced the categorizability assumption that states that a robot
can be driven to achieve a given task using only simple rules: i.e., rules including a reduced
set of feature detectors and elementary actions. This assumption is supported by our
experience within the behavior-based approach where controllers are formed by sets of rules
with relatively simple conditions and actions. We have shown that a learning algorithm
based on the categorizability assumption allows a large speed up in the learning process in
many realistic robotic applications with respect to existing algorithms.
To exploit the categorizability assumption both in the observations and action spaces,
we have introduced a new representation formalism based on the concept of partial rules
and not on the concepts of independent states and independent actions that are the kernel
of many existing reinforcement-learning approaches.
The introduction of the partial-rule concept provides a large flexibility on how problems
are formalized. With the same structure and algorithms, we can confront problems with
generalization in the perception side (usually considered in reinforcement learning), in the
action side (usually not considered), or in both of them.
When no generalization is possible at all via partial rules, we have to use complete rules:
rules involving all the available inputs and outputs. In this case, the partial-rule approach is
equivalent to the non-generalizing reinforcement learning. The algorithm we have presented
102

fiReinforcement Learning in Categorizable Environments

Leg Numbering
1
2

0

0

Step 3,6

Step 2,3,6

25

32

3

4

5

6

Step 1,4,5

Step 1,4,5

59

82

Step 2,3,6

Step 2,3,6

109

132

Figure 10: The hand-programmed gait strategy (top sequence) vs. a learned one (bottom
sequence). The advance position of the robot at each snapshot is indicated below
each picture.

can, if necessary, generate complete rules and, consequently, it can, in principle, solve any
problem that can be solved using a traditional reinforcement-learning algorithm. However,
we take the categorizability assumption as valid and so, the generation of complete rules
is an extreme case that is only likely to occur in a very limit situation. Therefore, in our
approach, we forego generality in order to increase the efficiently of the learning process in
the class of problems we want to address.
Another advantage of the partial-rule framework is that it allows the easy and robust
introduction of initial knowledge in the learning process in the form of rules that are easily understood by the programmer. This is in contrast with usual reinforcement-learning
algorithms where the introduction of initial knowledge is, in general, rather difficult.
In the partial-rule approach, there is a subtle change of emphasis as to the main goal
of learning: While in most work in reinforcement learning the emphasis is on learning the
value of each action in each state, our main purpose is to learn the relevance of (subsets of)
elementary actions and feature detectors. If the relevant subsets of elementary actions and
feature detectors are identified, the learning becomes straightforward.
103

fiPorta & Celaya

The main limitation of our work is that it is not possible to know a priori (except for trivial cases) whether or not an environment is categorizable by a given robot. Non-generalizing
reinforcement learning implicitly assumes that the environment is non-categorizable and
that, consequently, all the possible combination of features and actions have to be taken
into account separately. Our approach assumes just the opposite: that the environment is
categorizable and, so, only reduced combinations of features and actions need to be taken
into account. The drawback of using the non-generalizing approach is that robotic tasks
become intractable because of the curse of dimensionality. With generalization techniques
this problem can be partially alleviated, but not enough in general. In our approach we take
a more radical approach in order to be much less affected by the curse of dimensionality: we
introduce a strong bias in the learning process to drastically limit the use of combinations
of features and actions.
We have tested the partial-rule learning algorithm in many robotic-inspired problems
and two of them have been discussed in this paper (landmark based-navigation and sixlegged robot gait generation) and the categorizability assumption proved to be valid in all
cases we tested. The algorithm out-performs generalizing and non-generalizing reinforcementlearning algorithms in both memory requirements and convergence time. Additionally, we
have shown that our approach scales well when the number of inputs increases, while the
performance of existing algorithms is largely degraded. This is a very important result that
lets us think that it could be possible to use our approach to control more complex robots,
while the use of existing approaches has to be discarded.
From the work presented in this paper, we can extract two main proposals. First,
to apply reinforcement learning to agents with many sensors and actuators, we should
concentrate our efforts in determining the relevance of inputs and outputs and, second,
to achieve efficient learning in complex environments it could be necessary to introduce
additional assumptions into the reinforcement-learning algorithms, even at the risk of losing
generality.

Acknowledgments
The authors would like to express their gratitude to the anonymous reviewers of the paper.
Their contributions toward improving the quality of this paper are relevant enough to be
considered, in some sense, as a co-authors of the paper. The shortcomings still in the paper
can only be attributed to the nominal authors.
The second author has been partially supported by the Spanish Ministerio de Ciencia y Tecnologa and FEDER funds, under the project DPI2003-05193-C02-01 of the Plan
Nacional de I+D+I.

104

fiReinforcement Learning in Categorizable Environments

Appendix A: The Partial-Rule Learning Algorithm
In this appendix, we describe in detail the approach described in the main body of the
paper.

Partial Rule Learning Algorithm
(Initialize)
F D  Set of features detectors
EA  Set of elementary actions
C  {w }  {(v(fd), c(ea)), (v(fd), c(ea))|fd  F D, ea  EA}
For each w in C
qw  0
ew  0
iw  0
endfor
e0
Do for each episode
C 0  {w  C|w is active}
Repeat (for each step in the episode):
(Action Selection)
Action Evaluation
(Computes guess(a0 ) a0 )
0
a  arg max
{guess(a )}
0
a

Execute a
(System Update)
ra  Reward generated by a
0
Cant
 C0
0
C  {w  C|w is active}
Statistics Update
Partial-Rule Management
until terminal situation
enddo

Figure 11: The partial-rule learning algorithm. Text inside parentheses are comments. The
Action Evaluation, Statistics Update, and Partial-Rule Management procedures
are described next.

The partial-rule learning algorithm (whose top level form is shown in Figure 11) stores
the following information for each partial rule
 the value (i.e., the discounted cumulative reward) estimation qw ,
 the error estimation ew , and
 the confidence index iw .
105

fiPorta & Celaya

1.0
0.9





0.8
0.7

cw

0.6
0.5
0.4
0.3
0.2
0.1
1

2

3

4

5

iw

6

7



8

9

10

Figure 12: Confidence function with =7 and =0.8.
To estimate the confidence on qw and ew we use a confidence index iw that, roughly
speaking, keeps track of the number of times the partial rule is used. The confidence is
derived from iw using a confidence function in the following way:
cw =confidence function(iw ),
where the confidence function is a non-decreasing function in the range [0, ].  should be
less than 1 since, in this way, the system always keeps a certain degree of exploration and,
consequently, it is able to adapt to changes in the environment. Different confidence schemes
can be implemented by changing the confidence function. In our implementation, we use
a sigmoid-like function (see Figure 12) that increases slowly for low values of i w reducing
the confidence provided by the first obtained rewards. In this way we avoid a premature
increase of the confidence (and, thus, a decrease in the error and in the exploration) for
insufficiently-sampled rules. A parameter () determines the point at which this function
reaches the top value .
Additionally, the confidence index is used to define the learning rate (i.e., the weight
of new observed rewards in the statistics update). For this purpose we implement a MAM
function (Venturini, 1994) for each rule:
mw = max{, 1/(iw + 1)}.
Using a MAM-based updating rule, we have that, the lower the confidence, the higher
the effect of the last observed rewards on the statistics, and the faster the adaptation of the
statistics. This adaptive learning rate strategy is related to those presented by Sutton (1991)
and by Kaelbling (1993), and contrasts with traditional reinforcement-learning algorithms
where a constant learning rate is used.
After the initialization phase, the algorithm enters in a continuous loop for each task
episode consisting in estimating the possible effects of all actions, executing the most promis106

fiReinforcement Learning in Categorizable Environments

Action Evaluation
For each action a0
w  winner(C 0 , a0 )
guess(a0 )  qw + 2 random(w , w )
endfor

Figure 13: Action Evaluation procedure.
ing one, and updating the system so that its performance improves in the future. The system
update includes the statistics update and the partial-rule management.
Action Evaluation
The simplest procedure to get the estimated value for actions is a brute-force approach
consisting of the independent evaluation of each one of them. In simple cases, this approach
would be enough but, when the number of valid combinations of elementary actions (i.e.,
of actions) is large, the separate evaluation of each action would take long time, increasing
the time of each robot decision and decreasing the reactivity of the control. To avoid this,
Appendix B presents a more efficient procedure to get the value of any action.
Figure 13 summarizes the action-evaluation procedure using partial rules. The value for
each action is guessed using the most relevant rule for this action (i.e., the winner rule).
This winner rule is computed as
winner (C 0 , a) =arg

max {w },

wC 0 (a)

where w is the relevance of rule w
w =

1
.
1 + w

The value estimation using the winner rule is selected at random (uniformly) from the
interval
Iw = [qw  2w , qw + 2w ],
with
w = ew cw + e (1  cw ).
Here, e is the average error on the value prediction (i.e., the value error prediction of the
empty rule, w ).
Statistics Update
In the statistics-update procedure (Figure 14), qw and ew are adjusted for all rules that
were active in the previous time step and proposed a partial command in accordance with
a (the last executed action).
107

fiPorta & Celaya

Statistics Update
if terminal situation then
v0
else
v  max
{qw |w = winner(C 0 , a0 )}
0
a

endif
q  ra +  v
0
For each w = (v, c) in Cant
if c is in accordance with a then
if q  Iw then
iw  iw + 1
else
iw  min(  1, iw  1)
endif
qw  qw (1  mw ) + q mw
ew  ew (1  mw ) + |qw  q| mw
endif
endfor
e  ew

Figure 14: Statistics update procedure.

Both qw and ew are updated using a learning rate (mw ) computed using the MAM
function, which initially is 1, and consequently, the initial values of qw and ew have no
influence on the future values of these variables. These initial values become relevant when
using a constant learning rate, as many existing reinforcement-learning algorithms do.
If the observed effects of the last executed action agree with the current estimated
interval for the value (Iw ), then the confidence index is increased by one unit. Otherwise,
the confidence index is decreased allowing a faster adaptation of the statistics to the last
obtained, surprising values of reward.
Partial-Rule Management
This procedure (Figure 15) includes the generation of new partial rules and the removal of
previously generated ones that proved to be useless.
In our implementation, we apply a heuristic that produces the generation of new partial
rules when the value prediction error exceeds e. In this way, we concentrate our efforts to
improve the categorization on those situations with larger errors in the value prediction.
Every time a wrong prediction is made, at most  new partial rules are generated by
0 (a). Recall that this set includes the
combination of pairs of rules included in the set Cant
rules active in the previous time step and in accordance with the executed action a. Thus,
these are the rules related with the situation-action whose value prediction we need to
improve.
108

fiReinforcement Learning in Categorizable Environments

The combination of two partial rules w1  w2 consists of a new partial rule with a partial
view that includes all the features included in the partial views of either w1 or w2 and with a
partial command that includes all the elementary actions of the partial commands of either
w1 or w2 . In other words, the feature set of w1  w2 is the union of the feature sets in w1
and in w2 and the elementary actions in w1  w2 are the union of those in w1 and those in
0 (a), they have been simultaneously active
w2 . Note that, since both w1 and w2 are in Cant
and they are in accordance with the same action and, thus, they can not be incompatible
(i.e., they can not include inconsistent features or elementary actions).
In the partial-rule creation, we bias our system to favor the combination of those rules
(wi ) whose value prediction (qwi ) is closer to the observed one (q). Finally, the generation
of rules lexicographically equivalent to already existing ones is not allowed.
According to the categorizability assumption, only low-order partial rules are required
to achieve the task at hand. For this reason, to improve efficiency, we limit the number of
partial rules to a maximum of . However, our partial-rule generation procedure is always
generating new rules (concentrating on those situations with larger error). Therefore, when
we need to create new rules and there is no room for them, we must eliminate the less useful
partial rules.
A partial rule can be removed if its value prediction is too similar to some other rule in
the same situations.
The similarity between two rules can be measured using the normalized degree of intersection between their value distributions and the number of times both rules are used
simultaneously:
similarity(w, w 0 ) =

U (w  w0 )
kIw  Iw0 k
,
max{kIw k, kIw0 k} min{U (w), U (w 0 )}

where U (w) indicates the number of times rule w is actually used.
The similarity assessment for any pair of partial rules in the controller is too expensive
and, in general, determining the similarity of each rule with respect to those from which
it was generated (that are the rules we tried to refine when the new rule was created) is
sufficient. Thus, based on the above similarity measure, we define the redundancy of a
partial rule w = (w1  w2 ) as:
redundancy(w) = max{similarity(w, w1 ), similarity(w, w2 )}.
Observe that with w = (w1  w2 ), we have that w  w1 = w and U (w)  U (w1 ).
Therefore
U (w  w1 )
U (w)
U (w)
=
=
= 1.
min{U (w), U (w1 )}
min{U (w), U (w1 )}
U (w)
The same reasoning can be done with w2 and, consequently,
redundancy(w) = max{

kIw  Iw2 k
kIw  Iw1 k
,
}.
max{kIw k, kIw1 k} max{kIw k, kIw2 k}

When we need to create new rules but the maximum number of rules () has been
reached, the partial rules with a redundancy above a given threshold () are eliminated.
Since the redundancy of a partial rule can only be estimated after observing it a number of
109

fiPorta & Celaya

Partial Rule Management
0
w  winner(Cant
, a)
if |qw  q| > e
(If it is time to create new rules)
(Partial Rule Elimination)
(Test if there is no room for new rules)
if kCk >    then
(Rule elimination based on redundancy)
C  C  {w  C | redundancy(w) > }
(Rule elimination based on creation error)
if kCk >    then (If there is still no room)
SC  The  partial rules from C with:
- Lowest creation error(w), and
- creation error(w) < |qw  q|
C  C  SC
endif
endif
(Partial Rule Generation)
t0
while kCk <  and t < 
(Create a new rule w 0 )
0
(a)
Select two different rules w1 , w2 from Cant
preferring those that minimize
|qwi  q| cwi + e (1  cwi )
w0 = (w1  w2 )
creation error(w 0 )  |qw  q|
(Insert the new rule in the controller)
C  C  {w 0 }
tt+1
endwhile
endif

Figure 15: Partial Rule Management procedure. The value of q is calculated in the Statistics
Update procedure and a is the last executed action.

times, the redundancy of the partial rules with low confidence indexes is set to 0, so that
they are not immediately removed after creation.
Observe that, to compute the redundancy of a rule w, we use the partial rules from
which w was derived. For this reason, a rule w 0 cannot be removed from a controller C if
there exists any rule w  C such that w = w 0  w00 . Additionally, in this way we eliminate
first the useless rules with higher order.

110

fiReinforcement Learning in Categorizable Environments

Appendix B: Efficient Action Evaluation
In non-generalizing reinforcement learning the cost of executing a single learning step can be
neglected. However, algorithms with generalization in the spaces of sensors and/or actuators
are not so simple and the execution time of each iteration can be increased substantially.
In an extreme case, this increase can limit the reactivity of the learner and this is very
dangerous when working with an autonomous robot.
The most expensive procedure of our algorithm is that of computing the value of all
actions (i.e., all valid combinations of elementary actions). The cost of this procedure is
especially critical since it is used twice in each step: once to get the guess of each action
(in the Action Evaluation procedure detailed in Figure 13) and again to get the goodness
of the new achieved situation after the action execution (when computing the v value in
the Statistics Update procedure detailed in Figure 14). A trivial re-order of the algorithm
can avoid the double use of this expensive procedure at each learning step: we can select
the action to be executed next at the same time that we evaluate the goodness of the new
achieved situation. The drawback of this re-order is that the action is selected without
taking into account the information provided by the last reward value (the goodness of the
situation is assessed before the value adjustment). However, this is not a problem in tasks
that require many learning steps.
Even if we use the action-evaluation procedure only once per learning step, we have
to optimize it as much as possible since the brute-force approach described before, which
evaluates each action sequentially, is only feasible for simple problems.
The action-evaluation method presented next is based on the observation that many
of the actions would have the same value since the highest relevant partial rule at a given
moment would provide the value to all actions that are in accordance with the partial
command of the rule. The separate computation of the value of two actions that would end
up evaluated using the same rule is a waste of time. This can be avoided by performing the
action evaluation attending to the set of active rules in the first place and not to the set of
possible actions, as the brute-force approach does.
Figure 16 shows a general form of the algorithm we propose. In this algorithm, partial
rules are considered one at a time, ordered from the most relevant rule to the least relevant
one. The partial command of the rule under consideration (cow ) is used to process all the
actions that are in accordance with that partial command. This already processed sub-set
of actions need not to be considered any more in the action-evaluation procedure. While
the rules are processed, we update the current situation assessment (v) and the action to be
executed next (a) attending, respectively, to the value prediction (qw ) and the guess (gw )
of the rules.
Observe that partial rules can be maintained sorted by relevance by the statistics update
procedure, since it is in this procedure where rule relevance is modified. When the relevance
of a rule is changed, its position in the list can be also modified accordingly. In this way
we do not have to re-sort the list of rules every time we want to apply the procedure just
described.
When elementary actions are of the form (m  k) with m a motor and k a value in
the range of possible values for that motor, the above algorithm can be implemented in an
especially efficient way since there is no need to explicitly compute the set of actions A.
111

fiPorta & Celaya

Action Evaluation
(Initialization)
L  List of active rules sorted by relevance.
EA  Set of elementary actions
A  Set of combinations of EA
v  
(Situation assessment)
a
(Optimal action)
g  
(Optimal action value prediction)
(Process)
w first element(L)
do
cow  partial command of w
gw  qw + 2 random(w , w )
Aw  {a  A|cow in accordance with a}
if qw > v then
v  qw
endif
if gw > g then
g  gw
a  cow
endif
A  A  Aw
w next element(L)
until A 6= 

Figure 16: General form of the proposed situation-assessment and action-selection procedure.

In this case (see Figure 17 and 18), we construct a decision tree using motors as decision
attributes and that groups in the same leaf all those actions evaluated by the same partial
rule (all actions removed from the set A in each iteration of the algorithm in Figure 16).
Each internal node of the tree classifies the action according to one of the motor commands included in the action. These internal nodes store the following information:
 Partial command: A partial command that is in accordance with all the action classified under the node. This partial command can be constructed by collecting all the
motors whose values are fixed in the nodes from the root of the tree to the node under
consideration.
 Motor: The motor used in this node to classify actions. When a node is open (i.e.,
we have still not decided to which motor to attend) the motor value is set to a .
A node can be closed by deciding which motor to pay attention to (and adding the
corresponding subtrees) or by converting the node into a leaf.
112

fiReinforcement Learning in Categorizable Environments

Action Evaluation
(Initialization)
L  List of active rules sorted by relevance.
v  
a
g  
tree  new node(c )
open  1
closed  0
(Process)
w first element(L)
do
gw  qw + 2 random(w , w )
Include Rule(tree, w, gw )
w next element(L)
until closed = open

Figure 17: Top level algorithm of the efficient action evaluation algorithm. At the end of the
algorithm, v is the goodness of the current situation to be used in the Statistics
Update algorithm (see Figure 14), a is the action to be executed next and guess
its expected value. The Include Rule procedure is detailed in next figure.

 Subtrees: This is a list of the subtrees that start in that node. Each subtree has an
associated value that corresponds to one of the possible actions executable by the motor of the node. All the actions included in a given subtree have an elementary action
such as (m  k) where m is the motor of the node and k is the value corresponding
to this subtree.
The leaves of the tree have information about the value of the actions classified in that
leaf. This information is represented with the following set of attributes for each leaf:
 Value: The expected value for all the actions classified in this leaf. The maximum of
this value for all leaves is used to assess the goodness, v, of a new achieved situation.
 Guess: The value altered with noise for exploratory reasons. The leaf with a maximal
guess is the set of actions from where to select the action to be executed next.
 Relevance: The relevance of the value predictions (of both the value and the guess).
 Partial command: A partial command that is in accordance with all the actions
classified in that leaf. As in the case of internal nodes, this partial command can be
constructed by collecting all the motors whose values are fixed from the root of the
tree to the leaf under consideration.
113

fiPorta & Celaya

Include rule(n, w, gw )
if not(is leaf(n)) then
cow  command(w)
con  command(n)
if motor(n) 6= then
(Closed Node: Search for compatible sub-nodes)
if ea  cow with motor(ea) = motor(n) then
Include Rule(get subtree(value(ea), n), w, gw )
else
for all s in subtrees(n) do
Include Rule(s, w, gw )
endfor
endif
else
(Open Node: Specialize the node)
if cow  con 6=  then
(Extend a node)
ea  action in(cow  con )
set motor(n, motor(ea))
closed  closed + 1
for all k in values(motor(ea)) do
new subtree(n, {k, new node(con  (motor(ea)  k))})
open  open + 1
endfor
Include Rule(n, w, gw )
else
(Transform a node into a leaf )
transform to leaf(n, qw , gw , w , cow )
closed  closed + 1
if qw > v then
v  qw
endif
if gw > guess then
g  gw
a  cow
endif
endif
endif
endif

Figure 18: The Include rule algorithm searches for nodes from node n with a partial command compatible with the partial command of rule w and extends those nodes
to insert a leave in the tree.

At a given moment, the inclusion of a new partial rule in the tree produces the specialization of all open nodes compatible with the rule (see Figure 18). We say that an open
node n is compatible with a given rule w if the partial command of the node con and the
partial command of the rule cow does not assign different values to the same motor. The
specialization of an open node can result in the extension of the node (i.e., new branches
114

fiReinforcement Learning in Categorizable Environments

Partial rules
Partial View
Partial Command
T RU Ev
(m1  v1 )  (m2  v1 )
T RU Ev
(m1  v1 )
T RU Ev
(m2  v1 )  (m3  v1 )
T RU Ev
(m2  v1 )
T RU Ev
(m1  v0 )
T RU Ev
(m2  v0 )
T RU Ev
(m3  v1 )
T RU Ev
(m3  v0 )

q

e
5
7
8
3
2
10
1
6


0.1
0.9
2.0
3.1
3.5
3.6
4.0
4.5

0.83
0.52
0.33
0.24
0.22
0.21
0.20
0.18

guess
5.1
6.5
6.0
6.2
5.3
4.1
5.2
12.7

Table 2: Set of rules of the controller. The values q and e are stored and the  and guess
are computed from them. We define all partial views as T RU Ev to indicate that
they are active in the current time step.

are added to the tree under that node) or in the transformation of this node into a leaf. A
node is extended when the partial command of the rule affects some motors not included in
the partial command of the node. This means that there are some motor values not taken
into account in the tree but that have to be used in the action evaluation according to the
rule under consideration. When a node is extended, one of the motors not present in the
above layers of the tree is used to generate a layer of open nodes in the current node. After
that, the node is considered as closed and the inclusion rule procedure is repeated for this
node (with different effects because now the node is closed). When all the motors affected
by the partial command of the rule are also affected by the partial command of the node,
then the node is transformed into a leaf storing the value, guess, and relevance attributes
extracted from the information associated with the rule.
The process is stopped as soon as we detect that all nodes have been closed (i.e. all the
external nodes of the tree are leaves). In this case, the rules still to be processed can have
no effect in the tree form and, consequently are not useful for action evaluation. If a rule is
consistently not used for action evaluation, it can be removed from the controller.
A toy-size example can illustrate this tree-based action-evaluation algorithm. Suppose
that we have a robot with three motors that accept two different values (named v 0 and
v1 ). This produces a set of 8 different actions. Suppose that, at a given moment, the robot
controller includes the set of rules shown in Table 2. In the Action Evaluation algorithm
(Figure 17), rules are processed from the most to the least relevant one expanding an
initially empty tree using algorithm in Figure 18. The inclusion of a rule in the tree results
in an extension of the tree (see stages B, D and E in Figure 19) or in closing branches by
converting open nodes into leaves (stages C and F). In this particular case the tree becomes
completely closed after processing 5 rules out of the 8 active rules in the controller. At
the end of the process, we have a tree with five leaves. Three of them include two actions
and the other two only represent a single action. Using the tree we can say that the value
of the situation in which the tree is constructed, v, is 8 (this is given by the leaf circled
with a solid line in the figure). Additionally, the next action to be executed is of the form
115

fiPorta & Celaya

Motor: m1
Command:
TRUE c

B

A
Open
Node

v1

v0

Motor: m2
Command:
(m1,v1)
v0

Open
Node

v1
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Open
Node

C

Motor: m2
Command:
(m1,v1)
v0

Open
Node

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

E

Open
Node

v0

v1

v1

Open
Node

Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
v1

v1
Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
Value: 3
Guess: 6.2
Relevance: 0.24
Command:
(m1,v0)
(m2,v1)
(m3,v0)

v1

v1

v0

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

Motor: m2
Command:
(m1,v1)
v0

Motor: m2
Command:
(m1,v0)
v1

v0
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Value: 2
Guess: 5.3
Relevance: 0.22
Command:
(m1,v0)
(m2,v0)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Motor: m1
Command:
TRUE c

v1
Motor: m2
Command:
(m1,v1)
v0

v1

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Open
Node

F

Motor: m2
Command:
(m1,v0)

Motor: m2
Command:
(m1,v1)
v0

Motor: m2
Command:
(m1,v0)

Motor: m1
Command:
TRUE c
v0

v1

v0

v1

v0

v0

Motor: m1
Command:
TRUE c

D

Motor: m1
Command:
TRUE c

v1
Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
Value: 3
Guess: 6.2
Relevance: 0.24
Command:
(m1,v0)
(m2,v1)
(m3,v0)

v1

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

v1
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Figure 19: Six different stages during the construction of the tree for action evaluation.
Each stage corresponds to the insertion of one rule from Table 2.

116

fiReinforcement Learning in Categorizable Environments

8
7

log(Time)

6
5
4
3
2
1
0

0

1

2
3
Number of Void Motors

Brute Force Evaluation

4

5

TreeBased Evaluation

Figure 20: Log of the execution time (in seconds) of the brute-force approach vs. the treebased one.

(m1  v1 , m2  v0 , m3  ]) where ] represents any possible action. This optimal action
is given by the leaf circled with a dashed line that is the leaf with a larger guess value.
The cost of our algorithm largely depends on the specific set of partial rules to be
processed. In the worst case, the cost of our algorithm is:
O(nr lnm ),
with nr the number of rules, nm the number of motors and, l the maximal range of values
accepted by the motors. This is because, in the worst case, to insert a given rule, we have to
visit all the nodes of a maximally expanded tree (i.e., a tree where each node has l subtrees
and where all the final nodes of the branches are still opened). The number of nodes of
such a tree is
nm
X
lnm +1  1
li =
= O(lnm ).
l1
i=0

We can transform the cost expression taking into account that l nm is the total number of
possible combinations of elementary actions (nc ) or, in other words, the total amount of
actions. Therefore, the cost of the presented algorithm is
O(nr nc ).
On the other hand, the cost of the brute-force approach is always
(nr nc ).
117

fiPorta & Celaya

So, in the worst case, the cost of the presented algorithm is of the same order as the cost
of the brute-force approach. However, since at most l rules would be enough to close a
maximally expanded tree (one rule for the different values of the motor used in the last
still-open layer of the tree), the cost of the tree-based algorithm would be, on average,
much smaller than that of the brute-force approach.
Figure 20 exemplifies the different performance of the brute-force action-evaluation procedure and the tree-based one. The figure shows the time taken in the execution of the
toy example of Section 6.1. For this experiment, we defined some void motors or motors
whose actions have no effect in the environment. As it can be seen, as the number of void
motors increases, the cost of the tree-based evaluation is significantly less than that of the
brute-force approach.

118

fiReinforcement Learning in Categorizable Environments

Appendix C: Notation
Uppercase are used for sets, and Greek letters represent parameters of the algorithms.
S
Set of states.
0
s, s
Individual states. Full views.
ns
Number of states.
F D = {fdi | i = 1..nf }
Set of feature detectors.
Partial view of order k.
v(fdi1 , . . . , fdik )
A
Set of actions of the robot.
na
Number of actions.
EA = {eai | i = 1..ne }
Set of elementary actions.
nm
Number of motors of the robot.
eai = (mi  k)
Elementary action that assigns value k to motor mi .
c(eai1 , . . . , eaik )
Partial command of order k.
a = (ea1 , . . . , eanm )
Action. Combination of elementary actions. Full command.
w = (v, c)
Partial rule composed by partial view v and partial command c.
w
The empty partial rule.
w1  w 2
Composition of two partial rules.
C = {wi | i = 1..nr }
Controller or set of partial rules.

Maximum number of elements of C.
0
0
C , Cant
Subset of rules active at a given time step and at the previous one.
C 0 (a)
Active rules with a partial command in accordance with a.
qw
Expected value of the partial rule w.
ew
Expected error in the value estimation of the partial rule w.
e
Average error in the value prediction.
iw
Confidence index.
cw
Confidence on the statistics of the partial rule w.

Top value of the confidence.

Index where the confidence function reaches the value .
w = ew cw + e (1  cw ) Error in the return prediction of the partial rule w.
w = 1/(1 + w )
Relevance of rule w.
Iw = [qw  2w ]
Value interval of the partial rule w.
mw
Updating ratio for the statistics of the partial rule w.

Learning rate. Top value of mw .
U (w)
Number of times rule w has been used.
0
winner(C , a)
Most relevant active partial rule w.r.t. action a.
guess(a)
Most reliable value estimation for action a.
ra
Reward received after the execution of a.

Discount factor.
v
Goodness of a given situation.
q = ra + v
Value of executing action a in given situation.

Number of new partial rules created at a time.

Redundancy threshold used for partial-rule elimination.

119

fiPorta & Celaya

References
Arkin, R. C. (1998). Behavior-Based Robotics. Intelligent Robotics and Autonomous Agents.
The MIT Press.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 194.
Brooks, R. A. (1991). Intelligence without representation. Artificial Intelligence, 47, 139
159.
Butz,

M. (1999).
C-XCS: An implementation of
(http://www.cs.bath.ac.uk/ amb/LCSWEB/computer.htm).

the

XCS

in

C.

Celaya, E., & Porta, J. M. (1996). Control of a six-legged robot walking on abrupt terrain.
In Proceedings of the IEEE International Conference on Robotics and Automation,
pp. 27312736.
Celaya, E., & Porta, J. M. (1998). A control structure for the locomotion of a legged
robot on difficult terrain. IEEE Robotics and Automation Magazine, Special Issue on
Walking Robots, 5 (2), 4351.
Chapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcement
learning: An algorithm and performance comparisons. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 726731.
Claus, C., & Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative
multiagent systems. In Proceedings of the Fifteenth National Conference on Artificial
Intelligence, pp. 746752. American Association for Artificial Intelligence.
Drummond, C. (2002). Accelerating reinforcement learning by composing solutions of automatically identified subtasks. Journal of Artificial Intelligence Research, 16, 59104.
Edelman, G. M. (1989). Neuronal Darwinism. Oxford University Press.
Hinton, G., McClelland, J., & Rumelhart, D. (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, chap. Distributed
Representations. MIT Press, Cambridge, MA.
Ilg, W., Muhlfriedel, T., & Berns, K. (1997). Hybrid learning architecture based on neural
networks for adaptive control of a walking machine. In Proceedings of the 1997 IEEE
International Conference on Robotics and Automation, pp. 26262631.
Kaelbling, L. P. (1993). Learning in Embedded Systems. A Bradford Book. The MIT Press,
Cambridge MA.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey.
Journal of Artificial Intelligence Research, 4, 237  285.
Kanerva, P. (1988). Sparse Distributed Memory. MIT Press, Cambridge, MA.
Kirchner, F. (1998). Q-learning of complex behaviors on a six-legged walking machine.
Robotics and Autonomous Systems, 25, 253262.
120

fiReinforcement Learning in Categorizable Environments

Kodjabachia, J., & Meyer, J. A. (1998). Evolution and development of modular control
architectures for 1-d locomotion in six-legged animats. Connection Science, 2, 211
237.
Maes, P., & Brooks, R. A. (1990). Learning to coordinate behaviors. In Proceedings of the
AAAI-90, pp. 796802.
Mahadevan, S., & Connell, J. H. (1992). Automatic programming of behavior-based robots
using reinforcement learning. Artificial Intelligence, 55, 311363.
McCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden
State. Ph.D. thesis, Department of Computer Science.
Parker, G. B. (2000). Co-evolving model parameters for anytime learning in evolutionary
robotics. Robotics and Autonomous Systems, 33, 1330.
Pendrith, M. D., & Ryan, M. R. K. (1996). C-trace: A new algorithm for reinforcement
learning of robotic control. In Proceedings of the 1996 International Workshop on
Learning for Autonomous Robots (Robotlearn96).
Poggio, T., & Girosi, F. (1990). Regularization algorithms for learning that are equivalent
to multilayer networks. Science, pp. 978982.
Schmidhuber, J. (2002). The speed prior: A new simplicity measure yielding near-optimal
computable predictions. In Proceedings of the 15th Annual Conference on Computational Learning Theory (COLT 2OO2). Lecture Notes In Artificial Intelligence.
Springer., pp. 216228.
Sen, S. (1994). Learning to coordinate without sharing information. In Proceedings of
the Twelfth National Conference on Artificial Intelligence, pp. 426431. American
Association for Artificial Intelligence.
Sutton, R. S. (1991). Reinforcement learning architectures for animats. In Meyer, J. A., &
Wilson, S. W. (Eds.), Proceedings of the First International Conference on Simulation of Adaptive Behavior. From Animals to Animats, pp. 288296. The MIT Press,
Bradford Books.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. A Bradford
Book. The MIT Press.
Sutton, R. S., & Whitehead, S. D. (1993). Online learning with random representations.
In Proceedings of the Eleventh International Conference on Machine Learning, pp.
314321. Morgan Kaufman, San Francisco, CA.
Sutton, R. (1996). Generalization in reinforcement learning: Successful examples using
sparse coarse coding. In Proceedings of the 1995 Conference on Advances in Neural
Information Processing, pp. 10381044.
Sutton, R., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 12, 181211.
Tan, M. (1997). Multi-agent reinforcement learning: Independent vs. cooperative agents.
In Reading in Agents, pp. 487494. Morgan Kaufmann Publishers Inc.
121

fiPorta & Celaya

Vallejo, E. E., & Ramos, F. (2000). A distributed genetic programming architecture for
the evolution of robust insect locomotion controllers. In Meyer, J. A., Berthoz, A.,
Floreano, D., Roitblat, H. L., & Wilson, S. W. (Eds.), Supplement Proceedings of the
Sixth International Conference on Simulation of Adaptive Behavior: From Animals to
Animats, pp. 235244. The International Society for Adaptive Behavior.
Venturini, G. (1994). Apprentissage Adaptatif et Apprentissage Supervise par Algorithme
Genetique. Ph.D. thesis.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Widrow, B., & Hoff, M. (1960). Adaptive switching circuits. In Western Electronic Show
and Convention, Volume 4, pp. 96104. Institute of Radio Engineers (now IEEE).
Wilson, S. W. (1995). Classifier fitness based on accuracy. Evolutionary Computation, 3,
149175.
Wilson, S. W. (1996). Explore/exploit strategies in autonomy. In From Animals to Animats 4: Proceedings of the 4th International Conference on Simulation of Adaptive
Behavior, pp. 325332.

122

fiJournal of Artificial Intelligence Research 23 (2005) 245-297

Submitted 12/03; published 03/05

Graduality in Argumentation
Claudette Cayrol
Marie-Christine Lagasquie-Schiex

ccayrol@irit.fr
lagasq@irit.fr

IRIT-UPS, 118 route de Narbonne
31062 Toulouse Cedex, FRANCE

Abstract
Argumentation is based on the exchange and valuation of interacting arguments, followed
by the selection of the most acceptable of them (for example, in order to take a decision,
to make a choice). Starting from the framework proposed by Dung in 1995, our purpose
is to introduce graduality in the selection of the best arguments, i.e. to be able to
partition the set of the arguments in more than the two usual subsets of selected and
non-selected arguments in order to represent different levels of selection. Our basic idea
is that an argument is all the more acceptable if it can be preferred to its attackers. First,
we discuss general principles underlying a gradual valuation of arguments based on their
interactions. Following these principles, we define several valuation models for an abstract
argumentation system. Then, we introduce graduality in the concept of acceptability
of arguments. We propose new acceptability classes and a refinement of existing classes
taking advantage of an available gradual valuation.

1. Introduction
As shown by Dung (1995), argumentation frameworks provide a unifying and powerful
tool for the study of several formal systems developed for common-sense reasoning, as well
as for giving a semantics to logic programs. Argumentation is based on the exchange and
valuation of interacting arguments which support opinions and assertions. It can be applied,
among others, in the legal domain, for collective decision support systems or for negotiation
support.
The fundamental characteristic of an argumentation system is the interaction between arguments. In particular, a relation of attack may exist between arguments. For example, if
the argument takes the form of a logical proof, arguments for a proposition and arguments
against this proposition can be advanced. In that case, the attack relation relies on logical
inconsistency.
The argumentation process is usually divided in two steps: a valuation of the relative
strength of the arguments, followed by the selection of the most acceptable arguments.
In the valuation step, it is usual to distinguish two different types of valuations:
intrinsic valuation: here, the value of an argument is independent of its interactions
with the other arguments. This enables to simply express to what extent an argument
increases the confidence in the statement it supports (see Pollock, 1992; Krause, Ambler, Elvang, & Fox, 1995; Parsons, 1997; Prakken & Sartor, 1997; Amgoud & Cayrol,
1998; Kohlas, Haenni, & Berzati, 2000; Pollock, 2001).
c
2005
AI Access Foundation. All rights reserved.

fiCayrol, Lagasquie-Schiex

For example, in the work of Krause et al. (1995), using the following knowledge base,
composed of (formula, probability) pairs {(1 , 0.8), (2 , 0.8), (3 , 0.8), ((1  2 
4 ), 1), ((1  3  4 ), 1)}, two arguments can be produced1 :
A1 =< {1 , 2 , (1  2  4 )}, 4 >
and A2 =< {1 , 3 , (1  3  4 )}, 4 >.

Both arguments have the same weight 0.8  0.8  1 = 0.64, and the formula  4 has
the weight 0.64 + 0.64  0.512 = 0.7682 .
interaction-based valuation: here the value of an argument depends on its attackers
(the arguments attacking it), the attackers of its attackers (the defenders), etc. 3
Several approaches have been proposed along this line (see Dung, 1995; Amgoud &
Cayrol, 1998; Jakobovits & Vermeir, 1999; Besnard & Hunter, 2001) which differ in
the sets of values used. Usually, two values are considered. However, there are very
few proposals which use more than two values (three values in Jakobovits & Vermeir,
1999, and an infinity of values in Besnard & Hunter, 2001).
For example, in the work of Besnard and Hunter (2001), the set of values is the
interval of the real line [0, 1]. In this case, with the set of arguments4 {A1 , A2 , A3 }
and considering that A1 attacks A2 which attacks A3 , the value of the argument A1
(resp. A2 , A3 ) is 1 (resp. 21 , 32 ).
Intrinsic valuation and interaction-based valuation have often been used separately, according to the considered applications. Some recent works however consider a combination of
both approaches (see Amgoud & Cayrol, 1998; Karacapilidis & Papadias, 2001; Pollock,
2001).
Considering now the selection of the more acceptable arguments, it is usual to distinguish
two approaches:
individual acceptability: here, the acceptability of an argument depends only on its
properties. For example, an argument can be said acceptable if and only if it does not
have any attacker (in this case, only the interaction between arguments is considered,
see Elvang-Goransson et al., 1993). In the context of an intrinsic valuation, an argument can also be said acceptable if and only if it is better than each of its attackers
(see Amgoud & Cayrol, 1998).
collective acceptability: in this case, the acceptability of a set of arguments is explicitly
defined. For example, to be acceptable, a set of arguments may not contain two
1. Here, the arguments are under the form of an Explanation-Conclusion Pair. This is one possible way
to compute arguments (see also Lin & Shoham, 1989; Vreeswijk, 1997; Pollock, 1992; Prakken & Sartor,
1997; Simari & Loui, 1992; Elvang-Goransson, Fox, & Krause, 1993; Kohlas et al., 2000; Amgoud &
Cayrol, 2002).
2. Weights being probabilities, the weight of an argument is the probability of the conjunction of the
formulae of the argument, and the weight of 4 is the probability of the disjunction of A1 and A2 .
3. Here, we consider only the interactions corresponding to attacks between arguments. There exist also
some other types of interactions (for example, arguments which reinforce other arguments instead of attacking them, see Karacapilidis & Papadias, 2001; Verheij, 2002). For this kind of interaction, graduality
has not been considered.
4. Here, the initial knowledge base is useless.

246

fiGraduality in argumentation

arguments such that one attacks the other (interactions between arguments are used).
Dungs (1995) framework is well suited for this kind of approach but allows only for
a binary classification: the argument belongs or does not belong to an acceptable set.
It is clear that except for intrinsic valuations, most proposals do not allow for any gradual
notion of valuation or acceptability (i.e. there is a low number of levels to describe values
and the acceptability is usually binary). Our aim is therefore to introduce graduality in
these two steps.
However, the processes of valuation and of selection are often linked together. This is the
case when the selection is done on the basis of the value of arguments5 or when the selection
defines a binary valuation on arguments. We will therefore:
first consider and discuss the general principles concerning the definition of a gradual
interaction-based valuation and then define some valuation models in an abstract
argumentation system,
then, introduce the notion of graduality in the definition of the acceptability using the
previously defined gradual valuations, but also some more classical mechanisms.
Some graduality has already been introduced in argumentation systems. For instance, in
the work of Pollock (2001), degrees of justification for beliefs are computed. Arguments are
sequences of conclusive and/or prima-facie inferences. Arguments are collected in a graph
where a node represents the conclusion of an argument, a support link ties a node to nodes
from which it is inferred, and an attack link indicates an attack between nodes. The degree
of justification of a belief is computed from the strength of the arguments concluding that
belief and the strength of the arguments concluding on an attacker of the belief.
Our work takes place in a more abstract framework since we do not consider any argument
structure. Our valuation models are based on interactions between arguments and directly
apply to arguments.
We use the framework defined by Dung (1995): a set of arguments and a binary attack
relation between arguments. We also use a graphical representation of argumentation systems (see Section 2). The gradualisation of interaction-based valuations will be presented
in Section 3. Then, in Section 4, we will consider different mechanisms leading to gradual
acceptability, sometimes relying on the gradual valuations defined in Section 3. We will
conclude in Section 5.
All the proofs of the properties stated in Sections 3 and 4 will be given in Appendix A.

2. Dungs (1995) framework and its graphical representation
We consider the abstract framework introduced by Dung (1995). An argumentation system
<A, R> is a set A of arguments and a binary relation R on A called an attack relation:
consider Ai and Aj  A, Ai RAj means that Ai attacks Aj or Aj is attacked by Ai (also
denoted by (Ai , Aj )  R).
5. For example, using Besnard and Hunters (2001) valuation, we can decide that all the arguments whose
value is > 0.5 are selected, because 0.5 is the mean value of the set of values; Another possibility, with
different valuations (interaction-based or intrinsic), is to accept an argument when its value is better
than the value of each of its attackers.

247

fiCayrol, Lagasquie-Schiex

An argumentation system is well-founded if and only if there is no infinite sequence A 0 , A1 ,
. . . , An , . . . such that i, Ai  A and Ai+1 RAi .
Here, we are not interested in the structure of the arguments and we consider an arbitrary
attack relation.
Notation: <A, R> defines a directed graph G called the attack graph. Consider A  A,
the set R (A) is the set of the arguments attacking A6 and the set R+ (A) is the set of the
arguments attacked by A7 .
Example 1
The system <A = {A1 , A2 , A3 , A4 }, R = {(A2 , A3 ), (A4 , A3 ), (A1 , A2 )}> defines the following graph G with the root8 A3 :
A1

A2
A3
A4

Definition 1 (Graphical representation of an argumentation system) Let G be the
attack graph associated with the argumentation system <A, R>, we define:
Leaf of the attack graph A leaf of G is an argument A  A without attackers9 .
Path in the attack graph A path from A to B is a sequence of arguments C = A1 
. . .  An such that:
A = A1 ,
A1 RA2 ,
...,
An1 RAn ,
An = B.
The length of the path is n  1 (the number of edges that are used in the path) and
will be denoted by lC .
A special case is the path10 from A to A whose length is 0.
The set of paths from A to B will be denoted by C(A, B).
6. R (A) = {Ai  A|Ai RA}.
7. R+ (A) = {Ai  A|ARAi }.
8. The word root is used in an informal sense (it just means that there are in the graph some paths
leading to this node). This term and other terms (leaf, branch, path, . . . ) which are used in this
document are standard in graph theory but may have a different definition. They are usual terms in the
argumentation domain. Please see Definition 1 in order to know their precise meaning in this document.
These definitions simply take into account the fact that the directed edges of our graph link attackers to
attacked argument).
9. A is a leaf iff R (A) = .
10. We will assume that there exists an infinity of such paths. This assumption greatly simplifies the handling
of leaves later in the paper.

248

fiGraduality in argumentation

Dependence, independence, root-dependence of a path
Consider 2 paths CA  C(A1 , An ) and CB  C(B1 , Bm ).
These two paths will be said dependent iff Ai  CA , Bj  CB such that Ai = Bj .
Otherwise they are independent.
These two paths will be said root-dependent in An iff An = Bm and Ai 6= An  CA ,
6 Bj  CB such that Ai = Bj .
Cycles in the attack graph A cycle11 is a path C = A1  . . .  An  A1 such that i, j 
[1, n], if i 6= j, then Ai 6= Aj .
A cycle C is isolated iff A  C, 6 B  A such that BRA and B 6 C.
Two cycles CA = A1  . . .  An  A1 and CB = B1  . . .  Bm  B1 are interconnected
iff i  [1, n], j  [1, m] such that Ai = Bj .
We use the notions of direct and indirect attackers and defenders. The notions introduced
here are inspired by related definitions first introduced by Dung (1995) but are not strictly
equivalent12 .
Definition 2 (Direct/Indirect Attackers/Defenders of an argument) Consider A 
A:
The direct attackers of A are the elements of R (A).
The direct defenders of A are the direct attackers of the elements of R (A).
The indirect attackers of A are the elements Ai defined by:
C  C(Ai , A) such that lC = 2k + 1, with k  1.
the indirect defenders of A are the elements Ai defined by:
C  C(Ai , A) such that lC = 2k, with k  2.
If the argument A is an attacker (direct or indirect) of the argument B, we say that A
attacks B (or that B is attacked by A). In the same way, if the argument A is a defender
(direct or indirect) of the argument B, then A defends B (or B is defended by A).
Note that an attacker can also be a defender (for example, if A1 attacks A2 which attacks
A3 , and A1 also attacks A3 ). In the same way, a direct attacker can be an indirect attacker
(for example, if A1 attacks A2 which attacks A3 which attacks A4 , and A1 also attacks A4 )
and the same thing may occur for the defenders.
Definition 3 (Attack branch and defence branch of an argument) Consider A 
A, an attack branch (resp. defence branch) for A is a path in G from a leaf to A whose
length is odd (resp. even). We say that A is the root of an attack branch (resp. a defence
branch).
11. This definition of a cycle corresponds to the definition of an elementary cycle in graph theory (an
elementary cycle does not contain 2 edges with the same initial extremity, or the same ending extremity).
12. In Dungs (1995) work, direct attackers (resp. defenders) are also indirect attackers (resp. defenders)
which is not true in our definitions.

249

fiCayrol, Lagasquie-Schiex

Note that this notion of defence is the basis of the usual notion of reinstatement (B attacks
C, A attacks B and C is reinstated because of A). In this paper, reinstatement is taken
into account indirectly, because the value of the argument C and the possibility for selecting
C will be increased thanks to the presence of A.
All these notions are illustrated on the following example:
Example 2

On this graph G, we can see:
a path from C2 to A whose length is 2 (C2  B1  A),
2 cycles A1  A3  A2  A1 and A1  A3  A4  A1 , of length
3, which are not isolated (note that A1  A3  A2  A1 
A3  A4  A1 is not a cycle with our definition),
the two previous cycles are interconnected (in A1 and A3 ),
the paths D1 C1 B1 and C3 B2 A are independent, the
paths D1  C1  B1  A and C3  B2  A are root-dependent
and the paths D1 C1 B1 A and C2 B1 A are dependent,
D1 , C2 , E1 are the leaves of G,
D1  C1  B1  A is an attack branch for A whose length is
3, C2  B1  A is a defence branch for A whose length is 2,
C2 , B1 and B2 are the direct attackers of A,
C1 , C2 (which is already a direct attacker of A) and C3 are
the direct defenders of A,
D1 and D2 are the two indirect attackers of A,
E1 is the only indirect defender of A.

A3

A4

A1

A2

A
B2

B1

C1

D1

C2

C3

D2

E1

3. Graduality in interaction-based valuations
We consider two different valuation methods for taking into account the quality of attackers
and defenders of an argument in order to define the value of an argument using only the
interaction between arguments13 :
In the first approach, the value of an argument only depends on the values of the direct
attackers of this argument. Therefore, defenders are taken into account through the
attackers. This approach is called local.
In the second approach, the value of an argument represents the set of all the attack
and the defence branches for this argument. This approach is called global.
The main difference between these two approaches is illustrated by the following example:
D

C1

C

B

B

C2

13. We pursue a work initiated in (Cayrol & Lagasquie-Schiex, 2003c) and propose some improvements.

250

fiGraduality in argumentation

In the local approach, B has two direct attackers (C2 and C1 ) whereas B 0 has only one
(C 0 ). Thus B 0 is better than B (since B 0 suffers one attack whereas B suffers two attacks).
In the global approach, two branches (one of attack and one of defence) lead to B whereas
only one branch of attack leads to B 0 . Thus B is better than B 0 (since it has at least one
defence whereas B 0 has none). In this case, C1 loses its negative status of attacker, since it
is in fact carrying a defence for B.
3.1 Local approach (generic valuation)
Some existing proposals can already be considered as examples of local valuations.
In Jakobovits and Vermeirs (1999) approach, a labelling of a set of arguments assigns a
status (accepted, rejected, undecided) to each argument using labels from the set {+, , ?}.
+ (resp. , ?) represents the accepted (resp. rejected, undecided) status. Intuitively,
an argument labelled with ? is both supported and weakened.
Definition 4 (Jakobovits and Vermeirs labellings, 1999) Let <A, R> be an argumentation system. A complete labelling of <A, R> is a function Lab : A  {+, ?, } such
that:
1. If Lab(A)  {?, } then B  R (A) such that Lab(B)  {+, ?}
2. If Lab(A)  {+, ?} then B  R (A)  R+ (A), Lab(B)  {?, }
The underlying intuition is that an argument can only be weakened (label  or ?) if one of
its direct attackers is supported (condition 1); an argument can get a support only if all its
direct attackers are weakened and an argument which is supported (label + or ?) weakens
the arguments it attacks (condition 2). So:
If
If
If
If

A has no attacker Lab(A) = +.
Lab(A) =? then B  R (A) such that Lab(B) =?.
(B  R (A), Lab(B) = ) then Lab(A) = +.
Lab(A) = + then B  R (A)  R+ (A), Lab(B) = .

Every argumentation system can be completely labelled. The associated semantics is that
S is an acceptable set of arguments iff there exists a complete labelling Lab of <A, R> such
that S = {A|Lab(A) = +}.
Other types of labellings are introduced by Jakobovits and Vermeir (1999) among which the
so-called rooted labelling which induces a corresponding rooted semantics. The idea is
to reject only the arguments attacked by accepted arguments: an attack by an undecided
argument is not rooted since an undecided attacker may become rejected.
Definition 5 (Jakobovits and Vermeirs labellings, 1999  continuation)
The complete labelling Lab is rooted iff A  A, if Lab(A) =  then B  R  (A) such that
Lab(B) = +.
The rooted semantics enables to clarify the links between all the other semantics introduced
by Jakobovits and Vermeir (1999) and some semantics introduced by Dung (1995).
251

fiCayrol, Lagasquie-Schiex

Example 3 On the following example:
An

An1

A2

A1

For n even, we obtain Lab(An ) = Lab(An2 ) = . . . = Lab(A2 ) = + and Lab(An1 ) =
Lab(An3 ) = . . . = Lab(A1 ) = .
For n odd, we obtain Lab(An ) = Lab(An2 ) = . . . = Lab(A1 ) = + and Lab(An1 ) =
Lab(An3 ) = . . . = Lab(A2 ) = 
Another type of local valuation has been introduced recently by Besnard and Hunter (2001)
for deductive arguments. The approach can be characterised as follows. An argument
is structured as a pair hsupport, conclusioni, where support is a consistent set of formulae
that enables to prove the formula conclusion. The attack relation considered here is strict
and cycles are not allowed. The notion of a tree of arguments allows a concise and
exhaustive representation of attackers and defenders of a given argument, root of the tree.
A function, called a categoriser, assigns a value to a tree of arguments. This value
represents the relative strength of an argument (root of the tree) given all its attackers and
defenders. Another function, called an accumulator, synthesises the values assigned to all
the argument trees whose root is an argument for (resp. against) a given conclusion. The
phase of categorisation therefore corresponds to an interaction-based valuation. Besnard
and Hunter (2001) introduce the following function Cat:
if R (A) = , then Cat(A) = 1
if R (A) 6=  with R (A) = {A1 , . . . , An }, Cat(A) =

1
1+Cat(A1 )+...+Cat(An )

Intuitively, the larger the number of direct attackers of an argument, the lower its value.
The larger the number of defenders of an argument, the larger its value.
Example 3 (continuation) We obtain:
Cat(A
n ) = 1, Cat(An1 ) = 0.5, Cat(An2 ) = 0.66, Cat(An3 ) = 0.6, . . . , and Cat(A1 ) =

( 5  1)/2 when n   (this value is the inverse of the golden ratio14 ).
So, we have:
If n is even Cat(An1 )  . . .  Cat(A3 )  Cat(A1 )  Cat(A2 )  . . .  Cat(An ) = 1
If n is odd Cat(An1 )  . . .  Cat(A2 )  Cat(A1 )  Cat(A3 )  . . .  Cat(An ) = 1
Our approach for local valuations is a generalisation of these two previous proposals in the
sense that Besnard and Hunters (2001) Cat function and Jakobovits and Vermeirs (1999)
labellings are instances of our approach.
The main idea is that the value of an argument is obtained with the composition of two
functions:
one for aggregating the values of all the direct attackers of the argument; so, this
function computes the value of the direct attack;
the other for computing the effect of the direct attack on the value of the argument:
if the value of the direct attack increases then the value of this argument decreases,
if the value of the direct attack decreases then the value of this argument increases.
14. The golden ratio is a famous number since the antiquity which has several interesting properties in
several domains (architecture, for example).

252

fiGraduality in argumentation

Let (W, ) be a totally ordered set with a minimum element (VMin ) and a subset V of W ,
that contains VMin and with a maximum element VMax .
Definition 6 (Generic gradual valuation) Let <A, R> be an argumentation system.
A valuation is a function v : A  V such that:
1. A  A, v(A)  VMin
2. A  A, if R (A) = , then v(A) = VMax
3. A  A, if R (A) = {A1 , . . . , An } 6= , then v(A) = g(h(v(A1 ), . . . , v(An )))
with h : V   W such that (V  denotes the set of all finite sequences of elements of V )
h(x) = x
h() = VMin
For any permutation (xi1 , . . . , xin ) of (x1 , . . . , xn ), h(xi1 , . . . , xin ) = h(x1 , . . . , xn )
h(x1 , . . . , xn , xn+1 )  h(x1 , . . . , xn )
if xi  x0i then h(x1 , . . . , xi , . . . , xn )  h(x1 , . . . , x0i , . . . , xn )
and g : W  V such that
g(VMin ) = VMax
g(VMax ) < VMax
g is non-increasing (if x  y then g(x)  g(y))
Note that h(x1 , . . . , xn )  max(x1 , . . . , xn ) is a logical consequence of the properties of the
function h.
A first property on the function g explains the behaviour of the local valuation in the case
of an argument which is the root of only one branch (like in Example 3):
Property 1 The function g satisfies for all n  1:
g(VMax )  g 3 (VMax )  . . .  g 2n+1 (VMax )  g 2n (VMax )  . . .  g 2 (VMax )  VMax
Moreover, if g is strictly non-increasing and g(VMax ) > VMin , the previous inequalities
become strict.
A second property shows that the local valuation induces an ordering relation on arguments:
Property 2 (Complete preordering) Let v be a valuation in the sense of Definition 6.
v induces a complete15 preordering  on the set of arguments A defined by: A  B iff
v(A)  v(B).
A third property handles the cycles:
15. A complete preordering on A means that any two elements of A are comparable.

253

fiCayrol, Lagasquie-Schiex

Property 3 (Value in a cycle) Let C be an isolated cycle of the attack graph, whose
length is n. If n is odd, all the arguments of the cycle have the same value and this value
is a fixpoint of the function g. If n is even, the value of each argument of the cycle is a
fixpoint of the function g n .
The following property shows the underlying principles satisfied by all the local valuations
defined according to our schema:
Property 4 (Underlying principles) The gradual valuation given by Definition 6 respects the following principles:
P1 The valuation is maximal for an argument without attackers and non maximal for an
attacked and undefended argument.
P2 The valuation of an argument is a function of the valuation of its direct attackers (the
direct attack).
P3 The valuation of an argument is a non-increasing function of the valuation of the direct
attack.
P4 Each attacker of an argument contributes to the increase of the valuation of the direct
attack for this argument.
The last properties explain why Jakobovits and Vermeir (1999) and Besnard and Hunter
(2001) propose instances of the local valuation described in Definition 6:
Property 5 (Link with Jakobovits & Vermeir, 1999)
Every rooted labelling of <A, R> in the sense of Jakobovits and Vermeir (1999) can be
defined as an instance of the generic valuation such that:
V = W = {, ?, +} with  < ? < +,
VMin = ,
VMax = +,
g defined by g() = +, g(+) = , g(?) =?
and h is the function max.
Property 6 (Link with Besnard & Hunter, 2001) The gradual valuation of Besnard
and Hunter (2001) can be defined as an instance of the generic valuation such that:
V = [0, 1],
W = [0, [,
VMin = 0,
VMax = 1,
1
g : W  V defined by g(x) = 1+x
and h defined by h(x1 , . . . , xn ) = x1 + . . . + xn .
254

fiGraduality in argumentation

Note that, in the work of Besnard and Hunter (2001), the valued graphs are acyclic. However, it is easy to show that the valuation proposed by Besnard and Hunter (2001) can be
generalised to graphs with cycles (in this case, we must solve second degree equations  see
Example 5).


















A






























B1B2B3









































C1

C3





C2
C4























































D1












D2
D3

































































E1















































































Example 4 Consider the following graph:

B4

In this example, with the generic valuation, we obtain:
v(E1 ) = v(D2 ) = v(D3 ) = v(C4 ) = v(B4 ) = VMax
v(D1 ) = v(C2 ) = v(C3 ) = v(B3 ) = g(VMax )
v(C1 ) = v(B2 ) = g 2 (VMax )
v(B1 ) = g(h(g 2 (VMax ), g(VMax )))
v(A) = g(h(g(h(g 2 (VMax ), g(VMax ))), g 2 (VMax ), g(VMax ), VMax ))
So, we have:
E 1 , D 2 , D 3 , C 4 , B4

C 1 , B2

D 1 , C 2 , C 3 , B3
However, the constraints on v(A) and v(B1 ) are insufficient to compare A and B1 with the
other arguments.
The same problem exists if we reduce the example to the hatched part of the graph in the
previous figure; we obtain E1 , D2  C1  D1 , C2 , but A and B1 cannot be compared with
the other arguments16 .
Now, we use the instance of the generic valuation proposed by Besnard and Hunter (2001):
v(E1 ) = v(D2 ) = v(D3 ) = v(C4 ) = v(B4 ) = 1,
v(D1 ) = v(C2 ) = v(C3 ) = v(B3 ) = 12 ,
v(C1 ) = v(B2 ) = 23 ,
16. v(A) = g 2 (h(g 2 (VMax , g(VMax ))) and v(B1 ) = g(h(g 2 (VMax ), g(VMax ))).

255

fiCayrol, Lagasquie-Schiex

6
,
v(B1 ) = 13
78
v(A) = 283 .

So, we have:
E 1 , D 2 , D 3 , C 4 , B4

C 1 , B2

D 1 , C 2 , C 3 , B3

B1

A
However, if we reduce the example to the hatched part of the graph, then the value of A is
13
19 . So, v(A) is better than v(B1 ) and v(D1 ), but also than v(C1 ) (A becomes better than
its defender).
Example 5 (Isolated cycle) Consider the following graph reduced to an isolated cycle:
A

B

.

A generic valuation gives v(A) = v(B) = fixpoint of g 2 .
If we use the instance proposed by Besnard and Hunter (2001), v(A) and v(B) are solutions
of the following second degree equation:
x2 + x  1 = 0.

1+ 5
So, we obtain: v(A) = v(B) =
 0.618 (the inverse of the golden ratio again).
2
3.2 Global approach (with tuples)
We now consider a second approach for the valuation step, called the global approach. Here,
the key idea is that the value of A must describe the subgraph whose root is A. So, we
want to memorise the length of each branch leading to A in a tuple (for an attack branch,
we have an odd integer, and for a defence branch, we have an even integer).
In this approach, the main constraint is that we must be able to identify the branches
leading to the argument and to compute their lengths. This is very easy in the case of an
acyclic graph. We therefore introduce first a global gradual valuation for acyclic graphs.
Then, in the next sections, we extend our proposition to the case of graphs with cycles, and
we study the properties of this global gradual valuation.
3.2.1 Gradual valuation with tuples for acyclic graphs
First, in order to record the lengths of the branches leading to the arguments, we use the
notion of tuples and we define some operations on these tuples:
256

fiGraduality in argumentation

Definition 7 (Tuple) A tuple is a sequence of integers. The tuple (0, . . . , 0, . . .) will be
|
{z
}
denoted by 0 . The tuple (1, . . . , 1, . . .) will be denoted by 1 .
|
{z
}





Notation 1 T denotes the set of the tuples built with positive integers.
Definition 8 (Operations on the tuples) We have two kinds of operations on tuples:
the concatenation of two tuples is defined by the function ? : T  T  T such that
0 ? t = t ? 0 = t for t 6= ()

(x1 , . . . , xn , . . .) ? (x01 , . . . , x0n , . . .) = Sort(x1 , . . . , xn , . . . , x01 , . . . , x0n , . . .)
Sort being the function which orders a tuple by increasing values.
the addition of a tuple and an integer is defined by the function  : T 
that

 T such

0  k = (k)
()  k = ()

(x1 , . . . , xn )  k = (x1 + k, . . . , xn + k)

(x1 , . . . , xn , . . .)  k = (x1 + k, . . . , xn + k, . . .) if (x1 , . . . , xn , . . .) 6= 0
Note that we allow infinite tuples, among other reasons, because they are needed later in
order to compute the ordering relations described in Section 3.2.4 (in particular when the
graph is cyclic).
The operations on the tuples have the following properties:
Property 7 (Properties of ? and )
The concatenation ? is commutative and associative.
For any tuple t and any integers k and k 0 , (t  k)  k 0 = t  (k + k 0 ).
For any integer k and any tuples t and t0 different from 017 , (t ? t0 )  k = (t  k) ? (t0  k).
In order to valuate the arguments, we split the set of the lengths of the branches leading
to the argument in two subsets, one for the lengths of defence branches (even integers) and
the other one for the lengths of attack branches (odd integers). This is captured by the
notion of tupled values:
Definition 9 (Tupled value) A tupled value is a pair of tuples vt = [vtp , vti ] with:
vtp is a tuple of even integers ordered by increased values; this tuple is called the even
component of vt;
vti is a tuple of odd integers ordered by increased values; this tuple is called the odd
component of vt.
17. Otherwise it is false : (0 ? (p))  k = (p + k), whereas (0  k) ? ((p)  k) = (k) ? (p + k) = (k, p + k).

257

fiCayrol, Lagasquie-Schiex

Notation 2 V denotes the subset of T  T of all tupled values (so, vt  V, vt is a pair
of tuples satisfying Definition 9).
Using this notion of tupled-values, we can define the computation process of the gradual
valuation with tuples18 in the case of acyclic graphs.
Definition 10 (Valuation with tuples for acyclic graphs) Let <A, R> be an argumentation system without cycles. A valuation with tuples is a function v : A  V such
that:
If A  A is a leaf then

v(A) = [0 , ()].

If A  A has direct attackers denoted by B1 , . . . , Bn , . . . then
v(A) = [vp (A), vi (A)] with:

vp (A) = (vi (B1 )1)?. . .?(vi (Bn )1)?. . .
vi (A) = (vp (B1 )1)?. . .?(vp (Bn )1)?. . .

Notes: The choice of the value [0 , ()] for the leaves is justified by the fact that the value
of an argument memorises all the lengths of the branches leading to the argument. Using
the same constraint, either vp (A) or vi (A) may be empty but not both19 .
Note also that the set of the direct attackers of an argument can be infinite (this property
will be used when we take into account an argumentation graph with cycles).
Example 6 On this graph, the valuation with tuples gives the following results:
A
B2

B1

C1

D1

On this graph G, we have:

C2

C3

D2

v(D1 ) = v(C2 ) = v(E1 ) = [0 , ()],
v(C1 ) = v(D2 ) = [(), (1)],
v(C3 ) = [(2), ()],
v(B1 ) = [(2), (1)],
v(B2 ) = [(), (3)],
v(A) = [(2, 4), (1, 3)].

E1

18. This definition is different from the definition given in (Cayrol & Lagasquie-Schiex, 2003c). The ideas
are the same but the formalisation is different.
19. The proof is the following:.
If A is not a leaf, at least one of the tuples is not empty, because there exists at least one branch
whose length is > 0 leading to A (see Definitions 8 and 10).
And, if A is a leaf, there also exists at least one defence branch because the path from A to A is
allowed and its length is 0 (in fact, there are an infinity of such paths  see Definition 1) and no attack
branch leading to the leaf (see Definition 10).
So, the value of a leaf is [0 , ()], and it is impossible that vp (A) = vi (A) = ().

258

fiGraduality in argumentation

3.2.2 Study of cycles
Handling cycles raises some important issues: the notion of branch is not always useful in a
cycle (for example, in an unattacked cycle like in Examples 5 and 7), and when this notion
is useful, the length of a branch can be defined in different ways.
Let us consider different examples:
Example 7 (Unattacked cycle) The graph is reduced to an unattacked cycle A  B  A
which attacks the argument C:
A

B

C

The notion of branch is useless in this case, because there is no leaf in the graph.
There are two possibilities:
First, one can consider that the cycle is like an infinite branch; so A (resp. B) is the
root of one branch whose length is . But the parity of the length of this branch is
undefined, and it is impossible to say if this branch is an attack branch or a defence
branch.
The second possibility is to consider that the cycle is like an infinity of branches; so
A (resp. B) is the root of an infinity of attack branches and defence branches whose
lengths are known and finite.
The second possibility means that the cycle may have two representations which are acyclic
but also infinite graphs (one with the root A and the other one with the root B). This is a
rewriting process of the cycle:
B4

A4

B1

B5

B6

A1

A2

A3

B2

B3

B4

A1

A5

A6

B1

B2

B3

A2

A3

A4

B

A

The Ai and Bi must be new arguments created during the rewriting process of the cycle.
Example 8 (Attacked cycle) The cycle A  B  A is attacked by at least one argument
which does not belong to the cycle (here, the attacker is the unattacked argument D):
259

fiCayrol, Lagasquie-Schiex

D

A

B

C

E

In this case, the notion of branch is useful because there exists one leaf in the graph, but the
difficulty is to compute the length of this branch. As in Example 7, we can consider either
that there is only one infinite branch (so, it is impossible to know if this branch is an attack
or a defence branch), or that there is an infinity of attack branches and defence branches
whose lengths are known and finite.
In the second case, the graph can be rewritten into the following structures:
D

D

A6

A3

D

D

B3

A4

A5

D

B3

A1

A2

D

B1

B2

B1

B2

A1

A2

A3

A

B

C

E

The Ai and Bi must be new arguments created during the rewriting process of the graph.
From the previous examples, we have chosen to manage a cycle as an infinity of attack
branches and defence branches whose lengths are known and finite because we would like to
be able to apply Definition 10 in all cases (acyclic graphs and graphs with cycles). However,
we need a rewriting process of the graph with cycles into an acyclic graph. There are two
different cases, one for the unattacked cycles and one for the attacked cycles:
Definition 11 (Rewriting of an unattacked cycle) Let C = A0  A1  . . .  An1  A0
an unattacked cycle. The graph G which contains C is rewritten as follows:
260

fiGraduality in argumentation

1. the cycle C is removed,
2. and replaced by the infinite acyclic graphs, one for each Ai , i = 0 . . . n  1:

%
Ai 11

%
Ai 21

Ai 22

Ai
...
...
...
...
...

-

Ai n1
1

Ai n1
2
...

Ai n1
n1

Ai n1

Ai n2
...

Ai nn1

Ai nn

-

Ai n+1
1

Ai n+1
2
...

Ai n+1
n1

Ai n+1
n

Ai n+1
n+1

...
...
...
...
...
...
...
...
...
...
...

3. the edges between each of the Ai and an argument which does not belong to C are kept.
Example 7  Unattacked cycle (continuation) The graph G containing the unattacked
cycle A  B  A and the argument C, which is attacked by A, is rewritten as follows:
C

A
%
A11

%
A21

A22

B
A31

A32

A33

%
B11

...
...
...
...
...
...

%
B12

B22

B13

B23

B33

...
...
...
...
...
...

where the Alk and Bkl are new arguments.
Definition 12 (Rewriting of an attacked cycle) Let C = A0  A1  . . .  An1  A0
an attacked cycle, the direct attacker of each Ai is denoted Bi , if it exists. The graph G
which contains C is rewritten as follows:
1. the cycle C is removed,
2. and replaced by the infinite acyclic graphs, one for each Ai i = 0 . . . n  1:
261

fiCayrol, Lagasquie-Schiex

%
Bi

%
Ai 11


B(i1+n) mod n

Ai
...
...
...
...
...

Ai n1
1

Ai n1
2
...

Ai n1
n1


Ai n1

Ai n2
...

Ai nn1

Ai nn

Bi

B(i+1) mod n

Ai n+1
1

Ai n+1
2
...

Ai n+1
n1

Ai n+1
n

Ai n+1
n+1


B(i1+n) mod n

...
...
...
...
...
...
...
...
...
...
...
...
...

(the branches leading to Bk exist iff Bk exists20 ).
3. the edges between each of the Ai and an argument which does not belong to C are kept.
4. the edges between each of the Bi and an argument which does not belong to C are kept.
Example 8  Attacked cycle (continuation) The graph G containing the cycle A 
B  A attacked in A by the argument D and with the argument C (resp. E) attacked by A
(resp. B) is rewritten as follows:
E

B

C

A
%
D


A21

A22

D

A41

A42

A43

A44

D

%
B11

D

...
...
...
...
...
...
...
...
...
...

where the Alk and Bkl are new arguments.
20. The operator mod is the modulo function.

262


B13

B23

B33

D

B15

B25

B35

B45

B55

D

...
...
...
...
...
...
...
...
...
...
...
...

fiGraduality in argumentation

Note: If there exist several cycles in a graph, we have two cases.
If they are not interconnected, we rewrite each cycle, and the valuation of the resulting
graph after rewriting does not depend on the order of cycles we select to rewrite
because the valuation process only uses the length of the branches.
If they are interconnected, they are considered as a metacyle which is in turn attacked or unattacked and the previous methodology can be used leading to a more
complex rewriting process which is not formalized here (see details and examples in
Appendix B).
3.2.3 A gradual valuation with tuples for general graphs
Using the definitions given in Sections 3.2.1 and 3.2.2, the gradual valuation with tuples
given by Definition 10 is applicable for arbitrary graphs after the rewriting process.
Let us apply the rewriting process and Definition 10 on different examples.
Example 7  Unattacked cycle (continuation)
Consider the following graph:
A

B

C

The rewriting of this graph has been given in Section 3.2.2.
Definition 10 produces:
vp (A) = (vi (A11 )  1) ? . . . ? (vi (An1 )  1) ? . . .
vi (A) = (vp (A11 )  1) ? . . . ? (vp (An1 )  1) ? . . .
Applying Definition 10 for different arguments in the rewritten graph produces the following
equalities:
v(Ann ) = [0 , ()] for each n  1
v(Ann1 ) = [(), (1)] for each n  2
m
m
v(Am
n ) = [vp (An+2 )  2, vi (An+2 )  2] for each n  1 and m  n + 2

So, using the above equalities in the formulae giving vp (A) and vi (A), we define two sequences of tuples : a sequence (xk , k  1) of infinite tuples of even integers, and a sequence
(yk , k  1) of infinite tuples of odd integers
n
xk = (2) ? (vi (A2k+1
2k1 )  1) ? . . . ? (vi (A2k1 )  1) ? . . .
n
yk = (1) ? (vp (A2k+1
2k1 )  1) ? . . . ? (vp (A2k1 )  1) ? . . .

263

fiCayrol, Lagasquie-Schiex

From the results stated in Property 7, it is easy to prove that vp (A) = x1 and for each k  1,
xk = (2) ? (xk+1  2).
Similarly, vi (A) = y1 and for each k  1, yk = (1) ? (yk+1  2).
These equations enable to prove that :
For each even integer p p > 0, p belongs to each tuple xi , i  1.
For each odd integer p, p belongs to each tuple yi , i  1.

The proof is done by induction on p.
So, v(A) = v(B) = [(2, 4, 6, . . .), (1, 3, 5, . . .)].
Then, v(C) = [(2, 4, 6, . . .), (3, 5, 7, . . .)].
Note that all the above results can be readily extended to an unattacked cycle of length n,
n  2.
Property 8 (Properties of unattacked cycles)
For each unattacked cycle, for each argument A of the cycle, v(A) = [(2, 4, 6, . . .), (1, 3, 5, . . .)].
Example 8  Attacked cycle (continuation)

Consider the following graph:

D

A

B

C

E

The rewriting of this graph has been given in Section 3.2.2.
Definition 10 produces:
vp (A) = (vi (D)  1) ? (vi (A21 )  1) ? . . . ? (vi (A2n
1 )  1) ? . . .
vi (A) = (vp (D)  1) ? (vp (A21 )  1) ? . . . ? (vp (A2n
1 )  1) ? . . .
and also
v(D) = [0 , ()]
v(Ann ) = [(), (1)] for each n  2
As done in the treatment of Example 7, the formulae giving vp (A) and vi (A) can be rewritten
in order to bring to light some interesting sequences of tuples.
264

fiGraduality in argumentation

2(k+p)

x0k = (vi (A2k
2k1 )  1) ? . . . ? (vi (A2k1 )  1) ? . . .
2(k+p)

yk0 = (1) ? (vp (A2k
2k1 )  1) ? . . . ? (vp (A2k1 )  1) ? . . .
Then, it is easy to prove that vp (A) = x01 and for each k  1, x0k = (x0k+1  2).
0
 2).
Similarly, vi (A) = y10 and for each k  1, yk0 = (1) ? (yk+1

The first equation enables to prove that x01 is the empty tuple21 .
The second equation has already been solved and produces y10 = (1, 3, 5, . . .).
So, v(A) = [(), (1, 3, 5, . . .)]. For B, we can reason as for A, and we have v(B) = [(2, 4, 6, . . .), ()].
Then, v(C) = [(2, 4, 6, . . .), ()], v(E) = [(), (3, 5, 7 . . .)].
Notation: in order to simplify the writing, we will not repeat the values inside the tuples
(we will just indicate under each value how many times it appears). For example:
[(2, 4, 4, 6, 6, 6, 8, 8, 8, 8 . . .), (3, 5, 5, 7, 7, 7, 9, 9, 9, 9 . . .)]
will be denoted by
[(2, |{z}
4 , |{z}
6 , |{z}
8 , . . .), (3, |{z}
5 , |{z}
7 , |{z}
9 , . . .)]
2

3

4

2

3

4

Conclusion about cycles Cycles are expensive since all the values obtained are infinite.
In appendix B, we introduce an algorithm for computing these tupled values. It uses a
process of value propagation and is parameterised by a maximum number of runs through
a cycle. This number will be used in order to stop the propagation mechanism and to
obtain finite (thus incomplete) tupled values.
3.2.4 Comparison of tupled values
In this section, we define the comparison relation between arguments (so, between some particular tupled values), using the following idea: an argument A is better than an argument
B iff A has a better defence (for it) and a lower attack (against it).
The first idea is to use a lexicographic ordering on the tuples. This lexicographic ordering
denoted by lex on T is defined by:
21. The proof is the following:.
x01 contains only even integers.
For each k, x0k 6= 0 since x0k is the result of the addition of a tuple and an integer.
If x01 is not empty, let e1 denote the least even integer present in x01 . As x01 = x02  2, x02 is not empty
and e2 will denote the least integer present in x02 . We have e1 = e2 + 2. So, we are able to build
a sequence of positive even integers e1 , e2 , . . ., which is strictly decreasing. That is impossible. So,
x01 = ().

265

fiCayrol, Lagasquie-Schiex

Definition 13 (Lexicographic ordering on tuples)
Let (x1 , . . . , xn , . . .) and (y1 , . . . , ym , . . .) be 2 finite or infinite tuples  T .
(x1 , . . . , xn , . . .) <lex (y1 , . . . , ym , . . .) iff i  1 such that:
j < i, xj = yj and
yi exists and:
either the tuple (x1 , . . . , xn , . . .) is finite with a number of elements equal to i  1
(so, xi does not exist),
or xi exists and xi < yi .
(x1 , . . . , xn , . . .) =lex (y1 , . . . , ym , . . .) iff the tuples contain the same number p   {}
of elements and i, 1  i  p, xi = yi .
So, we define: (x1 , . . . , xn , . . .) lex (y1 , . . . , ym , . . .) iff
(x1 , . . . , xn , . . .) =lex (y1 , . . . , ym , . . .) or (x1 , . . . , xn , . . .) <lex (y1 , . . . , ym , . . .).
The ordering <lex is a generalisation of the classical lexicographic ordering (see Xuong,
1992) to the case of infinite tuples. This ordering is complete but not well-founded (there
exist infinite sequences which are strictly non-increasing: (0) <lex (0, 0) <lex . . . <lex
(0, . . . , 0, . . .) <lex . . . <lex (0, 1)).
Since the even values and the odd values in the tupled value of an argument do not play
the same role, we cannot use a classical lexicographic comparison. So, we compare tupled
values in two steps:
The first step compares the number of attack branches and the number of defence
branches of each argument. So, we have two criteria (one for the defence and the other
for the attack). These criteria are aggregated using a cautious method: we conclude if
one of the arguments has more defence branches (it is better according to the defence
criterion) and less attack branches than the other argument (it is also better according
to the attack criterion). Note that we conclude positively only when all the criteria
agree: if one of the arguments has more defence branches (it is better according to
the defence criterion) and more attack branches than the other argument (it is worse
according to the attack criterion), the arguments are considered to be incomparable.
Else, the arguments have the same number of defence branches and the same number
of attack branches, and a second step compares the quality of the attacks and the
quality of the defences using the length of each branch. This comparison is made
with a lexicographic principle (see Definition 13) and gives two criteria which are
again aggregated using a cautious method. In case of disagreement, the arguments
are considered to be incomparable.
Let us consider some examples:
[(2), (1)] is better than [(2), (1, 1)] because there are less attack branches in the first
tupled value than in the second tupled value, the numbers of defence branches being
the same (first step).
[(2), (1)] is incomparable with [(2, 2), (1, 1)] because there are less defence branches
and less attack branches in the first tupled value than in the second tupled value (first
step).
266

fiGraduality in argumentation

[(2), (3)] is better than [(2), (1)] because there are weaker attack branches in the first
tupled value than in the second tupled value (the attack branch of the first tupled
value is longer than the one of the second tupled value), the defence branches being
the same (second step, using the lexicographic comparison applied on even parts then
on odd parts of the tupled values).
[(2), (3)] is better than [(4), (3)] because there are stronger defence branches in the
first tupled value than in the second tupled value (the defence branch is shorter in
the first tupled value than in the second tupled value), the attack branches being the
same (second step).
[(2), (1)] is incomparable with [(4), (3)] because there are worse attack branches and
better defence branches in the first tupled value than in the second tupled value
(second step).
The comparison of arguments is done using Algorithm 1 which implements the principle
of a double comparison (first quantitative, then qualitative) with two criteria (one defence
criterion and one attack criterion) using a cautious method.
Algorithm 1: Comparison of two tupled values
% Description of the parameters:
% v, w: 2 tupled values
% Notations:
%
|vp | (resp. |wp |): number of elements in the even component of v (resp. w)
%
if vp (resp. wp ) is infinite then |vp | (resp. |wp |) is taken equal to 
%
|vi | (resp. |wi |): number of elements in the odd component of v (resp. w)
%
if vi (resp. wi ) is infinite then |vi | (resp. |wi |) is taken equal to 
%
As usual,  will denote the strict relation associated with  defined by:
%
v  w iff v  w and not(w  v).

%
%
%
%
%
%
%
%
%

begin
if v = w then v  w AND w  v
% Case 1 %
2
else
3
if |vi | = |wi | AND |vp | = |wp | then
% lexicographic comparisons between vp and wp and between vi and wi %
4
if vp lex wp AND vi lex wi then v  w
% case 2 %
5
else
6
if vp lex wp AND vi lex wi then v  w
% case 3 %
7
else v 6 w AND v 6 w
% Incomparable tupled values. case 4 %
1

8
9
10
11
12

else
if |vi |  |wi | AND |vp |  |wp | then v  w
% case 5 %
else
if |vi |  |wi | AND |vp |  |wp | then v  w
% case 6 %
else v 6 w AND v 6 w
% Incomparable tupled values. Case 7 %

end

Algorithm 1 defines a partial preordering on the set v(A):
Property 9 (Partial preordering) Algorithm 1 defines a partial preordering  on the
set v(A).
267

fiCayrol, Lagasquie-Schiex

The tupled value [0 , ()] is the only maximal value of the partial preordering .
The tupled value [(), 1 ] is the only minimal value of the partial preordering .
Notation: the partial preordering  on the set v(A) induces a partial preordering on the
arguments (the partial preordering on A will be denoted like the partial preordering on
v(A)): A  B if and only if v(A)  v(B)22 .
In order to present the underlying principles satisfied by the global valuation, we first
consider the different ways for modifying the defence part or the attack part of an argument:
Definition 14 (Adding/removing a branch to an argument)
Let A be an argument whose tupled value is v(A) = [vp (A), vi (A)] with vp (A) = (xp1 , . . . , xpn )
and vi (A) = (xi1 , . . . , xim ) (vp (A) or vi (A) may be empty but not simultaneously).
Adding (resp. removing) a defence branch to A is defined by:
vp (A) becomes Sort(xp1 , . . . , xpn , xpn+1 ) where xpn+1 is the length of the added branch (resp.
j  [1..n] such that vp (A) becomes (xp1 , . . . , xpj1 , xpj+1 , . . . , xpn )).
And the same thing on vi (A) for adding (resp. removing) an attack branch to A.
Definition 15 (Increasing/decreasing the length of a branch of an argument)
Let A be an argument whose tupled value is v(A) = [vp (A), vi (A)] with vp (A) = (xp1 , . . . , xpn )
and vi (A) = (xi1 , . . . , xim ) (vp (A) or vi (A) may be empty but not simultaneously).
Increasing (resp. decreasing) the length of a defence branch of A is defined by:
p
p
0p
p
j  [1..n] such that vp (A) becomes (xp1 , . . . , xpj1 , x0p
j , xj+1 , . . . , xn ) where xj > xj (resp.
p
0p
p
x0p
j < xj ) and the parity of xj is the parity of xj .
And the same thing on vi (A) for increasing (resp. decreasing) an attack branch to A.
Definition 16 (Improvement/degradation of the defences/attacks)
Let A be an argument whose tupled value is v(A) = [vp (A), vi (A)] (vp (A) or vi (A) may be
empty but not simultaneously). We define:
An improvement (resp. degradation) of the defence consists in
adding a defence branch to A if initially vp (A) 6= 0 (resp. removing
a defence branch of A);
or decreasing (resp. increasing) the length of a defence branch of A;
or removing the only defence branch leading to A (resp. adding a defence branch leading to A if initially vp (A) = 0 );
An improvement (resp. degradation) of the attack consists in
adding (resp. removing) an attack branch to A;
or decreasing (resp. increasing) the length of an attack branch of A.
Property 10 (Underlying principles) Let v be a valuation with tuples (Definition 10)
associated with Algorithm 1, v respects the following principles:
P10 The valuation is maximal for an argument without attackers and non maximal for an
argument which is attacked (whether it is defended or not).
22. We will also use the notation B  A defined by: B  A iff A  B.

268

fiGraduality in argumentation

P20 The valuation of an argument takes into account all the branches which are rooted in
this argument.
P30 The improvement of the defence or the degradation of the attack of an argument leads
to an increase of the value of this argument.
P40 The improvement of the attack or the degradation of the defence of an argument leads
to a decrease of the value of the argument.
Example 4 (continuation)

With the valuation with tuples, we obtain:

v(E1 ) = v(D2 ) = v(D3 ) = v(C4 ) = v(B4 ) = [0 , ()],
v(D1 ) = v(C2 ) = v(C3 ) = v(B3 ) = [(), (1)],
v(C1 ) = v(B2 ) = [(2), ()],
v(B1 ) = [(2), (3)],
v(A) = [(2, 4), (1, 3, 3)].
So, we have:
E 1 , D 2 , D 3 , C 4 , B4

C 1 , B2

B1

D 1 , C 2 , C 3 , B3

but also

E 1 , D 2 , D 3 , C 4 , B4

A

A is incomparable with almost all the other arguments (except with the leaves of the graph).
Similarly, on the hatched part of the graph, we obtain the following results:
E1 , D2  C 1  B 1  A  D 1 , C 2
A is now comparable with all the other arguments (in particular, A is worse than its
defender C1 and than its direct attacker B1 ).
3.3 Main differences between local and global valuations
Cayrol and Lagasquie-Schiex (2003c) give a comparison of these approaches with some existing approaches (Dung, 1995; Jakobovits & Vermeir, 1999; Besnard & Hunter, 2001), and
also a comparison of the local approaches and the global approach. The improvement
of the global approach proposed in this paper does not modify the main results of this
comparison.
Let us recall here an example of the essential point which differentiates them (this example
has already been presented at the beginning of Section 3):
D

C1

C

B
C2

269

B

fiCayrol, Lagasquie-Schiex

In the local approach, B 0 is better than B (since B 0 suffers one attack whereas B suffers
two attacks).
In the global approach, B is better than B 0 (since it has at least a defence whereas B 0 has
none). In this case, C1 loses its negative status of attacker, since it is in fact carrying a
defence for B.
The following table synthesises the results about the different proposed valuations:
global approach
arguments
ing only
branches

havattack



arguments having
attack
branches
and
defence
branches



arguments
having only defence
branches



arguments
never attacked

local approach
arguments having
arguments
havonly one attacked
ing
only
one


direct
attacker
unattacked direct
(possibly
defended)
attacker
arguments having several attacked direct attackers (possibly defended)

arguments having
several unattacked
direct attackers



arguments
never attacked

The difference between the local approaches and the global approach is also illustrated by
the following property:
Property 11 (Independence of branches in the global approach)
Let A be an argument having the following direct attackers:
A1 whose value is v(A1 ) = [(a1p1 , . . . , a1pm ), (a1i1 , . . . , a1im )],
1
1
...,
An whose value is v(An ) = [(anp1 , . . . , anpmn ), (ani1 , . . . , animn )].
Let A0 be an argument having the following direct attackers:
A1p1 whose value is v(A1p1 ) = [(a1p1 )()],
...,
A1pm whose value is v(A1pm ) = [(a1pm )()],
1

1

1

A1i1 whose value is v(A1i1 ) = [()(a1i1 )],
...,
A1im whose value is v(A1im ) = [()(a1im )],
1
1
1
...,
Anp1 whose value is v(Anp1 ) = [(anp1 )()],
...,
270

fiGraduality in argumentation

Anpmn whose value is v(Anpmn ) = [(anpmn )()],
Ani1 whose value is v(Ani1 ) = [()(ani1 )],
...,
Animn whose value is v(Animn ) = [()(animn )].
Then v(A) = v(A0 ).
This property illustrates the independence of branches during the computation of the
values in the global approach, even when these branches are not graphically independent.
On the following example, A and A0 have the same value [(2, 2)()] though they are the root
of different subgraphs:
C1

C1
B

B1
A

A
C2

C2

B2

This property is not satisfied by the local approach since, using the underlying principles
of the local approach (see Property 4), the value of the argument A must be at least as
good as (and sometimes better than23 ) the value of the argument A0 (A having one direct
attacker, and A0 having two direct attackers).
3.4 Conclusion about valuation step
We have proposed two different gradual valuation models and we are now able to make a
distinction between different arguments using the preordering associated with a valuation
model. These valuations will be used for the selection of the arguments (see Section 4).

4. Graduality and acceptability
In this section, we now shift to the selection step and introduce graduality in the notion of
acceptability24 .
The basic idea is to select an argument depending on the non-selection of its direct attackers.
Following this idea, we propose two different methods:
The first method consists in refining the classical partition issued from Dungs collective acceptability; this refinement may be achieved using the gradual valuations
defined in Section 3.
The second method takes place in an individual acceptability and consists in defining
a new acceptability using only the gradual valuations defined in Section 3.
4.1 Dungs (1995) collective acceptability
In the framework of collective acceptability, we have to consider the acceptability of a set
of arguments. This acceptability is defined with respect to some properties and the sets
which satisfy these properties are called acceptable sets or extensions. An argument will be
said acceptable if and only if it belongs to an extension.
23. With the valuation proposed by Besnard and Hunter (2001), we obtain: v(A) = 34 and v(A0 ) = 12 .
24. This work has been presented in a workshop (Cayrol & Lagasquie-Schiex, 2003b).

271

fiCayrol, Lagasquie-Schiex

Definition 17 (Basic properties of extensions following Dung, 1995)
Let <A, R> be an argumentation system, we have:
Conflict-free set A set E  A is conflict-free if and only if 6 A, B  E such that ARB.
Collective defence Consider E  A, A  A. E collectively defends A if and only if
B  A, if BRA, C  E such that CRB. E defends all its elements if and only if
A  E, E collectively defends A.
Dung (1995) defines several semantics for collective acceptability: mainly, the admissible
semantics, the preferred semantics and the stable semantics (with corresponding extensions:
the admissible sets, the preferred extensions and the stable extensions).
Definition 18 (Some semantics and extensions following Dung, 1995) Let <A, R>
be an argumentation system.
Admissible semantics (admissible set) A set E  A is admissible if and only if E is
conflict-free and E defends all its elements.
Preferred semantics (preferred extension) A set E  A is a preferred extension if
and only if E is maximal for set inclusion among the admissible sets.
Stable semantics (stable extension) A set E  A is a stable extension if and only if E
is conflict-free and E attacks each argument which does not belong to E (A  A \ E,
B  E such that BRA).
Note that in all the above definitions, each attacker of a given argument is considered
separately (the direct attack as a whole is not considered). Dung (1995) proves that:
Any admissible set of <A, R> is included in a preferred extension of <A, R>.
There always exists at least one preferred extension of <A, R>.
If <A, R> is well-founded then there is only one preferred extension which is also the
only stable extension.
Any stable extension is also a preferred extension (the converse is false).
There is not always a stable extension.
Property 12 The set of leaves (i.e. {A|R (A) = }) is included in every preferred extension and in every stable extension.
4.2 Different levels of collective acceptability
Under a given semantics, and following Dung, the acceptability of an argument depends on
its membership to an extension under this semantics. We consider three possible cases 25 :
25. The terminology used in this section is also used in the domain of nonmonotonic reasoning (see Pinkas
& Loui, 1992): the word uni comes from the word universal which is a synonym of the word skeptical,
and the word exi comes from the word existential which is a synonym of the word credulous. We have
chosen to use the words uni and exi because they recall the logical quantificators  (for all) and  (exists
at least one).

272

fiGraduality in argumentation

the argument can be uni-accepted, when it belongs to all the extensions of this semantics,
or the argument can be exi-accepted, when it belongs to at least one extension of this
semantics,
or the argument can be not-accepted when it does not belong to any extension of this
semantics.
However, these three levels seem insufficient. For example, what should be concluded in
the case of two arguments A and B which are exi-accepted and such that ARB or BRA?
So, we introduce a new definition which takes into account the situation of the argument
w.r.t. its attackers. This refines the class of the exi-accepted arguments under a given
semantics S.
Definition 19 (Cleanly-accepted argument) Consider A  A, A is cleanly-accepted
if and only if A belongs to at least one extension of S and B  A such that BRA, B does
not belong to any extension of S.
Thus, we capture the idea that an argument will be better accepted, if its attackers are
not-accepted.
Property 13 Consider A  A and a semantics S such that each extension for S is conflictfree. If A is uni-accepted then A is cleanly-accepted. The converse is false.
The notion of cleanly-accepted argument refines the class of the exi-accepted arguments.
For a semantics S and an argument A, we have the following states:
A can be uni-accepted, if A belongs to all the extensions for S (so, it will also be
cleanly-accepted);
or A can be cleanly-accepted (so, it is by definition also exi-accepted); note that it is
possible that the argument is also uni-accepted;
or A can be only-exi-accepted, if A is not cleanly-accepted, but A is exi-accepted;
or A is not-accepted if A does not belong to any extension for S.
Example 9 Consider the following argumentation system.
There are two preferred extensions {D, C2 , A, G} and
J
{D, C2 , E, G, I}. So, for the preferred semantics, the acI
ceptability
levels are the following:
E
A
G
H

D

B

F

C1

C2

D, C2 and G are uni-accepted,
I is cleanly-accepted but not uni-accepted,
A and E are only-exi-accepted,
B, C1 , F , H and J are not-accepted.

Note that, in all the cases where there is only one extension, the first three levels of acceptability coincide26 . This is the case:
26. If there is only one extension then the fact that A belongs to all the extensions is equivalent to the
fact that A belongs to at least one extension. Moreover, with only one extension containing A, all the
attackers of A do not belong to an extension. So, A is cleanly-accepted.

273

fiCayrol, Lagasquie-Schiex

Under the preferred semantics, when there is no even cycle (see Doutre, 2002).
Under the basic semantics (another semantics proposed by Dung  see Dung, 1995;
Doutre, 2002  which is not presented here and which has only one extension).
Looking more closely, we can prove the following result (proof in Appendix A):
Property 14 Under the stable semantics, the class of the uni-accepted arguments coincides
with the class of the cleanly-accepted arguments.
Then, using a result issued from the work of Dunne and Bench-Capon (2001, 2002) and
reused by Doutre (2002) which shows that, when there is no odd cycle, all the preferred
extensions are stable27 , we apply Property 14 and we obtain the following consequence:
Consequence 1 Under the preferred semantics, when there is no odd cycle, the class of
the uni-accepted arguments coincides with the class of the cleanly-accepted arguments.
Finally, the exploitation of the gradual interaction-based valuations (see Section 3) allows
us to define new levels of collective acceptability.
Let v be a gradual valuation and let  be the associated preordering (partial or complete)
on A. This preordering can be used inside each acceptability level (for example, the level of
the exi-accepted arguments) in order to identify arguments which are better accepted than
others.
Example 9 (continuation)
graph:

Two different gradual valuations are applied on the same

0,674

0,590

0,482

J

A

E
I
0,694

H

G 0,666

0,441
0,4 B

D
1

C1

F

0,5

C2 1

0.5

Besnard & Hunters (2001) valuation
With the instance of the generic valuation proposed by Besnard and Hunter (2001) (see
Section 3.1), we obtain the following comparisons:
D, C2  I  E  G  J  C1 , F  A  H  B
27. This corresponds to the consistent argumentation system proposed by Dung (1995).

274

fiGraduality in argumentation

{
{

[(6,8,10,12,...),
2 3
{
{
{

(7,9,10,11,...)]
2 2 3

J

[(4,6,8,10,...),
(3,5,7,9,...)]

E
H

2 2 3

2 3

[(2,4,6,8,...),
(3,5,7,9...)]
A

G [(2),()]

{
{
{

{
{

[(4,6,8,10,...),
2 3
(5,7,9,11,...)]

{
{
{

[(6,8,10,12,...),
2 2 3
(5,7,9,11,...)]
{
{

I

[(2),(1)]

D

B

C1

[(0,...,0),()]

F [(),(1)]

C2 [(0,...,0),()]

[(),(1)]

Valuation with tuples

With the global valuation with tuples presented in Section 3.2, we obtain the following comparisons:
D, C2  G  B  F, C1
D, C2  A  E
D, C2  H  E
D, C2  I
D, C2  J
So, all the arguments belonging to a cycle are incomparable with G, B, F , C 1 and, even
between them, there are few comparison results.
If we apply the preordering induced by a valuation without respecting the acceptability
levels defined in this section, counter-intuitive situations may happen. In Example 9, we
obtain:
With the valuation of Besnard and Hunter (2001) and under the preferred semantics,
E  G despite the fact that G is uni-accepted and E is only-exi-accepted.
With the valuation with tuples and under the preferred semantics, H  E despite the
fact that E is only-exi-accepted and H is not-accepted.
These counter-intuitive situations illustrate the difference between the acceptability definition and the valuation definitions (even if both use the interaction between arguments, they
do not use it in the same way).
275

fiCayrol, Lagasquie-Schiex

4.3 Towards a gradual individual acceptability
The individual acceptability is based on the comparison of an argument with its attackers.
The first proposal has been to select an argument if and only if it does not have any attacker
(see Elvang-Goransson et al., 1993).
This has later been extended by Amgoud and Cayrol (1998) where, using a preference
relation between arguments (an intrinsic valuation), an argument is accepted if and only if
it is preferred to each of its attackers.
Following this proposal, we propose the same mechanism but with the interaction-based
valuation.
Given v a gradual valuation, the preordering induced by v can be directly used in order
to compare, from the acceptability point of view, an argument and its attackers 28 . This
defines a new class of acceptable arguments: well-defended arguments.
Definition 20 (Well-defended argument) Consider A  A, A is well-defended (for v)
if and only if B  A such that BRA, B 6 A.
Thus, we capture the idea that an argument will be better accepted if it is at least as good
as its direct attackers (or incomparable with them in the case of a partial ordering). The
set of well-defended arguments will depend on the valuation used.
Using this new notion, the set of the arguments is partitioned in three classes:
the first class contains the arguments which are not attacked,
the second class contains the arguments which are attacked but are well-defended,
the third class contains the other arguments (attacked and not well-defended).
Note that the set of the well-defended arguments corresponds to the union of the two first
classes. A further refinement uses the gradual valuation inside each of the classes as in
Section 4.2.
In Example 9 presented in Section 4.2, the well-defended arguments are:
D, C2 , G, H and A (A is incomparable with B but better than E) for the valuation
with tuples,
though with the valuation of Besnard and Hunter (2001) the well-defended arguments
are D, C2 , G, I and E (E is better than A).
Note also that, as in the semantics of Dung (1995), Definition 20 considers the attackers
one by one. It is not suitable for a valuation which handles the direct attack as a whole
(as the valuation of Besnard and Hunter (2001)  see the counterexamples presented in
Section 4.4).
28. This idea is also used in the notion of defeat proposed by Bench-Capon (2002). So, there is a link
between a well-defended argument and an argument which is not attacked in the sense of BenchCapon (2002) by its direct attackers. Note that, in the work of Bench-Capon (2002), the valuation is an
extra knowledge added in the argumentation framework. In contrast, here, the v-preference is extracted
from the attack graph.

276

fiGraduality in argumentation

4.4 Compatibility between acceptability and gradual valuation
Following the previous sections, the set of arguments can be partitioned in two different
ways:
First, given a semantics S and a gradual valuation v, it is possible to use the partition
issued from Dung (1995) which we have refined:
Uni
accepted
Cleanly
accepted

Exi
accepted

OnlyExi
accepted

Not
accepted

Refinement of each level with the gradual valuation v

Second, given a gradual valuation v, it is possible to use the partition induced by the
notion of well-defended arguments:
Attaked but
not Welldefended
Arguments

Attaked
Arguments


















WellDefended



for
the
valuation
v
















Unattacked
Arguments

A very natural and interesting question is: is it possible to find a semantics S and a gradual
valuation v such that the associated partitions have some compatibilities?
The following examples show that the class of the well-defended arguments does not correspond to the class of the cleanly-accepted arguments (in some cases, some uni-accepted
arguments are even not well-defended).
277

fiCayrol, Lagasquie-Schiex

4.4.1 Examples showing the non-compatibility in the general case
We give examples for each usual valuation (the global valuation with tuples and 2 instances
of the generic local valuation: Besnard & Hunter, 2001; Jakobovits & Vermeir, 1999) and
for the most classical semantics for acceptability (preferred semantics and stable semantics
of Dung, 1995).
Cleanly-accepted argument but not well-defended: There are 3 examples (each using a distinct valuation: one for the global valuation and two for the two well-known instances of
the local valuation):
the argument A is cleanly-accepted but it is not well-defended:
A

0.4

0.5

B1

B2

B3
0.5

C2

C3
1

0.5

C1
1

1

Only 1 preferred and stable extension = { C1, C2, C3, A}
B1, B2, B3 do not belong to a preferred extension
Bi
A forall i = 1, 2, 3

the argument A is cleanly-accepted but it is not well-defended:
?

C

2 preferred and stable extensions : {C,A} and {D,A}
B doesnt belong to a preferred extension
B
A

A


B
?
D
?

the argument I is cleanly-accepted but it is not well-defended:
[(4,4),(3)]
A
[(0,...0),()]

B

C

[(),(1)]

[(2),()]

D
[(),(3)]
H
[(),(3)]

E

F
[(4),(5,5)]

G
[(6,6),(5)]

I
[(6),(7,7)]

Only 1 preferred and stable extension = {A,C,F,I}
G doesnt belong to a preferred extension
G
I

Well-defended argument but not cleanly-accepted: Similarly, for the same three valuations,
we have:
the argument C is well-defended but it is not cleanly-accepted:
?
D
+

A
?

B
C
?

Only 1 preferred and stable extension : {D, B}
C
B
C doesnt not belong to a preferred or a stable extension

the argument F is well-defended but it is not cleanly-accepted:
278

fiGraduality in argumentation

0.618

2 preferred and stable extensions = {A,H,E} and {B,H,F}
F belongs to a preferred and stable extension
F
E and E belongs also to a preferred and stable extension
while E attacks F

0.618
B

A

E

G
0.5

0.472

H
1

F
0.679

the argument G is well-defended but it is not cleanly-accepted:
[(4,4),(3)]
A
[(0,...,0),()]

B

C

[(),(1)]

[(2),()]

D
[(),(3)]

E

F
[(4),(5,5)]

G
[(6,6),(5)]

I
[(6),(7,7)]

H
[(),(3)]

Only 1 preferred and stable extension = {A,C,F,I}
G
F
G doesnt belong to a preferred or stable extension

4.4.2 Particular cases leading to compatibility
In the context of an argumentation system with a finite relation R without cycles 29 , the
stable and the preferred semantics provide only one extension and the levels of uni-accepted,
exi-accepted, cleanly-accepted coincide.
In this context, there are at least two particular cases leading to compatibility.
First case: It deals with the global valuation with tuples.
Theorem 1 Let G be the graph associated with <A, R>, <A, R> being an argumentation
system with a finite relation R without cycles and satisfying the following condition: 
A  A such that
Xi , leaf of G,  only one path from Xi to A, Xi1  . . .  Xili  A with Xi1 = Xi and
li the length of this path (if li is even, this path is a defence branch for A, else it is
an attack branch),
all the paths from Xi to A are root-dependent in A,
Ai  A, Xj a leaf of G such that Ai belongs to a path from Xj to A.
Let v be a valuation with tuples. Let S be a semantics  {preferred, stable}.
1. B  A, B 6= A, B (exi, uni, cleanly) accepted for S iff B well-defended for v.
2. If A is (exi, uni, cleanly) accepted for S then A is well-defended for v (the converse
is false).
3. If A is well-defended for v and if all the branches leading to A are defence branches
for A then A is (exi, uni, cleanly) accepted for S.
29. So, (A, R) is well-founded.

279

fiCayrol, Lagasquie-Schiex

Note that Theorem 1 is, in general, not satisfied by a local valuation. See the following
counterexample for the valuation of Besnard and Hunter (2001):
0,4 A

0,5

1

B1

C1

0,5

B2

1 C2

0,5

B3

1 C3

The graph satisfies the condition stated in Theorem 1. The set of well-defended arguments
is {C1 , C2 , C3 } (so, A is not well-defended). Nevertheless, {C1 , C2 , C3 , A} is the preferred
extension.
Second case: This second case concerns the generic local valuation:
Theorem 2 Let <A, R> be an argumentation system with a finite relation R without
cycles. Let S be a semantics  {preferred, stable}. Let v be a generic local valuation
satisfying the following condition ():
(i = 1 . . . n, g(xi )  xi )  (g(h(x1 , . . . , xn ))  h(x1 , . . . , xn ))
()
A  A, A (exi, uni, cleanly) accepted for S iff A well-defended for v.
This theorem is a direct consequence of the following lemma:
Lemma 1 Let <A, R> be an argumentation system with a finite relation R without cycles.
Let S be a semantics  {preferred, stable}. Let v be a generic local valuation satisfying the
condition ().
(i) If A is exi-accepted and A has only one direct attacker B then A  B.
(ii) If B is not-accepted and B has only one direct attacker C then C  B.
Remark: The condition () stated in Theorem 2 is:
false for the local valuation proposed by Besnard and Hunter (2001) as shown in the
following graph:
0,4 A

0,5

1

We know that g(x) =

1
1+x

and

B1

C1

0,5

B2

0,5

B3

1 C2

1 C3
h(x1 , . . . , xn ) = ni=1 xi

(see Property 6). We get:

i = 1 . . . 3, xi = v(Bi ) = 0.5,
i = 1 . . . 3, g(xi ) = 0.66, so g(xi )  xi ,
and nevertheless g(h(x1 , x2 , x3 )) = v(A) = 0.4 6 h(x1 , x2 , x3 ) = 1.5.
280

fiGraduality in argumentation

false for the local valuations defined with h such that n > 1 with h(x1 , . . . , xn ) >
max(x1 , . . . , xn ) (for all the functions g strictly non-increasing): see the previous graph
where h(x1 , x2 , x3 ) = 1.5 and max(x1 , x2 , x3 ) = 0.5.
true for the local valuations defined with h = max (for all the functions g): if h = max
then g(h(x1 , . . . , xn )) = g(max(x1 , . . . , xn )) = g(xj ), xj being the maximum of the xi ;
and, by assumption, g(xi )  xi , xi , so in particular for xj ; so, we get:
g(h(x1 , . . . , xn )) = g(xj )  xj = max(x1 , . . . , xn ) = h(x1 , . . . , xn ).

5. Conclusion
In this paper, we have introduced graduality in the two main related issues of argumentation
systems:
the valuation of the arguments,
the acceptability of the arguments.
Regarding the first issue, we have defined two formalisms introducing an interaction-based
gradual valuation of arguments.
First, a generic gradual valuation which covers existing proposals (for example Besnard
& Hunter, 2001 and Jakobovits & Vermeir, 1999). This approach is essentially local
since it computes the value of the argument only from the value of its direct attackers.
Then, an approach based on a labelling which takes the form of a pair of tuples;
this labelling memorises the structure of the graph representing the interactions (the
attack graph), associating each branch with its length (number of the edges from
the leaf to the current node) in the attack graph (if the length of the branch is an even
integer, the branch is a defence branch for the current node, otherwise the branch is
an attack branch for the current node). This approach is said to be global since
it computes the value of the argument using the whole attack graph influencing the
argument.
We have shown that each of these valuations induces a preordering on the set of the arguments, and we have brought to light the main differences between these two approaches.
Regarding the second issue, two distinct approaches have been proposed:
First, in the context of the collective acceptability of Dung (1995): three levels of
acceptability (uni-accepted, exi-accepted, not-accepted) were already defined. More
graduality can be introduced in the collective acceptability using the notion of cleanlyaccepted arguments (those whose direct attackers are not-accepted).
Then, in the context of individual acceptability: using the previously defined gradual
valuations, the new notion of well-defended arguments has been introduced (those
which are preferred to their direct attackers in the sense of a given gradual valuation
v).
The first concept induces a refinement of the level of exi-accepted in two sublevels (cleanlyaccepted arguments and only-exi-accepted arguments). The gradual valuation allows graduality inside each level of this collective acceptability.
281

fiCayrol, Lagasquie-Schiex

The second concept induces two new levels of acceptability (well-defended arguments and
not-well-defended arguments). The gradual valuation also allows graduality inside each
level of this individual acceptability.
Regarding our initial purpose of introducing graduality in the definition of acceptability, we
have adopted a basic principle:
acceptability is strongly related to the interactions between arguments (represented
on the graph of interactions),
and an argument is all the more acceptable if it is preferred to its direct attackers.
Then, we have followed two different directions. One is based on a refinement of an existing
partition and remains in the framework of Dungs work. The other one is based on the
original concept of being well-defended, and deserves further investigation, in particular
from a computational point of view.

Acknowledgements
Thanks to the reviewers for their very interesting and constructive comments.
Thanks to Thomas Schiex for his help.

Appendix A. The proofs
In this section, we give the proofs of all the properties presented in Sections 3 and 4.
Proof
(of Property 1) By induction from VMin  g(VMax ) < VMax and by applying
function g twice.


Proof
(of Property 2) The valuation function v associates each argument A with a
value v(A) belonging to a set V which is a subset of a completely ordered set
W.


Proof
(of Property 3) Let C = An  An1  . . .  A2  A1 be a cycle:
If n is even: n = 2k and v(A1 ) = g(v(A2 )) = . . . = g 2k1 (v(A2k )) =
g 2k (v(A1 )); so, v(A1 ) is a fixpoint of g 2k = g n . It is the same for each Ai ,
1  i  2k.
However, the Ai may have different values: for example, for n = 2, with
the valuation of Jakobovits and Vermeir (1999), v(A1 ) = + and v(A2 ) = 
with g(+) =  and g() = +. If all the Ai have the same value, then this
value will be a fixpoint of g (because v(A1 ) = g(v(A2 )) = g(v(A1 ))).
282

fiGraduality in argumentation

If n is odd: n = 2k + 1 and v(A1 ) = g(v(A2 )) = . . . = g 2k (v(A2k+1 )) =
g 2k+1 (v(A1 )); so, v(A1 ) is a fixpoint of g 2k+1 = g n . It is the same for each
Ai , 1  i  2k + 1.
Since the function g is non-increasing, the function g 2k+1 is also nonincreasing and we can apply the following result: if a non-increasing function has fixpoints, these fixpoints are identical30 . So, v(A1 ) = . . . =
v(A2k+1 ). But, v(A1 ) = g(v(A2 )) = g(v(A1 )), so v(A1 ) is a fixpoint of g.
So, for all the 1  i  2k + 1, v(Ai ) is a fixpoint of g.

Proof
(of Property 4)
P1 is satisfied because: A  A, if A has no direct attacker (R (A) is empty),
then v(A) = VMax and g(VMax ) < VMax .
P2 is satisfied because if R (A) = {A1 , . . . , An }, h(v(A1 ), . . . , v(An )) evaluates
the direct attack of A.
P3 is satisfied because the function g is supposed to be non-increasing.
P4 is satisfied due to the properties of the function h.



Proof
(of Property 5) The valuation proposed by Jakobovits and Vermeir (1999) is
the following:
Let <A, R> be an argumentation system. A complete labelling of <A, R> is a
function Et : A  {+, ?, } such that:
1. If Et(A)  {?, } then B  R (A) such that Et(B)  {+, ?}

2. If Et(A)  {+, ?} then B  R (A) or  R+ (A), Et(B)  {?, }
Moreover, Jakobovits and Vermeir (1999) also define a complete rooted labelling
Et with: A  A, if Et(A) =  then B  R (A) such that Et(B) = +.

The translation of Et into a local gradual valuation is very easy:

g is defined by g() = +, g(+) = , g(?) =? and h is the function max.



Proof
(of Property 6) Besnard and Hunter (2001) introduce the following function
Cat (in the context of deductive arguments and for an acyclic graph):
if R (A) = , then Cat(A) = 1
30. Proof: let g be a non-increasing function, let  and  be two fixpoints of g. If  6= , we may suppose
that  > , so g()  g() (since g is non-increasing), so    (since  and  are fixpoints of g), which
is in contradiction with the assumption  > .

283

fiCayrol, Lagasquie-Schiex

if R (A) 6=  with R (A) = {A1 , . . . , An }, Cat(A) =

1
1+Cat(A1 )+...+Cat(An )

The translation of Cat into a gradual valuation is: V = [0, 1], W = [0, [,
1
and h is
VMin = 0 and VMax = 1 and g : W  V is defined by g(x) = 1+x
defined by h({x1 , . . . , xn }) = x1 +    + xn .

Proof
(of Property 7) Let t = (x1 , . . . , xn , . . .), t0 = (y1 , . . . , yn , . . .), t00 = (z1 , . . . , zn , . . .)
be tuples.
Commutativity of ?: t ? t0 = t0 ? t There are two cases:
if t or t0 = 0 , the property is given by Definition 8.
if t and t0 6= 0 :
t ? t0 = Sort(x1 , . . . , xn , . . . , y1 , . . . , yn , . . .)
= Sort(y1 , . . . , yn , . . . , x1 , . . . , xn , . . .)
= t0 ? t
Associativity of ?: (t ? t0 ) ? t00 = t ? (t0 ? t00 ) There are two cases:
if t or t0 or t00 = 0 , we can simplify the expression. For example, if
t = 0 :
(t ? t0 ) ? t00 = t0 ? t00
= t ? (t0 ? t00 )
if t, t0 and t00 6= 0 :
(t ? t0 ) ? t00 = Sort(x1 , . . . , xn , . . . , y1 , . . . , yn , . . . , z1 , . . . , zn , . . .)
= t ? (t0 ? t00 )
Property of : (t  k)  k 0 = t  (k + k 0 ) We have:
(t  k)  k 0 = (x1 + k, . . . , xn + k, . . .)  k 0

= (x1 + k + k 0 , . . . , xn + k + k 0 , . . .)

= t  (k + k 0 )
Distributivity: (t ? t0 )  k = (t  k) ? (t0  k) We have:
(t ? t0 )  k = Sort(x1 , . . . , xn , . . . , x01 , . . . , x0n , . . .)  k

= Sort(x1 + k, . . . , xn + k, . . . , x01 + k, . . . , x0n + k, . . .)

= (t  k) ? (t0  k)


284

fiGraduality in argumentation

Proof
(of Property 9) First, we show that the relation  defined by Algorithm 1 is
a partial ordering:
Let u, v, w be three tupled values, the relation  defined by Algorithm 1 is:
reflexive: u  u because u = u, so u  u AND u  u (case 1 of
Algorithm 1);
transitive: suppose that u  v and v  w and consider all the
possible cases:
if u = v:
if v = w: then u = w so u  w,
if |vi |  |wi | AND |vp | > |wp |: then |vi | = |ui |  |wi | AND
|vp | = |up | > |wp |, so u  w,
if |vi | < |wi | AND |vp |  |wp |: then |vi | = |ui | < |wi | AND
|vp | = |up |  |wp |, so u  w,
if |vi | = |wi | AND |vp | = |wp | AND vp lex wp AND
vi lex wi : then |vi | = |ui | = |wi | AND |vp | = |up | = |wp |
AND vp = up lex wp AND vi = ui lex wi , so u  w;
if |ui |  |vi | AND |up | > |vp |:
if v = w: then |ui |  |vi | = |wi | AND |up | > |vp | = |wp | so
u  w,
if |vi |  |wi | AND |vp | > |wp |: then |ui |  |vi |  |wi | AND
|up | > |vp | > |wp |, so u  w,
if |vi | < |wi | AND |vp |  |wp |: then |ui |  |vi | < |wi | AND
|up | > |vp |  |wp |, so u  w,
if |vi | = |wi | AND |vp | = |wp |: then |ui |  |vi | = |wi | AND
|up | > |vp | = |wp |, so u  w;
if |ui | < |vi | AND |up |  |vp |:
if v = w: then |ui | < |vi | = |wi | AND |up |  |vp | = |wp | so
u  w,
if |vi |  |wi | AND |vp | > |wp |: then |ui | < |vi |  |wi | AND
|up |  |vp | > |wp |, so u  w,
if |vi | < |wi | AND |vp |  |wp |: then |ui | < |vi | < |wi | AND
|up |  |vp |  |wp |, so u  w,
if |vi | = |wi | AND |vp | = |wp |: then |ui | < |vi | = |wi | AND
|up |  |vp | = |wp |, so u  w;
if |ui | = |vi | AND |up | = |vp | AND up lex vp AND ui lex
vi :
if v = w: then |ui | = |vi | = |wi | AND |up | = |vp | = |wp |
AND up lex vp = wp AND ui lex vi = wi so u  w,
if |vi |  |wi | AND |vp | > |wp |: then |ui | = |vi |  |wi | AND
|up | = |vp | > |wp |, so u  w,
285

fiCayrol, Lagasquie-Schiex

if |vi | < |wi | AND |vp |  |wp |: then |ui | = |vi | < |wi | AND
|up | = |vp |  |wp |, so u  w,
if |vi | = |wi | AND |vp | = |wp | AND vp lex wp AND
vi lex wi : then |ui | = |vi | = |wi | AND |up | = |vp | = |wp |
AND up lex vp lex wp AND ui lex vi lex wi , so
u  w.
In all cases, u  w.
Now, consider the maximal and minimal values:
The tupled value [0 , ()] is the unique maximal element for the preordering
: let v be a tupled value such that v 6= [0 , ()], then |vp |   and |vi |  0.
Compare [0 , ()] and v with Algorithm 1: [0 , ()] 6= v so the case number
1 is not used; then, |()| = 0  |vi | AND |0 | =   |vp | so there are two
cases:
if |vp | =  and |vi | = 0, the case 3 of Algorithm 1 is applied and
[0 , ()]  v,
else |vp |   and |vi |  0, the case 5 of Algorithm 1 is applied and
[0 , ()]  v.
The tupled value [(), 1 ] is the unique minimal element for the preordering
: let v be a tupled value such that v 6= [(), 1 ], then |vi |   and |vp |  0.
Compare [(), 1 ] and v with Algorithm 1: [(), 1 ] 6= v so the case number
1 is not used; then, |()| = 0  |vp | AND |1 | =   |vi | so there are two
cases:
if |vi | =  and |vp | = 0, the case 2 of Algorithm 1 is applied and
[(), 1 ]  v,
else |vi |   and |vp |  0, the case 6 of Algorithm 1 is applied and
[(), 1 ]  v.

Proof
(of Property 10) The principle P10 is satisfied by Definition 10 and by the
fact that [0 , ()] is the unique maximal element of v(A) (see Property 9).
The principle P20 is satisfied because of Definition 10.
The principles P30 and P40 are satisfied: all the possible cases of improvement/degradation of the defence/attack for a given argument (see Definition 16)
are applied case by case31 . Each case leads to a new argument. Using Algorithm 1, the comparison between the argument before and after the application
of the case shows that the principle P30 (or P40 , depending on the applied case)
31. We work case by case in order to avoid the complex cases in which we have several simultaneous simple
modifications. For example, the modification of the length of a branch which changes the status of the
branch (an even integer replaced by an odd integer) is a complex case corresponding to two simple cases:
the removal of a branch with a given status, then the addition of a new branch with a different status.

286

fiGraduality in argumentation

is satisfied.



Proof
(of Property 11) From Definition 10.



Proof
(of Property 12) First, we consider the case of the preferred extensions: Let
E be a preferred extension  A, we assume that E does not contain all the
unattacked arguments of A. So, let A  A be an unattacked argument such
that A 6 E.
Consider E  {A}:
If E  {A} is conflict-free then, with A an unattacked argument and E
a preferred extension, E  {A} collectively defends itself, so E  {A} is
admissible and E  E {A}. This contradicts the fact that E is a preferred
extension.
If E  {A} contains at least one conflict, then:

B  E such that BRA. This is impossible since A is unattacked.
or B  E such that ARB. But, since A is unattacked, @C  E
such that CRA. So, E does not collectively defend B, which is in
contradiction with the fact that E is a preferred extension.

So, the assumption E does not contain all the unattacked arguments of A
cannot hold.
Now, we consider stable extensions: Let E be a stable extension  A, we assume
that E does not contain all the unattacked arguments of A. So, let A  A be
an unattacked argument such that A 6 E.
Since A 6 E there exists in E another argument B which attacks A; This is
impossible since A is unattacked.
So, the assumption E does not contain all the unattacked arguments of A
cannot hold.


Proof
(of Property 13) An argument and one of its direct attackers cannot belong
to the same extension in the sense of Dung (1995) because the extension must
be conflict-free. So, since A is uni-accepted, it means that A belongs to all the
extensions, and none of the direct attackers of A belongs to these extensions.
For the converse, we use the following counterexample in the case of the preferred
semantics:
287

fiCayrol, Lagasquie-Schiex

F

A

K

C

B
H

J

E

G

There are two preferred extensions
{K, H, G} and {A, E, K, H}. The argument A is cleanly-accepted (B and
C do not belong to any preferred extension, and A belongs to at least one
of the two extensions). But, A is not
uni-accepted because it does not belong
to all preferred extensions.



Proof
(of Property 14) First, A uni-accepted  A cleanly-accepted is the result of
Property 13.
Conversely, let A be a cleanly-accepted argument, there exists at least one stable extension E such that A  E and B, BRA, B 6 E 0 , E 0 stable extension.
Using a reductio ad absurdum, we assume that there exists a stable extension
E 00 such that A 6 E 00 ; but, if A 6 E 00 , it means that B  E 00 such that BRA,
so, the direct attacker B of A belongs to a stable extension; so, there is a contradiction with the assumption (A is cleanly-accepted); so, E 00 does not exist
and A is uni-accepted.


Proof
(of Theorem 1)
1. We consider the arguments B  A such that B 6= A. Let Xi be a leaf, the
path C  C(Xi , A) is Xi1  . . .  Xili  A with Xi1 = Xi and li denoting the
length of the path (if li is even, this path is a defence branch for A, else it
is an attack branch).
The constraints from Xi1 to Xili are the following:
Xi1  Xi3  . . .  Xili  Xili 1  . . .  Xi4  Xi2 if li odd  1
or
Xi1  Xi3  . . .  Xili 1  Xili  . . .  Xi4  Xi2 if li even  2
So, for the path Xi1  . . .  Xili , the set of the well-defended arguments is
{Xi1 , Xi3 , . . . , Xili } if li is odd, {Xi1 , Xi3 , . . . , Xili 1 } otherwise (this is the
set of all the arguments having a value strictly better than those of their
direct attackers). This set will denoted by Accepi .
By definition, this set is conflict-free, it defends all its elements (because it
contains only the leaf of the path and all the arguments which are defended
by this leaf) and it attacks all the other arguments of the path. If we try
288

fiGraduality in argumentation

to include another argument of the path X  {Xi1 , . . . , Xili }\ Accepi , we
obtain a conflict (because all the other arguments of the path are attacked
by the elements of Accepi ). So, for {Xi1 , . . . , Xili }, Accepi is the only
preferred and stable extension.
Consider A0 = A \ {A}, with R0 being the restriction of R to A032 and
Union Accep= i Accepi , then Union Accep is the only preferred
and stable extension of <A0 , R0 >.
So, B  A, B 6= A, B is accepted iff B well-defended.

2. Now, consider A. If A is accepted then Union Accep {A} is the only
preferred and stable extension of <A, R>. So, i, Xili does not belong to
the extension. Then, i, Xili 1  Xili . Therefore, each branch leading to
A is a defence branch for A. So, i, v(Xili ) = [()(li  1)]. So, v(A) =
[(l1 , l2 , . . . , ln )()]. Then, i, v(A)  v(Xili ). Therefore, A is well-defended.
Using the following example, we show that the converse is false:
[(2,2)(3)] A

A1

[()(1)]

B1

C1
[(0...0)()]

[()(1)]

B2

[()(1)]

C2
[(0...0)()]

[(2)()]

B3

C3
[(0...0)()]

A is well-defended (A  B1 , A  B2 and A is incomparable with A1 ) but
not accepted.
3. Now, if A is well-defended and all the branches leading to A are defence
branches for A, then Union Accep {A} is conflict-free and A is defended
against each of its direct attackers (because Xili 1  Union Accep for each
branch i). So, Union Accep {A} is the preferred and stable extension
of <A, R> and A is accepted.


Proof
(of Lemma 1) Let <A, R> be an argumentation system with a finite relation
R without cycles (so, there is only one non empty preferred and stable extension
denoted by E). We know that:
if A is exi-accepted and if A has a direct attacker denoted by B then B is
not-accepted,
32. R0 is the restriction of R to A0 if and only if R0 = {(a, b)|aRb, a  A0 , b  A0 }.

289

fiCayrol, Lagasquie-Schiex

if B is not-accepted then there exists at least one argument C such that
CRB and C is exi-accepted (because B does not belong to E and E is
stable, so C must  E). So, a fortiori, if B is not-accepted and has only
one direct attacker C, then C will be exi-accepted.
The proof is done by induction on the depth of a proof tree for A or C.
Basic case for (i): A is exi-accepted with only one direct attacker B (BRA)
and C1 . . . Cn are the direct attackers of B; so, we have a proof tree whose
depth is 2 for A and one of the unattacked Ci , for example C1 ; so:
v(B) = g(h(v(C1 ), . . . , v(Cn )))
 g(v(C1 ))

because h(v(C1 ), . . . , v(Cn ))  h(v(C1 )) = v(C1 )

and g is non-increasing

 g(VMax )

because v(C1 ) = VMax

so:

v(A) = g(v(B))
 g 2 (VMax )
But, Property 1 says that g 2 (VMax )  g(VMax ), so v(A)  v(B).
Basic case for (ii): CRB with C the only direct attacker of B; so, we have
a proof tree whose depth is 0 for C, i.e. C is unattacked; so, v(C) = VMax
and v(B) = g(VMax )  v(C) (following Definition 6).
General case for (i): A is exi-accepted with only one direct attacker B
(BRA) and C1 . . . Cn are the direct attackers of B, with one of the Ci exiaccepted, for example C1 ; we consider the subgraph leading to C1 to which
we add C1 RBRA, and we assume:
g(v(C1 ))  v(C1 ) (induction assumption issued from (ii))
So:

v(B) = g(h(v(C1 ), . . . , v(Cn )))
 g(v(C1 ))
 v(C1 )

for the same reasons as in the basic case
by induction assumption

 h(v(C1 ), . . . , v(Cn ))

property of h

and with the non-increasing of g:

v(A) = g(v(B))
 g(h(v(C1 ), . . . , v(Cn ))) = v(B)
290

fiGraduality in argumentation

General case for (ii): B is not-accepted, so C is exi-accepted; we assume
that C has several direct attackers D1 . . . Dp which are all not-accepted
(because C is exi-accepted); we consider each subgraph leading to Di to
which we add Di RCRB and we assume:
i = 1 . . . p, g(v(Di ))  v(Di ) (induction assumption issued from (i))
so:
v(C) = g(h(v(D1 ), . . . , v(Dp )))
 h(v(D1 ), . . . , v(Dp ))

application of the condition ()
since the induction assumption
corresponds to the premise of ()

so:
v(B) = g(v(C))
 g(h(v(D1 ), . . . , v(Dp ))) = v(C)

Proof
(of Theorem 2) Assume that () is true and consider A  A which is exiaccepted. Let Bi , i = 1 . . . n, be the direct attackers of A. Then, for all i =
1 . . . n, in the subgraph leading to Bi and completed with Bi RA, we apply the
lemma and we obtain: g(v(Bi ))  v(Bi ), i = 1 . . . n. Thus, we have:
v(A) = g(h(v(B1 ), . . . , v(Bn )))
 h(v(B1 ), . . . , v(Bn ))

 v(Bi ), i = 1 . . . n

by applying ()
property of h

So, A is well-defended.
For the converse, let A  A be well-defended. Let B1 , . . . , Bn be the direct
attackers of A and assume that A is not exi-accepted. Then, there exists at
least one direct attacker Bi of A such that Bi is exi-accepted (because there is
only one preferred and stable extension). We can apply (ii) of the lemma on the
subgraph leading to Bi completed with Bi RA and we obtain g(v(Bi ))  v(Bi ).
So, there exists Bi a direct attacker of A such that:

v(A) = g(h(v(B1 ), . . . , v(Bn )))
 g(v(Bi ))

 v(Bi )

property of h and non-increasing of g
using the lemma
291

fiCayrol, Lagasquie-Schiex

This is in contradiction with A well-defended. So, A is exi-accepted.



Appendix B. Computation of tupled values
We propose an algorithm for computing the tupled values for an arbitrary graph (cyclic or
acyclic, the cycles may be isolated or not). This algorithm uses a principle of propagation
of values: an argument is evaluated when the values of its direct attackers are known.
We must consider the cycles as meta-arguments which are evaluated when all the direct
attackers of the cycle (i.e. the direct attackers of one of the elements of the cycle which
do not belong to the cycle) are evaluated.
The beginning of the process is as follows: we consider that all the arguments have the
initial value [0 , ()], and only the leaves of the graph are marked as having their final
values. Thus, we have the following partition of the graph G:
Gv : the part of the graph already evaluated (at the beginning, this part contains only
the leaves of the graph),
Gv : the part of the graph which is not evaluated (at the beginning, this part contains
all the arguments of the graph G except the leaves).
The algorithm also relies on a special data structure denoted by L giving the list of the
cycles in the graph and their main characteristics:
list of the arguments which belong to this cycle,
list of the arguments which belong to this cycle and which have direct attackers outside
the cycle (these arguments are called inputs of the cycle; those which will be used in
order to propagate the values across the cycle in the case of a non isolated cycle); this
list will be empty in the case of an isolated cycle.
Remark: For the sake of efficiency, the interconnected cycles (see Definition 1) will be
considered as a whole by the algorithm and will be used like a meta-cycle. For example,
the two cycles A  B  A and B  C  B which do not have any direct attacker outside
of the cycles, will be described in the data structure L as only one meta-cycle with the
following lists:
A, B, C,
nothing (because it is an isolated meta-cycle).
In order to avoid some ambiguity, these meta-cycles are defined as mcycles:
Definition 21 (mcycle) Let G be an attack graph. Let CC be the set of all the cycles of
G. Let CC 0  CC and CC 0 = {C1 , . . . , Cn } be a set of cycles.
Let ACC 0 be the set: {Aj such that Ci  CC 0 and Aj  Ci }.
If CC 0 satisfies the following properties:
Aj , Ak  ACC 0 ,  a path from Aj to Ak such that each element (arguments or edges
between arguments) of the path belongs to cycles of CC 0 ,
and Ck  CC \ CC 0 , 6 Ci  CC 0 such that Ck is interconnected with Ci .
292

fiGraduality in argumentation

Then the union of the Ci belonging to CC 0 is a mcycle.
Thus, we make a partition of CC using the notion of interconnection between cycles, each
element of the partition being a different mcycle. See on the following example:
A

J
C

B

E

F

G

I

K
D
L

In this graph, there are 6 cycles:
{J},
{I, J, K},
{K, L},
{B, C, D},
{C, E},
{F, G}.
and 3 mcycles:
{I, J, K, L},
{B, C, D, E},
{F, G}.
Algorithm 2 is the main algorithm used for computing the tupled values.
The function Add-Node (respectively Remove-Node) whose parameters are a subgraph
Gx of the attack graph and a node s, adds (resp. removes) s in (resp. of) Gx . The other
functions are described in (Cayrol & Lagasquie-Schiex, 2003a).
Algorithm 2 has been applied on an example after the step of rewriting (see Figure 1). Note
that in order to make the understanding of the results easier, we do not have created new
arguments (as in Definitions 11 and 12), but of course, it would be necessary for a rigorous
formalization.

293

fiCayrol, Lagasquie-Schiex

Algorithm 2: Algorithm for computing tupled values
% Description of parameters:
%
G: attack graph (partitioned in Gv and Gv )
%
L: data structure describing the mcycles
%
n: number of propagation steps for the mcycles
% Used variables:
%
A: the current argument (to be evaluated)
%
C: the current mcycle (to be evaluated) (containing A)
%
LAD: list of the direct attackers of C
%
Bi : the current direct attackers of A, or of C

begin
while there is at least one argument in Gv do
2
A = Choose-Argument(Gv )
3
if A does not belong to a mcycle C described in L then
4
if Bi  R (A), Bi is already evaluated then
5
Gv = Add-Node(Gv ,Evaluate-Node(A, R (A), 1))

%
%
%
%
%
%
%
%
%

1

Gv = Remove-Node(Gv , A)

6
7
8
9
10
11
12
13
14
15

% The value of A
% is the value of its
% direct attackers
% in which we add 1
% see Definition 10

else
if C is isolated then
Gv = Add-Mcycle(Gv ,Evaluate-Mcycle-Isolated(G, C, n))
Gv = Remove-Mcycle(Gv , C)

else
LAD = Find-Direct-Attackers-Mcycle(C, G)
if Bi  LAD, Bi is already evaluated then
Gv = Add-Mcycle(Gv ,
Evaluate-Mcycle-Not-Isolated(G, C, LAD,n))
Gv = Remove-Mcycle(Gv , C)

return G
end

16

294

%
%
%
%
%

fiGraduality in argumentation

A

C

B

D

E

G

F

The previous argumentation graph can be rewritten as follows:

A

A

A

A

B

B

B

C

A

C

A

C

D

B

B

D

B

B

E

C

C

E

C

C

C

D

D

A

A

A

B

B

A

B

C

A

C

B

A

C

D

B

B

C

B

B

E

A

C

C

C

C

C

B

B

B

....

....
B

C

....
D

....
E

G
F

The results of the valuation obtained after one propagation step are:
v(A) = [(0, . . . , 0)()],
v(B) = [(6, 8, 8, . . .)(1, 3, 5, . . .)],
v(C) = [(2, 4, 6, . . .)(5, . . .)],
v(D) = [(6, . . .)(3, 5, . . .)],
v(E) = [(4, 6, . . .)()],
v(F ) = [(8, . . .)(3, 5, 5, 7, 7, . . .)],
v(G) = [(2, 4, 6, . . .)(7, . . .)],
Figure 1: Example of rewriting

295

fiCayrol, Lagasquie-Schiex

References
Amgoud, L., & Cayrol, C. (1998). On the acceptability of arguments in preference-based
argumentation. In Cooper, G. F., & Moral, S. (Eds.), Proc. of the 14th Uncertainty
in Artificial Intelligence, pp. 17, Madison, Wisconsin. Morgan-Kaufmann.
Amgoud, L., & Cayrol, C. (2002). Inferring from inconsistency in preference-based argumentation frameworks. Journal of Automated Reasoning, 29, 125169.
Bench-Capon, T. J. (2002). Value based argumentation frameworks. In Benferhat, &
Giunchiglia (Eds.), Proc. of the 9th International Workshop on Nonmonotonic Reasoning (session on Argument, Dialogue and Decision), pp. 444453, Toulouse, France.
Besnard, P., & Hunter, A. (2001). A logic-based theory of deductive arguments. Artificial
Intelligence, 128 (1-2), 203235.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2003a). Critique et amelioration de levaluation
graduelle par tuples pour le traitement des circuits. Rapport de recherche 2003-13-R,
Institut de Recherche en Informatique de Toulouse (I.R.I.T.), France.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2003b). Gradual acceptability in argumentation
systems. In Proc. of the 3rd CMNA (International workshop on computational models
of natural argument), pp. 5558, Acapulco, Mexique.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2003c). Gradual handling of contradiction in argumentation frameworks. In Bouchon-Meunier, B., L.Foulloy, & Yager, R. (Eds.),
Intelligent Systems for Information Processing: From representation to Applications,
chap. Reasoning, pp. 179190. Elsevier.
Doutre, S. (2002). Autour de la semantique preferee des systemes dargumentation. These,
Universite Paul Sabatier, IRIT.
Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence,
77, 321357.
Dunne, P. E., & Bench-Capon, T. J. (2001). Coherence in finite argument systems. Technical
report 01-006, University of Liverpool, Department of Computer Science (U.L.C.S.).
Dunne, P. E., & Bench-Capon, T. J. (2002). Coherence in finite argument system. Artificial
Intelligence, 141 (1-2), 187203.
Elvang-Goransson, M., Fox, J., & Krause, P. (1993). Dialectic reasoning with inconsistent
information. In Heckerman, D., & Mamdani, A. (Eds.), Proc. of the 9th UAI, pp.
114121, Washington, DC. Morgan-Kaufmann.
Jakobovits, H., & Vermeir, D. (1999). Robust semantics for argumentation frameworks.
Journal of logic and computation, 9(2), 215261.
Karacapilidis, N., & Papadias, D. (2001). Computer supported argumentation and collaborative decision making: the hermes system. Information systems, 26 (4), 259277.
Kohlas, J., Haenni, R., & Berzati, D. (2000). Probabilistic argumentation systems and
abduction. In Proc. of the 8th International Workshop on Non-Monotonic Reasoning
- special session on Uncertainty Frameworks in Non-Monotonic Reasoning, pp. 391
398, Breckenridge, Colorado.
296

fiGraduality in argumentation

Krause, P., Ambler, S., Elvang, M., & Fox, J. (1995). A logic of argumentation for reasoning
under uncertainty. Computational Intelligence, 11 (1), 113131.
Lin, F., & Shoham, Y. (1989). Argument systems - a uniform basis for non-monotonic
reasoning. In Proc. of the first International Conference On Principles of Knowledge
Representation and Reasoning (KR), pp. 245255.
Parsons, S. (1997). Normative argumentation and qualitative probability. In Proc. of the
first International Joint Conference on Qualitative and quantitative practical reasoning, ECSQARU-FAPR, LNAI 1244, pp. 466480, Germany.
Pinkas, G., & Loui, R. P. (1992). Reasoning from inconsistency: A taxonomy of principles
for resolving conflict. In Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proc. of the 3rd
KR, pp. 709719, Cambridge, MA. Morgan-Kaufmann.
Pollock, J. L. (1992). How to reason defeasibly. Artificial Intelligence, 57, 142.
Pollock, J. L. (2001). Defeasible reasoning with variable degrees of justification. Artificial
Intelligence, 133, 233282.
Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming with defeasible priorities. Journal of Applied Non-Classical Logics, 7, 2575.
Simari, G., & Loui, R. (1992). A mathematical treatment of defeasible reasoning and its
implementation. Artificial Intelligence, 53, 125157.
Verheij, B. (2002). On the existence and multiplicity of extension in dialectical argumentation. In Benferhat, S., & Giunchiglia, E. (Eds.), Proceedings of the 9th International
Workshop on Non-Monotonic Reasoning (NMR2002), pp. 416425.
Vreeswijk, G. (1997). Abstract argumentation systems. Artificial Intelligence, 90, 225279.
Xuong, N. (1992). Mathematiques discretes et informatique. Masson.

297

fiJournal of Artificial Intelligence Research 23 (2005) 533-585

Submitted 4/04; published 5/05

Using Memory to Transform Search on the Planning Graph
Terry Zimmerman

WIZIM@CS.CMU.EDU

Robotics Institute, Carnegie Mellon University
Pittsburgh, PA 15213-3890

Subbarao Kambhampati

RAO@ASU.EDU

Department of Computer Science & Engineering
Arizona State University, Tempe AZ 85287-5406

Abstract
The Graphplan algorithm for generating optimal make-span plans containing parallel sets of actions remains one of the most effective ways to generate such plans. However, despite enhancements on a range of fronts, the approach is currently dominated in terms of speed, by state space
planners that employ distance-based heuristics to quickly generate serial plans. We report on a family of strategies that employ available memory to construct a search trace so as to learn from various
aspects of Graphplans iterative search episodes in order to expedite search in subsequent episodes.
The planning approaches can be partitioned into two classes according to the type and extent of
search experience captured in the trace. The planners using the more aggressive tracing method are
able to avoid much of Graphplans redundant search effort, while planners in the second class trade
off this aspect in favor of a much higher degree of freedom than Graphplan in traversing the space
of states generated during regression search on the planning graph. The tactic favored by the second approach, exploiting the search trace to transform the depth-first, IDA* nature of Graphplans
search into an iterative state space view, is shown to be the more powerful. We demonstrate that
distance-based, state space heuristics can be adapted to informed traversal of the search trace used
by the second class of planners and develop an augmentation targeted specifically at planning graph
search. Guided by such a heuristic, the step-optimal version of the planner in this class clearly
dominates even a highly enhanced version of Graphplan. By adopting beam search on the search
trace we then show that virtually optimal parallel plans can be generated at speeds quite competitive
with a modern heuristic state space planner.

1. Introduction
When Graphplan was introduced in 1995 (Blum & Furst, 1995) it became one of the fastest programs
for solving the benchmark planning problems of that time and, by most accounts, constituted a radically different approach to automated planning. Despite the recent dominance of heuristic state-search
planners over Graphplan-style planners, the Graphplan approach is still one of the most effective ways
to generate the so-called optimal parallel plans. State-space planners are drowned by the exponential
branching factors of the search space of parallel plans (the exponential branching is a result of the fact
that the planner needs to consider each subset of non-interfering actions). Over the 8 years since its
introduction, the Graphplan system has been enhanced on numerous fronts, ranging from planning
graph construction efficiencies that reduce both its size and build time by one or more orders of magnitude (Smith & Weld, 1998; Long & Fox, 1999), to search speedup techniques such as variable and
value ordering, dependency-directed backtracking, and explanation based learning (Kambhampati,
2000). In spite of these advances, Graphplan has ceded the lead in planning speed to a variety of heuristic-guided planners (Bonet & Geffner, 1999; Nguyen & Kambhampati, 2000; Gerevini & Serina,
2002). Notably, several of these exploit the planning graph for powerful state-space heuristics, while
 2005 AI Access Foundation. All rights reserved.

fiZIMMERMAN & KAMBHAMPATI

eschewing search on the graph itself. Nonetheless, the Graphplan approach remains perhaps the fastest in parallel planning mainly because of the way it combines an iterative deepening A* (IDA*,
Korf, 1985) search style with a highly efficient CSP-based incremental generation of applicable action
subsets.
We investigate here the use of available memory so as to surmount some of Graphplans major
drawbacks, such as redundant search effort and the need to exhaustively search a k-length planning
graph before proceeding to the k+1 length graph. At the same time we wish to retain attractive features of Graphplans IDA* search such as rapid generation of parallel action steps and the ability to
find step optimal plans. The approach we describe remains rooted in iterative search on the planning
graph but greatly expedites this search by building and maintaining a concise search trace.
Graphplan alternates between two phases; one in which a data structure called a planning graph
is incrementally extended, and a backward phase where the planning graph is searched to extract a
valid plan. After the first regression search phase the space explored in any given episode is closely
correlated with that conducted in the preceding episode. The strategy we pursue in this work is to employ an appropriately designed trace of the search conducted in episode n (which failed to find a solution) to identify and avoid those aspects of the search that are provably unchanged in episode n+1, and
focus effort on features that may have evolved. We have identified precisely which features are dynamic across Graphplan search episodes and construct search traces that capture and exploit these features to different degrees. Depending on its design a search trace may provide benefits such as 1)
avoidance of much of Graphplans redundant search effort, 2) learning from its iterative search experience so as to improve its heuristics and the constraints embodied in the planning graph, and 3)
realizing a much higher degree of freedom than Graphplan, in traversing the space of states generated during the regression search process. We will show that the third advantage is particularly key to
search trace effectiveness, as it allows the planner to focus its attention on the most promising areas of
the search space.
The issue of how much memory is the right amount to use to boost an algorithms performance
cuts across a range of computational approaches from search to the paging process in operating systems, and Internet browsing to database processing operations. In our investigation we explore several alternative search trace based methods that differ markedly in terms of memory demands. We
describe four of these approaches in this paper. Figure 1 depicts the pedigree of this family of search
trace-based planners, as well as the primary impetus leading to the evolution of each system from its
predecessor. The figure also suggests the relative degree to which each planner steps away from the
original IDA* search process underlying Graphplan. The two tracks correspond to two genres of
search trace that we have developed;


left track: The EGBG planners (Explanation Guided Backward search for Graphplan) employ a
more comprehensive search trace focused on minimizing redundant search.



right track: The PEGG planners (Pilot Explanation Guided Graphplan) use a more skeletal
trace, incurring more of Graphplans redundant search effort in exchange for reduced memory
demands and increased ability to exploit the state space view of the search space.

The EGBG planner (Zimmerman & Kambhampati, 1999) adopts a memory intensive structure for
the search trace as it seeks primarily to minimize redundant consistency-checking across Graphplans
search iterations. This proves to be effective in a range of smaller problems but memory constraints

534

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

XY

Search
Trace
Exploiting Graphplan
Symmetry & Redundancy

Exploiting the
state space view

EGBG
Leveraging CSP & memory efficiency

so-PEGG
me-EGBG
Trading off step-optimality for
speedup in all episodes

PEGG

Figure 1: Applying available memory to step away from the Graphplan search process;
A family of search trace-based planners
impede its ability to scale up. Noting that Graphplans search process can be viewed as a specialized
form of CSP search (Kambhampati, 2000), we explore some middle ground in terms of memory usage by augmenting EGBG with several methods known to be effective as speedup techniques for CSP
problems.
Our primary interest in these techniques, however, is the impact on memory reduction and we describe how they accomplish this above and beyond any search speedup benefit they afford. The implemented planner, me-EGBG, markedly outperforms EGBG in speed and capabilities, but a variety
of problems still lie beyond the planners reach due to memory constraints.
The search trace structure used by the PEGG track planners trades off minimization of redundant
search in exchange for a much smaller memory footprint. In addition to its greatly reduced memory
demands, the PEGG search trace structure can be exploited for its intrinsic state space view of what is
essentially Graphplans CSP-oriented search space. A significant speedup advantage of this approach
over Graphplan and the EGBG track planners derives from its ability to employ the distance-based
heuristics that power many of the current generation of state-space planners (Bonet & Geffner, 1999;
Nguyen & Kambhampati, 2000; Hoffman, 2001). We adapt these heuristics to the task of identifying
the most promising states to visit in the search trace and implement the approach first in the so-PEGG
planner (step-optimal PEGG, Zimmerman & Kambhampati, 2003). So-PEGG outperforms even a
highly enhanced version of Graphplan by up to two orders of magnitude in terms of speed, and does
so while maintaining the guarantee of finding a step-optimal plan.
Finally we explore adoption of a beam search approach in visiting the state space implicit in the
PEGG-style trace. Here we employ the distance-based heuristics extracted from the planning graph
itself, not only to direct the order in which search trace states are visited, but also to prune and restrict
that space to only the heuristically best set of states, according to a user-specified metric. We show
that the planning graph can be further leveraged to provide a measure of the likelihood that a previously generated regression state might spawn new search branches at a higher planning graph level.
535

fiZIMMERMAN & KAMBHAMPATI

We term this metric flux and employ it in an effective filter for states that can be skipped over even
though they might appear promising based on the distance-based heuristic. Implemented in the PEGG
system (Zimmerman & Kambhampati, 2003), this approach to exploiting a search trace produces a
two-fold benefit over our previous approaches; 1) further reduction in search trace memory demands
and 2) effective release from Graphplans exhaustive search of the planning graph in all search episodes. PEGG exhibits speedups ranging to more than 300x over the enhanced version of Graphplan
and is quite competitive with a recent state space planner using similar heuristics. In adopting beam
search PEGG necessarily sacrifices the guarantee of step-optimality but empirical evidence indicates
the secondary heuristics are remarkably effective in ensuring the make-span of solutions produced are
virtually at the optimal.
The fact that these systems successfully employ a search trace at all is noteworthy. In general, the
tactic of adopting a search trace for algorithms that explicitly generate node-states during iterative
search episodes, has been found to be infeasible due to memory demands that are exponential in the
depth of the solution. In Sections 2 and 3 we describe how tight integration of the search trace with
the planning graph permits the EGBG and PEGG planners to largely circumvent this issue. The
planning graph structure itself can be costly to construct, in terms of both memory and time; there are
well-known problems and even domains that are problematic for planners that employ it. (PostGraphplan planners that employ the planning graph for some purpose include STAN, Long & Fox,
1999, Blackbox, Kautz & Selman, 1999, IPP, Koehler et al., 1997, AltAlt, Nguyen & Kambhampati, 2000, LPG Gerevini & Serina, 2002). The planning systems described here share that
memory overhead of course, but interestingly, we have found that search trace memory demands for
the PEGG class of planners have not significantly limited the range of problems they can solve.
The remainder of the paper is organized as follows: Section 2 provides a brief overview of the
planning graph and Graphplans search process. The discussion of both its CSP nature and the manner in which the process can be viewed as IDA* search motivates the potential for employing available memory to accelerate solution extraction. Section 3 addresses the two primary challenges in attempting to build and use a search trace to advantage with Graphplan: 1) How can this be done within
reasonable memory constraints given Graphplans CSP-style search on the planning graph? and, 2)
Once the trace is available, how can it most effectively be used? This section briefly describes EGBG
(Zimmerman & Kambhampati, 1999), the first system to use such a search trace to guide Graphplans
search, and outlines the limitations of that method (Details of the algorithm are contained in Appendix
A.) Section 4 summarizes our investigations into a variety of memory reduction techniques and reports the impact of a combination of six of them on the performance of EGBG. The PEGG planners
are discussed in Section 5 and the performance of so-PEGG and PEGG (using beam search) are compared to an enhanced version of Graphplan, EGBG, and a modern, serial state-space planner. Section
6 contains a discussion of our findings and Section 7 compares this work to related research. Finally,
Section 8 wraps up with our conclusions.
2. Background & Motivation: Planning Graphs and the Nature of Direct
Graph Search
Here we outline the Graphplan algorithm and discuss traits suggesting that judicious use of additional
memory might greatly improve its performance. We touch on three related views of Graphplans
search; 1) as a form of CSP, 2) as IDA* search and, 3) its state space aspect.
536

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Actions
Level 1

Initial
State
W

The Planning Graph
Propositions
Propositions
Actions
Level 1
Level 2
Level 2
nop
nop

~W
W

nop

a1
Y

Actions
Level 3

~W

nop

~W

W

nop

W

a1

a1

nop

~Y
Y

nop

nop

nop

X
~Y
Y
Z
H

a2

nop

I

a3

a3

nop

H

nop

I

nop

~Y
Y

nop

H

Propositions
Level 3

a3

I

a4
a5

a5

nop

J

a5

J

nop

J

Domain Actions

Y

a1

IHW
HJ

a2

X

action descriptions:

Y

a3

Y ~W
IJ

a4

Z
Y

a5

J ~Y

action-ID [effects]
[preconditions]

Figure 2: Planning graph representation for three levels in the Alpha domain
2.1 Construction and Search on a Planning Graph
The Graphplan algorithm employs two interleaved phases  a forward phase, where a data structure
called a planning graph is incrementally extended, and a backward phase where the planning graph
is searched to extract a valid plan. The planning graph consists of two alternating structures, called
proposition lists and action lists. At the bottom of Figure 2 is depicted a simple domain we will refer
to as the Alpha domain and use for illustration in this study. The figure shows four action and proposition levels of the planning graph engendered by the simple initial state given the domain. We start
with the initial state as the zeroth level proposition list. Given a k-level planning graph, the extension
of the graph structure to level k+1 involves introducing all actions whose preconditions are present in
the kth level proposition list. In addition to the actions of the domain model, no operation actions are
introduced, one for each condition in the kth level proposition list (abbreviated as nop in this papers
figures, but also termed persists by others). A nop-C action has C as its precondition and C as its
effect. Given the kth level actions, the proposition list at level k+1 is constructed as just the union of
the effects of all the introduced actions. The planning graph maintains the dependency links between

537

fiZIMMERMAN & KAMBHAMPATI

the actions at level k+1, their preconditions in the level k proposition list, and their effects in the level
k+1 proposition list.
During planning graph construction binary "mutex'' constraints are computed and propagated. In
Figure 2, the arcs denote mutex relations between pairs of propositions and pairs of actions. The
propagation starts at level 1 by labeling as mutex all pairs of actions that are statically interfering with
each other (static mutex), that is their preconditions or effects are logically inconsistent. Mutexes
are then propagated from this level forward using two simple propagation rules. Two propositions at
level k are marked mutex if all actions at level k that support one proposition are mutex with all actions that support the second proposition. Two actions at level 2 are then mutex if they are statically
interfering or if a precondition of the first action is mutually exclusive with a precondition of the second . (We term the latter dynamic mutex, since this constraint may relax at a higher planning graph
level).1 The propositions themselves can also be either static mutex (one negates the other) or dynamic mutex (all actions supporting one proposition are mutex with all actions supporting the other).
To reduce Figure 2 clutter mutex arcs for propositions and their negations are omitted.
The search phase on a k-level planning graph involves checking to see if there is a sub-graph of the
planning graph that corresponds to a valid solution to the problem. Figure 3 depicts Graphplan search
in a manner similar to the CSP variable-value assignment process. Beginning with the propositions
corresponding to the goals at level k, we incrementally select a set of actions from the level k action
list that support all the goals, such that no two actions selected for supporting two different goals are
mutually exclusive (if they are, we backtrack and try to change the selection of actions). This is essentially a CSP problem where the goal propositions at each level are the variables, actions that establish
a proposition are the values, and the mutex conditions constitute constraints. The search proceeds in
depth-first fashion: Once all goals for a level are supported, we recursively call the same search process on the k-1 level planning graph, with the preconditions of the actions selected at level k as the
goals for the k-1 level search. The search succeeds when we reach level 0 (the initial state) and the
solution is extracted by unwinding the recursive goal assignment calls. This process can be viewed as
a system for solving Dynamic CSPs (DCSP) (Mittal & Falkenhainer, 1990; Kambhampati 2000),
wherein the standard CSP formalism is augmented with the concept of variables that do not appear
(a.k.a. get activated) until other variables are assigned.
During the interleaved planning graph extension and search phases, the graph may be extended to a
stasis condition, after which no further changes occur in actions, propositions, or mutex conditions. A
sufficient condition defining this level-off is a level where no new actions are introduced and no
existing mutex conditions between propositions go away. We will refer to all planning graph levels at
or above level-off as static levels. Note that although the graph becomes static at this point, finding a
solution may require many more episodes composed of adding identical static levels and conducting
regression search on the problem goals.
Like many fielded CSP solvers, Graphplan's search process benefits from a simple form of nogood learning. When a set of (sub)goals for a level k is determined to be unsolvable, they are memoized at that level in a hash table. Subsequently, when the backward search process later enters level k
with a set of subgoals they are first checked against the hash table, and if a match is found the search
1

The static mutex condition has also been called eternal mutex and the dynamic mutex termed conditional mutex (Smith
& Weld, 1998).

538

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

J nop

S

a3

S

a5

W

J

D

a5

S

a5

J

a3

YJ

Y

S
D

a3

W

Z nop

S

J

Y
J

a4

nop

I nop

H nop
Y

I a1
a5

J

I
a3

nop

S

J

nop

a2

X

a3

nop

S

Level 1

a1

X

Y
S

YH I J

a5

S

W

a4

Ha1 nop

a5

S

nop

nop

a1

I

nop

D
J

S

a1

nop

S

Y

I

a3

Z nop

WXYZ

D

H nop
Y

a1

D nop J
a5
S
nop

I nop

WYHIJ

Initial State

a5
S

Goal State

D nop

a1

a2

a3
S

a3

nop

H
a1

a5

Level 2

Level 3

YHI

Icon explanation:
Action a5 assigned to give
Assigned action is
D Assigned action is
J
S
goal J
static mutex with a
dynamic mutex with a
a5
previous assigned action
previous assigned action
a1
Goal I is already satisfied
Set of regressed subgoals
I
by a previously assigned action a1
to be satisfied at next lower level

Figure 3: CSP-style trace of Graphplans regression search on the Figure 2 planning graph
process backtracks. This constitutes one of three conditions for backtracking: the two others arise
from attempts to assign static mutex actions and dynamic mutex actions (See the Figure 3 legend).
We next discuss Graphplans search from a higher-level view that abstracts away its CSP nature.
2.2 Graphplan as State Space Search
From a more abstract perspective, Graphplan can be viewed as conducting regression state space
search from the problem goals to the initial state. In this view, the states that are generated and expanded are the subgoals that result when the CSP process for a given set of subgoals finds a consistent
set of actions satisfying the subgoals at that planning graph level (c.f. Kambhampati & Sanchez,
2000). In this view the state-generator function is effectively Graphplans CSP-style goal assignment routine that seeks a non-mutex set of actions for a given set of subgoals within a given planning
graph level. This view is depicted in Figure 4, where the top graph casts the CSP-style search trace of
539

fiZIMMERMAN & KAMBHAMPATI

1

Init
State

W
Y

2

Proposition Levels

6

2 valid sets of action
assignments to satisfy
goals WXYZ at level 7
Y
J a1

1

2

Init
State

W

7

W
Y a2 a4
H
Goal
I
W
X
Y
Z
Y
H a1, a2
I a
4

6

Proposition Levels

S11

S6

S5

S9

S8

7

S4
W
Y
H
I

S7

S10
S18

S12

8

S14

S13

S15

Y
J

Goal

W
X
Y
Z

Y
S19

S17

S16

Y
H
I

S20

S21
1

2

3
S23
S24

S22

W

8

S8

S27

S25

S7

W
Y
H
I

S14

S13

S26

S15

Y
J

Y
H
I

S29
S19

S17

9

S5

S11

S12

7

S4

S10
S18

S30

S6
S9

S28

Init
State

Y

Proposition Levels

S16
S21

Goal
W
X
Y
Z

S20

Figure 4: Graphplans regression search space: Three consecutive search episodes
Figure 3 as a high-level state-space search trace. The terms in each box depict the set of (positive)
subgoals that result from the action assignment process for the goals in the higher-level state to which
the box is linked.2
Recognizing the state-space aspect of Graphplans search helps in understanding its connection to
IDA* search. First noted and briefly discussed in (Bonet & Geffner, 1999), we highlight and expand
upon this relationship here. There are three correspondences between the algorithms:
1. Graphplans episodic search process in which all nodes generated in the previous episode are regenerated in the new episode (and possibly some new nodes), corresponds to IDA*s iterative
search. Here the Graphplan nodes are the states (sets of subgoals) that result when its regres2

Figure 4 facilitates discussion of the search trace in the next section, by conjuring up a hypothetical problem in which the
first search episode begins on level 7 of the planning graph instead of level 3, as in Figure 3.

540

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

sion search on a given plan graph level succeeds. From this perspective the node-generator
function is effectively Graphplans CSP-style goal assignment routine that seeks a non-mutex
set of actions for a given set of propositions within a given planning graph level.
2. From the state space view of Graphplans search (ala Figure 4), within a given search episode/iteration the algorithm conducts its search in the depth-first fashion of IDA*. This ensures
that the space requirements are linear in the depth of a solution node.
3. The upper bound that is iteratively deepened ala IDA* is the node-state heuristic f-value;
f = g + h. In this context h is the distance in terms of associated planning graph levels between a
state generated in Graphplans regression search and the initial state3 and g is the cost of reaching
the state from the goal state in terms of number of CSP epochs (i.e. the numerical difference between the highest graph level and the states level).
For our purposes, perhaps he most important observation is that the implicit f-value bound for a
given iteration is just the length of the planning graph associated with that iteration. That is, for any
node-state, its associated planning graph level determines both the distance to the initial state (h) and
the cost to reach it from the goal state (g), and the total must always equal the length of the plan graph.
This heuristic is clearly admissible; there can be no shorter distance to the goal because Graphplan
exhaustively searches all shorter length planning graphs in (any) previous iterations. It is this heuristic
implicit in the Graphplan algorithm which guarantees that a step-optimal solution is returned. Note
that from this perspective all nodes visited in a given Graphplan search iteration implicitly have the
same f-value: g + h = length of planning graph. We will consider implications of this property when
we address informed traversal of Graphplans search space in Section 5.
The primary shortcoming of a standard IDA* approach to search is that it regenerates so many of
the same nodes in each of its iterations. It has long been recognized that IDA*s difficulties in some
problem spaces can be traced to using too little memory (Russell, 1992; Sen & Bagchi, 1989). The
only information carried over from one iteration to the next is the upper bound on the f-value. Graphplan partially addresses this shortcoming with its memo caches that store no-goods -states found to
be inconsistent in successive episodes. However, the IDA* nature of its search can make it an inefficient planner for problems in which the goal propositions appear non-mutex in the planning graph
many levels before a valid plan can actually be extracted.
A second shortcoming of the IDA* nature of Graphplans search is that all node-states generated in
a given Graphplan episode have the same f-value (i.e. the length of the graph). As such, within an
iteration (search episode) there is no discernible preference for visiting one state over another. We
next discuss the use of available memory to target these shortcomings of Graphplans search.
3. Efficient Use of a Search Trace to Guide Planning Graph Search
The search space Graphplan explores in a given search episode is defined and constrained by three
factors: the problem goals, the plan graph associated with the episode, and the cache of memoized nogood states created in all previous search episodes. Typical of IDA* search there is considerable
3

Bonet & Geffner define hG (the Graphplan h-value) somewhat differently as the first level at which the goals of a
state appear non-mutex and have not been memoized. Our definition (which is not necessarily the first level at
which the Sm goals appear non-mutex) produces the most informed admissible estimate in all cases. This
guarantees that all states generated by Graphplan have an f-value equal to the planning graph length, which is
the property of primary interest to us.

541

fiZIMMERMAN & KAMBHAMPATI

similarity (i.e. redundancy) in the search space for successive episodes as the plan graph is extended.
In fact, as discussed below, the backward search conducted at any level k+1 of the graph is essentially
a replay of the search conducted at the previous level k with certain well-defined extensions. More
specifically, essentially every set of subgoals generated in the backward search of episode n, starting at
level k, will be regenerated by Graphplan during episode n+1 starting at level k+1 (unless a solution is
found first).4
Now returning to Figure 4 in its entirety, note that it depicts a state space tree structure corresponding to Graphplans search over three consecutive iterations. The top graph, as discussed above, represents the subgoal states generated in the course of Graphplans first attempt to satisfy the WXYZ
goal of a problem resembling our running example. (It is implied here that the W,X,Y,Z propositions
are present in the planning graph at level 7 and that this is the first level at which no pair of them is
mutex.) In the second search episode (the middle Figure 4 graph), the same states are generated again,
but each at one level higher. In addition, these states are expanded to generate a number of children,
shown in a darker shade. (Since Figure 4 is a hypothetical variation of the Alpha domain problem
detailed in Figures 2 and 3, all states created beyond the first episode are labeled only with state numbers representing the order in which they are generated.) Finally, in the third episode, Graphplan regenerates the states from the previous two episodes in attempting to satisfy WXYZ at level 9, and ultimately finds a solution (the assigned actions associated with the figures double outlined subgoal
sets) after generating the states shown with darkest shading in the bottom graph of Figure 4.
Noting the extent to which consecutive iterations of Graphplans search overlap, we investigate the
application of additional memory to store a trace of the explored search tree. The first implemented
approach, EGBG (which is summarized in the following subsection), seeks to leverage an appropriately designed search trace to avoid as much of the inter-episode redundant search effort as possible
(Zimmerman & Kambhampati, 1999).
3.1 Aggressive Use of Memory in Tracing Search: The EGBG Planner
Like other types of CSP-based algorithms, Graphplan consumes most of its computational effort on a
given problem in checking constraints. An instrumented version of the planner reveals that typically,
60 - 90% of the cpu run-time is spent in creating and checking action and proposition mutexes -both
during planning graph construction and the search process. (Mutex relations incorporated in the planning graph are the primary constraints in the CSP view of Graphplan, Kambhampati, 2000) As
such, this is an obvious starting point when seeking efficiency improvements for this planner and is
the primary tactic adopted by EGBG. We provide here only an overview of the approach, referring the
interested reader to Appendix A for details.
EGBG exploits four features of the planning graph and Graphplans search process:

4

The set of actions that can establish a given proposition at level k+1 is always a superset of
those establishing the proposition at level k.

Strictly speaking, this is not always the case due to the impact of Graphplans memoizing process. For some problems a
particular branch of the search tree generated in search episode n and rooted at planning graph level k may not be revisited in
episode n+1 at level k+1 due to a no-good proposition set memoized at level k+1. However, the memo merely acts to
avoid some redundant search and neglecting these relatively rare cases serves to simplify visualization of the symmetry
across Graphplans search episodes. .

542

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH



The constraints (mutexes) that are active at level k monotonically decrease with increasing
planning graph levels. That is, a mutex that is active at level k may or may not continue to be
active at level k+1 but once it becomes inactive it never gets re-activated at future levels.



Two actions in a level that are statically mutex (i.e. their effects or preconditions conflict with
each other) will be mutex at all succeeding levels.



The problem goal set that is to be satisfied at a level k is the same set that will be searched on at
level k+1 when the planning graph is extended. That is, once a subgoal set is present at level k
with no two propositions being mutex, it will remain so for all future levels.

Given an appropriate trace of the search conducted in episode n (which failed to find a solution) we
would like to ignore those aspects of the search that are provably unchanged in episode n+1, and focus effort on only features that may have evolved. If previous search failed to extract a solution from
the k-length planning graph, search on the k+1 length graph can succeed only if one or more of the
following conditions holds:
1. The dynamic mutex condition between some pair of actions whose concurrent assignment
was attempted in episode n no longer holds in episode n+1.
2. For a subgoal that was generated in the regression search of episode n at planning graph level
k, there is an action that establishes it in episode n+1 and first appears in level k+1.
3. An episode n regression state (subgoal set) at level k that matched a cached memo at that
level has no memo-match when it is generated at level k+1 in episode n+1.
(The discussion in Appendix A formalizes these conditions.) In each instance where one of these
conditions does not hold, a complete policy must resume backward search under the search parameters associated with the instance in the previous episode, n. Such resumed partial search episodes will
either find a solution or generate additional trace subgoal sets to augment the parent trace. This specialized search trace can be used to direct all future backward search episodes for this problem, and
can be viewed as an explanation for the failure of the search process in each episode. We hereafter use
the terms pilot explanation (PE) and search trace interchangeably. The following definitions are useful in describing the search process:
Search segment: This is essentially a state, specifically a set of planning graph level-specific subgoals
generated in regression search from the goal state (which is itself the first search segment). Each
EGBG search segment Sn , generated at planning graph level k contains:
 A subgoal set of propositions to be satisfied
 A pointer to the parent search segment (Sp ), (the state at level k+1 that gave rise to Sn)
 A list of the actions that were assigned in Sp which resulted in the subgoals of Sn
 A pointer to the PE level (as defined below) associated with the Sn
 A sequential list of results of the action consistency-checking process during the attempt to satisfy Sns subgoals. The possible trace results for a given consistency check are: static mutex,
dynamic mutex, or action is consistent with all other prior assigned actions. Trace results are
stored as a list of bit vectors for efficiency.
A search segment therefore represents a state plus some path information, but we often use search
segment and state interchangeably. As such, all the boxes in Figure 4 (whether the state goals are
explicitly shown or not) can be viewed as search segments.

543

fiZIMMERMAN & KAMBHAMPATI

Pilot explanation (PE): This is the search trace. It consists of the entire linked set of search segments
representing the search space visited in a Graphplan backward search episode. It is convenient to
visualize it as in Figure 4: a tiered structure with separate caches for segments associated with search
on each planning graph level. We adopt the convention of numbering the PE levels in the reverse
order of the plan graph: The top PE level is 0 (it contains a single search segment whose goals are the
problem goals) and the level number is incremented as we move towards the initial state. When a
solution is found, the PE will necessarily extend from the highest plan graph level to the initial state,
as shown in the third graph of Figure 4.
PE transposition: When a state is first generated in search episode n it is associated with a specific
planning graph level, say k. The premise of using the search trace to guide search in episode n+1 is
based on the idea of re-associating each PE search segment (state) generated (or updated) in episode n
with the next higher planning graph level. That is, we define transposing the PE as: For each search
segment in the PE associated with a planning graph level k after search episode n, associate it with
level k+1 for episode n+1.
Given these definitions, we note that the states in the PE after a search episode n on plan graph
level k, loosely constitute the minimal set 5 of states that will be visited when backward search is conducted in episode n+1 at level k+1. (This bound can be visualized by sliding the fixed tree of search
segments in the first graph of Figure 4 up one level.)
3.2 Conducting Search with the EGBG Search Trace
EGBG builds the initial pilot explanation during the first regression search episode while tracing the
search process with an augmented version of Graphplans assign-goals routine. If no solution is
possible on the k-length planning graph, the PE is transposed up one level, and key features of its previous search are replayed such that significant new search effort only occurs at points where one of the
three conditions described above holds. During any such new search process the PE is augmented according to the search space visited.
The EGBG search algorithm exploits its search trace in essentially bi-modal fashion: It alternates
informed selection of a state from the search trace of its previous experience with a focused CSP-type
search on the states subgoals. Our discussion here of EGBGs bi-modal algorithm revolves around
the second mode; minimizing redundant search effort once a state has been chosen for visitation.
When we describe PEGGs use of the search trace in Section 5 we will see that greater potential for
dramatic efficiency increases lies with the first mode; the selection of a promising state from the
search trace.
After choosing a state to visit, EGBG uses the trace from the previous episode to focus on only
those aspects of the entailed search that could possibly have changed. For each search segment Si at
planning graph level k+1, visitation is a 4step process:
1. Perform a memo check to ensure the subgoals of Si are valid at level k+1
2. Replay the previous episodes action assignment sequence for all subgoals in Si, using the
segments ordered trace vectors. Mutex checking is conducted on only those pairs of actions
that were dynamic mutex at level k. For actions that are no longer dynamic mutex, add the can5

It is possible for Graphplans memoizing process to preclude some states from being regenerated in a subsequent episode.
See footnote 2 for an brief explanation of conditions under which this may occur.

544

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

didate action to Sis list of consistent assignments and resume Graphplan-style search on the remaining goals. Si ,is augmented and the PE extended in the process. Whenever Sis goals are
successfully assigned, entailing a new set of subgoals to be satisfied at lower level k, a child
search segment is created, linked to Si , and added to the PE.
3. For each Si subgoal in the replay sequence, check also for new actions appearing at level k+1
that establish the subgoal. New actions that are inconsistent with a previously assigned action
are logged as such in Sis assignments. For new actions that do not conflict with those previously assigned, assign them and resume Graphplan-style search from that point as for step 2.
4. Memoize Sis goals at level k+1 if no solution is found via the search process of steps 2 and 3.
As long as all the segments in the PE are visited in this manner, the planner is guaranteed to find an
optimal plan in the same search episode as Graphplan. Hereafter we refer to a PE search segment that
is visited and extended via backward search to find a valid plan, as a seed segment. In addition, all
segments that are part of the plan extracted from the PE we call plan segments. Thus, in the third
graph of Figure 4, S18 is the apparent seed segment while the plan segments (in bottom up order) are;
S30, S29, S18, S17, S16, S15, labeled segments YH, YHI, and the goal state WXYZ.
In principle we have the freedom to traverse the search states encapsulated in the PE in any order
and are no longer restricted to the (non-informed) depth-first nature of Graphplans search process.
Unfortunately, EGBG incurs a high overhead associated with visiting the search segments in any order other than bottom up (in terms of PE levels). If an ancestor of any state represented in the PE
were to be visited before the state itself, EGBGs search process would regenerate the state and any of
its descendents (unless it first finds a solution). There is a non-trivial cost associated with generating
the assignment trace information in each of EGBGs search segments; its search advantage lies in reusing that trace data without having to regenerate it.
On the other hand, top-down visitation of the segments in the PE levels is the degenerate mode.
Such a search process essentially mimics Graphplans, since each episode begins with search on the
problem goal set, and (with the exception of the replay of the top-level search segments assignments)
regenerates all the states generated in the previous episode -plus possibly some new states- during its
regression search. The search trace provides no significant advantage under a top-down visitation
policy.
The bottom-up policy, on the other hand, has intuitive appeal since the lowest levels of the PE correspond to portions of the search space that lie closest to the initial state (in terms of plan steps). If a
state in one of the lower levels can in fact be extended to a solution, the planner avoids all the search
effort that Graphplan would expend in reaching the state from the top-level problem goals.
Adopting a bottom-up visitation policy amounts to layering a secondary heuristic on the primary
IDA* heuristic, which is the planning graph length that is iteratively deepened. Recalling from Section 2.2 that all states in the PE have the same f-value in terms of the primary heuristic, we are essentially biasing here in favor of states with low h-values. Support for such a policy comes from work on
heuristic guided state-space planning (Bonet & Geffner, 1999; Nguyen & Kambhampati, 2000) in
which weighting h by a factor of 5 relative to the g component of the heuristic f-value generally improved performance. However, unlike these state-space planning systems, for which this is the primary heuristic, EGBG employs it as a secondary heuristic so the guarantee of step optimality does not

545

fiZIMMERMAN & KAMBHAMPATI

Standard Graphplan

Speedup Ratios

EGBG

Problem
Time

Bktrks

Mutex
Chks

7919

2.7x

3.2x

5.5x

3,400 K

1020

10.0x

11.4x

22x

240 K

548 K

2722

24.5x

33x

42x

977 K

8901 K

6611

5.1x

6.0x

9.1x

Total
Time

Backtracks

Mutex
Checks

Total
Time

Backtracks

Mutex
Checks

Size of
PE

BW-Large-B
(18/18)

213

2823 K

121,400 K

79

880 K

21,900 K

Rocket-ext-a
(7/36)

402

8128 K

74,900 K

40

712 K

Tower-5
(31/31)

811

7907 K

23040 K

33

Ferry-6
(39/39)

319

5909 K

81000 K

62

Table 1: Comparison of EGBG with standard Graphplan.
Numbers in parentheses give number of time steps / number of actions respectively. Search backtracks and
mutex checks performed during the search are shown. "Size of PE" is pilot explanation size in terms of the
final number of search segments. Standard Graphplan is the Lisp version by Smith and Peot.

depend on its admissibility. We have found bottom-up visitation to be the most efficient mode for
EGBG and it is the default order for all EGBG results reported in this study.
3.3 EGBG Experimental Results
Table 1 shows some of the performance results reported for the first version of EGBG (Zimmerman &
Kambhampati, 1999). Amongst the search trace designs we tried, this version is the most memory
intensive and records the greatest extent of the search experience. Runtime, the number of search
backtracks, and the number of search mutex checks performed is compared to the Lisp implementation of the original Graphplan algorithm. EGBG exhibits a clear advantage over Graphplan for this
small set of problems;




total problem runtime: 2.7 - 24.5x improvement
Number of backtracks during search: 3.2 - 33x improvement
Number of mutex checking operations during search: 5.5 - 42x improvement

Since total time is, of course, highly dependent on both the machine as well as the coding language 6
(EGBG performance is particularly sensitive to available memory), the backtrack and mutex checking
metrics provide a better comparative measure of search efficiency. For Graphplan, mutex checking is
by far the biggest consumer of computation time and, as such, the latter metric is perhaps the most
complete indicator of search process improvements. Some of the problem-to-problem variation in
EGBGs effectiveness can be attributed to the static/dynamic mutex ratio characterizing Graphplans
action assignment routine. The more action assignments rejected due to pair-wise statically mutex
actions, the greater the advantage enjoyed by a system that doesnt need to retest them. Tower-ofHanoi problems fall into this classification.
As noted in the original study (Zimmerman & Kambhampati, 1999) the range of problems that can
6

All planners developed for this report were coded in Allegro Lisp and run on a Pentium 900 mhz, with 384 M RAM.
Runtimes include plangraph construction time and exclude garbage collection time. Values in Table 1 differ from those
published in 1999 because problems were re-run on this platform. They also reflect some changes in the tracking of statistics.

546

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

be handled by this implementation is significantly restricted by the amount of memory available to the
program at runtime. For example, with a PE consisting of almost 8,000 search segments, the very
modest sized BW-Large-B problem challenges the available memory limit on our test machine. We
consider next an approach (me-EGBG in Figure 1) that occupies a middle ground in terms of memory
demands amongst the search trace approaches we have investigated.
4. Engineering to Reduce EGBG Memory Requirements: The me-EGBG Planner
The memory demands associated with Graphplans search process itself are not a major concern, since
it conducts depth-first search with search space requirements linear in the depth of a solution node.
Since we seek to avoid the redundancy inherent in the IDA* episodes of Graphplans search by using
a search trace, we must deal with a much different memory-demand profile. The search trace design
employed by EGBG has memory requirements that are exponential in the depth of the solution. However, the search trace grows in direct proportion to the search space actually visited, so that techniques
which prune search also act to greatly reduce its memory demands.
We examined a variety of methods with respect to this issue, and eventually implemented a suite of
seven that together have proven instrumental in helping EGBG (and later, PEGG) overcome memorybound limitations. Six of these are known techniques from the planning and CSP fields: variable ordering, value ordering, explanation based learning (EBL), dependency directed backtracking (DDB),
domain preprocessing and invariant analysis, and transitioning to a bi-partite planning graph. Four of
the six most effective methods are CSP speedup techniques, however our interest lies primarily in
their impact on search trace memory demands. While there are challenging aspects to adapting these
methods to the planning graph and search trace context, it is not the focus of this paper. Thus details
on the motivation and implementation of these methods is relegated to Appendix B.
The seventh method, a novel variant of variable ordering we call EBL-based reordering, exploits the
fact that we are using EBL and have a search trace available. Although the method is readily implemented in PEGG, the strict ordering of the trace vectors required by the EGBG search trace make it
costly to implement for that planner. As such, memory-efficient EGBG (me-EGBG) does not use
EBL-based reordering and we defer further discussion until PEGG is introduced in Section 5.
4.1 Impact of Enhancements on EGBG Memory Demands
There are two major modes in which the first six techniques impact memory demand for me-EGBG:
1) Reduction in the size of the pilot explanation (search trace), either in the number of search segments
(states), or the average trace content within the segments, and 2) Reduction in the requirements of
structures that compete with the pilot explanation for available memory (i.e. the planning graph and
the memo caches). Admittedly, these two dimensions are not independent, since the number of
memos (though not the size) is linear in the number of search segments. We will nonetheless consider
this partition in our discussion to facilitate the comparison of each methods impact on the search
trace.
In general, the impact of each these enhancements on the search process depends significantly, not
only on the particular problem, but also on the presence (or absence) of any of the other methods. No
single configuration of techniques proves to be optimal across a wide range of problems. Indeed, due
to computational overhead associated with these methods, it is generally possible to find a class of
problems for which planner performance degrades due to the presence of the method. We chose this
547

fi90
DDB

40

50

60

70

80

All six in combination
( in me-EGBG )

30

EBL

20

Domain preprocess /
Invariant Analysis
Value Ordering

10

% reduction in PE (search trace) memory requirement

100

ZIMMERMAN & KAMBHAMPATI

Variable Ordering

10

20

30

Bi-partite graph

40

50

60

70

80

90

100

% reduction in planning graph, memo cache memory requirements

Figure 5: Memory demand impact along two dimensions for six memory reduction/speedup
techniques. Plots for each applied independently and as a suite (within EGBG).
set of techniques then, based on their joint average impact on the me-EGBG / PEGG memory footprint
over an extensive variety of problems.
Figure 5 illustrates for each method the impact on memory reduction relative to the two dimensions above, when the method operates in isolation of the others. The plot reflects results based on
twelve problems in three domains (logistics, blocksworld, and tower-of-hanoi), chosen to include a
mix of problems entailing large planning graphs, problems requiring extensive search, and problems
requiring both. The horizontal axis plots percent reduction in the end-of-run memory footprint of the
combined memo caches and the planning graph. The ratios along this ordinate are assessed based on
runs with Graphplan (no search trace employed) where the memo cache and planning graph are the
only globally defined structures of significant size that remain in the Lisp interpreted environment at
run completion.7 Similarly, the vertical axis plots percent reduction in the space required for the PE at
the end of EGBG runs with and without each method activated, and with the planning graph and
memo cache structures purged from working memory.
The plot crossbars for each method depict the spread of reduction values seen across the twelve
problems along both dimensions, with the intersection being the average. The bi-partite planning
graph, not surprisingly, impacts only the graph aspect, but five of the six methods are seen to have an
impact on both search trace size and graph/memo cache size. Of these, DDB has the greatest influence on PE size but little impact on the graph or memo cache size, while EBL has a more modest influence on the former and a larger impact on the latter (due both to the smaller memos that it creates
7

The Allegro Common Lisp global scavenging function was used to purge all but the target global data structures from
the workspace.

548

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

and the production of more general memos, which engender more backtracks). Domain preprocessing/ invariant analysis can have a major impact on both the graph size and the PE size due to processes
such as the extraction of invariants from operator preconditions. It is highly domain dependent, having little effect in the case of blocksworld problems, but can be of great consequence in tower-ofHanoi and some logistics problems.
That these six methods combined can complement each other is evidenced by the crossbars plotting space reduction when all six are employed at once. Over the twelve problems average reduction
in PE size approaches 90% and average reduction in the planning graph/memo cache aspect exceeds
80%. No single method in isolation averages more than a 55% reduction along these dimensions.
The runtime reduction associated with each of these methods in isolation is also highly dependent
on the problem and which of the other methods are active. In general, the relative time reduction for
any two methods does not correlate closely with their relative memory reduction. However, we found
that similarly, the techniques broadly complement each other such that net speedup accrues.
All of the techniques listed above can be (and have been) used to improve Graphplans performance also, in terms of speed. In order to focus on the impact of planning with the search trace, we use
a version of Graphplan that has been enhanced by these six methods for all comparisons to me-EGBG
and PEGG in this study (We hereafter refer to this enhanced version of Graphplan as GP-e).
4.2 Experimental Results with me-EGBG
Table 2 illustrates the impact of the six augmentations discussed in the previous section on EGBGs
(and Graphplans) performance, in terms of both space and runtime. Standard Graphplan, GP-e,
EGBG, and me-EGBG are compared across 37 benchmark problems in a wide range of domains, including problems from the first three AIPS planning competitions held to date. The problems were
selected to satisfy three objectives: a subset that both standard Graphplan and EGBG could solve for
comparison to me-EGBG, different subsets that exceed the memory limitations of each of the three
planners in terms of either planning graph or PE size, and a subset that gives a rough impression of
search time limitations.
Not surprisingly, the memory efficient EGBG clearly outperforms the early version on all problems attempted. More importantly, me-EGBG is able to solve a variety of problems beyond the reach
of both standard Graphplan and EGBG. Of the 37 problems, standard Graphplan solves 12, the original EGBG solves 14, GP-e solves 32, and me-EGBG solves 32. Wherever me-EGBG and GP-e solve
the same problem, me-EGBG is faster by up to a factor of 62x, and averages ~4x speedup. Standard
Graphplan (on the twelve problems it can solve), is bested by me-EGBG by factors ranging from 3x to
over 1000x.
The striking improvement of the memory efficient version of EGBG over the first version is not
simply due to the speedup associated with the five techniques discussed in the previous section, but is
directly tied to their impact on search trace memory requirements. Table 2 indicates one of three reasons for each instance where a problem is not solved by a planner: 1) s: planner is still in search after
30 cpu minutes, 2) pg: memory is exhausted or exceeded 30 minutes during the planning graph building phase, 3) pe: memory is exhausted during search due to pilot explanation extension. The third reason clearly favors me-EGBG as the size of the PE (reported in terms of search segments at the time
the problem is solved) indicates that it generates and retains in its trace up to 100x fewer states than
EGBG. This translates into a much broader reach for me-EGBG; it exhausts memory on 14% of the
549

fiZIMMERMAN & KAMBHAMPATI

Table 2 problems compared to 49% for the first version of EGBG. Regardless, GP-e solves three
problems on which me-EGBG fails in 30 minutes due to search trace memory demands
The table also illustrates the dramatic impact of the speedup techniques on Graphplan itself. The
enhanced version, GP-e, is well over 10x faster than the original version on problems they can both
solve in 30 minutes, and it can solve many problems entirely beyond standard Graphplans reach.
Nonetheless, me-EGBG modestly outperforms GP-e on the majority of problems that they both can
solve. Since the EGBG (and PEGG) planners derive their strength from using the PE to shortcut
Graphplans episodic search process, their advantage is realized only in problems with multiple search
episodes and a high fraction of runtime devoted to search. Thus, no speedup is seen for grid-y-1 and
all problems in the mystery, movie, and mprime domains where a solution can be extracted as
soon as the planning graph reaches a level containing the problem goals in a non-mutex state.
The bottom-up order in which EGBG visits PE search segments turns out to be surprisingly effective for many problems. For Table 2 problems we found that in the great majority the PE for the final
episode contains a seed segment (a state from which search will reach the initial state) within the
deepest two or three PE levels. This supports the intuition discussed in Section 3.2 and suggests that
the advantage of a low h-value bias as observed for heuristic state-space planners (Bonet & Geffner,
1999; Nguyen & Kambhampati, 2000) trans-lates to search on the planning graph.
Results for even the memory efficient version of EGBG reveal two primary weaknesses:
1. The action assignment trace vectors that allow EGBG to avoid redundant search are somewhat
costly to generate, make significant demands on available memory for problems that elicit large
search (e.g. Table 2 problems: log-y-4, 8puzzle-1, freecell-2-1), and are difficult to revise when
search experience alters drastically in subsequent visits.
2. Despite its surprising effectiveness in many problems, the bottom up visitation of PE search
segments is inefficient in others. For Table 2 problems such as freecell-2-1 and essentially all
schedule domain problems, when the planning graph gets extended to the level from which a
solution can be extracted, that solution arises via a new search branch generated from the root
search segment (i.e. the problem goal state). Thus, the only seed segment in the PE is the topmost search segment, and bottom-up visitation of the PE states is more costly than Graphplans
top-down approach.
The first shortcoming is particularly manifest in problems that do not allow EGBG to exploit the
PE (e.g. problems in which a solution can be extracted in the first search episode). The hit that EGBG
takes on such problems relative to Graphplan is closely tied to the overhead associated with building
its search trace. A compelling tactic to address the second shortcoming is to traverse the search space
implicit in the PE according to state space heuristics. We might wish, for example, to exploit any of
the variety of state-space heuristics that have revolutionized state space planners in recent years
(Bonet & Geffner, 1999; Nguyen & Kambhampati, 2000; Gerevini & Serina, 2002). However, as we
noted in Section 3.2, when we depart from a policy of visiting EGBG search segments in level-bylevel, bottom-up order, we face more costly bookkeeping and high memory management overhead.
More informed traversal of the state-space view of Graphplans search space is taken up next, where
we argue that its perhaps the key benefit afforded by a trace of search on the planning graph.

550

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Problem

(steps/actions)

bw-large-B (18/18)
huge-fct
(18/18)
rocket-ext-a (7/34)
att-log-a
(11/79)
gripper-8 (15/23)
Tower-6
(63/63)
Tower-7 (127/127)
8puzzle-1 (31/31)
8puzzle-2 (30/30)
TSP-12
(12/12)
AIPS 1998
grid-y-1 (14/14)
grid-y-2 (??/??)
gripper-x-3 (15/23)
gripper-x-4 (19/29)
gripper-x-5 (23/35)
log-y-4
(11/56)
mprime-x-29 (4/6)
movie-x-30 (2/7)
mysty-x-30 (6/14)
AIPS 2000
blocks-10-1 (32/32)
blocks-12-0 (34/34)
logistics-10-0 (15/56)
logistics-11-0 (13/56)
logistics-12-1 (15/77)
freecell-2-1 (6/10)
schedule-8-5 (4/14)
schedule-9-2 (5/13)
AIPS 2002
depot-6512 (10/26)
depot-7654a (10/28)
driverlog-2-3-6a (10/24)
driverlog-2-3-6e (12/28)
roverprob1425 (10/32)
roverprob1423 (9/30)
strips-sat-x-5 (7/22)
strips-sat-x-9 (6/35)
ztravel-3-8a (7/25)
ztravel-3-7a (10/21)

EGBG

Graphplan
cpu sec

Stnd.
126
165

GP-e

cpu sec

(enhanced)

11.4
13.0
s
3.5
s
12.2
125
14.2
s
43.1
s
158
667
57.1
304
48.3
s
454
Graphplan
GP-e
388
16.7
pg
pg
291
16.1
s
190
s
s
pg
470
15.7
5.5
.1
.05
83
13.5
Graphplan
GP-e
s
101
s
24.2
s
30.0
s
78.6
s
s
s
98.0
pg
63.5
pg
58.1
Graphplan
GP-e
239
5.1
s
32.5
1280
2.8
s
169
s
18.9
s
170
313
47.0
s
s
s
972
s
s

79
98
40.3
pe
88
39.1
s
pe
pe
pe
393
pg
200
pe
pe
pg
6.6
.06
85

size of
PE

7919
8410
1020
9790
3303

EGBG
19
9888

4
2
32
EGBG

pe
pe
s
pe
pe
pe
pg
pg
219
s
807
s
979
pe
272
s
pe
pe

EGBG
4272
1569
10028
4111

me-EGBG

SPEEDUP
(me-EGBG
vs. GP-e)

2090
2964
174
1115
2313
80
166
pe
>16000
26.9
10392
97.0
7155
me-EGBG
16.9
15
pg
8.4
2299
65.7
6351
433
13572
pe
>25000
5.5
4
.05
2
13.5
19
me-EGBG
16.1
6788
14.5
3220
16.3
1259
10.0
1117
1205
7101
pe
>12000
42.9
6
46.8
6
me-EGBG
4.1
456
14.8
1199
1.0
230
83.3
7691
10.3
1522
94.7
10217
23.0
2717
84.4
306
15.6
1353
>20000
pe

1.2x
1.4x
1.9x
1.7x
1.8x
5.7x
7.9x
(pe)
1.8x
4.7x
Speedup
1x
~
1.9x
2.9x
> 5x
(pe)
1x
1x
1x
Speedup
6.3x
1.7x
1.8x
7.9x
> 2x
(pe)
1.5x
1.2x
Speedup
1.25x
2.2x
2.8x
2x
1.8x
1.8x
2.0x
>21x
62x
~

(memory efficient EGBG)
cpu sec
size of
PE

9.2
9.1
1.8
7.2
7.9
7.6
20.0

Table 2: Search for step-optimal plans: EGBG, me-EGBG, standard & enhanced Graphplan
Standard Graphplan: Lisp version by Smith and Peot
GP-e: Graphplan enhanced per Section 4.1 me-EGBG: memory efficient EGBG
Size of PE is the final search trace size in terms of the number of "search segments"
Search failure modes: pg Exceeded 30 mins. or memory constraints during graph building
pe Exceeded memory limit during search due to size of PE
s Exceeded 30 mins. during search
Parentheses adjacent to cpu time give (# of steps / # of actions) in the solution.
551

fiZIMMERMAN & KAMBHAMPATI

5. Focusing on the State Space View: The so-PEGG and PEGG Planners
The costs associated with EGBGs generation and use of its search trace are directly attributable to the
storage, updating, and replay of the CSP value assignments for a search segments subgoals. We
therefore investigated a stripped down version of the search trace that abandons this tactic and focuses
instead on the embodied state space information. We will show that the PEGG planners employing
this search trace (both so-PEGG, the step-optimal version and PEGG, a version using beam search),
outperform the EGBG planners on larger problems. The key difference between EGBGs pilot explanation and the pared down, skeletal PE used by the PEGG planners, is the elimination of the detailed
mutex-checking information contained in the bit vectors of the former (i.e. the last item in the bullet
list of EGBG search segment contents in Section 3.1). The PEGG planners then apply state-space
heuristics to rank the PE search segments based on their associated subgoal sets (states) and are free to
visit this state space in a more informed manner. The tradeoff is that for each PE state so visited the
planner must regenerate the CSP effort of finding consistent action assignments for the subgoals.
Figure 6 illustrates the PEGG advantage in a small hypothetical search trace at the final search episode. Here search segments in the PE at the onset of the episode appear in solid lines and all plan
segments (states extendable to a valid plan) are shown as double-lined boxes. The figure reflects the
fact that typically there may be many such latent plan segments in diverse branches of the search trace
at the solution-bearing episode. Clearly a planner that can discriminate plan segment states from other
states in the PE could solve the problem more quickly than a planner restricted to a bottom-up traversal (deepest PE level first). State space heuristics endow the PEGG planners with this capability.
The so-PEGG planner visits every search segment in the PE during each search episode (comparable to Graphplans exhaustive search on a given length graph) thereby guaranteeing that returned
plans are step-optimal. As such, any advantage of heuristic-guided traversal is realized only in the
final episode. For many problems, the computational effort expended by Graphplan in the last search
episode greatly exceeds that of all previous episodes combined, so this can still be a powerful advantage. However, as we scale up to problems that are larger in terms of the number and size of search
episodes, the cost of exhaustive search in even the intermediate episodes becomes prohibitive. The

3
Init
State

Goal

1

W

W
X
Y
Z

2

Y

4
.
.

.
.

.
.

1

2

3

.
.

.
.
.

Proposition Levels

7

8

Figure 6: The PE for the final search episode of a hypothetical problem. Search segments in the PE
at onset of search appear in solid lines, double-lined boxes represent plan segments,
dashed lined boxes are states newly generated in regression search during the episode.
Visitation order as dictated by the secondary heuristic is shown via numbering.
552

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

planner we refer to simply as PEGG employs beam search, applying the search trace heuristics in all
intermediate search episodes to visit only a select subset of the PE segments. In so doing PEGG
trades off the step-optimality guarantee for often greatly reduced solution times.
There are several challenges that must be dealt with to effectively use the pared down search trace
employed by so-PEGG and PEGG, including adaptation and augmentation of distance-based heuristics to guide search trace traversal and dealing with memory management problems induced by the
tactic of skipping about the search space. Before we describe how we addressed such issues and
give a more complete description of the algorithm, we first present some results that provide perspective on the effectiveness of these planners.
5.1 Experimental Results With so-PEGG and PEGG
Table 3 compares Graphplan (standard and GP-e), me-EGBG, so-PEGG, and PEGG over most of the
same problems as Table 2, and adds a variety of larger problems that only the latter two systems can
handle. Table 2 problems that were easily solved for GP-e and me-EGBG (e.g. those in the AIPS-98
movie and mystery domains) are omitted from Table 3. Here, all planners that employ variable and
value ordering (i.e. all except standard Graphplan), are configured to use value ordering based on the
planning graph level at which an action first appears and goal ordering based on proposition distance
as determined by the adjusted-sum heuristic (which will be defined below). There are a variety of
other parameters for the so-PEGG and PEGG planners for which optimal configurations tend to be
problem-dependent. We defer discussion of these to Sections 5.3, 5.4, and 5.6 but note here that for
the Table 3 results the following parameter settings were used based on good performance on average
across a variety of domains and problems:
 Secondary heuristic for visiting states: adjusted-sum with w0=1 (eqn 5-1)
 Beam search: visit the best 20% (lowest f-value) search segments per search episode, with a
minimum of 25 and a maximum of 50. Search segments with flux lower than 10% of average
are not visited regardless of heuristic rank. (wcf = .01, see section 5.6.1)
Focusing first on the GP-e, me-EGBG, and so-PEGG columns, we clearly see the impact of the
tradeoff between storing and exploiting all the intra-segment action assignment information in the PE.
In this set of 37 problems, 16 result in me-EGBG exceeding available memory due to the size of the
PE while only one pushes that limit for so-PEGG. Seven of the problems that cause me-EGBG to run
out of memory are actually solved by so-PEGG while the remainder exceed the time limit during
search. In addition, so-PEGG handles five problems in the table that GP-e fails on. These problems
typically entail extensive search in the final episode, where the PE efficiently shortcuts the full-graph
search conducted by GP-e. The speedup advantage of so-PEGG relative to GP-e ranges between a
modest slowdown on three problems to almost 87x on the Zeno-Travel problems, with an average of
about 5x. (Note that the speedup values reported in the table are not for so-PEGG.)
Generally, any planner using a search trace will under perform GP-e on single search episode problems such as grid-y-1, in which the cost of building the trace is not recovered. The low overhead associated with building so-PEGGs search trace means it suffers little relative to GP-e in this case. On
most problems that both me-EGBG and so-PEGG can solve, me-EGBG has the upper hand due to its
ability to avoid redundant consistency-checking effort. The fact that me-EGBGs advantage over soPEGG is not greater for such problems is attributable both to so-PEGGs ability to move about the PE
search space in the final search episode (versus me-EGBGs bottom-up traversal) and its lower
553

fiZIMMERMAN & KAMBHAMPATI

Graphplan

Problem

cpu sec (steps/acts)

Stnd.
bw-large-B
bw-large-C
bw-large-D
att-log-a
att-log-b
Gripper-8
Gripper-15
Tower-7
Tower-9
8puzzle-1
8puzzle-2
TSP-12
AIPS 1998
grid-y-1
gripper-x-5
gripper-x-8
log-y-5
AIPS 2000
blocks-10-1
blocks-12-0
blocks-16-2
logistics-10-0
logistics-12-1
logistics-14-0
freecell-2-1
freecell-3-5
schedule-8-9
AIPS 2002
depot-7654a
depot-4321
depot-1212
driverlog-2-3-6e
driverlog-3-3-6b
roverprob1423
roverprob4135
roverprob8271
sat-x-5
sat-x-9
ztravel-3-8a
ztravel-3-7a

194.8
s
s
s
s
s
s
s
s
2444
1546
s
Stnd GP
388
s
s
pg
Stnd GP
s
~
s
~
s
s
pg
pg
pg
Stnd GP
s
s
s
s
s
s
s
s
313
s
s
s

GP-e

me-EGBG
cpu sec
(steps/acts)

(enhanced )

11.4 (18/18)
s (28/28)
s (38/38)
31.8 (11/79)
s
14.2 (15/23)
s
158 (127/127)
s (511/511)
57.1 (31/31)
48.3 (30/30)
454 (12/12)
GP-e
16.7 (14/14)
s
s
470 (16/41)
GP-e
95.4 (32/32)
26.6 (34/34)
s
30.0 (15/56)
s
s
98.0 (6/10)
1885 (7/16)
300 (5/12)
GP-e
32.5 (10/28)
s
s
166 (12/28)
s
170 (9/30)
s
s
45 (7/22)
s
972 (7/25)
s

9.2

so-PEGG
heur:istic:
adjsum

cpu sec
(steps/acts)

7.0
1104
pe
7.2
2.9 (11/72)
pe
s
7.9
30.6
pe
s
20.0
14.3
232
118
pe
31.1
26.9
31.3
97.0
390
me-EGBG
so-PEGG
17.9
16.8
433
512
pe
s
pe
361
me-EGBG
so-PEGG
16.1
18.7
14.5
23.0
pe
s
16.6
21
1205 (15/77) 1101 (15/75)
pe
s
pe
102
pe
511
615
719
me-EGBG
so-PEGG
14.8
12.9
s
s
s
s
83.3
109
pe
1437 (11/39)
pe
63.4
pe
s
pe
s
43.0
27.0
918
9.9
15.6
11.2
pe
s
pe
pe

PEGG

heur: adjsum-u

cpu sec
(steps/acts)

Speedup
(PEGG
vs. GP-e)

4.1
24.2
388
2.2
21.6
5.5
46.7
6.1
23.6
9.2
7.0
6.9

(18/18)
(28/28)
(38/38)
(11/62)
(13/64)
(15/23)
(31/45)
(127/127)
(511/511)
(31/31)
(32/32)
(12/12)
PEGG
16.8 (14/14)
110 (23/35)
520 (35/53)
30.5 (16/34)
PEGG
6.9 (32/32)
9.4 (34/34)
28.1 (56/56)
7.3 (15/53)
17.4 (15/75)
678 (13/74)
19.5 (6/10)
101 (7/17)
719 (5/12)
PEGG
13.2 (10/26)
42.6 (14/37)
79.1 (22/53)
80.6 (12/26)
169 (14/45)
15.0 (9/26)
379 (12 / 43)
220 (11 / 39)
25.1 (7 / 22)
9.9
(6 / 35)
15.1 (9/26)
101 (10/23)

2.8x
> 74x
> 4.6x
14.5x
> 83x
2.6x
> 38.5x
26x
> 76x
6.2x
6.9x
51x
Speedup
1x
> 16x
> 3.5x
15.4x
Speedup
13.8x
2.8x
> 64 x
4.1x
> 103x
> 2.7x
>92x
18.7x
(.42x)
Speedup
2.7x
>42x
>22.8x
2.1x
> 10.7x
11.3x
> 4.7x
> 8.2x
1.7x
>182x
119x
> 18x

Table 3: so-PEGG and PEGG comparison to Graphplan, GP-e, and me-EGBG
GP-e: Graphplan enhanced per Section 4.1 me-EGBG: memory efficient EGBG
so-PEGG: step-optimal, search via the PE, segments ordered by adjusted-sum-u heuristic
PEGG: beam search, best 20% of segments in PE ordered by adjusted-sum-u heuristic
Parentheses give (# of steps/ # of actions) in plan. Boldface values exceed a known
step-optimal.
See Table 2 for definitions of s, pg, and pe
554

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

overhead due to its more concise search trace. Note that there is no obvious reason to prefer one state
traversal order over the other in non solution-bearing episodes since these step-optimal planners visit
all the states in their PE for these search episodes. 8
Now turning attention to the PEGG results, its apparent that the beam search greatly extends the
size of problems that can be handled. PEGG solves ten larger problems of Table 3 that could not be
solved by either so-PEGG or enhanced Graphplan. Speed-wise PEGG handily outperforms the other
planners on every problem except schedule-8-9, where GP-e has a factor of 2.3x advantage. As indicated by the tables right-hand column, the speedup of PEGG over GP-e ranges from .42x to over
182x. This is a conservative bound on PEGGs maximum advantage relative to GP-e since speedup
values for the seventeen problems that GP-e fails to solve were conservatively assessed at the time
limit of 1800 seconds.
We defer further analysis of these results to Section 6 in order to first describe the PEGG algorithm
and the advantages it extracts from its search trace.
5.2 The Algorithm for the PEGG Planners
The high-level algorithm for so-PEGG and PEGG is given in Figure 7. As for Graphplan, search begins once the planning graph it has been extended to the level where all problem goals first appear
with no binary mutex conditions. (The routine, find_1st_level_with_goals is virtually the same as
Graphplans and is not defined here). The first search episode is then conducted in Graphplan fashion,
except that the assign_goals and assign_next_level_goals routines of Figure 8 initialize the PE as they
create search segments that hold all states generated during the regression search process. The assign_goals pseudo-code outlines the process of compiling conflict sets (see Appendix B) as a means
of implementing DDB and EBL during the action assignment search. The assign_next_level_goals
routine illustrates the role of the top-level conflict set for recording a minimal no-good when search on
a state is completed (EBL) and depicts how variable ordering need be done only once for a state
(when the search segment is created). A child segment is created and linked to its parent (extending
the PE) in assign_next_level_goals whenever all parent goals are successfully assigned. The assign_next_level_goals routine determines the subgoals for the child search segment by regressing the
parents goals over the actions assigned and then checks to see if either the initial state has been
reached or there are no remaining goals. If so, success is signaled by returning the child search segment which can then be used to extract the ordered actions in the plan.
Subsequent to the first episode, PEGG_plan enters an outer loop that employs the PE to conduct
successive search episodes. For each episode, the newly generated search segments from the previous
episode are evaluated according to a state space heuristic, ranked, and merged into the already ordered
PE. In an inner loop each search segment is visited in turn by passing its subgoals to the Graphplanlike assign_goals routine.
It is the exit conditions on the inner loop that primarily differentiate so-PEGG and PEGG.
Whereas so-PEGG will visit every search segment whose goals are not found to match a memo,
PEGG restricts visitation to a best subset, based on a user-specified criterion. As such, expansion of
the planning graph can be deferred until a segment is chosen for visitation that transposes to a planning graph level exceeding the current graph length. As a consequence, in some problems the PEGG
8

We have in fact found advantages with respect to traversal order even in intermediate search episodes for some problems. However, this is highly problem-dependent, and we do not consider it in this study.

555

fiZIMMERMAN & KAMBHAMPATI

PEGG_PLAN (Ops, Init, Goals) /*{ Ops, Init, Goals} constitutes a planning problem */
/* build plangraph, PG, until level n where goals first occur in non-mutex state*/

Let PG  find_1st_level_with_goals( Ops, Init, Goals )
if PG reached level-off and goals are not present in non-mutex state then Return FAIL
Let n be the number of levels in PG
Reorder Goals according to variable ordering method
Let SS0 be a new search segment with fields:
goalsGoals, parent root, PE-level 0, parent-actions {}
Let PE be the pilot explanation structure with fields: ranked-segs  {SS0}, new-segs {}

/* Conduct Graphplan-style backward search on the n-length planning graph, storing trace in PE..*/

Let search-reslt  assign_goals(Goals, {}, n, SS0, PG, PE)
if search-reslt is a search segment /* Success */
then Let Plan  extract plan actions from the ancestors linked to search-reslt
Return Plan
else /* no n-length solution possible ...use the PE to search for a longer length solution */
loop forever
n  n+1
/* rank newly generated states and merge into existing ordered PE segments list */
PE<ranked-segs>  merge sort(PE<ranked-segs> U heuristic_sort( PE<new-segs> ) )

loop while there are unvisited search segments in PE[ranked-segs] (optionally: for all segments
below the heuristic threshold)
Let SS be the highest ranked, unvisited segment from PEs ranked-segs
Let k = n  (PE-level of SS)
/*... the planning graph level for SS based on transposed PE */
if k = n then PG  extend_plangraph(PG) /*.. delays extending graph until unavoidable! */
optionally: if flux metric for SS goals < user-specified threshold then continue loop.
if SS goals  memos for level k of PG then remove SS from PEs ranked-segs
else /* visit search segment SS... */
search-reslt  assign_goals (SS<goals>, {}, k, SS, PG, PE)
if search-reslt is a search segment /* Success...*/
then Let Plan  extract plan actions from the ancestors linked to search-reslt
Return Plan
else search-reslt is a conflict set..
add conflict set to level k memos of PG /* memoize the minimal nogood */
reorder SS goals so that goals conflict set appear first /* EBL-based reordering */
end while
end-loop
end

Figure 7: Top-level algorithm for PEGG and so-PEGG planners.
planners may be able to extract a step-optimal solution while building one less level than other Graphplan-based planners.9

9

Interestingly, PEGG under beam search could conceivably extract an optimal solution from a planning graph that is an arbitrary number of levels shorter than that required by Graphplan. Consider the case where the PE, on average, extends at least
one level deeper in each episode and the subset of PE search segments visited always resides on the deepest levels of the PE.
Here an arbitrary number of search episodes might be completed without extending the planning graph. Based on experiments with problems to date however, this advantage seldom saves more than one planning graph level extension.

556

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Conduct DDB & EBL-enhanced Graphplan-style search while building search trace
arguments> G: goals still to be assigned, A: action set already assigned, k: PG level,
SS1: search segment, PG: planning graph, PE: pilot explanation (search trace)
ASSIGN_GOALS (G, A, k, SS1, PG, PE)
Let g be a goal selected from G
Let Ag be the actions from PG level k that support g, ordered by value-ordering heuristic
Let cs  {g} /* initialize a conflict set for DDB */
loop for act  Ag
Let search-reslt = {}
if act is mutex with an action in A
then Let b be the goal that the conflicted action in A was assigned to support
cs  cs U {b} /* augment the conflict set and continue to loop*/
else /* act has no conflict with actions already in A */
if G-{g} is not empty
then /* continue goal assignment at this level */
search-reslt  assign_goals (G-{g}, A U{act}, k, SS1, PG, PE)
else /* there are no SS1 goals left to satisfy...setup for search at the next lower level */
search-reslt  assign_next_level-goals (A U{act}, k, SS1, PG, PE)
if search-reslt is a conflict set, check if it contains current goal..
if g  search-reslt
then /* absorb returned conflict set & try next action */
cs  cs U search-reslt
else Return search-reslt /*just return this conflict set */
else search-reslt is a search segment: /* Success.. */
Return search-reslt
end loop (actions)
Return cs /* no soln reached .. compiled conflict set is returned*/
end-if
end
Set up search on graph level k-1 given that SS1 goals have been satisfied by actions in A at level k
ASSIGN_NEXT_LEVEL_GOALS (A, k, SS1, PG, PE)
Let nextgoals regress SS1 goals over A (the actions assigned to satisfy goals)
if nextgoals is empty or k = 0 (its the initial state)
then Return SS1 /* Success */
else if there is an M  memos at level k-1 of PG such that M  nextgoals
then Return M as the conflict set /* backtrack due to nogood */
else /*  initiate search on the next lower PG level*/
Let SS2 be a new search segment holding nextgoals, a pointer to SS1, & actions A assigned in SS1
Add SS2 to PE new-segs list
Let search-reslt  assign_goals (nextgoals, {}, k-1, SS2, PG, PE)
if search-reslt is a search segment /* Success.. */
then Return search-reslt
else search-reslt is a conflict set: /*memoize minimal nogood and return conflict set */
add conflict set to level k-1 memos of PG
reorder SS2 goals such that goals  conflict set appear first /* .. EBL-based reordering */
Return search-reslt
end-if
end

Figure 8: PEGG / so-PEGG regression search algorithm for Graphplan-style regression search
on subgoals while concurrently building the search trace (PE)
557

fiZIMMERMAN & KAMBHAMPATI

Note that PEGGs algorithm combines both state-space and CSP-based aspects in its search:
 It chooses for expansion the most promising state based on the previous search iteration and
state space heuristics. PEGG and so-PEGG are free to traverse the states in its search trace in
any order.
 A selected state is expanded in Graphplans CSP-style, depth-first fashion, making full use of
all CSP speedup techniques outlined above.
The first aspect most clearly distinguishes PEGG from EGBG: traversal of the state space in the PE is
no longer constrained to be bottom-up and level-by-level. As it was for EGBG, management of
memory associated with the search trace is a challenge for PEGG once we stray from bottom-up traversal, but it is less daunting. It will be easier to outline how we address this if we first discuss the
development and adaptation of heuristics to search trace traversal.
5.3 Informed Traversal of the Search Trace Space
The HSP and HSP-R state space planners (Bonet & Geffner, 1999) introduced the idea of using the
reachability of propositions and sets of propositions (states) to assess the difficulty degree of a relaxed version of a problem. This concept underlies their powerful distance based heuristics for selecting the most promising state to visit. Subsequent work demonstrated how the planning graph can
function as a rich source of such heuristics (Nguyen & Kambhampati, 2000). Since the planning
graph is already available to PEGG, we adapt and extend heuristics from the latter work to serve in a
secondary heuristic role to direct PEGGs traversal of its search trace states. Again, the primary heuristic is the planning graph length that is iteratively deepened (Section 2.2), so the step-optimality
guarantee for the so-PEGG planner does not depend on the admissibility of this secondary heuristic.
There are important differences between heuristic ranking of states generated by a state space planner and ordering of the search segments (states) in PEGGs search trace. For example, a state space
planner chooses to visit a given state only once while the PEGG planners often must consider whether
to revisit a state in many consecutive search episodes. Ideally, a heuristic to rank states in the search
trace should reflect level-by-level evolutions of the planning graph, since the transposition process
associates a search segment with a higher level in each successive episode. For each higher planning
graph level that a given state is associated with, the effective regression search space below it
changes as a complex function of the number of new actions that appear in the graph, the number of
dynamic mutexes that relax, and the no-goods in the memo caches. Moreover, unlike a state space
planners queue of previously unvisited states, the states in a search trace include all children of each
state generated when it was last visited. Ideally the value of visiting a state should be assessed independently of the value associated with any of its children, since they will be assessed in turn. Referring back to the search trace depicted in Figure 6, we desire a heuristic that can, for example, discriminate between the #4 ranked search segment and its ancestor, top goal segment (WXYZ). Here we
would like the heuristic assessment of segment WXYZ to discount the value associated with its children already present in the trace, so that it is ranked based only on its potential for generating new local search branches.
We next discuss adaptation of known planning graph based heuristics for the most effective use
with the search trace.

558

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

5.3.1 ADOPTION OF DISTANCE-BASED STATE SPACE HEURISTICS
The heuristic value for a state, S, generated in backward search from the problem goals can be expressed as:
5-1)

f ( S ) = g ( S ) + w0 * h( S )
where: g(S) is the distance from S to the problem goals (e.g. in terms of steps)
h(S) is a distance estimate from S to the initial state (e.g. in steps)
w0 is an optional weighting factor

The value of g for any state generated during the search (e.g. the states in the PE) is easily assessed as
the cumulative cost of the assigned actions up to that point. The h values we consider here are taken
from the distance heuristics adapted to exploit the planning graph by (Nguyen & Kambhampati,
2000). One heuristic that is readily extractable from the planning graph is based on the notion of the
level of a set of propositions:
Set Level heuristic: Given a set S of propositions, denote lev(S) as the index of the first level in the
leveled serial planning graph in which all propositions in S appear and are non-mutex with one another. (If S is a singleton, then lev(S) is just the index of the first level where the singleton element
occurs.) If no such level exists, then lev(S) = .
This admissible heuristic embodies a lower bound on the number of actions needed to achieve S from
the initial state and also captures some of the negative interactions between actions (due to the planning graph binary mutexes). In the Nguyen & Kambhampati, 2000 study, the set level heuristic was
found to be moderately effective for the backward state space (BSS) planner AltAlt, but tended to result in too many states having the same f-value. In directing search on PEGGs search trace it is
somewhat more effective, but still suffers from a lower level of discrimination than some of the other
heuristics they examined -especially for problems that engender a planning graph with relatively few
levels. Nonetheless, as noted in the Appendix B discussion of memory efficiency improvements we
use it during planning graph construction as the default heuristic for value ordering, due to both its low
computational cost and its synergy with building and using a bi-partite planning graph.
The inadmissible heuristics investigated in the Nguyen & Kambhampati, 2000 work are based on
computing the heuristic cost h(p) of a single proposition iteratively to fixed point as follows. Each
proposition p is assigned cost 0 if it is in the initial state and  otherwise. For each action, a, that adds
p, h(p) is updated as:
5-2) h(p) := min{ h(p), 1+h(Prec(a) }
where h(Prec(a)) is the sum of the h values for the preconditions of action a.
Given this estimate for a propositions h-value, a variety of heuristic estimates for a state have been
studied, including summing the h values for each subgoal and taking the maximum of the subgoal hvalues. For this study we will focus on a heuristic termed the adjusted-sum (Nguyen & Kambhampati, 2000), that combines the set-level heuristic measure with the sum of the h-values for a states
goals. Though not the most powerful heuristic tested by them, it is computationally cheap for a planning graph based planner and was found to be quite effective for the BSS planners they tested.
Adjusted-sum heuristic: Define lev(p) as the first level at which p appears in the plan graph and
lev(S) as the first level in the plan graph in which all propositions in state S appear and are nonmutexed with one another. The adjusted-sum heuristic may be stated as:
559

fiZIMMERMAN & KAMBHAMPATI

5-3)

hadjsum ( S ) :=

 h( p ) + ( lev(S )  max lev( p ) )
i

pi S

pi S

i

This is a 2-part heuristic; a summation, which is an estimate of the cost of achieving S under the assumption that its goals are independent, and an estimate of the cost incurred by negative interactions
amongst the actions that must be assigned to achieve the goals. The latter factor is estimated by taking
the difference between the planning graph level at which the propositions in S first become non-mutex
with each other and the level in which these propositions first appear together in the graph.
More complex heuristics have been proposed that include a measure of the positive interactions between subgoals in a state, that is, the extent to which an action establishes more than one relevant subgoal. The so-called relaxed plan distance-based heuristics focus on the positive interactions, and
several studies have demonstrated their power for backward and forward state-space planners
(Nguyen & Kambhampati, 2000; Hoffman, 2001). However, as reported in the former study, the primary advantage of adding positive interactions to the adjusted-sum heuristic is to produce shorter
make-span plans at the expense of a modest increase in planning time. Since PEGGs IDA* search
already ensures optimal make-span there is little incentive to incur the expense of the relaxed plan
calculation, and we restricted our work here to the simpler adjusted-sum heuristic of eqn 5-3.
The adjusted-sum heuristic can be further adapted to search on the planning graph by leveraging the
information in PEGGs search trace. This takes the form of heuristic updating to dynamically improve the h value estimate of states in the PE. The lev(S) term in the adjusted-sum heuristic represents
the first planning graph level at which the subgoals in state S appear and are binary non-mutex with
each other. However, once regression search on S at graph level k fails in a given episode, the search
process has essentially discovered an n-ary mutex condition between some subset of the goals in S at
level k (This subset is the conflict set, C, that gets memoized in the PEGG algorithm of Figures 7 and
8). At this point the lev(S) value can be updated to k+1, indicating that k+1 is a conservative estimate
of the first level that the S goals appear in n-ary non-mutex state. This has a desirable property for
ranking search trace states; the longer a state resides in the search trace, the more often its h-value gets
increased, and the less appealing it becomes as a candidate to visit again. That is, this heuristic update
biases against states that have been visited the most and failed to extend to a solution. We use this
augmented adjusted-sum heuristic for the PEGG runs in this work and refer to it as adjusted-sum-u.
Experimentally, we find that the advantage of any given heuristic for ordering PE states is highly
domain dependent (but less sensitive to a particular domain problem). For example, compared to a
simple bottom-up visitation strategy, the adjusted-sum-u heuristic improves so-PEGG runtimes by up
to an order of magnitude in some domains (e.g. Freecell and, Satellite) while degrading it by up to a
factor of 2x to 7x in others (e.g. Zenotravel). Figure 9 depicts the performance of the adjusted-sum-u
heuristic relative to a bottom-up heuristic in so-PEGG on several sets of problems. Here the heuristics are compared in terms of so-PEGGs average computation time as a percentage of GP-es in the
final search episode -the most important measure for exhaustive search on the planning graph. The
more informed heuristic will not only find a seed segment sooner but, in the event there are many
(typical of logistics domains), it will find one that lies on a planning graph level that is closer to the
initial state. A less informed heuristic may cause PEGG to end up conducting more search in its final
episode than GP-e, as there may be many states in the PE that would not be regenerated by Graphplan
in its final regression search before it finds a solution. This is a direct measure of the power of the

560

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

bottom-up

adjusted-sum-u

Depot
Schedule

Problem Set

ZenoTravel
Freecell
Driverlog
Satellite
Logistics
Blocks
TSP
0.00

20.00

40.00

60.00

80.00 100.00 120.00 140.00

% of Graphplan's search cost in final episode

Figure 9: Heuristic accuracy in so-PEGG for the final search episode --relative to GP-e
search segment selection heuristic. Since performance can vary considerably with the specific problem the results in the figure are averages for three representative examples in each domain10.
5.4 Memory Management Under Arbitrary Search Trace Traversal Order

We return now to the memory management problems induced by the strategy of skipping about the
search space. Consider again the PE at the time of the final search episode in Figure 6. If search segments are visited in an order other than deepest PE level first, we encounter the problem of regenerating states that are already contained in the PE. The visitation order depicted by numbered segments in
the figure could result from a fairly informed heuristic (the 4th segment it chooses to visit is a plan
segment), but it implies that many states already resident in the PE will be regenerated. This includes,
for example, all the as yet unvisited descendents of the third segment visited. Unchecked, this process
can significantly inflate search trace memory demands as well as the overhead associated with regenerating search segments. In addition, heuristic information about a state is lost when the state is regenerated instead of revisited as an extant PE search segment. This is because due to the adjustedsum-u secondary heuristic PEGG learns an improved n-ary mutex level for a search segments goals
and updates its f-value accordingly in each search episode.
We address this issue by hashing every search segment generated into an associated PE state hash
table according to its canonically ordered goal set. One such hash table is built for each PE level.
Prior to initiating regression search on a subgoal set of a search segment, Sn , PEGG first checks the
planning graph memo caches and, if no relevant memo is found, it then checks the PE state hash table
to see if Sns goals are already embodied in an existing PE search segment at the relevant PE level. If
10

Problem sets used- Blocksworld: bw-large-b, blocks-10-1 and 12-0 Logistics: att-log-a, logistics-10-0, 12-0, Gripper:
gripper8, gripper-x-3, x-5, Depot: depotprob6512, -5646, 6587, Driverlog: dlog-3-36a, -2-3-6a, 2-3-6e, Zenotravel:
ztravel-3-8a, -3-8b, 3-7b, Freecell: freecell-2-1, -2-2, -3-5, Satellite: strips-sat-x-4, x-5, x-9.

561

fiZIMMERMAN & KAMBHAMPATI

such a search segment, Se,, is returned by this PE state check, Se is made a child of Sn (if it is not already) by establishing a link, and search can then proceed on the Se goals.11
Another search trace memory management issue is associated with the fact that PEGG only visits a
subset of the PE states -a set we will call the active PE. It is tempting to pursue a minimal memory
footprint strategy by retaining in memory only the active search segments in the PE. However unlike
Graphplan, when the initial state is reached PEGG cannot extract a solution by unwinding the complete sequence of action assignment calls since it may have begun this regression search from an arbitrary state in any branch of the search trace tree. PEGG depends instead on the link between a child
search segment and its parent to extract the plan actions once a solution is found. As such we must
retain as a minimum, the active search segments and all of their ancestor segments up to the root node.
Beyond the requirement to retain search segments tied to the active PE, there are many strategies
that might be used in managing the inactive portion. For this study we have not attempted to reduce
PE memory requirements in this manner, instead focusing on what might be termed the search space
field of view. Under beam search, heuristic effectiveness depends not only on how informed it is,
but the search trace states available for it to rank. The reduced memory footprint of PEGGs skeletal
search trace allows us to adopt a strategy of retaining in memory all search segments generated. All
such segments have their f-values updated before they are ranked, giving the beam search a wide selection of states contending for active status in a given search episode.
5.5 Learning to Order a States Subgoals

The PEGG planners employ both EBL and a search trace, and this allows them to overlay a yet more
sophisticated version of variable ordering on top of the distance-based ordering heuristic. The guiding
principle of variable ordering in search is to fail early, when failure is inevitable. In terms of Graphplan-style search on a regressed state, this translates as Since all state goals must be assigned an action, its best to attempt to satisfy the most difficult goals first. The adjusted-sum heuristic described
above, applied to a single goal, provides an estimate of this difficulty based on the structure of the
planning graph. However, EBL provides additional information on the difficulty of goal achievement
based directly on search experience. To wit, the conflict set that is returned by the PEGGs assign_goals routine during search on a goal set explicitly identifies which of these goals were responsible for the search failure. The intuition behind the EBL-based reordering technique then, is that these
same goals are likely to be the most difficult to assign when the search segment is revisited in the next
search episode. This constitutes a dynamic form of variable ordering in that, unlike the distance-based
ordering, a search segments goals may be reordered over successive search episodes based on the
most recent search experience.
Figure 10 compares the influence of adjusted sum variable ordering and EBL-based reordering
methods on memory demand, in a manner similar to Figure 5. Here the impact of EBL-based reordering on EGBGs performance is reported because PEGG tightly integrates the various CSP and efficiency methods, and their independent influence cannot be readily assessed.12 We isolate the impact
of EBL-based reordering from that of EBL itself by activating the EBL but using the produced con11
12

In the interests of simplicity, the Figure 8 algorithm does not outline this memory management process.
Given the success of the various memory-efficiency methods within EGBG, all versions of PEGG implement them by
default. A graph analogous to Figure 5 for the PEGG planner would differ in terms of the actual memory reduction values, but we are confident that the overall benefits of the methods would persist, as would the relative benefit relationship
between methods.

562

fi30
20

Variable Ordering (adjsum) & EBLReordering
Variable ordering
EBL- Reordering
10

% reduction in PE (search trace) memory requirement

USING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

10
20
30
% reduction in planning graph, memo cache memory requirements

Figure 10: Memory demand impact along two dimensions for adjusted-sum
variable ordering and EBL-based reordering techniques when applied
independently and together.
flict sets only in reordering, not during memoization. The average reduction in search trace memory
over the 12-problem sample is seen to be about 18% for EBL-based reordering alone. This compares
favorably with the 22% average reduction of the distance-based ordering, especially since, unlike the
adjusted sum ordering, the EBL-based reordering only takes effect in the 2nd search episode. The plot
also reveals that the two modes of ordering are quite complimentary.
Across a variety of problems and domains we found the following approach to be most effective in
combining distance-based variable ordering and EBL-based reordering: 1) a newly created search
segments goals are ordered according to the distance-based heuristic. 2) After each visit of the search
segment, the subset of its goals that appear in the conflict set are reordered to appear first. 3) The goals
in the conflict set are then ordered by the distance-based heuristic and appended to non-conflict goals,
which are also set in distance-based order.
As indicated in Figure 10, this hybrid form of variable ordering not only boosts the average memory reduction to almost 30%, but also significantly reduces the wide fluctuation in performance of
either method in isolation. We re-emphasize here that this search experience-informed goal ordering
is only available to a search algorithm that maintains a memory of states it has visited. It is therefore
not portable to any Graphplan-based planner we know of.
5.6 Trading Off Guaranteed Step-Optimality for Speed and Reach:
PEGG Under Beam Search

Many of the more difficult benchmark problems for Graphplans IDA* style search have 20 or more
such search episodes before the reaching the episode in which a solution can be extracted. The cumulative search time tied to these episodes can be a large portion of the total search time and, as indicated
in Table 3, so-PEGG exhausts search time limits well before reaching the episode in which a solution
can be extracted. The strategy of exhaustively searching the planning graph, in each episode up to the
solution bearing level, gives the step-optimal guarantee to Graphplans solutions but it can exact a
563

fiZIMMERMAN & KAMBHAMPATI

high cost to ensure what is, after all, only one aspect of plan quality. We explore with PEGG, a nonexhaustive search version of so-PEGG, the extent to which search episodes can be truncated while
producing plans with virtually the same makespan as Graphplans solution.
PEGG shortcuts the time spent in search during these intermediate episodes by using the secondary
heuristic to not only direct the order in which PE states are visited but to prune the search space visited
in the episode. This beam search seeks to visit only the most promising PE states, as measured by
their f-values against a user-specified limit. In addition, the beam search has an important dual benefit
for PEGG in that it further reduces the memory demands of its search trace and, depending on the
problem, even the planning graph. In the PEGG algorithm of Figure 8, the loop while statement is
the point at which a beam search f-value threshold can be optionally applied to PE states that are candidates for visitation. When the first segment exceeding this threshold is reached on the sorted queue
the search episode ends.
To devise an effective threshold test must reconcile competing goals: minimizing search in nonsolution bearing episodes while maximizing the likelihood that the PE retains and visits (preferably as
early as possible), a search segment thats extendable to a solution once the graph reaches the first
level with an extant solution. The narrower the window of states to be visited, the more difficult it is
for the heuristic that ranks states to ensure it includes a plan segment, i.e. one that is part of a stepoptimal plan. PEGG will return a step-optimal plan as long as its search strategy leads it to visit any
plan segment (including the top, root segment in the PE) belonging to any plan latent in the PE, during search on the first solution-bearing planning graph. The heuristics job in selecting the window of
search segments to visit is made less daunting for many problems because there are many step-optimal
plans latent at the solution-bearing level.
We next describe an effective planning graph based metric that augments the state space heuristic
in choosing the set of PE states to visit in each search episode.
5.6.1 MINING THE PLANNING GRAPH TO FILTER THE BEAM
Beyond the heuristic updating we introduced in Section 3, the distance-based heuristics are virtually
insensitive to planning graph evolution as a search segment is transposed up successive levels. Since
the search trace contains all children states that were generated in regression search on a state S in episode n, a heuristic preference to include S over other states in the trace to visit in episode n+1 should
reflect the chance that it will directly generate new and promising search branches. The child states of
S from search episode n are competitors with S, so ideally a heuristics rank for S should reflect in
some sense the value of visiting the state beyond the importance of its children.
Consider now the sensitivity of the adjusted-sum heuristic (or any of the distance-based heuristics)
to possible differences in the implicit regression search space below a set of propositions, S, at planning graph level k versus level k+1. Given that the propositions are present and binary non-mutex
with each other at level k, only the cost summation factor in equation 5-3 could conceivably change
when S is evaluated at level k+1. This would require two conditions: a new action must establish one
of the S propositions for the first time at level k+1 and the actions precondition costs must sum to less
than the precondition costs of any other establisher of the proposition. In practice this happens infrequently since the later that an action appears in the graph construction process, the higher its cost tends
to be. Consequently h-values for states based on distance-based heuristics remain remarkably constant for planning graph levels beyond that at which the propositions appear and are binary non564

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

mutex13. We desire a means of compensating for a static h-value when a state is transposed to a planning graph level at which promising new branches of regression search open up.
The likelihood that a state visited in episode n at graph level k will give rise to new child states if
visited in episode n+1 at level k+1 is rooted in the graph dynamics summarized by Observations A-1
and A-2 of Appendix A. Three planning graph and memo cache properties determine whether regression search on a subgoal set S will evolve over successive episodes:
1. There are new actions at level k+1 that establish a subgoal of S
2. There are dynamic mutexes at level k between actions establishing subgoals of S that relax
at level k+1
3. There were no-good memos encountered in regression search on state S during episode n
that will not be encountered at level k+1 (and also the converse).
This set of measures of the potential for new search branches to result from visiting a state in the PE
we refer to as the flux; the intuition being that the higher the flux, the more likely that search on a
given state will differ from that seen in the previous search episode (and captured in the PE). If none
of the three factors applies to a state under consideration, there is no point in visiting it, as no new
search can result relative to the previous episode.
The first factor above can be readily assessed for a state (thanks in part to the bi-partite graph structure). The second flux factor is unfortunately expensive to assess; a direct measure requires storing all
pairs of attempted action assignments for the goals of S that were inconsistent in episode n and retesting them at the new planning graph level. However, the graph mechanics are such that relaxation of a
dynamic mutex between two actions at level k requires the relaxation of a dynamic mutex condition
between some pair of their preconditions at level k-1 (one precondition for each action). This relaxation, in turn, is either due to one or more new establishing actions for the preconditions at level k-1 or
recursively, relaxations in existing actions establishing the preconditions. As such, the number of new
actions establishing the subgoals of a state S in the PE (factor 1 above) provide not only a measure of
the flux for S, but also a predictor of the flux due to factor 2 for the parent (and higher ancestors) of S.
Thus, it turns out that by simply tracking the number of new actions for each state subgoal at its current level and propagating an appropriately weighted measure up to its parent, we can compile a useful
estimate of flux for factors 1 and 2 above.
The third flux factor above is the most unwieldy and costly to estimate; an exact measure requires
storing all child states of S generated in regression search at level k that caused backtracking due to
cached memos, and retesting them to see if the same memos are present at level k+1.14 Ignoring this
factor, we sum just the two flux measures that depend on new actions to derive a filtering metric
which is then used to assist the largely static adjusted-sum distance-based heuristic in culling the
beam. The resulting (inexact) metric is sensitive to the evolution in search potential as a state is transposed to higher planning graph levels:

13

This, in part, explains the observation (Nguyen & Kambhampati, 2000) that the AltAlt state space planner performance
generally degrades very little if the planning graph used to extract heuristic values is built only to the level where the problem goals appear and are non-mutex, rather than extending to level-off.
14
Note that as long as we are using EBL/DDB, it is not sufficient to just test whether some memo exists for each child state.
This is because no-good goals themselves contribute to the conflict set used to direct search within S whenever such backtracking occurs.

565

fiZIMMERMAN & KAMBHAMPATI

5-4)

flux( S ) =

 newacts( p )
i

pi  S

|S|

+ wcf 

 childflux(s )

si  S c

i

where: pi is a proposition in state S
newacts(pi) is the number of new actions that establish proposition pi of S at its
associated planning graph level
| S | is a normalization factor; the number of propositions in S
Sc is the set of all child states of S currently represented in the search trace
childflux(si) is the sum of the two flux terms in eqn 5-4 applied to child state si of S
wcf weights the contribution of flux from child states to the parent state

Here the number of new actions establishing the subgoals of a state is normalized relative to the number of subgoals in the state.
We report elsewhere (Zimmerman, 2003) on the use of this flux to directly augment the secondary
heuristic. Depending on the domain and the weighting of flux contribution to the adjusted-sum heuristic, speedups of up to an order of magnitude are observed15. However its impact is highly domain
dependent and since we are primarily concerned with the performance of a general purpose planner, in
this study we only consider its use as a beam filter.
Under beam search, the flux measure can strongly impact every search episode, as it influences the
states actually included in the active PE. When used in this mode, search segments with an assessed
flux below a specified threshold are skipped over even if their f-value places them in the active PE.
Flux proves to be broadly effective across most domains when used in this mode. As mentioned in
Section 5.1, we use a flux cutoff in each search episode of 10% of the average flux for all search segments in the PE; any segment below this value is not visited regardless of its heuristic rank. At this
setting the impact on speedup for the PEGG column problems of Table 3 ranges from nil to a factor of
9x over PEGGs performance without the flux filter. Higher settings can dramatically speed up the
solution search, but often at the expense of greater solution makespan.
5.6.2 PEGGS ABILITY TO FIND STEP-OPTIMAL PLANS
The variety of parameters associated with the beam search approach described above admits considerable flexibility in biasing PEGG towards producing plans of different quality. Shorter makespan plans
are favored by more extensive search of the PE states in each episode while heuristically truncated
search tends to generate non-optimal plans more quickly, often containing redundant or unnecessary
actions. The settings used in this study clearly bias PEGG solutions towards step-optimality: The
step-optimal plan produced by enhanced Graphplan is matched by PEGG for all but four of the 37
problems reported in Table 3, as indicated by the annotated steps and actions numbers given in parenthesis next to successful GP-e and PEGG runs16. (PEGG solutions with a longer makespan than the
step-optimal have boldface step/action values.) In these four problems, PEGG returns solutions
15

For example, compared to a simple bottom-up visitation strategy, the flux-augmented adjusted-sum heuristic improves
so-PEGG runtimes by up to 11x in some domains (e.g. Freecell and, Satellite) while degrading it by as much as 2x to 7x in
others (e.g. Zenotravel).

16

Where more than one of the guaranteed step-optimal planners (GP-e, me-EGBG and so-PEGG) finds a solution the steps
and actions are reported only for one of them, since they all will have the same makespan.

566

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Problem

N-best first
PEGG
[N=100, state-space search] [ adjusted-sum-u heuristic with beam search on
the 100 best search segments]

SATPLAN
(optimal)

bw-large-a

8

6

(.1 s)

6

bw-large-b

12

9 (4.5 s)

9

bw-large-c

21

14 (39.0 s)

14

bw-large-d

25

18 (412 s)

18

Table 4: Quality comparison (in terms of plan steps) for PEGG and N-best beam search
on a forward state space planner (Bonet et al., 1997).
within four steps of optimum, in spite of its highly pruned search. This proved to be a fairly robust
property of PEGGs beam search under these settings across all problems tested to date.
PEGG under the adjusted-sum-u secondary heuristic often finds plans with fewer actions than
GP-e in parallel domains, and this Graphplan hybrid system is also impressive in serial domains such
as blocksworld (which are not exactly Graphplans forte).
The tactic of trading off optimal plan length in favor of reduced search effort is well known in the
planning community. By comparison, PEGGs beam search approach is biased towards producing a
very high quality plan at possibly some expense in runtime. For example, in their paper focusing on
an action selection mechanism for planning, Bonet et. al. briefly describe some work with an N-best
first algorithm (Bonet, Loerincs, & Geffner, 1997). Here they employ their distance-based heuristic
to conduct beam search in forward state space planning. They report a small set of results for the case
where the 100 best states are retained in the queue to be considered during search.
Table 4 reproduces those results alongside PEGGs performance on the same problems using beam
search. Here, to approximate the N-best algorithm, PEGG is also run with 100 states visited in each
intermediate search episode. The 1997 study compared the N-best first approach against SATPLAN,
which produces the optimal length plan, to make the point that the approach could produce plans reasonably close to optimal with much less search. The N-best first code is not available to run on our
test platform, so only PEGGs runtime is reported. Focusing on plan makespan, its clear that even in
this serial domain, the parallel planner PEGG produces a much shorter plan than the N-best first
state space approach, and in fact finds the optimum length plan generated by SATPLAN in all cases.
More recently LPG (Gerevini & Serina, 2002), another planner whose search is tightly integrated
with the planning graph, was awarded top honors at the AIPS-2002 planning competition, due to its
ability to quickly produce high quality plans across a variety of domains currently of interest. In the
Figure 11 scatter plot, solution quality in terms of steps for LPG and PEGG are compared against the
optimal for 22 problems from three domains of the 2002 AIPS planning competition. We chose these
particular problems because the optimal solution is known, and we are interested in comparing to a
quality baseline. LPGs results are particularly apt in this case, because that planner also nonexhaustively searches the planning graph at each level before extending it, although its search process
differs markedly from PEGGs. LPG, too, can be biased to produce plans of higher quality (generally
at the expense of speed) and here we report the competition results for its quality mode. In terms of
number of actions in the solutions neither planner consistently dominates for these problems but
PEGG clearly excels in step-optimality. Its maximum deviation from optimum is four steps and most
567

fiZIMMERMAN & KAMBHAMPATI

18

LPG: dlog

16

LPG: depot

Steps over optimal

14

LPG: ztravel

12

PEGG: dlog

10

PEGG: depot

8

PEGG: ztravel

6
4
2
0
0

2

4

6

8

10

Problem number

12

14

16

18

Figure 11: Makespan comparison of PEGG and LPG -Departure from step-optimal
plan length. (LPG data taken from the AIPS 2002 competition results.)
of the plot points for its solutions lie right on the optimal makespan axis. It is possible that there are
sets of actions within LPGs solutions that could be conducted in parallel but the algorithms quality
mode heuristic is insensitive to them .
It should be noted that LPG produced solutions for some difficult problems in these domains that
PEGG currently does not solve within a reasonable time limit. We are investigating the characteristics
of these problems that make them so difficult for PEGG.
5.6.3 PEGG COMPARED TO HEURISTIC STATE SPACE SEARCH
We have not attempted here to run PEGG head-to-head for speed against recent IPC planners, in part
due to platform difficulties (PEGG is written in Lisp while the competition planners are generally
coded in C and published results are based on the execution on the competition machines) and partly
due to our focus on near-optimal makespan for parallel plans rather than speed. Given PEGGs close
coupling with the planning graph, the most relevant comparisons are with other parallel planners that
also employ the graph in some form. For such comparisons, we would like to isolate the search component of the runtime from planning graph construction, since there are a variety of routines that produce essentially the same graph with widely different expenditures of computational time and memory. The reported runtimes for the LPG planner in the AIPS-02 competition are generally much
smaller than PEGGs, but its difficult to isolate the impact of graph construction and platform-related
effects, not to mention the disparity in the makespan of the plans produced.
Table 5 compares PEGG against a Lisp version of a fast distance-based heuristic state space planner using most of the same problems as Table 3. AltAlt (Srivastava et al., 2001), like PEGG, depends
on the planning graph to derive the powerful heuristics it uses to direct its regression search on the
problem goals. This facilitates planner performance comparison based on differences in search without confusing graph construction time issues. The last column of Table 5 reports AltAlt performance
(runtime and makespan) for two of the most effective heuristics developed for that planner (Nguyen &
Kambhampati, 2000), the first of which is the adjusted-sum heuristic as described in Section 5.3.1.

568

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Problem
bw-large-B
bw-large-C
bw-large-D
rocket-ext-a
att-log-a
att-log-b
Gripper-8
Gripper-15
Gripper-20
Tower-7
Tower-9
8puzzle-1
8puzzle-2
TSP-12
AIPS 1998
grid-y-1
gripper-x-5
gripper-x-8
log-y-5
mprime-1
AIPS 2000
blocks-10-1
blocks-12-0
blocks-16-2
logistics-10-0
logistics-12-1
freecell-2-1
schedule-8-9
AIPS 2002
depot-6512
depot-1212
driverlog-2-3-6e
driverlog-4-4-8
roverprob1423
roverprob4135
roverprob8271
sat-x-5
sat-x-9
ztravel-3-7a
ztravel-3-8a

PEGG
heuristic: adjusted-sum-u
cpu sec (steps/acts)
4.1
24.2
388
1.1
2.2
21.6
5.5
46.7
1110.8
6.1
23.6
9.2
7.0
6.9

(18/18)
(28/28)
(38/38)
(7/34)
(11/62)
(13/64)
(15/23)
(36/45)
(40/59)
(127/127)
(511/511)
(31/31)
(32/32)
(12/12)
PEGG
16.8 (14/14)
110
(23/35)
520
(35/53)
30.5 (16/34)
2.1 (4/6)
PEGG
6.9 (32/32)
9.4 (34/34)
40.9 (56/56)
7.3 (15/ 53)
17.4 (15/75)
19.1 (6/10)
297
(5/12)
PEGG
2.1 (14/31)
79.1 (22/53)
80.6 (12/26)
889
(23/38)
15
(9/28)
379
(12 / 43)
220
(11 / 39)
25.1 (7/22)
9.1 (6/35)
101
(10/23)
15.1 (9/26)

Alt Alt (Lisp version)
cpu sec ( / acts)
heuristics:
combo
adjusum2
67.1 (/ 18 ) 19.5 (/28 )
608 (/ 28) 100.9 (/38)
950 (/ 38)
~
23.6 (/ 40) 1.26 (/ 34)
16.7 ( /56) 2.27( / 64)
189 (/ 72) 85.0 (/77)
6.6 (/ 23)
*
10.1 (/ 45)
6.98 (/45)
38.2 (/ 59) 20.9 (/59)
7.0 (/127)
*
28.0 (/511)
*
33.7 ( / 31) 9.5 ( /39)
28.3 (/ 30)
5.5 (/ 48)
21.1 (/12) 18.9 (/12)
Alt Alt
17.4 (/14)
17.5 (/14)
9.9 (/35)
8.0 (/37)
73 (/48)
25.0 (/53)
44 (/38)
29.0 (/42)
722.6 (/ 4)
79.6 (/ 4)
Alt Alt
13.3 (/32)
7.1 (/36)
17.0 (/34)
61.9 (/56)
31.5 (/53)
80 (/77)
49 (/12)
123 (/15)
Alt Alt
1.2 (/33)
290 (/61)
50.9 (/28)
461 (/44)
2.0 ( /33)
292 ( /45)
300 ( / 45)
3.1 (/25)
5.9 (/ 35)
77 (/28)
15.4 (/31)

Table 5: PEGG and a state space planner using variations of the adjusted-sum heuristic
PEGG: bounded PE search, best 20% of search segments visited in each search episode, as ordered by
adjusted-sum-u state space heuristic
AltAlt: Lisp version, state space planner with two of the most effective planning graph distance-based
heuristics: adjusum2 and combo (combo results are not reported for all problems since
adjusum2 produces plans that are more competitive with PEGG in terms of makespan.)

569

fiZIMMERMAN & KAMBHAMPATI

Surprisingly, in the majority of problems PEGG returns a parallel, generally step-optimal plan faster
than AltAlt returns its serial plan. (AltAlt cannot construct a plan with parallel actions, however recent
work with a highly modified version of AltAlt does, in fact, construct such plans -Nigenda & Kambhampati, 2003). The PEGG plans are also seen to be of comparable length, in terms of number of actions, to the best of the AltAlt plans.
6. Discussion of Results

A distinguishing feature of the EGBG and PEGG planners relative to other planners that exploit the
planning graph, is their aggressive use of available memory to learn online from their episodic search
experience so as to expedite search in subsequent episodes. Although they each employ a search trace
structure to log this experience, the EGBG and PEGG systems differ in both the content and granularity of the search experience they track and the aggressiveness in their use of memory. They also differ
in how they confront a common problem faced by learning systems; the utility of learned information
versus the cost of storing and accessing it when needed.
Our first efforts focused primarily on using a search trace to learn mutex-related redundancies in
the episodic search process. Although the resulting planners, EGBG and me-EGBG, can avoid virtually all redundant mutex checking based on search experience embodied in their PEs, empirically we
find that its a limited class of problems for which this is a winning strategy. The utility of tracking
the mutex checking experience during search is a function of the number of times that information is
subsequently used. Specifically:
EPS ( p )

6  1) U mt ( p) 

 PE

visit

( e)

 PE

add

(e)

e =1
EPS ( p )
e =1

where: Umt is the utility of tracking mutex checking experience
p is a planning problem
EPS(p) is the number of search episodes in problem p
PEvisit (e) is the number of PE search segments visited in search episode e
PEadd (e)is the number of new search segments added to PE in episode e
Thus the payback for EGBGs incurred overhead of tracing consistency-checking experience during
search depends on the number of times the sets are revisited relative to the total number of subgoal
sets generated (and added to the PE) during the problem run. This characteristic explains the less than
2x speedups observed for me-EGBG on many Table 2 problems. The approach is a handicap for single search episode problems. It is also ineffectual for problems where final search episode search generates a large number of states relative to previous episodes and when the only seed segment(s) are at
the top levels of the PE (due to need for bottom-up visitation of the search segments in EGBGs
search trace).
The PE can be thought of as a snapshot of the regression search reachable (RS reachable) states
for a search episode. That is, once the regression search process generates a state at level k of the
planning graph, the state is reachable during search at all higher levels of the graph in future search
episodes. Essentially, the search segments in the PE represent not just the RS reachable states, but a
570

fispeedup wrt enhanced Graphplan

USING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

so-PEGG: log

1000.0

PEGG: log
so-PEGG: ztravel
PEGG:ztravel

100.0

10.0

1.0
1
0.1

2

3

4

5

6

7

search episodes in problem

Figure 12: Speedup vs. number of search episodes: Logistics '00 and Zenotravel '02 domains
candidate set of partial plans with each segments state being the current tail state of such a plan. Table 3 and 5 results indicate that the utility of learning the states that are RS reachable in a given search
episode generally outweighs the utility of learning details of the episodes consistency-checking, and
require much less memory. Freed from the need to regenerate the RS reachable states in IDA* fashion during each search episode, PEGG can visit such states in any heuristically preferred order.
Tables 2, 3 and 5 shed light on several classes of problems that are problematic for search trace
guided search on the planning graph:
1. Domains with high branching factors and operator descriptions that thwart DDB and EBL
(e.g. the larger Schedule, Satellite, and Zenotravel domain problems)
2. Problems with a significant fraction of runtime consumed in planning graph construction.
(e.g. Grid domain,, dlog-2-3-6e, freecell-2-1)
3. Problems with only one or two search episodes ( grid-y-1, schedule-8-5)
For problems in the first class, the Graphplan-style CSP assignment search is prone to bogging
down after certain PE states are selected for visitation. For those in the second class any search time
reduction can be dominated by the large graph construction time (a problem shared by any planner
that builds the complete planning graph). Problems in the third class do not give PEGG sufficient
opportunity to exploit the PE, since it is built in the first episode (and the first episode PE is typically
small) and of no benefit until subsequent episodes. This aspect of PEGGs behavior is illustrated in
Figure 12. Here the speedup factors of both so-PEGG and PEGG (under beam search) are plotted for
a series of problems ordered according to the number of search episodes that Graphplan would conduct prior to finding a solution. The data was gathered by running the GP-e, so-PEGG, and PEGG
planners on two different domains (the Logistics domain from the AIPS-00 planning competition, and
the Zenotravel domain from the AIPS-02 competition) and then averaging the speedups observed for
problems with the same number of observed search episodes. The downturn for the PEGG/ Ztravel
curve at seven episodes is not surprising given that there was only one such problem and there are
many factors beyond the number of search episodes that impact solution time. Noting that the speed571

fiZIMMERMAN & KAMBHAMPATI

ups are plotted on a logarithmic scale, the power of a search trace given multiple search episode problems is evident. PEGG using beam search handily outperforms so-PEGG for all problems of three or
more search episodes, largely because it shortcuts exhaustive search in the intermediate episodes.
There are several avenues for addressing the above-listed limitations of PEGG that we have explored or anticipate investigating. For example, unlike the N-best first state space planner reported
in Table 4, PEGG enforces the user-specified limit on state f-values only when selecting PE search
segments to visit. Once a search segment is chosen for visitation, Graphplan-style regression search
on the state goals continues until either a solution is found or all sub-branches fail. A more greedy
approach would be to also apply the heuristic bound during this regression search. That is, we could
backtrack whenever a state is generated that exceeds the f-value threshold applied to search segments
before they are visited. This translates the Greedy Best First Search (GBFS) algorithm employed by
HSP-r (Bonet & Geffner, 1999) for state space search, into a form of hill-climbing search on the planning graph.
Experimentally we find that when PEGG is adapted to enforce the PE state f-value limit during its
regression search, improvements are unpredictable at best. Speedups of up to a factor of 100 were
observed in a few cases (all logistics problems) but in many cases runtimes increased or search failed
entirely within the time limit. In addition, the quality (make-span) of the returned solutions suffered
across a broad range of problems. There are two factors that may explain this result: 1) PEGGs regression search is greatly expedited by DDB and EBL, but the regressed conflict set that they rely on
is undefined when the regression search space below a state is not fully explored, such as when an fvalue limit is enforced. Without conducting such search there is no informed basis for returning anything other than the full set of subgoals in the state, which essentially forces search towards chronological backtracking. 2) Assessing an f-value for a newly generated state to compare against an fvalue bound that is based on states generated in previous episodes is problematic. This is because the
heuristic values of PE states that determine the f-value bound have been increased by PEGGs use of
search experience to improve h-value estimates (Section 5.1.2).
Degradation of solution quality as we shift PEGG closer to a greedy search approach may be an indicator that PEGGs ability to return step-optimal plans (as evidenced by Table 3 results) is rooted in
its interleaving of best-state selection from the PE with Graphplan-style depth-first search on the
states subgoals.
7. Related Work

We focus here on related or alternative strategies for employing search heuristics in planning, generating parallel plans, or making use of memory to expedite search. Related work pertaining to some of
the search techniques, efficiencies, and data structures that enable EGBG and PEGG to successfully
employ a search trace were cited as they arose above and are not further considered here.17
As noted in Section 2.2, a shortcoming of IDA* search (and Graphplan) is its inadequate use of
available memory: The only information carried over from one iteration to the next is the upper
bound on the f-value. Exploitation of a search trace directly addresses this shortcoming by serving as
a memory of the states in the visited search space of the previous episode in order to reduce redun17

Support methodologies include memory efficient bi-partite planning graph models, explanation based learning and dependency directed backtracking in the context of planning graph search, variable and value ordering strategies, and the evolution
and extraction of distance-based heuristics from the planning graph.

572

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

dant regeneration. In this respect PEGGs search is closely related to methods such as MREC (Sen,
Anup & Bagchi, 1989), MA*, and SMA* (Russell, 1992) which lie in the middle ground between the
memory intensive A* and IDA*s scant use of memory. A central concern for these algorithms is
using a prescribed amount of available memory as efficiently as possible. Like EGBG and PEGG,
they retain as much of their search experience as memory permits to avoid repeating and regenerating
nodes, and depend on a heuristic to order the nodes in memory for visitation. Unlike our search trace
based algorithms though, all three of the above algorithms backup a deleted nodes f-value to the parent node. This ensures the deleted branch is not re-expanded until no other more promising node remains on the open list. We have not implemented this extended memory management in PEGG
(though it would be straight-forward to do so) primarily because, at least under beam search, PEGG
has seldom confronted PE-related memory limitations.
EGBG and PEGG are the first planners to directly interleave the CSP and state space views in problem search, but there are related approaches that synthesize different views of the planning problem.
The Blackbox system (Kautz & Selman 1999) constructs the planning graph but instead of exploiting
its CSP nature, it is converted into a SAT encoding after each extension and a k-step solution is
sought. GP-CSP (Do & Kambhampati, 2000), similarly alternates between extending a planning
graph and converting it, but it transforms the graph into CSP format and seeks to satisfy the constraint
set in each search phase.
The beam search concept is employed in the context of propositional satisfiability in GSAT (Selman, Levesque, & Mitchell, 1992) and is an option for the Blackbox planner (Kautz & Selman, 1999).
For these systems greedy local search is conducted by assessing in each episode, the n-best flips of
variable values in a randomly generated truth assignment (Where the best flips are those that lead to
the greatest number of satisfied clauses). If n flips fail to find a solution, GSAT restarts with a new
random variable assignment and again tries the n-best flips. There are several important differences
relative to PEGGs visitation of the n-best search trace states. The search trace captures the state aspect engendered by Graphplans regression search on problem goals and as such, PEGG exploits
reachability information implicit in its planning graph. In conducting their search on a purely propositional level, SAT solvers can leverage a global view of the problem constraints but cannot exploit
state-space information. Whereas GSAT (and Blackbox) do not improve their performance based on
the experience from one n-best search episode to the next, PEGG learns in a variety of modes; improving its heuristic estimate for the states visited, reordering the state goals based on prior search experience, and memorizing the most general no-goods based on its use of EBL.
Like PEGG, the LPG system (Gerevini & Serina, 2002) heavily exploits the structure of the planning graph, leverages a variety of heuristics to expedite search, and generates parallel plans. However,
LPG conducts greedy local search in a space composed of subgraphs on a given length planning
graph, while PEGG combines a state space view of its search experience with Graphplans CSP-style
search on the graph itself. LPG does not systematically search the planning graph before heuristically
moving to extend it, so the guarantee of step-optimality is forfeited. PEGG can operate either in a
step-optimal mode or in modes that trade off optimality for speed to varying degrees.
We are currently investigating an interesting parallel to LPGs ability to simultaneously consider
candidate partial plans of different lengths. In principle, there is nothing that prevents PEGG from
simultaneously considering a given PE search segment Sn, in terms of its heuristic rankings when its
transposed onto various levels of the planning graph. This is tantamount to simultaneously consider573

fiZIMMERMAN & KAMBHAMPATI

ing which of an arbitrary number of candidate partial plans of different implied lengths to extend first
(each such partial plan having Sn as its tail state). The search trace again proves to be very useful in
this regard as any state it contains can be transposed up any desired number of levels -subject to the
ability to extend the planning graph as needed- and have its heuristics re-evaluated at each level. Referring back to Figure 4, after the first search episode pictured (top), the YJ state in the PE could be
expanded into multiple distinct states by transposing it up from graph level 5 to levels 6, 7, or higher,
and heuristically evaluating it at each level. These graph-level indexed instances of YJ can now be
simultaneously compared. Ideally wed like to move directly to visiting YJ at planning graph level 7,
since at that point it becomes a plan segment for this problem (bottom graph of Figure 4). If our secondary heuristic can discriminate between the solution potential for a state at the sequential levels it
can be transposed to, we should have an effective means for further shortcutting Graphplans level-bylevel search process. The flux adjunct is likely to be one key to boosting the sensitivity of a distancebased heuristic in this regard.
Generating and assessing an arbitrarily large number of graph-level transposed instances of PE
states would be prohibitive in terms of memory requirements if we had to store multiple versions of
the PE. However we can simply store any level-specific heuristic information in the search segments
of a single PE as values indexed to their associated planning graph levels. Challenging issues include
such things as the range of plan lengths to be considered at one time and the potential for plans with
steps consisting entirely of persists actions.
We havent examined PEGG in the context of real-time planning here, but its use of the search
trace reflects some of the flavor of the real-time search methods, such as LRTA* (Korf, 1990) and
variants such as B-LRTA* (Bonet, Loerincs, & Geffner, 1997), -a variant that applies a distance-based
heuristic oriented to planning problems. Real-time search algorithms interleave search and execution,
performing an action after a limited local search. LRTA* employs a search heuristic that is based on
finding a less-than-optimal solution then improving the heuristic estimate over a series of iterations. It
associates an h-value with every state to estimate the goal distance of the state (similar to the h-values
of A*). It always first updates the h-value of the current state and then uses the h-values of the successors to move to the successor believed to be on a minimum-cost path from the current state to the goal.
Unlike traditional search methods, it can not only act in real-time but also amortize learning over consecutive planning episodes if it solves the same planning task repeatedly. This allows it to find a suboptimal plan fast and then improve the plan until it converges on a minimum-cost plan.
Like LRTA*, the PEGG search process iteratively improves the h-value estimates of the states it
has generated until it determines an optimal make-span plan. Unlike LRTA*, PEGG doesnt actually
find a sub-optimal plan first. Instead it converges on a minimum-cost plan by either exhaustively extending all candidate partial plans of monotonically increasing length (so-PEGG) or extending only the
most promising candidates according to its secondary heuristic (PEGG with beam search). A realtime version of PEGG more closely related to LRTA* might be based on the method described above,
in which search segments are simultaneously transposed onto multiple planning graph levels. In this
mode PEGG would be biased to search quickly for a plan of any length, and then search in anytime
fashion on progressively shorter length planning graphs for lower cost plans.
This methodology is of direct relevance to work we have reported elsewhere on multi-PEGG
(Zimmerman & Kambhampati, 2002; Zimmerman 2003), a version of PEGG that operates in an anytime fashion, seeking to optimize over multiple plan quality criteria. Currently multi-PEGG first re-

574

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

turns the optimal make-span plan, and then exploits the search trace in a novel way to efficiently
stream plans that monotonically improve in terms of other quality metrics. As discussed in that paper,
an important step away from multi-PEGGs bias towards the make-span plan quality metric would be
just such a modification. Co-mingling versions of the same state transposed onto multiple planning
graph levels would enable the planner to concurrently consider for visitation candidate search segments that might be seed segments for latent plans of various lengths.
8. Conclusions

We have investigated and presented a family of methods that make efficient use of available memory
to learn from different aspects of Graphplans iterative search episodes in order to expedite search in
subsequent episodes. The motivation, design, and performance of four different planners that build
and exploit a search trace are described. The methods differ significantly in either the information
content of their trace or the manner in which they leverage it. However, in all cases the high-level
impact is to transform the IDA* nature of Graphplans search by capturing some aspect of the search
experience in the first episode and using it to guide search in subsequent episodes, dynamically updating it along the way.
The EGBG planners employ a more aggressive mode of tracing search experience than the PEGG
planners. They track and use the action assignment consistency checking performed during search on
a subgoal set (state) to minimize the effort expended when the state is next visited. The EGBG approach was found to be memory intensive, motivating the incorporation of a variety of techniques
from the planning and CSP fields which, apart from their well-known speedup benefits, are shown to
have a dramatic impact on search trace and planning graph memory demands. The resulting planner,
me-EGBG, is frequently two orders of magnitude faster than either standard Graphplan or EGBG and
for problems it can handle, it is generally the fastest of the guaranteed step-optimal approaches we
investigated. In comparisons to GP-e, a version of Graphplan enhanced with the same space saving
and speedup techniques, me-EGBG solves problems on average 5 times faster.
The PEGG planners adopt a more skeletal search trace, a design more conducive to informed traversal of the search space. Ultimately this proves to be a more powerful approach to exploiting the
episodic search experience. We adapt distance-based, state space heuristics to support informed traversal of the states implicit in the search trace and describe a metric we call flux which effectively
focuses search on states worth visiting. This flux measure is sensitive to the potential for a search
trace state to seed new search branches as it is transposed to higher planning graph levels. We also
describe some new techniques that leverage the search experience captured in the search trace and
demonstrate their effectiveness.
The so-PEGG planner, like me-EGBG, produces guaranteed optimal parallel plans and similarly
averages a 5x speedup over GP-e. Its greatly reduced memory demands allow so-PEGG to handles all
but one of the 16 problems for which me-EGBG exceeds available memory. More compelling evidence of the speedup potential for a search trace guided planner is provided by PEGG under beam
search. Since it no longer exhaustively searches the planning graph in each episode, PEGG sacrifices
the guarantee of returning an optimal make-span plan. Nonetheless, even under beam search limited
to just the best 20% of PE states in each episode, PEGG returns the step-optimal plan in almost 90%
of the test bed problems and comes within a few steps of optimal in the others. It does so at speedups

575

fiZIMMERMAN & KAMBHAMPATI

ranging to almost two orders of magnitude above GP-e, and quite competitively with a modern state
space planner (which finds only serial plans).
The code for the PEGG planners (including GP-e) with instructions for running them in various
modes is available for download at http://rakaposhi.eas.asu.edu/pegg.html
Acknowledgements

This research was improved by many discussions with Binh Minh Do, XuanLong Nguyen, Romeo
Sanchez Nigenda and William Cushing. Thanks also to David Smith and the anonymous reviewers,
whose copious suggestions greatly improved the presentation of this paper. This research is supported
in part by the NSF grants IRI-9801676 and IIS-0308139, DARPA AASERT Grant DAAH04-96-10247 and the NASA grants NAG2-1461 and NCC-1225.

Appendix A: The EGBG Planner

The insight behind EGBGs use of a search trace is based on the characterization of Graphplans
search given at the beginning of Section 3.1 and some entailed observations:
Observation A-1) The intra-level CSP-style search process conducted by Graphplan on a set of propositions (subgoals) S , at planning graph level k+1 in episode n+1 is identical to the search process on S at
level k in episode n IF:
1. All mutexes between pairs of actions that are establishers of propositions of S at level k remain
mutex for level k+1. (this concerns dynamic mutexes; static mutexes persist by definition)
2. There are no actions establishing a proposition of S at level k+1 that were not also present at level k.
Observation A-2) The trace of Graphplans search during episode n+1, on a set of goals G, at planning
graph level m+1, is identical to its episode n search at level m IF:
1. The two conditions of observation A-1 hold for every subgoal set (state) generated by Graphplan in
the episode n+1 regression search on G.
2. For every subgoal set S at planning graph level j in search episode n for which there was a matching
level j memo, there exists an equivalent memo at level j+1 when S is generated in episode n+1.
Conversely, for every subgoal set S at level j in search episode n for which no matching level j memo
existed at the time it was generated, there is also no matching memo at level j+1 at the time S is generated in episode n+1.

Now, suppose we have a search trace of all states (including no-good states) generated by Graphplans regression search on the problem goals from planning graph level m in episode n. If that search
failed to extract a solution from the m-length planning graph (i.e. reach the initial state), then a necessary condition to extract a solution from the m+1 length graph is that one or more of the conditions of
observations in A-1 or A-2 fails to hold for the states in the episode n search trace.
With observations A-1 and A-2 in mind, we can exploit the search trace in a new episode in a
sound and complete manner by focusing search effort on the only three situations that could lead to a
solution: 1) under state variables with newly extended value ranges (i.e. search segment goals that
have at least one new establishing action at their newly associated graph level), 2) at points in the previous search episode that backtracked due to violation of a dynamic constraint (i.e. two actions that
576

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

W

nop a1
OK OK
1

Y

7

nop a3
OK SM

nop a3
OK SM

2

nop a1
OK OK

H

3

I

6

8

nop a3 a1
OK SM OK
4 nop a55

J

DM SM

nop a5 nop a5
DM SM DM SM

nop a5
DM SM

Figure A1: Bit vector representation of the search trace for the WYHIJ state in Figure 3.
Semantics: OK > assigned (no action conflicts) SM > action is static mutex with previous assign
DM > action is dynamic mutex with previous assign
NG > a no-conflict action results in a no-good state at lower graph level

were dynamic mutex), and 3) at states that matched a cached memo in episode n. All other assignment and mutex checking operations involved in satisfying a set of subgoals are static across search
episodes.
We experimented with several search trace designs for capturing key decision points. The design
adopted for EGBG employs an ordered sequence of bit vectors, where each vector contains the results
of Graphplans CSP-style action assignment process related to a given subgoal in a search segment.
Efficient action assignment replay is possible with a trace that uses vectors of two-bit tags to represent
four possible assignment outcomes: 1) dynamic mutex, 2) static mutex, 3) no conflict, and 4) a complete, consistent set of assignments that is rejected at the next level due to a memoized no-good. Figure A1 illustrates how a sequence of eight such bit vectors can be used to capture the search experience for the search segment with state goals WYHIJ from our Figure 3 Alpha problem. Here the propositional goals (the variables) appear to the left of the sets of bit vectors (depicted as segmented bars)
which encode the outcome of all possible action assignment (the values). Each possible establishing
action for a goal appears above a bit vector tag.
The numbered edges reflect the order in which the trace vectors are initially created when the first
goal action is tried. Note that whenever a candidate action for a goal is conflict free with respect to
previously assigned actions (indicated by the OK in the figure), action checking for the goal is suspended, the process jumps to the next goal, and a new bit vector is initialized for this goals possible
establishers. The edge numbering also reflects the order in which the vectors are popped from the
search segment trace list when the segment is revisited in the next episode. For this scheme to work,
the bit vectors must be pushed onto the search segment trace list after all actions for a goal are tried, in
the reverse of the numbered edge order. Long edges that skip over one or more goals indicate that
those goals are already established by previously assigned actions.
As long as the order of actions appearing under the establishers list for a planning graph proposition remains constant, the bit vectors can be used to replay the search in the next episode on the next
higher planning graph level. The graph building routine for EGBG enforces this constraint.

577

fiZIMMERMAN & KAMBHAMPATI

The EGBG Algorithm

The high-level EGBG algorithm is given in Figure A2. As for Graphplan, search on the planning
graph occurs only after it has been extended to the level where all problem goals first appear with no
binary mutex conditions. (the call to find_1st_level_with_goals). The first search episode is conducted
in Graphplan fashion except that the assign_goals routines of Figure A3 create search segments to
hold the states and trace information generated during the regression search process. The necessary
trace information for a search segment is captured in trace vectors as described above. These segments are stored in the PE structure indexed according to the level at which they where generated
(where the current highest planning graph level corresponds to 0 and contains the problem goals).
Subsequent to the first episode, EGBG_plan enters an outer loop that employs the PE to conduct
successive episodes (Referred to as search trace guided). The search strategy alternates between the
selection and visitation of a promising state from the trace of previous experience (select_searchseg_from_PE routine), with a focused CSP-type search on the states subgoals (the replay_trace_goals and assign_goals routines of Figures A3 and A4).
For each episode, an inner loop visits the PE search segments in level-by-level, bottom-up fashion
(for the reasons discussed in Section 3). The extend_plangraph routine called only when a state being
visited corresponds to a level beyond the current graph length.
The replay_trace_goals routine is the counterpart to Graphplans assign_goals routine, except that
it avoids the latters full-blown mutex checking by stepping through the trace vectors that captured
previous search experience for a given state. Unlike assign_goals, it does not branch to any child
states already contained in the PE. The conditional checking of the trace vectors against establishing
actions initiates new search by calling assign_goals under two conditions: 1) when dynamic mutexes
from previous episodes no longer hold 2) when new establishing actions appear for a subgoal (These
are tried after all other establishers are replayed.) When a dynamic mutex no longer holds or a new
establishing action is considered the trace vector is modified accordingly and EGBG resumes Graphplans CSP-style search, adding new trace vectors to the search segment in the process.

578

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

EGBG-PLAN ( Ops, Init, Goals) /* {Ops,Init,Goals} is a planning problem */
/* build plangraph, PG, until level n where goals first occur in non-mutex state*/

Let PG  find_1st_level_with_goals( Ops, Init, Goals )
if PG reached level-off and goals are not present in non-mutex state then Return FAIL
Let n be the number of levels in PG
Let SS0 be a new search segment with fields:
goalsGoals, parentroot, PE-level 0, parent-actions {} trace {}
Let PE  pilot explanation structure with fields to hold search segments at each plangraph level
PE[0] {SS0} /* 0 is top level of PE */
/* Conduct Graphplan-style backward search on the n-length plangraph, store trace in the PE...*/

Let search-reslt  assign_goals(Goals, {}, n, SS0, PG, PE)
if search-reslt is a search segment /* Success */
then Plan  extract plan actions from the ancestors linked to search-reslt
Return Plan
else /* No n-length solution possible ...use the PE to search for a longer length solution */
loop forever
n  n+1
loop for pe-lev ranging from the number of deepest level of PE to 0 (top level)
let k  planning graph level associated with PE level pe-lev
= n  pe-lev /* ..essentially translates the PE up one planning graph level */
if pe-lev = 0 then PG  extend_plangraph(PG) /*.. must extend plangraph at this point */
loop for search segments in PE[pe-lev]
SS  select_searchseg_from_PE[pe-lev]
SSassigns  SS<trace> /* get ordered, goal-by-goal trace vectors from search segment */
SS<trace>  {} /* Clear the search segment trace vectors field */
if SS<goals>  memos(k, PG) /*check for nogoods at level k */
then these goals match a nogood at this level, loop to next search segment
else /* use SS trace to avoid redundant search effort on SS goals.. */
search-reslt  replay_trace_goals (SS<goals>, {}, k, SSassigns, SS, PG, PE)
if search-reslt is a search segment
then Plan  extract plan actions from the ancestors linked to search-reslt
Return Plan
else Add SS<goals> to memos(k, PG) /*memoize nogood */
end loop (search segments)
end loop (PE levels)
end-loop (PG level)
end

Figure A2: EGBG planner top level algorithm

579

fiZIMMERMAN & KAMBHAMPATI

Conduct Graphplan-style search on a subgoal set at planning graph level k
arguments> G: goals still to be assigned, A: actions already assigned, k: PG level,
SS1: search segment, PG: planning graph, PE: pilot explanation (search trace)
ASSIGN_GOALS (G, A, k, SS1, PG, PE)
if G is empty or k = 0 (the initial state) then Return SS1 /* Success */
else /* there are goals left to satisfy*/
Let g-assigns ={} /* trace vector will hold ordered action assignment tags */
Let g be a goal selected from G
Let Ag = actions from PG level k that support g, ordered by value-ordering heuristic
loop for act  Ag
Let search-reslt = {}
if an action in A is dynamic mutex with act then append dm tag to g-assigns
else if an action in A is static mutex with act then append sm tag to g-assigns
else /* act has no conflict with actions already in A */
if G is empty
then /* done with G goals, setup for search at next lower level */
search-reslt  assign_next_level_goals (A U{act}, k, SS1, PG, PE)
if search-reslt is a nogood then append ng tag to g-assigns
else append ok tag to g-assigns /* search occurred at lower level */
else /* search continues at this level with the next goal */
append ok tag to g-assigns
search-reslt  assign_goals (G-{g}, A U{act}, k, SS1, PG, PE)
end-if
if search-reslt is a search segment then Return search-reslt /* Success */
/* else we loop and try another action*/

end-if
end loop (actions)
push g-assigns into trace field of SS1
Return nil /* no solution found */
end-if
end

/*add trace data to search segment */

Setup for search on graph level k-1 given that the actions in A satisfy the goals of SS1 at level k
ASSIGN_NEXT_LEVEL_GOALS (A, k, SS1, PG, PE)
Let nextgoals regress SS1 goals over assigned actions in A
if nextgoals  memos at PG level k-1 then Return nogood /* backtrack on nogood goals */
else /*  initiate search on next lower PG level*/
Let SS2 be a new search segment with fields:
goalsnextgoals, parentSS1, parent-actionsA, trace{}
Add SS2 to the PE at level: (maximum PG level)  (k-1)
Let search-reslt  assign_goals (nextgoals, {}, k-1, SS2, PG, PE)
if search-reslt is nil /* search at level k-1 failed */
then Add nextgoals to memos to level k-1 of PG /*memoize nogood */
Return search-reslt
end-if
end
ASSIGN_NEW_ACTIONS (G, A, Ag, g, g-assigns, k, SS, PG, PE)
/* Routine is essentially the same as assign_goals, except it attempts to satisfy goal g actions
with only the new actions (i.e. those first appearing in the most recent plangraph extension */

Figure A3: EGBGs non-guided regression search algorithm

580

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Regression search using search trace (PE) replay
arguments> G: goals still to be assigned, A: actions already assigned, k: PG level,
SS1: search segment, PG: planning graph, PE: pilot explanation (search trace)
REPLAY_TRACE_GOALS (G, A, k, SS1, PG, PE)
if G is empty /* all SS1 goals in this branch were successfully assigned during last episode...*/
then Return /* .. continue with level k replay, ignoring search replay at next lower level */
else /* there are goals left to satisfy*/
Let g  select goal from G
Let g-assigns  pop the front trace vector from SS<trace>
Let Ag set of actions from level k of PG that support g
/* Now replay assignments for g from previous episode, rechecking only those that may have changed..*/

loop for tag  g-assigns
Let search-reslt {}, Let act  pop action from Ag
if tag = ok /* act had no conflict with actions in A during last episode.. go to next goal */
then search-reslt  replay_trace_goals (G-{g}, A U{act}, k, SSassigns, SS1, PG, PE)
else-if tag =ng then loop /* its the last action assign at this level & act was not mutex in the last
episode --So the next-level regressed goals reside in a child search segment that was already visited */

else if tag = sm then loop /* act was static mutex with action in A last episode */
else tag = dm /* act was dynamic mutex with action in A last episode retest it...*/
if dynamic mutex persists then loop
else change tag to ok in g-assigns vector /* act is no longer mutex with A actions */
if G-{g} is not empty /* resume backward search at this level */
then search-reslt  assign_goals (G-{g}, A U{act}, k, SS1, PG,PE)
else /* no goals left to satisfy in SS1, setup for search at lower level */
search-reslt  assign_next_level_goals (A U{act}, k, SS1, PG, PE)
if search-reslt =nogood then change g-assigns vector tag to ng
end-if
if search-reslt is a search segment then Return search-reslt (Success)
else loop (check next action)
end-loop /* all establishment possibilities from prior episode tried ..Now check for new actions */
if Ag still contains actions /* they are new actions establishing g at this level .. attempt to assign */
then search-reslt  assign_new_actions(G, A, Ag, g, g-assigns, k, SS1, PG, PE)
if search-reslt is a search segment then Return search-reslt /* Success */
else push g-assigns  SS1<trace>
Return nil /* no solution found in search stemming from SS1 goals */
end-if
end

Figure A4: EGBGs search-trace guided algorithm

581

fiZIMMERMAN & KAMBHAMPATI

Appendix B: Exploiting CSP Speedup Methods to Reduce Memory Demands

Background and implementation details are provided here for the six techniques from the planning
and CSP fields which proved to be key to controlling memory demands in our search trace based
planners. They are variable ordering, value ordering, explanation based learning (EBL), dependency
directed backtracking (DDB), domain preprocessing and invariant analysis, and replacing the redundant multi-level planning graph with a bi-partite version.
Domain preprocessing and invariant analysis:
The speedups attainable through preprocessing of domain and problem specifications are well
documented (Fox & Long, 1998; Gerevini & Schubert, 1996). Static analysis prior to the planning
process can be used to infer certain invariant conditions implicit in the domain theory and/or problem
specification. The domain preprocessing for me-EGBG and PEGG is fairly basic, focusing on identification and extraction of invariants in action descriptions, typing constructs, and subsequent rewrite
of the domain in a form that is efficiently handled by the planning graph build routines. Our implementation discriminates between static (or permanent) mutex relations and dynamic mutex relations
(in which a mutex condition may eventually relax) between actions and proposition pairs. This information is used to both expedite graph construction and during me-EGBGs replay of action assignments when a search segment is visited.
Domain preprocessing can significantly reduce memory requirements to the extent that it identifies
propositions that do not need to be explicitly represented in each level of the graph. (Examples of
terms that can be extracted from action preconditions -and hence do not get explicitly represented in
planning graph levels- include the (SMALLER ?X ?Y) term in the MOVE action of the towers of Hanoi domain and typing terms such as (AUTO ?X) and (PLACE ?Y) in logistics domains.) This benefit is
further compounded in EGBG and PEGG since propositions that can be removed from action preconditions directly reduce the size of the subgoal sets generated during the regression search episodes, and
hence the size of the search trace.
Bi-partite planning graph:
The original Graphplan maintains the level-by-level action, proposition, and mutex information in
distinct structures for each level, thereby duplicating -often many times over- the information contained in previous levels. This multi-level planning graph can be efficiently represented as an indexed
two-part structure and finite differencing techniques employed to focus on only those aspects of the
graph structure that can possibly change during extension. This leads to more rapid construction of a
more concise planning graph (Fox & Long 1998; Smith & Weld, 1998).
For me-EGBG and PEGG, the bi-partite graph offers a benefit beyond the reduced memory demands and faster graph construction time; the PE transposition process described in section 3.1 is reduced to simply incrementing each search segments graph level index. This is not straightforward
with the multi-level graph built by Graphplan, since each proposition (and action) referenced in the
search segments is a unique data structure in itself.
Explanation Based Learning and Dependency Directed Backtracking:
The application of explanation based learning (EBL) and dependency directed backtracking (DDB)
were investigated in a preliminary way in (Zimmerman & Kambhampati, 1999), where the primary
interest was in their speedup benefits. The techniques were shown to result in modest speedups on
582

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

several small problems but the complexity of integrating them with the maintenance of the PE replay
vectors limited the size of problem that could be handled. We have since succeeded in implementing
a more robust version of these methods, and results reported here reflect that.
Both EBL and DDB are based on explaining failures at the leaf-nodes of a search tree, and propagating those explanations upwards through the search tree (Kambhampati, 1998). DDB involves using the propagation of failure explanations to support intelligent backtracking, while EBL involves
storing interior-node failure explanations, for pruning future search nodes. An approach that implements these complimentary techniques for Graphplan is reported in (Kambhampati, 2000) where
speedups ranged from ~2x for blocksworld problems to ~100x for ferry domain problems. We
refer to that study for a full description of EBL/DDB in a Graphplan context, but note here some aspects that are particularly relevant for me-EGBG and PEGG.
As for conflict directed back-jumping (Prosser, 1993), the failure explanations are compactly represented in terms of conflict sets that identify the specific action/goal assignments that gave rise to
backtracking. This liberates the search from chronological backtracking, allowing it to jump back to
the most recent variable taking part in the conflict set. When all attempts to satisfy a set of subgoals (a
state) fail, the conflict set that is regressed back represents a useful minimal no-good for memoization. (See the PEGG algorithm in Figures 8 and 9 for a depiction of this process.) This conflict set
memo is usually shorter and hence more general than the one generated and stored by standard
Graphplan. Additionally, an EBL-augmented Graphplan generally requires less memory for memo
caches.
Less obvious than their speedup benefit perhaps, is the role EBL and DDB often play in dramatically reducing the memory footprint of the pilot explanation. Together EBL and DDB shortcut the
search process by steering it away from areas of the search space that are provably devoid of solutions.
Search trace memory demands decrease proportionally.
Both me-EGBG and PEGG have been outfitted with EBL/DDB for all non-PE directed Graphplan-style search. me-EGBG however, does not use EBL/DDB in the replay of the action assignment results for a PE search segment due to the complexity of having to retract parts of assignment
vectors whenever the conflict set in a new episode entails a new replay order.
Value and Variable Ordering:
Value and variable ordering are also well known speedup methods for CSP solvers. In the context of
Graphplans regression search on a given planning graph level k, the variables are the regressed subgoals and the values are the possible actions that can give these propositions at level k of the graph. In
their original paper, Blum and Furst (1997) argue that variable and value ordering heuristics are not
particularly useful in improving Graphplan, mainly because exhaustive search is required in the levels
before the solution bearing level anyway. Nonetheless, the impact of dynamic variable ordering
(DVO) on Graphplan performance was examined in (Kambhampati, 2000), and modest speedups
were achieved using the standard CSP technique of selecting for assignment the subgoal (variable)
that has the least number of remaining establishers (values). More impressive results are reported in
a later study (Nguyen & Kambhampati, 2000) where distance-based heuristics rooted in the planning
graph were exploited to order both subgoals and goal establishers. In this configuration, Graphplan
exhibits speedups ranging from 1.3 to over 100x, depending on the particular heuristic and problem.

583

fiZIMMERMAN & KAMBHAMPATI

For this study we fix variable ordering according to the adjusted sum heuristic and value ordering
according the set level heuristic, as we found the combination to be reasonably robust across the
range of our test bed problems. These heuristics are described in Section 5 where they are used to
direct the traversal of the PE states is discussed. Section 4.1 describes the highly problem-dependent
performance of distance-based variable and value ordering for our search trace-based planners.
The manner in which EGBG/PEGG builds and maintains the planning graph and search trace
structures actually reduces the cost of variable and value ordering. The default order in which Graphplan considers establishers (values) for satisfying a proposition (variable) at a given level is set by the
order in which they appear in the planning graph structure. During graph construction in me-EGBG
and PEGG we can set this order to correspond to the desired value ordering heuristic, so that the ordering is only computed once. For its part, the PE that is constructed during search can record the
heuristically-best ordering of each regression states goals, so that this variable ordering is also done
only once for the given state. This contrasts with versions of Graphplan that have been outfitted with
variable and value ordering (Kambhampati, 2000) where the ordering is reassessed each time a state is
regenerated in successive search episodes.

References
Blum, A. & Furst, M.L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90(1-2).
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism for
planning. In Proceedings of AAAI-97.
Bonet, B. & Geffner, H. (1999). Planning as heuristic search: New results. In Proceedings of
ECP-99.
Do, M.B. & Kambhampati, S. (2000). Solving Planning-Graph by compiling it into CSP. In
Proceedings of AIPS-00.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal of
Artificial Intelligence Research, 9, 317-371.
Frost, D. & Dechter, R. (1994). In search of best constraint satisfaction search. In Proceedings of
AAAI-94.
Gerevini , A., & Schubert, L. (1996). Accelerating Partial Order Planners: Some techniques for
effective search control and pruning. Journal of Artificial Intelligence Research 5, 95-137.
Gerevini, A. & Serina, I., (2002). LPG: A planner based on local search for planning graphs with
action costs. In Proceedings of AIPS-02.
Haslum, P., & Geffner, H. (2000). Admissible Heuristics for Optimal Planning. In Proceedings.
of AIPS-00.
Hoffman, J. (2001) A heuristic for domain independent planning and its use in an enforced hillclimbing algorithm. Technical Report No. 133, Albert Ludwigs University.
Kambhampati, S. (1998). On the relations between Intelligent Backtracking and Failure-driven
Explanation Based Learning in Constraint Satisfaction and Planning. Artificial Intelligence,
105(1-2).
Kambhampati, S. (2000). Planning Graph as a (dynamic) CSP: Exploiting EBL, DDB and other
CSP search techniques in Graphplan. Journal of Artificial Intelligence Research, 12, 1-34.
Kambhampati, S. & Sanchez, R. (2000). Distance-based Goal-ordering heuristics for Graphplan.
In Proceedings of AIPS-00.
584

fiUSING MEMORY TO TRANSFORM SEARCH ON THE PLANNING GRAPH

Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding and extending Graphplan.
In Proceedings of ECP-97.
Kautz, H. & Selman, B. (1996). Pushing the envelope:
stochastic search. In Proceedings of AAAI-96.

Planning, prepositional logic and

Kautz, H. & Selman, B. (1999). Unifying SAT-based and Graph-based Planning. In Proceedings
of IJCAI-99, Vol 1.
Koehler, D., Nebel, B., Hoffman, J., & Dimopoulos, Y., (1997). Extending planning graphs to an
ADL subset. In Proceedings of ECP-97, 273-285.
Korf, R. (1985). Depth-first iterative-deepening: an optimal admissible tree search. Artificial
Intelligence, 27(1), 97-109.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189-211.
Long, D. & Fox, M. (1999). Efficient implementation of the plan graph in STAN. Journal of
Artificial Intelligence Research, 10, 87-115.
Mittal, S., & Falkenhainer, B. (1990). Dynamic constraint satisfaction problems. In Proceedings
of AAAI-90.
McDermott, D. (1999). Using regression graphs to control search in planning. Artificial
Intelligence, 109(1-2), 111-160.
Nigenda, R., & Kambhampati, S. (2003). AltAltp: Online Parallelization of Plans with Heuristic
State Search. Journal of Artificial Intelligence Research, 19, 631-657.
Nguyen, X. & Kambhampati, S. (2000). Extracting effective and admissible state space heuristics
from the planning graph. In Proceedings of AAAI-00.
Prosser, P. (1993). Domain filtering can degrade intelligent backtracking search. In Proceedings
of IJCAI-93.
Russell, S.J., (1992). Efficient memory-bounded search methods. In Proceedings of ECAI 92.
Sen, A.K., & Bagchi, A., (1989). Fast recursive formulations for best-first search that allow
controlled use of memory. In Proceedings of IJCAI-89.
Selman, B, Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability
problems. In Proceedings of AAAI-92.
Smith, D., Weld, D. (1998). Incremental Graphplan. Technical Report 98-09-06. Univ. of Wash.
Srivastava, B., Nguyen, X., Kambhampati, S., Do, M., Nambiar, U. Nie, Z., Nigenda, R.,
Zimmerman, T. (2001). AltAlt: Combining Graphplan and Heuristic State Search. In AI
Magazine, 22(3), American Association for Artificial Intelligence, Fall 2001.
Zimmerman, T. (2003). Exploiting memory in the search for high quality plans on the planning
graph. PhD dissertation, Arizona State University.
Zimmerman, T. & Kambhampati, S. (1999). Exploiting Symmetry in the Planning-graph via
Explanation-Guided Search. In Proceedings of AAAI-99.
Zimmerman, T., Kambhampati, S. (2002). Generating parallel plans satisfying multiple criteria in
anytime fashion. In Proceedings of workshop on Planning and Scheduling with Multiple
Criteria, AIPS-02.
Zimmerman, T. & Kambhampati, S. (2003). Using available memory to transform Graphplans
search. Poster paper in Proceedings of IJCAI-03.

585

fiJournal of Artificial Intelligence Research 23 (2005) 331 -- 366

Submitted 06/04; published 03/05

Learning From Labeled And Unlabeled Data: An Empirical Study
Across Techniques And Domains
Nitesh V. Chawla
Department of Computer Science & Engg.,
University of Notre Dame,
IN 46556, USA
Grigoris Karakoulas
Department of Computer Science
University of Toronto
Toronto, Ontario
Canada M5S 1A4

NCHAWLA@CSE.ND.EDU

GRIGORIS@CS.TORONTO.EDU

Abstract
There has been increased interest in devising learning techniques that combine unlabeled data
with labeled data  i.e. semi-supervised learning. However, to the best of our knowledge, no study
has been performed across various techniques and different types and amounts of labeled and
unlabeled data. Moreover, most of the published work on semi-supervised learning techniques
assumes that the labeled and unlabeled data come from the same distribution. It is possible for the
labeling process to be associated with a selection bias such that the distributions of data points in the
labeled and unlabeled sets are different. Not correcting for such bias can result in biased function
approximation with potentially poor performance. In this paper, we present an empirical study of
various semi-supervised learning techniques on a variety of datasets. We attempt to answer various
questions such as the effect of independence or relevance amongst features, the effect of the size of
the labeled and unlabeled sets and the effect of noise. We also investigate the impact of
sample-selection bias on the semi -supervised learning techniques under study and implement a
bivariate probit technique particularly designed to correct for such bias.

1. Introduction
The availability of vast amounts of data by applications has made imperative the need to combine
unsupervised and supervised learning (Shahshahni & Landgrebe, 1994; Miller & Uyar, 1997;
Nigam et al., 2000; Seeger, 2000; Ghani et al., 2003). This is because the cost of assigning labels to
all the data can be expensive, and/or some of the data might not have any labels due to a selection
bias. The applications include, but are not limited to, text classification, credit scoring, and fraud
and intrusion detection. The underlying challenge is to formulate a learning task that uses both
labeled and unlabeled data such that generalization of the learned model can be improved.
In recent years various techniques have been proposed for utilizing unlabeled data (Miller &
Uyar, 1997; Blum & Mitchell, 1998; Goldman & Zhou, 2000; Nigam et al., 2000; Blum & Chawla,
2001; Bennett et al., 2002; Joachims, 2003). However, to the best of our knowledge, there is no
empirical study evaluating different techniques across domains and data distributions. Our goal in
2005 AI Access Foundation. All rights reserved.

fiCHAWLA & KARAKOULAS

this paper is to examine various scenarios of dealing with labeled and unlabeled data in conjunction
with multiple learning techniques.
Incorporating unlabeled data might not always be useful, as the data characteristics and/or
technique particulars might dictate that the labeled data are sufficient even in the presence of a
corpus of unlabeled data. There have been conflicting experiences reported in the literature. On the
one hand, the NIPS01 competition on semi-supervised learning required the contestants to achieve
better classification results in most of the cases than just supervised learning (Kremer & Stacey,
2001). This implies an expectation that in most of the cases semi-supervised techniques could help.
However, the results from the competition did not meet this expectation. Shahshahni and
Landgrebe (1994) note that unlabeled data can delay the occurrence of Hughes phenomenon and
the classif ication performance can be improved. Zhang and Oles (2000), using the Fisher
Information criterion, show that for parametric models unlabeled data should always help. On the
other hand, Cozman et al. (2002; 2003), using asymptotic analysis, show that unlabeled data can in
fact decrease classification accuracy. They concluded that if the modeling assumptions for labeled
data were incorrect, then unlabeled data would increase classification error, assuming the labeled
and unlabeled data come from the same distribution. But they did not formulate the asymptotic
behavior of semi-supervised techniques on scenarios in which the labeled and unlabeled data come
from different distributions. In addition, they only evaluate their algorithm on one real dataset, by
varying the amount of labeled and unlabeled data.
The aforementioned semi-supervised learning techniques do not distinguish the different
reasons for the data being missing. Most of the techniques assume that the data are missing
completely at random (MCAR), i.e. P(labeled=1| x, y) = P(labeled=1) where y is the class label
assigned and labeled=1 if a given example is labeled (Little & Rubin, 1987). In this case the
labeling process is not a function of variables occurring in the feature space. The labeled and
unlabeled data are assumed to come from the same distribution.
When the labeling process is a function of the feature vector x then the class label is missing at
random (MAR), i.e. P(labeled=1| x, y) = P(labeled=1|x). This scenario of MAR labeling can occur,
for example, in credit scoring applications since many creditors use quantitative models for
approving customers. In that case the class label y is observed only if some (deterministic) function
g of variables in x exceeds a threshold value, i.e. g(x)c, where c is some constant. From the
definition of MAR it follows that P(y=1| x, labeled=1) = P(y=1| x, labeled=0)=P(y = 1|x), i.e. at
any fixed value of x the distribution of the observed y is the same as the distribution of the missing
y. However, due to the aforementioned screening, the conditional distribution of x given y is not the
same in the labeled and unlabeled data, i.e. there is a bias. Thus, in modeling techniques that aim to
learn such conditional distributions one needs to correct for this bias by incorporating information
from unlabeled data.
In contrast, when labeling depends on y, the class label is missing not at random (MNAR), i.e.
P(labeled=1| x, y)  P(labeled=0| x, y). In that case there is sample-selection bias in constructing
the labeled set that has to be corrected (Heckman, 1979). For example, sample -selection bias can
occur in credit scoring applications when the selection (approval) of some customers (labels) is
performed based on human judgment rather than a deterministic model (Greene, 1998; Crook &
Banasik, 2002; Feelders, 2000). Sample -selection bias can also occur in datasets from marketing
332

fiLearning From Labeled And Unlabeled Data

campaigns. In the MNAR scenario the labeled and unlabeled data come from different distributions
associated with a censoring mechanism, i.e. P(y=1| x, labeled=1)  P(y=1| x, labeled=0). In such a
scenario learning only from labeled data can give estimates that are biased downwards. When a
selection bias is associated with the labeling process it is necessary to model the underlying missing
data mechanism. Shahshahani and Landgrebe (1994) also note that if the training sample is not very
representative of the distribution in the population then the unlabeled data might help.
In this paper we present an empirical study of some of the existing techniques in learning from
labeled and unlabeled data under the three different missing data mechanisms, i.e. MCAR, MAR
and MNAR. To the best of our knowledge, the effect of unlabeled data under different missing data
mechanisms has not been previously addressed in the semi-supervised learning literature. We also
introduce two techniques from Econometrics, namely reweighting and bivariate probit, for
semi-supervised learning. We try to answer various questions that can be important to learning
from labele d and unlabeled datasets, such as:






What is the process by which data have become labeled vs. unlabeled?
How many unlabeled vs. labeled examples in a dataset?
What is the effect of label noise on semi-supervised learning techniques?
Are there any characteristics of the feature space that can dictate successful
combination of labeled and unlabeled data?
What is the effect of sample-selection bias on semi-supervised learning methods?

For our experiments we have chosen datasets with unbalanced class distributions, since they
can be typical of real-world applications with unlabeled data, such as information filtering, credit
scoring, customer marketing and drug design. For such datasets, accuracy can be a misleading
metric as it treats both kinds of errors, false positives and false negatives, equally. Thus, we use
AUC, Area Under the Receiver Operating Curve (ROC), as the performance metric (Swets, 1988;
Bradley, 1987). AUC has become a popular performance measure for learning from unbalanced
datasets (Provost & Fawcett, 2000; Chawla et al., 2002).
The paper is organized as follows. In Section 2 we present an overview of the techniques that
were evaluated in this work. These techniques are mainly applicable to MCAR or MAR type of
data. In Section 3 we present two techniques that purport to deal with sample -selection bias in
MNAR data. In Section 4 we describe the experimental framework and in Section 5 we analyze the
results from the experiments. We conclude the paper with a discussion of the main findings.

2. Semi-Supervised Learning Methods
The techniques we evaluate for learning from labeled and unlabeled data are: Co-training (Blum &
Mitchell, 1998; Goldman & Zhou, 2000), Reweighting (Crook & Banasik, 2002), ASSEMBLE
(Bennett et al., 2002) and Common-component mixture with EM (Ghahramani & Jordan, 1994;
Miller & Uyar, 1997). We chose the variant of the co-training algorithm proposed by Goldman and
Zhou (2000) since it does not make the assumption about redundant and independent views of data.
For most of the datasets considered, we do not have a "natural" feature split that would offer
redundant views. One could try to do a random feature split, but again that is not in-line with the
original assumption of Blum and Mitchell (1998). Essentially, two different algorithms looking at
333

fiCHAWLA & KARAKOULAS

same data can potentially provide better than random information to each other in the process of
labeling. By construction, co-training and ASSEMBLE assume that the labeled and unlabeled data
are sampled from the same distribution, namely they are based on the MCAR assumption. The
re-weighting and common-component mixture techniques are based on the assumption that data
are of MAR type. Thus, they can be applied to cases where the conditional distribution of x given y
is not the same in the labeled and unlabeled data. Details of these techniques are given in the rest of
this Section.
For sample -selection correction in MNAR data we use the Bivariate Probit technique
(Heckman, 1979; Greene, 1998) and Sample -Select, an adapted version of the technique used by
Zadrozny and Elkan (2000). Both of these techniques will be presented in Section 3.
It is worth pointing out that we selected some of the most popular algorithms for learning from
labeled and unlabeled data. However, the list of algorithms under study is not meant to be
exhaustive. This is because our main goal is to evaluate the behavior of such algorithms with
different amounts, distributions, and characteristics of labeled and unlabeled data.
We used the Nave Bayes algorithm as the underlying supervised learner for all the
aforementioned semi-supervised learning methods. We chose Nave Bayes due to its popularity in
the semi-supervised learning framework as well as the need for a generative model for the
common-component mixture technique. Thus, this choice for the base learner helped us achieve
consistency and comparability in our experimental framework. Only for the co-training
experiments, which by design require two classifiers, we also used C4.5 decision tree release 8
(Quinlan, 1992). Please note that in the rest of the paper, we will use Nave Bayes and supervised
learner, interchangeably.
We next introduce a notation that will be used for describing the various methods in this paper.
Consider a classification problem with K classes y k , k = 0,1,...,K  1 . We will assume that the
training set consists of two subsets: X = {X L , X U } , where

X L = {( x1 , y1 ), ( x2 , y 2 ),..., ( xl , yl )} is the labeled subset,
X U = {xl +1 , x l + 2 ,..., xl + u } is the unlabeled subset,
l and u are the number of examples in the labeled and unlabeled sets, respectively, and
x is the n-dimensional feature vector.
2.1 Co-training
Co-training, proposed by Blum and Mitchell (1998), assumes that there exist two independent and
compatible feature sets or views of data. That is, each feature set (or view) defining a problem is
independently sufficient for learning and classification purposes. For instance, a web page
description can be partitioned into two feature subsets: words that exist on a web page and words
that exist on the links leading to that web page. A classifier learned on each of those redundant
feature subsets can be used to label data for the other and thus expand each others training set. This
should be more informative than assigning random labels to unlabeled data. However, in a
real-world application, finding independent and redundant feature splits can be unrealistic, and this
can lead to deterioration in performance (Nigam & Ghani, 2001).
Goldman and Zhou (2000) proposed a co-training strategy that does not assume feature
independence and redundancy. Instead, they learn two different classifiers (using two different
334

fiLearning From Labeled And Unlabeled Data

supervised learning algorithms) from a dataset. The idea behind this strategy is that since the two
algorit hms use different representations for their hypotheses they can learn two diverse models that
can complement each other by labeling some unlabeled data and enlarging the training set of the
other. Goldman and Zhou derive confidence intervals for deciding which unlabeled examples a
classifier should label. We adopt this co-training strategy because it allows us to apply it to various
real-world datasets and thus compare it with other semi-supervised learning techniques.
For illustration purposes, let us assume there are two classifiers A and B. A labels data for B, and
B labels data for A until there are no unlabeled data left or none can be labeled due to a lack of
enough confidence in the label. The decision to label data for the other classifier is taken on the
basis of statistical techniques. Following Goldman and Zhou (2000) we construct confidence
intervals for A and B using 10-fold cross-validation. Let lA , lB , h A , h B , be the lower and upper
confidence intervals for A and B, respectively. Additionally, the upper and lower confidence
intervals for each class k can be defined as lAk, h Ak, l Bk, and h Bk. These intervals define the confidence
assigned to each of the classifiers in their labeling mechanism. In addition, the amount of data
labeled is subject to a noise control mechanism.
Let us consider the co-training round for A that is labeling data for B (the other will follow
similarly). Let XL be the original labeled training set (which is same for A and B), XLB be the data
labeled by A for B, wB be the conservative estimate for the mislabeling errors in XLB , m = |XL  XLB |
be the sample size, and  = wB /|XL  XLB | be the noise rate. The relationship between a sample of
size m, classification noise rate of , and hypothesis error of  can be given as

m=

k
 2 (1   2 )

,

k is a constant, assumed to be 1

This relationship is used to decide if the amount of additional data labeled can compensate for
the increase in the classification noise rate. Thus, based on the expressions for m and , the
conservative estimate of 1/ 2 for classifier B, q B , can be given as
q B = X L  X LB



1 




2wB

2
 X L  X LB


 

 



2

Similarly, we can compute the conservative estimate for 1/2 , qk, for each class k in the labeled set:

q k = X L  X LB  X Uk

 
2( w B + w k )
1  
  X L  X LB  X Uk
 

2


 ,



| X Uk | = Examples in X U mapped to class k
w k = (1  l k ) | X Uk |

335

fiCHAWLA & KARAKOULAS

Thus, for classifier A the labeling process consists of two tests:
(i)
(ii)

h Ak > lB
q k > qB

The first test ensures that the class k used by classifier A to label data has an accuracy at least as
good as the accuracy of classifier B. The second test is to help prevent a degradation in performance
of classifier B due to the increased noise in labels. The unlabeled examples that satisfy both criteria
are la beled by classifier A for classifier B and vice versa. This process is repeated until no more
unlabeled examples satisfy the criteria for either classifier. At the end of the co-training procedure
(no unlabeled examples are labeled), the classifiers learned on the corresponding final labeled sets
are evaluated on the test set.
In our experiments we used two different classification algorithms  Nave Bayes and C4.5
decision tree. We modified the C4.5 decision tree program to convert leaf frequencies into
probabilities using the Laplace correction (Provost & Domingos, 2003). For the final prediction,
the probability estimates from both Nave Bayes and C4.5 are averaged.
2.2 ASSEMBLE
The ASSEMBLE algorithm (Bennett et al., 2002) won the NIPS 2001 Unlabeled data competition.
The intuition behind this algorithm is that one can build an ensemble that works consistently on the
unlabeled data by maximizing the margin in function space of both the labeled and unlabeled data.
To allow the same margin to be used for both labeled and unlabeled data, Bennett et al. introduce
the concept of a pseudo-class. The pseudo-class of an unlabeled data point is defined as the
predicted class by the ensemble for that point. One of the variations of the ASSEMBLE algorithm
is based on AdaBoost (Freund & Schapire, 1996) that is adapted to the case of mixture of labeled
and unlabeled data. We used the ASSEMBLE.AdaBoost algorithm as used by the authors for the
NIPS Challenge.
ASSEMBLE.AdaBoost starts the procedure by assigning pseudo-classes to the instances in the
unlabeled set, which allows a supervised learning procedure and algorithm to be applied. The initial
pseudo-classes can be assigned by using a 1-nearest-neighbor algorithm, ASSEMBLE-1NN, or
simply assigning the majority class to each instance in the unlabeled datasets, ASSEMBLE-Class0.
We treat majority class as class 0 and minority class as class 1 in all our datasets. The size of the
training set over boosting iterations is the same as the size of the labeled set. Figure 1 shows the
pseudo code of the ASSEMBLE.AdaBoost algorithm.
In the pseudo code, L signifies the labeled set, U signifies the unlabeled set, l (size of the labeled
set) is the sample size within each iteration and  is the weight assigned to labeled or unlabeled data
in the distribution D0 . The initial misclassification costs are biased towards the labeled data, due to
more confidence in the labels. In the subsequent iterations of the algorithm, a, indicates the relative
weight given to each type of error  either labeled data or unlabeled data error. In the pseudo-code,
f indicates the supervised learning algorithm, which is Nave Bayes for our experiments. T indicates
the number of iterations, and f t (xi ) = 1 if an instance xi is correctly classified and f t (xi ) = -1, if it is
incorrectly classified;  is the error computed for each iteration t, and Dt is the sampling
distribution.
336

fiLearning From Labeled And Unlabeled Data

1. l :=| L | and u :=| U |
 / l
2. D1 ( i) := 
(1   ) / l

i  L 

i U 

3. yi := c, where c is the class of 1NN for i  U or class 0 for i  U,
and original class for i  L
4. f 1 = NaiveBayes (L + U)
5. for t := 1 to T do
6. Let y i := f t (x i ), i = 1,..., L + U

7.  =  Dt [yi  y i], i = 1,..., L + U
i

8. if  > 0.5 then Stop
1 -  
9. w t = 0.5 * log 

  
10. Let Ft := Ft -1 + w t f t
11. Let y i = Ft (x i ) if i  U
12. Dt +1 =

 i e  yi Ft +1 ( xi )
 i e

 yi Ft +1 ( xi )

i

i

13. S = Sample(L + U, Y, Dt + 1)
14. f t +1 = NaiveBayes (S)
15. end for
16. return Ft+1
Figure 1: ASSEMBLE.AdaBoost pseudo-code
2.3 Re-weighting
Re-weighting (Crook & Banasik, 2002) is a popular and simple technique for reject-inferencing in
credit scoring, but it has not been considered as a semi-supervised learning technique. It uses
information on the examples from approved credit applications, i.e. labeled data, to infer
conditional class probabilities for the rejected applications, i.e. unlabeled data.
To apply re-weighting one assumes that the unlabeled data are MAR, i.e.

P(y = 1 | x,labeled = 1) = P(y = 1 | x,labeled = 0) = P(y = 1 | x)
That is, at any x, the distribution of labeled cases having a y label is the same as the distribution
of missing y (in the unlabeled population).
There are several variants of re-weighting. According to the one we adopt here, also called
extrapolation, the goal is first to estimate the distribution of classes for the la beled data within each
337

fiCHAWLA & KARAKOULAS

score group, and then extrapolate that distribution to the unlabeled data. Thus, a model is learned on
the labeled training set and applied to the unlabeled data. The labeled and unlabeled data are
grouped by score, i.e. posterior class probability from the model. The probabilities are
percentile -binned, and each bin forms a score group. The unlabeled data are assigned labels based
on the distribution of classes in corresponding score group from the labeled data. Thus, the key
assumption is that the corresponding score-bands in the labeled and unlabeled data have the same
distribution for the classes. This can be formulated as:

P( y k | S j , X L ) = P( yk | S j , X U )
 y kjL / X Lj = y Ukj / X Uj
where S j is the score band of group j, y kjL is the number of labeled examples belonging t o class yk in
group j, and y U
kj is the proportion of unlabeled examples that could be class y k in the score group j.
The number of labeled cases in j are weighted by (|XLj +XUj |)/|XLj |, which is the probability sampling
weight.
To explain the re-weighting concept, let us consider the example given in Table 1. Let 0 and 1
be the two classes in the datasets. The score group of (0.6 - 0.7) is a bin on the posterior class
probabilities of the model. The group weight for the example in Table 1 can be computed as
(XL+XU )/XL = 1.2. Therefore, weighting the number of class0 and class1 in the group, we get class0
= 12; class1 = 108. Thus, we label at random 2 (=12-10) data points from the unlabeled set as
class0, and 18 (=108-90) data points from the unlabeled set as class1.
After labeling the unlabeled examples we learn a new model on the expanded training set
(inclusive of both labeled and unlabeled data).
Score Group
0.6 - 0.7

Unlabeled
20

Labeled

Class 0

Class 1

100

10

90

Table 1: Re-weighting example

2.4 Common Components Using EM
The idea behind this technique is to eliminate the bias in the estimation of a generative model from
labeled/unlabeled data of the MAR type. The reason for this bias is that in the case of such data the
distributions of x given y in the labeled and unlabeled sets are different. We can remove this bias by
estimating the generative model from both labeled and unlabeled data by modeling the missing
labels through a hidden variable within the mixture model framework.
In particular, suppose that the data was generated by M components (clusters), and that these
components can be well modeled by the densities p ( x | j , j ) , j = 1,2,..., M , with  j denoting
the corresponding parameter vector. The feature vectors are then generated according to the
density:
M

p ( x |  ) =   j p ( x | j , j )
j =1

338

fiLearning From Labeled And Unlabeled Data

where  j  p( j ) are the mixing proportions that have to sum to one.
The joint data log-likelihood that incorporates both labeled and unlabeled data takes the form:

L(  ) =

M



( xi , yi ) X l

log   j p( xi | j , j ) p ( yi | xi , j ,  j ) +
j =1



xi X u

M

log   j p ( xi | j ,  j )
j =1

Note that the likelihood function contains a "supervised" term, which is derived from X l labeled
data, and an "unsupervised" term, which is based on X u unlabeled data.
Consider applying the following simplistic assumption: the posterior probability of the class
label is conditionally independent of the feature vector given the mixture component, i.e.
p ( y i | xi , j ,  j ) = p( yi | j ) . This type of model makes the assumption that the class conditional
densities are a function of a common component (CC) mixture (Ghahramani & Jordan, 1994;
Miller & Uyar, 1997). In general, we are interested in applying the CC mixture model to solve a
classification problem. Therefore we need to compute the posterior probability of the class label
given the observation feature vector. This posterior takes the form:

p ( y i | xi , ) =

M

 p ( j | xi , ) p ( y i | xi , j,  j ) =
j =1

  p( x | j ,  ) 
j
i
j
 p( y i | j )
  M


p
(
x
|
l
,

)
j =1  l =1 l
i
l 
M

One could also consider applying a separate component (SC) mixture model, where each class
conditional density is modeled by its own mixture components. This model was essentially presented by
Nigam et al. (2000) for the case of labeled/unlabeled data in document classification. All of these
models, as well as more powerful ones that condition on the input feature vector, can be united under the
mixture-of-experts framework (Jacobs et al., 1991). The application of this framework to the
semi-supervised setting is further studied by Karakoulas and Salakhutdinov (2003).

2.4.1 Model Training
Assume that p ( x | j , j ) is parameterized by a Gaussian mixture component distribution for
continuous data. We define the parameter vector for each component, j, to be  j = (  j ,  j ) for
j = 1, 2,..., M ; where  j is the mean, and  j is the covariance matrix of the jth component. In all
of our experiments we constrain the covariance matrix to be diagonal. We also have two more
model parameters that have to be estimated: the mixing proportion of the components,  j = p ( j ) ,
and  y | j = p ( y i | j ) the posterior class probability.
i
According to the general mixture model assumptions, there is a hidden variable directly related
to how the data are generated from the M mixture components. Due to the labeled/unlabeled nature
of the data, there is an additional hidden variable for the missing class labels of the unlabeled data.
These hidden variables are introduced into the above log-likelihood L() .
A general method for maximum likelihood estimation of the model parameters in the presence
of hidden variables is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The
EM algorithm alternates between estimating expectations for the hidden variables (incomplete data)
given the current model parameters and refitting the model parameters given the estimated,
complete data. We now derive fixed-point EM iterations for updating the model parameters.
339

fiCHAWLA & KARAKOULAS

For each mixture component, j, and each feature vector, i, we compute the expected
responsibility of that component via one of the following two equations, depending on whether the
ith vector belongs to the labeled or unlabeled set (E-Step):

 tj  yi | j p( xi | j ,  tj )

p ( j | xi , yi ,  ) =
t

M

  lt  y |l p (x i | l , lt )
i

l =1

p ( j | xi ,  ) =
t

 tj p ( x i | j ,  tj )
M


l =1

 lt

xi  X L

xi  X U

p ( x i | l , lt )

Given the above equations the solution for the model parameters takes the form (M-Step):


1 
t
t 
p
(
j
|
x
,
y
,

)
+
p
(
j
|
x
,

)


i i
i

N  ( x , y )X
xi X u
 i i l


 tj+1 =

t +1
k| j

y

 p ( j | xi , y i ,  t )

=

xi  X l  y i = k

 p ( j | x i , y i , t )

( xi , yi )X l

 tj +1 =



t
t


p
(
j
|
x
,
y
,

)
x
+
p
(
j
|
x
,

)
x


i i
i
i
i
t +1 
N j  ( xi , yi )X l
xi  X u


 tj+1 =



t
t
t
t 

p
(
j
|
x
,
y
,

)
S
+
p
(
j
|
x
,

)
S


i i
ij
i
ij 
N tj+1  ( xi , yi )X l
xi  X u


1

1

where we define N =| X l | + | X u | , and Sij  ( xi   j )( xi   j ) .
t

t T

Iterating through the E-step and M-step guarantees an increase in the likelihood function. For
discrete-valued data, instead of using a mixture of Gaussians, we apply a mixture of multinomials
(Kontkanen et al., 1996).
In general, the number of components can be tuned via cross-validation. However, the large
number of experiments reported in this work makes the application of such tuning across all
experiments impractical, because of the computational overhead it would entail. Furthermore, the
purpose of this paper is to provide insight into learning from labeled and unlabeled data with
different techniques, rather than to find out which technique performs the best. For this reason we
report the results from experiments using two, six, twelve and twenty-four components. We further
340

fiLearning From Labeled And Unlabeled Data

discuss this point in Section 4.

3. Dealing With Sample Selection Bias
If there is a censoring mechanism that rules the assignment of a label to instances in the data then a
model learned on only the labeled data will have a sample selection bias. In this case the decision
for labeling can be due to unobserved features that create a dependency between assigning a label
and the class label itself. Thus the data are of MNAR type. For example, suppose we are interested
in assigning relevance to a text corpus, e.g., web pages, and that someone has subjectively decided
which documents to label. The estimates for probability of relevance can then be underestimated,
as the training set might not be representative of the complete population. This can result in an
increase in the estimation bias of the model, thus increasing the error.
We present two techniques, previously proposed in the literature, for dealing with MNAR data.
3.1 Bivariate Probit
Heckman (1979) first studied the sample -selection bias for modeling labor supply in the field of
Econometrics. Heckman's sample selection model in its canonical form consists of a linear
regression model and a binary probit selection model. A probit model is a variant of a regression
model such that the latent variable is normally distributed. Heckman's method is a two-step
estimation process, where the probit model is the selection equation and the regression model is the
observation equation. The selection equation represents the parameters for classification between
labeled or unlabeled data  namely its purpose is to explicitly model the censoring mechanism and
correct for the bias  while the observation equation represents the actual values (to regress on) in
the labeled data. The regression model is computed only for the cases that satisfy the selection
equation and have therefore been observed. Based on the probit parameters that correct for the
selection bias, a linear regression model is developed only for the labeled cases.
The following equations present the regression and probit models. The probit model is
estimated for y2 . The dependent variable y2 represents whether data is labeled or not; y2 = 1 then
data is labeled, y2 = 0 then data is unlabeled. The equation for y1 , the variable of interest, is the
regression equation. The dependent variable y1 is the known label or value for the labeled data. The
latent variables u 1 and u 2 are assumed to be bivariate and normally distributed with correlation .
This is the observation equation for all the selected cases (the cases that satisfy y2 >0). Let us
denote by y1 * the observed values of y1 . Then we have:

y1 =  ' x1 + u1
y 2 =  ' x 2 + u2
2

u1 , u2  N [ 0,0,  u1 ,  ]
y1 = y1* , if y 2

>0

y1  missing if y2  0
The estimate of  will be unbiased if the latent variables u 1 and u2 are uncorrelated, but that is
341

fiCHAWLA & KARAKOULAS

not the case as the data are MNAR. Not taking this into account can bias the results. The bias in
regression depends on the sign of  (biased upwards if  is positive, and downwards if  is
negative). The amount of bias depends on the magnitude of  and . The crux of Heckmans
method is to estimate the conditional expectation of u 1 given u 2 and use it as an additional variable
in the regression equation for y1 . Let us denote the univariate normal density and CDF of u 1 by 
and  , respectively. Then,

E[ y1* | x1 , y 2 > 0] =  ' x1 +  u 
1
 ( x2  ' )
where  =
( x2  ' )
 is also called the Inverse Mills Ratio (IMR), and can be calculated from the parameter estimates.
Heckman's canonical form requires that one of the models be regression based. However, in this
paper we deal with semi-supervised classification problems. For such problems a class is assigned
to the labeled data, y1 = 0 or 1, only if y2 = 1. Given the two equations in Heckmans method, the
labeled data are only observed for y2 = 1. Thus, we are interested in E[y1 |x, y 2 = 1] or P(y1 |x, y 2 = 1).
For example, to predict whether someone will default on a loan (bad customer), a model needs to be
learnt from the accepted cases. Thus, in such scenarios one can use the bivariate probit model for
two outcomes, default and accept (Crook & Banasik, 2002; Greene, 1998). The bivariate probit
model can be written as follows:

y1 =  x1 +u1
y 2 =  x 2 +u 2
y1 = y1* for y 2 = 1
Cov(u1 , u 2 ) = 
If  = 0 then there is no sample -selection bias associated with the datasets, that is the latent
variables are not correlated and using the single probit model is sufficient. The log-likelihood of the
bivariate model can be written as (where  2 is the bivariate normal CDF and  is the univariate
normal CDF).

Log ( L ) =  y

2 =1, y1 =1

log F 2 [ 1 x1 ,  2 x 2 ,  ]

+ y

2 =1, y1 =1

log F 2 [ 1 x 1 ,  2 x 2 ,  ]

  y 2=0 log F [  2 x 2 ]
We apply the bivariate probit model in the semi-supervised learning framework by introducing
a selection bias in various datasets.
3.2 Sample -Select
Zadrozny and Elkan (2000) applied a sample -selection model to the KDDCup-98 donors' dataset to
assign mailing costs to potential donors. There is sample sele ction bias when one tries to predict the
donation amount from only donors data. Using Nave Bayes or the probabilistic version of C4.5,
342

fiLearning From Labeled And Unlabeled Data

they computed probabilistic estimates for membership of a person in the ''donate'' or ''not donate''
category, i.e. P(labeled=1|x). They then imputed the membership probability as an additional
feature for a linear regression model that predicts the amount of donation. Zadrozny and Elkan did
not cast the above problem as a semi-supervised learning one. We adapted their method for the
semi-supervised classification setting as follows:

= P( labeled = 1 | x, L  U )
n +1
 P( y = 1 |x  x
, L)
n +1
x

L is the labeled training set, U is the unlabeled set, labeled=1 denotes an instance being labeled,
labeled = 0 denotes an instance being unlabeled, and y is the label assigned to an instance in the
labeled set. A new training set of labeled and unlabeled datasets (L U ) is constructed, wherein a
class label of 1 is assigned to labeled data and a class label of 0 is assigned to unlabeled data. Thus,
a probabilistic mode l is learned to classify data as labeled and unlabeled data. The probabilities,
P(labeled = 1|x), assigned to labeled data are then included as another feature in the labeled data
only. Then, a classifier is learned on the modified labeled data (with the additional feature). This
classifier learns on "actual" labels existing in the labeled data.
For our experiments, we learn a Nave Bayes classifier on L U, and then impute the posterior
probabilities as another feature of L, and relearn the Nave Bayes classifier from L. For the rest of
the paper we will call this method Sample-Select.
While we apply the Sample -Select method along with other semi-supervised techniques, we
only apply the bivariate probit model to the datasets particularly constructed with the
sample-selection bias. This is because Sample -Select is more appropriate for the MAR case rather
than the MNAR case due to the limiting assumption it makes. More specifically, in the MNAR case
we have
P(labeled,y|x,,)=P(y|x,labeled,)*P(labeled|x,)
However, since Sample -Select assumes
P(y|x,labeled=0,)= P(y|x,labeled=1,)
this reduces the problem to MAR.

4. Experimental Set-up
The goal of our experiments is to investigate the following questions using the aforementioned
techniques on a variety of datasets:
(i)
(ii)
(iii)
(iv)
(v)

What is the effect of independence or dependence among features?
How much of unlabeled vs. labeled data can help?
What is the effect of label noise on semi-supervised techniques?
What is the effect of sample -selection bias on semi-supervised techniques?
Does semi-supervised learning always provide an improvement over supervised
learning?

For each of these questions we designed a series of experiments real-world and artificial
343

fiCHAWLA & KARAKOULAS

datasets. The latter datasets were used for providing insights on the effects of unlabeled data within
a controlled experimental framework. Since most of the datasets did not come with a fixed
labeled/unlabeled split, we randomly divided them into ten such splits and built ten models for each
technique. We reported results on the average performance over the ten models for each technique.
Details for this splitting are given in Section 4.1. Each of the datasets came with a pre-defined test
set. We used those test sets to evaluate performance of each model. The definition of the AUC
performance measure is given in Section 4.2.
As mentioned earlier, the purpose of this work is to provide insight into learning from labeled
and unlabeled data with different techniques, rather than to find out which technique performs the
best. The latter would have required fine-tuning of each technique. However, to understand the
sensitivity of the various techniques with respect to key parameters, we run experiments with
different parameter settings. Thus, for the common-component technique, we tried 2, 6, 12, and 24
components; for ASSEMBLE-1NN and ASSEMBLE-Class0 we tried 0.4, 0.7 and 1.0 as values for
, the parameter that controls the misclassification cost; for co-training we tried 90%, 95% and
99% confidence intervals for deciding when to label. Appendix A (Tables 6 to 9) presents the
results for these four techniques, respectively. The common-component technique with 6
components gave consistently good performance across all datasets. We found marginal or no
difference in the performance of co-training for different confidence intervals. We found small
sensitivity to the value of a at lower amounts of labeled data, but as the labeled data increases the
differences between the values of a for various datasets becomes smaller. Moreover, not all datasets
exhibited sensitivity to the value of a. Our observations agree with Bennett et al. (2002) that choice
of a might not be critical in the cost function.
Thus, for analyzing the above questions we set the parameters of the techniques as follows: 6
components for common-component mixture,  = 1 for ASSEMBLE-1NN and
ASSEMBLE-Class0, and confidence of 95% for co-training.
4.1 Datasets
We evaluated the methods discussed in Sections 2 and 3 on various datasets, artificial and
real-world, in order to answer the above questions. Table 2 summarizes our datasets. All our
datasets have binary classes and are unbalanced in their class distributions. This feature makes the
datasets relevant to most of the real-world applications as well as more difficult for the
semi-supervised techniques, since these techniques can have an inductive bias towards the majority
class. To study the effects of feature relevance and noise we constructed 5 artificial datasets with
different feature characteristics and presence of noise. We modified the artificial data generation
code provided by Isabelle Guyon for the NIPS 2001 Workshop on Variable and Feature Selection
(Guyon, 2001). The convention used for the datasets is: A_B_C_D, where
 A indicates the percentage of independent features,
 B indicates the percentage of relevant independent features,
 C indicates the percentage of noise in the datasets, and
 D indicates the percentage of the positive (or minority) class in the datasets.
The independent features in the artificial datasets are drawn by N(0,1). Random noise is added
344

fiLearning From Labeled And Unlabeled Data

to all the features by N(0,0.1). The features are then rescaled and shifted randomly. The relevant
features are centered and rescaled to a standard deviation of 1. The class labels are then assigned
according to a linear classification from a random weight vector, drawn from N(0,1), using only the
useful features, centered about their mean. The mislabeling noise, if specified, is added by
randomly changing the labels in the datasets. The naming of the artificial datasets in Table 2 are
self-explanatory using the convention outlined earlier.
Datasets

Training Size
|L+U|

Testing
Size

Class-distribution
(majority:minority)

Features

30_30_00_05

8000

4000

95:5

30

30_80_00_05

8000

4000

95:5

30

80_30_00_05

8000

4000

95:5

30

80_80_00_05

8000

4000

95:5

30

30_80_05_05

8000

4000

95:5

30

30_80_10_05

8000

4000

95:5

30

30_80_20_05

8000

4000

95:5

30

SATIMAGE (UCI)

4435

2000

90.64:9.36

36

WAVEFORM (UCI)

3330

1666

67:33

21

ADULT (UCI)

32560

16281

76:24

14

2710

71.7:28.3

12

800

61.75:38.25

25

0.6:99.4

200

NEURON (NIPS)

L

530 +
2130 U

HORSE-SHOE (NIPS)

400 L +
400 U

KDDCUP-98 (UCI)

4843 L +

96367

90569 U
Table 2: Datasets details

Three of our real-world datasets  waveform, satimage, and adult  are from the UCI repository
(Blake et al., 1999). We transformed the original six-class satimage datasets into a binary class
problem by taking class 4 as the goal class (y=1), since it is the least prevalent one (9.73%), and
merging the remaining classes into a single one. Two of the datasets come from the NIPS 2001
(Kremer & Stacey, 2001) competition for semi-supervised learning with pre-defined
labeled/unlabeled subsets.
We randomly divided ten times all the above artificial and UCI datasets into five different
labeled-unlabeled partitions: (1,99)%, (10,90)%, (33,67)%, (67,33)%, (90,10)%. This provided us
with a diverse test bed of varying labeled and unlabeled amounts. We report the results averaged
345

fiCHAWLA & KARAKOULAS

over the ten random runs.
The last real-world dataset is from the KDDCup-98 Cup (Heittich & Bay, 1999; Zadrozny &
Elkan, 2000). This is a typical case of sample selection bias, i.e. MNAR. We converted the original
KDDCup-98 donors regression problem into a classification problem so that is consistent with the
semi-supervised classification setting of the experiments. We considered all the respondents to the
donation mailing campaign as the labeled set and the non-respondents as the unlabeled set. We then
transformed the labeled set into a classification problem, by assigning a label of 1 to all the
individuals that make a donation of greater than $2, and a label of 0 to all the individuals that
make a donation of less than $2. This gave us a very difficult dataset since due to the sample
selection bias built into the dataset the labeled and unlabeled sets had markedly different class
distributions. The positive class in the training set is 99.5%, while in the testing set it is only 5%.
We also created a biased version of the Adult dataset and the (30_80_00_05) artificial dataset
with MAR labels in order to study the effect of this missing label mechanism on the techniques.
More specifically, for the biased Adult dataset we constructed the labeled and unlabeled partitions
of the training set by conditioning on the education of an individual. Using this censoring
mechanism all the individuals without any post high-school education were put into the unlabeled
set, and the rest into the labeled set. Thus, we converted the original classification problem into a
problem of return of wages from adults with post high-school education. For the (30_80_00_05)
dataset we divided it into labeled and unlabeled sets by conditioning on two features such that: if
(xin <= ci || x jn <= cj ), n  U. This introduced a selection bias into the construction of the labeled
and unlabeled sets. In both datasets the test set was not changed. We further discuss the above three
biased datasets in Section 5.3.
4.2 Learning And Performance Measurement
In most of our experiments we use the Nave Bayes algorithm as the base learner. For the
co-training experiments we use a C4.5 decision tree learner in addition to Nave Bayes. To apply
the Nave Bayes algorithm, we pre-discretize the continuous features using Fayyad and Irani's
entropy heuristic (Fayyad & Irani, 1993).
We use Area Under the Receiver Operating Characteristic (ROC) curve (AUC) as the
performance metric in our experiments (Hand, 1997). Due to the imbalanced class distribution of
the datasets studied in this paper we chose to use AUC over the standard classification accuracy
metric, because the classification accuracy can be misleading for unbalanced datasets. To compare
the performance of a model with that of a random model we define AUC as

AUC := 2 * auc  1
where auc is the original area and AUC is the normalized one. Thus, with this formulation, AUC is
normalized with respect to the diagonal line in the ROC space that represents the random performance.
If the ROC curve of a model is below the diagonal line, then AUC is worse than random and hence
negative.

346

fiLearning From Labeled And Unlabeled Data

5. Empirical Analysis
In the following we present the analysis of the results from our experiments. We structure the
analysis around the questions stated in Section 4. Figures 2 to 6 show the AUCs for the various
datasets. The Figures show the performance of each of the semi-supervised technique, and the
(supervised) Nave Bayes classifier. The x-axis shows the percentage of labeled/unlabeled data in a
dataset as (Labeled, Unlabeled)%, and the y-axis shows the AUC from applying the
semi-supervised and supervised techniques. For each of the datasets that does not come with a
pre-defined labeled/unlabeled split we average the AUC over 10 random (labeled/unlabeled split)
runs. We present the results in charts using the following convention and order for the techniques:








Supervised: Supervised Nave Bayes
ASS1NN: ASSEMBLE-1NN
ASSCLS0: ASSEMBLE-Class0
Samp-Sel: Sample-Select
Reweight: Reweighting
Co-train: Co-training
CC: Common-Component Mixture

5.1 What Is The Effect Of Independence Or Dependence Among Features?
We first investigate the effect of feature independence on semi-supervised learning. For this
purpose we use artificial datasets. We consider the first four artificial datasets from Table 2 that do
not have noise. Since these datasets were constructed with varying amounts of feature
independence and relevance, they offer a diverse test-bed. The results are shown in Figure 2. The
results are averaged for 10 different runs for each of the (labeled, unlabeled)% splits.
The two graphs on the left in Figure 2 show the effect of increasing the amount of feature
independence, whereas the two graphs on the right show the effect of increasing the amount of
feature relevance amongst the independent features. It is worth pointing out that when the amount
of feature independence is increased without increasing the amount of feature relevance (top-left to
bottom-left) adding unlabeled data hurts performance. When the amount of independence and
relevance increases (bottom-right graph) then more semi-supervised learning techniques improve
performance over the supervised learner for different amounts of labeled/unlabeled data. This is
because in that case the Naive Bayes model underlying the semi-supervised techniques is a better
approximation of the true generative model.
Some of the semi-supervised techniques were either comparable or better than learning a
classifier from the labeled datasets. The largest improvement in the average AUC was observed for
the (1,99)% case across all four combinations of feature independence and relevance, as one would
expect. However, (some of) the results were not statistically significant as for some of the (random)
labeled and unlabeled splits, the labeled set contained only one or two examples of class 1, thus
making it a highly skewed training set. In those cases, the performance was worse than random.
This led to a high variance in AUC, causing the confidence intervals to overlap. The lowest gain
observed from using semi-supervised learning techniques over learning a supervised Nave Bayes
classifier is in the case of the artificial datasets with only 30% independent and 30% relevant
347

fiCHAWLA & KARAKOULAS

30_30_00_05

30_80_00_05

1

1
0.8

0.8

0.6
0.6
0.4
0.4
AUC

AUC

0.2

0.2

0
-0.2

0
-0.4
-0.2

-0.6

-0.4

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

(1,99)

Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

80_30_00_05
1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2
AUC

AUC

-0.8

0

-0.2

-0.4

-0.4

-0.6

-0.6

-0.8

-0.8
(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

80_80_00_05

0

-0.2

-1

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

-1

(90,10)

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

Figure 2 : Artificial datasets without noise. Effect of different amounts of feature independence (top to bottom)
and relevance (left to right) at different percentages of labeled and unlabeled data.
348

fiLearning From Labeled And Unlabeled Data

features. This is where the Naive Bayes model achieves relatively high AUC even with only 1/3 rd
of he instances in the labeled set, and addition of more unlabeled data does not help much.
Regarding specific techniques, re-weighting is consistently lower than learning a Nave Bayes
classifier on the labeled set only. In re-weighting, the labels are assigned to the unlabeled data as a
function of P(y|x). With smaller amounts of labeled data the function could be overfitting given the
rich feature space. It is also evident from the much lower performance of the Naive Bayes model on
the testing set. Sample -Select does not perform better than the supervised learner even in the
(1,99)% case.
At higher amounts of independence and relevance, ASSEMBLE-1NN and ASSEMBLE-Class0
are (almost) always better than the supervised technique. Co-training is also sensitive to the amount
of feature relevance and independence. At higher amounts of independence and relevance,
co-training becomes more sensitive to the amount of labeled and unlabeled data in the mixture.
The common-component mixture approach provides a large improvement in the average AUC
across all experiments when the amount of labeled data is very small, i.e. (1,99)%, as one would
expect. The common-component mixture model seems to be more sensitive to the amount of
independent features (top vs. bottom graphs in Figure 2) than the other semi-supervised techniques
considered in this paper. This is probably because features that are not correlated with each other
might not be very useful for clustering. The goal of clustering is to group feature vectors that are
cohesive and distinct (Fisher, 1987). Thus, correlated features are likely to be more relevant for
clustering than independent features. In fact, Talavera (2000) proposes a feature-dependency
measure based on Fisher's results for selecting features for clustering. Thus, the decrease in the
performance of the common-component mixture technique (top vs. bottom graphs in Figure 2) can
be attributed to the increase in the number of independent features.
5.2 How Much of Unlabeled vs. Labeled Data Can Help?
To discuss the trade -off between labeled and unlabeled data, we present the results on the UCI and
NIPS competition datasets. Figure 3 shows the result on the three UCI datasets  satimage,
waveform, and adult. It is evident from the graphs that, as with the artificial datasets, the maximal
gain of semi-supervised learning over supervised learning occurs at smaller amounts of labeled
data. At lower amounts of labeled data, the common-mixture model gives the biggest gain over
Naive Bayes amongst all semi-supervised learning techniques. Also, common component was
relatively stable in its performance as the amount of labeled data is increased. We note that the
waveform dataset is helped by unlabeled data even at the (90,10)% split, when using
ASSEMBLE-1NN, ASSEMBLE-Class0 and common component. For the adult dataset, most of
the semi-supervised techniques do not help for any amount of labeled/unlabeled split. In general,
addition of labeled data helped co-training, re-weighting, and Sample -Select. Co-training noted the
largest improvement compared to other techniques as we increased to the (90,10)% split, namely to
more labeled data.
Figure 4 shows the results on the two NIPS datasets  neuron and horseshoe. For the neuron
dataset, ASSEMBLE-1NN provides a marginal improvement over Nave Bayes. Co-training is the
only technique that helps on the horseshoe dataset. It is worth pointing out that horseshoe has a (50,
50)% split in labeled and unlabeled data.
349

fiCHAWLA & KARAKOULAS

Satimage

Waveform

0.9

1

0.8

0.8

0.7
0.6
0.6
0.4
AUC

AUC

0.5
0.4

0.2

0.3
0
0.2
-0.2

0.1
0

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

-0.4

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

Adult
0.9
0.8
0.7

i

Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

0.6

AUC

0.5
0.4
0.3
0.2
0.1
0

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

Figure 3: Effect of different percentages of labeled and unlabeled data on three UCI datasets.

350

(90,10)

fiLearning From Labeled And Unlabeled Data

0.7

Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

0.6

0.5

AUC

0.4

0.3

0.2

0.1

0

Neuron

Horse-shoe
Data set

Figure 4: Performance comparison on the two datasets from the NIPS semi -supervised learning
competition.

Thus, the amount of labeled and unlabeled data is very much domain dependent. No consistent
observation can be drawn from the above experiments. Nevertheless, on average some of the
semi-supervised techniques  common-component and ASSEMBLE-1NN  show improvement
over the supervised learner for most of the datasets, particularly at the (1,99)% split.
Most of the prior work states that unlabeled data will only help if there is very small amount of
labeled data (Nigam et al., 2000; Cozman et al., 2003). However, we observe that even if there is
very small amount of labeled data, depending on the technique unlabeled data might not help (see
adult in Figure 3). And even if there is a large amount of labeled data unlabeled data can help (see
waveform in Figure 3).
5.3 What Is The Effect Of Label Noise On Semi-supervised Techniques?
We considered this question because label noise could be detrimental to learning. Therefore if the
initial label estimates from a semi-supervised learning technique based on the labeled data are not
accurate then adding unlabeled data might provide minimal gain, if any. To understand the
sensitivity of the semi-supervised techniques to the amount of noise, we added 5%, 10%, and 20%
mislabeling noise to the 30_80_*_05 artificial datasets, where * denotes the level of mislabeling
noise.
Figure 5 shows the effect of noise on each of the semi-supervised techniques. As before, unlabeled
data help only when the amount of labeled data is relatively very small, i.e. (1,99)%. Once again,
the common-component mixture and ASSEMBLE-1NN are the ones exhibiting the biggest
improvement over Naive Bayes, particularly at (1,99)%. At (10,90)%, ASSEMBLE-Class0 also
improves over Nave Bayes. This is observed across all noise levels. It is worth pointing out that for
the datasets with 20% mislabeling noise adding unlabeled data improves performance even at the

351

fiCHAWLA & KARAKOULAS

30_80_05_05

30_80_10_05

1

0.8

0.8

0.6

0.6

0.4

0.4
0.2
AUC

AUC

0.2
0

0
-0.2

-0.2
-0.4

-0.4

-0.6

-0.6
-0.8

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

-0.8

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

30_80_20_05
0.6
0.4
Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweight
Co-train
CC

0.2

AUC

0
-0.2
-0.4
-0.6
-0.8
-1

(1,99)

(10,90)
(33,67)
(67,33)
(Labeled, Unlabeled)%

(90,10)

Figure 5: Effect of noise on artificial datasets at varying percentages of labeled and unlabeled data.

352

(90,10)

fiLearning From Labeled And Unlabeled Data

(10,90)% labeled/unlabeled mix. This is in contrast to what one would expect since mislabeling
noise could have caused the model learned from the labeled data to deviate from the underlying
distribution. Hence, it could have eliminated any potential improvement from adding unlabeled
data. That improvement is probably because the supervised learner is overfitting in the (1,99)% and
(10,90)% datasets with the 20% mislabeling noise, due to the high level of noise and small amount
of labeled data. This overfitting does not occur at lower levels of noise. Thus, in those two datasets
the unlabeled data are acting as a regularizer by preventing the semi-supervised learner from
overfitting.
5.4 What Is The Effect Of Sample-selection Bias On Semi-supervised Techniques?
We used the three datasets with sample selection bias  adult, 30_80_00_05, and KDDCup-98.
Section 4.1 detailed the procedure for introducing MAR label bias in the first two datasets. The
KDDCup-98 dataset comes with a built-in MNAR bias. Table 3 summarizes the sizes and class
distributions of the training set in the three datasets. It is worth noting the different class
distributions in the labeled and unlabeled partitions of the training set.
Datasets

Training Size (Labeled;
Unlabeled)

Class Distribution (Labeled;
Unlabeled)

Adult

(17807; 14754)

(66.75, 33.25; 87, 13)

30_80_00_05

(2033; 5967)

(88.15, 11.85; 97.33, 2.67)

KDDCup-98

(4843; 90569)

(0.5, 99.5; 100, 0)

Table 3: Composition of the training set in the biased datasets

Table 4 shows the effect of sample selection bias on the feature distributions for the adult and
30_80_00_05 datasets. More specifically, we compared the feature distributions between the
labeled and unlabeled sets for each of the two datasets under two different missing data scenarios:
biased and random. We used the Kolmogorov-Smirnov statistical test for continuous features and
the Chi-squared statistical test for nominal features. The results show that, as expected, for MCAR
data there is no statistically significant difference in feature distributions between labeled and
unlabeled sets. In contrast there are differences in feature distributions between the two sets when
there is some bias in the missing data mechanism.

Adult
30_80_00_05

MAR or MNAR data
13 out of 14 different
20 out of 30 different

MCAR data
0 different
0 different

Table 4: Differences in feature distributions between labeled and unlabeled data under different missing data
mechanisms.

353

fiCHAWLA & KARAKOULAS

Figure 6 shows that the supervised learner (Nave Bayes) as well as some of the semi-supervised
techniques perform at a reasonably good level on the two MAR datasets, but perform very poorly on the
MNAR dataset. In particular, co-training and ASSEMBLE-1NN, which did moderately well under the
assumption of MCAR, now exhibit significantly worse performance. In fact, ASSEMBLE-Class0 does
much better than ASSEMBLE-1NN for the 30_80_00_05 dataset. This is because due to the selection
bias the labeled and unlabeled sets have different feature distributions. Thus, the 1NN can be a weaker
classification model for initialization of the class labels in ASSEMBLE when the dataset is biased.
1.2
Supervised
ASS1NN
ASSCLS0
Samp-Sel
Reweigh
Co-train
CC
Probit
Bivar Probit

1

0.8

AUC

0.6

0.4

0.2

0

-0.2

30-30-00-05

Adult
Dataset

KDDCup-98

Figure 6: AUC performance for the biased datasets.

Re-weighting does better than a supervised classifier for the adult and 30_80_00_05 MAR
datasets and fails on the KDDCup-98 MNAR dataset. This is because re-weighting is not suitable
to MNAR datasets since it randomly assigns labels based on the biased posterior probability
distribution. Sample -Select has performance comparable to the supervised classifier. Thus, it
exhibits poor performance on the KDDCup-98 MNAR dataset. As explained in Section 3.2,
Sample-Select is not appropriate for MNAR datasets. The common-component mixture algorithm
does not seem to be sensitive to the sample selection bias, even in the MNAR datasets. The
common-component technique does better than most of the semi-supervised techniques considered
in this paper in the presence of bias. Although the common-component technique scores less than
the supervised classifier on the 30_80_00_05 dataset, its performance is consistent with what was
observed without a sample selection bias. With the exception of bivariate probit the
common-components technique has the best performance amongst the semi-supervised techniques
across the three datasets. The supervised Nave Bayes learner is significantly affected by the
introduced selection bias, and its performance decreases from 0.89 to 0.82 at the (approximately)
corresponding amounts of labeled and unlabeled data for the 30_80_00_05 dataset. There is a
similar effect from the bias in the adult dataset.
354

fiLearning From Labeled And Unlabeled Data

Most noteworthy is the improvement provided by the bivariate probit model over the probit
model in all three datasets. The bivariate probit successfully corrects for the sample selection bias
in the training set. This provides further evidence that just using a biased dataset for prediction on
the general population can be detrimental to the classification performance, especially in the case of
an MNAR dataset. Part of the reason that probit and bivariate probit perform well on the artificial
datasets versus the other techniques is because their assumption of log-normal feature distribution
is satisfied in the data. However, they do not perform as well in the Adult dataset since it consists of
nominal and real-valued features. In the KDDCup-98 datasets, the bivariate model has the best
performance amongst all techniques and the biggest improvement over probit because of the degree
of selection bias in the datasets. This is the only dataset that comes with selection bias built in, a
true MNAR case. The other two datasets only have a MAR bias.
Based on the results above we believe that in the presence of sample -selection bias availability
of more unlabeled data would only help since a better representation of the population could then
be induced. Of course, choosing an appropriate learning technique for such data, like bivariate
probit or even common-component mixture, is also critical.
5.5 Does Semi-Supervised Learning Always Provide An Improvement Over Supervised
Learning?
Figure 7 summarizes the performance of each semi-supervised technique across all datasets. The
y-axis is the average AUC (over the 10 runs) for the datasets that were partitioned into labeled and
unlabeled sets 10 random times, and for the other datasets it is the AUC obtained on the available
labeled-unlabeled split or the biased labeled-unlabeled split. The bar in the box-plot shows the
median performance across all the datasets and labeled-unlabeled partition. As observed in the
figure, ASSEMBLE-1NN and common components never do worse than random. Although, the
median bar of common component is lower than the median bar of the supervised model, its box is
the most compact of all. This highlights the relatively less sensitivity of common components to the
size of labeled and unlabeled data. On average, most of the other techniques are usually better than
only learning from the labeled data  one can observe this from the distribution of the outliers.
Table 5 contains the statistical significance of the comparisons between the various
semi-supervised learning methods and the supervised learning technique (Nave Bayes). To derive
this we grouped the AUCs obtained from the 10 different random runs by each technique across all
the datasets. Thus, we had 100 observation points (10 datasets times 10 random runs) for each
technique. Since we only had one observation point for the NIPS and biased datasets, we did not
include those in this analysis. Using the Kruskal-Wallis statistical test we applied a multiple
comparison procedure for comparing the different techniques. Kruskal-Wallis is a non-parametric
one-way ANOVA test that does not assume that the values are coming from a normal distribution.
The test performs an analysis of variance based on the ranks of the different values and not just their
means. We report the Wins-Losses (W-T-L) counts for each semi-supervised learning technique
against the supervised learning method. At 1% of labeled data, ASSEMBLE-1NN and
common-components mixture have more wins than ties and 1 or 0 loss. The common-component
model does not do as well at larger amounts of labeled data, while at smaller amounts of labeled

355

fiCHAWLA & KARAKOULAS

data  a more prevalent scenario  it seems to help. In general the semi-supervised techniques tie
with the supervised one.

1
0.8
0.6
0.4
0.2
A
U
C

0
-0.2
-0.4
-0.6
-0.8
-1
Supervised ASS-1NN

ASS-CLS0

CoTrain
Technique

Reweigh

Sample-Sel

CC

Figure 7: Box plots summarizing the AUC's obtained by various techniques across all datasets. The y-axis indicates
the AUC spread, and the x-axis indicates the corresponding method. Each box has lines at the lower quartile, median
(red line) and upper quartile. The + symbol denotes outliers.

(Labeled, Unlabeled)%
(1,99)

(10,90)

(33,67)

(67,33)

(90,10)

Method

W-T-L

W-T-L

W-T-L

W-T-L

W-T-L

Assemble -1NN

5-4-1

1-5-4

1-7-2

1-7-2

2-7-1

Assemble -Class0

0-9-1

2-7-1

2-7-1

1-6-3

2-6-2

Re-weighting

0-10-0

0-7-3

1-4-5

0-6-4

0-9-1

Sample-Select

0-10-0

0-10-0

0-10-0

0-10-0

1-9-0

Co-training

0-10-0

0-10-0

0-10-0

2-8-0

4-6-0

Common Component

6-4-0

2-4-4

1-1-8

1-2-7

1-4-5

Table 5: Win-Tie-Loss (W-T-L) tally for the various techniques versus supervised learning at varying
proportions of labeled and unlabeled data.

356

fiLearning From Labeled And Unlabeled Data

6. Conclusions
We presented various existing techniques from machine learning and econometrics for learning
from labeled and unlabeled data. We introduced the concept of sample -selection bias that can be
prevalent in real-world labeled sets and potentially detrimental to supervised learning. For this
reason we also introduced a bias correction technique from econometrics  bivariate probit.
We empirically evaluated each technique under a set of five main objectives (hypotheses) on a
collection of artificial and real-world datasets. For each dataset we varied the proportion of labeled
and unlabeled data and compared the performance of each technique against the performance of a
supervised classifier. We tried to include datasets from various domains with different
characteristics in order to make the study as generalizable as possible. As we observed, different
characteristics of data are favorable to different kinds of semi-supervised learning frameworks.
Moreover, one may need to perform parameter tuning for optimizing the performance of a specific
technique since such optimization was not the focus of this paper.
The main conclusions for each of the five objectives can be enlisted as follows:
 What is the effect of independence or dependence amongst features?
We investigated this objective using artificial datasets. We observed that at fewer
independent and relevant features (30_*) and at higher amounts of labeled data, the
semi-supervised techniques generally do not provide much improvement over the supervised
learning method. Given that only 3 or 4 of the 10 independent features are relevant, the fewer
labeled examples are usually sufficient to learning the classifier. When the amount of feature
independence is increased without increasing the amount of relevant features adding unlabeled
data hurts performance. When the amount of independence and relevance increases then more
techniques, particularly ASSEMBLE-1NN and ASSEMBLE-Class0, improve performance
over the supervised technique. The common-component mixture technique seems to be more
sensitive (less effective) when the amount of independent features increases than the other
semi-supervised techniques. This is because correlated features are more relevant for
clustering.
 How much of unlabeled vs. labeled data can help?
For this objective we used both artificial and real-world datasets. We did not observe a
consistent pattern that at lower amounts of labeled data, unlabeled data would always help
regardless of the technique and domain. We saw that even if there is a small amount of labeled
data unlabeled data might not help depending on the technique and domain. Moreover, even if
there is a large amount of labeled data, unlabeled data can still help (for example, see
Waveform). On average the common-component mixture and ASSEMBLE-1NN helped the
most for the (1,99)% labeled/unlabeled mix. Also, common component was relatively stable in
its performance as the amount of labeled data is increased. That is, its performance did not
deviate much from what it scored at lower amounts of labeled data. As we increased the
amount of labeled data, the wins for ASSEMBLE-1NN and common component changed to
ties and losses, when compared to the supervised learner. On the contrary, the addition of
labeled data helped co-training, re-weighting, and Sample -Select. These techniques ranked
357

fiCHAWLA & KARAKOULAS

very low at the (1,99)% split. Co-training noted the largest improvement compared to other
techniques as we increased to (90,10)% split, namely at more labeled data.
 What is the effect of label noise on semi-supervised learning?
We investigated this objective using artificial datasets. Noise did not have a detrimental
effect on semi-supervised learning. On the contrary, there were fewer semi-supervised models
that performed worse than the single supervised model compared to the no-noise cases. This is
despite the significant reduction in performance of the single Naive Bayes model. We believe
that this is because the single Naive Bayes learner is overfitting as noise level increases. In such
cases unlabeled data are acting as a regularizer by preventing the semi-supervised learner to
overfit. In fact, at 20% mislabeling noise adding unlabeled data improves performance even at
the (10,90)% labeled/unlabeled mix.
 What is the effect of sample-selection bias on semi-supervised learning?
For this objective we used two real-world datasets that we transformed them into MAR and
a real-world MNAR dataset. Selection bias, especially in the MNAR case, can be detrimental
to the performance of the supervised learner and most of the semi-supervised techniques. This
is the type of problem where choosing an appropriate learning technique, such as bivariate
probit that specifically addresses selection bias or even common-component mixture, can be
critical. We observed that ASSEMBLE, which did reasonably well on the datasets for which
the labels were missing completely at random, did not perform well on the biased datasets
(MAR or MNAR). As expected, Sample -Select did not do well on the MNAR dataset. The one
technique that does reasonably well and is again fairly consistent is the common component
technique. Bivariate probit has the best performance amongst all techniques in the MNAR
dataset. In the presence of selection bias more unlabeled data in conjunction with the
appropriate technique seem to always improve performance.
 Which of the semi-supervised techniques considered is consistently better than supervised
learning?
There is no single technique that is consistently better than the supervised Nave Bayes
classifier for all the labeled/unlabeled splits. At the (1,99)% mix, ASSEMBLE-1NN and
common-component mixture were generally better than Nave Bayes. All the other
semi-supervised techniques considered were neither better nor worse than Nave Bayes. As we
increase the amount of labeled data, the wins for ASSEMBLE-1NN and common component
move to ties and losses. Common component had more losses as we added more labeled data,
particularly in the artificial datasets. Sample -select was comparable to the Nave Bayes
classifier, sometimes slightly better. Re-weighting generally did worse than Nave Bayes.
Co-training noted the largest improvement (over Nave Bayes) compared to other techniques
when we increased the percentage of labeled data.
Semi-supervised learning is still an open problem, especially in the context of MNAR. Current
techniques behave very differently depending on the nature of the datasets. Understanding the type
of missing data mechanism and data assumptions is key to devising the appropriate
358

fiLearning From Labeled And Unlabeled Data

semi-supervised learning technique and improving performance by adding unlabeled data. In this
work, we dealt with datasets with varying amounts of imbalance in the class distribution. For this
purpose, we utilized AUC as the performance measure. An extension to our work could be the
utilization of other performance metrics, e.g. classification error, for the analysis of the effects of
unlabeled data.

Acknowledgments
Thanks to Ruslan Salakhutdinov for his help in the experiments. Also, our thanks to Brian
Chambers, Danny Roobaert and the anonymous reviewers for their comments and suggestions on
earlier versions of this paper.

Appendix A
Tables 6 to 9 present the results from the experiments with the different parameter settings of the
semi-supervised techniques: the number of components in the common-component mixture, the
parameter  in ASSEMBLE, and the significance level for the confidence intervals in co-training.

359

fiCHAWLA & KARAKOULAS

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

2
0.5488
0.6870
0.6891
0.6911
0.6927
0.4105
0.5145
0.5150
0.5160
0.5167
0.2253
0.2268
0.2317
0.2379
0.2431
0.1323
0.2222
0.2289
0.2392
0.2454
0.6590
0.5736
0.7465
0.7160
0.7578
0.0190
0.0264
0.0890
0.0575
0.0832
0.7109
0.7135
0.7225
0.7311
0.7348
0.3052
0.5570

Number of Mixture Components
6
12
AUC
0.6904
0.3536
0.8369
0.7245
0.8377
0.7277
0.8754
0.7558
0.8833
0.7458
0.6753
0.2107
0.7454
0.4874
0.7605
0.5344
0.7760
0.5520
0.7881
0.5577
0.3914
0.0982
0.6728
0.1567
0.6954
0.2499
0.7057
0.2516
0.7138
0.2224
0.5054
0.3014
0.6821
0.4553
0.6921
0.4806
0.7310
0.5116
0.7595
0.5099
0.7954
0.6805
0.8210
0.7377
0.8237
0.7523
0.8378
0.7607
0.8432
0.7634
0.7972
0.6919
0.8362
0.8199
0.8454
0.8259
0.8544
0.8295
0.8565
0.8264
0.9074
0.8986
0.9053
0.9384
0.9209
0.9394
0.9334
0.9437
0.9407
0.9455
0.4000
0.3300
0.6500
0.6539

24
0.4529
0.7676
0.8092
0.8057
0.8171
0.3138
0.4834
0.5430
0.5670
0.5831
0.1046
0.2190
0.2674
0.3247
0.3560
0.2713
0.4386
0.5073
0.5302
0.5710
0.7076
0.7453
0.7676
0.7693
0.7807
0.6447
0.8299
0.8438
0.8519
0.8520
0.8758
0.9298
0.9393
0.9443
0.9449
0.3400
0.6600

Table 6. Sensitivity of common components with respect to number of components for different datasets and
labeled/unlabeled split. The bold entries show the components with maximum AUC.

360

fiLearning From Labeled And Unlabeled Data

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

0.4
0.7244
0.8621
0.9295
0.9557
0.9599
0.5538
0.7047
0.8502
0.8989
0.8992
0.2730
0.8824
0.9305
0.9416
0.9409
0.4760
0.8610
0.9046
0.9207
0.9127
0.3059
0.7754
0.8027
0.8210
0.8323
0.6940
0.8566
0.8582
0.8402
0.8165
0.8206
0.8517
0.8998
0.9041
0.8923
0.0397
0.7108

Value of 
0.7
AUC
0.7602
0.8485
0.9111
0.9496
0.9595
0.5673
0.6690
0.8011
0.8927
0.9004
0.3085
0.8741
0.9388
0.9427
0.9452
0.4563
0.8283
0.9011
0.9261
0.9197
0.2955
0.7687
0.7997
0.8192
0.8313
0.6797
0.8545
0.8600
0.8483
0.8214
0.8297
0.8381
0.8947
0.9119
0.8969
0.1778
0.6956

1
0.6989
0.8386
0.9024
0.9440
0.9587
0.5542
0.6534
0.7603
0.8840
0.8990
0.3011
0.8677
0.9409
0.9484
0.9440
0.4531
0.8039
0.8969
0.9289
0.9174
0.2773
0.3055
0.7959
0.8157
0.831
0.7133
0.8517
0.8600
0.8506
0.8258
0.8160
0.8327
0.8815
0.9137
0.8989
0.2347
0.6929

Table 7. Sensitivity of ASSEMBLE-1NN with respect to the value of  for different datasets and
labeled/unlabeled split. The bold entries show the  value with maximum AUC.

361

fiCHAWLA & KARAKOULAS

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

0.4
0.2521
0.9082
0.9397
0.9565
0.9605
-0.1739
0.8398
0.8734
0.8985
0.9007
-0.7405
0.8874
0.9334
0.9410
0.9412
-0.6767
0.8618
0.9073
0.9202
0.9152
0.6875
0.7754
0.8022
0.8202
0.8323
0.2113
0.8285
0.8613
0.8321
0.8107
0.0017
0.8424
0.8902
0.8945
0.8884
0.2254
0.6572

Value of 
0.7
AUC
0.0139
0.8931
0.9302
0.9515
0.9587
-0.3721
0.8105
0.8466
0.8947
0.9025
-0.6104
0.8760
0.9447
0.9477
0.9427
-0.8587
0.8507
0.9112
0.9257
0.9137
0.6743
0.7687
0.7991
0.8182
0.8313
0.2024
0.8255
0.8619
0.8455
0.8162
-0.2123
0.8247
0.8794
0.9057
0.8902
0.1641
0.6754

1
-0.3505
0.8849
0.9235
0.9473
0.9578
-0.6730
0.7946
0.8208
0.8877
0.8979
-0.8355
0.8677
0.9465
0.9499
0.9441
-0.8990
0.8096
0.9112
0.9290
0.9201
0.6573
0.7671
0.7975
0.8148
0.8306
0.2024
0.8192
0.8624
0.8525
0.8245
-0.3601
0.8158
0.8718
0.9067
0.8928
0.3711
0.6436

Table 8. Sensitivity of ASSEMBLE-Class0 with respect to the value of  for different datasets and
labeled/unlabeled split. The bold entries show the  value with maximum AUC.

362

fiLearning From Labeled And Unlabeled Data

Dataset

30_30_00_05

30_80_00_05

80_30_00_05

80_80_00_05

Adult

Satimage

Waveform

Horseshoe
Neuron

(Labeled,
Unlabeled)%
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)
(1,99)
(10,90)
(33,67)
(67,33)
(90,10)

90%
0.3255
0.9338
0.9651
0.9820
0.9852
-0.1281
0.8169
0.8858
0.9132
0.9224
-0.3532
0.6624
0.8734
0.9229
0.9331
-0.5479
0.4828
0.8145
0.8734
0.8839
0.7208
0.7871
0.8050
0.8131
0.8264
0.5213
0.8308
0.8566
0.8625
0.8650
0.8000
0.8400
0.8679
0.8856
0.8931
0.4735
0.6234

Confidence Value
95%
AUC
0.3255
0.9343
0.9571
0.9799
0.9851
-0.1281
0.8129
0.8748
0.9121
0.9227
-0.3532
0.6694
0.8734
0.9229
0.9331
-0.5479
0.4828
0.8145
0.8734
0.8839
0.7208
0.7871
0.8050
0.8131
0.8264
0.5213
0.8346
0.8566
0.8625
0.8650
0.8000
0.8391
0.8681
0.8855
0.8927
0.4735
0.6234

99%
0.3255
0.9316
0.9538
0.9768
0.9844
-0.1281
0.8051
0.8709
0.9099
0.9200
-0.3532
0.7108
0.8734
0.9229
0.9331
-0.5479
0.4828
0.8145
0.8734
0.8839
0.7208
0.7871
0.8050
0.8131
0.8264
0.5213
0.8346
0.8566
0.8625
0.8650
0.8000
0.8377
0.8681
0.8859
0.8930
0.4735
0.6234

Table 9. Sensitivity of Co-training with respect to the confidence value for different datasets and
labeled/unlabeled split. The bold entries show the  value with maximum AUC.

363

fiCHAWLA & KARAKOULAS

References
Bennett, K., Demiriz, A. & Maclin, R. (2002). Exploiting unlabeled data in ensemble methods. In
Proceedings of Sixth International Conference on Knowledge Discovery and Databases, pp.
289-296, Edmonton, Canada.
Blake, C., Keogh, E., & Merz, C.J. (1999). UCI repository of machine learning databases. (URL:
http://www.ics.uci.edu/~mlearn/MLRepository.html)
Blum, A. & Chawla, S. (2001). Learning from Labeled and Unlabeled Data using Graph Mincuts.
In Proceedings of the Eighteenth International Conference on Machine Learning, pp. 19-26,
San Francisco, CA.
Blum, A. & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In
Proceedings of the Workshop on Computational Learning Theory, pp. 92-100, Madison, WI.
Bradley, A.P. (1997). The Use of the Area Under the ROC Curve in the Evaluation of Machine
Learning Algorithms. Pattern Recognition, 30(6), 145-1159.
Chawla, N.V., Bowyer, K.W., Hall, L.O. & Kegelmeyer, W.P. (2002). SMOTE: Synthetic
Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321-357.
Cozman, F., Cohen, I., & Cirelo, M. (2002). Unlabeled data can degrade classification performance
of generative classifiers. In Proceedings Fifteenth International Florida Artificial Intelligence
Society Conference, pp. 327-331, Pensacola, FL.
Cozman, F., Cohen, I., & Cirelo, M. (2003). Semi-supervised learning of mixture models. In
Proceedings of Twentieth International Conference on Machine Learning, pp. 99-106,
Washington, DC.
Crook, J. & Banasik, J (2002). Sample selection bias in credit scoring models. International
Conference on Credit Risk Modeling and Decisioning, Philadelphia, PA.
Dempster, A., Laird, N. & Rubin, D. (1977). Maximum Likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1), 1-38.
Fayyad, U.M. & Irani, K.B. (1993). Multi-Interval discretization of continuous-valued attributes
for classification learning. In Proceedings of Thirteenth International Joint Conference on AI,
pp 1022-1027, San Francisco, CA.
Feelders, A.J. (2000). Credit scoring and reject inferencing with mixture models. International
Journal of Intelligent Systems in Accounting, Finance & Management, 9, 1-8.
Freund, Y. & Schapire, R. (1996). Experiments with a new boosting algorithm. In Proceedings of
the Thirteenth International Conference on Machine Learning, pp. 148-156, Bari, Italy.
Ghahramani, Z. & Jordan, M.I. (1994). Learning from incomplete data. Technical Report 108, MIT
Center for Biological and Computational Learning.
Ghani, R., Jones, R. & Rosenberg, C. (2003). Workshop on the Continuum from Labeled to
364

fiLearning From Labeled And Unlabeled Data

Unlabeled Data in Machine Learning and Data Mining. In Twenty-first International
Conference on Machine Learning, Washington, D.C.
Goldman, S. & Zhou, Y. (2000). Enhancing supervised learning with unlabeled data. In
Proceedings of the Seventeenth International Conference on Machine Learning, pp.327-334,
San Francisco, CA.
Greene, W. (1998). Sample selection in credit-scoring models. Japan and the World Economy, 10:
299-316.
Guyon, I. (2001). NIPS 2001 Workshop on variable and feature Selection.
http://www.clopinet.com/isabelle/Projects/NIPS2001/.
Hand, D.J. (1997). Construction and assessment of classification rules. Chichester: John Wiley and
Sons.
Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive, http://kdd.ics.uci.edu. University of
California at Irvine, Department of Information and Computer Science.
Heckman, J. (1979). Sample selection bias as a specification error. Econometrica, 47, 153-161.
Jacobs, B.A., Jordan, M.I., Nowlan, S.J., and Hinton, G.E. (1991). Adaptive mixtures of local
experts. Neural Computation, 3(1), 79-87.
Joachims, T. (2003). Transductive Learning via Spectral Graph Partitioning. In Proceedings of the
Twentieth International Conference on Machine Learning, pp. 290-297, Washington, DC.
Karakoulas, G & Salakhutdinov, R. (2003). Semi-supervised Mixture of Experts Classification. In
Proceedings of the Fourth IEEE International Conference on Data Mining, pp. 138-145,
Brighton UK.
Kontkanen, P., Myllymaki, P., & Tirri, H. (1996). Constructing bayesian finite mixture models by
the EM algorithm. Technical Report NC-TR-97-003, ESPRIT: Neural and Computational
Learning (NeuroCOLT).
Kremer, S. & Stacey, D. (2001). NIPS 2001 Workshop and Competition on unlabeled data for
supervised learning. http://q.cis.guelph.ca/~skremer/NIPS2001/.
Little, R.J.A & Rubin, D.R. (1987). Statistical Analysis with Missing Data. Wiley: New York.
Miller, D. & Uyar, S. (1997). A mixture of experts classifier with learning based on both labeled
and unlabeled data. Advances in Neural Information Processing Systems 9, pp. 571-578, MIT
Press.
Nigam, K., McCallum, A., Thrun, S., & Mitchell, T. (2000). Text classification from labeled and
unlabeled documents using EM. Machine Learning, 39(2/3), 103-134.
Nigam, K. & Ghani, R. (2000). Analyzing the effectiveness and applicability of co-training. In
Proceedings of Ninth International Conference on Information and Knowledge Management
pp. 86-93.
365

fiCHAWLA & KARAKOULAS

Provost, F. & Fawcett, T. (2001). Robust classification for imprecise environments. Machine
Learning, 42, 203-231.
Provost, F. & Domingos, P. (2003). Tree induction for probability based ranking. Machine
Learning, 52, 199-215.
Quinlan R. (1992) C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.
Seeger, M. (2000). Learning with labeled and unlabeled data. Technical report, Institute for ANC,
Edinburgh, UK. http://www.dai.ed.ac.uk/~seeger/papers.html.
Shahshahni, B. & Landgrebe, D. (1994). The effect of unlabeled samples in reducing the small
sample size problem and mitigating the Hughes phenomenon. IEEE Transactions on
Geoscience and Remote Sensing, 32(5), 1087-1095.
Talavera, L. (2000). Dependency-based feature selection for clustering symbolic data. Intelligent
Data Analysis, 4(1), 19-28.
Swets, J. (1988). Measuring the Accuracy of Diagnostic Systems. Science, 240, 1285-1293.
Zadrozny, B. & Elkan, C. (2000). Learning and making decisions when costs and probabilities
are both unknown. In Proceedings of the Seventh International Conference on Knowledge
Discovery and Data Mining, pp 204-213, San Francisco, CA.
Zhang, T., & Oles, F.J. (2000). A probability analysis on the value of unlabeled data for
classification problems. In Proceedings of Seventeenth International conference on Machine
Learning, pp 1191-1198, Stanford, CA.

366

fiJournal of Artificial Intelligence Research 23 (2005) 123-165

Submitted 10/03; published 2/05

Restricted Value Iteration: Theory and Algorithms
Weihong Zhang

wzhang@cs.wustl.edu

Department of Computer Science
Washington University, Saint Louis, MO 63130 USA

Nevin L. Zhang

lzhang@cs.ust.hk

Department of Computer Science
Hong Kong University of Science & Technology
Clear Water Bay Road, Kowloon, Hong Kong, CHINA

Abstract
Value iteration is a popular algorithm for finding near optimal policies for POMDPs.
It is inefficient due to the need to account for the entire belief space, which necessitates
the solution of large numbers of linear programs. In this paper, we study value iteration
restricted to belief subsets. We show that, together with properly chosen belief subsets,
restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time. We also
apply restricted value iteration to two interesting classes of POMDPs, namely informative
POMDPs and near-discernible POMDPs.

1. Introduction
Partially Observable Markov Decision Processes (POMDPs) provide a general framework
for sequential decision-making tasks where the effects of an agents actions are nondeterministic and the states of the world or environment are not known with certainty. Due to the
model generality, POMDPs have found a variety of potential applications in reality (Monahan, 1982; Cassandra, 1998b). However, solving POMDPs is computationally intractable.
Extensive efforts have been devoted to developing efficient algorithms for finding solutions
to POMDPs (Parr & Russell, 1995; Cassandra, Littman, & Zhang, 1997; Cassandra, 1998a;
Hansen, 1998; Zhang, 2001).
Value iteration is a popular algorithm for solving POMDPs. Two central concepts in
value iteration are belief state and value function. A belief state, a probability distribution
over the state space, measures the probability that the environment is in each state. All
possible belief states constitute a belief space. A value function specifies a payoff or cost
for each belief state in the belief space. Value iteration proceeds in an iterative fashion.
Each iteration, referred to as a dynamic programming (DP) update, computes a new value
function from the current one. When the algorithm terminates, the final value function is
used for the agents action selection. Value iteration is computationally expensive because,
at each iteration, it updates the current value function over the entire belief space, which
necessitates the solution of a large number of linear programs.
One generic strategy to accelerate value iteration is to restrict value iteration, that is, DP
updates, to a subset of the belief space. For simplicity, a subset of the belief space is referred
to as belief subset. Existing value iteration algorithms working with belief subsets include a
family of grid-based algorithms where DP updates calculate values for a finite grid (Lovejoy,
c
2005
AI Access Foundation. All rights reserved.

fiZhang & Zhang

1991; Hauskrecht, 1997; Zhou & Hansen, 2001), and several (maybe anytime) algorithms
where DP updates calculate values for a growing belief subset (Dean, Kaelbling, Kirman,
& Nicholson, 1993; Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet &
Geffner, 2000). By restricting value iteration into a belief subset, the complexity of value
functions is reduced and also DP updates are more efficient. These advantages have been
observed by several researchers (Hauskrecht & Fraser, 1998; Roy & Gordon, 2002; Zhang
& Zhang, 2001b; Pineau, Gordon, & Thrun, 2003).
A fundamental issue in restricted value iteration is how to select a belief subset. The
efficiency of value iteration and the quality of its generated value functions strongly depend
on the selected belief subset. In one extreme case, if the subset is chosen to be a singleton
set, value iteration is efficient, but the quality of value functions can be arbitrarily poor.
In the other extreme case, if the subset is the belief space, the quality of value functions
is retained, but the algorithm is inefficient. There exists a tradeoff between the size of the
belief subset and the quality of value functions.
In this paper, we show that it is indeed possible for value iteration to not only work with
a belief subset but also retain the quality of value functions. This is achieved by deliberately
selecting a belief subset for value iteration. Sometimes, we refer to the algorithm working
with our selected belief subset as subset value iteration. (For distinction, restricted value
iteration refers to value iteration working with any belief subset.) The efficiency of subset
value iteration depends on the size of the selected subset. We characterize a condition to a
priori determine whether the subset is proper 1 with respect to the belief space for a given
POMDP. If this is the case, subset value iteration carries the space and time advantages.
We also study two special POMDP classes, namely informative POMDPs and neardiscernible POMDPs. An informative POMDP assumes that an agent has a good albeit
imperfect idea about world states at any time point. For an informative POMDP, there
exists a natural belief subset so that value iteration restricted to it can be more efficient
than standard value iteration (Zhang & Liu, 1997). A near-discernible POMDP assumes
that an agent has a good idea about world states once in a while. For a near-discernible
POMDP, we propose a restricted value iteration algorithm that starts with a small belief
subset and grows it gradually. The algorithm terminates as a proper tradeoff between size
of the subset and policy quality is found. Because of near-discernibility, the algorithm is
able to find a good tradeoff before the subset grows too large.
The algorithms developed in this paper have been tested in a variety of small maze problems designed to possess various properties as desired, and a number of problems adapted
from existing research or created from our office environment. Our results show that by
exploiting problem characteristics, restricted value iterations can solve larger POMDPs
than standard value iteration. We show how the algorithmic performances vary with the
properties of the selected belief subset for the maze problems. These small problems facilitate exposition of the properties of the chosen belief subsets. Meanwhile, the experiments
provide clues on which POMDP classes are amenable to perspective algorithms.
The rest of the paper is organized as follows. In the next section, we introduce the
POMDP model and value iteration. In the two subsequent sections, we present our subset
value iteration algorithm and analyze its theoretical properties. In particular, in Section
1. Set A is a proper subset of set B if (1) A is a subset of B, and (2) there exists at least one element in B
such that it does not belong to A.

124

fiRestricted Value Iteration: Theory and Algorithms

3, we show how to select the belief subset and how the selected subset is related to the
belief space. In Section 4, we describe the subset value iteration algorithm and discuss why
it is able to achieve near optimality. In Section 5, we examine informative POMDPs and
show how the algorithm that exploits the informativeness is related to the general subset
value iteration (Zhang & Liu, 1997). In Section 6, we examine near-discernible POMDPs
and develop an anytime algorithm. We empirically demonstrate that the algorithm is able
to compute value functions of high quality. In Section 7, we survey related work to this
research.

2. POMDPs and Value Iteration
This section gives a brief overview of the POMDP model and value iteration.
2.1 POMDPs
A POMDP is a sequential decision model for an agent who acts in a stochastic environment
with only partial knowledge about the state of its environment. The set of possible states
of the environment is referred to as the state space and is denoted by S. At each point in
time, the environment is in one of the possible states. The agent does not directly observe
the state. Rather, it receives an observation about it. We denote the set of all possible
observations as Z. After receiving the observation, the agent chooses an action from a set
A of possible actions and executes that action. Thereafter, the agent receives an immediate
reward and the environment evolves stochastically into a next state.
Mathematically, a POMDP is specified by: three sets S, Z, and A; a reward function
r(s, a) for s in S and a in A; a transition probability function P (s0 |s, a); and an observation
probability function P (z|s0 , a) for z in Z and s0 in S. The reward function characterizes the
dependency of the immediate reward on the current state s and the current action a. The
transition probability characterizes the dependency of the next state s0 on the current state
s and the current action a. The observation probability characterizes the dependency of
the observation z at the next time point on the next state s0 and the current action a.
2.2 Policies and Value Functions
Since the current observation does not necessarily fully reveal the identity of the current
state, the agent needs to consider all previous observations and actions when choosing an
action. Information about the current state contained in the current observation, previous
observations, and previous actions can be summarized by a probability distribution over
the state space (Astrom, 1965). The probability distribution is sometimes called a belief
state and denoted by b. For any possible state s, b(s) is the probability that the current
state is s. The set of all possible belief states is called the belief space. We denote it by B.
A policy prescribes an action for each possible belief state. In other words, it is a
mapping from B to A. Associated with a policy  is its value function V  . For each belief
state b, V  (b) is the expected total discounted reward that the agent receives by following
P
t
the policy starting from b, i.e., V  (b) = E,b [ 
t=0  rt ] where rt is the reward received
at time t and  (0<1) is the discount factor. It is known that there exists a policy  

such that V  (b)V  (b) for any other policy  and any belief state b (Puterman, 1994).
125

fiZhang & Zhang

Such a policy is called an optimal policy. The value function of an optimal policy is called
the optimal value function. We denote it by V  . For any positive number , a policy  is
-optimal if V  (b) +   V  (b) for any b in B.
2.3 Value Iteration
To explain value iteration, we need to consider how the belief state evolves over time. Let b
be the current belief state. The belief state at the next point in time is determined by the
current belief state, the current action a, the next observation z. We denote it by  (b, a, z).
For any state s0 ,  (b, a, z) is given by
P

s P (z, s

 (b, a, z)(s0 ) =

0 |s, a)b(s)

P (z|b, a)

,

(1)

P

where P (z, s0 |s, a)=P (z|s0 , a)P (s0 |s, a) and P (z|b, a)= s,s0 P (z, s0 |s, a)b(s) is the renormalization constant. As the notation suggests, the constant can also be interpreted as the
probability of observing z after taking action a in belief state b.
With the concept of belief state, a POMDP model can be transformed into a belief space
MDP as follows.
 The state space is B and the action space is A.
 Given a belief state b and an action a, the transition model specifies the transition
probability as follows.
(

0

P (b |b, a) =

P (z|b, a) if b0 =  (b, a, z) for some z,
0
otherwise.

 Given a belief state b and action a, the reward model specifies immediate reward
P
r(b, a) as r(b, a) = sS b(s)r(s, a).
Due to this reformulation, the task of solving a POMDP can be accomplished by solving
the reformulated MDP. It has been proven that the reformulated MDP has a stationary
optimal policy, which can be found by stochastic dynamic programming (Bellman, 1957;
Puterman, 1994).
Value iteration is a dynamic programming algorithm for finding -optimal policies for
an MDP. It starts with an initial value function V0 and iterates using the following formula:
Vn+1 (b) = max[r(b, a) + 
a

X

P (z|b, a)Vn ( (b, a, z))] b  B

(2)

z

where Vn is referred to as the nth-step value function. It is known that Vn geometrically
converges to V  as n goes to infinity.
For a given value function V , a policy  is said to be V -improving if
(b) = arg max[r(b, a) + 
a

X

P (z|b, a)V ( (b, a, z))]

b  B.

(3)

z

The following theorem tells one when to terminate value iteration given a precision requirement  (Puterman, 1994). The stopping criterion depends on the quantity maxbB |Vn (b)
Vn1 (b)|, which is the maximum difference between Vn and Vn1 over the belief space. The
quantity is often called Bellman residual between Vn and Vn1 (Puterman, 1994).
126

fiRestricted Value Iteration: Theory and Algorithms

Theorem 1 If maxb |Vn (b)  Vn1 (b)|  (1  )/(2), then the Vn1 -improving policy is
-optimal.

Since there are infinitely many belief states, value functions cannot be explicitly represented. Fortunately, value functions that one encounters in the process of value iteration
admit implicit finite representations (Sondik, 1971).
2.4 Technical and Notational Considerations
For convenience, we view functions over the state space as vectors of size |S|. We use lower
case Greek letters  and  to refer to vectors and script letters V and U to refer to sets of
vectors. In contrast, the upper case letters V and U always refer to value functions, that is
functions over the belief space B.
A set V of vectors induces a piecewise linear convex value function (say f ) as follows:
f (b) = maxV b for any b in B where b is the inner product of  and b. For convenience,
we shall abuse notation and use V to denote both a set of vectors and the value function
induced by the set. Under this convention, the quantity f (b) can be written as V(b).
A vector in a set is useless if its removal does not affect the function that the set induces.
It is useful otherwise. A set of vectors is minimal if it contains no useless vectors. Let 
be a vector in set V. It is known that  is useful if and only if there is at least one belief
state b such that b > 0 b, 0  V\{}. Such a belief state is called a witness point of
 because it testifies to the fact that  is useful (Kaelbling, Littman, & Cassandra, 1998).
To determine the usefulness of a vector in a set, it is sufficient to solve one linear program.
To compute a minimal set for a given set V of vectors, it is sufficient to solve |V| linear
programs. The procedure of computing a minimal set for a given set of vectors is often
referred to as pruning a set.
2.5 Finite Representation of Value Functions and Value Iteration
A value function V is represented by a set of vectors if it equals the value function induced
by the set. When a value function is representable by a finite set of vectors, there is a
unique minimal set that represents the function (Littman, Cassandra, & Kaelbling, 1995).
Sondik (1971) has shown that if a value function is representable by a finite set of
vectors, then so are the subsequent value functions derived by DP updates. The process
of obtaining the minimal representation for Vn+1 from the minimal representation of Vn is
usually referred to as dynamic programming (DP) update.
In practice, value iteration for POMDPs is not carried out directly in terms of value
functions themselves. Rather, it is carried out in terms of sets of vectors that represent the
value functions. One begins with an initial set of vectors V0 (often set to a zero-vector).
At each iteration, one performs a DP update on the previous minimal set Vn of vectors
and obtains a new minimal set Vn+1 of vectors. One continues until the Bellman residual
maxb |Vn+1 (b)  Vn (b)|, which is determined by solving a sequence of linear programs, falls
below a threshold.
127

fiZhang & Zhang

3. Belief Subset Selection
In this section, we show how to select a belief subset for value iteration. We describe
a condition determining whether the selected subset is proper w.r.t. the belief space. In
addition, we discuss the minimal representation of value functions w.r.t. the selected subset.
In the next section, we develop the subset value iteration algorithm and show why it is able
to achieve near optimality.
3.1 Subset Selection
Our belief subset selection rests on belief updating. Let the agents current belief be b. Its
next belief state is  (b, a, z) if it performs action a and receives observation z. If we vary
the belief state b in the belief space B, we obtain a set { (b, a, z)|b  B}. Abusing our
notation, we denote this set by  (B, a, z). In words, no matter which belief state the agent
starts with, if it receives z after performing a, its next belief state must be in  (B, a, z).
The union a,z  (B, a, z) takes into account the sets of belief states for all possible combinations of actions and observations. It contains all the belief states that the agent can
encounter. In other words, the agents belief state at any time point must belong to this
set regardless of its initial belief state, performed actions and received observations. We
denote the set by  (B, A, Z) or simply  (B). It is a closed set in the sense that no action
can lead the agent to belief states outside  (B) if the agent starts with a belief state in it.
Furthermore, any belief subset between the set  (B) and the belief space B is closed.
Lemma 1 The set  (B) is closed. Moreover, if  (B)  B0  B, B0 is closed.
As is apparent, the set  (B) is a subset of the belief space B. Its definition is an application
of reachability analysis (Boutilier, Brafman, & Geib, 1998; Dean et al., 1993). Under the
terminology in reachability analysis, the subset  (B, a, z) comprises the one-step reachable
belief states if the agent performs action a and receives observation z, while the subset  (B)
comprises the one-step reachable belief states regardless of performed actions and received
observations. Although the belief subset  (B) is the set of one-step reachable belief states,
an appealing property, to be shown in the next subsection, is that value iteration working
with it can preserve the quality of the generated value functions.
3.2 Subset Representation
Subset representation addresses how to represent the subsets  (B, a, z) and  (B). For this,
we introduce the concept of belief simplex.
Definition 1 Let B = {b1 , b2 , ..., bk } be a set of belief states. A belief simplex  generated
P
P
by B is the set of belief states { ki=1 i bi |i  0 and ki=1 i = 1.0}.
The set B is said to be a basis of the belief simplex . From the definition, the belief
simplex (or simply simplex) is the set of convex combinations of the belief states in the
basis. Following the standard terms in linear algebra, we can also talk about the minimum
basis of a simplex. For convenience, we use notation B to denote a basis of a given simplex
. Additionally, the simplex with the basis {b1 , b2 ,    , bk } is denoted by (b1 , b2 ,    , bk ).
Our result is that for any a and z, the subset  (B, a, z) is a simplex. The intuition
follows. Let the number of states in a POMDP be n. For each i  {1, 2, . . . , n}, bi is a unit
128

fiRestricted Value Iteration: Theory and Algorithms

vector, i.e., bi (s) equals 1.0 for s = i and 0.0 otherwise. For belief state bi , if P (z|bi , a) > 0,
 (bi , a, z) is a belief state in  (B, a, z); if P (z|bi , a) = 0, by the belief update equation
 (bi , a, z) is undefined. In the belief space B, it is trivial to note that any belief state can
be represented as a convex combination of belief states in {b1 , b2 ,    , bn }. Correspondingly,
in the belief subset  (B, a, z), any belief state can be represented as a convex combination
of belief states in { (bi , a, z)|P (z|bi , a) > 0}. Hence, { (bi , a, z)|P (z|bi , a) > 0} is a basis of
 (B, a, z). For convenience, we denote such a basis by B (B,a,z) .
Theorem 2 For any pair [a, z], the subset  (B, a, z) is a simplex.
Proof: See Appendix A.
2
By the above theorem, the subset  (B) is a union of simplices. Although the subset
 (B) is not linearly representable in its own, it is the union of linearly representable sets.
Later in the section, this property is crucial to and will be exploited in finding the minimal
representing sets of value functions w.r.t. the belief subset  (B).
To concretize the ideas on subset representation, we give a POMDP example and visualize the simplices for actions and observations. Before presenting the example, we mention
that we shall use it for additional purposes later on in this paper. First, we shall use it
to show the difference between two conditions determining whether the subset  (B) is a
proper subset of the belief space. Second, we shall use it to demonstrate the fundamental
differences between two restricted value iteration algorithms.
Example The POMDP has three states {s1 , s2 , s3 }, two actions {a1 , a2 } and two observations {z1 , z2 }. We define the transition and observation model for action a1 . These models
for a2 can be defined similarly. To shorten notations, we use pij to denote the transition
probability P (sj |si , a1 ) and qij to denote the observation probability P (zj |si , a1 ). We assume that (1) for any state si , the probability pi1 is equal to pi2 , i.e., pi1 = pi2 ; (2) at each
state si , observations z1 and z2 are received with the same probability, i.e., qi1 = qi2 = 0.5
for each i; and (3) p11 > p21 > p31 . Under these assumptions, the matrix


Pa1 z1

0.5p11

0.5p11
=
0.5(1  2p11 )

0.5p21
0.5p21
0.5(1  2p21 )



0.5p31

0.5p31
.
0.5(1  2p31 )

(4)

Because of the first assumption, the first two rows of the matrix are the same. In the third
row, the probability pi3 is replaced with 1  pi1  pi2 , i.e., 1  2pi1 .
We compute the basis of the belief subset  (B, a1 , z1 ). Let the basis of the belief space
B be the set {(1.0, 0, 0)T , (0, 1.0, 0)T , (0, 0, 1.0)T }. (For a matrix or vector A, AT denotes
its transpose.) For action a1 and observation z1 , the next belief states, denoted by Ai s, are:
A1
A2
A3

=  ((1.0, 0, 0)T , a1 , z1 ) = (p11 , p11 , 1.0  2p11 )T
=  ((0, 1.0, 0)T , a1 , z1 ) = (p21 , p21 , 1.0  2p21 )T
=  ((0, 0, 1.0)T , a1 , z1 ) = (p31 , p31 , 1.0  2p31 )T

(5)

Interestingly, it can be shown that A2 is a convex combination of A1 and A3 . In fact, in
can be verified that
 ((0, 1.0, 0)T , a1 , z1 ) = 1  ((1.0, 0, 0)T , a1 , z1 ) + 2  ((0, 0, 1.0)T , a1 , z1 )
129

fiZhang & Zhang

p31
p21
where 1 = pp21
and 2 = pp11
. Thus 1 + 2 = 1.0. Our third assumption ensures
11 p31
11 p31
that 1 and 2 are greater than 0.0. Because A2 is a convex combination of A1 and A3 , the
three belief states A1 to A3 lie in the same straight line.
Figure 1 visualizes the belief space B in the left and the simplex  (B, a1 , z1 ) in the
right. The right chart is based on these parameters: p11 = 0.5, p21 = 0.4 and p31 = 0.1.
The belief states Ai s are the following: A1 = (0.5, 0.5, 0.0)T , A2 = (0.4, 0.4, 0.2)T and
A3 = (0.1, 0.1, 0.8)T . We see that the belief space is a triangle area and the belief simplex is
a line segment in that area. In the next subsection, we shall return to this point and show
why.

b(s3)

b(s3)

11111111
00000000
(0,0,1.0)11111111
00000000

(0,0,1.0)

11111111
00000000
11111111
00000000
11111111
00000000
b(s1)
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
(1.0,0,0)
11111111
00000000
00000000
(0,0,0) 11111111
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
11111111
00000000
00000000
11111111

(0,1.0,0)

* A3
b(s1)
A2
(1.0,0,0)
*
(0,0,0)
* A1

b(s2)

(0,1.0,0)

b(s2)

Figure 1: A graphical representation of belief space and belief simplex. See text for explanations.
To show the belief subset  (B), we continue to define the transition and the observation
models for action a2 .2 We may follow the three assumptions for a1 to define these models
for a2 , but give a different set of transition probabilities so that a2 differs from a1 .
By the second assumption that observation z1 and z2 are received with the same probability, the matrix Pai ,z1 is identical to Pai ,z2 for i  {1, 2}. So, the simplex  (B, ai , z1 ) is
identical to  (B, ai , z2 ) for each i. As such, the subset  (B) consists of two line segments in
the entire belief space.
2
3.3 Belief Subset and Belief Space
We discuss the relationship between the set  (B) and the belief space B. Since the set  (B)
is a union of simplices, it helps to show how each simplex  (B, a, z) is related to the belief
space B. For an action a and observation z, it turns out that a matrix derived from the
transition and observation models plays a central role in determining the simplex. Such
a matrix, denoted by Paz , is of dimension |S|  |S| and its entry at (s, s0 ) is the joint
probability P (s0 , z|s, a), i.e.,





Paz = 

P (s01 , z|s1 , a)
P (s02 , z|s1 , a)

0
P (sn , z|s1 , a)

P (s01 , z|s2 , a)
P (s02 , z|s2 , a)

0
P (sn , z|s2 , a)






P (s01 , z|sn , a)
P (s02 , z|sn , a)

0
P (sn , z|sn , a)




.


2. Other components of the POMDP do not affect our discussions here and are omitted for convenience.

130

fiRestricted Value Iteration: Theory and Algorithms

The matrix can be used to relate the next belief  (b, a, z) and the current b. If b and
 (b, a, z) are viewed in column vector form, the belief update Equation (1) can be rewritten
1
as  (b, a, z) = p(z|b,a)
Paz b. Hence,  (b, a, z) is a transformation of b and the matrix Paz is
called the transformational matrix. The following lemma characterizes a condition under
which the simplex  (B, a, z) is the same as the belief space B.
Lemma 2 For any [a, z], there exists a bijection between the simplex  (B, a, z) and the
space B if the matrix Paz is invertible 3 .
1
Paz b and  (B, a, z) is the set of the
This can be seen from the fact  (b, a, z) = p(z|b,a)
transformed belief states from the belief space. Consequently, the simplex  (B, a, z) is a
proper subset of B if the matrix Paz is degenerate. We note that the matrix Paz is degenerate
if there exists a state s0 such that P (z|s0 , a) = 0.0, i.e., there is a state such that the agent
can never receive the observation z (if action a is performed). This is because all the entries
in the row corresponding to s0 are 0.0 in the matrix. In this case, by the lemma, the set
 (B, a, z) must be a proper subset of the belief space B.

Corollary 1 If there is a state s such that P (z|s, a) = 0.0, the set  (B, a, z) is a proper
subset of the belief space B.
However, that  (B, a, z) is a proper subset does not necessarily imply that there exists
a state s such that P (z|s, a) = 0.0. Consequently, to determine if a belief simplex is
proper, the above corollary provides a sufficient condition; in contrast, Lemma 2 provides
a sufficient and necessary condition. This will be illustrated by continuing our discussions
on the POMDP example.
Example (Continued) We consider the simplex  (B, a1 , z1 ). By the second assumption,
z1 can be observed at any state. So, there does not exist a state s such that P (z1 |s, a1 ) = 0.0.
On the other hand, the matrix Pa1 z1 is degenerate because it has the same two rows. By
Lemma 2, the simplex  (B, a1 , z1 ) is a proper set of the belief space. In fact, as seen in
Figure 1, it is a line segment, which can be viewed as a degenerate belief space.
2
We proceed to discover the relationship between the belief set  (B, a, z) and the belief
space B. Since  (B, a, z) is the union of the simplices  (B, a, z), it is a proper subset of the
belief space if so is each simplex. In turn, this requires that each transformational matrix
is degenerate.
Theorem 3 The subset  (B) is a proper subset of belief space B only if the transformational
matrices for all actions and observations are degenerate.
3.4 Subset Value Functions
We discuss value functions whose domains are belief subset  (B, a, z) or  (B). For simplicity,
we refer to them as subset value functions. The problem we will examine is, given a set of
vectors representing a subset value function, how to compute a minimal set w.r.t. a belief
subset. We first consider the case where the subset is a simplex.
3. A matrix is invertible if its determinant is non-zero. It is degenerate otherwise.

131

fiZhang & Zhang

In order to calculate a minimal set of vectors, one needs to determine the usefulness of
a vector in a set w.r.t. the simplex. Let  be a vector in set V and the simplex be  (B, a, z).
The vector  is useful w.r.t.  (B, a, z) if and only if there is a belief state b in the simplex
such that   b    b + x where x is a sufficiently small positive number and  is a vector
in the set V{}. Moreover, if such a belief state b exists, since it is in the simplex, b must
P
be representable by the belief states in the basis B (B,a,z) , i.e., b = i i  (bi , a, z). If we
P
replace b by i i  (bi , a, z) in   b    b + x, the condition of determining s usefulness
is equivalent to this: whether there exists a series of nonnegative numbers i s such that for
any vector  in V,


X

i  (bi , a, z)   

i

X

i  (bi , a, z) + x.

i

Rewriting the above inequality, we have
X

[   (bi , a, z)] i 

i

X

[   (bi , a, z)] i + x.

(6)

i

To determine s usefulness, the procedure simplexLP in Table 1 is used. When the
optimality of the linear program is reached, one checks its objective x. If it is positive, there
exists a belief state in belief simplex  (B, a, z) such that at this belief state  dominates
P
other vectors. The belief state is a witness point of . It is represented as i i  (bi , a, z)
where i s are the solutions (values of variables) of the linear program. In this case, the
P
belief state i i  (bi , a, z) is returned. For other cases, no belief state is returned and  is
a useless vector.
simplexLP(, V, B (B,a,z) ):
1. Variables: x, i for each i
2. Maximize: x.
3. Constraints:
P
P
4.
i [   (bi , a, z)] i 
i [   (bi , a, z)] i + x for   V  {}
and for each i,  (bi , a, z)  B (B,a,z)
P
5.

i i = 1, i  0 for i.
simplexPrune(V, B (B,a,z) ):
1. U  V, V  
2. For each  in U
3.
b  simplexLP(, U, B (B,a,z) )
4.
If b 6= null
5.
V  V  {}
6. Return V
Table 1: The procedure to compute the minimal set of vectors over a simplex
To determine a vectors usefulness in a given set, one linear program needs to be solved.
If a vector is useless, its removal does not change the value function that the set induces.
132

fiRestricted Value Iteration: Theory and Algorithms

Therefore, to compute a minimal set for a given set V w.r.t. a simplex, one needs to solve
|V| linear programs.
This procedure is implemented in simplexPrune of Table 1. Its input has two arguments:
a set of vectors V 4 and a basis of the simplex B (B,a,z) . A set U is initialized to be the
set V and the set V to be empty at line 1. Useful vectors are added to the set V in the
sequel. For each vector in set U, at line 3 the procedure simplexLP is called to determine its
uselessness. If it returns a belief state, the vector is added to the set V at line 5. Eventually,
the set V becomes the minimal representation of U w.r.t. simplex  (B, a, z).
To compute a minimal set of vectors w.r.t. the subset  (B), one needs to determine a
vectors usefulness w.r.t. the subset. In turn, one needs to determine its usefulness w.r.t.
each simplex. Again, let the set be V and the vector be . If  is useful w.r.t. a simplex, it
must be useful w.r.t. the subset. However, if it is useless w.r.t. a simplex, it may be useful
w.r.t. another simplex. Hence, for a simplex, if  has been identified as useful, there is no
need to check it again for subsequent simplices. After all the simplices have been examined,
if  is useless w.r.t. all simplices, it is useless w.r.t. the subset. By removing all useless
vectors w.r.t. the subset, one obtains the minimal set.

4. Subset Value Iteration
In this section, we first describe the value iteration algorithm in belief subset  (B). We
then show that the algorithm is able to achieve near optimality. Finally, we analyze its
complexity and report empirical studies.
4.1 Belief Subset MDP
Because the subset  (B) is closed, we are able to define a so-called belief subset MDP (or
simply subset MDP). Its state space is the chosen subset  (B) and other components are
the same as those in the MDP transformed from the original POMDP (Section 2.3). The
only difference between the two MDPs lies in their state spaces: the state space of the belief
subset MDP is a subset of the state space of the belief space MDP.
4.2 Subset DP Updates
 (B)

By MDP theory, the subset MDP admits the following DP update equation where Vn
represents its nth-step value function.
 (B)

Vn+1 (b) = max{r(b, a) + 
a

X

P (z|b, a)Vn (B) ( (b, a, z))} b   (B).

(7)

z
 (B)

Following the equation, an implicit DP update computes the minimal set Vn+1 represent (B)

 (B)

 (B)

ing value function Vn+1 from Vn
representing Vn . Note that the domains of value
functions are belief subset  (B). For simplicity, such a step is called subset DP update.
Implicit subset DP updates can be carried out as standard DP updates. Here, we present
a two-pass algorithm due to its conceptual simplicity (Monahan, 1982). It constructs the
4. For simplicity, we assume that the set V does not contain duplicate vectors. Duplicates can be removed
by a simple componentwise check.

133

fiZhang & Zhang

next set of vectors in two steps  an enumeration step enumerating all possible vectors and
a reduction step removing useless vectors. In the following, we focus on the enumeration
step. For the reduction step, since the usefulness of a vector is w.r.t. the subset  (B), the
techniques in the preceding section can be used.
 (B)
 (B)
Given a set Vn , a vector in the representing set of Vn+1 can be defined by a pair of
 (B)

action and a mapping from the set Z of observations to the set Vn . To be precise, for an
action a and a mapping , a vector, denoted by a, , is defined as follows 5 . For each s  S,
a, (s) = r(s, a) + 

XX
z

P (s0 |s, a)P (z|s0 , a)z (s0 )

(8)

s0

where z is the mapped vector for observation z.
If we enumerate all possible combinations of actions and mappings above, we can define
various vectors. These vectors form a set
{a, |a  A,  : Z  Vn (B) & z, z  Vn (B) }.

(9)
 (B)

 (B)

The set is denoted by Vn+1 . By MDP theory, it represents the value function Vn+1 if the
 (B)

set Vn

 (B)

represents Vn

.

 (B)

 (B)

 (B)

Lemma 3 The set Vn+1 represents value function Vn+1 if Vn

 (B)

represents Vn

.

The above DP update works in a collective fashion in that it directly computes value
functions over  (B). An alternative way to conduct DP updates is to compute value functions for individual simplices one by one. The rationale is that, by letting a DP update
work with the finer-grained belief subsets, it could be more efficient than its collective ver (B,a,z)
sion. A DP update in an individual fashion constructs a collection {Vn+1 } of vector sets
 (B,a,z)

 (B,a,z)

 (B)

from a given collection {Vn
|a  A, z  Z} where each Vn
represents Vn
in the
 (B,a0 ,z 0 )
simplex  (B, a, z). We consider how to construct a set Vn+1
for one simplex  (B, a0 , z 0 ).
 (B,a0 ,z 0 )

Likewise, a vector a, in Vn+1
can be defined by an action a and a mapping . The
fact that  (b, a, z) must be in  (B, a, z) for any b implies that for any z, z can be restricted
 (B,a,z)
to a vector in the set Vn
. By altering actions and mappings, one obtains the following
set:
{a, |a  A,

 : Z  a,z Vn (B,a,z) , & z, z  Vn (B,a,z) }.

(10)

It differs from (8) in that for an observation z, the mapped vector by  is restricted to the
 (B,a,z)
 (B,a0 ,z 0 )
set Vn
. The above set is denoted by Vn+1
. To obtain its minimal representation,
one removes useless vectors from the set w.r.t. the simplex  (B, a0 , z 0 ). The value function
 (B,a0 ,z 0 )
 (B)
that the set Vn+1
induces is equal to the value function Vn
in the belief simplex
 (B, a0 , z 0 ).
5. The procedure of defining a vector actually constructs an (n+1)th-step policy tree. (See, e.g., Zhang
and Liu 1997, for details.)

134

fiRestricted Value Iteration: Theory and Algorithms

 (B,a,z)

Lemma 4 For any action a and observation z, each set Vn+1
function
plex.

 (B)
Vn+1

in the simplex  (B, a, z) if each

 (B,a,z)
Vn

represents the same value
 (B)

represents Vn

in the same sim-

Although subset DP updates can be carried out either collectively or individually, they
are essentially equivalent in terms of value functions induced.
 (B,a,z)

Theorem 4 Let U = a,z Vn+1

 (B)

. For any b   (B), U(b) = Vn+1 (b).

It is worthwhile to note that for two pairs of actions and observations, the simplices
 (B, a1 , z1 ) and  (B, a2 , z2 ) might not be disjoint. A few remarks are in order for this
 (B,a ,z )
case. First, by Theorem 4, for any b in the intersection of the simplices, Vn+1 1 1 (b) =
 (B,a ,z )

 (B)

Vn+1 2 2 (b). This is because both sets represent Vn+1 in  (B). Second, if a subset DP
update is carried out individually, it may generate more vectors than its collective version.
 (B,a ,z )
 (B,a ,z )
This is because the two sets Vn+1 1 1 and Vn+1 2 2 may contain duplicate vectors.
Finally, we note that to achieve computational savings, sophisticated algorithms for
standard DP updates can be applied to subset DP updates. Let us take incremental pruning,
one of the most efficient algorithms, as an example (Cassandra et al., 1997; Zhang & Liu,
1997). In standard incremental pruning, all the pruning operations are w.r.t. the belief
space; however, when it is used in subset DP updates, all the pruning operations are w.r.t.
the belief subsets.
4.3 Analysis
We analyze several theoretical properties of the subset value iteration algorithm. Our main
results include: value functions generated by subset value iteration are equivalent to those
by standard value iteration in some sense; to achieve near optimality, value iteration needs
to account for at least the belief subset  (B); the value function generated by subset value
iteration can be used for near optimal decision-making in the entire belief space if the
algorithm is appropriately terminated.
4.3.1 Belief Subset, Value Functions and Value Iterations
 (B)

Subset value iteration generates a series {Vn } of value functions. If its initial value
 (B)
function V0
is the same as the initial V0 of standard value iteration in  (B), subset value
iteration generates the same series of value functions as standard value iteration in  (B).
 (B)

Theorem 5 If V0
any b in  (B).

 (B)

(b) = V0 (b) for any b in  (B), then Vn

(b) = Vn (b) for any n and

Proof: We first consider one DP update computing value function Vn+1 from the current
Vn by DP Equation (2). In its right hand side, since  (b, a, z) must belong to the subset
 (B), the notation Vn ( (., ., .)) can be interpreted as a value function over subset  (B) rather
than belief space B. Comparing DP Equation (2) for the belief space B and Equation (7)
 (B)
for belief subset  (B), we see that Vn+1 and Vn+1 represent the same value function in  (B)
 (B)

if so do Vn and Vn

.
135

fiZhang & Zhang

The theorem is true for n = 0 by the given condition. It is true for n > 0 by induction.
2

 (B)

More interestingly, the value function Vn
at step n can be used to derive the value
function Vn+1 in standard value iteration. To see why, we first note that a subset value function V  (B) can be used to define a value function (say V ) by one-step lookahead operation
as follows:
V (b) = max{r(b, a) + 
a

X

P (z|b, a)V  (B) ( (b, a, z))}  b  B.

(11)

z

The so-defined V is called V  (B) -improving value function. Second, comparing Equations
 (B)
 (B)
(11) and (2), we see that the Vn -improving value function is actually Vn+1 if Vn
is
equal to Vn in  (B).
Consequently, although subset value iteration works with  (B), value functions generated in standard value iteration can be derived. In this sense, we say  (B) is a sufficient
belief subset since it enables subset value iteration to preserve standard value functions
without loss.
Since subset value iteration retains the quality of value functions, it can be regarded as
an exact algorithm. One interesting question is, if value iteration intends to retain quality,
can it work with a proper subset of  (B)? In general, the answer is no. The reason follows.
 (B)
To compute Vn+1 , one needs to keep values Vn
for belief states in  (B). Otherwise, if
one accounts for a proper subset B0 of  (B), it can be proven that there exists a belief state
b in B, an action a and an observation z such that  (b, a, z) does not belong to B0 . Its
known that the value update of Vn+1 (b) depends on the values for all possible next belief
 (B)
states. Due to the unavailability of Vn ( (b, a, z)), the value Vn+1 (b) cannot be calculated
exactly. Consequently, if value iteration works with a proper subset of  (B), it cannot be
exact. In other words, it should be an approximate algorithm. To make it be exact, value
iteration needs consider at least  (B). In this sense, the subset  (B) is said to be a minimal
sufficient set.
Informally we use Figure 2 to illustrate the relationship between belief subsets and value
iteration. In the figure, circles represent belief sets. The minimum belief subset for value
iteration to retain quality is  (B), while the maximum subset is the belief space B itself. If
value iteration works with belief subset B0 (denoted by dashed circles) between  (B) and
B, its quality is also retained. However, if it works with a proper belief subset of  (B), in
general it is unable to retain the quality of value functions.

 (B)

B

B

Figure 2: The relationship between belief subsets and value iteration

136

fiRestricted Value Iteration: Theory and Algorithms

4.3.2 Stopping criterion and decision making
Subset value iteration starts with an initial value function. As it continues, the Bellman
 (B)
 (B)
residual maxb (B) |Vn (b)  Vn1 (b)|, the maximum difference between two value functions over the subset  (B), must become smaller. By MDP theory, if the quantity falls below
 (B)
(1  )/(2), the value function Vn1 is -optimal w.r.t. the subset MDP. In the following,
we show that the -optimality can be extended to the entire belief space by appropriately
terminating the subset value iteration algorithm.
 (B)
Let the output value function be Vn1 . It can be used to define a policy for any belief
 (B)

state in B as in Equation (3) where V is replaced by Vn1 . The policy  is said to be
 (B)

Vn1 -improving. Note that the policy prescribes an action for any belief state in the belief
space. The following theorem tells one when to terminate subset value iteration such that
 (B)
the Vn1 -improving policy is -optimal over the belief space.
 (B)

 (B)

 (B)

Theorem 6 If maxb (B) |Vn (b)Vn1 (b)|  (1  )/(22 |Z|), then the Vn1 -improving
policy is -optimal over the entire belief space B.
Proof: See Appendix A.
2
This theorem is important for two reasons. First, although subset value iteration outputs
a subset value function, -optimal value functions over the entire space can be induced by
 (B)
one-step lookahead operation. Second, it implies that the Vn1 -improving policy is optimal if the condition is met. We know that  (B) consists of all possible belief states the
agent encounters after the initial belief state. However, we have no assumption about the
initial belief state. It may or may not belong to this set. The theorem means that the agent
is still able to select a near optimal action for an initial belief state even if it is not in the
subset. In fact, the agent can always select near optimal action for any belief state in the
entire belief space.
Finally, we note that to guarantee the -optimality, when compared with the condition
in Theorem 1, subset value iteration uses a more restrictive condition. For convenience, it
is sometimes called a strict stopping criterion. In contrast, the condition in standard value
iteration is called a loose stopping criterion.
4.4 Complexity
To put in use for a POMDP, the subset value iteration algorithm would take two steps:
determining if the algorithm can bring about savings in time and then running the subset
value iteration if it can. The first step needs to compute |A||Z| determinants |Paz |. Since
the complexity of computing |Paz | is |S|3 , the first step has the complexity of O(|A||Z||S|3 ).
This is the polynomial part of the complexity of subset value iteration. The second step
is much harder than the first step. It is known that finding the optimal policy for even a
simplified finite horizon POMDP is PSPACE-complete (Papadimitriou & Tsitsiklis, 1987;
Burago, de Rougemont, & Slissekno, 1996; Littman, Goldsmith, & Mundhenk, 1998). Recently, it has been proven that finding the optimal policy for an infinite-horizon POMDP
is incomputable (Madani., Hanks, & Condon, 1999).
We compare the subset value iteration with standard value iteration. Standard DP
updates improve values for the space B, while subset DP updates improve values for the
137

fiZhang & Zhang

 (B)

subset  (B). If the initial set V0
is equal to the initial set V0 in standard value iteration,
 (B)
 (B)
because  (B) is a subset of B, the vectors in V1
must be in V1 , and V1
is a subset of
 (B)
V1 . Inductively, Vn
is a subset of Vn for any n. This analysis suggests two advantages
of subset value iteration if the subset  (B) is a proper subset of the belief space B. First,
fewer vectors are needed to represent a value function over a belief subset. This is the
representational advantage in space. For Vn+1 , its size can be as large as |A||Vn ||Z| . For
 (B)

 (B) |Z|

Vn+1 , its size can be as large as |A||Vn | . Clearly, subset DP update generates fewer
vectors. Second, fewer vectors means lesser degree of time complexity since computing
vectors needs to solve linear programs. This is the computational advantage in time.
However, the advantages strongly depend upon the size of the subset  (B). If each
simplex  (B, a, z) is the same as the belief space B and DP updates are conducted in an
individual fashion, subset DP updates could be |A||Z| times slower than standard DP
updates. This is the worst case complexity. Fortunately, by discussions in the previous
section (Theorem 3), we know that given a POMDP we are able to determine whether the
selected subset  (B) is a proper subset of the belief space before solving it.
Although Theorem 3 gives a condition to determine when the subset value iteration is
more efficient than standard value iteration for a POMDP, it does not answer the question
that how much savings the algorithm can bring about, which turns out to be a very difficult
problem in theoretical analysis. The difficulty lies in not only the size of the set  (B) but
also the vectors representing the step and the optimal value functions. Let us assume that
 (B) is a proper subset of the belief space. We can imagine at least two cases. In one case,
if at each iteration the step value function has very few useful vectors in the subset  (B),
the subset value iteration can be very efficient. In the other case, if at each iteration the
step value function has all useful vectors in the subset  (B), subset value iteration has the
same complexity as standard value iteration in an asymptotic sense. In general, given a
POMDP, it is difficult to predict how these vectors scatter around the belief subsets and the
belief space. Consequently, it is hard to predict how much saving the subset value iteration
algorithm can bring about for a POMDP without solving it.
4.5 Empirical Studies
We present our empirical results on two variants of a designed maze problem and the
problems in a standard test-bed in this subsection. Some common settings to all experiments
in the paper are as follows. The experiments are conducted on an UltraSparc II machine
with dual CPUs and 256MB of RAM. Our codes are written in C and executed under
a UNIX operating system Sola 2.6. When solving linear programs, we use a commercial
package CPLEX V6.0. The discount factor is set at 0.95 and round-off precision is set
at 106 . When not stated otherwise, the quality requirement  is set to 0.01. We use
incremental pruning to compute representing sets of value functions over the belief space
or belief subsets.
We compare the performances of subset and standard value iteration. For simplicity,
we denote them respectively by ssVI and VI. At each iteration, we compare VI and ssVI
in two measures: sizes of sets representing value functions and total time of DP updates.
138

fiRestricted Value Iteration: Theory and Algorithms

4.5.1 The Maze Problem
The maze problem is specified in Figure 3. There are 10 locations and the goal is location 9.
A robot agent can execute four move actions to change its position, optionally a look
action to observe its surroundings and a declare action to announce its success of goalattainment. The move actions can achieve their intended effects with probability 0.8,
but might have no effects with probability 0.1 (the agents position remains unchanged) or
lead to overshooting with probability 0.1. Moving against maze walls leaves the agent at its
original location. Other actions do not change the agents position. At each time point, the
robot receives a null observation giving no useful information at all, or reads four sensors
so as to reason about its current position. Each sensor informs the robot whether there is a
wall or nothing along a direction. In the figure thick lines stand for walls and thin lines for
nothing (open). For instance, if the agent is at location 2, ideally a string owow (in the
order of East, South, West and North) is received. Specific parameters will be instantiated
in relevant empirical analysis. The robot is required to maximize the infinite discounted
sum of rewards.

1

2

9

10

7

8

3

4

5

6

Figure 3: A maze problem
Two variants of the above maze are designed to test ssVI and VI. They are denoted by
maze1 and maze2. For maze1, ssVI is more efficient; for maze2, ssVI is less efficient.
Case I:  (B)  B
The maze1 problem has a state space of 10 locations, an action space of size 5 (four
move and one declare) and an observation space of size 6 (strings of four letters). An
ideal string is received with certainty after any action is performed. When the agent declares
goal at location 9, it receives a reward of 1 unit; if it does so at location 10, it receives a
reward of 1. Other combinations of actions and observations lead to no reward.
We collect the results in Figure 4. The first chart in the figure depicts the total time of
DP updates in log-scale for VI with the loose stopping criterion and ssVI with the strict one
(Section 4.3.2). To compute a 0.01-optimal value function, VI took 20,000 seconds after 162
iterations while ssVI with strict stopping criterion took 900 seconds after 197 iterations.
We note that ssVI needs more iterations but it still takes much less time. The performance
difference is big. Moreover, more iterations means that the value function generated by
ssVI is closer to optimality.
This is not a surprising result if we take a look at the matrix Paz for an action a
and observation z. We know that the matrix impacts the size of the simplex  (B, a, z).
The dimension of the matrix is 10  10. The entry of Paz at (i, j) is the product of the
transition probability P (sj |si , a) and observation probability P (z|sj , a). Let us assume that
139

fisizes of representing sets(logscale)

Zhang & Zhang

CPU seconds(logscale)

100000
10000
1000
100
10

VI
ssVI

1
0.1
0.01
0 20 40 60 80 100 120 140 160 180 200
number of iterations

10000

1000

100

10

VI
ssVI

1
0 20 40 60 80 100 120 140 160 180 200
number of iterations

Figure 4: Comparative studies for VI and ssVI on Maze1
the observation be owow. Hence, the possible locations may be 2 or 5. Regardless of actions
executed, only entries in row 2 and 5 of Paz can be non-zero. Therefore, the matrix is highly
sparse and non-invertible and the simplex  (B, a, z) is much smaller than B. This analysis
holds similarly for other combinations of actions and observations. Hence, ssVI accounts
for only a small portion of the belief space. This explains why ssVI is more efficient than
VI. In addition, we expect that the sets generated by ssVI are much smaller than those by
VI.
This is confirmed in the second chart of the figure. It depicts the sizes of the sets
representing the value functions generated by ssVI and VI at each iteration. When counting
 (B)
the size of Vn , we collect the sum of the sizes of representing sets over |A||Z| simplices.
We note that at the same iteration VI always generates much more vectors than ssVI. The
sizes at both curves increase sharply at first iterations and then stabilize. The size for VI
reaches its peak of 2466 at iteration 11 and the maximum size for ssVI is 139 at iteration
10. This size in VI is about 20 times many as that in ssVI. This is a magnitude consistent
with the performance difference. After the sizes stabilize, the sizes of the sets generated by
VI are around 130 and they are around 50 in ssVI.
Case II:  (B) = B
The problem maze2 is designed to show that ssVI could be less efficient than VI when
the selected belief subset  (B) is equal to the belief space B. The problem has a state space
of 10 locations, an action space of size 6 (four moves, one stay and one declare) and an
observation space of size 7 (6 strings and a null telling nothing). The action stay does not
change the agents position. maze2 has more complications on the observation model. Due
to hardware limitations, after a move action, with a probability of 0.1, the agent receives a
wrong report where the string owow is collected as owww and woww as wowo. If the declare
action is executed, the agent always receives a null observation. In addition, if the agent
executes stay, it receives either a null observation with probability 0.9 or the ideal string
about the surrounding locations with probability 0.1.
The reward model is accordingly changed to reflect new design considerations. We
assume that the agent needs to pay for its information about states. For this purpose, if the
140

fiRestricted Value Iteration: Theory and Algorithms

sizes of representing sets(logscale)

agent executes stay, it really does nothing and thus yields no cost (i.e., negative reward).
In contrast, the move actions always cause a cost of 2. Depending on the locations at
which it executes declare, it receives rewards or costs: if the location is state 9, it receives
a reward of 1; if state 10, it receives a cost of 1; otherwise, it leads to no rewards. The
stay action is attractive in that it yields no cost but it leads to an useful observation about
states with a small likelihood.
The empirical results are collected in Figure 5. First, we note both VI and ssVI are
able to run only 11 iterations within a reasonable time limit (8 hours). The first chart in
the figure presents the time costs along iterations. To run 11 iterations, ssVI takes 53,000
seconds while VI takes around 30,900 seconds. Therefore, ssVI is slower than VI for this
problem. However, the magnitude of performance difference is not big. To explain this, let
us consider the matrix Paz for action stay and observation null. The transition matrix
is an identity and each state can lead to the null observation with probability of 0.9 if
stay is executed. Therefore, the matrix Paz is invertible and the simplex  (B, a, z) is the
same as the belief space B. Because ssVI needs to account for additional simplices for other
combinations of actions and observations, ssVI must be less efficient than VI. This explains
the performance difference in time between ssVI and VI.

CPU seconds(logscale)

100000
10000

ssVI
VI

1000
100
10
1
0.1
0.01
0

2

4
6
8
10
number of iterations

12

10000

ssVI
VI

1000

100

10

1
0

2

4
6
8
number of iterations

10

12

Figure 5: Comparative studies for VI and ssVI on maze2
It is also anticipated that ssVI should generate more vectors than VI at the same
 (B)
iteration because the size of Vn
is defined to be sum of the individual sets in it. This is
confirmed and demonstrated in the second chart of Figure 5. The curve for ssVI is always
on the upper side of that for VI. For the 11th iteration, ssVI generates 3,300 vectors and
VI generates around 1,700 vectors.
4.5.2 More Experiments on the Test-Bed
To validate the performance of the subset value iteration over different problem domains,
we collected the results of the algorithm on the standard test-bed maintained by Tony
Cassandra 6 . In the literature, the eight problems are commonly referred to as 4x3CO,
Cheese, 4x4, Part Painting, Tiger, Shuttle, Network, and Aircraft. Table 2 presents detailed
6. See the URL http://pomdp.org/pomdp/examples/index.shtml

141

fiZhang & Zhang

|S|
|Z|
|A|
VI(Time)
ssVI(Time)
VI(#)
ssVI(#)
subspace

4x3CO
11
4
11
3.52
63.28
4
43/1
yes

Cheese
11
4
7
14.06
85.44
14
32/2
yes

4x4
16
2
4
27.47
85.44
20
42/10
yes

Paint
4
4
2
38.75
85.20
9
22/9
no

Tiger
2
2
3
82.40
145.21
9
22/9
no

Shuttle
8
2
3
6130.69
1437.32
208
98/45
yes

Network
7
2
4
13283.15
2810.21
491
201/50
yes

Aircraft
12
5
6
1723193.34
425786.49
2071
3236/428
yes

Table 2: Comparative studies for ssVI and VI on the standard test-bed

statistics for these problems. In the table, Rows 24 give the sizes of problem parameters,
namely the number of states, observations and actions. Row 5 and 6 show the CPU seconds
for the standard and subset value iteration algorithms to compute the 0.01-optimal policy
for each problem. Row 7 shows the number of the vectors representing the 0.01-optimal
value function in standard value iteration. In our experiments, we implemented the subset
value iteration in the individual fashion. In Row 8, an entry takes the form /, denoting
the total number of vectors over all |A|  |Z| simplices and the maximum number of vectors
among these simplices when the subset value iteration terminates. The last row shows
whether the belief subset  (B) is a proper subset of the belief space.
In discussing the performances of the subset value iteration algorithm, we categorize the
tested problems into three classes. In the first class, the subset  (B) is actually the same as
the belief space. Subset value iteration must be less efficient than standard value iteration.
The reason is as follows: there exists at least one belief simplex such that value iteration
over it has the same complexity as standard value iteration; moreover, subset value iteration
needs to account for other simplices. Example problems are tiger and paint. Let us take the
paint problem as an instance. Our results show that there are two simplices are the same as
the belief space. The 0.01-optimal value functions over them are represented by 9 vectors,
each of which has the same number of vectors representing the 0.01-optimal value function
over the entire belief space. In the second class of the tested problems, the set  (B) is a
proper subset of the belief space, and meanwhile the numbers of the vectors representing
value functions over the belief space and individual simplices are very small. Subset value
iteration may not be so efficient as standard value iteration because of the overhead of
accounting for a large number of simplices. Example problems include 4X3CO, cheese and
4X4. Let us take 4X3CO as an instance. The 0.01-optimal value function over the entire
belief space is represented by only 4 vectors, whereas the 0.01-optimal value function over
each simplex is represented by only 1 vector. Since the subset value iteration has to account
for 44 simplices, subset value iteration is less efficient than standard value iteration. In the
third class of tested problems, the set  (B) is a proper subset of the belief space, and
meanwhile the numbers of the vectors representing value functions over the belief space are
moderately large. The subset value iteration algorithm is more efficient than the standard
value iteration algorithm. Examples include shuttle, network and aircraft. Let us take
network as an instance. The 0.01-optimal value function in the belief space is represented
142

fiRestricted Value Iteration: Theory and Algorithms

by 491 vectors, whereas the 0.01-optimal value function in  (B) is represented by less than
201 vectors (note that there are duplicates across belief simplices). The maximum size of
the representing sets over all simplices is 50. In this case, we expect that the savings brought
by the subset value iteration outweighs the overhead of accounting for the simplices. The
result shows that subset value iteration is about 5 times faster than standard value iteration.
Combining these results with those on the maze problem, we see that the computational
savings brought by the subset value iteration vary with different problem domains. Theorem
3 can be used to determine whether subset value iteration can bring about computational
savings for a POMDP. In the event that the belief set  (B) is a proper subset of the belief
space, the magnitude of the savings needs to be determined through empirical evaluation.

5. Informative POMDPs
In this section, we study a special POMDP class, namely informative POMDPs. For this
POMDP class, there are natural belief subsets for value iteration to work with. We will
show how to formally define these subsets. As the value iteration over these belief subsets
has been described (Zhang & Liu, 1997), our focus is to compare the algorithm with the
general subset value iteration developed in the previous section.
5.1 Motivation
As noted by some authors, in reality an agent often has a good, although imperfect, idea
about its locations (Roy & Gordon, 2002). For instance, mobile robots and other real world
systems have local uncertainty, but rarely encounter global uncertainty. Let us exemplify
this using the maze in Figure 3. Suppose that at each time point an agent receives a string
of four letters with certainty. In total, there are 6 observations, owww, owow, owoo, wwow,
wowo and woww regardless of executed actions. If we enumerate all possible observations
and the set of locations at which the agent receives such observations, we end up with the
following table.
observations
owww
owoo
wowo

states
{1 }
{ 3,4 }
{ 7,8}

observations
owow
wwow
woww

states
{ 2, 5}
{6}
{ 9,10 }

On the other hand, the strings can be used to infer the agents locations. For instance, if a
string owoo is received, the world must be at location 3 or 4. Hence, the observation owoo
restricts the world into a small range of world states. In fact, any observation can restrict
the world into at most two states although the world has ten. For this reason, the POMDP
is said to be informative.
In general, an agent perceives the world via observations. Starting from any state, if the
agent executes an action a and receives an observation z, the world states can be categorized
into two classes by the observation model: states the agent can be in and states it cannot.
Formally, the former is {s|s  S and P (z|s, a) > 0}. The set is denoted by S az . We use the
set to define the informativeness. An [a, z] pair is said to be informative if the size |S az |
is much smaller than |S|. An observation z is informative if [a, z] is informative for every
action a giving rise to z. A POMDP is informative if all observations are informative.
143

fiZhang & Zhang

In informative POMDPs, since any observation restricts the world into a small set of
states, the agent knows that the world cannot be in the state outside this small set. In
other words, for those states outside the set, the agent has zero beliefs. Consequently, an
observation can also restrict belief states into a belief subset.
5.2 Belief Subset Selection
For informative POMDPs, we can select belief subset  (B) as before. Combining the informativeness assumption and Corollary 1, we know that  (B) is a proper subset of the belief
space. So, value iteration over  (B) carries the space and time savings. In this section, we
choose an alternative belief subset for value iteration. Compared against the subset  (B),
the subset that we choose yields several advantages. First, it is conceptually simple and
geometrically intuitive. Second, it facilitates employing the low dimensional representation
of vectors. Third, it may lead to additional savings in time if the observation models of a
POMDP are independent of actions. The latter two advantages will be shown later.
To define the belief subset (say (B)), we first define a subset (B, a, z) for an action and
observation pair. Then, the belief subset (B) is formed by taking the union of (B, a, z)
over all action and observation pairs. To be specific,
X

(B, a, z) = {b|

b(s) = 1.0, s  S az , b(s)  0}

(12)

sS az

and

(B) = a,z (B, a, z).

It is trivial to see that (B, a, z) is a belief simplex. It can be proven that for any belief state
b,  (b, a, z) must be in (B, a, z). Therefore,  (B, a, z) is a subset of (B, a, z). Consequently,
 (B) is a subset of (B). This is summarized in the lemma below. The lemma is useful
when we discuss the value iteration algorithm working with the belief subset (B).
Lemma 5 For a POMDP,  (B)  (B).
It is of interest to compare the  -simplex and -simplex for a pair of a and z. Although
both simplices are generated by a list of belief states, -simplex has more intuitive geometric
meaning. Each belief state in the basis of (B, a, z) is a unit vector, i.e., it has probability
mass on one state. Therefore, the belief state in the basis must be a boundary point of the
belief space. In contrast, a belief state in the basis of a  -simplex can be an interior point.
See Figure 1 for an example, where A2 , A3 are interior points and A1 is a boundary point
of the belief space.
5.3 Value Iteration over (B)
From the theoretical perspective, the feasibility of conducting value iteration in (B) is
justified by Lemma 1. Combined with Lemma 5, the subset (B) is a closed set. Hence, the
MDP theory is applicable to defining the DP update equation. By our discussions on the
relationship and value iteration in Section 4.3, value iteration working with (B) retains
the quality of value functions.
We further exploit the informative feature in value iteration over (B). We briefly outline
the subset value iteration algorithm and refer the readers to a detailed description (Zhang
144

fiRestricted Value Iteration: Theory and Algorithms

& Liu, 1997). The basic idea is to reduce the dimensions of vectors in representing sets of
value functions. Note that for any pair [a, z], since the beliefs in states outside S az are zero,
a vector in the representing set of a value function over the simplex (B, a, z) needs only
|S az | components. In an individual fashion, a DP update over (B) computes a collection
(B,a,z)
(B,a,z)
(B,a,z)
{Vn+1 } from a collection {Vn
} where Vn
is the nth-step value function and
az
the vectors in it have |S | dimensions. The procedure of conducting a DP update is parallel
to that in Section 4.2 except that  (B, a, z) is replaced by (B, a, z). In the enumeration
step, when building a vector  in a belief simplex (B, a0 , z 0 ) using Equation (10), we need
0 0
only define its components corresponding to the set S a z . In the reduction step, for each
0
0
(B,a ,z )
constructed set Vn+1
, a pruning procedure is called to remove useless vectors to obtain
the minimal representation of the set. Note that the lower dimension feature is also used
to cut down the number of variables in setting up linear programs.
Interestingly, DP updates over (B) account for a larger subset than those over  (B).
Hopefully, since DP updates over (B) explicitly employ the economy of representation,
they could be more efficient. In addition, DP updates over (B) have another advantage
in the event that the observation models of a POMDP are independent of actions, i.e., the
probabilities P (z|s, a) being independent of a. Hence, given an observation z, the simplices
(B, a, z) are the same for all actions. Therefore, DP updates over (B) only account
for |Z| -simplices. However, DP updates over  (B) usually need to account for |A||Z|
 -simplices because an observation determines different  -simplices when combined with
different actions.
5.4 Empirical Studies
We have conducted experiments to compare VI, ssVI and infoVI, which refers to value iteration exploiting the low-dimension feature. The experiments on maze1 (defined in Section
4.5) can be found elsewhere (Zhang & Zhang, 2001; Zhang, 2001). The results, together
with existing results (Zhang & Liu, 1997), showed that value iteration over (B) can be
significantly more efficient than standard value iteration. For reference, we mention that
it is feasible to integrate a point-based technique and value iteration over (B) in order to
take advantage of both reducing the iteration number and accelerating the iterative steps
(Zhang & Zhang, 2001b). To demonstrate this, we include results on a 96-state POMDP
in Appendix B.
5.5 Restricted Value Iteration and Dimension Reduction
We compare the value iteration algorithms in this and the previous section. Through the
comparison, we would like to emphasize that working with belief subsets does not imply
working with low-dimensional vectors.
Although both algorithms work with belief subsets, the mechanisms exploited to achieve
the computational gains are different. The general value iteration works with the belief
subset  (B) but the dimension of representing vectors is the same as the number of states,
whereas value iteration over (B) works with a superset of  (B) but the dimension of the
vectors is smaller than the number of the states. To facilitate demonstrating how a reduced
belief set and the low-dimensional representation respectively contribute to the computational gains, we experimented with a carefully designed maze problem that is amenable to
145

fiZhang & Zhang

both algorithms. However, it is worth pointing out that working with a reduced belief set
does not mean that the vectors can be represented in low dimensions. We illustrate this
point by continuing our discussions of the example in Section 3.3. Such an example shows
that the primary advantage of value iteration over  (B) stems from the size of the chosen
belief subset rather than dimension reduction of the representing vectors.
Example (Continued) For the POMDP example presented in Section 3.2, the subset  (B)
consists of only two line segments in the entire belief space. Clearly value iteration over
 (B) is more efficient than standard value iteration. However, if one runs value iteration
over (B) on this POMDP anyhow, the algorithm is less efficient than the standard value
iteration algorithm. This follows from (1) the set S ai zj is equal to the set of states S for any
action ai and observation zj by the second assumption, and (2) each -simplex is actually
the same as the belief space by the definition in Equation (12). To solve this POMDP,
the susbet value iteration algorithm is definitely a better choice than the value iteration
algorithm for informative POMDPs.
2

6. Near-Discernible POMDPs
In this section, we study near-discernible POMDPs. For this POMDP class, we develop an
anytime value iteration algorithm working with growing belief subsets.
6.1 Motivation
A discernible POMDP assumes that once in a while the uncertainty about world states
vanishes if a particular action is executed and the observations pertain to the action fully
reveal the identities of the world (Hansen, 1998). Our research on near-discernible POMDPs
was motivated by two aspects. One of them arises from the origin of applying POMDP as
a framework for planning under uncertainty. To achieve a goal location, an agent has to
not only change its positions by performing goal-achieving actions but also reason about its
surroundings by performing information-gathering actions. However, at one time point the
agent cannot simultaneously move its positions and observe its environments. For instance,
if an information-gathering action is performed, the agent cannot move its positions meanwhile. The other aspect motivating the concept of near-discernibility arises from existing
research in the community. Near-discernible POMDPs generalize discernible POMDPs in
that even when an information-gathering action is performed, the agent can get a rough,
rather than exact, idea about world states and uncertainty vanishes in some sense.
We revise the maze problem to fix ideas on the first motivation. The action space consists
of six actions: four moving actions, look and declare. If move actions or declare are
performed, an observation null is received and the agent gets no information at all. If
look is performed, an ideal string is received and the agent gets imperfect information since
different locations might yield the same string. On one hand, to achieve the goal location,
the agent has to change its positions. On the other hand, to declare goal attainment with
confidence, it has to perform look and reason about the environment. Arbitrarily declaring
goal attainment leads to a penalty. Consequently, at a time point the agent faces the
problem of choosing a move or look.
146

fiRestricted Value Iteration: Theory and Algorithms

We note that the subset value iteration algorithm usually yields no computational advantage for near-discernible POMDPs. We give an example in which the subset  (B) is the
same as the belief space B under some assumptions. Suppose the maze is a square grid.
Locations are numbered such that at each row the indices of the locations increase from
left to right. We assume that a move action achieves its intended effects with a high likelihood, but may have no effect (i.e., the agents location remains unchanged) or may lead to
overshooting with a small probability. Under these assumptions, the transition matrix for
action east is upper-triangular and invertible. If at each location null is received with a
positive probability after a move, the transformational matrix Peast,null is invertible. By
Theorem 3, the belief subset  (B, east, null) is equal to the belief space B.
Our solution to near-discernible POMDPs rests on the intuition that the agent needs
to interleave goal-achieving actions and information-gathering actions. A typical sequence
of executed actions should consist of several goal-achieving actions and an informationgathering action. The difficulty is how frequently the agent should execute an informationgathering action. In this section, we consider the action and observation sequences containing more goal-achieving actions incrementally. We show that such sequences can be
used to determine belief simplices. As more sequences are added, the union of the belief
simplices grows. In the following, we give some technical preparations and then describe the
algorithm designed for near-discernible POMDPs. In order to put our discussions under
a general context, we shall use information-rich and information-poor actions instead of
information-gathering and goal-achieving actions respectively.
6.2 Histories, Belief Subsets and Value Functions
A history is a sequence of ordered pairs of actions and observations. We usually denote a
history by h. The number of pairs of actions and observations is referred to as the length of
a history. A history of length l is denoted by [a1 , z1 ,    , al , zl ]. If an agents initial belief
state is b and a history h of length l is realized, its belief state can be updated at each
time step. The notation  (b, h) denotes the belief at the time point l. The set  (B, h) is
defined to be bB  (b, h), consisting of all possible belief states that the agent can be in at
step l if it starts with any belief and history h is realized. Note that if h is of length 1(say
h = [a, z]),  (B, h) degenerates to our previous notation  (B, a, z).
Lemma 6 For any history h, the belief subset  (B, h) is a simplex.
A set of histories is usually denoted by H. The belief subset  (B, H) denotes the union
of simplices for all histories in the set H, i.e., hH  (B, h). Value functions over the simplex
 (B, h) and belief subset  (B, H) are referred to as V  (B,h) and V  (B,H) respectively. Given
a set V  (B,h) representing value function V  (B,h) , the procedure simplexPrune(V, B (B,h) )
in Table 1 computes the minimal representation of V  (B,h) . In the context of history, the
occurrences of the basis B (B,a,z) should be replaced by B (B,h) .
6.3 Space Progressive Value Iteration
We describe the space progressive value iteration (SPVI) algorithm. As an anytime algorithm, SPVI begins with a belief subset and gradually grows it. When a certain stopping
147

fiZhang & Zhang

criterion is met, SPVI terminates and returns a set of vectors for the agents decision making.
6.3.1 Algorithmic structure
SPVI interleaves value iteration (computing a value function for a belief subset) and subset
expansion (expanding the current belief subset to a larger one). The belief subsets in SPVI
are introduced by sets of histories. Subset expansion is achieved through incorporating more
histories. For convenience, the set of histories determining the i-th belief subset is denoted
by Hi . The belief subset determined by Hi is  (B, Hi ). The value function constructed by
SPVI for Hi is V  (B,Hi ) .
The pseudo-code in Table 3 implements SPVI. A set of histories H0 (and therefore the
belief subset  (B, H0 )), a value function V  (B,H0 ) and the quality precision  are initialized
at line 1. This step can be regarded as the 0th-step expansion of belief subset. Note that we
set the initial value function to be the minimum reward for all pairs of actions and states.
(This is for the convergence issue discussed later.) Value iteration over the current subset
 (B, Hi ) is conducted at line 3, and the belief subset is expanded to the subset  (B, Hi+1 )
through constructing a superset Hi+1 of the current set Hi at line 4. Value function V  (B,Hi )
for the current belief subset is set to be the initial value function for the next subset at
line 5. If the stopping condition is not satisfied at line 7, SPVI goes to the next iteration;
otherwise, it terminates and returns the latest value function V  (B,Hi1 ) .
To ensure the efficiency of SPVI, its initial belief subset should be chosen to be small. To
this end, we set H0 to be {[a, z] | a  AIR , z  ZIR } where AIR is the set of information-rich
actions and ZIR is the set of observations led to by those actions. The subset  (B, H0 ) is
small due to the discernability property.
In the sequel, we discuss value iteration in a belief subset, subset expansion and the
stopping criterion in detail.
6.3.2 Value iteration in a belief subset
Given a set V of vectors, a set H of histories and a precision threshold , value iteration
computes an improved value function over the belief subset  (B, H). This is accomplished
by conducting a sequence of DP updates. In the following, we discuss implicit DP updates,
the convergence issue and the stopping criterion in the value iteration step.
An implicit DP update computes a new value function from the current one for belief
subset  (B, H). Let Uj (U0 = V) denote the j-step value function. Thus, a DP update
computes value function Uj+1 from Uj . The procedure of computing Uj+1 from Uj is parallel
to the collective DP update in Section 4. In particular, when defining a vector a, given
 (B)
an action a and a mapping  in Equation (9), the occurrences of Vn
are replaced by Uj .
By enumerating actions and mappings, all defined vectors form the set Uj+1 . Its minimal
representation is obtained by removing useless vectors w.r.t. the subset  (B, H).
The convergence issue arises because the subset  (B, H) may not be a closed set. To
guarantee the convergence of value iteration, we set Uj+1 to be the union of set Uj+1
and Uj after a DP update. Together with the fact that the initial value function is set
to be the minimum reward for all actions and states, the sequence {Uj } monotonically
increases in terms of induced value functions. On the other hand, the value functions in the
148

fiRestricted Value Iteration: Theory and Algorithms

SPVI:
1. i  0, initialize H0 , V  (B,H0 )  minsS,aA r(s, a),   (1  )/2
2. Do
3.
V  (B,Hi )  subsetVI(V  (B,Hi ) , Hi , )
4.
< Hi+1 ,  (B, Hi+1 ) > expandSubset(V  (B,Hi ) , Hi )
5.
V  (B,Hi+1 )  V  (B,Hi )
6.
ii+1
7. Until (stopping condition is met)
8. Return V  (B,Hi1 )
subsetVI(V, H, ):
1. j  0, U0  V
2. Do
3.
Uj+1  subsetDPUpdate(Uj ,  (B, H))
4.
Uj+1  Uj+1  Uj
5.
j j+1
6. While ( maxb (B,H) |Uj (b)  Uj1 (b)|   )
7. Return Uj1
expandSubset(V, H)
1. H0  H
2. For each  in the set V
3.
If .history is maximal in H and .action is information-poor
4.
For each [a, z] in AIP  ZIP
5.
H0  H0  {[h, a, z]}
0
6. Return < H ,  (B, H0 ) >
Table 3: Space progressive value iteration (SPVI)
sequence are upper bounded by the optimal value function. Consequently, value iteration
in  (B, H) must converge. As a result, the Bellman residual between value functions,
maxb (B,H) |Uj+1 (b)Uj (b)|, becomes smaller in  (B, H) as value iteration continues. When
the residual falls below the threshold , value iteration terminates.
The value iteration step is implemented as the procedure subsetVI in Table 3. Given a
set V of vectors, a set H of histories and a threshold , the procedure computes an improved
value function for belief subset  (B, H). Value function U0 is set to be the input set V at
line 1. The new value function Uj+1 is computed by a DP update at line 3. To guarantee
convergence, Uj+1 to set to be the union of Uj and Uj+1 at line 4. The stopping criterion is
tested at line 6. If it is met, the latest value function Uj1 is returned.
6.3.3 Subset expansion
Given a set V of vectors and a set H of histories, the subset expansion step expands the belief
subset  (B, H) to a larger one. This is achieved by generating a superset H0 of H. The new
149

fiZhang & Zhang

belief subset  (B, H0 ) is thus a superset of  (B, H). Hence, the key in subset expansion is
how to generate a history set H0 . In the following, we propose two approaches to generating
the history set using our intuition for near-discernible POMDPs. Both approaches generate
new histories by exploiting the vectors in V. We begin with an analysis of the vectors in
the set V and show how to use them to generate histories.
Let  be a vector in the set V. Remember that  is defined by a pair of action a and
mapping . For convenience, such an action is said to be the associated action of . In
addition, if  is useful in the set V w.r.t. the belief subset  (B, H), there must exist a history
h in H such that  is useful w.r.t. the belief simplex  (B, h). The history h is said to be
the associated history of . The associated history of a vector can be used to generate new
histories by extending the history, i.e., appending the pairs of informative-poor actions and
observations to the history. Let AIP be the set of the information-poor actions and ZIP
be the set of the observations led to by those actions. Extending history h results in a set
{[h, a, z]|a  AIP , z  ZIP }. The set contains |AIP ||ZIP | histories. Each history in the set
is called an extension of history h.
To generate H0 from the set H and the set V, one generic approach works as follows.
Each vector  in the set V is examined. If its associated history is long and the associated
action is information-poor, we produce all extensions of its associated history. An extension
is added to H0 if it is not in H0 . (The reason is that the associated action of a vector should
be information-rich if its associated history is sufficiently long.) To ensure that H0 is a
superset of H, we set H0 to be H in the beginning. Apparently, this approach to generating
histories suffers from the exponential increase of the size |H0 | in |AIP | and |ZIP |. In the
worst case where all the vectors in V are associated with information-poor actions, the size
|H0 | is |H||AIP ||ZIP |. Consequently, after the i-step subset expansion, |Hi+1 | can be as
large as |AIR ||ZIR |(|AIP ||ZIP |)i where |AIR ||ZIR | is the size of initial history set.
To alleviate this problem, we use a heuristic approach to generating H0 in hope that
the size H0 increases moderately. The above exhaustive approach extends the histories
associated with the vectors prescribing informative-poor actions. The heuristic approach
does not extend all such histories. Instead it extends only maximal histories in the set
H. (A history is said to be maximal in a set if none of its extensions is in the set.) This
is the only change made from the above approach. As indicated in the experiments, this
restriction can effectively cut down the size of the history sets. Nonetheless, the heuristic
approach shares the same worst-case complexity with the exhaustive approach.
The subset expansion step is implemented as the procedure subsetExpansion in Table
3. Given a set V of vectors and a set H of histories, it computes an expanded set H0 and an
expanded belief subset  (B, H0 ). The set H0 is initialized to be H at line 1. For each vector
 in V at line 2, if its associated history is maximal in H and its action is information-poor
(line 3), all the extensions of its associated history are added to H0 (line 5). The expanded
set H0 and also the expanded belief subset  (B, H0 ) are returned at line 6.
6.3.4 Stopping criterion and Decision-Making
As an anytime algorithm, SPVI can be terminated if a hard deadline is reached. Another
stopping criterion of interest can be set as follows. Given a sufficiently large amount of
time, SPVI would account for as many histories as possible. If a (near) optimal policy of the
150

fiRestricted Value Iteration: Theory and Algorithms

POMDP requires that information-rich actions be executed after a sequence of informationpoor actions, SPVI should be able to compute a value function for the belief subset, which
consists of all possible belief states that the agent encounters if it is guided by such a
policy. After sufficiently many expansions of history sets and hence belief subsets, any
vector associated with a maximal history prescribes an information-rich action. If all the
vectors in the representing set prescribe information-rich actions, SPVI terminates. If a
(near) optimal policy has the desired structure in its sequence of actions, the output value
function should be near optimal in the final belief subset.
When SPVI terminates, the value function V  (B,Hi1 ) can be used for decision making.
Similarly to Equation (3), a V  (B,Hi1 ) -improving policy can be defined over the belief space.
6.3.5 Efficiency of SPVI
The efficiency of SPVI depends on the selected belief subsets. If these belief subsets are
close to the belief space in size, SPVI must be inefficient. Fortunately, our approach for
belief subset expansion ensures that the initial belief subset is small and the subsequent
subsets grow slowly. First, since H0 is the set of the pairs of information-rich actions and
observations, the initial belief  (B, H0 ) is relatively small. Second, the subsequent belief
subsets  (B, Hi ) do not grow too quickly. The reason follows. In extending a history, the
information-poor pairs are added to its end. Hence, the first action and observation pair of
the histories in a set Hi must be information-rich. Therefore, for a history h in the set Hi ,
 (B, h) is small in size. Meanwhile, due to the heuristic for generating history sets, the sizes
|Hi | would not increase too fast. These characteristics make SPVI efficient when compared
with the standard value iteration algorithm.
Although the above analysis is empirically confirmed in our experiments below, it is
worthy to mention that in the worst case the number of belief simplices grows exponentially
in the number of |AIP ||ZIP |. Since a history determines a belief simplex, in the worst
case the number of the belief simplices after the i-step subset expansion is the same as the
number of histories, i.e., |AIR ||ZIR |(|AIP ||ZIP |)i (see the third paragraph of Section 6.3.3).
6.4 Empirical Results
Since SPVI works in an anytime manner, our primary interest is to demonstrate how the
quality of the generated value functions varies with the time cost. However, the availability
of optimal solutions strongly depends on the tractability of the problems. If a near
optimality is available, we compare it directly with the value function generated by SPVI
by simulations. Otherwise, we simply compare value functions from SPVI against those
from an approximate algorithm QMDP (Littman et al., 1995; Hauskrecht, 2000). Although
the comparison is not strict in a formal sense, it can provide clues about the quality of value
functions.
We report our results on two variants of the base maze problem and an office navigation
problem. In one variant, SPVI terminated after a finite number of iterations and the output
value function is near optimal; in the other variant, SPVI can quickly find a high-quality
value function as time goes by (Zhang & Zhang, 2001a; Zhang, 2001). In the rest of this
section, we report our results on the office navigation problem.
151

fiZhang & Zhang

The environment is modeled after the floor plan of the authors home department. The
layout is shown in Figure 6. There are 35 states: 34 locations plus one terminal state.
The action space is of size 6 (four move, one look and one beep replacing declare in the
maze problem). Any action except look leads to a null observation. To introduce other
observations, we note that in the figure, black bars represents doors and grey bars represent
walls with display boards. The look action yields observation of strings of four letters for
a location indicating, for each of the four directions, where there is a door (d), an empty
wall (w), a wall with a display board (b), or nothing (o). In total, there are 22 different
strings. Hence, plus the null observation, the observation space is of size 23. Transition
probabilities for moves are specified identically as in the maze problem. Neither look nor
beep changes the states of the environment. At each location, look produces the ideal
string for that location with probability 0.75. With probability 0.05, it produces the null
observation. Also with probability 0.05, it produces a string that is ideal for some other
location and that differs from the ideal string for the current location by only 1 character.
The robot receives a reward of 50 when beeping at location 22 and a reward of -10 when
beeping at any other location (we dont want the robot to make a lot of noise). The move
actions bring about a reward of -2 if they lead the robot to bumping into walls or doors.
They have no rewards otherwise. The reward for the look action is always -1. The robot
needs to get to location 22 and beep so that someone in the main office can come out and
hand the robot some mail.

3
2
1

6

11

16

19

5

10

15

18

9

14

17

4

7

8

12

13

Main
Office

20

21

22

23

34

25

26

27
28
29
34

33

32

30
31

Figure 6: HKUST-CSD office environment
We conduct simulations about the generated value functions because no existing exact
algorithm can find the near optimal value function. The simulation consists of 1000 trials.
Each trial starts from a random initial belief state and is allowed to run up to 100 steps.
The average reward across all the trials is used as a measurement of the quality of policies
derived by value functions.
Figure 7 presents the results about the quality against time costs. We see that SPVI
found a policy whose average reward is 19.6 in about 80,000 seconds. SPVI was manually
terminated after running about 24 hours. It is found that the algorithm conducted three
steps of subset expansion. By our data, after the first and second expansion steps, both
152

fiRestricted Value Iteration: Theory and Algorithms

rewards by simulation are 18.4. This is not far from 19.6 obtained after the third expansion
step although it is difficult to say how close those polices are to the optimal. Compared
with the solutions generated by QMDP, the policies generated by SPVI are clearly better.

Quality of Value Functions

20
19
18
17
16
15

SPVI
QMDP

14
13
12
100

1000
10000
Time in Seconds (in log-scale)

100000

Figure 7: Performance of SPVI on office navigation problem
For reference, Table 4 gives detailed statistics on the number of the histories, iterations
and vectors after each subset expansion. A note is about the number of iterations in the
third column: when conducting value iteration over subsets, we also use the point-based
improvements (Zhang, 2001). In the column, the number of point-based steps are excluded.
The fourth column about the number of vectors provides some idea of why SPVI takes long
time for this problem. This is because it generates a great number of vectors. After the
third expansion, it uses 7,225 vectors to represent a value function over the belief subset.

i-step expansion
0
1
2

histories#
53
86
131

iterations#
4
6
7

vectors#
464
5178
7225

rewards
18.44
18.44
19.65

time
163
16157
78183

Table 4: Statistics on HKUST-CSD environment for SPVI

7. Related Work
In this paper, we propose restricted value iteration algorithms to accelerate value iteration
for POMDPs. Two basic ideas behind restricted value iterations are (1) reducing the complexity of DP updates and (2) reducing the complexity of value functions. In this section, we
discuss related work under these two categories. In addition, we give an overview on special
POMDPs in the literature and the algorithms exploiting their problem characteristics.
153

fiZhang & Zhang

7.1 Reducing the Complexity of DP Updates
In a broad sense, approaches reducing the complexity of DP updates can be roughly categorized into two classes: approaches conducting value updates over a (stationary) belief subset
and approaches conducting value updates over a growing subset, although the boundary
between these two classes is ambiguous in some cases.
The first class includes a family of grid-based algorithms, algorithms based on reachability analysis, algorithms using state-space decomposition and others. Grid-based algorithms
update values for a finite grid and extrapolate values for non-grid belief states (Lovejoy,
1991; Hauskrecht, 1997; Zhou & Hansen, 2001). However, to guarantee optimality, the
grid size is often exponential in the dimension of the state space. To tackle POMDPs with
large state spaces, reachability analysis is a generally applicable technique. If an agent is
informed of its initial belief, all belief states it can encounter form a finite set in case of a
finite decision horizon. These belief states can be structured in a decision tree or AND/OR
tree (Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet & Geffner, 2000).
Although sometimes near optimality can be achieved at the initial belief state (Hansen,
1998), the algorithms in the cited articles cannot be applied to the case with unknown
initial belief. State-space decomposition is an effective way to alleviate the curse of dimensionality. This approach has been successfully applied to MDPs (Dean & Lin, 1995; Dean,
Givan, & Kim, 1998; Parr, 1998; Koller & Parr, 2000). Typically, to solve an MDP, one
solves a number of small MDPs and uses their solutions to approximate that of the original
MDP. However, the state-space decomposition approach cannot directly generalize to the
POMDP context because of the inherent difficulty incurred by the continuum of the belief
space.
Our theory and algorithms on restricted value iteration have significant differences from
the above approaches. Through a well chosen belief subset, restricted value iterations can
achieve convergence and optimality. It differs from grid-based algorithms in that it computes
vector-based representations of value functions. Despite this difference, it is possible that
grid-based algorithms can benefit from our theory on belief subset selection. This possibility
is yet to be investigated. For instance, while choosing grid points, one should choose those
within the belief subset  (B). The reason follows. Since the belief states outside the set
are never reachable, their values do not directly contribute to value updates for beliefs in
the grid. With regard to the differences between the aforementioned algorithms and ours,
our approach has no assumption about an agents initial belief, although the belief subset is
chosen via reachability analysis. Our algorithm differs from the decomposition techniques
in that it solves a reformulated MDP instead of a set of small MDPs.
Approaches conducting value updates for a growing belief subset include real-time dynamic programming (RTDP) in the POMDP context (Barto, Bradtke, & Singh, 1995;
Geffner & Bonet, 1998), a synthetic projection algorithm (Drummond & Bresina, 1990)
and the envelope algorithm for Plexus planner in the MDP context (Dean et al., 1993).
Naturally, they run as anytime algorithms. In RTDP, value updates are carried out for
a belief subset, which grows as an agent explores the belief space. The main difference
between SPVI and the above algorithms is how they expand the belief/state subset and
how they choose beliefs/states for value updates. For subset expansion, SPVI adds more
belief simplices, which often contains an infinite number of belief states, while the above
154

fiRestricted Value Iteration: Theory and Algorithms

algorithms mostly add a finite number of belief states. (It is also noted that reachability
analysis is used in these expansions.) For value updates, SPVI improves values for the
entire belief subset, while the above algorithms typically select a limited number of beliefs
or states in the current subset.
7.2 Reducing the Complexity of Value Functions
Another idea behind restricted value iteration is concerned with the representational complexity of value functions. Intuitively, the representing set of a value function over a belief
subset contains fewer vectors than that of the same value function over the belief space.
This fact has been observed (Boutilier & Poole, 1996; Hauskrecht & Fraser, 1998), where
POMDPs are represented compactly. When states are depicted by a set of variables, they
are classified into observable variables and hidden variables. It is also noted that some
belief states cannot be reached for certain combinations of observable variables and hidden variables. This fact has been exploited in approximating the solution to a medical
treatment example (Hauskrecht & Fraser, 1998). Recent work along this thread includes
a state-space compression technique exploiting the representational advantage (Poupart &
Boutilier, 2002), and a technique of Principle Component Analysis (PCA) aiming at reducing the complexity of value functions (Roy & Gordon, 2002). However, it is unclear whether
it is feasible to combine state-space compression with subset value iteration before we know
how to conduct value iteration over a belief space induced by a compressed state space.
7.3 Solving Special POMDPs
Since solving a POMDP generally is computationally intractable, it is advisable to study
POMDPs with special characteristics. The hope is that these characteristics may be exploited to find their near optimal solutions more efficiently. Special POMDPs examined in
the literature include regional-observable POMDPs (Zhang & Liu, 1997), memory-resetting
and discernible POMDPs (Hansen, 1998), even-odd POMDPs (Zubek & Dietterich, 2000)
and generalized near-discernible POMDPs (Zhang, 2001). Interestingly, these POMDPs
assume the existence of informative actions or observations such that somehow an agent
is able to get more information about the world. In the following, we briefly discuss the
assumptions behind informative POMDPs and near-discernible POMDPs and review existing work closely related to them. Before concluding this subsection, we also mentioned a
couple of extensions to our current work.
An informative POMDP assumes that any observation restricts the world into a small
set of states. This assumption is validated by a few problem instances with compact representations of state space. In the literature, some POMDP examples are actually informative
POMDPs. One example is the slotted Aloha protocol problem (Bertsekas & Gallagher, 1995;
Cassandra, 1998a), where the state of the system consists of the number of backlogged messages and the channel status. The channel status is observable and its possible assignments
form the observation space. However, the system has no access to the number of backlogged
messages. If the maximum number of backlogged messages is set to m and there are n possible values for the channel status, the number of states is m  n. A particular assignment
of channel status will restrict the system into m states out of m  n. A similar problem
155

fiZhang & Zhang

characteristic also exists in a non-stationary environment model proposed for reinforcement
learning (Choi, Yeung, & Zhang, 1999).
A regional observable POMDP assumes that at any point in time an agent is restricted
to a handful of world states (Zhang & Liu, 1997). The assumption leads to a value iteration
algorithm that works with a belief subset and also exploits the low dimensional representation of vectors. We used the algorithm to solve informative POMDPs. However, we would
like to discuss several differences. First, conceptually the assumptions are different for the
two POMDP classes. In regional observable POMDPs, when the agent is restricted to a set
of states (i.e., a region), the states in such a set are geometrically neighboring ones. However, in informative POMDPs, when the agent is restricted to a set of states, the states in a
set are obtained by formally analyzing the observation model of the POMDP. It is possible
that the states in the set are spatially distant from one another. Second, the algorithms for
the two POMDP classes work in a quite different way. To ease presentation, we use infoVI
and roVI to respectively denote our value iteration for informative POMDPs and that for
regional observation POMDPs. In infoVI, the number of state sets is the product of the
number of actions and the number of observations, while in roVI, the number of regions is
subjectively chosen. In addition, the observations in roVI are augmented. An augmented
observation consists of an original observation and a specific region. So, the number of augmented observations is the product of the number of original observations and the number
of regions. Hence, roVI has to account for many more observations than infoVI. This
fact is useful when comparing the efficiency of infoVI and roVI. Imagine what happens
if roVI works with the region system, which consists of the state sets defined by infoVI,
for an informative POMDP. Because infoVI accounts for fewer observations than roVI, it
should be more efficient. Finally, the quality of the value function returned by infoVI is
guaranteed for the entire belief space when it terminates with the strict stopping criterion.
However, the quality of the value function by roVI in its original description is problematic
even for the considered belief subset.
The other POMDP class examined in this paper is near-discernible POMDPs. A near
discernible POMDP assumes that the actions are classified into information-rich ones and
information-poor ones. The assumption is reasonable in several realistic domains. The first
domain is the path planning problems (Cassandra, 1998a). The actions are categorized into
goal-achieving and information-gathering ones, as discussed earlier. Another application
domain is machine maintenance problems (Smallwood & Sondik, 1973; Hansen, 1998),
where an agent usually can execute the following set of actions: manufacture, examine,
inspect and replace. Among these actions, inspect is information-rich and the remaining
three actions are information-poor.
A near discernible POMDP is a generalization of a memory-resetting (discernible)
POMDP, which assumes that there exists actions resetting the world to an unique state. If
such actions are performed, the agent knows that the world must be in a definite state. If
the initial belief state is known and an optimal policy must execute one of such actions periodically, the number of belief states that the agent visits is finite. Accordingly, DP updates
over a finite set of beliefs are much cheaper. However, after the discernibility assumption
is relaxed, the agent may visit an infinite number of states and DP updates become more
expensive. We therefore developed an anytime algorithm seeking a tradeoff between the
solution quality and the size of the belief subset.
156

fiRestricted Value Iteration: Theory and Algorithms

We also experimented with one extension of using SPVI to approximate the solutions of
more general POMDPs (Zhang, 2001). The approximation scheme employs a thresholding
technique. Given a POMDP and a threshold, a POMDP can be transformed to a new one,
which differs from the original one in the observation model. The observation model in the
transformed POMDP is obtained by ignoring the probabilities (in the original model) less
than the threshold 7 . If the transformed POMDP is near discernible, its solution can be
found by SPVI and be used to approximate that of the original POMDP. We have designed
another maze problem that has no informative action/observation pair and therefore is
expected to be not amenable to SPVI (Zhang, 2001). However, the transformed POMDP is
amenable to SPVI. The experiments show that SPVI can quickly find a high quality solution
for the transformed POMDP. In another case, if the transformed POMDP is informative,
the algorithm exploiting low dimensional representations for informative POMDPs can be
applied.

8. Conclusions
In this paper, we studied value iterations working with belief subset. We applied reachability
analysis to select a particular subset. The subset is (1)closed in that no actions can lead
the agent to belief states outside it; (2)sufficient in that value function defined over it can
be extended into the belief space; and (3)minimal in that value iteration needs to consider
at least the subset if it intends to achieve the quality of value functions. That the subset is
closed enables one to formulate a subset MDP. We addressed the issues of representing the
subset and pruning a set of vectors w.r.t. the subset. We then described the subset value
iteration algorithm. For a given POMDP, whether the subset is proper can be determined
a priori. If this is the case, subset value iteration carries the advantages of representation
in space and efficiency in time. We also studied informative POMDPs and near-discernible
POMDPs. For informative POMDPs, there are natural belief subsets for value iteration
to work with. For near-discernible POMDPs, we developed an anytime value iteration
algorithm seeking a tradeoff between the policy quality and the size of belief subsets.

Acknowledgments
Research was partially supported by Hong Kong Research Grants Council under grant
HKUST6088 / 01E. The authors thank Tony Cassandra and Eric Hansen for sharing with us
their programs. The first author would like to thank Eric Hansen for in-depth discussions on
belief subset selection and low dimensional representation, and Judy Goldsmith for valuable
comments on an earlier writeup of the ideas in this paper. We are also grateful for the three
anonymous reviewers who provided insightful comments and suggestions on this paper.

Appendix A. Proofs
Theorem 2 For any pair [a, z], the subset  (B, a, z) is a simplex.
7. To complete the definition of the approximate observation model, one needs to re-normalize model
parameters such that for an action and a state, the probabilities for all observations sum up to 1.0.

157

fiZhang & Zhang

Proof: Suppose bi is a belief state such that bi (s) = 1.0 for s = si and 0 otherwise.
It can be seen that {b1 , b2 ,    , bn } is a basis of belief space B. Each belief state b(=
P
(b(s1 ), b(s2 ),    , b(sn )) can be represented as ni=1 b(si )bi .
Let k be the cardinality of the set { (bi , a, z)|P (z|bi , a) > 0}. Without loss of generality,
we enumerate the set as { (b1 , a, z),   ,  (bk , a, z)}. It suffices to show that  (B, a, z) =
( (b1 , a, z),  (b2 , a, z),    ,  (bk , a, z)). To prove it, we prove:
(1)  (B, a, z) ( (b1 , a, z),  (b2 , a, z),    ,  (bk , a, z)) and
(2) ( (b1 , a, z),  (b2 , a, z),    ,  (bk , a, z))   (B, a, z).
First, we prove (1). It suffices to show that any belief state b0 in  (B, a, z) must belong
to the simplex . Since b0 is in  (B, a, z), there must exist a belief state b in B such that
b0 =  (b, a, z). We define a few constants as follows.
 For any i  {1,    , k}, Cbi is the probability of observing z when action a is executed
P
in belief state bi . Formally, Cbi = s0 ,s P (z|s0 , a)P (s0 |s, a)bi (s).
 Cb is the probability of observing z when action a is performed in b. Formally, Cb =
P
0
0
s0 ,s P (z|s , a)P (s |s, a)b(s).
 For any i  {1,    , k}, define i = Cbi /Cb .
P

Given these constants, we are going to prove b0 = i i  (bi , a, z). If this is true, i.e., b0 can
be represented as a convex combination of the vectors in the basis, (1) is proven.
We start from b0 =  (b, a, z). If  (b, a, z) is replaced by its definition, for a state s0 ,
b0 (s0 ) =

1 X
P (z|s0 , a)P (s0 |s, a)b(s)
Cb s

By the definition of belief state bi , we can rewrite the above equation as
b0 (s0 ) =

1 X X
P (z|s0 , a)P (s0 |s, a)bi (s).
Cb s i{1,,k}

Trivially,
X

b0 (s0 ) =

1 X
Cb

P (z|s0 , a)P (s0 |s, a)bi (s)

i

Cbi

Cbi

s

.

By the definition of  (bi , a, z), rewriting the above equation, we have
b0 (s0 ) =

X Cb
i

(

i

Cb

) (bi , a, z)(s0 ).

By the definition of i , the above equation yields
b0 (s0 ) =

X

i  (bi , a, z)(s0 ).

i

158

fiRestricted Value Iteration: Theory and Algorithms

If b0 and  (bi , a, z) are regarded as column vectors, the above equation means
b0 =

X

i  (bi , a, z).

i

Therefore, we prove that if there is a belief state b such that b0 =  (b, a, z), b0 can be
represented as a convex combination of the vectors in the basis. This means b0 must be in
the simplex .
To prove (2), we prove that any belief state b0 in the simplex  must be in the subset
 (B, a, z). It suffices to show that there exists a belief state b in B such that b0 =  (b, a, z).
P
Since b0 is in , there must exist a set of nonnegative i s such that b0 = ki=1 i  (bi , a, z).
If we replace  (bi , a, z) by its definition, then: for a state s0 ,
0

0

b (s ) =
If we denote

X
i

P

P
P (z|s0 , a)P (s0 |s, a)bi (s)
i P s
.
0
0
s0 ,s P (z|s

0
0
s0 ,s P (z|s , a)P (s |s, a)bi (s)

b0 (s0 ) =

XX
i

, a)P (s |s, a)bi (s)

by a constant Cbi , then

P (z|s0 , a)P (s0 |s, a)bi (s)

s

i
.
Cbi

Exchanging the summation order of i and s and making use of the definition of bs (i), we
have
X s
P (z|s0 , a)P (s0 |s, a).
b0 (s0 ) =
C
bs
s
We define a belief state b as follows: for any s,
s /Cbs
.
b(s) = X
s /Cbs
s

It can be seen that

P
P (z|s0 , a)P (s0 |s, a)b(s)
.
b (s ) = P s
0
0
0

0

s0 ,s P (z|s

, a)P (s |s, a)b(s)

Therefore, we proved that for any b0 in  there exists a belief state b such that b0 =  (b, a, z).
Consequently, b0   (B, a, z).
2
 (B)

 (B)

 (B)

Theorem 6 If maxb (B) |Vn (b)Vn1 (b)|  (1  )/(22 |Z|), then the Vn1 -improving
policy is -optimal over the entire belief space B.
Proof: It suffices to show maxbB |Vn+1 (b)  Vn (b)|  (1  )/(2). For any b  B,
=






|Vn+1 (b)  Vn (b)|
P
P
 (B,a,z)
 (B,a,z)
| maxa {r(b, a)+ z Vn
( (b, a, z))} maxa {r(b, a)+ z Vn1
( (b, a, z))}|


P
P
 (B,a ,z)
 (B,a ,z)




|r(b, a ) +  z Vn
( (b, a , z))  r(b, a )   z Vn1
( (b, a , z))|
P
 (B,a ,z)
 (B,a ,z)


| z (Vn
( (b, a , z))  Vn1
( (b, a , z)))|
 (B,a ,z)
 (B,a ,z)
|Z| maxz |Vn
( (b, a , z))  Vn1
( (b, a , z))|
|Z|(1  )/(22 |Z|)
(1  )/(2)
159

(1)
(2)
(3)
(4)
(5)
(6)

fiZhang & Zhang

where
 At Step (1), value functions are replaced by their definitions;
 (B)

 At Step (2), a is the Vn
improving;

 (B)

-improving action for b but it is not necessarily Vn1 -

 At Step (5), the given condition is used;
 Other steps are trivial.

2

Appendix B. Informative POMDPs: An Elevator Problem
This appendix describes a 96-state informative POMDP and empirical results of value
iteration over (B). The problem is adapted from existing research (Choi et al., 1999).
Our purpose is to show that restricted value iteration is able to solve larger POMDPs than
standard value iteration.
Problem Formulation
An elevator operates in a two-floor residential building. There are three patterns on the
passengers arrival: high arrival rate in the first floor and low in the second floor; low
arrival rate in the first floor and high in the second floor; equal arrival rates. As time varies
from the morning to the night in a day, these patterns change according to a probability
distribution. To keep track of the pick-up and drop-off requests, the elevator sets up four
buttons in its control panel: two buttons record the pick-up and drop-off requests for the
first floor, two other buttons keep the same information for the second floor. The elevator
is also aware of which floor it is on. In order to fulfill the requests at a floor, the elevator
first moves upwards or downwards so that it reaches the floor; then, the elevator stays at
the floor until the passengers finish entering or exiting. The objective of the elevator is to
minimize certain penalty or cost in the long run.
The problem can be formulated into a POMDP framework. A state consists of six
components: the arrival pattern, the pick-up requests for two floors, the drop-off requests
for two floors and the elevators position. We use six variables to denote the components
respectively. A state is an assignment to all the variables. The arrival pattern takes on three
possible values for three different patterns. If there are passengers waiting in the lobby of
the first floor, the pick-up request is set; otherwise, it is unset. If there are passengers in the
elevator intending to get off at the first floor, the drop-off request for the first floor is set;
otherwise, it is unset. Similarly, for the second floor, the variables for the pick-up/drop-off
requests can be set accordingly. If the elevator is at the first floor, its position is set to first;
for the second floor, its position is set to second. The number of states is 322222 = 96.
Each observation has five components; it has the same components as a state except the
arrival pattern. There are as many as 2  2  2  2  2 = 32 observations. The elevator may
execute one of three actions, namely go.up, go.down and stay. The restriction is, when it
is at the first floor, it cannot perform go.down; when it is at the second floor, the action
go.up cannot be performed.
160

fiRestricted Value Iteration: Theory and Algorithms

The uncertainty stems from the probabilities of the changes of the arrival patterns.
When the elevator executes go.up, each component evolves as follows. The arrival pattern
changes according to a predetermined probability distribution. The components for pickup/drop-off requests remain. The position changes from first to second. The effects of the
action go.down can be described similarly. When the elevator performs the action stay,
the arrival pattern changes similarly. All the requests at the floor are fulfilled and the
corresponding variables are reset. For instance, if a passenger would like to get off at the
first floor, when the elevator at the first floor performs stay, the passenger is able to get
off. We say that the elevator fulfills the drop-off requests for the first floor. For another
instance, if passengers like to enter the elevator at the second floor, they can do so only
when the elevator performs action stay at the second floor. We say the pickup request at
the second floor is fulfilled in this case. It is also allowable for the elevator to fulfill two
requests at one time point. For example, if there are both pick-up and drop-off requests at
the first floor, when the elevator performs action stay, the passengers can enter and exit
within one time point. We say it fulfills two requests. Note that only when an action stay is
performed, the elevator can fulfill its request. Since the variable of arrival pattern changes
at any time moment, the elevator changes its states probabilistically after performing any
action.
The elevator is informed of partial knowledge of its state transition. After the elevator
performs an action, it knows the changes of components of its states: variables pick-up
and drop-off for each floor and its position. However, since it does not know the arrival
pattern and it is a component of the state, the observations cannot reveal the identities of
the states. This is the partial observability. However, since there are only three possible
arrival patterns, each observation reveals that the elevator must be in only three possible
states. Therefore, the POMDP is informative.
The performance of the elevator can be measured in different ways for diverse applications. We define a measure to minimize the unsatisfactory degree of the service the elevator
provides. We encode this in our reward model. At any time point, the elevator serves
one of four requests: pick-up requests at the first/second floor, drop-off requests at the
first/second floor. After performing an action, if any of these 4 requests is unfulfilled, the
elevator receives a penalty of 0.25. For instance, if the elevator un-fulfills either the pick-up
or drop-off request(if they are set) at the first floor, it receives a penalty of 0.25  2 = 0.5.
The objective of the elevator is to minimize total discounted penalty in a long run.
For convenience, we use A.i to denote arrival patterns for i = 1, 2, 3. In our experiments,
the transition probabilities are set as in the following table. Basically, each pattern remains
fixed with probability 0.90 and changes to another with 0.05.

A.1
A.2
A.3

A.1
0.90
0.05
0.05

A.2
0.05
0.90
0.05

161

A.3
0.05
0.05
0.90

fisizes of representing sets(logscale)

Zhang & Zhang

CPU seconds(logscale)

1e+06
100000
10000
1000
VI
ssVI
infoVI
infoVIPB

100
10
1
0.1
0

5

10 15 20 25
number of iterations

30

35

100000
10000
1000
100

VI
ssVI
infoVI
infoVIPB

10
1
0

5

10 15 20 25
number of iterations

30

35

Figure 8: Performance of VI, ssVI, infoVI and infoVIPB on Elevator
Empirical Studies
We collect the time costs and the actual number of vectors generated at each iteration of
algorithms VI, ssVI, infoVI and infoVIPB referring to infoVI integrated with a pointbased procedure (Zhang, 2001). The results are presented in Figure 8.
The first chart in the figure shows the time costs against the iterations. The algorithms
are set to compute a 0.1-optimal value function. For infoVIPB, we exclude the iterations
for point-based improvements. Overall, we see that VI and ssVI by no means can solve
the problem, infoVI is likely to solve it given sufficient time and infoVIPB is able to solve
it easily. When infoVIPB runs, it uses the loose stopping criterion. This is because if the
strict one is used, the threshold is close to the round-off precision parameter.
For the first seven iterations, ssVI takes 190,000 seconds, infoVI only 32 seconds. The
performance difference is drastic. As infoVI proceeds, it takes about 1,100 seconds for
one iteration. It is evident that infoVI is able to compute a near optimal value function
if given sufficient time. When the point-based technique is integrated, infoVIPB is able
to terminate in 94 seconds after five steps of DP updates over (B). Since most of these
algorithms cannot terminate within a reasonable time limit, we compare the data on the
6th iteration among them. This is the last iteration we are able to gather statistics for VI.
For this iteration, VI takes 76,000 seconds, ssVI 6,000 seconds, infoVIPB only 8 seconds.
The second chart in Figure 8 depicts the number of vectors generated at each iteration for
the tested algorithms. For ssVI, we collect the sum of the numbers of vectors representing
value functions over |A|  |Z|  -simplices. For infoVI and infoVIPB, we collect the sum of
the numbers of representing vectors for |Z| -simplices. This is because for this problem
the observation models are independent of the actions.
From the chart, we see that VI generates significantly more vectors than ssVI and
infoVI. In our experiments, after infoVIPB terminates, it produces 1,132 vectors. For the
same reason as above, we compare the numbers of the vectors after the 6th iterations for
these algorithms. After the iteration, VI generates 12,000 vectors. For ssVI and infoVI,
this number is 252 and 136 respectively. As DP updates proceed, it is conceivable that
the number of vectors generated by VI will increase sharply and hence the DP updates are
extremely inefficient. For infoVIPB, since the final number of generated vectors are rather
162

fiRestricted Value Iteration: Theory and Algorithms

small, together with the fact that point-based improvement effectively reduces the number
of iterations over the (B), it is possible to compute a near optimal value function within
a rather small time limit as it turns out.

References
Astrom, K. J. (1965). Optimal control of Markov decision processes with incomplete state
estimation. Journal of Mathematical Analysis and Applications, 10, 403406.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72, 81138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Gallagher, R. G. (1995). Data Networks. Prentice Hall.
Bonet, B., & Geffner, H. (2000). Planning with incomplete information as heuristic search
in belief space. In Proceedings of the 6th International Conference on Artificial Intelligence in Planning Systems (AIPS), pp. 5261. AAAI Press.
Boutilier, C., Brafman, R. I., & Geib, C. (1998). Structured reachability analysis for Markov
decision processes. In Proceedings of the 14th Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 2432.
Boutilier, C., & Poole, D. (1996). Computing optimal policies for partially observable
decision processes using compact representations. In Thirteenth National Conference
on Artificial Intelligence (AAAI), pp. 11681175. Portland, Oregon.
Burago, D., de Rougemont, M., & Slissekno, A. (1996). On the complexity of partially
observed Markov decision processes. Theoretical Computer Science, 157 (2), 161183.
Cassandra, A. R. (1998a). Exact and approximate algorithms for partially observable Markov
decision processes. Ph.D. thesis, Department of Computer science, Brown university.
Cassandra, A. R. (1998b). A survey of POMDP applications. In Working Notes of AAAI
1998 Fall Symposium on Planning with Partially Observable Markov Decision Processes, pp. 1724.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: a simple,
fast, exact method for partially observable Markov decision processes. In Proceedings
of the 13th Conference on Uncertainty in Artificial Intelligence, pp. 5461.
Choi, S. P. M., Yeung, D. Y., & Zhang, N. L. (1999). An environment model for nonstationary reinforcement learning. In Advances in Neural Information Processing Systems
12, pp. 987993.
Dean, T., Givan, R., & Kim, K. (1998). Solving planning problems with large state and
action spaces. In Proceedings of the 4th International Conference on Artificial Intelligence in Planning Systems (AIPS), pp. 102110. Pittsburgh, Pennsylvania.
Dean, T. L., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines
in stochastic domains. In Proceedings of the 9th National Conference on Artificial
Intelligence (AAAI), pp. 574579.
163

fiZhang & Zhang

Dean, T. L., & Lin, S. H. (1995). Decomposition techniques for planning in stochastic
domains. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pp. 11211127.
Drummond, M., & Bresina, J. (1990). Anytime synthetic projection: maximizing the probability of goal satisfaction. In Proceedings of National Conference on Artificial Intelligence (AAAI), pp. 138144.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. In Working Notes Fall AAAI Symposium on POMDPs, pp. 6168.
Hansen, E. A. (1998). Finite memory control of partially observable systems. Ph.D. thesis,
Dept of Computer Science, University of Massachusetts at Amherst.
Hansen, E. A., & Ziberstein, S. (1998). Heuristic search in cyclic AND/OR graphs. In
Proceedings of National Conference on Artificial Intelligence (AAAI), pp. 412417.
Hauskrecht, M. (1997). Incremental methods for computing bounds in partially observable Markov decision processes. In Proceedings of National Conference on Artificial
Intelligence (AAAI), pp. 734739.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov
decision processes. Journal of Artificial Intelligence Research, 13, 3394.
Hauskrecht, M., & Fraser, H. (1998). Modeling treatment of ischemic heart disease with
partially observable Markov decision processes. In American Medical Informatics
Association annual symposium on Computer Applications in Health Care, pp. 538
542. Orlando, Florida.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101 (1-2).
Koller, D., & Parr, R. (2000). Policy iteration for factored MDPs. In Proceedings of the
Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI), pp. 326334.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Efficient dynamic programming updates in partially observable Markov decision processes. Tech. rep. CS-95-19,
Department of Computer Science, Brown University.
Littman, M. L., Goldsmith, J., & Mundhenk, M. (1998). The computational complexity of
probabilistic planning. Journal of Artificial Intelligence Research, 9, 136.
Lovejoy, W. S. (1991). Computationally feasible bounds for partially observed Markov
decision processes. Operations Research, 39 (1), 162175.
Madani., O., Hanks, S., & Condon, A. (1999). On the undecidability of probabilistic planning and infinite horizon partially observable Markov decision problems..
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: theory,
models, and algorithms. Management Science, 28 (1), 116.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes. Mathematics of Operations Research, 12 (3), 441450.
Parr, R. (1998). Flexible decomposition algorithms for weakly coupled Markov decision
problems. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 422430.
164

fiRestricted Value Iteration: Theory and Algorithms

Parr, R., & Russell, S. (1995). Approximating optimal policies for partially observable
stochastic domains. In Proceedings of the 14th International Joint Conference on
Artificial Intelligence (IJCAI), pp. 10881094.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: an anytime algorithm for POMDPs. In International Joint Conference on Artificial Intelligence
(IJCAI), pp. 10251032.
Poupart, P., & Boutilier, C. (2002). Value-directed compresseion of POMDPs. In Proceedings of Advances in Neural Information Processing Systems (NIPS), pp. 15471554.
Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming. Wiley, New York, NY.
Roy, N., & Gordon, G. (2002). Exponential family PCA for belief compression in POMDPs.
In Proceedings of Advances in Neural Information Processing Systems (NIPS), pp.
16351642.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable
Markov processes over a finite horizon. Operations Research, 21, 10711088.
Sondik, E. J. (1971). The optimal control of partially observable decision processes. Ph.D.
thesis, Stanford University, Stanford, California, USA.
Washington, R. (1997). BI-POMDP: Bounded, incremental partially-observable Markovmodel planning. In Proceedings of the 4th European Conference on Planning (ECP),
pp. 440451.
Zhang, N. L., & Liu, W. (1997). A model approximation scheme for planning in partially
observable stochastic domains. Journal of Artificial Intelligence Research, 7, 199230.
Zhang, N. L., & Zhang, W. (2001a). Space-progressive value iteration: an anytime algorithm
for a class of POMDPs. In Sixth European Conference on Symbolic and Quantitative
Approaches to Reasoning with Uncertainty (ECSQARU), pp. 7283.
Zhang, N. L., & Zhang, W. (2001b). Speeding up the convergence of value iteration in
partially observable Markov decision processes. Journal of Artificial Intelligence Research, 14, 2951.
Zhang, W. (2001). Algorithms for partially observable Markov decision processes. Ph.D.
thesis, Department of Computer Science, the Hong Kong University of Science and
Technology.
Zhang, W., & Zhang, N. L. (2001). Solving informative partially observable Markov decision
processes. In Proceedings of the 6th European Conference on Planning (ECP).
Zhou, R., & Hansen, E. (2001). An improved grid-based approximation algorithm for
POMDPs. In Proceedings of the 17th International Joint Conference on Artificial
Intelligence, pp. 707716.
Zubek, V. B., & Dietterich, T. G. (2000). A POMDP approximation algorithm that anticipates the need to observe. In Proceedings of PRICAI-2000, pp. 521532. Lecture
Notes in Computer Science, New York: Springer-Verlag.

165

fiJournal of Artificial Intelligence Research 23 (2005) 4178

Submitted 07/04; published 01/05

Research Note
Extremal Behaviour in Multiagent Contract Negotiation
Paul E. Dunne

ped@csc.liv.ac.uk

Department of Computer Science
The University of Liverpool, Liverpool, UK

Abstract
We examine properties of a model of resource allocation in which several agents exchange resources in order to optimise their individual holdings. The schemes discussed relate to well-known negotiation protocols proposed in earlier work and we consider a number
of alternative notions of rationality covering both quantitative measures, e.g. cooperative
and individual rationality and more qualitative forms, e.g. Pigou-Dalton transfers. While
it is known that imposing particular rationality and structural restrictions may result in
some reallocations of the resource set becoming unrealisable, in this paper we address the
issue of the number of restricted rational deals that may be required to implement a particular reallocation when it is possible to do so. We construct examples showing that this
number may be exponential (in the number of resources m), even when all of the agent
utility functions are monotonic. We further show that k agents may achieve in a single
deal a reallocation requiring exponentially many rational deals if at most k  1 agents can
participate, this same reallocation being unrealisable by any sequences of rational deals in
which at most k  2 agents are involved.

1. Introduction
Mechanisms for negotiating allocation of resources within a group of agents form an important body of work within the study of multiagent systems. Typical abstract models
derive from game-theoretic perspectives in economics and among the issues that have been
addressed are strategies that agents use to obtain a particular subset of the resources available, e.g. (Kraus, 2001; Rosenschein & Zlotkin, 1994; Sandholm, 1999), and protocols by
which the process of settling upon some allocation of resources among the agents involved is
agreed, e.g. (Dignum & Greaves, 2000; Dunne, 2003; Dunne & McBurney, 2003; McBurney
et al., 2002).
The setting we are concerned with is encapsulated in the following definition.
Definition 1 A resource allocation setting is defined by a triple hA, R, Ui where
A = {A1 , A2 , . . . , An }

;

R = {r1 , r2 , . . . , rm }

are, respectively, a set of (at least two) agents and a collection of (non-shareable) resources.
A utility function, u, is a mapping from subsets of R to rational values. Each agent Ai  A
has associated with it a particular utility function ui , so that U is hu1 , u2 , . . . , un i. An
allocation P of R to A is a partition hP1 , P2 , . . . , Pn i of R. The value ui (Pi ) is called
the utility of the resources assigned to Ai . A utility function, u, is monotone if whenever
S  T it holds that u(S )  u(T ), i.e. the value assigned by u to any set of resources, T ,
is never less than the value u attaches to any subset, S of T .
c
2005
AI Access Foundation. All rights reserved.

fiDunne

Two major applications in which the abstract view of Definition 1 has been exploited are
e-commerce and distributed task realisation. In the first R represents some collection of
commodities offered for sale and individual agents seek to acquire a subset of these, the
value an agent attaches to a specific set being described by that agents utility function.
In task planning, the resource set describes a collection of sub-tasks to be performed in
order to realise some complex task, e.g. the complex task may be to transport goods
from a central warehouse to some set of cities. In this example R describes the locations
to which goods must be dispatched and a given allocation defines those places to which an
agent must arrange deliveries. The utility functions in such cases model the cost an agent
associates with carrying out its alloted sub-tasks.
Within the very general context of Definition 1, a number of issues arise stemming from
the observation that it is unlikely that some initial allocation will be seen as satisfactory
either with respect to the views of all agents in the system or with respect to divers global
considerations. Thus, by proposing changes to the initial assignment individual agents
seek to obtain a better allocation. This scenario raises two immediate questions: how to
evaluate a given partition and thus have a basis for forming improved or optimal allocations;
and, the issue underlying the main results of this paper, what restrictions should be imposed
on the form that proposed deals may take.
We shall subsequently review some of the more widely studied approaches to defining
conditions under which some allocations are seen as better than others. For the purposes
of this introduction we simply observe that such criteria may be either quantitative or
qualitative in nature. As an example of the former we have the approach wherein the
value of an allocation P is simply the sum of the values given by the agents utility
P
functions to the subsets of R they have been apportioned within P , i.e. ni=1 ui (Pi ): this
is the so-called utilitarian social welfare, which to avoid repetition we will denote by u (P ).
A natural aim for agents within a commodity trading context is to seek an allocation under
which u is maximised. One example of a qualitative criterion is envy freeness: informally,
an allocation, P , is envy-free if no agent assigns greater utility to the resource set (Pj ) held
by another agent than it does with respect to the resource set (Pi ) it has actually been
allocated, i.e. for each distinct pair hi , j i, ui (Pi )  ui (Pj ).
In very general terms there are two approaches that have been considered in treating the
question of how a finite collection of resources might be distributed among a set of agents
in order to optimise some criterion of interest: contract-net based methods, e.g. (Dunne
et al., 2003; Endriss et al., 2003; Endriss & Maudet, 2004b; Sandholm, 1998, 1999) deriving
from the work of Smith (1980); and combinatorial auctions, e.g. (Parkes & Ungar, 2000a,
2000b; Sandholm et al., 2001; Sandholm, 2002; Sandholm & Suri, 2003; Tennenholz, 2000;
Yokoo et al., 2004, amongst others). The significant difference between these is in the extent
to which a centralized controlling agent determines the eventual distribution of resources
among agents.
One may view the strategy underlying combinatorial auctions as investing the computational effort into a pre-processing stage following which a given allocation is determined.
Thus a controlling agent (the auctioneer) is supplied with a set of bids  pairs hSj , pj i
wherein Sj is some subset of the available resources and pj the price agent Aj is prepared
to pay in order to acquire Sj . The problem faced by the auctioneer is to decide which bids

42

fiExtremal Behaviour in Multiagent Contract Negotiation

to accept in order to maximise the overall profit subject to the constraint that each item
can be obtained by at most one agent.
What we shall refer to as contract-net schemes typically eschew the precomputation
stage and subordination to a controlling arbiter employed in auction mechanisms, seeking
instead to realise a suitable allocation by an agreed sequence of deals. The contract-net (in
its most general instantiation) for scenarios of m resources distributed among n agents is
the complete directed graph with n m vertices (each of which is associated with a distinct
allocation). In this way a possible deal hP , Qi is represented as an edge directed from the
vertex labelled with P to that labelled Q. Viewed thus, identifying a sequence of deals can
be interpreted as a search process which, in principle, individual agents may conduct in an
autonomous fashion.
Centralized schemes can be effective in contexts where the participants cooperate (in
the sense of accepting the auctioneers arbritration). In environments within which agents
are highly self-interested to the extent that their aims conflict with the auction process or
in which there is a high degree of uncertainty about the outcome, in working towards a
final allocation, the agents involved may only be prepared to proceed cautiously: that is,
an agent will only accept a proposed reallocation if satisfied that such would result in an
immediate improvement from its own perspective. In such cases, the process of moving from
the initial allocation, Pinit , to the eventual reallocation Pfin is by a sequence of local rational
deals, e.g. an agent might refuse to accept deals which reduced u because of the possibility
that it suffers an uncompensated loss in utility. A key issue here is the following: if the deal
protocol allows only moves in which at each stage some agent Aj offers a single resource to
another agent Aj then the rational reallocation hPinit , Pfin i can always be implemented; if,
however, every single move must be rational then hPinit , Pfin i may not be realisable.
We may, informally, regard the view of such agents as myopic, in the sense that they
are unwilling to accept a short-term loss (a deal hP , Qi under they might incur a loss of
utility) despite the prospect of a long-term gain (assuming u (Pfin ) > u (Pinit ) holds).
There are a number of reasons why an agent may adopt such views, e.g. consider the
following simple protocol for agreeing a reallocation.
A reallocation of resources is agreed over a sequence of stages, each of which
involves communication between two agents, Ai and Aj . This communication
consists of Ai issuing a proposal to Aj of the form (buy, r , p), offering to purchase
r from Aj for a payment of p; or (sell , r , p), offering to transfer r to Aj in return
for a payment p. The response from Aj is simply accept (following which the
deal is implemented) or reject.
This, of course, is a very simple negotiation structure, however consider its operation within
a two agent setting in which one agent, A1 say, wishes to bring about an allocation Pfin
(and thus can devise a plan  sequence of deals  to realise this from an initial allocation
Pinit ) while the other agent, A2 , does not know Pfin . In addition, assume that A1 is the only
agent that makes proposals and that a final allocation is fixed either when A1 is satisfied
or as soon as A2 rejects any offer.
While A2 could be better off if Pfin is realised, it may be the case that the only proposals
A2 will accept are those under which it does not lose, e.g. some agents may be sceptical
about the bona fides of others and will accept only deals from which they can perceive an
43

fiDunne

immediate benefit. There are several reasons why an agent may embrace such attitudes
within the schema outlined: once a deal has been implemented A2 may lose utility but no
further proposals are made by A1 so that the loss is permanent. We note that even if we
enrich the basic protocol so that A1 can describe Pfin , A2 may still reject offers under which
it suffers a loss, since it is unwilling to rely on the subsequent deals that would ameliorate
its loss actually being proposed. Although the position taken by A2 in the setting just
described may appear unduly cautious, we would claim that it does reflect real behaviour
in certain contexts. Outside the arena of automated allocation and negotiation in multiagent
systems, there are many examples of actions by individuals where promised long-term gains
are insufficient to engender the acceptance of short term loss. Consider chain letter
schemes (or their more subtle manifestation as pyramid selling enterprises): such have
a natural lifetime bounded by the size of the population in which they circulate, but may
break down before this is reached. Faced with a request to send $10 to the five names at
the head of the list and forward the letter to ten others after adding your name despite the
possibility of significant gain after a temporary loss of $50, to ignore such blandishments is
not seen as overly sceptical and cautious: there may be reluctance to accept that one will
eventually receive sufficient recompense in return and suspicion that the name order has
been manipulated.
In summary, we can identify two important influences that lead to contexts in which
agents prefer to move towards a reallocation via a sequence of rational deals. Firstly,
the agents are self-interested but operating in an unstable environment, e.g. in the chain
letter setting, an agent cannot reliably predict the exact point at which the chain will fail.
The second factor is that computational restrictions may limit the decisions an individual
agent can make about whether or not to accept a proposed deal. For example in settings
where all deals involve one resource at a time, A2 may reject a proposal to accept some
resource, r , since r is only useful following a further sequence of deals: if this number
of further deals is small then A2 could decide to accept the proposed deal since it has
sufficient computational power to determine that there is a context in which r is of value;
if this number is large however, then A2 may lack sufficient power to scan the search
space of future possibilities that would allow it to accept r . Notice that in the extreme
case, A2 makes its decision solely on whether r is of immediate use, i.e. A2 is myopic. A
more powerful A2 may be able to consider whether r is useful should up to k further deals
take place: in this case, A2 could still refuse to accept r since, although of use, A2 cannot
determine this with a bounded look ahead.
In total for the scenario we have described, if A1 wishes to bring about an allocation Pfin
then faced with the view adopted by A2 and the limitations imposed by the deal protocol,
the only effective plan that A1 could adopt is to find a sequence of rational deals to
propose to A2 .
Our aim in this article is to show that combining structural restrictions (e.g. only one
resource at a time is involved in a local reallocation) with rationality restrictions can result
in settings in which any sequence to realise a reallocation hP , Qi must involve exponentially
many (in |R|) separate stages. We refine these ideas in the next sub-section.

44

fiExtremal Behaviour in Multiagent Contract Negotiation

1.1 Preliminary Definitions
To begin, we first formalise the concepts of deal and contract path.
Definition 2 Let hA, R, Ui be a resource allocation setting. A deal is a pair hP , Qi where
P = hP1 , . . . , Pn i and Q = hQ1 , . . . , Qn i are distinct partitions of R. The effect of implementing the deal hP , Qi is that the allocation of resources specified by P is replaced with that
specified by Q. Following the notation of (Endriss & Maudet, 2004b) for a deal  = hP , Qi,
we use A to indicate the subset of A involved, i.e. Ak  A if and only if Pk 6= Qk .
Let  = hP , Qi be a deal. A contract path realising  is a sequence of allocations
 = hP (1) , P (2) , . . . , P (t1) , P (t) i
in which P = P (1) and P (t) = Q. The length of , denoted || is t  1, i.e. the number of
deals in .
There are two methods which we can use to reduce the number of deals that a single
agent may have to consider in seeking to move from some allocation to another, thereby
avoiding the need to choose from exponentially many alternatives: structural and rationality
constraints. Structural constraints limit the permitted deals to those which bound the
number of resources and/or the number of agents involved, but take no consideration of the
view any agent may have as to whether its allocation has improved. In contrast, rationality
constraints restrict deals hP , Qi to those in which Q improves upon P according to
particular criteria. In this article we consider two classes of structural constraint: Ocontracts, defined and considered in (Sandholm, 1998), and what we shall refer to as M (k )contracts.
Definition 3 Let  = hP , Qi be a deal involving a reallocation of R among A.
a.  is a one contract (O-contract) if
O1. A = {i , j }.
O2. There is a unique resource r  Pi  Pj for which Qi = Pi  {r } and Qj = Pj \ {r }
(with r  Pj ) or Qj = Pj  {r } and Qi = Pi \ {r } (with r  Pi )
b. For a value k  2, the deal  = hP , Qi is an M (k )-contract if 2  |A |  k and
iA Qi = iA Pi .
Thus, O-contracts involve the transfer of exactly one resource from a particular agent to
another, resulting in the number of deals compatible with any given allocation being exactly
(n  1)m: each of the m resources can be reassigned from its current owner to any of the
other n  1 agents.
Rationality constraints arise in a number of different ways. For example, from the
standpoint of an individual agent Ai a given deal hP , Qi may have three different outcomes:
ui (Pi ) < ui (Qi ), i.e. Ai values the allocation Qi as superior to Pi ; ui (Pi ) = ui (Qi ), i.e.
Ai is indifferent between Pi and Qi ; and ui (Pi ) > ui (Qi ), i.e. Ai is worse off after the
deal. When global optima such as utilitarian social welfare are to be maximised, there is
the question of what incentive there is for any agent to accept a deal hP , Qi under which it
45

fiDunne

is left with a less valuable resource holding. The standard approach to this latter question
is to introduce the notion of a pay-off function, i.e. in order for Ai to accept a deal under
which it suffers a reduction in utility, Ai receives some payment sufficient to compensate
for its loss. Of course such compensation must be made by other agents in the system who
in providing it do not wish to pay in excess of any gain. In defining notions of pay-off the
interpretation is that in any transaction each agent Ai makes a payment, i : if i < 0
then Ai is given i in return for accepting a deal; if i > 0 then Ai contributes i to the
amount to be distributed among those agents whose pay-off is negative.
This notion of sensible transfer is captured by the concept of individual rationality,
and is often defined in terms of an appropriate pay-off vector existing. It is not difficult,
however, to show that such definitions are equivalent to the following.
Definition 4 A deal hP , Qi is individually rational (IR) if and only if u (Q) > u (P ).
We shall consider alternative bases for rationality constraints later: these are primarily of
interest within so-called money free settings (so that compensatory payment for a loss in
utility is not an option).
The central issue of interest in this paper concerns the properties of the contract-net
graph when the allowed deals must satisfy both a structural and a rationality constraint.
Thus, if we consider arbitrary predicates  on deals hP , Qi  where the cases of interest are
 combining a structural and rationality condition  we have,
Definition 5 For  a predicate over distinct pairs of allocations, a contract path
hP (1) , P (2) , . . . , P (t1) , P (t) i
realising hP , Qi is a -path if for each 1  i < t, hP (i) , P (i+1) i is a -deal, that is
(P (i) , P (i+1) ) holds. We say that  is complete if any deal  may be realised by a -path.
We, further, say that  is complete with respect to -deals (where  is a predicate over
distinct pairs of allocations) if any deal  for which () holds may be realised by a -path.
The main interest in earlier studies of these ideas has been in areas such as identifying
necessary and/or sufficient conditions on deals to be complete with respect to particular
criteria, e.g. (Sandholm, 1998); and in establishing convergence and termination properties, e.g. Endriss et al. (2003), Endriss and Maudet (2004b) consider deal types, , such
that every maximal1 -path ends in a Pareto optimal allocation, i.e. one in which any
reallocation under which some agent improves its utility will lead to another agent suffering
a loss. Sandholm (1998) examines how restrictions e.g. with (P , Q) = > if and only if
hP , Qi is an O-contract, may affect the existence of contract paths to realise deals. Of
particular interest, from the viewpoint of heuristics for exploring the contract-net graph,
are cases where (P , Q) = > if and only if the deal hP , Qi is individually rational. For the
case of O-contracts the following are known:
Theorem 1
a. O-contracts are complete.
1. Maximal in the sense that if hP (1) , . . . , P (t) i is such a path, then for every allocation, Q, (P (t) , Q)
does not hold.

46

fiExtremal Behaviour in Multiagent Contract Negotiation

b. IR O-contracts are not complete with respect to IR deals.
In the consideration of algorithmic and complexity issues presented in (Dunne et al., 2003)
one difficulty with attempting to formulate reallocation plans by rational O-contracts is
already apparent, that is:
Theorem 2 Even in the case n = 2 and with monotone utility functions the problem of
deciding if an IR O-contract path exists to realise the IR deal hP , Qi is nphard.
Thus deciding if any rational plan is possible is already computationally hard. In this
article we demonstrate that, even if an appropriate rational plan exists, in extreme cases,
there may be significant problems: the number of deals required could be exponential in
the number of resources, so affecting both the time it will take for the schema outlined to
conclude and the space that an agent will have to dedicate to storing it. Thus in his proof
of Theorem 1 (b), Sandholm observes that when an IR O-contract path exists for a given
IR deal, it may be the case that its length exceeds m, i.e. some agent passes a resource to
another and then accepts the same resource at a later stage.
The typical form of the results that we derive can be summarised as:
For  a structural constraint (O-contract or M (k )-contract) and  a rationality
constraint, e.g. (P , Q) holds if hP , Qi is individually rational, there are resource allocation settings hAn , Rm , Ui in which there is a deal hP , Qi satisfying
all of the following.
a. hP , Qi is a -deal.
b. hP , Qi can be realised by a contract path on which every deal satisfies the
structural constraint  and the rationality constraint .
c. Every such contract path has length at least g(m).
For example, we show that there are instances for which the shortest IR O-contract path has
length exponential in m.2 In the next section we will be interested in lower bounds on the
values of the following functions: we introduce these in general terms to avoid unnecessary
subsequent repetition.
Definition 6 Let hA, R, Ui be a resource allocation setting. Additionally let  and  be
two predicates on deals. For a deal  = hP , Qi the partial function Lopt (, hA, R, Ui, )
is the length of the shortest -contract path realising hP , Qi if such a path exists (and is
undefined if no such path is possible). The partial function Lmax (hA, R, Ui, , ) is
Lmax (hA, R, Ui, , ) =

max
Lopt (, hA, R, Ui, )
-deals 

Finally, the partial function max (n, m, , ) is
max (n, m, , ) =

max

U=hu1 ,u2 ,...,un i

Lmax (hAn , Rm , Ui, , )

where consideration is restricted to those -deals  = hP , Qi for which a realising -path
exists.
2. Sandholm (1998) gives an upper bound on the length of such paths which is also exponential in m, but
does not explicitly state any lower bound other than that already referred to.

47

fiDunne

The three measures, Lopt , Lmax and max distinguish different aspects regarding the length
of contract-paths. The function Lopt is concerned with -paths realising a single deal hP , Qi
in a given resource allocation setting hA, R, Ui: the property of interest being the number
of deals in the shortest, i.e. optimal length, -path. We stress that Lopt is a partial function
whose value is undefined in the event that hP , Qi cannot be realised by a -path in the
setting hA, R, Ui. The function Lmax is defined in terms of Lopt , again in the context of
a specific resource allocation setting. The behaviour of interest for Lmax , however, is not
simply the length of -paths realising a specific hP , Qi but the worst-case value of Lopt
for deals which are -deals. We note the qualification that Lmax is defined only for -deals
that are capable of being realised by -paths, and thus do not consider cases for which no
appropriate contract path exists. Thus, if it should be the case that no -deal in the setting
hA, R, Ui can be realised by a -path then the value Lmax (hA, R, Ui, , ) is undefined, i.e.
Lmax is also a partial function. We may interpret any upper bound on Lmax in the following
terms: if Lmax (hA, R, Ui, , )  K then any -deal for which a -path exists can be
realised by a -path of length at most K .
Our main interest will centre on max which is concerned with the behaviour of Lmax as
a function of n and m and ranges over all n-tuples of utility functions hu : 2R  Qin . Our
approach to obtaining lower bounds for this function is constructive, i.e. for each h, i
that is considered, we show how the utility functions U may be defined in a setting with m
resources so as to yield a lower bound on max (n, m, , ). In contrast to the measures Lopt
and Lmax , the function max is not described in terms of a single fixed resource allocation
setting. It is, however, still a partial function: depending on hn, m, , i it may be the case
that in every n agent, m resource allocation setting, regardless of which choice of utility
functions is made, there is no -deal, hP , Qi capable of being realised by -path, and for
such cases the value of max (n, m, , ) will be undefined.3
It is noted, at this point, that the definition of max allows arbitrary utility functions
to be employed in constructing worst-case instances. While this is reasonable in terms
of general lower bound results, as will be apparent from the given constructions the utility
functions actually employed are highly artificial (and unlikely to feature in real application
settings). We shall attempt to address this objection by further considering bounds on the
following variant of max :
max
mono (n, m, , ) =

U=hu1 ,u2 ,...,un i

max
Lmax (hAn , Rm , Ui, , )
: each ui is monotone

Thus, max
mono deals with resource allocation settings within which all of the utility functions
must satisfy a monotonicity constraint.
The main results of this article are presented in the next sections. We consider two
general classes of contract path: O-contract paths under various rationality conditions in
3. In recognising the possibility that max (n, m, , ) could be undefined, we are not claiming that such
behaviour arises with any of the instantiations of h, i considered subsequently: in fact it will be
max
clear from the constructions that, denoting by max
(n, m, , ) for a fixed
, (n, m) the function 
instantiation of h, i, with the restricted deal types and rationality conditions examined, the function
max
, (n, m) is a total function. Whether it is possible to formulate sensible choices of h, i with
which max
, (n, m) is undefined for some values of hn, mi (and, if so, demonstrating examples of such) is,
primarily, only a question of combinatorial interest, whose development is not central to the concerns of
the current article.

48

fiExtremal Behaviour in Multiagent Contract Negotiation

Section 2; and, similarly, M (k )-contract paths for arbitrary values of k  2 in Section 3.
Our results are concerned with the construction of resource allocation settings hA, Rm , Ui
for which given some rationality requirement, e.g. that deals be individually rational, there
is some deal hP , Qi that satisfies the rationality condition, can be realised by a rational
O-contract path (respectively, M (k )-contract path), but with the number of deals required
by such paths being exponential in m. We additionally obtain slightly weaker (but still
exponential) lower bounds for rational O-contract paths within settings of monotone utility
functions, i.e. for the measure max
mono , outlining how similar results may be derived for
M (k )-contract paths.
In the resource allocation settings constructed for demonstrating these properties with
M (k )-contract paths, the constructed deal hP , Qi is realisable with a single M (k + 1)contract but unrealisable by any rational M (k  1)-contract path. We discuss related work,
in particular the recent study of (Endriss & Maudet, 2004a) that addresses similar issues
to those considered in the present article, in Section 4. Conclusions and some directions for
further work are presented in the final section.

2. Lower Bounds on Path Length  O-contracts
In this section we consider the issue of contract path length when the structural restriction
requires individual deals to be O-contracts. We first give an overview of the construction
method, with the following subsections analysing the cases of unrestricted utility functions
and, subsequently, monotone utility functions.
2.1 Overview
The strategy employed in proving our results involves two parts: for a given class of restricted contract paths we proceed as follows in obtaining lower bounds on max (n, m, , ).
a. For the contract-net graph partitioning m resources among n agents, construct a
path, m = hP (1) , P (2) , . . . , P (t) i realising a deal hP (1) , P (t) i. For the structural
constraint, 0 influencing  it is then proved that:
a1. The contract path m is a 0 -path, i.e. for each 1  i < t, the deal hP (i) , P (i+1 i
satisfies the structural constraint 0 .
a2. For any pair of allocations P (i) and P (i+j ) occurring in m , if j  2 then the
deal hP (i) , P (i+j ) i is not a 0 -deal.
Thus (a1) ensures that m is a suitable contract path, while (a2) will guarantee that
there is exactly one allocation, P (i+1) , that can be reached within m from any given
allocation P (i) in m by means of a 0 -deal.
b. Define utility functions Un = hu1 , . . . , un i with the following properties
b1. The deal hP (1) , P (t) i is a -deal.
b2. For the rationality constraint, 00 influencing , every deal hP (i) , P (i+1) i is a
00 -deal.

49

fiDunne

b3. For every allocation P (i) in the contract path  and every allocation Q other
than P (i+1) the deal hP (i) , Qi is not a -deal, i.e. it violates either the stuctural
constraint 0 or the rationality constraint 00 .
Thus, (a1) and (b2) ensure that hP (1) , P (t) i has a defined value with respect to the
function Lopt for the -deal hP (1) , P (t) i, i.e. a -path realising the deal is possible.
The properties given by (a2) and (b3) indicate that (within the constructed resource
allocation setting) the path m is the unique -path realising hP (1) , P (t) i. It follows
that t  1, the length of this path, gives a lower bound on the value of Lmax and hence
a lower bound on max (n, m, , ).
Before continuing it will be useful to fix some notational details.
We use Hm to denote the m-dimensional hypercube. Interpreted as a directed graph, Hm
has 2m vertices each of which is identified with a distinct m-bit label. Using  = a1 a2 . . . am
to denote an arbitrary such label, the edges of Hm are formed by
{ h, i :  and  differ in exactly one bit position}
We identify m-bit labels  = a1 a2 . . . am with subsets S  of Rm , via ri  S  if and only if
ai = 1. Similarly, any subset S of R can be described by a binary word, (S ), of length m,
i.e. (S ) = b1 b2 . . . bm with bi = 1 if and only if ri  S . For a label  we use || to denote
the number of bits with value 1, so that || is the size of the subset S  . If  and  are m-bit
labels, then  is a 2m-bit label, so that if Rm and Tm are disjoint sets, then  describes
the union of the subset S  of Rm with the subset S  of Tm . Finally if  = a1 a2 . . . am
is an m-bit label then  denotes the label formed by changing all 0 values in  to 1 and
vice versa. In this way, if S  is the subset of Rm described by  then  describes the set
Rm \ S  . To avoid an excess of superscripts we will, where no ambiguity arises, use  both
to denote the m-bit label and the subset of Rm described by it, e.g. we write    rather
than S   S  .
For n = 2 the contract-net graph induced by O-contracts can be viewed as the mdimensional hypercube Hm : the m-bit label,  associated with a vertex of Hm describing
the allocation h, i to hA1 , A2 i. In this way the set of IR O-contracts define a subgraph,
Gm of Hm with any directed path from (P ) to (Q) in Gm corresponding to a possible IR
O-contract path from the allocation hP , R \ P i to the allocation hQ, R \ Qi.
2.2 O-contract Paths  Unrestricted Utility Functions
Our first result clarifies one issue in the presentation of (Sandholm, 1998, Proposition 2):
in this an upper bound that is exponential in m is proved on the length of IR O-contract
paths, i.e. in terms of our notation, (Sandholm, 1998, Proposition 2) establishes an upper
bound on max (n, m, , ). We now prove a similar order lower bound.
Theorem 3 Let (P , Q) be the predicate which holds whenever hP , Qi is an IR O-contract
and (P , Q) that which holds whenever hP , Qi is IR. For m  7
max (2, m, , ) 

50



77
256



2m  2

fiExtremal Behaviour in Multiagent Contract Negotiation

Proof. Consider a path C = h1 , 2 , . . . , t i in Hm , with the following property4
 1  i < j  t (j  i + 2)  (i and j differ in at least 2 positions)

(SC)

e.g. if m = 4 then
, {r1 }, {r1 , r3 }, {r1 , r2 , r3 }, {r2 , r3 }, {r2 , r3 , r4 }, {r2 , r4 }, {r1 , r2 , r4 }
is such a path as it corresponds to the sequence h0000, 1000, 1010, 1110, 0110, 0111, 0101, 1101i.
Choose C (m) to be a longest such path with this property that could be formed in Hm ,
letting m = hP (1) , P (2) , . . . , P (t) i be the sequence of allocations with P (i) = hi , i i. We
now define the utility functions u1 and u2 so that for   Rm ,
(

u1 () + u2 () =

k
0

if
if

 = k
 6 {1 , 2 , . . . , t }

With this choice, the contract path m describes the unique IR O-contract path realising
the IR deal hP (1) , P (t) i: that m is an IR O-contract path is immediate, since
u (P (i+1) ) = i + 1 > i = u (P (i) )
That it is unique follows from the fact that for all 1  i  t and i + 2  j  t, the deal
hP (i) , P (j ) i is not an O-contract (hence there are no short-cuts possible), and for each
P (i) there is exactly one IR O-contract that can follow it, i.e. P (i+1) .5
From the preceding argument it follows that any lower bound on the length of C (m) ,
i.e. a sequence satisfying the condition (SC), is a lower bound on max (2, m, , ). These
paths in Hm were originally studied by Kautz (1958) in the context of coding theory and
the lower bound on their length of (77/256)2m  2 established in (Abbott & Katchalski,
1991).
2
Example 1 Using the path
C (4)

=
=

h0000, 1000, 1010, 1110, 0110, 0111, 0101, 1101i
h1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 i

in the resource allocation setting h{a1 , a2 }, {r1 , r2 , r3 , r4 }, hu1 , u2 ii, if the utility functions
are specified as in Table 1 below then u (h1 , 1 i) = 1 and u (h8 , 8 i) = 8. Furthermore,
C (4) describes the unique IR O-contract path realising the reallocation hh1 , 1 i, h8 , 8 ii
There are a number of alternative formulations of rationality which can also be considered.
For example
Definition 7 Let  = hP , Qi be a deal.
4. This defines the so-called snake-in-the-box codes introduced in (Kautz, 1958).
5. In our example with m = 4, the sequence h0000, 1000, 1001, 1101i, although defining an O-contract path
gives rise to a deal which is not IR, namely that corresponding to h1000, 1001i.

51

fiDunne

S
0000
0001
0010
0011
0100
0101
0110
0111

R\S
1111
1110
1101
1100
1011
1010
1001
1000

u1 (S )
1
0
0
0
0
4
3
3

u2 (R \ S )
0
0
0
0
0
3
2
3

u
1
0
0
0
0
7
5
6

1

7
5
6

S
1000
1001
1010
1011
1100
1101
1110
1111

R\S
0111
0110
0101
0100
0011
0010
0001
0000

u1 (S )
1
0
2
0
0
4
2
0

u2 (R \ S )
1
0
1
0
0
4
2
0

u
2
0
3
0
0
8
4
0

2
3

8
4

Table 1: Utility function definitions for m = 4 example.
a.  is cooperatively rational if for every agent, Ai , ui (Qi )  ui (Pi ) and there is at least
one agent, Aj , for whom uj (Qj ) > uj (Pj ).
b.  is equitable if miniA ui (Qi ) > miniA ui (Pi ).
c.  is a Pigou-Dalton deal if A = {i , j }, ui (Pi ) + uj (Pj ) = ui (Qi ) + uj (Qj ) and
|ui (Qi )  uj (Qj )| < |ui (Pi )  uj (Pj )| (where | . . . | is absolute value).
There are a number of views we can take concerning the rationality conditions given in Definition 7. One shared feature is that, unlike the concept of individual rationality for which
some provision to compensate agents who suffer a loss in utility is needed, i.e. individual
rationality presumes a money-based system, the forms defined in Definition 7 allow concepts of rationality to be given in money-free enviroments. Thus, in a cooperatively
rational deal, no agent involved suffers a loss in utility and at least one is better off. It may
be noted that given the characterisation of Definition 4 it is immediate that any cooperatively rational deal is perforce also individually rational; the converse, however, clearly does
not hold in general. In some settings, an equitable deal may be neither cooperatively nor
individually rational. One may interpret such deals as one method of reducing inequality
between the values agents place on their allocations: for those involved in an equitable deal,
it is ensured that the agent who places least value on their current allocation will obtain a
resource set which is valued more highly. It may, of course, be the case that some agents
suffer a loss of utility: the condition for a deal to be equitable limits how great such a loss
could be. Finally the concept of Pigou-Dalton deal originates from and has been studied in
depth within the theory of exchange economies. This is one of many approaches that have
been proposed, again in order to describe deals which reduce inequality between members
of an agent society, e.g. (Endriss & Maudet, 2004b). In terms of the definition given,
such deals encapsulate the so-called Pigou-Dalton principle in economic theory: that any
transfer of income from a wealthy individual to a poorer one should reduce the disparity
between them. We note that, in principle, we could define related rationality concepts
based on several extensions of this principle that have been suggested, e.g. (Atkinson, 1970;
Chateauneaf et al., 2002; Kolm, 1976).
Using the same O-contract path constructed in Theorem 3, we need only vary the
definitions of the utility functions employed in order to obtain,
Corollary 1 For each of the cases below,

52

fiExtremal Behaviour in Multiagent Contract Negotiation

a. () holds if and only if  is a cooperatively rational O-contract.
() holds if and only if  is cooperatively rational.
b. () holds if and only if  is an equitable O-contract.
() holds if and only if  is equitable.
c. () holds if and only if  is a Pigou-Dalton O-contract.
() holds if and only if  is a Pigou-Dalton deal.



max



(2, m, , ) 

77
2m  2
256


Proof. We employ exactly the same sequence of allocations m described in the proof of
Theorem 3 but modify the utility functions hu1 , u2 i for each case.
a. Choose hu1 , u2 i with u2 () = 0 for all   R and
(

u1 () =

k
0

if
if

 = k
 6 {1 , . . . , t }

The resulting O-contract path is cooperatively rational: the utility enjoyed by A2 remains constant while that enjoyed by A1 increases by 1 with each deal. Any deviation
from this contract path (employing an alternative O-contract) will result in a loss of
utility for A1 .
b. Choose hu1 , u2 i with u2 () = u1 () and
(

u1 () =

k
0

if
if

 = k
 6 {1 , . . . , t }

The O-contract path is equitable: both A1 and A2 increase their respective utility
values by 1 with each deal. Again, any O-contract deviating from this will result in
both agents losing some utility.
c. Choose hu1 , u2 i as
(

u1 () =

k
0

if
if

 = k
 6 {1 , . . . , t }

(

;

u2 () =

2m  k
2m

if
if

 = k
 6 {1 , . . . , t }

To see that the O-contract path consists of Pigou-Dalton deals, it suffices to note that
u1 (i ) + u2 (i ) = 2m for each 1  i  t. In addition, |u2 (i+1 )  u1 (i+1 )| = 2m  2i  2
which is strictly less than |u2 (i )  u1 (i )| = 2m  2i . Finally, any O-contract hP , Qi which
deviates from this sequence will not be a Pigou-Dalton deal since
|u2 (Q2 )  u1 (Q1 )| = 2m > |u2 (P2 )  u1 (P1 )|
which violates one of the conditions required of Pigou-Dalton deals.
The construction for two agent settings, easily extends to larger numbers.
53

2

fiDunne

Corollary 2 For each of the choices of h, i considered in Theorem 3 and Corollary 1,
and all n  2,


77
max
 (n, m, , ) 
2m  2
256
Proof. Fix allocations in which A1 is given 1 , A2 allocated 1 , and Aj assigned  for each
3  j  n. Using identical utility functions hu1 , u2 i as in each of the previous cases, we
employ for uj : uj () = 1, uj (S ) = 0 whenever S 6=  (h, i as in Theorem 3); uj (S ) = 0 for
all S (Corollary 1(a)); uj () = 2m , uj (S ) = 0 whenever S 6=  (Corollary 1(b)); and, finally,
uj (S ) = 2m for all S , (Corollary 1(c)). Considering a realisation of the -deal hP (1) , P (t) i
the only -contract path admissible is the path m defined in the related proofs. This gives
the lower bound stated.
2
We note, at this point, some other consequences of Corollary 1 with respect to (Endriss &
Maudet, 2004b, Theorems 1, 3), which state
Fact 1 We recall that a -path, hP (1) , . . . , P (t) i is maximal if for each allocation Q, hP (t) , Qi
is not a -deal.
a. If hP (1) , . . . , P (t) i is any maximal path of cooperatively rational deals then P (t) is
Pareto optimal.
b. If hP (1) , . . . , P (t) i is any maximal path of equitable deals then P (t) maximises the
value e (P ) = min1in ui (Pi ), i.e. the so-called egalitarian social welfare.
The sequence of cooperatively rational deals in Corollary 1(a) terminates in the Pareto
optimal allocation P (t) : the allocation for A2 always has utility 0 and there is no allocation
to A1 whose utility can exceed t. Similarly, the sequence of equitable deals in Corollary 1(b)
terminates in the allocation P (t) , for which e (P (t) ) = t the maximum that can be attained
for the instance defined. In both cases, however, the optima are reached by sequences of
exponentially many (in m) deals: thus, although Fact 1 guarantees convergence of particular
deal sequences to optimal states it may be the case, as illustrated in Corollary 1(ab), that
the process of convergence takes considerable time.
2.3 O-contract Paths  Monotone Utility Functions
We conclude our results concerning O-contracts by presenting a lower bound on max
mono , i.e.
the length of paths when the utility functions are required to be monotone.
In principle one could attempt to construct appropriate monotone utility functions that
would have the desired properties with respect to the path used in Theorem 3. It is, however,
far from clear whether such a construction is possible. We do not attempt to resolve this
question here. Whether an exact translation could be accomplished is, ultimately, a question
of purely combinatorial interest: since our aim is to demonstrate that exponential length
contract paths are needed with monotone utility functions we are not, primarily, concerned
with obtaining an optimal bound.

54

fiExtremal Behaviour in Multiagent Contract Negotiation

Theorem 4 With (P , Q) and (P , Q) be defined as in Theorem 3 and m  14

max
mono (2, m, , )




 
77
m/2  3


 128 2

if

m is even






if

m is odd

77
128



2(m1)/2  3

Proof. We describe the details only for the case of m being even: the result when m is
odd is obtained by a simple modification which we shall merely provide in outline.
Let m = 2s with s  7. For any path
s = h1 , 2 , . . . , t i
in Hs (where i describes a subset of Rs by an s-bit label), the path double(s ) in H2s is
defined by
double(s )

=
=

h 1 1 , 2 2 , . . . , i i , i+1 i+1 , . . . , t t i
h1 , 3 , . . . , 2i1 , 2i+1 , . . . , 2t1 i

(The reason for successive indices of  increasing by 2 will become clear subsequently)
Of course, double(s ) does not describe an O-contract path6 : it is, however, not difficult
to interpolate appropriate allocations, 2i , in order to convert it to such a path. Consider
the subsets 2i (with 1  i < t) defined as follows:
(

2i =

i+1 i
i i+1

if
if

i  i+1
i  i+1

If we now consider the path, ext(s ), within H2s given by
ext(s ) = h1 , 2 , 3 , . . . , 2(t1) , 2t1 i
then this satisfies,
a. If s has property (SC) of Theorem 3 in Hs then ext(s ) has property (SC) in H2s .
b. If j is odd then |j | = s.
c. If j is even then |j | = s + 1.
From (a) and the bounds proved in (Abbott & Katchalski, 1991) we deduce that ext(s )
can be chosen so that with P (i) denoting the allocation hi , i i
d. ext(s ) describes an O-contract path from P (1) to P (2t1) .
e. For each pair hi , j i with j  i + 2, the deal hP (i) , P (j ) i is not an O-contract.
f. If s is chosen as in the proof of Theorem 3 then the number of deals in ext(s ) is
as given in the statement of the present theorem.
6. In terms of the classification described by Sandholm (1998), it contains only swap deals (S -contracts):
each deal swaps exactly one item in 2i1 with an item in 2i1 in order to give 2i+1 .

55

fiDunne

We therefore fix s as the path from Theorem 3 so that in order to complete the proof
we need to construct utility functions hu1 , u2 i that are monotone and with which ext(s )
defines the unique IR O-contract path realising the reallocation hP (1) , P (2t1) i.
The choice for u2 is relatively simple. Given S  R2s ,
u2 (S ) =



 0

2t + 1


2t + 2

if
if
if

|S |  s  2
|S | = s  1
|S |  s

In this t is the number of allocations in s . The behaviour of u2 is clearly monotone.
The construction for u1 is rather more complicated. Its main idea is to make use of
the fact that the size of each set i occurring in ext(s ) is very tightly constrained: |i |
is either s or s + 1 according to whether i is odd or even. We first demonstrate that each
set of size s + 1 can have at most two strict subsets (of size s) occurring within ext(s ):
thus, every S of size s + 1 has exactly 2 or 1 or 0 subsets of size s on ext(s ). To see this
suppose the contrary. Let , 2i1 , 2j 1 , and 2k 1 be such that || = s + 1 with
2i1   ; 2j 1   ; 2k 1  
Noting that 2i1 = i i and that s has the property (SC) it must be the case that (at
least) two of the s-bit labels from {i , j , k } differ in at least two positions. Without loss
of generality suppose this is true of i and k . As a result we deduce that the sets 2i1
and 2k 1 have at most s  2 elements in common, i.e. |2i1  2k 1 |  s  2: 2i1 = i i
and 2k 1 = k k so in any position at which i differs from k , i differs from k at
exactly the same position. In total |2i1 \ 2k 1 |  2, i.e. there are (at least) two elements
of 2i1 that do not occur in 2k 1 ; and in the same way |2k 1 \ 2i1 |  2, i.e. there are
(at least) two elements of 2k 1 that do not occur in 2i1 . The set , however, has only
s + 1 members and so cannot have both 2i1 and 2k 1 as subsets: this would require
2i1  2k 1  2i1 \ 2k 1  2k 1 \ 2i1  
but, as we have just seen,
| 2i1  2k 1  2i1 \ 2k 1  2k 1 \ 2i1 |  s + 2
One immediate consequence of the argument just given is that for any set  of size s +1 there
are exactly two strict subsets of  occurring on ext(s ) if and only if  = 2i1 2i+1 = 2i
for some value of i with 1  i < t. We can now characterise each subset of R2s of size s + 1
as falling into one of three categories.
C1. Good sets, given by { :  = 2i }.
C2. Digressions, consisting of
{  : 2i1  ,  6= 2i and i < t}
C3. Inaccessible sets, consisting of
{  :  is neither Good nor a Digression}
56

fiExtremal Behaviour in Multiagent Contract Negotiation

Good sets are those describing allocations to A1 within the path defined by ext(s );
Digressions are the allocations that could be reached using an O-contract from a set of
size s on ext(s ), i.e. 2i1 , but differ from the set that actually occurs in ext(s ), i.e.
2i . Finally, Inaccessible sets are those that do not occur on ext(s ) and cannot be reached
via an O-contract from any set on ext(s ). We note that we view any set of size s + 1
that could be reached by an O-contract from 2t1 as being inaccessible: in principle it is
possible to extend the O-contract path beyond 2t1 , however, we choose not complicate
the construction in this way.
We now define u1 as

u1 () =


2i  1





2i + 1


 2i

0




0




2t  1

if
if
if
if
if
if

 = 2i1
 = 2i
|| = s + 1 and  is a Digression from 2i1
||  s  1
|| = s and  6 ext(s )
 is Inaccessible or ||  s + 2

It remains only to prove for these choices of hu1 , u2 i that the O-contract path hP (1) , . . . , P (2t1) i
defined from ext(s ) is the unique IR O-contract path realising the IR deal hP (1) , P (2t1) i
and that u1 is monotone.
To show that hP (1) , . . . , P (2t1) i is IR we need to demonstrate
 1  j < 2t  1 u1 (j ) + u2 (j ) < u1 (j +1 ) + u2 (j +1 )
We have via the definition of hu1 , u2 i
u1 (2i1 ) + u2 (2i1 )

=
<
=
<
=

2(t + i ) + 1
u1 (2i ) + u2 (2i )
2(t + i ) + 2
u1 (2i+1 ) + u2 (2i+1 )
2(t + i ) + 3

Thus, via Definition 4, it follows that ext(s ) gives rise to an IR O-contract path.
To see that this path is the unique IR O-contract path implementing hP (1) , P (2t1) i,
consider any position P (j ) = hj , j i and allocation Q other than P (j +1) or P (j 1) . It may be
assumed that the deal hP (j ) , Qi is an O-contract. If j = 2i 1 then u (P (2i1) ) = 2(t +i )+1
and |j | = s. Hence |Q1 |  {s 1, s +1}. In the former case, u1 (Q1 ) = 0 and u2 (Q2 ) = 2t +2
from which u (Q) = 2t + 2 and thus hP (j ) , Qi is not IR. In the latter case u1 (Q1 ) = 2i
since Q1 is a Digression from 2i1 and u2 (Q2 ) = 2t + 1 giving u (Q) = 2(t + i ) + 1. Again
hP (j ) , Qi fails to be IR since Q fails to give any increase in the value of u . We are left with
the case j = 2i so that u (P (2i) ) = 2(t + i ) + 2 and |j | = s + 1. Since hP (j ) , Qi is assumed
to be an O-contract this gives |Q1 |  {s, s + 2}. For the first possibility Q1 could not be a
set on ext(s ): 2i1 and 2i+1 are both subsets of 2i and there can be at most two such
subsets occurring on ext(s ). It follows, therefore, that u1 (Q1 ) = 0 giving u (Q) = 2t + 2
so that hP (j ) , Qi is not IR. In the second possibility, u1 (Q1 ) = 2t  1 but u2 (Q2 ) = 0 as
|Q2 | = s  2 so the deal would result in an overall loss. We deduce that for each P (j ) the
only IR O-contract consistent with it is the deal hP (j ) , P (j +1) i.
57

fiDunne

The final stage is to prove that the utility function u1 is indeed a monotone function.
Suppose S and T are subsets of R2s with S  T . We need to show that u1 (S )  u1 (T ). We
may assume that |S | = s, that S occurs as some set within ext(s ), and that |T | = s + 1.
If |S | < s or |S | = s but does not occur on ext(s ) we have u1 (S ) = 0 and the required
inequality holds; if |S |  s + 1 then in order for S  T to be possible we would need
|T |  s + 2, which would give u1 (T ) = 2t  1 and this is the maximum value that any
subset is assigned by u1 . We are left with only |S | = s, |T | = s + 1 and S on ext(s ) to
consider. It has already been shown that there are at most two subsets of T that can occur
on ext(s ). Consider the different possibilities:
a. T = 2i so that exactly two subsets of T occur in ext(s ): 2i1 and 2i+1 . Since
u1 (2i ) = 2i + 1 and this is at least max{u1 (2i1 ), u1 (2i+1 )}, should S be either of
2i1 or 2i+1 then u1 (S )  u1 (T ) as required.
b. T is a Digression from S = 2i1 , so that u1 (T ) = 2i and u1 (S ) = 2i  1 and, again,
u1 (S )  u1 (T ).
We deduce that u1 is monotone completing our lower bound proof for max
mono for even values
of m.
We conclude by observing that a similar construction can be used if m = 2s + 1 is odd:
use the path ext(s ) described above but modifying it so that one resource (rm ) is always
held by A2 . Only minor modifications to the utility function definitions are needed.
2
Example 2 For s = 3, we can choose 3 = h000, 001, 101, 111, 110i so that t = 5. This
gives double(3 ) as
h000111, 001110, 101010, 111000, 110001i
with the O-contract path being defined from ext(3 ) which is
=

h000111, 001111, 001110, 101110, 101010, 111010, 111000, 111001, 110001i
h1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 i

Considering the 15 subsets of size s + 1 = 4, gives
Good
Digression
Inaccessible

=
=
=

{001111, 101110, 111010, 111001}
{010111, 100111, 101011, 011110, 111100}
{011011, 011101, 101101, 110110, 110011, 110101}

Notice that both of the sets in {110011, 110101} are Inaccessible: in principle we could
continue from 9 = 110001 using either, however, in order to simplify the construction the
path is halted at 9 .
Following the construction presented in Theorem 4, gives the following utility function
definitions with S  R = {r1 , r2 , r3 , r4 , r5 , r6 }.

u2 (S ) =



 0

11

 12

58

if
if
if

|S |  1
|S | = 2
|S |  3

fiExtremal Behaviour in Multiagent Contract Negotiation

For u1 we obtain

u1 (S ) =





































































0
0
1
2
2
3
3
4
5
5
6
7
7
8
9
9
9

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

|S |  2
|S | = 3 and S 6 {000111, 001110, 101010, 111000, 110001}
S = 000111 (1 )
S = 010111 (digression from 1 )
S = 100111 (digression from 1 )
S = 001111 (2 )
S = 001110 (3 )
S = 011110 (digression from 3 )
S = 101110 (4 )
S = 101010 (5 )
S = 101011 (digression from 5 )
S = 111010 (6 )
S = 111000 (7 )
S = 111100 (digression from 7 )
S = 111001 (8 )
S = 110001 (9 )
|S |  5 or S  {011011, 011101, 101101, 110110, 110011, 110101}

The monotone utility functions, hu1 , u2 i, employed in proving Theorem 4 are defined so that
the path arising from ext(s ) is IR: in the event of either agent suffering a loss of utility the
gain made by the other is sufficient to provide a compensatory payment. A natural question
that now arises is whether the bound obtained in Theorem 4 can be shown to apply when
the rationality conditions preclude any monetary payment, e.g. for cases where the concept
of rationality is one of those given in Definition 7. Our next result shows that if we set the
rationality condition to enforce cooperatively rational or equitable deals then the bound of
Theorem 4 still holds.
Theorem 5 For each of the cases below and m  14
a. () holds if and only if  is a cooperatively rational O-contract.
() holds if and only if  is cooperatively rational.
b. () holds if and only if  is an equitable O-contract.
() holds if and only if  is equitable.

max
mono (2, m, , )




 
77
m/2  3


 128 2

if

m is even






if

m is odd

77
128



2(m1)/2  3

Proof. We again illustrate the constructions only for the case of m being even, noting the
modification to deal with odd values of m outlined at the end of the proof of Theorem 4.
The path ext(s ) is used for both cases.

59

fiDunne

For (a), we require hu1 , u2 i to be defined as monotone functions with which ext(s ) will
be the unique cooperatively rational O-contract path to realise the cooperatively rational
deal hP (1) , P (2t1) i where P (j ) = hj , j i. In this case we set hu1 , u2 i to be,

hu1 (), u2 ()i =


hi , i i





hi
+ 1, i i




hi , i  1i


h0, 2t  1i




h0, 2t  1i




h2t  1, 0i

if
if
if
if
if
if

 = 2i1
 = 2i
|| = s + 1 and  is a Digression from 2i1
||  s  1
|| = s and  6 ext(s )
 is Inaccessible or ||  s + 2

Since,
hu1 (2i1 ), u2 (2i1 )i
hu1 (2i ), u2 (2i )i
hu1 (2i+1 ), u2 (2i+1 )i

=
=
=

hi , i i
hi + 1, i i
hi + 1, i + 1i

it is certainly the case that hP (1) , P (2t1) i and all deals on the O-contract path defined
by ext(s ) are cooperatively rational. Furthermore if Q = h, i is any allocation other
than P (j +1) then the deal hP (j ) , Qi will fail to be a cooperatively rational O-contract.
For suppose the contrary letting hP (j ) , Qi without loss of generality be an O-contract,
with Q 6 {P (j 1) , P (j +1) }  we can rule out the former case since we have already shown
such an deal is not cooperatively rational. If j = 2i  1 so that hu1 (j ), u2 (j )i = hi , i i
then ||  {s  1, s + 1}: the former case leads to a loss in utility for A1 ; the latter,
(since  is a Digression from 2i1 ) a loss in utility for A2 . Similarly, if j = 2i so that
hu1 (j ), u2 (j )i = hi + 1, i i then ||  {s, s + 2}: for the first  6 ext(s ) leading to a loss of
utility for A1 ; the second results in a loss of utility for A2 . It follows that the path defined by
ext(s ) is the unique cooperatively rational O-contract path that realises hP (1) , P (2t1) i.
It remains only to show that these choices for hu1 , u2 i define monotone utility functions.
Consider u1 and suppose S and T are subsets of R2s with S  T . If |S |  s  1,
or S does not occur on ext(s ) then u1 (S ) = 0. If |T |  s + 2 or is Inaccessible then
u1 (T ) = 2t  1 which is the maximum value attainable by u1 . So we may assume that
|S | = s, occurs on ext(s ), i.e. S = 2i1 , for some i , and that |T | = s + 1 and is either
a Good set or a Digression. From the definition of u1 , u1 (S ) = i : if T  {2i , 2i2 } then
u1 (T )  i = u1 (S ); if T is a Digression from 2i1 then u1 (T ) = i = u1 (S ). We deduce
that if S  T then u1 (S )  u1 (T ), i.e. the utility function is monotone.
Now consider u2 with S and T subsets of R2s having S  T . If |T |  s + 1 or
R2s \ T does not occur in ext(s ) then u2 (T ) = 2t  1 its maximal value. If |S |  s  2
or R2s \ S is Inaccessible then u2 (S ) = 0. Thus we may assume that T = 2i1 giving
u2 (T ) = i and |S | = s  1, so that R2s \ S is either a Digression or one of the Good sets
{2i , 2i2 }. If R2s \ S is a Digression then u2 (S ) = i  1; if it is the Good set 2i2 then
u2 (S ) = i  1 < u2 (T ); if it is the Good set 2i then u2 (S ) = i = u2 (T ). It follows that
u2 is monotone completing the proof of part (a).

60

fiExtremal Behaviour in Multiagent Contract Negotiation

For (b) we use,

hu1 (), u2 ()i =


h2i  1, 2i i





h2i + 1, 2i i












h2i , 2i  1i
h0, 2t  1i
h0, 2t  1i
h2t  1, 0i

if
if
if
if
if
if

 = 2i1
 = 2i
|| = s + 1 and  is a Digression from 2i1
||  s  1
|| = s and  6 ext(s )
 is Inaccessible or ||  s + 2

These choices give ext(s ) as the unique equitable O-contract path to realise the equitable
deal hP (1) , P (2t1) i, since
min{u1 (2i1 ), u2 (2i1 )}
min{u1 (2i ), u2 (2i )}
min{u1 (2i+1 ), u2 (2i+1 )}

=
=
=

2i  1
2i
2i + 1

each deal hP (j ) , P (j +1) i is equitable. If Q = h, i is any allocation other than P (j +1)
then the deal hP (j ) , Qi is not an equitable O-contract. Assume that hP (j ) , Qi is an Ocontract, and that Q 6 {P (j 1) , P (j +1) }. If j = 2i  1, so that P (j ) = h2i1 , 2i1 i
and min{u1 (2i1 ), u2 (2i1 )} = 2i  1 then ||  {s  1, s + 1}. In the first of these
min{u1 (), u2 ()} = 0; in the second min{u1 (), u2 ()} = 2i  1 since  must be a
Digression. This leaves only j = 2i with P (j ) = h2i , 2i i and min{u1 (2i ), u2 (2i )} = 2i .
For this, ||  {s, s + 2}: if || = s then min{u1 (), u2 ()}  2i  1 (with equality when
 = 2i1 ); if || = s + 2 then min{u1 (), u2 ()} = 0. In total these establish that ext(s )
is the unique equitable O-contract path realising the equitable deal hP (1) , P (2t1) i.
That the choices for hu1 , u2 i describe monotone utility functions can be shown by a
similar argument to that of part (a).
2
Example 3 For s = 3 using the same O-contract path ext(3 ) as the previous example,
i.e.
=

h000111, 001111, 001110, 101110, 101010, 111010, 111000, 111001, 110001i
h1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 i

For hu1 , u2 i in (a) we obtain

hu1 (S ), u2 (R \ S )i =















































h0, 9i
h0, 9i
h1, 1i
h1, 0i
h1, 0i
h2, 1i
h2, 2i
h2, 1i
h3, 2i
h3, 3i
h3, 2i
h4, 3i
h4, 4i
h4, 3i
h5, 4i
h5, 5i
h9, 0i

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

|S |  2
|S | = 3 and S 6 {000111, 001110, 101010, 111000, 110001}
S = 000111 (1 )
S = 010111 digression from 1
S = 100111 digression from 1
S = 001111 (2 )
S = 001110 (3 )
S = 011110 digression from 3
S = 101110 (4 )
S = 101010 (5 )
S = 101011 digression from 5
S = 111010 (6 )
S = 111000 (7 )
S = 111100 digression from 7
S = 111001 (8 )
S = 110001 (9 )
|S |  5 or S  {011011, 011101, 101101, 110110, 110011, 110101}

61

fiDunne

Similarly, in (b)

hu1 (S ), u2 (R \ S )i =















































h0, 9i
h0, 9i
h1, 2i
h2, 1i
h2, 1i
h3, 2i
h3, 4i
h4, 3i
h5, 4i
h5, 6i
h6, 5i
h7, 6i
h7, 8i
h8, 7i
h9, 8i
h9, 10i
h9, 0i

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

|S |  2
|S | = 3 and S 6 {000111, 001110, 101010, 111000, 110001}
S = 000111 (1 )
S = 010111 digression from 1
S = 100111 digression from 1
S = 001111 (2 )
S = 001110 (3 )
S = 011110 digression from 3
S = 101110 (4 )
S = 101010 (5 )
S = 101011 digression from 5
S = 111010 (6 )
S = 111000 (7 )
S = 111100 digression from 7
S = 111001 (8 )
S = 110001 (9 )
|S |  5 or S  {011011, 011101, 101101, 110110, 110011, 110101}

That we can demonstrate similar extremal behaviours for contract path length with
rationality constraints in both money-based (individual rationality) and money-free (cooperative rationality, equitable) settings irrespective of whether monotonicity properties are
assumed, has some interesting parallels with other contexts in which monotonicity is relevant. In particular we can observe that in common with the complexity results already
noted from (Dunne et al., 2003)  deciding if an allocation is Pareto optimal, if an allocation maximises u , or if an IR O-contract path exists  requiring utility functions to be
monotone does not result in a setting which is computationally more tractable.

3. M (k )-contract paths
We now turn to similar issues with respect to M (k )-contracts, recalling that in one respect
these offer a form of deal that does not fit into the classification of Sandholm (1998). This
classification defines four forms of contract type: O-contracts, as considered in the previous
section; S -contracts, that involve exactly 2 agents swapping single resources; C -contracts,
in which one agent tranfers at least two of its resources to another; and M -contracts in which
three or more agents reallocate their resource holding amongst themselves. Our definition
of M (k )-contracts permits two agents to exchange resources (thus are not M -contracts in
Sandholms (1998) scheme) and the deals permitted are not restricted to O, S , and C contracts. In one regard, however, M (k )-contracts are not as general as M -contracts since
a preset bound (k ) is specified for the number of agents involved.
Our main result on M (k )-contract paths is the following development of Theorem 3.
Theorem 6 Let k (P , Q) be the predicate which
 holds whenever hP , Qi is an IR M (k )k
contract. For all k  3, n  k and m 
, there is a resource allocation setting
2
hA, R, Ui and an IR deal  = hP , Qi for which,
Lopt (, hA, R, Ui, k )
Lopt (, hA, R, Ui, k 1 )
Lopt (, hA, R, Ui, k 2 )

=


62

1
2b2m/k (k 1)c

1
is undefined

(a)
(b)
(c)

fiExtremal Behaviour in Multiagent Contract Negotiation

Before presenting the proof, we comment about the formulation of the theorem statement
and give an overview of the proof structure.
We first note that the lower bounds (where defined) have been phrased in terms of
the function Lopt as opposed to max used in the various results on O-contract paths in
Section 2.2. It is, of course, the case that the bound claimed for Lopt (, hA, R, Ui, k 1 )
will also be a lower bound on max (n, m, k 1 , ) when n  k and (P , Q) holds whenever
the deal hP , Qi is IR. The statement of Theorem 6, however, claims rather more than this,
namely that a specific resource allocation setting hA, R, Ui can be defined for each n  k
and each m, together with an IR deal hP , Qi in such a way that: hP , Qi can be achieved by
a single M (k )-contract and cannot be realised by an IR M (k  2)-contract path. Recalling
that Lopt is a partial function, the latter property is equivalent to the claim made in part
(c) for the deal hP , Qi of the theorem statement. Furthermore, this same deal although
achievable by an IR M (k  1)-contract path can be so realised only by one whose length is
as given in part (b) of the theorem statement.
Regarding the proof itself, there are a number of notational complexities which we have
attempted to ameliorate by making some simplifying assumptions concerning the relationship between m  the size of the resource set R  and k  the number of agents which are
needed to realise hP
 , Qi
 in a single IR deal. In particular, we shall assume that m is an
exact multiple of k2 . We observe that by employing a similar device to that used in
the proof
 of Theorem 4 we can deal with cases for which
 m does not have this property: if
k
m = s 2 + q for integer values s  1 and 1  q < k2 , we simply employ exactly the
same construction using m q resources with the missing q resources from Rm being allocated to A1 and never being reallocated within the M (k  1)-contract path. This approach
accounts for the rounding operation (b. . .c) in the exponent term of the lower bound. We
shall also assume that the number of agents in A is exactly k . Within the proof we use a
running example for which k = 4 and m = 18 = 3  6 to illustrate specific features.
We first give an outline of its structure.
Given hA, R, Ui a resource allocation setting involving k agents and m resources, our
aim is to define an IR M (k  1)-contract path
 = hP (1) , P (2) , . . . , P (t) i
that realises the IR M (k ) deal hP (1) , P (t) i. We will use d to index particular allocations
within , so that 1  d  t.
In order to simplify the presentation we employ
 a setting in which the k agents are
A = {A0 , A1 , . . . , Ak 1 }. Recalling that m = s k2 , the resource set Rm is formed by
the union of



k
2



pairwise disjoint sets of size s. Given distinct values i and j with
{i,j }

{i,j }

{i,j }

0  i < j  k  1, we use Ri,j to denote one of these subsets with {r1 , r2
, . . . , rs }
{i,j
}
the s resources that form R
.
There are two main ideas underpinning the structure of each M (k  1)-contract in .
Firstly, in the initial and subsequent allocations, the resource set R{i,j } is partitioned
between Ai and Aj and any reallocation of resources between Ai and Aj that takes place
within the deal hP (d) , P (d+1) i will involve only resources in this set. Thus, for every al(d)
location P (d) and each pair {i , j }, if h 6 {i , j } then Ph  R{i,j } = . Furthermore, for
63

fiDunne

 = hP (d) , P (d+1) i should both Ai and Aj be involved, i.e. {Ai , Aj }  A , then this reallocation of R{i,j } between Ai and Aj will be an O-contract. That is, either exactly one
(d)
(d+1)
element of R{i,j } will be moved from Pi to become a member of the allocation Pj
or
(d)

exactly one element of R{i,j } will be moved from Pj

to become a member of the allocation

(d+1)
Pi  . Intotal, every M (k  1)-contract  in  consists of a simultaneous implementation
1
of k 
O-contracts: a single O-contract for each of the distinct pairs {Ai , Aj } of agents
2
from the k  1 agents in A .

The second key idea is to exploit one well-known property of the s-dimensional hypercube network: for every s  2, Hs contains a Hamiltonian cycle, i.e. a simple directed cycle
formed using only the edges of Hs and containing all 2s vertices.7 Now, suppose
S (v ) = v (0) , v (1) , . . . , v (i) , . . . , v (2

s 1)

, v (0)

is a Hamiltonian cycle in the hypercube Hs and
S (w ) = w (0) , w (1) , . . . , w (i) , . . . , w (2

s 1)

, w (0)

the Hamiltonian cycle in which w (i) is obtained by complementing each bit in v (i) . As we
have described in the overview of Section 2.1 we can interpret the s-bit label v = v1 v2 . . . vs
{i,j }
as describing a particular subset of R{i,j } , i.e. that subset in which rk
occurs if and only if
{i,j
}
vk = 1. Similarly from any subset of R
we may define a unique s-bit word. Now suppose
(d)
that Pi is the allocation held by Ai in the allocation P (d) of . The deal  = hP (d) , P (d+1) i
(d)
(d+1)
will affect Pi  R{i,j } in the following way: if i 6 A or j 6 A then Pi
 R{i,j } =
(d)
(d+1)
(d)
Pi  R{i,j } and Pj
 R{i,j } = Pj  R{i,j } . Otherwise we have {i , j }  A and
(d)

(d)

the (complementary) holdings Pi  R{i,j } and Pj  R{i,j } define (complementary) s-bit
labels of vertices in Hs : if these correspond to places hv (h) , w (h) i in the Hamiltonian cycles,
(d+1)
(d+1)
(d+1)
(d+1)
then in Pi
and Pj
the s-bit labels defined from Pi
 R{i,j } and Pj
 R{i,j }
produce the s-bit labels v (h+1) and w (h+1) , i.e. the vertices that succeed v (h) and w (h) in
the Hamiltonian cycles. In total, for each j , Ai initially holds either the subset of R{i,j } that
maps to v (0) or that maps to w (0) and, at the conclusion of the M (k  1)-path, holds the
s
s
subset that maps to v (2 1) (or w (2 1) ). The final detail is that the progression through the
Hamiltonian cycles is conducted over a series of rounds each round comprising k M (k  1)deals.
(d)
(d+1) i that occurs in this path  can
We have noted that each
 M (k1)-contract, hP , P
k 1
be interpreted as a set of
distinct O-contracts. An important property of the utility
2
functions employed is that unless p  k  1 there will beno individually
rational M (p)
1
contract path that realises the deal hP (d) , P (d+1) i, i.e. the k 
O-contract
deals must
2
occur simultaneously in order for the progression from P (d) to P (d+1) to be IR. Although
the required deal could be realised by a sequence of O-contracts (or, more generally, any
suitable M (k  2)-contract path), such realisations will not describe an IR contract path.
7. This can be shown by an easy inductive argument. For s = 2, the sequence h00, 01, 11, 10, 00i defines a
Hamiltonian cycle in H2 . Inductively assume that h1 , 2 , . . . , p , 1 i (with p = 2s ) is such a cycle in
Hs then h01 , 11 , 1p , 1p1 , . . . , 12 , 02 . . . , 0p , 01 i defines a Hamiltonian cycle in Hs+1 .

64

fiExtremal Behaviour in Multiagent Contract Negotiation

The construction of utility functions to guarantee such behaviour provides the principal
component in showing that the IR deal hP (1) , P (t) i cannot be realised with an IR M (k  2)contract path: if Q is any allocation for which hP (1) , Qi is an M (k  2)-contract then
hP (1) , Qi is not IR.
We now proceed with the proof of Theorem 6.
Proof. (of Theorem 6) Fix A = {A0 , A1 , . . . , Ak 1 }. R consists of
sets of s resources
{i,j } {i,j }
R{i,j } = {r1 , r2 , . . . , rs{i,j } }



k
2



pairwise disjoint

For k = 4 and s = 3 these yield A = {A0 , A1 , A2 , A3 } and
R{0,1}
R{0,2}
R{0,3}
R{1,2}
R{1,3}
R{2,3}

=
=
=
=
=
=

{0,1}

{0,1}

{0,1}

, r2
, r3
}
{r1
{0,2} {0,2} {0,2}
{r1
, r2
, r3
}
{0,3} {0,3} {0,3}
{r1
, r2
, r3
}
{1,2} {1,2} {1,2}
, r2
, r3
}
{r1
{1,3} {1,3} {1,3}
{r1
, r2
, r3
}
{2,3} {2,3} {2,3}
{r1
, r2
, r3
}

We use two ordering structures in defining the M (k  1)-contract path.
a.
S (v ) = v (0) , v (1) , . . . , v (i) , . . . , v (2

s 1)

, v (0)

a Hamiltonian cycle in Hs , where without loss of generality, v (0) = 111 . . . 11.
b.
S (w ) = w (0) , w (1) , . . . , w (i) , . . . , w (2

s 1)

, w (0)

the complementary Hamiltonian cycle to this, so that w (0) = 000 . . . 00.
Thus for k = 4 and s = 3 we obtain
a.
b.

S (v ) = h111, 110, 010, 011, 001, 000, 100, 101i
S (w ) = h000, 001, 101, 100, 110, 111, 011, 010i

We can now describe the M (k  1)-contract path.
 = hP (1) , P (2) , . . . , P (t) i

Initial Allocation: P (1) .
Define the k  k Boolean matrix, B = [bi,j ] (with 0  i , j  k  1) by

bi,j =



 

b

j ,i

 b
i,j 1

65

if
if
if

i =j
i >j
i <j

fiDunne

We then have for each 1  i  k ,
(1)

Pi

=

i1
[

{ R {j ,i} : bi,j = >} 

j =0

k[
1

{ R {i,j } : bi,j = >}

j =i+1

Thus, in our example,





B = 



>


>


>


>



>

>








Yielding the starting allocation
(1)

P0
(1)
P1
(1)
P2
(1)
P3

=
=
=
=

R{0,1}  R{0,3}
R{1,2}
R{0,2}  R{2,3}
R{1,3}

=
=
=
=

h111, 000, 111i
h000, 111, 000i
h111, 000, 111i
h000, 111, 000i






R{0,1}  R{0,2}  R{0,3}
R{0,1}  R{1,2}  R{1,3}
R{0,2}  R{1,2}  R{2,3}
R{0,3}  R{1,3}  R{2,3}

(1)

The third column in Pi indicating the 3-bit labels characterising each of the subsets of
R{i,j } for the three values that j can assume.
Rounds: The initial allocation is changed over a series of rounds
Q 1, Q 2, . . . , Q z
each of which involves exactly k distinct M (k  1)-contracts. We use Q x ,p to indicate the
allocation resulting after stage p in round x where 0  p  k  1. We note the following:
a. The initial allocation, P (1) will be denoted by Q 0,k 1 .
b. Q x ,0 is obtained using a single M (k  1)-contract from Q x 1,k 1 (when x  1).
c. Q x ,p is obtained using a single M (k  1)-contract from Q x ,p1 (when 0 < p  k  1).
Our final item of notation is that of the cube position of i with respect to j in an allocation
P , denoted (i , j , P ). Letting u be the s-bit string describing Pi  R{i,j } in some allocation
(1)
P , (i , j , P ) is the index of u in the Hamiltonian cycle S (v ) (when R{i,j }  Pi ) or the
(1)
Hamiltonian cycle S (w ) (when R{i,j }  Pj ). When P = Q x ,p for some allocation in the
sequence under construction we employ the notation (i , j , x , p), noting that one invariant
of our path will be (i , j , x , p) = (j , i , x , p), a property that certainly holds true of P (1) =
Q 0,k 1 since (i , j , 0, k  1) = (j , i , 0, k  1) = 0.
The sequence of allocations in  is built as follows. Since Q 1,0 is the immediate successor
of the initial allocation Q 0,k 1 , it suffices to describe how Q x ,p is formed from Q x ,p1 (when
p > 0) and Q x +1,0 from Q x ,k 1 . Let Q y,q be the allocation to be formed from Q x ,p . The
deal  = hQ x ,p , Q y,q i will be an M (k  1) contract in which A = A \ {Aq }. For each pair
{i , j }  A we have (i , j , x , p) = (j , i , x , p) in the allocation Q x ,p . In moving to Q y,q
exactly one element of R{i,j } is reallocated between Ai and Aj in such a way that in Q y,q ,
66

fiExtremal Behaviour in Multiagent Contract Negotiation

(i , j , y, q) = (i , j , x , p)+1, since Ai and Aj are tracing complementary Hamiltonian cycles
with respect to R{i,j } this ensures that (j , i , y, q) = (j , i , x , p) + 1, thereby maintaining
the invariant property.
Noting that for each distinct pair hi , j i, we either have R{i,j } allocated to Ai in P (1)
or R{i,j } allocated to Aj in P (1) , the description just outlined indicates that the allocation
P (d) = Q x ,p is completely specified as follows.
The cube position, (i , j , x , p), satisfies,

(i , j , x , p) =


0





 1 + (i , j , x  1, k  1)

(i , j , x  1, k  1)




1 + (i , j , x , p  1)




(i , j , x , p  1)

if
if
if
if
if

x = 0 and p = k  1
x  1, p = 0, and p 6 {i , j }
x  1, p = 0, and p  {i , j }
1  p  k  1, and p 6 {i , j }
1  p  k  1, and p  {i , j }

For each i , the subset of R{i,j } that is held by Ai in the allocation Q x ,p is,

v ((i,j ,x ,p))
w ((i,j ,x ,p))

if
if

(1)

R{i,j }  Pi
(1)
R{i,j }  Pj

(where we recall that s-bit labels in the hypercube Hs are identified with subsets
of R{i,j } .)
The tables below illustrates this process for our example.

d
1
2
3
4
5
6
7
8
9
..
.

x
0
1
1
1
1
2
2
2
2
..
.

p
3
0
1
2
3
0
1
2
3
..
.

i j
01
111
111
111
110
010
010
010
011
001

A0
i j i j
02 03
000 111
000 111
001 110
001 010
101 010
101 011
100 001
100 001
110 001
..
.
Subsets

i j
10
000
000
000
001
101
101
101
100
110

A1
i j
12
111
110
110
110
010
011
011
011
001
..
.

i j
13
000
001
001
101
101
100
100
110
110

i j
20
111
111
110
110
010
010
011
011
001

of R{i,j } held by Ai

67

A2
i j i j
21 23
000 111
001 110
001 010
001 010
101 010
100 011
100 001
100 001
110 001
..
.
in Q x ,p (k

i j
30
000
000
001
101
101
101
100
110
110
= 4,

A3
(d1)
,P (d) i
i j i j AhP
31 32
111 000

110 001 {A1 , A2 , A3 }
110 101 {A0 , A2 , A3 }
010 101 {A0 , A1 , A3 }
010 101 {A0 , A1 , A2 }
011 100 {A1 , A2 , A3 }
011 110 {A0 , A2 , A3 }
001 110 {A0 , A1 , A3 }
001 110 {A0 , A1 , A2 }
..
..
.
.
s = 3)

fiDunne

d
1
2
3
4
5
6
7
8
9
..
.

x
0
1
1
1
1
2
2
2
2
..
.

p
3
0
1
2
3
0
1
2
3
..
.

A0
A1
A2
A3
(d1) ,P (d) i
i j i j i j i j i j i j i j i j i j i j i j i j AhP
01 02 03 10 12 13 20 21 23 30 31 32
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
1
1
0
1
1
0
1
1 {A1 , A2 , A3 }
0
1
1
0
1
1
1
1
2
1
1
2 {A0 , A2 , A3 }
1
1
2
1
1
2
1
1
2
2
2
2 {A0 , A1 , A3 }
2
2
2
2
2
2
2
2
2
2
2
2 {A0 , A1 , A2 }
2
2
2
2
3
3
2
3
3
2
3
3 {A1 , A2 , A3 }
2
3
3
2
3
3
3
3
4
3
3
4 {A0 , A2 , A3 }
3
3
4
3
3
4
3
3
4
4
4
4 {A0 , A1 , A3 }
4
4
4
4
4
4
4
4
4
4
4
4 {A0 , A1 , A2 }
..
..
..
..
..
.
.
.
.
.
Cube Positions (i , j , x , p) (k = 4, s = 3)

It is certainly the case that this process of applying successive rounds of k deals could be
continued, however, we wish to do this only so long as it is not possible to go from some
allocation P (d) in the sequence to another P (d+r ) for some r  2 via an M (k  1)-contract.
Now if Q x ,p and Q y,q are distinct allocations generated by the process above then the
deal  = hQ x ,p , Q y,q i is an M (k  1)-contract if and only if for some Ai , Qix ,p = Qiy,q . It
follows that if hP (d) , P (d+r ) i is an M (k  1)-contract for some r > 1, then for some i and
(d+r )
(d)
all j 6= i , Pi
 R{i,j } = Pi  R{i,j } .
(d+r )
(d)
To determine the minimum value of r > 1 with which Pi
= Pi , we observe that
without loss of generality we need consider only the case d = i = 0, i.e. we determine
(1)
the minimum number of deals before P0 reappears. First note that in each round, Q x , if
(0, j , x  1, k  1) = p then (0, j , x , k  1) = p + k  2, i.e. each round advances the cube
position k  2 places: (0, j , x  1, k  1) = (0, j , x , 0) and (0, j , x , j ) = (0, j , x , j  1).
(1)
We can also observe that P0 = Q00,k 1 6= Q0x ,p for any p with 0 < p < k  1, since
(0, 1, x , p) = (0, 2, x , p) = . . . = (0, k  1, x , p)
only in the cases p = 0 and p = k  1. It follows that our value r > 1 must be of the form
qk where q must be such that q(k  2) is an exact multiple of 2s . From this observation we
see that,
(1)

min{ r > 1 : P0

(1+r )

= P0

} = min{ qk : q(k  2) is a multiple of 2s }

Now, if k is odd then q = 2s is the minimal such value, so that r = k 2s . If k is even then
it may be uniquely written in the form z 2l + 2 where z is odd so giving q as 1 (if l  s) or
2sl (if l  s), so that these give r = k and r = z 2s + 2sl+1 , e.g. for k = 4 and s = 3, we
(1)
(17)
get k = 1  21 + 2 so that r = 23 + 231+1 = 16 and in our example P0 = P0 may be
easily verified. In total,
r


s

 k2

k


 2s

if
if
if

k is odd
k = z 2l + 2, z is odd, and l  s
k = z 2l + 2, z is odd and l  s
68

fiExtremal Behaviour in Multiagent Contract Negotiation

All of which immediately give r  2s (in the second case k  2s , so the inequality holds
s
trivially), and thus we
 can
 continue the chain of M (k  1) contracts for at least 2 moves.
Recalling that m = s k2 , this gives the length of the M (k  1)-contract path
 = hP (1) , P (2) , . . . , P (t) i
written in terms of m and k as at least8
m/



2

k
2


2m

 1 = 2 k (k 1)  1

It remains to define appropriate utility functions U = hu0 , . . . , uk 1 i in order to ensure that
 is the unique IR M (k  1)-contract path realising the IR M (k )-deal hP (1) , P (t) i. In
defining U it will be convenient to denote  as the path
 = hQ 0,k 1 , Q 1,0 , Q 1,1 , . . . , Q 1,k 1 , . . . , Q x ,p , . . . , Q r ,k 1 i
and, since rk  2s , we may without loss of generality, focus on the first 2s allocations in
this contract path.
Recalling that (i , j , x , p) is the index of the s-bit label u corresponding to Qix ,p  R{i,j }
in the relevant Hamiltonian cycle  i.e. S (v ) if R{i,j }  Qi0,k , S (w ) if R{i,j }  Qj0,k 1  we
note the following properties of the sequence of allocations defined by  that hold for each
distinct i and j .
P1.  x , p (i , j , x , p) = (j , i , x , p)
P2. If Q y,q is the immediate successor of Q x ,p in  then (i , j , y, q)  (i , j , x , p) + 1
with equality if and only if q 6 {i , j }.
P3.  i 0 , j 0 with 0  i 0 , j 0  k  1, (i , j , x , k  1) = (i 0 , j 0 , x , k  1).
The first two properties have already been established in our description of . The third
follows from the observation that within each round Q x , each cube position is advanced by
exactly k  2 in progressing from Q x 1,0 to Q x ,k 1 .
The utility function ui is now given, for S  Rm , by
( P

j 6=i
2km

ui (S ) =

(i , j , x , p)

if S = Qix ,p for some 0  x  r , 0  p  k  1
otherwise

We claim that, with these choices,
 = hQ 0,k 1 , Q 1,0 , Q 1,1 , . . . , Q 1,k 1 , . . . , Q x ,p , . . . , Q r ,k 1 i
is the unique IR M (k  1)-contract path realising the IR M (k )-deal hQ 0,k 1 , Q r ,k 1 i. Certainly,  is an IR M (k  1)-contract path: each deal  = hQ x ,p , Q y,q i on this path has
|A | = k  1 and since for each agent Ai in A = A \ {Aq } the utility of Qiy,q has increased
8. We omit the
operation b. . .c in the exponent, which is significant only if m is not an exact
 rounding

multiple of

k
2

, in which event the device described in our overview of the proof is applied.

69

fiDunne

by exactly k  2, i.e. each cube position of i with respect to j whenever q 6 {i , j } has
increased, it follows that u (Q y,q ) > u (Q x ,p ) and hence hQ x ,p , Q y,q i is IR.
We now show that  is the unique IR M (k  1)-contract path continuation of Q 0,k 1
Suppose  = hQ x ,p , P i is a deal that deviates from the contract path  (having followed
it through to the allocation Q x ,p ). Certainly both of the following must hold of P : for
each i , Pi  j 6=i R{i,j } ; and there is a k -tuple of pairs h(x0 , p0 ), . . . , (xk 1 , pk 1 )i with
which Pi = Qixi ,pi , for if either fail to be the case for some i , then ui (Pi ) = 2km with the
consequent effect that u (P ) < 0 and thence not IR. Now, if Q y,q is the allocation that
would succeed Q x ,p in  then P 6= Q y,q , and thus for at least one agent, Qixi ,pi 6= Qiy,q .
It cannot be the case that Qixi ,pi corresponds to an allocation occurring strictly later than
Qiy,q in  since such allocations could not be realised by an M (k  1)-contract. In addition,
since Pi = Qixi ,pi it must be the case that |A | = k  1 since exactly k  1 cube positions in
the holding of Ai must change. It follows that there are only two possibilities for (yi , pi ):
Pi reverts to the allocation immediately preceding Qix ,p or advances to the holding Qiy,q .
It now suffices to observe that a deal in which some agents satisfy the first of these while
the remainder proceed in accordance with the second either does not give rise to a valid
allocation or cannot be realised by an M (k 1)-contract. On the other hand if P corresponds
to the allocation preceding Q x ,p then  is not IR. We deduce, therefore, that the only IR
M (k  1) deal that is consistent with Q x ,p is that prescribed by Q y,q .
This completes the analysis needed for the proof of part (b) of the theorem. It is
clear that since the system contains only k agents, any deal hP , Qi can be effected with
a single M (k )-contract, thereby establishing part (a). For part (c)  that the IR deal
hP (1) , P (t) i cannot be realised using an individually rational M (k  2)-contract path, it
suffices to observe that since the class of IR M (k  2)-contracts are a subset of the class
of IR M (k  1)-contracts, were it the case that an IR M (k  2)-contract path existed to
implement hP (1) , P (t) i, this would imply that  was not the unique IR M (k  1)-contract
path. We have, however, proved that  is unique, and part (c) of the theorem follows. 2
We obtain a similar development of Corollary 1 in
Corollary 3 For all k  3, n  k , m 



k
2



and each of the cases below,

a. k () holds if and only if  is a cooperatively rational M (k )-contract.
() holds if and only if  is cooperatively rational.
b. k () holds if and only if  is  is an equitable M (k )-contract.
() holds if and only if  is is equitable.
there is a resource allocation setting hA, R, Ui and a -deal  = hP , Qi for which
Lopt (, hA, R, Ui, k )
Lopt (, hA, R, Ui, k 1 )
Lopt (, hA, R, Ui, k 2 )

=


1
2b2m/k (k 1)c  1
is undefined

(a)
(b)
(c)

Proof. As with the proof of Corollary 1 in relation to Theorem 3, in each case we employ
the contract path from the proof of Theorem 6, varying the definition of U = hu1 , u2 , . . . , uk i
in order to establish each result. Thus let
m

=
=

hP (1) , P (2) , . . . , P (r ) , . . . , P (t) i
hQ 0,k 1 , Q 1,0 , . . . , Q x ,p , . . . , Q z ,r i
70

fiExtremal Behaviour in Multiagent Contract Negotiation

be the M (k  1)-contract path realising the M (k )-deal hP (1) , P (t) i described in the proof
of Theorem 6, this path having length t  2b2m/k (k  1)  1.
a. The utility functions U = hu0 , . . . , uk 1 i of Theorem 6 ensure that hP (1) , P (t) i is
cooperatively rational and that m is a cooperatively rational M (k  1)-contract
path realising hP (1) , P (t) i: the utility held by Ai never decreases in value and there is
at least one agent (in fact exactly k  1) whose utility increases in value. Furthermore
m is the unique cooperatively rational M (k  1)-contract path realising hP (1) , P (t) i
since, by the same argument used in Theorem 6, any deviation will result in some
agent suffering a loss of utility.
b. Set the utility functions U = hu0 , . . . , uk 1 i as,

ui (S ) =



1





xk 2 + k  i



2

(x  1)k + k + p


(x  1)k 2 + k  i + p + 1





xk 2 + 1


 xk 2 + 1 + p  i

if
if
if
if
if
if

S
S
S
S
S
S

6= Qix ,p for any Q x ,p  m
= Qix ,k 1
= Q0x ,p , p < k  1 and i = 0
= Qix ,p , p < i  1 and i 6= 0.
= Qix ,i1 = Qix ,i and i 6= 0.
= Qix ,p , p > i and i 6= 0

To see that these choices admit m as an equitable M (k  1)-contract path realising
the equitable deal hQ 0,k 1 , Q z ,r i, we first note that
min

0ik 1

{ui (Qiz ,r )} > 1 =

min

0ik 1

{ui (Qi0,k 1 )}

thus, hQ 0,k 1 , Q z ,r i is indeed equitable. Consider any deal  = hQ x ,p , Q y,q i occurring
within m . It suffices to show that
min

0ik 1

{ui (Qix ,p )} 6= uq (Qqx ,p )

since Aq 6 A , and for all other agents ui (Qiy,q ) > ui (Qix ,p ). We have two possibilities:
q = 0 (in which case p = k  1 and y = x + 1); q > 0 (in which case p = q  1).
Consider the first of these: u0 (Q0x ,k 1 ) = xk 2 + k , however,
,k 1
)
min{ui (Qix ,k 1 )} = xk 2 + 1 = uk 1 (Qkx1

and hence every deal hQ x ,k 1 , Q x +1,0 i forming part of m is equitable.
In the remaining case, uq (Qqx ,q1 ) = xk 2 + 1 and
min{ui (Qix ,q1 )}


=
<
=
<
=

u0 (Q0x ,q1 )
(x  1)k 2 + k + q  1
xk 2  (k 2  2k + 1)
xk 2  (k  1)2
xk 2 + 1
uq (Qqx ,q1 )

and thus the remaining deals hQ x ,q1 , Q x ,q i within m are equitable. By a similar
argument to that employed in Theorem 6 it follows that m is the unique equitable
M (k  1)-contract path realising hQ 0,k 1 , Q z ,r i.
2

71

fiDunne

Monotone Utility Functions and M (k )-contract paths
The device used to develop Theorem 3 to obtain the path of Theorem 4 can be applied
to the rather more intricate construction of Theorem 6, thereby allowing exponential lower
bounds on max
mono (n, m, k , ) to be derived. We will merely outline the approach rather than
present a detailed technical exposition. We recall that it became relatively straightforward
to define suitable monotone utility functions once it was ensured that the subset sizes of
interest  i.e. those for allocations arising in the O-contract path  were forced to fall into
a quite restricted range. The main difficulty that arises in applying similar methods to the
path  of Theorem 6 is the following: in the proof of Theorem 4 we consider two agents
so that converting s from a setting with s resources in Theorem 3 to ext(s ) with 2s
resources in Theorem 4 is achieved by combining complementary allocations, i.e.   Rs
with   Ts . We can exploit two facts, however, to develop a path multi () for which
monotone
utility functions could be defined: the resource set Rm in Theorem 6 consists of


k
disjoint sets of size s; and any deal  on the path  involves a reallocation of R{i,j }
2
between Ai and Aj when {i , j }  A . Thus letting Tm be formed by
T {i,j } each of size s, suppose that
(d)

(d)
Pi

(d)



k
2



disjoint sets,

is described by
(d)

(d)

(d)

i,0 i,1    i,i1 i,i+1    i,k 1
(d)

with i,j the s-bit label corresponding to the subset of R {i,j } that is held by Ai in P (d) .
Consider the sequence of allocations,
multi () = hC (1) , C (2) , . . . , C (t) i
(d)

in a resource allocation setting have k agents and 2m resources  Rm  Tm for which Ci
is characterised by
(d) (d)
(d)
(d)
(d)
i,0 i,1    i,i1 i,i+1    i,k 1
(d)

In this, i,j , indicates the subset of R{i,j }  T {i,j } described by the 2s-bit label,
(d)

i,j
(d)

(d)

(d)

= i,j i,j
(d)

i.e. i,j selects a subset of R{i,j } while i,j a subset of T {i,j } .
It is immediate from this construction that for each allocation C (d) in multi () and each
(d)
Ai , it is always the case that |Ci | = (k  1)s. It follows, therefore, that the only subsets
that are relevant to the definition of monotone utility functions with which an analogous
result to Theorem 6 for the path multi () could be derived, are those of size (k  1)s: if
S  Rm Tm has |S | < (k 1)s, we can fix ui (S ) as a small enough negative value; similarly
if |S | > (k  1)s then ui (S ) can be set to a large enough positive value.9
Our description in the preceding paragraphs, can be summarised in the following result, whose proof is omitted: extending the outline given above to a formal lower bound
9. It is worth noting that the interpolation stage used in Theorem 4 is not needed in forming multi():
the deal hC (d) , C (d+1) i is an M (k 1)-contract. We recall that in going from s of Theorem 3 to ext(s )
the intermediate stage  double(s )  was not an O-contract path.

72

fiExtremal Behaviour in Multiagent Contract Negotiation

proof, is largely a technical exercise employing much of the analysis already introduced, and
since nothing signifcantly new is required for such an analysis we shall not give a detailed
presentation of it.
Theorem 7 Let k (P , Q) be the predicate which
 holds whenever hP , Qi is an IR M (k )contract. For all k  3, n  k and m  2 k2 , there is a resource allocation setting
hA, R, Ui in which every u  U is monotone, and an IR deal  = hP , Qi for which,
Lopt (, hA, R, Ui, k )
Lopt (, hA, R, Ui, k 1 )
Lopt (, hA, R, Ui, k 2 )

=


1
2bm/k (k 1)c

1
is undefined

(a)
(b)
(c)

4. Related Work
The principal focus of this article has considered a property of contract paths realising rational reallocations hP , Qi when the constituent deals are required to conform to a structural
restriction and satisfy a rationality constraint. In Section 2 the structural restriction limited
deals to those involving a single resource, i.e. O-contracts. For the rationality constraint
forcing deals strictly to improve utilitarian social welfare, i.e. to be individually rational
(IR) we have the following properties.
a. There are resource allocation settings hA, R, Ui within which there are IR reallocations
hP , Qi that cannot be realised by a sequence of IR O-contracts. (Sandholm, 1998,
Proposition 2)
b. Every IR reallocation, hP , Qi, that can be realised by an IR O-contract path, can be
realised by an IR O-contract path of length at most n m  (n  1)m. (Sandholm, 1998,
Proposition 2)
c. Given hA, R, Ui together with an IR reallocation hP , Qi the problem of deciding if
hP , Qi can be implemented by an IR O-contract path is nphard, even if |A| = 2 and
both utility functions are monotone. (Dunne et al., 2003, Theorem 11).
d. There are resource allocation settings hA, R, Ui within which there are IR reallocations
hP , Qi that can be realised by an IR O-contract path, but with any such path having
length exponential in m. This holds even in the case |A| = 2 and both utility functions
are monotone. (Theorem 3 and Theorem 4 of Section 2)
In a recent article Endriss and Maudet (2004a) analyse contract path length also considering
O-contracts with various rationality constraints. Although the approach is from a rather
different perspective, the central question addressed  How many rational deals are required
to reach an optimal allocation?, (Endriss & Maudet, 2004a, Table 1, p. 629)  is closely
related to the issues discussed above. One significant difference in the analysis of rational Ocontracts from Sandholms (1998) treatment and the results in Section 2 is that in (Endriss
& Maudet, 2004a) the utility functions are restricted so that every rational reallocation
hP , Qi can be realised by a rational O-contract path. The two main restrictions examined
P
are requiring utility functions to be additive, i.e. for every S  R, u(S ) = r S u(r );
73

fiDunne

and, requiring the value returned to be either 0 or 1, so-called 0  1 utility functions.
Additive utility functions are considered in the case of IR O-contracts (Endriss & Maudet,
2004a, Theorems 3, 9), whereas 01 utility functions for cooperatively rational O-contracts
max
(Endriss & Maudet, 2004a, Theorems 4, 11). Using max
add (n, m, , ) and 01 (n, m, , )
to denote the functions introduced in Definition 6 where all utility functions are additive
(respectively 01), cf. the definition of max
mono , then with 1 (P , Q) holding if hP , Qi is an IR
O-contract; 2 (P , Q) holding if hP , Qi is a cooperatively rational O-contract and (P , Q)
true when hP , Qi is IR, we may formulate Theorems 9 and 11 of (Endriss & Maudet, 2004a)
in terms of the framework used in Definition 6, as
max
add (n, m, 1 , )
max
01 (n, m, 2 , )

=
=

m
m

(Endriss & Maudet, 2004a, Theorem 9)
(Endriss & Maudet, 2004a, Theorem 11)

We can, of course, equally couch Theorems 3 and 4 of Section 2 in terms of the shortestpath convention adopted in (Endriss & Maudet, 2004a), provided that the domains of
utility and reallocation instances are restricted to those for which an appropriate O-contract
path exists. Thus, we can obtain the following development of (Endriss & Maudet, 2004a,
Table 1) in the case of O-contracts.
Utility Functions
Rationality
Shortest Path
Complete

Additive
IR
m
Yes

0-1
CR
m
Yes

Unrestricted
IR
(2m )
No

Monotone
IR
(2m/2 )
No

Unrestricted
CR
(2m )
No

Monotone
CR
(2m/2 )
No

Table 2: How many O-contract rational deals are required to reach an allocation?
Extension of Table 1 from (Endriss & Maudet, 2004a, p. 629)

5. Conclusions and Further Work
Our aim in this article has been to develop the earlier studies of Sandholm (1998) concerning
the scope and limits of particular practical contract forms. While Sandholm (1998) has
established that insisting on individual rationality in addition to the structural restriction
prescribed by O-contracts leads to scenarios which are incomplete (in the sense that there
are individually rational deals that cannot be realised by individually rational O-contracts)
our focus has been with respect to deals which can be realised by restricted contract paths,
with the intention of determining to what extent the combination of structural and rationality conditions increases the number of deals required. We have shown that, using a number
of natural definitions of rationality, for settings involving m resources, rational O-contract
paths of length (2m ) are needed, whereas without the rationality restriction on individual
deals, at most m O-contracts suffice to realise any deal. We have also considered a class
of deals  M (k )-contracts  that were not examined in (Sandholm, 1998), establishing for
these cases that, when particular rationality conditions are imposed, M (k  1)-contract
2
paths of length (22m/k ) are needed to realise a deal that can be achieved by a single
M (k )-contract.
We note that our analyses have primarily been focused on worst-case lower bounds
on path length when appropriate paths exist, and as such there are several questions of
74

fiExtremal Behaviour in Multiagent Contract Negotiation

practical interest that merit further discussion. It may be noted that the path structures
and associated utility functions are rather artificial, being directed to attaining a path of a
specific length meeting a given rationality criterion. We have seen, however, in Theorems 4
and 5 as outlined in our discussion concluding Section 3 that the issue of exponential length
contract paths continues to arise even when we require the utility functions to satisfy a
monotonicity condition. We can identify two classes of open question that arise from these
results.
Firstly, focusing on IR O-contract paths, it would be of interest to identify natural
restrictions on utility functions which would ensure that, if a deal hP , Qi can be implemented
by an IR O-contract path, then it can be realised by one whose length is polynomially
bounded in m, e.g. such as additivity mentioned in the preceding section. We can interpret
Theorem 4, as indicating that monotonicity does not guarantee short IR contract paths.
We note, however, that there are some restrictions that suffice. To use a rather trivial
example, if the number of distinct values that u can assume is at most m p for some
constant p then no IR O-contract path can have length exceeding m p : successive deals
must strictly increase u and if this can take at most K different values then no IR contract
path can have length exceeding K . As well as being of practical interest, classes of utility
function with the property being considered would also be of some interest regarding one
complexity issue. The result proved in (Dunne et al., 2003) establishing that deciding if an
IR O-contract path exists is np-hard, gives a lower bound on the computational complexity
of this problem. At present, no (non-trivial) upper bound on this problems complexity
has been demonstrated. Our results in Theorems 3 and 4 indicate that if this decision
problem is in np (thus its complexity would be npcomplete rather than nphard) then
the required polynomial length existence certificate may have to be something other than
the path itself.10 We note that the proof of nphardness in (Dunne et al., 2003) constructs
an instance in which u can take at most O(m) distinct values: thus, from our example of
a restriction ensuring that if such are present then IR O-contract paths are short, this
result of (Dunne et al., 2003) indicates that the question of deciding their existence might
remain computationally hard.
Considering restrictions on the form of utility functions is one approach that could be
taken regarding finding tractable cases. An alternative would be to gain some insight
into what the average path length is likely to be. In attempting to address this question,
however, a number of challenging issues arise. The most immediate of these concerns,
of course, the notion of modeling a distribution on utility function given our definitions
of rationality in terms of the value agents attach to their resource holdings. In principle
an average-case analysis of scenarios involving exactly two agents could be carried out in
purely graph-theoretic terms, i.e. without the complication of considering utility functions
directly. It is unclear, however, whether such a graph-theoretic analysis obviating the need
for consideration of literal utility functions, can be extended beyond settings involving
exactly two agents. One difficulty arising with three or more agents is that our utility
10. The use of may rather than must is needed because of the convention for representing utility functions
employed in (Dunne et al., 2003).

75

fiDunne

functions have no allocative externalities, i.e. given an allocation hX , Y , Z i to three agents,
u1 (X ) is unchanged should Y  Z be redistributed among A2 and A3 .11
As one final set of issues that may merit further study we raise the following. In
our constructions, the individual deals on a contract path must satisfy both a structural
condition (be an O-contract or involve at most k agents), and a rationality constraint.
Focusing on O-contracts we have the following extremes: from (Sandholm, 1998), at most
m O-contracts suffice to realise any rational deal; from our results above, (2m ) rational
O-contracts are needed to realise some rational deals. There are a number of mechanisms
we can employ to relax the condition that every single deal be an O-contract and be
rational. For example, allow a path to contain some number of deals which are not Ocontracts (but must still be IR) or insist that all deals are O-contracts but allow some to
be irrational. Thus, in the latter case, if we go to the extent of allowing up to m irrational
O-contracts, then any rational deal can be realised efficiently. It would be of some interest
to examine issues such as the effect of allowing a constant number, t, of irrational deals
and questions such as whether there are situations in which t irrational contracts yield
a short contract path but t  1 force one of exponential length. Of particular interest,
from an application viewpoint, is the following: define a ((m), O)-path as an O-contract
path containing at most (m) O-contracts which are not individually rational. We know
that if (m) = 0 then individually rational (0, O)-paths are not complete with respect to
individually rational deals; similarly if (m) = m then (m, O)-paths are complete with
respect to individually rational deals. A question of some interest would be to establish
if there is some (m) = o(m) for which ((m), O)-paths are complete with respect to
individually rational deals and with the maximum length of such a contract path bounded
by a polynomial function of m.

Acknowledgements
The author thanks the reviewers of an earlier version of this article for their valuable comments and suggestions which have contributed significantly to its content and organisation.
The work reported in this article was carried out under the support of EPSRC Grant
GR/R60836/01.

References
Abbott, H. L., & Katchalski, M. (1991). On the construction of snake in the box codes.
Utilitas Mathematica, 40, 97116.
Atkinson, A. (1970). On the measurement of inequality. Jnl. Econ. Theory, 2, 244263.
Chateauneaf, A., Gajdos, T., & Wilthien, P.-H. (2002). The principle of strong diminishing
transfer. Jnl. Econ. Theory, 103, 311333.
11. A very preliminary investigation of complexity-theoretic questions arising in settings with allocative
externalities is presented in (Dunne, 2004) where these are referred to as contextdependent: such
utility functions appear to have been neglected in the computational and algorithmic analysis of resource
allocation problems, although the idea is well-known to game-theoretic models in economics from which
the term allocative externality originates.

76

fiExtremal Behaviour in Multiagent Contract Negotiation

Dignum, F., & Greaves, M. (2000). Issues in Agent Communication, Vol. 1916 of LNCS.
Springer-Verlag.
Dunne, P. (2003). Prevarication in dispute protocols. In Proc. Ninth International Conf.
on A.I. and Law (ICAIL03), pp. 1221, Edinburgh. ACM Press.
Dunne, P. (2004). Context dependence in multiagent resource allocation. In Proc. ECAI04,
pp. 100001, Valencia.
Dunne, P., & McBurney, P. (2003). Optimal utterances in dialogue protocols. In Proc. Second International Joint Conf. on Autonomous Agents and Multiagent Systems (AAMAS03), pp. 608615. ACM Press.
Dunne, P., Wooldridge, M., & Laurence, M. (2003). The complexity of contract negotiation.
Tech. rep. ULCS-03-002, Dept. of Computer Science, Univ. of Liverpool. (to appear
Artificial Intelligence).
Endriss, U., & Maudet, N. (2004a). On the communication complexity of multilateral trading. In Proc. Third International Joint Conf. on Autonomous Agents and Multiagent
Systems (AAMAS04), pp. 622629.
Endriss, U., & Maudet, N. (2004b). Welfare engineering in multiagent systems. In Omicini,
A., Petta, P., & Pitt, J. (Eds.), Proc. Fourth International Workshop on Engineering
Societies in the Agents World (ESAW-2003), Vol. 3071 of LNAI, pp. 93106. SpringerVerlag.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2003). On optimal outcomes of negotiations
over resources. In Proc. Second International Joint Conf. on Autonomous Agents and
Multiagent Systems (AAMAS03), pp. 177184. ACM Press.
Kautz, W. H. (1958). Unit distance error checking codes. IRE Trans. on Electronic Computers, 7, 179180.
Kolm, S.-C. (1976). Unequal inequalities. Jnl. Econ. Theory, 13, 82111.
Kraus, S. (2001). Strategic negotiation in multiagent environments. MIT Press.
McBurney, P., Parsons, S., & Wooldridge, M. (2002). Desiderata for argumentation protocols. In Proc. First International. Joint Conf. on Autonomous Agents and Multiagent
Systems (AAMAS02), pp. 402409. ACM Press.
Parkes, D. C., & Ungar, L. H. (2000a). Iterative combinatorial auctions: theory and practice.
In Proc. 17th National Conf. on Artificial Intelligence (AAAI-00), pp. 7481.
Parkes, D. C., & Ungar, L. H. (2000b). Preventing strategic manipulation in iterative
auctions: proxy agents and price adjustment. In Proc. 17th National Conf. on Artificial
Intelligence (AAAI-00), pp. 8289.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter. MIT Press.
Sandholm, T. W. (1998). Contract types for satisficing task allocation: I theoretical results.
In AAAI Spring Symposium: Satisficing Models.
Sandholm, T. W. (1999). Distributed rational decision making. In Wei, G. (Ed.), Multiagent Systems, pp. 201258. MIT Press.

77

fiDunne

Sandholm, T. W. (2002). Algorithm for optimal winner determination in combinatorial
auctions. Artificial Intelligence, 135, 154.
Sandholm, T. W., & Suri, S. (2003). Bob: Improved winner determination in combinatorial
auctions and generalizations. Artificial Intelligence, 145, 3358.
Sandholm, T. W., Suri, S., Gilpin, A., & Levine, D. (2001). Cabob: A fast optimal algorithm
for combinatorial auctions.. In Proc. IJCAI-01, pp. 11021108.
Smith, R. G. (1980). The contract net protocol: high-level communication and control in a
distributed problem solver. IEEE Trans. on Computers, C-29 (12), 11041113.
Tennenholz, M. (2000). Some tractable combinatorial auctions. In Proc. 17th National
Conf. on Artificial Intelligence (AAAI-00).
Yokoo, M., Sakurai, Y., & Matsubara, S. (2004). The effect of false-name bids in combinatorial auctions: new fraud in internet auctions. Games and Economic Behavior,
46 (1), 174188.

78

fi