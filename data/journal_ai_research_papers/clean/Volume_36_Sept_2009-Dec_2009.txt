Journal of Artificial Intelligence Research 36 (2009) 229-266

Submitted 03/09; published 10/09

Relaxed Survey Propagation for
The Weighted Maximum Satisfiability Problem
Hai Leong Chieu

chaileon@dso.org.sg

DSO National Laboratories,
20 Science Park Drive, Singapore 118230

Wee Sun Lee

leews@comp.nus.edu.sg

Department of Computer Science, School of Computing,
National University of Singapore, Singapore 117590

Abstract
The survey propagation (SP) algorithm has been shown to work well on large instances
of the random 3-SAT problem near its phase transition. It was shown that SP estimates
marginals over covers that represent clusters of solutions. The SP-y algorithm generalizes
SP to work on the maximum satisfiability (Max-SAT) problem, but the cover interpretation
of SP does not generalize to SP-y. In this paper, we formulate the relaxed survey propagation (RSP) algorithm, which extends the SP algorithm to apply to the weighted Max-SAT
problem. We show that RSP has an interpretation of estimating marginals over covers
violating a set of clauses with minimal weight. This naturally generalizes the cover interpretation of SP. Empirically, we show that RSP outperforms SP-y and other state-of-the-art
Max-SAT solvers on random Max-SAT instances. RSP also outperforms state-of-the-art
weighted Max-SAT solvers on random weighted Max-SAT instances.

1. Introduction
The 3-SAT problem is the archetypical NP-complete problem, and the difficulty of solving
random 3-SAT instances has been shown to be related to the clause to variable ratio,
 = M/N , where M is the number of clauses and N the number of variables. A phase
transition occurs at the critical value of c  4.267: random 3-SAT instances with  < c
are generally satisfiable, while instances with  > c are not. Instances close to the phase
transition are generally hard to solve using local search algorithms (Mezard & Zecchina,
2002; Braunstein, Mezard, & Zecchina, 2005).
The survey propagation (SP) algorithm was invented in the statistical physics community using approaches used for analyzing phase transitions in spin glasses (Mezard &
Zecchina, 2002). The SP algorithm has surprised computer scientists by its ability to solve
efficiently extremely large and difficult Boolean satisfiability (SAT) instances in the phase
transition region. The algorithm has also been extended to the SP-y algorithm to handle
the maximum satisfiability (Max-SAT) problem (Battaglia, Kolar, & Zecchina, 2004).
Progress has been made in understanding why the SP algorithm works well. Braunstein
and Zecchina (2004) first showed that SP can be viewed as the belief propagation (BP)
algorithm (Pearl, 1988) on a related factor graph where only clusters of solutions represented
by covers have non-zero probability. It is not known whether a similar interpretation can be
given to the SP-y algorithm. In this paper, we extend the SP algorithm to handle weighted
c
2009
AI Access Foundation. All rights reserved.

fiChieu & Lee

Max-SAT instances in a way that preserves the cover interpretation, and we call this new
algorithm the Relaxed Survey Propagation (RSP) algorithm. Empirically, we show that
RSP outperforms SP-y and other state-of-the-art solvers on random Max-SAT instances.
It also outperforms state-of-the-art solvers on a few benchmark Max-SAT instances. On
random weighted Max-SAT instances, it outperforms state-of-the-art weighted Max-SAT
solvers.
The rest of this paper is organized as follows. In Section 2, we describe the background
literature and mathematical notations necessary for understanding this paper. This includes
a brief review of the definition of joint probability distributions over factor graphs, an
introduction to the SAT, Max-SAT and the weighted Max-SAT problem, and how they can
be formulated as inference problems over a probability distribution on a factor graph. In
Section 3, we give a review of the BP algorithm (Pearl, 1988), which plays a central role in
this paper. In Section 4, we give a description of the SP (Braunstein et al., 2005) and the
SP-y (Battaglia et al., 2004) algorithm, explaining them as warning propagation algorithms.
In Section 5, we define a joint distribution over an extended factor graph given a weighted
Max-SAT instance. This factor graph generalizes the factor graph defined by Maneva,
Mossel and Wainwright (2004) and by Chieu and Lee (2008). We show that, for solving
SAT instances, running the BP algorithm on this factor graph is equivalent to running
the SP algorithm derived by Braunstein, Mezard and Zecchina (2005). For the weighted
Max-SAT problem, this gives rise to a new algorithm that we call the Relaxed Survey
Propagation (RSP) algorithm. In Section 7, we show empirically that RSP outperforms
other algorithms for solving hard Max-SAT and weighted Max-SAT instances.

2. Background
While SP was first derived from principles in statistical physics, it can be understood as
a BP algorithm, estimating marginals for a joint distribution defined over a factor graph.
In this section, we will provide background material on joint distributions defined over
factor graphs. We will then define the Boolean satisfiability (SAT) problem, the maximum
satisfiability (Max-SAT) problem, and the weighted maximum satisfiability (weighted MaxSAT) problem, and show that these problems can be solved by solving an inference problem
over joint distributions defined on factor graphs. A review of the definition and derivation of
the BP algorithm will then follow in the next section, before we describe the SP algorithm
in Section 4.
2.1 Notations
First, we will define notations and concepts that are relevant to the inference problems over
factor graphs. Factor graphs provide a framework for reasoning and manipulating the joint
distribution over a set of variables. In general, variables could be continuous in nature, but
in this paper, we limit ourselves to discrete random variables.
In this paper, we denote random variables using large Roman letters, e.g., X, Y . The
random variables are always discrete in this paper, taking values in a finite domain. Usually,
we are interested in vectors of random variables, for which we will write the letters in bold
face, e.g., X, Y. We will often index random variables by the letters i, j, k..., and write,
for example, X = {Xi }iV , where V is a finite set. For a subset W  V , we will denote
230

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem


X1

'
X2

''
X3

X4

Figure 1: A simple factor graph for p(x) =  (x1 , x2 ) 0 (x1 , x3 ) 00 (x2 , x4 ).
XW = {Xi }iW . We call an assignment of values to the variables in X a configuration,
and will denote it in small bold letters, e.g. x. We will often write x to represent an event
X = x and, for a probability distribution p, write p(x) to mean p(X = x). Similarly, we
will write x to denote the event X = x, and write p(x) to denote p(X = x).
A recurring theme in this paper will be on defining message passing algorithms for joint
distributions on factor graphs (Kschischang, Frey, & Loeliger, 2001). In a joint distribution
defined as a product of local functions (functions defined on a small subset of variables),
we will refer to the local functions as factors. We will index factors, e.g.  , with Greek
letters, e.g., ,  (avoiding  which is used as the symbol for clause to variable ratio in SAT
instances). For each factor  , we denote V ()  V as the subset of variables on which
 is defined, i.e.  is a function defined on the variables XV () . In message passing
algorithms, messages are vectors of real numbers that are sent from factors to variables or
vice versa. A vector message sent from a variable Xi to a factor  will be denoted as
Mi , and a message from  to Xi will be denoted as Mi .
2.2 Joint Distributions and Factor Graphs
Given a large set of discrete, random variables X = {Xi }iV , we are interested in the joint
probability distribution p(X) over these variables. When the set V is large, it is often of
interest to assume a simple decomposition, so that we can draw conclusions efficiently from
the distribution. In this paper, we are interested in the joint probability distribution that
can be decomposed as follows
p(X = x) =

1 Y
 (xV() ),
Z F

(1)

where the set F indexes a set of functions { }F . Each function  is defined on a
subset of variables XV () of the set X, and maps configurations xV () into non-negative
real numbers. Assuming that each function  is defined on a small subset of variables
XV () , we hope to do efficient inference with this distribution, despite the large number
of variables in X. The constant Z is a normalization constant, which ensures that the
distribution sums to one over all configurations x of X.
A factor graph (Kschischang et al., 2001) provides a useful graphical representation
illustrating the dependencies defined in the joint probability distribution in Equation 1. A
factor graph G = ({V, F }, E), is a bipartite graph with two sets of nodes, the set of variable
nodes, V , and the set of factor nodes, F . The set of edges E in the factor graph connects
variable nodes to factor nodes, hence the bipartite nature of the graph. For a factor graph
representing the joint distribution in Equation 1, an edge e = (, i) is in E if and only if
231

fiChieu & Lee

the variable Xi is a parameter of the factor  (i.e. i  V ()). We will denote V (i) as the
set of factors depending on the variable Xi , i.e.
V (i) = {  F | i  V ()}

(2)

We show a simple example of a factor graph in Figure 1. In this small example, we have for
example, V () = {1, 2} and V (2) = {,  00 }. The factor graph representation is useful for
illustrating inference algorithms on joint distributions in the form of Equation 1 (Kschischang et al., 2001). In Section 3, we will describe the BP algorithm by using the factor
graph representation.
Equation 1 defines the joint distribution as a product of local factors. It is often useful
to represent the distribution in the following exponential form:
p(x) = exp (

X

 (xV () )  )

(3)

F

The above equation is a reparameterization of Equation 1, with  (xV () ) = exp( (xV () ))
and  = ln Z. In statistical physics, the exponential form is often written as follows:
p(x) =

1
1
exp(
E(x)),
Z
kB T

(4)

where E(x) is the Hamiltonian or energy function, kB is the Boltzmanns constant, and T
is the temperature. For simplicity, we set kB T = 1, and Equations 3 and 4 are equivalent
P
with E(x) =  F  (xV () ).
Bayesian (belief) networks and Markov random fields are two other graphical representations often used to describe multi-dimensional probability distributions. Factor graphs
are closely related to both Bayesian networks and Markov random fields, and algorithms
operating on factor graphs are often directly applicable to Bayesian networks and Markov
random fields. We refer the reader to the work of Kschischang et al. (2001) for a comparison
between factor graphs, Bayesian networks and Markov random fields.
2.3 Inference on Joint Distributions
In the literature, inference on a joint distribution can refer to solving one of two problems.
We define the two problems as follows:
Problem 1 (MAP problem). Given a joint distribution, p(x), we are interested in the configuration(s) with the highest probability. Such configurations, x , are called the maximuma-posteriori configurations, or MAP configurations
x = arg max p(x)
x

(5)

From the joint distribution in Equation 4, the MAP configuration minimizes the energy
function E(x), and hence the MAP problem is sometimes called the energy minimization
problem.

232

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

Problem 2 (Marginal problem). Given a joint distribution, p(x), of central interest are
the calculation or estimation of probabilities of events involving a single variable Xi = xi .
We refer to such probabilities as marginal probabilities:
X

pi (xi ) =

p(x).

(6)

x\xi

P

The notation x\xi means summing over all configurations of X with the variable Xi set
to xi . Marginals are important as they represent the underlying distribution of individual
variables.
In general, both problems are not solvable in reasonable time by currently known methods. Naive calculation of pi (xi ) involves summing the probabilities of all configurations for
the variables X for which Xi = xi . For example, in a factor graph with n variables of
cardinality q, finding the marginal of one of the variables will involve summing over q n1
configurations. Furthermore, NP-complete problems such as 3-SAT can be simply coded
as factor graphs (see Section 2.4.1). As such, the MAP problem is in general NP-complete,
while the marginal problem is equivalent to model counting for 3-SAT, and is #P-complete
(Cooper, 1990). Hence, in general, we do not expect to solve the inference problems (exactly) in reasonable time, unless the problems are very small, or have special structures
that can be exploited for efficient inference.
Of central interest in this paper is a particular approximate inference method known
as the (sum-product) belief propagation (BP) algorithm. We defer the discussion of the
BP algorithm to the next section. In the rest of this section, we will describe the SAT,
Max-SAT and weighted Max-SAT problems, and how they can be simply formulated as
inference problems on a joint distribution over a factor graph.
2.4 The SAT and Max-SAT Problem
A variable is Boolean if it takes values in {FALSE, TRUE}. In this paper, we will follow
conventions in statistical physics, where Boolean variables take values in {1, +1}, with 1
corresponding to FALSE, and +1 corresponding to TRUE.
The Boolean satisfiability (SAT) problem is given as a Boolean propositional formula
written with the operators AND (conjunction), OR (disjunction), NOT (negation), and
parenthesis. The objective of the SAT problem is to decide whether there exists a configuration such that the propositional formula is satisfied (evaluates to TRUE). The SAT
problem is the first problem shown to be NP-complete in Stephen Cooks seminal paper in
1971 (Cook, 1971; Levin, 1973).
The three operators in Boolean algebra are defined as follows: given two propositional
formulas A and B, OR(A, B) is true if either A or B is true; AND(A, B) is true only if both
A and B are true; and NOT(A) is true if A is false. In the rest of the paper, we will use the
standard notations in Boolean algebra for the Boolean operators: A  B means OR(A, B),
A  B means AND(A, B), and A means NOT(A). The parenthesis is available to allow
nested application of the operators, e.g. (A  B)  (B  C).
The conjunctive normal form (CNF) is often used as a standard form for writing Boolean
formulas. The CNF consists of a conjunction of disjunctions of literals, where a literal is
either a variable or its negation. For example, (X1  X 2 )  (X 3  X4 ) is in CNF, while
233

fiChieu & Lee

X1  X2 and (X1  X2 )  (X2  X3 ) are not. Any Boolean formula can be re-written in CNF
using De Morgans law and the distributivity law, although in practice, this may lead to an
exponential blowup in the size of the formula, and the Tseitin transformation is often used
instead (Tseitin, 1968). In CNF, a Boolean formula can be considered to be the conjunction
of a set of clauses, where each clause is a disjunction of literals. Hence, a SAT problem is
often given as (X, C), where X is the vector of the Boolean variables, and C is a set of
clauses. Each clause in C is satisfied by a configuration if it evaluates to TRUE for that
configuration. Otherwise, it is said to be violated by the configuration. We will use Greek
letters (e.g. , ) as indices for clauses in C, and denote by V () as the set of variables in
the clause   C. The K-SAT problem is a SAT problem for which each clause in C consists
of exactly K literals. The K-SAT problem is NP-complete, for K  3 (Cook, 1971).
The maximum satisfiability problem (Max-SAT) problem is the optimization version of
the SAT problem, where the aim is to minimize the number of violated constraints in the
formula. We define a simple working example of the Max-SAT problem that we will use
throughout the paper:
Example 1. Define an instance of the Max-SAT problem in CNF with the following clauses
1 = (x1 x2 ), 2 = (x2 x3 ), 3 = (x3 x1 ), 4 = (x1 x2 x3 ), 5 = (x1 x2 x3 ) and 6 =
(x1  x2 ). The Boolean expression representing this problem would be
(x1  x2 )  (x2  x3 )  (x3  x1 )  (x1  x2  x3 )  (x1  x2  x3 )  (x1  x2 ).

(7)

The objective of the Max-SAT problem would be to find a configuration minimizing the
number of violated clauses.
2.4.1 Factor Graph Representation for SAT Instances
The SAT problem in CNF can easily be represented as a joint distribution over a factor
graph. In the following definition, we give a possible definition of a joint distribution over
Boolean configurations for a given SAT instance, where the Boolean variables take values
in {1, +1}.
Definition 1. Given an instance of the SAT problem, (X, C) in conjunctive normal form,
where X is a vector of N Boolean variables. We define the energy, E(x), and the distribution, p(x), over configurations of the SAT instance (Battaglia et al., 2004)
  C, C (xV () ) =
E(x) =

1
(1 + J,i xi ),
2
iV ()

(8)

X

C (xV () ),

(9)

1
exp(E(x)),
Z

(10)

Y

C

p(x) =

where x  {1, +1}N , and J,i takes values in {1, +1}. If J,i = +1 (resp. 1), then
 contains Xi as a negative (resp. positive) literal. Each clause  is satisfied if one of its
variables Xi takes the value J,i . When a clause  is satisfied, C (xV () ) = 0. Otherwise
C (xV () ) = 1.
234

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

6

1

3

X1

2

X2

5

X3

4

Figure 2: The factor graph for the SAT instance given in Example 1. Dotted (resp. solid)
lines joining a variable to a clause means the variable is a negative (resp. positive)
literal in the clause.

With the above definition, the energy function is zero for satisfying configurations, and
equals the number of violated clauses for non-satisfying configuration. Hence, satisfying
configurations of the SAT instance are the MAP configurations of the factor graph.
In this section, we make some definitions that will be useful in the rest of the paper.
For a clause  containing a variable Xi (associated with the value of J,i ), we will say that
Xi satisfies  if Xi = J,i . In this case, the clause  is satisfied regardless of the values
taken by the other variables. Conversely, we say that Xi violates  if Xi does not satisfy .
In this case, it is still possible that  is satisfied by other variables.
Definition 2. For a clause   C, we define u,i (resp. s,i ) as the value of Xi  {1, +1}
that violates (resp. satisfies) clause . This means that s,i = J,i and u,i = +J,i . We
define the following sets
V + (i)
V  (i)
Vs (i)
Vu (i)

=
=
=
=

{  V (i); s,i = +1},
{  V (i); s,i = 1},
{  V (i) \ {}; s,i = s,i },
{  V (i) \ {}; s,i 6= s,i }.

(11)

In the above definitions, V + (i) (resp. V  (i)) is the set of clauses that contain Xi as a
positive literal (resp. negative literal). Vs (i) (resp. Vu (i)) is the set of clauses containing
Xi that agrees (resp. disagrees) with the clause  concerning Xi . These sets will be useful
when we define the SP message passing algorithms for SAT instances.
The factor graph representing the Max-SAT instance given in Example 1 is shown in
Figure 2. For this example, V + (1) = {3 , 5 , 6 }, V  (1) = {1 , 4 }, Vs3 (1) = {5 , 6 }, and
Vu3 (1) = {1 , 4 }. The energy for this example is as follows:
1
1
1
E(x) = (1 + x1 )(1  x2 ) + (1 + x2 )(1  x3 ) + (1 + x3 )(1  x1 ) +
4
4
4
1
1
1
(1 + x1 )(1 + x2 )(1 + x3 ) + (1  x1 )(1  x2 )(1  x3 ) + (1  x1 )(1  x2 ) (12)
8
8
4

235

fiChieu & Lee

2.4.2 Related Work on SAT
The SAT problem is well studied in computer science: as the archetypical NP-complete
problem, it is common to reformulate other NP-complete problems such as graph coloring
as a SAT problem (Prestwich, 2003). SAT solvers are either complete or incomplete. The
best known complete solver for solving the SAT problem is probably the Davis-PutnamLogemann-Loveland (DPLL) algorithm (Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962). The DPLL algorithm is a basic backtracking algorithm that runs by choosing a
literal, assigning a truth value to it, simplifying the formula and then recursively checking if
the simplified formula is satisfiable; if this is the case, the original formula is satisfiable; otherwise, the same recursive check is done assuming the opposite truth value. Variants of the
DPLL algorithm such as Chaff (Moskewicz & Madigan, 2001), MiniSat (Een & Sorensson,
2005), and RSAT (Pipatsrisawat & Darwiche, 2007) are among the best performers in recent SAT competitions (Berre & Simon, 2003, 2005). Solvers such as satz (Li & Anbulagan,
1997) and cnfs (Dubois & Dequen, 2001) have also been making progress in solving hard
random 3-SAT instances.
Most solvers that participated in recent SAT competitions are complete solvers. While
incomplete or stochastic solvers do not show that a SAT instance is unsatisfiable, they are
often able to solve larger satisfiable instances than complete solvers. Incomplete solvers
usually start with a randomly initialized configuration, and different algorithms differ in
the way they flip selected variables to move towards a solution. One disadvantage of such
an approach is that in hard SAT instances, a large number of variables have to be flipped to
move a current configuration out of a local minimum, which acts as a local trap. Incomplete
solvers differ in the strategies used to move the configuration out of such traps. For example,
simulated annealing (Kirkpatrick, Jr., & Vecchi, 1983) allows the search to move uphill,
controlled by a temperature parameter. GSAT (Selman, Levesque, & Mitchell, 1992) and
WalkSAT (Selman, Kautz, & Cohen, 1994) are two algorithms developed in the 1990s that
allow randomized moves when the solution cannot be improved locally. The two algorithms
differ in the way they choose the variables to flip. GSAT makes the change which minimizes
the number of unsatisfied clauses in the new configuration, while WalkSAT selects the
variable that, when flipped, results in no previously satisfied clauses becoming unsatisfied.
Variants of algorithms such as WalkSAT and GSAT use various strategies, such as tabusearch (McAllester, Selman, & Kautz, 1997) or adapting the noise parameter that is used,
to help the search out of a local minima (Hoos, 2002). Another class of approaches is based
on applying discrete Lagrangian methods on SAT as a constrained optimization problem
(Shang & Wah, 1998). The Lagrange mutlipliers are used as a force to lead the search out
of local traps.
The SP algorithm (Braunstein et al., 2005) has been shown to beat the best incomplete
solvers in solving hard random 3-SAT instances efficiently. SP estimates marginals on all
variables and chooses a few of them to fix to a truth value. The size of the instance is then
reduced by removing these variables, and SP is run again on the remaining instance. This
iterative process is called decimation in the SP literature. It was shown empirically that SP
rarely makes any mistakes in its decimation, and SP solves very large 3-SAT instances that
are very hard for local search algorithms. Recently, Braunstein and Zecchina (2006) have
236

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

shown that by modifying BP and SP updates with a reinforcement term, the effectiveness
of these algorithms as solvers can be further improved.
2.5 The Weighted Max-SAT Problem
The weighted Max-SAT problem is a generalization of the Max-SAT problem, where each
clause is assigned a weight. We define an instance of the weighted Max-SAT problem as
follows:
Definition 3. A weighted Max-SAT instance (X, C, W) in CNF consists of X, a vector of
N variables taking values in {1, +1}, C, a set of clauses, and W, the set of weights for
each clause in C. We define the energy of the weighted Max-SAT problem as
E(x) =

w
(1 + J,i xi ),
2
C iV ()
X

Y

(13)

where x  {1, +1}N , and J,i takes values in {1, +1}, and w is the weight of the clause
. The total energy, E(x), of a configuration x equals the total weight of violated clauses.
Similarly to SAT, there are also complete and incomplete solvers for the weighted MaxSAT problem. Complete weighted Max-SAT solvers involve branch and bound techniques by
calculating bounds on the cost function. Larrosa and Heras (2005) introduced a framework
that integrated the branch and bound techniques into a Max-DPLL algorithm for solving
the Max-SAT problem. Incomplete solvers generally employ heuristics that are similar to
those used for SAT problems. An example of an incomplete method is the min-conflicts hillclimbing with random walks algorithm (Minton, Philips, Johnston, & Laird, 1992). Many
SAT solvers such as WalkSAT can be extended to solve weighted Max-SAT problems, where
the weights are used as a criterion in the selection of variables to flip.
As a working example in this paper, we define the following instance of a weighted
Max-SAT problem:
Example 2. We define a set of weighted Max-SAT clauses in the following table:

Id
1
2
3
4
5
6

Clause
Weight
x1  x2
1
x2  x3
2
x3  x1
3
x1  x2  x3
4
x1  x2  x3
5
x1  x2
6
Energy

--3
3
3
3
5
5
1

--+
3
3
5
3
3
5
9

-+3
5
3
3
3
3
2

-++
3
3
5
3
3
3
3

+-5
3
3
3
3
3
1

+-+
5
3
3
3
3
3
1

++3
5
3
3
3
3
2

+++
3
3
3
5
3
3
4

This weighted Max-SAT example has the same variables and clauses as the Max-SAT
example given in Example 1. In the above table, we show the clauses satisfied (a tick) or
violated (a cross) by each of the 8 possible configurations of the 3 variables. In the first
237

fiChieu & Lee

row, the symbol  corresponds to the value 1, and + corresponds to +1. For example, the
string    +  corresponds to the configuration (X1 , X2 , X3 ) = (1, 1, +1). The last row
of the table shows the energy of the configuration in each column.
The factor graph for this weighted Max-SAT example is the same as the one for the
Max-SAT example in Example 1. The differences between the two examples are in the
clause weights, which are reflected in the joint distribution, but not in the factor graph.
The energy for this example is as follows:
1
2
3
E(x) = (1 + x1 )(1  x2 ) + (1 + x2 )(1  x3 ) + (1 + x3 )(1  x1 ) +
4
4
4
4
5
6
(1 + x1 )(1 + x2 )(1 + x3 ) + (1  x1 )(1  x2 )(1  x3 ) + (1  x1 )(1  x2 )(14)
8
8
4
2.6 Phase Transitions
The SP algorithm has been shown to work well on 3-SAT instances near its phase transition,
where instances are known to be very hard to solve. The term phase transition arises
from the physics community. To understand the notion of hardness in optimization
problems, computer scientists and physicists have been studying the relationship between
computational complexity in computer science and phase transitions in statistical physics.
In statistical physics, the phenomenon of phase transitions refers to the abrupt changes
in one or more physical properties in thermodynamic or magnetic systems with a small
change in the value of a variable such as the temperature. In computer science, it has
been observed that in random ensembles of instances such as K-SAT, there is a sharp
threshold where randomly generated problems undergo an abrupt change in properties. For
example, in K-SAT, it has been observed empirically that as the clause to variable ratio 
changes, randomly generated instances change abruptly from satisfiable to unsatisfiable at
a particular value of , often denoted as c . Moreover, instances generated with a value of
 close to c are found to be extremely hard to solve.
Computer scientists and physicists have worked on bounding and calculating the precise value of c where the phase transition for 3-SAT occurs. Using the cavity approach,
physicists claim that c  4.267 (Mezard & Zecchina, 2002). While their derivation of
the value of c is non-rigorous, it is based on this derivation that they formulated the SP
algorithm. Using rigorous mathematical approaches, the upper bounds to the value of c
can be derived using first-order methods. For example, in the work of Kirousis, Kranakis,
Krizanc, and Stamatiou (1998), c for 3-SAT was upper bounded by 4.571. Achlioptas,
Naor and Peres (2005) lower-bounded the value of c using a weighted second moments
method, and their lower bound is close to the upper bounds for K-SAT ensembles for large
values of K. However, their lower bound for 3-SAT is 2.68, rather far from the conjectured
value of 4.267. A better (algorithmic) lower bound of 3.52 can be obtained by analyzing
the behavior of algorithms that find SAT configurations (Kaporis, Kirousis, & Lalas, 2006).
Physicists have also shown rigorously using second moment methods that as  approaches c , the search space fractures dramatically, with many small solution clusters
appearing relatively far apart from each other (Mezard, Mora, & Zecchina, 2005). Clusters
of solutions are generally defined as a set of connected components of the solution space,
where two adjacent solutions have a Hamming distance of 1 (differ by one variable). Daude,
238

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

k

'

M'j

M''j
''

Mk

Mj

j



Mi

i

Ml
l

Figure 3: Illustration of messages in a BP algorithm.
Mezard, Mora, and Zecchina (2008) redefined the notion of clusters by using the concept
of x-satisfiability: a SAT instance is x-satisfiable if there exists two solutions differing by
N x variables, where N is the total number of variables. They showed that near the phase
transition, x goes from around 12 to very small values, without going through a phase of
intermediate values. This clustering phenomenon explains why instances generated with 
close to c are extremely hard to solve with local search algorithm: it is difficult for the
local search algorithm to move from a local minimum to the global minimum.

3. The Belief Propagation Algorithm
The BP algorithm has been reinvented in different fields under different names. For example,
in the speech recognition community, the BP algorithm is known as the forward-backward
procedure (Rabiner & Juang, 1993). On tree-structured factor graphs, the BP algorithm is
simply a dynamic programming approach applied to the tree structure, and it can be shown
that BP calculates the marginals for each variable in the factor graph (i.e. solving Problem
2). In loopy factor graphs, the BP algorithm has been found to provide a reasonable
approximation to solving the marginal problem when the algorithm converges. In this case,
the BP algorithm is often called the loopy BP algorithm. Yedidia, Freeman and Weiss (2005)
have shown that the fixed points of the loopy BP algorithm correspond to the stationary
points of the Bethe free energy, and is hence a sensible approximate method for estimaing
marginals.
In this section, we will first describe the BP algorithm as a dynamic programming
method for solving the marginal problem (Problem 2) for tree-structured factor graphs. We
will also briefly describe how the BP algorithm can be applied to factor graphs with loops,
and refer the reader to the work of Yedidia et al. (2005) for the underlying theoretical
justification in this case.
Given a factor graph representing a distribution p(x), the BP algorithm involves iteratively passing messages from factor nodes   F to variable nodes i  V , and vice versa.
Each factor node  represents a factor  , which is a factor in the joint distribution given
in Equation 1. In Figure 3, we give an illustration of how the messages are passed between
factor nodes and variable nodes. Each Greek alphabet (e.g.   F ) in a square represents
a factor (e.g.  ) and each Roman alphabet (e.g. i  V ) in a circle represents a variable
(e.g. Xi ).
The factor to variable messages (e.g. Mi ), and the variable to factor messages (e.g.
Mi ) are vectors of real numbers, with length equal to the cardinality of the variable Xi .
239

fiChieu & Lee

We denote by Mi (xi ) or Mi (xi ) the component of the vector corresponding to the
value Xi = xi .
The message update equations are as follows:
Y

Mj (xj ) =

 0 V

(15)

(j)\

X

Mi (xi ) =

M 0 j (xj )
 (xV () )

xV () \xi

Y

Mj (xj ),

(16)

jV ()\i

P

where xV () \xi means summing over all configurations XV () with Xi set to xi .
For a tree-structured factor graph, the message updates can be scheduled such that after
two parses over the tree structure, the messages will converge. Once the messages converge,
the beliefs at each variable node are calculated as follows:
Y

Bj (xj ) =

Mj (xj ).

(17)

V (j)

For a tree-structured graph, the normalized beliefs for each variable will be equal to its
marginals.
INPUT: A joint distribution p(x) defined over a tree-structured factor graph ({V, F }, E),
and a variable Xi  X.
OUTPUT: Exact marginals for the variable Xi .
ALGORITHM :
1. Organize the tree so that Xi is the root of the tree.
2. Start from the leaves, propagate the messages from child nodes to parent nodes
right up to the root Xi with Equations 15 and 16.
3. The marginals of Xi can then be obtained as the normalized beliefs in Equation 17.

Figure 4: The BP algorithm for calculating the marginal of a single variable, Xi , on a
tree-structured factor graph

The algorithm for calculating the exact marginals of a given variable Xi , is given in
Figure 4. This algorithm is simply a dynamic programming procedure for calculating the
marginals, pi (Xi ), by organizing the sums so that the sums at the leaves are done first. For
the simple example in Figure 1, for calculating p1 (x1 ), the sum can be ordered as follows:
p1 (x1 ) =

X

p(x)

x2 ,x3 ,x4

=  (x1 , x2 )

X X

.

x2

x3

240

 0 (x1 , x3 )

X
x4

 00 (x2 , x4 )

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

The BP algorithm simply carries out this sum by using the node for X1 as the root of the
tree-structured factor graph in Figure 1.
The BP algorithm can also be used for calculating marginals for all variables efficiently,
with the message passing schedule given in Figure 5. This schedule involves selecting a
random variable node as the root of the tree, and then passing the messages from the leaves
to the root, and back down to the leaves, After the two parses, all the message updates
required in the algorithm in Figure 4 for any one variable would have been performed, and
the beliefs of all the variables can be calculated from the messages. The normalized beliefs
for each variable will be equal to the marginals for the variable.

INPUT: A joint distribution p(x) defined over a tree-structured factor graph (V, F ).
OUTPUT: Exact marginals for all variables in V .
ALGORITHM :
1. Randomly select a variable as a root.
2. Upward pass: starting from leaves, propagate messages from the leaves right up
to the tree.
3. Downward pass: from the root, propagate messages back down to the leaves.
4. Calculate the beliefs of all variables as given in Equation 17.

Figure 5: The BP algorithm for calculating the marginals of all variables on a treestructured factor graph

If the factor graph is not tree-structured (i.e. contains loops), then the message updates
cannot be scheduled in the simple way described in the algorithm in Figure 5. In this case,
we can still apply BP by iteratively updating the messages with Equations 15 and 16, often
in a round-robin manner over all factor-variable pairs. This is done until all the messages
converge (i.e. the messages do not change over iterations). There is no guarantee that
all the messages will converge for general factor graphs. However, if they do converge, it
was observed that the beliefs calculated with Equation 17 are often a good approximation
of the exact beliefs of the joint distribution (Murphy, Weiss, & Jordan, 1999). When
applied in this manner, the BP algorithm is often called the loopy BP algorithm. Recently,
Yedidia, Freeman and Weiss (2001, 2005) have shown that loopy BP has an underlying
variational principle. They showed that the fixed points of the BP algorithm correspond to
the stationary points of the Bethe free energy. This fact serves in some sense to justify the
BP algorithm even when the factor graph it operates on has loops, because minimizing the
Bethe free energy is a sensible approximation procedure for solving the marginal problem.
We refer the reader to the work of Yedidia et al. (2005) for more details.
241

fiChieu & Lee

4. Survey Propagation: The SP and SP-y Algorithms
Recently, the SP algorithm (Braunstein et al., 2005) has been shown to beat the best
incomplete solvers in solving hard 3-SAT instances efficiently. The SP algorithm was first
derived from principles in statistical physics, and can be explained using the cavity approach
(Mezard & Parisi, 2003). It was first given a BP interpretation in the work of Braunstein
and Zecchina (2004). In this section, we will define the SP and the SP-y algorithms for
solving SAT and Max-SAT problems, using a warning propagation interpretation for these
algorithms.
4.1 SP Algorithm for The SAT Problem
In Section 2.4.1, we have defined a joint distribution for the SAT problem (X, C), where
the energy function of a configuration is equal to the number of violated clauses for the configuration. In the factor graph ({V, F }, E) representing this joint distribution, the variable
nodes in V correspond to the Boolean variables in X, and each factor node in F represents
a clause in C. In this section, we provide an intuitive overview of the SP algorithm as it
was formulated in the work of Braunstein et al. (2005).
The SP algorithm can be defined as a message passing algorithm on the factor graph
({V, F }, E). Each factor   F passes a single real number, i to a neighboring variable
Xi in the factor graph. This real number i is called a survey. According to the warning
propagation interpretation given in the work of Braunstein et al. (2005), the survey i
corresponds to the probability1 of the warning that the factor  is sending to the variable
Xi . Intuitively, if i is close to 1, then the factor  is warning the variable Xi against
taking a value that will violate the clause . If i is close to 0, then the factor  is
indifferent over the value taken by Xi , and this is because the clause  is satisfied by other
variables in V ().
We first define the messages sent from a variable Xj to a neighboring factor , as
a function of the inputs from other factors containing Xj , i.e. { 0 j } 0 V (j)\ . In SP,
this message is a vector of three numbers, uj , sj , and 0j , with the following
interpretations:
uj is the probability that Xj is warned (by other clauses) to take a value that will violate
the clause .
sj is the probability that Xj is warned (by other clauses) to take a value that will satisfy
the clause .
0
j is the probability that Xj is free to take any value.
With these defintions, the update equations are as follows:
uj = [1 
sj = [1 

Y

(1   0 j )]

Y

(1   0 j ),

 0 Vu (j)

 0 Vs (j)

Y

Y

(1   0 j )]

 0 Vs (j)

(1   0 j ),

(18)
(19)

 0 Vu (j)

1. SP reasons over clusters of solutions, and the probability of a warning in this section is used loosely in
the SP literature to refer to the fraction of clusters for which the warning applies. In the next section,
we will define a rigorous probability distribution over covers for the RSP algorithm.

242

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

0j =

Y

(1   0 j ),

(20)

 0 V (j)

i =

Y

uj

jV ()i

uj + sj + 0j

(21)

These equations are defined using the sets of factors Vu (j) and Vs (j), which has been
defined in Section 2.4.1. For the event where the variable Xj is warned to take on a value
violating , it has to be (a) warned by at least one factor  0  Vu (j) to take on a satisfying
value for  0 , and (b) all the other factors in Vs (j) are not sending warnings. In Equation
18, the probability of this event, uj , is a product of two terms, the first corresponding to
event (a) and the second to event (b). The definitions of sj and 0j are defined in a
similar manner. In Equation 21, the final survey i is simply the probability of the joint
event that all incoming variables Xj are violating the clause , forcing the last variable Xi
to satisfy .
The SP algorithm consists of iteratively running the above update equations until the
surveys converge. When the surveys converged, we can then calculate local biases as follows:
= [1 
+
j

Y

(1   0 j )]

Y
V

0j

=

Y

(1  j ),

(22)

V  (j)

V + (j)

= [1 
+
j

Y

(1   0 j )]

 (j)

Y
V

(1  j ),

(1  j ),

(23)

+ (j)

(24)

V (j)

Wi+ =
Wi =

+
j

(25)


0
+
j +  j + j


j

(26)


0
+
j +  j + j

To solve an instances of the SAT problem, the SP algorithm is run until it converges,
and a few variables that are highly constrained are set to their preferred values. The SAT
instance is then reduced to a smaller instance, and SP can be run again on the smaller
instance. This continues until SP fails to set any more variables, and in this case, a local
search algorithm such as WalkSAT is run on the remaining instance. This algorithm, called
the survey inspired decimation algorithm (Braunstein et al., 2005), is given in the algorithm
in Figure 6.
4.2 The SP-y Algorithm
In contrast to the SP algorithm, the SP-y algorithms objective is to solve Max-SAT instances, and hence clauses are allowed to be violated, at a price. The SP algorithm can
be understood as a special case of the SP-y algorithm, with y taken to infinity (Battaglia
et al., 2004). In SP-y, a penalty value of exp(2y) is multiplied into the distribution for
each violated clause. Hence, although the message passing algorithm allows the violation of
clauses, but as the value of y increases, the surveys will prefer configurations that violate a
minimal number of clauses.
243

fiChieu & Lee

INPUT: A SAT problem, and a constant k.
OUTPUT: A satisfying configuration, or report FAILURE.
ALGORITHM :
1. Randomly initialize the surveys.
2. Iteratively update the surveys using Equations 18 to 21.
3. If SP does not converge, go to step 7.
4. If SP converges, calculate Wi+ and Wi using Equations 25 and 26.
5. Decimation: sort all variables based on the absolute difference |Wi+  Wi |, and
set the top k variables to their preferred value. Simplify the instance with these
variables removed.
6. If all surveys equal zero, (no variables can be removed in step 5), output the
simplified SAT instance. Otherwise, go back to the first step with the smaller
instance.
7. Run WalkSAT on the remaining simplified instance, and output a satisfying
configuration if WalkSAT succeeds. Otherwise output FAILURE.

Figure 6: The survey inspired decimation (SID) algorithm for solving a SAT problem
(Braunstein et al., 2005)

The SP-y algorithm can still be understood as a message passing algorithm over factor
graphs. As in SP, each factor, , passes a survey, i , to a neighboring variable Xi ,
+
corresponding to the probability of the warning. To simplify notations, we define i

(resp. i
) to be the probability of the warning against taking the value +1 (resp. 1),
+

0
and we define i
= 1  i
 i
. In practice, since a clause can only warn against
J

J

,i
+

either +1 or 1 but not both, either i
or i
equals zero: i
= i , and i,i = 0,
where J,i is defined in Definition 1.
Since clauses can be violated, it is insufficient to simply keep track of whether a variable
has been warned against a value or not. It is now necessary to keep track of how many
times the variable has been warned against each value, so that we know how many clauses
+

will be violated if the variable was to take a particular value. Let Hj
(resp. Hj
) be
0
the number of times the variable Xj is warned by factors in { } 0 V (j)\ against the value
+
+1 (resp. 1). In SP-y, the variable Xj will be forced by  to take the value +1 if Hj
is

+
smaller than Hj , and the penalty in this case will be exp(2yHj ). In notations used
+

in the work of Battaglia et al. (2004) describing SP-y, let hj = Hj
 Hj
.
Battaglia et al. (2004) defined the SP-y message passing equations that calculate the
probability distribution over hj , based on the input surveys,

{ 0 j } 0 V (j)\ = {1 j , 2 j , ..., (|V (j)|1) j },
244

(27)

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

where |V (j)| refers to the cardinality of the set V (j). The unnormalized distributions
Pej (h) are calculated as follows:
(1)

Pej (h) = 01 i (h) + +1 i (h  1) + 1 i (h + 1), (28)
  [2, |V (j)|  1],

()

(1)

Pej (h) = 0 i Pej (h)
(1)

++ i Pej (h  1) exp [2y(h)]
(1)

+ i Pej (h + 1) exp [2y(h)],
(|V (j)|1)

Pej (h) = Pej

(h),

(29)
(30)

where (h) = 1 if h = 0, and zero otherwise, and (h) = 1 if h  0, and zero otherwise.
The above equations take into account each neighbor of j excluding , from  = 1 to
 = |V (j)|1. The penalties exp(2y) are multiplied every time the value of hj decreases
in absolute value, as each new neighbor of Xj ,  , is added. At the end of the procedure,
+

this is equivalent to multiplying the messages with a factor of exp(2ymin(Hj
, Hj
)).
The Pej (h) are then normalized into Pj (h) by computing Pej (h) for all possible
values of h in [|V (j)| + 1, |V (j)|  1]. The message updates for the surveys are as follows:
|V (j)|1
+
Wj
=

X

Pj (h),

h=1
1
X


Wj
=

Pj (h),

(31)
(32)

h=|V (j)|+1
J

i,i

= 0,

J

,i
i
=

(33)
J

Y

,j
Wj
,

(34)

jV (j)\i
J

,i
0
i
= 1  i
,

(35)

+

The quantity Wj
(resp. Wj
) is the probability of all events warning against the value
+1 (resp. 1). Equation 34 reflects the fact that a warning is sent from  to the variable
Xi if and only if all other variables in  are warning  that they are going to violate .
When SP-y converges, the preference of each variable is calculated as follows:

|V (j)|

Wj+ =
Wj =

X

Pj (h),

h=1
1
X

Pj (h),

(36)
(37)

h=|V (j)|

where the Pj (h) are calculated in a similar manner as the Pj (h), except that it does not
exclude  in its calculations.
With the above definitions for message updates, the SP-y algorithm can be used to
solve Max-SAT instances by a survey inspired decimation algorithm similar to the one for
245

fiChieu & Lee

SP given in the algorithm in Figure 6. At each iteration of the decimation process, the SP-y
decimation procedure selects variables to fix to their preferred values based on the quantity
bf ix (j) = |Wj+  Wj |

(38)

In the work of Battaglia et al. (2004), an additional backtracking process was introduced to make the decimation process more robust. This backtracking process allows the
decimation procedure to unfix variables already fixed to their values. For a variable Xj
fixed to the value xj , the following quantities are calculated:
bbacktrack (j) = xj (Wj+  Wj )

(39)

Variables are ranked according to this quantity and the top variables are chosen to be
unfixed. In the algorithm in Figure 7, we show the backtracking decimation algorithm
for SP-y (Battaglia et al., 2004), where the value of y is either given as input, or can be
determined empirically.
INPUT: A Max-SAT instance and a constant k. Optional input: yin and a backtracking
probability r.
OUTPUT: A configuration.
ALGORITHM :
1. Randomly initialize the surveys.
2. If yin is given, set y = yin . Otherwise, determine the value of y with the bisection
method.
3. Run SP-y until convergence. If SP-y converges, for each variable Xi , extract a
random number q  [0, 1].
(a) If q > r, sort the variables according to Equation 38 and fix the top k most
biased variables.
(b) If q < r sort the variables according to Equation 39 and unfix the top k most
biased variables.
4. Simplify the instance based on step (3). If SP-y converged and return a nonparamagnetic solution (a paramagnetic solution refers to a set of {bf ix (j)}jV
that are not biased to any value for all variables), go to step (1).
5. Run weighted WalkSAT on the remaining instance and outputs the best configuration found.

Figure 7: The survey inspired decimation (SID) algorithm for solving a Max-SAT instance
(Battaglia et al., 2004)

246

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

5. Relaxed Survey Propagation
It was shown (Maneva et al., 2004; Braunstein & Zecchina, 2004) that SP for the SAT
problem can be reformulated as a BP algorithm on an extended factor graph. However,
their formulation cannot be generalized to explain the SP-y algorithm which is applicable to
Max-SAT problems. In a previous paper (Chieu & Lee, 2008), we extended the formulation
in the work of Maneva et al. (2004) to address the Max-SAT problem. In this section,
we will modify the formulation in our previous paper (Chieu & Lee, 2008) to address the
weighted Max-SAT problem, by setting up an extended factor graph on which we run the BP
algorithm. In Theorem 3, we show that this formulation generalizes the BP interpretation
of SP given in the work of Maneva et al. (2004), and in the main theorem (Theorem 2), we
show that running the loopy BP algorithm on this factor graph estimates marginals over
covers of configurations violating a set of clauses with minimal total weight.
We will first define the concept of covers in Section 5.1, before defining the extended
factor graph in Section 5.2. In the rest of this section, given a weighted Max-SAT problem
(X, C, W), we will assume that variables in X take values in {1, +1, }: the third value is
a dont care state, corresponding to a no-warning message for the SP algorithm defined
in the Section 4.
5.1 Covers in Weighted Max-SAT
First, we need to define the semantics of the value  as a dont care state.
Definition 4. (Maneva et al., 2004) Given a configuration x, we say that a variable Xi
is the unique satisfying variable for a clause   C if it is assigned s,i whereas all other
variables Xj in the clause are assigned u,j (see Definition 2 for the definitions of s,i and
u,i ). A variable Xi is said to be constrained by the clause  if it is the unique satisfying
variable for . A variable is unconstrained if it is not constrained by any clauses. Define
CONi, (x ) = Ind(xi is constrained by ),

(40)

where Ind(P ) equals 1 if the predicate P is true, and 0 otherwise.
As an illustration, consider the configuration X = (+1, 1, 1) in Example 2. In this
configuration, X1 = +1 is constrained by the clauses 5 and 6 , X2 = 1 is constrained
by 2 , while X3 = 1 is unconstrained: flipping X3 to +1 will not violate any additional
clauses for the configuration.
In the following definition, we redefine when a configuration taking values in {1, +1, }
satisfies or violates a clauses.
Definition 5. A configuration satisfies a clause  if and only if (i)  contains a variable Xi
set to the value s,i , or (ii) when at least two variables in  take the value . A configuration
violates a clause  if all the variables Xj in  are set to u,j . A configuration x is invalid
for clause  if and only if exactly one of the variables in  is set to , and all the other
remaining variables in  are set to u,i . A configuration is valid if it is valid for all clauses
in C.
The above definition for invalid configurations reflects the interpretation that the  value
is a dont care state: clauses containing a variable Xi =  should already be satisfied by
247

fiChieu & Lee

other variables, and the value of Xi does not matter. So Xi =  cannot be the last remaining
possibility of satisfying any clause. In the case where a clause contains two variables set to
, the clause can be satisfied by either one of these two variables, so the other variable can
take the dont care value.
We define a partial order on the set of all valid configurations as follows (Maneva et al.,
2004):
Definition 6. Let x and y be two valid configurations. We write x  y if i, (1) xi = yi
or (2) xi =  and yi 6= .
This partial order defines a lattice, and Maneva et al. (2004) showed that SP is a
peeling procedure that peels a satisfying configuration to its minimal element in the
lattice. A cover is a minimal element in the lattice. In the SAT region, a cover can be
defined as follows (Kroc, Sabharwal, & Selman, 2007):
Definition 7. A cover is a valid configuration x  {1, +1, }N that satisfies all clauses,
and has no unconstrained variables assigned -1 or +1.
The SP algorithm was shown to return marginals over covers (Maneva et al., 2004).
In principle, there are two kinds of covers: true covers which correspond to satisfying
configurations, and false covers which do not. Kroc et al. (2007) showed empirically that
the number of false covers is negligible for SAT instances. For RSP to apply to weighted
Max-SAT instances, we introduce the notion of v-cover:
Definition 8. A v-cover is a valid configuration x  {1, +1, }N such that
1. the total weight of clauses violated by the configuration equals v,
2. x has no unconstrained variables assigned -1 or +1.
Hence the covers defined in Definition 7 are simply v-covers with v = 0 (i.e. 0-covers).
5.2 The Extended Factor Graph
In this section, we will define a joint distribution over an extended factor graph that is
positive only over v-covers. First, we will need to define functions that will be used to
define the factors in the extended factor graph.
Definition 9. For each clause,   C, the following function assigns different values to
configurations that satisfy, violate or are invalid (see Definition 5) for :

VAL (xV () ) =



 1

exp(w y)

 0

if xV () satisfies 
if xV () violates 
if xV () is invalid

(41)

In the above definition, we introduced a parameter y in the RSP algorithm, which plays
a similar role to the y in the SP-y algorithm. The term exp(w y) is the penalty for
violating a clause with weight w .
248

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

Definition 10. (Maneva et al., 2004) Given a configuration x, we define the parent set
Pi (x) of a variable Xi to be the set of clauses for which Xi = xi is the unique satisfying
variable in a configuration x, (i.e. the set of clauses constraining Xi to its value). Formally,
Pi (x) = {  C| CONi, (xV() ) = 1}

(42)

In Example 2, for the configuration x = (+1, 1, 1), the parent sets are P1 (x) =
{5 , 6 }, P2 (x) = {2 }, and P3 (x) = .
Given the weighted Max-SAT instance (X, C, W) and its factor graph, G = ({V, F }, E),
we now construct another distribution with an associated factor graph Gs = ({V, Fs }, Es ) as
follows. For each i  V , let P (i) be the set of all possible parent sets of the variable Xi . Due
to the restrictions imposed by our definition, Pi (x) must be contained in either V + (i) or
+

V  (i), but not both. Therefore, the cardinality of P (i) is 2|V (i)| +2|V (i)| 1. Our extended
factor graph is defined on set of the variables  = (1 , 2 , ..., n )  X1  X2  ...  Xn ,
where Xi := {1, +1, }  P (i). Hence this factor graph has the same number of variables
as the original SAT instance, but each variable has a large cardinality. Given configurations
x for the SAT instance, we denote configurations of  as (x) = {i (x)}iV , where i (x) =
(xi , Pi (x)).
The definitions given so far define the semantics of valid configurations and parent sets,
and in the rest of this section, we will define factors in the extended factor graph Gs to
ensure that the above definitions are satisfied by configurations of .
The single variable compatibilities (i ) are defined by the following factor on each
variable i (x):
i (i (x) = {xi , Pi (x)}) =



 0

1


 1

if Pi (x) = , xi 6= 
if Pi (x) = , xi = 
.
for any other valid (xi , Pi (x))

(43)

The first case in the above definition for Pi (x) =  and xi 6=  corresponds to the case where
the variable Xi is unconstrained, and yet takes a value in {1, +1}. Valid configurations
that are not v-covers (with unconstrained variables set to 1 or +1) have a zero value in
the above factor. Hence only v-covers have a positive value for these factors. In the last
case in the above definition, the validity of (xi , Pi (x)) simply means that if xi = +1 (resp.
xi = 1), Pi (x)  V + (i) (resp. Pi (x)  V  (i).).
The clause compatibilities ( ) are:
 ((x)V () ) = VAL (xV() )

Q





kV () Ind [  Pk (x)] = CON,k (xV () ) ,

(44)

where Ind is defined in Definition 4. These clause compatibilities introduce the penalties in
VAL (xV () ) into the joint distribution. The second term in the above equation enforces
that the parent sets Pk (x) are consistent with the definitions of parent sets in Definition 10
for each variable Xk in the clause .
The values of x determines uniquely the values of P = {Pi (x)}iV , and hence the
distribution over (x) = {xi , Pi (x)}iV is simply a distribution over x.
Theorem 1. Using the notation UNSAT(x) to represent the set of all clauses violated by
x, the underlying distribution p() of the factor graph defined in this section is positive only
249

fiChieu & Lee

'6

'1

'1

'3

1

'2

2

'5

'2

3

'3

'4

Figure 8: The extended factor graph for the SAT instance given in Example 1. The factor
nodes i0 correspond to the clause compatibility factors i , and the single variable
factor nodes i0 represents the single variable compatibility factors i . This factor
graph is similar to the original factor graph of the SAT instance in Figure 2, except
that it has additional factor nodes i0 .

over v-covers, and for a v-cover x, we have:
p(X = x) = p( = (x)) 

Y

exp(w y),

(45)

UNSAT(x)

Proof. Configurations that are not v-covers are either invalid or contains unconstrained
variables set to 1 or +1. For invalid configurations, the distribution is zero because of the
definition of VAL , and for configurations with unconstrained variables set to 1 or +1,
the distribution is zero due to the definition of the factors i . For each v-cover, the total
penalty from violated clauses is the product term in Equation 45.
The above definition defines a joint distribution over a factor graph. The RSP algorithm
is a message passing algorithm defined on this factor graph:
Definition 11. The RSP algorithm is defined as the loopy BP algorithm applied to the
extended factor graph Gs associated with a MaxSAT instance (X, C, W).
In Section 6, we will formulate the message passing updates for RSP, as well as a
decimation algorithm for using RSP as a solver for weighted Max-SAT instances. As an
example, Figure 8 shows the extended factor graph for the weighted Max-SAT instance
defined in Example 1.
Definition 12. We define a min-cover for a weighted Max-SAT instance as the m-cover,
where m is the minimum total weight of violated clauses for the instance.
Theorem 2. When y is taken to , RSP estimates marginals over min-covers in the
following sense: the stationary points of the RSP algorithm correspond to the stationary
points of the Bethe free energy on a distribution that is uniform over min-covers.
250

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

-1,-1,-1

Energy = 11

-1,-1,+1

Energy = 9

+1,+1,+1

Energy = 4

-1,+1,+1

Energy = 3
-1,+1,-1

+1,-1,-1

+1,-1,+1

+1,+1,-1
*,+1,-1

+1,-1,*

Energy = 2

Energy = 1

Figure 9: Energy landscape for the weighted Max-SAT instance given in Example 2. Each
node represents a configuration for the variables (x1 , x2 , x3 ). For example, the
node (1, +1, 1) represents the configuration (x1 , x2 , x3 ) = (1, +1, 1).

Proof. The ratio of the probability of a v-cover to that of a (v + )-cover equals exp(y).
When y is taken to , the distribution in Equation 45 is positive only over min-covers.
Hence RSP, as the loopy BP algorithm over the factor graph representing Equation 45,
estimates marginals over min-covers.
In the application of RSP to weighted Max-SAT instances, taking y to  would often
cause the RSP algorithm to fail to converge. Taking y to a sufficiently large value is often
sufficient for RSP to be used to solve weighted Max-SAT instances.
In Figure 9, we show the v-covers of a small weighted Max-SAT example in Example 2.
In this example, there is a unique min-cover with X1 = +1, X2 = 1, and X3 = .
Maneva et al. (2004) formulated the SP- algorithm, which is equivalent to the SP
algorithm (Braunstein et al., 2005) for  = 1. The SP- algorithm is the loopy BP algorithm
on the extended factor graph defined in the work of Maneva et al. (2004). Comparing the
definitions of the extended factor graph and factors for RSP and SP-, we have (Chieu &
Lee, 2008):
Theorem 3. By taking y  , RSP is equivalent to SP- with  = 1.
Proof. The definitions of the joint distribution for SP- for  = 1 (Maneva et al., 2004),
and for RSP in this paper differ only in Definition 9, and with y   in RSP, their
definitions become identical. Since SP- and RSP are both equivalent to the loopy BP
on the distribution defined on their extended factor graphs, the equivalence of their joint
distribution means that the algorithms are equivalent.
Taking y to infinity corresponds to disallowing violated clauses, and SP- was formulated
for satisfiable SAT instances, where all clauses must be satisfied. For SP-, clause weights
are inconsequential as all clauses have to be satisfied.
In this paper, we disallow unconstrained variables to take the value . In the Appendix
A, we give an alternative definition for the single variable potentials in Equation 43. With
251

fiChieu & Lee

this definition, Maneva et al. (2004) defines a smoothing interpretation for SP-. This
smoothing can also be applied to RSP. See Theorem 6 in the work of Maneva et al. (2004)
and the Appendix A for more details.
5.3 The Importance of Convergence
It was found that message passing algorithms such as the BP and the SP algorithms perform
well whenever they converge (e.g., see Kroc, Sabharwal, & Selman, 2009). While the success
of the RSP algorithm on random ensembles of Max-SAT and weighted Max-SAT instances
are believed to be due to the clustering phenomenon on such problems, we found that
RSP could also be successful in cases where the clustering phenomenon is not observed.
We believe that the presence of large clusters help the SP algorithm to converge well, but
as long as the SP algorithm converges, the presence of clusters is not necessary for good
performance.
When covers are simply Boolean configurations (with no variables taking the  value),
they represent singleton clusters. We call such covers degenerate covers. In many structured
and non random weighted Max-SAT problems, we have found that the covers we found are
often degenerate. In a previous paper (Chieu, Lee, & Teh, 2008), we have defined a modified
version of RSP for energy minimization over factor graphs, and we show in Lemma 2 in
that paper that configurations with * have zero probability, i.e. all covers are degenerate.
In that paper, we showed that the value of y can be tuned to favor the convergence of the
RSP algorithm.
In Section 7.3, we show the success of RSP on a few benchmark Max-SAT instances.
In trying to recover the covers of the configurations found by RSP, we found that all the
benchmark instances used have degenerate covers. The fact that RSP converged on these
instances is sufficient for RSP to outperform local search algorithms.

6. Using RSP for Solving the Weighted Max-SAT Problem
In the previous section, we defined the RSP algorithm in Definition 11 to be the loopy BP
algorithm over the extended factor graph. In this section, we will derive the RSP message
passing algorithm based on this definition, before giving the decimation-based algorithm
used for solving weighted Max-SAT instances.
6.1 The Message Passing Algorithm
The variables in the extended factor graphs are no longer Boolean. They are of the form
i (x) = (xi , Pi (x)), which are of large cardinalities. In the definition of the BP algorithm,
we have stated that the message vector passed between factors and variables are of length
equal to the cardinality of the variables. In this section, we show that the messages passed
in RSP can be grouped into a few groups, so that each message passed between variables
and factors has only three values.
In RSP, the factor to variable messages are grouped as follows:
s
Mi
if xi = s,i , Pi (x) = S  {}, where S  Vs (i),
(all cases where the variable xi is constrained by the clause ),

252

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

u
Mi
if xi = u,i , Pi (x)  Vu (i),
(all cases where the variable xi is constrained to be u,i by other clauses),
s
Mi
if xi = s,i , Pi (x)  Vs (i),
(all cases where the variable xi = s,i is not constrained by . At least one other
variable xj in  satisfies  or equals . Otherwise xi will be constrained),

Mi
if xi = , Pi (x) = .

The last two messages are always equal:

s

Mi
= Mi
= Mi
.

This equality is due to the fact that for a factor that is not constraining its variables, it
does not matter whether a variable is satisfying or is , as long as there are at least two
variables that are either satisfying or is . In the following, we will consider the two equal
 .
messages as a single message, Mi
The variable to factor messages are grouped as follows:
s
:=
Ri
SVs (i) Mia (s,i , S  {}),
Variable xi is constrained by  to be s,i ,

P

u
Ri
:=
Pi (x)Vu (i) Mia (u,i , Pi (x)),
Variable xi is constrained by other clauses to be u,i ,

P

s :=
Ri
Pi (x)Vs (i) Mia (s,i , Pi (x)),
Variable xi is not constrained by , but constrained by other clauses to be s,i ,

P

 := M
Ri
i (, ),
Variable xi unconstrained and equals *.

The last two messages can again be grouped as one message (as was done in our previous
paper, Chieu & Lee, 2008) as follows,

s

Ri
= Ri
+ Ri
,

since in calculating the updates of the Mj messages from the Ri messages, only Ri
is required. The update equations of RSP for weighted Max-SAT are given in Figure 10.
These update equations are derived based on loopy BP updates in Equations 15 and 16 in
Section 3. In the worst case in a densely connected factor graph, each iteration of updates
can be performed in O(M N ) time, where N is the number of variables, and M the number
of clauses.

6.1.1 Factor to Variable Messages
We will begin with the update equations for the messages from factors to variables, given
s
in Equations 46, 47 and 48. The message Mi
groups cases where Xi is constrained by
253

fiChieu & Lee

Y

s
Mi
=

u
Rj

(46)

jV ()\{i}




Y

u
Mi
= 

u

(Rj
+ Rj
)+

jV ()\{i}
w y

+(e

Mi
=



s

(Rk
 Rk
)

kV ()\{i}

Y

u

Rj

jV ()\{i,k}

Y

u
1)
Rj
jV ()\{i}

u

(Rj
+ Rj
)

Y

X

jV ()\{i}

(47)
Y

u
Rj

(48)

jV ()\{i}



s
Ri
=


Y


u
s

Mi
(Mi
+ Mi
)


Y
Vu (i)

(49)

Vs (i)


u
Ri
=

Y


s

(Mi
+ Mi
)

 Y

Vs (i)

u
Mi


Vu (i)

Y


Mi


Vau (i)



Ri
=

Y
Vu (i)

+

(50)





Y
 Y
u
s

Mi
(Mi
+ Mi
)

Vs (i)


Mi




Vs (i)


Mi

Y

(51)

Vs (i)Vu (i)



Bi (1) 

Y
V


Y

u

Mi

+ (i)

V

s

(Mi
+ Mi
)

 (i)

Y
V

Bi (+1) 

V

Bi () 

u

Mi

 (i)

Y

(52)

 (i)


Y

 
Mi


Y

V

s

(Mi
+ Mi
)

+ (i)


Mi

Y
V

 
Mi

(53)

+ (i)

(54)

V (i)

Figure 10: The update equations for RSP. These equations are BP equations for the factor
graph defined in the text.

254

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

the factor . This means that all other variables in  are violating the factor , and hence
we have Equation 46
Y

s
Mi
=

u
Rj
,

jV ()\{i}
u
where Rj
are messages from neighbors of  stating that they will violate .
u
The next equation for Mi
states that the variable Xi is violating . In this case, the
other variables in  are in these possible cases

1. Two or more variables in  satisfying , with the message update
Y

u

(Rj
+ Rj
)

jV ()\{i}


Rk

X

Y

kV ()\{i}

u
Rj


jV ()\{i,k}

Y

u
Rj
.

jV ()\{i}

2. Exactly one variable in V ()\{i} constrained by , and all other variables are violating
, with the message update
X

Y

s
Rk

u
Rj

jV ()\{i,k}

kV ()\{i}

3. All other variables are violating , and in this case, there is a penalty factor of
exp(w y), with the message update
Y

exp(w y)

u
Rj

jV ()\{i}

The sum of these three cases result in Equation 48.

The third update equation for Mi
is for the case where the variable Xi is uncons ) or  (for M  ). This means that
strained by , satisfying  with s,i (for the case Mi
i
there is at least one other satisfying variable that is unconstrained by , with the message
update
Y

u

(Rj
+ Rj
)

jV ()\{i}

Y

u
Rj

jV ()\{i}

6.1.2 Variable to Factor Messages
s
consists of the case where the variable Xi is constrained by the
The first message Ri
factor , which means that it satisfies neighboring factors in Vs (i), and violates factors in
Vu (i), with probability


Y
Vu (i)

 Y

u
Mi



s
s
(Mi
+ Mi
) .



Vs (i)

u
The second message Ri
is the case where Xi violates . In this case, all other variables
u
in V (i) are satisfied, while clauses in Vs (i) are violated. In this case, the variable Xi must

255

fiChieu & Lee

be constrained by one of the clauses in Vu (i). Hence the message update is

Y
Vs (i)



Y
 Y
u
s
s
Mi
(Mi
+ Mi
)

Vu (i)

Vu (i)

s
Mi





s
 . For the message
The third message Ri
is the sum of two messages Ri
and Ri
the variable Xi satisfies  but is not constrained by , and so it must be constrained
by some other factors:

s ,
Ri


Y



 Y

Vu (i)

u
Mi


s
s
(Mi
+ Mi
)

Vs (i)

Y

Vs (i)

s
Mi




 , is the case where X = , :
The second part of the message, Ri
i

Mi
,

Y
Vs (i)Vu (i)

and the sum of the above two equations results in Equation 51.
6.1.3 The Beliefs
The beliefs can be calculated from the factor to variable messages once the algorithm converges, to obtain estimates of the marginals over min-covers. The calculation of the beliefs
is similar to the calculation of the variable to factor messages.
The belief Bi (1) is the belief or the variable Xi taking the value 1. This is the case
where the variable Xi satisfies clauses in V  (i), and violates clauses in V + (i). In this case,
Xi must be constrained by one of the factors in V  (i). Hence the belief is as follows:

Y
V + (i)

u

Mi


Y

s
s
(Mi
+ Mi
)

V  (i)

Y

s 
Mi
.

V  (i)

The calculation of the belief Bi (+1) is similar to Bi (1). The belief Bi () is the case where
Xi = , and hence it is calculated as follows:
Y


Mi
.

V ( i)

6.2 Comparing the RSP and SP-y Message Passing Algorithms
The message passing algorithms for RSP and SP-y share many similarities. Both algorithms
1. include a multiplicative penalty into the distribution for each violated clause.
2. contain a mechanism for a dont care state. For SP-y, this occurs when a variable
receives no warnings from neighboring factors.
However, there are a number of significant differences in the two algorithms.
256

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

1. In RSP, the penalties are imposed as each factor passes a message to a variable. For
SP-y, the penalties are imposed when a variable compiles all the incoming warnings,
and decides how many factors it is going to violate.
2. Importantly, in RSP, variables participating in violated clauses can never take the *
value. For SP-y, a variable receiving an equal number of warnings from the set of
+
factors { 0 } 0 V (i)\ against taking the +1 and the 1 value (i.e. hj = Hj


Hj = 0) will decide to pass a message with no warning to . Hence for SP-y, it is
possible for variables in violated clauses to take a dont care state.
3. In the work of Battaglia et al. (2004) where SP-y was formulated with the cavity
approach, it was found that the optimal value of y for a given Max-SAT problem is
y  = 
e , where  is the complexity in statistical physics, and e is the energy density
(Mezard & Zecchina, 2002). They stated that y  is a finite value when the energy of
the Max-SAT problem is not zero. In Theorem 2, we show that for RSP, y should
be as large as possible so that the underlying distribution is over min-covers. In our
experimental results in Figure 12, we showed that this is indeed true for RSP, as long
as it converges.

INPUT: A (weighted) Max-SAT instance, a constant k, and yin
OUTPUT: A configuration.
ALGORITHM :
1. Randomly initialize the surveys and set y = yin .
2. Run RSP with y. If RSP converges, sort the variables according to the quantities
bi = |P (xi = +1)  P (xi = 1)|, and fix the top k variables to their preferred
values, subject to the condition that bi > 0.5.
3. (For weighted Max-SAT) If RSP fails to converge, adjust the value of y.
4. If RSP converges and at least one variable is set, go back to step (1) with the simplified instance. Otherwise, run the (weighted) WalkSAT solver on the simplified
instance and output the configuration found.
Figure 11: The decimation algorithm for RSP for solving a (weighted) Max-SAT instance

6.3 The Decimation Algorithm
The decimation algorithm is shown in Figure 11. This is the algorithm we used for our
experiments described in Section 7. In comparing RSP with SP-y on random Max-SAT
instances in Section 7.1, we run both algorithms with a fixed yin , and vary the yin over
a range of values. Comparing Figure 11 to Figure 7 for SP-y, the condition used in SPy to check for a paramagnetic solution is replaced by the condition given in Step (2) in
Figure 11. In the experimental results in Section 7.1, we used the SP-y implementation
257

fiChieu & Lee

available online (Battaglia et al., 2004), which contains a mechanism for backtracking on
decimation decisions (see Figure 7). In Section 7.1, RSP still outperforms SP-y despite not
backtracking on its decisions. When running RSP on weighted Max-SAT, we found that
it was necessary to adjust y dynamically during the decimation process. For details on
experimental settings, please refer to Section 7.

7. Experimental Results
We run experiments on random Max-3-SAT, random weighted Max-SAT, as well as on a
few benchmark Max-SAT instances used in the work of Lardeux, Saubion, and Hao (2005).
#Viols

#Viols

Figure 12: Behaviour of SP-y and RSP over varying values of y on the x-axis, and the
number of violated clauses (#viols) on the y-axis. The comparison of the performances between RSP and SP-y are shown in Table 1. The objective of showing
the graphs in this figure is to show that the behavior of RSP over varying y is
consistent with Theorem 2: as long as RSP converges, its performance improves
as y increases. In the graph, RSP reaches a plateau when it fails to converge.This
property allows for a systematic search for a good value of y to be used. The
behavior of SP-y over varying y is less consistent.
.

7.1 Random Max-3-SAT
We run experiments on randomly generated Max-3-SAT instances of 104 variables, with
different clause-to-variable ratios. The random instances are generated by the SP-y code
available online (Battaglia et al., 2004). In Figure 12, we compare SP-y and RSP on random
Max-3-SAT with different clause-to-variable ratio, . We vary  from 4.2 to 5.2 to show the
performance of SP-y and RSP in the UNSAT region of 3-SAT, beyond its phase transition
at c  4.267. For each value of , the number of violated clauses (y-axis) is plotted against
the value of y used.
258

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

We perform the decimation procedure in Figure 11 for RSP, for a fixed value of yin ,
decimating 100 variables at a time (i.e. k = 100). For SP-y, we run the SP-y code available
on line, with the option of decimating 100 variables at each iteration, and with two different
settings: with and without backtracking (Battaglia et al., 2004). Backtracking is a procedure used in SP-y to improve performance, by unfixing previously fixed variables at a rate
r = 0.2, so that errors made by the decimation process can be corrected. For RSP, we do
not run backtracking. Note that the y in our formulation equals to 2y in the formulation
in the work of Battaglia et al. (Battaglia et al., 2004).
Both SP-y and RSP fail to converge when y becomes large enough. When this happens,
the output of the algorithm is the result returned by WalkSAT on the original instance. In
Figure 12, we see this happening when a curve reaches a horizontal line, signifying that the
algorithm is returning the same configuration regardless of y (we seed the randomized
WalkSAT so that results are identical when instances are identical). From Figure 12, we
see RSP performs more consistently than SP-y: as y increases, the performance of RSP
improves, until a point where RSP fails to converge. Interestingly for Max-3-SAT instances,
we observed that once RSP converges for a value of y for a given instance, it will continue to
converge for the same value of y throughout the decimation process. Hence, the best value
of y for RSP is obtainable without going through the decimation process: we can commence
decimation at the largest value of y for which RSP converges. In Table 1, we show that RSP
outperforms SP-y for   4.7, despite the fact that we did not allow backtracking for RSP.
We also compare RSP and SP-y with the local search solvers implemented in UBCSAT
(Tompkins & Hoos, 2004). We run 1000 iterations of each of the 20 Max-SAT solvers in
UBCSAT, and take the best result among the 20 solvers. The results are shown in Table 1.
We see that the local solvers in UBCSAT does worse than both RSP and SP-y. We have
also tried running complete solvers such as toolbar (de Givry, Heras, Zytnicki, & Larrosa,
2005) and maxsatz (Li, Manya, & Planes, 2006). They are unable to deal with instances of
size 104 .
7.2 Random Weighted Max-3-SAT
We have also run experiments on randomly generated weighted Max-3-SAT instances. These
instances are generated in the same way as the instances for Max-3-SAT, and in addition,
the weights of each clause is uniformly sampled as integers in the set [1, M ], where M is
the upper bound on the weights. We show the experimental results for M = 5 and M = 10
in Figure 13. We compare RSP with the 13 weighted Max-SAT solvers implemented in
UBCSAT. For RSP, we run all our experiments with an initial y set to 10, and whenever
the algorithm fails to converge, we lower the value of y by 1, or halve the value of y if y
is less than 1 (see Figure 11). We see that RSP outperforms UBCSAT consistently in all
experiments in Figure 13.
7.3 Benchmark Max-SAT Instances
We compare RSP with UBCSAT on instances used in the work of Lardeux et al. (2005),
which were instances used in the SAT 2003 competition. Among the 27 instances, we use
the seven largest instances with more than 7000 variables. We run RSP in two settings:
decimating either 10 or 100 variables at a time. We run RSP for increasing values of y: for
259

fiChieu & Lee

Table 1: Number of violated clauses attained by each method. For SP-y, SP-y (BT) (SPy with backtracking), and RSP, the best result is selected over all y. For each ,
we show the best performance in bold face. The column Fix shows the number
of variables fixed by RSP at the optimal y, and Time the time taken by RSP
(in minutes) to fix those variables, on an AMD Opteron 2.2GHz machine.

4.2
4.3
4.4
4.5
4.6
4.7
4.8
4.9
5.0
5.1
5.2

UBCSAT
47
68
95
128
140
185
232
251
278
311
358

SP-y
0
9
42
67
98
137
204
223
260
294
362

SP-y(BT)
0
7
31
67
89
130
189
211
224
280
349

RSP
0
10
36
65
90
122
172
193
218
267
325

Fix
7900
7200
8938
9024
9055
9287
9245
9208
9307
9294
9361

Time (minutes)
24
43
82
76
45
76
52
62
66
42
48

W-viol

W-viol





Figure 13: Experimental results for weighted Max-SAT instances. The x-axis shows the
value of , and the y-axis (W-viol) is the number of violated clauses returned
by each algorithm.

each y, RSP fixes a number of spins, and we stop increasing y when the number of spins
fixed decreases over the previous value of y. For UBCSAT, we run 1000 iterations for each
of the 20 solvers. Results are shown in Table 2. Out of the seven instances, RSP fails to
fix any spins on the first one, but outperforms UBCSAT on the rest. Lardeux et al. (2005)
did not show best performances in their paper, but their average results were an order of
magnitude higher than results in Table 2. Figure 12 shows that finding a good y for SP-y
is hard. On the benchmark instances, we run SP-y with the -Y option (Battaglia et al.,
2004) that uses dichotomic search for y: SP-y failed to fix any spins on all 7 instances.
260

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

Table 2: Benchmark Max-SAT instances. Columns: instance shows the instance name in
the paper of Lardeux et al. (2005), nvar the number of variables, ubcsat and
rsp-x (x is the number of decimations at each iteration) the number of violated
clauses returned by each algorithm, and fx-x the number of spins fixed by RSP.
Best results are indicated in bold face.
instance
fw
nc
nw
35-4-03
35-4-04
40-4-02
40-4-03

nvar ubcsat rsp-100 fx-100 rsp-10
family: purdom-10142772393204023
9366
83
357
0
357
8372
74
33
8339
35
8589
73
24
8562
28
family: pyhala-braun-unsat
7383
58
68
7295
44
7383
62
53
7302
41
9638
86
57
9547
65
9638
76
77
9521
41

fx-10
0
8316
8552
7299
7304
9521
9568

The success of the SP family of algorithms on random ensembles of SAT or Max-SAT
problem are usually due to the clustering phenomenon on such random ensembles. As the
benchmark instances are not random instances, we attempted to see if the configurations
found by RSP do indeed belong to a cover representing a cluster of solutions. Rather
disappointingly, we found that for all 6 solutions where RSP outperformed local search
algorithms, the variables in the solutions are all constrained by at least one clause. Hence,
the v-covers found are degenerate covers, i.e. the covers do not contain variables set to
. It appears that the success of RSP on these benchmark instances is not due to the
clustering phenomenon, but simply because RSP manages to converge for these instances,
for some value of y. Kroc, Sabharwal, and Selman (2009) made a similar observation: the
convergence of BP or SP like algorithms is often sufficient for obtaining a good solution to
a given problem. As discussed in Section 5.3, the ability to vary y to improve convergence
is a useful feature of RSP, but one that is distinct from its ability to exploit the clustering
phenomenon.

8. Conclusion
While recent work on Max-SAT or weighted Max-SAT tends to focus more on complete
solvers, these solvers are unable to handle large instances. In the Max-SAT competition
2007 (Argelich, Li, Manya, & Planes, 2007), the largest Max-3-SAT instances used have
only 70 variables. For large instances, complete solvers are still not practical, and local
search procedures have been the only feasible alternative. SP-y, generalizing SP, has been
shown to be able to solve large Max-3-SAT instances at its phase transition, but lacks the
theoretical explanations that recent work on SP has generated.
For 3-SAT, there is an easy-hard-easy transition as the clause-to-variable ratio increases.
For Max-3-SAT, however, it has been shown empirically that beyond the phase transition
of satisfiability, all instances are hard to solve (Zhang, 2001). In this paper, we show that
261

fiChieu & Lee

RSP outperforms SP-y as well as other local search algorithms on Max-SAT and weighted
Max-SAT instances, well beyond the phase transition region.
Both RSP and SP-y do well on Max-SAT instances near the phase transition. The
mechanisms behind SP-y and RSP are similar: both algorithms impose a penalty term for
each violated constraint, and both reduce to SP when y  . SP-y uses a population
dynamics algorithm, which can also be seen as a warning propagation algorithm. In this
paper, we have formulated the RSP algorithm as a BP algorithm over an extended factor
graph, enabling us to understand RSP as estimating marginals over min-covers.

Acknowledgments
This work is supported in part by NUS ARF grant R-252-000-240-112.

Appendix A. Smoothing Interpretation for RSP
In the definition of SP- (Maneva et al., 2004), the parameter  was introduced to define a
whole family of algorithms. For  = 1, the SP- algorithm corresponds to the SP algorithm,
while for  = 0, the SP- algorithm corresponds to the BP algorithm. In this section, we
develop a more general version of the extended factor graph defined in Section 5, that
incorporates the  in SP-. We will call the corresponding RSP algorithm on this new
factor graph the RSP- algorithm.
The only difference between the factor graph for RSP- and the one in Section 5 is the
definition of the variable compatibilities in Equation 43. Following notations in the work
of Maneva et al. (2004), we introduce the parameters 0 and  , and we restrict ourselves
to the case where 0 +  = 1 (The  in SP- or RSP- is equal to  ). We redefine the
variable compatibilities as follows

i (i (x) = {xi , Pi (x)}) =



 0





 1

if Pi (x) = , xi 6= 
if Pi (x) = , xi = 
,
for any other valid (xi , Pi (x))

(55)

with 0 +  = 1. The definition in Equation 43 corresponds to the particular case where
0 = 0 and  = 1. In Section 5, we have defined the factor graph so that unconstrained
variables must take the value . With the new definition of i above, unconstrained variables are allowed to take on the values 1 or +1 with weight 0 , and the  value with
weight  .
With the above definition, the joint distribution in Equation 45 is redefined as follows:
n (x) n (x) Y
exp(w y).

UNSAT(x)

P (x) = P ({xk , Pk }k )  0 0

(56)

where n0 (x) is the number of unconstrained variables in x taking +1 or 1, and n (x) the
number of unconstrained variables taking  in x.
Case  = 1: we have studied this case in the main paper: the underlying distribution
is a distribution which is positive only over v-covers.
262

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

Case  = 0: in this case, only configurations x with n (x) = 0 have non-zero probability in the distribution given in Equation 56. Hence, the value  is forbidden, and all
variables take values in 1, +1. A Boolean configuration violating clauses with total weight
W has a probability proportional to exp(yW ). Hence we retreive the weighted Max-SAT
energy defined in Equation 13. In this case, the factor graph is equivalent to the original
weighted Max-SAT factor graph defined in Definition 3, and hence RSP- is equivalent to
the loopy BP algorithm on the original weighted Max-SAT problem.
Case  6= 1 and  6= 0: in this case, all valid configurations x violating clauses
n (x) n (x)
with a total weight W has a probability proportional to 0 0   exp(yW ). Hence,
the probability of v-covers in the case where  = 1 are spread over the lattice for which it
is the minimal element.
With the above formulation, RSP- can be seen as a family of algorithms that include
the BP and the RSP algorithm, moving from BP to RSP as  (or  ) varies from 0 to 1.

References
Achlioptas, D., Naor, A., & Peres, Y. (2005). Rigorous location of phase transitions in hard
optimization problems. Nature, 435 (7043), 759764.
Argelich, J., Li, C. M., Manya, F., & Planes, J. (2007). Second evaluation of max-sat
solvers. In SAT07: Tenth International Conference on Theory and Applications of
Satisfiability Testing.
Battaglia, D., Kolar, M., & Zecchina, R. (2004). Minimizing energy below the glass thresholds. Physical Review E, 70.
Berre, D. L., & Simon, L. (2003). The essentials of the SAT 2003 competition. In SAT03:
Sixth International Conference on Theory and Applications of Satisfiability Testing.
Berre, D. L., & Simon, L. (2005). Special volume on the SAT 2005 competitions and
evaluations. Journal on Satisfiability, Boolean Modeling and Computation, 2.
Braunstein, A., Mezard, M., & Zecchina, R. (2005). Survey propagation: An algorithm for
satisfiability. Random Structures and Algorithms, 27 (2).
Braunstein, A., & Zecchina, R. (2004). Survey propagation as local equilibrium equations.
Journal of Statistical Mechanics: Theory and Experiment, 2004 (06).
Braunstein, A., & Zecchina, R. (2006). Learning by message-passing in networks of discrete
synapses. Physical Review Letters, 96, 030201.
Chieu, H. L., & Lee, W. S. (2008). Relaxed survey propagation: a sum-product algorithm
for Max-SAT. In AAAI08: Twenty-Third AAAI Conference on Artificial Intelligence.
Chieu, H. L., Lee, W. S., & Teh, Y. W. (2008). Cooled and relaxed survey propagation for
MRFs. In NIPS07: Advances in Neural Information Processing Systems 20, Cambridge, MA. MIT Press.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In STOC 71: Third
annual ACM symposium on Theory of computing, pp. 151158, New York, NY, USA.
ACM.
263

fiChieu & Lee

Cooper, G. F. (1990). The computational complexity of probabilistic inference using
bayesian belief networks (research note). Artif. Intell., 42 (2-3), 393405.
Daude, H., Mezard, M., Mora, T., & Zecchina, R. (2008). Pairs of sat-assignments in random
boolean formul. Theor. Comput. Sci., 393 (1-3), 260279.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem-proving.
Commun. ACM, 5 (7), 394397.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal
of the ACM (JACM), 7 (3), 201215.
de Givry, S., Heras, F., Zytnicki, M., & Larrosa, J. (2005). Existential arc consistency:
Getting closer to full arc consistency in weighted CSPs.. In IJCAI05: Nineteenth
International Joint Conference on Artificial Intelligence.
Dubois, O., & Dequen, G. (2001). A backbone-search heuristic for efficient solving of hard 3SAT formulae. In IJCAI05: Seventeenth International Joint Conference on Artificial
Intelligence, pp. 248253.
Een, N., & Sorensson, N. (2005). MiniSat - a SAT solver with conflict-clause minimization.
In SAT05: Eighth International Conference on Theory and Applications of Satisfiability Testing.
Hoos, H. H. (2002). An adaptive noise mechanism for walksat. In AAAI02: Eighteenth
National Conference on Artificial Intelligence, pp. 655660.
Kaporis, A. C., Kirousis, L. M., & Lalas, E. G. (2006). The probabilistic analysis of a greedy
satisfiability algorithm. Random Structures and Algorithms, 28 (4), 444480.
Kirkpatrick, S., Jr., C. D. G., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220, 671680.
Kirousis, L. M., Kranakis, E., Krizanc, D., & Stamatiou, Y. C. (1998). Approximating the
unsatisfiability threshold of random formulas. Random Structures and Algorithms,
12 (3), 253269.
Kroc, L., Sabharwal, A., & Selman, B. (2007). Survey propagation revisited. In UAI07:
Twenty-Third Conference on Uncertainty in Artificial Intelligence.
Kroc, L., Sabharwal, A., & Selman, B. (2009). Message-passing and local heuristics as
decimation strategies for satisfiability. In SAC-09. 24th Annual ACM Symposium on
Applied Computing.
Kschischang, F. R., Frey, B., & Loeliger, H.-A. (2001). Factor graphs and the sum-product
algorithm. IEEE Transactions on Information Theory, 47 (2).
Lardeux, F., Saubion, F., & Hao, J.-K. (2005). Three truth values for the SAT and MAXSAT problems. In IJCAI05: Nineteenth International Joint Conference on Artificial
Intelligence.
Larrosa, J., & Heras, F. (2005). Resolution in Max-SAT and its relation to local consistency in weighted CSPs. In IJCAI05: Nineteenth International Joint Conference on
Artificial Intelligence.
Levin, L. A. (1973). Universal search problems. Problemy Peredaci Informacii, 9, 115116.
264

fiRelaxed Survey Propagation for The Weighted Max-SAT Problem

Li, C. M., & Anbulagan (1997). Heuristics based on unit propagation for satisfiability problems. In IJCAI97: Fifteenth International Joint Conference on Artificial Intelligence,
pp. 366371.
Li, C. M., Manya, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulas for
computing lower bounds for max-sat. In AAAI06: Twenty-First AAAI Conference
on Artificial Intelligence.
Maneva, E., Mossel, E., & Wainwright, M. (2004). A new look at survey propagation and
its generalizations. http://arxiv.org/abs/cs.CC/0409012.
McAllester, D., Selman, B., & Kautz, H. (1997). Evidence for invariants in local search. In
AAAI97: Proceedings of the Fourteenth National Conference on Artificial Intelligence,
pp. 321326, Providence, Rhode Island.
Mezard, M., Mora, T., & Zecchina, R. (2005). Clustering of solutions in the random satisfiability problem. Physical Review Letters, 94, 197205.
Mezard, M., & Parisi, G. (2003). The cavity method at zero temperature. Journal of
Statistical Physics, 111.
Mezard, M., & Zecchina, R. (2002). The random k-satisfiability problem: from an analytic
solution to an efficient algorithm. Physical Review E, 66.
Minton, S., Philips, A., Johnston, M. D., & Laird, P. (1992). Minimizing conflicts: a heuristic
repair method for constraint satisfaction and scheduling problems. Artificial Intelligence, 58, 161205.
Moskewicz, M. W., & Madigan, C. F. (2001). Chaff: Engineering an efficient SAT solver.
In DAC01: Thirty-Ninth Design Automation Conference, pp. 530535.
Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy belief propagation for approximate
inference: An empirical study. In UAI99: Fifteenth Annual Conference on Uncertainty
in Artificial Intelligence, pp. 46747, San Francisco, CA. Morgan Kaufmann.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
Pipatsrisawat, K., & Darwiche, A. (2007). Rsat 2.0: Sat solver description. Tech. rep.,
Automated Reasoning Group, Computer Science Department, UCLA.
Prestwich, S. D. (2003). Local search on sat-encoded colouring problems.. In Giunchiglia,
E., & Tacchella, A. (Eds.), SAT, Vol. 2919 of Lecture Notes in Computer Science, pp.
105119. Springer.
Rabiner, L., & Juang, B. (1993). Fundamentals of Speech Recognition. Prentice-Hall.
Selman, B., Kautz, H. A., & Cohen, B. (1994). Noise strategies for improving local search.
In AAAI97: Twelfth National Conference on Artificial Intelligence, pp. 337343.
Selman, B., Levesque, H. J., & Mitchell, D. (1992). A new method for solving hard satisfiability problems. In AAAI92: Tenth National Conference on Artificial Intelligence,
pp. 440446. AAAI Press.
Shang, Y., & Wah, B. W. (1998). A discrete lagrangian-based global-searchmethod for
solving satisfiability problems. Journal of Global Optimization, 12 (1), 6199.
265

fiChieu & Lee

Tompkins, D., & Hoos, H. (2004). UBCSAT: An implementation and experimentation environment for SLS algorithms for SAT and MAX-SAT. In SAT04: Seventh International
Conference on Theory and Applications of Satisfiability Testing.
Tseitin, G. S. (1968). On the complexity of derivations in the propositional calculus. Studies
in Mathematics and Mathematical Logic, Part II, 115125.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. In NIPS00:
Advances in Neural Information Processing Systems 13, pp. 689695.
Yedidia, J., Freeman, W., & Weiss, Y. (2005). Constructing free-energy approximations
and generalized belief propagation algorithms. IEEE Transactions on Information
Theory, 51 (7).
Zhang, W. (2001). Phase transitions and backbones of 3-SAT and maximum 3-SAT. In
Proceedings of the Seventh International Conference on Principles and Practice of
Constraint Programming.

266

fiJournal of Artificial Intelligence Research 36 (2009) 471-511

Submitted 7/09; published 12/09

The Role of Macros in Tractable Planning
Anders Jonsson

anders.jonsson@upf.edu

Dept. of Information and Communication Technologies
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, Spain

Abstract
This paper presents several new tractability results for planning based on macros.
We describe an algorithm that optimally solves planning problems in a class that we call
inverted tree reducible, and is provably tractable for several subclasses of this class. By
using macros to store partial plans that recur frequently in the solution, the algorithm is
polynomial in time and space even for exponentially long plans. We generalize the inverted
tree reducible class in several ways and describe modifications of the algorithm to deal with
these new classes. Theoretical results are validated in experiments.

1. Introduction
A growing number of researchers in planning are investigating the computational complexity
of solving different classes of planning problems. Apart from the theoretical value of such
knowledge, of particular interest are tractable planning problems that can be provably solved
in polynomial time. Many, if not most, state-of-the-art planners are based on heuristic
search. Acquiring an informative heuristic requires quickly solving a relaxed version of the
planning problem. Known tractable classes of planning problems are ideal candidates for
projecting the original problem onto in order to generate heuristics.
This paper introduces the class IR of inverted tree reducible planning problems and
presents an algorithm that uses macros to solve instances of this class. The algorithm
is provably complete and optimal for IR, and its complexity depends on the size of the
domain transition graph that the algorithm constructs. In the worst case, the size of the
graph is exponential in the number of state variables, but we show that the algorithm runs
in polynomial time for several subclasses of IR, proving that plan generation is tractable
for these classes.
The tractable subclasses of IR include planning problems whose optimal solution has
exponential length in the number of state variables. The reason that our algorithm can solve
these problems in polynomial time is that subsequences of operators are frequently repeated
in the solution. Since the algorithm stores each such subsequence as a macro, it never needs
to generate the subsequence again. The algorithm can be viewed as a compilation scheme
where state variables are compiled away. The only information kept about each state
variable is the set of macros acting on it.
We extend the class IR in two ways and modify our algorithm so that it solves planning
problems in the new classes. First, we define the notion of a relaxed causal graph, which
contains less edges than the conventional causal graph, and an associated class RIR of
relaxed inverted tree reducible planning problems. We also define the notion of reversible
c
2009
AI Access Foundation. All rights reserved.

fiJonsson

variables, and define the class AR of planning problems with acyclic causal graphs and
reversible variables. We show that our algorithm can be modified to solve planning problems
in this class. Finally, we combine the results for IR and AR to define the class AOR of
planning problems with acyclic causal graphs and only some of the variables reversible.
The class IR and the algorithm for solving instances of the class previously appeared in
a conference paper (Jonsson, 2007). The present paper includes new tractability results for
the classes IR, AR, and AOR. In addition, the notion of relaxed causal graphs and the
resulting class RIR are novel.
Perhaps the most important contribution of this work is establishing several novel
tractability results for planning. In particular, we present several classes of planning problems that can be solved in polynomial time. This contribution is particularly significant
since the number of known tractable classes of planning problems is very small. In addition, some of our new classes can be solved optimally. The novel approach taken by the
paper is the use of macros to store solutions to subproblems, making it possible to represent
exponentially long plans in polynomial time and space.
A related contribution is the possibility to use the tractable classes to generate heuristics.
We discuss two possibilities for generating heuristics here. One idea is to project a planning
problem onto IR by removing some pre-conditions and effects of operators to make the
causal graph acyclic and inverted tree reducible. In fact, this is the strategy used by
Helmert (2006) to compute the causal graph heuristic. Next, we could duplicate each
operator by introducing pre-conditions on missing variables. The resulting problem is
tractable according to Theorem 3.5. Thus, the algorithm could solve the projected problem
in polynomial time to compute an admissible heuristic for the original problem. Note,
however, that the number of resulting actions is exponential in the number of missing
variables, so the strategy will only be tractable when the number of missing variables is a
small constant.
Another idea is to perform the same reduction until the causal graph is acyclic and each
variable has a constant number of ancestors. Next, we could make the current problem part
of the class AOR by introducing actions to make certain variables reversible. Both types of
changes have the effect of making the problem easier to solve, in the sense that a solution to
the relaxed problem is guaranteed to be no longer than the solution to the original problem.
We could then solve the resulting problem using our algorithm for AOR. Although this
algorithm is not provably optimal, it might be an informative heuristic in some cases.
Another contribution is the possibility of solving some planning problems in polynomial
time, since they fall into one of the classes of planning problems studied in this paper. For
example, we show that this is possible for the Gripper and Logistics domains. Although
these domains were previously known to be polynomial-time solvable, it was not known
whether they could be solved using the type of macro approach taken here. If a planning
problem cannot be solved directly, parts of the causal graph might have the structure
required by our algorithms. In this case, the algorithms could be used to solve part of
the planning problem. Since one feature of the algorithms is compiling away variables and
replacing them with macros, the reduced problem could then be fed to a standard planner
to obtain the final solution.
Perhaps a more interesting use of the algorithms is the possibility to reuse macros.
If part of the causal graph structure coincides between two problems, macros generated
472

fiThe Role of Macros in Planning

in one problem could immediately be substituted in the other problem. Since variables
are compiled away and replaced by macros, the resulting problem has fewer variables and
actions. For example, in Tower of Hanoi, the macros generated for the n  1 disc problem
could immediately be substituted in the n disc problem. The resulting problem only includes
the macros for the n  1 disc problem and a single variable corresponding to the n-th disc.
The rest of the paper is organized as follows. Section 2 introduces notation and definitions. Section 3 introduces the class IR of inverted tree reducible planning problems,
presents an algorithm for solving problems in this class, and proves several theoretical properties about the algorithm. Section 4 introduces several extensions of the class IR, as well as
corresponding algorithms. Section 5 presents experimental results validating the theoretical
properties of the algorithms. Section 6 relates the paper to existing work in planning, and
Section 7 concludes with a discussion.

2. Notation
Let V be a set of variables, and let D(v) be the finite domain of variable v  V . We define
a state s as a function on V that maps each variable v  V to a value s(v)  D(v) in its
domain. A partial state x is a function on a subset Vx  V of variables that maps each
variable v  Vx to x(v)  D(v). Sometimes we use the notation (v1 = x1 , . . . , vk = xk ) to
denote a partial state x defined by Vx = {v1 , . . . , vk } and x(vi ) = xi for each vi  Vx .
We define several operations on partial states. For a subset W  V of variables, x | W
is the partial state obtained by restricting the scope of x to Vx  W . Two partial states x
and y match, which we denote x  y, if x | Vy = y | Vx , i.e., x(v) = y(v) for each v  Vx Vy .
Given two partial states x and y, let x  y, the composition of x and y, be a partial state z
defined by Vz = Vx  Vy , z(v) = y(v) for each v  Vy , and z(v) = x(v) for each v  Vx  Vy .
Note that composition is not symmetric since the right operand overrides the values of the
left operand.
A planning problem is a tuple P = hV, init, goal, Ai, where V is the set of variables,
init is an initial state, goal is a partial goal state, and A is a set of actions. An action
a = hpre(a); post(a)i  A consists of a partial state pre(a) called the pre-condition and a
partial state post(a) called the post-condition. Action a is applicable in any state s such
that s  pre(a) and results in a new state s  post(a).
The causal graph of a planning problem P is a directed graph (V, E) with the variables
as nodes. There is an edge (w, v)  E if and only if w 6= v and there exists an action a  A
such that w  Vpre(a)  Vpost(a) and v  Vpost(a) . If the causal graph is acyclic, each action
a  A is unary, i.e., |Vpost(a) | = 1. For each variable v  V , we define Anc(v) as the set of
ancestors of v in the causal graph, and Desc(v) as the set of descendents of v.
A macro-action, or macro for short, is an ordered sequence of actions. For a macro
to be well-defined, the pre-condition of each action has to coincide with the cumulative
post-condition of the actions that precede it in the sequence. Although an action sequence
implicitly induces a pre- and a post-condition, we explicitly associate a pre- and a postcondition with each macro. Since a macro is functionally an action, we can define hierarchies
of macros such that the action sequence of a macro includes other macros, as long as this
does not cause the definitions of macros to be cyclic.
473

fiJonsson

v1

v3

v1

v3

v2

v4

v5
v2

v5

v4

Figure 1: An acyclic causal graph and its transitive reduction.
Definition 2.1. A sequence of actions seq = ha1 , . . . , ak i is well-defined for a state s if
s  pre(a1 ) and s  post(a1 )  . . .  post(ai1 )  pre(ai ) for each i  {1, . . . , k}.
We sometimes refer to the post-condition post(seq) of an action sequence, defined as
post(seq) = post(a1 )  . . .  post(ak ). Given two sequences seq1 and seq2, hseq1, seq2i
is the concatenation of the two sequences, and post(hseq1, seq2i) is the post-condition of
the resulting sequence, defined as post(seq1)  post(seq2). We are now ready to formally
define macros as we use them in the paper:
Definition 2.2. A macro m = hpre(m), seq(m), post(m)i consists of a pre-condition pre(m),
a sequence seq(m) = ha1 , . . . , ak i of actions in A and other macros, and a post-condition
post(m). The macro m is well-defined if seq(m) is well-defined for pre(m) and pre(m) 
post(seq(m)) = post(m).

3. The Class IR
Katz and Domshlak (2008a) defined I as the class of planning problems whose causal graphs
are inverted trees, i.e., the outdegree of each variable in the causal graph is less than or equal
to 1, and there is a unique root variable v with outdegree 0. In this section, we study a
class of planning problems that we call IR, which stands for inverted tree reducible:
Definition 3.1. A planning problem P is inverted tree reducible if and only if the causal
graph of P is acyclic and the transitive reduction of the causal graph is an inverted tree.
The transitive reduction (V, E  ) of a graph (V, E) is defined on the same set of nodes
V , while E  contains the minimal set of edges such that the transitive closure of E  is equal
to the transitive closure of E. In other words, the transitive reduction only retains edges
necessary to maintain connectivity. For acyclic graphs, the transitive reduction is unique
and can be computed efficiently. Figure 1 illustrates the causal graph and its transitive
reduction for a planning problem in the class IR. Here, the root variable is v5 .
For a graph G = (V, E) on the variables V of a planning problem P and each variable
v  V , let P a(v) = {w  V : (w, v)  E} be the set of parents of v in G, i.e., variables on
incoming edges. In this paper, we always refer to the parents of v in the transitive reduction
(V, E  ), as opposed to the causal graph itself. For example, in Figure 1 the parents of v5 in
the causal graph are {v1 , . . . , v4 }, but P a(v5 ) = {v3 , v4 }. Note that the transitive reduction
preserves the set of ancestors Anc(v) and descendants Desc(v) of a variable v.
Without loss of generality, in what follows we assume that the goal state is defined on
the root variable v of the transitive reduction, i.e., v  Vgoal . Since there are no outgoing
edges from v, the pre-condition of any action a  A that changes the value of a variable
474

fiThe Role of Macros in Planning

w 6= v is independent of v. If v 
/ Vgoal , a plan that solves P never needs to change the value
of v, so we can eliminate v from the problem, resulting in one or several subgraphs of the
causal graph. It is trivial to show that the transitive reductions of the resulting subgraphs
are inverted trees, so the problem can be decomposed into one or several planning problems
in IR that can be solved independently.
For convenience, we add a dummy variable v  to V , as well as a dummy action a =
hgoal; (v  = 1)i to A. The transitive reduction of the resulting causal graph contains an
additional node v  and an additional edge (v, v  ), where v is the original root variable (note
that v  becomes the root variable of the new transitive reduction). Since v  Vgoal by
assumption, the causal graph contains the edge (v, v  ), and v  is reachable from each other
variable in the transitive reduction via v.
The rest of Section 3 is organized as follows. Subsection 3.1 presents MacroPlanner, an algorithm that solves planning problems in the class IR via decomposition into
subproblems. The subsection includes detailed descriptions of how the subproblems are generated and solved. Subsection 3.2 shows examples of how MacroPlanner solves planning
problems in IR. Subsection 3.3 proves several theoretical properties of MacroPlanner,
and Subsection 3.4 discusses how the algorithm can be improved by pruning the search for
macros.
3.1 Plan Generation for IR
In this section, we present a plan generation algorithm for the class IR called MacroPlanner. For each planning problem P  IR, MacroPlanner generates one or several plans
that solve P in the form of macros. The algorithm uses a divide-and-conquer strategy to
define and solve several subproblems for each variable v  V of the problem. It then stores
the solutions to each subproblem as macros, and incorporates these macros into the action
sets of higher-level subproblems.
Algorithm 1 MacroPlanner(P )
1: G  causal graph of P
2: R  transitive reduction of G
3: v  root variable of R
4: M  GetMacros(v, init, A, R)
5: return M , or fail if M = 

The main routine of MacroPlanner appears in Algorithm 1. MacroPlanner takes
a planning problem P  IR as input and constructs the causal graph G of P , as well
as the transitive reduction R of G. It then identifies the root variable v in the transitive
reduction, and calls GetMacros(v, init, A, R) to solve P . Note that M is a set of macros,
implying that MacroPlanner may return multiple solutions to P . If M does not contain
any solution to P , MacroPlanner returns fail. Also note that v refers to the original
root variable of the transitive reduction (excluding the dummy variable v  ). However, in
what follows we still consider v  a descendant of v.
475

fiJonsson

A

B
t2

t3

C
t4

t1
s

Figure 2: Illustration of the macros generated by MacroPlanner.
3.1.1 Defining Subproblems
In this section we describe the subroutine GetMacros which, for each variable v  V ,
defines several subproblems for v. Intuitively, there are only two reasons to change the
values of v and its ancestors: to satisfy the pre-condition of some action a  A, or to reach
the goal state. The idea is to exhaustively define all subproblems for v that may be needed
to achieve this.
Let Pv = hVv , init , goal , Oi be a subproblem associated with variable v  V . The set
of variables Vv = {v}  Anc(v) consists of v and its ancestors in the causal graph. The set
of actions O contains each action that changes the value of v and each macro representing
the solution to a subproblem for a parent w  P a(v) of v. Since Vv and O are fixed, two
subproblems for v only differ in their initial and goal states init and goal .
To define all subproblems that may be needed to change the values of v and its ancestors, we project onto Vv the pre-condition of each action that changes the value of some
descendant of v. Let Z = {pre(a) | Vv : a  A  Vpost(a)  Desc(v)}  {()} be the set
of such projected pre-conditions. We exclude the empty partial state () from Z, implying
|Vz | > 0 for each z  Z. Note that Z includes the projected goal state goal | Vv since goal
is a pre-condition of the dummy action a that changes the value of v  , a descendant of v.
For each projected pre-condition z  Z, GetMacros defines a subproblem Pv with
initial state init = init | Vv and goal state goal = z. GetMacros calls the subroutine
Solve to generate one or several solutions to Pv , each in the form of a sequence seq =
ha1 , . . . , ak i of actions and macros in O. Let s = init  post(seq) be the state that results
from applying seq in init . Then Solve returns the solution to Pv in the form of a macro
m = hinit , seq, si, where init and s are fully specified states for Vv . For each such macro m
and each z  Z, GetMacros defines a new subproblem Pv with initial state init = post(m)
and goal state goal = z. This process continues until no new subproblems are defined.
The subroutine Solve may generate multiple macros for satisfying the same projected
pre-condition. An intuitive understanding of what macros are generated is provided by
Lemma B.3. The lemma states that for each state u that is reachable from init and each
projected pre-condition z  Z such that u  z, Solve generates a macro hinit , seq, ti such
that t  z and t is on a shortest path from init to u with prefix seq. In other words,
Solve generates only the shortest possible macros for satisfying z. However, it might have
to generate multiple such macros to ensure completeness and optimality.
Figure 2 illustrates the macros that MacroPlanner generates. Let s be the current
state, let z be a projected pre-condition, and let A, B, and C be three sets of states that
satisfy z. If t1 is on a shortest path from s to any state in A, MacroPlanner generates a
macro from s to t1 that represents such a shortest path (the same holds for t4 and C). If no
476

fiThe Role of Macros in Planning

state in B is on the shortest path from s to all other states in B, the algorithm generates
several macros to states in B, in this case to t2 and t3 , such that either t2 or t3 is on a
shortest path from s to any state in B.
Algorithm 2 GetMacros(v, init, A, G)
1: M  
2: L  list containing the projected initial state init | Vv
3: Z  {pre(a) | Vv : a  A  Vpost(a)  Desc(v)}  {()}
4: O  {a  A : v  Vpost(a) }
5: for all w  P a(v) do
6:
O  O  GetMacros(w, init, A, G)
7: end for
8: while there are more elements in L do
9:
s  next element in L
10:
M  M  Solve(v, s, Z, O, G)
11:
for all m = hpre(m), seq(m), post(m)i  M such that post(m) 
/ L do
12:
append post(m) to L
13:
end for
14: end while
15: return M
Algorithm 2 gives pseudo-code for GetMacros. The input of GetMacros is a variable
v  V , an initial state init, an action set A, and a graph G = (V, E) on the variables V .
To obtain the action set O, GetMacros recursively calls itself for each parent w  P a(v)
of v. The list L is initialized with the projected initial state init | Vv . The subroutine
Solve, which is described in the following section, simultaneously solves all subproblems
for v with initial state s and returns the corresponding macros. For each distinct postcondition post(m) of a macro m returned by Solve, GetMacros adds post(m) to the list
L. Consequently, Solve is later called with initial state post(m). Finally, GetMacros
returns all macros generated by Solve.
3.1.2 Solving the Subproblems
In this section we describe the subroutine Solve, which solves the subproblems defined by
GetMacros for a variable v  V . The idea is to construct a graph with the states for
Vv as nodes. This graph can be seen as a joint domain transition graph of the variables in
Vv . For each initial state s and each projected pre-condition z  Z, Solve computes the
shortest path from s to states that match z using a straightforward application of Dijkstras
algorithm. However, the number of states is exponential in the cardinality of Vv , so Solve
constructs the graph implicitly, only adding nodes as needed.
For each state p in the graph, Solve generates two types of successor states. Successor
states of the first type are those that change the value of v. To find such states, Solve
finds each action a  O such that pre(a)  (v = p(v)), i.e., the pre-condition of a is
satisfied with respect to the current value of v. For each parent w  P a(v) such that
(p | Vw ) 6 (pre(a) | Vw ), Solve looks for macros in O from p | Vw to states that satisfy
pre(a) | Vw . For each sequence seq that represents a combination of such macros, Solve
477

fiJonsson

adds a successor state p  post(hseq, ai) of p to the graph. The cost of the corresponding
edge is the length of the sequence hseq, ai.
Successor states of the second type are those that match a projected pre-condition z  Z.
To find such states, Solve finds each z  Z such that z  (v = p(v)). For each parent
w  P a(v) such that (p | Vw ) 6 (z | Vw ), Solve looks for macros in O from p | Vw to states
that match z | Vw . For each sequence seq that represents a combination of such macros,
Solve generates a successor state p  post(seq) of p. However, this successor state is not
added to the graph. Instead, it is only used to generate a solution to the corresponding
subproblem by defining a macro from the initial state s to the state p  post(seq).
Algorithm 3 Solve(v, s, Z, O, G)
1: M  
2: Q  priority queue containing the pair (s, hi)
3: while Q is non-empty do
4:
(p, seq)  remove highest priority state-sequence pair from Q
5:
for all a  O such that pre(a)  (v = p(v)) do
6:
S  Compose(v, p, pre(a), O, G)
7:
for all action sequences seq2  S do
8:
insert (p  post(hseq2, ai), hseq, seq2, ai) into Q
9:
end for
10:
end for
11:
for all z  Z such that z  (v = p(v)) do
12:
S  Compose(v, p, z, O, G)
13:
for all action sequences seq2  S do
14:
M  M  {hs, hseq, seq2i, p  post(seq2)i}
15:
end for
16:
end for
17: end while
18: return M
Algorithm 3 gives pseudo-code for Solve, which takes as input a variable v  V , an
initial state s, a set of projected pre-conditions Z, an action set O, and a graph G = (V, E) on
V . The priority queue Q contains state-sequence pairs, and elements are ordered according
to the total length of the associated sequence. Successor states of the first type are generated
on lines 510, and successor states of the second type are generated on lines 1116. Inserting
a state-sequence pair (p, seq) into Q (line 8) only succeeds if seq is the shortest sequence to
p so far, replacing any previous state-sequence pair involving p. Likewise, inserting a macro
hs, seq, pi into M (line 14) only succeeds if seq is the shortest sequence to p so far, replacing
any previous macro to p.
Algorithm 4 gives pseudo-code for the subroutine Compose called by Solve. The input
of Compose is a variable v  V , a state s, a partial state x, an action set O, and a graph
G = (V, E) on V . Compose generates all sequences from s to states that match x that can
be composed using macros for the parents of v. We use the Cartesian product S  T to
denote the set of sequences obtained by appending a macro in T to a sequence in S. If s
478

fiThe Role of Macros in Planning

Algorithm 4 Compose(v, s, x, O, G)
1: S  {hi}
2: for all w  P a(v) do
3:
if (s | Vw ) 6 (x | Vw ) then
4:
T  {hs | Vw , seq(m), post(m)i  O : post(m)  (x | Vw )}
5:
if T =  then
6:
return 
7:
end if
8:
S ST
9:
end if
10: end for
11: return S
v1

v2

v3

v4

v5

v1

v2

v3

v4

v5

Figure 3: The causal graph and its transitive reduction for P5 .
matches x, Compose returns a set containing the empty sequence hi. If, for some parent
w  P a(v), there is no macro to a state matching x, Compose returns the empty set.
3.2 Examples
We illustrate MacroPlanner on the well-known Tower of Hanoi problem. An instance
of Tower of Hanoi with n discs and 3 pegs can be represented as a planning problem
Pn = hV, init, goal, Ai, where V = {v1 , . . . , vn } are variables representing the discs (v1
being the smallest and vn the largest), each with domain D(vi ) = {A, B, C}. The initial
state is init = (v1 = A, . . . , vn = A) and the goal state is goal = (v1 = C, . . . , vn = C). For
each vi  V and each permutation (j, k, l) of (A, B, C), there is an action for moving disc
vi from peg j to peg k, formally defined as aj,k
i = h(v1 = l, . . . , vi1 = l, vi = j); (vi = k)i.
All discs smaller than vi thus have to be on the third peg l to perform the movement.
Figure 3 shows the causal graph and its transitive reduction for the planning problem
P5 , representing the 5-disc instance of Tower of Hanoi. Each action for moving disc vi
has a pre-condition on each variable vj , j < i. Clearly, the causal graph is acyclic and its
transitive reduction is an inverted tree, implying P5  IR. Since the transitive reduction
is a directed path, the set of parents of variable vi , i > 1, is P a(vi ) = {vi1 }. The root
variable is v5 or, more generally, vn for planning problem Pn .
To solve Tower of Hanoi, MacroPlanner(Pn ) calls GetMacros(vn , init, A, G) to
generate the set M of solution macros. In turn, the subroutine GetMacros(vn , init, A, G)
calls GetMacros(vn1 , init, A, G). This recursion continues until the base case is reached
for GetMacros(v1 , init, A, G). For each variable vi  V , let xdi = (v1 = d, . . . , vi = d),
d  {A, B, C}, be the partial state assigning the same value d to each variable in Vvi .
479

fiJonsson

B,...,B,B

A,...,A,B

A,...,A,C

C,...,C,B

C,...,C,C

B,...,B,C

C,...,C,A

B,...,B,A

A,...,A,A

Figure 4: The implicit graph traversed by Solve for variable vi .
B
C
The set of projected pre-conditions for vi is Z = {xA
i , xi , xi }. Since each projected precondition specifies a value for each ancestor of vi , each macro has to end in either of these
three partial states. It is easy to show by induction that the set of macros M returned
by GetMacros(vi , init, A, G) contains precisely 9 macros, corresponding to each pair of
projected pre-conditions (including macros with an empty action sequence from a projected
pre-condition to itself).
Let us study the behavior of GetMacros(vi , init, A, G). Here, we assume that the 9
macros for vi1 in the set O are those described above. The first partial state added to L on
A
line 2 is the projected initial state xA
i . As a result, GetMacros calls Solve(vi , xi , Z, O, G)
on line 10. In the call to Solve, the first state-sequence pair added to Q on line 2 is (xA
i , hi).
A,B
C
A
The action ai has a pre-condition xi1  (vi = A) that matches (vi = xi (vi )) = (vi = A).
C
As a result, Solve calls Compose(vi , xA
i , xi1  (xi = A), O, G) on line 6.
A
In Compose, there is a single parent of vi , namely vi1 . The projection xA
i | Vvi1 = xi1
C
A
C
is not equal to xi1 . There is a single macro m in the set O from xi1 to xi1 , so the set
S returned by Compose contains a single sequence hmi. As a result, Solve generates a
successor state xC
i1  (vi = B) of the first type, which is added to Q on line 8 together with
i.
its associated sequence hm, aA,B
i

(v
In the iteration for xC
i = B), there is a single projected pre-condition which
i1
B
B
matches (vi = B), namely xi , so Solve calls Compose(vi , xC
i1  (vi = B), xi , O, G) on
B
C
line 12. For the only parent vi1 , the projection xi1 does not match xi | Vvi1 = xB
i1 .
B , so the set S returned by Compose
There is a single macro m in the set O from xC
to
x
i1
i1
contains a single sequence hm i. As a result, Solve generates a successor state xB
i of the
A,B
 i, xB i is added to M . The other eight
,
m
,
hm,
a
second type on line 14, and a macro hxA
i
i
i
macros are added to M in a similar way. In the next section we prove that MacroPlanner
is guaranteed to generate a solution to Tower of Hanoi in polynomial time.
Figure 4 shows the implicit graph traversed by Solve for a variable vi , i > 1, in Tower
B
of Hanoi. The path corresponding to the macro from xA
i to xi in the example is marked
in bold. Successor states of the first type, as well as their incoming edges, are denoted by
solid lines. Successor states of the second type, as well as their incoming edges, are denoted
by dashed lines. Note that the size of the graph is constant and does not depend on i. The

480

fiThe Role of Macros in Planning

only difference is the cost of each edge (omitted in the figure), which equals the length of
the corresponding action sequence in Solve and depends on the length of the macros for
vi1 .
Our second example is a planning domain first suggested by Jonsson and Backstrom
(1998b). Again, we use Pn to denote the instance containing n variables v1 , . . . , vn . Each
variable vi  V has domain D(vi ) = {0, 1}. The initial state is (v1 = 0, . . . , vn = 0) and the
goal state is (v1 = 0, . . . , vn1 = 0, vn = 1). For each variable vi , there are two actions:
ai = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 0); (vi = 1)i,
ai = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 1); (vi = 0)i.
The causal graph and its transitive reduction are identical to Tower of Hanoi, so Figure 3
applies here as well, and it follows that Pn  IR.
Jonsson and Backstrom (1998b) showed that an optimal plan for Pn has length 2n  1.
To solve Pn we need to change the value of vn from 0 to 1 using the action an . Since the
pre-condition of an is (vn1 = 1), while the goal state is (vn1 = 0), we need to insert an1
before an and an1 after an , resulting in the sequence han1 , an , an1 i. Actions an1 and
an1 specify the pre-condition (vn2 = 1), while an and the goal state specify (vn2 = 0),
requiring the value of vn2 to change four times. It is easy to show that the value of variable
vi has to change 2ni times, for a minimum of 2n  1 total actions.
For each variable vi  V , there are two projected pre-conditions in the set Z, namely
(v1 = 0, . . . , vi = 0) and (v1 = 0, . . . , vi1 = 0, vi = 1). Since both specify values for
each ancestor of vi , the set M of macros returned by GetMacros contains precisely four
macros, one for each pair of projected pre-conditions. In spite of the fact that an optimal
solution has exponential length in the number of variables, MacroPlanner is guaranteed
to generate a solution to Pn in polynomial time due to the complexity results in the next
section.
3.3 Theoretical Properties
In this section we prove several theoretical properties of the algorithm MacroPlanner.
For ease of presentation the proofs of several theorems have been moved to the appendix.
The first two theorems are related to the correctness and optimality of MacroPlanner.
Theorem 3.2. For each planning problem P  IR, each macro m  M returned by
MacroPlanner(P ) is well-defined and solves P .
The proof of Theorem 3.2 appears in Appendix A. The proof is an induction on variables
v  V to show that each macro returned by Compose and Solve is well-defined. In
addition, each macro returned by Solve satisfies a projected pre-condition. For the root
variable, Solve is called for the initial state init, and the only projected pre-condition is
goal. The macros returned by MacroPlanner(P ) are precisely those returned by this
call to Solve, and thus solve P .
Theorem 3.3. For each planning problem P  IR, if there exists a plan solving P ,
MacroPlanner(P ) returns an optimal plan for P , else it returns fail.
481

fiJonsson

The proof of Theorem 3.3 appears in Appendix B. The proof is a double induction on
variables v  V and states s for Vv visited during a call to Solve, to show that the macros
returned by Solve represent the shortest solutions to the subproblems corresponding to
this call. Thus some macro returned by Solve for the root variable has to be an optimal
solution for achieving goal starting in init, which is precisely the definition of an optimal
plan for P .
We also prove two complexity results for MacroPlanner. Specifically, we study two
subclasses of the class IR and prove that plan generation is polynomial for these classes,
provided that we are allowed to represent the resulting plan using a hierarchy of macros.
For each variable v  V and each value d  D(v), we define Adv = {a  A : post(a)(v) = d}
as the set of actions that change the value of v to d (implying v  Vpost(a) ). We first prove
a lemma that relates the complexity of MacroPlanner to the number of states visited
during calls to Solve.
Lemma 3.4. Let gv be the number of states visited during the various calls to Solve by
P
GetMacros(v, init, A, G). The complexity of MacroPlanner is O(|A| vV gv3 ).
Proof. Solve is basically a modified version of Dijkstras algorithm, which is quadratic in
the number of nodes of the underlying graph, or O(gv2 ). Each state p is only dequeued from
the priority queue Q once. For an action a  O, the call to Compose on line 6 returns a
set of distinct action sequences, so a generates at most one edge from p to successor states
of the first type. However, two different actions in O may generate two edges from p to
the same state. The same is true for projected pre-conditions in Z and successor states
of the second type. Since there are at most |A| actions in O and at most |A| projected
pre-conditions in Z, the worst-case complexity of a single call to Solve is O(|A|gv2 ).
The subroutine GetMacros calls Solve for each successor state of the second type.
Since there are at most gv such states, the complexity of GetMacros is O(|A|gv3 ), so the
P
overall complexity of MacroPlanner is O(|A| vV gv3 ). Note that the complexity of
Compose is included in this analysis. Compose is called by Solve to generate successor
states of either type, so the number gv of states visited by Solve is proportional to the
total number of sequences returned by Compose.
Theorem 3.5. If Vpre(a) = Vv for each variable v  V  {v  } and each action a  A such
that Vpost(a) = {v}, the complexity of MacroPlanner is O(|V ||A|4 ).
Theorem 3.6. Assume that for each variable v  V , each value d  D(v), and each pair of
actions a, a  Adv , Vpre(a) = Vpre(a ) = {v}  P a(v) and pre(a) | P a(v) = pre(a ) | P a(v). If
pre(a) | P a(v) = init | P a(v) for each action a  A with post(a)(v) = init(v), the algorithm
P
runs in polynomial time with complexity O(|A| vV |D(v)|3 ).
The proofs of Theorems 3.5 and 3.6 appear in Appendix C. The proof of Theorem 3.5 is
based on the observation that if the pre-conditions are fully specified on the ancestors of a
variable v  V , the number of states visited during the various calls to Solve is bounded by
the number of actions. The proof of Theorem 3.6 follows from the fact that the algorithm
always achieves a value d  D(v) in the domain of v  V in the same state, so the number
of states is linear in the size of the variable domains.

482

fiThe Role of Macros in Planning

Two examples of planning problems with the properties established in Theorem 3.5 are
Tower of Hanoi and the domain suggested by Jonsson and Backstrom (1998b), both from
the previous example section.
An example of a planning problem with the properties established in Theorem 3.6 is a
domain proposed by Domshlak and Dinitz (2001). The set of variables is V = {v1 , . . . , vn },
each with domain D(vi ) = {0, 1, 2}, the initial state is init = (v1 = 0, . . . , vn = 0), and the
goal state is goal = (v1 = 2, . . . , vn = 2). For each variable vi  V , there are four actions:
a1i = h(vi1 = 2, vi = 0); (vi = 1)i,
a2i = h(vi1 = 0, vi = 1); (vi = 2)i,
a3i = h(vi1 = 2, vi = 2); (vi = 1)i,
a4i = h(vi1 = 0, vi = 1); (vi = 0)i,
For v1 , the pre-condition on vi1 is dropped. Domshlak and Dinitz (2001) showed that
the length of an optimal plan solving the planning problem is 2n+1  2. However, due to
Theorem 3.6, MacroPlanner generates an optimal solution in polynomial time.
3.4 Pruning
To improve the running time, it is possible to prune some states before they are visited.
Specifically, let (p, seq) be a state-sequence pair visited during a call to Solve(v, s, Z, O, G).
For each state-sequence pair (t, seq2) previously visited during the same call such that
t(v) = p(v), attempt to reach p from t using Compose(v, t, p, O, G). Since p is a fully
specified state, Compose(v, t, p, O, G) returns at most one action sequence. Call it seq3.
If p is reachable from t and hseq2, seq3i is at least as short as seq, there is no need to
visit p, since any state reachable from s via p is also reachable via t on a path of equal or
shorter length. The same reasoning holds for macros to a state p that matches a projected
pre-condition z, under the additional restriction that t also match z.
If it is not possible to reach the projected goal state goal | Vv from s, we can remove all
macros whose pre-condition equals s from the set M of macros in GetMacros(v, init, A, G),
since no plan that solves the original planning problem P could contain such macros. In
addition, we can remove all macros whose post-condition equals s from the set M . If it is
not possible to reach the projected goal state from the projected initial state init | Vv , there
exists no plan that solves the original planning problem P , so we can immediately return
fail without generating the remaining set of macros for other variables.

4. Extending MacroPlanner
In this section, we study ways to extend the algorithm MacroPlanner to a broader class
of planning problems. In Subsection 4.1, we propose a relaxed definition of the causal graph
that, when applied, extends the class IR. We show that MacroPlanner can be modified
to solve planning problems in the new class. In Subsection 4.2, we extend MacroPlanner
to planning problems with acyclic causal graphs. In Subsection 4.3, we show how to combine
the results for different classes to form a more general class of planning problems. Subsection
4.4 illustrates the algorithms for the new classes on two well-known examples: Gripper
and Logistics.
483

fiJonsson

4.1 Relaxed Causal Graph
The conventional definition of the causal graph (V, E) states that there is an edge (w, v)  E
if and only if w 6= v and there exists an action a  A such that w  Vpre(a)  Vpost(a) and
v  Vpost(a) . Consider an action a  A such that Vpost(a) = {w, v}. When applied, a changes
the values of both w and v. Under the conventional definition, a induces a cycle since the
causal graph includes the two edges (w, v) and (v, w).
In this section, we propose a relaxed definition of the causal graph. Under the new
definition, an acyclic causal graph does not necessarily imply that all actions are unary.
Assume that there exists at least one action a  A such that Vpost(a) = {w, v}. Further
assume that there are multiple actions for changing the value of w, whose pre- and postconditions do not specify values for v. Finally, assume that there exists no action for
changing the value of v that does not change the value of w.
Under the given assumption, the only way to satisfy the pre-condition of an action that
changes the value of v is to first change the value of w. This is not changed by the fact
that such an action might also change the value of w. The opposite is not true of actions
that change the value of w, since the value of w can change independently of v. Under the
relaxed definition, the causal graph includes the edge (w, v) but excludes the edge (v, w).
4.1.1 Definition of the Class RIR
Definition 4.1. The relaxed causal graph (V, E  ) of a planning problem P is a directed
graph with the variables in V as nodes. There is an edge (w, v)  E  if and only if w 6= v
and either
1. there exists a  A such that w  Vpre(a)  Vpost(a) and v  Vpost(a) , or
2. there exists a  A such that w, v  Vpost(a) and either
(a) there exists a  A such that w  Vpost(a ) and v 
/ Vpost(a ) , or
(b) there does not exist a  A such that w 
/ Vpost(a ) and v  Vpost(a ) .
Edges of type 2(b) ensure that there always is at least one edge between two variables
w, v  V that appear in the same post-condition of an action a  A. If it is not possible
to change the value of either without changing the value of the other, the relaxed causal
graph contains a cycle just as before. We can now define the class RIR of relaxed inverted
tree reducible planning problems:
Definition 4.2. A planning problem P is relaxed inverted tree reducible if and only if its
relaxed causal graph is acyclic and the transitive reduction of the relaxed causal graph is an
inverted tree.
Consider a planning problem P  IR. Since each action is unary, the relaxed causal
graph contains no edges of type 2. Consequently, the relaxed causal graph is identical
to the conventional causal graph, so any tree-reducible planning problem is also relaxed
tree-reducible, implying IR  RIR.
As an example, consider a planning problem with just two variables v and w. Let
D(v) = {0, 1, 2, 3}, D(w) = {0, 1}, init = (v = 0, w = 0), and goal = (v = 3, w = 1).
Assume that there are three actions:
484

fiThe Role of Macros in Planning

h(v = 0); (v = 1)i,
h(v = 2); (v = 3)i,
h(v = 1, w = 0); (v = 2, w = 1)i.
The third action is not unary, since it changes the value of both v and w. There are actions
for changing the value of v that do not affect w, while the opposite is not true. Hence the
relaxed causal graph contains a single edge (v, w).
4.1.2 Algorithm
We show that a minor modification of MacroPlanner is sufficient to solve planning
problems in the class RIR. Since actions are no longer unary, an action a  A that
changes the value of a variable v  V may also change the values of other variables. When
generating macros for v, we only consider the set A of actions that change the value of v
without changing the value of any descendant of v in the relaxed causal graph. Formally,
A = {a  A : v  Vpost(a)  |Vpost(a)  Desc(v)| = 0}.
Just as before, we project the pre-conditions of actions that change the value of some
descendant of v onto Vv in order to define subproblems Pv for v. Let A = {a  A :
|Vpost(a)  Desc(v)| > 0  (pre(a) | Vv ) 6= ()} be the set of such actions whose projected
pre-condition onto Vv is non-empty. Let a  A be such an action, and let m be a macro
for v that achieves the pre-condition of a, i.e., post(m)  pre(a).
The only reason for applying m (unless it also satisfies the projected pre-condition of
some other action) is to satisfy the pre-condition of a. If a changes the value of v, we
are no longer in the projected state post(m) as a result of applying a. In other words, we
should not generate macros for v starting in post(m) as before. Rather, we should generate
macros from post(m)  (post(a) | Vv ), i.e., the projected state that results from applying a
in post(m).
Algorithm 5 RelaxedPlanner(P )
1: G  relaxed causal graph of P
2: R  transitive reduction of G
3: v  root variable of R
4: M  RelaxedMacros(v, init, A, R)
5: return M , or fail if M = 
Algorithm 5 shows the modified version of MacroPlanner for the class RIR, which
we call RelaxedPlanner. The only difference with respect to MacroPlanner is that
G now refers to the relaxed causal graph. Algorithm 6 shows the modified version of
GetMacros for the class RIR, which we call RelaxedMacros. The modifications with
respect to GetMacros appear on lines 35 and 1217. The subroutine Solve remains the
same as before.
4.1.3 Theoretical Properties
Theorem 4.3. For each planning problem P  RIR, each macro m  M returned by
RelaxedPlanner(P ) is well-defined and solves P .
485

fiJonsson

Algorithm 6 RelaxedMacros(v, init, A, G)
1: M  
2: L  list containing the projected initial state init | Vv
3: A  {a  A : |Vpost(a)  Desc(v)| > 0  (pre(a) | Vv ) 6= ()}
4: Z  {pre(a) | Vv : a  A }
5: O  {a  A : v  Vpost(a)  |Vpost(a)  Desc(v)| = 0}
6: for all u  P a(v) do
7:
O  O  RelaxedMacros(u, init, A, G)
8: end for
9: while there are more elements in L do
10:
s  next element in L
11:
M  M  Solve(v, s, Z, O, G)
12:
for all m = hpre(m), seq(m), post(m)i  M do
13:
for all a  A  {a } such that post(m)  pre(a) do
14:
if post(m)  (post(a) | Vv ) 
/ L then
15:
append post(m)  (post(a) | Vv ) to L
16:
end if
17:
end for
18:
end for
19: end while
20: return M
v2
v1
v3

Figure 5: Transitive reduction graph with outdegree > 1.
Theorem 4.4. For each planning problem P  RIR, if there exists a plan solving P ,
RelaxedPlanner returns an optimal plan for P , else it returns fail.
The proofs of Theorems 4.3 and 4.4 are simple adaptations of the corresponding proofs
for MacroPlanner, and appear in Appendix D.
4.2 Acyclic Causal Graph
In this section we present a second extension of MacroPlanner, this time to the class
of planning problems with acyclic causal graphs. In other words, variables in the causal
graph may have unbounded outdegree, even when considering the transitive reduction.
Unbounded outdegree poses a challenge to the macro compilation approach, as illustrated
by the following example.
Consider a planning problem P with V = {v1 , v2 , v3 }, D(v1 ) = {0, 1, 2}, and D(v2 ) =
D(v3 ) = {0, 1}. The initial state is (v1 = 0, v2 = 0, v3 = 0), the goal state is (v2 = 1, v3 = 1),
486

fiThe Role of Macros in Planning

and A contains the following actions:
a11 = hv1 = 0; v1 = 1i,
a21 = hv1 = 0; v1 = 2i,
a12 = hv1 = 1, v2 = 0; v2 = 1i,
a13 = hv1 = 2, v3 = 0; v3 = 1i.
Figure 5 shows the causal graph of P , identical to its transitive reduction in this case. Since
variable v1 has outdegree 2, it follows that P 
/ IR (implying P 
/ RIR since actions are
unary). From the point of view of variable v2 , it is possible to set v2 to 1: apply a11 to
change v1 from 0 to 1, followed by a12 to change v2 from 0 to 1. From the point of view of
v3 , it is possible to set v3 to 1 by applying a21 followed by a13 . Nevertheless, there is no valid
plan for solving the planning problem.
The algorithm MacroPlanner solves planning problems by decomposing them into
subproblems for each variable. We have seen that this is efficient for certain subclasses of
IR, and that it guarantees optimality. However, for variables with unbounded outdegree
it is no longer possible to define subproblems for each state variable in isolation. If we
run MacroPlanner on the example planning problem above, it will generate a macro for
setting v2 to 1, and a macro for setting v3 to 1. This might lead us to believe that P has a
solution, while in fact it does not.
4.2.1 Definition of the Class AR
To extend MacroPlanner to planning problems with acyclic causal graphs, we impose an
additional restriction on planning problems. For each state variable v  V with outdegree
greater than 1 in the causal graph, we require v to be reversible.
Definition 4.5. A state variable v  V is reversible if and only if, for each state s for Vv
that is reachable from the projected initial state init | Vv , the projected initial state init | Vv
is reachable from s.
Definition 4.6. A planning problem P belongs to the class AR if the causal graph of P is
acyclic and each variable is reversible.
4.2.2 Algorithm
In this section we show how to modify the algorithm MacroPlanner so that it solves planning problems in AR. We first describe an algorithm called ReversiblePlanner which
requires all variables to be reversible. ReversiblePlanner, which appears in Algorithm 7,
operates directly on the causal graph of the planning problem P . Thus, it does not compute
the transitive reduction of the causal graph. Another feature is that the subroutine GetMacros is omitted. Instead, ReversiblePlanner directly calls ReversibleCompose,
the equivalent of the subroutine Compose.
Algorithm 8 describes the subroutine ReversibleCompose. While Compose returns
a set of sequences for reaching a partial state x, ReversibleCompose only returns a
pair of sequences. Here, U is the set of variables that appear in the post-condition of
the action whose pre-condition x we wish to satisfy. The first sequence seq satisfies x
487

fiJonsson

Algorithm 7 ReversiblePlanner(P )
1: G  [relaxed] causal graph of P
2: (seq, seq2)  ReversibleCompose(, init, goal, A, G)
3: return seq
Algorithm 8 ReversibleCompose(U, s, x, A, G)
1: seq  hi, seq2  hi
2: for all w  Vx in topological order do
3:
if x(w) 6= s(w) then
4:
m  ReversibleSolve(w, s, x(w), A, G)
5:
if m = fail then
6:
return (fail,fail)
7:
else
8:
seq  hm, seqi
9:
if w 
/ U then

10:
m  ReversibleSolve(w, s  (w = x(w)), s(w), A, G)
11:
seq2  hseq2, m i
12:
end if
13:
end if
14:
end if
15: end for
16: return (seq, seq2)

starting in the state s. If this fails, ReversibleCompose returns the pair of sequences
(fail,fail). Otherwise, the second sequence seq2 returns to s from the post-condition of
the first sequence when applied in s. An exception occurs for variables in U  V , which
are not returned to their values in s.
ReversibleCompose requires a topological sort of the variables of the causal graph G,
which can be obtained in polynomial time for acyclic graphs. An example of a topological
sort is v1 , v2 , v3 for the causal graph in Figure 5. The values of x are satisfied in reverse
topological order because of the way the sequence seq is constructed, i.e., each new macro
m is inserted first. ReversibleCompose calls ReversibleSolve to obtain a macro for
changing the value of a variable w  Vx from s(w) to x(w). If this succeeds and w 
/
U , ReversibleCompose calls ReversibleSolve a second time to obtain a macro for
resetting the value of w from x(w) to s(w). This macro is appended to the sequence seq2,
which resets the values of variables in Vx  U in topological order.
Algorithm 9 describes ReversibleSolve, the equivalent of the subroutine Solve. Unlike Solve, ReversibleSolve only generates a single macro for changing the value of
v from s(v) to d. Moreover, ReversibleSolve does not invoke Dijkstras algorithm; instead, it performs a breadth-first search over the values in the joint domain of W . A variable
w  Vv belongs to W if there exists an action for changing the value of v or one of its descendants with w in its post-condition. Note that if actions are unary, W = {v}, so the
breadth-first search is over values in the domain D(v) of v.
488

fiThe Role of Macros in Planning

Algorithm 9 ReversibleSolve(v, s, d, A, G)
1: W  {w  Vv : a  A such that w  Vpost(a) and Vpost(a)  (Desc(v)  {v}) 6= }
2: A  {a  A : v  Vpost(a) [Desc(v)  Vpost(a) = ] }
3: L  list containing the state-sequence pair (s | W, hi)
4: while L contains more elements do
5:
(p, seq)  next element in L
6:
if p = (s | W )  (v = d) then
7:
return h(s | Vv , seq, (s | Vv )  (v = d)i
8:
end if
9:
for all a  A such that pre(a)  (v = p(v)) do
10:
(seq2, seq3)  ReversibleCompose(Vpost(a) , s  p, pre(a), A, G)
11:
if seq2 6= fail and 6 (p , seq  )  L such that p = p  post(a)(v) then
12:
insert (p  post(a)(v), hseq, seq2, a, seq3i) into L
13:
end if
14:
end for
15: end while
16: return fail
Each partial state p visited during search is associated with a sequence seq used to arrive
at p. If p = (s | W )  (v = d), ReversibleSolve returns a macro corresponding to the
given sequence. Note that this macro only changes the value of v, leaving the values of all
other variables unchanged. Otherwise, ReversibleSolve tries all actions for changing the
value of v whose pre-condition is compatible with (v = p(v)). To satisfy the pre-condition
of an action a from the state s  p, ReversibleSolve calls ReversibleCompose. If
ReversibleCompose returns a legal pair of sequences (seq2, seq3) and the partial state
p  post(v)(a) has not been previously visited, ReversibleSolve generates a new statesequence pair (ppost(v)(a), hseq, seq2, a, seq3i). Since U = Vpost(a) , seq3 resets all variables
to their values in s  p except those whose values are changed by a.
Since an equivalent of GetMacros is omitted, ReversibleMacros is goal-driven in
the sense that ReversibleSolve is only called whenever needed by the subroutine ReversibleCompose. We use memoization to cache the results of ReversibleCompose
and ReversibleSolve to avoid recomputing the same result more than once. The expressions within square brackets indicate the changes that are necessary for the algorithm
to work on planning problems with acyclic relaxed causal graphs. Note, however, that
ReversiblePlanner is incomplete for planning problems in AR with non-unary actions.
4.2.3 Theoretical Properties
Theorem 4.7. For each planning problem P  AR, if the sequence seq returned by
ReversiblePlanner(P ) is different from fail, seq is well-defined for init and satisfies (init  post(seq))  goal.
Theorem 4.8. For each planning problem P  AR with unary actions, if P is solvable,
ReversiblePlanner(P ) returns a sequence seq solving P , else it returns fail.

489

fiJonsson

Theorem 4.9. For each planning problem P  AR, the complexity of ReversiblePlanner is O(Dk |A|(Dk+1 + |V |)), where D = maxvV D(v) and k = maxvV |W |.
The proofs of Theorems 4.7, 4.8, and 4.9 appear in Appendix E. The proofs of Theorems
4.7 and 4.8 are similar to those for MacroPlanner, and show by induction on variables
v  V that the macros returned by ReversibleCompose and ReversibleSolve are
well-defined. However, instead of optimal plans, ReversiblePlanner returns a plan if
and only if one exists.
The proof of Theorem 4.9 is based on the fact that in each call to ReversibleSolve,
state variables not in Vv  W always take on their initial values. This bounds the total
number of states in calls to ReversibleSolve. Note that Theorem 4.9 implies that the
complexity of ReversiblePlanner is O(D|A|(D2 + |V |)) for planning problems in AR
with unary actions, since |W | = 1 for each v  V in this case. In general, if the size of the
sets W are fixed, the complexity of ReversiblePlanner is polynomial. Also note that
Tower of Hanoi belongs to AR since each variable is reversible, so planning problems in
AR may have exponentially long solutions.
4.3 The Class AOR
We can now combine the two results for the classes IR and AR. The idea is to use
the original algorithm to exhaustively generate macros for each variable v, as long as the
outdegree of v in the transitive reduction of the causal graph is less than or equal to 1.
Whenever we encounter a variable v with outdegree larger than 1, we switch to the algorithm
for reversible variables from the previous section. This way, we can handle acyclic causal
graphs even when some variables are not reversible. We call the resulting class AOR.
4.3.1 Definition of the Class AOR
Definition 4.10. A planning problem P belongs to the class AOR if the causal graph of
P is acyclic and each variable v  V with outdegree > 1 in the transitive reduction of the
causal graph is reversible.
The example planning problem from the previous section does not belong to the class
AOR, since variable v1 with outdegree 2 is not reversible. However, assume that we add
the following two actions to A:
a31 = hv1 = 1; v1 = 0i,
a41 = hv1 = 2; v1 = 0i.
With the new actions, variable v1 is reversible since each value of v1 is reachable from
each other value. Since v1 is the only state variable with outdegree greater than 1 in the
transitive reduction, P  AOR.
4.3.2 Algorithm
In this section, we describe AcyclicPlanner, which combines ideas from MacroPlanner and ReversiblePlanner to solve planning problems in the class AOR. AcyclicPlanner appears in Algorithm 10. Just like MacroPlanner, AcyclicPlanner operates
490

fiThe Role of Macros in Planning

Algorithm 10 AcyclicPlanner(P )
1: G  [relaxed] causal graph of P
2: R  transitive reduction of G
3: (S, seq, seq2)  AcyclicCompose(v  , init, goal, A, G, R)
4: if S =  then
5:
return fail
6: else
7:
return S  seq, or hseq  , seqi for some seq   S
8: end if
on the transitive reduction R of the causal graph. However, since ReversiblePlanner
operates on the causal graph G itself, AcyclicPlanner passes on both G and R to the
subroutine AcyclicCompose.
Algorithm 11 AcyclicCompose(v, s, x, A, G, R)
1: S  {hi}
2: x  x projected onto non-reversible variables
3: for all w  P a(v) do
4:
if outdegree(R, w)  1 and (s | Vw ) 6 (x | Vw ) then
5:
M  AcyclicSolve(w, s | Vw , A, G, R)
6:
T  {hs | Vw , seq(m), post(m)i  M : post(m)  (x | Vw )}
7:
if T =  then
8:
return (,fail,fail)
9:
end if
10:
S ST
11:
end if
12: end for
13: seq  hi, seq2  hi
14: for all w  Vx in reverse topological order do
15:
if outdegree(R, w) > 1 and (s | Vw ) 6 (x | Vw ) then
16:
(seq3, seq4)  ReversibleCompose(, s | Vw , x, A, G).
17:
if seq3 = fail then
18:
return (,fail,fail)
19:
else
20:
seq  hseq, seq3i
21:
seq2  hseq4, seq2i
22:
end if
23:
end if
24: end for
25: return (S, seq, seq2)
As can be seen, AcyclicPlanner calls AcyclicCompose with the dummy variable
Note that, contrary to before, the transitive reduction of the causal graph may contain
several edges to v  . The reason is that there is not a single root variable of the causal graph.

v.

491

fiJonsson

Instead, there may be multiple sink variables in the causal graph, i.e., variables with no
outgoing edges. The transitive reduction contains an edge from each sink variable in Vgoal
to v  .
Algorithm 11 describes the subroutine AcyclicCompose. Although it looks somewhat
complex, lines 3 through 12 are really just an adaptation of Compose to the class AOR.
The only difference is that AcyclicSolve does not generate macros for satisfying the
projected pre-condition x; instead, it generates macros for satisfying the projection x of x
onto non-reversible variables. The idea is for reversible variables to always remain in their
initial values. This way, we can treat the parents w  P a(v) of v as if they were independent,
since any common ancestors have to have outdegree larger than 1 in the transitive reduction,
and are thus reversible by definition.
Lines 13 through 24 have the effect of satisfying the values of x for variables in Vx whose
outdegree in the transtitive reduction is larger than 1. This is done simply by calling the
subroutine ReversibleCompose from the previous section. AcyclicCompose returns
three values: a set S of sequences for satisfying the partial state x with respect to nonreversible variables, a sequence seq for satisfying the partial state x with respect to reversible
variables, and a sequence seq2 for resetting the reversible variables to their initial values.
Note that once ReversibleCompose is called for a variable w, each predecessor of w
is processed using ReversibleSolve and ReversibleCompose as well. For this to work,
the predecessors of w have to be reversible, which is guaranteed by the following lemma:
Lemma 4.11. If v is reversible, so is every predecessor of v.
Proof. By contrapositive. Assume that w is a non-reversible predecessor of v. Then there
exists a state s for Vw that is reachable from the projected initial state init | Vw such that
init | Vw is not reachable from s. Since w is a predecessor of v, Vw is a subset of Vv , so
the state (init | Vv )  s is reachable from init | Vv , but not vice versa. Thus v is not
reversible.
Algorithm 12 describes the subroutine AcyclicSolve. The only difference between
Solve and AcyclicSolve is that AcyclicSolve calls AcyclicCompose to obtain sequences for satisfying a projected pre-condition. Moreover, whenever satisfying the precondition pre(a) of an action, AcyclicCompose applies the sequence seq3 to reset reversible variables to their initial values. In addition, AcyclicCompose does not satisfy
a projected pre-condition z for reversible variables, since it does not append the sequence
seq2 to the resulting sequence. Just as before, the expressions within square brackets are
those that need to be added to deal with acyclic relaxed causal graphs.
4.3.3 Theoretical Properties
Theorem 4.12. For each planning problem P  AOR with unary actions, if there exists a
plan solving P , each sequence seq in the set S returned by AcyclicPlanner is well-defined
for init and satisfies (init  post(seq))  goal.
Proof. To prove Theorem 4.12, it is sufficient to combine results for MacroPlanner and
ReversiblePlanner. The subroutines AcyclicCompose and AcyclicSolve are identical to Compose and Solve for variables with outdegree less than or equal to 1, so Lemmas
492

fiThe Role of Macros in Planning

Algorithm 12 AcyclicSolve(v, s, A, G, R)
1: M  
2: A  {a  A : v  Vpost(a) [Desc(v)  Vpost(a) = ] }
3: Z  {pre(a) | Vv : a  A  |Vpost(a)  Desc(v)| > 0}  {()}
4: Q  priority queue containing the state-sequence pair (s, hi)
5: while Q is non-empty do
6:
(p, seq)  remove highest priority state-sequence pair from Q
7:
for all a  A such that pre(a)  (v = p(v)) do
8:
(S, seq2, seq3)  AcyclicCompose(v, p, pre(a), A, G, R)
9:
for all sequences seq4  S do
10:
insert (p  post(hseq4, seq2, a, seq3i), hseq, seq4, seq2, a, seq3i) into Q
11:
end for
12:
end for
13:
for all z  Z such that z  (v = p(v)) do
14:
(S, seq2, seq3)  AcyclicCompose(v, p, z, P, G, R)
15:
for all sequences seq4  S do
16:
M  M  {hs, hseq, seq4i, p  post(seq4)i}
17:
end for
18:
end for
19: end while
20: return M
A.1 and A.2 imply that the resulting sequences and macros have the desired properties. For
variables with outdegree larger than 1, ReversibleCompose returns sequences with the
desired properties due to Lemma E.1.
Theorem 4.13. Assume that there exists a constant k such that for each non-reversible
variable v  V , the number of non-reversible ancestors of v is less than or equal to k.
In addition, for each reversible variable v  V , |W |  k, where W is the set defined in
ReversibleSolve. Then AcyclicPlanner runs in polynomial time.
Proof. Again, we can combine results for MacroPlanner and ReversiblePlanner.
First note that any call to ReversibleCompose is polynomial due to Theorem 4.9. Lemma
P
3.4 states that the complexity of MacroPlanner is O(|A| vV gv3 ), where gv is the number of states visited during calls to Solve for v. Since AcyclicCompose and AcyclicSolve are identical to Compose and Solve for non-reversible variables, the argument in
the proof of Lemma 3.4 holds for AcyclicSolve as well. The fact that v has at most k
non-reversible predecessors implies gv = O(Dk+1 ), since reversible predecessors always take
on their initial values.
Note that the argument in Lemma 3.4 does not take into account the external call
to AcyclicCompose by AcyclicPlanner, which may be exponential in the number
|P a(v  )| of parents of v  . A slight modification of the algorithm is necessary to prove
the theorem. We modify the external call so that it only returns a single sequence. In
other words, for each w  P a(v  ), we only keep one of the macros returned by the call to
AcyclicSolve for w. Note that this does not alter the admissibility of the solution.
493

fiJonsson

(a)

(b)

(c)

v1
vl

vh

v1
vl

vh

v1
vl

v2

v2

vh

v*
v2

Figure 6: (a) Conventional and (b) relaxed causal graph, and (c) transitive reduction of the
relaxed causal graph for Gripper.

4.4 Examples
In this section, we provide examples of planning problems in AR and AOR, and describe
how ReversiblePlanner and AcyclicPlanner solve them.
4.4.1 Gripper
Our first example is the well-known Gripper domain, in which a robot has to transport balls
between two rooms. In Gripper, a planning problem Pn is defined by the number n of balls
that the robot has to transport. The set of variables is V = {vl , vh , v1 , . . . , vn } with domains
D(vl ) = {0, 1}, D(vh ) = {0, 1, 2}, and D(vi ) = {0, 1, R} for each 1  i  n. The initial state
is init = (vl = 0, vh = 0, v1 = 0 . . . , vn = 0) and the goal state is goal = (v1 = 1, . . . , vn = 1).
The two rooms are denoted 0 and 1. Variable vl represents the location of the robot, i.e.,
either of the two rooms. Variable vh represents the number of balls currently held by the
robot, for a maximum of 2. Finally, variable vi , 1  i  n, represents the location of ball
i, i.e., either of the two rooms or held by the robot (R). The set A contains the following
actions:
h(vl = a); (vl = 1  a)i,
h(vl = a, vh = b, vi = a); (vh = b + 1, vi = R)i,
h(vl = a, vh = b + 1, vi = R); (vh = b, vi = a)i,

a  {0, 1},
a, b  {0, 1}, 1  i  n,
a, b  {0, 1}, 1  i  n.

Actions of the first type move the robot between rooms. Actions of the second type cause
the robot to pick up a ball at its current location, incrementing the number of balls held.
Actions of the third type cause the robot to drop a ball at its current location, decrementing
the number of balls held.
For each 1  i  n, there exist actions for changing the value of vi that also change
the value of vh . Consequently, the conventional causal graph contains the edges (vh , vi ) and
(vi , vh ), inducing cycles. However, there are no actions for changing the value of vi that do
not also change the value of vh . The opposite is not true; there are actions for vh that do
not affect the value of vi (assuming n > 1). Thus, the relaxed causal graph contains the
edge (vh , vi ), but not (vi , vh ). Figure 6 shows the conventional and relaxed causal graphs,
as well as the transitive reduction of the relaxed causal graph, for Gripper with n = 2.
The transitive reduction includes the dummy variable v  as well as its incoming edges.
494

fiThe Role of Macros in Planning

The relaxed causal graph is acyclic, and each variable is reversible since we can return to the initial state from any other state. Thus Gripper belongs to the class AR
when considering the relaxed causal graph. Since actions in Gripper are non-unary, ReversiblePlanner is not guaranteed to find a solution. However, we show that it does in
fact find a solution to Gripper.
ReversiblePlanner calls ReversibleCompose with the initial and goal states. In
turn, ReversibleCompose goes through each variable w  Vgoal and calls ReversibleSolve to obtain a macro for setting the value of w to its goal value. Since there are no
directed paths between any variables representing balls, a topological sort orders these variables arbitrarily. Without loss of generality, assume that the topological order is v1 , . . . , vn .
Thus the first variable processed by ReversibleCompose is v1 . ReversibleCompose
calls ReversibleSolve with v = v1 , s = init and d = 1.
For v1 , the set W = {vh , v1 } contains variables vh and v1 . The set A contains all
actions of the second and third type for picking up and dropping ball 1. The list L initially
contains the partial state (vh = 0, v1 = 0). There are two actions whose pre-conditions
match (v1 = 0): one for picking up ball 1 from room 0 assuming no ball is currently held
(vh = 0), and one for picking up ball 1 assuming one ball is currently held (vh = 1). For
each of these, ReversibleSolve calls ReversibleCompose with U = Vpost(a) , s = init,
and x = pre(a) to obtain a sequence for satisfying the pre-condition of the action.
The pre-condition of the first action is already satisfied in init, so ReversibleCompose
returns a pair of empty sequences. As a result, ReversibleSolve adds the partial state
(vh = 1, v1 = R) to L, corresponding to the post-condition of this action. Note that the
set A is empty for variable vh , since there are no actions for changing the value of vh
that do not also change a value of its descendants. Thus it is not possible to satisfy the
pre-condition (vh = 1) of the second action starting in the initial state without changing
the value of v1 . Consequently, ReversibleCompose returns (fail,fail), and no new
partial state is added to L.
From (vh = 1, v1 = R), there are four actions whose pre-conditions match (v1 = R):
two for dropping ball 1 in room 0, and two for dropping ball 1 in room 1. Two of these
have pre-condition (vh = 2) which is impossible to satisfy without changing the value of
v1 . Dropping ball 1 in room 0 returns to the initial state, so no new partial states are
added to L. Dropping ball 1 in room 1 requires a pre-condition (vl = 1, vh = 1). Since
(vh = 1) already holds, ReversibleCompose calls ReversibleSolve to change the value
of vl from 0 to 1. This is possible using the action for changing the location of the robot.
ReversibleCompose also returns a sequence for returning the robot to room 0.
As a result, ReversibleSolve adds the partial state (vh = 0, v1 = 1) to L, corresponding to the post-condition of this action. Note that this partial state satisfies
(vh = 0, v1 = 1) = (vh = 0, v1 = 0)  (v1 = 1), so ReversibleSolve returns a macro
that changes the value of v1 without changing the values of any other variables. Since ReversibleSolve successfully returned a macro, ReversibleCompose calls ReversibleSolve again to obtain a macro for returning ball 1 to its initial location. This continues
for the other balls. The final solution contains macros for changing the location of each ball
from room 0 to room 1 one at a time.
Note that the complexity of ReversibleSolve is polynomial for Gripper due to Theorem 4.9, since |W |  2 for each vi , 1  i  n. Also note that the solution is not
495

fiJonsson

t1

tn

p1
v*

u1

pm

uq

Figure 7: Causal graph and transitive reduction for Logistics.
optimal, since the robot could carry two balls at once. We could also solve Gripper using
AcyclicPlanner, which would treat variables vi , 1  i  n, as non-reversible since they
have outdegree 0 in the transitive reduction of the relaxed causal graph. The solution is in
fact the same in this case, since there is just a single way to move a ball from room 0 to
room 1.
4.4.2 Logistics
Our second example is the Logistics domain, in which a number of packages have to
be moved to their final location using trucks and airplanes. Let m denote the number of
packages, n the number of trucks, and q the number of airplanes. A multi-valued variable
formulation of a planning problem in Logistics is given by V = {p1 , . . . , pm }  T , where
T = {t1 , . . . , tn , u1 , . . . , uq }, pi , 1  i  m, are packages, tj , 1  j  n, are trucks, and uk ,
1  k  q, are airplanes.
Let L be a set of possible locations of packages. There is a partition L1  . . .  Lr of
L such that each Ll , 1  l  r, corresponds to locations in the same city. There is also a
subset C of L corresponding to airports, with |C  Ll |  1 for each 1  l  r. For each
pi , the domain D(pi ) = L  T corresponds to all possible locations plus inside all trucks
and airplanes. For each tj , D(tj ) = Ll for some 1  l  r. For each uk , D(uk ) = C. The
initial state is an assignment of values to all variables, and the goal state is an assignment to
the packages pi among the values L. The set A contains the following three types of actions:
h(t = l1 ); (t = l2 )i,
h(t = l, pi = l); (pi = t)i,
h(t = l, pi = t); (pi = l)i,

t  T , l1 6= l2  D(t),
t  T , l  D(t), 1  i  m,
t  T , l  D(t), 1  i  m.

The first type of action allows a truck or airplane to move between two locations in its
domain. The second type of action loads a package in a truck or airplane at the same
location. The third type of action unloads a package from a truck or airplane.
Figure 7 shows the causal graph and its transitive reduction for a planning problem
in Logistics. The transitive reduction is identical to the causal graph in this case. Just
as for Gripper, the figure shows the dummy variable v  as well as its incoming edges.
It is easy to see that the causal graph is acyclic. In addition, all variables are reversible
496

fiThe Role of Macros in Planning

Discs

Time (ms)

Macros

Length

10
20
30
40
50
60

31
84
161
279
457
701

27/82
57/172
87/262
117/352
147/442
177/532

> 103
> 106
> 109
> 1012
> 1015
> 1018

Table 1: Results in Tower of Hanoi.

since it is possible to return to the initial state from any other state. Thus we could apply
ReversiblePlanner to solve planning problems in Logistics.
To solve a planning problem in Logistics, ReversiblePlanner calls ReversibleCompose with the initial and goal states. Since there are no directed paths between any
variables representing the location of packages, a topological sort orders these variables arbitrarily. Without loss of generality, assume that the topological order is p1 , . . . , pn . Thus
the first variable processed by ReversibleCompose is p1 . ReversibleCompose calls
ReversibleSolve with v = p1 , s = init and d equal to the goal location of the package.
For p1 , the set W = {p1 } just contains p1 since actions are unary. The set A contains any
actions for picking up or dropping the package. Although there may be many ways to move
a package from its initial location to its goal location, ReversibleSolve just considers
once such way of moving the package. After moving the package, all trucks and airplanes
are returned to their initial locations. ReversibleCompose also calls ReversibleSolve
to generate a macro for returning the package to its initial location. The final solution
consists of macros for moving each package to its final location one at a time.
Note that the complexity of ReversibleSolve is polynomial for Logistics due to
Theorem 4.9, since |W |  1 for each pi , 1  i  m. Also note that the solution is not
optimal, since trucks and airplanes are always returned to their initial locations. We could
also solve Logistics using AcyclicPlanner, which would treat variables pi , 1  i  m,
as non-reversible since they have outdegree 0 in the transitive reduction of the relaxed causal
graph. Again, the solution is the same since each ancestor of pi is reversible, so the states
in AcyclicSolve always have trucks and airplanes in their initial locations.

5. Experimental Results
To test our algorithm, we ran experiments in two domains: Tower of Hanoi and an extended
version of Gripper. The results of the experiments in Tower of Hanoi appear in Table
1. We varied the number of discs in increments of 10 and recorded the running time of
MacroPlanner. The table also shows the number of macros generated by our algorithm,
and out of those, how many were used to represent the resulting global plan. For example,
27 out of 82 generated macros formed part of the solution to Tower of Hanoi with 10 discs.
For the second set of experiments, we modified the version of Gripper from the previous section. Instead of two rooms, the environment consists of a maze with 967 rooms. To
transport balls, the robot must navigate through the maze from the initial location to the
497

fiJonsson

Balls

Time (ms)

Macros

Length

100
101
102
103

1528
1535
1570
2906

3
14
104
1004

 300  100
 300  101
 300  102
 300  103

Table 2: Results in Gripper.

goal location. The robot can only pick up and drop balls at the initial and goal locations.
The results of running ReversiblePlanner on the corresponding planning problems appear in Table 2. Since ReversiblePlanner only generates macros as necessary, all of
them are used in the solution. The experiments illustrate the benefit of using macros to
store the solution for navigating through the maze. Any algorithm that does not use macros
has to recompute a path through the maze every time it goes to pick up a new ball.

6. Related Work
For ease of presentation, we group related work into three broad categories: complexity
results, macro-based planning, and factored planning. Each of the following subsections
presents related work in one of these categories.
6.1 Complexity Results
Early complexity results for planning focused on establishing the hardness of different formulations of the general planning problem. If the STRIPS formalism is used, deciding whether
a solution exists is undecidable in the first-order case (Chapman, 1987) and PSPACEcomplete in the propositional case (Bylander, 1994). For PDDL, the representation language used at the International Planning Competition, the fragment including delete lists
is EXPSPACE-complete (Erol, Nau, & Subrahmanian, 1995).
Recently, several researchers have studied tractable subclasses of planning problems that
can be provably solved in polynomial time. Most of these subclasses are based on the notion
of a causal graph (Knoblock, 1994), which models the degree of independency between the
problem variables. However, Chen and Gimenez (2008a) showed that any connected causal
graph causes the problem to be hard (unless established assumptions such as P = NP fail),
so additional restrictions on the problem are necessary.
A common restriction is that the variables of the problem are binary. Jonsson and
Backstrom (1998a) defined the class 3S of planning problems with acyclic causal graphs
and binary variables. In addition, variables are either static, symmetrically reversible, or
splitting. The authors showed that it is possible to determine in polynomial time whether
or not a solution exists, although solution plans may be exponentially long.
Gimenez and Jonsson (2008) designed a macro-based algorithm that solve planning
problems in 3S in polynomial time. The algorithm works by generating macros that change
the value of a single variable at a time, maintaining the initial values for the ancestors of
the variable. In fact, this is the same strategy used in our algorithm ReversiblePlanner,
498

fiThe Role of Macros in Planning

which can thus be seen as an extension of their algorithm to multi-valued reversible variables
and general acyclic causal graphs.
Brafman and Domshlak (2003) designed a polynomial-time algorithm for solving planning problems with binary variables and polytree acyclic causal graphs of bounded indegree.
Gimenez and Jonsson (2008) showed that if the causal graph has unbounded indegree, the
problem becomes NP-complete. Restricting the pre-condition of each action to be on at
most two variables causes the problem class to become tractable again (Katz & Domshlak, 2008a). Since the hardness proof is based on a reduction to planning problems with
inverted tree causal graphs, these results all apply to the class IR if restricting to inverted
tree reducible planning problems.
Another result that applies to the class IR is the hardness of planning problems with
directed path causal graphs (Gimenez & Jonsson, 2008). Gimenez and Jonsson (2009)
extended this result to multi-valued variables with domains of size at most 5. Katz and
Domshlak (2008b) showed that planning problems whose causal graphs are inverted forks
are tractable as long as the root variable has a domain of fixed size. This class is also
a fragment of IR. In other words, the class IR includes several known tractable and
intractable fragments, although the tractability results proven in the present paper are
novel.
Other tractability results include the work by Haslum (2008), who defined planning
problems in terms of graph grammars, and showed that the resulting class is tractable under
certain restrictions of the grammar. Chen and Gimenez (2007) defined the width of planning
problems and designed an algorithm for solving planning problems whose complexity is
exponential in the width. In other words, planning problems with constant width are
tractable.
The idea of using reversible variables is related to the work by Williams and Nayak
(1997), who designed a polynomial-time algorithm for solving planning problems with
acyclic causal graphs and reversible actions. In other words, each action has a symmetric counterpart with the same pre-condition except that it reverses the effect of the former.
In addition, their algorithm requires that no two actions have pre-conditions such that one
is a proper subset of the other. Our definition of reversible variables is more flexible and
does not require actions to be reversible.
Since the class IR and the associated algorithm were introduced, two other tractability
results based on macros have appeared in the literature. We have already mentioned the
work by Gimenez and Jonsson (2008) for the class 3S. Also, Chen and Gimenez (2008b)
presented a polynomial-time algorithm that generates all macros within Hamming distance
k of a state, and defined an associated tractable class of planning problems with constant
Hamming width. Their class also contains Tower of Hanoi, but its relationship to IR has
not been fully established.
6.2 Macro-Based Planning
It is also worth mentioning the relationship to other macro-based algorithms for planning.
Macros have been long used in planning, beginning with the advent of the STRIPS representation (Fikes & Nilsson, 1971). Minton (1985) and Korf (1987) developed the idea
further, the latter showing that macros can exponentially reduce the search space if chosen
499

fiJonsson

carefully. (Knoblock, 1994) developed an abstraction technique similar to that of macros,
where the problem is treated at different levels of abstractions until fully solved. Vidal
(2004) extracted macros from relaxed plans used to generate heuristics.
Methods such as Macro-FF (Botea, Enzenberger, Muller, & Schaeffer, 2005) automatically generate macros that are experimentally shown to be useful for search, which proved
competitive at the fourth International Planning Competition. Typically, the macros introduced are flat sequences of two or three actions. This stands in stark contrast to our
algorithms, in which macros may be hierarchical and have arbitrarily long sequences. In
other words, the two approaches are very different: one tries to augment PDDL with slightly
longer sequences in the hope of speeding up search, while the other attempts to generate
some or all macros that are needed to solve the subproblems associated with a variable.
Recent work on macros admits longer action sequences (Botea, Muller, & Schaeffer, 2007;
Newton, Levine, Fox, & Long, 2007).
6.3 Factored Planning
Another related field of work is factored planning, which attempts to decompose a planning
problem into one or several subproblems. Typically, a planning problem is factored into
several subdomains, which are organized in a tree structure. Each variable and action of
the problem belongs to one of the subdomains. Amir and Engelhardt (2003) introduced
an algorithm called PartPlan that solves planning problems for which a tree decomposition already exists. The algorithm is exponential in the maximum number of actions and
variables of a subdomain.
Brafman and Domshlak (2006) introduced an algorithm called LID-GF which decomposes planning problems based on the causal graph. LID-GF is polynomial for planning
problems with fixed local depth and causal graphs of fixed tree-width. The local depth of
a variable is defined as the number of times that the value of the variable has to change on
a plan solving the problem. Interestingly, Tower of Hanoi, which is solved in polynomial
time by MacroPlanner, has exponential local depth and a causal graph with unbounded
tree-width. Kelareva, Buffet, Huang, and Thiebaux (2007) proposed an algorithm for factored planning that automatically chooses the order of solving subproblems. The algorithm
requires a subproblem clustering to be given, while our algorithms automatically derives a
subproblem order.

7. Conclusion
In this paper, we have introduced the class IR of inverted tree reducible planning problems
and presented an algorithm called MacroPlanner that uses macros to solve planning
problems in this class. The algorithm is provably complete and optimal, and runs in polynomial time for several subclasses of IR.
We also extended the class IR in several ways. The class RIR allows the causal graph to
be relaxed, and an associated algorithm called RelaxedPlanner is able to solve planning
problems in RIR optimally. The class AR allows general acyclic causal graphs as long
as variables are reversible, and an associated algorithm called ReversiblePlanner can
solve planning problems with unary actions in AR non-optimally in polynomial time. For
non-unary actions, ReversiblePlanner is not complete, although it can still solve some
500

fiThe Role of Macros in Planning

problems such as Gripper. Finally, the class AOR allows acyclic causal graphs as long
as each variable with outdegree larger than 1 in the transitive reduction is reversible. The
algorithm AcyclicPlanner combines ideas from the other algorithms to solve planning
problems in the class AOR.
We validated the theoretical properties of the algorithm in two sets of experiments. In
the first set, the algorithm was applied to Tower of Hanoi. Although the optimal solution
is exponential, the algorithm is guaranteed to solve problems in the domain in polynomial
time. The reason is that macros obviate the need to solve the same subproblem multiple
times. In the second set of experiments, we applied the algorithm to an extended version of
Gripper. Again, the results demonstrate the power of using macros to store the solution
to subproblems.
A major contribution of the paper is extending the set of known tractable classes of
planning problems. Katz and Domshlak (2008b) suggested projecting planning problems
onto known tractable fragments in order to compute heuristics. This is perfectly possible to
do using our algorithms. In the classes IR and RIR, the resulting heuristic will be admissible since the corresponding solution is optimal. Even if the solution is exponentially long,
computing the solution length can be done in polynomial time using dynamic programming.
If two domains share part of the causal graph structure, macros generated in one domain
could be used in the other. This could save significant computational effort since the scheme
compiles away variables and replaces them with macros. Since macros are functionally
equivalent to standard actions, they can be used in place of the actions and variables they
replace. Macros can also prove useful in domains for which we have to backtrack to find
an optimal solution. If macros have already been generated and stored for some variables,
there is no need to recompute partial plans for these variables from scratch.
If the multi-valued representation included a notion of objects, it would be possible to
generate macros for one object and share those macros among identical objects, just like
the parametrized operators of a standard PDDL planning domain. For example, in the
Logistics domain, each truck that operates in the same city is functionally equivalent,
and a macro generated for one truck can be applied to another. The same is true for
airplanes. Finally, to obtain optimal plans for the classes AR and AOR, one would have
to test all ways to interleave subplans for different variables, a process which is not likely
to be tractable unless the number of interleaved subplans is constant.

Acknowledgments
This work was partially funded by APIDIS and MEC grant TIN2006-15387-C03-03.

Appendix A. Proof of Theorem 3.2
In this appendix we prove Theorem 3.2, which states that each macro m returned by
MacroPlanner(P ) is well-defined and solves P . First, we prove a series of lemmas which
state that the sequences and macros returned by Compose and Solve are well-defined.
Lemma A.1. For each v  V , each state s for Vv , and each partial state x for Vv such
that x  (v = s(v)), if each macro for the parents of v is well-defined, each action sequence
seq returned by Compose(v, s, x, O, G) is well-defined for s and (s  post(seq))  x.
501

fiJonsson

Proof. Compose(v, s, x, O, G) returns sequences of the type seq = hm1 , . . . , mk i, where
mi = hs | Vw , seq(mi ), post(mi )i is a macro for a parent w  P a(v) of v such that (s | Vw ) 6
(x | Vw ). Since pre(mi ) = s | Vw holds in s, mi is applicable in s. Since the transitive
reduction of the causal graph is an inverted tree, the parents of v have no common ancestors,
so the sets of variables Vw are disjoint. As a consequence, the application of m1 , . . . , mi1
does not change the values of variables in Vw , so s | Vw still holds when mi is applied.
Assuming that the macro mi is well-defined, (s | Vw )  post(seq(mi )) = post(mi ), and
Compose(v, s, x, O, G) only considers macros mi for w such that post(mi )  (x | Vw ). Since
no other macro in seq changes the values of variables in Vw , it holds that (s  post(seq)) 
(x | Vw ). If (s | Vw )  (x | Vw ) for a parent w  P a(v) of v, seq does not contain a macro for
w, so (s  post(seq)) | Vw = s | Vw , implying (s  post(seq))  (x | Vw ). Finally, no macro in
seq changes the value of v, so (s  post(seq))(v) = s(v). Since x  (v = s(v)) by definition
and (s  post(seq))  (x | Vw ) for each w  P a(v), it holds that (s  post(seq))  x.
Lemma A.2. For each v  V and each state s for Vv , each macro hpre(m), seq(m), post(m)i
returned by a call to Solve(v, s, Z, O, G) is well-defined, and post(m)  z for some z  Z.
Proof. The proof is a double induction on v and the state-sequence pairs (p, seq) visited
during a call to Solve(v, s, Z, O, G). In particular, we show that seq is well-defined for
s and that p = s  post(seq). The first state-sequence pair removed from Q on line 4
is (s, hi). Clearly, hi is well-defined for s and s = s  post(hi). Otherwise, assume that
seq is well-defined for s and that p = s  post(seq). On line 8, a state-sequence pair
(p  post(hseq2, ai), hseq, seq2, ai) is added to Q for each action a  O such that pre(a) 
(v = p(v)) and each sequence seq2 returned by Compose(v, p, pre(a), O, G).
By induction, the macros returned by Solve(w, s , Z  , O , G) are well-defined for each
w  P a(v) and each state s for Vw . Lemma A.1 now implies that each sequence seq2
returned by Compose(v, p, pre(a), O, G) is well-defined for p and that (p  post(seq2)) 
pre(a). In other words, a is applicable in p  post(seq2), implying that hseq2, ai is welldefined for p. By assumption, seq is well-defined for s and p = s  post(seq), implying
that hseq, seq2, ai is well-defined for s. If applied in s, hseq, seq2, ai results in the state
p  post(hseq2, ai), implying p  post(hseq2, ai) = s  hseq, seq2, ai.
On line 14, a macro m = hs, hseq, seq2i, p  post(seq2)i is added to M for each projected pre-condition z  Z such that z  (v = p(v)) and each sequence seq2 returned
by Compose(v, p, z, O, G). Lemma A.1 implies that seq2 is well-defined for p and that
(p  post(seq2))  z. Since seq is well-defined for s and p = s  post(seq), hseq, seq2i is
well-defined for s and results in the state p  post(seq2) when applied in s. It follows that
the macro m is well-defined and that post(m) = p  post(seq2)  z.
We now proceed to prove Theorem 3.2. First note that MacroPlanner(P ) calls
GetMacros(v, init, A, G) to obtain a set of macros M , where G = R is the transitive
reduction of the causal graph of P and v is the root variable of R. Since each other variable
is an ancestor of v, it follows that Vv = V and init | Vv = init. The only projected
pre-condition in Z is that of the dummy action a , which equals goal | Vv = goal. Thus
GetMacros(v, init, A, G) calls Solve(v, s, Z, O, G) with s = init and Z = {goal}.
Lemma A.2 implies that each macro m returned by Solve(v, init, {goal}, O, G) is welldefined and that post(m)  goal. Since the pre-condition of each macro m returned by
502

fiThe Role of Macros in Planning

Solve(v, init, {goal}, O, G) is init and m is well-defined, the sequence seq(m) = ha1 , . . . , ak i
is well-defined for init and, when applied in init, results in the state init  post(seq(m)) =
post(m)  goal which satisfies the goal state. Thus, m solves P .

Appendix B. Proof of Theorem 3.3
In this appendix we prove Theorem 3.3, which states that MacroPlanner(P ) returns an
optimal solution to P  IR if and only if one exists. First, we prove a series of lemmas
which state that the macros returned by Solve represent the shortest solutions to the
corresponding subproblems.
Definition B.1. For each v  V and each pair of states (s, t) for Vv , let s  t denote that
there exists a sequence of actions in Av = {a  A : Vpost(a)  Vv } that, when applied in
state s, results in state t.
Lemma B.2. For each v  V , let (p, seq) be a state-sequence pair visited during a call
to Solve(v, s, Z, O, G). Then for each w  P a(v), GetMacros(w, init, A, G) previously
called Solve(w, p | Vw , Z  , O , G), where Z  and O are the values of Z and O for w.
Proof. GetMacros(w, init, A, G) calls Solve(w, s , Z  , O , G) for each state s in the list
L. If p = init | Vv is the projected initial state for Vv , p | Vw = init | Vw is added to L on
line 2 of GetMacros. Otherwise, the only actions in O for changing the values of variables
in Vw are the macros generated by GetMacros(w, init, A, G). The projection p | Vw thus
has to equal the post-condition of some such macro. Each distinct post-condition post(m)
of a macro m for w is added to L on line 12 of GetMacros.
Lemma B.3. For each v  V , each pair of states (s, u) for Vv such that s  u, and
each projected pre-condition z  Z such that u  z, Solve(v, s, Z, O, G) returns a macro
hs, seq, ti such that t  z and t is on a shortest path from s to u with prefix seq.
Proof. By induction on v. If P a(v) = , the set of actions is O = Av , and s  u implies
the existence of a sequence of actions in Av from s to u. Since each projected pre-condition
z  Z is non-empty, u  z implies u = z. Since state-sequence pairs are visited in order
of shortest sequence length, Solve(v, s, Z, O, G) is guaranteed to return a macro hs, seq, ui
such that seq is a shortest path from s to u. Clearly, u is on a shortest path from s to u,
and seq is a prefix of itself.
If |P a(v)| > 0, we use induction on the state-sequence pairs (p, seq) visited during
the call to Solve(v, s, Z, O, G) to prove the lemma. Namely, we prove that if p  u,
Solve(v, s, Z, O, G) returns a macro hs, hseq, seq2i, ti such that t  z and t is on a shortest
path from p to u with prefix seq2. The only exception to this rule is if there exists an action
sequence from s to t that is shorter than hseq, seq2i and does not pass through p.
The base case is given by p(v) = u(v). For each w  P a(v), (p | Vw ) 6 (z | Vw ) implies
z | Vw  Z  , u  z implies (u | Vw )  (z | Vw ), GetMacros(w, init, A, G) previously called
Solve(w, p | Vw , Z  , O , G) due to Lemma B.2, and p  u implies (p | Vw )  (u | Vw ) by
removing actions not in Aw . By induction, Solve(w, p | Vw , Z  , O , G) returned a macro
hp | Vw , seq  , t i such that t  (z | Vw ) and t is on a shortest path from p | Vw to u | Vw
with prefix seq  . In particular, the macro hp | Vw , seq  , t i is now part of the set O.
503

fiJonsson

In the iteration of Solve(v, s, Z, O, G) for (p, seq), Compose(v, p, z, O, G) is called on
line 12 since u  z and p(v) = u(v) imply z  (v = p(v)). The resulting set S contains
a sequence seq2 which consists of the macros hp | Vw , seq  , t i for each w  P a(v) such
that (p | Vw ) 6 (z | Vw ). On line 14 the macro hs, hseq, seq2i, ti is added to M , where
t = p  post(seq2)  z due to Lemma A.1. Since seq2 does not change the value of v, since
the subsets Vw are disjoint, and since t is on a shortest path from p | Vw to u | Vw with
prefix seq  for each w  P a(v), t is on a shortest path from p to u with prefix seq2.
For p(v) 6= u(v), any path from p to u has to include actions that change the value
of v. Let a  A be the first such action on a shortest path from p to u, and assume
that a is applied in state r, implying p  r, r  post(a)  u, and r  pre(a). For each
w  P a(v), (p | Vw ) 6 (pre(a) | Vw ) implies pre(a) | Vw  Z  , p  r implies (p | Vw ) 
(r | Vw ), r  pre(a) implies (r | Vw )  (pre(a) | Vw ), and GetMacros(w, init, A, G) called
Solve(w, p | Vw , Z  , O , G) due to Lemma B.2. By induction, Solve(w, p | Vw , Z  , O , G)
returned a macro hp | Vw , seq  , t i such that t  (pre(a) | Vw ) and t is on a shortest path
from p | Vw to r | Vw with prefix seq  . The macro hp | Vw , seq  , t i is now part of the set O.
In the iteration of Solve(v, s, Z, O, G) for (p, seq), Compose(v, p, pre(a), O, G) is called
on line 6 since pre(a)  (v = p(v)) due to the fact that a is the first action changing the
value of v on a shortest path from p to u. The resulting set S contains a sequence seq2
which consists of the macros hp | Vw , seq  , t i for each w  P a(v) such that (p | Vw ) 6
(pre(a) | Vw ). On line 8 the state-sequence pair (q  post(a), hseq, seq2, ai) is added to Q,
where q = p  post(seq2)  pre(a) due to Lemma A.1.
Since seq2 does not change the value of v and t is on a shortest path from p | Vw to
r | Vw with prefix seq  for each w  P a(v), q is on a shortest path from p to r with prefix
seq2. Consequently, there exists a shortest path hseq2, seq3i from p to r. This path does
not change the value of v, else a would not be applied in state r on a shortest path from p
to u. Since a is applicable in q due to q  pre(a), the sequence hseq2, a, seq3i is a shortest
path from p to r  post(a). It follows that q  post(a) is on a shortest path from p to
r  post(a) with prefix hseq2, ai. Thus r  post(a)  u implies q  post(a)  u.
In a future iteration of Solve(v, s, Z, O, G), the state-sequence pair removed from Q on
line 4 is (q  post(a), hseq, seq2, ai). Since q  post(a)  u, by induction on state-sequence
pairs, Solve(v, s, Z, O, G) returns a macro hs, hseq, seq2, a, seq4i, ti such that t  u and t
is on a shortest path from q  post(a) to u with prefix seq4. Then there exists a shortest
path hseq4, seq5i from q  post(a) to u. Since r  post(a) is on a shortest path from p to
u by assumption, and q  post(a) is on a shortest path from p to r  post(a) with prefix
hseq2, ai, hseq2, a, seq4, seq5i has to be a shortest path from p to u. It follows that t is on
a shortest path from p to u with prefix hseq2, a, seq4i.
The proof now follows since the first iteration of Solve(v, s, Z, O, G) is for (s, hi), implying that Solve(v, s, Z, O, G) returns a macro hs, hhi, seq2i, ti = hs, seq2, ti such that t  u
and t is on a shortest path from s to u with prefix seq2. In this case there can be no
exception since any sequence from s to t passes through s. Note that if v 
/ Vz , i.e., z does
not specify a value for v, the set of macros M returned by Solve(v, s, Z, O, G) contains
macros to z for each reachable value of v, including u(v) since u is reachable from s.
We now proceed to prove Theorem 3.3. Recall that the set of macros M returned by
MacroPlanner(P ) is equal to the set of macros returned by Solve(v, init, {goal}, O, G),
504

fiThe Role of Macros in Planning

where G = R is the transitive reduction of the causal graph and v is the root variable of R.
An optimal plan solving P is a sequence of actions in Av = A from init to a state u such
that u  goal, implying init  u. We can now apply Lemma B.3 to prove that M contains
a macro m = hinit, seq, ti such that t  goal and t is on a shortest path from s to u with
prefix seq. This is only possible if seq is an optimal plan.
If there does not exist a plan solving P , we show that M is empty by contradiction.
Assume not. Then Theorem 3.2 implies that each macro in M is a solution to P , which
contradicts the fact that P is unsolvable. Thus M has to be empty, and as a consequence,
MacroPlanner(P ) returns fail.

Appendix C. Proof of Theorems 3.5 and 3.6
In this section we prove Theorems 3.5 and 3.6, which establish two subclasses of the class
IR for which MacroPlanner generates solutions in polynomial time.
To prove Theorem 3.5, note that for each variable v  V and each action a with v 
Vpost(a) , it holds that Vpre(a) = Vv . It follows that v  Vz for each projected pre-condition
z  Z. Let Zvd = {z  Z : z(v) = d} be the set of projected pre-conditions that specify
the value d for v. Any state in the domain transition graph that specifies the value d for
v has to match either the pre-condition of an action in Adv or a projected pre-condition in
Zvd . Otherwise, the corresponding node would not be added to the graph by the algorithm.
The only exception is the projected initial state init | Vv in case init(v) = d.
Since each pre-condition is specified on each ancestor of v, only one state matches
each pre-condition. The number of projected pre-conditions is bounded by the number of
actions for descendants of v, so the number of nodes in the domain transition graph is
P
O( dD(v) (|Adv | + |Zvd |)) = O(|A| + |Z|) = O(|A|). From Lemma 3.4 it follows that the
P
complexity of the algorithm is O(|A| vV |A|3 ) = O(|V ||A|4 ), proving the theorem.
Theorem 3.6 states that each action for changing the value of v to d  D(v) has the
same pre-condition on the parents of v. Since the pre-condition is undefined on all other
ancestors, each projected pre-condition for v is specified on v alone. This implies that
successor states of the first and second type always coincide, since matching a projected
pre-condition only depends on the value of v.
We prove by induction that the domain transition graphs for v contain at most one node
for each value d  D(v). If |P a(v)| = 0, the proof follows from the fact that the nodes are
values of D(v). If |P a(v)| > 0, by induction the domain transition graphs of each parent
v   P a(v) of v has at most one node per value d  D(v  ). This implies that each macro
m  O with post(m)(v  ) = d has the same post-condition (else there would be at least two
nodes for v  = d ).
For each value d  D(v), the algorithm generates a successor state s of the first type
with s(v) = d by applying any action a  Adv . Each such action a has the same pre-condition
on each parent v   P a(v). This implies that the pre-condition of a is always satisfied in
the same state, since any macro m  O with post(m)(v  ) = pre(a)(v  ) has the same postcondition. Thus, any successor state s with s(v) = d is the same, no matter which action
we use or which previous state we come from.
Note that for d = init(v), the projected initial state init | Vv could be different from
the successor state si discussed above. However, the theorem states that the pre-condition
505

fiJonsson

of actions with post(a)(v) = init(v) on P a(v) equals init | P a(v). Since the domain
transition graphs of each parent v   P a(v) contain at most one node for init(v  ), this node
has to correspond to the projected initial state init | Vv . Thus, any successor state s with
s(v) = init(v) equals the projected initial state init | Vv , since pre(a) | P a(v) = init | P a(v)
for each action a  Adv , d = init(v), and post(m) = init | Vv for each macro m that satisfies
the pre-condition of a. The proof of the theorem now follows from Lemma 3.4.

Appendix D. Proof of Theorems 4.3 and 4.4
In this appendix we prove Theorems 4.3 and 4.4, which together state that RelaxedPlanner returns a well-defined optimal solution to P  RIR if and only if one exists. We
first show that Definition B.1 and Lemma B.2 apply to RelaxedPlanner. The notion of
reachability in Definition B.1 refers to the set Av of actions such that Vpost(a)  Vv , thus
excluding actions that change the value of some successor of v.
Lemma B.2 states that if (p, seq) is a state-sequence pair visited during a call to
Solve(v, s, Z, O, G) and w  P a(v) is a parent of v in G, GetMacros(w, init, A, G) previously called Solve(w, p | Vw , Z  , O , G), where Z  and O are the values of Z and O for
w. We show that this holds for RelaxedMacros as well, where G is now the transitive
reduction of the relaxed causal graph.
If p equals the projected initial state init | Vv , p | Vw = init | Vw is added to L on line 2
of RelaxedMacros. Otherwise, p is a successor state of the first type. In other words, it
is reached by applying some action a  Av  Aw . Solve only uses macros for w to satisfy
the pre-condition of a with respect to Vw . Consequently, p | Vw equals the post-condition
post(m) of some such macro m, followed by the application of a. This is precisely the state
post(m)  (post(a) | Vw ) added to L on line 15 of the modified RelaxedMacros.
Since the subroutines Solve and Compose are the same as before, Lemmas A.2 and
B.3 apply verbatim to RelaxedPlanner. We can now use the same reasoning as for
MacroPlanner to prove the two theorems.

Appendix E. Proofs of Theorems 4.7, 4.8, and 4.9
In this appendix we prove Theorem 4.7, which states that the sequence seq returned by
ReversiblePlanner(P ) is well-defined for init and satisfies (init  post(seq))  goal.
We also prove Theorem 4.8, which states that ReversiblePlanner is complete for planning problems in AR with unary actions. Finally, we prove Theorem 4.9 regarding the
complexity of ReversiblePlanner. We first prove lemmas similar to Lemmas A.1 and
A.2.
Lemma E.1. For each state s and each partial state x, let (seq, seq2) be the pair of sequences returned by ReversibleCompose(U, s, x, P, G). If the macros returned by ReversibleSolve are well-defined, seq is well-defined for s and s  post(seq) = s  x. Furthermore, seq2 is well-defined for s  x and post(seq2) = s | (Vx  U ).
Proof. Let w1 , . . . , wk be a topological sort of the variables in Vx such that x(wi ) 6= s(wi ).
The first sequence returned by ReversibleCompose(U, s, x, P, G) is seq = hm1k , . . . , m11 i,
where m1i = hs | Vwi , seq  , (s | Vwi )  (wi = x(wi ))i is the macro for changing the value of wi
506

fiThe Role of Macros in Planning

from s(wi ) to x(wi ). The fact that wi comes before wj in topological order implies wj 
/ V wi .
Thus, after changing the values of wk , . . . , wi+1 , the pre-condition s | Vwi of the macro m1i
still holds, so the sequence seq is well-defined for s. Furthermore, seq changes the value
of each variable w  Vx to x(w), but leaves the values of all other variables unchanged, so
s  post(seq) = s  x.
Let u1 , . . . , ul be a topological sort of the variables in W = {w1 , . . . , wk }  U . The
second sequence returned by ReversibleCompose(U, s, x, P, G) is seq2 = hm21 , . . . , m2l i,
where m2i = h(s | Vui )  (ui = x(ui )), seq  , s | Vui i is the macro for resetting the value of ui
from x(ui ) to s(ui ). If W contains ancestors of ui , they come before ui in topological order.
Hence their values are reset to s prior to ui , so the pre-condition (s | Vui )  (ui = x(ui ))
of the macro m2i holds after the application of m21 , . . . , m2i1 . Since seq2 resets the value of
each variable w  Vx  U to s(v), post(seq2) = s | (Vx  U ).
Note that if some ancestor u of ui were part of U , its value would not be reset to s(u ),
and as a consequence, seq2 would not be well-defined. However, ReversibleSolve always
calls ReversibleCompose with U = Vpost(a) and x = pre(a) for some action a. Taken
together, u  Vpost(a) and ui  Vpre(a)  Vpost(a) imply that the causal graph contains an
edge (ui , u ), which is true even if we consider the relaxed causal graph. Thus u could not
be an ancestor of ui in an acyclic causal graph.
Lemma E.2. For each v  V , each state s, and each value d  D(v), the macro returned
by ReversibleSolve(v, s, d, A, G) is well-defined.
Proof. By induction on the state-sequence pairs (p, seq) visited during a call to the subroutine ReversibleSolve(v, s, d, A, G). In particular, we show that seq is well-defined for s
and satisfies s  post(seq) = s  p. The base case is given by the first state-sequence pair
(s | W, hi), whose sequence is clearly well-defined for s and satisfies spost(hi) = s(s | W ).
Otherwise, assume that the statement holds for (p, seq), and let a be an action such that
pre(a)  (v = p(v)).
Let (seq2, seq3) be the result of ReversibleCompose(Vpost(a) , s  p, pre(a), A, G). If
(seq2, seq3) does not equal (fail,fail), Lemma E.1 states that seq2 is well-defined for sp
and that s  p  post(seq2) = s  p  pre(a). This implies that hseq, seq2, ai is well-defined
for s and that s  post(hseq, seq2, ai) = s  p  pre(a)  post(a). Lemma E.1 also states
that seq3 is well-defined for s  p  pre(a) and that post(seq3) = (s  p) | (Vpre(a)  Vpost(a) ).
Since seq3 only changes the values of variables in Vpre(a)  Vpost(a) , it is still well-defined
in the state s  p  pre(a)  post(a) that results from applying a. Thus hseq, seq2, a, seq3i
is well-defined for s and s  post(hseq, seq2, a, seq3i) = s  p  post(a). It is easy to show
that composition is commutative, implying s  p  post(a) = s  (p  post(a)). Thus the
statement holds for the new state-sequence pair (p  post(a), hseq, seq2, a, seq3i) inserted
into L on line 12 of ReversibleSolve.
The proof now follows by induction on variables v  V . If v has no ancestors in
the causal graph, any call to ReversibleCompose(Vpost(a) , s  p, pre(a), A, G) returns
a pair of empty sequences, since pre(a)  (v = p(v)) and the set Vpre(a)  Vpost(a) is
empty. Thus the macro returned by ReversibleSolve is well-defined. Otherwise, by
hypothesis of induction the macros generated by ReversibleSolve for ancestors of v
are well-defined. Thus ReversibleCompose(Vpost(a) , s  p, pre(a), A, G) returns a well507

fiJonsson

defined pair of sequences due to Lemma E.1 and consequently, the macros generated by
ReversibleSolve for v are also well-defined.
The proof of Theorem 4.7 now follows by a straightforward application of Lemma E.1 for
the call ReversibleCompose(, init, goal, A, G) made by ReversiblePlanner, taking
advantage of Lemma E.2 to ensure that the macros returned by ReversibleSolve are
well-defined.
To prove Theorem 4.8, we first prove a lemma regarding the completeness of ReversibleSolve for unary actions.
Lemma E.3. For each v  V , each state s, and each value d  D(v), if there exists a state
u such that u(v) = d and s  u, ReversibleSolve(v, s, d, A, G) returns a valid macro
different from fail.
Proof. By a double induction on variables v  V and the state-sequence pairs (p, seq)
visited during a call to ReversibleSolve(v, s, p, A, G). In particular, we show that if
s  p  u, ReversibleSolve(v, s, d, A, G) returns a valid macro. The base case is given
by p(v) = d, implying p = (s | W )  (v = d) since W =  for unary actions. In this case,
ReversibleSolve returns a macro on line 7. Else the path from s to u has to contain
actions for changing the values of v. Let a be the first such action, implying s  t for some
state t such that t  pre(a).
By induction, for each w  Vpre(a) , ReversibleSolve(w, s, pre(a)(w), A, G) returns
a valid macro. The fact that w is reversible implies that there exists a state u such
that u (w) = s(w) and s  (w = pre(a)(w))  u . As a consequence, by induction
ReversibleSolve(w, s(w = pre(a)(w)), s(w), A, G) also returns a valid macro. Thus the
pair of sequences (seq2, seq3) returned by ReversibleCompose(Vpost(a) , sp, pre(a), A, G)
is well-defined and different from (fail,fail). Consequently, ReversibleSolve inserts
the state-sequence pair (p  post(a)(v), hseq, seq2, a, seq3i) into the list L on line 12.
The sequence hseq, seq2, a, seq3i generated by ReversibleSolve results in the state
s  (v = post(a)(v)), which may be different from the state on the given path from s to u
after applying action a. However, since v is reversible, that other state is reachable from
s  (v = post(a)(v)), implying s  (v = post(a)(v))  u. By induction on state-sequence
pairs, ReversibleSolve(v, s, d, A, G) returns the corresponding macro.
The proof of Theorem 4.8 now follows from the fact that in the call to the subroutine
ReversibleCompose(, init, goal, A, G), if each value in the goal state can be satisfied
starting in init, Lemma E.3 implies that ReversibleSolve is guaranteed to return a
macro that satisfies the goal state for each variable in Vgoal . Thus, due to Lemma E.1,
ReversibleCompose will successfully return a pair of well-defined sequences (seq, seq2)
with the property init  post(seq) = init  goal, so seq is a solution to P . If some value in
the goal state is unreachable from init, ReversibleSolve does not return a corresponding
macro, so ReversibleCompose (and ReversiblePlanner) return fail.
To prove Theorem 4.9, we first prove a lemma regarding the values of variables in Vv W :
Lemma E.4. For each v  V and any call to ReversibleSolve(v, s, d, A, G), it holds that
s | (Vv  W ) = init | (Vv  W ).
508

fiThe Role of Macros in Planning

Proof. The lemma states that variables in Vv  W always take on their initial values in any
call to ReversibleSolve. First note that ReversiblePlanner calls ReversibleCompose with s = init. In turn, ReversibleCompose makes two calls to ReversibleSolve
for each variable w  Vx . In these calls, the only variable that may have a different value
from that in s is w. However, for any call to ReversibleSolve for w, it holds that w  W
(else there is no action for changing the value of w, so we can remove it from the problem).
Thus the lemma holds for the call to ReversibleCompose by ReversiblePlanner.
Now assume that the lemma holds for a call to ReversibleSolve(v, s, d, A, G). In a
call to ReversibleCompose, the value of s is s  p, where p is a partial states on the
variables in W . Note that because of the definition of W , if w  W for v, w  W for any
ancestor u of v such that w  Vu . Thus if the value of w is different from init(w) in a
call to ReversibleCompose, w  W in any subsequent call to ReversibleSolve. Since
all variables not in W take on their initial values by assumption, they take on their initial
values in subsequent calls to ReversibleSolve as well.
Because of Lemma E.4, the number of distinct choices of s in calls to ReversibleSolve
for a variable v is O(Dk ), where k = |W |. Since the number of distinct choices of d is
O(D), the total number of calls to ReversibleSolve for v is O(Dk+1 ). Each call to
ReversibleSolve for a variable v  V is a breadth-first search in a graph with O(Dk )
nodes. For each action a  A , there is at most one edge for each node, so the complexity
of breadth-first search is O(Dk (1 + |A |)) = O(Dk |A |), and the total complexity of calls to
ReversibleSolve for v is O(D2k+1 |A |). Since the sets A are distinct, the total complexity
P
P
of calls to ReversibleSolve is O( vV D2k+1 |A |) = O(D2k+1 vV |A |) = O(D2k+1 |A|).
ReversibleCompose is called at most Dk |A| times, once for each edge in one of the
graphs traversed by ReversibleSolve. The worst-case complexity of ReversibleCompose is linear in the number of variables in Vx , or O(|V |). The total complexity of ReversibleCompose is thus O(Dk |A||V |), so the total complexity of ReversiblePlanner
is O(D2k+1 |A| + Dk |A||V |) = O(Dk |A|(Dk+1 + |V |)).

References
Amir, E., & Engelhardt, B. (2003). Factored Planning. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pp. 929935.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI
Planning with Automatically Learned Macro-Operators. Journal of Artificial Intelligence Research, 24, 581621.
Botea, A., Muller, M., & Schaeffer, J. (2007). Fast Planning with Iterative Macros. In
Proceedings of the 20th International Joint Conference on Artificial Intelligence, pp.
18281833.
Brafman, R., & Domshlak, C. (2003). Structure and Complexity in Planning with Unary
Operators. Journal of Artificial Intelligence Research, 18, 315349.
Brafman, R., & Domshlak, C. (2006). Factored Planning: How, When, and When Not.. In
Proceedings of the 21st National Conference on Artificial Intelligence.
509

fiJonsson

Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69, 165204.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32(3), 333377.
Chen, H., & Gimenez, O. (2007). Act Local, Think Global: Width Notions for Tractable
Planning. In Proceedings of the 17th International Conference on Automated Planning
and Scheduling.
Chen, H., & Gimenez, O. (2008a). Causal Graphs and Structurally Restricted Planning. In
Proceedings of the 18th International Conference on Automated Planning and Scheduling.
Chen, H., & Gimenez, O. (2008b). On-the-fly macros. CoRR, abs/0810.1186.
Domshlak, C., & Dinitz, Y. (2001). Multi-Agent Off-line Coordination: Structure and Complexity. In Proceedings of the 6th European Conference on Planning, pp. 277288.
Erol, K., Nau, D., & Subrahmanian, V. (1995). Complexity, decidability and undecidability
results for domain-independent planning. Artificial Intelligence, 76(1-2), 7588.
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 5 (2), 189208.
Gimenez, O., & Jonsson, A. (2008). The Complexity of Planning Problems with Simple
Causal Graphs. Journal of Artificial Intelligence Research, 31, 319351.
Gimenez, O., & Jonsson, A. (2009). Planning over Chain Causal Graphs for Variables
with Domains of Size 5 Is NP-Hard. Journal of Artificial Intelligence Research, 34,
675706.
Haslum, P. (2008). A New Approach To Tractable Planning. In Proceedings of the 18th
International Conference on Automated Planning and Scheduling.
Helmert, M. (2006). The Fast Downward Planning System. Journal of Artificial Intelligence
Research, 26, 191246.
Jonsson, A. (2007). The Role of Macros in Tractable Planning Over Causal Graphs. In
Proceedings of the 20th International Joint Conference on Artificial Intelligence, pp.
19361941.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100(1-2), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence does not imply tractable
plan generation. Annals of Mathematics and Artificial Intelligence, 22(3-4), 281296.
Katz, M., & Domshlak, C. (2008a). New Islands of Tractability of Cost-Optimal Planning.
Journal of Artificial Intelligence Research, 32, 203288.
Katz, M., & Domshlak, C. (2008b). Structural Patterns Heuristics via Fork Decompositions. In Proceedings of the 18th International Conference on Automated Planning
and Scheduling, pp. 182189.
Kelareva, E., Buffet, O., Huang, J., & Thiebaux, S. (2007). Factored Planning Using Decomposition Trees. In Proceedings of the 20th International Joint Conference on Artificial
Intelligence, pp. 19421947.
510

fiThe Role of Macros in Planning

Knoblock, C. (1994). Automatically generating abstractions for planning. Artificial Intelligence, 68(2), 243302.
Korf, R. (1987). Planning as search: A quantitative approach. Artificial Intelligence, 33(1),
6588.
Minton, S. (1985). Selectively generalizing plans for problem-solving. In Proceedings of the
9th International Joint Conference on Artificial Intelligence, pp. 596599.
Newton, M., Levine, J., Fox, M., & Long, D. (2007). Learning Macro-Actions for Arbitrary Planners and Domains. In Proceedings of the 17th International Conference on
Automated Planning and Scheduling, pp. 256263.
Vidal, V. (2004). A Lookahead Strategy for Heuristic Search Planning. In Proceedings of the
14th International Conference on Automated Planning and Scheduling, pp. 150159.
Williams, B., & Nayak, P. (1997). A reactive planner for a model-based executive. In
Proceedings of the 15th International Joint Conference on Artificial Intelligence, pp.
11781185.

511

fiJournal of Artificial Intelligence Research 36 (2009) 415-469

Submitted 03/09; published 12/09

Friends or Foes? On Planning as Satisfiability
and Abstract CNF Encodings
Carmel Domshlak

dcarmel@ie.technion.ac.il

Technion  Israel Institute of Technology,
Haifa, Israel

Jorg Hoffmann

joerg.hoffmann@inria.fr

INRIA,
Nancy, France

Ashish Sabharwal

sabhar@cs.cornell.edu

Cornell University,
Ithaca, NY, USA

Abstract
Planning as satisfiability, as implemented in, for instance, the SATPLAN tool, is a
highly competitive method for finding parallel step-optimal plans. A bottleneck in this
approach is to prove the absence of plans of a certain length. Specifically, if the optimal plan
has n steps, then it is typically very costly to prove that there is no plan of length n1. We
pursue the idea of leading this proof within solution length preserving abstractions (overapproximations) of the original planning task. This is promising because the abstraction
may have a much smaller state space; related methods are highly successful in model
checking. In particular, we design a novel abstraction technique based on which one can, in
several widely used planning benchmarks, construct abstractions that have exponentially
smaller state spaces while preserving the length of an optimal plan.
Surprisingly, the idea turns out to appear quite hopeless in the context of planning as
satisfiability. Evaluating our idea empirically, we run experiments on almost all benchmarks
of the international planning competitions up to IPC 2004, and find that even hand-made
abstractions do not tend to improve the performance of SATPLAN. Exploring these findings
from a theoretical point of view, we identify an interesting phenomenon that may cause
this behavior. We compare various planning-graph based CNF encodings  of the original
planning task with the CNF encodings  of the abstracted planning task. We prove that,
in many cases, the shortest resolution refutation for  can never be shorter than that for .
This suggests a fundamental weakness of the approach, and motivates further investigation
of the interplay between declarative transition-systems, over-approximating abstractions,
and SAT encodings.

1. Introduction
The areas of model checking and AI planning are well-known to be closely related as
they both develop tools for automatic behavior analysis of large-scale, declaratively specified transition systems. In particular, both in planning and in model checking of safety
propertieschecking reachability of non-temporal formulasproblems are given by a description of a transition system, by an initial system state, and by a target condition. A
solution for such a problem corresponds to a legal path of transitions bringing the system
from the initial state to a state satisfying the target condition.
c
2009
AI Access Foundation. All rights reserved.

fiDomshlak, Hoffmann, & Sabharwal

For a model checking problem, a solution corresponds to an error path in the system,
that is, to an unwanted system behavior. Proving absence of such error paths is the ultimate
goal of system verification, and thus the traditional focus of the field is exactly on that.
Besides clever symbolic representations of the state space, the key technique to accomplish
this ambitious task is abstraction. System abstraction corresponds to an over-approximation
of the considered transition system, and thus abstraction preserves all the transitions of the
original system. Hence, if the abstract transition system does not contain a solution, then
neither does the original system. The key to success in model checking is that, in many cases,
one can prove the absence of solutions in rather coarse abstractions with a comparatively
small state space. Techniques of this kind have been explored in depth for a long time.
Arguably its most wide-spread instance in model checking is predicate abstraction (Graf
& Sadi, 1997), where system states form equivalence classes defined in terms of the truth
values of a number of expressions (the predicates), such as linear expressions over integer
system variables. Predicates can be learned by analyzing spurious error paths in too-coarse
abstractions (Clarke, Grumberg, Jha, Lu, & Veith, 2003). Methods of this kind have been
extremely successful in the verification of temporal safety properties (e.g., Ball, Majumdar,
Millstein, & Rajamani, 2001; Chaki, Clarke, Groce, Jha, & Veith, 2003; Henzinger, Jhala,
Majumdar, & McMillan, 2004).
In contrast to system verification, the focus in AI planning is on finding solutions in
instances that are assumed to be solvable. In particular, in optimizing planning, the task
is to find a solution that optimizes a certain criterion such as (the focus of our analysis
here) the sequential/parallel length of the solution path. Unlike in general planning where
any solution is good enough, the main bottleneck in length-optimizing planning is always
to prove the absence of solutions of a certain length. In particular, if the optimal plan has
n steps, then the hardest bit is typically to prove that there is no plan of length n  1.
Note that this is where the plan is actually proved to be optimal, and no length-optimizing
planner can avoid constructing this proof, no matter what computational techniques it is
based on.
Our agenda in this research is to apply the above idea from model checking to lengthoptimizing planning. We lead the optimality proofnon-existence of a plan of length n1
within an abstraction. In particular, our focus is on the interplay between abstraction and
proving optimality in parallel step-optimal planning as satisfiability. This approach was
originally proposed by Kautz and Selman (1992), who later developed the SATPLAN tool
(Kautz & Selman, 1999; Kautz, 2004; Kautz, Selman, & Hoffmann, 2006). SATPLAN
performs an iteration of satisfiability tests on CNF formulas b encoding the existence of a
parallel plan of length at most b, where b starts from 0 and is increased incrementally. If
n is the first satisfiable formula, then n equals the length of an optimal parallel plan, and
hence SATPLAN is a parallel optimal, or step-optimal, planner. In that class of planners,
SATPLAN is highly competitive. In particular, SATPLAN won 1st prizes for optimal
planners in the International Planning Competition (IPC), namely in IPC 2004 (Kautz,
2004; Hoffmann & Edelkamp, 2005) and in IPC 2006 (Kautz et al., 2006). One property of
the CNF encodings employed by SATPLAN that plays a key role in our analysis later on
is that these are based on the planning graph structure (Blum & Furst, 1995, 1997).
Of course, our objective closely relates to the many approaches developed in planning
for computing lower bounds based on over-approximations, (e.g., Haslum & Geffner, 2000;
416

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Edelkamp, 2001; Haslum, Botea, Helmert, Bonet, & Koenig, 2007; Helmert, Haslum, &
Hoffmann, 2007; Katz & Domshlak, 2008; Bonet & Geffner, 2008). The key difference,
however, is our focus on exact lower bounds, that is, the attempt to actually prove optimality
within the abstraction. If we want to be able to prove optimality within the abstraction,
then the abstraction must be what we term solution length preserving: the abstraction must
not introduce any solutions shorter than the optimal solution for the original problem. If
the lower bound for step-optimal planning is not exact, then there will be no support for
SATPLANs iteration n  1, which constitutes the main bottleneck.
In what follows, we briefly explain our initial motivation behind this work, and we
summarize our empirical and theoretical results.
1.1 Initial Motivation
It would of course be interesting to explore whether predicate abstraction can be applied
to planning. Indeed, that had been our initial idea. However, while our discussions of
this idea lead to nowhere,1 we instead made a different discovery. Planning state spaces
can often be dramatically reduced, without introducing any shorter solutions, based on an
abstraction technique that we call variable domain abstraction. That technique essentially
adapts the work by Hernadvolgyi and Holte (1999) to the propositional STRIPS formalism.
We abstract the STRIPS task by not distinguishing between certain values of the multiplevalued variables underlying the STRIPS encoding. That is, if p and q are propositions
corresponding to non-distinguished values, the abstracted STRIPS task acts as if p and q
were the same. Note that this generalizes the abstraction used by Edelkamp (2001) which,
for each multi-valued variable, either abstracts it away completely or does not abstract it
at all; see the details in Section 2.
The first example where we noticed the compression power of variable domain abstraction is the classical Logistics domain. In this domain, packages must be transported within
cities using trucks and between cities using airplanes. Actions load/unload packages, and
move vehicles. Importantly, there are no constraints on (either of) vehicle capacities, fuel,
or travel links. As a consequence, if a package p starts off in city A and has its destination in
city B, then all other cities C 6= A,B are completely irrelevant to p. That is, one can choose
an arbitrary location x in such a city C, and replace all facts of the form at(p,l), where l is
a location outside A and B, with at(p,x). Also, in(p,t), where t is a truck outside A and B,
can be replaced with at(p,x). One can completely abstract away the positions of packages
that have no destination, and some other minor optimizations are possible. This way we
lose many distinctions between different positions of objectswithout introducing a shorter
solution! An optimal plan will not rely on storing a package p in a city other than ps origin
or destination. The state space reduction is dramatic: the abstracted state space contains
at least (((C  2)  S)  1)P states less, where C, S, and P respectively are the number of
cities, the city size (number of locations in each city), and the number of packages. Similar
1. We are still skeptical about the prospects. Software artifacts (rigid control structure, numeric expressions
essential for the flow of control) have a rather different nature than planning problems (loose control
structure, numeric expressions non-existent or mostly used to encode resource consumption). For example, a major advantage of predicate abstraction is that it can capture loop invariantsa great feature,
but seemingly rather irrelevant to plan generation.

417

fiDomshlak, Hoffmann, & Sabharwal

abstractions can be made, and similar state space reductions can be obtained, in other IPC
domains such as Zenotravel, Blocksworld, Depots, Satellite, and Rovers (see Section 3).
1.2 Summary of Empirical Results
In our first experiment, we have implemented our Logistics-specific abstraction by abstracting a set of planning tasks at the level of their description, modifying their actions and
their initial state. All these planning tasks feature 2 airplanes, 2 locations in each city,
and 6 packages. The number of cities scales from 1 to 14. To account for the variance
in the hardness of individual instances, we took average values over 5 random instances
for each problem size. The increasing number of cities introduces an increasing amount of
irrelevance, which we measure by the percentage RelFrac of facts considered relevant (not
abstracted). Note here that the additional cities are irrelevant only for some of the individual packagesthey cant be removed completely from the task like standard irrelevance
detection mechanisms, e.g. RIFO (Nebel, Dimopoulos, & Koehler, 1997), would try to do.
We provided all the abstracted tasks to three optimizing planners, namely Mips.BDD
(Edelkamp & Helmert, 1999), IPP (Hoffmann & Nebel, 2001), and SATPLAN04,2 in order to examine how the abstraction affects different approaches to optimizing planning.
Mips.BDD searches blindly while exploiting a sophisticated symbolic representation of the
state space. IPP is equivalent to a parallel state-space heuristic search with the widely
used h2 heuristicthe parallel version of h2 as originally introduced in Graphplan (Blum &
Furst, 1997; Haslum & Geffner, 2000). Thus, Mips.BDD, IPP, and SATPLAN04 represent
orthogonal approaches to optimizing planning.3 For each abstract task and planner, we
measured runtime, and compared the latter to the time taken by the same planner on the
original task. Time-out was set to 1800 seconds, which was also used as the value for the
average computation if a time-out occurred. We stopped evaluating a planner if it had 2
time-outs within the 5 instances of one size.
Figure 1(a), (b), and (c) respectively show our results for Mips.BDD, IPP, and SATPLAN04. Comparing the performance on the original and abstracted tasks, it is apparent
from Figure 1 that proving optimality within the abstraction dramatically improved the
performance of Mips.BDD, and significantly improved the performance of IPP. At the right
end of the scale (with 14 cities), Mips.BDD using the abstraction can find optimal sequential
plans almost as fast as SATPLAN04 can find step-optimal plans. Given that it is usually
much harder to find optimal sequential plans than optimal parallel plans, especially in highly
parallel domains such as Logistics, this performance improvement is quite remarkable. (In
addition to the reduced state space size, Mips.BDD benefits from the small state encoding,
which stops growing at some point because the maximal number of locations relevant to
each package is constant.)
The above findings for Mips.BDD and IPP were in line with our original intuition and,
for SATPLAN04 as well, we expected to see much improved runtime behavior within the
abstraction. To our surprise, we did not. As appears in Figure 1(c), the improvement
obtained for SATPLAN04 by proving optimality within the abstraction was hardly dis2. SATPLAN04 is the version of SATPLAN that competed in the 2004 International Planning Competition.
3. Importantly, Mips.BDD is sequentially optimal while SATPLAN04 and IPP are step-optimal. Hence
one should not compare the performance of those planners directly, and in particular this is not our
purpose here. We focus on how each of the planners reacts to the abstraction.

418

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

10000

100

Mips.BDD-abstract
Mips.BDD-real
RelFrac

10000

1000

80

100

IPP-abstract
IPP-real
RelFrac

90

90

1000

80

70
100

70

60

100

60

50
10

50

40

10

40

30
1

30

20

1

20

10
0.1

10

0
1

2

3

4

5

6

7

8

9

10

11

12

13

0.1

14

0
1

2

3

4

5

6

(a)

7

8

9

10

11

12

13

14

(b)
10000

100

SATPLAN-abstract
SATPLAN-real
RelFrac

90

1000

80
70

100

60
50

10

40
30

1

20
10

0.1

0
1

2

3

4

5

6

7

8

9

10

11

12

13

14

(c)

Figure 1: Runtime performance of (a) MIPS, (b) IPP, and (c) SATPLAN04, with (abstract) and without (real) our hand-made variable domain abstraction, in
Logistics instances explicitly scaled to increase the amount of irrelevance. Horizontal axis scales the number of cities, left vertical axis shows total runtime in
seconds, right vertical axis shows the percentage RelFrac of relevant facts.

cernible. At the right end of the scale, abstraction yields a humble speed-up factor of 2.8.
That is particularly insignificant since this speed-up is obtained at the drastically small
RelFrac value of 24%in the IPC 2000 Logistics benchmarks, RelFrac is 42% on average.
The latter corresponds to 6 cities in Figure 1, where SATPLAN04 has a slight advantage
on the original tasks.
To investigate the above more broadly, we conducted experiments in almost all STRIPS
domains used in all international planning competitions (IPC) up to IPC 2004. In many
cases, we tailored the abstraction of the domain by hand. The results of this exhaustive
evaluation (discussed in detail in Section 3) do not significantly depart from those for
the Logistics-specific abstraction above. For Mips.BDD we almost consistently obtained a
significant improvement. For IPP the improvements happened only rarely, and typically
these were not substantial. (While IPP is improved in Figure 1, note that, at the IPC419

fiDomshlak, Hoffmann, & Sabharwal

average RelFrac of 42%, the improvement is not yet strong.) Finally, for SATPLAN04, we
hardly ever obtained an improvement.
What causes the difference in profiting from abstraction between these three different
planning techniques? An intuitive interpretation of our results is that the informedness of
the abstraction must compete with the informedness of the search itself. In other words, the
better the planner is at exploiting the structure of a particular example, the more difficult it
is for the abstraction to exploit structure that is not already being exploited. This intuition
is in good correspondence with the Logistics results in Figure 1: while optimizing exactly the
same measure, on the original examples, SATPLAN04 is faster than IPP, while the inverse
relation holds regarding which planner profits more from the abstraction. That said, such
intuitive interpretations of our results are, at this point, mere speculation. It is left open for
future research to determine more accurately what precisely causes the difference. Herein,
we concentrate only on planning as satisfiability, and identify a fundamental weakness of
this approach with respect to profiting from abstraction.
1.3 Summary of Theoretical Results
Intrigued by our results for SATPLAN04,4 we wondered what kind of effect an abstraction
actually has on a CNF encoding of the planning task formulated as a Boolean satisfiability
problem instance. Recall here that our abstractions are over-approximations, that is, any
action sequence applicable in the original task is applicable in the abstract task, and any
plan in the original task is a plan in the abstract task. So, intuitively, the abstract task
is more generous than the original task. With this in mind, consider the CNF formula
n1 encoding the existence of a plan one step shorter than the optimal plan, and consider
the same formula, n1 , generated for the abstract task. We need to prove that n1 is
unsatisfiable. (Note that n1 is, in fact, unsatisfiable when  is a solution length preserving
abstraction.) Intuitively, the more constrained a formula is, the easier it is to lead such a
proof. But n1 is more generous, and hence less constrained, than n1 . Does this mean
that it is actually harder to refute n1 than to refute n1 ?
For some abstraction methods, it is in fact trivial to see that the answer to that question
is yes. Say we abstract n1 by ignoring some of its clauses. n1 is then a sub-formula
of n1 , immediately implying that any resolution refutation for n1 is also a resolution
refutation for n1 . In particular, the shortest possible refutation cannot be shorter for
n1 . A similar situation sometimes occurs in the interplay between abstractions and CNF
encodings of planning problems. For instance, suppose we abstract by ignoring a subset
of the goals. In most CNF encodings of planning, and in particular in the planning-graph
based CNF encodings (Kautz & Selman, 1999) underlying SATPLAN, each goal fact yields
one clause in the CNF. Hence, with a goal ignoring abstraction, n1 is a sub-formula of
n1 , just as above.
A more complex example would correspond to abstraction by ignoring preconditions or
delete effects. In the encodings used by SATPLAN, one or several clauses related to the
ignored precondition/delete effect disappear. However, the CNF changes also in other ways
because, with one precondition/delete effect less, more actions and facts become possible
at later time steps. Intuitively, those additional actions and facts do not help proving
4. Translation: Deeply frustrated by our results for SATPLAN04, . . . 

420

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

unsatisfiability of n1 . The formal proof of this intuitive statement, however, is less obvious
than the one for the goal ignoring abstraction above. Matters are most complicated, and
much less intuitive, for Edelkamps (2001) abstraction and for variable domain abstraction,
where the changes made to the abstract task also affect the add effects of actions. Recall
that variable domain abstraction is of special interest here because it is most likely to satisfy
the constraint of solution length preservation.
To investigate these issues in detail, one has to consider various possible combinations of
CNF encodings and abstraction methods. Many different encodings of planning into SAT
have been proposed. We focus on planning-graph based encodings because those were used
in the SATPLAN system, in all of its appearances in the international planning competitions (Kautz & Selman, 1999; Long, Kautz, Selman, Bonet, Geffner, Koehler, Brenner,
Hoffmann, Rittinger, Anderson, Weld, Smith, & Fox, 2000; Kautz, 2004; Kautz et al., 2006).
Indeed, according to Kautz and Selman (Kautz & Selman, 1999; Long et al., 2000), such
CNF encodingsin particular the mutex relations computed by Graphplanare vital to
SATPLANs performance. While recent results on more effective encodings may challenge
this assessment (Rintanen, Heljanko, & Niemela, 2006; Chen, Huang, Xing, & Zhang, 2009;
Robinson, Gretton, Pham, & Sattar, 2008), even so the graphplan-based encodings are of
interest simply because they have been widely used during almost a decade. It remains of
course an important question whether and to what extent our results carry over to alternative CNF encodings. We discuss this issue in some depth when concluding the paper in
Section 5.
We consider four different encodings, three of which have been used in some edition of
the IPC; the fourth encoding is considered for the sake of completeness. The encodings
differ in two parameters: whether they use only action variables, or action as well as fact
variables; and whether they include all planning graph mutexes between actions or only the
direct interferences. (The latter is motivated by the fact that there is often an enormous
amount of action mutexes, seriously blowing up the size of the formula.)
On the abstractions side, we focus on abstraction methods that can be formulated as
manipulating planning tasks at the language level, i.e., modifying the tasks actions and/or
initial/goal states. Many commonly used abstractions for propositional STRIPS can be
formulated this way. We consider six such abstractions, namely (1) removing goals, (2)
adding initial facts, (3) removing preconditions, (4) removing delete effects, (5) Edelkamps
(2001) abstraction (removing entire facts), and (6) variable domain abstraction.
For all 24 combinations of CNF encoding and abstraction method, we prove that the
shortest possible resolution refutation can be exponentially longer for n1 than for n1 .
For all 20 combinations involving abstractions other than variable domain abstraction, we
prove that the shortest possible resolution refutation cannot be shorter for n1 than for
n1 . For abstraction (1), this is trivial as outlined above. For abstractions (2)(4), the
proof exploits the fact that these abstractions lead to larger planning graphs containing
more actions and facts. For abstraction (5), this reasoning does not work because some
facts disappear from the planning graph. However, one can start by removing a fact from
the goal and all action preconditions; afterwards the fact is irrelevant and one can remove
it also from the initial state and action effects.
Matters are most complicated for abstraction (6), that is, variable domain abstraction.
For the encoding with only action variables and full mutexes, we show that, as before,
421

fiDomshlak, Hoffmann, & Sabharwal

the shortest possible resolution refutation cannot be shorter for n1 than for n1 . For
the encoding with only action variables and only direct action mutexes, we show that the
possible improvement is bounded from above by the effort it takes to recover the indirect
action mutexes. For the two encodings with both action and fact variables, it remains an
open question whether such bounds exist.5
Importantly, all our proofs are valid not only for general resolution, but also for many
of the known restricted variants of resolution, in particular for the tree-like resolution refutations generated by DPLL (Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962).
Naturally, our proofs are not separate for each of the combinations, but rather exploit and
exhibit some of their common features.
The practical significance of our theoretical results is, to some extent, debatable as
there is no direct connection between best-case resolution refutation size and empirical
SAT solver performance. Even a very large refutation may be easy to find if it mostly
consists of unit resolutions. Vice versa, just because a small refutation exists, that does not
mean the SAT solver will find it. This notwithstanding, it appears unlikely that best-case
resolution refutation size and practical SAT solver performance are completely unrelated
(beyond the obvious lower bound). One example that indicates the opposite are planning
graph mutexes. Mutexes do reduce the best-case refutation size by doing some of the work
before the resolution is even invoked.6 In other words, SAT solvers can exploit the mutexes
to prune their search trees more effectively. We are not aware of explicit empirical proof
that this tends to happen often, but there seems to be little doubt that it does. That is also
suggested explicitly by Kautz and Selman (Kautz & Selman, 1999; Long et al., 2000) by
ways of explaining the improved performacce of their system when run on graphplan-based
encodings.
An interesting situation arises in (all) our experiments. We use variable domain abstraction on the encoding with only action variables and only direct action mutexes (as
employed in SATPLANs IPC04 version). In this setting, resolution refutations can get
shorter in principle, although only by the effort it takes to recover the indirect action mutexes. Further, we employ some trivial post-abstraction simplification methods (such as
removing action duplicates) which, as we show, also have the potential to shorten resolution refutations. Still, as reported above, there is no discernible empirical improvement.
The reason might be that the SAT solver does not find the shorter refutations, or that such
shorter refutations do not actually appear on a significant scale. There is some evidence
indicating the latter: mutex recovery becomes necessary only in rather special situations,
where the abstraction turns an indirect mutex into a direct one. This will typically concern only a small fraction of the indirect mutexes. In addition, for both mutex recovery
and simplifications, in a well-designed variable domain abstraction the affected actions will
typically be irrelevant anyway. For example, with our hand-made Logistics abstraction,
5. The reason for complications is that answering this question requires determining, for planning-graph
based encodings in general, whether fact variables are only syntactic sugar or may lead to more succinct
refutations. Such a proof appears quite challenging; we say some more on this in Section 4.
6. General resolution can recover the mutexes effectively, c.f. the related investigations by Brafman (2001)
and Rintanen (2008). It does not seem likely that the same is the case for tree-like resolution, but to the
best of our knowledge this is not yet known.

422

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

the effect of the potential improvements is limited to actions appearing only in redundant
plans. We get back to this in detail in Section 4.
In our view, the theoretical results would be of potential importance even with no
evidence of empirical relevance, simply because they are quite surprising. After a moment
of thought, it is clear that resolution refutation does not become easier by ignoring goals.
However, variable domain abstraction in domains such as Logistics deflates state spaces
immensely, up to a point where they have only a tiny fraction of their original size. Before
performing this work, we would never have expected the best-case refutation size to remain
the same.
The paper is organized as follows. Section 2 discusses preliminaries, covering the employed notions of planning, planning graphs, propositional encodings, resolution, and abstraction methods; in particular, it formally defines variable domain abstraction. Section 3
summarizes our empirical results. Section 4 presents our results regarding resolution refutations in abstract CNF encodings. Related work is discussed during the text where appropriate. We conclude in Section 5. Appendix A contains most proofs, which are replaced
with brief proof sketches in the main body of the text. Additional empirical data can be
found in an online appendix (see JAIR web page for this article).

2. Preliminaries
We begin with a discussion of various concepts needed in the rest of the paper: propositional
STRIPS planning, planning graphs, propositional CNF encodings of planning problems,
resolution proofs of unsatisfiability, and abstraction methods used in planning. As a general
rule of notation, we will use variants of: P for planning tasks; F, A, and G for sets of facts,
actions, and goals, respectively;  for abstractions; P G for planning graphs; and  for
propositional formulas and encodings.
2.1 STRIPS and Planning Graphs
Classical planning is devoted to goal reachability analysis in state transition models with deterministic actions and complete information. Such a model is a tuple M = hS, s0 , SG , A, i
where S is a finite set of states, s0  S is the initial state, and SG  S is a set of alternative
goal states, A is a finite set of actions, and  : S  A  S is a transition function, with
(s, a) specifying the state obtained by applying a in s. A solution, that is a plan, for a
state transition model is a sequence of actions a1 , . . . , am from A that generate a sequence
of states s0 , . . . , sm such that, for 0  i < m, (si , ai+1 ) = si+1 , and sm  SG .
While AI planning targets large-scale state transition models with huge numbers of
states, these models are assumed to be described in a concise manner via some intuitive
declarative language. Here we use a propositional fragment of the STRIPS language (Fikes
& Nilsson, 1971). For brevity, we will refer to this fragment as STRIPS herein. Informally,
a planning task or planning instance in STRIPS consists of a set of propositional facts, some
of which hold initially and some of which must hold simultaneously at the end of the plan
execution. The state of the system at any time is defined by the set of propositional facts
that hold at that time. The task specifies a set of actions, each of which is defined by a
set of precondition facts, a set of facts that are added to the state, and a set of facts that
are removed from the state, if the action is taken. Formally, a STRIPS planning task is
423

fiDomshlak, Hoffmann, & Sabharwal

given by a quadruple P = (P, A, I, G) with fact set P , initial state description I  P , goal
description G  P , and action set A where for every action a  A we have pre(a), add (a),
and del (a), each of which is a subset of P . Such a planning task defines a state transition
model M = hS, s0 , SG , A, i with the state space S = 2P , the initial state s0 = I, and for
each s  S, we have s  SG iff G  s. For each s  S, A(s) = {a  A | pre(a)  s} are the
actions applicable in s, and for each a  A(s), we have (s, a) = (s \ del (a))  add (a). Here
we assume that the actions are reasonable in the sense that add (a)  del (a) = . This is
satisfied in most known planning benchmarks; in particular it is satisfied in all benchmarks
used in our experiments.7
Many planning algorithms, including SATPLAN, employ some form of approximate
reachability analysis. One of the primary tools for this purpose is the planning graph, first
introduced in the scope of the Graphplan planner (Blum & Furst, 1997). For a length bound
b, the planning graph P G(P) associated with P is a layered graph with two kinds of nodes:
fact nodes and action nodes. The layers alternate between fact layers F (0), F (1), . . . , F (b),
and action layers A(0), A(1), . . . , A(b  1), with each pair of layers F (t), A(t) forming a
time step t. The first vertex layer F (0) contains the initial state. A(t) and F (t + 1) for
0  t < b are the action sets and fact sets, respectively, available at time step t and t + 1.
More precisely, each A(t) includes all actions a  A where pre(a)  F (t) and no pair of
facts p, p0  pre(a) is mutex in layer t (c.f. below); further, A(t) contains the standard noop
action for every fact in F (t).8 Each F (t + 1) contains the union of the add effects of all
actions in A(t). Obviously, we have A(t)  A(t + 1) and F (t)  F (t + 1). The goal facts G
label appropriate vertices in F (b). P G(P) has four kinds of edges:
(1) Epre (t)  F (t)  A(t) connect the actions in A(t) with their preconditions in F (t),
(2) Eadd (t)  A(t)  F (t + 1) connect the actions in A(t) with their add effects in F (t + 1),
(3) Ea-mutex (t)  A(t)  A(t) capture a pair-wise mutual exclusion relation between actions in A(t); if (a(t), a0 (t))  Ea-mutex (t), then actions a and a0 cannot be applied
simultaneously at time t,
(4) Ef -mutex (t)  F (t)  F (t) capture a pair-wise mutual exclusion relation between facts
in F (t); if (f (t), f 0 (t))  Ef -mutex (t), then facts f and f 0 cannot hold together at time
t.
Note that P G(P) does not have explicit edges for the deletion effects of actions; these
effects are captured in the mutual exclusion relation (e.g., if p  add (a1 )  del (a2 ), then
(a1 , a2 )  Ea-mutex at all times). The mutex edges Emutex = Ea-mutex  Ef -mutex are
computed by an iterative calculation of interfering action and fact pairs (Blum & Furst,
1997). Namely, two actions (directly) interfere if the effects of one contradict the effects
of the other, or if one deletes a precondition of the other. Two actions have competing
7. In the IPC-2002 domain Rovers, some operators add and delete the same artificial fact in order to prevent
their parallel application. We implement this restriction via duplicating the respective operators and
sequentializing original and duplicate via two artificial facts. Similar fixes have been implemented in a
couple of other domains as well.
8. For a fact p  P , the associated noop(p) has no delete effects, and has {p} as both its preconditions and
add effects. These are dummy actions that simply propagate facts from one fact layer to the next.

424

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

needs if they have mutex preconditions. Combining these two scenarios together, we say
that two actions are mutex if they either (directly) interfere or have competing needs. In
a similar spirit, two facts are mutex if there is no non-mutex pair of actions (in the graph
layer directly below) together achieving both facts. A variant that will be of interest is the
planning graph in which, after the iterative computation, Ea-mutex is reduced to contain only
the directly interfering actions. We will call this the reduced planning graph, and denote it
with P Gred (P). The motivation for considering this is that, often, the reduced planning
graph results in much smaller SAT encodings; we get back to this below.
2.2 Propositional Encodings
We consider three CNF encodings used by (one or the other version of) SATPLAN, as well
as a fourth encoding that fits naturally into the picture. Each of the encodings takes as input
a planning task P with length bound b, and creates a formula in the standard Conjunctive
Normal Form (CNF). The CNF formula is then solved by an off-the-shelf SAT solver. This
process constitutes the basic step in a SAT-based approach to planning as implemented in
SATPLAN (Kautz & Selman, 1992, 1996, 1999), where one starts with b = 0 and iteratively
increments b until the CNF becomes satisfiable for the first time. The plan corresponding
to the satisfying assignment is then a plan with minimal b, and is hence optimal in that
sense.9
A CNF formula  is logically a conjunction (and) of clauses, where a clause is a disjunction (or) of literals, and a literal is a propositional (Boolean) variable or its negation.
CNF formulas are often written as a set of clauses, and each clause written as a set of literals, the underlying logical conjunction and disjunction, respectively, being implicit. Our
propositional encodings of bounded-length planning tasks are specified in terms of various
kinds of clauses generated by the encoding method.
Encoding (A) is constructed from P G(P) and uses the propositional action variables
{a(t) | 0  t < b, a  A(t)}. For each goal fact g there is a goal clause of the form
{a1 (b  1), . . . , al (b  1)}, where a1 , . . . al are the actions in A(b  1) that add g.
Similarly, for every a(t) with t > 0 and every p  pre(a) we have a precondition clause
{a(t), a1 (t  1), . . . , al (t  1)}, where a1 , . . . al are the actions in A(t  1) that add p.
Finally, we have a mutex clause {a(t), a0 (t)} for every t and (a, a0 )  Ea-mutex (t).
(Note here that the dependence on the initial state is taken into account already in
terms of which actions are contained in the sets A(t), and does not need to be stated
explicitly in the CNF.)
Encoding (B) is similar to (A) except that it uses variables (and appropriate clauses)
also for the facts. More specifically, in addition to the action variables, it has fact
variables {f (t) | 0  t  b, f  F (t)}. For each goal fact g, the goal clause is simply a
unit clause asserting g(b). For t > 0 and each fact f (t), we now have an effect clause
of the form {f (t), a1 (t  1), . . . , al (t  1)}, where a1 , . . . al are the actions in A(t  1)
that add f . For every a(t) and every p  pre(a) we have a precondition clause, which
9. While all versions of SATPLAN use this naive incremental update on b, it has been shown that there are
more clever strategies, exploiting the typical distribution of runtime over different values of b (Rintanen,
2004; Streeter & Smith, 2007).

425

fiDomshlak, Hoffmann, & Sabharwal

takes the form {a(t), p(t)}. We have action mutex clauses {a(t), a0 (t)} for every
t and (a, a0 )  Ea-mutex (t), and fact mutex clauses {f (t), f 0 (t)} for every t and
(f, f 0 )  Ef -mutex (t). Finally, for each fact f  F (0), we have an initial state clause
{f (0)} (these are not strictly necessary but are implemented in SATPLAN which is
why we include them here).
Encoding (C) is like (A) except that it is based on the reduced planning graph P Gred (P),
so that mutex clauses are present only for action pairs whose preconditions and effects
interfere directly.
Encoding (D) is like (B) except that, as in (C), it is based on P Gred (P), with mutex
clauses only for action pairs whose preconditions and effects interfere directly. Note,
however, that the fact mutexes are those of the full planning graph, P G(P).
All these encodings are reasonable ways of turning a planning graph into a CNF formula. The encodings essentially underly the competition implementations of SATPLAN.
We will detail this below. First, note that the different encodings have different benefits
and drawbacks. First, observe that the encodings are characterized by two decisions: (1)
Should we include all action mutexes from Graphplan, or only the direct interferences? (2)
Should we include only action variables, or both action and fact variables? Regarding (1),
the empirical observation that mutexes help was one of the major observations in the
design of SATPLAN (then called Blackbox) (Kautz & Selman, 1999; Long et al., 2000), in
particular in comparison to earlier encoding methods (Kautz, McAllester, & Selman, 1996).
On the other hand, since mutexes talk about pairs of facts and actions, the encodings may
become quite largethere will be one clause for every pair of mutex actions or mutex facts.
This is particularly critical for actions, of which in many planning benchmarks there are
thousands (compared to at most a few hundred facts). Indeed, it turns out that action mutexes often consume critically large amounts of memory. It is not uncommon to have CNF
formulas with millions of clauses, most of which are action mutexes (Kautz & Selman, 1999;
Kautz, 2004; Kautz et al., 2006). This motivates encodings (C) and (D). As for question
(2), this does not make as much of a difference, empirically, in most planning benchmarks.
We consider this distinction only because it was used in some versions of SATPLAN.
Let us say a few words to clarify exactly how encodings (A)(D) relate to the SATPLAN
literature and implementations. Due to the long history of SATPLAN, as well as a few
imprecisions in the literature, this is a little complicated. Our foremost reference is the
actual program code underlying SATPLAN04 and SATPLAN06, i.e., the most recent
versions used in the 2004 and 2006 competitions. The encoding methods in these versions
were implemented by one of the authors of this paper. There are four different encoding
methods: action-based, graphplan-based, skinny action-based, and skinny graphplan-based.
The action-based encoding is exactly (A), the graphplan-based encoding is exactly (B), and
the skinny graphplan-based encoding is exactly (D).10 The skinny action-based encoding is
like (C) except that, to save some runtime, the planning graph implementation does not
propagate mutexes (after all, only direct interferences are present in the final encoding),
effectively computing a relaxed planning graph (Hoffmann & Nebel, 2001). We use a normal
10. In the 2004 version, the skinny graphplan-based encoding did not feature fact mutexes. This is of no
consequence because that encoding was not used in the 2004 competition.

426

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

planning graph for (C) only for the sake of readabilitythe greater similarity to the other
encodings significantly simplifies the write-up, and our theoretical results hold as stated
also for relaxed planning graphs.
How did the SATPLAN encodings develop historically, how is this reflected in the literature, and which encodings were used in the competitions? We answer these questions to
the extent necessary for explaining encodings (A)(D). The original paper on SATPLAN
(Kautz & Selman, 1992) introduced rather different encodings. Graphplan-based encodings
with only direct action mutexes were introduced next, and observed to yield performance
comparable to Graphplan itself (Kautz & Selman, 1996). Subsequently, it was observed
that modern SAT solvers profit from full (fact and action) mutex relations and actually
beat other planners, in several domains (Kautz & Selman, 1999).11 Consequently, such
a graphlan-based encoding, i.e., our encoding (B), was used in the 1998 and 2000 competitions (Long et al., 2000). Prior to the 2004 competition, the encoding methods were
re-implemented, yielding the four methods explained above. The IPC04 booklet paper on
SATPLAN04 (Kautz, 2004) describes these four encodings.12 The version run in the competition is the skinny action-based encoding that is (for our theoretical results) equivalent
to encoding (C). When running the planner in IPC 2006, it turned out that having full
fact mutexes helped in some domains, and so encoding (D) was used instead (Kautz et al.,
2006). We consider encoding (A) for the sake of completeness.
2.3 Resolution Refutations
Our theoretical results are with respect to the resolution proof system (Robinson, 1965),
which forms the basis of most of the complete SAT solvers around today (cf. Beame, Kautz,
& Sabharwal, 2004). It is a sound and complete proof system, and has been studied extensively for theoretical and practical reasons. It works on CNF formulas and has only one
simple rule of inference: given clauses {A, x} and {B, x}, one can derive the clause {A, B}
by resolving upon the variable x. Here A and B are shorthands for arbitrary lists of literals.
Note that the choice of clauses to resolve is arbitrary, as long as they share a variable, with
opposite signs. A resolution derivation  of a clause C from a formula  consists of a series
of applications of the resolution rule starting from the clauses in  such that one eventually
derives C; when C is the (unsatisfiable) empty clause, {},  is called a resolution proof
(of unsatisfiability) or refutation of . The size of  is the number of applications of the
resolution rule in . When  is unsatisfiable, RC() denotes the resolution complexity of ,
i.e., the size of the smallest resolution proof of unsatisfiability of . We will be interested in
whether applying an abstraction to a planning task can convert its encoding into one that
has smaller resolution complexity.
A commonly studied sub-class of (still sound and complete) resolution derivations is
that of tree-like resolution derivations, where each derived clause is used at most once in
the whole derivation; the underlying graph structure of the proof is then a tree. Other
11. Kautz and Selman (1999) cite graphplan-based encodings from their earlier work (which used only action
mutexes). However, the Blackbox program code includes functions that generate full mutexes, and Kautz
and Selman explicitly emphasize the importance of these mutexes.
12. The paper is only an abstract and is a little imprecise in this description: initial state, goal, and fact
mutex clauses are not mentioned; the skinny action-based encoding is stated to be identical to our
encoding (C), i.e., based on a full planning graph rather than a relaxed planning graph.

427

fiDomshlak, Hoffmann, & Sabharwal

interesting sub-classes of resolution include regular resolution, which is provably exponentially more powerful than tree-like resolution and in which each variable is resolved upon
at most once in any path from the root to a leaf in the underlying proof graph, and ordered
resolution, where in addition variables respect a fixed ordering in any root-to-leaf path.
Tree-like resolution captures proofs of unsatisfiability generated by SAT solvers that are
based on the DPLL procedure (Davis & Putnam, 1960; Davis et al., 1962) but dont employ so-called clause learning techniques; the latter kind of SAT solvers can be provably
exponentially more powerful than even regular resolution although still within the realm of
general resolution (Beame et al., 2004).
We note that the arguments presented in this paper are for general (unrestricted) resolution. However, since most of aour constructions do not affect or rely on the structural
properties of the resolution refutations under consideration, the results hold as stated (except for a slight weakening in the case of Lemma 4.14) for all known variants of resolution
for which setting variables to True or False or replacing one variable with another preserves
proof structure. These variants include tree-like (DPLL), regular, and ordered resolution.
We state a standard property of resolution proofs which we will use in our arguments,
pointing out that certain modifications (such as variable restrictions and shortening of
clauses) of a given formula do not cause proofs to become longer with general resolution
or any of its natural sub-classes, including those mentioned above. Let x be a variable of
 and y be True, False, or another (possibly negated) variable of . The variable restriction
x  y on  is a transformation that replaces x with y throughout , and simplifies the
resulting formula by removing all clauses containing True or a variable and its negation, and
by removing False or duplicate literals from clauses. In other words, a variable restriction
involves fixing the value of a variable or identifying it with another literal, and simplifying
the formula. If  is a sequence of variable restrictions on , then by | we denote the
outcome of applying  to .
The following proposition combines two basic facts together in a form that will be useful
for us: (1) variable restrictions cannot increase the resolution complexity of a formula, and
(2) lengthening clauses and/or removing clauses cannot decrease the resolution complexity
of a formula.
Proposition 2.1. Let  and  be CNF formulas. If there exists a sequence  of variable
restrictions on  such that every clause of  | contains as a sub-clause a clause of , then
RC()  RC( ).
Some explanation of this proposition and how we will use it is in order. The notation
used here is chosen to match the way we will eventually utilize this proposition in our
proofs; more on this below. The conditions in the proposition imply that one may obtain
 from  by applying the restriction  , possibly throwing away some literals from some
clauses, and possibly adding new clauses. Intuitively, each of these three modifications to
 can only reduce the number of solutions and cannot make it any harder to prove the
formula unsatisfiable. This property of resolution refutations of propositional formulas has
been previously used (at least indirectly) in various contexts. For completeness, we include
a proof in Appendix A, based on folklore ideas from proof complexity literature. An
alternative proof, with a somewhat different notation, may also be found in the appendix
of a recent article by Hoffmann, Gomes, and Selman (2007).
428

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

The way we will use this proposition is the following:  will be the CNF encoding of
an abstracted planning task,  will be the encoding of the original planning task, and 
will be a carefully chosen restriction of  that will bring our focus to variables already
appearing in . Proposition 2.1 will then imply that the original encoding is no harder to
refute, using resolution or its natural sub-classes, than the abstracted encoding.
2.4 Abstraction in Planning
Abstraction methods of various kinds have been put to use in planning, often quite successfully. One line of work uses abstraction methods for problem decomposition (cf. Sacerdoti,
1973; Knoblock, 1990; Koehler & Hoffmann, 2000). To the best of our knowledge, our
approachexamining the abstract state space in order to prove the absence of solutions
has not been pursued before. The line of work most relevant to ours is the work on domainindependent heuristic functions (cf. McDermott, 1999; Bonet & Geffner, 2001; Hoffmann &
Nebel, 2001; Edelkamp, 2001; Haslum et al., 2007; Helmert et al., 2007; Katz & Domshlak,
2008). There, abstraction means over-approximation of the state space, as in our work;
what differs is how the abstractions are used. Of course, the kinds of over-approximations
that are useful for either purpose can differ a lot. To use abstraction as we do in this
paper, one has to define over-approximations that preserve, to a very large extent, the real
structure of the problem. In particular, our ideal goal is to find abstractions that preserve the length of an optimal solutionsomething one definitely wouldnt expect from the
abstraction underlying a heuristic function, since that has to be solved in every search state.
We briefly review some of the over-approximation methods that have been used in
planning so far; we then formally introduce our novel one, variable domain abstraction. We
use the Logistics domain as an illustrative example.13
One wide-spread over-approximation method in planning is the 2-subset relaxation
underlying the computation made in a planning graph (Blum & Furst, 1997), which is generalized to an m-subset relaxation by Haslum and Geffner (2000). In a nutshell, one assumes
that achieving a set of facts is only as hard as achieving its hardest m-subset. It is known
that, in most domains, including Logistics, the 2-subset solution length (corresponding to
the length of a planning graph constructed up to the first fact layer containing no mutexes
between the goal facts) is typically strictly lower than the length of an optimal plan. For
m > 2, on the other hand, computing m-subset solution length is typically too costly, and
still, for no m = o(|P |) can one typically guarantee solution length preservation (Helmert
& Mattmuller, 2008).
A second wide-spread over-approximation method is ignoring delete lists (McDermott,
1999; Bonet & Geffner, 2001). For this approximation, one simply removes (some of)
the negative effects of the actions. If all negative effects are removed, then the problem
becomes solvable in time linear in the instance size. The latter is the basis of the heuristic
functions used in many modern planners (cf. Bonet & Geffner, 2001; Hoffmann & Nebel,
2001; Gerevini, Saetti, & Serina, 2003). Ignoring deletes is very likely to introduce shorter
solutions. For example, in the Towers of Hanoi problem, it leads to plans of length n instead
of 2n  1 (Hoffmann, 2005). In Logistics, if one ignores the deletes of moving actions then
13. As stated, an open topic is to explore model checking abstractions, in particular predicate abstraction
(Graf & Sadi, 1997; Clarke et al., 2003), instead of planning abstractions.

429

fiDomshlak, Hoffmann, & Sabharwal

the plans may get shorter because vehicles never have to move back in the abstraction.
Interestingly, ignoring the deletes of load/unload does not decrease plan length, since these
actions never have to be undone in an optimal plan. We use this observation in some of our
experiments.
A third abstraction was introduced by Edelkamp (2001) for his pattern database
heuristic. For this approximation, one completely removes some facts from the problem
description, notably facts corresponding to all values of some multi-valued variables. If
enough facts are removed, the task becomes sufficiently simple. Obviously, this approximation will hardly be solution length preserving. In Logistics, if we remove, for example,
a fact at(package1,airport2) then, in particular, package1 can be loaded onto an airplane
at airport2 without actually being there because that precondition is removed. An optimal
plan can now just make the package pop up anywhere.
A fourth abstraction, finally, involves removing some preconditions (Sacerdoti, 1973)
and/or goal facts. As with Edelkamps abstraction, this cannot be expected to be solution
length preserving in interesting cases.
The above calls for a new abstraction method, which we designed following Hernadvolgyi
and Holte (1999). Considering STRIPS-like state transition systems with multiple-valued
(instead of Boolean) variables, they propose to reduce variable domains by not distinguishing between certain values. For example, if the content of a cell in the 8-puzzle can be in
{blank , 1, 2, 3, 4, 6, 7, 8} then that may be replaced with {blank , 1, 2, 3} where {3, . . . , 8} are
all mapped onto 3. Our observation is that, in many planning benchmarks, this can be done
without introducing shorter plans. For example, in Logistics it is unnecessary to distinguish
the positions of packages in irrelevant cities. Therefore, we can replace the domain of at(p),
{A1 , A2 , B1 , B2 , C1 , C2 , . . . }, where A and B are the initial and goal cities of p and Ai , Bi , . . .
are locations in cities A, B, . . ., with an abstract domain {A1 , A2 , B1 , B2 , C1 }. In STRIPS,
this amounts to replacing a set of irrelevant facts at(p, l) with the single fact at(p, C1 ). We
now formalize this idea.
Let persistently mutex denote the standard notion that two facts are mutex in the
fixpoint layer of a planning graph: typically, different values of a multiple-valued variable.
Definition 2.2. Let P = (P, A, I, G) be a STRIPS planning task, p, p0  P a pair of
persistently mutex facts such that, for all a  A, we have ({p, p0 }  del (a))  pre(a). Then
((P ), {(a)|a  A}, (I), (G)) is called a variable domain abstraction of P, where  is
defined as follows:
1. For a fact set F , (F ) = F if p0 6 F ; otherwise, (F ) = (F \ {p0 })  {p}.
2. For an action a = (pre, add , del ), (a) = ((pre), (add ), (del )) if p 6 (add )(del );
otherwise, (a) = ((pre), (add ), (del ) \ {p}).
In words, Definition 2.2 simply says that we replace p0 with p. If p now appears both
in the add list and in the delete list of an action, we remove it from the delete list.14 This
situation will arise, for instance, if the action moves a package from one irrelevant position
14. The reader may wonder why p remains in the add list, although by prerequisite p  (pre). The reason
is that we distinguish between abstractions and simplifications: both change the planning task;
abstractions, but not simplifications, do so in a way that may alter the tasks semantics. However,
simplifications may as well affect resolution complexity. We will get back to this later in the paper.

430

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

to another irrelevant position. After the operation, p is equivalent to what was originally
p  p0 : p is True after an abstracted action sequence if and only if p  p0 is True after
the corresponding real action sequence. In particular, Proposition 2.3 states that variable
domain abstraction is an over-approximation in the usual sense.
Proposition 2.3. Let P = (P, A, I, G) be a STRIPS planning task, and let (P) =
((P ), {(a) | a  A}, (I), (G)) be a variable domain abstraction of P. Then, whenever
ha1 , . . . , an i is a plan for P, h(a1 ), . . . , (an )i is a plan for (P).
Proof. Let M and M be the state models induced by P and (P). First, let us show
that, for each state s  M, and each action a  M, if a is applicable in s, then (i)
(a) is applicable in (s), and (ii) p  ((s), (a)) if and only if {p, p0 }  (s, a) 6= .
Note that, since the abstraction  has no effect on facts other than {p, p0 }, (ii) implies
((s), (a)) = ((s, a)). Thus, together, (i) and (ii) imply that (iii) M is homomorphic
to M . Finally, it is straightforward from Definition 2.2 that (iv) the initial state in M
is (I), and that the goal states in M are exactly {(s) | s  SG }. Together, (iii) and (iv)
imply the claim of Proposition 2.3.
Let a = (pre, add , del ). The applicability of (a) in (s) is straightforward; if p0 6 pre,
then we have (pre) = pre and (s)(pre) = spre, and otherwise (pre) = (pre\{p0 }){p}
and (s) = (s \ {p0 })  {p}. In both cases, pre  s implies (pre)  (s). Consider now the
sub-claim (ii) on a case-by-case basis.
{p0 , p}  add = , {p0 , p}  del =  We have p 6 add ((a)) and p 6 del ((a)), and thus
p  ((s), (a)) iff p  (s) iff {p, p0 }  s 6=  iff (see assumption on del in the case)
{p, p0 }  (s, a) 6= .
{p0 , p}  add 6= , {p0 , p}  del =  We have p  add ((a)) and p 6 del ((a)), and thus
p  ((s), (a)). On the other hand, {p, p0 }  (s, a) 6=  also trivially holds here.
{p0 , p}  add = , {p0 , p}  del 6=  We have p 6 add ((a)) and p  del ((a)), and thus
p 6 ((s), (a)). On the other hand, since p, p0 are persistently mutex facts in P, and
{p0 , p}  del = {p0 , p}  pre, we also have {p, p0 }  (s, a) = .
{p0 , p}  add 6= , {p0 , p}  del 6=  We have p  add ((a)) and p 6 del ((a)), and thus
p  ((s), (a)). On the other hand, from add  del =  and {p0 , p}  add 6=  we
immediately have {p, p0 }  (s, a) 6= .
This completes the proof of (ii).
Arbitrarily coarse variable domain abstractions may be generated by iterating the application of Definition 2.2. Note that variable domain abstraction is a refinement of Edelkamps
(2001) abstractioninstead of acting as if the irrelevant positions could be totally ignored,
we do distinguish whether or not the package currently is at such a position. This makes
all the difference between preserving optimal solution length or not.15
As hinted above, after variable domain abstraction we may be able to apply further
simplifications. A simplification, in our terminology, is similar to an abstraction in that it
15. A topic for future work is to explore whether the refined abstraction can lead to better pattern database
heuristics for STRIPS problems.

431

fiDomshlak, Hoffmann, & Sabharwal

manipulates a planning task at the language level. However, while abstractions may alter
the tasks semantics, simplifications do not; i.e., simplifications do not introduce any new
transitions or goal states. Concretely, we consider two simplifications. A planning task
P = (P, A, I, G) has duplicate actions if there exist a, a0  A so that pre(a) = pre(a0 ),
add (a) = add (a0 ), and del (a) = del (a0 ).16 The simplified planning task is like P except
that a0 is removed. A planning task P = (P, A, I, G) has irrelevant add effects if there
exists a  A so that pre(a)  add (a) 6= . The simplified planning task is like P except that
we remove pre(a) from add (a).
Obviously, duplicate actions and irrelevant add effects may arise as an outcome of variable domain abstraction. An example of the latter is an action moving a package from an
irrelevant location into an irrelevant truck. An example of the former are two actions loading
a package onto an airplane from distinct but irrelevant locations.17 In our implementation,
we have a simple post-abstraction processing in which we perform all these simplifications.
As we shall see in Section 4.3, the simplifications can lead to decreased resolution complexity, thereby offsetting our result that abstractions as such, in many cases, cannot. It
may seem a little artificial to distinguish abstractions and simplifications in this way, seeing
as many abstractions are bound to enable us to simplify. However, note that this distinction
only serves to identify the borderline between what can, and what cannot, reduce resolution
complexity. Anyhow, as we shall see in the next section, abstraction does not tend to help
empirically with the performance of SATPLAN even with post-abstraction simplifications.

3. Empirical Results
We have performed a broad empirical evaluation of the effect of abstractions on the efficiency
of optimizing planning algorithms. We mostly focus on variable domain abstraction, as in
Definition 2.2, since it is clearly most promising for obtaining solution length preserving
abstractions.
Section 3.1 explains the specific variable domain abstractions we employ in our experiments. Section 3.2 explains the experimental setting and how we chose to present the huge
data set of the results. Section 3.3 describes our experiments with variable domain abstraction in the IPC benchmarks, and Section 3.4 discusses our results with domain-specific
abstractions of hand-made instances in certain benchmark domains where the amount of
irrelevance can be controlled. Section 3.5 briefly summarizes our findings with abstraction
methods other than variable domain abstraction.
3.1 Variable Domain Abstractions
We designed three different methods to automatically generate variable domain abstractions. The methods as listed below are based on increasingly conservative approximations
16. Note that we define actions not to be triples of pre, add , del , but to have these components; hence
two actions with identical pre, add , del may be contained in the set A. This reflects practical planner
implementations, where actions have names and/or unique IDs, and checks for duplicate actions are not
usually performed.
17. In a similar fashion, duplicate actions may arise as an outcome of Edelkamps (2001) pattern database
abstraction.

432

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

of relevance. As is common in relevance approximations (cf. Nebel et al., 1997), the algorithmic basis is, in all cases, a simplified backchaining from the goals.
1Support starts in the first layer of a planning graph that contains all goal facts (possibly
with mutexes between them). For each goal fact in that layer, it selects one achiever in
the preceding action layer and marks the preconditions of that action as new sub-goals;
then the process is iterated for the created sub-goals.
AllSupports proceeds similarly to 1Support except that it selects all achievers for each
(sub-)goal.
AllSupportsNonMutex proceeds similarly to AllSupports except that it starts the backchaining at the first plan graph layer that contains the goals without mutexes.
In all three methods, the selected set of relevant facts R  P is taken to be the set
of goals and sub-goals created during the backchaining. This set of facts is turned into
a variable domain abstraction as follows. First, we compute a partition of the problems
fact set P into subsets P1 , . . . , Pk of pairwise persistently mutex facts. We take these
subsets to correspond to the underlying multiple-valued variables (e.g., the position of a
package). Then we perform abstraction within those Pi where not all values are relevant,
i.e., Pi \ R 6= . Within each such subset Pi , we arbitrarily choose one irrelevant fact p, i.e.,
p  Pi \ R. We then replace all other irrelevant facts, i.e., all q  Pi \ R where q 6= p, with
p.
As an example, in Logistics, 1Support abstracts away all in(p, v) facts for each package p except for those vehicles v that were selected as a supportin particular, a single
airplane. In contrast, AllSupports will mark in(p, v) as relevant for all airplanes v unless
some special case applies (e.g., p must be transported within its origin city only). Finally,
AllSupportsNonMutex is even more conservative and covers some of the special cases in
which AllSupports abstracts an in(p, v) fact away. Note that identifying positions inside
airplanes with positions outside airplanes may well affect the length of an optimal plan.
In addition to the domain-independent, automatic variable domain abstractions, for six
IPC domains we have designed domain-specific solution length preserving variable domain
abstractions by hand. For Logistics, the domain-specific abstraction was explained in the
introduction. For Zenotravel, we use a similar abstraction exploiting irrelevant object positions. In Blocksworld, on(A, B) is considered irrelevant if B is neither the initial nor the
goal position of A, and B is initially clear.18 For Depots, which is a combination of Logistics
and Blocksworld, our abstraction is a combination of the two individual abstractions. For
Satellite, our abstraction performs a simple analysis of goal relevance to detect directions
that are irrelevant for a satellite to turn to. A direction is relevant only if it is the satellites initial direction, its goal direction, or a potential goal or camera calibration target.
Similarly, in Rovers, a waypoint (location) is considered relevant for a rover only if it is
either the initial position, or it is relevant for a needed rock sample/soil sample/image, or
it necessarily lies on a path the rover must traverse to reach some other relevant location.
18. The last of these conditions is necessary to avoid the possibility of clearing a block C by moving A
away from C although A is actually placed on some third block.

433

fiDomshlak, Hoffmann, & Sabharwal

3.2 Experiment Setup and Presentation
The presented data were generated on a set of work stations running Linux, each with a Pentium 4 processor running at 3 GHz with 1 GB RAM. We used a time cutoff of 30 minutes.
We experimented with the plan-length optimizing planners SATPLAN04, IPP (Koehler,
Nebel, Hoffmann, & Dimopoulos, 1997), and Mips-BDD (Edelkamp & Helmert, 1999).19
Our choice of SATPLAN04 rather than SATPLAN06 is arbitrary, except that, by using
the naive encoding (C), the resolution best case of SATPLAN04 can be improved by variable domain abstractionmaking our bad empirical results below even more significant.
Note also that, although SATPLAN06 could be considered more recent, it contains no
developments beyond SATPLAN04, other than switching back to an older version of the
encoding method.
As test examples, we took, with few exceptions listed below, all STRIPS domains used
in all international planning competitions until and including IPC-2004. Precisely, we use
(IPC-2004) Airport, Dining Philosophers, Optical Telegraph, Pipesworld NoTankage,
Pipes- world Tankage, and PSR.
(IPC-2002) Depots, Driverlog, Freecell, Rovers, Satellite, and Zenotravel.
(IPC-2000) Blocksworld and Logistics. (Miconic-STRIPS is just a very simple version of
Logistics, Freecell is part of our IPC-2002 set.)
(IPC-1998) Grid, Mprime, and Mystery. (Movie is trivial, in Gripper variable domain
abstraction cannot preserve solution length, Logistics is part of our IPC-2000 set.)
Our measurements are aimed at highlighting the potential that abstraction in principle
has of speeding up the computation of information about a task. Concretely, given a
planning task T , we create an abstract version T  of T , and run a planner X on it. There
are three possible outcomes:
(1) X finds a plan for T  , an abstract plan, and it happens to be a real plan (that is, a plan
for T ). We record the time taken to find the plan, along with the time taken by X to
find a plan given the original task T .
(2) X finds a plan for T  that is not a real plan. Since all our planners optimize plan
length, the information we still gain is the length of the optimal abstract plan, which
is a lower bound on the length of the real plan. We record the time taken to compute
that bound (for example, for SATPLAN04, the time taken up to the last unsatisfiable
iteration), along with the time taken by X to compute the same lower bound given the
original task T .
(3) X runs out of time or memory. In this case, one could record the time taken up to the
last lower bound proved successfully. For the sake of readability, we omit this here and
consider only cases (1) and (2) above.
19. While SATPLAN04 and IPP optimize step-length of the plan, Mips-BDD optimizes sequential plan
length. However, again, as the performance of the planners does not stand for a comparative evaluation
here, we refer to all three simply as plan-length optimizing planners.

434

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Note that, in the spirit of being optimistic about the usefulness of abstraction, we do not
include the time taken to create the abstract task T  . Note also that we actually obtain
several results for each pair T and X, namely one result for every particular variable domain
abstraction. For the sake of readability, we do not include these distinctions in the results
(the distinctions are mostly inconclusive and uninteresting anyway), and instead present
the results from the following best abstraction perspective. We skip abstract tasks that
were either not solved, or that are not abstract since all facts are considered relevant.
If no abstract task remains, we skip the instance. Otherwise, we select the abstract task
providing the best information about the instance: the best case is that the abstract plan
is real, else we select the highest lower bound.20 If there are several abstractions providing
the same best information, we choose the one with lowest runtime.
3.3 IPC Benchmarks
Due to the sheer size of our experiments3 planners multiplied with 17 domainsdiscussing
the entire result set is neither feasible nor would it be useful. An online Appendix (see JAIR
web page for this article) contains detailed data for the three optimal planners. Herein,
we provide a summary analysis showing the main points, with a particular focus on SATPLAN04.
Detailed data for SATPLAN04 on 4 of our 17 IPC domains is given in Table 1:
 Depots and Satellite are selected into the table because they are the only 2 of our 17
domains where the abstraction brings a somewhat significant advantage.
 Logistics is selected because it is our illustrative example.
 PSR is selected due to being an interesting caseunusually, the current optimal planners do just as well (or badly) on PSR as the current sub-optimal (satisficing) planners.
In each domain, we selected the 13 most challenging instances, where challenging is
measured as the runtime taken in the original task. Note that this problem-instance selection
criterion for our presentation is also optimistic from the point of view of abstraction. For
each instance, Table 1 first specifies whether the found abstract plan was a real plan or not.
This characterizes the problem instance in terms of cases (1) and (2) explained above, and
the corresponding runtimes of SATPLAN04 on abstract and real tasks are then given by
the rows ta and tr . The table then specifies the lower bound lg proved for the real task by
its planning graph (that is, t if F (t) is the first fact layer to contain all goal facts with no
mutexes between them), the lower bound la proved by SATPLAN04 in the abstract task,
and, finally, the actual length lr of the optimal plans for the real task. The last row RelFrac
in the table specifies the percentage of facts considered relevant.
For Depots, the best-case data shown in Table 1 is scattered across all four kinds of
variable domain abstractions, with the automatic abstractions being sometimes more and
sometimes less aggressive than our handmade abstraction. For example, instances numbers
11 and 15 have their best case with the very aggressive 1Support strategy. Most of the time
20. Note here that the quality of the information is essential. If the abstraction only tells us that the plan
must have at least n  1 steps, and the real plan length is n, then we must still prove the bound n, which
typically takes more time than all other bounds together.

435

fiDomshlak, Hoffmann, & Sabharwal

Index
IsReal?
ta
tr
lg , l a , L
RelFrac
Abs

Index
IsReal?
ta
tr
lg , l a , L
RelFrac
Abs

Index
IsReal?
ta
tr
lg , l a , L
RelFrac
Abs

Index
IsReal?
ta
tr
lg , l a , L
RelFrac
Abs

2
Y
64.43
74.26
6,12,12
37%
1S

16
N
0.63
0.8
5,15,15
47%
ASnm

10
Y
0.99
2.14
10,15,15
33%
1S

3
Y
73.29
45.77
11,12,12
88%
AS

4
Y
112.30
104.05
6,10,10
91%
HM

22
Y
64.29
71.86
7,25,25
75%
ASnm

13
Y
91.43
75.28
9,13,13
48%
ASnm

4
Y
429.74
472.01
12,14,14
88%
AS

5
Y
53.06
51.53
4,7,7
95%
HM

29
Y
11.16
12.05
7,18,18
79%
ASnm

14
Y
25.41
32.87
9,12,12
48%
ASnm

7
Y
10.44
12.25
7,10,10
77%
ASnm

8
Y
310.76
250.08
4,8,8
90%
HM

31
N
1.04
8.38
5,16,16
49%
ASnm

15
N
70.57
68.12
9,13,13
55%
ASnm

8
N
228.30
22.52
9,13,14
76%
AS

9
Y
34.35
40.65
4,6,6
84%
HM

33
N
0.93
1.33
5,16,16
48%
ASnm

16
N
111.66
75.79
9,13,13
47%
ASnm

10
Y
47.77
33.11
8,10,10
87%
ASnm

Depots
11
13
N
Y
49.73
9.07
2.13
12.42
13,10,?
9,9,9
27%
85%
1S
ASnm
Logistics
17
18
N
N
150.37
770.44
106.61
642.97
9,13,14 9,15,15
44%
42%
ASnm
HM
PSR
36
37
Y
Y
2.44
2.4
4.96
2.26
8,16,16 7,19,19
90%
60%
ASnm
ASnm
Satellite
10
11
Y
Y
168.03
84.47
176.47
160.03
4,8,8
4,8,8
85%
74%
HM
HM
12
Y
874.84

6,14,14
76%
HM

40
N
0.9
6.39
5,14,15
48%
ASnm

19
N
684.19
672.25
9,15,15
30%
HM

14
N
118.90
12.72
9,9,?
50%
HM

13
Y
931.17

4,13,13
76%
HM

42
N
0.73
0.87
5,16,16
53%
ASnm

20
N
820.59
721.73
9,15,15
33%
HM

15
N
56.83
4.45
10,8,?
20%
1S

14
Y
256.16
425.44
4,8,8
78%
HM

47
Y
17.75
17.41
4,23,23
47%
ASnm

21
N
615.01
430.95
9,14,?
48%
AS

16
Y
13.81
6.54
8,8,8
89%
ASnm

15
Y
282.79
429.81
4,8,8
75%
HM

48
Y
125.49
131.94
7,26,26
80%
ASnm

22
N
929.64
721.82
9,15,?
28%
HM

17
Y
18.4
17.41
6,7,7
58%
ASnm

17
Y
65.79
152.37
4,6,6
67%
HM

49
N
3.02
3.11
8,19,?
37%
1S

23
N
965.49
769.36
9,15,?
43%
AS

19
Y
460.75

8,10,10
92%
AS

18
Y
112.50
217.76
4,8,8
74%
HM

50
Y
0.59
1.03
4,16,16
30%
ASnm

39
N
1.1
1.9
9,8,?
14%
1S

21
N
342.20
55.70
7,7,7
51%
HM

Table 1: Full results, in some selected domains, for SATPLAN04 and variable domain abstraction (best-of, see text). Notations: Index:
index (number) of instance in respective IPC suite; IsReal: whether the abstract plan real (Y) or not (N); L: the length of the
optimal plan (? if not known), lg : lower bound on plan length proved by planning graph, la lower bound proved in abstract task;
ta : runtime (secs) needed to prove the lower bound la in abstract task; tr runtime (secs) needed to prove the same lower bound la
but in real task; RelFrac: fraction of facts considered relevant; dashes: time-out; Abs: is the corresponding form of variable
domain abstraction, 1Support (1S), AllSupports (AS), AllSupportsNonMutex (ASnm), and handmade (HM).

436

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Domain
Airport
Blocksworld
Depots
Dining Philosophers
Driverlog
Freecell
Grid
Logistics
Mprime
Mystery
Optical Telegraph
Pipesworld NoTankage
Pipesworld Tankage
PSR
Rovers
Satellite
Zenotravel

Index
20
7
19
29
13
1
2
23
5
20
13
12
8
48
8
12
13

ta
33.9
118.3
460.8
6.2
342.0
0.8
96.4
965.5
6.2
180.7
43.6
521.7
393.8
125.5
74.6
874.8
338.4

SATPLAN04
tr
IsReal?
lg , l a , l r
29.5
Y
25,32,32
11.2
Y
16,20,20

Y
8,10,10
5.5
Y
7,11,11
113.7
N
9,11,12
0.8
Y
4,5,5
3.1
N
19,13,?
769.4
N
9,15,15
5.2
Y
6,6,6
112.4
Y
7,7,7
32.0
N
11,13,13
455.5
Y
8,14,14
143.4
N
5,6,?
131.9
Y
7,26,26
97.8
Y
5,9,9

Y
6,14,14
244.8
N
4,7,7

RelFrac
73%
47%
92%
71%
61%
82%
10%
43%
78%
76%
53%
86%
89%
80%
87%
76%
67%

Table 2: Results for SATPLAN04 with the best-case variable domain abstraction on the
most challenging successful instances of each domain. Notation as in Table 1.

the runtime is better on the original task, yet there are a few cases where the abstraction
brings a quite significant advantage. Most notably, in instance number 19 SATPLAN04
runs out of time on the original task, but solves the abstract task, finding a real plan, within
a few minutes. In Logistics, the best-case data is mostly, though not exclusively, due to the
conservative AllSupportsNonMutex and our handmade abstractions. The abstract runtime
is worse in all but three cases (nos. 10, 14, 39), where it is slightly better. In PSR, the best
cases are almost exclusively due to the conservative AllSupportsNonMutex abstraction. As
for runtimes, abstraction is usually faster, but only marginally. Satellite is the only one
of our 17 domains where abstraction brings a significant (and largely consistent) runtime
advantage. The best cases are almost exclusively due to our hand-made abstraction. All
abstract plans are real plans, often found significantly faster than for the original task. It is
unclear to us why the results are good in Satellite, but, for example, not in Logistics, where
the state space reduction is much larger.
Next, Table 2 provides an overview of the results for SATPLAN04 in our 17 IPC
domains. To make data presentation feasible, we select just one instance per domainthe
most challenging successful instance. By successful, we mean that at least one abstract
task of that instance was solved (abstract plan found), and this abstract task was indeed
abstract (not all facts relevant). By challenging, we mean maximum runtime on the original
task.21
21. Another strategy would be to select the task that maximizes tr  ta , the time advantage given by
abstraction. However, in most cases this strategy would select a trivial instance: namely, because tr  ta
is consistently negative, and maximal in the easiest tasks.

437

fiDomshlak, Hoffmann, & Sabharwal

Domain
Airport
Blocksworld
Depots
Dining Philosophers
Driverlog
Grid
Logistics
Mprime
Mystery
Optical Telegraph
Pipesworld NoTankage
PipesworldTankage
PSR
Rovers
Satellite
Zenotravel

Index
8
7
17
5
9
1
8
2
2
2
5
7
10
6
7
12

ta
67.9
3.1
254.4
170.4
1.1
0.1
0.2
0.6
0.4
15.7
0.0
32.2
0.0
592.5
100.3
344.3

tr
0.3
0.0
268.4
138.0
0.7
0.2
1.0
1.0
0.7
5.2
0.0
0.6
0.0
375.7
2010.7
322.4

IPP
IsReal?
Y
Y
Y
Y
N
N
Y
N
N
N
Y
N
N
N
Y
Y

lg , l a , l r
25,26,26
16,20,20
6,7,7
7,11,11
7,10,10
14,7,14
9,11,11
5,4,5
5,4,5
11,13,13
4,6,6
4,5,6
5,4,5
7,12,12
4,6,6
4,6,6

RelFrac
77%
47%
58%
71%
84%
43%
49%
62%
60%
53%
88%
82%
37%
90%
87%
67%

Table 3: Similar to Table 2, but for the IPP planner.

It is useful to discuss the 17 domains in groups with similar behavior. Depots, Logistics,
PSR, and Satellite have already been discussed. In each of Airport, Dining Philosophers,
Driverlog, Mystery, Mprime, Optical Telegraph, Pipesworld NoTankage, and Pipesworld
Tankage, SATPLAN04 runtimes are consistently lower on the original tasks, with few exceptions mostly among the easiest instances. The picture is less consistent but qualitatively
similar in Zenotravel. The degree of the advantage varies. It is relatively moderate in
Dining Philosophers (up to 7% less runtime on original task), Optical Telegraph (up to
23%), Airport (up to 28%), Pipesworld Tankage (up to 28%), and Mprime (up to 36%); it
is much stronger in Zenotravel (up to 75%), Mystery (up to 80%), Driverlog (up to 89%),
and Pipesworld NoTankage (up to 92%).
In Rovers, the runtime results are inconclusive, with minor advantages for abstract or
real depending on the instance. In Blocksworld, SATPLAN04 solves abstract tasks with
up to 7 blocks only, independently of the abstraction used; we dont know what causes this
bad behavior. In Freecell, most of the time AllSupports and AllSupportsNonMutex do not
abstract anything, and in all abstractions generated with 1Support, SATPLAN04 runs out
of time, leaving instance number 1 as the only successful case, shown in Table 2. In Grid,
finally, the IPC 1998 test suite contains only 5 instances, which become huge very quickly.
SATPLAN04 can solve (abstract or real) only instances numbers 1 and 2, and the latter is
shown in Table 2.
Tables 3 and 4 provide a similar snapshot on the results with IPP and Mips.BDD,
respectively. The picture for IPP is, roughly, similar to that for SATPLAN04. The main
difference is, in fact, that IPP is a weaker solver than SATPLAN04 in many domains, to
the effect that some more domains contain no interesting data. Specifically, in Driverlog,
Mprime, Mystery, Pipesworld NoTankage, and PSR, IPP either solves the instances in no
time, or not at all. Like for SATPLAN04, we see an advantage for abstraction in Depots
438

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Domain
Airport
Blocksworld
Depots
Dining Philosophers
Driverlog
Freecell
Grid
Logistics
Mprime
Mystery
Optical Telegraph
Pipesworld NoTankage
Pipesworld Tankage
PSR
Rovers
Satellite
Zenotravel

Index


2

ta


1.1

Mips.BDD
tr
IsReal?





Y

lg , l a , l r


7,15,15

RelFrac


81%

10

1
12



503.6

1.8
7.2










Y

N
Y



5,17,17

14,7,?
10,42,42



84%

43%
43%



25
7

0.7
142.6

13.5
340.6

Y
Y

4,9,9
5,18,18

37%
86%

11

271.0



Y

4,14,14

66%

Table 4: Similar to Tables 23, but for the Mips.BDD planner.

and Satellite, and we note that for Satellite this difference is consistently huge. We also
see a vague advantage for abstraction in Logistics. For Mips.BDD, even more domains
gave no meaningful data. In the domains dashed out in Table 4, Mips.BDD runs out of
time on even the smallest instances. In the domains left empty, we either could not run
Mips.BDD for some technical reasons, or it stopped abnormally. In the remaining data
set of 7 domains, however, our abstractions (as expected) bring a consistent advantage for
Mips.BDD. In particular, consider the behavior in Logistics, Rovers, and Zenotravelin
these domains, Mips.BDD is vastly improved by abstraction while SATPLAN04 and IPP
are more or less inconclusive.
3.4 Constructed Benchmarks
The above has shown that the use of abstractionof variable domain abstraction, at least
to speed up state of the art planning systems varies from quite promising for Mips.BDD
to rather hopeless for SATPLAN04. We ran a number of focused experiments to examine
the more subtle reasons for this phenomenon. These experiments have been done on three
IPC benchmarksLogistics, Rovers, and Zenotravelwhere the results on the IPC test
suites are relatively bad, although we are in possession of hand-made abstractions. We
wanted to test what happens when we scale the instances on irrelevance. The respective
experiment for Logistics, Figure 1, was discussed in the introduction. For Rovers, we tried
a large number of instance size parameters, and even minor modifications of the operators,
but we could not find a setting that contained a lot of irrelevance and was challenging for
SATPLAN04 and IPP. In short, it appears that the Rovers domain is not amenable to
abstraction techniques. For Zenotravel, we obtained the picture shown in Figure 2.
439

fiDomshlak, Hoffmann, & Sabharwal

10000

100

10000

100

90
1000

90

80

1000

80

70
100

70

60

100

60

50
10

50

40

10

40

30
1

30

20
Mips.BDD-abstract
Mips.BDD-real
RelFrac

0.1
2

3

4

5

6

7

8

9

10

11

1

20
IPP-abstract
IPP-real
RelFrac

10
0

12

0.1

13

2

3

4

5

6

(a)

7

8

9

10

11

12

10
0
13

(b)
10000

100
90

1000

80
70

100

60
50

10

40
30

1

20
SATPLAN-abstract
SATPLAN-real
RelFrac

0.1
2

3

4

5

6

7

8

9

10

11

12

10
0
13

(c)
Figure 2: Runtime performance of Mips.BDD (a), IPP (b), and SATPLAN04 (c), with
(abstract) and without (real) our hand-made variable domain abstraction,
in Zenotravel instances explicitly scaled to increase the amount of irrelevance.
Horizontal axis scales the number of cities, left vertical axis shows total runtime
in seconds, right vertical axis shows the percentage RelFrac of relevant facts.

The shown Zenotravel instances always feature 2 airplanes and 5 persons. The number
of cities scales from 2 to 13. As in Logistics, we generated 5 random instances per size,
and show average values with a time-out of 1800 seconds, stopping plots when 2 timeouts occurred at an instance size. All in all, the relative behavior of the abstract and real
curves for each planner is quite similar to what we observed in Figure 1 with Logistics.
For SATPLAN04 and IPP, abstraction has a slight disadvantage with high RelFrac, and
becomes much more efficient as RelFrac decreases. For Mips.BDD, the advantage brought
by the abstraction is much more pronounced, and decreasing RelFrac consistently widens
the gap between solving abstract and real tasks. The average value of RelFrac in the IPC
2000 Zenotravel benchmarks is 64%, lying in between 5 cities (67%) and 6 cities (63%) in
Figure 2, where there is not yet much gained by the abstraction.
440

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

In summary, it appears that some planning benchmarks (like Rovers) do not have good
abstractions, and most others (like Logistics and Zenotravel) do not have enough irrelevance
in the IPC test suites.
It is important to note that the situation may not be quite as bad for unsolvable examples. Consider the IPC benchmarks Dining Philosophers and Optical Telegraph (Edelkamp,
2003). Dining Philosophers is an extremely basic benchmark that cannot be abstracted
much further. In contrast, Optical Telegraph is essentially a version of Dining Philosophers
with a complex inner life (exchanging data between the two hands of each philosopher).
This inner life does not affect the existence of a solution (deadlock situation), which depends exclusively on the outer interfaces of the philosophers, that is, taking and releasing
forks. However, the inner life does, of course, affect the length of a solution, if one exists.
We constructed an unsolvable version of the domain (without deadlock situation) by giving
the philosophers more flexibility in releasing forks. As one would expect, in this setting
abstracting the inner life away gives huge savings, i.e., the tasks can be proved unsolvable
much more efficiently. This suggests that it may be easier to abstract unsolvable tasks,
without invalidating the property of interest. Exploring this is a topic for future work.
While most planning benchmark domains do not naturally contain unsolvable instances, in
over-subscription planning this issue may become relevant (Sanchez & Kambhampati, 2005;
Meuleau, Brafman, & Benazera, 2006).
3.5 Other Abstractions
As discussed earlier, one cannot expect that removing preconditions, goals, or entire facts
preserves plan length in interesting cases. There are, however, certain cases where some
delete effects can safely be ignored. Specifically: in Driverlog, Logistics, Mprime, Mystery,
and Zenotravel, one can ignore those deletes of load and unload actions which state
that an object is no longer at its origin location (load) respectively that an object is no
longer inside the vehicle (unload); in Rovers one can ignore some deletes of actions taking
rock or soil samples, namely those deletes stating that the sample is no longer at its origin
location. We ran each of our planners on the respective abstracted tasks. The results can
be summarized as follows.
SATPLAN04 has a clear loss in runtime from using the abstraction in Driverlog (e.g.,
task number 15 is solved abstract vs. real in 693.0 vs. 352.3 sec).
IPP has a vast gain from abstraction in Logistics (e.g., 52.8 vs 5540.1 sec in number 12),
and a vast loss in Zenotravel (e.g., 318.5 vs 2.5 sec in number 12).
Mips.BDD has a vast loss from abstraction in Driverlog, Logistics, and Zenotravel (e.g.,
163.8 vs. 8.3 sec in Zenotravel number 8).
The results are inconclusive for all other planner/domain pairs.

4. Resolution Complexity
As discussed in the introduction, we were surprised to see very little improvement to SATPLAN in our experiments, despite the dramatic state space reductions brought about by
441

fiDomshlak, Hoffmann, & Sabharwal

variable domain abstraction. We now shed some light on this issue, by examining resolution
complexity in the original vs. the abstracted planning tasks. Throughout the section, we
consider the situation where the plan length boundthe number of time steps in the CNF
encodingis too small, and thus the CNFs are unsatisfiable. Note that this is the case in
all but one of the SAT tests performed by SATPLAN. In particular, it is the case in the
SAT tests where SATPLAN proves optimality of the plan, that is, the non-existence of a
plan with n  1 steps where n is the length of an optimal plan. This proof is typically very
costly, accounting for a large fraction of the runtime taken by SATPLAN.
We consider all abstraction methods introduced in Section 2.4, plus (for completeness)
a hypothetical abstraction method that adds new initial facts. We show in Section 4.1 that,
in many cases, the resolution complexity cannot be improved by delegating the optimality
proof to within the abstraction. In Section 4.2 we then show that, in all considered cases,
the resolution complexity can become exponentially worse. Section 4.3 briefly examines the
effect of post-abstraction simplifications. For the sake of readability, herein most proofs are
replaced with proof sketches. The full proofs are available in Appendix A.
Recall that resolution complexity is defined as the length of the shortest possible resolution refutation. In all proofs, our arguments are for general (unrestricted) resolution.
However, our constructions do not affect the structure of the resolution refutations, and
hence the results hold as stated (except for a slight weakening in the case of Lemma 4.14)
for many known variants of resolution, including tree-like (DPLL), regular, and ordered
resolution. In general, the results hold for any variant of resolution for which setting variables to True or False or replacing one variable with another preserves proof structure (the
slightly exceptional status of Lemma 4.14 will be explained below when we discuss that
result).
In the remainder of the paper, if P is a planning task and  is an abstraction, then
by P  we denote the respective abstracted planning task, that is, the planning task that
results from applying  to P.
4.1 Can Resolution Complexity Become Better?
We prove three main results, which are captured by Theorems 4.14.3 below. The first
result holds for all four SAT encodings (A)(D) as listed in Section 2.2; the other two
results apply to encodings (A) and (C), respectively. For the respective encodings and
abstraction methods, the results essentially say that resolution complexity cannot decrease
by applying the abstraction. As outlined in the introduction, a catchy (if imprecise) intuition
behind these results is that over-approximations (abstractions) result in less constrained
formulas, which are harder to refute. For encoding (C), the result is offset by the effort
required to recover all Graphplan mutexes; we will get back to this below. For the theorems
that follow, recall from Section 2.3 that RC() denotes the resolution complexity of , i.e.,
the size of the smallest resolution proof of unsatisfiability of .
Theorem 4.1. Let P be a planning task. Assume we use any of the encoding methods
(A)(D). Let  be an abstraction of P that consists of any combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, or deletes; and
442

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

(c) removing a fact completely.
Let n be the length of a shortest plan for P  , and let b < n. Let  and  be the encodings
of b-step plan existence in P and P  , respectively. Then RC()  RC( ).
Theorem 4.2. Let P be a planning task. Assume we use encoding method (A). Let  be
an abstraction of P that consists of any combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, or deletes;
(c) removing a fact completely; and
(d) variable domain abstraction.
Let n be the length of a shortest plan for P  , and let b < n. Let  and  be the encodings
of b-step plan existence in P and P  , respectively. Then RC()  RC( ).
Theorem 4.3. Let P be a planning task. Assume we use encoding method (C). Let  be
an abstraction of P that consists of any combination of:
(a) adding initial facts;
(b) ignoring preconditions, goals, or deletes;
(c) removing a fact completely; and
(d) variable domain abstraction.
Let n be the length of a shortest plan for P  , and let b < n. Let  and  be the encodings
of b-step plan existence in P and P  , respectively. Let M be the number of resolution steps
required to infer from  the additional mutex clauses that appear in A , where A is the
encoding of b-step plan existence in P as per encoding (A). Then RC()  RC( ) + M .
Note in all these theorems that n, defined to be the length of a shortest plan for P  ,
necessarily satisfies n  m where m is the length of a shortest plan for P. Hence, for
n  b  m,  is satisfiable. Detecting this  finding out that P  has a plan of length b 
does not give us any information about the length of a shortest plan for P. For 0  b < n,
however,  is unsatifiable, which tells us that b + 1 is a lower bound on plan length in P.
Hence, what the theorems say is this: either b  n and  is too coarse to disprove existence
of a plan of length b; or b < n and  does not decrease the resolution complexity of that
disproofat least by no more than the complexity of deriving the additional mutexes, in
the case of Theorem 4.3.
Let us first linger a bit on Theorem 4.3. The general intuition of our results is that
abstractions induce less constrained formulas, and hence resolution complexity cannot decrease. So why does this hold for encoding (A) as stated in Theorem 4.2 but not, in a strict
sense (see Proposition 4.13 later in this section), for encoding (C)? Basically, the answer is
that the intuition is imprecise in this general formulation, and the devil is in the details.
In this particular case, the issue is that variable domain abstraction makes use of mutex
443

fiDomshlak, Hoffmann, & Sabharwal

relations which encoding (C) is not aware of. Sometimes, an indirect mutex in the original
task (omitted in encoding (C)) becomes a direct mutex in the abstraction (included in encoding (C)). Refuting  might then involve recovering that mutex, which a refutation of 
need not do. Hence, a potential improvement in resolution complexity may stem from the
power of mutex relations. The upper bound specified in Theorem 4.3 shows that this is the
only thing that an improvement can be due to. Proposition 4.13 below provides an example
where a mutex must be recovered, and hence proves that an analogue of Theorem 4.2 does
not hold for encoding (C).
It is an open question whether an analogue of Theorem 4.2 holds for encoding (B), and
whether an analogue of Theorem 4.3 holds for encoding (D). As we will discuss further
below, these open questions appear to be related to some intricate properties of Graphplanbased encodings with vs. without fact variables. What we do know is that mutexes may
need to be recovered also in encoding (D): the example provided by Proposition 4.13 works
for both encodings (C) and (D). Further, we establish a connection between the two open
questions: if an analogue of Theorem 4.2 holds for encoding (B), then we immediately get
that an analogue of Theorem 4.3 holds for encoding (D).
We now consider all this in detail. Note that, as far as the removal of goals is concerned,
the theorems are actually trivial: for all four encoding methods, if  only removes part of
the goals, then  is a sub-formula of . For all other abstraction methods, the latter is not
the case. We treat removal of goals together with the other methods since that treament
does not cause any overhead, and the goal clauses need to be discussed anyway (the set of
achievers of a goal may change).
For some of the proofs, we need a helper notion that captures over-approximated planning graphs. Assume a planning task P and its planning graph P G(P), and assume that
 is an abstraction. Then P G(P  ) will typically have many more vertices than P G(P).
This captures the fact that P  allows no fewer (and often more) facts and actions than P
does. This will, in general, result in many more constraints in a propositional translation
of the planning task. With more constraints, it may seem like the abstraction could, in
principle, make it possible to derive an easier/shorter proof of the fact that no plan exists
within the specified bound. However, a closer inspection restricted to facts and actions
already available in the original planning graph reveals that one often ends up with fewer
and weaker constraints than for the original task. We now introduce some notations to
make this formal.
Definition 4.4. For a planning task P and an abstraction  of it, P G (P) is defined to be
the subgraph of P G(P  ) induced by the vertices of P G(P). Similarly, P Gred (P) is defined
to be the subgraph of P Gred (P  ) induced by the vertices of P Gred (P).
Definition 4.5. Let P be a planning task. An abstraction  is called a planning graph
abstraction of P if P G (P) and P G(P) have identical sets of vertices and the following
conditions hold:
(1) Eadd (P G (P))  Eadd (P G(P)),
(2) Epre (P G (P))  Epre (P G(P)),
(3) Emutex (P G (P))  Emutex (P G(P)), and
444

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

(4) (G)  G,
where G and (G) are the goal states of P and P  , respectively. An abstraction  is called a
reduced planning graph abstraction if the above conditions hold for P Gred (P) and P Gred (P)
instead.
Lemma 4.6. Let P be a planning task. Assume we use encoding method (A) or (B). Let
 be a planning graph abstraction of P. Let n be the length of a shortest plan for P  , and
let b < n. Let  and  be the encodings of b-step plan existence in P and P  , respectively.
Then RC()  RC( ).
Proof Sketch. Say we set to False all variables of  that do not appear in , i.e., we
fix the value of those variables to be 0. Because of the way we defined P G (P), this
yields precisely the propositional encoding of P G (P). We show that, after this variable
restriction, the clauses surviving in  are also present in , either as is or in a stronger
form (i.e., with fewer literals). For example, in encoding (A) each precondition clause
C  of  has a corresponding clause C in  due to condition (2) of Definition 4.5, which
states that  does not introduce new preconditions. We have C   C due to condition
(1) of Definition 4.5, which states that  preserves all add effectshence the set of actions
achieving the precondition in P  contains the corresponding set in P. A similar observation
holds for the effect clauses in encoding (B), and similar arguments apply to all the other
kinds of clauses. The claim then follows with Proposition 2.1.
Lemma 4.7. Let P be a planning task. Assume we use encoding method (C) or (D). Let
 be a reduced planning graph abstraction of P. Let n be the length of a shortest plan for
P  , and let b < n. Let  and  be the encodings of b-step plan existence in P and P  ,
respectively. Then RC()  RC( ).
Proof. The argument is identical to the proof of Lemma 4.6, except that the underlying
planning graph for encodings (C) and (D) is the reduced planning graph, resulting in potentially fewer mutex clauses than in encodings (A) and (B), respectively. This, however,
does not in any way affect the proof arguments.
Lemma 4.8. Let P be a planning task. Let  by any modification of P that respects the
following behavior:
(a)  does not shrink the list of initial facts,
(b)  does not grow the set of goal facts,
(c)  preserves the add lists unchanged, and
(d)  does not grow the pre and del lists.
Then  is a planning graph abstraction of P as well as a reduced planning graph abstraction
of P.
445

fiDomshlak, Hoffmann, & Sabharwal

Proof Sketch. The proof is straightforward, but a little tedious in the details. Suppose  is
an abstraction for P satisfying the prerequisites. We must argue that P G (P) and P Gred (P)
satisfy the conditions in Definition 4.5. Condition (4) involving goal states easily follows
from property (b) of . Once P G(P) and P G (P) (as well as their reduced counterparts)
are shown to have the same set of vertices, conditions (1) and (2) involving precondition
and effect relations follow directly from properties (c) and (d). It hence remains to prove
that all facts and actions available in P G(P) are also available in P G (P) (showing (1) and
(2) with what we just said), and that no new mutex relations are created between facts and
actions that are not mutex in P G(P) (showing (3)). This proof is a little tedious, proceeding
inductively over the construction of the planning graph. The underlying intuition, however,
is simple: if P G (P) up to layer t abstracts P G(P) up to layer t, and properties (a), (c)
and (d) are respected by , then necessarily P G (P) up to layer t + 1 abstracts P G(P) up
to layer t + 1. This concludes the argument.
The following is an immediate consequence of Lemmas 4.6, 4.7, and 4.8.
Corollary 4.9. Let P be a planning task. Assume we use any of the encoding methods
(A)(D). Let  be an abstraction of P that consists of any combination of:
(a) adding initial facts; and
(b) ignoring preconditions, goals, or deletes.
Let n be the length of a shortest plan for P  , and let b < n. Let  and  be the encodings
of b-step plan existence in P and P  , respectively. Then RC()  RC( ).
This result essentially states the rather intuitive fact that, if the abstraction does anything that yields a larger planning graph, then the resulting Graphplan-based encodings
will be less constrained and hence have a higher resolution complexity (if anything).
Matters become much less intuitive once we consider abstractions that remove entire
factsclearly, these no longer result in over-approximated planning graphs, since they remove some of the vertices. In other words, the condition in Definition 4.5 that P G (P) and
P G(P) have identical sets of vertices does not hold, and we need a slightly different line of
reasoning that does not rely strictly on abstracted planning graphs. We first show that it is
harmless to remove a fact if it does not appear in the goal and in any pre or del list. Then
we rely on Corollary 4.9 to reason that this requirement on a fact can be easily achieved.
Lemma 4.10. Let P be a planning task. Assume we use any of the encoding methods
(A)(D). Let p be a fact that does not appear in the goal and in any of the pre or del lists,
and let  be the abstraction of P that removes p from the initial facts and the add lists. Let
n be the length of a shortest plan for P  , and let b < n. Let  and  be the encodings of
b-step plan existence in P and P  , respectively. Then RC() = RC( ).
Proof Sketch. The key point is that, if p does not appear in the goal and is never required or
deleted by an action, then p is completely irrelevant to the planning task, and in particular
to the resolution refutations we consider here. Concretely, we first prove that at every layer
of the planning graph, the available facts and the mutex fact pairs remain the same, up
to facts or fact pairs involving p. That is, the only thing that is lost in the fact layers of
446

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

P G (P) is p. Since p does not occur in any precondition, the action layers remain exactly
the same; since p does not occur in any preconditions or delete effects, the action mutexes
also remain exactly the same (they were not caused by p).
The above discussion implies that the precondition clauses in all the encodings are
identical. Given that p does not appear in the goal, the same is true of the goal clauses.
Since the action mutexes are unchanged, it follows that  and  are actually identical
for encodings (A) and (C). For encodings (B) and (D), the only difference between  and
 is that  does not contain the initial state, effect, and mutex clauses involving p.
However, these clauses can never participate in a resolution refutation of : all effect and
mutex clauses contain p in the same polarity (negative); while an initial state clause has
the positive form {p(0)}, the time index is different from those of p in all effect and mutex
clauses. Hence every variable corresponding to p occurs in only one polarity. This concludes
the argument.
Corollary 4.11. Let P be a planning task. Assume we use any of the encoding methods
(A)(D). Let  be an abstraction of P that removes a fact completely. Let n be the length of
a shortest plan for P  , and let b < n. Let  and  be the encodings of b-step plan existence
in P and P  , respectively. Then RC()  RC( ).
Proof.  is equivalent to the following two steps. First, remove p from the goal facts (if
present) and from all pre and del lists. By Corollary 4.9, this step cannot improve resolution
complexity. Second, now that p has been removed from the goal and the pre and del lists,
remove p from the problem completely by removing it from the initial facts and all add lists
as well. By Lemma 4.10, this step as well cannot improve resolution complexity, and we
are done.
Corollaries 4.9 and 4.11 together prove our first main result, Theorem 4.1. We now
move on to variable domain abstraction, where matters are most complicated, and which is
most interesting because that abstraction method enables us to construct solution length
preserving abstractions with exponentially smaller state spaces, in many benchmarks. First
we show that, in its original form, the result holds for encoding (A).
Lemma 4.12. Let P be a planning task. Assume we use encoding method (A). Let  be
a variable domain abstraction of P. Let n be the length of a shortest plan for P  , and let
b < n. Let  and  be the encodings of b-step plan existence in P and P  , respectively.
Then RC()  RC( ).
Proof Sketch.  combines two persistently mutex facts p and p0 into a single fact p. We first
show that if an action pair (a, a0 ) is mutex in P  , then it will also be mutex in P. The only
way for (a, a0 ) to become mutex per  requires, w.l.o.g., that in P, p  del (a)  pre(a) and
p0  pre(a0 )  add (a0 ). Supposing (a, a0 ) is not mutex in P, we have that p 6 del (a0 ) and p
is not mutex with any fact in pre(a0 ). But then, (noop(p), a0 ) is not mutex in P and hence
(p, p0 ) is not a persistent mutex, in contradiction.
With this in hand, we can derive a property rather similar to that of planning graph
abstractions given in Definition 4.5. By the above, we know that the abstract encoding 
does not have any mutexes that do not appear in . Further, the set of actions achieving
each fact only grows by applying this abstraction, and the goal can only shrink. The most
447

fiDomshlak, Hoffmann, & Sabharwal

subtle issue regards precondition clauses. If an action a has p0 as its precondition in P, then
this is replaced by p in P  , so there is no direct correspondence between the two. However,
that lack of correspondence does not affect the precondition clause of encoding (A), which
takes the form {a, a1 , . . . , ak }; this omits the actual precondition fact being achieved, so
it does not matter whether that fact is p0 or p.
Next, as in the proof to Lemma 4.6, we set all variables to False which appear in  but
not in . With the above arguments, it is then not difficult to see that the clauses surviving
in  are also present in , either as is or in a stronger form (i.e., with fewer literals). For
mutex clauses, this is obvious. For goal clauses, the argument is exactly the same as in the
proof sketch for Lemma 4.6 given above. For precondition clauses, observe that a1 , . . . , ak
in the above will contain all achievers of p0 plus all achievers of p. The claim now follows
with Proposition 2.1.
Corollaries 4.9 and 4.11 together with Lemma 4.12 prove our second main result, Theorem 4.2. For encodings (B)(D), matters are more complicated.
Consider first encodings (B) and (D), which differ from (A) in that they also have
fact variables. This changes the precondition clauses. If an action a in P has p0 as its
precondition, but p in P  , then we no longer get the clause {a, a1 , . . . , ak } as in the proof
sketch. Instead, we get the clause {a, p}. For this clause, there is no correspondence in .
In particular, consider the case where we have two actions in P, action a with precondition
p and action a0 with precondition p0 . This gives us the clauses {a, p}, {a0 , p0 } in  and
the clauses {a, p}, {a0 , p} in  . Now,  does not distinguish between the achievers
of p and those of p0 , so there is no problem in that regard. But can the fact that the two
clauses now share a literalwhich they dont in be exploited to obtain shorter resolution
refutations? This is an open question; we discuss its implications in a little more detail at
the end of this sub-section.
Consider now encoding (C), which differs from (A) in that it includes only direct action
mutexes. This invalidates a different argument in the proof of Lemma 4.12. It is still true
that, if an action pair (a, a0 ) is marked mutex in P  , then it will also be mutex in P.
However, it can happen that (a, a0 ) is mutex in P  due to a direct interference between
a and a0 , while (a, a0 ) is mutex in P due to mutex preconditions, rather than a direct
interference. Since encoding (C) accounts only for direct interferences, we then have a
mutex in  that does not appear in . This can result in improved resolution complexity
for  . The following proposition proves this formally.
Proposition 4.13. Assume we use encoding method (C). There exist a planning task P, a
variable domain abstraction  of P, and b < n such that RC() > RC( ), where n is the
length of a shortest plan for P  , and  and  are the encodings of b-step plan existence in
P and P  , respectively.
Proof Sketch. We construct P, , and b as specified. The key property of the construction
is that there are two actions, getg1 and getg2 , that are both needed to achieve the goal facts
g1 and g2 , respectively. More precisely, getg1 = ({x}, {g1 , p0 }, {x}) and getg2 = ({p, y},
{g2 }, {p}). The task is constructed, along with the help of a few other actions, in a way so
that x, p, and p0 are pairwise persistently mutex. The variable domain abstraction replaces
p0 with p, and b is set to 2. In the action layer directly beneath the goal layer, i.e., in action
448

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

layer A(1), the planning graph marks getg1 and getg2 as mutex because their preconditions
are mutex. Encoding (C), however, does not include this mutex clause because there is no
direct conflict. This situation changes after the abstraction. Now getg1 adds p instead of p0 ,
and hence there is a direct conflict with the delete effect of getg2 . As a consequence, in the
abstraction two resolution steps suffice: applying both getg1 and getg2 in A(1) is the only
option to achieve the goals, and the new mutex clause immediately excludes that option.
This is not so in the encoding of the original task, where the required mutex must first be
derived by reasoning over the preconditions x and p.
Note that the only reason why we get a shorter refutation for  is that variable domain abstraction turns an indirect action mutex (due to competing preconditions) into a
direct interference. In doing so, the abstraction exploits the knowledge that p and p0 are
persistently mutexa fact that is ignored in encoding (C). Hence the positive result stated
by Proposition 4.13 is less related to the power of abstraction than to the power of planning
graph mutexes. We now capture this formally. In what follows, note that using the same
plan length bound, the CNF formula as per encoding (C) is a sub-formula of the CNF
formula as per encoding (A), and all the additional clauses of (A) can be inferred from it.
Lemma 4.14. Let P be a planning task. Let  be a variable domain abstraction of P.
Let n be the length of a shortest plan for P  , and let b < n. Let A and C be the
encodings of b-step plan existence in P as per encoding (A) and (C), respectively. Let C
be the encoding of b-step plan existence in P  as per encoding (C). Let M be the number of
resolution steps required to infer from C the additional mutex clauses that appear in A .
Then RC(C )  RC(C ) + M .
Proof. Denote by A the encoding of b-step plan existence in P  as per encoding (A). We
have:
(1) By the preconditions of the lemma, RC(C )  RC(A ) + M : with M resolution steps,
C can be turned into A , and hence from the shortest resolution refutation for A
we can construct one for C that is M steps longer.
(2) From Lemma 4.12, RC(A )  RC(A ).
(3) C is a sub-formula of A , and hence RC(A )  RC(C ).
Combining these observations, we have:
RC(C )  RC(A ) + M

from observation (1)

RC(A ) + M
RC(C ) + M

from observation (2)




from observation (3)

This finishes the proof.
Clearly, this proof argument applies also when  is a combination of variable domain
abstraction with all the other abstractions. Hence Corollaries 4.9 and 4.11 together with
Lemmas 4.12 and 4.14 prove our third main result, Theorem 4.3. Note that this latter
result does not hold for all variants of resolution. In the claim of Lemma 4.14, M is the
449

fiDomshlak, Hoffmann, & Sabharwal

number of resolution steps it takes to derive the action mutexes not present in the original
encoding. These are then used in the resolution refutation. If the variant of resolution
under consideration is, say, DPLL or tree-like resolution, deriving a mutex clause once is
not enoughit must be re-derived as many times as it is used in the tree-like resolution
refutation. Hence the effective value of M for such variants of resolution would be larger.
Note that this is not the case if the DPLL solver learns the mutex clauses by virtue of the
wide-spread clause learning technique.
Lemma 4.14 is particularly relevant for our empirical results, because SATPLAN04 uses
encoding (C) and our experiments mostly focus on variable domain abstraction. While we
have no explicit empirical proof (and such a proof would be difficult to come by, requiring
a deep analysis of the SAT solvers search spaces), it seems reasonable to assume that, at
least to some extent, the disappointing results for SATPLAN04 are due to whats proved
in Lemma 4.14. The abstraction cannot improve resolution complexity beyond the effort
required to recover the indirect action mutexes. Note here that the bound M given in
the lemma is rather pessimistic. A mutex (a, a0 ) needs to be recovered only in the case
where a and a0 have competing needs in P, and replacing p0 with p results in a direct
interference but does not incur simplifications. In the Logistics domain, for example, with
our abstraction this happens only for actions loading a package onto an airplane in two
different but irrelevant cities. Since these load actions are involved only in redundant
solutions anyway, it seems doubtful that such mutexes play a role for resolution complexity.
More generally, it is interesting to consider upper bounds on M in Lemma 4.14. How
many resolution steps does it take to recover the indirect action mutexes? For general resolution, the number of steps is polynomially bounded, since the inference process conducted
by the planning graph can be simulated (for a related investigation, see Brafman, 2001).
For restricted variants of resolution, matters are more complicated. Of particular interest
is the behavior of DPLL+clause learning, c.f. the above. There is so far no known formula for which DPLL+clause learning proofs are provably substantially worse than general
resolution proofs; it would be rather surprising if planning graph mutexes were to be the
first. Also, Rintanen (2008) provides a related investigation, showing that mutexes can be
recovered in polynomial time by a particular 2-step lookahead procedure, which is related
(but not identical) to clause learning.
Concluding this sub-section, let us again turn our attention to encodings (B) and (D).
As mentioned before, it is an open question whether an analogue of Theorem 4.2 holds for
encoding (B), and whether an analogue of Theorem 4.3 holds for encoding (D). We are
facing two problematic issues:
(I) Fact variables. In both encoding (B) and (D), there are fact variables in addition
to the action variables used in encodings (A) and (C).
(II) Mutexes. Like for encoding (C), it may happen that variable domain abstraction
converts an implicit mutex in encoding (D) of the original task into an explicit one in
the abstraction.
We consider first issue (II). The situation is exactly as for encoding (C), in this regard.
Proposition 4.13 holds as stated for encoding (D) as well; indeed it can be proved using
450

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

exactly the same example and only very minor adaptations of the proof arguments.22 Similarly, Lemma 4.14 holds for encoding (D), with exactly the same proof argumentsprovided
an analogue of Lemma 4.12 (and hence an analogue of Theorem 4.2) holds for encoding (B).
Namely, the proof arguments of Lemma 4.14 all remain valid, except that we need to refer
to encoding (B) rather than (A), and that accordingly we need a corresponding version of
Lemma 4.12. This brings us to issue (I).
Variable domain abstraction can be perceived as gluing sets of facts together. Recall
here the example with clauses {a, p}, {a0 , p0 } in  not sharing any literals, and clauses
{a, p}, {a0 , p} in  sharing the literal p; this was discussed above to explain why the
proof of Lemma 4.12 does not work for encodings (B) and (D). If k + 1 facts are glued
together, then groups of k clauses can become linked together in this fashion. The question
is:
(*) Can resolution fruitfully exploit this increased linkage?
This issue appears to be related to some quite intricate properties of Graphplan-based
encodings with vs. without fact variables. For encoding (A), which differs from encoding
(B) only in that it does not use fact variables, Lemma 4.12 tells us that resolution cannot
exploit variable domain abstraction. Now, it appears reasonable to think that adding fact
variables does not help a lot, the intuition being:
(**) Whatever one can do with encoding (B), one can easily simulate with encoding (A).
If statement (**) is true, then the answer to question (*) has to be no, because a yes
answer would contradict Lemma 4.12. Hence, in an initial attempt to prove the no answer,
we tried to prove statement (**). However, our initial investigation has indicated that by
explicitly keeping fact variables (and non-trivial constraints on them) around, encoding (B)
might facilitate significantly shorter resolution derivations in general, and hence statement
(**) might be false. Namely, there appear to be families of formulas that can be suitably
encoded into planning tasks to yield an exponential separation between encodings (A) and
(B). If this is true, then it suggests that reasoning in the presence of fact variables might
be more powerful and hence might indeed be able to exploit the linkage gain yielded by
variable domain abstraction.
Since the purpose of this paper is not to compare the relative power of various
Graphplan-based encodings (such as that of (A) and (B)), we do not detail our progress
towards disproving statement (**). Besides, note that, if statement (**) is indeed false, then
that does not have any immediate implications on the answer to question (*). A definite
answer to (*) is left open for future research.
4.2 Can Resolution Complexity Become Worse?
The answer to the title of this sub-section is a definite yes. With all four encodings,
any of the abstractions we consider may exponentially deteriorate resolution complexity.
Formally, we have the following theorem.
22. We include the full proof for both (C) and (D) in Appendix A.

451

fiDomshlak, Hoffmann, & Sabharwal

Theorem 4.15. Assume we use any of the encoding methods (A)(D). There exist an
infinite sequence of planning tasks P(i), abstractions (i) of P(i), and b(i) < n(i) such that
RC( (i)) is exponential in i while RC((i)) is a constant independent of i, where n(i) is
the length of a shortest plan for P  (i), (i) and  (i) are the encodings of b(i)-step plan
existence in P(i) and P  (i) respectively, and (i) consists of any one of:
(a) adding initial facts;
(b) ignoring preconditions, goals, or deletes;
(c) removing a fact completely; or
(d) variable domain abstraction.
Proof Sketch. The idea is to construct P(i) as a planning task that consists of two separate
sub-tasks, and whose overall goal is to achieve the goals of both of these sub-tasks. Each
of the sub-tasks themselves is infeasible within the given plan length bound b(i). (The
tasks and bounds are constructed such that their size grows polynomially with i.) However,
while the first sub-task is constructed to require exponential size resolution refutations, the
second allows constant size refutations. If an abstraction over-approximates the easy-torefute sub-task in a way so that it becomes feasible within b(i) steps, then the resolution
refutation of the overall task must rely on the hard-to-refute sub-task. This leads to an
exponential growth, over i, in resolution complexity for  (i), as opposed to constant resolution complexity for (i). With any single one of the listed abstractions, feasibility of the
easy-to-refute sub-task can be accomplished in a simple manner, hence proving the theorem.
In order to construct planning tasks whose CNF encodings require exponential size
resolution refutations, we resort to the pigeon hole problem formula PHP(i). It is well
known that any resolution proof of PHP(i) must be of size exponential in i (Haken, 1985).
We construct a simple pigeon hole planning task PP HP (i) to capture this problem. We
show that, for any of the four encoding methods (A)(D), the CNF encoding for b(i) = 1
is either identical to PHP(i), or transforms into PHP(i) by variable restrictions. Hence,
by Proposition 2.1, any resolution refutation must have size exponential in i. The final
construction uses a combination of two such tasks: PP HP (i) serves as the hard-to-refute
sub-task, and PP HP (1) on disjoint sets of pigeon and hole objects serves as the easy-to-refute
sub-task.
Essentially, Theorem 4.15 states the intuitive fact that abstractions can make bad
choices, approximating away the most concise reason for why a planning task cannot be
solved in a particular number of steps. To illustrate the significance of this, consider once
more the comparison to mutex relations. An analogue of Theorem 4.15 does not hold for
them: adding a mutex clause to a CNF encoding can only improve resolution complexity. In
that sense, mutex relations are considerably less risky than the abstractions we consider
here.
While the pigeon hole problem used in the proof of Theorem 4.15 may seem artificial, it is
indeed contained as a sub-problem in wide-spread domains such as some of those concerned
with transportation. For example, in Gripper, the available time steps serve as holes
and the actions picking/dropping balls are the pigeons (for a related investigation, see
452

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Hoffmann et al., 2007). It also seems quite natural that a planning task may consist of two
disconnected parts, one of which is complex while the other one is easy to prove unsolvable
in the given number of steps. Just think of transporting two packages, one of which is
close to many vehicles and requires just one more step than the bound allows, while the
other one is already inside a vehicle and needs to be transported along a single path of road
connections that is much longer than the bound (a concrete example for the latter situation
is formalized by Hoffmann et al., 2007).
4.3 A Note on Simplifications
As pointed out in Section 2.4, there may be actions that after abstraction can obviously be
simplified without altering the semantics of the planning task. In particular, an abstraction
might create duplicate actions (that is, actions having identical preconditions and effects),
as well as redundant add effects (that are contained in the respective actions precondition).
It turns out that such a natural post-abstraction simplification of the planning task can lead
to lower resolution complexity.
Proposition 4.16. Assume we use any of the encoding methods (A)(D). There exist a
planning task P, a planning task P 0 that is identical to P except that either an irrelevant
add affect or a duplicate action have been removed, and b < n so that RC() > RC(0 ),
where n is the length of a shortest plan in P, and  and 0 are the encodings of b-step plan
existence in P and P 0 , respectively.
Proof Sketch. To show the claim for duplicate actions, we consider a task P 0 encoding the
pigeon hole problem for 3 pigeons and 2 holes, with actions that put pigeon p into hole h.
The plan length bound is 1. To point out that the proof works for solvable tasks P, an
extra action a, whose preconditions are two of the goals and which achieves the third goal,
ensures solvability in two steps. There are three goals, one for each pigeon, and there are
no mutex clauses other than direct action interference, because each pair of goals can be
achieved  but not the three of them. In particular, every resolution refutation must resolve
on all three goal clauses. We obtain P by adding a duplicate action for one of the pigeons
and one of the holes. The respective goal clause then becomes one literal longer. Any
refutation must get rid of that literal, hence necessitating one more step. The construction
works for all four encodings.
To show the claim for removal of redundant add effects, we slightly modify P 0 , replacing
as effect with a new fact x and including another action that achieves the third goal given
the precondition x. The optimal plan length now is 3, and the length bound is 2. Any
refutation must resolve on all three goal clauses. If, for P, we give a one of its preconditions
as an additional add effect, then the refutations become longer because the respective goal
clause does. Again, the construction works for all four encodings.
It is easy to modify the constructions used in the proof of Proposition 4.16 in a way
so that the duplicate action, respectively the redundancy of the add effect, arise as an
outcome of variable domain abstraction. Hence, via enabling such simplifications, variable
domain abstraction may improve the resolution best-case behavior. For duplicate actions,
this is also true for Edelkamps (2001) pattern database abstraction. It is an open question
453

fiDomshlak, Hoffmann, & Sabharwal

whether the improvement may be exponential, or whether it is bounded polynomially. We
conjecture that the latter is the case, at least for unrestricted resolution.
It is also notable that the examples in the proof to Proposition 4.16 are specifically
constructed to include duplicate actions/redundant add effects for actions that are relevant
to solving the problem  they form part of an optimal solution. In a well-constructed variable
domain abstraction this is not likely to happen since abstraction should target only the facts
that are irrelevant to solution length. Consider the Logistics domain as an example. The
only actions on which simplifications apply are loads/unloads of packages to/from locations
in cities other than the packages origin and destination. These actions are involved only
in redundant solutions, and it seems doubtful that their simplification affects resolution
complexity. Of course, these simplifications might help SAT solvers anyway. This, however,
is not observed, at least not significantly, in our experiments.

5. Conclusion
Abstractions, as used here, have the power to allow proving certain properties within much
smaller state spaces. In particular, if an abstraction preserves the length of an optimal
solution, then optimality can be proved within the abstraction. We designed a novel abstraction method for STRIPS planning that is suitable for this purpose. Surprisingly, the
approach yields little or no benefits for the planning-as-satisfiability approach as represented by SATPLAN, even in domains featuring hand-made abstractions with exponentially smaller state spaces. Towards explaining this, we have shown that, in many cases,
our abstraction method (as well as some other commonly used abstractions) lacks the ability to introduce shorter resolution refutationsother than through exploiting mutexes, or
enabling certain post-abstraction simplifications. In contrast, we have shown that these
abstractions may exponentially increase the size of the shortest resolution refutations.
Several questions are left open by our theoretical results. We do not know whether variable domain abstraction can improve resolution complexity in combination with encoding
(B), whether there is a polynomial upper bound on the improvement that variable domain
abstraction can bring for encoding (D), and whether there is a polynomial upper bound on
the improvement that can result from simplifications. Apart from answering these questions, most importantly it remains to be seen to what extent the results generalize. Bluntly
stated, the intuition behind the results is that over-approximations usually result in less
constrained formulas which are harder to refute. However, the actual technicalities of
the results depend quite a lot on the detailsof both encoding method and abstraction
and hence it is largely unclear to what extent this intuitive statement captures reality. In
particular: does it hold for other encodings of planning into SAT?
It would be interesting, e.g., to look at the alternative encodings described by Kautz
and Selman (1992), Kautz et al. (1996), Kautz and Selman (1996), Ernst, Millstein, and
Weld (1997). Many of these encodings are based on unit clauses for initial and goal state,
and action clauses stating that an action implies each of its effects and preconditions. With
such a structureand the lack of a mechanism such as a planning graph that propagates
changessome of the properties proved herein are obvious. Removing goals or initial state
facts corresponds directly to removing clauses; the same is true for preconditions. Removing
a fact completely may in some cases simply correspond to removing all clauses that mention
454

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

the fact. Hence, for these encodings, it seems that proving our results might indeed be
comparatively easy. A more challenging subject may be some more recent developments,
such as the encodings by Rintanen et al. (2006) which often give substantial speed-ups
through novel notions of parallelity, the encodings by Chen et al. (2009) which introduce
long-distance mutex relations, or the encodings by Robinson et al. (2008) which make use
of effective operator splitting and factoring methods.
More generally: do our results hold for methods employed in other fields? In particular,
do they hold in model-checking, where both abstraction (e.g., Graf & Sadi, 1997; Clarke
et al., 2003) and SAT encodings (e.g., Clarke, Biere, Raimi, & Zhu, 2001; Prasad, Biere,
& Gupta, 2005) are highly successful? There is one example where it is actually obvious
that our results hold. Gupta and Strichman (2005) abstract by ignoring clauses in a CNF
encoding of the original transition system (the motivation is that the much smaller CNF
formula causes less overhead in the SAT solver).
Most ambitiously: can we define a generic framework with formal notions of declarative
transition systems, CNF encodings, and abstractions that are suitable to capture our
results, and prove more generic statements? All these questions appear to be worthwhile
research challenges. Indeed, we think that a key contribution of our work may lie in asking
the question about resolution complexity with vs. without an abstraction.
From a more practical perspective, we see mainly four lines of further research. First, an
important question is whether our observations carry over to modified/extended planningas-SAT systems, such as as the one by Ray and Ginsberg (2008) which guarantees plan
optimality through branching restrictions within a single SAT call, rather than through
calling the SAT solver iteratively. Second, it remains open to explore whether very different
abstraction techniquesbased e.g. on predicate abstractioncan be suitably adapted to
planning. Third, it is important to note that our empirical results are not entirely negative.
Mips.BDD is often substantially improved, even up to the point where, as in Figure 1, this
optimal sequential planner is highly competitive with a strong optimal parallel planner such
as SATPLAN, and this in a highly parallel domain such as Logistics. This is a direction that
may well be worth exploring in more depth. Finally, more effective abstraction methods may
exist for unsolvable examples, and could potentially play a crucial role in over-subscription
planning (Sanchez & Kambhampati, 2005; Meuleau et al., 2006).

Acknowledgments
We thank the anonymous reviewers, whose detailed comments helped greatly to improve
the paper. A preliminary version of this work appeared at ICAPS06, the 16th International
Conference on Automated Planning and Scheduling (Hoffmann, Sabharwal, & Domshlak,
2006). The work of Carmel Domshlak was supported by Israel Science Foundation (ISF)
Grants 2008100 and 2009580, as well as by the C. Wellner Research Fund. For part of
this work, Jorg Hoffmann was employed at Max Planck Institute for Computer Science,
Saarbrucken, Germany, and at SAP Research, Karlsruhe, Germany. The work of Ashish
Sabharwal was supported by IISI, Cornell University (AFOSR Grant FA9550-04-1-0151),
the National Science Foundation (NSF) Expeditions in Computing award (Computational
455

fiDomshlak, Hoffmann, & Sabharwal

Sustainability Grant 0832782), NSF IIS award (Grant 0514429), and the Defense Advanced
Research Projects Agency (DARPA, REAL Grant FA8750-04-2-0216).

Appendix A. Proof Details
Proof of Proposition 2.1. Suppose the sequence  of transformations consists of ` restrictions, 1 , 2 , . . . , ` . Further, let  0 be the strengthening transformation that replaces each
clause of  | with a (not necessarily strict) sub-clause that is also a clause of ; such a
transformation exists because of the assumptions in the proposition. Observe that these
` + 1 transformation steps together convert  into a (not necessarily strict) sub-formula of
. We will show that each of these ` + 1 transformation steps individually does not increase
the resolution complexity of the underlying formula. Without loss of generality, we will
prove this fact for a single restriction transformation and then for a generic strengthening
transformation. Since each of these will individually be shown to not increase the resolution
complexity of the formula, they can be applied in any sequence or combination, any number
of times, without increasing the resolution complexity. This would prove, in particular, that
the resolution complexity of a sub-formula of  is no more than that of  , implying that
the resolution complexity of  itself is no more than that of  (as additional initial clauses
cannot hurt a resolution refutation), as desired.
We start with a single restriction transformation x  y. For ease of notation, we
will assume that the initial formula is F = {C1 , C2 , . . . , Cm } and the resulting simplified
0 }, with m0  m. Without loss of
formula after the transformation is F 0 = {C10 , C20 , . . . , Cm
0
0
generality, we will assume that no Ci equals the empty clause {} and there are no duplicate
clauses in F 0 . Let  = (C1 , C2 , . . . , Cm , Cm+1 , . . . , CM = {}) be a resolution refutation of
F of the smallest possible size; note that  involves m initial clauses and M  m resolution
steps. From , we construct a resolution refutation  0 of F 0 which will be of size no larger
than that of . We do this in the following three steps.
Step 1. Transform  into  = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), where Ci is defined as
follows. If the application of the transformation x  y results in Ci containing True
or a variable and its negation, then Ci equals True; if it results in Ci containing
False or duplicate literals, Ci consists of Ci with False or a duplicate literal removed;
otherwise Ci = Ci . Note that Ci does not contain x and is either True, the empty
clause, or a non-empty (not necessarily strict) sub-clause of C. The key property
here is that Ci is still a logical implicant of Cj and Ck if Ci was derived by resolving
Cj and Ck in the original proof . Ci may not necessarily be a usual resolution
resolvent of Cj and Ck , which is what the next two steps will fix.
Step 2. Transform  into  = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), where Ci equals Ci for
i  m and for i > m is defined sequentially, for increasing i, as follows. Suppose
Ci was derived in  by resolving clauses Cj and Ck , where j < k < i. Assume
without loss of generality that we have already defined Cj and Ck . If Ci equals
True, then Ci equals True as well; otherwise, if one of the two clauses Cj and Ck
equals True, then Ci equals the other clause; otherwise, Cj and Ck can be resolved
together on some variable and Ci is the resolvent of these two clauses. The key
property here, which can be seen easily by considering the sequential nature of the
456

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

transformation, is that each Ci is either True or a (not necessarily proper) subclause of Ci ; in particular, CM = {}. Further, each Ci that does not equal True is
either the resolution resolvent of Cj and Ck , or equals {} with one of Cj and Ck
also being {}.
Step 3. Finally, transform  into  0 by simply removing any clauses that equal True or
occur previously in the clause sequence, and stopping the sequence as soon as the
first empty clause is encountered. By construction, , , and  have exactly M
clauses each and  0 has no more than M clauses. Further, the first m0 clauses of
 0 are exactly the clauses of F 0 and  0 is a resolution refutation starting from these
initial clauses, as desired.
We now consider the strengthening transformation  0 , which essentially replaces each
clause of the formula with a sub-clause (thereby strengthening this clause). We will show
that applying  0 does not increase the resolution complexity of the underlying formula.
Again, for ease of notation, let the initial formula be F = {C1 , C2 , . . . , Cm } with a resolution
refutation  = (C1 , C2 , . . . , Cm , Cm+1 , . . . , CM = {}) of the smallest possible size. The
argument here is along the same lines as but simpler than for restriction transformations.
Transform  into  = (C1 , . . . , Cm , Cm+1 , . . . , CM = {}), where for i  m, Ci equals the
sub-clause of Ci that  0 maps Ci to, and for i > m, Ci is defined sequentially, for increasing
i, as follows. Suppose Ci was derived in  by resolving clauses Cj and Ck on variable x,
where j < k < i. Assume without loss of generality that we have already defined Cj and
Ck . If x is present in both Cj and Ck , then Ci is simply the resolution resolvent of these
two clauses; otherwise, if x is not present in Cj , then Ci equals Cj ; otherwise x must not be
present in Ck and we set Ci to equal Ck . The key property here, which can again be seen
easily by considering the sequential nature of the transformation, is that each Ci is a (not
necessarily proper) sub-clause of Ci ; in particular, CM = {}. Further, each Ci is either Cj
or Ck or the resolution resolvent of the two. Now transform  into  0 by simply removing
any clauses that occur previously in the clause sequence. By construction,  and  have
exactly M clauses each and  0 has no more than M clauses. Further, the first m0 clauses of
 0 are exactly the clauses of F 0 and  0 is a resolution refutation starting from these initial
clauses, as desired.
Proof of Lemma 4.6. Let P be a planning task to which an abstraction  that abstracts
P G(P) is applied. Let ,  denote propositional encodings of P and P  , respectively,
where we use either the action-only encoding (A) or the action-fact encoding (B) for both
 and  . Let U denote the set of variables in  that are not variables of . Finally,
let  be the variable restriction that sets every variable in U to False. In this setting, the
propositional formula  | is nothing but the CNF encoding of the planning graph P G (P)
(using the same encoding method, (A) or (B), as used for  and  ). In particular, all
clauses of  that correspond to actions and facts not in P G(P) are trivially satisfied by
 , because they all contain the negation of a variable in U , which is set to False by  . Call
the remaining, yet unsatisfied clauses in  | the surviving clauses. We will argue that
each surviving clause is already present, perhaps in a stronger but not a weaker form, in 
itself, showing that it can be no easier to prove  | unsatisfiable than it is to prove  itself
unsatisfiable.
457

fiDomshlak, Hoffmann, & Sabharwal

First consider encoding (A). By conditions (2) and (4) of Definition 4.5, every surviving
precondition and goal clause in  | has a corresponding clause in  itself. Further, by
condition (1) concerning which facts are added by which action, each such precondition or
goal clause in  | contains as a sub-clause the corresponding clause in . Finally, each
surviving mutex clauses in  | , by condition (3), is also present as a mutex clause in .
From these observations, it follows that every surviving clause in  | contains as a (possibly
non-strict) sub-clause the corresponding clause in . Applying Proposition 2.1, we obtain
RC()  RC( ), finishing the proof for encoding (A).
Now consider encoding (B). First, because by Definition 4.5 P G (P) and P G(P) have
identical sets of vertices, in particular the fact vertices F (0) are the same, and hence each
surviving initial state clause in  | is also present as an initial state clause in . Further, precondition clauses are binary and, by condition (2) of Definition 4.5, each surviving
precondition clause in  | is also present as a precondition clause in . Similarly, each
goal clause is a unit clause and, by condition (4), each surviving goal clause in  | is also
present as a goal clause in . In a similar vein, each surviving mutex clause in  | is also
present as a mutex clause in . Finally, each surviving effect clause in  | , by condition
(1), contains as a sub-clause the corresponding effect clause in  | . Hence we again see
that every surviving clause in  | contains as a (possibly non-strict) sub-clause the corresponding clause in . Applying Proposition 2.1 as before, we obtain RC()  RC( ),
finishing the proof for encoding (B).
Proof of Lemma 4.8. Let P be a planning task to which an abstraction  that respects
the behavior specified in the lemma is applied. We will show that the four conditions
in Definition 4.5 hold for both P G (P) and P Gred (P). Observe that condition (4) in
Definition 4.5 trivially holds because of property (b) of . We therefore focus on showing
that V (P G(P)) = V (P G (P)), V (P Gred (P)) = V (P Gred (P)), and that conditions (1)(3)
in Definition 4.5 hold. In fact, once we show that P G (P) and P Gred (P) have the same set
of vertices as the original (reduced) planning graph, conditions (1) and (2) of Definition 4.5
would be immediately satisfied due to properties (c)23 and (d) of , and all that would
remain would be condition (3), saying that no new mutex clauses are added by applying .
Hence, our task is reduced to proving that the following four new properties hold at
each step t  0, 1, . . . , b of the planning task:
(i) F  (t)  F (t),
(ii) A (t)  A(t),
(iii) Ef-mutex (t)|F (t)  Ef -mutex (t), and

(iv) Ea-mutex
(t)|A(t)  Ea-mutex (t).

where F (t), A(t), Ef -mutex (t), and Ea-mutex (t) denote the sets of facts, actions (including
noops), fact mutexes, and action mutexes generated for P at step t, and the -versions of
these denote the corresponding sets for P  . For   F  (t), Ef-mutex (t)| denotes {(f1 , f2 ) 
23. What one needs here is only that add lists do not shrink by applying . However, another argument will
shortly require that add lists do not grow either, justifying the strict requirement of property (c).

458

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings


Ef-mutex (t) | f1 , f2  }, the subset of Ef-mutex (t) restricted to the facts in . Ea-mutex
(t)|

is defined similarly for   A (t).
In order to prove that these four properties hold, we give an inductive argument on t,
alternating between F (t) and Ef -mutex (t) on one hand, and A(t) and Ea-mutex (t) on the
other. For the base case of t = 0, note that F  (0)  F (0) by property (a) of  and
Ef-mutex (0)|F (0)  Ef -mutex (0) because both of these sets are empty.
In words, our goal is to prove that if certain facts and actions are available in P at a
certain step, they remain available in P  . Similarly, if two facts or actions are mutually
compatible in P, they remain mutually compatible in P  . This seems intuitively justifiable
given the properties of . The following argument formalizes this intuition. In terms of
notation, we will use pre, del , and add to specify actions in P and pre  , del  , and add  to
specify actions in P  .
For the first part of the inductive step, suppose F  (t)  F (t) and Ef-mutex (t)|F (t) 

Ef -mutex (t). We will show that A (t)  A(t) and Ea-mutex
(t)|A(t)  Ea-mutex (t), inductively
proving conditions (ii) and (iv).
Let a  A(t). Then pre  (a)  F (t)  F  (t). Further, f, f 0  pre  (a)  pre(a) 
F (t), we have that (f, f 0 ) 6 Ef -mutex (t)  Ef-mutex (t)|F (t) and hence (f, f 0 ) 6 Ef-mutex (t).
Therefore a  A (t), proving that A (t)  A(t).
Now let a, a0  A(t) be such that (a, a0 ) 6 Ea-mutex (t). For the reduced planning graph

(t) due to properties
with only direct mutexes, we immediately have that (a, a0 ) 6 Ea-mutex

(c) and (d) of , and we are done proving that Ea-mutex (t)|A(t)  Ea-mutex (t). Otherwise, for
more general mutexes, several things hold. First, we have pre  (a)  pre(a)  F (t)  F  (t),
and the same for a0 . Likewise, we have del  (a)  del (a) and del  (a0 )  del (a0 ). Finally,
by property (c), we have add  (a)  add (a). Hence (c.1) (pre  (a)  add  (a))  del  (a0 ) 
(pre(a)add (a))del (a0 ) = ; the last equality holds because (a, a0 ) 6 Ea-mutex (t). Similarly,
(c.2) (pre  (a0 )add  (a0 ))del  (a) = . Finally, for all f  pre  (a)  pre(a)  F (t)  F  (t)
and for all f 0  pre  (a0 )  pre(a0 )  F (t)  F  (t), we have that (f, f 0 ) 6 Ef -mutex (t) 
Ef-mutex (t)|F (t) , which implies (c.3) (f, f 0 ) 6 Ef-mutex (t). From (c.1), (c.2), and (c.3), we


have (a, a0 ) 6 Ea-mutex
(t), proving that Ea-mutex
(t)|A(t)  Ea-mutex (t).

For the second part of the inductive step, suppose A (t)  A(t) and Ea-mutex
(t)|A(t) 


Ea-mutex (t). We will show that F (t+1)  F (t+1) and Ef -mutex (t+1)|F (t+1)  Ef -mutex (t+
1), proving conditions (i) and (iii).
S
S
Let f  F (t + 1). Then f  aA(t) add (a)  aA (t) add  (a). (Recall that noop
actions are included in A(t), so that we need not explicitly include F (t) in F (t + 1).) It
follows that f  F  (t + 1), proving that F  (t + 1)  F (t + 1).
Now let f, f 0  F (t + 1) be such that (f, f 0 ) 6 Ef -mutex (t + 1). Then there must exist
0
a, a  A(t)  A (t) such that (c.1) f  add (a)  add  (a), (c.2) f 0  add (a0 )  add  (a),


and (a, a0 ) 6 Ea-mutex (t)  Ea-mutex
(t)|A(t) , which implies that (c.3) (a, a0 ) 6 Ea-mutex
(t).
0


From (c.1), (c.2), and (c.3), we have (f, f ) 6 Ef -mutex (t + 1), proving that Ef -mutex (t +
1)|F (t+1)  Ef -mutex (t + 1).
This finishes the inductive argument, showing that conditions (i)(iv) outlined above
hold. By our earlier reasoning, this proves both that the vertices of P G(P) and P G (P),
as well as their reduced counterparts, are the same (so that conditions (1) and (2) of
Definition 4.5 follow directly from properties (c) and (d) of ) and the mutex relations of
P  and red (P), restricted to the facts and actions of P, are a subset of the mutex relations

459

fiDomshlak, Hoffmann, & Sabharwal

and the reduced mutex relations, respectively, of P (so that condition (3) of Definition 4.5
holds). Hence  abstracts the planning graph as well as the reduced planning graph of
P.
Proof of Lemma 4.10. Let  be the abstraction that removes p from the initial facts and
the add lists of a given planning task P in which p does not appear in the goal facts and
any of the pre or del lists. As in the proof of Lemma 4.6, let ,  denote propositional
encodings of P and P  , respectively, where we use one of the encodings (A), (B), (C), and
(D) for both  and  . We will show that for encodings (A) and (C),  and  are in fact
identical, and for encodings (B) and (D), differ only in clauses that cannot be part of any
resolution proof.
To this end, we use the planning graph notation from the proof of Lemma 4.8 and
begin by arguing by induction that F  (t) = F (t) \ {p}, Ef-mutex (t) = Ef -mutex (t) \ {(p, p0 ) |

p0  F (t)}, A (t) = A(t), and Ea-mutex
(t) = Ea-mutex (t). For the base case of t = 0,

F (0) = F (0)\{p} by the definition of , and Ef-mutex (t) = Ef -mutex (0)\{(p, p0 ) | p0  F (0)}
because both these sets are empty. For the first part of the induction, suppose that the
inductive conditions on F and Ef -mutex hold at time step t. Since p is not in any pre list,
this implies that A (t) = A(t) as well. Further, since p is not in any del list, we also

have Ea-mutex
(t) = Ea-mutex (t). Hence the conditions on A and Ea-mutex hold at time step
t. For the second part of the induction, suppose that the inductive conditions on A and
Ea-mutex hold at time step t. This implies that F (t + 1) consists of F  (t + 1) and possibly
p. Further, Ef -mutex (t + 1) and Ef-mutex (t + 1) are the same as far as mutexes not involving
p are concerned. It follows that the conditions on F and Ef -mutex hold at time step t + 1,
finishing the induction.

To summarize, we have shown that at every step, the sets A, A and Ea-mutex , Ea-mutex
are exactly the same, and the sets F, F  and Ef -mutex , Ef-mutex are the same up to facts or
pairs of facts involving p. In other words, no new actions or facts become available or are mutually excluded in the planning graph because of , and everything not involving p remains
unchanged. Given this, observe that with any of the four encodings, the goal and precondition clauses of P  are exactly the same as those of P because p does not appear in the goal

or the pre lists at all. Similarly, Ea-mutex
(t) = Ea-mutex (t) and Ef-mutex (t) = Ef -mutex (t)

implies that the action mutexes of P , and fact mutexes if present in the encoding, are the
same as those of P as well. Therefore, for encodings (A) and (C),  = .
Finally, for encodings (B) and (D), we have initial state, effect, and mutex clauses in
 which do get removed by applying , i.e., which are not present in  . However, these
are the only clauses that mention propositional variables corresponding to p, and each of
these variables appears only with one polarity throughout . Namely, an initial state clause
{p(0)} is the only clause that may contain p in positive polarity; all effect and mutex clauses
that contain p do so with a time index t > 0. Because of that, these clauses cannot be part
of any resolution refutation of every variable appearing in a resolution refutation must
eventually be resolved away in order to derive the empty clause. It follows that  and 
are the same with respect to resolution refutations.
Proof of Lemma 4.12. Let P be a planning task to which , a variable domain abstraction,
is applied.  combines two persistently mutex facts p and p0 into a single fact p. For
brevity, let G denote P G(P). Define G 0 to be the graph obtained by unifying any p and
460

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

p0 fact vertices in each fact layer of G into a single vertex p in that layer, and similarly
any noop(p) and noop(p0 ) vertices in action layers. Finally, let G  denote the subgraph
of P G(P  ) induced by the vertices of G 0 . We will show that G  is an abstracted planning
graph in a sense very similar to Definition 4.5.
We begin by arguing that if an action pair (a, a0 ) is mutex in P  , then it is also mutex
in P. To see this, observe that the only way for (a, a0 ) to become mutex per  requires,
w.l.o.g., that in P, p  del (a)  pre(a) and p0  pre(a0 )  add (a0 ). Suppose for the sake
of contradiction that (a, a0 ) is not already mutex in P. In particular, this means that
p 6 del (a0 ) and p is not mutex with any fact in pre(a0 ). This, however, implies that
(noop(p), a0 ) is not mutex in P so that the fact pair (p, p0 ) is not mutex in the next layer
of P G(P), a contradiction because p and p0 are persistently mutex. It follows that edges
Ea-mutex in G  are a subset of those in G.
Since G 0 and P  have the same initial facts, the above argument implies that all actions
and facts available at any layer of G 0 are also available at that layer of P G(P  ). In particular,
G  , by construction, has exactly the same set of vertices as G 0 . Further, since  is a variable
domain abstraction, edges Eadd and Epre in G 0 and G  are exactly the same.
Define ,  , U, and  as in the proof of Lemma 4.6.  | is the CNF encoding (A)
of the planning graph G  , and all clauses of  corresponding to actions and facts not in
G  are trivially satisfied by  . Call the remaining clauses in  | the surviving clauses as
before.
By our observation about edges Ea-mutex in G  and G, the surviving mutex clauses of
 | are also mutex clauses of . The surviving precondition and goal clauses not involving
p appear unchanged in . Since we are considering a variable domain abstraction, the
actions achieving p in G  are precisely the actions achieving either of p and p0 in G. Hence,
the surviving precondition and goal clauses of  | involving p contain as a sub-clause
a precondition or goal clause of  itself. It follows from Proposition 2.1 that RC() 
RC( ).
Proof of Proposition 4.13. The same example planning task, denoted P, works for both
encoding (C) and encoding (D). Let  denote the variable domain abstraction to be applied.
The example uses the following six facts: facts p and p0 , which will be glued together by ;
goal facts g1 and g2 ; and helper facts x, y. Based on these facts, the task P is defined as
follows:
 Initial state {p}; Goal {g1 , g2 }
 Action set A containing five actions:
getx = ({p}, {x}, {p}),
gety = (, {y}, ),
getg1 = ({x}, {g1 , p0 }, {x}),
getg2 = ({p, y}, {g2 }, {p}),
getp = ({p0 }, {p}, {p0 }).
The plan length bound is 2, which makes the problem infeasible: the shortest (parallel)
plan requires 4 steps: h{getx, gety}, {getg1 }, {getp}, {getg2 }i. Observe that all pairs of
actionsexcept (getg1 , getg2 ) and any pair that involves getydirectly interfere with each
461

fiDomshlak, Hoffmann, & Sabharwal

other and are therefore mutex. In P G(P), we get the following fact and action sets up to
step 2:
 F (0) = {p}, A(0) = {noop(p), getx, gety}
 F (1) = {p, x, y}, A(1) = {noop(p), noop(x), noop(y), getx, gety, getg1 , getg2 }
 F (2) = {p, x, y, g2 , g1 , p0 }
It is easy to verify, iteratively, that, in P G(P): p and x are mutex in F (1); p and x are
mutex in F (2); p and p0 are mutex in F (2); x and p0 are mutex in F (2); all these mutexes
we get also in F (3), where the planning graph reaches its fixpoint. In particular, we have
that getg1 and getg2 are always (indirectly) mutex because their preconditions x and p
are persistently mutex. The variable domain abstraction  will glue p and p0 , converting
the conflict between getg1 and getg2 into a direct interference, thereby allowing a shorter
resolution refutation.
Consider encoding (C) of P G(P). It contains two goal clauses: {getg1 (1)} and
{getg2 (1)}. These clauses clearly must be used in any resolution refutation of the formula, because it is possible to achieve each goal individually within the given time bound,
but not both together. Hence, a shortest refutation must involve at least two steps. We
argue that such a shortest refutation can be achieved in the abstracted task P  but not in
P itself.
In P  , we get the same fact and action sets in the planning graph, except that F (2) =
{p, x, y, g1 , g2 }, i.e., p0 is of course not present, and both A(0) and A(1) contain also getp
that acts here similarly to noop(p). The corresponding encoding (C) consists of exactly
the same clauses as before (plus clauses with noop(p) being mirrored with getp), except
that we get the additional clause {getg1 (1), getg2 (1)}. This mutex clauses arises because
getg1 interferes directly with getg2 (rather than only indirectly through incompatible preconditions), because now getg1 adds p instead of p0 , and p is deleted by getg2 . This yields a
trivial two-step (tree-like) resolution proof for P  , using the two goal clauses and the mutex
clause (namely, resolve the second goal clause and the mutex clause deriving {getg1 (1)},
and then resolve this clause with the first goal clause). On the other hand, in the original
task P, getg1 and getg2 are not marked mutex in layer A(1), because they dont directly
interfere. Therefore, the corresponding mutex clause is not immediately available, and any
resolution proof takes more than two steps because it must reason about and involve x.
Encoding (D) works similarly, and  lets us derive the new mutex clause discussed
above. The goal clauses in this case are simply {g1 (2)} and {g2 (2)}. From these, using the
two corresponding effect clauses, we can derive the two goal clauses of encoding (C) in two
steps. From here, the two-step refutation discussed above derives the empty clause. Thus,
we have a four-step resolution refutation for P  in encoding (D). There is no similarly small
resolution refutation in P itself, because any such refutation must, as mentioned earlier,
reason on x to figure out that getg1 (1) and getg2 (1) cannot both be True.
Proof of Theorem 4.15. We construct a family of STRIPS tasks whose CNF encodings are
very similar to the pigeon hole problem formula PHP(i). It is well known that any
resolution proof of PHP(i) must be of size exponential in i (Haken, 1985). Concretely,
PHP(i) is an unsatisfiable formula encoding the fact that there is no way to assign i + 1
462

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

pigeons to i holes such that each pigeon is assigned to at least one hole and no hole gets
more than one pigeon. The formula has i(i + 1) variables xp,h where p  {1, . . . , i + 1}, h 
{1, . . . , i}. For each pigeon p, there is a pigeon clause (xp,1 , xp,2 , . . . , xp,n ), and for each pair
of pigeons {p, q} and hole h, there is a hole clause {xp,h , xq,h }.
The pigeon hole planning task PP HP (i) is defined as follows. For each pigeon p, there is
a fact assigned (p). For each hole h, there is a fact free(h). The initial state contains all i free
facts but no assigned facts. The goal state contains all i+1 assigned facts. The only available
actions (other than noops) are put(p, h), which puts pigeon p into a free hole h, after which
h no longer remains free. Formally, put(p, h) = ({free(h)}, {assigned (p)}, {free(h)}). The
plan length bound b(i) is set to 1.
Consider any one of the four encoding methods (A)(D), and let (i) be the encoding of
PP HP (i). Restrict (i) by setting all noop variables to False; this has no real restrictive
implication in terms of planning since the plan length bound is 1 and none of the goal facts
are available at time step 0. For the action-only encodings (A) and (C), identifying the
action variables put(p, h) with the PHP(i) variables xp,h immediately yields precisely the
clauses of PHP(i): the goal clauses of (i) become the pigeon clauses of PHP(i) and the
action mutex clauses become the hole clauses. For the action-fact encodings (B) and (D),
fix all free fact variables at time step 0 as well as all assigned fact variables at time step 1 to
True, and identify put(p, h) action variables as above with xp,h . This again yields precisely
the clauses of PHP(i). It follows from the resolution hardness of PHP(i) and Proposition 2.1
that any resolution proof of the fact that the planning task PP HP (i) does not have a plan
of length 1 must require size exponential in i.
The claim now follows from a planning task P 0 (i) which consists of a combination of two
disconnected pigeon hole planning sub-tasks, PP HP (i) and PP HP (1), over two separate sets
of pigeon and hole objects. The goal for P 0 (i) is naturally defined as follows: put the first
set of i + 1 pigeons into the first set of i holes and put the second set of two pigeons into
the second set of holes (which consists of only a single hole). The overall CNF encoding
0 (i) of P 0 (i) is the logical conjunction of the encodings (i) and (1) (on disjoint sets
of variables) of PP HP (i) and PP HP (1). Observe that 0 (i) can be proved unsatisfiable by
proving unsatisfiability of either of the two pigeon hole problems. In particular, there is a
constant size resolution refutation of 0 (i) which involves refuting the (1) component.
On the other hand, we argue that all of the listed abstractions can make the one-hole
component of P 0 (i) trivially satisfiable, so that a resolution refutation of the abstracted
task must resort to a proof of unsatisfiability of the i-hole component (i) of P 0 (i), which
we have shown requires exponential size. Hence the single example P 0 (i) serves to show the
claim for all combinations of abstraction method and CNF encoding.
It is easily verified that PP HP (1) becomes solvable when ignoring the precondition
free(1) of both put(1, 1) and put(2, 1): we can then put both pigeons into the single hole. The
same happens when ignoring the delete effect free(1) of both put(1, 1) and put(2, 1).When
ignoring the goal assigned (2), or when inserting assigned (2) into the initial state, or when
completely removing assigned (2), the one-hole component of P 0 (i) requires to assign only
one pigeon, which is of course possible. Finally, for variable domain abstraction, note that
assigned (1) and assigned (2) are persistently mutex in PP HP (1) because the only actions
achieving them are put(1, 1) and put(2, 1), respectively. According to Definition 2.2, we
can hence replace assigned (2) with assigned (1). In the resulting planning task, we have the
463

fiDomshlak, Hoffmann, & Sabharwal

single goal assigned (1) which can be achieved in 1 step by, for example, the put(1, 1) action.
This concludes the argument.
Proof of Proposition 4.16. We first consider removal of duplicate actions. The same example planning task, denoted P 0 , works for all four encodings; P 0 is defined as follows:
 Fact set {r1 , r2 , g1 , g2 , g3 }
 Initial state {r1 , r2 }; Goal {g1 , g2 , g3 }
 Action set A containing seven actions:
1 1 = ({r1 }, {g1 }, {r1 }),
1 2 = ({r1 }, {g2 }, {r1 }),
1 3 = ({r1 }, {g3 }, {r1 }),
2 1 = ({r2 }, {g1 }, {r2 }),
2 2 = ({r2 }, {g2 }, {r2 }),
2 3 = ({r2 }, {g3 }, {r2 }),
help = ({g1 , g2 }, {g3 }, ).
In this planning task, all actions that are applicable in the initial state consume one of
the two resources r1 or r2 . Each of the actions achieves just one of the goals, so each pair of
goals can be reached, but not all three of them. The only solution is to perform two steps,
in the second of which the help action serves to accomplish g3 . We set the plan length
bound to 1.
The planning graph P G(P 0 ) up to step 1 has no mutex relations other than the direct
mutexes between actions competing for the same resource. Hence, encoding (A) is identical
to encoding (C), and encoding (B) is identical to encoding (D). The same properties clearly
hold also for the planning task P that is like P 0 except that it has an additional action 1 10
identical to 1 1.
Consider encoding (A) of P 0 . The goal clauses are {1 1(0), 2 1(0)}, {1 2(0), 2 2(0)},
and {1 3(0), 2 3(0)}. The only other clauses are mutex clauses of the form {i j, i k}.
It is not difficult to verify that a shortest resolution refutation involves 12 steps. One
such derivation proceeds via deriving {2 2(0), 2 1(0)}, {2 3(0), 2 1(0)}, {2 1(0)}, {1 2(0)},
{1 3(0)}, {}; each of these can be derived, in this sequence, with 2 steps involving resolution
against one mutex clause. For P, the only thing that changes is that we now have the clause
{1 1(0), 1 10 (0), 2 1(0)} instead of {1 1(0), 2 1(0)}, plus the additional mutex clauses. Now,
obviously every resolution refutation must resolve on all three goal clauses. To end up with
an empty clause, we hence additionally need to get rid of the literal 1 10 (0). Clearly, there
is no way to do this other than to resolve that literal away with an additional step involving
one of the new mutex clauses. Hence the shortest possible resolution refutation now has
 13 steps.
For encoding (B), the resolution proofs first need to make three steps resolving the goal
clauses {g1 (1)}, {g2 (1)}, {g3 (1)} with the respective effect clauses {g1 (1), 1 1(0), 2 1(0)},
{g2 (1), 1 2(0), 2 2(0)}, and {g3 (1), 1 3(0), 2 3(0)}; thereafter, matters are the same as
before.
To show the claim for removal of redundant add effects, we slightly modify the example,
and define P 0 as follows:
464

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

 Fact set {r1 , r2 , g1 , g2 , g3 , x}
 Initial state {r1 , r2 }; Goal {g1 , g2 , g3 }
 Action set A containing eight actions:
1 1 = ({r1 }, {g1 }, {r1 }),
1 2 = ({r1 }, {g2 }, {r1 }),
1 3 = ({r1 }, {g3 }, {r1 }),
2 1 = ({r2 }, {g1 }, {r2 }),
2 2 = ({r2 }, {g2 }, {r2 }),
2 3 = ({r2 }, {g3 }, {r2 }),
help1 = ({g1 , g2 }, {x}, ),
help2 = ({x}, {g3 }, ).
In this task, the single help action from before is replaced with two help actions that need to
be applied consecutively. We set the plan length bound to 2. As before, the planning graph
P G(P 0 ) has no mutex relations other than the direct mutexes between actions competing
for the same resource; encodings (A)/(C) and (B)/(D) respectively are identical. The same
properties clearly hold also for the planning task P that is like P 0 except that help1 has the
additional add effect g1 .
Consider encoding (A) of P 0 . The goal clauses are {1 1(1), 2 1(1), noop(g1 )(1)}, {1 2(1),
2 2(1), noop(g2 )(1)}, and {1 3(1), 2 3(1), noop(g3 )(1)}. Refuting this now involves showing
that the three goals cannot all be achieved in step 1, nor in step 0, nor in a combination
of the two. Any refutation needs to resolve on all three clauses. As before, for P we get
an additional literal in the first clause, which is now {1 1(1), 2 1(1), help1 , noop(g1 )(1)}.
Clearly, getting rid of that additional literal involves at least one more resolution step. For
encoding (B), matters are essentially the same except that we first need to resolve the goal
fact clauses against the respective effect clauses.

References
Ball, T., Majumdar, R., Millstein, T., & Rajamani, S. (2001). Automatic predicate abstraction of C programs. In PLDI2001: Programming Language Design and Implementation, pp. 203213.
Beame, P., Kautz, H., & Sabharwal, A. (2004). Towards understanding and harnessing the
potential of clause learning. Journal of Artificial Intelligence Research, 22, 319351.
Beck, C., Hansen, E., Nebel, B., & Rintanen, J. (Eds.). (2008). Proceedings of the 18th
International Conference on Automated Planning and Scheduling (ICAPS-08). AAAI
Press.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90 (1-2), 279298.
Blum, A. L., & Furst, M. L. (1995). Fast planning through planning graph analysis. In
Mellish, S. (Ed.), Proceedings of the 14th International Joint Conference on Artificial
Intelligence (IJCAI-95), pp. 16361642, Montreal, Canada. Morgan Kaufmann.
465

fiDomshlak, Hoffmann, & Sabharwal

Boddy, M., Fox, M., & Thiebaux, S. (Eds.). (2007). Proceedings of the 17th International
Conference on Automated Planning and Scheduling (ICAPS-07). AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Geffner, H. (2008). Heuristics for planning with penalties and rewards formulated in logic and computed through circuits. Artificial Intelligence, 172 (12-13),
15791604.
Brafman, R. (2001). On reachability, relevance, and resolution in the planning as satisfiability approach. Journal of Artificial Intelligence Research, 14, 128.
Chaki, S., Clarke, E., Groce, A., Jha, S., & Veith, H. (2003). Modular verification of software
components in C. In ICSE2003: Int. Conf. on Software Engineering, pp. 385395.
Chen, Y., Huang, R., Xing, Z., & Zhang, W. (2009). Long-distance mutual exclusion for
planning. Artificial Intelligence, 173 (2), 365391.
Clarke, E. M., Biere, A., Raimi, R., & Zhu, Y. (2001). Bounded model checking using
satisfiability solving. Formal Methods in System Design, 19 (1), 734.
Clarke, E. M., Grumberg, O., Jha, S., Lu, Y., & Veith, H. (2003). Counterexample-guided
abstraction refinement for symbolic model checking. Journal of the Association for
Computing Machinery, 50 (5), 752794.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem proving.
Communications of the ACM, 5 (7), 394397.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal
of the Association for Computing Machinery, 7 (3), 201215.
Edelkamp, S. (2001). Planning with pattern databases. In Cesta, A., & Borrajo, D. (Eds.),
Recent Advances in AI Planning. 6th European Conference on Planning (ECP01),
pp. 1324, Toledo, Spain. Springer-Verlag.
Edelkamp, S. (2003). Promela planning. In Ball, T., & Rajamani, S. (Eds.), Proceedings
of the 10th International SPIN Workshop on Model Checking of Software (SPIN-03),
pp. 197212, Portland, OR. Springer-Verlag.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge in planning problems to minimize state encoding length. In Biundo, S., & Fox, M. (Eds.), Recent Advances in AI
Planning. 5th European Conference on Planning (ECP99), Lecture Notes in Artificial
Intelligence, pp. 135147, Durham, UK. Springer-Verlag.
Ernst, M., Millstein, T., & Weld, D. (1997). Automatic sat-compilation of planning problems. In Pollack, M. (Ed.), Proceedings of the 15th International Joint Conference on
Artificial Intelligence (IJCAI-97), pp. 11691176, Nagoya, Japan. Morgan Kaufmann.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2 (34), 198208.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239290.
466

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Graf, S., & Sadi, H. (1997). Construction of abstract state graphs with PVS. In CAV1997:
Computer Aided Verification, pp. 7283.
Gupta, A., & Strichman, O. (2005). Abstraction refinement for bounded model checking. In
Etessami, K., & Rajamani, S. (Eds.), Proceedings of the 17th International Conference
on Computer Aided Verification (CAV05), Lecture Notes in Computer Science, pp.
112124, Edinburgh, UK. Springer-Verlag.
Haken, A. (1985). The intractability of resolution. Theoretical Computer Science, 39, 297
308.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Chien, S.,
Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the 5th International Conference on Artificial Intelligence Planning Systems (AIPS-00), pp. 140149, Breckenridge, CO. AAAI Press, Menlo Park.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction of pattern database heuristics for cost-optimal planning. In Proceedings
of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI-2007), pp.
10071012. AAAI Press.
Helmert, M., & Mattmuller, R. (2008). Accuracy of admissible heuristic functions in selected planning domains. In Proceedings of the 23rd AAAI Conference on Artificial
Intelligence, pp. 938943, Chicago, IL. AAAI Press.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics for optimal
sequential planning.. In Boddy et al. (Boddy, Fox, & Thiebaux, 2007), pp. 176183.
Henzinger, T., Jhala, R., Majumdar, R., & McMillan, K. (2004). Abstractions from proofs.
In POPL2004: Principles of Programming Languages, pp. 232244.
Hernadvolgyi, I., & Holte, R. (1999). PSVN: A vector representation for production systems.
Tech. rep. 1999-07, University of Ottawa.
Hoffmann, J., & Edelkamp, S. (2005). The deterministic part of IPC-4: An overview. Journal
of Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
Hoffmann, J., Gomes, C., & Selman, B. (2007). Structure and problem hardness: Goal asymmetry and dpll proofs in sat-based planning. Logical Methods in Computer Science,
3 (1:6).
Hoffmann, J., Sabharwal, A., & Domshlak, C. (2006). Friends or foes? an AI planning
perspective on abstraction and search.. In Long, & Smith (Long & Smith, 2006), pp.
294303.
Katz, M., & Domshlak, C. (2008). Structural pattern heuristics via fork decomposition.. In
Beck et al. (Beck, Hansen, Nebel, & Rintanen, 2008), pp. 182189.
467

fiDomshlak, Hoffmann, & Sabharwal

Kautz, H., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In Pollack, M. (Ed.), Proceedings of the 16th International Joint Conference on Artificial
Intelligence (IJCAI-99), pp. 318325, Stockholm, Sweden. Morgan Kaufmann.
Kautz, H. (2004). SATPLAN04: Planning as satisfiability. In Edelkamp, S., Hoffmann,
J., Littman, M., & Younes, H. (Eds.), Proceedings of the 4th International Planning
Competition (IPC-04), Whistler, BC, Canada.
Kautz, H., Selman, B., & Hoffmann, J. (2006). SATPLAN: Planning as satisfiability. In
Gerevini, A., Dimopoulos, Y., Haslum, P., Saetti, A., Bonet, B., & Givan, B. (Eds.),
Proceedings of the 5th International Planning Competition (IPC-06), Ambleside, UK.
Kautz, H. A., McAllester, D., & Selman, B. (1996). Encoding plans in propositional logic. In
Aiello, L. C., Doyle, J., & Shapiro, S. (Eds.), Principles of Knowledge Representation
and Reasoning: Proceedings of the 5th International Conference (KR-96), pp. 374384,
Cambridge, MA. Morgan Kaufmann.
Kautz, H. A., & Selman, B. (1992). Planning as satisfiability. In Neumann, B. (Ed.),
Proceedings of the 10th European Conference on Artificial Intelligence (ECAI-92),
pp. 359363, Vienna, Austria. Wiley.
Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic,
and stochastic search. In Proceedings of the 13th National Conference of the American
Association for Artificial Intelligence (AAAI-96), pp. 11941201, Portland, OR. MIT
Press.
Knoblock, C. A. (1990). Learning abstraction hierarchies for problem solving. In Proceedings
of the 8th National Conference of the American Association for Artificial Intelligence
(AAAI-90), pp. 923928, Boston, MA. MIT Press.
Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs
to an ADL subset.. In Steel, & Alami (Steel & Alami, 1997), pp. 273285.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. Journal of Artificial Intelligence Research,
12, 338386.
Long, D., Kautz, H. A., Selman, B., Bonet, B., Geffner, H., Koehler, J., Brenner, M.,
Hoffmann, J., Rittinger, F., Anderson, C. R., Weld, D. S., Smith, D. E., & Fox, M.
(2000). The aips-98 planning competition. AI Magazine, 21 (2), 1333.
Long, D., & Smith, S. (Eds.)., ICAPS-06 (2006). Proceedings of the 16th International Conference on Automated Planning and Scheduling (ICAPS-06), Ambleside, UK. Morgan
Kaufmann.
McDermott, D. (1999). Using regression-match graphs to control search in planning. Artificial Intelligence, 109 (1-2), 111159.
Meuleau, N., Brafman, R., & Benazera, E. (2006). Stochastic over-subscription planning
using hierarchies of MDPs.. In Long, & Smith (Long & Smith, 2006), pp. 121130.
Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts and operators in
plan generation.. In Steel, & Alami (Steel & Alami, 1997), pp. 338350.
468

fiFriends or Foes? On Planning as Satisfiability and Abstract CNF Encodings

Prasad, M. R., Biere, A., & Gupta, A. (2005). A survey of recent advances in sat-based
formal verification. International Journal on Software Tools for Technlogy Transfer,
7 (2), 156173.
Ray, K., & Ginsberg, M. L. (2008). The complexity of optimal planning and a more efficient
method for finding solutions.. In Beck et al. (Beck et al., 2008), pp. 280287.
Rintanen, J. (2004). Evaluation strategies for planning as satisfiability. In Saitta, L. (Ed.),
Proceedings of the 16th European Conference on Artificial Intelligence (ECAI-04), pp.
682687, Valencia, Spain. Wiley.
Rintanen, J. (2008). Planning graphs and propositional clause-learning. In Brewka, G., &
Doherty, P. (Eds.), Principles of Knowledge Representation and Reasoning: Proceedings of the 11th International Conference (KR-08), pp. 535543, Sydney, Australia.
AAAI Press.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning as satisfiability: parallel plans and
algorithms for plan search, artificial intelligence. Artificial Intelligence, 170 (12-13),
10311080.
Robinson, J. A. (1965). A machine oriented logic based on the resolution principle. Journal
of the Association for Computing Machinery, 12 (1), 2341.
Robinson, N., Gretton, C., Pham, D.-N., & Sattar, A. (2008). A compact and efficient sat
encoding for planning.. In Beck et al. (Beck et al., 2008), pp. 296303.
Sacerdoti, E. (1973). Planning in a hierarchy of abstraction spaces. In Proceedings of the
3rd International Joint Conference on Artificial Intelligence (IJCAI-73), pp. 412422,
Stanford, CA. William Kaufmann.
Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics for selecting objectives in over-subscription planning problems. In Biundo, S., Myers, K., & Rajan, K.
(Eds.), Proceedings of the 15th International Conference on Automated Planning and
Scheduling (ICAPS-05), pp. 192201, Monterey, CA, USA. Morgan Kaufmann.
Steel, S., & Alami, R. (Eds.). (1997). Recent Advances in AI Planning. 4th European Conference on Planning (ECP97), Vol. 1348 of Lecture Notes in Artificial Intelligence,
Toulouse, France. Springer-Verlag.
Streeter, M., & Smith, S. (2007). Using decision procedures efficiently for optimization.. In
Boddy et al. (Boddy et al., 2007), pp. 312319.

469

fiJournal of Artificial Intelligence Research 36 (2009) 307-340

Submitted 06/09; published 11/09

Cross-lingual Annotation Projection of Semantic Roles
Sebastian Pado

pado@ims.uni-stuttgart.de

Institut fur maschinelle Sprachverarbeitung
Universitat Stuttgart, 70174 Stuttgart, Germany

Mirella Lapata

mlap@inf.ed.ac.uk

School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 10 AB, UK

Abstract
This article considers the task of automatically inducing role-semantic annotations in
the FrameNet paradigm for new languages. We propose a general framework that is based
on annotation projection, phrased as a graph optimization problem. It is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic
resources. Within this framework, we present projection models that exploit lexical and
syntactic information. We provide an experimental evaluation on an English-German parallel corpus which demonstrates the feasibility of inducing high-precision German semantic
role annotation both for manually and automatically annotated English data.

1. Introduction
Semantic roles play a prominent role in linguistic theory (Fillmore, 1968; Jackendoff, 1990;
Dowty, 1991). They describe the relations that hold between a predicate and its arguments,
abstracting over surface syntactic configurations. As an example, consider sentences (1a)
and (1b) where butter is uniformly assigned the semantic role Undergoer (since it undergoes a physical change) even though it is syntactically realized as the object of the verb
melt in (1a) and its subject in (1b):
(1)

a.
b.

[Bob]Agent melted [the butter]Undergoer .
[The butter]Undergoer melted.

This intermediate representation seems a promising first step towards text understanding,
and can ultimately benefit many natural language processing tasks that require broad coverage semantic processing.
Methods for the automatic identification and labeling of semantic roles, often referred to
as shallow semantic parsing (Gildea & Jurafsky, 2002), are an important prerequisite for the
widespread use of semantic role information in large-scale applications. The development
of shallow semantic parsers1 has been greatly facilitated by the availability of resources like
FrameNet (Fillmore, Johnson, & Petruck, 2003) and PropBank (Palmer, Gildea, & Kingsbury, 2005), which document possible surface realization of semantic roles. Indeed, semantic
1. Approaches to building shallow semantic parsers are too numerous to list. We refer the interested reader
to the proceedings of the 2005 CoNLL shared task (Carreras & Marquez, 2005) and to the 2008 Computational Linguistics Special Issue on Semantic Role Labeling (Marquez, Carreras, Litkowski, & Stevenson,
2008) for an overview of the state-of-the-art.
c
2009
AI Access Foundation. All rights reserved.

fiPado & Lapata

Departing
An object (the Theme) moves away from a Source.
Theme

The officer left the house.
The plane leaves at seven.
His departure was delayed.

Source

We departed from New York.
He retreated from his opponent.
The woman left the house.

FEEs

abandon.v, desert.v, depart.v, departure.n,
emerge.v, emigrate.v, emigration.n, escape.v,
escape.n, leave.v, quit.v, retreat.v, retreat.n,
split.v, withdraw.v, withdrawal.n

Table 1: Abbreviated FrameNet entry for the Departing frame
roles have recently found use in applications ranging from information extraction (Surdeanu,
Harabagiu, Williams, & Aarseth, 2003) to the modeling of textual entailment relations (Tatu
& Moldovan, 2005; Burchardt & Frank, 2006), text categorization (Moschitti, 2008), question answering (Narayanan & Harabagiu, 2004; Frank, Krieger, Xu, Uszkoreit, Crysmann,
Jorg, & Schafer, 2007; Moschitti, Quarteroni, Basili, & Manandhar, 2007; Shen & Lapata,
2007), machine translation (Wu & Fung, 2009a, 2009b) and its evaluation (Gimenez &
Marquez, 2007).
In the FrameNet paradigm, the meaning of a predicate (usually a verb, noun, or adjective) is represented by reference to a frame, a prototypical representation of the situation
the predicate describes (Fillmore, 1982). The semantic roles, which are called frame elements, correspond to entities present in this situation, and are therefore frame-specific. For
each frame, the English FrameNet database2 lists the predicates that can evoke it (called
frame-evoking elements or FEEs), gives the possible syntactic realizations of its semantic
roles, and provides annotated examples from the British National Corpus (Burnard, 2000).
An abbreviated example definition of the Departing frame is shown in Table 1. The semantic roles are illustrated with example sentences and the FEEs are shown at the bottom
of the table (e.g., abandon, desert, depart). The PropBank corpus, the second major semantic role resource for English, provides role realization information for verbs in a similar
manner on the Wall Street Journal portion of the Penn Treebank. It uses index-based role
names (Arg0Argn), where Arg0 and Arg1 correspond to Dowtys (1991) proto-agent and
proto-patient. Higher indices are defined on a verb-by-verb basis.
Unfortunately, resources such as FrameNet and PropBank are largely absent for almost
all languages except English, the main reason being that role-semantic annotation is an
expensive and time-consuming process. The current English FrameNet (Version 1.3) has
been developed over the past twelve years. It now contains roughly 800 frames covering
2. Available from http://framenet.icsi.berkeley.edu.

308

fiCross-lingual Annotation Projection for Semantic Roles

around 150,000 annotated tokens of 7,000 frame-evoking elements. Although FrameNets
are being constructed for German, Spanish, and Japanese, these resources are considerably
smaller. The same is true for PropBank-style resources, which have been developed for
Korean3 , Chinese (Xue & Palmer, 2009), Spanish and Catalan (Taule, Mart, & Recasens,
2008). Compared to the English PropBank, which covers 113,000 predicate-argument structures, the resources for the other languages are two to three times smaller (e.g., the Korean
PropBank provides 33,000 annotations).
Given the data requirements for supervised learning algorithms (Fleischman & Hovy,
2003) and the current paucity of such data, unsupervised methods could potentially enable
the creation of annotated data for new languages and reduce the human effort involved.
However, unsupervised approaches to shallow semantic parsing are still at an early stage,
and mostly applicable to resources other than FrameNet (Swier & Stevenson, 2004, 2005;
Grenager & Manning, 2006). In this article, we propose a method which employs parallel
corpora for acquiring frame elements and their syntactic realizations for new languages (see
the upper half of Table 1). Our approach leverages the existing English FrameNet to overcome the resource shortage in other languages by exploiting the translational equivalences
present in aligned data. Specifically, it uses annotation projection (Yarowsky, Ngai, & Wicentowski, 2001; Yarowsky & Ngai, 2001; Hwa, Resnik, Weinberg, & Kolak, 2002; Hi &
Hwa, 2005) to transfer semantic roles from English to less resource-rich languages. The key
idea of projection can be summarized as follows: (1) given a pair of sentences E (English)
and L (new language) that are translations of each other, annotate E with semantic roles;
and then (2) project these roles onto L using word alignment information. In this manner,
we induce semantic structure on the L side of the parallel text, which can then serve as
data for training a shallow semantic parser for L that is independent of the parallel corpus.
The annotation projection paradigm faces at least two challenges when considering semantic roles. Firstly, the semantic structure to be projected must be shared between the two
sentences. Clearly, if the role-semantic analysis of the source sentence E is inappropriate for
the target sentence L, simple projection will not produce valid semantic role annotations.
Secondly, even if two sentences demonstrate semantic parallelism, semantic role annotations
pertain to potentially arbitrarily long word spans rather than to individual words. Recovering the word span of the semantic roles in the target language is challenging given that
automatic alignment methods often produce noisy or incomplete alignments.
We address the first challenge by showing that, if two languages exhibit a substantial
degree of semantic correspondence, then annotation projection is feasible. Using an EnglishGerman parallel corpus as a test bed, we assess whether English semantic role annotations
can be transferred successfully onto German. We find that the two languages exhibit a
degree of semantic correspondence substantial enough to warrant projection. We tackle the
second challenge by presenting a framework for the projection of semantic role annotations
that goes beyond single word alignments. Specifically, we construct semantic alignments
between constituents of source and target sentences and formalize the search for the best
semantic alignment as an optimization problem in a bipartite graph. We argue that bipartite
graphs offer a flexible and intuitive framework for modeling semantic alignments that is able
to deal with noise and to represent translational divergences. We present different classes
3. The Korean PropBank is available from the LDC (http://www.ldc.upenn.edu/).

309

fiPado & Lapata

of models with varying assumptions regarding admissible correspondences between source
and target constituents. Experimental results demonstrate that constituent-based models
outperform their word-based alternatives by a large margin.4
The remainder of this article is organized as follows. Section 2 discusses annotation
projection in general and presents an annotation study examining the degree of semantic
parallelism on an English-German corpus. In Section 3, we formalize semantic alignments
and present our modeling framework. Our experiments are detailed in Section 4. We review
related work in Section 5 and conclude the article with discussion of future work (Section 6).

2. Annotation Projection and Semantic Correspondence
In recent years, interest has grown in parallel corpora for multilingual and cross-lingual
natural language processing. Beyond machine translation, parallel corpora can be exploited
to relieve the effort involved in creating annotations for new languages. One important
paradigm, annotation projection, creates new monolingual resources by transferring annotations from English (or other resource-rich languages) onto resource-scarce languages
through the use of word alignments. The resulting (noisy) annotations can be then used
in conjunction with robust learning algorithms to obtain NLP tools such as taggers and
chunkers relatively cheaply. The projection approach has been successfully used to transfer a wide range of linguistic annotations between languages. Examples include parts of
speech (Yarowsky et al., 2001; Hi & Hwa, 2005), chunks (Yarowsky et al., 2001), dependencies (Hwa et al., 2002), word senses (Diab & Resnik, 2002; Bentivogli & Pianta, 2005),
information extraction markup (Riloff, Schafer, & Yarowsky, 2002), coreference chains (Postolache, Cristea, & Orasan, 2006), temporal information (Spreyer & Frank, 2008), and LFG
f-structures (Tokarczyk & Frank, 2009).
An important assumption underlying annotation projection is that linguistic analyses
in one language will be also valid in another language. It is however unrealistic to expect
any two languages, even of the same family, to be in perfect correspondence. There are
many well-studied systematic differences across languages often referred to as translational
divergences (van Leuven-Zwart, 1989; Dorr, 1995). These can be structural, where the same
semantic content in the source and in the target language can be realized using different
structures, or semantic, where the content itself undergoes a change in translation. Translational divergences (in conjunction with poor alignments) are a major stumbling block
towards achieving accurate projections. Yarowsky and Ngai (2001) find that parts of speech
that are transferred directly from English onto French contain considerable noise, even in
cases where inaccurate automatic alignments have been manually corrected (accuracies vary
between 69% and 78% depending on tagset granularity). For syntax, Hwa, Resnik, Weinberg, Cabezas, and Kolak (2005) find that only 37% of English dependency relations have
direct counterparts in Chinese, and 38% in Spanish. The problem is commonly addressed
with filtering mechanisms, which act as a post-processing step on the projection output.
For example, Yarowsky and Ngai (2001) exclude infrequent projections and poor alignments
4. A preliminary version of this work was published in the proceedings of EMNLP 2005 and COLING/ACL
2006. The current article contains a more detailed description of our approach, presents several novel
experiments, and a comprehensive error analysis.

310

fiCross-lingual Annotation Projection for Semantic Roles

and Hwa et al. (2005) apply transformation rules which encode linguistic knowledge about
the target language.
In the case of semantic frames there is reason for optimism. By definition, frames are
based on conceptual structure (Fillmore, 1982). The latter constitute generalizations over
surface structure and therefore ought to be less prone to syntactic variation. Indeed, efforts
to develop FrameNets manually in German, Japanese, and Spanish reveal that a large
number of English frames can be re-used directly to describe predicates and their arguments
in other languages (Ohara, Fujii, Saito, Ishizaki, Ohori, & Suzuki, 2003; Subirats & Petruck,
2003; Burchardt, Erk, Frank, Kowalski, Pado, & Pinkal, 2009). Boas (2005) even suggests
frame semantics as an interlingual meaning representation.
Computational studies on projection in parallel corpora have also obtained good results
for semantic annotation. Fung and Chen (2004) induce FrameNet-style annotations in Chinese by mapping English FrameNet entries directly onto concepts listed in HowNet5 , an
on-line ontology for Chinese, without using parallel texts. In their experiment, they transfer
semantic roles from English to Chinese with an accuracy of 68%. Basili, Cao, Croce, Coppola, and Moschitti (2009) use gold standard annotations to transfer semantic roles from
English to Italian with 73% accuracy. Bentivogli and Pianta (2005) project EuroWordNet
sense tags, which represent more fine-grained semantic information than FrameNet, also
from English to Italian. They obtain a precision of 88% and a recall of 71%, without applying any filtering. Fung, Wu, Yang, and Wu (2006, 2007) analyse an automatically annotated
EnglishChinese parallel corpus and find high cross-lingual agreement for PropBank roles
(in the range of 75%95%, depending on the role).
To provide a sound empirical justification for our projection-based approach, we conducted a manual annotation study on a parallel English-German corpus. We identified
semantic role information in bi-sentences and assessed the degree to which frames and semantic roles agree and diverge in English and German. The degree of divergence provides
a natural upper bound for the accuracy attainable with annotation projection.
2.1 Sample Selection
English-German bi-sentences were drawn from the second release of Europarl (Koehn, 2005),
a corpus of professionally translated proceedings of the European Parliament. Europarl is
aligned at the document and sentence level and is available in 11 languages. The English
German section contains about 25 million words on both sides. Even though restricted in
genre (transcriptions of spoken text), Europarl is fairly open-domain, covering a wide range
of topics such as foreign politics, cultural and economic affairs, and procedural matters.
A naive sampling strategy would involve randomly selecting bi-sentences from Europarl
which contain a FrameNet predicate on the English side aligned to some word on the
German side. There are two caveats here. First, the alignment between the two predicates
may be wrong, leading us to assign a wrong frame to the German predicate. Secondly, even
if the alignment is accurate, it is possible that a randomly chosen English predicate evokes
a frame that is not yet covered by FrameNet. For example, FrameNet 1.3 documents the
receive sense of the verb accept (as in the sentence Mary accepted a gift), but has no
entry for the admit sense of the predicate (e.g., in I accept that this is a problem for
5. See http://www.keenage.com/zhiwang/e_zhiwang.html.

311

fiPado & Lapata

Measure
Frame Match
Role Match
Span Match

English
89.7
94.9
84.4

German
86.7
95.2
83.0

All
88.2
95.0
83.7

Table 2: Monolingual inter-annotator agreement on the calibration set

the EU ) which is relatively frequent in Europarl. Indeed, in a pilot study, we inspected a
small random sample consisting of 100 bi-sentences, using the publicly available GIZA++
software (Och & Ney, 2003) to induce English-German word alignments. We found that 25%
of the English predicates did not have readings documented in FrameNet, and an additional
9% of the predicate pairs were instances of wrong alignments. In order to obtain a cleaner
sample, our final sampling procedure was informed by the English FrameNet and SALSA,
a FrameNet-compatible database, for German (Erk, Kowalski, Pado, & Pinkal, 2003).
We gathered all GermanEnglish sentences in the corpus that had at least one pair of
GIZA++-aligned predicates (we , wg ), where we was listed in FrameNet and wg in SALSA,
and where the intersection of the two frame lists for wg and we was non-empty. This corpus
contains 83 frame types, 696 lemma pairs, and 265 unique English and 178 unique German
lemmas. Sentence pairs were grouped into three bands according to their frame frequency
(High, Medium, Low). We randomly selected 380 pairs from each band for annotation.
The total sample consisted of 1,140 bi-sentences. Before semantic annotation took place,
constituency parses for the corpus were obtained from Collins (1997) parser for English and
Dubeys (2005) for German. The automatic parses were then corrected manually, following
the annotation guidelines of the Penn Treebank (English) and the TIGER corpus (German).
2.2 Annotation
After syntactic correction, two annotators with native-level proficiency in German and English annotated each bi-sentence with the frames evoked by we and wg and their semantic
roles (i.e., one frame per monolingual sentence). For every predicate, the task involved two
steps: (a) selecting the appropriate frame and (b) assigning the instantiated semantic roles
to sentence constituents. Annotators were provided with detailed guidelines that explained
the task with multiple examples.
The annotation took place on the gold standard parsed corpus and proceeded in three
phases: a training phase (40 bi-sentences), a calibration phase (100 bi-sentences), and a
production mode phase (1000 bi-sentences). During training, annotators were acquainted
with the annotation style. In the calibration phase, each bi-sentence was doubly annotated to
assess inter-annotator agreement. Finally, in production mode, each of the 1000 bi-sentences
in the main dataset was split and each half randomly assigned to one of the coders for
single annotation. We thus ensured that no annotator saw both parts of any bi-sentence
to avoid any language bias in the role assignment (annotators may be prone to label an
English sentence similar to its German translation and vice versa). Each coder annotated
approximately the same amount of data in English and German and had access to the
FrameNet and SALSA resources.
312

fiCross-lingual Annotation Projection for Semantic Roles

Measure
Frame Match
Role Match

Precision
71.6
90.5

Recall
71.6
92.3

F1-Score
71.6
91.4

Table 3: Semantic parallelism between English and German
The results of our inter-annotator agreement study are given in Table 2. The widely used
Kappa statistic is not directly applicable to our task as it requires a fixed set of items to be
classified into a fixed set of categories. In our case, however, there are no fixed items, since the
span for the frame elements can have any length. In addition, the categories (i.e., frames and
roles) are predicate-specific, and vary from item to item (for a discussion of this issue, see also
the work of Miltsakaki et al., 2004). Instead, we compute three different agreement measures
defined as: the ratio of common frames between two sentences (Frame Match), the ratio of
common roles (Role Match), and the ratio of roles with identical spans (Span Match). As
shown in Table 2, annotators tend to agree in frame assignment; disagreements are mainly
due to fuzzy distinctions between closely related frames (e.g., between Awareness and
Certainty). Annotators also agree on what roles to assign and on identifying role spans.
Overall, we obtain high agreement for all aspects of the annotation, which indicates that
the task is well-defined. We are not aware of published agreement figures on the English
FrameNet annotations, but our results are comparable to numbers reported by Burchardt,
Erk, Frank, Kowalski, Pado, and Pinkal (2006) for German, viz. 85% agreement on frame
assignment (Frame Match) and 86% agreement on role annotation.6
2.3 Evaluation
Recall that the main dataset consists of 1,000 English-German bi-sentences annotated with
FrameNet semantic roles. Since the annotations for each language have been created independently, they can be used to provide an estimate of the degree of semantic parallelism
between the two languages. We measured parallelism using precision and recall, treating
the German annotations as gold standard. This evaluation scheme directly gauges the usability of English as a source language for annotation projection. Less than 100% recall
means that the target language has frames or roles which are not present in English and
cannot be retrieved by annotation projection. Conversely, imperfect precision indicates that
there are English frames or roles whose projection yields erroneous annotations in the target language. Frames and roles are counted as matching if they occur in both halves of a
bi-sentence, regardless of their role spans, which are not comparable across languages.
As shown in Table 3, about 72% of the time English and German evoke the same
frame (Frame Match). This result is encouraging, especially when considering that frame
disagreements also arise within a single language as demonstrated by our inter-annotator
study on the calibration set (see the row Frame Match in Table 2). However, it also indicates
that there is a non-negligible number of cases of translational divergence on the frame level.
These are often cases where one language chooses a single predicate to express a situation
whereas the other one uses complex predication. In the following example, the English
transitive predicate increase evokes the frame Cause change of scalar position (An
6. The parallel corpus we created is available from http://nlpado.de/~sebastian/srl_data.html.

313

fiPado & Lapata

agent or cause increases the position of a variable on some scale). Its German translation
fuhrt zu hoheren (leads to higher) combines the Causation frame evoked by fuhren with
the inchoative Change of scalar position frame introduced by hoher :
(2)

This will increase the level of employment.
Dies wird zu einer hoheren Erwebsquote
fuhren.
This will to a
higher
level of employment lead

At the level of semantic roles, agreement (Role Match) reaches an F1-Score of 91%. This
means that when frames correspond across languages, the roles agree to a large extent.
Role mismatches are frequently cases of passivization or infinitival constructions leading
to role elision. In the example below, remembered and denkt both evoke the Memory
frame. English uses a passive construction which leaves the deep subject position unfilled.
In contrast, German uses an active construction where the deep subject position is filled by
a semantically light pronoun, man (one).
(3)

I ask that [Ireland]Content be remembered.
Ich mochte
darum bitten, dass [man]Cognizer [an Irland]Content denkt.
I would like
to ask that one
of Ireland
thinks

In sum, we find that there is substantial cross-lingual semantic correspondence between
English and German provided that the predicates evoke the same frame. We enlisted the
help of the SALSA database to meet this requirement. Alternatively, we could have used an
existing bilingual dictionary (Fung & Chen, 2004), aligned the frames automatically using a
vector-based representation (Basili et al., 2009) or inferred FrameNet-style predicate labels
for German following the approach proposed by Pado and Lapata (2005).

3. Modeling Semantic Role Projection with Semantic Alignments
Most previous work on projection relies on word alignments to transfer annotations between
languages. This is not surprising, since the annotations of interest are often defined at the
word level (e.g., parts of speech, word senses, or dependencies) and rarely span more than
one token. In contrast, semantic roles can cover sentential constituents of arbitrary length,
and simply using word alignments for projection is likely to result in wrong role spans.
As an example, consider the bi-sentence in Figure 1.7 Assume for now that the (English) source has been annotated with semantic roles which we wish to project onto the
(German) target. Although some alignments (indicated by the dotted lines below the sentence) are accurate (e.g., promised  versprach, to  zu), others are noisy or incomplete
(e.g., time  punktlich instead of on time  punktlich). Based on these alignments, the
Message role would be projected into German onto the (incorrect) word span punktlich zu
instead of punktlich zu kommen, since kommen is not aligned with any English word.
It is of course possible to devise heuristics for amending alignment errors. However, this
process does not scale well: different heuristics need to be created for different errors, and
7. A literal gloss of the German sentence is Kim promises timely to come.

314

fiCross-lingual Annotation Projection for Semantic Roles

S

NP SPEAKER

S
VP

NP SPEAKER

VP
S MESSAGE

S MESSAGE
Kim promised to be on time

Kim versprach, pnktlich zu kommen

Figure 1: Bilingual projection of semantic role information with semantic alignments between constituents.

the process has to be repeated for each new language pair. Instead, our projection model
alleviates this problem in a more principled manner by taking constituency information into
account. Specifically, we induce semantic alignments between source and target sentences
by relying on syntactic constituents to introduce a bias towards linguistically meaningful
spans. For a constituent to be aligned correctly, it is sufficient that a subset of its yield
is correctly word-aligned. So, in Figure 1, we can align to be on time with punktlich zu
kommen and project the role Message accurately, despite the fact that be and kommen are
not aligned with each other. In the following, we describe in detail how semantic alignments
are computed and subsequently guide projection onto the target language.
3.1 Framework Formalization
Each bi-sentence is represented as a set of linguistic units. These are distinguished into
source (us  Us ) and target (ut  Ut ) units and can be words, chunks, constituents, or other
groupings. The semantic roles for the source sentence are modeled as a labeling function
as : R  2Us which maps roles to sets of source units. We view projection as the construction
of a similar role labeling function on the target sentence, at : R  2Ut . Without loss of
generality, we limit ourselves to one frame per sentence, as does FrameNet.8
A semantic alignment A between Us and Ut is a subset of the Cartesian product of the
source and target units:
A  Us  Ut
(4)
An alignment link between us  Us and ut  Ut implies that us and ut are semantically
equivalent. Provided with A and the role assignment function for the source sentence, as ,
projection consists simply of transferring the source labels r onto the union of the target
units that are semantically aligned with the source units bearing the label r:
at (r) = {ut k  us  as (r) : (us , ut )  A}

(5)

8. This entails that we cannot take advantage of potentially beneficial dependencies between the arguments of different predicates within one sentence, which have been shown to improve semantic role
labeling (Carreras, Marquez, & Chrupala, 2004).

315

fiPado & Lapata

We phrase the search for a semantic alignment A as an optimization problem. Specifically,
we seek the alignment that maximizes the product of bilingual similarities sim between
source and target units:
Y
sim(us , ut )
(6)
A = argmax
AA

(us ,ut )A

There are several well-established methods in the literature for computing semantic similarity within one language (see the work of Weeds, 2003, and Budanitsky & Hirst, 2006,
for overviews). Measuring semantic similarity across languages is not as well studied and
there is less consensus on what the appropriate methods are. In this article, we employ a
very simple method, using automatic word alignments as a proxy for semantic equivalence;
however, other similarity measures can be used (see the discussion in Section 6). Following
general convention, we assume that sim is a function ranging from 0 (minimal similarity)
to 1 (maximal similarity).
A wealth of optimization methods can be used to solve (6). In this article, we treat
constituent alignment as a bipartite graph optimization problem. Bipartite graphs provide
a simple and intuitive modeling framework for alignment problems and their optimization algorithms are well-understood and computationally moderate. More importantly, by
imposing constraints on the bipartite graph, we can bias our model against linguistically
implausible alignments, for example alignments that map multiple English roles onto a single German constituent. Different graph topologies correspond to different constraints on
the set of admissible alignments A. For instance, we may want to ensure that all source and
target units are aligned, or to restrict alignment to one-to-one matches (see Section 3.3 for
further details).
More formally, a weighted bipartite graph is a graph G = (V, E) whose node set V is
partitioned into two nonempty sets V1 and V2 in such a way that every edge E joins a
node in V1 to a node in V2 and is labeled with a weight. In our projection application, the
two partitions are the sets of linguistic units Us and Ut , in the source and target sentence,
respectively. We assume that G is complete, that is, each source node is connected to all
target nodes and vice versa.9 Edge weights model the (dis-)similarity between a pair of
source and target units.
The optimization problem from Equation (6) identifies the alignment that maximizes the
product of link similarities which are equivalent to edges in our bipartite graph. Finding an
optimal alignment amounts to identifying a minimum-weight subgraph (Cormen, Leiserson,
& Rivest, 1990)  a subgraph G0 of G that satisfies certain structural constraints (see the
discussion below) while incurring a minimal edge cost:
A = argmin
AA

X

weight(us , ut )

(7)

(us ,ut )A

The minimization problem in Equation (7) is equivalent to the maximization problem in (6)
when setting weight(us , ut ) to:
weight(us , ut ) =  log sim(us , ut )
9. Unwanted alignments can be excluded explicitly by setting their similarity to zero.

316

(8)

fiCross-lingual Annotation Projection for Semantic Roles

S4

S'4
VP2

NP3

NP'3

S1

Kim promised to be on time

VP'2
S'1

Kim versprach, pnktlich zu kommen

(a) Bi-sentence with word alignments

S1
VP2
NP3
S4

S01
0.58
0.45
0
0.37

VP02
0.45
0.68
0
0.55

NP03
0
0
1
0.18

S04
0.37
0.55
0.18
0.73

S1
VP2
NP3
S4

(b) Constituent Similarities

S01
0.54
0.80

1.00

VP02
0.80
0.39

0.60

NP03


0
1.70

S04
1.00
0.60
1.70
0.31

(c) Edge weights

Figure 2: Example of bi-sentence represented as an edge weight matrix
As an example, consider Figure 2. It shows the bi-sentence from Figure 1 and its representation as an edge weight matrix for the corresponding complete bipartite graph. The nodes of
the graph (S1 S4 for source side and S01 S04 for target side) model the sentential constituents.
The numbers in Figure 2b are similarity scores, the corresponding edge weights are shown in
Figure 2c. High similarity scores correspond to low edge weights. Edges with zero similarity
are set to infinity (in practice, to a very large number). Finally, notice that alignments with
high similarity scores (low edge weights) occur in the diagonal of the matrix.
In order to obtain complete projection models we must (a) specify the linguistic units
over which alignment takes place; (b) define an appropriate similarity function; and (c)
formulate the alignment constraints. In the following, we describe two models, one that
uses words as linguistic units and one that uses constituents. We also present appropriate
similarity functions for these models and detail our alignment constraints.
3.2 Word-based Projection
In our first model the linguistic units are word tokens. Source and target sentences are represented by sets of words, Us = {ws1 , ws2 , . . . } and Ut = {wt1 , wt2 , . . . }, respectively. Semantic
alignments here are links between individual words. We can thus conveniently interpret
off-the-shelf word alignments as semantic alignments. Formally, this is achieved with the
following binary similarity function, which trivially turns a word alignment into an optimal
semantic alignment.
(
1 if ws and wt are word-aligned
sim(ws , wt ) =
(9)
0 else
Constraints on admissible alignments are often encoded in word alignment models either
heuristically (e.g., by enforcing one-to-one alignments as in Melamed, 2000) or by virtue of
317

fiPado & Lapata

the translation model used for their computation. For example, the IBM models introduced
in the seminal work of Brown, Pietra, Pietra, and Mercer (1993) require each target word to
be aligned to exactly one source word (which may be the empty word), and therefore allow
one-to-many alignments in one direction. Our experiments use the alignments induced by
the publicly available GIZA++ software (Och & Ney, 2003). GIZA++ yields alignments
by interfacing the IBM models 14 (Brown et al., 1993) with HMM extensions of models 1
and 2 (Vogel, Ney, & Tillmann, 1996). This particular configuration has been shown to
outperform several heuristic and statistical alignment models (Och & Ney, 2003). We thus
take advantage of the alignment constraints already encoded in GIZA++ and assume that
the optimal semantic alignment is given by the set of GIZA++ links. The resulting target
language labeling function is:
aw
t (r) = {wt k  ws  as (r) : ws and wt are GIZA++ word-aligned}

(10)

This labeling function corresponds to the (implicit) labeling functions employed in other
word-based annotation projection models. Such models can be easily derived for different
language pairs without recourse to any corpus-external resources. Unfortunately, as discussed in Section 2, automatically induced alignments are often noisy, thus leading to
projection errors. Cases in point are function words (e.g., prepositions) and multi-word
expressions, which are systematically misaligned due to their high degree of cross-lingual
variation.
3.3 Constituent-based Projection
In our second model the linguistic units are constituents. Source and target sentences
are thus represented by constituent sets (Us = {c1s , c2s , . . . }) and (Ut = {c1t , c2t , . . . }). A
constituent-based similarity function should capture the extent to which cs and its projection ct express the same semantic content. We approximate this by measuring word
alignment-based word overlap between cs and ct with Jaccards coefficient.
Let yield(c) denote the set of words in the yield of a constituent c, and al(c) the set of
words in the target language aligned to the yield of c. Then the word overlap o between a
source constituent cs and a target constituent ct is defined as:
o(cs , ct ) =

|al(cs )  yield(ct )|
|al(cs )  yield(ct )|

(11)

Jaccards coefficient is asymmetric: it will consider how well the projection of a source
constituent al(cs ) matches the target constituent ct , but not vice versa. In order to take
target-source and source-target correspondences into account, we measure word overlap in
both directions and use their mean as a measure of similarity:
sim(cs , ct ) = (o(cs , ct ) + o(ct , cs ))/2

(12)

In addition to a similarity measure, a constituent-based projection model must also specify
the constraints that characterize the set of admissible alignments A. In this paper, we
consider three types of alignment constraints that affect the number of alignment links per
constituent (in graph-theoretic terms, the degree of the nodes in Us ). This focus is motivated
318

fiCross-lingual Annotation Projection for Semantic Roles

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

5

e

5

5

6

e

6

6

r1

(a) Perfect matching

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

r1
r1
r2

(b) Edge cover

Us

Ut

1

1

2

2

r2

3

3

r2

4

4

r1
r1
r2

r1
r2

(c) Total alignment

Figure 3: Constituent alignments and role projections resulting from three families of alignment constraints (Us , Ut : source and target constituents; r1 , r2 : semantic roles).

by patterns that we observe on our gold standard corpus (cf. Section 2). For each English
and German constituent, we determined whether it corresponded to none, exactly one, or
several constituents in the other language, according to a gold standard word alignment.
The majority of constituents correspond to exactly one constituent (67%), followed by a
substantial number of one-to-many/many-to-one correspondences (32%), while cases where
constituents are not translated (i.e., do not have a corresponding node on the other side of
the bi-sentence) are very rare (1%).
This analysis indicates that on perfect data, we should expect the best performance from
a model that allows one-to-many alignments. However, it is a common finding in machine
learning that more restrictive models, even though not appropriate for the data at hand,
yield better results by limiting the hypothesis space. In this spirit, we compare three families
of admissible alignments which range from the more restrictive to the more permissive, and
evaluate them against each other.
3.3.1 Alignments as Perfect Matchings
We first consider the most restrictive case, where each constituent has exactly one adjacent
alignment edge. Semantic alignments with this property can be thought of as bijective
functions: each source constituent is mapped to one target constituent, and vice versa. The
resulting bipartite graphs are perfect matchings. An example of a perfect bipartite matching
is given in Figure 3a. Note that the target side contains two nodes labeled (e), a shorthand
for empty node. Since bi-sentences will often differ in size, the resulting graph partitions
will have different sizes as well. In such cases, we introduce empty nodes in the smaller
partition to enable perfect matching. Empty nodes are assigned a similarity of zero with
all other nodes. Alignments to empty nodes (such as for source nodes (3) and (6)) are
ignored for the purposes of projection; this gives the model the possibility of abstaining
from aligning a node when no good correspondence is found.
Perfect matchings assume a strong equivalence between the constituent structures of
the two languages; neither of the alignments in Figure 3(b) and 3(c) is a perfect matching.
319

fiPado & Lapata

Perfect matchings cannot model one-to-many matches, i.e., cases where semantic material
expressed by one constituent in one language is split into two constituents in the other
language. This means that perfect matchings are most appropriate when the source and
target role annotations span exactly one constituent. While this is not always the case,
perfect matchings also have an advantage over more expressive models: by allowing each
node only one adjacent edge, they introduce strong competition between edges. As a result,
errors in the word alignment can be corrected to some extent.
Perfect matchings can be computed efficiently using algorithms for network optimization (Fredman & Tarjan, 1987) in time approximately cubic in the total number of constituents in a bi-sentence (O(|Us |2 log |Us | + |Us |2 |Ut |)). Furthermore, perfect matchings are
equivalent to the well-known linear assignment problem, for which many solution algorithms
have been developed (e.g., Jonker and Volgenant 1987, time O(max(|Us |, |Ut |)3 )).
3.3.2 Alignments as Edge Covers
We next consider edge covers, a generalization of perfect matchings. Edge covers are bipartite graphs where each source and target constituent is adjacent to at least one edge. This is
illustrated in Figure 3b, where all source and target nodes have one adjacent edge (i.e., alignment link), and some nodes more than one (see source node (2) and target node (4)). Edge
covers impose weaker correspondence assumptions than perfect matchings, since they allow one-to-many alignments between constituents in either direction.10 So, in theory, edge
covers have a higher chance of delivering a correct role projection than perfect matchings
when the syntactic structures of the source and target sentences are different. They can also
deal better with situations where semantic roles are assigned to more than one constituent
in one of the languages (cf. source nodes (3) and (4), labeled with role r2 , in the example
graph). Notice that perfect matchings as shown in Figure 3a are also edge covers, whereas
the graph in Figure 3c is not, as the target-side nodes (1) and (3) have no adjacent edges.
Eiter and Mannila (1997) develop an algorithm for computing optimal edge covers.
They show that minimum-weight edge covers can be reduced to minimum weight perfect
matchings (see above) of an auxiliary bipartite graph with two partitions of size |Us | + |Ut |.
This allows the computation of minimum weight edge covers in time O((|Us | + |Ut |)3 ) =
O(max(|Us |, |Ut |)3 ), which is also cubic in the number of constituents in the bi-sentence.
3.3.3 Total Alignments
The last family of admissible alignments we consider are total alignments. Here, each source
constituent is linked to some target constituent (i.e.,the alignment forms a total function
on the source nodes). Total alignments do not impose any constraints on the target nodes,
which can therefore be linked to an arbitrary number of source nodes, including none. Total
alignments are the most permissive alignment class. In contrast to perfect matchings and
edge covers, constituents in the target sentence can be left unaligned. Total alignments can
10. The general definition of edge covers also allows many-to-many alignments. However, optimal edge covers
according to Equation (7) cannot be many-to-many, since the weight of edge covers with many-to-many
alignments can never be minimal: From each many-to-many edge cover, one edge can be removed,
resulting in an edge cover with a lower weight.

320

fiCross-lingual Annotation Projection for Semantic Roles

be computed by linking each source node to its maximally similar target node:
At = {(cs , ct ) | cs  Us  ct = argmax sim(cs , c0t )}

(13)

c0t Ut

Due to the independence of source nodes, this local optimization results in a globally optimal
alignment. The time complexity of this procedure is quadratic in the number of constituents,
O(|Us ||Ut |).
An example is shown in Figure 3c, where source constituents (1) and (2) correspond
to target constituent (2), and source constituents (3)(6) correspond to (4). Target constituents (1) and (3) are not aligned. The quality of total alignments relies heavily on
the underlying word alignment. Since there is little competition between edges, there is a
tendency to form alignments mostly with high (similarity) scoring target constituents. In
practice, this means that potentially important, but idiosyncratic, target constituents with
low similarity scores will be left unaligned.
3.4 Noise Reduction
As discussed in Section 2, noise elimination techniques are an integral part of projection
architectures. Although our constituent-based model will not be overly sensitive to noise 
we expect the syntactic information to compensate for alignment errors  the word-based
model will be more error-prone since it relies solely on automatically obtained alignments
for transferring semantic roles. Below we introduce three filtering techniques that either
correct or discard erroneous projections.
3.4.1 Filling-the-gaps
According to our definition of projection in Equation (5), the span of a projected role r
corresponds to the union of all target units that are aligned to source units labeled with r.
This definition is sufficient for constituent-based projection models, where roles rarely span
more than one constituent but will yield many wrong alignments for word-based models
where roles will typically span several source units (i.e., words). Due to errors and gaps in
the word alignment, the target span of a role will often be a non-contiguous set of words.
We can repair non-contiguous projections where the first and last word has been projected
correctly by applying a simple heuristic which fills the gaps in a target role span. Let pos
be the index of a word token t in a given sentence, and by extension the set of indices for
a set of words. The target span of role r without any gaps is then defined as:
acc
t (r) = {u | min(pos(at (r)))  pos(u)  max(pos(at (r))}

(14)

We apply this heuristic to all word-based models in this article.
3.4.2 Word Filtering
This technique removes words form a bi-sentence prior to projection according to certain
criteria. We apply two intuitive instantiations of word filtering in our experiments. The
first removes non-content words, i.e., all words which are not adjectives, adverbs, verbs,
or nouns, from the source and target sentences, since alignments for non-content words
321

fiPado & Lapata

S

NP

VP
S

Kim versprach, pnktlich zu kommen

Figure 4: Argument filtering (predicate in boldface, potential arguments in boxes).
are notoriously unreliable and may negatively impact the similarity computations. The
second filter removes all words which remain unaligned in the output of the automatic word
alignment. Both filters aim at distinguishing genuine word alignments from noisy ones and
speed up the computation of semantic alignments.
3.4.3 Argument Filtering
Our last filter applies only to constituent-based models defined over full parse trees. Previous
work in shallow semantic parsing has demonstrated that not all nodes in a tree are equally
probable as semantic roles for a given predicate (Xue & Palmer, 2004). In fact, assuming a
perfect parse, there is a set of likely arguments, to which almost all semantic roles should
be assigned. This set of likely arguments consists of all constituents which are a child
of some ancestor of the predicate, provided that (a) they do not dominate the predicate
themselves and (b) there is no sentence boundary between a constituent and its predicate.
This definition covers long-distance dependencies such as control constructions for verbs, or
support constructions for nouns, and can be extended to accommodate coordination. We
apply this filter to reduce the size of the target tree. In the example in Figure 4, all tree
nodes are removed except the NP Kim and the S punktlich zu kommen.
3.5 Discussion
We have presented a framework for the bilingual projection of semantic roles which is based
on the notion of semantic alignment. We have discussed two broad instantiations of the
framework, namely word-based and constituent-based models. In the latter case, we operationalize the search for an optimal semantic alignment as a graph optimization problem.
Specifically, each bi-sentence is conceptualized as a bipartite graph. Its nodes correspond
to the syntactic constituents of the bi-sentence, and its weighted edges to the cross-lingual
pairwise similarity between constituents. Assumptions about the semantic correspondence
between the languages are formalized as constraints on the graph structure.
We have also discussed three families of constraints. Perfect matching forces correspondences between English and German constituents to be bijective. In contrast, total alignments assume a looser correspondence by leaving constituents on the target side unaligned.
Edge covers occupy a middle ground, assuming that all constituents must be aligned without strictly enforcing one-to-one alignments. While perfect matching is linguistically implausible, by assuming no structural divergence between languages, it can overcome word
alignment errors. Total alignments can model structural changes and are therefore linguis322

fiCross-lingual Annotation Projection for Semantic Roles

LingUnit
words
constituents
constituents
constituents

Similarity
binary
overlap
overlap
overlap

Correspondence
one-to-one
one-to-one
one-to-at-least-one
one-to-many

BiGraph
n/a
perfect matching
edge cover
total

Complexity
linear
cubic
cubic
quadratic

Table 4: Framework instantiations
tically more appropriate, but at the same time more sensitive to alignment errors. Finding
an optimal alignment corresponds to finding the optimal subgraph consistent with our constraints. Efficient algorithms exist for this problem. Finally, we have introduced a small
number of filtering techniques which further reduce the impact of alignment errors.
Our models and their properties are summarized in Table 4. They vary along the following dimensions: the linguistic units employed (LingUnit), the similarity measure (Similarity), their assumptions about semantic correspondence (Correspondence) and the structure
of the bipartite graph this entails (BiGraph). We also mention the complexity of their computation (Complexity). We empirically assess their performance in the following sections.

4. Experiments
We now describe our evaluation of the framework developed in Section 3. We present two
experiments, both of which consider the projection of semantic roles from English sentences
onto their German translations, and evaluate them against German gold standard role annotation. Experiment 1 uses gold standard data for both syntactic and semantic annotation.
This oracle setting assesses the potential of role projection on its own, separating the errors due to translational divergence and our modeling assumptions from those incurred by
preprocessing (e.g., parsing and automatic alignment). Experiment 2 investigates a more
practical setting which employs automatic tools for syntactic and semantic parsing, thus
approximating the conditions of large-scale role projection on parallel corpora.
4.1 Setup
4.1.1 Data
All models were evaluated on the parallel corpus described in Section 2. The corpus was
randomly shuffled and split into a development and a test set (each 50% of the data).
Table 5 reports the number of tokens, sentences, frames, and arguments in the development
and test set for English and German.
Word alignments were computed with the GIZA++ toolkit (Och & Ney, 2003). We
used the entire English-German Europarl bitext as training data to induce alignments for
both directions (source-target, target-source), with the default GIZA++ settings. Following
common practice in Machine Translation, the alignments were symmetrized using the intersection heuristic (Koehn, Och, & Marcu, 2003), which is known to lead to high-precision
alignments. We also produced manual word alignments for all sentences in our corpus,
using the GIZA++ alignments as a starting point and following the Blinker annotation
guidelines (Melamed, 1998).
323

fiPado & Lapata

Language
Dev-EN
Test-EN
Dev-DE
Test-DE

Tokens
11,585
12,019
11,229
11,548

Sentences
491
496
491
496

Frames
491
496
491
496

Roles
2,423
2,465
2,576
2,747

Table 5: Statistics of gold standard parallel corpus broken down into development (Dev)
and test (Test) set.

4.1.2 Method
We implemented the four models shown in Table 4 on their own and with the filtering
techniques introduced in Section 3.4. This resulted in a total of sixteen models, all of which
were evaluated on the development set. Results for the best-performing models were next
validated on the test set. We found the practical runtime of our experiment to be dominated
by input/output XML processing rather than the optimization problem itself.
In both experiments, constituent-based models were compared against the word-based
model, which we treat as a baseline. This latter model is relatively simple: projection relies
exclusively on word alignments, it does not require syntactic analysis, and has linear time
complexity. It thus represents a good point of comparison for models that take linguistic
knowledge into account.
4.1.3 Evaluation Measure
We measure model performance using labeled Precision, Recall, and F1 in the Exact
Match condition, i.e., both the label and the span of the projected English role have to
match the German gold standard role to count as a true positive. We also assess whether differences in performance are statistically significant using stratified shuffling (Noreen, 1989),
an instance of assumption-free approximate randomization testing (see Yeh, 2000, for a discussion).11 Whenever we discuss changes in F1, we refer to absolute (rather than relative)
differences.
4.1.4 Upper Bound
Our annotation study (see Table 2, Section 2.2) obtained an inter-annotator agreement
of 0.84 in the Span Match condition (annotation of the same roles with the same span).
This number can be seen as a reasonable upper bound for the performance of an automatic
semantic role labeling system within a language. It is more difficult to determine a ceiling
for the projection task, since in addition to inter-annotator agreement, we have to take into
account the effect of bilingual divergence. Our annotation study did provide an estimate of
the former, but not of the latter. In default of a method for measuring bilingual agreement
on spans, we adopt the monolingual Span Match agreement as an upper bound for our
projection experiments. Note, however, that this upper bound is not strict  a system with
an oracle should be able to outperform it.
11. The software is available at http://www.nlpado.de/~sebastian/sigf.html.

324

fiCross-lingual Annotation Projection for Semantic Roles

No
Model
WordBL
PerfMatch
EdgeCover
Total
UpperBnd

Filter
Prec
52.0
75.8
71.7
68.9
85.0

Rec
46.2
57.1
61.8
61.3
83.0

F1
48.9
65.1
66.4
64.9
84.0

NA Filter
Model
Prec Rec
WordBL
52.0 46.2
PerfMatch 81.4 69.4
EdgeCover
77.9 69.3
Total
78.8 70.0
UpperBnd
85.0 83.0

F1
48.9
74.9
73.3
74.1
84.0

NC
Model
WordBL
PerfMatch
EdgeCover
Total
UpperBnd

Filter
Prec
37.1
79.4
75.0
69.7
85.0

Rec
32.0
62.2
63.0
60.1
83.0

F1
34.4
69.8
68.5
64.5
84.0

Arg Filter
Model
Prec Rec
WordBL
52.0 46.2
PerfMatch
88.8 56.2
EdgeCover 81.4 69.7
Total
81.2 69.6
UpperBnd
85.0 83.0

F1
48.9
68.8
75.1
75.0
84.0

Table 6: Model comparison on the development set using gold standard parses and semantic
roles and four filtering techniques: no filtering (No Filter), removal of non-aligned
words (NA Filter), removal of non-content words (NC Filter), and removal of nonarguments (Arg Filter). Best performing models are indicated in boldface.

4.2 Experiment 1: Projection on Gold Standard Data
In Experiment 1, we use manually annotated semantic roles and hand-corrected syntactic
analyses for the constituent-based projection models. As explained in Section 4.1, we first
discuss our results on the development set. The best model instantiations are next evaluated
on the test set.
4.2.1 Development Set
Table 6 shows the performance of our models on their own (No Filter) and in combination
with filtering techniques. In the No Filter condition, the word-based model (WordBL) yields
a modest F1 of 48.9% with the application of filling-the-gaps heuristic12 (see Section 3.4 for
details). In the same condition, constituent-based models deliver an F1 increase of approximately 20% (all differences between WordBL and constituent-based models are significant,
p < 0.01). The EdgeCover model performs significantly better than total alignments (Total,
p < 0.05) but comparably to perfect matchings (PerfMatch).
Filtering schemes generally improve the resulting projections for the constituent-based
models. When non-aligned words are removed (NA Filter), F1 increases by 9.8% for PerfMatch, 6.9% for EdgeCover, and 9.2% for Total. PerfMatch and Total are the best performing models with the NA Filter. They significantly outperform EdgeCover (p < 0.05) in
the same condition and all constituent-based models in the No Filter condition (p < 0.01).
The word-based models performance remains constant. By definition WordBL considers
aligned words only; thus, the NA Filter has no impact on its performance.
12. Without filling-the-gaps, F1 drops to 40.8%.

325

fiPado & Lapata

Moderate improvements are observed for constituent-based models when non-content
words are removed (NC filter). PerfMatch performs best in this condition. It is significantly
better than PerfMatch, EdgeCover or Total in the No Filter condition (p < 0.01), but significantly worse than all constituent-based models in the NA filter condition (p < 0.01). The
NC filter yields significantly worse results for WordBL (p < 0.01). This is not surprising,
since the word-based model cannot recover words that have been deleted by a filter, such
as role-initial prepositions or subordinating conjunctions.
Note also that combinations of different filtering techniques can be applied to our
constituent- and word-based models. For example, we can create a constituent-based model
where non-aligned and content words are removed as well as non-arguments. For the sake
of brevity, we do not present results for filter combinations here as they generally do not
improve results further. We find that combining filters tends to remove a large number of
words, and as a result, good alignments are also removed.
Overall, we obtain best performing models when non-argument words are removed
(Arg Filter). Arg Filter is an aggressive filtering technique, since it removes all constituents
but those likely to occupy argument positions. EdgeCover and Total are significantly better
than PerfMatch in the Arg Filter condition (p < 0.01), but perform comparably to PerfMatch when the NA Filter is applied. Moreover, EdgeCover and Total construct almost
identical alignments. This indicates that the two latter models behave similarly when the
alignment space is reduced after removing many possible bad alignments, despite imposing
different constraints on the structure of the bipartite graph. Interestingly, the strict correspondence constraints imposed by PerfMatch result in substantially different alignments.
Recall that PerfMatch attempts to construct a bilingual one-to-one mapping between arguments. When no direct correspondence can be identified for source nodes, it abstains from
projecting. As a result, the alignment produced by PerfMatch shows the highest precision
among all models (88.8%), which is offset by the lowest recall (56.2%). These results tie
in with our earlier analysis of constituent alignments (Section 3.3), where we found that
about one-third of the corpus correspondences are of the one-to-many type. Consider the
following example:
(15)

The Charter means [NP an opportunity to bring the EU closer to the people].
Die Charta bedeutet [NP eine Chance], [S die EU den Burgern naherzubringen].
The Charter means [NP a chance], [S the EU to the citizens to bring closer].

Ideally, the English NP should be aligned to both the German NP and S. EdgeCover, which
can model one-to-many relationships, acts confidently and aligns the NP to the German S
to maximize the overlap similarity, incurring both a precision and recall error. PerfMatch,
on the other hand, cannot handle one-to-many alignments and acts cautiously and makes
only a recall error by aligning the English NP to an empty node. Thus, according to our
evaluation criteria, the analysis of EdgeCover is deemed worse than that of PerfMatch, even
though the former is partly correct.
In sum, filtering improves the resulting projections by making the syntactic analyses
of the source and target sentences more similar to each other. Best results are observed in
NA Filter (PerfMatch) and Arg Filter conditions (Total and EdgeCover). Finally, note that
the best models obtain precision figures that are close to or above the upper bound. The
326

fiCross-lingual Annotation Projection for Semantic Roles

Intersective word
Model
Prec
WordBL
52.9
EdgeCover 86.6
PerfMatch
85.1
UpperBnd
85.0

alignment
Rec
F1
47.4 50.0
75.2 80.5
73.3 78.8
83.0 84.0

Manual word alignment
Model
Prec Rec
F1
WordBL
76.1 73.9 75.0
EdgeCover 86.0 81.8 83.8
PerfMatch
82.8 76.3 79.4
UpperBnd
85.0 83.0 84.0

Table 7: Model performance on the test set with intersective and manual alignments. EdgeCover uses Arg Filter and PerfMatch uses NA Filter. Best performing models are
indicated in boldface.

best recall values are around 70%, significantly below the upper bound of 83%. Aside from
wrongly assigned roles, recall errors are due to short semantic roles (e.g., pronouns), for
which the intersective word alignment often does not contain any alignment links, so that
projection cannot proceed.
4.2.2 Test Set
Our experiments on the test set focus on models which have obtained best results on the
development set using a specific filtering technique. In particular, we report performance
for EdgeCover and PerfMatch in the Arg Filter and NA Filter conditions, respectively. In
addition, we assess the effect of the automatic word alignment on these models by using
both intersective and manual word alignments.
Our results are summarized in Table 7. When intersective alignments are used (lefthand side), EdgeCover performs numerically better than PerfMatch, but the difference is
not statistically significant. This corresponds to our findings on the development set.13 The
right-hand side shows the results when manually annotated word alignments are used. As
can be seen, the performance of WordBL increases sharply from 50.0% to 75.0% (F1). This
underlines the reliance of the word-based model on clean word alignments. Despite this
performance improvement, WordBL still lags behind the best constituent-based model by
approximately 9% F1. This means that there are errors made by the word-based model that
can be corrected by constituent-based models, mostly cases where translation introduced
material in the target sentence that cannot be word-aligned to any expressions in the source
sentence or recovered by the filling-the-gaps heuristic. An example is shown below, where
the translation of clarification as more detailed explanation leads to the introduction of two
German words, die naheren. These words are unaligned at the word level and thus do not
form part of the role when word-based projection is used.
(16)

[Commissioner Barniers clarification]Role
[die naheren
Erlauterungen von Kommissar
Barnier]Role
[the more detailed explanations of Commissioner Barnier]Role

13. Our results on the test set are slightly higher in comparison to the development set. The fluctuation
reflects natural randomness in the partitioning of our corpus.

327

fiPado & Lapata

Evaluation condition
All predicates
Verbs only

Prec
81.3
81.3

Rec
58.6
63.8

F1
68.1
71.5

Table 8: Evaluation of Giuglea and Moschittis (2004) shallow semantic parser on the English side of our parallel corpus (test set)

Constituent-based models generally profit less from cleaner word alignments. Their performance increases by 1%3% F1. The improvement is due to higher recall (approximately
5% in the case of EdgeCover) but not precision. In other words, the main effect of the manually corrected word alignment is to make possible the projection of previously unavailable
roles. EdgeCover performs close to the human upper bound when gold standard alignments
are used. Under noise-free conditions it is able to account for one-to-many constituent alignments, and thus models our corpus better.
Aside from alignment noise, most of the errors we observe in the output of our models
are due to translational divergences, or problematic monolingual role assignments, such as
pronominal adverbs in German. Many German verbs such as glauben (believe) exhibit a
diathesis alternation: they subcategorize either for a prepositional phrase (X glaubt an Y,
X believes in Y), or for an embedded clause which must be preceded by the pronominal
adverb daran (X glaubt daran, dass Y, X believes that Y). Even though the pronominal
adverb forms part of the complement clause (and therefore also of the role assigned to it),
it has no English counterpart. In contrast to example (16) above, the incomplete span dass
Y forms a complete constituent. Unless it is removed by Arg Filter prior to alignment, this
error cannot be corrected by the use of constituents.
4.3 Experiment 2: Projection with Automatic Roles
In this experiment, we evaluate our projection models in a more realistic setting, using
automatic tools for syntactic and semantic parsing.
4.3.1 Preprocessing
For this experiment, we use the uncorrected syntactic analyses of the bilingual corpus as
provided by Collins (1997) and Dubeys (2005) parsers (cf. Section 2.1). We automatically
assigned semantic roles using a state-of-the-art semantic parser developed by Giuglea and
Moschitti (2004). We trained their parser on the FrameNet corpus (release 1.2) using their
standard features and not the extended set which is based on PropBank and would
have required a PropBank analysis of the entire FrameNet corpus.
We applied the shallow semantic parser to the English side of our parallel corpus to
obtain semantic roles, treating the frames as given.14 The task involves locating the frame
elements in a sentence and finding the correct label for a particular frame element. Table 8
shows an evaluation of the parsers output on our test set against the English gold standard
annotation. Giuglea and Moschitti report an accuracy of 85.2% on the role classification
14. This decomposition of frame-semantic parsing has been general practice in recent role labeling tasks,
e.g. at Senseval-3 (Mihalcea & Edmonds, 2004).

328

fiCross-lingual Annotation Projection for Semantic Roles

Model
WordBL
PerfMatch
EdgeCover
UpperBnd

Filter
NA Filter
Arg Filter

Prec
52.5
73.0
70.0
81.3

Rec
34.5
45.4
45.1
58.6

F1
41.6
56.0
54.9
68.1

Table 9: Performance of best constituent-based model on the test set (automatic syntactic
and semantic analysis, intersective word alignment)

task, using the standard feature set.15 Our results are not strictly comparable to theirs,
since we identify the role-bearing constituents in addition to assigning them a label. Our
performance can thus be expected to be worse, since we inherit errors from the frame element
identification stage. Secondly, our test set differs from the training data in vocabulary
(affecting the lexical features) and suffers from parsing errors. Since Giuglea and Moschittis
(2004) implementation can handle only verbs, we also assessed performance on the subset of
verbal predicates (87.5% of test tokens). The difference between the complete and verbs-only
data sets amounts to 3.4% F1, which represents the unassigned roles for nouns.
4.3.2 Setup
We report results for the word-based baseline model, and the best projection models from
Experiment 1, namely PerfMatch (NA filter) and EdgeCover (Arg Filter). We use the the
complete test set (including nouns and adjectives) in order to make our evaluation comparable to Experiment 1.
4.3.3 Results
The results are summarized in Table 9. Both PerfMatch (NA Filter) and EdgeCover
(Arg Filter) perform comparably at 5556% F1. This is approximately 25 points F1 worse
than the results obtained on manual annotation (compare Table 9 to Table 7). WordBLs
performance (now 41.6% F1) degrades less (around 8% F1), since it is only affected by
semantic role assignment errors. However, consistently with Experiment 1, the constituentbased models sill outperform WordBL by more than 10% F1 (p < 0.01). These results
underscore the ability of bracketing information, albeit noisy, to correct and extend word
alignment. Although the Arg Filter performed well in Experiment 1, we observe less of an
effect here. Recall that the filter uses not only bracketing, but also dominance information,
and is therefore particularly vulnerable to misparses. Like in Experiment 1, we find that
our models yield overall high precision but low recall. Precision drops by 15% F1 when
automatic annotations are used, whereas recall drops by 30%; however, note that this drop
includes about 5% nominal roles that fall outside the scope of the shallow semantic parser.
Further analysis showed that parsing errors form a large source of problems in Experiment 2. German verb phrases are particularly problematic. Here, the relatively free
word order combines with morphological ambiguity to produce ambiguous structures, since
15. See the work of Giuglea and Moschitti (2006) for an updated version of their shallow semantic parser.

329

fiPado & Lapata

Band
Error 0
Error 1
Error 2+

Prec
85.1
75.9
40.7

Rec
74.1
34.6
18.5

F1
79.2
47.6
25.4

Table 10: PerfMatchs performance in relation to error rate in the automatic semantic role
labeling (Error 0: no labeling errors, Error 1: one labeling error, Error 2+: two
or more labeling errors).

third person plural verb forms (FIN) are identical to infinitival forms (INF). Consider the
following English sentence, (17a), and two syntactic analyses of its German translation,
(17b)/(17c):
(17)

a.
b.
c.

when we recognize [that we have to work on this issue]
wenn wir erkennenFIN , [dass wir [daran arbeitenINF ] mussenFIN ]
when we recognize [that we have to [work on it]]
wenn wir [erkennenINF , [dass wir daran arbeitenFIN ]] mussenFIN
when we have to [recognize that [we work on it]]

(17b) gives the correct syntactic analysis, but the parser we used produced the highly
implausible (17c). As a result, the English sentential complement that we have to work on
this issue cannot be aligned to a single German constituent, nor to a combination of them.
In this situation, PerfMatch will generally not align the constituent at all and thus sacrifice
recall. EdgeCover (and Total) will produce a (wrong) alignment and sacrifice precision.
Finally, we evaluated the impact of semantic role labeling errors on projection. We split
the semantic parsers output into three bands: (a) sentences with no role labeling errors
(Error 0, 35% of the test set), (b) sentences with one labeling error (Error 1, 33% of the
test set), and (c) sentences with two or more labeling errors (Error 2+, 31% of the test set).
Table 10 shows the performance of the best model, PerfMatch (NA filter), for each of these
bands. As can be seen, when the output of the automatic labeler is error-free, projection
attains an F1 of 79.2%, comparable to the results obtained in Experiment 1.16 Even though
projection clearly degrades with the quality of the semantic role input, PerfMatch still
delivers projections with high precision for the Error 1 band. As discussed above, the low
recall values for bands Error 1 and 2+ result from the labelers low recall.

5. Related Work
Previous work on annotation projection has primarily focused on annotations spanning
short linguistic units. These range from POS tags (Yarowsky & Ngai, 2001), to NP chunks
(Yarowsky & Ngai, 2001), dependencies (Hwa et al., 2002), and word senses (Bentivogli &
Pianta, 2005). A different strategy for the cross-lingual induction of frame-semantic information is presented by Fung and Chen (2004), who do not require a parallel corpus. Instead,
16. It is reasonable to assume that in these sentences, at least the relevant part of the syntactic analysis is
correct.

330

fiCross-lingual Annotation Projection for Semantic Roles

they use a bilingual dictionary to construct a mapping between FrameNet entries and concepts in HowNet, an on-line ontology for Chinese.17 In a second step, they use HowNet
knowledge to identify monolingual Chinese sentences with predicates that instantiate these
concepts, and label their arguments with FrameNet roles. Fung and Chen report high accuracy values, but their method relies on the existence of resources which are presumably
unavailable for most languages (in particular, a rich ontology). Recently, Basili et al. (2009)
propose an approach to semantic role projection that is not word-based and does not employ syntactic information. Using a phrase-based SMT system, they heuristically assemble
target role spans out of phrase translations, defining phrase similarity in terms of translation probability. Their method occupies a middle ground between word-based projection
and constituent-based projection.
The work described in this article relies on a parallel corpus for harnessing information
about semantic correspondences. Projection works by creating semantic alignments between
constituents. The latter correspond to nodes in a bipartite graph, and the search for the best
alignment is cast as an optimization problem. The view of computing optimal alignments
by graph matching is relatively widespread in the machine translation literature (Melamed,
2000; Matusov, Zens, & Ney, 2004; Tiedemann, 2003; Taskar, Lacoste-Julien, & Klein, 2005).
Despite individual differences, most approaches formalize word alignment as a minimumweight matching problem, where each pair of words in a bi-sentence is associated with a
score representing the desirability of that pair. The alignment for the bi-sentence is the
highest scoring matching under some constraints, for example that matchings must be oneto-one. Our work applies graph matching to the level of constituents and compares a larger
class of constraints (see Section 3.3) than previous approaches. For example, Taskar et al.
(2005) examine solely perfect matchings and Matusov et al. (2004) only edge covers.
A number of studies have addressed the constituent alignment problem in the context of
extracting of translation patterns (Kaji, Kida, & Morimoto, 1992; Imamura, 2001). However,
most approaches only search for pairs of constituents which are perfectly word aligned, an
infeasible strategy when alignments are obtained automatically. Other work focuses on the
constituent alignment problem, but uses greedy search techniques that are not guaranteed to
find an optimal solution (Matsumoto, Ishimoto, & Utsuro, 1993; Yamamoto & Matsumoto,
2000). Meyers, Yangarber, and Grishman (1996) propose an algorithm for aligning parse
trees which is only applicable to isomorphic structures. Unfortunately, this restriction limits
their application to structurally similar languages and high-quality parse trees.
Although we evaluate our models only on the semantic role projection task, we believe
they also show promise in the context of SMT, especially for systems that use syntactic
information to enhance translation quality. For example, Xia and McCord (2004) exploit
constituent alignment for rearranging sentences in the source language so as to make their
word order similar to that of the target language. They learn tree reordering rules by aligning
constituents heuristically, using an optimization procedure analogous to the total alignment
model presented in this article. A similar approach is described in a paper by Collins, Koehn,
and Kucerova (2005); however, the rules are manually specified and the constituent alignment step reduces to inspection of the source-target sentence pairs. The different alignment
17. For information on HowNet, see http://www.keenage.com/zhiwang/e_zhiwang.html.

331

fiPado & Lapata

models presented in this article could be easily employed for the reordering task common
to both approaches.

6. Conclusions
In this article, we have argued that parallel corpora show promise in relieving the lexical acquisition bottleneck for new languages. We have proposed annotation projection as a means
of obtaining FrameNet annotations automatically, using resources available for English and
exploiting parallel corpora. We have presented a general framework for projecting semantic
roles that capitalizes on the use of constituent information during projection, and modelled
the computation of a constituent alignment as the search for an optimal subgraph in a
bipartite graph. This formalization allows us to solve the search problem efficiently using
well-known graph optimization methods. Our experiments have focused on three modeling
aspects: the level of noise in the linguistic annotation, constraints on alignments, and noise
reduction techniques.
We have found that constituent information yields substantial improvements over word
alignments. Word-based models offer a starting point for low-density languages for which
parsers are not available. However, word alignments are too noisy and fragmentary to deliver
accurate projections for annotations with long spans such as semantic roles. Our experiments have compared and contrasted three constituent-based models which differ in their
assumptions regarding cross-lingual correspondence (total alignments, edge covers, and perfect matchings). Perfect matchings, a restrictive alignment model that enforces one-to-one
alignments, performed most reliably across all experimental conditions. In particular, its
precision surpassed all other models. This indicates that a strong semantic correspondence
can be assumed as a modelling strategy, at least for English and German and the parsing
tools available for these languages. As a side effect, the performance of constituent-based
models increases only slightly when manual word alignments are used, which means that
near-optimal results can be obtained using automatic alignments.
As far as alignment noise reduction techniques are concerned, we find that removing
non-aligned words (NA Filter) and non-arguments (Arg Filter) yields the best results. Both
filters are independent of the language pair and make only weak assumptions about the
underlying linguistic representations in question. The choice of the best filter depends on
the goals of projection. Removing non-aligned words is relatively conservative and tends
to balance precision and recall. In contrast, the more aggressive filtering of non-arguments
yields projections with high precision and low recall. Arguably, for training shallow semantic
parsers on the target language (Johansson & Nugues, 2006), it is more desirable to have
access to high-quality projections. However, there is a number of options for increasing
precision subsequent to projection that we have not explored in this article. One fully automatic possibility is generalization over multiple occurrences of the same predicate to detect
and remove suspicious projection instances, e.g. following the work by Dickinson and Lee
(2008). Another direction is postprocessing by annotators, e.g., by adopting the annotate
automatically, correct manually methodology used to provide high volume annotation in
the Penn Treebank project. Our models could also be used in a semi-supervised setting,
e.g., to provide training data for unknown predicates.
332

fiCross-lingual Annotation Projection for Semantic Roles

The extensions and improvements of the framework presented here are many and varied.
Firstly, we believe that the models developed in this article are useful for other semantic
role paradigms besides FrameNet, or indeed for other types of semantic annotation. Potential applications include the projection of PropBank roles18 , discourse structure, and
named entities. As mentioned earlier, our models are also relevant for machine translation
and could be used for the reordering of constituents. Our results indicate that syntactic
knowledge on the target side plays an important role in projecting annotations with longer
spans. Unfortunately, for many languages there are no broad-coverage parsers available.
However, it may not be necessary to obtain complete parses for the semantic role projection
task. Two types of syntactic information are especially valuable here: bracketing information (which guides projection towards linguistically plausible role spans) and knowledge
about the arguments of sentence predicates. Bracketing information can be acquired in an
unsupervised fashion (Geertzen, 2003). Argument structure information could be obtained
from dependency parsers (e.g., McDonald, 2006) or partial parsers that are able to identify
predicate-argument relations (e.g., Hacioglu, 2004). Another interesting direction concerns
the combination of longer phrases, like those provided by phrase-based SMT systems, with
constituent information obtained from the output of a parser or chunker.
The experiments presented in this article made use of a simple semantic similarity
measure based on word alignment. A more sophisticated approach could have combined
the alignment scores with information provided in a bilingual dictionary. Inspired by crosslingual information retrieval, Widdows, Dorow, and Chan (2002) propose a bilingual vector
model. The underlying assumption is that words that have similar co-occurrences in a
parallel corpus are also semantically similar. Source and target words are represented as
n-dimensional vectors whose components correspond to the most frequent content words in
the source language. In this framework, the similarity of any source-target word pair can be
computed using a geometric measure such as cosine or Euclidean distance. The more recent
polylingual topic models (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009) offer
a probabilistic interpretation of a similar idea.
In this article, we have limited ourselves to parallel sentences where the frame is preserved. This allows us to transfer roles directly from the source onto the target language
without having to acquire knowledge about possible translational divergences first. A generalization of the framework presented here could adopt a strategy where some form of
mapping is applied during projection, akin to the transfer rules used in machine translation.
Thus far, we have only explored models applying the identity mapping. Knowledge about
other possible mappings can be acquired from manually annotated parallel corpora (Pado
& Erk, 2005). An interesting avenue for future work is to identify semantic role mappings
in a fully automatic fashion.
Acknowledgments
We are grateful to the three anonymous referees whose feedback helped to improve the
present article. Special thanks are due to Chris Callison-Burch for the linearb word alignment user interface, to Ana-Maria Giuglea and Alessandro Moschitti for providing us with
18. Note, however, that the cross-lingual projection of PropBank roles raises the question of their interpretation; see the discussion by Fung et al. (2007).

333

fiPado & Lapata

their shallow semantic parser, and to our annotators Beata Kouchnir and Paloma Kreischer. We acknowledge the financial support of DFG (Pado; grant Pi-154/9-2) and EPSRC
(Lapata; grant GR/T04540/01).

References
Basili, R., Cao, D., Croce, D., Coppola, B., & Moschitti, A. (2009). Cross-language frame
semantics transfer in bilingual corpora. In Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, pp. 332345,
Mexico City, Mexico.
Bentivogli, L., & Pianta, E. (2005). Exploiting parallel texts in the creation of multilingual semantically annotated resources: the MultiSemCor corpus. Natural Language
Engineering, 11 (3), 247261.
Boas, H. C. (2005). Semantic frames as interlingual representations for multilingual lexical
databases. International Journal of Lexicography, 18 (4), 445478.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). Mathematics
of statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2006). Consistency and coverage: Challenges for exhaustive semantic annotation. 28. Jahrestagung,
Deutsche Gesellschaft fur Sprachwissenschaft. Bielefeld, Germany.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado, S., & Pinkal, M. (2009). Framenet
for the semantic analysis of German: Annotation, representation and automation. In
Boas, H. (Ed.), Multilingual FrameNet. Mouton de Gruyter. To appear.
Burchardt, A., & Frank, A. (2006). Approaching textual entailment with LFG and
FrameNet frames. In Proceedings of the RTE-2 Workshop, Venice, Italy.
Burnard, L. (2000). The Users Reference Guide for the British National Corpus (World
Edition). British National Corpus Consortium, Oxford University Computing Service.
Carreras, X., & Marquez, L. (2005). Introduction to the CoNLL-2005 shared task: Semantic
role labeling. In Proceedings of the 9th Conference on Computational Natural Language
Learning, pp. 152164, Ann Arbor, MI.
Carreras, X., Marquez, L., & Chrupala, G. (2004). Hierarchical recognition of propositional
arguments with perceptrons. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pp. 106109, Boston, MA.
Collins, M. (1997). Three generative, lexicalised models for statistical parsing. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics, pp. 16
23, Madrid, Spain.
Collins, M., Koehn, P., & Kucerova, I. (2005). Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pp. 531540, Ann Arbor, MI.
334

fiCross-lingual Annotation Projection for Semantic Roles

Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction to Algorithms. MIT Press.
Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging using
parallel corpora. In Proceedings of the 40th Annual Meeting of the Association of
Computational Linguistics, pp. 255262, Philadelphia, PA.
Dickinson, M., & Lee, C. M. (2008). Detecting errors in semantic annotation. In Proceedings of the 6th International Conference on Language Resources and Evaluation,
Marrakech, Morocco.
Dorr, B. (1995). Machine translation divergences: A formal description and proposed solution. Computational Linguistics, 20 (4), 597633.
Dowty, D. (1991). Thematic proto-roles and argument selection. Language, 67, 547619.
Dubey, A. (2005). What to do when lexicalization fails: parsing German with suffix analysis
and smoothing. In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pp. 314321, Ann Arbor, MI.
Eiter, T., & Mannila, H. (1997). Distance measures for point sets and their computation..
Acta Informatica, 34 (2), 109133.
Erk, K., Kowalski, A., Pado, S., & Pinkal, M. (2003). Towards a resource for lexical semantics: A large German corpus with extensive semantic annotation. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguistics, pp. 537544,
Sapporo, Japan.
Fillmore, C. J. (1968). The case for case. In Bach, & Harms (Eds.), Universals in Linguistic
Theory, pp. 188. Holt, Rinehart, and Winston, New York.
Fillmore, C. J. (1982). Frame semantics. In Linguistics in the Morning Calm, pp. 111137.
Hanshin, Seoul, Korea.
Fillmore, C. J., Johnson, C. R., & Petruck, M. R. (2003). Background to FrameNet. International Journal of Lexicography, 16, 235250.
Fleischman, M., & Hovy, E. (2003). Maximum entropy models for FrameNet classification. In Proceedings of the 8th Conference on Empirical Methods in Natural Language
Processing, pp. 4956, Sapporo, Japan.
Frank, A., Krieger, H.-U., Xu, F., Uszkoreit, H., Crysmann, B., Jorg, B., & Schafer, U.
(2007). Question answering from structured knowledge sources. Journal of Applied
Logic, 5 (1), 2048.
Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps and their uses in improved network
optimization algorithms. Journal of the ACM, 34 (3), 596615.
Fung, P., & Chen, B. (2004). BiFrameNet: Bilingual frame semantics resources construction
by cross-lingual induction. In Proceedings of the 20th International Conference on
Computational Linguistics, pp. 931935, Geneva, Switzerland.
Fung, P., Wu, Z., Yang, Y., & Wu, D. (2006). Automatic learning of Chinese-English
semantic structure mapping. In Proceedings of the IEEE/ACL Workshop on Spoken
Language Technology, Aruba.
335

fiPado & Lapata

Fung, P., Wu, Z., Yang, Y., & Wu, D. (2007). Learning bilingual semantic frames: Shallow
semantic parsing vs. semantic role projection. In Proceedings of the 11th Conference
on Theoretical and Methodological Issues in Machine Translation, pp. 7584, Skovde,
Sweden.
Geertzen, J. (2003). String alignment in grammatical inference: what suffix trees can do.
Masters thesis, ILK, Tilburg University, Tilburg, the Netherlands.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational
Linguistics, 28 (3), 245288.
Gimenez, J., & Marquez, L. (2007). Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Workshop on Statistical Machine
Translation, pp. 256264, Prague, Czech Republic.
Giuglea, A.-M., & Moschitti, A. (2004). Knowledge discovery using FrameNet, VerbNet and
PropBank. In Proceedings of the Workshop on Ontology and Knowledge Discovering
at the 15th European Conference on Machine Learning, Pisa, Italy.
Giuglea, A.-M., & Moschitti, A. (2006). Semantic role labeling via FrameNet, VerbNet
and PropBank. In Proceedings of the 44th Annual Meeting of the Association for
Computational Linguistics, pp. 929936, Sydney, Australia.
Grenager, T., & Manning, C. (2006). Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 11th Conference on Empirical Methods in Natural Language
Processing, pp. 18, Sydney, Australia.
Hacioglu, K. (2004). A lightweight semantic chunker based on tagging. In Proceedings of
the joint Human Language Technology Conference and Annual Meeting of the North
American Chapter of the Association for Computational Linguistics, pp. 145148,
Boston, MA.
Hi, C., & Hwa, R. (2005). A backoff model for bootstrapping resources for non-english
languages. In Proceedings of the joint Human Language Technology Conference and
Conference on Empirical Methods in Natural Language Processing, pp. 851858, Vancouver, BC.
Hwa, R., Resnik, P., Weinberg, A., & Kolak, O. (2002). Evaluation translational correspondance using annotation projection. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, pp. 392399, Philadelphia, PA.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers
via syntactic projection across parallel texts. Journal of Natural Language Engineering, 11 (3), 311325.
Imamura, K. (2001). Hierarchical phrase alignment harmonized with parsing.. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pp. 377384,
Tokyo, Japan.
Jackendoff, R. S. (1990). Semantic Structures. The MIT Press, Cambridge, MA.
Johansson, R., & Nugues, P. (2006). A FrameNet-Based Semantic Role Labeler for Swedish.
In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, pp. 436443, Sydney, Australia.
336

fiCross-lingual Annotation Projection for Semantic Roles

Jonker, R., & Volgenant, A. (1987). A shortest augmenting path algorithm for dense and
sparse linear assignment problems. Computing, 38, 325340.
Kaji, H., Kida, Y., & Morimoto, Y. (1992). Learning translation templates from bilingual
text. In Proceedings of the 14th International Conference on Computational Linguistics, pp. 672678, Nantes, France.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. In Proceedings of the joint Human Language Technology Conference and Annual Meeting of
the North American Chapter of the Association for Computational Linguistics, pp.
4854, Edmonton, AL.
Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit X, Phuket, Thailand.
Marquez, L., Carreras, X., Litkowski, K. C., & Stevenson, S. (2008). Semantic role labeling:
An introduction to the special issue. Computational Linguistics, 34 (2), 145159.
Matsumoto, Y., Ishimoto, H., & Utsuro, T. (1993). Structural matching of parallel texts.
In Proceedings of ACL 31st Annual Meeting of the Association for Computational
Linguistics, pp. 2330, Columbus, OH.
Matusov, E., Zens, R., & Ney, H. (2004). Symmetric word alignments for statistical maching
translation. In Proceedings of the 20th International Conference on Computational
Linguistics, pp. 219225, Geneva, Switzerland.
McDonald, R. (2006). Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.
Melamed, I. D. (1998). Manual annotation of translational equivalence: The Blinker project.
Tech. rep. IRCS TR #98-07, IRCS, University of Pennsylvania.
Melamed, I. D. (2000). Models of translational equivalence among words. Computational
Linguistics, 2 (23), 221249.
Meyers, A., Yangarber, R., & Grishman, R. (1996). Alignment of shared forests for bilingual corpora. In Proceedings of the 16th International Conference on Computational
Linguistics, pp. 460465, Copenhagen, Denmark.
Mihalcea, R., & Edmonds, P. (Eds.). (2004). Proceedings of Senseval-3: The 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,
Barcelona, Spain.
Miltsakaki, E., Prasad, R., Joshi, A., & Webber, B. (2004). Annotating discourse connectives
and their arguments. In Proceedings of the NAACL/HLT Workshop on Frontiers in
Corpus Annotation, Boston, MA.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual topic models. In Proceedings of the 14th Conference on Empirical Methods in
Natural Language Processing, pp. 880889, Singapore.
Moschitti, A. (2008). Kernel methods, syntax and semantics for relational text categorization. In Proceedings of the 17th ACM Conference on Information and Knowledge
Management, pp. 253262, Napa Valley, CA.
337

fiPado & Lapata

Moschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting syntactic
and shallow semantic kernels for question answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computational Linguistics, pp. 776783,
Prague, Czech Republic.
Narayanan, S., & Harabagiu, S. (2004). Question answering based on semantic structures.
In Proceedings of the 20th International Conference on Computational Linguistics, pp.
693701, Geneva, Switzerland.
Noreen, E. (1989). Computer-intensive Methods for Testing Hypotheses: An Introduction.
John Wiley and Sons Inc.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment
models. Computational Linguistics, 29 (1), 1952.
Ohara, K. H., Fujii, S., Saito, H., Ishizaki, S., Ohori, T., & Suzuki, R. (2003). The Japanese
FrameNet project: A preliminary report. In Proceedings of the 6th Meeting of the
Pacific Association for Computational Linguistics, pp. 249254, Halifax, Nova Scotia.
Pado, S., & Erk, K. (2005). To cause or not to cause: Cross-lingual semantic matching for
paraphrase modelling. In Proceedings of the EUROLAN Workshop on Cross-Linguistic
Knowledge Induction, Cluj-Napoca, Romania.
Pado, S., & Lapata, M. (2005). Cross-lingual bootstrapping for semantic lexicons. In
Proceedings of the 22nd National Conference on Artificial Intelligence, pp. 10871092,
Pittsburgh, PA.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The Proposition Bank: An annotated
corpus of semantic roles. Computational Linguistics, 31 (1), 71106.
Postolache, O., Cristea, D., & Orasan, C. (2006). Tranferring coreference chains through
word alignment. In Proceedings of the 5th International Conference on Language
Resources and Evaluation, Genoa, Italy.
Riloff, E., Schafer, C., & Yarowsky, D. (2002). Inducing information extraction systems for
new languages via cross-language projection. In Proceedings of the 19th International
Conference on Computational Linguistics, pp. 828834, Taipei, Taiwan.
Shen, D., & Lapata, M. (2007). Using semantic roles to improve question answering. In
Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing, pp. 1221, Prague, Czech Republic.
Spreyer, K., & Frank, A. (2008). Projection-based acquisition of a temporal labeller. In
Proceedings of the 3rd International Joint Conference on Natural Language Processing,
pp. 489496, Hyderabad, India.
Subirats, C., & Petruck, M. (2003). Surprise: Spanish FrameNet. In Proceedings of the
Workshop on Frame Semantics, XVII. International Congress of Linguists, Prague,
Czech Republic.
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using predicate-argument
structures for information extraction. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pp. 815, Sapporo, Japan.
338

fiCross-lingual Annotation Projection for Semantic Roles

Swier, R. S., & Stevenson, S. (2004). Unsupervised semantic role labelling. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing, pp. 95102.
Bacelona, Spain.
Swier, R. S., & Stevenson, S. (2005). Exploiting a verb lexicon in automatic semantic
role labelling. In Proceedings of the Joint Human Language Technology Conference
and Conference on Empirical Methods in Natural Language Processing, pp. 883890,
Vancouver, British Columbia.
Taskar, B., Lacoste-Julien, S., & Klein, D. (2005). A discriminative matching approach to
word alignment. In Proceedings of the joint Human Language Technology Conference
and Conference on Empirical Methods in Natural Language Processing, pp. 7380,
Vancouver, BC.
Tatu, M., & Moldovan, D. (2005). A semantic approach to recognizing textual entailment.
In Proceedings of the joint Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing, pp. 371378, Vancouver, BC.
Taule, M., Mart, M., & Recasens, M. (2008). Ancora: Multilevel annotated corpora for
Catalan and Spanish. In Proceedings of 6th International Conference on Language
Resources and Evaluation, Marrakesh, Morocco.
Tiedemann, J. (2003). Combining clues for word alignment. In Proceedings of the 16th
Meeting of the European Chapter of the Association for Computational Linguistics,
pp. 339346, Budapest, Hungary.
Tokarczyk, A., & Frank, A. (2009). Cross-lingual projection of LFG f-structures: Resource
induction for Polish. In Proceedings of Lexical Functional Grammar 2009, Cambridge,
UK.
van Leuven-Zwart, K. M. (1989). Translation and original: Similarities and dissimilarities.
Target, 1 (2), 151181.
Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics, pp. 836841, Copenhagen.
Weeds, J. (2003). Measures and Applications of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex.
Widdows, D., Dorow, B., & Chan, C.-K. (2002). Using parallel corpora to enrich multilingual
lexical resources. In Proceedings of the 3rd International Conference on Language
Resources and Evaluation, pp. 240245, Las Palmas, Canary Islands.
Wu, D., & Fung, P. (2009a). Can semantic role labeling improve SMT?. In Proceedings
of the 13th Annual Conference of the European Association for Machine Translation,
pp. 218225, Barcelona, Spain.
Wu, D., & Fung, P. (2009b). Semantic roles for SMT: A hybrid two-pass model. In Proceedings of the joint Human Language Technology Conference and Annual Meeting of
the North American Chapter of the Association for Computational Linguistics, pp.
1316, Boulder, CO.
339

fiPado & Lapata

Xia, F., & McCord, M. (2004). Improving a statistical MT system with automatically
learned rewrite patterns. In Proceedings of the 20th International Conference on
Computational Linguistics, pp. 508514, Geneva, Switzerland.
Xue, N., & Palmer, M. (2004). Calibrating features for semantic role labeling. In Proceedings
of the 9th Conference on Empirical Methods in Natural Language Processing, pp. 88
94, Barcelona, Spain.
Xue, N., & Palmer, M. (2009). Adding semantic roles to the Chinese treebank. Natural
Language Engineering, 15 (1), 143172.
Yamamoto, K., & Matsumoto, Y. (2000). Acquisition of phrase-level bilingual correspondence using dependency structure. In Proceedings of the 18th International Conference
on Computational Linguistics, pp. 933939, Saarbrucken, Germany.
Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers and NP bracketers via
robust projection across aligned corpora. In Proceedings of the 2nd Annual Meeting
of the North American Chapter of the Association for Computational Linguistics, pp.
200207, Pittsburgh, PA.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis
tools via robust projection across aligned corpora. In Proceedings of the 1st Human
Language Technology Conference, pp. 161168, San Francisco, CA.
Yeh, A. (2000). More accurate tests for the statistical significance of result differences. In
Proceedings of the 18th International Conference on Computational Linguistics, pp.
947953, Saarbrucken, Germany.

340

fiJournal of Artificial Intelligence Research 36 (2009) 547-556

Submitted 06/09; published 12/09

Research Note
Soft Goals Can Be Compiled Away
Emil Keyder

emil.keyder@upf.edu

Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona Spain

Hector Geffner

hector.geffner@upf.edu

ICREA & Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona Spain

Abstract
Soft goals extend the classical model of planning with a simple model of preferences.
The best plans are then not the ones with least cost but the ones with maximum utility,
where the utility of a plan is the sum of the utilities of the soft goals achieved minus
the plan cost. Finding plans with high utility appears to involve two linked problems:
choosing a subset of soft goals to achieve and finding a low-cost plan to achieve them. New
search algorithms and heuristics have been developed for planning with soft goals, and a
new track has been introduced in the International Planning Competition (IPC) to test
their performance. In this note, we show however that these extensions are not needed:
soft goals do not increase the expressive power of the basic model of planning with action
costs, as they can easily be compiled away. We apply this compilation to the problems
of the net-benefit track of the most recent IPC, and show that optimal and satisficing
cost-based planners do better on the compiled problems than optimal and satisficing netbenefit planners on the original problems with explicit soft goals. Furthermore, we show
that penalties, or negative preferences expressing conditions to avoid, can also be compiled
away using a similar idea.

1. Models
A STRIPS problem is a tuple P = hF, I, O, Gi where F is a set of fluents, I  F and
G  F are the initial state and goal situation, and O is a set of actions or operators with
precondition, add, and delete lists P re(a), Add(a), and Del(a) respectively, all of which
are subsets of F . An action sequence  = ha0 , . . . , an i is applicable in P if the actions ai ,
i = 0, . . . , n, are all in O, and there exists a sequence of states hs0 , . . . , sn+1 i, such that
s0 = I, P re(ai )  si and si+1 = si  Add(ai ) \ Del(ai ) for i = 0, . . . , n. The applicable
action sequence  achieves a fluent g if g  sn+1 , and is a plan for P if it achieves each goal
g in G, which we write as  |= G. In the classical setting, the cost of a plan c() is given
by ||, the number of actions in . This cost structure is generalized with the addition of
a cost function over the operators:
c
2009
AI Access Foundation. All rights reserved.

fiKeyder & Geffner

Definition 1 A STRIPS problem with action costs is a tuple Pc = hF, I, O, G, ci, where
+
P = hF, I, O, Gi is a STRIPS problem and c is a function c : O 
7 R+
0 where R0 stands for
the non-negative reals.
The cost of a plan  for a problem Pc is then given by
||
X

c() =

c(ai )

(1)

i=1

where ai denotes the i th action in . The cost function c() = || is obtained as a special
case when c(o) = 1 for all o  O. Adding utilities or soft goals to the problem formulation
results in a new model:
Definition 2 A STRIPS problem with action costs and soft goals is a tuple Pu = hF, I, O,
G, c, ui, where P = hF, I, O, G, ci is a STRIPS problem with action costs, and u is a partial
function u : F 7 R+ that maps a subset of fluents (the soft goals) into positive reals.
In a STRIPS problem with soft goals Pu , the utility of a plan is given by the difference
between the total utility obtained by the plan and its cost:
u() =

X

u(p)  c() .

(2)

p:|=p

A plan  for a problem with soft goals Pu is optimal when no other plan  0 has a utility
u( 0 ) higher than u(). The utility of an optimal plan for a problem with no hard goals is
never negative, as the empty plan has non-negative utility and zero cost.
The most recent International Planning Competition (IPC6) featured Sequential Optimal and Net Benefit Optimal tracks in which the objective was to find optimal plans with
respect to the models captured by Equation 1 and Equation 2 respectively (Helmert, Do,
& Refanidis, 2008).1

2. Equivalence
Given a problem P with soft goals, an equivalent problem P 0 with action costs and no soft
goals can be defined whose plans encode corresponding plans for P . This transformation,
first introduced by Keyder and Geffner (2007), is simple and direct, yet seems to have escaped the attention of researchers in the area (Smith, 2004; Sanchez & Kambhampati, 2005;
Bonet & Geffner, 2008; Baier, Bacchus, & McIlraith, 2007). Also, unlike the compilation of
soft goals into numeric variables and arbitrary plan metrics (Edelkamp, 2006), the proposed
transformation makes use of neither and requires from planners only the ability to handle
1. In PDDL3, soft goals are represented by expressions of the form ( u (is-violated hprefi)) appearing
in the problem metric where pref is a preference or soft goal associated with a formula A. When A
is a single fluent, the expression corresponds to u(A) = u in the terminology used here. Most of the
competition benchmarks contain only preferences of this form. The more general case that arises when
A is a compound formula over fluents is considered in Section 4.

548

fiSoft Goals Can Be Compiled Away

action costs, the basic functionality required in the satisficing track of the most recent IPC
(Helmert et al., 2008).2
We write actions as tuples of the form o = hPre(o), Eff(o)i, where the effects can be
positive (Adds) or negative (Deletes). We assume that for each soft goal fluent p, P also
contains a fluent p representing its negation. These can be introduced in the standard way,
adding p to the initial state if p is not initially true, and including p in the Add and Delete
lists of all actions deleting or adding p respectively (Gazen & Knoblock, 1997; Nebel, 2000).
The problem P 0 with action costs and no soft goals that is equivalent to the problem P
with soft goals can then be obtained by the following transformation:
Definition 3 For a STRIPS problem with action costs and soft goals P = hF, I, O, G, c, ui,
the compiled STRIPS problem with action costs is P 0 = hF 0 , I 0 , O0 , G0 , c0 i with
 F 0 = F  S 0 (P )  S(P )  {normal-mode, end-mode}
 I 0 = I  S(P )  {normal-mode}
 G0 = G  S 0 (P )
 O0 = O00  {collect(p), f orgo(p) | p  SG(P )}  {end}

 c(o) if o  O00
0
u(p) if o = forgo(p)
 c (o) =

0
if o = collect(p) or o = end
where
 SG(P ) = {p | (p  F )  (u(p) > 0)}
 S 0 (P ) = {p0 | p  SG(P )}
 S(P ) = {p0 | p0  S 0 (P )}
 end = h{normal-mode}, {end-mode, normal-mode}i
 collect(p) = h{end-mode, p, p0 }, {p0 , p0 }i
 forgo(p) = h{end-mode, p, p0 }, {p0 , p0 }i
 O00 = {hPre(o)  {normal-mode}, Eff(o)i | o  O}
2. Edelkamps transformation associates with the soft goals p1 , . . . , pm numeric
variables n1 , . . . , nm , each
P
with domain {0, 1}. The utility for a plan is then expressed as U () = n
i=1 ni  u(pi )  cost(), where
u(pi ) represents the utility associated with soft goal pi and ni represents the value of the numeric variable
in the final state achieved by the plan. This transformation also eliminates soft goals, but requires in its
place a plan metric whose terms (namely, whether the variables u(pi ) are 1 or 0) are state-dependent.
Current heuristics can not deal with such metrics (See Sections 3 and 4).

549

fiKeyder & Geffner

For each soft goal p in P , the transformation adds a dummy hard goal p0 in P 0 that
can be achieved in two ways: with the action collect(p) that has cost 0 but requires p to be
true, or with the action forgo(p) that has cost equal to the utility of p yet can be performed
when p is false, or equivalently when p is true. These two actions can be used only after
the end action that makes the fluent end-mode true, while the actions from the original
problem P can be used only when the fluent normal-mode is true prior to the execution of
the end action. Moreover, exactly one of {collect(p), forgo(p)} can appear for each soft goal
p in the plan, as both delete their shared precondition p0 , which no action makes true. As
there is no way to make normal-mode true again after it is deleted by the end action, all
plans  0 for P 0 have the form  0 = h, end,  00 i, where  is a plan for P and  00 is a sequence
of |S 0 (P )| collect(p) and forgo(p) actions in any order, the former appearing when  |= p,
and the latter otherwise.
The two problems P and P 0 are equivalent in the sense that there is a correspondence
between the plans for P and P 0 , and corresponding plans are ranked in the same way. More
specifically, for any plan  for P , there is a plan  0 for P 0 that extends  with the end action
and a set of collect and forgo actions, and this plan has cost c( 0 ) = u() + , where 
is a constant that is independent of both  and  0 . Finding an optimal (maximum utility)
plan  for P is therefore equivalent to finding an optimal (minimum cost) plan  0 for P 0 .
Proposition 1 (Correspondence between plans) For an applicable action sequence 
in P , let an extension  0 of  denote any sequence obtained by appending to  the end action
followed by some permutation of the actions collect(p) and forgo(p) for all p  SG(P ), when
 |= p and  6|= p respectively. Then
 is a plan for P   0 is a plan for P 0
Proof: () The new actions in P 0 do not delete any p  F , so any hard goal achieved by
 will remain true in the final state reached by  0 , and we have that  0 |= G. For all p  F
such that u(p) > 0, either  |= p or  6|= p. In the first case, p0 is achieved by collect(p), in
the second, by forgo(p), therefore  0 |= S 0 (P ). Since G0 = G  S 0 (P ), we have that  0 |= G0 .
() If  0 is a plan for P 0 , then all hard goals G in P must be made true by  0 before the
end action, as after this action only collect and forgo actions can be applied and these can
not make any p  F true. The plan obtained by removing the end action and all collect
and forgo actions must therefore achieve G and thus is a valid plan for P .
2
Proposition 2 (Correspondence between utilities and costs) Let 1 and 2 be two
plans for P , and let 10 and 20 be extensions of 1 and 2 respectively. Then,
u(1 ) > u(2 )  c(10 ) < c(20 )

0
0
Proof: Let
P  be a plan for P and  an extension of . We demonstrate that c( ) =
u() + pSG(P ) u(p). Since the summation in this expression is a constant for a given
problem P , the assertion follows directly:

550

fiSoft Goals Can Be Compiled Away

X

c( 0 ) = c() + c0 (end) +

c0 (forgo(p)) +

forgo(p) 0

X

= c() +

X

c0 (collect(p))

collect(p) 0

c0 (forgo(p))

forgo(p) 0

= c() +

X

u(p)

p:6|=p

X

= c() +

u(p) 

X

u(p)

p:|=p

pSG(P )

= u() +

X

u(p)

pSG(P )
2

Proposition 3 (Equivalence) Let  be a plan for P , and  0 be a plan for P 0 that extends
. Then,
 is an optimal plan for P   0 is an optimal plan for P 0

Proof: Direct from the two propositions above.

2

In the following section, we empirically compare the performance of net-benefit planners
on problems P with explicit soft goals to that of sequential planners on problems P 0 in
which soft goals have been compiled away. In order to improve the latter, we make the
transformation of Definition 3 more effective with a simple trick. Recall that for a single
plan  for P , there are many extensions  0 in P 0 , all containing the same actions and
having the same cost, but differing in the way the collect and forgo actions are ordered. For
efficiency purposes, the implementation enforces a fixed but arbitrary ordering p1 , . . . , pm
on the soft goals in P by adding the dummy hard goal p0i as a precondition of the actions
collect(pi+1 ) and forgo(pi+1 ) for i = 1, . . . , m  1. The result is that there is a single possible
extension  0 of every plan  in P , and the space of plans to search is therefore reduced.
This optimization is used in the experiments reported below.

3. Experimental Results
The formal results above imply that the best plans for a problem P with action costs and
soft goals can be computed by looking for the best plans for the compiled problem P 0
with action costs and no soft goals, to which standard classical planning techniques can be
applied. To test the practical value of the transformation, we evaluate the performance of
both optimal and satisficing planning techniques for soft goals. Some problems in the test
suite contain preferences over conjunctions rather than single fluents. Such preferences are
handled with a variant of the approach described above, detailed in Section 4.
The results shown in the three columns in Table 1 labelled Net-benefit optimal planners
are the results as reported by the organizers of the 2008 International Planning Competition
(IPC6) (Helmert et al., 2008). All other results were obtained using the same machines and
551

fiKeyder & Geffner

Domain
crewplanning(30)
elevators (30)
openstacks (30)
pegsol (30)
transport (30)
woodworking (30)
total

Net-benefit optimal planners
Gamer HSP*P Mips-XXL
4
16
8
11
5
4
7
5
2
24
0
23
12
12
9
13
11
9
71
49
55

Sequential optimal planners
Gamer HSP*F HSP*0 Mips-XXL
8
21
8
19
8
8
3
6
4
6
1
22
26
14
22
15
15
9
10
14
7
71
78
50

Table 1: Coverage for optimal planners: The leftmost three columns give the number of problems
solved by each of the planners in the Net Benefit Optimal track of IPC6, as reported
by the competition organizers. The rightmost four columns give the number of compiled
problems solved by the Sequential Optimal versions of these planners. Dashes indicate
that the version of the planner could not be run on that domain.

settings as used in the competition: Xeon Woodcrest computers with clock speeds of 2.33
GHz, with a time limit of 30 minutes and a memory limit of 2GB.
In the first set of experiments, we consider the problems used in the Net Benefit Optimal
(NBO) track of IPC6, in which soft goals are defined in terms of goal-state preferences
(Gerevini & Long, 2006), and compare the results obtained by the three optimal netbenefit planners with the results obtained by their Sequential Optimal (SO) variants on
their compilations.3 The three planners entered in the NBO track of IPC6 were Gamer,
Mips-XXL, and HSP*P . The SO planners we test on the compiled versions of the NBO
problems are the SO versions of Gamer (Edelkamp & Kissmann, 2008) and Mips-XXL
(Edelkamp & Jabbar, 2008) and the two SO planners HSP*F and HSP*0 (Haslum, 2008).4
These were ranked first, fifth, second, and third, respectively, in the SO track (Helmert
et al., 2008). Three out of the six domains from the NBO track of IPC6 involve numeric
variables that appear in the preconditions of actions. The SO version of Gamer does not
handle numeric variables, and we are therefore unable to run Gamer on such problems.
Numeric variables never appear as soft goals and are left untouched by our compilation.
The data in Table 1 show that the two HSP* planners from the SO track run on the
compiled problems do as well as, or better than, the best planner from the NBO track run
on the original problems with soft goals. The maximum number of solved problems for a
domain is higher for the NBO track planners in only a single domain, openstacks (7 vs. 6).
In all other domains, SO planners are able to solve a larger number of problems than the
3. The compiled problems are currently available at http://ipc.informatik.uni-freiburg.de/Domains.
4. All versions of HSP* have a bug which may cause suboptimal or invalid solutions to be computed in
domains with non-monotonic numeric variables (numeric variables whose values may both increase and
decrease) that occur in preconditions of actions or goals (See http://ipc.informatik.uni-freiburg.
de/Planners). Such variables are present only in the transport domain out of all those tested, yet plans
computed by HSP* for both versions of the domain turn out to be valid (as verified by the VAL plan
validator, Howey & Long, 2003) and optimal in the instances in which they can be checked against the
costs of plans computed by other planners.

552

fiSoft Goals Can Be Compiled Away

Domain
elevators (30)
openstacks (30)
pegsol (30)
rovers (20)
total

Net-benefit satisficing planners
SGPlan YochanPS Mips-XXL
0
0
8
2
0
2
0
5
23
8
2
1
10
7
34

Cost satisficing planners
Lama
23
28
29
17
97

Table 2: Coverage and quality for satisficing planners: The entries indicate the number of problems
for which the planner generated the best quality plan.

maximum number solved by any NBO planner. Considering the performance of the NBO
and SO variants of each planner, the compilation benefits most the two versions of the
heuristic search planner HSP* , leaving the BDD planners Gamer and Mips-XXL relatively
unaffected. Interestingly, HSP*0 using the compilation ends up solving more problems than
Gamer, the winner of the NBO track (78 vs. 71). The drastically better performance of the
SO versions of HSP* compared to the net-benefit version is the result of the simple scheme
for handling soft goals in the latter, in which optimal plans are computed for each possible
subset of soft goals in the problem (roughly), and a change in the search algorithm from
IDA* to A*.
In the second set of experiments, we consider the three domains from the NBO track
of IPC6 which do not contain numeric variables in the preconditions of actions, and the
domain rovers from the net-benefit track of IPC5. Domains containing numeric variables in
the preconditions of actions are not considered due to the lack of state-of-the-art cost-based
planners able to handle them. Domains other than rovers from the NB track of IPC5 are
not considered as they contain disjunctive, existentially qualified, or universally qualified
soft goals which our current implementation does not support. The satisficing net-benefit
planners we test on these problems are SGPlan (Hsu & Wah, 2008), the winner of the net
benefit track from IPC5, YochanPS (Benton, Do, & Kambhampati, 2009), which received
a distinguished performance award in the same competition, and a satisficing variant of
MIPS-XXL, which also received a distinguished performance award in that competition and
competed in the optimal track of IPC6. We solve the compiled versions of the problems with
LAMA, the winner of the sequential satisficing track from IPC6. YochanPS, MIPS-XXL,
and LAMA are anytime planners, and the results discussed below refer to the cost of the
best plan found by each at the end of the evaluation period of 30 minutes.
Entries in Table 2 show the number of problems in each domain for which the plan
generated by a planner is the best or only plan produced. We report this data rather than
showing graphs of plan utilities as the absolute difference between the quality of plans is not
meaningful in itself except when the shortest plans (that ignore costs and/or soft goals) for
the problem are significantly more costly. The results show that running a state-of-the-art
cost-based planner on the compiled problems yields the best plan in 98 out of the total
110 instances, almost three times the number of instances in which the best-performing
native soft goals planner, MIPS-XXL, gives the best plan. Furthermore, in 22 out of the 23
553

fiKeyder & Geffner

problems for which MIPS-XXL finds the best plan in the pegsol domain, LAMA finds a plan
with the same quality. The problems in which satisficing net-benefit planners outperform
LAMA run on the compiled problems are therefore very few.
These results appear to contradict the results reported by Benton et al. (2009), where
the native net-benefit planner, YochanPS , yields better results than a cost-based planner,
YochanCOST , run on problems compiled according to an earlier version of our transformation
(Keyder & Geffner, 2007). The discrepancy appears to be the result of the non-informative
cost-based heuristic used in YochanCOST , which leads to plans that forgo all soft goals, and
the fact that they do not make use of the optimization discussed at the end of Section 2,
which results in an unnecessary blowup of the state space. For an analysis of the differences
between some recent cost-based planners, see the paper by Keyder and Geffner (2008).

4. Extensions
We have shown that it is possible to compile away positive utilities u(p) associated with
single fluents p. We show now that this compilation can be extended to deal with positive
utilities defined on formulas over fluents and to negative utilities defined on both single
fluents and formulas. Negative utilities stand for conditions to be avoided rather than
sought; for example, a utility u(p  q) = 10 penalizes a plan that results in a state where
both p and q are true with an extra cost of 10. The compilation of soft goals defined on
formulas is based on the standard compilation of goal and precondition formulas in classical
planning (Gazen & Knoblock, 1997; Nebel, 1999).
A positive utility on a logical formula A can be compiled away by introducing a new
fluent pA that can be achieved at zero cost from any end state where A holds, and by
assigning the utility associated with A to pA . If A is a DNF formula D1  . . .  Dn , it
suffices to add n new actions a1 , . . . , an with ai = hDi , pA i for i = 1, . . . , n. If A is a
CNF formula C1  . . .  Cn , a fluent pi is introduced for each i = 1, . . . , n, along with
actions aij = hCij , pi i for j = 1, . . . , |Ci |, where Cij stands for the jth fluent of Ci . We also
introduce an action a = h{p1 , . . . , pn }, pA i that allows the addition of fluent pA in states
where A holds. All the newly introduced actions have zero cost, and must be applicable in
P 0 after the actions of the original problem P and before the collect and forgo actions. The
best extensions of any plan  that achieves A in P will then achieve pA and use the collect
action to achieve the hard goal fluent p0A associated with pA at zero cost.
A negative utility u(A) < 0 on a formula A in DNF or CNF can be compiled away in
two steps, by first substituting a positive utility u(A) on the negation A of A and then
compiling this positive utility on a formula into a utility on a single fluent as described
above. This makes use of the fact that the negation of a formula in CNF is a formula in
DNF and vice versa.

5. Summary
We have shown that soft goals do not add expressive power and can be easily compiled
away. This implies that no new search algorithms or heuristics are strictly required for
handling them. From a practical standpoint, experiments indicate that state-of-the-art
sequential planners outperform state-of-the-art net-benefit planners on compiled versions of
554

fiSoft Goals Can Be Compiled Away

the benchmarks used in recent planning competitions. Furthermore, similar transformations
can be used to compile away positive and negative utilities on logical formulas in DNF or
CNF.

Acknowledgments
We thank Malte Helmert for his help with compiling and running many of the IPC6 planners,
Patrik Haslum for his help with all aspects of various versions of HSP, and J. Benton for
his help with compiling and running YochanPS . H. Geffner is partially supported by Grant
TIN2006-15387-C03-03 from MEC, Spain.

References
Baier, J. A., Bacchus, F., & McIlraith, S. A. (2007). A heuristic search approach to planning
with temporally extended preferences. In Proc. IJCAI-07, pp. 18081815.
Benton, J., Do, M., & Kambhampati, S. (2009). Anytime heuristic search for partial satisfaction planning. Artificial Intelligence, 173 (5-6), 562592.
Bonet, B., & Geffner, H. (2008). Heuristics for planning with penalties and rewards formulated in logic and computed through circuits. Artificial Intelligence, 172 (12-13),
15791604.
Edelkamp, S. (2006). On the compilation of plan constraints and preferences. In Proc.
ICAPS-06, pp. 374377.
Edelkamp, S., & Jabbar, S. (2008). MIPS-XXL: Featuring external shortest path search for
sequential optimal plans and external branch-and-bound for optimal net benefit. In
6th. Int. Planning Competition Booklet (ICAPS-08).
Edelkamp, S., & Kissmann, P. (2008). Gamer: Bridging planning and general game playing
with symbolic search. In 6th. Int. Planning Competition Booklet (ICAPS-08).
Gazen, B., & Knoblock, C. (1997). Combining the expressiveness of UCPOP with the
efficiency of Graphplan. In Steel, S., & Alami, R. (Eds.), Proc. 4th European Conf.
on Planning, pp. 221233. Springer.
Gerevini, A., & Long, D. (2006). Preferences and soft constraints in PDDL3. In Proc.
ICAPS-06 Workshop on Preferences and Soft Constraints in Planning, pp. 4653.
Haslum, P. (2008). Additive and reversed relaxed reachability heuristics revisited. In 6th.
Int. Planning Competition Booklet (ICAPS-08).
Helmert, M., Do, M., & Refanidis, I. (2008). IPC 2008 deterministic competition. In 6th.
Int. Planning Competition Booklet (ICAPS-08).
Howey, R., & Long, D. (2003). VALs progress: The automatic validation tool for PDDL2.1
used in the international planning competition. In Proc. 2003 ICAPS Workshop on
The Competition: Impact, Organization, Evaluation, Benchmarks.
Hsu, C.-W., & Wah, B. W. (2008). The SGPlan planning system in IPC6. In 6th. Int.
Planning Competition Booklet (ICAPS-08).
555

fiKeyder & Geffner

Keyder, E., & Geffner, H. (2007). Set-additive and TSP heuristics for planning with action costs and soft goals. In Proc. ICAPS-06 Workshop on Heuristics for DomainIndependent Planning.
Keyder, E., & Geffner, H. (2008). Heuristics for planning with action costs revisited. In
Proc. 18th European Conference on Artificial Intelligence, pp. 588592.
Nebel, B. (1999). Compilation schemes: A theoretical tool for assessing the expressive
power of planning formalisms. In Proc. KI-99: Advances in Artificial Intelligence, pp.
183194. Springer-Verlag.
Nebel, B. (2000). On the compilability and expressive power of propositional planning.
Journal of Artificial Intelligence Research, 12, 271315.
Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics for selecting objectives
in over-subscription planning problems. In Proc. ICAPS-05, pp. 192201.
Smith, D. E. (2004). Choosing objectives in over-subscription planning. In Proc. ICAPS-04,
pp. 393401.

556

fiJournal of Artificial Intelligence Research 36 (2009) 169

Submitted 04/09; published 10/09

The DL-Lite Family and Relations
Alessandro Artale
Diego Calvanese

artale@inf.unibz.it
calvanese@inf.unibz.it

KRDB Research Centre
Free University of Bozen-Bolzano
Piazza Domenicani, 3 I-39100 Bolzano, Italy

Roman Kontchakov
Michael Zakharyaschev

roman@dcs.bbk.ac.uk
michael@dcs.bbk.ac.uk

Department of Computer Science and Information Systems
Birkbeck College
Malet Street, London WC1E 7HX, U.K.

Abstract
The recently introduced series of description logics under the common moniker DLLite has attracted attention of the description logic and semantic web communities due to
the low computational complexity of inference, on the one hand, and the ability to represent
conceptual modeling formalisms, on the other. The main aim of this article is to carry out
a thorough and systematic investigation of inference in extensions of the original DL-Lite
logics along five axes: by (i) adding the Boolean connectives and (ii) number restrictions to
concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry,
asymmetry, reflexivity, irreflexivity and transitivity constraints, and (v) adopting or dropping the unique name assumption. We analyze the combined complexity of satisfiability
for the resulting logics, as well as the data complexity of instance checking and answering
positive existential queries. Our approach is based on embedding DL-Lite logics in suitable fragments of the one-variable first-order logic, which provides useful insights into their
properties and, in particular, computational behavior.

1. Introduction
Description Logic (cf. Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003 and
references therein) is a family of knowledge representation formalisms developed over the
past three decades and, in recent years, widely used in various application areas such as:
 conceptual modeling (Bergamaschi & Sartori, 1992; Calvanese et al., 1998b, 1999;
McGuinness & Wright, 1998; Franconi & Ng, 2000; Borgida & Brachman, 2003; Berardi, Calvanese, & De Giacomo, 2005; Artale et al., 1996, 2007, 2007b),
 information and data integration (Beeri, Levy, & Rousset, 1997; Levy & Rousset,
1998; Goasdoue, Lattes, & Rousset, 2000; Calvanese et al., 1998a, 2002a, 2002b,
2008; Noy, 2004; Meyer, Lee, & Booth, 2005),
 ontology-based data access (Dolby et al., 2008; Poggi et al., 2008a; Heymans et al.,
2008),
 the Semantic Web (Heflin & Hendler, 2001; Horrocks, Patel-Schneider, & van Harmelen, 2003).
c
2009
AI Access Foundation. All rights reserved.

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Description logics (DLs, for short) underlie the standard Web Ontology Language OWL,1
which is now in the process of being standardized by the W3C in its second edition, OWL 2.
The widespread use of DLs as flexible modeling languages stems from the fact that,
similarly to more traditional modeling formalisms, they structure the domain of interest into
classes (or concepts, in the DL parlance) of objects with common properties. Properties
are associated with objects by means of binary relationships (or roles) to other objects.
Constraints available in standard DLs also resemble those used in conceptual modeling
formalisms for structuring information: is-a hierarchies (i.e., inclusions) and disjointness
for concepts and roles, domain and range constraints for roles, mandatory participation in
roles, functionality and more general numeric restrictions for roles, covering within concept
hierarchies, etc. In a DL knowledge base (KB), these constraints are combined to form a
TBox asserting intensional knowledge, while an ABox collects extensional knowledge about
individual objects, such as whether an object is an instance of a concept, or two objects are
connected by a role. The standard reasoning services over a DL KB include checking its
consistency (or satisfiability), instance checking (whether a certain individual is an instance
of a concept), and logic entailment (whether a certain constraint is logically implied by
the KB). More sophisticated services are emerging that can support modular development
of ontologies by checking, for example, whether one ontology is a conservative extension
of another with respect to a certain vocabulary (see, e.g., Ghilardi, Lutz, & Wolter, 2006;
Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008; Kontchakov, Wolter, & Zakharyaschev,
2008; Kontchakov, Pulina, Sattler, Schneider, Selmer, Wolter, & Zakharyaschev, 2009).
Description logics have recently been used to provide access to large amounts of data
through a high-level conceptual interface, which is of relevance to both data integration and
ontology-based data access. In this setting, the TBox constitutes the conceptual, high-level
view of the information managed by the system, and the ABox is physically stored in a
relational database and accessed using the standard relational database technology (Poggi
et al., 2008a; Calvanese et al., 2008). The fundamental inference service in this case is answering queries to the ABox with the constraints in the TBox taken into account. The kind
of queries that have most often been considered are first-order conjunctive queries, which
correspond to the commonly used Select-Project-Join SQL queries. The key properties
for such an approach to be viable in practice are (i) efficiency of query evaluation, with the
ideal target being traditional database query processing, and (ii) that query evaluation can
be done by leveraging the relational technology already used for storing the data.
With these objectives in mind, a series of description logicsthe DL-Lite familyhas
recently been proposed and investigated by Calvanese, De Giacomo, Lembo, Lenzerini,
and Rosati (2005, 2006, 2008a), and later extended by Artale, Calvanese, Kontchakov,
and Zakharyaschev (2007a), Poggi, Lembo, Calvanese, De Giacomo, Lenzerini, and Rosati
(2008a). Most logics of the family meet the requirements above and, at the same time, are
capable of representing many important types of constraints used in conceptual modeling.
In particular, inference in various DL-Lite logics can be done efficiently both in the size
of the data (data complexity) and in the overall size of the KB (combined complexity):
it was shown that KB satisfiability in these logics is polynomial for combined complexity,
while answering queries is in AC0 for data complexitywhich, roughly, means that, given a
1. http://www.w3.org/2007/OWL/

2

fiThe DL-Lite Family and Relations

conjunctive query over a KB, the query and the TBox can be rewritten (independently of the
ABox) into a union of conjunctive queries over the ABox alone. (It is to be emphasized that
the data complexity measure is very important in the application context of the DL-Lite
logics, since one can reasonably assume that the size of the data largely dominates the size
of the TBox.) Query rewriting techniques have been implemented in various systems such as
QuOnto2 (Acciarri, Calvanese, De Giacomo, Lembo, Lenzerini, Palmieri, & Rosati, 2005;
Poggi, Rodriguez, & Ruzzi, 2008b), ROWLKit (Corona, Ruzzi, & Savo, 2009), Owlgres
(Stocker & Smith, 2008) and REQUIEM (Perez-Urbina, Motik, & Horrocks, 2009). It has
also been demonstrated (Kontchakov et al., 2008) that developing, analyzing and re-using
DL-Lite ontologies (TBoxes) can be supported by efficient tools capable of checking various
types of entailment between such ontologies with respect to given vocabularies, in particular,
by minimal module extraction tools (Kontchakov et al., 2009)which do not yet exist for
richer languages.
The significance of the DL-Lite family is testified by the fact that it forms the basis
of OWL 2 QL, one of the three profiles of OWL 2.3 The OWL 2 profiles are fragments of
the full OWL 2 language that have been designed and standardized for specific application
requirements. According to (the current version of) the official W3C profiles document, the
purpose of OWL 2 QL is to be the language of choice for applications that use very large
amounts of data and where query answering is the most important reasoning task.
The common denominator of the DL-Lite logics constructed so far is as follows: (i) quantification over roles and their inverses is not qualified (in other words, in concepts of the
form R.C we must have C = >) and (ii) the TBox axioms are concept inclusions that cannot represent any kind of disjunctive information (say, that two concepts cover the whole
domain). The other DL-Lite-related dialects were designedwith the aim of capturing
more conceptual modeling constraints, but in a somewhat ad hoc mannerby extending
this core language with a number of constructs such as global functionality constraints,
role inclusions and restricted Boolean operators on concepts (see Section 4 for details).
Although some attempts have been made (Calvanese et al., 2006; Artale et al., 2007a;
Kontchakov & Zakharyaschev, 2008) to put the original DL-Lite logics into a more general
perspective and investigate their extensions with a variety of DL constructs required for
conceptual modeling, the resulting picture still remains rather fragmentary and far from
comprehensive. A systematic investigation of the DL-Lite family and relatives has become
even more urgent and challenging in view of the choice of the constructs to be included in
the specification of the OWL 2 QL profile4 (in particular, because OWL does not make the
unique name assumption, UNA, which was usually adopted in DL-Lite, and uses equalities
and inequalities between object names instead).
The main aim of this article is to fill in this gap and provide a thorough and comprehensive understanding of the interaction between various DL-Lite constructs and their impact
on the computational complexity of reasoning. To achieve this goal, we consider a spectrum
of logics, classified according to five mutually orthogonal features:
(1) the presence or absence of role inclusions;
2. http://www.dis.uniroma1.it/quonto/
3. http://www.w3.org/TR/owl2-profiles/
4. http://www.w3.org/TR/owl2-profiles/#OWL_2_QL

3

fiArtale, Calvanese, Kontchakov & Zakharyaschev

(2) the form of the allowed concept inclusions, where we consider four classes, called core,
Krom, Horn, and Bool, that exhibit different computational properties;
(3) the form of the allowed numeric constraints, ranging from none, to global functionality
constraints only, and to arbitrary number restrictions;
(4) the presence or absence of the unique name assumption (and the equalities and inequalities between object names, if this assumption is dropped); and
(5) the presence or absence of standard role constraints such as disjointness, symmetry,
asymmetry, reflexivity, irreflexivity, and transitivity.
For all the resulting cases, we investigate the combined and data complexity of KB satisfiability and instance checking, as well as the data complexity of query answering. The
obtained tight complexity results are summarized in Section 3.4 (Table 2 and Remark 3.1).
As already mentioned, the original motivation and distinguishing feature for the logics in
the DL-Lite family was their lite-ness in the sense of low computational complexity of the
reasoning tasks (query answering in AC0 for data complexity and tractable KB satisfiability
for combined complexity). In the broader perspective we take here, not all of our logics
meet this requirement, in particular, those with Krom or Bool concept inclusions.5 However,
we identify another distinguishing feature that can be regarded as the natural logic-based
characterization of the DL-Lite family: embeddability into the one-variable fragment of firstorder logic without equality and function symbols. This allows us to relate the complexity of
DL-Lite logics to the complexity of the corresponding fragments of first-order logic, and thus
to obtain a deep insight into the underlying logical properties of each DL-Lite variant. For
example, most upper complexity bounds established below follow from this embedding and
well-known results on the classical decision problem (see, e.g., Borger, Gradel, & Gurevich,
1997) and descriptive complexity (see, e.g., Immerman, 1999).
One of the most interesting findings in this article is that number restrictions, even
expressed locally, instead of global role functionality, can be added to the original DL-Lite
logics (under the UNA and without role inclusions) for free, that is, without changing
their computational complexity. The first-order approach shows that in most cases we can
also extend the DL-Lite logics with the role constraints mentioned above, again keeping the
same complexity. It also gives a framework to analyze the effect of adopting or dropping
the UNA and using (in)equalities between object names. For example, we observe that if
equality is allowed in the language of DL-Lite (which only makes sense without the UNA)
then query answering becomes LogSpace-complete for data complexity, and therefore not
first-order rewritable. It also turns out that dropping the UNA results in P-hardness of
reasoning (for both combined and data complexity) in the presence of functionality constraints (NLogSpace-hardness was shown by Calvanese et al., 2008), and in NP-hardness
if arbitrary number restrictions are allowed.
Another interesting finding is the dramatic impact of role inclusions, when combined
with number restrictions (or even functionality constraints), on the computational complexity of reasoning. As was already observed by Calvanese et al. (2006), such a combination increases data complexity of instance checking from membership in LogSpace to
5. Note, by the way, that logics with Bool concept inclusions turn out to be quite useful in conceptual
modeling and reasonably manageable computationally (Kontchakov et al., 2008).

4

fiThe DL-Lite Family and Relations

NLogSpace-hardness. We show here that the situation is actually even worse: for data
complexity, instance checking turns out to be P-complete in the case of core and Horn
logics and coNP-complete in the case of Krom and Bool logics; moreover, KB satisfiability,
which is NLogSpace-complete for combined complexity in the simplest core casei.e., efficiently tractable, when role inclusions or number restrictions are used separatelybecomes
ExpTime-completei.e., provably intractable, when they are used together.
To retain both role inclusions and functionality constraints in the language and keep
complexity within the required limits, Poggi et al. (2008a) introduced another DL-Lite
dialect, called DL-LiteA , which restricts the interaction between role inclusions and functionality constraints. Here we extend this result by showing that the DL-Lite logics with
such a limited interaction between role inclusions and number restrictions can still be embedded into the one-variable fragment of first-order logic, and so exhibit the same behavior
as their fragments with only role inclusions or only number restrictions.
The article is structured in the following way. In Section 2, we introduce the logics of the
extended DL-Lite family and illustrate their features as conceptual modeling formalisms.
In Section 3, we discuss the reasoning services and the complexity measures to be analyzed
in what follows, and give an overview of the obtained complexity results. In Section 4,
we place the introduced DL-Lite logics in the context of the original DL-Lite family, and
discuss its relationship with OWL 2. In Section 5, we study the combined complexity of
KB satisfiability and instance checking, while in Section 6, we consider the data complexity
of these problems. In Section 7, we study the data complexity of query answering. In
Section 8, we analyze the impact of dropping the UNA and adding (in)equalities between
object names on the complexity of reasoning. Section 9 concludes the article.

2. The Extended DL-Lite Family of Description Logics
Description Logic (Baader et al., 2003) is a family of logics that have been studied and
used in knowledge representation and reasoning since the 1980s. In DLs, the elements of
the domain of interest are structured into concepts (unary predicates), and their properties
are specified by means of roles (binary predicates). Complex concept and role expressions
(or simply concepts and roles) are constructed, starting from a set of concept and role
names, by applying suitable constructs, where the set of available constructs depends on
the specific description logic. Concepts and roles can then be used in a knowledge base to
assert knowledge, both at the intensional level, in a so-called TBox (T for terminological),
and at the extensional level, in a so-called ABox (A for assertional). A TBox typically
consists of a set of axioms stating the inclusion between concepts and roles. In an ABox, one
can assert membership of objects (i.e., constants) in concepts, or that a pair of objects is
connected by a role. DLs are supported by reasoning services, such as satisfiability checking
and query answering, that rely on their logic-based semantics.
2.1 Syntax and Semantics of the Logics in the DL-Lite Family
We introduce now the (extended) DL-Lite family of description logics, which was initially
proposed with the aim of capturing typical conceptual modeling formalisms, such as UML
class diagrams and ER models (see Section 2.2 for details), while maintaining good computational properties of standard DL reasoning tasks (Calvanese et al., 2005). We begin
5

fiArtale, Calvanese, Kontchakov & Zakharyaschev

by defining the logic DL-LiteHN
bool , which can be regarded as the supremum of the original
DL-Lite family (Calvanese et al., 2005, 2006, 2007b) in the lattice of description logics.
HN
DL-LiteHN
bool . The language of DL-Litebool contains object names a0 , a1 , . . . , concept names
A0 , A1 , . . . , and role names P0 , P1 , . . . . Complex roles R and concepts C of this language
are defined as follows:

Pk ,

R

::=

Pk

|

B

::=



|

Ak

|

 q R,

C

::=

B

|

C

|

C1 u C2 ,

where q is a positive integer. The concepts of the form B will be called basic.
A DL-LiteHN
bool TBox, T , is a finite set of concept and role inclusion axioms (or simply
concept and role inclusions) of the form:
C1 v C2

and

R1 v R2 ,

and an ABox, A, is a finite set of assertions of the form:
Ak (ai ),

Ak (ai ),

Pk (ai , aj )

and

Pk (ai , aj ).

Taken together, T and A constitute the DL-LiteHN
bool knowledge base K = (T , A). In the
following, we denote by role(K) the set of role names occurring in T and A, by role (K)
the set {Pk , Pk | Pk  role(K)}, and by ob(A) the set of object names in A. For a role R,
we set:
(
Pk , if R = Pk ,
inv(R) =
Pk , if R = Pk .
As usual in description logic, an interpretation, I = (I , I ), consists of a nonempty
domain I and an interpretation function I that assigns to each object name ai an element
aIi  I , to each concept name Ak a subset AIk  I of the domain, and to each role name
Pk a binary relation PkI  I  I over the domain. Unless otherwise stated, we adopt
here the unique name assumption (UNA):
aIi 6= aIj

for all

i 6= j.

(UNA)

However, we shall always indicate which of our results depend on the UNA and which do
not, and when they do depend on this assumption, we discuss also the consequences of
dropping it (see also Sections 4 and 8).
The role and concept constructs are interpreted in I in the standard way:
(Pk )I = {(y, x)  I  I | (x, y)  PkI },
I



I

( q R)

= ,

	
= x  I | ]{y  I | (x, y)  RI }  q ,

(C)I = I \ C I ,
I

(C1 u C2 )

=

C1I



(inverse role)
(the empty set)
(at least q R-successors)
(not in C)

C2I ,

(both in C1 and in C2 )

6

fiThe DL-Lite Family and Relations

where ]X denotes the cardinality of X. We will use standard abbreviations such as
C1 t C2 = (C1 u C2 ),

> = ,

R = ( 1 R),

 q R = ( q + 1 R).

Concepts of the form  q R and  q R are called number restrictions, and those of the form
R are called existential concepts.
The satisfaction relation |= is also standard:
I |= C1 v C2

iff

C1I  C2I ,

I |= R1 v R2

iff

R1I  R2I ,

I |= Ak (ai )

iff aIi  AIk ,

I |= Pk (ai , aj )

iff

(aIi , aIj )  PkI ,

I |= Ak (ai )

iff aIi 
/ AIk ,

I |= Pk (ai , aj )

iff

(aIi , aIj ) 
/ PkI .

A knowledge base K = (T , A) is said to be satisfiable (or consistent) if there is an interpretation, I, satisfying all the members of T and A. In this case we write I |= K (as well as
I |= T and I |= A) and say that I is a model of K (and of T and A).
The languages of the DL-Lite family we investigate in this article are obtained by restricting the language of DL-LiteHN
bool along three axes: (i) the Boolean operators (bool ) on
concepts, (ii) the number restrictions (N ) and (iii) the role inclusions, or hierarchies (H).
Similarly to classical logic, we adopt the following definitions. A DL-LiteHN
bool TBox T
will be called a Krom TBox 6 if its concept inclusions are restricted to:
B1 v B2 ,

B1 v B2

or

B1 v B2

(Krom)

(here and below all the Bi and B are basic concepts). T will be called a Horn TBox if its
concept inclusions are restricted to:
l
Bk v B
(Horn)
k

(by definition, the empty conjunction is >). Finally, we will call T a core TBox if its
concept inclusions are restricted to:
B1 v B2

or

B1 v B2 .

(core)

As B1 v B2 is equivalent to B1 u B2 v , core TBoxes can be regarded as sitting in the
intersection of Krom and Horn TBoxes.
Remark 2.1 We will sometimes use conjunctions
on the right-hand side of concept includ
sions in these restricted languages: C v k Bk . Clearly, this syntactic sugar does not add
any extra expressive power.
HN
HN
HN
DL-LiteHN
krom , DL-Litehorn and DL-Litecore . The fragments of DL-Litebool with Krom,
HN
HN
Horn, and core TBoxes will be denoted by DL-Litekrom , DL-Litehorn and DL-LiteHN
core , respectively. Other fragments are obtained by limiting the use of number restrictions and role
inclusions.

6. The Krom fragment of first-order logic consists of all formulas in prenex normal form whose quantifier-free
part is a conjunction of binary clauses.

7

fiArtale, Calvanese, Kontchakov & Zakharyaschev

HN
DL-LiteH
 . The fragment of DL-Lite ,   {core, krom, horn, bool}, without number
restrictions  q R, for q  2, (but with role inclusions) will be denoted by DL-LiteH
 . Note
H
that, in DL-Lite , we can still use existential concepts R (that is,  1 R).
HF
DL-LiteHF
the fragment of DL-LiteHN
in which of all number
 . Denote by DL-Lite

restrictions  q R, we have existential concepts (with q = 1) and only those with q = 2
that occur in concept inclusions of the form  2 R v . Such an inclusion is called a
global functionality constraint because it states that role R is functional (more precisely, if
I |= ( 2 R v ) and both (x, y)  RI and (x, z)  RI , then y = z).
F
DL-LiteN
 , DL-Lite and DL-Lite . If role inclusions are excluded from the language,
then for each   {core, krom, horn, bool} we obtain three fragments: DL-LiteN
 (with arbitrary number restrictions), DL-LiteF
(with
functionality
constraints
and
existential
concepts

R), and DL-Lite (without number restrictions different from R).

As we shall see later on in this article, the logics of the form DL-LiteHF
and DL-LiteHN

 ,
even for  = core, turn out to be computationally rather costly because of the interaction
between role inclusions and functionality constraints (or, more generally, number restrictions). On the other hand, for the purpose of conceptual modeling one may need both of
these constructs; cf. the example in Section 2.2. A compromise can be found by artificially
limiting the interplay between role inclusions and number restrictions in a way similar to
the logic DL-LiteA proposed by Poggi et al. (2008a).
For a TBox T , let vT denote the reflexive and transitive closure of the relation

	
(R, R0 ), (inv(R), inv(R0 )) | R v R0  T
and let R T R0 iff R vT R0 and R0 vT R. Say that R0 is a proper sub-role of R in T if
R0 vT R and R0 
6 T R.
(HN )

(HN )

DL-Lite . We now introduce the logics DL-Lite ,
  {core, krom, horn, bool},
HN
which, on the one hand, restrict the logics DL-Lite by limiting the interaction between
role inclusions and number restrictions in order to reduce complexity of reasoning, and, on
the other hand, include additional constructs, such as limited qualified existential quantifiers, role disjointness, (a)symmetry and (ir)reflexivity constraints, which increase the
expressive power of the logics but do not affect their computational properties.
(HN )
DL-Lite
TBoxes T must satisfy the conditions (A1 )(A3 ) below. (We remind
the reader that an occurrence of a concept on the right-hand (left-hand) side of a concept
inclusion is called negative if it is in the scope of an odd (even) number of negations ;
otherwise the occurrence is called positive.)
(A1 ) T may contain only positive occurrences of qualified number restrictions  q R.C,
where C is a conjunction of concepts allowed on the right-hand side of -concept
inclusions;
(A2 ) if  q R.C occurs in T , then T does not contain negative occurrences of number
restrictions  q 0 R or  q 0 inv(R) with q 0  2;
(A3 ) if R has a proper sub-role in T , then T does not contain negative occurrences of
 q R or  q inv(R) with q  2.
8

fiThe DL-Lite Family and Relations

role
role
constraints inclusions
no

no

no

yes

disj.
(a)sym.
(ir)ref.
disj.
(a)sym.
(ir)ref.
tran.
a)

number
restrictions
R
R/funct.
 qR
R
R/funct.
 qR

concept inclusions
Krom
Horn
DL-Litekrom DL-Litehorn
DL-LiteF
DL-LiteF
krom
horn
N
DL-Litekrom DL-LiteN
horn
H
DL-LiteH
DL-Lite
krom
horn
HF
DL-Litekrom DL-LiteHF
horn
HN
DL-LiteHN
DL-Lite
krom
horn

core
DL-Litecore
DL-LiteF
core
DL-LiteN
core
DL-LiteH
core
DL-LiteHF
core
DL-LiteHN
core

(HF )

Bool
DL-Litebool
DL-LiteF
bool
DL-LiteN
bool
DL-LiteH
bool
DL-LiteHF
bool
DL-LiteHN
bool

(HF )

(HF )

)
R.C/funct.a) DL-Lite(HF
DL-Litekrom DL-Litehorn DL-Litebool
core
(HN )
(HN )
(HN )
(HN )
a)
 q R.C
DL-Litecore
DL-Litekrom DL-Litehorn DL-Litebool

yes

+

)
R.C/funct.a) DL-Lite(HF
core

yes

 q R.C a)

(HF )+

DL-Litekrom

+

(HN )+

)
DL-Lite(HN
DL-Litekrom
core

(HF )+

DL-Litehorn

(HN )+

DL-Litehorn

(HF )+

DL-Litebool

(HN )+

DL-Litebool

restricted by (A1 )(A3 ).

Table 1: The extended DL-Lite family.
(HN )

(It follows that no DL-Lite
TBox can contain both, say, a functionality constraint
 2 R v  and an occurrence of  q R.C, for any q  1.)
(HN )
Additionally, DL-Lite
TBoxes can contain role constraints (or axioms) of the form:
Dis(R1 , R2 ),

Asym(Pk ),

Sym(Pk ),

Irr(Pk ),

and

Ref(Pk ).

The meaning of these new constructs is defined as usual: for an interpretation I = (I , I ),

	
 ( q R.C)I = x  I | ]{y  C I | (x, y)  RI }  q ;
 I |= Dis(R1 , R2 )
 I |= Asym(Pk )
 I |= Sym(Pk )
 I |= Irr(Pk )
 I |= Ref(Pk )

iff
iff

iff
iff
iff

R1I  R2I =  (roles R1 and R2 are disjoint);
PkI  (Pk )I =  (role Pk is asymmetric);

PkI = (Pk )I

(Pk is symmetric);

(x, x) 
/ PkI for all x  I
(x, x)  PkI for all x  I

(Pk is irreflexive);
(Pk is reflexive).

It is to be emphasized that these extra constructs are often used in conceptual modeling
(HN )
and their introduction in DL-Lite
is motivated by the OWL 2 QL proposal. (Note that
(HN )
DL-Lite
contains both DL-LiteH
and
DL-LiteN

 as its proper fragments.)
(HN )+

(HN )+

DL-Lite
.
For   {bool, horn, krom, core}, denote by DL-Lite
the extension
(HN )
of DL-Lite
with role transitivity constraints of the form Tra(Pk ), the meaning of which
is as expected:
 I |= Tra(Pk )

iff

(x, y)  PkI and (y, z)  PkI imply (x, z)  PkI , for all x, y, z  I
(Pk is transitive).
9

fiArtale, Calvanese, Kontchakov & Zakharyaschev

DL-Litebool

DL-LiteHN


@
I
@
DL-Litehorn






DL-Litekrom






@
I
@

DL-Litecore

PP
i

6
DL-LiteHF

6
DL-LiteH


DL-LiteN


(HN ) )+
DL-Lite
DL-Lite(HN


1
6
6

6
PP
i


1

DL-LiteF


6

 



iP
P
DL-Lite

)+
(HF ) DL-Lite
DL-Lite(HF



Figure 1: Language inclusions in the extended DL-Lite family.
We remind the reader of the standard restriction limiting the use of transitive roles in DLs
(see, e.g., Horrocks, Sattler, & Tobies, 2000):
 only simple roles R are allowed in concepts of the form  q R, for q  2,
where by a simple role in a given TBox T we understand a role without transitive sub-roles
(including itself). In particular, if T contains Tra(P ) then P and P  are not simple, and
so T cannot contain occurrences of concepts of the form  q P and  q P  , for q  2.
(HF )

(HF )+

(HF )

DL-Lite
and DL-Lite .
We also define languages DL-Lite
as sub-languages
(HN )
of DL-Lite ,
in which only number restrictions of the form R, R.C and functionality
constraints  2 R v  are allowedprovided, of course, that they satisfy (A1 )(A3 ); in
(HF )+
particular, R.C is not allowed if R is functional. As before, DL-Lite
are the extensions
(HF )
of DL-Lite
with role transitivity constraints (satisfying the restriction above).
Thus, the extended DL-Lite family we consider in this article consists of 40 different
logics collected in Table 1. The inclusions between these logics are shown in Figure 1.
They are obtained by taking the product of the left- and right-hand parts of the picture,
where the subscript  on the right-hand part ranges over {core, krom, horn, bool}, i.e., the
subscripts on the left-hand part, and similarly, the superscript  on the left-hand part
ranges over { , F, N , H, HF, HN , (HF), (HN ), (HF)+ , (HN )+ }, i.e., the superscripts on
the right-hand part.
The position of these logics relative to other DL-Lite logics known in the literature and
the OWL 2 QL profile will be discussed in Section 4. And starting from Section 5, we begin
a thorough investigation of the computational properties of the logics in the extended DLLite family, both with and without the UNA. But before that we illustrate the expressive
power of the DL-Lite logics by a concrete example.
2.2 DL-Lite for Conceptual Modeling
A tight correspondence between conceptual modeling formalisms, such as the ER model
and UML class diagrams, and various description logics has been pointed out in various
papers (e.g., Calvanese et al., 1998b, 1999; Borgida & Brachman, 2003; Berardi et al.,
2005). Here we give an example showing how DL-Lite logics can be used for conceptual
modeling purposes; for more details see the work by Artale et al. (2007b).
10

fiThe DL-Lite Family and Relations

1..1

Employee

1..*

empCode: Integer
salary: Integer

worksOn
boss
3..*
Project

Manager

projectName: String

1..*
1..1
{disjoint, complete}

AreaManager

manages
TopManager

1..1

Figure 2: A UML class diagram.
Let us consider the UML class diagram depicted in Figure 2 and representing (a portion
of) a company information system. According to the diagram, all managers are employees and are partitioned into area managers and top managers. This information can be
represented by means of the following concept inclusions (where in brackets we specify the
minimal DL-Lite language the inclusion belongs to):
Manager v Employee

(DL-Litecore )

AreaManager v Manager

(DL-Litecore )

TopManager v Manager

(DL-Litecore )

AreaManager v TopManager

(DL-Litecore )

Manager v AreaManager t TopManager

(DL-Litebool )

Each employee has two functional attributes, empCode and salary, with integer values.
Unlike OWL, here we do not distinguish between abstract objects and data values. Hence
we model a datatype, such as Integer , by means of a concept, and an attribute, such as
employees salary, by means of a role. Thus, salary can be represented as follows:
Employee v salary
salary



(DL-Litecore )

v Integer

(DL-Litecore )
(DL-LiteF
core )

 2 salary v 

The functional attribute empCode with values in Integer is represented in the same way.
The binary relationship worksOn has Employee as its domain and Project as its range:
worksOn v Employee
worksOn



(DL-Litecore )

v Project

(DL-Litecore )

The binary relationship boss with domain Employee and range Manager is treated analogously. Each employee works on a project and has exactly one boss, while a project must
11

fiArtale, Calvanese, Kontchakov & Zakharyaschev

involve at least three employees:
Employee v worksOn

(DL-Litecore )

Employee v boss

(DL-Litecore )
(DL-LiteF
core )

 2 boss v 
Project v  3 worksOn 

(DL-LiteN
core )

A top manager manages exactly one project and also works on that project, while a project
is managed by exactly one top manager:
manages v TopManager
manages



v Project

(DL-Litecore )

TopManager v manages
Project v manages

(DL-Litecore )
(DL-Litecore )



(DL-Litecore )

 2 manages v 

(DL-LiteF
core )

 2 manages  v 

(DL-LiteF
core )
(DL-LiteH
core )

manages v worksOn

All in all, the only languages in the extended DL-Lite family capable of representing the
(HN )
UML class diagram in Figure 2 are DL-LiteHN
bool and DL-Litebool . Note, however, that except for the covering constraint, Manager v AreaManager t TopManager , all other concept
inclusions in the DL-Lite translation of the UML class diagram belong to variants of the
(HN )
core fragments DL-LiteHN
core and DL-Litecore . It is not hard to imagine a situation where
one needs Horn concept inclusions to represent integrity constraints over UML class diagrams, for example, to express (together with the above axioms) that no chief executive
officer may work on five projects and be a manager of one of them:
CEO u ( 5 worksOn) u manages v 

(DL-LiteN
horn )

In the context of UML class diagrams, the Krom fragment DL-Litekrom (with its variants)
seems to be useless: it extends DL-Litecore with concept inclusions of the form B1 v B2
or, equivalently, > v B1 t B2 , which are rarely used in conceptual modeling. Indeed,
this would correspond to partitioning the whole domain of interest in just two parts, while
more general and useful covering constraints of the form B v B1 t    t Bk require the full
Bool language. On the other hand, the Krom fragments are important for pinpointing the
borderlines of various complexity classes over the description logics of the DL-Lite family
and their extensions; see Table 2.

3. Reasoning in DL-Lite Logics
We discuss now the reasoning problems we consider in this article, their mutual relationships, and the complexity measures we adopt. We also provide an overview of the complexity
results for DL-Lite logics obtained in this article.

12

fiThe DL-Lite Family and Relations

3.1 Reasoning Problems
We will concentrate on three fundamental and standard reasoning tasks for description
logics: satisfiability (or consistency), instance checking, and query answering.
For a DL L in the extended DL-Lite family, we define an L-concept inclusion as any
concept inclusion allowed in L. Similarly, we define the notions of L-KB and L-TBox.
Finally, define an L-concept as any concept that can occur on the right-hand side of an
L-concept inclusion or a conjunction of such concepts.
Satisfiability. The KB satisfiability problem is to check, given an L-KB K, whether there
is a model of K. Clearly, satisfiability is the minimal requirement for any ontology. As is
well known in DL (Baader et al., 2003), many other reasoning tasks for description logics
are reducible to the satisfiability problem. Consider, for example, the subsumption problem:
given an L-TBox T and an L-concept inclusion C1 v C2 , decide whether T |= C1 v C2 ,
that is, C1I  C2I , for every model I of T . To reduce this problem to (un)satisfiability, take
a fresh concept name A, a fresh object name a, and set K = (T 0 , A), where
T 0 = T  {A v C1 , A v C2 }

and A = {A(a)}.

It is easy
d to see that T |= C1 v C2 iff K is not satisfiable. For core, Krom and Horn KBs, if
C2 = k Dk , where each Dk is a (possibly negated) basic concept, checking unsatisfiability
of K amounts to checking unsatisfiability of each of the KBs Kk = (Tk , A), where Tk =
T  {A v C1 , A v Dk } (for Horn KBs, replace A v B with the equivalent A u B v ).
The concept satisfiability problemgiven an L-TBox T and an L-concept C, decide
whether C I 6=  in a model I of T is also easily reducible to KB satisfiability. Indeed,
take a fresh concept name A, a fresh object name a, and set K = (T 0 , A), where
T 0 = T  {A v C}

and A = {A(a)}.

Then C is satisfiable with respect to T iff K is satisfiable.
Instance checking. The instance checking problem is to decide, given an object name a,
an L-concept C and an L-KB K = (T , A), whether K |= C(a), that is, aI  C I , for every
model I of K. Instance checking is also reducible to (un)satisfiability: an object a is an
instance of an L-concept C in every model of K = (T , A) iff the KB K0 = (T 0 , A0 ), with
T 0 = T  {A v C}

and

A0 = A  {A(a)},

is notdsatisfiable, where A is a fresh concept name. For core, Krom and Horn KBs, if
C = k Dk , where each Dk is a (possibly negated) basic concept, we can proceed as for
subsumption: checking the unsatisfiability of K0 amounts to checking the unsatisfiability of
each KB Kk0 = (Tk0 , A0 ) with Tk0 = T  {A v Dk }.
Conversely, KB satisfiability is reducible to the complement of instance checking: K is
satisfiable iff K 6|= A(a), for a fresh concept name A and a fresh object a.
Query answering. A positive existential query q(x1 , . . . , xn ) is any first-order formula
(x1 , . . . , xn ) constructed by means of conjunction, disjunction and existential quantification starting from atoms of the from Ak (t) and Pk (t1 , t2 ), where Ak is a concept name, Pk
13

fiArtale, Calvanese, Kontchakov & Zakharyaschev

a role name, and t, t1 , t2 are terms taken from the list of variables y0 , y1 , . . . and the list of
object names a0 , a1 , . . . (i.e.,  is a positive existential formula). More precisely,
t

::=

yi

|



::=

Ak (t)

ai ,
|

Pk (t1 , t2 )

|

1  2

|

1  2

|

yi .

The free variables of  are called distinguished variables of q and the bound ones are nondistinguished variables of q. We write q(x1 , . . . , xn ) for a query with distinguished variables
x1 , . . . , xn . A conjunctive query is a positive existential query that contains no disjunction
(it is constructed from atoms by means of conjunction and existential quantification only).
Given a query q(~x) = (~x) with ~x = x1 , . . . , xn and an n-tuple ~a of object names, we
write q(~a) for the result of replacing every occurrence of xi in (~x) with the ith member of
~a. Queries containing no distinguished variables will be called ground (they are also known
as Boolean).
Let I = (I , I ) be an interpretation. An assignment a in I is a function associating
with every variable y an element a(y) of I . We will use the following notation: aI,a
= aIi
i
and y I,a = a(y). The satisfaction relation for positive existential formulas with respect to
a given assignment a is defined inductively by taking:
I |=a Ak (t)

iff

tI,a  AIk ,

I |=a Pk (t1 , t2 )

iff

I,a
I
(tI,a
1 , t2 )  Pk ,

I |=a 1  2

iff

I |=a 1 and I |=a 2 ,

I |=a 1  2

iff

I |=a 1 or I |=a 2 ,

I |=a yi 

iff

I |=b , for some assignment b in I that may differ from a on yi .

For a ground query q(~a), the satisfaction relation does not depend on the assignment a,
and so we write I |= q(~a) instead of I |=a q(~a). The answer to such a query is either yes
or no.
For a KB K = (T , A), we say that a tuple ~a of object names from A is a certain answer
to q(~x) with respect to K, and write K |= q(~a), if I |= q(~a) whenever I |= K. The query
answering problem can be formulated as follows: given an L-KB K = (T , A), a query q(~x),
and a tuple ~a of object names from A, decide whether K |= q(~a).
Note that the instance checking problem is a special case of query answering: an object
a is an instance of an L-concept C with respect to a KB K iff the answer to the query A(a)
with respect to K0 is yes, where K0 = (T 0 , A) and T 0 = T  {C v A}, with A a fresh
concept name. For Horn-concepts B1 u    u Bk , we consider the query A1 (a)      Ak (a)
with respect to K0 , where K0 = (T 0 , A) and T 0 = T  {B1 v A1 , . . . , Bk v Ak }, with
the Ai fresh concept names. Similarly, we deal with Krom-concepts D1 u    u Dk , where
each Di is a possibly negated basic concept. For core-concepts, the reduction holds just for
conjunctions of basic concepts.
3.2 Complexity Measures: Data and Combined Complexity
The computational complexity of the reasoning problems discussed above can be analyzed
with respect to different complexity measures, which depend on those parameters of the
14

fiThe DL-Lite Family and Relations

problem that are regarded to be the input (i.e., can vary) and those that are regarded to
be fixed. For satisfiability and instance checking, the parameters to consider are the size
of the TBox T and the size of the ABox A, that is the number of symbols in T and A,
denoted |T | and |A|, respectively. The size |K| of the knowledge K = (T , A) is simply given
by |T | + |A|. For query answering, one more parameter to consider would be the size of the
query. However, in our analysis we adopt the standard database assumption that the size
of queries is always bounded by some reasonable constant and, in any case, negligible with
respect to both the size of the TBox and the size of the ABox. Thus we do not count the
query as part of the input.
Hence, we consider our reasoning problems under two complexity measures. If the whole
KB K is regarded as an input, then we deal with combined complexity. If, however, only
the ABox A is counted as an input, while the TBox T (and the query) is regarded to be
fixed, then our concern is data complexity (Vardi, 1982). Combined complexity is of interest
when we are still designing and testing the ontology. On the other hand, data complexity is
preferable in all those cases where the TBox is fixed or its size (and the size of the query) is
negligible compared to the size of the ABox, which is the case, for instance, in the context
of ontology-based data access (Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, & Rosati,
2007) and other data intensive applications (Decker, Erdmann, Fensel, & Studer, 1999; Noy,
2004; Lenzerini, 2002; Calvanese et al., 2008). Since the logics of the DL-Lite family were
tailored to deal with large data sets stored in relational databases, data complexity of both
instance checking and query answering is of particular interest to us.
3.3 Remarks on the Complexity Classes LogSpace and AC0
In this paper, we deal with the following complexity classes:
AC0 ( LogSpace  NLogSpace  P  NP  ExpTime.
Their definitions can be found in the standard textbooks (e.g., Garey & Johnson, 1979;
Papadimitriou, 1994; Vollmer, 1999; Kozen, 2006). Here we only remind the reader of the
two smallest classes LogSpace and AC0 .
A problem belongs to LogSpace if there is a two-tape Turing machine M such that,
starting with an input of length n written on the read-only input tape, M stops in an accepting or rejecting state having used at most log n cells of the (initially blank) read/write work
tape. A LogSpace transducer is a three-tape Turing machine that, having started with an
input of length n written on the read-only input tape, writes the result (of polynomial size)
on the write-only output tape using at most log n cells of the (initially blank) read/write
work tape. A LogSpace-reduction is a reduction computable by a LogSpace transducer;
the composition of two LogSpace transducers is also a LogSpace transducer (Kozen,
2006, Lemma 5.1).
The formal definition of the complexity class AC0 (see, e.g., Boppana & Sipser, 1990;
Vollmer, 1999 and references therein) is based on the circuit model, where functions are
represented as directed acyclic graphs built from unbounded fan-in And, Or and Not
gates (i.e., And and Or gates may have an unbounded number of incoming edges). For
this definition we assume that decision problems are encoded in the alphabet {0, 1} and
so can be regarded as Boolean functions. AC0 is the class of problems definable using
15

fiArtale, Calvanese, Kontchakov & Zakharyaschev

a family of circuits of constant depth and polynomial size, which can be generated by
a deterministic Turing machine in logarithmic time (in the size of the input); the latter
condition is called LogTime-uniformity. Intuitively, AC0 allows us to use polynomially
many processors but the run-time must be constant. A typical example of an AC0 problem
is evaluation of first-order queries over databases (or model checking of first-order sentences
over finite models), where only the database (first-order model) is regarded as the input
and the query (first-order sentence) is assumed to be fixed (Abiteboul, Hull, & Vianu, 1995;
Vollmer, 1999). On the other hand, the undirected graph reachability problem is known to
be in LogSpace (Reingold, 2008) but not in AC0 . A Boolean function f : {0, 1}n  {0, 1}
is called AC0 -reducible (or constant-depth reducible) to a function g : {0, 1}n  {0, 1} if
there is a (LogTime-uniform) family of constant-depth circuits built from And, Or, Not
and g gates that computes f . In this case we say that there is an AC0 -reduction. Note that
all the reductions considered in Section 3.1 are AC0 -reductions. Unless otherwise indicated,
in what follows we write reduction for AC0 -reduction.
3.4 Summary of Complexity Results
In this article, our aim is to investigate (i) the combined and data complexity of the satisfiability and instance checking problems and (ii) the data complexity of the query answering
problem for the logics of the extended DL-Lite family, both with and without the UNA.
(HF )+
The obtained and known results for the first 32 logics from Table 1 (the logics DL-Lite
(HN )+
and DL-Lite
are not included) are summarized in Table 2 (we remind the reader that
satisfiability and instance checking are reducible to the complements of each other and that
instance checking is a special case of query answering). In fact, all of the results in the
table follow from the lower and upper bounds marked with [] and [], respectively (by
taking into account the hierarchy of languages of the DL-Lite family): for example, the
NLogSpace membership of satisfiability in DL-LiteN
krom in Theorem 5.7 implies the same
N
F
upper bound for DL-Litekrom , DL-Litekrom , DL-Litecore , DL-LiteF
core and DL-Litecore because
.
all of them are sub-languages of DL-LiteN
krom
Remark 3.1 Two further complexity results are to be noted (they are not included in
Table 2):
(i) If equality between object names is allowed in the language of DL-Lite, which only
makes sense if the UNA is dropped, then the AC0 memberships in Table 2 are replaced by LogSpace-completeness (see Section 8, Theorem 8.3 and 8.9); inequality
constraints do not affect the complexity.
(ii) If we extend any of our languages with role transitivity constraints then the combined complexity of satisfiability remains the same, while for data complexity, instance
checking and query answering become NLogSpace-hard (see Lemma 6.3), i.e., the
membership in AC0 for data complexity is replaced by NLogSpace-completeness,
while all other complexity results remain the same.
In either case, the property of first-order rewritabilitythat is, the possibility of rewriting
a given query q and a given TBox T into a single first-order query q0 returning the certain
answers to q over (T , A) for every ABox A, which ensures that the query answering problem
is in AC0 for data complexityis lost.
16

fiThe DL-Lite Family and Relations

Complexity
Languages

UNA

Combined complexity
Satisfiability

|H]
DL-Lite[core
[ |H]
DL-Litehorn
[ |H]
DL-Litekrom
[ |H]
DL-Litebool
|N |(HF )|(HN )]
DL-Lite[F
core
[F |N |(HF )|(HN )]
DL-Litehorn
[F |N |(HF )|(HN )]
DL-Litekrom
[F |N |(HF )|(HN )]
DL-Litebool
[F |(HF )]
DL-Litecore/horn
[F |(HF )]
DL-Litekrom
[F |(HF )]
DL-Litebool
[N |(HN )]
DL-Litecore/horn
[N |(HN )]
DL-Litekrom/bool
DL-LiteHF
core/horn
DL-LiteHF
krom/bool
DL-LiteHN
core/horn
HN
DL-Litekrom/bool

yes/no

yes

no

yes/no

Data complexity
Instance checking
0

Query answering

NLogSpace  [A]

in AC

in AC0

P  [Th.8.2]  [A]

in AC0

in AC0  [C]

0

NLogSpace  [Th.8.2]

in AC

coNP  [B]

NP  [Th.8.2]  [A]

in AC0  [Th.8.3]

coNP

0

NLogSpace

in AC

in AC0

P  [Th.5.8, 5.13]

in AC0

in AC0  [Th.7.1]

NLogSpace  [Th.5.7,5.13]

in AC0

coNP

0

NP  [Th.5.6, 5.13]

in AC  [Cor.6.2]

coNP

P  [Cor.8.8]  [Th.8.7]

P  [Th.8.7]

P

P  [Cor.8.8]

P

coNP

NP

P  [Cor.8.8]

coNP

NP  [Th.8.4]

coNP  [Th.8.4]

coNP

NP  [Th.8.5]

coNP

coNP

ExpTime  [Th.5.10]

P  [Th.6.7]

P  [D]

ExpTime

coNP  [Th.6.5]

coNP

ExpTime

coNP  [Th.6.6]

coNP

ExpTime  [F]

coNP

coNP  [E]

[A] complexity of the respective fragment of propositional Boolean logic
[B] follows from the proof of the data complexity result for instance checking in ALE (Schaerf, 1993)
[C] (Calvanese et al., 2006)
[D] follows from Horn-SHIQ (Hustadt, Motik, & Sattler, 2005; Eiter, Gottlob, Ortiz, & Simkus, 2008)
[E] follows from SHIQ (Ortiz, Calvanese, & Eiter, 2006, 2008; Glimm, Horrocks, Lutz, & Sattler, 2007)
[F] follows from SHIQ (Tobies, 2001)

Table 2: Complexity of DL-Lite logics (all the complexity bounds save in AC0  are tight).

1 ||n ]
means any of DL-Lite1 , . . . , DL-Liten
DL-Lite[

(in particular, DL-Lite[ |H] is either DL-Lite or DL-LiteH
 ).

DL-Litecore/horn means DL-Litecore or DL-Litehorn (likewise for DL-Litekrom/bool ).
 [X] ( [X]) means that the upper (respectively, lower) bound follows from [X].

Detailed proofs of our results will be given in Sections 58. For the variants of logics
involving number restrictions, all upper bounds hold also under the assumption that the
numbers q in concepts of the form  q R are given in binary. (Intuitively, this follows from
the fact that in our proofs we only use those numbers that explicitly occur in the KB.) All
lower bounds remain the same for the unary coding, since in the corresponding proofs we
only use numbers not exceeding 4.
In the next section we consider the extended DL-Lite family in a more general context by
identifying its place among other DL-Lite-related logics, in particular the OWL 2 profiles.
17

fiArtale, Calvanese, Kontchakov & Zakharyaschev

4. The Landscape of DL-Lite Logics
The original family of DL-Lite logics was created with two goals in mind: to identify
description logics that, on the one hand, are capable of representing some basic features
of conceptual modeling formalisms (such as UML class diagrams and ER diagrams) and,
on the other hand, are computationally tractable, in particular, matching the AC0 data
complexity of database query answering.
As we saw in Section 2.2, to represent UML class diagrams one does not need the typical quantification constructs of the basic description logic ALC (Schmidt-Schau & Smolka,
1991), namely, universal restriction R.C and qualified existential quantification R.C: one
can always take the role filler C to be >. Indeed, domain and range restrictions for a
relationship P can be expressed by the concepts inclusions P v B1 and P  v B2 , respectively. Thus, almost all concept inclusions required for capturing UML class diagrams
are of the form B1 v B2 or B1 v B2 . These observations motivated the introduction by
Calvanese et al. (2005) of the first DL-Lite logic, which in our new nomenclature corresponds
to DL-LiteF
core . Their main results were a polynomial-time upper bound for the combined
complexity of KB satisfiability and a LogSpace upper bound for the data complexity of
conjunctive query answering (under the UNA). These results were extended by Calvanese
H
et al. (2006) to two larger languages: DL-LiteF
horn and DL-Litehorn , which were originally
called DL-Liteu,F and DL-Liteu,R , respectively. Calvanese et al. (2007b) introduced another member of the DL-Lite family (named DL-LiteR ), which extended DL-LiteH
core with
role disjointness axioms of the form Dis(R1 , R2 ). The computational behavior of the new
logic turned out to be the same as that of DL-LiteH
core . It may be worth mentioning that
DL-LiteH
covers
the
DL
fragment
of
RDFS
(Klyne
& Carroll, 2004; Hayes, 2004). Note
core
also that Calvanese et al. (2006) considered the variants of both DL-Liteu,F and DL-Liteu,R
with arbitrary n-ary relations (not only the usual binary roles) and showed that query answering in them is still in LogSpace for data complexity. We conjecture that similar results
can be obtained for the other DL-Lite logics introduced in this paper. Artale et al. (2007b)
demonstrated how n-ary relations can be represented in DL-LiteF
core by means of reification.
A further variant of DL-Lite, called DL-LiteA (A for attributes), was introduced by
Poggi et al. (2008a) with the aim of capturing as many features of conceptual modeling
formalisms as possible, while still maintaining the computational properties of the basic
variants of DL-Lite. One of the features in DL-LiteA , borrowed from conceptual modeling
formalisms and adopted also in OWL, is the distinction between (abstract) objects and data
values, and consequently, between concepts (sets of objects) and datatypes (sets of data
values), and between roles (i.e., object properties in OWL, relating objects with objects)
and attributes (i.e., data properties in OWL, relating objects with data values). However,
as far as the results in this paper are concerned, the distinction between concepts and
datatypes, and between roles and attributes has no impact on reasoning whatsoever, since
datatypes can simply be treated as special concepts that are mutually disjoint and are also
disjoint from the proper concepts. Instead, more relevant for reasoning is the possibility
to express in DL-LiteA both role inclusions and functionality, i.e., DL-LiteA includes both
F
HF
DL-LiteH
core and DL-Litecore , but not DL-Litecore .
As we have already mentioned, role inclusions and functionality constraints cannot be
combined in an unrestricted way without losing the good computational properties: in

18

fiThe DL-Lite Family and Relations

.
b
b

DL-LiteHN
krom
b

b

b

...

SHIQ
DL-LiteHN
horn
b

DL-LiteHN
bool
b

...
b

DL-Litekrom
b

b

DL-LiteHF
core
b

(HN )

b

b

b

P

b

DL-Litecore
(HF )
DL-Litecore

DL-LiteN
core

(HN )

DL-Litehorn
(HF )
DL-Litehorn
DL-LiteN
horn
+
DL-LiteA,u
b

b

b

b

b

DL-LiteHF
horn

DL-Lite+
A
DL-LiteA
DL-LiteF = DL-LiteF
DL-LiteF ,u = DL-LiteF
core
horn
H
DL-LiteR = DL-Litecore
DL-LiteR,u = DL-LiteH
horn
DL-Litecore
b

b

in AC0

DL-Litebool

Horn-SHIQ

coNP

.

b

DL-LiteHN
core

b

b

b

Figure 3: The DL-Lite family and relations.
Theorems 5.10 and 6.7, we prove that satisfiability of DL-LiteHF
core KBs is ExpTime-hard
for combined complexity, while instance checking is data-hard for P (NLogSpace-hardness
was shown by Calvanese et al., 2006). In DL-LiteA , to keep query answering in AC0 for
data complexity and satisfiability in NLogSpace for combined complexity, functional roles
(and attributes) are not allowed to be specialized, i.e., used positively on the right-hand
side of role (and attribute) inclusion axioms. So, condition (A3 ) is a slight generalization
of this restriction. DL-LiteA also allows axioms of the form B v R.C for non-functional
roles R, which is covered by conditions (A1 ) and (A2 ). Thus, DL-LiteA can be regarded
(HF )
(HN )
as a proper fragment of both DL-Litecore and DL-Litehorn . We show in Sections 5.3 and 7
that these three languages enjoy very similar computational properties under the UNA:
tractable satisfiability and query answering in AC0 .
We conclude this section with a picture in Figure 3 illustrating the landscape of DLLite-related logics by grouping them according to the data complexity of positive existential
query answering under the UNA. The original eight DL-Lite logics, called by Calvanese
et al. (2007b) the DL-Lite family, are shown in the bottom sector of the picture (the logics
+
DL-Lite+
A and DL-LiteA,u extend DL-LiteA and DL-LiteA,u with identification constraints,
(HN )

which are out of the scope of this article). Their nearest relatives are the logic DL-Litehorn
and its fragments, which are all in AC0 as well. The next layer contains the logics DL-LiteHF
core
and DL-LiteHF
,
in
which
query
answering
is
data-complete
for
P
(no
matter
whether
the
horn
UNA is adopted or not). In fact, these logics are fragments of the much more expressive DL
Horn-SHIQ, which was shown to enjoy the same data complexity of query answering by
Eiter et al. (2008). It remains to be seen whether polynomial query answering is practically
feasible; recent experiments with the DL EL (Lutz, Toman, & Wolter, 2008) indicate that
this may indeed be the case. Finally, very distant relatives of the DL-Lite family comprise

19

fiArtale, Calvanese, Kontchakov & Zakharyaschev

the upper layer of the picture, where query answering is data-complete for coNP, that is,
the same as for the very expressive DL SHIQ.
4.1 The DL-Lite Family and OWL 2
The upcoming version 2 of the Web Ontology Language OWL7 defines three profiles,8 that
is, restricted versions of the language that suit specific needs. The DL-Lite family, notably
DL-LiteH
core (or the original DL-LiteR ), is at the basis of one of these OWL 2 profiles, called
OWL 2 QL. According to http://www.w3.org/TR/owl2-profiles/, OWL 2 QL is aimed at
applications that use very large volumes of instance data, and where query answering is the
most important reasoning task. In OWL 2 QL, [. . . ] sound and complete conjunctive query
answering can be performed in LogSpace with respect to the size of the data (assertions)
[and] polynomial time algorithms can be used to implement the ontology consistency and
class expression subsumption reasoning problems. The expressive power of the profile is
necessarily quite limited, although it does include most of the main features of conceptual
models such as UML class diagrams and ER diagrams. In this section, we briefly discuss
the results obtained in this article in the context of additional constructs that are present
in OWL 2.
A very important difference between the DL-Lite family and OWL is the status of the
unique name assumption (UNA): this assumption is quite common in data management,
and hence adopted in the DL-Lite family, but not adopted in OWL. Instead, the OWL
syntax provides explicit means for stating that object names, say a and b, are supposed to
denote the same individual, a  b, or that they should be interpreted differently, a 6 b (in
OWL, these constructs are called sameAs and differentFrom).
The complexity results we obtain for logics of the form DL-LiteH
 do not depend on
whether the UNA is adopted or not (because every model of a DL-LiteH
 KB without UNA
can be untangled into a model of the same KB respecting the UNA; see Lemma 8.10).
N
However, this is not the case for the logics DL-LiteF
 and DL-Lite , where there is an obvious
interaction between the UNA and number restrictions (cf. Table 2). For example, under the
0
UNA, instance checking for DL-LiteF
core is in AC for data complexity, whereas dropping this
assumption results in a much higher complexity: in Section 8, we prove that it is P-complete.
H
The addition of the equality construct  to DL-LiteH
core and DL-Litehorn slightly changes
data complexity of query answering and instance checking, as it rises from membership in
AC0 to LogSpace-completeness; see Section 8. What is more important, however, is that
in this case we loose first-order rewritability of query answering and instance checking, and
as a result cannot use the standard database query engines in a straightforward manner.
Since the OWL 2 profiles are defined as syntactic restrictions of the language without
changing the basic semantic assumptions, it was chosen not to include in the OWL 2 QL
profile any construct that interferes with the UNA and which, in the absence of the UNA,
would cause higher complexity. That is why OWL 2 QL does not include number restrictions, not even functionality constraints. Also, keys (the mechanism of identifying objects
by means of the values of their properties) are not supported, although they are an impor7. http://www.w3.org/2007/OWL/
8. In logic, profiles would be called fragments as they are defined by placing restrictions on the OWL 2
syntax only.

20

fiThe DL-Lite Family and Relations

tant notion in conceptual modeling. Indeed, keys can be considered as a generalization of
functionality constraints (Toman & Weddell, 2005, 2008; Calvanese, De Giacomo, Lembo,
Lenzerini, & Rosati, 2007a, 2008b), since asserting a unary key, i.e., one involving only a
single role R, is equivalent to asserting the functionality of the inverse of R. Hence, in the
absence of the UNA, allowing keys would change the computational properties.
As we have already mentioned, some other standard OWL constructs, such as role disjointness, (a)symmetry and (ir)reflexivity constraints, can be added to the DL-Lite logics
without changing their computational behavior. Role transitivity constraints, Tra(R), as(HN )
serting that R must be interpreted as a transitive role, can also be added to DL-Litehorn but
this leads to the increase of the data complexity for all reasoning problems to NLogSpace,
although satisfiability remains in P for combined complexity. These results can be found
in Section 5.3.
Of other constructs of OWL 2 that so far are not supported by the DL-Lite logics we
mention nominals (i.e., singleton concepts), Boolean operators on roles, and role chains.

5. Satisfiability: Combined Complexity
DL-LiteHN
bool is clearly a sub-logic of the description logic SHIQ, the satisfiability problem
for which is known to be ExpTime-complete (Tobies, 2001).
In Section 5.1 we show, however, that the satisfiability problem for DL-LiteN
bool KBs is
1
reducible to the satisfiability problem for the one-variable fragment, QL , of first-order logic
without equality and function symbols. As satisfiability of QL1 -formulas is NP-complete
(see, e.g., Borger et al., 1997) and the logics under consideration contain full Booleans on
concepts, satisfiability of DL-LiteN
bool KBs is NP-complete as well. We shall also see that the
translations of Horn and Krom KBs into QL1 belong to the Horn and Krom fragments of
QL1 , respectively, which are known to be P- and NLogSpace-complete (see, e.g., Papadimitriou, 1994; Borger et al., 1997). In Section 5.2, we will show how to simulate the behavior of
polynomial-space-bounded alternating Turing machines by means of DL-LiteHF
core KBs. This
will give the (optimal) ExpTime lower bound for satisfiability of KBs in all the languages
of our family containing unrestricted occurrences of both functionality constraints and role
inclusions. In Section 5.3, we extend the embedding into QL1 , defined in Section 5.1, to the
(HN )
logic DL-Litebool , thereby establishing the same upper bounds as for DL-LiteN
bool and its
fragments. Finally, in Section 5.4 we investigate the impact of role transitivity constraints.
5.1 DL-LiteN
bool and its Fragments: First-Order Perspective
Our aim in this section is to construct a reduction of the satisfiability problem for DL-LiteN
bool
KBs to satisfiability of QL1 -formulas. We will do this in two steps: first we present a lengthy
yet quite natural and transparent (yet exponential) reduction  , and then we shall see from
the proof that this reduction can be substantially optimized to a linear reduction  .

Let K = (T , A) be a DL-LiteN
bool KB. Recall that role (K) denotes the set of direct and
inverse role names occurring in K and ob(A) the set of object names occurring in A. For
R  role (K), let QR
T be the set of natural numbers containing 1 and all the numbers q
for which the concept  q R occurs in T (recall that the ABox does not contain number
restrictions). Note that |QR
T |  2 if T contains a functionality constraint for R.

21

fiArtale, Calvanese, Kontchakov & Zakharyaschev

With every object name ai  ob(A) we associate the individual constant ai of QL1 and
with every concept name Ai the unary predicate Ai (x) from the signature of QL1 . For each
role R  role (K), we introduce |QR
T |-many fresh unary predicates
for q  QR
T.

Eq R(x),

The intended meaning of these predicates is as follows: for a role name Pk ,
 Eq Pk (x) and Eq Pk (x) represent the sets of points with at least q distinct Pk -successors
and at least q distinct Pk -predecessors, respectively. In particular, E1 Pk (x) and
E1 Pk (x) represent the domain and range of Pk , respectively.
Additionally, for every pair of roles Pk , Pk  role (K), we take two fresh individual constants
dpk
and
dp
k
of QL1 , which will serve as representatives of the points from the domains of Pk and
	
Pk , respectively (provided that they are not empty). Let dr(K) = dr | R  role (K) .
Furthermore, for each pair of object names ai , aj  ob(A) and each R  role (K), we take
a fresh propositional variable Rai aj of QL1 to encode the ABox assertion R(ai , aj ).9
1
By induction on the construction of a DL-LiteN
bool concept C we define the QL -formula
C :
 = ,

(Ai ) = Ai (x),

( q R) = Eq R(x),

(C) = C  (x),

(C1 u C2 ) = C1 (x)  C2 (x).

1

The DL-LiteN
bool TBox T corresponds then to the QL -sentence x T (x), where
^

T  (x) =
C1 (x)  C2 (x) .

(1)

C1 vC2 T

The ABox A is translated into the following pair of QL1 -sentences
^
^
1
A =
Ak (ai ) 
Ak (ai ),
A

2

=

^

(2)

Ak (ai )A

Ak (ai )A

Pk ai aj



^

Pk ai aj .

(3)

Pk (ai ,aj )A

Pk (ai ,aj )A

For every role R  role (K), we need two QL1 -formulas:
R (x) = E1 R(x)  inv(E1 R)(inv(dr)),
^

R (x) =
Eq0 R(x)  Eq R(x) ,

(4)
(5)

0
q,q 0 QR
T , q >q
0
00
q >q >q for no q 00 QR
T

9. In what follows, we slightly abuse notation and write R(ai , aj )  A to indicate that Pk (ai , aj )  A if
R = Pk , or Pk (aj , ai )  A if R = Pk .

22

fiThe DL-Lite Family and Relations

where (by overloading the inv operator),
(
Eq Pk , if R = Pk ,
inv(Eq R) =
Eq Pk , if R = Pk ,

and

(
dp
k,
inv(dr) =
dpk ,

if R = Pk ,
if R = Pk .

Formula (4) says that if the domain of R is not empty then its range is not empty either:
it contains the constant inv(dr), the representative of the domain of inv(R).
We also need formulas representing the relationship of the propositional variables Rai aj
with the unary predicates for the role domain and range: for a role R  role (K), let R be
the following QL1 -sentence
^

^

^

q
^


Rai ajk  Eq R(ai )

ai ob(A) qQR
aj1 ,...,ajq ob(A) k=1
T
jk 6=jk0 for k6=k0



^


Rai aj  inv(R)aj ai , (6)

ai ,aj ob(A)

where inv(R)aj ai is the propositional variable Pk aj ai if R = Pk and Pk aj ai if R = Pk .
Note that the first conjunct of (6) is the only part of the translation that relies on the UNA.
Finally, for the DL-LiteN
bool knowledge base K = (T , A), we set
i
h 1
h
^
^
i
2

A  A

R .
K = x T  (x) 
R (x)  R (x)
Rrole (K)

Rrole (K)

Thus, K is a universal sentence of QL1 .
Example 5.1 Consider, for example, the KB K = (T , A) with

	
T = A v P  , P  v A, A v  2 P, > v  1 P  , P v A
and A = {A(a), P (a, a0 )}. Then we obtain the following first-order translation:
K = x (x)  A(a)  P aa0 


P aa0  E1 P (a)  P aa  E1 P (a) 


P a0 a  E1 P (a0 )  P a0 a0  E1 P (a0 ) 


P  aa0  E1 P  (a)  P  aa  E1 P  (a) 


P  a0 a  E1 P  (a0 )  P  a0 a0  E1 P  (a0 ) 


P aa0  P aa  E2 P (a)  P a0 a  P a0 a0  E2 P (a0 ) 


P  aa0  P  aa  E2 P  (a)  P  a0 a  P  a0 a0  E2 P  (a0 ) 




P aa0  P  a0 a  P a0 a  P  aa0  P aa  P  aa  P a0 a0  P  a0 a0 .
where
(x) =

A(x)  E1 P  (x)



E1 P  (x)  A(x)  A(x)  E2 P (x) 


>  E2 P  (x)  E1 P (x)  A(x) 


E1 P (x)  E1 P  (dp )  E1 P  (x)  E1 P (dp) 


E2 P (x)  E1 P (x)  E2 P  (x)  E1 P  (x) . (7)




23

fiArtale, Calvanese, Kontchakov & Zakharyaschev

1
Theorem 5.2 A DL-LiteN
bool knowledge base K = (T , A) is satisfiable iff the QL -sentence

K is satisfiable.

Proof () If K is satisfiable then there is a model M of K whose domain consists of
all the constants occurring in K i.e., ob(A)  dr(K) (say, an Herbrand model of K ). We
denote this domain by D and the interpretations of the (unary) predicates P , propositional
variables p and constants a of QL1 in M by P M , pM and aM , respectively. Thus, for every
constant a, we have aM = a. Let D0 be the set of all constants a, a  ob(A). Without loss
of generality we may assume that D0 6= .
I
We construct an interpretation I for DL-LiteN
bool based on some domain   D0 that
will be inductively defined as the union

[

I =

where

Wm ,

W0 = D0 .

m=0

The interpretations of the object names ai in I are given by their interpretations in M,
namely, aIi = aM
i  W0 . Each set Wm+1 , for m  0, is constructed by adding to Wm some
new elements that are fresh copies of certain elements from D \ D0 . If such a new element
w0 is a copy of w  D \ D0 then we write cp(w0 ) = w, while for w  D0 we let cp(w) = w.
The set Wm \ Wm1 , for m  0, will be denoted by Vm (for convenience, let W1 = , so
that V0 = D0 ).
The interpretations AIk of concept names Ak in I are defined by taking

	
AIk = w  I | M |= Ak [cp(w)] .
(8)
The interpretation PkI of a role name Pk in I will be defined inductively as the union
PkI

=


[

Pkm ,

where

Pkm  Wm  Wm ,

m=0

along with the construction of I . First, for a role R  role (K), we define the required
R-rank r(R, d) of a point d  D by taking

r(R, d) = max {0}  { q  QR
T | M |= Eq R[d] } .
It follows from (5) that if r(R, d) = q then, for every q 0  QR
T , we have M |= Eq 0 R[d]
whenever q 0  q, and M |= Eq0 R[d] whenever q < q 0 . We also define the actual R-rank
rm (R, w) of a point w  I at step m by taking
(
]{w0  Wm | (w, w0 )  Pkm }, if R = Pk ,
rm (R, w) =
]{w0  Wm | (w0 , w)  Pkm }, if R = Pk .
For the basis of induction we set, for each role name Pk  role(K),

	
M
Pk0 = (aM
i , aj )  W0  W0 | M |= Pk ai aj .

(9)

Observe that, by (6), for all R  role (K) and w  W0 ,
r0 (R, w)  r(R, cp(w)).
24

(10)

fiThe DL-Lite Family and Relations

Suppose now that Wm and the Pkm , for m  0, have already been defined. If we had
rm (R, w) = r(R, cp(w)), for all roles R  role (K) and points w  Wm , then the interpretation I we need would be constructed. However, in general this is not the case because
there may be some defects in the sense that the actual rank of some points is smaller than
the required rank.
For a role name Pk  role(K), consider the following two sets of defects in Pkm :

	
m
w  Vm | rm (Pk , w) < r(Pk , cp(w)) ,
k =

	
m
= w  Vm | rm (Pk , w) < r(Pk , cp(w)) .
k
The purpose of, say, m
k is to identify those defective points w  Vm from which precisely
r(Pk , cp(w)) distinct Pk -arrows should start (according to M), but some arrows are still
missing (only rm (Pk , w) many arrows exist). To cure these defects, we extend Wm and
Pkm respectively to Wm+1 and Pkm+1 according to the following rules:
m
(m
k ) Let w  k , q = r(Pk , cp(w))  rm (Pk , w) and d = cp(w). We have M |= Eq 0 Pk [d]
0
for some q 0  QR
T with q  q > 0. Then, by (5), M |= E1 Pk [d] and, by (4),


M |= E1 Pk [dpk ]. In this case we take q fresh copies w10 , . . . , wq0 of dp
k (and set

0
0
cp(wi ) = dpk , for 1  i  q), add them to Wm+1 and add the pairs (w, wi ), 1  i  q,
to Pkm+1 .
m



(m
k ) Let w  k , q = r(Pk , cp(w))  rm (Pk , w) and d = cp(w). Then M |= Eq 0 Pk [d]

0
for some q 0  QR
T with q  q > 0. So, by (5), we have M |= E1 Pk [d] and, by (4),
0
0
M |= E1 Pk [dpk ]. Take q fresh copies w1 , . . . , wq of dpk (and set cp(wi0 ) = dpk , for
1  i  q), add them to Wm+1 and add the pairs (wi0 , w), 1  i  q, to Pkm+1 .

Example 5.3 Consider again the KB K and its first-order translation K from Example 5.1.
Consider also a model M of K with the domain D = {a, a0 , dp, dp }, where
AM = (E1 P )M = (E1 P  )M = (E2 P )M = D,

(E2 P  )M = ,
(P aa0 )M = (P  a0 a)M = t.

We begin the construction of the interpretation I of K by setting W0 = V0 = D0 = {a, a0 }
and P 0 = {(a, a0 )}. Then we compute the required and actual ranks r(R, w) and r0 (R, w),
for R  {P, P  } and w  V0 :
(i) r(P, a) = 2 and r0 (P, a) = 1,
(iii) r(P  , a) = 1 and r0 (P  , a) = 0,

(ii) r(P, a0 ) = 2 and r0 (P, a0 ) = 0,
(iv) r(P  , a0 ) = 1 and r0 (P  , a0 ) = 1.

At the next step, we draw a P -arrow from a to a fresh copy of dp to cure defect (i), draw
two P -arrows from a0 to two more fresh copies of dp in order to cure defects (ii), and finally
we take a fresh copy of dp and connect it to a by a P -arrow, thereby curing defect (iii).
One more step of this unraveling construction is shown in Figure 4.
Observe the following important property of the construction: for m, m0  0, w  Vm0
and R  role (K),


if m < m0 ,
0,
rm (R, w) =
(11)
q,
if m = m0 , for some q  r(R, cp(w)),


r(R, cp(w)), if m > m0 .
25

fiArtale, Calvanese, Kontchakov & Zakharyaschev

.

V0

a0
V1

a

V2

dp

dp

.
Figure 4: Unraveling model M (first three steps).
To prove this property, consider all possible cases:
 If m < m0 then the point w has not been added to Wm yet, i.e., w 
/ Wm , and so we
have rm (R, w) = 0.
 If m = m0 and m0 = 0 then rm (R, w)  r(R, cp(w)) follows from (10).
 If m = m0 and m0 > 0 then w was added at step m0 to cure a defect of some point
w0  Wm0 1 . This means that there is Pk  role(K) such that either (w0 , w)  Pkm0
(m 1)
0 1
and w0  m
or (w, w0 )  Pkm0 and w0  k 0
. Consider the former case. We
k
.
Since
fresh
witnesses
are
picked
up every time the rule (km0 1 )
have cp(w) = dp
k

is applied, rm0 (Pk , w) = 1, rm0 (Pk , w) = 0 and rm0 (R, w) = 0, for every R 6= Pk , Pk .
0
So it suffices to show that r(Pk , dp
k )  1. Indeed, as M |= Eq Pk [cp(w )] for some

R
0
q  QT , we have, by (5), M |= E1 Pk [cp(w )], and so, by (4), M |= E1 Pk [dp
k ]. By the
)

1.
The
latter
case
is
considered
analogously.
definition of r, we have r(Pk , dp
k
 If m = m0 + 1 then, for each role name Pk , all defects of w are cured at step m0 + 1
m0 
0
by applying the rules (m
). Therefore, rm0 +1 (R, w) = r(R, cp(w)).
k ) and (k
 If m > m0 + 1 then (11) follows from the observation that new arrows involving w can
only be added at step m0 + 1, that is, for all m  0 and each role name Pk  role(K),
Pkm+1 \ Pkm



Vm  Vm+1



Vm+1  Vm .

(12)

I
It follows that, for all R  role (K), q  QR
T and w   , we have:

M |= Eq R[cp(w)]

iff

w  ( q R)I .

(13)

Indeed, if M |= Eq R[cp(w)] then, by definition, r(R, cp(w))  q. Let w  Vm0 . Then,
by (11), rm (R, w) = r(R, cp(w))  q, for all m > m0 . It follows from the definition of
26

fiThe DL-Lite Family and Relations

rm (R, w) and RI that w  ( q R)I . Conversely, let w  ( q R)I and w  Vm0 . Then,
by (11), q  rm (R, w) = r(R, cp(w)), for all m > m0 . So, by the definition of r(R, cp(w))
and (5), M |= Eq R[cp(w)].
By induction on the construction of concepts C in K one can readily see that, for every
w  I , we have
M |= C  [cp(w)]
iff
w  CI .
(14)
Indeed, the basis is trivial for B =  and follows from (8) for B = Ak and from (13)
for B =  q R, while the induction step for the Booleans (C = C1 and C = C1 u C2 )
immediately follows from the induction hypothesis.
Finally, we show that for each   T  A,
M |=  

iff

I |= .

The case  = C1 v C2 follows from (14); for  = Ak (ai ) and  = Ak (ai ) from the
definition of AIk . For  = Pk (ai , aj ) and  = Pk (ai , aj ), we have (aIi , aIj )  PkI iff, by (12),
(aIi , aIj )  Pk0 iff, by (9), M |= Pk ai aj .
Thus, we have established that I |= K.
() Conversely, suppose that I |= K is an interpretation with domain I . We construct
I
a model M of K based on the same I . For every ai  ob(A), we let aM
i = ai and, for
every R  role (K), we take some d  ( 1 R)I if ( 1 R)I 6=  and an arbitrary element
I
d  I otherwise, and let drM = d. Next, for every concept name Ak , we let AM
k = Ak

M = ( q R)I . Finally, for every
and, for every role R  role (K) and q  QR
T , we set Eq R

role R  role (K) and every pair of objects ai , aj  ob(A), we define (Rai aj )M to be true
iff I |= R(ai , aj ). One can readily check that M |= K . Details are left to the reader.
q
The first-order translation K of K is obviously too lengthy to provide us with reasonably
low complexity results: |K |  |K| + (2 + qT2 )  |role(K)| + 2  |role(K)|  |ob(A)|qT . However, it
follows from the proof above that a lot of information in this translation is redundant and
can be safely omitted.
Now we define a more concise translation K of K = (T , A) into QL1 by taking:
h
^
i
1
2
K = x T  (x) 
R (x)  R (x)
 A
 A ,
Rrole (K)
1

where T  (x), R (x), R (x) and A are defined as before by means of (1), (4), (5) and (2),
respectively, and
^
^
^
2
A =
EqR,a R(a) 
(Pk (ai , aj )) ,
(15)
aob(A)

Rrole (K)
a0 ob(A) R(a,a0 )A

Pk (ai ,aj )A

where qR,a is the maximum number in QR
T such that there are qR,a many distinct ai with
R(a, ai )  A (here we use the UNA) and (Pk (ai , aj )) =  if Pk (ai , aj )  A and >
2
otherwise. Now both the size of A and the size of K are linear in the size of A and K,
respectively, no matter whether the numbers are coded in unary or in binary.
27

fiArtale, Calvanese, Kontchakov & Zakharyaschev

More importantly, the translation  can actually be done in LogSpace. Indeed, this
1
2
is trivially the case for T  (x), R (x), R (x), A and the last conjunct of A . As for
2
the first conjunct of A then, for R  role (K) and a  ob(A), the maximum qR,a in
QR
T such that there are qR,a many distinct ai with R(a, ai )  A, can be computed using
log min(max QR
T , |ob(A)|) + log |ob(A)| cells. Initially we set q = 0, and then enumerate all
object names ai in A incrementing the current q each time we find R(a, ai )  A. We stop if
q = max QR
T or we reach the end of the object name list. The resulting qR,a is the maximum
number in QR
T not exceeding q.
Example 5.4 The translation K of the KB K from Example 5.1 looks as follows:
K = x (x)  A(a)  E1 P (a)  E1 P  (a0 ),
where (x) is defined by (7).
1

Corollary 5.5 A DL-LiteN
bool KB K is satisfiable iff the QL -sentence K is satisfiable.

Proof The claim follows from the fact that K is satisfiable iff K is satisfiable. Indeed, if
M |= K then clearly M |= K . Conversely, if M |= K then one can construct a new model
M0 based on the same domain D as M by taking:
0

M
 AM
k = Ak , for all concept names Ak ;
0

 Eq RM = Eq RM , for all R  role (K) and q  QR
T;
0

 (Rai aj )M is true iff R(ai , aj )  A;
0

M
 aM
i = ai , for all ai  ob(A);
0

 drM = drM , for all R  role (K).
0

We claim that M0 |= K . Indeed, Eq RM = Eq RM , for every R  role (K) and q  QR
T . It
1
2
follows then that M0 |= x T  (x) and M0 |= x R (x). By definition, M0 |=VA , M0 |= A
q
and M0 |= x R (x). It remains to show that M0 |= R . Suppose M0 |= i=1 Raaji , that
is R(a, aji )  A, for distinct aj1 , . . . , ajq , and q  QR
T . Clearly, we have q  qR,a and
M |= Eq R(a) and thus M0 |= Eq R(a).
q
As an immediate consequence of Corollary 5.5, the facts that the translation  can be
done in LogSpace, that the satisfiability problem for QL1 -formulas is NP-complete and
that DL-Litebool contains all the Booleansand so can encode full propositional logicwe
obtain the following result:
F
Theorem 5.6 Satisfiability of DL-LiteN
bool , DL-Litebool and DL-Litebool knowledge bases is
NP-complete for combined complexity.
1

Observe now that if K is a DL-LiteN
krom KB then K is in the Krom fragment of QL .
F
Theorem 5.7 Satisfiability of DL-LiteN
 , DL-Lite and DL-Lite knowledge bases, where
  {core, krom}, is NLogSpace-complete for combined complexity.

28

fiThe DL-Lite Family and Relations

Proof As the satisfiability problem for Krom formulas with the prefix of the form x (as
in K ) is NLogSpace-complete (see, e.g., Borger et al., 1997, Exercise 8.3.7) and  is a
LogSpace reduction, satisfiability is in NLogSpace for all the logics mentioned in the
theorem. As for the lower bound, it suffices to recall that the NLogSpace-hardness for
satisfiability of propositional Krom formulas is proved by reduction of the directed graph
reachability problem using only core propositional formulas (Borger et al., 1997), and so
satisfiability in all of the above logics is NLogSpace-hard.
q
1

If K is a DL-LiteN
horn KB then K belongs to the universal Horn fragment of QL .
F
Theorem 5.8 Satisfiability of DL-LiteN
horn , DL-Litehorn and DL-Litehorn KBs is P-complete
for combined complexity.

Proof As QL1 contains no function symbols and K is universal, satisfiability of K is
LogSpace-reducible to satisfiability of a set of propositional Horn formulas, namely, the
formulas that are obtained from K by replacing x with each of the constants occurring
in K . It remains to recall that the satisfiability problem for propositional Horn formulas
is P-complete (see, e.g., Papadimitriou, 1994), which gives the required upper bound for
q
DL-LiteN
horn and lower bound for DL-Litehorn .
5.2 DL-LiteHF
core is ExpTime-hard
Unfortunately, the translation  constructed in the previous section cannot be extended
to logics of the form DL-LiteHN
with both number restrictions and role inclusions. In this

section we show that the satisfiability problem for DL-LiteHF
core KBs is ExpTime-hard, which
matches the upper bound for satisfiability of DL-LiteHN
KBs
even under binary coding of
bool
natural numbers (Tobies, 2001).
Note first that, although intersection is not allowed on the left-hand side of DL-LiteHF
core
concept inclusions, in certain cases (when the right-hand side is consistent) we can simulate
it by using role inclusions and functionality constraints. Suppose that a knowledge base K
contains a concept inclusion of the form C1 u C2 v C. Define a new KB K0 by replacing
this axiom in K with the following set of new axioms, where R1 , R2 , R3 , R12 , R23 are fresh
role names:
C1 v R1

C2 v R2 ,

(16)

R1 v R12 ,

R2 v R12 ,

(17)

 2 R12 v ,
R1

v

(18)

R3 ,

(19)

R3 v C,

(20)

R3 v R23 ,



2 R23

R2 v R23 ,

v .

(21)
(22)

Lemma 5.9 (i) If I |= K0 then I |= K, for every interpretation I.
(ii) If I |= K and C I 6=  then there is a model I 0 of K0 which has the same domain as
I and agrees with it on every symbol from K.
29

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Proof (i) Suppose that I |= K0 and x  C1I  C2I . By (16), there is y with (x, y)  R1I ,
I , whence
and so y  (R1 )I , and there is z with (x, z)  R2I . By (17), {(x, y), (x, z)}  R12
y = z in view of (18). By (19), y  (R3 )I and hence there is u with (u, y)  R3I and
I and (x, y)  RI . Finally, it follows
u  (R3 )I . By (20), u  C I . By (21), (u, y)  R23
23
from (22) that u = x, and so x  C I . Thus, I |= K.
(ii) Take some point c  C I and define an extension I 0 of I to the new role names by
setting:
0

 R1I = {(x, x) | x  C1I },
0

 R2I = {(x, x) | x  C2I },
0

 R3I = {(x, x) | x  (C1 u C2 )I }  {(c, x) | x  (C1 u C2 )I },
0

0

I = RI  RI
 R12
1
2

0

and

0

0

0

I = RI  RI .
R23
2
3

It is readily seen that I 0 satisfies all the axioms (16)(22), and so I 0 |= K0 .

q

We are now in a position to prove the following:
Theorem 5.10 Satisfiability of DL-LiteHF
core KBs is ExpTime-hard for combined complexity
(with or without the UNA).
Proof We will prove this theorem in two steps. First we consider the logic DL-LiteHF
horn
and show how to encode the behavior of polynomial-space-bounded alternating Turing machines (ATMs, for short) by means of DL-LiteHF
horn KBs. As APSpace = ExpTime, where
APSpace is the class of problems recognized by polynomial-space-bounded ATMs (see, e.g.,
Kozen, 2006), this will establish ExpTime-hardness of satisfiability for DL-LiteHF
horn . Then,
using Lemma 5.9, we will show how to get rid of conjunctions on the left-hand side of the
concept inclusions involved in this encoding of ATMs and thus establish ExpTime-hardness
of DL-LiteHF
core .
Without loss of generality, we can consider only ATMs M with binary computational
trees. This means that, for every non-halting state q and every symbol a from the tape
alphabet, M has precisely two instructions of the form
(q, a) ;0M (q 0 , a0 , d0 )

and

(q, a) ;1M (q 00 , a00 , d00 ),

(23)

where d0 , d00  {, } and  (resp., ) means move the head right (resp., left) one cell.
We remind the reader that each non-halting state of M is either an and-state or an or-state.
Given such an ATM M, a polynomial function p(n) such that every run of M on every
input of length n does not use more than p(n) tape cells, and an input word ~a = a1 , . . . , an ,
we construct a DL-LiteHF
horn knowledge base KM,~a with the following properties: (i) the size
of KM,~a is polynomial in the size of M, ~a, and (ii) M accepts ~a iff KM,~a is not satisfiable.
Denote by Q the set of states and by  the tape alphabet of M.
To encode the instructions of M, we need the following roles:
 Sq , Sq0 , Sq1 , for each q  Q: informally, x  (Sq )I , for some interpretation I, means
that x represents a configuration of M with the state q, and x  (Sqk )I means that
the next state, according to the transition ;kM , is q, where k  {0, 1};
30

fiThe DL-Lite Family and Relations

 Hi , Hi0 , Hi1 , for each i  p(n): x  (Hi )I means that x represents a configuration of
M where the head scans the ith cell, and x  (Hik )I that, according to the transition
;kM , k  {0, 1}, in the next configuration the head scans the ith cell;
0 , C 1 , for each i  p(n) and each a  : x  (C  )I means that x represents
 Cia , Cia
ia
ia
k )I that, according
a configuration of M where the ith cell contains a, and x  (Cia
to ;kM , k  {0, 1}, in the next configuration the ith cell contains a.

This intended meaning can be encoded using the following concept inclusions: for every
instruction (q, a) ;kM (q 0 , a0 , ) of M and every i < p(n),

k
k
Sq u Hi u Cia
v Hi+1
u Sqk0 u Cia
0,

(24)

and for every instruction (q, a) ;kM (q 0 , a0 , ) of M and every i, 1 < i  p(n),

k
k
Sq u Hi u Cia
v Hi1
u Sqk0 u Cia
0.

(25)

To preserve the symbols on the tape that are not in the active cell, we use the following
concept inclusions, for k  {0, 1}, i, j  p(n) with j 6= i, and a  :

k
Hj u Cia
v Cia
.

(26)

To synchronize our roles, we need two more (functional) roles Tk and a number of role
inclusions to be added to the TBox: for all k  {0, 1}, i  p(n), q  Q, and a  ,
k
Cia
v Cia ,

Hik v Hi ,

Sqk v Sq ,

(27)

k
Cia
v Tk ,

Hik v Tk ,

Sqk v Tk ,

(28)

 2 Tk v .

(29)

It remains to encode the acceptance conditions for M on ~a. This can be done with the help
of the role names Yk , for k  {0, 1}, and the concept name A:
Sq v A,

q an accepting state,

Yk v Tk ,
 2 Tk
Tk u A
Sq u Yk
Sq

(31)

v ,
v

(32)

Yk ,

v A,

u Y0 u Y1 v A,

(30)

(33)
q an or-state,

(34)

q an and-state.

(35)

The TBox T of the DL-LiteHF
horn knowledge base KM,~a we are constructing consists of the
axioms (24)(35) together with the auxiliary axiom
A u D v ,

(36)

where D is a fresh concept name. The ABox A of KM,~a is comprised of the following
assertions, for some object names s and u:
Sq0 (u, s),

q0 the initial state,

H1 (u, s),
Ciai (u, s),

(37)
(38)

i  p(n), ai the ith symbol on the input tape,

(39)
(40)

D(s).
31

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Clearly, KM,~a = (T , A) is a DL-LiteHF
a.
horn KB and its size is polynomial in the size of M, ~
Lemma 5.11 The ATM M accepts ~a iff the KB KM,~a is not satisfiable.
Proof () Suppose that M accepts ~a but I |= KM,~a for some interpretation I. Then we
can reconstruct the full computation tree for M on ~a by induction in the following way.
Let the root of the tree be the point sI . By (37)(39), s represents the initial configuration of M on ~a in accordance with the intended meaning of the roles Sq0 , H1 and Ciai
explained above (it does not matter if, for instance, we also have sI  (H5 )I ).
Assume now that we have already found a point x  I representing some configuration
c = b1 , . . . , bi1 , (q, bi ), bi+1 , . . . , bp(n) ,

(41)

where q is the current non-halting state and the head scans the ith cell containing bi . This
means that we have
x  (Sq )I  (Hi )I

 I
and x  (Cjb
) ,
j

for all j  p(n).

Assume also that M contains two instructions of the form (23) for (q, bi ), that is q is nonhalting. If we have (q, bi ) ;kM (q 0 , b0 , ), for k = 0 or 1, then, by (24) and (26), there are
points ys , yh and yj , for j  p(n), in I such that
(x, ys )  (Sqk0 )I ,

k
(x, yh )  (Hi+1
)I ,

k I
(x, yi )  (Cib
0) ,

k I
) ,
(x, yj )  (Cjb
j

for j 6= i.

0 , C 0 and the C 0 , j 6= i, are all sub-roles of the functional role T ,
By (28)(29), Sq00 , Hi+1
k
jbj
ib0i
and so all the points ys , yh and yj coincide; we denote this point by xk . By (27), we then
have:

(x, xk )  TkI ,

 I
 I
)  (Cib
xk  (Sq0 )I  (Hi+1
0)

 I
) ,
and xk  (Cjb
j

for j 6= i.

Similarly, if we have (q, bi ) ;kM (q 00 , b00 , ), for k = 0 or 1, then, by (25) and (26), there is
a point xk  I such that
(x, xk )  TkI ,

 I
 I
)  (Cib
xk  (Sq00 )I  (Hi1
00 )

 I
) ,
and xk  (Cjb
j

for j 6= i.

Thus, for k = 0, 1, xk is a Tk -successor of x representing the configuration ck of M after it
has executed (q, bi ) ;kM (q 00 , b00 , d) in c; in this case ck is called the k-successor of c.
According to (30), every point in the constructed computation tree for M on ~a representing a configuration with an accepting state is in AI . Suppose now, inductively, that
x represents some configuration c of the form (41), q is an or-state, xk represents the ksuccessor of c and (x, xk )  TkI , for k = 0, 1, and one of the xk , say x0 , is in AI . In
view of (33), we have x0  (Y0 )I . As T0 is functional by (32) and Y0 is a sub-role of
T0 by (31), (x, x0 )  Y0I , and so, by (34), x  AI . The case of x being an and-state is
considered analogously with the help of (35).
Since M accepts ~a, we then conclude that sI  AI , contrary to (36) and (40).
() Conversely, suppose now that M does not accept ~a. Consider the full computation
tree (, <0  <1 ) with nodes labeled with configurations of M in such a way that the root
is labeled with the initial configuration
(q0 , a1 ), a2 , . . . , an , an+1 , . . . , ap(n) ,
32

fiThe DL-Lite Family and Relations

(where the ai , for n + 1  i  p(n), are all blank), and if some node x in the tree is labeled
with a non-halting c of the form (41) and M contains two instructions of the form (23), then
x has one <0 -successor labeled with the 0-successor of c and one <1 -successor labeled with
the 1-successor of c. (It should be emphasized that (, <0  <1 ) is a tree, where different
nodes may be labeled with the same configuration.)
We use this tree to construct an interpretation I = (I , I ) as follows:
 I =   {u}, for some u 
/ ;
 sI is the root of  and uI = u;
 DI = {sI };
k )I , (x, x )  (C k )I , and (x, x )  (C k )I , for j 6= i,
 (x, xk )  (Sqk0 )I , (x, xk )  (Hi+1
k
k
jbj
ib0
iff x is labeled with c of the form (41), (q, bi ) ;kM (q 0 , b0 , ) and x <k xk , for k = 0, 1;
k )I , (x, x )  (C k )I , and (x, x )  (C k )I , for j 6= i,
 (x, xk )  (Sqk0 )I , (x, xk )  (Hi1
k
k
jbj
ib0
k
iff x is labeled with c of the form (41), (q, bi ) ;M (q 0 , b0 , ) and x <k xk , for k = 0, 1;

 (u, sI )  (Sq0 )I , (u, sI )  (H1 )I , (u, sI )  (Ciai )I , i  p(n) and over  the extensions
for the roles Sq , Hi and Cia are defined according to (27);
 TkI = <k , for k = 0, 1;
 Y0I , Y1I and AI are defined inductively:
 Induction basis: if x   is labeled with an accepting configuration, then x  AI .
 Induction step: (i) if x <k xk , for k = 0, 1, and xk  AI , then (x, xk )  YkI ; (ii) if
x is an or-state (respectively, and-state) and (x, xk )  YkI for some (respectively,
all) k  {0, 1}, then x  AI .
It follows from the given definition that I |= KM,~a . Details are left to the reader.

q

The lemma we have just proved establishes that satisfiability of DL-LiteHF
horn KBs is
ExpTime-hard. Our next aim is to show how one can eliminate the conjunctions in the
left-hand side of the TBox axioms (24)(26), (33)(35). We will do this with the help of
Lemma 5.9. Before applying it, we check first that if KM,~a is satisfiable then it is satisfiable
in an interpretation I such that I |= KM,~a and C I 6= , for every C occurring in an
axiom of the form C1 u C2 v C in K. Consider, for instance, axiom (24) and assume that
I |= KM,~a , but (Sqk0 )I = . Then, we can construct a new interpretation I 0 by adding two
0
0
new points, say x and y, to the domain of I, and setting (x, y)  (Sqk0 )I , (x, y)  (Sq0 )I ,
0
0
0
(x, y)  (Tk )I . Furthermore, if q 0 is an accepting state, we also set y  AI and (x, y)  YkI .
One can readily check that I 0 is still a model for KM,~a . The other conjuncts of (24) and
the remaining axioms are considered analogously.
After an application of Lemma 5.9 to an axiom of the form C1 uC2 v C with C2 = C20 uC200
we obtain, by (16)(22), a new KB K0 with the concept inclusion of the form C20 uC200 v R1 ,
which also requires treatment by means of the same lemma. To be able to do this, we again
33

fiArtale, Calvanese, Kontchakov & Zakharyaschev

00

have to check that K0 is satisfiable in some interpretation I 00 with (R1 )I 6= . Suppose
0
that I 0 |= K0 and (R1 )I = . Then we can construct I 00 by adding two new points, say x
0
0
I 0 , RI 0 and RI 0 . It
and y, to the domain of I 0 , adding x to C I and (x, y) to each of R1I , R12
23
3
is readily seen that I 00 |= K0 .
It is to be noted that the proof above does not depend on whether the UNA is adopted
or not.
q
As an immediate consequence we obtain:
Corollary 5.12 Satisfiability of DL-LiteHF
and DL-LiteHN
KBs with or without the UNA


is ExpTime-complete for combined complexity, where   {core, krom, horn, bool}.
5.3 Reconciling Number Restrictions and Role Inclusions
As we have seen in the previous section, the unrestricted interaction between number restrictions and role inclusions allowed in the logics of the form DL-LiteHN
results in high

combined complexity of satisfiability. In Section 6.2, we shall see that the data complexity
of instance checking and query answering also becomes unacceptably high for these logics.
A quick look at the proof of Theorem 5.10 reveals the culprit: the interplay between role
inclusions R1 v R, R2 v R and functionality constraints  2 R v , which effectively mean
that if R1 (x, y) and R2 (x, z) then y = z. In this section we study the case when such an
interplay is not allowed.
(HN )
Recall from Section 2.1 that DL-Lite
TBoxes T , for   {core, krom, horn, bool},
satisfy the following conditions:
(A1 ) T may contain only positive occurrences of qualified number restrictions  q R.C,
where C is a conjunction of concepts allowed on the right-hand side of -concept
inclusions;
(A2 ) if  q R.C occurs in T , then T does not contain negative occurrences of number
restrictions  q 0 R or  q 0 inv(R) with q 0  2;
(A3 ) if R has a proper sub-role in T , then T does not contain negative occurrences of
 q R or  q inv(R) with q  2.
(HN )

DL-Lite
TBoxes can contain role constraints such as Dis(R1 , R2 ), Asym(Pk ), Sym(Pk ),
Irr(Pk ), and Ref(Pk ).
Our main aim in this section is to prove the following theorem and develop the technical
(HN )
tools we need to investigate the data complexity of reasoning with DL-Litebool and its
sublogics later on in the paper.
(HN )

Theorem 5.13 For combined complexity, (i) satisfiability of DL-Litebool KBs is NP(HN )
complete; (ii) satisfiability of DL-Litehorn KBs is P-complete; and (iii) satisfiability of
(HN )
(HN )
DL-Litekrom and DL-Litecore KBs is NLogSpace-complete.

34

fiThe DL-Lite Family and Relations

(HN )

Let us consider first the sub-language of DL-Litebool

without qualified number restric(HN )

tions and the role constraints mentioned above; we denote it by DL-Litebool . This sub-

(HN )

language is required for purely technical reasons. In Section 7, we will also use DL-Litehorn ,
but we do not need the core or Krom fragments.
(HN )
Suppose we are given a DL-Litebool
KB K = (T , A). Let Id be a distinguished
role name. We will use it to simulate the identity relation required for encoding the role
constraints. We assume that either K does not contain Id at all or satisfies the following
conditions:
(Id1 ) Id(ai , aj )  A iff i = j, for all ai , aj  ob(A),

	
Id = {1},
(Id2 ) > v Id, Id v Id  T , and QId
T = QT
(Id3 ) Id is only allowed in role inclusions of the form Id v Id and Id v R.
In what follows, without loss of generality, we will assume that
0

0
R

(Q) QR
T  QT whenever R vT R
0

(for if this is not the case we can always add the missing numbers to QR
T , e.g., by introducing
fictitious concept inclusions of the form  v  q R0 ).
Now, in the same way as in Section 5.1, we define two translations e and e of K into
the one-variable fragment QL1 of first-order logic. The former translation, e , retains the
information about the relationships between ABox objects, and we show that every model
of Ke can again be unraveled into a model of K. We define e by taking:
h
Ke = x T  (x)  T R (x) 

^

R (x)  R (x)

i



Rrole (K)

h

1

2

A  A 

^


R 

Rrole (K)

^

Rai aj  R0 ai aj

i

,

RvR0 T
ai ,aj ob(A)

1

2

where T  (x), A , A , R (x), R (x) and R are as in (1)(6) and
^
^

T R (x) =
Eq R(x)  Eq R0 (x) .
RvR0 T or
inv(R)vinv(R0 )T

(42)

qQR
T

The following lemma is an analogue of Theorem 5.2:
(HN )

Lemma 5.14 A DL-Litebool

KB K is satisfiable iff the QL1 -sentence Ke is satisfiable.

Proof The proof basically follows the lines of the proof of Theorem 5.2 with some modifications. We present a modified unraveling construction here; the converse direction is exactly
the same as in Theorem 5.2.
In each equivalence class [Ri ] = {Rj | Ri T Rj } we select a single role (a representative
of that class) and denote it by repT (Ri ). When extending Pkm to Pkm+1 , we use the following
modified curing rules:
35

fiArtale, Calvanese, Kontchakov & Zakharyaschev



(m
k ) If Pk 6= repT (Pk ) do nothing: the defects are cured for repT (Pk ). Otherwise, let
m
w  k , q = r(Pk , cp(w))  rm (Pk , w) and d = cp(w). We have M |= Eq0 Pk [d] for
some q 0  q > 0. Then, by (5), M |= E1 Pk [d] and, by (4), M |= E1 Pk [dp
k ]. In this
0 ) = dp , for 1  i  q),
case we take q fresh copies w10 , . . . , wq0 of dp
(and
set
cp(w
i
k
k
add them to Wm+1 and

 add the pairs (w, wi0 ), 1  i  q, to each Pjm+1 with Pk vT Pj (including
Pj = Pk );
 add the pairs (wi0 , w), 1  i  q, to each Pjm+1 with Pk vT Pj ;
 if Id occurs in K, add the pairs (wi0 , wi0 ), 1  i  q, to each Pjm+1 with Id vT Pj .

m
(m
k ) This rule is the mirror image of (k ): Pk and dpk are replaced everywhere with

Pk and dpk , respectively; see the proof of Theorem 5.2.

It follows from this definition that Id never has any
in the resulting
 defects and is interpreted
	
interpretation I by the identity relation IdI = (w, w) | w  I ; the interpretations of
roles respect all the role inclusions, i.e., R1I  R2I whenever R1 vT R2 .
It remains to show that the constructed interpretation I is indeed a model of K.
First, (11) trivially holds for Id as both the required and actual ranks are equal to 1. Second, (11) holds for R such that R 6= Id and R has no proper sub-roles: the proof is exactly
the same as in Theorem 5.2, taking into account that we cure defects only for a single role in
each equivalence class and that, by (42), for all R0  [R], we have r(R0 , cp(w)) = r(R, cp(w))
and r(inv(R), cp(w)) = r(inv(R0 ), cp(w)). It follows that (13) holds for Id and any role R
without proper sub-roles. However, (13) does not necessarily hold for roles R with proper
sub-roles: as follows from the construction, the actual rank may be greater than the required
rank, in which case we only have the following:
if M |= Eq R[cp(w)]

then

w  ( q R)I .

However, this is enough for our purposes. By induction on the structure of concepts and
using (A3 ), one can show that I |= C1 v C2 whenever M |= x (C1 (x)  C2 (x)), for each
concept inclusion C1 v C2  T , and therefore, I |= T . We also have I |= A (see the proof
of Theorem 5.2) and thus I |= K.
q
Remark 5.15 It follows from the proofs of Theorem 5.2 and Lemma 5.14 that, for the
(HN )
DL-Litebool
KB K = (T , A), every model M of Ke induces a model IM of K with the
following properties:
(ABox) For all ai , aj  ob(A), we have (aIi M , aIj M )  RIM iff R(ai , aj )  CleT (A), where
CleT (A) =



	
R2 (ai , aj ) | R1 (ai , aj )  A, R1 vT R2 .

(forest) The object names a  ob(A) induce a partitioning of IM into disjoint labeled
trees Ta = (Ta , Ea , `a ) with nodes Ta , edges Ea , root aIM , and a labeling function
`a : Ea  role (K) \ {Id, Id }.
36

fiThe DL-Lite Family and Relations

(copy) There is a function cp : IM  ob(A)  dr(K) such that
 cp(aIM ) = a for a  ob(A), and
 cp(w) = dr if, for some a and w0  Ta , (w0 , w)  Ea and `a (w0 , w) = inv(R).
(iso) For each R  role (K), all labeled subtrees generated by elements w  IM with
cp(w) = dr are isomorphic.
(concept) w  B IM iff M |= B  [cp(w)], for each basic concept B in K and each w  IM .
fi
	

(role) IdIM = (w, w) fi w  IM and, for every other role name Pk ,
PkIM =



	

	
(aIi M , aIj M ) | R(ai , aj )  A, R vT Pk

(w, w) | Id vT Pk

[ 
	
(w, w0 )  Ea | `a (w, w0 ) = R, R vT Pk .
aob(A)

Such a model will be called an untangled model of K (the untangled model of K induced by
M, to be more precise).
The translation e generalizes  and thus suffers from the same exponential blowup. So
we define an optimized translation, e , which is linear in the size of K, by taking:
h
^
i
1
2
 A
 A e ,
R (x)  R (x)
Ke = x T  (x)  T R (x) 
Rrole (K)
1

where T  (x), T R (x), R (x), R (x) and A are defined by (1), (42), (4), (5) and (2),
respectively, and
^
^
^
2
e R(a)
Ae =
EqR,a

(43)
(Pk (ai , aj ))e ,
aob(A)

Rrole (K)
a0 ob(A) R(a,a0 )CleT (A)

Pk (ai ,aj )A

e
e
where qR,a
is the maximum number in QR
T such that there are qR,a many distinct ai with
R(a, ai )  CleT (A) (here we use the UNA) and (Pk (ai , aj ))e =  if Pk (ai , aj )  CleT (A)

and > otherwise; cf. (15). We note again that if QR
T = {1}, for all roles R  role (K), then
the translation does not depend on whether the UNA is adopted or not.
The following corollary is proved similarly to Corollary 5.5:
(HN )

Corollary 5.16 A DL-Litebool

KB K is satisfiable iff the QL1 -sentence Ke is satisfiable.

It should be clear that the translation e can be computed in NLogSpace (for combined
1
complexity). Indeed, this is readily seen for T  (x), T R (x), R (x), R (x), and A . In
2
order to compute Ae , we need to be able to check whether R(ai , aj )  CleT (A): this test
can be performed by a non-deterministic algorithm using logarithmic space in |role (K)|
(it is basically the same as the standard directed graph reachability problem, which is
NLogSpace-complete; see, e.g., Kozen, 2006); it can be done using N  log |role (K)| +
2 log |ob(A)| cells on the work tape, where N is a constant (in fact, N = 3 is enough: one
37

fiArtale, Calvanese, Kontchakov & Zakharyaschev

has to store R, the current role R0 and the path length for the graph reachability subroutine,
which is also bounded by log |role (K)|). Therefore, the translation e can be computed
by an NLogSpace transducer.
(HN )
Now we show how satisfiability of DL-Litebool KBs can be easily reduced to satisfiability
(HN )

(HN )

of DL-Litebool
KBs. First, we assume that DL-Litebool KBs contain no role symmetry
and asymmetry constraints because Asym(Pk ) can be equivalently replaced with Dis(Pk , Pk )
and Sym(Pk ) with Pk v Pk (it should be noted that the introduction of Pk v Pk in the
TBox does not violate (A3 )). The following lemma allows us to get rid of qualified number
restrictions as well as role disjointness, reflexivity and irreflexivity constraints:
(HN )

Lemma 5.17 For every DL-Litebool
KB K = (T , A) such that

(HN )

KB K0 = (T 0 , A0 ), one can construct a DL-Litebool

 every untangled model IM of K is a model of K0 , provided that
there are no R1 (ai , aj ), R2 (ai , aj )  CleT (A) with Dis(R1 , R2 )  T 0 ,
there is no R(ai , ai )  CleT (A) with Irr(R)  T 0 ;

(44)

 every model I 0 of K0 gives rise to a model I of K based on the same domain as I 0 and
such that I agrees with I 0 on all symbols from K0 .
(HN )

(HN )

If K0 is a DL-Litehorn KB then K is a DL-Litehorn

KB.

Proof First, for every pair R, C such that  q R.C occurs in T 0 , we introduce a fresh role
name RC . Then we replace each (positive) occurrence of  q R.C in T 0 with  q RC and
add the following concept and role inclusions to the TBox:

vC
RC

and

RC v R.

We repeat this procedure until all the occurrences of qualified number restrictions are eliminated. Denote by T 00 the resulting TBox. Observe that (A1 ) and (A2 ) ensure that T 00
satisfies (A3 ). We also notice that C occurs only on the right-hand side of those extra
axioms and thus T 00 belongs to the same fragment as T 0 . It should be clear that, since the
 q R.C occur only positively, every model of T 00 is a model of T 0 . Conversely, for every
model I 0 of T 0 , there is a model I 00 of T 00 based on the same domain such that I 00 coincides
I 00 = {(w, u)  RI 0 | u  C I 0 }, for each new role R . So,
with I 0 on all symbols in T 0 and RC
C
without loss of generality we may assume that T 0 = T 00 .
Let
0
0
0
 Tirref
 Tdisj
,
T 0 = T00  Tref
0 , T0
0
where Tref
irref and Tdisj are the sets of role reflexivity, irreflexivity and disjointness con(HN )

straints in T 0 and T00 is the remaining DL-Litebool
TBox. Let

	

	
0
T10 = > v Id, Id v Id
 Id v P | Ref(P )  Tref
,

	
A01 = Id(ai , ai ) | ai  ob(A0 ) .
(HN )

We construct K by modifying the DL-Litebool
KB K0 = (T00  T10 , A0  A01 ) in two steps:
0 , take a fresh role name S and
Step 1. For every reflexivity constraint Ref(P )  Tref
P
38

fiThe DL-Lite Family and Relations

 add a new role inclusion SP v P to the TBox;
 replace every basic concept B in T00 with B SP , which is defined inductively as follows:
 ASP = A, for each concept name A,
 ( q R)SP =  q R, for each role R 
/ {P, P  },
 ( q P )SP =  (q  1) SP and ( q P  )SP =  (q  1) SP , for q  2,
 (P )SP = > and (P  )SP = >;
 replace R(ai , aj )  A0 such that R T 0 P with SP (ai , aj ) whenever i 6= j.
Intuitively, we split the role P into its irreflexive part SP and Id. Note that if P has a
reflexive proper sub-role then, by (A3 ), there are no restrictions on the maximal number
of P -successors and P -predecessors, and therefore on SP if Ref(P )  T 0 . Let (T1 , A) be the
(HN )
resulting DL-Litebool
KB. Clearly, (T1 , A) satisfies (Id1 )(Id3 ). Observe that
CleT1 (A) role(K0 ) = CleT 0 T 0 (A0 ),
0

1

(45)

where role(K0 ) means the restriction to the role names in K0 .
Let IM be an untangled model of (T1 , A). We show that IM |= T00 . Consider a role P
with Ref(P )  T 0 . Notice that SP has no proper sub-roles in T1 and IdIM is disjoint with
SPIM . Thus, SPIM  IdIM  P IM and
(*) (B SP )IM  B IM , for B =  q R with q  2, whenever Ref(P )  T 0 , R  {P, P  } and
P has a proper sub-role in T 0 .
If P has no proper sub-roles in T 0 (i.e., no proper sub-roles in T1 different from SP and Id)
then we have SPIM  IdIM = P IM . So, for all basic concepts B in T00 not covered by (*), we
have B IM = (B SP )IM . It follows from (A3 ) that IM |= T00 .
0
0 }
Step 2. Next we take into account the set D = Tdisj
 {Dis(Pk , Id) | Irr(Pk )  Tirref
of disjointness constraints by modifying the KB (T1 , A) constructed at the previous step.
Observe that R1 v  is a logical consequence of any T {Dis(R1 , R2 )} whenever R1 vT R2 .
Let T = T1  T2 , where T2 is defined by taking
fi

	
T2 = R1 v  fi R1 vT1 R2 and either Dis(R1 , R2 )  D or Dis(R2 , R1 )  D .
By (role), for any untangled model IM of (T , A) and R1 , R2  role (K), IM |= Dis(R1 , R2 )
if there are no R1 (ai , aj ), R2 (ai , aj )  CleT1 (A), which, by (45), means that there are no
R1 (ai , aj ), R2 (ai , aj )  CleT 0 T 0 (A0 ). So, if (44) holds then every untangled model IM of
0

1

0 . As IdIM is the identity relation,
(T , A) is also a model of T1  D and thus, IM |= Tdisj
0
0
0
we have IM |= Tref  Tirref . By (45), IM |= A and as we have shown above, IM |= T00 .
Therefore, IM |= K0 .
Conversely, suppose I 0 is a model of K0 . Let I be an interpretation such that IdI
0
0
0
is the identity relation, SPI = P I \ IdI , for all P with Ref(P )  T 0 , and AI = AI ,
0
0
P I = P I and aI = aI , for all concept, role and object names A, P and a in K0 . Clearly,
I |= (T00  T10 , A0  A01 ). By the definition of the SP , I |= T1 and, since I |= D, we obtain
I |= T2 and thus I |= T . By (45), I |= A, whence I |= K.
q

39

fiArtale, Calvanese, Kontchakov & Zakharyaschev

(HN )

Now, as follows from Lemma 5.17, given a DL-Lite
KB K0 , for   {krom, horn,

(HN )
bool}, we can compute the DL-Litebool
KB K using a LogSpace transducer (which is
essentially required for checking whether R T 0 P ). We immediately obtain Theorem 5.13
from Lemma 5.14 by observing that, for each   {krom, horn, bool}, Ke belongs to the
respective first-order fragment and that condition (44) can be checked in NLogSpace
(HN )
(computing CleT (A) requires directed graph accessibility checks). The result for DL-Litecore
(HN )
follows from the corresponding result for DL-Litekrom .
5.4 Role Transitivity Constraints
(HN )+

We now consider the languages DL-Lite
,   {core, krom, horn, bool}, which extend
(HN )
DL-Lite
with role transitivity constraints of the form Tra(Pk ). We remind the reader
that a role is called simple (see, e.g., Horrocks et al., 2000) if it has no transitive sub-roles
(including itself) and that only simple roles R are allowed in concepts of the form  q R, for
q  2. In particular, if T contains Tra(P ) then P and P  are not simple, and so T cannot
contain occurrences of concepts of the form  q P and  q P  , for q  2.
(HN )+
For a DL-Lite
KB K = (T , A), define the transitive closure TraT (A) of A by
taking

	
TraT (A) = A  P (ai1 , ain ) | ai2 . . . ain1 P (ai1 , aij+1 )  A, 1  j < n, Tra(P )  T .
Clearly, TraT (A) can be computed in NLogSpace: for each pair (ai , aj ) of objects in ob(A),
we add P (ai , aj ) to TraT (A) iff there is a P -path of length < |ob(A)| between ai and aj in
A (recall that the directed graph reachability problem is NLogSpace-complete).
(HN )+

(HN )

Lemma 5.18 A DL-Lite
KB (T , A) is satisfiable iff the DL-Lite
KB (T 0 , A0 ) is
0
satisfiable, where T results from T by removing all the transitivity axioms and
A0 = CleT (TraT (CleT (A))).
Proof Indeed, if the KB (T 0 , A0 ) is satisfiable then we construct a model I for it as described
in the proofs of Lemmas 5.14 and 5.17 and then take the transitive closure of P I for every P
with Tra(P )  T (and update each RI with P vT R). As P and P  are simple, T contains
no axioms imposing upper bounds on the number of P -successors and predecessors, and so
the resulting interpretation must be a model of (T , A). The converse direction is trivial. q
We note that an analogue of Remark 5.15 also holds in this case: just replace CleT (A)
with CleT (TraT (CleT (A))) in (ABox) and take the transitive closure for each transitive subrole in (role).
Remark 5.19 It should be noted that there are two different reasons for the reduction in
Lemma 5.18 to be in NLogSpace rather than in LogSpace (as the reduction  is). First,
in order to compute CleT (A), for each pair of ai , aj , one has to find a path in the directed
graph induced by the role inclusion axioms. Second, in order to compute TraT (CleT (A)), one
has to find a path in the graph induced by the ABox A itself. So, if we are concerned with
the data complexity, CleT (A) can be computed in LogSpace (in fact, in AC0 , as we shall
40

fiThe DL-Lite Family and Relations

see in Section 6.1) because the role inclusion graph (and hence its size) does not depend on
A. The second reason, however, is more dangerous for data complexity as we shall see in
Section 6.1.
As a consequence of Lemma 5.18 and Theorem 5.13 we obtain the following:
(HN )+

Corollary 5.20 For combined complexity, (i) satisfiability of DL-Litebool
complete; (ii)
(HN )+
DL-Litekrom

(HN )+
satisfiability of DL-Litehorn KBs is P-complete;
(HN )+
and DL-Litecore
KBs is NLogSpace-complete.

KBs is NP-

and (iii) satisfiability of

Note again that if the KBs do not contain number restrictions of the form  q R, for
q  2, (as in the extensions of the DL-LiteH
 languages) then the result does not depend on
the UNA.
Remark 5.21 It should be noted that role disjointness, symmetry, asymmetry and transitivity constraints can be added to any of the logics DL-LiteHF
and DL-LiteHN

 , for
  {core, krom, horn, bool}, without changing the combined complexity of their satisfiability problems (which, by Corollary 5.12, are all ExpTime-complete). Indeed, as follows
from Theorem 10 of Glimm et al. (2007), KB satisfiability in the extension of SHIQ
with role conjunction is in ExpTime if the length of role conjunctions is bounded by
some constant (in our case, this constant is 2 because Dis(R1 , R2 ) can be encoded by
(R1 u R2 ).> v ; Asym(R) is dealt with similarly). We conjecture that role reflexivity and irreflexivity constraints do not change complexity either.

6. Instance Checking: Data Complexity
So far we have assumed the whole KB K = (T , A) to be the input for the satisfiability problem. According to the classification suggested by Vardi (1982), we have been considering
its combined complexity. Two other types of complexity for knowledge bases are:
 the schema (or TBox ) complexity, where only the TBox T is regarded to be the input,
while the ABox A is assumed to be fixed; and
 the data (or ABox ) complexity, where only the ABox A is regarded to be the input.
It is easy to see that the schema complexity of the satisfiability problem for all our logics
considered above coincides with the corresponding combined complexity. In this section,
we analyze the data complexity of satisfiability and instance checking.
(HN )

H
6.1 DL-LiteN
bool , DL-Litebool and DL-Litebool

are in AC0

In what follows, without loss of generality we assume that all role and concept names of
a given knowledge base K = (T , A) occur in its TBox and write role(T ), role (T ) and
dr(T ) instead of role(K), role (K) and dr(K), respectively; the set of concept names in T
(HN )
is denoted by con(T ). In this section we reduce satisfiability of DL-Litebool KBs to model
checking in first-order logic. To this end, we fix a signature containing two unary predicates
Ak and Ak , for each concept name Ak , and two binary predicates Pk and Pk , for each role
name Pk .
41

fiArtale, Calvanese, Kontchakov & Zakharyaschev

(HN )

Consider first the case of a DL-Litebool
KB K. We represent the ABox A of K as
a first-order model AA of the above signature. The domain of AA is ob(A) and, for all
ai , aj  ob(A) and all predicates Ak , Ak , Pk and Pk in the signature,
AA |= Ak [ai ]

iff

Ak (ai )  A,

AA |= Pk [ai , aj ]

iff

Pk (ai , aj )  A,

AA |= Ak [ai ]

iff

Ak (ai )  A,

AA |= Pk [ai , aj ]

iff

Pk (ai , aj )  A.

Now we construct a first-order sentence T in the same signature such that (i) T depends
on T but does not depend on A, and (ii) AA |= T iff Ke is satisfiable.
To simplify presentation, we denote by ext(T ) the extension of T with the following
concept inclusions:
0
0
00
  q 0 R v  q R, for all R  role (T ) and q, q 0  QR
T such that q > q and q > q > q
00
R
for no q  QT , and
0
0
  q R v  q R0 , for all q  QR
T and R v R  T or inv(R) v inv(R )  T .
V
Clearly, (ext(T )) (x) is equivalent (in first-order logic) to T  (x)T R (x) Rrole (T ) R (x);
see (1), (5) and (42).
Let Bcon(T ) be the set of basic concepts occurring in T (i.e., concepts of the form A
and  q R, for A  con(T ), R  role (T ) and q  QR
T ). To indicate which basic concepts
hold or do not hold on a domain element of a first-order model of Ke , we use functions
 : Bcon(T )  {>, }, which will be called types. Denote by Tp the set of all such types
(there are 2|Bcon(T )| of them). For a complex concept C, we define (C) by induction:
(C) = (C) and (C1 u C2 ) = (C1 )  (C2 ). The propositional variable-free formula
^

T =
(C1 )  (C2 )
C1 vC2 ext(T )

ensures that the type  is consistent with concept and role inclusions in T . It should be
emphasized that  T is built from  and > using the Boolean connectives and therefore does
not depend on a particular domain element of AA . The following formula is true if a given
1
2
element x of AA is of type  (see A and Ae ; (2) and (43), respectively):
  (x) =

^


(Ak (x)  (Ak ))  (Ak (x)  (Ak ))



Ak con(T )

^

^

Eq RT (x)  ( q R)



^



Rrole (T ) qQR
T


xy PkT (x, y)  Pk (x, y)   ,

Pk role(T )

where Eq RT (x) and RT (x, y), for R  role (T ), are abbreviations defined by
^
^

Eq RT (x) = y1 . . . yq
(yi 6= yj ) 
RT (x, yi ) ,
1i<jq

RT (x, y) =

_
Pk vT

Pk (x, y)



_
Pk vT

R

42

(46)

1iq

Pk (y, x).
R

(47)

fiThe DL-Lite Family and Relations

Clearly, we have R(ai , aj )  CleT (A) iff AA |= RT [ai , aj ] and AA |= Eq RT [a] iff a has at
least q distinct R-successors in CleT (A) (and thus in every model of K).
Without loss of generality we may assume that role (T ) = {R1 , . . . , Rk } 6= . Denote
by Tpk the set of k-tuples ~ containing a type dri  Tp for each role Ri  role (T ). We
then set
_
~
x T (x),
T
=
k
~
Tp

where
(dr1 ,...,drk )

T

(x)

=

_ 

  (x)   T

^



T
dr
i





Tp

Ri role (T )

^



(Ri ) 

Ri role (T )

_



ds (Ri )  inv(dri ) (inv(Ri )) .

Srole (T )

To explain the meaning of the subformulas of T , assume that (T , A) is satisfiable. In order
to construct a model M for Ke from the first-order model AA , we have to specify the basic
concepts that contain a given constant of Ke . In other words, we have to select a type for
each dri  dr(T ) and each a  ob(A). The formula T says that one can select a k-tuple
of types ~ = (dr1 , . . . , drk )  Tpk such that one of its disjuncts is true in AA . Such a
k-tuple fixes the witness part of the model M, consisting of the dri , and determines the
basic concepts these dri belong to. Then each disjunct of T says that (having fixed the
witness part of the model), for every a  ob(A), there is a type  (determining the basic
concepts a belongs to) such that
  is consistent with the information about a in A (cf.   (x));
  is also consistent with the concept and role inclusions of T (cf.  T );
T );
 each of dr1 , . . . , drk is consistent with the concept and role inclusions of T (cf. dr
i

 each role Ri with a nonempty domain (i.e., either  or any of ds is > on Ri ) has
a nonempty range, in particular, inv(dri ) (inv(Ri )) = >; see also R (x) as defined
by (4).
Lemma 6.1 AA |= T iff Ke is satisfiable.
~
Proof () Fix some ~ = (dr1 , . . . , drk )  Tpk such that AA |= x T (x). Then, for each
~

a  ob(A), fix some type such that the respective disjunct of T (x) holds on a in AA and
denote it by a . Define a first-order model M over the domain ob(A)  dr(T ) by taking:
 M |= B  [c] iff c (B) = >, for all c  ob(A)  dr(T ) and B  Bcon(T )
(B  is the unary predicate for B as defined on p. 22). It is easy to check that M |= Ke .
() Suppose now that Ke is satisfiable. Then there is a model M of Ke with domain
ob(A)  dr(T ). To see that AA |= T , it suffices to take the functions dri and a defined
by:
43

fiArtale, Calvanese, Kontchakov & Zakharyaschev

 dri (B) = > iff M |= B  [dri ], for dri  dr(T ) and B  Bcon(T ),
 a (B) = > iff M |= B  [a], for a  ob(A) and B  Bcon(T ).
Details are left to the reader.

q

It follows from Lemmas 6.1 and 5.17 and Corollary 5.16 that we have:
H
Corollary 6.2 The satisfiability and instance checking problems for DL-LiteN
bool , DL-Litebool
(HN )
and DL-Litebool KBs are in AC0 for data complexity.
(HN )

H
Proof DL-LiteN
bool and DL-Litebool are sub-languages of DL-Litebool , and for them the
(HN )
result immediately follows from Lemma 6.1 and Corollary 5.16. For a DL-Litebool KB
(HN )

K0 = (T 0 , A0 ), by Lemma 5.17, we construct a DL-Litebool
KB K = (T , A) such that
0
K is satisfiable iff K is satisfiable and (44) holds. The latter condition corresponds to the
following first-order sentence
^
^


T 0 =
xy R1T (x, y)  R2T (x, y)  

x PkT (x, x)   ,
Dis(R1 ,R2 )T 0

Irr(Pk )T 0

evaluated in AA . Therefore, K0 is satisfiable iff AA |= T  T 0 . Let  = T  T 0 and  0
be the result of replacing each SP (t1 , t2 ), for Ref(P )  T 0 , with P (t1 , t2 )  (t1 6= t2 ); see the
proof of Lemma 5.17. It remains to observe that AA |=  iff AA0 |=  0 .
q
As before, this result does not depend on the UNA for any member of the DL-Lite family
that has no number restrictions of the form  q R, for q  2 (in particular, for DL-LiteH
bool
and its fragments).
We also note that transitive roles cannot be included in our languages for free if we are
concerned with data complexity:
Lemma 6.3 Satisfiability and instance checking of DL-Litecore KBs extended with role transitivity constraints are NLogSpace-hard for data complexity.
Proof Suppose we are given a directed graph. Let P be a role name. Define an ABox
A by taking P (ai , aj )  A iff there is an edge (ai , aj ) in the graph. Then a node an is
reachable from a node a0 iff the DL-Litecore ABox A  {P (a0 , an )} is not satisfiable in
models with transitive P . This encoding immediately gives the claim of the lemma because
the directed graph reachability problem is NLogSpace-complete, NLogSpace is closed
under the complement (see, e.g., Kozen, 2006) and the TBox {Tra(P )} does not depend on
the input.
q
On the other hand, as the reduction of Lemma 5.18 is computable in NLogSpace, we
obtain the following:
(HN )+

Corollary 6.4 Satisfiability and instance checking of DL-Litebool
complete for data complexity.

KBs are NLogSpace-

Proof The upper bound is obtained by applying the NLogSpace reduction of Lemma 5.18
and using Corollary 6.2. The lower bound follows from Lemma 6.3.
q

44

fiThe DL-Lite Family and Relations

6.2 P- and coNP-hardness for Data Complexity
Let us now turn to the data complexity of instance checking for the DL-Lite logics with
arbitrary number restrictions and role inclusions. As follows from the results of Ortiz et al.
(2006) for SHIQ, instance checking (and in fact query answering) for DL-LiteHN
bool is in
coNP for data complexity, while the results of Hustadt et al. (2005) and Eiter et al. (2008)
for Horn-SHIQ imply a polynomial-time upper bound for DL-LiteHF
horn .
Here we show that these upper bounds are optimal in the following sense: on the one
hand, instance checking in DL-LiteHF
core is P-hard for data complexity; on the other hand,
HN
it becomes coNP-hard for both DL-LiteHF
krom and DL-Litecore (that is, if we allow negated
concept names or arbitrary number restrictionsin fact,  2 R is enough). Note that the
results of this section do not depend on whether we adopt the UNA or not.
Theorem 6.5 The instance checking (and query answering) problem for DL-LiteHF
krom KBs
is data-hard for coNP (with or without the UNA).
Proof The proof is by reduction of the unsatisfiability problem for 2+2CNF, which is
known to be coNP-complete (Schaerf, 1993). Given a 2+2CNF formula
 =

n
^

(ak,1  ak,2  ak,3  ak,4 ),

k=1

where each ak,j is one of the propositional variables a1 , . . . , am , we construct a KB (T , A )
whose TBox T does not depend on . We will use the object names f , ck , for 1  k  n,
and ai , for 1  i  m, role names S, Sf and Pj , Pj,t , Pj,f , for 1  j  4, and concept names
A and D.
Define A to be the set of the following assertions, for 1  k  n:
S(f, ck ),

P1 (ck , ak,1 ),

P2 (ck , ak,2 ),

P3 (ck , ak,3 ),

P4 (ck , ak,4 ),

and let T consist of the axioms
 2 Pj v ,
Pj,f v Pj ,

Pj,t v Pj ,

Pj,t v Pj,f ,

Pj,f


Pj,t

v A,

P1,f u P2,f u P3,t u P4,t v

Sf ,

v A,

for1  j  4,

(48)

for 1  j  4,

(49)

for 1  j  4,

(50)

for 1  j  4,

(51)
(52)



 2 S v ,

(53)

Sf v S,

(54)

Sf v D.

(55)

Note that axiom (52) does not belong to DL-LiteHF
krom because of the conjunctions in its
left-hand side. However, it can be eliminated with the help of Lemma 5.9. So let us prove
that (T , A ) |= D(f ) iff  is not satisfiable.
() Suppose that  is not satisfiable and I |= (T , A ). Define an assignment a of the
truth values t and f to propositional variables by taking a(ai ) = t iff aIi  AI . As  is false
45

fiArtale, Calvanese, Kontchakov & Zakharyaschev

under a, there is k, 1  k  n, such that a(ak,1 ) = a(ak,2 ) = f and a(ak,3 ) = a(ak,4 ) = t.
In view of (50), for each j, 1  j  4, we have cIk  (Pj,t )I  (Pj,f )I , and by (49),
cIk  (Pj )I . Therefore, by (48) and (51), cIk  (Pj,t )I if a(ak,j ) = t and cIk  (Pj,f )I if
a(ak,j ) = f, and hence, by (52), cIk  (Sf )I . Then by (53) and (54), we have f I  (Sf )I ,
from which, by (55), f I  DI . It follows that (T , A ) |= D(f ).
() Conversely, suppose that  is satisfiable. Then there is an assignment a such that
a(ak,1 ) = t or a(ak,2 ) = t or a(ak,3 ) = f or a(ak,4 ) = f, for all 1  k  n. Define I by taking

	 
	  	
  I = x i | 1  i  m  yk | 1  k  n  z ,
 aIi = xi , for 1  i  m,
cIk = yk , for 1  k  n,
f I = z,

	 
	  	
 AI = xi | a(ai ) = t  yk | 1  k  n  z ,

	 
	 
	
I = (y , aI ) | 1  k  n, a(a ) = t  (x , x ) | a(a ) = t  (z, z) ,
 Pj,t
i i
i
k k,j
k,j
	 
	

I = (y , aI ) | 1  k  n, a(a ) = f  (x , x ) | a(a ) = f ,
 Pj,f
i i
i
k,j
k k,j
I  P I , for 1  j  4,
 PjI = Pj,t
j,f

	
 SfI = (z, yk ) | a(ak,1  ak,2  ak,3  ak,4 ) = f = ,

	
 S I = (z, yk ) | 1  k  n ,

	
 DI = z | a() = f = .

It is not hard to check that I |= (T , A ) and I 6|= D(f ).

q

Theorem 6.6 The instance checking (and the query answering) problem for DL-LiteHN
core
KBs is data-hard for coNP (with or without the UNA).
Proof The proof is again by reduction of the unsatisfiability problem for 2+2CNF. The
HF
main difference from the previous one is that DL-LiteHN
core , unlike DL-Litekrom , cannot express
covering conditions like (50). It turns out, however, that we can use number restrictions
to represent constraints of this kind. Given a 2+2CNF formula , we take the same ABox
A constructed in the proof of Theorem 6.5. The ( independent) TBox T , describing the
meaning of any such representation of  in terms of A , is also defined in the same way as
in that proof, except that the axiom (50) is now replaced by the following set of axioms:
Tj,1 v Tj ,


2 Tj

v ,

Pj v Tj,1 ,

Tj,1

u


Tj,2

Tj,2 v Tj ,

v

(56)
(57)

Pj v Tj,2 ,


Tj,3
,

 2 Tj v Pj,t

Tj,3 v Tj ,

(58)
(59)

Tj,3 v Pj,f ,

(60)

where Tj , Tj,1 , Tj,2 , Tj,3 are fresh role names, for each j, 1  j  4. Note that axioms (52)
and (59) do not belong to DL-LiteHN
core because of the conjunctions in their left-hand side, but
46

fiThe DL-Lite Family and Relations

we can easily eliminate them using Lemma 5.9. So it remains to prove that (T , A ) |= D(f )
iff  is not satisfiable.
() Suppose that  is not satisfiable and I |= (T , A ). Define an assignment a of the
truth values t and f to propositional variables by taking a(ai ) = t iff aIi  AI . As  is
false under a, there is k, 1  k  n, such that a(ak,1 ) = a(ak,2 ) = f, a(ak,3 ) = a(ak,4 ) = t.
For each j, 1  j  4, we have cIk  (Pj )I ; by (58), cIk  (Tj,1 )I , (Tj,2 )I . So there
I and (cI , v )  T I . If v 6= v then cI  ( 2 T )I
are v1 , v2 such that (cIk , v1 )  Tj,1
1
2
j
j,2
k 2
k
 I
and, by (60), cIk  (Pj,t )I . Otherwise, if v1 = v2 = v, we have v  (Tj,3
) by (59),
and so by (56) and (57), cIk  (Tj,3 )I , from which, by (60), cIk  (Pj,f )I . Therefore,
cIk  (Pj,t )I  (Pj,f )I , and by (49), cIk  (Pj )I . Thus, by (48) and (51), cIk  (Pj,t )I
if a(ak,j ) = t and cIk  (Pj,f )I if a(ak,j ) = f, and hence, by (52), cIk  (Sf )I . Then
by (53) and (54), we have f I  (Sf )I , from which, by (55), f I  DI . It follows that
(T , A ) |= D(f ).
() Conversely, suppose that  is satisfiable. Then there is an assignment a such that
a(ak,1 ) = t or a(ak,2 ) = t or a(ak,3 ) = f or a(ak,4 ) = f, for all 1  k  n. Define I by taking

	 
	 
	  	
 I = xi | 1  i  m  yk | 1  k  n  uk,j,1 , uk,j,2 | 1  j  4, 1  k  n  z ,
 aIi = xi , for 1  i  m,

cIk = yk , for 1  k  n,

f I = z,

 AI = {xi | a(ai ) = t},
	

I = (y , aI ) | 1  k  n, a(a ) = t , for 1  j  4,
 Pj,t
k,j
k k,j
	

I = (y , aI ) | 1  k  n, a(a ) = f , for 1  j  4,
 Pj,f
k,j
k k,j
I  P I , for 1  j  4,
 PjI = Pj,t
j,f

	
I = (y , u
 Tj,1
k k,j,1 ) | 1  k  n , for 1  j  4,

	
I = (y , u
 Tj,2
k k,j,2 ) | 1  k  n, a(ak,j ) = t 

	
(yk , uk,j,1 ) | 1  k  n, a(ak,j ) = f , for 1  j  4,

	
I = (y , u
 Tj,3
i k,j,1 ) | 1  k  n, a(ak,j ) = f , for 1  j  4,
I  TI ,
 TjI = Tj,1
j,2

	
 SfI = (z, yk ) | a(ak,1  ak,2  ak,3  ak,4 ) = f = ,

	
 S I = (z, yk ) | 1  k  n ,

	
 DI = z | a() = f = .

It is not hard to check that I |= (T , A ) and I 6|= D(f ).

q

Our next lower bound would follow from Theorem 6, item 2 in the work of Calvanese
et al. (2006); unfortunately, the proof there is incorrect and cannot be repaired.
Theorem 6.7 The instance checking (and query answering) problem for DL-LiteHF
core KBs
is data-hard for P (with or without the UNA).
47

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Proof The proof is by reduction of the entailment problem for Horn-CNF, which is known
to be P-complete (see, e.g., Borger et al., 1997, Exercise 2.2.4). Given a Horn-CNF formula
 =

n
^

(ak,1  ak,2  ak,3 )



k=1

p
^

al,0 ,

l=1

where each ak,j and each al,0 is one of the propositional variables a1 , . . . , am , we construct a
KB (T , A ) whose TBox T does not depend on . We will need the object names c1 , . . . , cn
and vk,j,i , for 1  k  n, 1  j  3, 1  i  m (for each variable, we take one object name
for each possible occurrence of this variable in each non-unit clause), role names S, St and
Pj , Pj,t , for 1  j  3, and a concept name A.
Define A to be the set containing the assertions:
S(v1,1,i , v1,2,i ), S(v1,2,i , v1,3,i ), S(v1,3,i , v2,1,i ), S(v2,1,i , v2,2,i ), S(v2,2,i , v2,3,i ), . . .
. . . , S(vn,2,i , vn,3,i ), S(vn,3,i , v1,1,i ),
Pj (vk,j,i , ck )

iff

A(v1,1,i )

al,0 = ai ,

iff

ak,j = ai ,

for 1  i  m,

for 1  i  m, 1  k  n, 1  j  3,
for 1  i  m, 1  l  p

(all objects for each variable are organized in an S-cycle and Pj (vk,j,i , ck )  A iff the
variable ai occurs in the kth non-unit clause of  in the jth position). Let T consist of the
following concept and role inclusions:
St v S,

(61)

 2 S v ,

(62)

A v St ,
St

(63)

v A,

(64)

 2 P1 v 
P1,t v P1 ,
A v P1,t ,


P1,t

u

2 P3

 2 P2 v ,

(65)

P2,t v P2 ,

(66)

A v P2,t ,

(67)

v ,

(68)

P3,t v P3 ,

(69)


P2,t

v


P3,t
,

(70)

P3,t v A.

(71)

As before, here we have an axiom, namely (70), that does not belong to DL-LiteHF
core because
of the conjunction in its left-hand side, but again it can be eliminated with the help of
Lemma 5.9. Our aim is to show that (T , A ) |= A(v1,1,i0 ) iff  |= ai0 .
() Suppose that  |= ai0 . Consider an arbitrary model I of (T , A ) and define a to be
the assignment of the truth values t and f to propositional variables such that a(ai ) = t iff
I
I
v1,1,i
 AI , for 1  i  m. By (61)(64), for each i, 1  i  m, we have either vk,j,i
 AI ,
I
I
for all k, j with 1  k  n, 1  j  3, or vk,j,i 
/ A , for all k, j with 1  k  n,
1  j  3. Now, if we have a(ak,1 ) = t and a(ak,2 ) = t, for 1  k  n then, by (65)(67),
 I
 I
 I
I
cIk  (P1,t
) , (P2,t
) . By (70), cIk  (P3,t
) and hence, by (68) and (69), vk,3,i
 (P3,t )I ,
48

fiThe DL-Lite Family and Relations

St , S
S
Pj,t , Pj
Pj

a1  a2  a3
y1

a2  a4  a5
y2

A
A

.

xk,j,i
.

zk,j,i
a1

a2

a3

a4

a5

Figure 5: The model I satisfying (T , A ), for  = (a1  a2  a3 )  (a2  a4  a5 ).
I
I
where ak,3 = ai , which means, by (71), that vk,3,i
 AI , and so v1,1,i
 AI and a(ai ) = t. It
I
follows that a() = t, and hence a(ai0 ) = t, which, by definition, means that v1,1,i
 AI .
0
So we can conclude that (T , A ) |= A(v1,1,i0 ).

() Conversely, suppose that  6|= ai0 . Then there is an assignment a with a() = t
and a(ai0 ) = f. We construct a model I of (T , A ) such that I 6|= A(v1,1,i0 ). Define I by
taking

	 
	
 I = xk,j,i , zk,j,i | 1  k  n, 1  j  3, 1  i  m  yk | 1  k  n ,
 cIk = yk , for 1  k  n,
I
= xk,j,i , for 1  k  n, 1  j  3, 1  i  m,
 vk,j,i

	
 AI = xk,j,i | 1  k  n, 1  j  3, a(ai ) = t ,
[

	
 SI =
Si , where Si = (xk,1,i , xk,2,i ), (xk,2,i , xk,3,i ), (xk,3,i , xk1,1,i ) | 1  k  n
1im

and k  1 = k + 1 if k < n, and k  1 = 1 if k = n,
[
 StI =
Si ,
1im
a(ai )=t


	
 PjI = (xk,j,i , yk ) | 1  k  n, ai = ak,j 	
(xk,j,i , zk,j,i ) | 1  k  n, ai 6= ak,j , for 1  j  2,

	
 P3I = (xk,3,i , yk ) | 1  k  n, ai = ak,3 ,

	
I = (x
 Pj,t
 k,j,i , yk ) | 1  k  n, ai = ak,j , a(a
	 i) = t 
(xk,j,i , zk,j,i ) | 1  k  n, ai 6= ak,j , for 1  j  2,

	
I = (x
 P3,t
k,3,i , yk ) | 1  k  n, ai = ak,3 , a(ai ) = t .
It is routine to check that we indeed have I |= (T , A ) and I 6|= A(v1,1,i0 ). See Figure 5 for
an example.
q

49

fiArtale, Calvanese, Kontchakov & Zakharyaschev

7. Query Answering: Data Complexity
The positive existential query answering problem is known to be data-complete for coNP
in the case of DL-LiteHN
bool : the upper bound follows from the results of Ortiz et al. (2006),
while the lower bound was established for DL-Litekrom by Calvanese et al. (2006), Schaerf
(1993). In the case of DL-LiteHF
horn , query answering is data-complete for P, as follows
from the results of Hustadt et al. (2005) and Eiter et al. (2008) for Horn-SHIQ, while for
0
DL-LiteH
horn it is in AC (Calvanese et al., 2006).
In fact, the coNP upper bound holds for the extension of DL-LiteHN
bool with role disjointness and (a)symmetry constraints (this follows from Glimm et al., 2007, Theorem 10;
cf. Remark 5.21). We conjecture that the same result holds for role (ir)reflexivity constraints.
Our main result in this section is the following:
Theorem 7.1 The positive existential query answering problem for the logics DL-LiteN
horn ,
(HN )
H
0
DL-Litehorn and DL-Litehorn is in AC for data complexity.
(HN )

Proof Suppose that we are given a consistent DL-Litehorn KB K0 = (T 0 , A0 ) (with all its
concept and role names occurring in the TBox T 0 ) and a positive existential query in prenex
(HN )
form q(~x) = ~y (~x, ~y ) in the signature of K0 . Consider the DL-Litehorn KB K = (T , A)
(HN )

provided by Lemma 5.17 (the language DL-Litehorn

is defined in Section 5.3).

Lemma 7.2 For every tuple ~a of object names in K0 , we have K0 |= q(~a) iff I |= q(~a) for
all untangled models I of K.
Proof () Suppose that K0 |= q(~a) and I is an untangled model I of K. By Lemma 5.17
and in view of consistency of K0 , which ensures that (44) holds, we then have I |= K0 and
therefore, I |= q(~a).
() Suppose I 0 |= K0 . By Lemma 5.17, there is a model I of K with the same domain as
0
I that coincides with I 0 on all symbols in K0 . As I |= q(~a), we must then have I 0 |= q(~a),
and so K0 |= q(~a) as required.
q
Next we show that, as Ke is a Horn sentence, it is enough to consider just one special
model I0 of K in the formulation of Lemma 7.2. Let M0 be the minimal Herbrand model of
(the universal Horn sentence) Ke . We remind the reader (for details consult, e.g., Apt, 1990;
Rautenberg, 2006) that M0 can be constructed by taking the intersection of all Herbrand
models for Ke , that is, of all models based on the domain that consists of constant symbols
from Ke i.e.,  = ob(A)  dr(T ); cf. Remark 5.15. We then have the following
M0 |= B  [c]

iff

Ke |= B  (c),

for B  Bcon(T ) and c  .

Let I0 be the untangled model of K induced by M0 . Denote the domain of I0 by I0 .
Property (copy) of Remark 5.15 provides us with a function cp : I0  .
There are two consequences of Lemma 5.14. First, we have
aIi 0  B I0

iff

K |= B(ai ),

for B  Bcon(T ) and ai  ob(A).

50

(72)

fiThe DL-Lite Family and Relations

Second, for every R  role (T ), if RI0 6=  then RI 6= , for all models I of K. Indeed, if
RI0 6=  then M0 |= (R) [dr]. Therefore, (T  {R v }, A) is not satisfiable, and thus
RI 6= , for all I with I |= K. Moreover, if RI0 6=  then
w  B I0

iff

K |= R v B,

for B  Bcon(T ) and w  I0 with cp(w) = dr. (73)

Lemma 7.3 If I0 |= q(~a) then I |= q(~a) for all untangled models I of K.
Proof Suppose I |= K. As q(~a) is a positive existential sentence, it is enough to construct
a homomorphism h : I0  I. We remind the reader that, by (forest), the domain I0 of
I0 is partitioned into disjoint trees Ta , for a  ob(A). Define the depth of a point w  I0
to be the length of the shortest path in the respective tree to its root. Denote by Wm the
set of points of depth  m; in particular, W0 = {aI0 | a  ob(A)}. We construct h as the
union of maps hm , m  0, where each hm is defined on Wm and has the following properties:
hm+1 (w) = hm (w), for all w  Wm , and
(am ) for every w  Wm , if w  B I0 then hm (w)  B I , for each B  Bcon(T );
(bm ) for all u, v  Wm , if (u, v)  RI0 then (hm (u), hm (v))  RI , for each R  role (T ).
For the basis of induction, we set h0 (aIi 0 ) = aIi , for ai  ob(A). Property (a0 ) follows then
from (72) and (b0 ) from (ABox) of Remark 5.15.
For the induction step, suppose that hm has already been defined for Wm , m  0. Set
hm+1 (w) = hm (w) for all w  Wm . Consider an arbitrary v  Wm+1 \ Wm . By (forest),
there is a unique u  Wm such that (u, v)  Ea , for some Ta . Let `a (u, v) = S. Then,
by (copy), cp(v) = inv(ds). By (role), u  (S)I0 and, by (am ), hm (u)  (S)I , which
means that there is w  I with (hm (u), w)  S I . Set hm+1 (v) = w. As cp(v) = inv(ds)
and (inv(S))I0 6= , it follows from (73) that if v  B I0 then w0  B I whenever we have
w0  (inv(S))I . As w  (inv(S))I , we obtain (am+1 ) for v. To show (bm+1 ), we notice
that, by (role), we have (w, v)  RI0 , for some w  Wm+1 , just in two cases: either
w  Wm+1 \ Wm , and then w = v and Id vT R, or w  Wm , and then w = u and S vT R.
In the former case, (hm+1 (v), hm+1 (v))  RI because IdI is the identity relation by (role).
In the latter case, we have (u, v)  S I0 ; hence (hm+1 (u), hm+1 (v))  S I and, as S vT R,
(hm+1 (u), hm+1 (v))  RI .
q
Assume now that, in the query q(~x) = ~y (~x, ~y ), we have ~y = y1 , . . . , yk , and  is a
quantifier-free formula. Our next lemma shows that in this case to check whether I0 |= q(~a)
it suffices to consider only the points of depth  m0 in I0 , for some m0 that does not
depend on |A|.
Lemma 7.4 Let m0 = k + |role (T )|. If I0 |= ~y (~a, ~y ) then there is an assignment a0 in
Wm0 (i.e., a0 (yi )  Wm0 for all i) such that I0 |=a0 (~a, ~y ).
Proof Suppose that I0 |=a (~a, ~y ), for some assignment a in I0 , and that there is yi ,
1  i  k, with a(yi ) 
/ Wm0 . Let Y be the minimal subset of ~y that contains yi and
every y such that either P (y 0 , y) or P (y, y 0 ) is a subformula of , for some y 0  Y and
some role name P . Let yj  Y be such that there is m > |role (T )| with a(yj )  Wm and
51

fiArtale, Calvanese, Kontchakov & Zakharyaschev

a(y) 
/ Wm1 for all y  Y (for convenience, W1 =  as before). Clearly, such an m exists:
a(yi ) 
/ Wm0 , Y has at most k variables and, by (forest), relations P I0 can connect a point
in Wn \ Wn1 only with a point in Wn+1 \ Wn2 , for n  1. Let w = a(yj ) be a point in
Ta . As w  Wm \ Wm1 , we have cp(w) = dr, for some R  role (T ). As there are at most
|role (T )| distinct labels in each labeled tree Ta and in view of (copy), for each point u
of depth > |role (T )|, there is a point u0 of depth  |role (T )| in the same Ta such that
cp(u) = cp(u0 ); by (iso), the trees generated by u and u0 are isomorphic. So, there is an
isomorphism g from the labeled tree generated by w (which contains all a(y), for y  Y )
onto the labeled tree generated by some point of depth  |role (T )| in Ta . Define a new
assignment aY by taking aY (y) = g(a(y)) if y  Y and aY (y) = a(y) otherwise. By (copy),
(concept) and (role) we then have I0 |=aY (~a, ~y ) and aY (y)  Wm0 , for each y  Y . If
aY (yj ) 
/ Wm0 for some j, we repeat the described construction. After at most k iterations
we shall obtain an assignment a0 required by the lemma.
q
To complete the proof of Theorem 7.1, we encode the problem K |= q(~a)? as a model
checking problem for first-order formulas. In precisely the same way as in Section 6.1, we
fix a signature that contains unary predicates A, A, for each concept name A, and binary
predicates P , P , for each role name P , and then represent the ABox A of K as a first-order
model AA with domain ob(A). Now we define a first-order formula T ,q (~x) in the above
signature such that (i) T ,q (~x) depends on T and q but not on A, and (ii) AA |= T ,q (~a)
iff I0 |= q(~a).
We begin by defining formulas B (x), for B  Bcon(T ), that describe the types of the
elements of ob(A) in the model I0 in the following sense (see also (72)):
AA |= B [ai ]

iff aIi 0  B I0 ,

for B  Bcon(T ) and ai  ob(A).

(74)

0 (x),  1 (x), . . . of formulas
These formulas are defined as the fixed-points of sequences B
B
with one free variable, where
(
A(x),
if B = A,
0
B (x) =
T
Eq R (x), if B =  q R,
_

i1
i1
i
0
B
(x) = B
(x) 
B
(x)      B
(x) , for i  1,
1
k
B1 uuBk vBext(T )

and Eq RT (x) is given by (46). (As in Section 6.1, to simplify presentation we use ext(T ) instead of T .) It should be clear that if there is some i such that, for all B  Bcon(T ),
i (x)   i+1 (x) (i.e., every  i (x) is equivalent to  i+1 (x) in first-order logic), then
B
B
B
B
i (x)   j (x) for every B  Bcon(T ) and j  i. So the minimum such i does not
B
B
N (x).
exceed N = |Bcon(T )|, and we set B (x) = B
Next we introduce sentences B,dr , for B  Bcon(T ) and dr  dr(T ), that describe the
types of elements in dr(T ) in the following sense (see also (73)):
AA |= B,dr

iff

w  B I0 , for B  Bcon(T ) and each w  I0 with cp(w) = dr.

(75)

(By (concept), this definition is correct.) These sentences are defined similarly to B (x).
Namely, for each B  Bcon(T ) and each dr  dr(T ), we inductively define a sequence
52

fiThe DL-Lite Family and Relations

0
1
B,dr
, B,dr
, . . . by taking
0
B,dr
= 0B,dr

and

i
B,dr
= iB,dr 

_


i1
i1
B






, for i  1,
,dr
B
,dr
1
k

B1 uuBk vBext(T )

where iB,dr = , for all i  0, whenever B 6= R and
0R,dr = x inv(R) (x)

and

iR,dr =

_

i1
inv(R),ds
,

for i  1.

dsdr(T )
i+1
i
It should be clear that there is i  |role (T )|N such that B,dr
 B,dr
, for all B  Bcon(T )
|role (T )|N

and dr  dr(T ). So we set B,dr = B,dr
.
Now we consider the directed graph GT = (VT , ET ), where VT is the set of all equivalence
classes [R], [R] = {R0 | R T R0 }, such that R is not empty in some model of T , and ET
is the set of all pairs ([Ri ], [Rj ]) such that
(path) T |= inv(Ri ) v  q Rj

and

either inv(Ri ) 6vT Rj or q  2,

and Rj has no proper sub-role satisfying (path). We have ([Ri ], [Rj ])  ET iff, for any
ABox A0 , whenever the minimal untangled model I0 of (T , A0 ) contains a copy w of inv(dri0 ),
for Ri0  [Ri ], then w is connected to a copy of inv(drj0 ), for Rj0  [Rj ], by all relations S
with Rj vT S.
Recall now that we are given a query q(~x) = ~y (~x, ~y ), where  is a quantifier-free
positive formula and ~y = y1 , . . . , yk . Let T ,m0 be the set of all paths in the graph GT of
length  m0 . More precisely,
 	 
	
T ,m0 =   ([R1 ], [R2 ], . . . , [Rn ]) | 1  n  m0 , ([Rj ], [Rj+1 ])  ET , for 1  j < n .
R

For ,  0  T ,m0 and a role R  role (T ), we write    0 if one of the following three
conditions is satisfied: (i)  =  0 and Id vT R, (ii) .[S] =  0 or (iii)  =  0 .[inv(S)], for
some role S with S vT R.
Let kT ,m0 be the set of all k-tuples of the form ~ = (1 , . . . , k ), i  T ,m0 . Intuitively,
when evaluating the query ~y (~x, ~y ) over I0 , each bound, or non-distinguished, variable yi
is mapped to a point w in Wm0 . However, the first-order model AA does not contain the
points from Wm0 \ W0 , and to represent them, we use the following trick. By (forest),
every point w in Wm0 is uniquely determined by the pair (a, ), where aI0 is the root of the
tree Ta containing w, and  is the sequence of labels `a (u, v) on the path from aI0 to w.
It follows from the unraveling procedure and (path) that   T ,m0 . So, in the formula
T ,q we are about to define we assume that the yi range over W0 and represent the first
component of the pairs (a, ), whereas the second component is encoded in the ith member
of ~ (these yi should not be confused with the yi in the original query q, which range over
all of Wm0 ). In order to treat arbitrary terms t occurring in (~x, ~y ) in a uniform way, we
set t~ = , if t = a  ob(A) or t = xi , and t~ = i , if t = yi (the distinguished variables xi
and the object names a are mapped to W0 and do not require the second component of the
pairs).
Given an assignment a0 in Wm0 we denote by split(a0 ) the pair (a, ~ ), where a is an
assignment in AA and ~ = (1 , . . . , k )  kT ,m0 are such that
53

fiArtale, Calvanese, Kontchakov & Zakharyaschev

 for each distinguished variable xi , a(xi ) = a with aI0 = a0 (xi );
 for each bound variable yi , a(yi ) = a and i = ([R1 ], . . . , [Rn ]), n  m0 , with aI0
being the root of the tree containing a0 (yi ) and R1 , . . . , Rn being the sequence of
labels `a (u, v) on the path from aI0 to a0 (yi ).
Not every pair (a, ~ ), however, corresponds to an assignment in Wm0 because some paths
in ~ may not exist in our I0 : GT represents possible paths in all models for the fixed
TBox T and varying ABox. As follows from the unraveling procedure, a point in Wm0 \ W0
corresponds to a  ob(A) and  = ([R], . . . )  T ,m0 iff a does not have enough R-witnesses
0
R
in A, i.e., iff AA |= q
 ) with
R [a]  q R [a], for some q  QT . Thus, for every (a, ~
~ = (1 , . . . , k ), there is an assignment a0 in Wm0 with split(a0 ) = (a, ~ ) iff AA |=a ~ (~y ),
where
^
_

0
q
 (1 ,...,k ) (y1 , . . . , yk ) =
Ri (yi )  q Ri (yi )
R

1ik
i 6=

qQT i

and each Ri , for 1  i  k with i 6= , is such that i = ([Ri ], . . . ).
We define now, for every ~  kT ,m0 , concept name A and role name R,
(
A (t),
if t~ = ,
A~ (t) =
A,inv(ds) , if t~ =  0 .[S], for some  0  T ,m0 ,
 T
~

~


R (t1 , t2 ), if t1 = t2 = ,
R
R~ (t1 , t2 ) =
(t1 = t2 ),
if t~1  t~2 and either t~1 6=  or t~2 6= ,


,
otherwise,
where RT (y1 , y2 ) is given by (47). We claim that, for every assignment a0 in Wm0 and
(a, ) = split(a0 ),
I0 |=a0 A(t)

iff

AA |=a A~ (t),

I0 |=a0 R(t1 , t2 )

iff

AA |=a R~ (t1 , t2 ),

for all concept names A and terms t,
for all roles R and terms t1 , t2 .

(76)
(77)

For A(a), A(xi ) or A(yi ) with i =  the claim follows from (74). For A(yi ) with i =  0 .[S],
by (copy), we have cp(a(yi )) = inv(dr), for some R  [S]; the claim then follows from (75).
For R(yi1 , yi2 ) with i1 = i2 = , the claim follows from (ABox). Let us consider the case
of R(yi1 , yi2 ) with i2 6= : we have a0 (yi2 ) 
/ W0 and thus, by (role), I0 |=a0 R(yi1 , yi2 ) iff
 a0 (yi1 ), a0 (yi2 ) are in the same tree Ta , for a  ob(A), i.e., AA |=a (yi1 = yi2 ),
 and either (a0 (yi1 ), a0 (yi2 ))  Ea and then `a (a0 (yi1 ), a0 (yi2 )) = S for some S vT R,
or (a0 (yi2 ), a0 (yi1 ))  Ea and then `a (a0 (yi2 ), a0 (yi1 )) = S for some inv(S) vT R, or
R

a0 (yi1 ) = a0 (yi2 ) and then Id vT R, i.e., i1  i2 .
Other cases are similar and left to the reader.
Finally, let ~ (~x, ~y ) be the result of attaching the superscript ~ to each atom of  and

_ 
T ,q (~x) = ~y
~ (~x, ~y )  ~ (~y ) .
~
 kT ,m

0

54

fiThe DL-Lite Family and Relations

As follows from (76)(77), for every assignment a0 in Wm0 , we have I0 |=a0 (~x, ~y ) iff
AA |=a ~ (~x, ~y ) for (a, ) = split(a0 ). For the converse direction notice that, if AA |=a ~ (~y )
then there is an assignment a0 in Wm0 with split(a0 ) = (a, ~ ).
Clearly, AA |= T ,q (~a) iff I0 |= q(~a), for every tuple ~a. We also note that, for every
pair of tuples ~a and ~b of object names in ob(A), ~ (~a, ~b) is a positive existential sentence
with inequalities, and so is domain-independent.10 It is also easily seen that, for each ~b,
~ (~b) is domain-independent. It follows from the minimality of I0 that T ,q (~a) is domainindependent, for each tuple ~a of object names in ob(A).

Finally, note that the resulting query contains  |role (T )|k(k+|role (T )|) disjuncts. q

8. DL-Lite without the Unique Name Assumption
In this section, unless otherwise stated, we assume that the interpretations do not respect
the UNA, that is, we may have aIi = aIj for distinct object names ai and aj . The consequence
relation |=noUNA refers to the class of such interpretations.
Description logics without the UNA are usually extended with additional equality and
inequality constraints of the form:
ai  aj

and

ai 6 aj ,

where ai , aj are object names. Their semantics is quite obvious: we have I |= ai  aj iff
aIi = aIj , and I |= ai 6 aj iff aIi 6= aIj . The equality and inequality constraints are supposed
to belong to the ABox part of a knowledge base. It is to be noted, however, that reasoning
with equalities is LogSpace-reducible to reasoning without them:
Lemma 8.1 For every KB K = (T , A), one can construct in LogSpace in the size of
A a KB K0 = (T , A0 ) without equality constraints such that I |= K iff I |= K0 , for every
interpretation I.
Proof Let G = (V, E) be the undirected graph with

	
V = ob(A),
E = (ai , aj ) | ai  aj  A or aj  ai  A
and [ai ] the set of all vertices of G that are reachable from ai . Define A0 by removing all the
equality constraints from A and replacing every ai with aj  [ai ] with minimal j. Note that
this minimal j can be computed in LogSpace: just enumerate the object names aj with
respect to the order of their indexes j and check whether the current aj is reachable from
ai in G. It remains to recall that reachability in undirected graphs is SLogSpace-complete
and that SLogSpace = LogSpace (Reingold, 2008).
q
As we mentioned in Section 5.3, the logics of the form DL-LiteH
 do not feel whether
we adopt the UNA or not. With this observation and Lemmas 5.17, 5.18 and 8.1 at hand,
we obtain the following result as a consequence of Theorem 5.13:
10. A query q(~
x) is said to be domain-independent in case AA |=a q(~
x) iff A |=a q(~
x), for each A such that
the domain of A contains ob(A), the active domain of AA , and AA = AAA and P A = P AA , for all concept
and role names A and P .

55

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Theorem 8.2 With or without the UNA, for combined complexity, (i) satisfiability of
H
DL-LiteH
bool KBs is NP-complete; (ii) satisfiability of DL-Litehorn KBs is P-complete; and
H
(iii) satisfiability of DL-LiteH
krom and DL-Litecore KBs is NLogSpace-complete. These results hold even if the KBs contain role disjointness, (a)symmetry, (ir )reflexivity and transitivity constraints, equalities and inequalities.
On the other hand, from Corollary 6.2 and Lemmas 5.17, 5.18 and 8.1 we can derive
the following:
Theorem 8.3 Without the UNA, satisfiability and instance checking for DL-LiteH
bool KBs
0
0
are in AC for data complexity. These problems are also in AC if the KBs contain role
disjointness, (a)symmetry and (ir )reflexivity constraints and inequalities. However, they
are LogSpace-complete if the KBs may contain equalities, and NLogSpace-complete if
role transitivity constraints are allowed.
We also note that our complexity results (Corollary 5.12, Theorems 6.5, 6.6 and 6.7) for
the logics DL-LiteHF
and DL-LiteHN
do not depend on the UNA.


In this section, we analyze the combined and data complexity of reasoning in the logics
(HF )
(HN )
of the form DL-Lite
and DL-Lite
(as well as their fragments) without the UNA.
The obtained and known results are summarized in Table 2 on page 17.
(HN )

8.1 DL-Lite

: Arbitrary Number Restrictions

The following theorem shows that the interaction between number restrictions and the
possibility of identifying objects in the ABox results in a higher complexity.
Theorem 8.4 Without the UNA, satisfiability of DL-LiteN
core KBs (even without equality
and inequality constraints) is NP-hard for both combined and data complexity.
Proof The proof is by reduction of the following variant of the 3SAT problemcalled monotone one-in-three 3SAT which is known to be NP-complete (Garey & Johnson, 1979):
given a positive 3CNF formula
 =

n
^


ak,1  ak,2  ak,3 ,

k=1

where each ak,j is one of the propositional variables a1 , . . . , am , decide whether there is an
assignment for the variables aj such that exactly one variable is true in each of the clauses
k
in . To encode this problem in the language of DL-LiteN
core , we need object names ai , for
1  k  n, 1  i  m, and ck and tk , for 1  k  n, role names S and P , and concept
names A1 , A2 , A3 . Let A be the ABox containing the following assertions:
S(a1i , a2i ), . . . , S(ain1 , ani ), S(ani , a1i ),

for 1  i  m,

S(t1 , t2 ), . . . , S(tn1 , tn ), S(tn , t1 ),
P (ck , tk ),

for 1  k  n,

P (ck , akk,j ), Aj (akk,j ),

for 1  k  n, 1  j  3,

56

fiThe DL-Lite Family and Relations

and let T be the TBox with the following axioms:
A1 v A2 ,

A2 v A3 ,

A3 v A1 ,

 2 S v ,

 4 P v .

Clearly, (T , A ) is a DL-LiteN
core KB and T does not depend on  (so that we cover both
combined and data complexity). We claim that the answer to the monotone one-in-three
3SAT problem is positive iff (T , A ) is satisfiable without the UNA.
() Suppose I |= (T , A ). Define an assignment a of the truth values f and t to
propositional variables by taking a(ai ) = t iff (a1i )I = (t1 )I . Our aim is to show that
a(ak,j ) = t for exactly one j  {1, 2, 3}, for each k, 1  k  n. For all j  {1, 2, 3},
we have (cIk , (akk,j )I )  P I . Moreover, (akk,i )I 6= (akk,j )I for i 6= j. As cIk  ( 3 P )I and
(cIk , (tk )I )  P I , we then must have (akk,j )I = (tk )I for some unique j  {1, 2, 3}. It follows
from functionality of S that, for each 1  k  n, we have (a1k,j )I = (t1 )I for exactly one
j  {1, 2, 3}.
() Let a be an assignment satisfying the monotone one-in-three 3SAT problem. Take
some ai0 with a(ai0 ) = t (clearly, such an i0 exists, for otherwise a() = f) and construct
an interpretation I = (I , I ) by taking:

	 
	
 I = yk , z k | 1  k  n  xki | a(ai ) = f, 1  i  m, 1  k  n ,
 cIk = yk and (tk )I = z k , for 1  k  n,
(
xki , if a(ai ) = f,
 (aki )I =
for 1  i  m, 1  k  n,
z k , if a(ai ) = t,

	
 S I = ((a1i )I , (a2i )I ), . . . , ((ain1 )I , (ani )I ), ((ani )I , (a1i )I ) | 1  i  m ,
	

 P I = (cIk , (tk )I ), (cIk , (akk,1 )I ), (cIk , (akk,2 )I ), (cIk , (akk,3 )I ) | 1  k  n .
It is readily checked that I |= (T , A ).

q

In fact, the above lower bound is optimal:
(HN )

(HN )+

Theorem 8.5 Without the UNA, satisfiability of DL-LiteN
and DL-Lite
 , DL-Lite
KBs with equality and inequality constraints is NP-complete for both combined and data
complexity and any   {core, krom, horn, bool}.
Proof The lower bound is immediate from Theorem 8.4, and the matching upper bound
(HN )+
can be proved by the following non-deterministic algorithm. Given a DL-Litebool
KB
K = (T , A), we
 guess an equivalence relation  over ob(A);

 select in each equivalence class ai / a representative, say ai , and replace every occurrence of a  ai / in A with ai ;
 fail if the equalities and inequalities are violated in the resulting ABoxi.e., if it
contains ai 6 ai or ai  aj , for i 6= j;
57

fiArtale, Calvanese, Kontchakov & Zakharyaschev

 otherwise, remove the equality and inequality constraints from the ABox and denote
the result by A0 ;
(HN )+

 use the NP satisfiability checking algorithm for DL-Litebool
KB K0 = (T , A0 ) is consistent under the UNA.

to decide whether the

Clearly, if the algorithm returns yes, then I 0 |= K0 , for some I 0 respecting the UNA, and
we can construct a model I of K (not necessarily respecting the UNA) by extending I 0 with
0
the following interpretation of object names: aI = aIi , whenever ai is the representative
of a/ (I coincides with I 0 on all other symbols). Conversely, if I |= K then we take
the equivalence relation  defined by ai  aj iff aIi = aIj . Let I 0 be constructed from
I by removing the interpretations of all object names that are not representatives of the
equivalence classes for . It follows that I 0 respects the UNA and I 0 |= K0 , so the algorithm
returns yes.
q
(HF )

8.2 DL-Lite

: Functionality Constraints
(HF )+

Let us consider now DL-Litebool
and its fragments. The following lemma shows that for
these logics reasoning without the UNA can be reduced in polynomial time in the size of
the ABox to reasoning under the UNA.
(HF )+

Lemma 8.6 For every DL-Litebool

KB K = (T , A) with equality and inequality con(HF )+

straints, one can construct in polynomial time in |A| a DL-Litebool
KB K0 = (T , A0 ) such
that A0 contains no equalities and inequalities and K is satisfiable without the UNA iff K0
is satisfiable under the UNA.
Proof In what follows by identifying aj with ak in A we mean replacing each occurrence
of ak in A with aj . We construct A0 by first identifying aj with ak , for each aj  ak  A,
and removing the equality from A, and then exhaustively applying the following procedure
to A:
 if  2 R v   T and R(ai , aj ), R(ai , ak )  CleT (A), for distinct aj and ak , then
identify aj with ak (recall that a functional R cannot have transitive sub-roles and
thus CleT (A) is enough).
If the resulting ABox contains ai 6 ai , for some ai , then, clearly, K is not satisfiable, so
we add A(ai ) and A(ai ) to the ABox, for some concept name A. Finally, we remove
all inequalities from the ABox and denote the result by A0 . It should be clear that A0 is
computed from A in polynomial time and that, without the UNA, K is satisfiable iff K0 is
satisfiable. So it suffices to show that K0 is satisfiable without the UNA iff it is satisfiable
under the UNA. The implication () is trivial.
() Observe that the above procedure ensures that
e
qR,a
 1,

for each R with  2 S v   T , R vT S and a  ob(A0 )
(HN )

(see page 37 for definitions). Let K00 be the DL-Litebool
KB provided by Lemma 5.17 for
K0 . It follows from the above property and the proofs of Lemma 5.14 and Corollary 5.16
58

fiThe DL-Lite Family and Relations

that if K00 is satisfiable without the UNA then (K00 )e is satisfied in a first-order model
with some constants interpreted by the same domain element. As (K00 )e is a universal
first-order sentence containing no equality, it is satisfiable in a first-order model such that
all constants are interpreted by distinct elements. It follows from the proofs of Lemma 5.14
and Corollary 5.16 that this first-order model can be unraveled into a model J for K00
respecting the UNA. By Lemma 5.17, J is a model of K0 .
q
The reduction above cannot be done better than in P, as shown by the next theorem:
Theorem 8.7 Without the UNA, satisfiability of DL-LiteF
core KBs (even without equality
and inequality constraints) is P-hard for both combined and data complexity.
Proof The proof is by reduction of the entailment problem for Horn-CNF (as in the proof
of Theorem 6.7). Let
 =

n
^

ak,1  ak,2  ak,3



p
^



al,0

l=1

k=1

be a Horn-CNF formula, where each ak,j and each al,0 is one of the propositional variables
a1 , . . . , am and ak,1 , ak,2 , ak,3 are all distinct, for each k, 1  k  n. To encode the Pk
complete problem  |= ai ? in the language of DL-LiteF
core we need object names t, ai , for
1  k  n, 1  i  m, and fk and gk , for 1  k  n, and role names P , Q, S and T . The
ABox A contains the following assertions
S(a1i , a2i ), . . . , S(an1
, ani ), S(ani , a1i ),
i

for 1  i  m,

P (akk,1 , fk ), P (akk,2 , gk ), Q(gk , akk,3 ), Q(fk , akk,1 ),
T (t, a1l,0 ),

for 1  k  n,

for 1  l  p,

and the TBox T asserts that all of the roles are functional:
 2 P v ,

 2 Q v ,

 2S v 

and

 2 T v .

Clearly, K = (T , A) is a DL-LiteF
core KB and T does not depend on . We claim that
 |= aj iff (T , A  {T (t, a1j )}) is not satisfiable without the UNA. To show this, it suffices
to prove that  |= aj iff K |=noUNA T (t, a1j ).
() Suppose  |= aj . Then we can derive aj from  using the following inference rules:
  |= al,0 for each l, 1  l  p;
 if  |= ak,1 and  |= ak,2 , for some k, 1  k  n, then  |= ak,3 .
We show that K |=noUNA T (t, a1j ) by induction on the length of the derivation of aj from .
The basis of induction is trivial. So assume that aj = ak,3 ,  |= ak,1 ,  |= ak,2 , for some k,
1  k  n, and that K |=noUNA T (t, a1k,1 )  T (t, a1k,2 ). Suppose also that I |= K. Since T
0
0
is functional, we have (a1k,1 )I = (a1k,2 )I . Since S is functional, (akk,1 )I = (akk,2 )I , for all k 0 ,
1  k 0  n, and in particular, for k 0 = k. Then, since P is functional, fkI = gkI , from which,
0
0
by functionality of Q, (akk,3 )I = (akk,1 )I . Finally, since S is functional, (akk,3 )I = (akk,1 )I ,
59

fiArtale, Calvanese, Kontchakov & Zakharyaschev

for all k 0 , 1  k 0  n, and in particular, for k 0 = 1. Thus, I |= T (t, a1j ) and therefore
K |=noUNA T (t, a1j ).
() Suppose that  6|= aj . Then there is an assignment a such that a() = t and
a(aj ) = f. Construct an interpretation I by taking

	 
	  	
 I = xki | a(ai ) = f, 1  k  n, 1  i  m  z k , uk , vk | 1  k  n  w ,
(
xki , if a(ai ) = f,
 (aki )I =
for 1  k  n and 1  i  m,
z k , if a(ai ) = t,

	
 tI = w, T I = (w, z 1 ) ,

	
 S I = ((a1i )I , (a2i )I ), . . . , ((an1
)I , (ani )I ), ((ani )I , (a1i )I ) | 1  i  m ,
i
(
vk , if a(ak,2 ) = f,
 fkI = uk and gkI =
for 1  k  n,
uk , if a(ak,2 ) = t,
	

 P I = ((akk,1 )I , fkI ), ((akk,2 )I , gkI ) | 1  k  n ,
	

 QI = (gkI , (akk,3 )I ), (fkI , (akk,1 )I ) | 1  k  n .
It is readily checked that I |= K and I 6|= T (t, a1j ), and so K 6|=noUNA T (t, a1j ).

q

The above result strengthens the NLogSpace lower bound for instance checking in
DL-LiteF
core proved by Calvanese et al. (2008).
(HF )

(HF )+

Corollary 8.8 Without the UNA, satisfiability of DL-LiteF
and DL-Lite
 , DL-Lite
KBs,   {core, krom, horn}, with equalities and inequalities is P-complete for both combined and data complexity.
(HF )
(HF )+
Without the UNA, satisfiability of DL-LiteF
KBs with
bool , DL-Litebool and DL-Litebool
equalities and inequalities is NP-complete for combined complexity and P-complete for data
complexity.

Proof The upper bounds follow from Lemma 8.6 and the corresponding upper bounds for
the UNA case. The NP lower bound for combined complexity is obvious and the polynomial
lower bounds follow from Theorem 8.7.
q
8.3 Query Answering: Data Complexity
The P and coNP upper bounds for query answering without the UNA follow from the results for Horn-SHIQ (Hustadt et al., 2005; Eiter et al., 2008) and SHIQ (Ortiz et al., 2006,
2008; Glimm et al., 2007), respectively (see the discussion at the beginning of Section 7).
We present here the following result:
Theorem 8.9 Without the UNA, positive existential query answering for DL-LiteH
horn KBs
with role disjointness, (a)symmetry, (ir )reflexivity constraints and inequalities is in AC0
for data complexity. This problem is LogSpace-complete if, additionally, equalities are
allowed in the KBs.
60

fiThe DL-Lite Family and Relations

Proof The proof follows the lines of the proof of Theorem 7.1 and uses the observation
that models without the UNA give no more answers than their untangled counterparts.
More precisely, let KB K0 = (T 0 , A0 ) be as above. Suppose that it is consistent. Let q(~x)
be a positive existential query in the signature of K0 . Given K0 , Lemma 5.17 provides
us with a KB K. It is easy to see that K is a DL-LiteH
horn KB extended with inequality
constraints. The following is an analogue of Lemma 7.2, which also allows us to get rid of
those inequalities:
Lemma 8.10 For every tuple ~a of object names in K0 , we have K0 |=noUNA q(~a) iff I |= q(~a)
for all untangled models I of K (respecting the UNA).
Proof () Suppose that K0 |=noUNA q(~a) and I is an untangled model of K. As I respects
the UNA, by Lemma 5.17 and in view of satisfiability of K0 , which ensures that (44) holds,
we then have I |= K0 and therefore, I |= q(~a).
() Suppose I 0 |= K0 . We construct an interpretation J 0 respecting the UNA as follows.
0
0
0
0
Let J be the disjoint union of I and ob(A). Define a function h : J  I by taking
0
0
h(a) = aI , for each a  ob(A), and h(w) = w, for each w  I , and let


0
0
0	
0
0	
aJ = a,
AJ = u | h(u)  AI
and
P J = (u, v) | (h(x), h(v))  P I ,
for each object, concept and role name a, A, P . Clearly, J 0 respects the UNA and J 0 |= K0 .
It also follows that h is a homomorphism.
By Lemma 5.17, there is a model I of K with the same domain as J 0 that coincides
with J 0 on all symbols in K0 . As I |= q(~a), we must then have J 0 |= q(~a), and since h is a
homomorphism, I 0 |= q(~a). Therefore, K0 |=noUNA q(~a) as required.
q
The remaining part of the proof is exactly as in Theorem 7.1 (since now we may assume
that K is a DL-LiteH
horn KB containing no inequality constraints).
LogSpace-completeness for the case with equalities follows from Lemma 8.1.
q

9. Conclusion
In this article, we investigated the boundaries of the extended DL-Lite family of description
logics by providing a thorough and comprehensive understanding of the interaction between
various DL-Lite constructs and their impact on the computational complexity of reasoning.
We studied 40 different logics, classified according to five mutually orthogonal features:
(1) the presence or absence of role inclusion assertions, (2) the form of the allowed concept
inclusion assertions, distinguishing four main logical groups called core, Krom, Horn, and
Bool, (3) the form of the allowed numeric constraints, ranging from none, to global functionality constraints only, and to arbitrary number restrictions, (4) the presence or absence
of the unique name assumption (and equalities and inequalities between object names, if
this assumption is dropped), and (5) the presence or absence of standard role constraints
such as role disjointness, role symmetry, asymmetry, reflexivity, irreflexivity and transitivity. For all of the resulting logics, we studied the combined and data complexity of KB
satisfiability and instance checking, as well as the data complexity of answering positive
existential queries.
61

fiArtale, Calvanese, Kontchakov & Zakharyaschev

query answering
= instance checking

coNP
query answering

.
Legend
satisfiability
combined complexity

with/without UNA
role inclusions

F

N

ExpTime
NP
P
NLogSpace

no UNA
no role inclusions

F

UNA
no role inclusions

coNP
P
in AC0

oo
l

N

instance checking
data complexity

B

ro
m
K

co

H

or
n

.

re

F

N

Figure 6: Complexity of basic DL-Lite logics.
The obtained tight complexity results are illustrated in Figure 6, where the combined
complexity of satisfiability is represented by the height of vertical dashed lines, while the
data complexity of instance checking by the size and color of the circle on top of these lines
(recall that satisfiability and instance checking are reducible to the complement of each
other). The data complexity of query answering for the core and Horn logics, shown on
the left-hand side of the separating vertical plane, coincides with the data complexity of
instance checking; for the Krom and Bool logics, shown on the right-hand side of the plane,
query answering is always data-complete for coNP. The upper layer shows the complexity
of logics with role inclusions, in which case it does not depend on whether we adopt the
UNA or not. The middle and the lower layers deal with the logics without role inclusions
when the UNA is dropped or adopted, respectively. In each of these layers, the twelve
languages are arranged in the 4  3 grid: one axis shows the type of concepts inclusions
allowed (Horn, core, Krom, Bool), while the other the type of number restrictions (none,
global functionality F or arbitrary N ). Some observations are in order:
 Under the UNA but without role inclusions, number restrictions do not increase the
complexity of reasoning, which depends only on the form of concept inclusions allowed.
 On the other hand, without any form of number restrictions, the logics can have role
inclusions and are insensitive to the UNA; again, the complexity is determined by the
shape of concept inclusions only.
 In either of the above cases, instance checking is in AC0 for data complexity, which
means that the problems are first-order rewritable.

62

fiThe DL-Lite Family and Relations

 Without UNA adopted and without either disjunctions or role inclusions, functionality
leads to P-completeness of instance checking for data complexity, which suggests its
reducibility to Datalog.
 For data complexity, there is no difference between the core and Horn logics, and
between the Krom and Bool ones, which means that the core and Krom logics can be
extended with conjunctions on the left-hand side of concept inclusions for free.
(HF )

(HN )

Finally, for the logics DL-Lite
and DL-Lite
with both (qualified) number restrictions and role inclusions, whose interaction is restricted by conditions (A1 )(A3 ), the
complexity of reasoning always coincides with the complexity of the fragments DL-LiteF

and, respectively, DL-LiteN
 without role inclusions, no matter whether we adopt the UNA
or not.
Role disjointness, symmetry and asymmetry constraints can be added to any of the
(HN )
(HF )
languages without changing their complexity. In fact, the DL-Lite
and DL-Lite
logics contain all of the above types of constraints together with role reflexivity and irreflexivity. We conjecture that (ir)reflexivity constraints can be added to all other logics without
affecting their complexity. However, if we extend any DL-Lite logic with role transitivity
constraints, then the combined complexity of satisfiability remains the same, while instance
checking and query answering become data-hard for NLogSpace. And the addition of
equality between object nameswhich only makes sense if the UNA is droppedleads to
an increase from membership in AC0 to LogSpace-completeness for data complexity; all
other results remain unchanged.
The list of DL constructs considered in this paper is far from being complete. For
example, it would be of interest to analyze the impact of nominals, role chains and Boolean
operators on roles on the computational behavior of the DL-Lite logics. Another interesting
and practically important problem is to investigate in depth the interaction between various
constructs with the aim of pushing restrictions like (A1 )(A3 ) as far as possible.
One of the main ideas behind the DL-Lite logics was to provide efficient access to large
amounts of data through a high-level conceptual interface. This is supposed to be achieved
by representing the high-level view of the information managed by the system as a DL-Lite
TBox T , the data stored in a relational database as an ABox A, and then rewriting positive existential queries to the knowledge base (T , A) as standard first-order queries to the
database represented by A. Such an approach is believed to be viable because, for a number
of DL-Lite logics, the query answering problem is in AC0 for data complexity; cf. Theorems 7.1, 8.9 and Figure 6. The first-order rewriting technique has been implemented in
various system, notably in QuOnto (Acciarri et al., 2005; Poggi et al., 2008b), which can
query, relying on ontology-to-relational mappings, data stored in any standard relational
database management system, and in Owlgres (Stocker & Smith, 2008), which can access
an ABox stored in a Postgres database (though, to the best of our knowledge, the latter
implementation is incomplete for conjunctive query answering). It is to be noted, however,
that the size of the rewritten query can be substantially larger than the size of the original
query, which can cause problems even for a very efficient database query engine.
For a positive existential query q and TBox T , there are two major sources of high
complexity of the first-order formula T ,q in the proof of Theorem 7.1: (i) the formulas
B (x) computing whether an ABox object is an instance of a concept B (and the formulas
63

fiArtale, Calvanese, Kontchakov & Zakharyaschev

R,dr computing whether objects with outgoing R-arrows are instances of B), and (ii) the
(HN )
disjunction over the paths ~ in the graph GT . In the case of DL-Litecore , the size of
(HN )
B (x) is linear in |T |, while for DL-Litehorn it can become exponential (however, various
optimizations are possible). The size of the disjunction in (ii) is exponential in the number
of non-distinguished variables in q. One way of removing source (i) would be to extend
the given database (ABox) A by precomputing the Horn closure of the ABox with respect
to the TBox and storing the resulting data in a supplementary database. This approach
is advocated by Lutz et al. (2008) for querying databases via the description logic EL. It
could also be promising for the Horn fragments of expressive description logics such as
SHIQ (Hustadt et al., 2005; Hustadt, Motik, & Sattler, 2007)containing DL-LiteHF
horn as
a sub-languagefor which the data complexity of instance checking (Hustadt et al., 2005,
2007) and conjunctive query answering is polynomial (Eiter et al., 2008). The disadvantage
of using a supplementary database is the necessity to update it every time the ABox is
changed. It would be interesting to investigate this alternative approach for DL-Lite logics
and compare it with the approach described above. Another important problem is to
characterize those queries for which the disjunction in (ii) can be represented by a formula
of polynomial size.
As the unique name assumption is replaced in OWL by the constructs sameAs and
differentFrom (i.e.,  and 6), a challenging problem is to investigate possible ways of
dealing with equality (inequality does not require any special treatment as shown in the
proof of Lemma 8.10). Although reasoning with equality is LogSpace-reducible to reasoning without it (cf. Lemma 8.1), we lose the property of first-order rewritability, and
computing equivalence classes under  may be too costly for real-world applications.
DL-Lite logics are among those few examples of DLs for which usually very complex
non-standard reasoning problemssuch as checking whether one ontology is a conservative
extension of another one with respect to a given signature  (Kontchakov et al., 2008),
computing minimal modules of ontologies with respect to  (Kontchakov et al., 2009) or
uniform interpolants (Wang, Wang, Topor, & Pan, 2008)can be supported by practical
reasoning tools. However, only first steps have been made in this direction, and more
research is needed in order to include these reasoning problems and tools into the standard
OWL toolkit. It would be also interesting to investigate the unification problem for DL-Lite
logics (Baader & Narendran, 2001).
Finally, there exist certain parallels between the Horn logics of the DL-Lite family, EL,
Horn-SHIQ and the first-order language of tuple and equality generating dependencies,
TGDs and EGDs, used in the theory of databases (see, e.g., Gottlob & Nash, 2008). Further
investigations of the relationships between these logics may lead to a deeper understanding
of the role description logics can play in the database framework.
Acknowledgments
This research has been partially supported by FET project TONES (Thinking ONtologiES), funded within the EU 6th Framework Programme under contract FP6-7603, and
by the large-scale integrating project (IP) OntoRule (ONTOlogies meet Business RULEs
ONtologiES), funded by the EC under ICT Call 3 FP7-ICT-2008-3, contract number FP7231875. We thank the referees for their constructive criticism, comments, and suggestions.

64

fiThe DL-Lite Family and Relations

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison-Wesley.
Acciarri, A., Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Palmieri, M., &
Rosati, R. (2005). QuOnto: Querying ontologies. In Proc. of the 20th Nat. Conf.
on Artificial Intelligence (AAAI 2005), pp. 16701671.
Apt, K. (1990). Logic programming. In van Leeuwen, J. (Ed.), Handbook of Theoretical
Computer Science, Volume B: Formal Models and Sematics, pp. 493574. Elsevier
and MIT Press.
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2007a). DL-Lite in the
light of first-order logic. In Proc. of the 22nd Nat. Conf. on Artificial Intelligence
(AAAI 2007), pp. 361366.
Artale, A., Calvanese, D., Kontchakov, R., Ryzhikov, V., & Zakharyaschev, M. (2007b).
Reasoning over extended ER models. In Proc. of the 26th Int. Conf. on Conceptual
Modeling (ER 2007), Vol. 4801 of Lecture Notes in Computer Science, pp. 277292.
Springer.
Artale, A., Cesarini, F., & Soda, G. (1996). Describing database objects in a concept
language environment. IEEE Trans. on Knowledge and Data Engineering, 8 (2), 345
351.
Artale, A., Parent, C., & Spaccapietra, S. (2007). Evolving objects in temporal information
systems. Ann. of Mathematics and Artificial Intelligence, 50, 538.
Baader, F., & Narendran, P. (2001). Unification of concepts terms in description logics. J.
of Symbolic Computation, 31 (3), 277305.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). The Description Logic Handbook: Theory, Implementation and Applications.
Cambridge University Press. (2nd edition, 2007).
Beeri, C., Levy, A. Y., & Rousset, M.-C. (1997). Rewriting queries using views in description
logics. In Proc. of the 16th ACM SIGACT SIGMOD SIGART Symp. on Principles
of Database Systems (PODS97), pp. 99108.
Berardi, D., Calvanese, D., & De Giacomo, G. (2005). Reasoning on UML class diagrams.
Artificial Intelligence, 168 (12), 70118.
Bergamaschi, S., & Sartori, C. (1992). On taxonomic reasoning in conceptual design. ACM
Trans. on Database Systems, 17 (3), 385422.
Boppana, R., & Sipser, M. (1990). The complexity of finite functions. In van Leeuwen, J.
(Ed.), Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, pp. 757804. Elsevier and MIT Press.
Borger, E., Gradel, E., & Gurevich, Y. (1997). The Classical Decision Problem. Perspectives
in Mathematical Logic. Springer.
Borgida, A., & Brachman, R. J. (2003). Conceptual modeling with description logics. In
Baader et al. (Baader et al., 2003), chap. 10, pp. 349372.

65

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2007).
Ontology-based database access. In Proc. of the 15th Ital. Conf. on Database Systems
(SEBD 2007), pp. 324331.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rosati, R., & Ruzzi, M.
(2008). Data integration through DL-Lite A ontologies. In Schewe, K.-D., & Thalheim,
B. (Eds.), Revised Selected Papers of the 3rd Int. Workshop on Semantics in Data and
Knowledge Bases (SDKB 2008), Vol. 4925 of Lecture Notes in Computer Science, pp.
2647. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:
Tractable description logics for ontologies. In Proc. of the 20th Nat. Conf. on Artificial
Intelligence (AAAI 2005), pp. 602607.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Data
complexity of query answering in description logics. In Proc. of the 10th Int. Conf. on
the Principles of Knowledge Representation and Reasoning (KR 2006), pp. 260270.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007a). Can OWL
model football leagues?. In Proc. of the 3rd Int. Workshop on OWL: Experiences and
Directions (OWLED 2007), Vol. 258 of CEUR Workshop Proceedings.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007b). Tractable
reasoning and efficient query answering in description logics: The DL-Lite family. J.
of Automated Reasoning, 39 (3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2008a). Inconsistency tolerance in P2P data integration: An epistemic logic approach. Information
Systems, 33 (4), 360384.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2008b). Path-based
identification constraints in description logics. In Proc. of the 11th Int. Conf. on the
Principles of Knowledge Representation and Reasoning (KR 2008), pp. 231241.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002a). Description logics for information
integration. In Kakas, A., & Sadri, F. (Eds.), Computational Logic: Logic Programming and Beyond, Essays in Honour of Robert A. Kowalski, Vol. 2408 of Lecture Notes
in Computer Science, pp. 4160. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002b). A framework for ontology integration. In Cruz, I., Decker, S., Euzenat, J., & McGuinness, D. (Eds.), The Emerging
Semantic Web  Selected Papers from the First Semantic Web Working Symposium,
pp. 201214. IOS Press.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998a). Description
logic framework for information integration. In Proc. of the 6th Int. Conf. on the
Principles of Knowledge Representation and Reasoning (KR98), pp. 213.
Calvanese, D., Lenzerini, M., & Nardi, D. (1998b). Description logics for conceptual data
modeling. In Chomicki, J., & Saake, G. (Eds.), Logics for Databases and Information
Systems, pp. 229264. Kluwer Academic Publishers.
Calvanese, D., Lenzerini, M., & Nardi, D. (1999). Unifying class-based representation formalisms. J. of Artificial Intelligence Research, 11, 199240.
66

fiThe DL-Lite Family and Relations

Corona, C., Ruzzi, M., & Savo, D. F. (2009). Filling the gap between OWL 2 QL and
QuOnto: ROWLKit. In Proc. of the 22nd Int. Workshop on Description Logics
(DL 2009), Vol. 477 of CEUR Workshop Proceedings.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse of ontologies: Theory and practice. J. of Artificial Intelligence Research, 31, 273318.
Decker, S., Erdmann, M., Fensel, D., & Studer, R. (1999). Ontobroker: Ontology based
access to distributed and semi-structured information. In Meersman, R., Tari, Z.,
& Stevens, S. (Eds.), Database Semantic: Semantic Issues in Multimedia Systems,
chap. 20, pp. 351370. Kluwer Academic Publishers.
Dolby, J., Fokoue, A., Kalyanpur, A., Ma, L., Schonberg, E., Srinivas, K., & Sun, X. (2008).
Scalable grounded conjunctive query evaluation over large and expressive knowledge
bases. In Proc. of the 7th Int. Semantic Web Conf. (ISWC 2008), Vol. 5318 of Lecture
Notes in Computer Science, pp. 403418. Springer.
Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query answering in the description logic Horn-SHIQ. In Proc. of the 11th Eur. Conference on Logics in Artificial
Intelligence (JELIA 2008), pp. 166179.
Franconi, E., & Ng, G. (2000). The i.com tool for intelligent conceptual modeling. In Proc. of
the 7th Int. Workshop on Knowledge Representation meets Databases (KRDB 2000),
Vol. 29 of CEUR Workshop Proceedings, pp. 4553.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman.
Ghilardi, S., Lutz, C., & Wolter, F. (2006). Did I damage my ontology? A case for conservative extensions in description logics. In Doherty, P., Mylopoulos, J., & Welty,
C. (Eds.), Proc. of the 10th Int. Conf. on the Principles of Knowledge Representation
and Reasoning (KR 2006), pp. 187197.
Glimm, B., Horrocks, I., Lutz, C., & Sattler, U. (2007). Conjunctive query answering for the
description logic SHIQ. In Proc. of the 20th Int. Joint Conf. on Artificial Intelligence
(IJCAI 2007), pp. 399404.
Goasdoue, F., Lattes, V., & Rousset, M.-C. (2000). The use of CARIN language and
algorithms for information integration: The Picsel system. Int. J. of Cooperative
Information Systems, 9 (4), 383401.
Gottlob, G., & Nash, A. (2008). Efficient core computation in data exchange. J. of the
ACM, 55 (2), 149.
Hayes, P. (2004). RDF semantics. W3C Recommendation. http://www.w3.org/TR/
rdf-mt/.
Heflin, J., & Hendler, J. (2001). A portrait of the Semantic Web in action. IEEE Intelligent
Systems, 16 (2), 5459.
Heymans, S., Ma, L., Anicic, D., Ma, Z., Steinmetz, N., Pan, Y., Mei, J., Fokoue, A.,
Kalyanpur, A., Kershenbaum, A., Schonberg, E., Srinivas, K., Feier, C., Hench, G.,
Wetzstein, B., & Keller, U. (2008). Ontology reasoning with large data repositories.
In Hepp, M., De Leenheer, P., de Moor, A., & Sure, Y. (Eds.), Ontology Management,
67

fiArtale, Calvanese, Kontchakov & Zakharyaschev

Semantic Web, Semantic Web Services, and Business Applications, Vol. 7 of Semantic
Web And Beyond Computing for Human Experience, pp. 89128. Springer.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). From SHIQ and RDF to
OWL: The making of a web ontology language. J. of Web Semantics, 1 (1), 726.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Practical reasoning for very expressive description logics. J. of the Interest Group in Pure and Applied Logic, 8 (3), 239264.
Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning in description logics by a reduction
to disjunctive Datalog. J. of Automated Reasoning, 39 (3), 351384.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data complexity of reasoning in very expressive
description logics. In Proc. of the 19th Int. Joint Conf. on Artificial Intelligence
(IJCAI 2005), pp. 466471.
Immerman, N. (1999). Descriptive Complexity. Springer.
Klyne, G., & Carroll, J. J. (2004). Resource description framework (RDF): Concepts and
abstract syntax. W3C Recommendation. http://www.w3.org/TR/rdf-concepts/.
Kontchakov, R., Pulina, L., Sattler, U., Schneider, T., Selmer, P., Wolter, F., & Zakharyaschev, M. (2009). Minimal module extraction from DL-Lite ontologies using
QBF solvers. In Proc. of the 21st Int. Joint Conf. on Artificial Intelligence (IJCAI 2009), pp. 836840.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2008). Can you tell the difference
between DL-Lite ontologies?. In Proc. of the 11th Int. Conf. on the Principles of
Knowledge Representation and Reasoning (KR 2008), pp. 285295.
Kontchakov, R., & Zakharyaschev, M. (2008). DL-Lite and role inclusions. In Domingue, J.,
& Anutariya, C. (Eds.), Proc. of the 3rd Asian Semantic Web Conf. (ASWC 2008),
Vol. 5367 of Lecture Notes in Computer Science, pp. 1630. Springer.
Kozen, D. (2006). Theory of Computation. Springer.
Lenzerini, M. (2002). Data integration: A theoretical perspective. In Proc. of the 21st ACM
SIGACT SIGMOD SIGART Symp. on Principles of Database Systems (PODS 2002),
pp. 233246.
Levy, A. Y., & Rousset, M.-C. (1998). Combining Horn rules and description logics in
CARIN. Artificial Intelligence, 104 (12), 165209.
Lutz, C., Toman, D., & Wolter, F. (2008). Conjunctive query answering in EL using a
database system. In Proc. of the 5th Int. Workshop on OWL: Experiences and Directions (OWLED 2008).
McGuinness, D., & Wright, J. R. (1998). Conceptual modelling for configuration: A description logic-based approach. Artificial Intelligence for Engineering Design, Analysis, and
Manufacturing. Special Issue on Configuration, 12, 333344.
Meyer, T., Lee, K., & Booth, R. (2005). Knowledge integration for description logics. In
Proc. of the 20th Nat. Conf. on Artificial Intelligence (AAAI 2005), pp. 645650.
Noy, N. F. (2004). Semantic integration: A survey of ontology-based approaches. SIGMOD
Record, 33 (4), 6570.
68

fiThe DL-Lite Family and Relations

Ortiz, M., Calvanese, D., & Eiter, T. (2006). Characterizing data complexity for conjunctive
query answering in expressive description logics. In Proc. of the 21st Nat. Conf. on
Artificial Intelligence (AAAI 2006), pp. 275280.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity of query answering in expressive description logics via tableaux. J. of Automated Reasoning, 41 (1), 6198.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). A comparison of query rewriting
techniques for DL-Lite. In Proc. of the 22nd Int. Workshop on Description Logics
(DL 2009), Vol. 477 of CEUR Workshop Proceedings.
Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008a).
Linking data to ontologies. J. on Data Semantics, X, 133173.
Poggi, A., Rodriguez, M., & Ruzzi, M. (2008b). Ontology-based database access with
DIG-Mastro and the OBDA Plugin for Protege. In Clark, K., & Patel-Schneider,
P. F. (Eds.), Proc. of the 4th Int. Workshop on OWL: Experiences and Directions
(OWLED 2008 DC).
Rautenberg, W. (2006). A Concise Introduction to Mathematical Logic. Springer.
Reingold, O. (2008). Undirected connectivity in log-space. J. of the ACM, 55 (4).
Schaerf, A. (1993). On the complexity of the instance checking problem in concept languages
with existential quantification. J. of Intelligent Information Systems, 2, 265278.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Artificial Intelligence, 48 (1), 126.
Stocker, M., & Smith, M. (2008). Owlgres: A scalable OWL reasoner. In Proc. of the 5th
Int. Workshop on OWL: Experiences and Directions (OWLED 2008).
Tobies, S. (2001). Complexity results and practical algorithms for logics in Knowledge Representation. Ph.D. thesis, LuFG Theoretical Computer Science, RWTH-Aachen, Germany.
Toman, D., & Weddell, G. E. (2005). On the interaction between inverse features and pathfunctional dependencies in description logics. In Proc. of the 19th Int. Joint Conf. on
Artificial Intelligence (IJCAI 2005), pp. 603608.
Toman, D., & Weddell, G. E. (2008). On keys and functional dependencies as first-class
citizens in description logics. J. of Automated Reasoning, 40 (23), 117132.
Vardi, M. (1982). The complexity of relational query languages (extended abstract). In
Proc. of the 14th ACM SIGACT Symp. on Theory of Computing (STOC82), pp.
137146.
Vollmer, H. (1999). Introduction to Circuit Complexity: A Uniform Approach. Springer.
Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2008). Forgetting concepts in DL-Lite.
In Bechhofer, S., Hauswirth, M., Hoffmann, J., & Koubarakis, M. (Eds.), Proc. of the
5th Eur. Semantic Web Conf. (ESWC 2008), Vol. 5021 of Lecture Notes in Computer
Science, pp. 245257. Springer.

69

fiJournal of Artificial Intelligence Research 36 (2009) 387-414

Submitted 07/09; published 11/09

Approximate Strong Equilibrium in Job Scheduling Games
Michal Feldman

mfeldman@huji.ac.il

School of Business Administration
and Center for the Study of Rationality,
Hebrew University of Jerusalem, Israel.

Tami Tamir

tami@idc.ac.il

School of Computer Science,
The Interdisciplinary Center, Herzliya, Israel.

Abstract
A Nash Equilibrium (NE) is a strategy profile resilient to unilateral deviations, and is
predominantly used in the analysis of multiagent systems. A downside of NE is that it is
not necessarily stable against deviations by coalitions. Yet, as we show in this paper, in
some cases, NE does exhibit stability against coalitional deviations, in that the benefits
from a joint deviation are bounded. In this sense, NE approximates strong equilibrium.
Coalition formation is a key issue in multiagent systems. We provide a framework for
quantifying the stability and the performance of various assignment policies and solution
concepts in the face of coalitional deviations. Within this framework we evaluate a given
configuration according to three measures: (i) IR min : the maximal number , such that
there exists a coalition in which the minimal improvement ratio among the coalition members is , (ii) IR max : the maximal number , such that there exists a coalition in which
the maximal improvement ratio among the coalition members is , and (iii) DR max : the
maximal possible damage ratio of an agent outside the coalition.
We analyze these measures in job scheduling games on identical machines. In particular,
we provide upper and lower bounds for the above three measures for both NE and the wellknown assignment rule Longest Processing Time (LPT).
Our results indicate that LPT performs better than a general NE. However, LPT is not
the best possible approximation. In particular, we present a polynomial time approximation
scheme (PTAS) for the makespan minimization problem which provides a schedule with
IR min of 1 +  for any given . With respect to computational complexity, we show that
given an NE on m  3 identical machines or m  2 unrelated machines, it is NP-hard to
determine whether a given coalition can deviate such that every member decreases its cost.

1. Introduction
We consider job scheduling systems, in which n jobs are assigned to m identical machines
and incur a cost which is equal to the total load on the machine they are assigned to.1 These
problems have been widely studied in recent years from a game theoretic perspective (Koutsoupias & Papadimitriou, 1999; Andelman, Feldman, & Mansour, 2007; Christodoulou,
Koutsoupias, & Nanavati, 2004; Czumaj & Vocking, 2002; A. Fiat & Olonetsky., 2007).
In contrast to the traditional setting, where a central designer determines the allocation of
jobs to machines and all the participating entities are assumed to obey the protocol, mul1. This cost function characterizes systems in which jobs are processed in parallel, or when all jobs on a
particular machine have the same single pick-up time, or need to share some resource simultaneously.

c
2009
AI Access Foundation. All rights reserved.

fiFeldman & Tamir

tiagent systems are populated by heterogeneous, autonomous agents, which often display
selfish behavior. Different machines and jobs may be owned by different strategic entities,
who will typically attempt to optimize their own objective rather than the global objective.
Game theoretic analysis provides us with the mathematical tools to study such situations,
and indeed has been extensively used recently to analyze multiagent systems. This trend
is motivated in part by the emergence of the Internet, which is composed of distributed
computer networks managed by multiple administrative authorities and shared by users
with competing interests (Papadimitriou, 2001).
Most game theoretic models applied to job scheduling problems, as well as other network games (e.g., Fabrikant, Luthra, Maneva, Papadimitriou, & Shenker, 2003; Albers,
Elits, Even-Dar, Mansour, & Roditty, 2006; Roughgarden & Tardos, 2002; Anshelevich,
Dasgupta, Kleinberg, Tardos, Wexler, & Roughgarden, 2004), use the solution concept
of Nash equilibrium (NE), in which the strategy of each agent is a best response to the
strategies of all other agents. While NE is a powerful tool for analyzing outcomes in competitive environments, its notion of stability applies only to unilateral deviations.
In
numerous multiagent settings, selfish agents stand to benefit from cooperating by forming
coalitions (Procaccia & Rosenschein, 2006). Therefore, even when no single agent can profit
by a unilateral deviation, NE might still not be stable against a group of agents coordinating a joint deviation, which is profitable to all the members of the group. This stronger
notion of stability is exemplified in the strong equilibrium (SE) solution concept, coined by
Aumann (1959). In a strong equilibrium, no coalition can deviate and improve the utility
of every member of the coalition.

M3

3

2

5

M3

3

5

8

M2

3

2

5

M2

3

5

8

10

M1

M1

5

5
(a)

2

2

4

(b)

Figure 1: An example of a configuration (a) that is a Nash equilibrium but is not resilient against
coordinated deviations, since the jobs of load 5 and 2 all profit from the deviation demonstrated in (b).

As an example, consider the configuration depicted in Figure 1(a). In our figures, a
job is represented by a rectangle whose width corresponds to the jobs length. The jobs
scheduled on a specific machine form a vertical concatenation of rectangles. For example,
in Figure 1(a) there are three machines, and M1 processes two jobs of length 5. Note that
the internal order of jobs has no effect, since the cost of each job is defined to be the load on
the machine it is assigned to. This configuration is an NE since no job can reduce its cost
through a unilateral deviation. One might think that any NE on identical machines must
also be sustainable against joint deviations. Yet, as was already observed in (Andelman

388

fiApproximate Strong Equilibrium

et al., 2007), this may not be true.2 For example, the configuration above is not resilient
against a coordinated deviation of the coalition  consisting of the four jobs of load 5 and
2 deviating to configuration (b), where the jobs of load 5 decrease their costs from 10 to 8,
and the jobs of load 2 improve from 5 to 4. Note that the cost of the two jobs of load 3
(which are not members of the coalition) increases.
In the example above, every member of the coalition improves its cost by a (multiplicative) factor of 45 . By how much more can a coalition improve? Is there a bound on the
improvement ratio? As it will turn out, this example is in fact the most extreme one in a
sense that will be clarified below. Thus, while NE is not completely stable against coordinated deviations, in some settings, it does provide us with some notion of approximate
stability to coalitional deviations (or approximate strong equilibrium).
We also consider a subclass of NE schedules, produced by the Longest Processing Time
(LPT) rule (Graham, 1969). The LPT rule sorts the jobs in a non-increasing order of
their loads and greedily assigns each job to the least loaded machine. It is easy to verify
that every configuration produced by the LPT rule is an NE (Fotakis, S. Kontogiannis, &
Spiraklis, 2002). Is it also an SE? Note that for the instance depicted in Figure 1, LPT
would have produced an SE. However, as we show, this is not always the case.
In this paper we provide a framework for studying the notion of approximate stability
to coalitional deviations. In our analysis, we consider three different measures. The first
two measure the stability of a configuration, and uses the notion of an improvement ratio
of a job, which is defined as the ratio between the jobs costs before and after the deviation.
The third measures the worst possible effect on the non-deviating jobs, as will be explained
below.3
1. Minimum Improvement Ratio: By definition, all members of a coalition must
reduce their cost. That is, the improvement ratio of every member of the coalition is larger
than 1. Clearly, the coalition members might not share the same improvement ratio. The
minimum improvement ratio of a particular deviation is the minimal improvement ratio of
a coalition member. The minimum improvement ratio of a schedule s, denoted IR min (s),
is the maximum over all possible deviations originated from s of the minimal improvement
ratio of the deviation. In other words, there is no coalitional deviation originating from s
such that every member of the coalition reduces its cost by a factor greater than IR min (s).
A closely related notion has been suggested by Albers (2009), who defined a strategy
profile to be an -SE if there is no coalition in which each agent can improve by a factor
of more than . In our notation, a schedule s is an -SE if IR min (s) is at most . Albers
studied this notion in the context of SE existence in cost-sharing games, and showed that
for a sufficiently large , an -SE always exists. The justification behind this concept is
that agents may be willing to deviate only if they improve by a sufficiently high factor (due
to, for example, some overhead associated with the deviation).
2. This statement holds for m  3. For 2 identical machines, every NE is also an SE (Andelman et al.,
2007).
3. Throughout this paper, we define approximation by a multiplicative factor. Since the improvement and
damage ratios for all the three measures presented below are constants greater than one (as will be shown
below), the additive ratios are unbounded. Formally, for any value a it is possible to construct instances
(by scaling the instances we provide for the multiplicative ratio) in which the cost of all jobs is reduced,
or the cost of some jobs is increased, by at least an additive factor of a.

389

fiFeldman & Tamir

For three machines, we show that every NE is a 54 -SE. That is, there is no coalition
that can deviate such that every member improves by a factor larger than 54 . For this
case, we also provide a matching lower bound (recall Figure 1 above), that holds for any
2
m  3. For arbitrary m, we show that every NE is a (2  m+1
)-SE. Our proof technique
draws a connection between makespan approximation and approximate stability, where
the makespan of a configuration is defined as the maximum load on any machine in the
configuration.
We next consider schedules obtained
from the LPT rule. We show that for m = 3,

every LPT configuration is a ( 21 + 46 )-SE ( 1.1123), and we also provide a matching lower
1
bound, which holds for any m  3. For arbitrary m, we show an upper bound of 43  3m
.
The above results indicate that LPT is more stable than NE with respect to coalitional
deviations. Yet, LPT is not the best possible approximation of SE. Similar to this notion
in approximation algorithms, we define an SE-PTAS to be an assignment algorithm which
gets as input an additional parameter , specifying how close to an SE the schedule should
be and produces a (1 + )-SE in time polynomial in n, 1/. In this paper we devise an
SE-PTAS for any fixed number of machines, which also approximates the makespan within
a factor of 1 + .
2. Maximum Improvement Ratio: The maximum improvement ratio of a particular
deviation is the maximal improvement ratio experienced by some coalition member. The
maximum improvement ratio of a schedule s, denoted IR max (s), is the maximum over all
possible deviations originated from s of the maximal improvement ratio of the deviation.
In other words, there is no coalition deviation originating from s such that there exists a
member of the coalition that reduces its cost by a factor greater than IR max (s).
This notion establishes the bounds on how much an agent would gain in a deviating
coalition for which all agents gain something from the deviation. Also, this notion is similar
in spirit to stability against a large total improvement. It also suits environments in which
individuals are willing to obey a specific player as long as they are not hurt. Interestingly,
we find that given an NE configuration, the improvement ratio of a single agent may be
arbitrarily large, for any m  3. In contrast, for LPT configurations on three machines, no
agent can improve by a factor of 53 or more and this bound is tight. Thus, with respect to
IR max , the relative stability of LPT compared to NE is more significant than with respect
1
, which we believe to be tight.
to IR min . For arbitrary m, we provide a lower bound of 2  m
3. Maximum Damage Ratio: As is the case for the jobs of load 3 in Figure 1, some jobs
might get hurt as a result of a coalitional deviation. The third measure that we consider
is the worst possible effect of a deviation on jobs that are not members of the deviating
coalition. Formally, the maximum damage ratio is the maximal ratio between the costs of
a non-coalition member before and after the deviation. Variants of this measure have been
considered in distributed systems, e.g., the Byzantine Generals problem (Lamport, Shostak,
& Pease, 1982), and in rational secret sharing (Halpern & Teague, 2004).4 In Section 5,
we prove that the maximum damage ratio is less than 2 for any NE configuration, and less
4. In a rational secret sharing protocol, a set of players, each holding a share of a secret, aims to jointly
reconstruct it. Viewing the protocol as a game, the players utilities are typically assumed to satisfy the
following two basic constraints: (i) each player prefers learning the secret over not learning it, and (ii)
conditioned on having learned the secret, each player prefers as few as possible other players to learn it.

390

fiApproximate Strong Equilibrium

than 23 for any LPT configuration. Both bounds hold for any m  3, and for both cases we
provide matching lower bounds.
In summary, our results in Sections 3-5 (see Table 1) indicate that NE configurations
are approximately stable with respect to the IR min measure. Moreover, the performance of
jobs outside the coalition would not be hurt by much as a result of a coalitional deviation.
As for IR max , our results provide an additional strength of the LPT rule, which is already
known to possess attractive properties (with respect to, e.g., makespan approximation and
stability against unilateral deviations).

NE
LPT

IR min
upper bound
m3
m=3
2
5
2  m+1
4
4
3



1
3m

1
2

+


6
4

lower
bound
1
2

5
4

+

6
4

IR max
upper
lower
bound bound
unbounded
5
3 (m=3)

2

1
m

DR max
upper lower
bound bound
2
2
3
2

3
2

Table 1: Our results for the three measures. Unless specified otherwise, the results hold for
arbitrary number of machines m.
In Section 7, we study computational complexity aspects of coalitional deviations. We
find that it is NP-hard to determine whether an NE configuration on m  3 identical
machines is an SE. Moreover, given a particular configuration and a set of jobs, it is NPhard to determine whether this set of jobs can engage in a coalitional deviation. For
unrelated machines (i.e., where each job incurs a different load on each machine), the above
hardness results hold already for m = 2 machines. These results might have implications
on coalitional deviations with computationally restricted agents.
Related work: NE is shown in this paper to provide approximate stability against coalitional deviations. A related body of work studies how well NE approximates the optimal
outcome of competitive games. The Price of Anarchy was defined as the ratio between the
worst-case NE and the optimum solution (Papadimitriou, 2001; Koutsoupias & Papadimitriou, 1999), and has been extensively studied in various settings, including job scheduling
(Koutsoupias & Papadimitriou, 1999; Christodoulou et al., 2004; Czumaj & Vocking, 2002),
network design (Albers et al., 2006; Anshelevich et al., 2004; Anshelevich, Dasgupta, Tardos, Wexler, & Roughgarden, 2003; Fabrikant et al., 2003), network routing (Roughgarden
& Tardos, 2002; Awerbuch, Azar, Richter, & Tsur, 2003; Christodoulou & Koutsoupias,
2005), and more.
The notion of strong equilibrium (SE) (Aumann, 1959) expresses stability against coordinated deviations. The downside of SE is that most games do not admit any SE, even
amongst those admitting a Nash equilibrium. Various recent works have studied the existence of SE in particular families of games. For example, it has been shown that in every job
scheduling game and (almost) every network creation game, an SE exists (Andelman et al.,
2007). In addition, several papers (Epstein, Feldman, & Mansour, 2007; Holzman & LawYone, 1997, 2003; Rozenfeld & Tennenholtz, 2006) provided a topological characterization
for the existence of SE in different congestion games, including routing and cost-sharing
391

fiFeldman & Tamir

connection games. The vast literature on SE (e.g., Holzman & Law-Yone, 1997, 2003;
Milchtaich, 1998; Bernheim, Peleg, & Whinston, 1987) concentrate on pure strategies and
pure deviations, as is the case in our paper. In job scheduling settings, it was shown by
Andelman et al. (2007) that if mixed deviations are allowed, it is often the case that no SE
exists. When an SE exists, clearly, the price of anarchy with respect to SE (denoted the
strong price of anarchy by Andelman et al., 2007) is significantly better than the price of
anarchy with respect to NE (Andelman et al., 2007; A. Fiat & Olonetsky., 2007; Leonardi
& Sankowski, 2007).
Following our work, IR min bounds for the case of m = 4 machines have been provided
by Chen (2009), who extended our bound of 45 for NE schedules, and provided a bound of
( 12 +



345
30 )

 1.119 for LPT-based schedules.

2. Model and Preliminaries
In this section we give a formal description of the model and provide several useful observations and properties of deviations by coalitions.
2.1 Resilience to Deviations by Coalitions
We first present a general game theoretic setting and then describe the specific job scheduling
setting which is the focus of this paper.
A game is denoted by a tuple G = hN, (Sj ), (cj )i, where N = {1, . . . , n} is the set of
players, Sj is the finite action space of player j  N , and cj is the cost function of player j.
The joint action space of the players is S = ni=1 Si . For a joint action s = (s1 , . . . , sn )  S,
we denote by sj the actions of players j 0 6= j, i.e., sj = (s1 , . . . , sj1 , sj+1 , . . . , sn ).
Similarly, for a set of players , also called a coalition, we denote by s and s the actions
of players in  and not in , respectively. The cost function of player j maps a joint action
s  S to a real number, i.e., cj : S  R.
A joint action s  S is a pure Nash Equilibrium (NE) if no player j  N can benefit from
unilaterally deviating from his action to another action, i.e., j  N a  Sj : cj (sj , a) 
cj (s). A pure joint action of a coalition   N specifies an action for each player in the
coalition, i.e., s0  j Sj . A joint action s  S is not resilient to a pure deviation of a
coalition  if there is a pure joint action s0 of  such that cj (s , s0 ) < cj (s) for every
j   (i.e., the players in the coalition can deviate in such a way that each player strictly
reduces its cost). In this case we say that the deviation to s0 = (s , s0 ) is a profitable
deviation for coalition .
A pure joint action s  S is resilient to pure deviations by coalitions if there is no
coalition   N that has a profitable deviation from s.
Definition 2.1 A pure strong equilibrium (SE) is a pure joint action that is resilient to
pure deviations of coalitions.
Clearly, a strong equilibrium is a refinement of the notion of Nash equilibrium (in particular, if s is a strong equilibrium, it is resilient to deviations of coalitions of size 1, which
coincides with the definition of NE).

392

fiApproximate Strong Equilibrium

2.2 Job Scheduling on Identical Machines
A job scheduling setting with identical machines is characterized by a set of machines M =
{M1 , . . . , Mm }, a set {1, . . . , n} of jobs, where a job j has processing time pj . An assignment
method produces an assignment s of jobs into machines, where sj  M denotes the machine
job j is assigned to. The assignment is referred to as a schedule or a configuration (we use
the two terms interchangeably). The load of a machine Mi inP
a schedule s is the sum of the
processing times of the jobs assigned to Mi , that is Li (s) = j:sj =Mi pj . For a set of jobs
S
, let s() = j {sj } denote the set of machines on which the members of  are assigned
in schedule s.
The makespan of a schedule is the load on the most loaded machines. A social optimum
minimizes the makespan, i.e., OP T = mins makespan(s).
For each job scheduling setting define a job scheduling game with the jobs as players.
The action space Sj of player j  N are all the individual machines, i.e., Sj = M . The
joint action space is S = nj=1 Sj . A joint action s  S constitutes a schedule. In a schedule
s  S player j  N selects machine sj as its action and incurs a cost cj (s), which is the
load on the machine sj , i.e., cj (s) = Li (s), where sj = Mi . In a job scheduling game, the
makespan is also the highest cost among all players. Formally, makespan(s) = maxj cj (s).
0
Let s and s0 be two configurations. Let Pis,s
be a binary indicator whose value is 1 if
1 ,i2
there is a job j such that sj = Mi1 and s0j = Mi2 (i.e., if there is a job that chooses Mi1
in s but Mi2 in s0 ), and 0 otherwise. When clear from the context, we abuse notation and
0
denote Pis,s
by Pi1 ,i2 . In addition, we denote Li (s) and Li (s0 ) by Li and L0i , respectively.
1 ,i2
Let s0 = (s , s0 ) be a profitable deviation from s for a coalition . The improvement
ratio of a job j   such that sj = Mi1 and s0j = Mi2 (i.e., a job migrating from machine
0
Mi1 to machine Mi2 ) is denoted by IRs,s (j) = Li1 (s)/Li2 (s0 ). Clearly, for any job j  ,
0
IRs,s (j) > 1. The damage ratio of a job j 6  such that sj = s0j = Mi is denoted by
0
DRs,s (j) = Li (s0 )/Li (s).
If sj 6= s0j , we say that job j migrates in the deviation. Note that, in our terminology, a
job can be a member of a profitable deviation even if it does not migrate in the deviation.
Yet, every job that migrates in the deviation is a member of the deviating coalition by
definition.
Definition 2.2 Given schedules s and s0 = (s , s0 ), the minimal improvement ratio of s0
0
with respect to s is IR min (s, s0 ) = minj IRs,s (j). In addition, the minimal improvement
ratio of a schedule s is IR min (s) = maxs0 =(s ,s0 ),N IR min (s, s0 ).
Given schedules s and s0 = (s , s0 ), the maximal improvement ratio of s0 with respect
0
to s is IR max (s, s0 ) = maxj IRs,s (j). In addition, the maximal improvement ratio of a
schedule s is IR max (s) = maxs0 =(s ,s0 ),N IR max (s, s0 ).
Given schedules s and s0 = (s , s0 ), the maximal damage ratio of s0 with respect to s
0
is DR max (s, s0 ) = maxjN DRs,s (j). In addition, the maximal damage ratio of a schedule
s is DR max (s) = maxs0 =(s ,s0 ),N DR max (s, s0 ).
In particular, we can define the notion of -SE (Albers, 2009) in terms of the minimum
improvement ratio as follows:
Definition 2.3 A schedule s is an -strong equilibrium (-SE) if IR min (s)  .
393

fiFeldman & Tamir

We next provide several useful observations and claims we shall use in the sequel. We
refer to a profitable deviation from an NE-schedule as an NE-originated profitable deviation.
Similarly, a profitable deviations from a schedule produced by the LPT rule is referred to
as an LPT-originated profitable deviation.
The first observation shows that in any NE-originated profitable deviation, if a job
migrates to some machine, some other job must migrate out of that machine. Formally:
Observation 2.4 Let s be an NE and let s0 = (s , s0 ) be a profitable deviation. If s0j = Mi
for some j  , then j 0   such that sj 0 = Mi and s0j 0 = Mi0 for some i0 6= i.
This is obvious, since if job j strictly decreases its cost by migrating to a machine that no
other job leaves, it can also profitably migrates unilaterally, contradicting s being an NE.
We next define a special deviation structure, called a flower structure in which all the
deviations are from or to the most loaded machine.
Definition 2.5 Let M1 be the most loaded machine in a given schedule s. We say that a
s,s0
s,s0
deviation s0 obeys the flower structure if for all i > 1, P1,i
= Pi,1
= 1 and for all i, j > 1,
0

s,s
Pi,j
= 0 (See Figure 2).

M2

M3
M1

M5

M4

Figure 2: A graph representation of a coalition on 5 machines obeying the flower structure. There
0

s,s
is an edge from Mi to Mj if and only if Pi,j
= 1.

0

s,s
In particular, for m = 3, a deviation from s to s0 obeys the flower structure if P1,2
=
0

0

0

0

0

s,s
s,s
s,s
s,s
s,s
P2,1
= P1,3
= P3,1
= 1 and P2,3
= P3,2
= 0. Recall that for simplicity of presentation,
0

s,s
we write in the sequel Pi,j to denote Pi,j
and we also write Li and L0i to denote Li (s) and
Li (s0 ), respectively.

Claim 2.6 Any NE-originated profitable deviation on three machines obeys the flower structure.
Proof: Let s be an NE and M1 be the most loaded machine in s, and let s0 be a profitable
deviation. We first show that P2,3 = P3,2 = 0. Assume first that both P2,3
P = P3,2P= 1.
Thus, L02 < L3 and L03 < L2 . Clearly, since the total load does not change, i Li = i L0i .
Therefore, it must hold that L01 > L1 . However, no profitable deviation can increase the
load on the most loaded machine. A contradiction. Therefore, at most one of P2,3 , P3,2 can
be 1. Assume w.l.o.g that P2,3 = 1. By Observation 2.4 some job leaves M3 , and by the
above it cannot be to M2 . Thus, it must be that P3,1 = 1. Similarly, some job
M1 . If
Pleaves P
P1,2 = 1, then we get that L01 < L3 , L02 < L1 , and L03 < L2 , contradicting i Li = i L0i .
394

fiApproximate Strong Equilibrium

0
0
0
If P1,3 = 1 then
P we getPthat0 L1 < L3 , L2 < L2 (no job is added to M2 ), and L3 < L1 ,
contradicting i Li = i Li again. Thus, P2,3 = 0. The proof of P3,2 = 0 is analogous.
It remains to show that P1,2 = P1,3 = P2,1 = P3,1 = 1. We know that all three machines
are assigned jobs in s0 that were not assigned to them in s. By the above P2,3 = P3,2 = 0.
By Observation 2.4 some job leaves each of M2 , M3 , therefore, P2,1 = P3,1 = 1. Also, some
job leaves M1 , thus at least one of P1,2 , P1,3 equals 1. Assume w.l.o.g that P1,2 = 1. We
show that also P1,3 = 1. In particular, we show that L03 > L3 , and since P2,3 = 0 it must be
that P1,3 = 1. Assume the opposite, that is L03  L3 . We already know that
P2,1 = 1.
P P1,2 =P
Thus, L02 < L1 , L01 < L2 , and by our assumption L03  L3 . That is, i L0i < i Li . A
contradiction.

It is known that any NE schedule on two identical machines is also an SE (Andelman
et al., 2007). By the above claim, at least four jobs change machines in any profitable
deviation on three machines. Clearly, at least four jobs change machines in any coalition
on m > 3 machines. Therefore,

Corollary 2.7 Every NE-schedule is stable against deviations by coalitions of size three or
less.
The next two propositions further characterize any coalition deviation on three machines.
We show that while M1 is the most loaded machine before the deviation, it becomes the
least loaded after the deviation.
Proposition 2.8 In any NE-originated deviation on three machines, the loads on the two
less loaded machines in increasing, that is, L02 > L2 and L03 > L3 .
Proof: Assume on the contrary that L02  L2 . By Claim 2.6, P1,3 P
= P3,1 = P
1. Thus,
0 <
L03 < L1 , L01 < L3 , and by our assumption L02  L2 . That is,
L
i i
i Li . A
0
contradiction. The proof of L3 > L3 is analogous.

Proposition 2.9 In any NE-originated deviation on three machines the most loaded machine becomes the least loaded machine, that is, L01 < min(L02 , L03 ).
Proof: By Claim 2.6, P1,2 = 1, and thus L01 < L2 . By the above proposition, L2 < L02 .
Thus, L01 < L02 . The proof of L01 < L03 is symmetric.


3. -Strong Equilibrium
In this section, the stability of configurations is measured by the minimal improvement ratio
measure. We first provide a complete analysis (i.e. matching upper and lower bounds) for
three identical machines5 for both NE and LPT. For arbitrary m, we provide an upper
bound for NE and LPT, and show that the lower bounds for m = 3 hold for any m.
Theorem 3.1 Any NE schedule on three machines is a 45 -SE.
5. We note that for unrelated machines, the improvement ratio cannot be bounded within any finite factor
even for two machines. This can be seen by a simple example of two jobs and two machines, where the
load vector of job 1 is (1, ), and the load vector of job 2 is (, 1). If job i is assigned to machine i (for
i = 1, 2), the resulting configuration is an NE, with load 1 on each machine. However, both jobs can
reduce their load from 1 to  by swapping.

395

fiFeldman & Tamir

Proof: Let s be an NE-configuration on three machines, and let r = IR min (s). By Claim
2.6, the deviation obeys the flower P
structure. Therefore: L01  L2 /r , L01  L3 /r , L02 
L1 /r , and L03  L1 /r. Let P = j pj (also = L1 + L2 + L3 ). Summing up the above
inequalities we get r  (L1 + P )/(L01 + P ).
Proposition 3.2 The load on the most loaded machine is at most half of the total load,
that is, L1  P/2.
Proof: Let g = max(L1 L2 , L1 L3 ). By the flower structure, there are at least two jobs on
M1 , thus g  L1 /2 - since otherwise some job would benefit from leaving M1 , contradicting
the NE. By definition of g, we know that 2L1  L2 + L3 + 2g, and since 2g  L1 , we get
that L1  P/2.

Distinguish between two cases:
1. L01  P/5: in this case r  (L1 + P )/(L01 + P )  (3P/2)/(6P/5) = 5/4.
2. L01 < P/5: It means that L02 + L03 > 4P/5 (M2 and M3 have the rest of the load),
that is, at least one of L02 , L03 > 2P/5. W.l.o.g. let it be M2 . By the flower structure
some job from M1 migrates M2 . This jobs improvement ratio is L1 /L02 , which, by
Proposition 3.2, is less than (P/2)/(2P/5) = 5/4. Thus, again, r < 5/4.

The above analysis is tight as shown in Figure 1. Moreover, this lower bound can be
extended to any m > 3 by adding m  3 machines and m  3 heavy jobs assigned to these
machines. Thus,
Theorem 3.3 For m  3, there exists an NE schedule s such that IR min (s) = 54 .
For LPT configurations, the bound on the minimum improvement ratio is lower. The
proof of the following theorems appear in Appendix A.
Theorem 3.4 Any LPT schedule on three machines is a ( 12 +


6
4

 1.1123)-SE.

Theorem 3.5 For any m  3, there exists an LPT schedule s such that IR min (s) = 12 +


6
4 .

We next provide upper bounds for arbitrary m. Our analysis is based on drawing a
connection between the makespan approximation and the SE-approximation. Assume that
a given schedule is an r-approximation for the minimum makespan. We show that under
some conditions on the original schedule, if a subset of jobs form a coalition for which
IR min > r, then, by considering only a subset of machines, it is possible to get a schedule
which is apparently better than the optimal one. We first define the set of assignment rules
for which the above connection exists.
Definition 3.6 Let s be a schedule of an instance I = hN, M i. Given M  M , let I =
hN , M i be an instance induced by s, M such that N = {j|sj  M }. An assignment method,
A, is said to be subset-preserving if for any I and M  M , it holds that sj = sj for

any j  N , where s and s are the assignments produced by A on the instances I and I,
respectively.
396

fiApproximate Strong Equilibrium

Claim 3.7 LPT is a subset-preserving method.
Proof: The proof is by induction on the number of the jobs in N . We show that for any
k 6= N , the first k jobs in N 0 are assigned on the same machine when LPT is executed
 Note that since N is a sublist of N , the jobs in N are in the
on input I and on input I.
same order as in N . The first job is scheduled on the first empty machine among M . For
any other job j  N , by the induction hypothesis, when j is scheduled, the load on each
of the machines is identical to the load of the corresponding machines at the time j was
scheduled as a member of N . This load is generated only by jobs in N that come before j
in N . Therefore, by LPT, j is scheduled on the least loaded machine among the machines
M , that is, sj = sj . We assume that LPT uses a deterministic tie-breaking rule if there
are several least loaded machines in N . Therefore sj = sj also in this case.

Lemma 3.8 Let A be an assignment method that is (i) subset-preserving, (ii) yields Nash
equilibrium, and (iii) approximates the minimum makespan within a factor of r, where r  1
is non-decreasing in m. Then, A produces an r-SE.
Proof: Assume for contradiction that there exists an instance I such that in the schedule s
produced by A, there exists a coalition in which the improvement ratio of every member is
greater than r. Let  be such a coalition of minimum size. If there is a job j   that does
not migrate, then the set of jobs  \ {j} is a smaller coalition, contradicting the minimality
of ; therefore, for every j  , it holds that sj 6= s0j . We next show that s() = s0 ().
First, s()  s0 (), that is, for every machine in s() from which a job j migrates, there
must exist a job migrating to it, otherwise,  \ {j} is a smaller such coalition, contradicting
the minimality of . Second, s0 ()  s(), that is, for every machine to which a job j
migrates, there must exist a job migrating from it (otherwise job j can improve unilaterally,
in contradiction to s being an NE). Given that s() = s0 (), denote this set of machines by
M , and let m = |M |. Finally, let N  N be the set jobs assigned to machines in M by A,
Consider the instance I = hN , M i. Since A is subset-preserving, the jobs of N are
scheduled by A on M in I exactly as their schedule on M when scheduled as part of I. In
particular, when I is scheduled by A, the same deviation of  exists, in which every job in
 improves by a factor greater than r, and all the machines in M take part in it. In other
words, for any pair of machines i, j, such that Pi,j = 1, we have Li /L0j > r(m)  r(m),
where r(m) is the approximation ratio of A on m machines. On the other hand, since A
 where OP T (I)
 is
produces an r(m)-approximation, for any machine i, Li  r(m)OP T (I),

the minimum possible makespan of I on M machines. Therefore, if Pi,j = 1 then r(m) <
Li /L0j 


r(m)OP T (I)
.
L0j

This implies that for any machine j that receives at least one job,


L0j < OP T (I).
However, since at least one job has migrated to each of the m participating machines,
after the deviation the machines M are assigned all the jobs of N and they all have load
 A contradiction.
less than OP T (I).

Let s be an NE on M machines. Clearly, for any M  M , the induced schedule of s on
2
the set of machines M is also an NE. Also, it is known that any NE provides a (2  m+1
)approximation to the makespan (Finn & Horowitz, 1979; Schuurman & Vredeveld, 2007).
2
This implies that Lemma 3.8 can be applied with r = 2  m+1
to any assignment that yields
an NE. Therefore,
397

fiFeldman & Tamir

Corollary 3.9 Any NE schedule on m identical machines is a (2 

2
m+1 )

 SE.

The next result is a direct corollary of Lemma 3.8, Claim 3.7, and the fact that LPT
1
)-approximation to the makespan (Graham, 1969).
provides a ( 43  3m
1
Corollary 3.10 Any schedule produced by LPT on m identical machines is a ( 43  3m
)SE.

The above bounds are not tight, but the gap between the lower and upper bounds is
only a small constant.

4. Maximum Improvement Ratio
In this section, we analyze the maximum improvement ratio measure. We provide a complete analysis for NE configurations and any m  3, and for LPT configurations on three
machines. The lower bound for LPT on three machines can be extended to arbitrary m. In
contrast to the other measures we consider in this paper, where NE and LPT differ only by
a small constant, it turns out that with respect to the maximum improvement ratio, NE
and LPT are significantly different. While the improvement ratio of NE-originated deviations can be arbitrarily high, for deviations from LPT configurations, the highest possible
improvement ratio of any participating job is less than 35 .
Theorem 4.1 Fix r  1. For any m  3 machines, there exists an instance with m
machines and an NE s such that IR max (s) > r.
Proof: Given r, consider the NE-configuration on three machines given in Figure 3(a). The
coalition consists of {1, 1, 2r, 2r}. Their improved schedule is given in Figure 3(b). The
improvement ratio of the jobs of load 1 is 2r/2 = r. For m > 3, dummy machines and jobs
can be added.

M3

2r-1

1

2r

M3

2r-1

2r

4r-1

M2

2r-1

1

2r

M2

2r-1

2r

4r-1

M1

2r

4r

M1 1 1

2r
(a)

2
(b)

Figure 3: An NE-originated deviation in which the jobs of load 1 have improvement ratio
r.

In contrast to NE-originated deviations, for LPT-originated deviations we are able to
bound the maximum improvement ratio by a small constant. The proof of the following
claim is given in Appendix A.
Theorem 4.2 Let s be an LPT schedule on three machines. It holds that IRmax (s)  53 .

398

fiApproximate Strong Equilibrium

1+

2m-2+

Mm

M2

2m-3

1+

2m-2+

M2

M1

2m-3

2

2m-1

2m-3

2

2m-1



2m-3


Mm

2



2

1+ 2m-1+

M1 1+  1+ 1+

(a)

m+m

(b)

Figure 4: An LPT-originated deviation on m machines in which the job of load 1 +  assigned to
M1 has improvement ratio arbitrarily close to 2 

1
m.

The above bound is tight, as demonstrated in Figure 4 for m = 3 (where the im1
provement ratio is 2  m
= 35 ). Moreover, this figure shows that this lower bound can be
generalized for any m  3. Note that the coalitional deviation in this example obeys the
flower structure. We believe that this example is tight, as the flower structure seems to
enable the largest possible decrease in the load of a single machine. The job of load 1 + 
that remains on M1 improves its cost from 2m  1 +  to m(1 + ), that is, for this job, j,
1
IR(j) = 2m1+
m(1+) = 2  m  . Formally,
Theorem 4.3 For any m  3, there exists an LPT configuration s such that IR max (s) =
1
2 m
  for an arbitrarily small  > 0.

5. Maximum Damage Ratio
In this section, we provide results for the maximum damage a profitable deviation can
impose on jobs that do not take part in the coalition. Formally, the quality of a configuration is measured by maxj6 DR(j). We provide a complete analysis for NE and LPT
configurations and any m  3. Once again, we find that LPT provides a strictly better
performance guarantee compared to general NE: the cost of any job in an LPT schedule
cannot increase by a factor 32 or larger, while it can increase by a factor arbitrarily close to
2 for NE schedules.
Theorem 5.1 For any m, DR max (s) < 2 for every NE configuration s.
Proof: Let s0 = (s0 , s ) be a profitable deviation, and let M1 be the most loaded machine
among the machines that either a job migrated from or a job migrated into. By Observation 2.4, there must be a job that migrated out of M1 . This implies that there must be at
least two jobs on M1 in s, since if there were a single job, it could not benefit from any
deviation. Therefore, there exists a job j such that sj = M1 and pj  L1 /2. Using the fact
that s is an NE once again, we get that for any machine i 6= 1, Li  L1 /2 (otherwise job j
can improve by unilaterally migrating to Mi ).
In addition, for every machine i to which a job migrates, it must hold that L0i < L1 . This
is because a job that migrated to Mi left some machine j with load Lj  L1 . Combining

399

fiFeldman & Tamir

the above bounds, we get that for every job j that stays on some machine i to which a job
0
migrates it holds that DRs,s (j) = L0i /Li < L1 /Li  (2Li )/Li = 2.

The above analysis is tight as shown in Figure 3: The damage ratio of the jobs of load
2r  1 is (4r  1)/(2r), which can be arbitrarily close to 2. Formally,
Theorem 5.2 For any m  3, there exists an NE configuration s such that DR max (s) =
2   for an arbitrarily small  > 0.
For LPT configurations we obtain a smaller bound:
Theorem 5.3 For any m, DR max (s) <

3
2

for every LPT configuration s.

Proof: Let s0 = (s0 , s ) be a profitable deviation, and let M1 be the most loaded machine
among the machines that either a job migrated from or a job migrated into. Since every
LPT configuration is an NE, M1 must have at least two jobs (following the same arguments
as in the proof of Theorem 5.1). Assume w.l.o.g that the lightest (also last) job assigned
to M1 has load 1, and denote this job the 1-job. This assumption is valid because the
minimum improvement ratio is invariant to linear transformations. Let ` = L1  1. Since s
is an LPT configuration, for every machine i, it must hold that Li  ` (otherwise, the 1-job
would have been assigned to a different machine). In addition, since for every machine j
from which a job migrates, Lj  L1 , it must hold that for every machine i to which a job
migrates L0i < ` + 1. We distinguish between two cases.
3
case (a): `  2. Then, for every machine Mi to which a job migrates, L0i /Li < `+1
`  2.
case (b): ` < 2. We show that no profitable deviation exists in this case. If ` < 2, then
M1 has exactly 2 jobs, of loads ` and 1, since LPT assigns the jobs in a non-increasing order.
By LPT, every other machine must have (i) one job of load at least ` (and possibly other
small jobs), or (ii) two jobs of load at least 1 (and possible other small jobs). Let k and k 0
be the number of machines of type (i) and (ii), respectively (excluding M1 ). Thus, there is
a total of k + 1 jobs of load ` and 2k 0 + 1 jobs of load 1. After the deviation, no machine can
have jobs of load ` and 1 together, nor can it have three jobs of load 1. The k + 1 machines
assigned with the k + 1 jobs of load ` after the deviation cannot be assigned any other job
of load x. So, we end up with 2k 0 + 1 jobs of load 1 that should be assigned to k 0 machines.
Thus, there must be a machine with at least three jobs of load 1. Contradiction.

M3

1+2

1+

2+3

M3

1+2

1

M2

1+2

1+

2+3

M2

1+2

1+3

M1

1+3

1

3+3

M1

1+

1

(a)

1+

1

3+2
2+5
2+2

(b)

Figure 5: An LPT-originated deviation, in which the damage ratio of the job of load 1 + 2 on M3
is arbitrarily close to 32 .

The above analysis is tight as shown in Figure 5. Moreover, by adding dummy machines
and jobs it can be extended to any m  3. Formally,
400

fiApproximate Strong Equilibrium

Theorem 5.4 For any m  3, there exists an LPT configuration s such that DR max (s) =
3
2   for an arbitrarily small  > 0.

6. Approximation Scheme
In this section we present a polynomial time approximation scheme that provides a (1 + )SE. The PTAS can be applied to any fixed number of machines.
Definition 6.1 A vector (l1 , l2 , . . . lm ) is smaller than (l1 , l2 , . . . lm ) lexicographically if for
some i, li < li and for all b < i, lb = lb . A configuration s is lexicographically smaller than
s if the vector of machine loads L(s) = (L1 (s), . . . , Lm (s)), sorted in non increasing order,
is smaller lexicographically than L(s), sorted in non increasing order.
The PTAS combines a lexicographically minimal assignment of the longest k jobs with
the LPT rule applied to the remaining jobs. The value of k depends on the desired approximation ratio (to be defined later).
Formally, the algorithm Ak is defined as follows:
1. Find a lexicographically minimal assignment of the longest k jobs.
2. Add the remaining jobs greedily using the LPT rule.
In particular, since a lexicographically minimal assignment minimizes the makespan
(given by the load on the most loaded machine), the above algorithm is a PTAS for the
minimum makespan problem, as it is a restriction of the known PTAS of Graham (1966).
In Grahams algorithm, in step 1, the first k jobs are scheduled in a way that minimizes the
makespan. In our scheme, the requirement on the schedule of the long jobs is more strict.
In particular, as shown by Andelman et al. (2007), the schedule of the longest k jobs is an
SE of this sub-instance.
 
Given , let A denote the above algorithm with k = m
 . We first show that for any
subset of machines M  M , A provides a (1 + )-approximation to the makespan of the
subset of jobs scheduled on M . Formally,
Lemma 6.2 Let I = hN, M i be an instance of job scheduling with machines M and jobs
N . Let s be an output of A on I. For a given M  M , let N  N be the set of jobs
scheduled on M in s. Consider the instance I = hN , M i. Let s be the assignment of I

induced by s. Then s is (1 + )-approximation for the makespan of I.

Proof: Let LA
max (M ) denote the largest completion time of a machine in a set M in the
schedule produced by A , and let OP T (I) denote the minimum makespan of I. Let T
denote the largest completion time of a long job in N scheduled on M in the minimal
lexicographic schedule found in step 1. Since s is a minimal lexicographic assignment, T is

the minimum makespan of the long jobs of N . In particular, T is a lower bound for OP T (I),
A

thus, if the makespan on M is not increased in the second step, that is, Lmax (M ) = T ,
 Otherwise, the makespan of M is larger than T . Let j be the job
then A is optimal for I.
determining the makespan of M (the job who completes last in N ). By definition of LPT,
this implies that all the machines M were busy when job j started its execution (otherwise

401

fiFeldman & Tamir

job j could start earlier). Since the optimal schedule from step 1 has no intended idles, it

holds that all the machines M are busy during the time interval [0; LA
max (M )  pj ]
Pn
Let P =
j=1 pj be the total processing time of the n jobs in N . By the above,
A

P  m(Lmax (M )  pj ). Also, since the jobs are sorted in non-increasing order of processing

times, we have that pj  pk+1 and therefore P  m(LA
max (M )  pk+1 ). A lower bound for

the optimal solution of I is a schedule in which the load on the m machines is balanced;


  P /m, which implies that LA
thus OP T (I)
max (M )  OP T (I) + pk+1 .
A


In order to bound Lmax (M ) in terms of OP T (I), we need to bound pk+1 in terms of
 We first bound the gap between OP T (I) and OP T (I).
 The following assumption
OP T (I).
is used.
Claim 6.3 Let z be the job determining the makespan of A (I). W.l.o.g., z is not one of
the k long jobs.
Proof: Assume that the makespan of A (I) is determined by one of the long jobs. Let M1
be the machine on which z is scheduled. In particular, M1 processes only long jobs. Fix the
schedule on M1 and repeat the PTAS for the remaining jobs and machines with the same
value of k. Repeat if necessary until the makespan is determined by a job assigned using
the LPT rule.
Note that the above algorithm is still polynomial, as the PTAS might be repeated at
most m  1 times, which is a constant. The approximation ratio is improving for any subinstance: same number of jobs are considered long, but among a set of fewer jobs, that is,
a larger portion of the jobs is scheduled optimally, therefore the approximation ratio proof
is valid for the sub-instance. Finally, by merging the last PTAS result with the schedule on
the machines holding long jobs only, we get a PTAS for the whole instance, since the long
jobs were scheduled optimally in each step. Moreover, the load on each such machine is a
lower bound on the makespan of the sub-instance that was considered when the machine
gets these jobs.

 + pk+1 .
Claim 6.4 OP T (I)  OP T (I)
Proof: Let z be the job determining the makespan of A (I). By Claim 6.3, z can be assumed

to be assigned in step 2 (by LPT rule). If z  N then A (I) = LA
max (M ). Else, the load on
any machine in M is at least A (I)  pz , since otherwise job z should have been assigned
to one of the machines in M . Therefore, even if the total load of N is balanced among
  A (I)  pz . Since pz  pk+1 , and OP T (I)  A (I), we get
M , we have that OP T (I)
 + pk+1 .
OP T (I)  A (I)  OP T (I)


Claim 6.5 pk+1  OP T (I)

m
k .

Proof: Consider the k + 1 longest jobs. In an optimal schedule, some machine is assigned at
least d(k + 1)/me  1 + bk/mc of these jobs. Since each of these jobs has processing
time at least pk+1 , we conclude that OP T (I)  (1 + bk/mc)pk+1 , which implies that

402

fiApproximate Strong Equilibrium

0
pk+1  OP T (I)/(1 + bk/mc). By Claim 6.4, pk+1 
 mOP
 T (I)/(1 + bk/mc)  (OP T (I ) +

pk+1 )/(1 + bk/mc). It follows that pk+1  OP T (I)

k .
A
A





Back to the
on Lmax (I), we can now conclude that Lmax (I)  OP T (I) + pk+1 
 mbound


 + ).
OP T (I)(1 + k ) = OP T (I)(1

We can now prove the main result of this section, showing that the schedule s produced
by A , is a (1 + )-SE. The stability is proved in the following theorem. As for the running
time, for fixed m, k, a minimal lexicographic schedule of the first k jobs can be found in
O(mk ) steps. Applying the LPT rule takes additional O(nlogn). For A , we get that the
running time of the scheme is O(mm/ ), that is, exponential in m (that is assumed to be
constant) and 1/.

Theorem 6.6 A produces an (1 + )-SE.
Proof: The proof is similar to the proof of Lemma 3.8. Assume for contradiction that there
exists an instance I for A on m machines, such that in the schedule of I produced by
A , there exists a coalition in which the improvement ratio of every member is larger than
1 + . Let  be such a coalition of minimum size. For every machine from which a job
j migrates, there must exists a job migrating to it, otherwise,  \ {j} is also a coalition
having IR min > 1 + , in contradiction to the minimality of . Let M denote the set of
machines that are part of the coalition, let N  N be the set jobs assigned to M by A ,
and let m = |M |. Consider the instance I = hN , M i, and the schedule s  s. By Lemma
 The coalition  exists in s, and all
6.2, s is a (1 + )-approximation to the makespan of I.
the machines M take part in it. Moreover, each of the jobs in  improves by a factor of
more than (1 + ). In other words, for any pair of machines i, j, such that Pi,j = 1, we have
Li /L0j > 1 + . On the other hand, since s is a (1 + )-approximation, for any machine i,

T (I)
 Therefore, if Pi,j = 1 then 1 +  < L0i  (1+)OP
. In other words,
Li  (1 + )OP T (I).
0
Lj

Lj


for any machine j that receives at least one job,
< OP T (I).
However, since at least one job has migrated to each of the m participating machines,
after the deviation the machines M are assigned all the jobs of N and they all have load
 A contradiction.
less than OP T (I).

We note that for any   0, the schedule produced by algorithm A is an NE. Similar
to the stability proof of LPT (Fotakis et al., 2002), it is easy to verify that if some job
can benefit from leaving some machine Mi then also the shortest job on this machine can
benefit from the same migration. However, independent of whether this short job, of length
pj , is assigned in step 1 of the algorithm (as part of a minimal lexicographical schedule of
the long job) or in step 2 (by LPT), the gap between Li and the load on any other machine
is at most pj .
L0j

7. Computational Complexity
It is easy to see that one can determine in polynomial time whether a given configuration
is an NE. Yet, for SE, this task is more involved. In this section, we provide some hardness
results about coalitional deviations.
Theorem 7.1 Given an NE schedule on m  3 identical machines, it is NP-hard to determine if it is an SE.
403

fiFeldman & Tamir

M3

B-1

B-2

2B-3

M3

B-1

Jobs of A1

2B-1

M2

B-1

B-2

2B-3

M2

B-1

Jobs of A2

2B-1

2B

M1

B-2

2B-4

M1

Jobs of A

B-2
(b)

(a)

Figure 6: Partition induces a coalition in a schedule on identical machines.

Proof: We give a reduction from Partition. Given a set A of n integers a1 , . . . , an with
total size 2B, and the question whether there is a subset of total size B, construct the
schedule in Figure 6(a). In this schedule on three machines there are n + 4 jobs of loads
a1 , . . . , an , B  2, B  2, B  1, B  1. We assume w.l.o.g. that mini ai  3, else the whole
instance can be scaled. Thus, schedule 6(a) is an NE. For m  3, add m  3 machines each
with a single job of load 2B.
Claim 7.2 The NE schedule in Figure 6(a) is an SE if and only if there is no partition.
Proof: If there is a partition into K1 , K2 , each having total size B, then the schedule in
Figure 6(b) is better for the jobs originated from the partition instance and for the two
(B  2)-jobs. All the partition jobs improved from cost 2B to cost 2B  1, and the (B  2)jobs improved from 2B  3 to 2B  4.
Next, we show that if there is no partition then the initial schedule is an SE. By Theorem 2.7, in any action of a coalition on three machines, jobs must migrate to M1 from both
M2 and M3 . In order to decrease the load from 2B  3, the set of jobs migrating to M1
must be the set of two jobs of load B  2. Also, it must be that all the partition jobs move
away from M1 - otherwise, the total load on M1 will be at least 2B  4 + 3 = 2B  1, which
is not an improvement for the (B  2)-jobs. This implies that the jobs of M1 split between
M2 and M3 . However, since there is no partition, one of the two subsets is of total load at
least B + 1. These jobs will join a job of load B  1 to get a total load of at least 2B, which
is not an improvement over the 2B-load in the initial schedule.

This establishes the proof of the Theorem.

A direct corollary of the above proof is the following:
Corollary 7.3 Given an NE schedule and a coalition, it is NP-hard to determine whether
the coalition can deviate.
Theorem 7.1 holds for any m  3 identical machines. For m  2, a configuration is an
NE if and only if it is an SE (Andelman et al., 2007), and therefore it is possible to determine
whether a given configuration is SE in polynomial time. Yet, the following theorem shows
that for the case of unrelated machines, the problem is NP-hard already for m = 2. In the
unrelated machines environment, the processing time of a job depends on the machine on
which it is assigned. For every job j and machine i, pi,j denotes the processing time of job
j if processed by machine i.
Theorem 7.4 Given an NE schedule on m  2 unrelated machines, it is NP-hard to
determine if it is an SE.
404

fiApproximate Strong Equilibrium

Proof: We give a reduction from Partition. Given n integers a1 , . . . , an with total size
2B, and the question whether there is a subset of total size B, construct the following
instance for scheduling: there are 2 machines and n + 1 jobs with the following loads (for
 < 1/(n  1)):
pi,1 = ai +  and pi,2 = 2ai + , i  {1, . . . , n} ; pn+1,1 = B, and pn+1,2 = 2B + n.
Consider the schedule in which all the jobs 1, . . . , n are on M1 , and job n + 1 is on M2 . The
completion times of both machines are are 2B + n. It is an NE.

M2

Jn+1

2B+n

M2

M1

J1,,Jn

2B+n

M1

(a)

2B+|A2|

Jobs of A2
Jobs of A1

Jn+1

2B+|A1|

(b)

Figure 7: Partition induces a coalition in a schedule on related machines.

Claim 7.5 The NE schedule in Figure 7(a) is an SE if and only if there is no partition.
Proof: If there is a partition into A1 , A2 , each having total size B, then the schedule given
in Figure 7(b) is better for everyone. The completion time of M1 is 2B + |A1 | < 2B + n
and the completion time of M2 is 2B + |A2 | < 2B + n.
Next, we show that if there is no partition then the initial schedule is an SE. Since there
is no partition, in any partition into A1 , A2 , one of the two subsets, w.l.o.g., A1 has total
size at least B + 1. A1 will only increase its load by migrating to M2 even alone (bearing
a load of at least 2B + 2 + |A1 | instead of 2B + n). Therefore, A1 will not leave M1 .
However, if A1 stays at M1 , job n + 1 is better-off staying at M2 (since if it migrates, it
bears a load of at least 2B + 1 + |A1 | which is not smaller than 2B + n for any |A1 | and
  1/(n  1)).

This establishes the proof of the Theorem.

A direct corollary of the above proof is the following:
Corollary 7.6 Given an NE schedule on unrelated machines and a coalition, it is NP-hard
to determine whether the coalition can deviate.

8. Conclusions and Open Problems
In this paper we study how well NE schedules and a special subset of them  those obtained
as an outcome of the LPT assignment rule  approximate SE in job scheduling games. We
do so using the two measures IR min and IR max . In addition, we use the DR max measure to
study how hurtful coalitional deviations can be to agents outside the coalition. We present
upper and lower bounds for NE and LPT-based schedules, and demonstrate that LPTbased schedules perform better than general NE schedules, where the gap is more significant
under the IR max measure. For both NE and LPT, IR min is bounded by a small constant,
405

fiFeldman & Tamir

implying some notion of stability against coalitional deviations (assuming the existence of
a transition cost). As for IR max , it is bounded by a constant for LPT schedules, but there
is no universal bound for NE schedules. Yet, LPT is not the best possible approximation
to SE, as demonstrated by the SE-PTAS we design, which computes a schedule with IR min
arbitrarily close to 1.
Some of the problems that remain open are:
1. For the IR min measure, there is a gap between the upper and lower bounds for m > 4 6 .
1
2. For IR max of LPT-originated deviations, and m  3 we presented a lower bound of 2 m
5
and a matching upper bound of 3 for m = 3. Closing this gap for a general m is left as an
open problem.
3. This paper focuses on the case of identical machines. It would be interesting to study the
topic of approximate strong equilibrium in additional job scheduling settings. In particular,
the setting of uniformly related machines is part of our ongoing research, where already
the case of two machines seems rather involved. Note that, as mentioned in Section 7, for
unrelated machines, IR min is unbounded already for two machines.
4. Our measures are defined with respect to the strong equilibrium solution concept, where
a profitable deviation is defined as one in which every member of the coalition strictly
benefits. It would be interesting to consider the measures we introduce here with respect
to additional solution concepts, such as coalition-proof Nash equilibrium (Bernheim et al.,
1987) (which is stable against profitable deviations that are themselves stable against further
deviations of sub-coalitions), and also with respect to profitable deviations in which none
of the coalition members is worse-off and at least one member is strictly better-off.
In summary, we introduced three general measures for the stability and performance
of schedules under coalitional deviations. We believe that these measures can be used
to measure the stability and performance of various algorithms to coalitional deviations
and their performance in additional settings and games. We hope to see more work that
makes use of these measures within the framework of algorithmic game theory. It would be
interesting to study in what families of games Nash equilibria approximate strong equilibria
as defined by the measures introduced here.
Acknowledgments. We thank Leah Epstein and Alon Rosen for helpful discussions. We
also thank the anonymous reviewers for their insightful remarks and suggestions. The work
was partially supported by the Israel Science Foundation (grant number 1219/09).

Appendix A. Bounding IR min and IR max in LPT-originated Deviations
We first provide several observation that are valid for any LPT-originated deviation. This
observations will be used later in the analysis. Moreover, the observations characterize the
coalitions that might exist in schedules produced by the LPT-rule. Combined with the
flower structure (that characterizes all NE-originated deviations on three machines), we
get that the set of LPT-originated deviation are very limited and must follow a very strict
structure.
Let M1 be the most loaded machine. Assume w.l.o.g that the lightest (also last) job
assigned to M1 has load 1, and denote this job the 1-job. This assumption is valid because
6. Our paper provides tight bounds for m = 3 and the case of m = 4 is considered by Chen (2009).

406

fiApproximate Strong Equilibrium

the minimum improvement ratio is invariant to linear transformations. For i = 2, 3, denote
by Ki the set (and also the total load) of jobs that remain on Mi . Denote by Hi,j the set
(and also total load) of jobs migrating from Mi to Mj . For i = 1, we let K1 , H1,2 , H1,3 be
as above, but excluding the 1-job.
The next propositions show that the total size of jobs migrating from M2 , M3 to M1
and remaining on M2 , M3 is at least as large as the last job on M1 .
Proposition A.1 Each of H2,1 , H3,1 is at least 1.
Proof: We show that H2,1  1, the proof for H3,1 is analogous. Assume for contradiction
that H2,1 < 1. Since LPT schedule the jobs in non-increasing order, all jobs composing
H2,1 were assigned after the 1-job. Therefore, when the 1-job is assigned, the load on M2 is
at most K2 and at least H1,2 + H1,3 + K1 (else, LPT would assign the 1-job to M2 ). Thus,
K2  H1,2 + H1,3 + K1 . By the flower structure, some job is migrating from M1 to M2 .
Such a migration is beneficial only if L02 < L1 . Distinguish between two cases:
1. The 1-job migrates to M2 . In this case, L02 = K2 + H1,2 + 1. Therefore, K2 +
H1,2 + 1 < H1,2 + H1,3 + K1 + 1, or K2 < H1,3 + K1 . However, by the above,
K2  H1,2 + H1,3 + K1  H1,3 + K1 . A contradiction.
2. The 1-job does not migrate to M2 . In this case, L02 = K2 + H1,2 . Therefore, K2 +
H1,2 < H1,2 + H1,3 + K1 + 1, or K2 < H1,3 + K1 + 1. However, by the above,
K2  H1,2 + H1,3 + K1  1 + H1,3 + K1 . A contradiction. The last inequality follows
from the fact that H1,2 is not empty and consists of at least one job at least as big as
the smallest job on M1 .

Proposition A.2 Each of K2 , K3 is at least 1.
Proof: We first show K2  1. Assume K2 < 1, it means that when the 1-job is assigned
to M1 , the load on M2 is composed of jobs that are a subset of H2,1 only. Therefore, by
the LPT rule, H2,1  K1 + H1,2 + H1,3 . However, by Proposition 2.8, L02 > L2 , therefore
H2,1 < H1,2 + 1. Thus, K1 + H1,3 < 1. However, H1,3  1. A contradiction. To show
K3  1, note that if K3 > 1 then by a similar argument to the above H3,1  H1,2 + H1,3 . By
Proposition 2.8, L03 > L3 . Therefore H1,3 > H3,1 , implying K1 + H1,2 < 0. A contradiction.



Theorem 3.4 Any LPT configuration on three machines is a ( 21 + 46  1.1123)-SE.
Proof: Let M1 be the most loaded machine in the schedule. Recall that the lightest (also
last) job assigned to M1 is a 1-job having load 1. Let ` = L1  1. For a give LPT schedule
s and a deviation s0 = (s0 , s ), let r = IR min (s, s0 ).
By Claim 2.6,  obeys the flower structure. Therefore:
(i) r  L2 /L01 ; (ii) r 
P
0
0
0
L3 /L1 ; (iii) r  L1 /L2 ; and (iv) r  L1 /L3 . Let P = j pj , Clearly, P = L1 + L2 + L3 =
L01 + L02 + L03 . Summing up (i) and (ii), we get
L01 

L2 + L3
P  (` + 1)
=
.
2r
2r
407

(1)

fiFeldman & Tamir

By LPT, L2 , L3  `, thus P  3` + 1. Summing up (iii) and (iv), and using Equation (1)
we get
r

2L1
2(` + 1)
2(` + 1)
=

0
0
1
+ L3
P  L1
P (1  2r
)+

L02

Implying,
r(3` + 1) 

`+1
2r



2(` + 1)
1
(3` + 1)(1  2r
)+

`+1
2r

.

3` + 1 ` + 1
+
 2` + 2
2
2

and,
r

3` + 2
.
3` + 1

(2)

Case 1: `  3. In this case, Equation (2) implies r  1.1.
Case 2: ` < 3. This case requires a closer analysis. Let I be an instance for which LPT
creates a schedule with a deviation s0 = (s0 , s ) achieving the maximal IR min and ` < 3.
For i = 2, 3, denote by Hi the total load of jobs migrating from Mi to M1 , and by Ki the
total load of jobs that remain on Mi . By the flower structure, L01  H2 + H3 , therefore
H2 < K3 and H3 < K2 , else it would not be beneficial for the jobs composing H2 , H3 to
join the coalition. By Propositions A.1 and A.2, each of H2 , H3 , K2 , K3 is at least 1.
Claim A.3 The load ` on M1 is incurred by exactly two jobs.
Proof: Clearly, since we consider the case ` < 3 and the lightest job on M1 has load 1, the
load ` is incurred by at most two jobs. Assume for contradiction that ` consists of a single
job. Then, there are exactly two jobs on M1 , of loads ` and 1. By the flower structure, the
`-job must join the coalition. W.l.o.g assume it migrates to M2 . This migration is profitable
only if K2 < 1, contradicting Proposition A.2.

Therefore, we can assume w.l.o.g that in the instance achieving the maximal IR min , M1
is assigned three jobs of loads 1 + , 1 + , 1, for ,   0.
Having ` = 2 +  + , the bound in Equation (2) implies
r

8 + 3( + )
3` + 2
=
.
3` + 1
7 + 3( + )

(3)

Consider first the case in which one of the two big jobs on M1 does not migrate away
from M1 . We show that no coalition deviation is beneficial in this case. W.l.o.g, assume
that the job of length 1 +  remains on M1 and the job of length 1 +  migrates to M2 .
The migration of 1 +  is profitable only if K2 < 2 + . On the other hand, the migration
of the jobs migrating from M2 to M1 is profitable only if K2 > H3 + 1 +   2 + . A
contradiction.
Consider next the case in which the 1-job does not migrate away from M1 . W.l.o.g,
assume that the job of length 1 +  migrates to M2 and the job of length 1 +  migrates to
M3 . In order to bound r according to Equation (3), we find a lower bound for ( + ). By
Equation (1),
2r 

L2 + L3
K2 + H2 + K3 + H3
K2 + K3 + 2
6++
=


.
0
L1
1 + H2 + H3
3
3
408

(4)

fiApproximate Strong Equilibrium

The third inequality is due to the fact that the ratio is decreasing with H2 + H3 , which is
known to be at least 2 by Proposition A.1. The last inequality is since the migrations are
beneficial for the jobs leaving M1 , that is, K2 < 2 + , and K3 < 2 + .
Equation (4) implies 6r < 6 +  +  or  +  > 6r  6. Next, we apply this bound on
 +  into Equation (3) and obtain
r<

18r  10
.
18r  11



6
1
This implies r < 10
9 < 2 + 4 .
The case we did not analyze yet is the one in which all three jobs assigned to M1 migrate
away from M1 in the deviation. Assume w.l.o.g that the jobs of size 1+ and 1 are migrating
to M2 and the job of size 1 +  is migrating to M3 . Clearly, the jobs of size 1 + , 1 +  do
not migrate to the same machine because they are currently assigned with additional load
of 1 while both K2 and K3 are at least 1, by Proposition A.2. Figure 8 shows the schedule
before the migration (Figure 8(a)) and after the migration (Figure 8(b)).

M3

K3

H3

M2

K2

H2

M1

1+

1+

l

1

K3+H3

M3

K2+H2

M2

K2

1+

3+ + 

M1

H2

H3

K3

1+

K3+1+ 
1

K2+2+ 
H2+H 3

(b)

(a)

Figure 8: An LPT coalition achieving maximal IR min .
Next, we find a lower bound for  + . Considering the migration from M1 to M2 ,
we know that r  (3 +  + )/(2 +  + K2 ). Therefore  +   2r + r + K2 r  3 
2r + rK2  3 (because   0). Considering the migration from M2 to M1 , we know that
r  (K2 + H2 )/(H2 + H3 ). Therefore, K2  H2 (r  1) + H3 r. LPT assigns the 1-job
on M1 with load 2 +  + , while the load on M2 at that time was at most K2 + H2 .
Therefore 2 +  +   K2 + H2 , implying H2  2 +  +   K2 . Also, by Proposition A.1,
H3  1. We can now use these bounds on H2 , H3 to get improved bound on K2 . Specifically,
K2  (2 +  +   K2 )(r  1) + r. This implies K2 r  3r + r( + )  (2 + ( + )). Back
to the bound of  + , we now have  +   2r + 3r + r( + )  (2 + ( + ))  3. Thus,
 +   (5r  5)/(2  r). Note that (2  r) is positive since by Theorem 3.1, r < 5/4.
Finally, we apply this bound on  +  into Equation (3) and obtain
r

1 + 7r
8 + 3(5r  5)/(2  r)
=
.
7 + 3(5r  5)/(2  r)
1 + 8r



This implies r  12 + 46 .

The above bound is tight. Specifically,

Theorem 3.5 For any m  3, there exists an LPT schedule s such that IR min (s) = 12 + 46 .

6
1
+
2
4 , and consider Figure 8,
r(3+)2
, H2 = 2 +   K2 , K3 = 1
r


10+5
 6,
6 6

Proof: Let r =

where we substitute  =

K2 =

+ , and H3 = 1 (the instance with the

409

 = 0,

fiFeldman & Tamir

rounded values appears in Figure 9). It is easy to verify that all three jobs leaving M1 have
improvement ratio of exactly r = 12 + 46 , and the same holds for the two jobs migrating


to M1 . Thus, in this instance IR min = 12 + 46 . Moreover, this lower bound can be easily
extended to any m > 3 by adding dummy jobs and machines. Thus,
M3

1.633

1

2.633

M3

M2

1.367

1.266

2.633

M2

1.266

M1

1.633

1

3.633

M1

1.367

1

1.633
1

1.633

3.266

1

3.266

1

2.367

(b)

(a)

Figure 9: An LPT-originated
deviation on three machines in which all migrating jobs im
6
1
prove by 2 + 4 .


Theorem 4.2 Let s be an LPT schedule on three machines. It holds that IR max (s)  53 .
Proof: Let M1 be the most loaded machine. Recall that the lightest (also last) job assigned
to M1 is a 1-job having load 1. For i = 2, 3, Ki is the set (and also the total load) of jobs
that remain on Mi , and Hi,j is the set (and also total load) of jobs migrating from Mi to
Mj . For i = 1, we let K1 , H1,2 , H1,3 be as above, but excluding the 1-job.
The 1-job is assigned to M1 by LPT, meaning that the load on M2 and M3 is at least
K1 + H1,2 + H1,3 at that time. Since the load on M2 , M3 could only increase after the time
the 1-job is assigned, we get that
K1 + H1,2 + H1,3  K2 + H2,1

and

K1 + H1,2 + H1,3  K3 + H3,1 .

(5)

Therefore (sum up the two):
2(K1 + H1,2 + H1,3 )  K2 + K3 + H2,1 + H3,1 .

(6)

Distinguish between two cases:
(i) The 1-job remains on M1 . In This case, L1 = K1 + H1,2 + H1,3 + 1; L2 =
K2 +H2,1 ; L3 = K3 +H3,1 , while after the coalition is active L01 = K1 +H2,1 +H3,1 +1; L02 =
K2 + H1,2 ; L03 = K3 + H1,3 .
Since the jobs in H1,2 and H1,3 are part of the coalition, L02 + L03 < 2L1 . Deducing
H1,2 and H1,3 from both sides we get K2 + K3 < H1,2 + H1,3 + 2K1 + 2. Combining with
Equation 6, we get:
H1,2 + H1,3 < H2,1 + H3,1 + 2.
(7)
By Proposition A.1, each of H2,1 , H3,1 , K2 , K3 is at least 1. By Proposition 2.9, the
improvement ratio of the 1-job, which equals L1 /L01 , is the largest among the coalition.
This ratio can now be bounded as follows:
K1 + H1,2 + H1,3 + 1
K1 + H2,1 + H3,1 + 3
L1
5
=
<
 .
0
L1
K1 + H2,1 + H3,1 + 1
K1 + H2,1 + H3,1 + 1
3
410

fiApproximate Strong Equilibrium

The left inequality follows from Equation 7. The right one follow from Proposition A.1 and
from the fact that K1 might be empty.
(ii) The 1-job leaves M1 . We assume w.l.o.g that the 1-job moves to M2 . In This
case, L1 = K1 + H1,2 + H1,3 + 1; L2 = K2 + H2,1 ; L3 = K3 + H3,1 , while after the coalition
is active L01 = K1 + H2,1 + H3,1 ; L02 = K2 + H1,2 + 1; L03 = K3 + H1,3 .
Since the jobs in H1,2 and H1,3 are part of the coalition, L02 + L03 < 2L1 . Deducing
1, H1,2 and H1,3 from both sides we get K2 + K3 < H1,2 + H1,3 + 2K1 + 1. Combining with
Equation 6, we get:
H1,2 + H1,3 < H2,1 + H3,1 + 1.
(8)
By Propositions A.1 A.2, each of H2,1 , H3,1 , K2 , K3 is at least 1. If K1 is not empty
then the jobs of K1 have improvement ratio L1 /L01 which is, by Proposition 2.9, the largest
ratio among the coalition. This ratio can now be bounded as follows:
K1 + H1,2 + H1,3 + 1
K1 + H2,1 + H3,1 + 2
L1
5
=

< .
0
L1
K1 + H2,1 + H3,1
K1 + H2,1 + H3,1
3
The left inequality follows from Equation 8. The right one follows from Proposition A.1,
and from the fact that K1 is not empty and includes at least one job of load at least 1.
If K1 is empty, then as we show below, the maximal improvement ratio is less than 3/2.
We bound separately the improvement ratio of H1,2 , H1,3 , and Hi,1 (i  {1, 2}). Denote
by ri,j the IR of jobs moving from Mi to Mj . In addition to Equations 5 and 8, and to
Propositions A.1 and A.2, we also use below Proposition 2.8. Specifically, H2,1 < H1,2 + 1
and H3,1 < H1,3 . Finally, bear in mind that K1 = .
r1,2 =

H1,2 + H1,3 + 1
K2 + H2,1 + 1
K2 + H2,1 + 1
3
L1
=

<
< .
0
L2
K2 + H1,2 + 1
K2 + H1,2 + 1
K2 + H2,1
2

r1,3 =

H1,2 + H1,3 + 1
K3 + H3,1 + 1
K3 + H3,1 + 1
3
L1
=

<
< .
0
L3
K3 + H1,3
K3 + H1,3
K3 + H3,1
2

ri,1 =

Ki + Hi,1
H1,2 + H1,3
H2,1 + H3,1 + 1
Li
3
=
<
<
< .
0
L1
H2,1 + H3,1
H2,1 + H3,1
H2,1 + H3,1
2


Appendix B. List Scheduling
List Scheduling (LS) is a greedy scheduling algorithms in which the jobs are assigned to the
machines in arbitrary order, but similar to LPT, each job is assigned to the least loaded
1
machine at the time of assignment. LS is known to provide a (2  m
)-approximation to
the minimum makespan (Graham, 1966). While LS does not depart qualitatively from
LPT with respect to makespan approximation (i.e., both provide a constant approximation
to the optimal makespan), they are qualitatively different with respect to game theoretic
properties. First, LS does not necessarily produce an NE. Moreover, as we next show, LS
performs poorly with respect to the measures introduced in this paper.
The improvement ratio of a job is not bounded even if the coalition consists of a single
job. Consider for example an instance with 2 machines and jobs of lengths 1, 1, X (in that
411

fiFeldman & Tamir

order) such that X > 1. LS will produce a schedule with loads 1, 1 + X. The job of length
1 scheduled with the long job can migrate and join the other short job. Its improvement
ratio is 1 + X/2 which can be arbitrarily large.
The damage ratio of a deviation from an LS schedule is not bounded either. Consider
an instance with three machines and jobs of lengths {1  2, 1  , 1, 2, X, 2, 3}. It is easy
to verify that in the resulting LS-configuration, there exists a coalition in which the job of
length X migrates. Since X can be arbitrarily large, the damage ratio of the job in the
machine into which X migrates is arbitrarily large. We note that the damage ratio caused
by a deviation of a single job is at most 2. To see this, consider an LS configuration and
assume that a job j of length pj migrates from M1 to M2 . Denote by Bj , Aj the total
load of jobs on M1 that were assigned before and after j respectively. If Aj = 0 (j is last)
then it is not beneficial for j to migrate (Bj < L2 , else j should have been assigned to
M2 ). Else, the first job after j was assigned to M1 because Bj + pj was less than the load
at that time on M2 . Therefore L2  Bj + pj , and in particular pj  L2 . The damage
ratio is (L1 + pj )/L1  2. The analysis is tight as can be exemplified by the instance
m = 2, I = {1, 1, X}.

References
A. Fiat, H. Kaplan, M. L., & Olonetsky., S. (2007). Strong price of anarchy for machine
load balancing. In International Colloquium on Automata, Languages and Programming(ICALP).
Albers, S. (2009). On the value of coordination in network design. SIAM J. Comput., 38(6),
22732302.
Albers, S., Elits, S., Even-Dar, E., Mansour, Y., & Roditty, L. (2006). On Nash Equilibria for
a Network Creation Game. In Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA).
Andelman, N., Feldman, M., & Mansour, Y. (2007). Strong Price of Anarchy. In SODA07.
Anshelevich, E., Dasgupta, A., Kleinberg, J. M., Tardos, E., Wexler, T., & Roughgarden, T.
(2004). The price of stability for network design with fair cost allocation.. In FOCS,
pp. 295304.
Anshelevich, E., Dasgupta, A., Tardos, E., Wexler, T., & Roughgarden, T. (2003). Nearoptimal network design with selfish agents. In ACM Symposium on Theory of Computing (STOC).
Aumann, R. (1959). Acceptable Points in General Cooperative n-Person Games. In Contributions to the Theory of Games, Vol. 4.
Awerbuch, B., Azar, Y., Richter, Y., & Tsur, D. (2003). Tradeoffs in Worst-Case Equilibria.
In 1st International Workshop on Approximation and Online Algorithms.
Bernheim, D. B., Peleg, B., & Whinston, M. D. (1987). Coalition-proof nash equilibria: I
concepts. Journal of Economic Theory, 42, 112.
Chen, B. (2009). Equilibria in load balancing games. Acta Mathematica Applicata Sinica,
to appear.
412

fiApproximate Strong Equilibrium

Christodoulou, G., Koutsoupias, E., & Nanavati, A. (2004). Coordination mechanisms. J.
Daz, J. Karhumaki, A. Lepisto, and D. Sannella (Eds.), Automata, Languages and
Pro- gramming, Volume 3142 of Lecture Notes in Computer Science. Berlin: Springer,
pp. 345357.
Christodoulou, G., & Koutsoupias, E. (2005). On the Price of Anarchy and Stability of
Correlated Equilibria of Linear Congestion Games. In Annual European Symposium
on Algorithms (ESA).
Czumaj, A., & Vocking, B. (2002). Tight bounds for worst-case equilibria. In ACM-SIAM
Symposium on Discrete Algorithms (SODA), pp. 413420.
Epstein, A., Feldman, M., & Mansour, Y. (2007). Strong Equilibrium in Cost Sharing
Connection Games. In ACM Conference on Electronic Commerce (ACMEC).
Fabrikant, A., Luthra, A., Maneva, E., Papadimitriou, C., & Shenker, S. (2003). On a
network creation game. In ACM Symposium on Principles of Distriubted Computing
(PODC).
Finn, G., & Horowitz, E. (1979). A linear time approximation algorithm for multiprocessor
scheduling. BIT Numerical Mathematics, 19 (3), 312320.
Fotakis, D., S. Kontogiannis, M. M., & Spiraklis, P. (2002). The Structure and Complexity of Nash Equilibria for a Selfish Routing Game. In International Colloquium on
Automata, Languages and Programming (ICALP), pp. 510519.
Graham, R. (1966). Bounds for certain multiprocessing anomalies. Bell Systems Technical
Journal, 45, 15631581.
Graham, R. (1969). Bounds on multiprocessing timing anomalies. SIAM J. Appl. Math.,
17, 263269.
Halpern, J. Y., & Teague, V. (2004). Rational secret sharing and multiparty computation.
In ACM Symposium on Theory of Computing (STOC), pp. 623632.
Holzman, R., & Law-Yone, N. (1997). Strong equilibrium in congestion games. Games and
Economic Behavior, 21, 85101.
Holzman, R., & Law-Yone, N. (2003). Network structure and strong equilibrium in route
selection games. Mathematical Social Sciences, 46, 193205.
Koutsoupias, E., & Papadimitriou, C. H. (1999). Worst-case equilibria.. In Symposium on
Theoretical Aspects of Computer Science (STACS), pp. 404413.
Lamport, L., Shostak, R., & Pease, M. (1982). The byzantine generals problem. ACM
Trans. Prog. Lang. Sys., 4, 382401.
Leonardi, S., & Sankowski, P. (2007). Network formation games with local coalitions. In
ACM Symposium on Principles of Distriubted Computing (PODC).
Milchtaich, I. (1998). Crowding games are sequentially solvable. International Journal of
Game Theory, 27, 501509.
Papadimitriou, C. H. (2001). Algorithms, games, and the internet. In ACM Symposium on
Theory of Computing (STOC), pp. 749753.

413

fiFeldman & Tamir

Procaccia, A. D., & Rosenschein, J. S. (2006). The communication complexity of coalition
formation among autonomous agents. In Int. Conference on Autonomous Agents and
Multiagent Systems (AAMAS), pp. 505512.
Roughgarden, T., & Tardos, E. (2002). How bad is selfish routing?. Journal of the ACM,
49 (2), 236  259.
Rozenfeld, O., & Tennenholtz, M. (2006). Strong and correlated strong equilibria in monotone congestion games. In Workshop on Internet and Network Economics (WINE).
Schuurman, P., & Vredeveld, T. (2007). Performance guarantees of local search for
multiproces- sor scheduling. INFORMS Journal on Computing, 19(1), 52  63.

414

fiJournal of Artificial Intelligence Research 36 (2009) 129163

Submitted 04/09; published 10/09

Content Modeling Using Latent Permutations
Harr Chen
S.R.K. Branavan
Regina Barzilay
David R. Karger

harr@csail.mit.edu
branavan@csail.mit.edu
regina@csail.mit.edu
karger@csail.mit.edu

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
32 Vassar Street, Cambridge, Massachusetts 02139 USA

Abstract
We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose
a global model in which both topic selection and ordering are biased to be similar across a
collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We
apply our method to three complementary discourse-level tasks: cross-document alignment,
document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements
in performance over previously proposed methods.

1. Introduction
A central problem of discourse analysis is modeling the content structure of a document.
This structure encompasses the topics that are addressed and the order in which these topics
appear across documents in a single domain. Modeling content structure is particularly
germane for domains that exhibit recurrent patterns in content organization, such as news
and encyclopedia articles. These models aim to induce, for example, that articles about
cities typically contain information about History, Economy, and Transportation, and that
descriptions of History usually precede those of Transportation.
Previous work (Barzilay & Lee, 2004; Elsner, Austerweil, & Charniak, 2007) has demonstrated that content models can be learned from raw unannotated text, and are useful in
a variety of text processing tasks such as summarization and information ordering. However, the expressive power of these approaches is limited: by taking a Markovian view on
content structure, they only model local constraints on topic organization. This shortcoming is substantial since many discourse constraints described in the literature are global in
nature (Graesser, Gernsbacher, & Goldman, 2003; Schiffrin, Tannen, & Hamilton, 2001).
In this paper, we introduce a model of content structure that explicitly represents two
important global constraints on topic selection.1 The first constraint posits that each document follows a progression of coherent, nonrecurring topics (Halliday & Hasan, 1976).
Following the example above, this constraint captures the notion that a single topic, such
1. Throughout this paper, we will use topic to refer interchangeably to both the discourse unit and
language model views of a topic.

c
2009
AI Access Foundation. All rights reserved.

fiChen, Branavan, Barzilay, & Karger

as History, is expressed in a contiguous block within the document, rather than spread over
disconnected sections. The second constraint states that documents from the same domain
tend to present similar topics in similar orders (Bartlett, 1932; Wray, 2002). This constraint
guides toward selecting sequences with similar topic ordering, such as placing History before Transportation. While these constraints are not universal across all genres of human
discourse, they are applicable to many important domains, ranging from newspaper text to
product reviews.2
We present a latent topic model over related documents that encodes these discourse
constraints by positing a single distribution over the entirety of a documents content ordering. Specifically, we represent content structure as a permutation over topics. This
naturally enforces the first constraint since a permutation does not allow topic repetition.
To learn the distribution over permutations, we employ the Generalized Mallows Model
(GMM). This model concentrates probability mass on permutations close to a canonical
permutation. Permutations drawn from this distribution are likely to be similar, conforming to the second constraint. A major benefit of the GMM is its compact parameterization
using a set of real-valued dispersion values. These dispersion parameters allow the model
to learn how strongly to bias each documents topic ordering toward the canonical permutation. Furthermore, the number of parameters grows linearly with the number of topics,
thus sidestepping tractability problems typically associated with the large discrete space of
permutations.
We position the GMM within a larger hierarchical Bayesian model that explains how a
set of related documents is generated. For each document, the model posits that a topic
ordering is drawn from the GMM, and that a set of topic frequencies is drawn from a multinomial distribution. Together, these draws specify the documents entire topic structure, in
the form of topic assignments for each textual unit. As with traditional topic models, words
are then drawn from language models indexed by topic. To estimate the model posterior,
we perform Gibbs sampling over the topic structures and GMM dispersion parameters while
analytically integrating out the remaining hidden variables.
We apply our model to three complex document-level tasks. First, in the alignment
task, we aim to discover paragraphs across different documents that share the same topic.
In our experiments, our permutation-based model outperforms the Hidden Topic Markov
Model (Gruber, Rosen-Zvi, & Weiss, 2007) by a wide margin  the gap averaged 28% percentage points in F-score. Second, we consider the segmentation task, where the goal is to
partition each document into a sequence of topically coherent segments. The model yields
an average Pk measure of 0.231, a 7.9% percentage point improvement over a competitive
Bayesian segmentation method that does not take global constraints into account (Eisenstein & Barzilay, 2008). Third, we apply our model to the ordering task, that is, sequencing
a held out set of textual units into a coherent document. As with the previous two applications, the difference between our model and a state-of-the-art baseline is substantial:
our model achieves an average Kendalls  of 0.602, compared to a value of 0.267 for the
HMM-based content model (Barzilay & Lee, 2004).
The success of the permutation-based model in these three complementary tasks demonstrates its flexibility and effectiveness, and attests to the versatility of the general document
2. An example of a domain where the first constraint is violated is dialogue. Texts in such domains follow
the stack structure, allowing topics to recur throughout a conversation (Grosz & Sidner, 1986).

130

fiContent Modeling Using Latent Permutations

structure induced by our model. We find that encoding global ordering constraints into
topic models makes them more suitable for discourse-level analysis, in contrast to the local
decision approaches taken by previous work. Furthermore, in most of our evaluation scenarios, our full model yields significantly better results than its simpler variants that either
use a fixed ordering or are order-agnostic.
The remainder of this paper proceeds as follows. In Section 2, we describe how our approach relates to previous work in both topic modeling and statistical discourse processing.
We provide a problem formulation in Section 3.1 followed by an overview of our content
model in Section 3.2. At the heart of this model is the distribution over topic permutations,
for which we provide background in Section 3.3, before employing it in a formal description
of the models probabilistic generative story in Section 3.4. Section 4 discusses the estimation of the models posterior distribution given example documents using a collapsed Gibbs
sampling procedure. Techniques for applying our model to the three tasks of alignment,
segmentation, and ordering are explained in Section 5. We then evaluate our models performance on each of these tasks in Section 6 before concluding by touching upon directions
for future work in Section 7. Code, data sets, annotations, and the raw outputs of our
experiments are available at http://groups.csail.mit.edu/rbg/code/mallows/.

2. Related Work
We describe two areas of previous work related to our approach. From the algorithmic
perspective our work falls into a broad class of topic models. While earlier work on topic
modeling took the bag of words view of documents, many recent approaches have expanded
topic models to capture some structural constraints. In Section 2.1, we describe these extensions and highlight their differences from our model. On the linguistic side, our work
relates to research on modeling text structure in statistical discourse processing. We summarize this work in Section 2.2, drawing comparisons with the functionality supported by
our model.
2.1 Topic Models
Probabilistic topic models, originally developed in the context of language modeling, have
today become popular for a range of NLP applications, such as text classification and document browsing. Topic models posit that a latent state variable controls the generation of
each word. Their parameters are estimated using approximate inference techniques such as
Gibbs sampling and variational methods. In traditional topic models such as Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004), documents are
treated as bags of words, where each word receives a separate topic assignment and words
assigned to the same topic are drawn from a shared language model.
While the bag of words representation is sufficient for some applications, in many cases
this structure-unaware view is too limited. Previous research has considered extensions of
LDA models in two orthogonal directions, covering both intrasentential and extrasentential
constraints.

131

fiChen, Branavan, Barzilay, & Karger

2.1.1 Modeling Intrasentential Constraints
One promising direction for improving topic models is to augment them with constraints
on topic assignments of adjoining words within sentences. For example, Griffiths, Steyvers,
Blei, and Tenenbaum (2005) propose a model that jointly incorporates both syntactic and
semantic information in a unified generative framework and constrains the syntactic classes
of adjacent words. In their approach, the generation of each word is controlled by two hidden
variables, one specifying a semantic topic and the other specifying a syntactic class. The
syntactic class hidden variables are chained together as a Markov model, whereas semantic
topic assignments are assumed to be independent for every word.
As another example of intrasentential constraints, Wallach (2006) proposes a way to
incorporate word order information, in the form of bigrams, into an LDA-style model. In
this approach, the generation of each word is conditioned on both the previous word and
the topic of the current word, while the word topics themselves are generated from perdocument topic distributions as in LDA. This formulation models text structure at the
level of word transitions, as opposed to the work of Griffiths et al. (2005) where structure
is modeled at the level of hidden syntactic class transitions.
Our focus is on modeling high-level document structure in terms of its semantic content.
As such, our work is complementary to methods that impose structure on intrasentential
units; it should be possible to combine our model with constraints on adjoining words.
2.1.2 Modeling Extrasentential Constraints
Given the intuitive connection between the notion of topic in LDA and the notion of topic in
discourse analysis, it is natural to assume that LDA-like models can be useful for discourselevel tasks such as segmentation and topic classification. This hypothesis motivated research
on models where topic assignment is guided by structural considerations (Purver, Kording,
Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly
relationships between the topics of adjacent textual units. Depending on the application, a
textual unit may be a sentence, paragraph, or speaker utterance. A common property of
these models is that they bias topic assignments to cohere within local segments of text.
Models in this category vary in terms of the mechanisms used to encourage local topic
coherence. For instance, the model of Purver et al. (2006) biases the topic distributions
of adjacent utterances (textual units) to be similar. Their model generates each utterance
from a mixture of topic language models. The parameters of this topic mixture distribution is assumed to follow a type of Markovian transition process  specifically, with high
probability an utterance u will have the same topic distribution as the previous utterance
u  1; otherwise, a new topic distribution is drawn for u. Thus, each textual units topic
distribution only depends on the previous textual unit, controlled by a parameter indicating
whether a new topic distribution is drawn.
In a similar vein, the Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) posits
a generative process where each sentence (textual unit) is assigned a single topic, so that
all of the sentences words are drawn from a single language model. As with the model of
Purver et al., topic transitions between adjacent textual units are modeled in a Markovian
fashion  specifically, sentence i has the same topic as sentence i  1 with high probability,
or receives a new topic assignment drawn from a shared topic multinomial distribution.

132

fiContent Modeling Using Latent Permutations

In both HTMM and our model, the assumption of a single topic per textual unit allows
sections of text to be related across documents by topic. In contrast, Purver et al.s model
is tailored for the task of segmentation, so each utterance is drawn from a mixture of topics.
Thus, their model does not capture how utterances are topically aligned across related
documents. More importantly, both HTMM and the model of Purver et al. are only able
to make local decisions regarding topic transitions, and thus have difficulty respecting longrange discourse constraints such as topic contiguity. Our model instead takes a global view
on topic assignments for all textual units by explicitly generating an entire documents topic
ordering from one joint distribution. As we show later in this paper, this global view yields
significant performance gains.
The recent Multi-Grain Latent Dirichlet Allocation (MGLDA) model (Titov & McDonald, 2008) has also studied topic assignments at the level of sub-document textual units.
In MGLDA, a set of local topic distributions is induced for each sentence, dependent on a
window of local context around the sentence. Individual words are then drawn either from
these local topics or from document-level topics as in standard LDA. MGLDA represents
local context using a sliding window, where each window frame comprises overlapping short
spans of sentences. In this way, local topic distributions are shared between sentences in
close proximity.
MGLDA can represent more complex topical dependencies than the models of Purver
et al. and Gruber et al., because the window can incorporate a much wider swath of local
context than two adjacent textual units. However, MGLDA is unable to encode longer
range constraints, such as contiguity and ordering similarity, because sentences not in close
proximity are only loosely connected through a series of intervening window frames. In
contrast, our work is specifically oriented toward these long-range constraints, necessitating
a whole-document notion of topic assignment.
2.2 Modeling Ordering Constraints in Statistical Discourse Analysis
The global constraints encoded by our model are closely related to research in discourse
on information ordering with applications to text summarization and generation (Barzilay,
Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004;
Elsner et al., 2007). The emphasis of that body of work is on learning ordering constraints
from data, with the goal of reordering new text from the same domain. These methods build
on the assumption that recurring patterns in topic ordering can be discovered by analyzing
patterns in word distribution. The key distinction between prior methods and our approach
is that existing ordering models are largely driven by local constraints with limited ability
to capture global structure. Below, we describe two main classes of probabilistic ordering
models studied in discourse processing.
2.2.1 Discriminative Models
Discriminative approaches aim directly to predict an ordering for a given set of sentences.
Modeling the ordering of all sentences simultaneously leads to a complex structure prediction
problem. In practice, however, a more computationally tractable two-step approach is taken:
first, probabilistic models are used to estimate pairwise sentence ordering preferences; next,
these local decisions are combined to produce a consistent global ordering (Lapata, 2003;

133

fiChen, Branavan, Barzilay, & Karger

Althaus, Karamanis, & Koller, 2004). Training data for pairwise models is constructed
by considering all pairs of sentences in a document, with supervision labels based on how
they are actually ordered. Prior work has demonstrated that a wide range of features
are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji &
Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). For instance, Lapata (2003) has
demonstrated that lexical features, such as verb pairs from the input sentences, serve as
a proxy for plausible sequences of actions, and thus are effective predictors of well-formed
orderings. During the second stage, these local decisions are integrated into a global order
that maximizes the number of consistent pairwise classifications. Since finding such an
ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in
practice (Lapata, 2003; Althaus et al., 2004).
While these two-step discriminative approaches can effectively leverage information
about local transitions, they do not provide any means for representing global constraints.
In more recent work, Barzilay and Lapata (2008) demonstrated that certain global properties can be captured in the discriminative framework using a reranking mechanism. In
this set-up, the system learns to identify the best global ordering given a set of n possible
candidate orderings. The accuracy of this ranking approach greatly depends on the quality
of selected candidates. Identifying such candidates is a challenging task given the large
search space of possible alternatives.
The approach presented in this work differs from existing discriminative models in two
ways. First, our model represents a distribution over all possible global orderings. Thus,
we can use sampling mechanisms that consider this whole space rather than being limited
to a subset of candidates as with ranking models. The second difference arises out of the
generative nature of our model. Rather than focusing on the ordering task, our order-aware
model effectively captures a layer of hidden variables that explain the underlying structure
of document content. Thus, it can be effectively applied to a wider variety of applications,
including those where sentence ordering is already observed, by appropriately adjusting the
observed and hidden components of the model.
2.2.2 Generative Models
Our work is closer in technique to generative models that treat topics as hidden variables.
One instance of such work is the Hidden Markov Model (HMM)-based content model (Barzilay & Lee, 2004). In their model, states correspond to topics and state transitions represent
ordering preferences; each hidden states emission distribution is then a language model
over words. Thus, similar to our approach, these models implicitly represent patterns at
the level of topical structure. The HMM is then used in the ranking framework to select an
ordering with the highest probability.
In more recent work, Elsner et al. (2007) developed a search procedure based on simulated annealing that finds a high likelihood ordering. In contrast to ranking-based approaches, their search procedure can cover the entire ordering space. On the other hand,
as we show in Section 5.3, we can define an ordering objective that can be maximized very
efficiently over all possible orderings during prediction once the model parameters have
been learned. Specifically, for a bag of p paragraphs, only O(pK) calculations of paragraph
probabilities are necessary, where K is the number of topics.

134

fiContent Modeling Using Latent Permutations

Another distinction between our proposed model and prior work is in the way global
ordering constraints are encoded. In a Markovian model, it is possible to induce some global
constraints by introducing additional local constraints. For instance, topic contiguity can
be enforced by selecting an appropriate model topology (e.g., by augmenting hidden states
to record previously visited states). However, other global constraints, such as similarity in
overall ordering across documents, are much more challenging to represent. By explicitly
modeling the topic permutation distribution, we can easily capture this kind of global
constraint, ultimately resulting in more accurate topic models and orderings. As we show
later in this paper, our model substantially outperforms the approach of Barzilay and Lee
on the information ordering task to which they applied the HMM-based content model.

3. Model
In this section, we describe our problem formulation and proposed model.
3.1 Problem Formulation
Our content modeling problem can be formalized as follows. We take as input a corpus
{d1 , . . . dD } of related documents, and a specification of a number of topics K.3 Each
document d is comprised of an ordered sequence of Nd paragraphs (pd,1 , . . . , pd,Nd ). As
output, we predict a single topic assignment zd,p  {1, . . . , K} for each paragraph p.4
These z values should reflect the underlying content organization of each document 
related content discussed within each document, and across separate documents, should
receive the same z value.
Our formulation shares some similarity with the standard LDA setup in that a common
set of topics is assigned across a collection of documents. The difference is that in LDA
each words topic assignment is conditionally independent, following the bag of words view
of documents, whereas our constraints on how topics are assigned let us connect word
distributional patterns to document-level topic structure.
3.2 Model Overview
We propose a generative Bayesian model that explains how a corpus of D documents can
be produced from a set of hidden variables. At a high level, the model first selects how
frequently each topic is expressed in the document, and how the topics are ordered. These
topics then determine the selection of words for each paragraph. Notation used in this and
subsequent sections is summarized in Figure 1.
For each document d with Nd paragraphs, we separately generate a bag of topics td and
a topic ordering d . The unordered bag of topics td , which contains Nd elements, expresses
how many paragraphs of the document are assigned to each of the K topics. Equivalently,
td can be viewed as a vector of occurrence counts for each topic, with zero counts for
topics that do not appear at all. Variable td is constructed by taking Nd samples from a
3. A nonparametric extension of this model would be to also learn K.
4. In well structured documents, paragraphs tend to be internally topically consistent (Halliday & Hasan,
1976), so predicting one topic per paragraph is sufficient. However, we note that our approach can be
applied with no modifications to other levels of textual granularity such as sentences.

135

fiChen, Branavan, Barzilay, & Karger



 parameters of distribution over
topic counts



 parameters of distribution over
topic orderings

t

 vector of topic counts

v

 vector of inversion counts



 topic ordering

z

 paragraph topic assignment



 language model parameters of
each topic

  Dirichlet(0 )
for j = 1 . . . K  1:
j  GMM0 (0 , 0 )
for k = 1 . . . K:
k  Dirichlet(0 )
for each
td 
vd 
d =
zd =

w  document words

document d
Multinomial()
GMM()
Compute-(vd )
Compute-z(td , d )

for each paragraph p in d
for each word w in p
w  Multinomial(zd,p )

K  number of topics
D  number of documents in corpus
Nd  number of paragraphs in
document d
Np  number of words in paragraph p

Algorithm: Compute-
Input: Inversion count vector v

Algorithm: Compute-z
Input: Topic counts t, permutation 

Create an empty list 
[1]  K
for j = K  1 down to 1
for i = K  1 down to v[j]
[i + 1]  [i]
[v[j]]  j

Create an empty list z
end  1
for k = K to 1
for i = 1 to t[[k]]
z[end]  [k]
end  end + 1

Output: Permutation 

Output: Paragraph topic vector z

Figure 1: The plate diagram and generative process for our model, along with a table of
notation for reference purposes. Shaded circles in the figure denote observed
variables, and squares denote hyperparameters. The dotted arrows indicate that
 is constructed deterministically from v according to algorithm Compute-, and
z is constructed deterministically from t and  according to Compute-z.
136

fiContent Modeling Using Latent Permutations

distribution over topics , a multinomial representing the probability of each topic being
expressed. Sharing  between documents captures the notion that certain topics are more
likely across most documents in the corpus.
The topic ordering variable d is a permutation over the numbers 1 through K that
defines the order in which topics appear in the document. We draw d from the Generalized
Mallows Model, a distribution over permutations that we explain in Section 3.3. As we will
see, this particular distribution biases the permutation selection to be close to a single
centroid, reflecting the discourse constraint of preferring similar topic structures across
documents.
Together, a documents bag of topics td and ordering d determine the topic assignment
zd,p for each of its paragraphs. For example, in a corpus with K = 4, a seven-paragraph
document d with td = {1, 1, 1, 1, 2, 4, 4} and d = (2, 4, 3, 1) would induce the topic sequence
zd = (2, 4, 4, 1, 1, 1, 1). The induced topic sequence zd can never assign the same topic to
two unconnected portions of a document, thus satisfying the constraint of topic contiguity.
We assume that each topic k is associated with a language model k . The words of a
paragraph assigned to topic k are then drawn from that topics language model k . This
portion is similar to standard LDA in that each topic relates to its own language model.
However, unlike LDA, our model enforces topic coherence for an entire paragraph rather
than viewing a paragraph as a mixture of topics.
Before turning to a more formal discussion of the generative process, we first provide
background on the permutation model for topic ordering.
3.3 The Generalized Mallows Model over Permutations
A central challenge of the approach we have presented is modeling the distribution over possible topic orderings. For this purpose we use the Generalized Mallows Model (GMM) (Fligner
& Verducci, 1986; Lebanon & Lafferty, 2002; Meila, Phadnis, Patterson, & Bilmes, 2007;
Klementiev, Roth, & Small, 2008), which exhibits two appealing properties in the context
of this task. First, the model concentrates probability mass on some canonical ordering
and small perturbations (permutations) of that ordering. This characteristic matches our
constraint that documents from the same domain exhibit structural similarity. Second,
its parameter set scales linearly with the number of elements being ordered, making it
sufficiently constrained and tractable for inference.
We first describe the standard Mallows Model over orderings (Mallows, 1957). The
Mallows Model takes two parameters, a canonical ordering  and a dispersion parameter .
It then sets the probability of any other ordering  to be proportional to ed(,) , where
d(, ) represents some distance metric between orderings  and . Frequently, this metric
is the Kendall  distance, the minimum number of swaps of adjacent elements needed to
transform ordering  into the canonical ordering . Thus, orderings which are close to the
canonical ordering will have high probability, while those in which many elements have been
moved will have less probability mass.
The Generalized Mallows Model, first introduced by Fligner and Verducci (1986), refines
the standard Mallows Model by adding an additional set of dispersion parameters. These
parameters break apart the distance d(, ) between orderings into a set of independent
components. Each component can then separately vary in its sensitivity to perturbation.

137

fiChen, Branavan, Barzilay, & Karger

To tease apart the distance function into components, the GMM distribution considers the
inversions required to transform the canonical ordering into an observed ordering. We first
discuss how these inversions are parameterized in the GMM, then turn to the distributions
definition and characteristics.
3.3.1 Inversion Representation of Permutations
Typically, permutations are represented directly as an ordered sequence of elements 
for example, (3, 1, 2) represents permuting the initial order by placing the third element
first, followed by the first element, and then the second. The GMM utilizes an alternative
permutation representation defined by a vector (v1 , . . . , vK1 ) of inversion counts with
respect to the identity permutation (1, . . . , K). Term vj counts the number of times when
a value greater than j appears before j in the permutation. Note that the jth inversion
count vj can only take on integer values from 0 to K  j inclusive. Thus the inversion count
vector has only K  1 elements, as vK is always zero. For instance, given the standard
form permutation (3, 1, 5, 6, 2, 4), v2 = 3 because 3, 5, and 6 appear before 2, and v3 = 0
because no numbers appear before it; the entire inversion count vector would be (1, 3, 0, 2, 0).
Likewise, our previous example permutation (2, 4, 3, 1) maps to inversion counts (3, 0, 1).
The sum of all components of an entire inversion count vector is simply that orderings
Kendall  distance from the canonical ordering.
A significant appeal of the inversion representation is that every valid, distinct vector
of inversion counts corresponds to a distinct permutation and vice versa. To see this,
note that for each permutation we can straightforwardly compute its inversion counts.
Conversely, given a sequence of inversion counts, we can construct the unique corresponding
permutation. We insert items into the permutation, working backwards from item K.
Assume that we have already placed items j + 1 through K in the proper order. To insert
item j, we note that exactly vj of items j + 1 to K must precede it, meaning that it
must be inserted after position vj in the current order (see the Compute- algorithm in
Figure 1). Since there is only one place where j can be inserted that fulfills the inversion
counts, induction shows that exactly one permutation can be constructed to satisfy the
given inversion counts.
In our model, we take the canonical topic ordering to always be the identity ordering
(1, . . . , K). Because the topic numbers in our task are completely symmetric and not linked
to any extrinsic meaning, fixing the global ordering to a specific arbitrary value does not
sacrifice any representational power. In the general case of the GMM, the canonical ordering
is a parameter of the distribution.
3.3.2 Probability Mass Function
The GMM assigns probability mass to a particular order based on how that order is permuted from the canonical ordering. More precisely, it associates a distance with every
permutation, where the canonical ordering has distance zero and permutations with many
inversions with respect to this canonical ordering have larger distance. The distance assignment is based on K  1 real-valued dispersion parameters P
(1 , . . . , K1 ). The distance of a
permutation with inversion counts v is then defined to be j j vj . The GMMs probability

138

fiContent Modeling Using Latent Permutations

mass function is exponential in this distance:
P

e j j vj
GMM(v; ) =
()
=

K1
Y
j=1

where () =

Q

j

ej vj
,
j (j )

(1)

j (j ) is a normalization factor with value:
j (j ) =

1  e(Kj+1)j
.
1  ej

(2)

Setting all j equal to a single value  recovers the standard Mallows Model with a Kendall
 distance function. The factorization of the GMM into independent probabilities per
inversion count makes this distribution particularly easy to apply; we will use GMMj to refer
to the jth multiplicand of the probability mass function, which is the marginal distribution
over vj :
GMMj (vj ; j ) =

ej vj
.
j (j )

(3)

Due to the exponential form of the distribution, requiring that j > 0 constrains the GMM
to assign highest probability mass to each vj being zero, i.e., the distributional mode is the
canonical identity permutation. A higher value for j assigns more probability mass to vj
being close to zero, biasing j to have fewer inversions.
3.3.3 Conjugate Prior
A major benefit of the GMM is its membership in the exponential family of distributions;
this means that it is particularly amenable to a Bayesian representation, as it admits a
natural independent conjugate prior for each parameter j (Fligner & Verducci, 1990):
GMM0 (j | vj,0 , 0 )  e(j vj,0 log j (j ))0 .

(4)

This prior distribution takes two parameters 0 and vj,0 . Intuitively, the prior states that
over 0 previous trials, the total number of inversions observed was 0 vj,0 . This distribution
can be easily updated with the observed vj to derive a posterior distribution.
Because each vj has a different range, it is inconvenient to set the prior hyperparameters
vj,0 directly. In our work, we instead assign a common prior value for each parameter j ,
which we denote as 0 . Then we set each vj,0 such that the maximum likelihood estimate of
j is 0 . By differentiating the likelihood of the GMM with respect to j , it is straightforward
to verify that this works out to setting:
vj,0 =

e0

K j+1
1
 (Kj+1)
.
0  1
1 e

139

(5)

fiChen, Branavan, Barzilay, & Karger

3.4 Formal Generative Process
We now fully specify the details of our content model, whose plate diagram appears in
Figure 1. We observe a corpus of D documents, where each document d is an ordered
sequence of Nd paragraphs and each paragraph is represented as a bag of words. The number
of topics K is assumed to be pre-specified. The model induces a set of hidden variables
that probabilistically explain how the words of the corpus were produced. Our final desired
output is the posterior distributions over the paragraphs hidden topic assignment variables.
In the following, variables subscripted with 0 are fixed prior hyperparameters.
1. For each topic k, draw a language model k  Dirichlet(0 ). As with LDA, these are
topic-specific word distributions.
2. Draw a topic distribution   Dirichlet(0 ), which expresses how likely each topic is
to appear regardless of position.
3. Draw the topic ordering distribution parameters j  GMM0 (0 , 0 ) for j = 1 to
K  1. These parameters control how rapidly probability mass decays for having
more inversions for each topic. A separate j for every topic allows us to learn that
some topics are more likely to be reordered than others.
4. For each document d with Nd paragraphs:
(a) Draw a bag of topics td by sampling Nd times from Multinomial().
(b) Draw a topic ordering d , by sampling a vector of inversion counts vd  GMM(),
and then applying algorithm Compute- from Figure 1 to vd .
(c) Compute the vector of topic assignments zd for document ds paragraphs by
sorting td according to d , as in algorithm Compute-z from Figure 1.5
(d) For each paragraph p in document d:
i. Sample each word w in p according to the language model of p: w 
Multinomial(zd,p ).
3.5 Properties of the Model
In this section we describe the rationale behind using the GMM to represent the ordering
component of our content model.
 Representational Power The GMM concentrates probability mass around one centroid permutation, reflecting our preferred bias toward document structures with similar topic orderings. Furthermore, the parameterization of the GMM using a vector of
dispersion parameters  allows for flexibility in how strongly the model biases toward
a single ordering  at one extreme ( = ) only one ordering has nonzero probability, while at the other ( = 0) all orderings are equally likely. Because  is comprised
5. Multiple permutations can contribute to the probability of a single documents topic assignments zd ,
if there are topics that do not appear in td . As a result, our current formulation is biased toward
assignments with fewer topics per document. In practice, we do not find this to negatively impact model
performance.

140

fiContent Modeling Using Latent Permutations

of independent dispersion parameters (1 , . . . , K1 ), the distribution can assign different penalties for displacing different topics. For example, we may learn that middle
sections (in the case of Cities, sections such as Economy and Culture) are more likely
to vary in position across documents than early sections (such as Introduction and
History).
 Computational Benefits The parameterization of the GMM using a vector of dispersion parameters  is compact and tractable. Since the number of parameters grows
linearly with the number of topics, the model can efficiently handle longer documents
with greater diversity of content.
Another computational advantage of this model is its seamless integration into a larger
Bayesian model. Due to its membership in the exponential family and the existence
of its conjugate prior, inference does not become significantly more complex when the
GMM is used in a hierarchical context. In our case, the entire document generative
model also accounts for topic frequency and the words within each topic.
One final beneficial effect of the GMM is that it breaks the symmetry of topic assignments by fixing the distribution centroid. Specifically, topic assignments are not
invariant to relabeling, because the probability of the underlying permutation would
change. In contrast, many topic models assign the same probability to any relabeling
of the topic assignments. Our model thus sidesteps the problem of topic identifiability, the issue where a model may have multiple maxima with the same likelihood due
to the underlying symmetry of the hidden variables. Non-identifiable models such as
standard LDA may cause sampling procedures to jump between maxima or produce
draws that are difficult to aggregate across runs.
Finally, we will show in Section 6 that the benefits of the GMM extend from the theoretical to the empirical: representing permutations using the GMM almost always leads to
superior performance compared to alternative approaches.

4. Inference
The variables that we aim to infer are the paragraph topic assignments z, which are determined by the bag of topics t and ordering  for each document. Thus, our goal is to estimate
the joint marginal distributions of t and  given the document text while integrating out
all remaining hidden parameters:
P (t, , | w).
(6)
We accomplish this inference task through Gibbs sampling (Geman & Geman, 1984; Bishop,
2006). A Gibbs sampler builds a Markov chain over the hidden variable state space whose
stationary distribution is the actual posterior of the joint distribution. Each new sample
is drawn from the distribution of a single variable conditioned on previous samples of the
other variables. We can collapse the sampler by integrating over some of the hidden
variables in the model, in effect reducing the state space of the Markov chain. Collapsed
sampling has been previously demonstrated to be effective for LDA and its variants (Griffiths
& Steyvers, 2004; Porteous, Newman, Ihler, Asuncion, Smyth, & Welling, 2008; Titov &

141

fiChen, Branavan, Barzilay, & Karger

P (td,i = t | . . .)  P (td,i = t | t(d,i) , 0 ) P (wd | td , d , wd , zd , 0 )


N (t(d,i) , t) + 0

P (wd | z, wd , 0 ),
|t(d,i) | + K0

P (vd,j = v | . . .)  P (vd,j = v | j ) P (wd | td , d , wd , zd , 0 )
= GMMj (v; j ) P (wd | z, wd , 0 ),

P


d vd,j + vj,0 0
P (j | . . .) = GMM0 j ;
, N + 0 ,
N + 0
Figure 2: The collapsed Gibbs sampling inference procedure for estimating our models
posterior distribution. In each plate diagram, the variable being resampled is
shown in a double circle and its Markov blanket is highlighted in black; other
variables, which have no impact on the variable being resampled, are grayed out.
Variables  and , shown in dotted circles, are never explicitly depended on or
re-estimated, because they are marginalized out by the sampler. Each diagram is
accompanied by the conditional resampling distribution for its respective variable.

142

fiContent Modeling Using Latent Permutations

McDonald, 2008). It is typically preferred over the explicit Gibbs sampling of all the hidden
variables because of the smaller search space and generally shorter mixing time.
Our sampler analytically integrates out all but three sets of hidden variables: bags of
topics t, orderings , and permutation inversion parameters . After a burn-in period,
we treat the last samples of t and  as a draw from the posterior. When samples of the
marginalized variables  and  are necessary, they can be estimated based on the topic
assignments as we show in Section 5.3. Figure 2 summarizes the Gibbs sampling steps of
our inference procedure.
Document Probability As a preliminary step, consider how to calculate the probability
of a single documents words wd given the documents paragraph topic assignments zd and
the remaining documents and their topic assignments. Note that this probability is decomposable into a product of probabilities over individual paragraphs where paragraphs with
different topics have conditionally independent word probabilities. Let wd and zd indicate the words and topic assignments to documents other than d, and W be the vocabulary
size. The probability of the words in d is then:
K Z
Y
P (wd | z, wd , 0 ) =
P (wd | zd , k ) P (k | z, wd , 0 ) dk

=

k=1 k
K
Y

DCM({wd,i : zd,i = k} | {wd,i : zd,i = k}, 0 ),

(7)

k=1

where DCM() refers to the Dirichlet compound multinomial distribution, the result of
integrating over multinomial parameters with a Dirichlet prior (Bernardo & Smith, 2000).
For a Dirichlet prior with parameters  = (1 , . . . , W ), the DCM assigns the following
probability to a series of observations x = {x1 , . . . , xn }:
P
W
( j j ) Y
(N (x, i) + i )
P
DCM(x; ) = Q
,
(8)
(|x| + j j )
j (j )
i=1

where N (x, i) refers to the number of times word i appears in x. Here, () is the Gamma
function, a generalization of the factorial for real numbers. Some algebra shows that the
DCMs posterior probability density function conditioned on a series of observations y =
{y1 , . . . , yn } can be computed by updating each i with counts of how often word i appears
in y:
DCM(x | y, ) = DCM(x; 1 + N (y, 1), . . . , W + N (y, W )).

(9)

Equations 7 and 9 will be used to compute the conditional distributions of the hidden
variables. We now turn to how each individual random variable is resampled.
Bag of Topics First we consider how to resample td,i , the ith topic draw for document
d conditioned on all other parameters being fixed (note this is not the topic of the ith
paragraph, as we reorder topics using d , which is generated separately):
P (td,i = t | . . .)  P (td,i = t | t(d,i) , 0 ) P (wd | td , d , wd , zd , 0 )


N (t(d,i) , t) + 0

P (wd | z, wd , 0 ),
|t(d,i) | + K0
143

(10)

fiChen, Branavan, Barzilay, & Karger

where td is updated to reflect td,i = t, and zd is deterministically computed in the last step
using Compute-z from Figure 1 with inputs td and d . The first step reflects an application
of Bayes rule to factor out the term for wd ; we then drop superfluous terms from the
conditioning. In the second step, the former term arises out of the DCM, by updating
the parameters 0 with observations t(d,i) as in Equation 9 and dropping constants. The
latter document probability term is computed using Equation 7. The new td,i is selected
by sampling from this probability computed over all possible topic assignments.
Ordering The parameterization of a permutation d as a series of inversion values vd,j
reveals a natural way to decompose the search space for Gibbs sampling. For each document
d, we resample vd,j for j = 1 to K  1 independently and successively according to its
conditional distribution:
P (vd,j = v | . . .)  P (vd,j = v | j ) P (wd | td , d , wd , zd , 0 )
= GMMj (v; j ) P (wd | z, wd , 0 ),

(11)

where d is updated to reflect vd,j = v, and zd is computed deterministically according to
td and d . The first term refers to Equation 3; the second is computed using Equation 7.
This probability is computed for every possible value of v, which ranges from 0 to K  j,
and term vd,j is sampled according to the resulting probabilities.
GMM Parameters For each j = 1 to K  1, we resample j from its posterior distribution:
P


d vd,j + vj,0 0
P (j | . . .) = GMM0 j ;
, N + 0 ,
(12)
N + 0
where GMM0 is evaluated according to Equation 4. The normalization constant of this
distribution is unknown, meaning that we cannot directly compute and invert the cumulative distribution function to sample from this distribution. However, the distribution
itself is univariate and unimodal, so we can expect that an MCMC technique such as slice
sampling (Neal, 2003) should perform well. In practice, Matlabs built-in slice sampler
provides a robust draw from this distribution.6
Computational Issues During inference, directly computing document probabilities on
the basis of Equation 7 results in many redundant calculations that slow the runtime of
each iteration considerably. To improve the computational performance of our proposed
inference procedure, we apply some memoization techniques during sampling. Within a
single iteration, for each document, the Gibbs sampler requires computing the documents
probability given its topic assignments (Equation 7) many times, but each computation
frequently conditions on only slight variations of those topic assignments. A nave approach
would compute a probability for every paragraph each time a document probability is
desired, performing redundant calculations when topic assignment sequences with shared
subsequences are repeatedly considered.
Instead, we use lazy evaluation to build a three-dimensional cache, indexed by tuple
(i, j, k), as follows. Each time a document probability is requested, it is broken into independent subspans of paragraphs, where each subspan takes on one contiguous topic assignment. This is possible due to the way Equation 7 factorizes into independent per-topic
6. In particular, we use the slicesample function from the Matlab Statistics Toolbox.

144

fiContent Modeling Using Latent Permutations

multiplicands. For a subspan starting at paragraph i, ending at paragraph j, and assigned topic k, the cache is consulted using key (i, j, k). For example, topic assignments
zd = (2, 4, 4, 1, 1, 1, 1) would result in cache lookups at (1, 1, 2), (2, 3, 4), and (4, 7, 1). If a
cached value is unavailable, the correct probability is computed using Equation 7 and the
result is stored in the cache at location (i, j, k). Moreover, we also record values at every
intermediate cache location (i, l, k) for l = i to j  1, because these values are computed as
subproblems while evaluating Equation 7 for (i, j, k). The cache is reset before proceeding to
the next document since the conditioning changes between documents. For each document,
this caching guarantees that there are at most O(Nd2 K) paragraph probability calculations.
In practice, because most individual Gibbs steps are small, this bound is very loose and the
caching mechanism reduces computation time by several orders of magnitude.
We also maintain caches of word-topic and paragraph-topic assignment frequencies,
allowing us to rapidly compute the counts used in equations 7 and 10. This form of caching
is the same as what is used by Griffiths and Steyvers (2004).

5. Applications
In this section, we describe how our model can be applied to three challenging discourselevel tasks: aligning paragraphs of similar topical content between documents, segmenting
each document into topically cohesive sections, and ordering new unseen paragraphs into
a coherent document. In particular, we show that the posterior samples produced by our
inference procedure from Section 4 can be used to derive a solution for each of these tasks.
5.1 Alignment
For the alignment task we wish to find how the paragraphs of each document topically
relate to paragraphs of other documents. Essentially, this is a cross-document clustering
task  an alignment assigns each paragraph of a document into one of K topically related
groupings. For instance, given a set of cell phone reviews, one group may represent text
fragments that discuss Price, while another group consists of fragments about Reception.
Our model can be readily employed for this task: we can view the topic assignment
for each paragraph z as a cluster label. For example, for two documents d1 and d2 with
topic assignments zd1 = (2, 4, 4, 1, 1, 1, 1) and zd2 = (4, 4, 3, 3, 2, 2, 2), paragraph 1 of d1 is
grouped together with paragraphs 5 through 7 of d2 , and paragraphs 2 and 3 of d1 with 1
and 2 of d2 . The remaining paragraphs assigned to topics 1 and 3 form their own separate
per-document clusters.
Previously developed methods for cross-document alignment have been primarily driven
by similarity functions that quantify lexical overlap between textual units (Barzilay & Elhadad, 2003; Nelken & Shieber, 2006). These methods do not explicitly model document
structure, but they specify some global constraints that guide the search for an optimal
alignment. Pairs of textual units are considered in isolation for making alignment decisions. In contrast, our approach allows us to take advantage of global structure and shared
language models across all related textual units without requiring manual specification of
matching constraints.

145

fiChen, Branavan, Barzilay, & Karger

5.2 Segmentation
Segmentation is a well-studied discourse task where the goal is to divide a document into
topically cohesive contiguous sections. Previous approaches have typically relied on lexical
cohesion  that is, similarity in word choices within a document subspan  to guide the
choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, &
Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay,
2008). Our model relies on this same notion in determining the language models of topics,
but connecting topics across documents and constraining how those topics appear allow it
to better learn the words that are most indicative of topic cohesion.
The output samples from our models inference procedure map straightforwardly to
segmentations  contiguous spans of paragraphs that are assigned the same topic number are taken to be one segment. For example, a seven-paragraph document d with topic
assignments zd = (2, 4, 4, 1, 1, 1, 1) would be segmented into three sections, comprised of
paragraph 1, paragraphs 2 and 3, and paragraphs 4 through 7. Note that the segmentation ignores the specific values used for topic assignments, and only heeds the paragraph
boundaries at which topic assignments change.
5.3 Ordering
A third application of our model is to the problem of creating structured documents from
collections of unordered text segments. This text ordering task is an important step in
broader NLP tasks such as text summarization and generation. For this task, we assume
we are provided with well structured documents from a single domain as training examples;
once trained, the model is used to induce an ordering of previously unseen collections of
paragraphs from the same domain.
During training, our model learns a canonical ordering of topics for documents within
the collection, via the language models associated with each topic. Because the GMM
concentrates probability mass around the canonical (1, . . . , K) topic ordering, we expect that
highly probable words in the language models of lower -numbered topics tend to appear early
in a document, whereas highly probable words in the language models of higher -numbered
topics tend to appear late in a document. Thus, we structure new documents according to
this intuition  paragraphs with words tied to low topic numbers should be placed earlier
than paragraphs with words relating to high topic numbers.
Formally, given an unseen document d comprised of an unordered set of paragraphs
{p1 , . . . , pn }, we order paragraphs according to the following procedure. First, we find
the most probable topic assignment zi independently for each paragraph pi , according to
parameters  and  learned during the training phase:
zi = arg max P (zi = k | pi , , )
k

= arg max P (pi | zi = k, k )P (zi = k | ).

(13)

k

Second, we sort the paragraphs by topic assignment zi in ascending order  since (1 . . . K)
is the GMMs canonical ordering, this yields the most likely ordering conditioned on a single
estimated topic assignment for each paragraph. Due to possible ties in topic assignments,
146

fiContent Modeling Using Latent Permutations

the resulting document may be a partial ordering; if a full ordering is required, ties are
broken arbitrarily.
A key advantage of this proposed approach is that it is closed-form and computationally
efficient. Though the training phase requires running the inference procedure of Section 4,
once the model parameters are learned, predicting an ordering for a new set of p paragraphs
requires computing only pK probability scores. In contrast, previous approaches have
only been able to rank a small subset of all possible document reorderings (Barzilay &
Lapata, 2008), or performed a search procedure through the space of orderings to find an
optimum (Elsner et al., 2007).7
The objective function of Equation 13 depends on posterior estimates of  and  given
the training documents. Since our collapsed Gibbs sampler integrates out these two hidden
variables, we need to back out the values of  and  from the known posterior samples of
z. This can easily be done by computing a point estimate of each distribution based on the
word-topic and topic-document assignment frequencies, respectively, as is done by Griffiths
and Steyvers (2004). The probability mass kw of word w in the language model of topic k
is given by:
N (k, w) + 0
kw =
,
(14)
N (k) + W 0
where N (k, w) the total number of times word w was assigned to topic k, and N (k) is the
total number of words assigned to topic k, according to the posterior sample of z. We can
derive a similar estimate for k , the prior likelihood of topic k:
k =

N (k) + 0
,
N + K0

(15)

where N (k) is the total number of paragraphs assigned to topic k according to the sample
of z, and N is the total number of paragraphs in the entire corpus.

6. Experiments
In this section, we evaluate the performance of our model on the three tasks presented in
Section 5: cross-document alignment, document segmentation, and information ordering.
We first describe some preliminaries common to all three tasks, covering the data sets,
reference comparison structures, model variants, and inference algorithm settings shared by
each evaluation. We then provide a detailed examination of how our model performs on
each individual task.
6.1 General Evaluation Setup
Data Sets In our experiments we use five data sets, briefly described below (for additional
statistics, see Table 1):
7. The approach we describe is not the same as finding the most probable paragraph ordering according to
data likelihood, which is how the optimal ordering is derived for the HMM-based content model. Our
proposed ordering technique essentially approximates that objective by using a per-paragraph maximum
a posteriori estimate of the topic assignments rather than the full posterior topic assignment distribution.
This approximation makes for a much faster prediction algorithm that performs well empirically.

147

fiChen, Branavan, Barzilay, & Karger

Articles about
Corpus
CitiesEn
CitiesEn500
CitiesFr

large cities from Wikipedia
Language Documents Sections
English
100
13.2
English
500
10.5
French
100
10.4

Paragraphs
66.7
45.9
40.7

Vocabulary
42,000
95,400
31,000

Tokens
4,920
3,150
2,630

Articles about chemical elements from Wikipedia
Corpus
Language Documents Sections
Elements
English
118
7.7

Paragraphs
28.1

Vocabulary
18,000

Tokens
1,920

Cell phone reviews from PhoneArena.com
Corpus
Language Documents Sections
Phones
English
100
6.6

Paragraphs
24.0

Vocabulary
13,500

Tokens
2,750

Table 1: Statistics of the data sets used in our evaluations. All values except vocabulary
size and document count are per-document averages.

 CitiesEn: Articles from the English Wikipedia about the worlds 100 largest cities by
population. Common topics include History, Culture, and Demographics. These articles are typically of substantial size and share similar content organization patterns.
 CitiesEn500 : Articles from the English Wikipedia about the worlds 500 largest cities
by population. This collection is a superset of CitiesEn. Many of the lower-ranked
cities are not well known to English Wikipedia editors  thus, compared to CitiesEn
these articles are shorter on average and exhibit greater variability in content selection
and ordering.
 CitiesFr : Articles from the French Wikipedia about the same 100 cities as in CitiesEn.
 Elements: Articles from the English Wikipedia about chemical elements in the periodic table,8 including topics such as Biological Role, Occurrence, and Isotopes.
 Phones: Reviews extracted from PhoneArena.com, a popular cell phone review website. Topics in this corpus include Design, Camera, and Interface. These reviews are
written by expert reviewers employed by the site, as opposed to lay users.9
This heterogeneous collection of data sets allows us to examine the behavior of the
model under diverse test conditions. These sets vary in how the articles were generated,
the language in which the articles were written, and the subjects they discuss. As a result,
patterns in topic organization vary greatly across domains. For instance, within the Phones
corpus, the articles are very formulaic, due to the centralized editorial control of the website,
which establishes consistent standards followed by the expert reviewers. On the other hand,
Wikipedia articles exhibit broader structural variability due to the collaborative nature of
8. All 118 elements at http://en.wikipedia.org/wiki/Periodic table, including undiscovered element 117.
9. In the Phones set, 35 documents are very short express reviews without section headings; we include
them in the input to the model, but did not evaluate on them.

148

fiContent Modeling Using Latent Permutations

Wikipedia editing, which allows articles to evolve independently. While Wikipedia articles
within the same category often exhibit similar section orderings, many have idiosyncratic
inversions. For instance, in the CitiesEn corpus, both the Geography and History sections
typically occur toward the beginning of a document, but History can appear either before
or after Geography across different documents.
Each corpus we consider has been manually divided into sections by their authors,
including a short textual heading for each section. In Sections 6.2.1 and 6.3.1, we discuss
how these author-created sections with headings are used to generate reference annotations
for the alignment and segmentation tasks. Note that we only use the headings for evaluation;
none of the heading information is provided to any of the methods under consideration. For
the tasks of alignment and segmentation, evaluation is performed on the datasets presented
in Table 1. For the ordering task, however, this data is used for training, and evaluation is
performed using a separate held-out set of documents. The details of this held-out dataset
are given in Section 6.4.1.
Model Variants For each evaluation, besides comparing to baselines from the literature,
we also consider two variants of our proposed model. In particular, we investigate the
impact of the Mallows component of the model by alternately relaxing and tightening the
way it constrains topic orderings:
 Constrained : In this variant, we require all documents to follow the exact same canonical ordering of topics. That is, no topic permutation inversions are allowed, though
documents may skip topics as before. This case can be viewed as a special case of
the general model, where the Mallows inversion prior 0 approaches infinity. From
an implementation standpoint, we simply fix all inversion counts v to zero during
inference.10
 Uniform: This variant assumes a uniform distribution over all topic permutations,
instead of biasing toward a small related set. Again, this is a special case of the full
model, with inversion prior 0 set to zero, and the strength of that prior 0 approaching
infinity, thus forcing each item of  to always be zero.
Note that both of these variants still enforce the long-range constraint of topic contiguity,
and vary from the full model only in how they capture topic ordering similarity.
Evaluation Procedure and Parameter Settings For each evaluation of our model
and its variants, we run the collapsed Gibbs sampler from five random seed states, and take
the 10,000th iteration of each chain as a sample. Results presented are the average over
these five samples.
Dirichlet prior hyperparameters for the bag of topics 0 and language models 0 are set
to 0.1. For the GMM, we set the prior dispersion hyperparameter 0 to 1, and the effective
10. At first glance, the Constrained model variant appears to be equivalent to an HMM where each state i
can transition to either i or i + 1. However, this is not the case  some topics may appear zero times
in a document, resulting in multiple possible transitions from each state. Furthermore, the transition
probabilities would be dependent on position within the document  for example, at earlier absolute
positions within a document, transitions to high-index topics are unlikely, because that would require
all subsequent paragraphs to have a high-index topic.

149

fiChen, Branavan, Barzilay, & Karger

sample size prior 0 to be 0.1 times the number of documents. These values are minimally
tuned, and similar results are achieved for alternative settings of 0 and 0 . Parameters 0
and 0 control the strength of the bias toward structural regularity, trading off between the
Constrained and Uniform model variants. The values we have chosen are a middle ground
between those two extremes.
Our model also takes a parameter K that controls the upper bound on the number of
latent topics. Note that our algorithm can select fewer than K topics for each document,
so K does not determine the number of segments in each document. In general, a higher K
results in a finer-grained division of each document into different topics, which may result
in more precise topics, but may also split topics that should be together. We report results
in each evaluation using both K = 10 and 20.
6.2 Alignment
We first evaluate the model on the task of cross-document alignment, where the goal is to
group textual units from different documents into topically cohesive clusters. For instance,
in the Cities-related domains, one such cluster may include Transportation-related paragraphs. Before turning to the results we first present details of the specific evaluation setup
targeted to this task.
6.2.1 Alignment Evaluation Setup
Reference Annotations To generate a sufficient amount of reference data for evaluating alignments we use section headings provided by the authors. We assume that two
paragraphs are aligned if and only if their section headings are identical. These headings
constitute noisy annotations in the Wikipedia datasets: the same topical content may be
labeled with different section headings in different articles (e.g., for CitiesEn, Places of
interest in one article and Landmarks in another), so we call this reference structure the
noisy headings set.
It is not clear a priori what effect this noise in the section headings may have on evaluation accuracy. To empirically estimate this effect, we also use some manually annotated
alignments in our experiments. Specifically, for the CitiesEn corpus, we manually annotated each articles paragraphs with a consistent set of section headings, providing us an
additional reference structure to evaluate against. In this clean headings set, we found
approximately 18 topics that were expressed in more than one document.
Metrics To quantify our alignment output we compute a recall and precision score of
a candidate alignment against a reference alignment. Recall measures, for each unique
section heading in the reference, the maximum number of paragraphs with that heading
that are assigned to one particular topic. The final score is computed by summing over each
section heading and dividing by the total number of paragraphs. High recall indicates that
paragraphs of the same section headings are generally being assigned to the same topic.
Conversely, precision measures, for each topic number, the maximum number of paragraphs with that topic assignment that share the same section heading. Precision is summed
over each topic and normalized by the total number of paragraphs. High precision means
that paragraphs assigned to a single topic usually correspond to the same section heading.

150

fiContent Modeling Using Latent Permutations

Recall and precision trade off against each other  more finely grained topics will tend
to improve precision at the cost of recall. At the extremes, perfect recall occurs when every
paragraph is assigned the same topic, and perfect precision when each paragraph is its own
topic.
We also present one summary F-score in our results, which is the harmonic mean of
recall and precision.
Statistical significance in this setup is measured with approximate randomization (Noreen,
1989), a nonparametric test that can be directly applied to nonlinearly computed metrics
such as F-score. This test has been used in prior evaluations for information extraction and
machine translation (Chinchor, 1995; Riezler & Maxwell, 2005).
Baselines

For this task, we compare against two baselines:

 Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): As explained in Section 2, this model represents topic change between adjacent textual units in a Markovian fashion. HTMM can only capture local constraints, so it would allow topics to
recur non-contiguously throughout a document. We use the publicly available implementation,11 with priors set according to the recommendations made in the original
work.
 Clustering: We use a repeated bisection algorithm to find a clustering of the paragraphs that maximizes the sum of the pairwise cosine similarities of the items in each
cluster.12 This clustering was implemented using the CLUTO toolkit.13 Note that
this approach is completely structure-agnostic, treating documents as bags of paragraphs rather than sequences of paragraphs. These types of clustering techniques
have been shown to deliver competitive performance for cross-document alignment
tasks (Barzilay & Elhadad, 2003).
6.2.2 Alignment Results
Table 2 presents the results of the alignment evaluation. On all of the datasets, the best
performance is achieved by our model or its variants, by a statistically significant and usually
substantial margin.
The comparative performance of the baseline methods is consistent across domains 
surprisingly, clustering performs better than the more complex HTMM model. This observation is consistent with previous work on cross-document alignment and multidocument
summarization, which use clustering as their main component (Radev, Jing, & Budzikowska,
2000; Barzilay, McKeown, & Elhadad, 1999). Despite the fact that HTMM captures some
dependencies between adjacent paragraphs, it is not sufficiently constrained. Manual examination of the actual topic assignments reveals that HTMM often assigns the same topic
for disconnected paragraphs within a document, violating the topic contiguity constraint.
In all but one domain the full GMM-based approach yields the best performance compared to its variants. The one exception is in the Phone domain. There the Constrained
11. http://code.google.com/p/openhtmm/
12. This particular clustering technique substantially outperforms the agglomerative and graph partitioningbased clustering approaches for our task.
13. http://glaros.dtc.umn.edu/gkhome/views/cluto/

151

fiK = 20

K = 10

K = 20

K = 10

Chen, Branavan, Barzilay, & Karger

Clustering
HTMM
Constrained
Uniform
Our model
Clustering
HTMM
Constrained
Uniform
Our model

CitiesEn
Clean headings
Recall
Prec
F-score
0.578
0.439  0.499
0.446
0.232  0.305
0.579
0.471  0.520
0.520
0.440  0.477
0.639 0.509
0.566
0.486
0.541  0.512
0.260
0.217  0.237
0.458
0.519  0.486
0.499
0.551  0.524
0.578 0.636
0.606

CitiesEn
Noisy headings
Recall
Prec
F-score
0.611
0.331  0.429
0.480
0.183  0.265
0.667
0.382  0.485
0.599
0.343  0.436
0.705 0.399
0.510
0.527
0.414  0.464
0.304
0.187  0.232
0.553
0.415  0.474
0.571
0.423  0.486
0.648 0.489
0.557

CitiesEn500
Noisy headings
Recall
Prec
F-score
0.609
0.329  0.427
0.461
0.269  0.340
0.643
0.385  0.481
0.582
0.344  0.432
0.722 0.426
0.536
0.489
0.391  0.435
0.351
0.234  0.280
0.515
0.394  0.446
0.557
0.422  0.480
0.620 0.473
0.537

Clustering
HTMM
Constrained
Uniform
Our model
Clustering
HTMM
Constrained
Uniform
Our model

CitiesFr
Noisy headings
Recall
Prec
F-score
0.588
0.283  0.382
0.338
0.190  0.244
0.652
0.356
0.460
0.587
0.310  0.406
0.657 0.360
0.464
0.453
0.317  0.373
0.253
0.195  0.221
0.584
0.379  0.459
0.571
0.373  0.451
0.633 0.431
0.513

Elements
Noisy headings
Recall
Prec
F-score
0.524
0.361  0.428
0.430
0.190  0.264
0.603
0.408  0.487
0.591
0.403  0.479
0.685 0.460
0.551
0.477
0.402  0.436
0.248
0.243  0.246
0.510
0.421  0.461
0.550
0.479  0.512
0.569 0.498
0.531

Phones
Noisy headings
Recall
Prec
F-score
0.599
0.456  0.518
0.379
0.240  0.294
0.745 0.506
0.602
0.656
0.422  0.513
0.738
0.493
0.591
0.486
0.507  0.496
0.274
0.229  0.249
0.652 0.576
0.611
0.608
0.471  0.538
0.683
0.546
0.607

Table 2: Comparison of the alignments produced by our model and a series of baselines and
model variations, for both 10 and 20 topics, evaluated against clean and noisy sets
of section headings. Higher scores are better. Within the same K, the methods
which our model significantly outperforms are indicated with  for p < 0.001 and
 for p < 0.01.

152

fiContent Modeling Using Latent Permutations

baseline achieves the best result for both K by a small margin. These results are to be
expected, given the fact that this domain exhibits a highly rigid topic structure across all
documents. A model that permits permutations of topic ordering, such as the GMM, is too
flexible for such highly formulaic domains.
Finally, we observe that the evaluations based on manual and noisy annotations exhibit
an almost entirely consistent ranking of the methods under consideration (see the clean and
noisy headings results for CitiesEn in Table 2). This consistency indicates that the noisy
headings are sufficient for gaining insight into the comparative performance of the different
approaches.
6.3 Segmentation
Next we consider the task of text segmentation. We test whether the model is able to
identify the boundaries of topically coherent text segments.
6.3.1 Segmentation Evaluation Setup
Reference Segmentations As described in Section 6.1, all of the datasets used in this
evaluation have been manually divided into sections by their authors. These annotations
are used to create reference segmentations for evaluating our models output. Recall from
Section 6.2.1 that we also built a clean reference structure for the CitiesEn set. That structure encodes a clean segmentation of each document because it adjusts the granularity
of section headings to be consistent across documents. Thus, we also compare against the
segmentation specified by the CitiesEn clean section headings.
Metrics Segmentation quality is evaluated using the standard penalty metrics Pk and
WindowDiff (Beeferman, Berger, & Lafferty, 1999; Pevzner & Hearst, 2002). Both pass a
sliding window over the documents and compute the probability of the words at the end
of the windows being improperly segmented with respect to each other. WindowDiff is
stricter, and requires that the number of segmentation boundaries between the endpoints
of the window be correct as well.14
Baselines We first compare to BayesSeg (Eisenstein & Barzilay, 2008),15 a Bayesian
segmentation approach that is the current state-of-the-art for this task. Interestingly, our
model reduces to their approach when every document is considered completely in isolation,
with no topic sharing between documents. Connecting topics across documents makes for
a much more difficult inference problem than the one tackled by Eisenstein and Barzilay.
At the same time, their algorithm cannot capture structural relatedness across documents.
Since BayesSeg is designed to be operated with a specification of a number of segments,
we provide this baseline with the benefit of knowing the correct number of segments for
each document, which is not provided to our system. We run this baseline using the
14. Statistical significance testing is not standardized and usually not reported for the segmentation task,
so we omit these tests in our results.
15. We do not evaluate on the corpora used in their work, since our model relies on content similarity across
documents in the corpus.

153

fiBayesSeg
U&I
U&I
Constrained
Uniform
Our model
Constrained
Uniform
Our model

CitiesEn
Clean headings
Pk
WD
# Segs
0.321
0.376
12.3
0.337
0.404
12.3
0.353
0.375
5.8
0.260 0.281
7.7
0.268
0.300
8.8
0.253
0.283
9.0
0.274
0.314
10.9
0.234
0.294
14.0
0.221 0.278
14.2

CitiesEn
Noisy headings
Pk
WD
# Segs
0.317
0.376
13.2
0.337
0.405
13.2
0.357
0.378
5.8
0.267
0.288
7.7
0.273
0.304
8.8
0.257 0.286
9.0
0.274
0.313
10.9
0.234
0.290
14.0
0.222 0.278
14.2

CitiesEn500
Noisy headings
Pk
WD
# Segs
0.282
0.335
10.5
0.292
0.350
10.5
0.321
0.346
5.4
0.221
0.244
6.8
0.227
0.257
7.8
0.196 0.225
8.1
0.226
0.261
9.1
0.203
0.256
12.3
0.196 0.247
12.1

BayesSeg
U&I
U&I
Constrained
Uniform
Our model
Constrained
Uniform
Our model

CitiesFr
Noisy headings
Pk
WD
# Segs
0.274
0.332
10.4
0.282
0.336
10.4
0.321
0.342
4.4
0.230
0.244
6.4
0.214 0.233
7.3
0.216 0.233
7.4
0.230
0.250
7.9
0.203
0.234
10.4
0.201 0.230
10.8

Elements
Noisy headings
Pk
WD
# Segs
0.279
0.316
7.7
0.248
0.286
7.7
0.294
0.312
4.8
0.227
0.244
5.4
0.226
0.250
6.6
0.201 0.226
6.7
0.231
0.257
6.6
0.209
0.248
8.7
0.203 0.243
8.6

Phones
Noisy headings
Pk
WD
# Segs
0.392
0.457
9.6
0.412
0.463
9.6
0.423
0.435
4.7
0.312 0.347
8.0
0.332
0.367
7.5
0.309
0.349
8.0
0.295 0.348
10.8
0.327
0.381
9.4
0.302
0.357
10.4

K = 20 K = 10

K = 20 K = 10

Chen, Branavan, Barzilay, & Karger

Table 3: Comparison of the segmentations produced by our model and a series of baselines
and model variations, for both 10 and 20 topics, evaluated against clean and noisy
sets of section headings. Lower scores are better. BayesSeg and U&I are given the
true number of segments, so their segments counts reflect the reference structures
segmentations. In contrast, U&I automatically predicts the number of segments.

154

fiContent Modeling Using Latent Permutations

authors publicly available implementation;16 its priors are set using a built-in mechanism
that automatically re-estimates hyperparameters.
We also compare our method with the algorithm of Utiyama and Isahara (2001), which
is commonly used as a point of reference in the evaluation of segmentation algorithms.
This algorithm computes the optimal segmentation by estimating changes in the predicted
language models of segments under different partitions. We used the publicly available
implementation of the system,17 which does not require parameter tuning on a held-out
development set. In contrast to BayesSeg, this algorithm has a mechanism for predicting
the number of segments, but can also take a pre-specified number of segments. In our
comparison, we consider both versions of the algorithm  U&I denotes the case when the
correct number of segments is provided to the model and U&I denotes when the model
estimates the optimal number of segments.
6.3.2 Segmentation Results
Table 3 presents the segmentation experiment results. On every data set our model outperforms the BayesSeg and U&I baselines by a substantial margin regardless of K. This result
provides strong evidence that learning connected topic models over related documents leads
to improved segmentation performance.
The best performance is generally obtained by the full version of our model, with three
exceptions. In two cases (CitiesEn with K = 10 using clean headings on the WindowDiff
metric, and CitiesFr with K = 10 on the Pk metric), the variant that performs better than
the full model only does so by a minute margin. Furthermore, in both of those instances,
the corresponding evaluation with K = 20 using the full model leads to the best overall
results for the respective domains.
The only case when a variant outperforms our full model by a notable margin is the
Phones data set. This result is not unexpected given the formulaic nature of this dataset
as discussed earlier.
6.4 Ordering
The final task on which we evaluate our model is that of finding a coherent ordering of a
set of textual units. Unlike the previous tasks, where prediction is based on hidden variable
distributions, ordering is observed in a document. Moreover, the GMM model uses this
information during the inference process. Therefore, we need to divide our data sets into
training and test portions.
In the past, ordering algorithms have been applied to textual units of various granularities, most commonly sentences and paragraphs. Our ordering experiments operate at
the level of a relatively larger unit  sections. We believe that this granularity is suitable
to the nature of our model, because it captures patterns at the level of topic distributions
rather than local discourse constraints. The ordering of sentences and paragraphs has been
studied in the past (Karamanis et al., 2004; Barzilay & Lapata, 2008) and these two types
of models can be effectively combined to induce a full ordering (Elsner et al., 2007).
16. http://groups.csail.mit.edu/rbg/code/bayesseg/
17. http://www2.nict.go.jp/x/x161/members/mutiyama/software.html#textseg

155

fiChen, Branavan, Barzilay, & Karger

Corpus
CitiesEn
CitiesFr
Phones

Set
Training
Testing
Training
Testing
Training
Testing

Documents
100
65
100
68
100
64

Sections
13.2
11.2
10.4
7.7
6.6
9.6

Paragraphs
66.7
50.3
40.7
28.2
24.0
39.3

Vocabulary
42,000
42,000
31,000
31,000
13,500
13,500

Tokens
4,920
3,460
2,630
1,580
2,750
4,540

Table 4: Statistics of the training and test sets used for the ordering experiments. All values
except vocabulary are the average per document. The training set statistics are
reproduced from Table 1 for ease of reference.

6.4.1 Ordering Evaluation Setup
Training and Test Data Sets We use the CitiesEn, CitiesFr and Phones data sets
as training documents for parameter estimation as described in Section 5. We introduce
additional sets of documents from the same domains as test sets. Table 4 provides statistics
on the training and test set splits (note that out-of-vocabulary terms in the test sets are
discarded).18
Even though we perform ordering at the section level, these collections still pose a
challenging ordering task: for example, the average number of sections in a CitiesEn test
document is 11.2, comparable to the 11.5 sentences (the unit of reordering) per document
of the National Transportation Safety Board corpus used in previous work (Barzilay & Lee,
2004; Elsner et al., 2007).
Metrics We report the Kendalls  rank correlation coefficient for our ordering experiments. This metric measures how much an ordering differs from the reference order  the
underlying assumption is that most reasonable sentence orderings should be fairly similar
to it. Specifically, for a permutation  of the sections in an N -section document,  () is
computed as
d(, )
 () = 1  2 N  ,
(16)
2

where d(, ) is, as before, the Kendall  distance, the number of swaps of adjacent textual
units necessary to rearrange  into the reference order. The metric ranges from -1 (inverse
orders) to 1 (identical orders). Note that a random ordering will yield a zero score in expectation. This measure has been widely used for evaluating information ordering (Lapata,
2003; Barzilay & Lee, 2004; Elsner et al., 2007) and has been shown to correlate with human
assessments of text quality (Lapata, 2006).
Baselines and Model Variants Our ordering method is compared against the original
HMM-based content modeling approach of Barzilay and Lee (2004). This baseline delivers
18. The Elements data set is limited to 118 articles, preventing us from splitting it into reasonably sized
training and test sets. Therefore we do not consider it for our ordering experiments. For the Citiesrelated sets, the test documents are shorter because they were about cities of lesser population. On the
other hand, for Phones the test set does not include short express reviews and thus exhibits higher
average document length.

156

fiContent Modeling Using Latent Permutations

HMM-based Content Model
Constrained
K = 10
Our model
Constrained
K = 20
Our model

CitiesEn
0.245
0.587
0.571
0.583
0.575

CitiesFr
0.305
0.596
0.541
0.575
0.571

Phones
0.256
0.676
0.678
0.711
0.678

Table 5: Comparison of the orderings produced by our model and a series of baselines and
model variations, for both 10 and 20 topics, evaluated on the respective test sets.
Higher scores are better.

state-of-the art performance in a number of datasets and is similar in spirit to our model
 it also aims to capture patterns at the level of topic distribution (see Section 2). Again,
we use the publicly available implementation19 with parameters adjusted according to the
values used in their previous work. This content modeling implementation provides an A*
search procedure that we use to find the optimal permutation.
We do not include in our comparison local coherence models (Barzilay & Lapata, 2008;
Elsner et al., 2007). These models are designed for sentence-level analysis  in particular,
they use syntactic information and thus cannot be directly applied for section-level ordering.
As we state above, these models are orthogonal to topic-based analysis; combining the two
approaches is a promising direction for future work.
Note that the Uniform model variant is not applicable to this task, since it does not
make any claims to a preferred underlying topic ordering. In fact, from a document likelihood perspective, for any proposed paragraph order the reverse order would have the same
probability under the Uniform model. Thus, the only model variant we consider here is
Constrained.
6.4.2 Ordering Results
Table 5 summarizes ordering results for the GMM- and HMM-based content models. Across
all data sets, our model outperforms content modeling by a very large margin. For instance,
on the CitiesEn dataset, the gap between the two models reaches 35%. This difference is
expected. In previous work, content models were applied to short formulaic texts. In
contrast, documents in our collection exhibit higher variability than the original collections.
The HMM does not provide explicit constraints on generated global orderings. This may
prevent it from effectively learning non-local patterns in topic organization.
We also observe that the Constrained variant outperforms our full model. While the
difference between the two is small, it is fairly consistent across domains. Since it is not
possible to predict idiosyncratic variations in the test documents topic orderings, a more
constrained model can better capture the prevalent ordering patterns that are consistent
across the domain.
19. http://people.csail.mit.edu/regina/code.html

157

fiChen, Branavan, Barzilay, & Karger

6.5 Discussion
Our experiments with the three separate tasks reveal some common trends in the results.
First, we observe that our single unified model of document structure can be readily and
successfully applied to multiple discourse-level tasks, whereas previous work has proposed
separate approaches for each task. This versatility speaks to the power of our topic-driven
representation of document structure. Second, within each task our model outperforms
state-of-the-art baselines by substantial margins across a wide variety of evaluation scenarios. These results strongly support our hypothesis that augmenting topic models with
discourse-level constraints broadens their applicability to discourse-level analysis tasks.
Looking at the performance of our model across different tasks, we make a few notes
about the importance of the individual topic constraints. Topic contiguity is a consistently
important constraint, allowing both of our model variants to outperform alternative baseline
approaches. In most cases, introducing a bias toward similar topic ordering, without requiring identical orderings, provides further benefits when encoded in the model. Our more
flexible models achieve superior performance in the segmentation and alignment tasks. In
the case of ordering, however, this extra flexibility does not pay off, as the model distributes
its probability mass away from strong ordering patterns likely to occur in unseen data.
We can also identify the properties of a dataset that most strongly affect the performance
of our model. The Constrained model variant performs slightly better than our full model
on rigidly formulaic domains, achieving highest performance on the Phones data set. When
we know a priori that a domain is formulaic in structure, it is worthwhile to choose the
model variant that suitably enforces formulaic topic orderings. Fortunately, this adaptation
can be achieved in the proposed framework using the prior of the Generalized Mallows
Model  recall that the Constrained variant is a special case of the full model.
However, the performance of our model is invariant with respect to other data set characteristics. Across the two languages we considered, the model and baselines exhibit the
same comparative performance for each task. Moreover, this consistency also holds between
the general-interest cities articles and the highly technical chemical elements articles. Finally, between the smaller CitiesEn and larger CitiesEn500 data sets, we observe that our
results are consistent.

7. Conclusions and Future Work
In this paper, we have shown how an unsupervised topic-based approach can capture content
structure. Our resulting model constrains topic assignments in a way that requires global
modeling of entire topic sequences. We showed that the Generalized Mallows Model is a
theoretically and empirically appealing way of capturing the ordering component of this
topic sequence. Our results demonstrate the importance of augmenting statistical models
of text analysis with structural constraints motivated by discourse theory. Furthermore,
our success with the GMM suggests that it could be applied to the modeling of ordering
constraints in other NLP applications.
There are multiple avenues of future extensions to this work. First, our empirical results
demonstrated that for certain domains providing too much flexibility in the model may
in fact be detrimental to predictive accuracy. In those cases, a more tightly constrained
variant of our model yields superior performance. An interesting extension of our current
158

fiContent Modeling Using Latent Permutations

model would be to allow additional flexibility in the prior of the GMM by drawing it from
another level of hyperpriors. From a technical perspective, this form of hyperparameter
re-estimation would involve defining an appropriate hyperprior for the Generalized Mallows
Model and adapting its estimation into our present inference procedure.
Additionally, there may be cases when the assumption of one canonical topic ordering
for an entire corpus is too limiting, e.g., if a data set consists of topically related articles
from multiple sources, each with its own editorial standards. Our model can be extended
to allow for multiple canonical orderings by positing an additional level of hierarchy in the
probabilistic model, i.e., document structures can be generated from a mixture of several
Generalized Mallows Models, each with its own distributional mode. In this case, the
model would take on the additional burden of learning how topics are permuted between
these multiple canonical orderings. Such a change to the model would greatly complicate
inference as re-estimating a Generalized Mallows Model canonical ordering is in general NPhard. However, recent advances in statistics have produced efficient approximate algorithms
with theoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008) and
exact methods that are tractable for typical cases (Meila et al., 2007).
More generally, the model presented in this paper assumes two specific global constraints
on content structure. While domains that satisfy these constraints are plentiful, there are
domains where our modeling assumptions do not hold. For example, in dialogue it is well
known that topics recur throughout a conversation (Grosz & Sidner, 1986), thereby violating
our first constraint. Nevertheless, texts in such domains still follow certain organizational
conventions, e.g. the stack structure for dialogue. Our results suggest that explicitly incorporating domain-specific global structural constraints into a content model would likely
improve the accuracy of structure induction.
Another direction of future work is to combine the global topic structure of our model
with local coherence constraints. As previously noted, our model is agnostic toward the
relationships between sentences within a single topic. In contrast, models of local coherence
take advantage of a wealth of additional knowledge, such as syntax, to make decisions about
information flow across adjoining sentences. Such a linguistically rich model would provide
a powerful representation of all levels of textual structure, and could be used for an even
greater variety of applications than we have considered here.

Bibliographic Note
Portions of this work were previously presented in a conference publication (Chen, Branavan,
Barzilay, & Karger, 2009). This article significantly extends our previous work, most notably
by introducing a new algorithm for applying our models output to the information ordering
task (Section 5) and considering new data sets for our experiments that vary in genre,
language, and size (Section 6).

Acknowledgments
The authors acknowledge the funding support of the NSF CAREER grant IIS-0448168 and
grant IIS-0712793, the NSF Graduate Fellowship, the Office of Naval Research, Quanta,
Nokia, and the Microsoft Faculty Fellowship. We thank the many people who offered

159

fiChen, Branavan, Barzilay, & Karger

suggestions and comments on this work, including Michael Collins, Aria Haghighi, Yoong
Keok Lee, Marina Meila, Tahira Naseem, Christy Sauper, David Sontag, Benjamin Snyder,
and Luke Zettlemoyer. We are especially grateful to Marina Meila for introducing us to
the Mallows model. This paper also greatly benefited from the thoughtful feedback of the
anonymous reviewers. Any opinions, findings, conclusions, or recommendations expressed
in this paper are those of the authors, and do not necessarily reflect the views of the funding
organizations.

160

fiContent Modeling Using Latent Permutations

References
Ailon, N., Charikar, M., & Newman, A. (2008). Aggregating inconsistent information:
Ranking and clustering. Journal of the ACM, 55 (5).
Althaus, E., Karamanis, N., & Koller, A. (2004). Computing locally coherent discourses.
In Proceedings of ACL.
Bartlett, F. C. (1932). Remembering: a study in experimental and social psychology. Cambridge University Press.
Barzilay, R., & Elhadad, N. (2003). Sentence alignment for monolingual comparable corpora.
In Proceedings of EMNLP.
Barzilay, R., Elhadad, N., & McKeown, K. (2002). Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Artificial Intelligence Research,
17, 3555.
Barzilay, R., & Lapata, M. (2008). Modeling local coherence: An entity-based approach.
Computational Linguistics, 34 (1), 134.
Barzilay, R., & Lee, L. (2004). Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of NAACL/HLT.
Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion in the context of
multi-document summarization. In Proceedings of ACL.
Beeferman, D., Berger, A., & Lafferty, J. D. (1999). Statistical models for text segmentation.
Machine Learning, 34, 177210.
Bernardo, J. M., & Smith, A. F. (2000). Bayesian Theory. Wiley Series in Probability and
Statistics.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Blei, D. M., & Moreno, P. J. (2001). Topic segmentation with an aspect hidden markov
model. In Proceedings of SIGIR.
Blei, D. M., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. Journal of Machine
Learning Research, 3, 9931022.
Bollegala, D., Okazaki, N., & Ishizuka, M. (2006). A bottom-up approach to sentence
ordering for multi-document summarization. In Proceedings of ACL/COLING.
Chen, H., Branavan, S., Barzilay, R., & Karger, D. R. (2009). Global models of document
structure using latent permutations. In Proceedings of NAACL/HLT.
Chinchor, N. (1995). Statistical significance of MUC-6 results. In Proceedings of the 6th
Conference on Message Understanding.
Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning to order things. Journal of
Artificial Intelligence Research, 10, 243270.
Eisenstein, J., & Barzilay, R. (2008). Bayesian unsupervised topic segmentation. In Proceedings of EMNLP.
Elsner, M., Austerweil, J., & Charniak, E. (2007). A unified local and global model for
discourse coherence. In Proceedings of NAACL/HLT.
161

fiChen, Branavan, Barzilay, & Karger

Fligner, M., & Verducci, J. (1986). Distance based ranking models. Journal of the Royal
Statistical Society, Series B, 48 (3), 359369.
Fligner, M. A., & Verducci, J. S. (1990). Posterior probabilities for a consensus ordering.
Psychometrika, 55 (1), 5363.
Galley, M., McKeown, K. R., Fosler-Lussier, E., & Jing, H. (2003). Discourse segmentation
of multi-party conversation. In Proceedings of ACL.
Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions and the bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12, 609628.
Graesser, A., Gernsbacher, M., & Goldman, S. (Eds.). (2003). Handbook of Discourse
Processes. Erlbaum.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National
Academy of Sciences, 101, 52285235.
Griffiths, T. L., Steyvers, M., Blei, D. M., & Tenenbaum, J. B. (2005). Integrating topics
and syntax. In Advances in NIPS.
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, and the structure of discourse.
Computational Linguistics, 12 (3), 175204.
Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2007). Hidden topic markov models. In Proceedings
of AISTATS.
Halliday, M. A. K., & Hasan, R. (1976). Cohesion in English. Longman.
Hearst, M. (1994). Multi-paragraph segmentation of expository text. In Proceedings of
ACL.
Ji, P. D., & Pulman, S. (2006). Sentence ordering with manifold-based classification in
multi-document summarization. In Proceedings of EMNLP.
Karamanis, N., Poesio, M., Mellish, C., & Oberlander, J. (2004). Evaluating centeringbased metrics of coherence for text structuring using a reliably annotated corpus. In
Proceedings of ACL.
Klementiev, A., Roth, D., & Small, K. (2008). Unsupervised rank aggregation with distancebased models. In Proceedings of the ICML, pp. 472479.
Lapata, M. (2003). Probabilistic text structuring: Experiments with sentence ordering. In
Proceedings of ACL.
Lapata, M. (2006). Automatic evaluation of information ordering: Kendalls tau. Computational Linguistics, 32 (4), 471484.
Lebanon, G., & Lafferty, J. (2002). Cranking: combining rankings using conditional probability models on permutations. In Proceedings of ICML.
Malioutov, I., & Barzilay, R. (2006). Minimum cut model for spoken lecture segmentation.
In Proceedings of ACL.
Mallows, C. L. (1957). Non-null ranking models. Biometrika, 44, 114130.

162

fiContent Modeling Using Latent Permutations

Meila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus ranking under the
exponential model. In Proceedings of UAI.
Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31, 705767.
Nelken, R., & Shieber, S. M. (2006). Towards robust context-sensitive sentence alignment
for monolingual corpora. In Proceedings of EACL.
Noreen, E. W. (1989). Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley.
Pevzner, L., & Hearst, M. A. (2002). A critique and improvement of an evaluation metric
for text segmentation. Computational Linguistics, 28, 1936.
Porteous, I., Newman, D., Ihler, A., Asuncion, A., Smyth, P., & Welling, M. (2008). Fast
collapsed gibbs sampling for latent dirichlet allocation. In Proceedings of SIGKDD.
Purver, M., Kording, K., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic
modelling for multi-party spoken discourse. In Proceedings of ACL/COLING.
Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation and user studies. In
Proceedings of ANLP/NAACL Summarization Workshop.
Riezler, S., & Maxwell, J. T. (2005). On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or Summarization.
Schiffrin, D., Tannen, D., & Hamilton, H. E. (Eds.). (2001). The Handbook of Discourse
Analysis. Blackwell.
Titov, I., & McDonald, R. (2008). Modeling online reviews with multi-grain topic models.
In Proceedings of WWW.
Utiyama, M., & Isahara, H. (2001). A statistical model for domain-independent text segmentation. In Proceedings of ACL.
van Mulbregt, P., Carp, I., Gillick, L., Lowe, S., & Yamron, J. (1998). Text segmentation and
topic tracking on broadcast news via a hidden markov model approach. In Proceedings
of ICSLP.
Wallach, H. M. (2006). Topic modeling: beyond bag of words. In Proceedings of ICML.
Wray, A. (2002). Formulaic Language and the Lexicon. Cambridge University Press, Cambridge.

163

fiJournal of Artificial Intelligence Research 36 (2009) 267-306

Submitted 06/09; published 10/09

ParamILS: An Automatic Algorithm Configuration Framework
Frank Hutter
Holger H. Hoos
Kevin Leyton-Brown

HUTTER @ CS . UBC . CA
HOOS @ CS . UBC . CA
KEVINLB @ CS . UBC . CA

University of British Columbia, 2366 Main Mall
Vancouver, BC, V6T1Z4, Canada

Thomas Stutzle

STUETZLE @ ULB . AC . BE

Universite Libre de Bruxelles, CoDE, IRIDIA
Av. F. Roosevelt 50 B-1050 Brussels, Belgium

Abstract
The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm
configuration problem. More formally, we provide methods for optimizing a target algorithms
performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and
present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation
of our methods, based on the configuration of prominent complete and incomplete algorithms for
SAT. We also present what is, to our knowledge, the first published work on automatically configuring the C PLEX mixed integer programming solver. All the algorithms we considered had default
parameter settings that were manually identified with considerable effort. Nevertheless, using our
automated algorithm configuration procedures, we achieved substantial and consistent performance
improvements.

1. Introduction
Many high-performance algorithms have parameters whose settings control important aspects of
their behaviour. This is particularly the case for heuristic procedures used for solving computationally hard problems.1 As an example, consider C PLEX, a commercial solver for mixed integer
programming problems.2 CPLEX version 10 has about 80 parameters that affect the solvers search
mechanism and can be configured by the user to improve performance. There are many acknowledgements in the literature that finding performance-optimizing parameter configurations of heuristic algorithms often requires considerable effort (see, e.g., Gratch & Chien, 1996; Johnson, 2002;
Diao, Eskesen, Froehlich, Hellerstein, Spainhower & Surendra, 2003; Birattari, 2004; Adenso-Diaz
& Laguna, 2006). In many cases, this tedious task is performed manually in an ad-hoc way. Automating this task is of high practical relevance in several contexts.
 Development of complex algorithms Setting the parameters of a heuristic algorithm is a
highly labour-intensive task, and indeed can consume a large fraction of overall development
1. Our use of the term heuristic algorithm includes methods without provable performance guarantees as well as
methods that have such guarantees, but nevertheless make use of heuristic mechanisms. In the latter case, the use
of heuristic mechanisms often results in empirical performance far better than the bounds guaranteed by rigorous
theoretical analysis.
2. http://www.ilog.com/products/cplex/
c
2009
AI Access Foundation. All rights reserved.

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

time. The use of automated algorithm configuration methods can lead to significant time
savings and potentially achieve better results than manual, ad-hoc methods.
 Empirical studies, evaluations, and comparisons of algorithms A central question in comparing heuristic algorithms is whether one algorithm outperforms another because it is fundamentally superior, or because its developers more successfully optimized its parameters (Johnson, 2002). Automatic algorithm configuration methods can mitigate this problem of unfair
comparisons and thus facilitate more meaningful comparative studies.
 Practical use of algorithms The ability of complex heuristic algorithms to solve large and
hard problem instances often depends critically on the use of suitable parameter settings.
End users often have little or no knowledge about the impact of an algorithms parameter
settings on its performance, and thus simply use default settings. Even if it has been carefully
optimized on a standard benchmark set, such a default configuration may not perform well on
the particular problem instances encountered by a user. Automatic algorithm configuration
methods can be used to improve performance in a principled and convenient way.
A wide variety of strategies for automatic algorithm configuration have been explored in the literature. Briefly, these include exhaustive enumeration, hill-climbing (Gratch & Dejong, 1992), beam
search (Minton, 1993), genetic algorithms (Terashima-Marn, Ross & Valenzuela-Rendon, 1999),
experimental design approaches (Coy, Golden, Runger & Wasil, 2001), sequential parameter optimization (Bartz-Beielstein, 2006), racing algorithms (Birattari, Stutzle, Paquete & Varrentrapp,
2002; Birattari, 2004; Balaprakash, Birattari & Stutzle, 2007), and combinations of fractional experimental design and local search (Adenso-Diaz & Laguna, 2006). We discuss this and other
related work more extensively in Section 9. Here, we note that while some other authors refer to the
optimization of an algorithms performance by setting its (typically few and numerical) parameters
as parameter tuning, we favour the term algorithm configuration (or simply, configuration). This is
motivated by the fact that we are interested in methods that can deal with a potentially large number
of parameters, each of which can be numerical, ordinal (e.g., low, medium, or high) or categorical (e.g., choice of heuristic). Categorical parameters can be used to select and combine discrete
building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics); consequently,
our general view of algorithm configuration includes the automated construction of a heuristic algorithm from such building blocks. To the best of our knowledge, the methods discussed in this article
are yet the only general ones available for the configuration of algorithms with many categorical
parameters.
We now give an overview of what follows and highlight our main contributions. After formally stating the algorithm configuration problem in Section 2, in Section 3 we describe ParamILS
(first introduced by Hutter, Hoos & Stutzle, 2007), a versatile stochastic local search approach for
automated algorithm configuration, and two of its instantiations, BasicILS and FocusedILS.
We then introduce adaptive capping of algorithm runs, a novel technique that can be used to
enhance search-based algorithm configuration procedures independently of the underlying search
strategy (Section 4). Adaptive capping is based on the idea of avoiding unnecessary runs of the
algorithm to be configured by developing bounds on the performance measure to be optimized.
We present a trajectory-preserving variant and a heuristic extension of this technique. After discussing experimental preliminaries in Section 5, in Section 6 we present empirical evidence showing that adaptive capping speeds up both BasicILS and FocusedILS. We also show that BasicILS
268

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

outperforms random search and a simple local search, as well as further evidence that FocusedILS
outperforms BasicILS.
We present extensive evidence that ParamILS can find substantially improved parameter configurations of complex and highly optimized algorithms. In particular, we apply our automatic
algorithm configuration procedures to the aforementioned commercial optimization tool C PLEX,
one of the most powerful, widely used and complex optimization algorithms we are aware of. As
stated in the C PLEX user manual (version 10.0, page 247), A great deal of algorithmic development effort has been devoted to establishing default ILOG C PLEX parameter settings that achieve
good performance on a wide variety of MIP models. We demonstrate consistent improvements
over this default parameter configuration for a wide range of practically relevant instance distributions. In some cases, we were able to achieve an average speedup of over an order of magnitude
on previously-unseen test instances (Section 7). We believe that these are the first results to be
published on automatically configuring C PLEX or any other piece of software of comparable complexity.
In Section 8 we review a wide range of (separately-published) ParamILS applications. Specifically, we survey work that has considered the optimization of complete and incomplete heuristic
search algorithms for the problems of propositional satisfiability (SAT), most probable explanation
(MPE), protein folding, university time-tabling, and algorithm configuration itself. In three of these
cases, ParamILS was an integral part of the algorithm design process and allowed the exploration of
very large design spaces. This could not have been done effectively in a manual way or by any other
existing automated method. Thus, automated algorithm configuration in general and ParamILS in
particular enables a new way of (semi-)automatic design of algorithms from components.
Section 9 presents related work and, finally, Section 10 offers discussion and conclusions. Here
we distill the common patterns that helped ParamILS to succeed in its various applications. We also
give advice to practitioners who would like to apply automated algorithm configuration in general
and ParamILS in particular, and identify promising avenues of research for future work.

2. Problem Statement and Notation
The algorithm configuration problem we consider in this work can be informally stated as follows:
given an algorithm, a set of parameters for the algorithm and a set of input data, find parameter
values under which the algorithm achieves the best possible performance on the input data.
To avoid potential confusion between algorithms whose performance is optimized and algorithms used for carrying out that optimization task, we refer to the former as target algorithms
and to the latter as configuration procedures (or simply configurators). This setup is illustrated in
Figure 1. Different algorithm configuration problems have also been considered in the literature, including setting parameters on a per-instance basis and adapting the parameters while the algorithm
is running. We defer a discussion of these approaches to Section 9.
In the following, we define the algorithm configuration problem more formally and introduce
notation that we will use throughout this article. Let A denote an algorithm, and let p1 , . . . , pk be
parameters of A. Denote the domain of possible values for each parameter pi as i . Throughout
this work, we assume that all parameter domains are finite sets. This assumption can be met by
discretizing all numerical parameters to a finite number of values. Furthermore, while parameters
269

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Figure 1: A configuration scenario includes an algorithm to be configured and a collection of problem instances. A configuration procedure executes the target algorithm with specified parameter settings
on some or all of the instances, receives information about the performance of these runs, and uses
this information to decide which subsequent parameter configurations to evaluate.

may be ordered, we do not exploit such ordering relations. Thus, we effectively assume that all
parameters are finite and categorical.3
Our problem formulation allows us to express conditional parameter dependencies (for example,
one algorithm parameter might be used to select among search heuristics, with each heuristics
behaviour controlled by further parameters). In this case, the values of these further parameters
are irrelevant if the heuristic is not selected. ParamILS exploits this and effectively searches the
space of equivalence classes in parameter configuration space. In addition, our formulation supports
constraints on feasible combinations of parameter values. We use   1  . . .  k to denote
the space of all feasible parameter configurations, and A() denoting the instantiation of algorithm
A with parameter configuration   .
Let D denote a probability distribution over a space  of problem instances, and denote an element of  as . D may be given implicitly, as through a random instance generator or a distribution
over such generators. It is also possible (and indeed common) for  to consist of a finite sample of
instances; in this case, we define D as the uniform distribution over .
There are many ways of measuring an algorithms performance. For example, we might be interested in minimizing computational resources consumed by the given algorithm (such as runtime,
memory or communication bandwidth), or in maximizing the quality of the solution found. Since
high-performance algorithms for computationally-challenging problems are often randomized, their
behaviour can vary significantly between multiple runs. Thus, an algorithm will not always achieve
the same performance, even when run repeatedly with fixed parameters on a single problem instance. Our overall goal must therefore be to choose parameter settings that minimize some cost
statistic of the algorithms performance across the input data. We denote this statistic as c(). For
example, we might aim to minimize mean runtime or median solution cost.
With this intuition in mind, we now define the algorithm configuration problem formally.
Definition 1 (Algorithm Configuration Problem). An instance of the algorithm configuration problem is a 6-tuple hA, , D, max , o, mi, where:
 A is a parameterized algorithm;
  is the parameter configuration space of A;
3. We are currently extending our algorithm configuration procedures to natively support other parameter types.

270

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

 D is a distribution over problem instances with domain ;
 max is a cutoff time (or captime), after which each run of A will be terminated if still running;
 o is a function that measures the observed cost of running A() on an instance    with
captime   R (examples are runtime for solving the instance, or cost of the solution found)
 m is a statistical population parameter (such as expectation, median, or variance).
Any parameter configuration    is a candidate solution of the algorithm configuration
problem. For each configuration , O denotes the distribution of costs induced by function o,
applied to instances  drawn from distribution D and multiple independent runs for randomized
algorithms, using captime  = max . The cost of a candidate solution  is defined as
c() := m(O ),

(1)

the statistical population parameter m of the cost distribution O . An optimal solution,   , minimizes c():
   arg min c().
(2)


An algorithm configuration procedure is a procedure for solving the algorithm configuration
problem. Unfortunately, at least for the algorithm configuration problems considered in this article, we cannot optimize c in closed form since we do not have access to an algebraic representation of the function. We denote the sequence of runs executed by a configurator as R =
((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , on )). The ith run is described by five values:
 i   denotes the parameter configuration being evaluated;
 i   denotes the instance on which the algorithm is run;
 si denotes the random number seed used in the run (we keep track of seeds to be able to block
on them, see Section 5.1.2);
 i denotes the runs captime; and
 oi denotes the observed cost of the run
Note that each of , , s, , and o can vary from one element of R to the next, regardless of whether
or not other elements are held constant. We denote the ith run of R as R[i], and the subsequence
of runs using parameter configuration  (i.e., those runs with i = ) as R . The configuration
procedures considered in this article compute empirical estimates of c() based solely on R , but
in principle other methods could be used. We compute these cost estimates both online, during
runtime of a configurator, as well as offline, for evaluation purposes.
Definition 2 (Cost Estimate). Given an algorithm configuration problem hA, , D, max , o, mi,
we define a cost estimate of a cost c() based on a sequence of runs R = ((1 , 1 , s1 , 1 , o1 ), . . . ,
(n , n , sn , n , on )) as c(, R) := m({oi | i = }), where m is the sample statistic analogue to
the statistical population parameter m.
For example, when c() is the expected runtime over a distribution of instances and random number
seeds, c(, R) is the sample mean runtime of runs R .
All configuration procedures in this paper are anytime algorithms, meaning that at all times they
keep track of the configuration currently believed to have the lowest cost; we refer to this configuration as the incumbent configuration, or in short the incumbent, inc . We evaluate a configurators
performance at time t by means of its incumbents training and test performance, defined as follows.
271

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Definition 3 (Training performance). When at some time t a configurator has performed a sequence of runs R = ((1 , 1 , s1 , 1 , o1 ), . . . , (n , n , sn , n , on )) to solve an algorithm configuration problem hA, , D, max , o, mi, and has thereby found incumbent configuration inc , then its
training performance at time t is defined as the cost estimate c(inc , R).
The set of instances {1 , . . . , n } discussed above is called the training set. While the true cost
of a parameter configuration cannot be computed exactly, it can be estimated using training performance. However, the training performance of a configurator is a biased estimator of its incumbents
true cost, because the same instances are used for selecting the incumbent as for evaluating it. In
order to achieve unbiased estimates during offline evaluation, we set aside a fixed set of instances
{10 , . . . , T0 } (called the test set) and random number seeds {s01 , . . . , s0T }, both unknown to the
configurator, and use these for evaluation.
Definition 4 (Test performance). At some time t, let a configurators incumbent for an algorithm
configuration problem hA, , D, max , o, mi be inc (this is found by means of executing a sequence of runs on the training set). Furthermore, let R0 = ((inc , 10 , s01 , max , o1 ), . . . , (inc , T0 ,
s0T , max , oT )) be a sequence of runs on the T instances and random number seeds in the test set
(which is performed offline for evaluation purposes), then the configurators test performance at
time t is defined as the cost estimate c(inc , R0 ).
Throughout this article, we aim to minimize expected runtime. (See Section 5.1.1 for a discussion
of that choice.) Thus, a configurators training performance is the mean runtime of the runs it
performed with the incumbent. Its test performance is the mean runtime of the incumbent on the
test set. Note that, while the configurator is free to use any i  max , test performance is always
computed using the maximal captime, max .
It is not obvious how an automatic algorithm configurator should choose runs in order to best
minimize c() within a given time budget. In particular, we have to make the following choices:
1. Which parameter configurations 0   should be evaluated?
2. Which problem instances 0   should be used for evaluating each  0  0 , and how
many runs should be performed on each instance?
3. Which cutoff time i should be used for each run?
Hutter, Hoos and Leyton-Brown (2009) considered this design space in detail, focusing on the
tradeoff between the (fixed) number of problem instances to be used for the evaluation of each
parameter configuration and the (fixed) cutoff time used for each run, as well as the interaction of
these choices with the number of configurations that can be considered. In contrast, here, we study
adaptive approaches for selecting the number of problem instances (Section 3.3) and the cutoff
time for the evaluation of a parameter configuration (Section 4); we also study which configurations
should be selected (Sections 3.1 and 6.2).

3. ParamILS: Iterated Local Search in Parameter Configuration Space
In this section, we address the first and most important of the previously mentioned dimensions
of automated algorithm configuration, the search strategy, by describing an iterated local search
framework called ParamILS. To start with, we fix the other two dimensions, using an unvarying
benchmark set of instances and fixed cutoff times for the evaluation of each parameter configuration. Thus, the stochastic optimization problem of algorithm configuration reduces to a simple
272

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

optimization problem, namely to find the parameter configuration that yields the lowest mean runtime on the given benchmark set. Then, in Section 3.3, we address the second question of how many
runs should be performed for each configuration.
3.1 The ParamILS framework
Consider the following manual parameter optimization process:
1. begin with some initial parameter configuration;
2. experiment with modifications to single parameter values, accepting new configurations whenever they result in improved performance;
3. repeat step 2 until no single-parameter change yields an improvement.
This widely used procedure corresponds to a manually-executed local search in parameter configuration space. Specifically, it corresponds to an iterative first improvement procedure with a search
space consisting of all possible configurations, an objective function that quantifies the performance
achieved by the target algorithm with a given configuration, and a neighbourhood relation based on
the modification of one single parameter value at a time (i.e., a one-exchange neighbourhood).
Viewing this manual procedure as a local search algorithm is advantageous because it suggests
the automation of the procedure as well as its improvement by drawing on ideas from the stochastic
local search community. For example, note that the procedure stops as soon as it reaches a local optimum (a parameter configuration that cannot be improved by modifying a single parameter value).
A more sophisticated approach is to employ iterated local search (ILS; Lourenco, Martin & Stutzle,
2002) to search for performance-optimizing parameter configurations. ILS is a prominent stochastic
local search method that builds a chain of local optima by iterating through a main loop consisting of
(1) a solution perturbation to escape from local optima, (2) a subsidiary local search procedure and
(3) an acceptance criterion to decide whether to keep or reject a newly obtained candidate solution.
ParamILS (given in pseudocode as Algorithm 1) is an ILS method that searches parameter configuration space. It uses a combination of default and random settings for initialization, employs
iterative first improvement as a subsidiary local search procedure, uses a fixed number (s) of random moves for perturbation, and always accepts better or equally-good parameter configurations,
but re-initializes the search at random with probability prestart .4 Furthermore, it is based on a
one-exchange neighbourhood, that is, we always consider changing only one parameter at a time.
ParamILS deals with conditional parameters by excluding all configurations from the neighbourhood of a configuration  that differ only in a conditional parameter that is not relevant in .
3.2 The BasicILS Algorithm
In order to turn ParamILS as specified in Algorithm Framework 1 into an executable configuration
procedure, it is necessary to instantiate the function better that determines which of two parameter settings should be preferred. We will ultimately propose several different ways of doing this.
Here, we describe the simplest approach, which we call BasicILS. Specifically, we use the term
BasicILS(N ) to refer to a ParamILS algorithm in which the function better(1 , 2 ) is implemented
as shown in Procedure 2: simply comparing estimates cN of the cost statistics c(1 ) and c(2 ) that
are based on N runs each.
4. Our original parameter choices hr, s, prestart i = h10, 3, 0.01i (from Hutter et al., 2007) were somewhat arbitrary,
though we expected performance to be quite robust with respect to these settings. We revisit this issue in Section 8.4.

273

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Algorithm Framework 1: ParamILS(0 , r, prestart , s)
Outline of iterated local search in parameter configuration space; the specific variants of ParamILS
we study, BasicILS(N) and FocusedILS, are derived from this framework by instantiating procedure
better (which compares ,  0  ). BasicILS(N) uses betterN (see Procedure 2), while FocusedILS
uses betterF oc (see Procedure 3). The neighbourhood Nbh() of a configuration  is the set of all
configurations that differ from  in one parameter, excluding configurations differing in a conditional
parameter that is not relevant in .
Input : Initial configuration 0  , algorithm parameters r, prestart , and s.
Output : Best parameter configuration  found.
1 for i = 1, . . . , r do
2
  random   ;
3
if better(, 0 ) then 0  ;
4
5
6

ils  IterativeFirstImprovement (0 );
while not TerminationCriterion() do
  ils ;

7

// ===== Perturbation
for i = 1, . . . , s do   random  0  Nbh();

8

// ===== Basic local search
  IterativeFirstImprovement ();

9
10

// ===== AcceptanceCriterion
if better(, ils ) then ils  ;
with probability prestart do ils  random   ;

11

return overall best inc found;

12
13
14
15
16

Procedure IterativeFirstImprovement ()
repeat
 0  ;
foreach  00  N bh( 0 ) in randomized order do
if better( 00 ,  0 ) then    00 ; break;

17
18

until  0 = ;
return ;

BasicILS(N ) is a simple and intuitive approach since it evaluates every parameter configuration
by running it on the same N training benchmark instances using the same random number seeds.
Like many other related approaches (see, e.g., Minton, 1996; Coy et al., 2001; Adenso-Diaz &
Laguna, 2006), it deals with the stochastic part of the optimisation problem by using an estimate
based on a fixed training set of N instances. When benchmark instances are very heterogeneous or
Procedure 2: betterN (1 , 2 )
Procedure used in BasicILS(N ) and RandomSearch(N ) to compare two parameter configurations. Procedure objective(, N ) returns the user-defined objective achieved by A() on the first N instances
and keeps track of the incumbent solution, inc ; it is detailed in Procedure 4 on page 279.
Input
: Parameter configuration 1 , parameter configuration 2
Output
: True if 1 does better than or equal to 2 on the first N instances; false otherwise
Side Effect : Adds runs to the global caches of performed algorithm runs R1 and R2 ; potentially
updates the incumbent inc
1 cN (2 )  objective(2 , N )
2 cN (1 )  objective(1 , N )
3 return cN (1 )  cN (2 )

274

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

when the user can identify a rather small representative subset of instances, this approach can find
good parameter configurations with low computational effort.
3.3 FocusedILS: Adaptively Selecting the Number of Training Instances
The question of how to choose the number of training instances, N , in BasicILS(N ) has no straightforward answer: optimizing performance using too small a training set leads to good training
performance, but poor generalization to previously unseen test benchmarks. On the other hand,
we clearly cannot evaluate every parameter configuration on an enormous training setif we did,
search progress would be unreasonably slow.
FocusedILS is a variant of ParamILS that deals with this problem by adaptively varying the
number of training samples considered from one parameter configuration to another. We denote
the number of runs available to estimate the cost statistic c() for a parameter configuration  by
N (). Having performed different numbers of runs using different parameter configurations, we
face the question of comparing two parameter configurations  and  0 for which N ()  N ( 0 ).
One option would be simply to compute the empirical cost statistic based on the available number
of runs for each configuration. However, this can lead to systematic biases if, for example, the first
instances are easier than the average instance. Instead, we compare  and  0 based on N () runs
on the same instances and seeds. This amounts to a blocking strategy, which is a straight-forward
adaptation of a known variance reduction technique; see 5.1 for a more detailed discussion.
This approach to comparison leads us to a concept of domination. We say that  dominates  0
when at least as many runs have been conducted on  as on  0 , and the performance of A() on the
first N ( 0 ) runs is at least as good as that of A( 0 ) on all of its runs.
Definition 5 (Domination). 1 dominates 2 if and only if N (1 )  N (2 ) and cN (2 ) (1 ) 
cN (2 ) (2 ).
Now we are ready to discuss the comparison strategy encoded in procedure betterF oc (1 , 2 ),
which is used by the FocusedILS algorithm (see Procedure 3). This procedure first acquires one
additional sample for the configuration i having smaller N (i ), or one run for both configurations if
they have the same number of runs. Then, it continues performing runs in this way until one configuration dominates the other. At this point it returns true if 1 dominates 2 , and false otherwise. We
also keep track of the total number of configurations evaluated since the last improving step (i.e.,
since the last time betterF oc returned true); we denote this number as B. Whenever betterF oc (1 , 2 )
returns true, we perform B bonus runs for 1 and reset B to 0. This mechanism ensures that we
perform many runs with good configurations, and that the error made in every comparison of two
configurations 1 and 2 decreases on expectation.
It is not difficult to show that in the limit, FocusedILS will sample every parameter configuration
an unbounded number of times. The proof relies on the fact that, as an instantiation of ParamILS,
FocusedILS performs random restarts with positive probability.
Lemma 6 (Unbounded number of evaluations). Let N (J, ) denote the number of runs FocusedILS
has performed with parameter configuration  at the end of ILS iteration J to estimate c(). Then,
for any constant K and configuration    (with finite ||), limJ P [N (J, )  K] = 1.
Proof. After each ILS iteration of ParamILS, with probability prestart > 0 a new configuration is
picked uniformly at random, and with probability 1/||, this is configuration . The probability of
275

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Procedure 3: betterF oc (1 , 2 )
Procedure used in FocusedILS to compare two parameter configurations. Procedure objective(, N )
returns the user-defined objective achieved by A() on the first N instances, keeps track of the incumbent solution, and updates R (a global cache of algorithm runs performed with parameter configuration ); it is detailed in Procedure 4 on page 279. For each , N () = length(R ). B is a global
counter denoting the number of configurations evaluated since the last improvement step.
Input
: Parameter configuration 1 , parameter configuration 2
Output
: True if 1 dominates 2 , false otherwise
Side Effect: Adds runs to the global caches of performed algorithm runs R1 and R2 ; updates the
global counter B of bonus runs, and potentially the incumbent inc
1 B B+1
2 if N (1 )  N (2 ) then
3
min  1 ; max  2
4
if N (1 ) = N (2 ) then B  B + 1
else min  2 ; max  1
repeat
i  N (min ) + 1
ci (max )  objective(max , i) // If N (min ) = N (max ), adds a new run to Rmax .
ci (min )  objective(min , i) // Adds a new run to Rmin .
10 until dominates(1 , 2 ) or dominates(2 , 1 )
11 if dominates(1 , 2 ) then
5
6
7
8
9

// ===== Perform B bonus runs.
12
13
14

cN (1 )+B (1 )  objective(1 , N (1 ) + B) // Adds B new runs to R1 .
B 0
return true

15

else return false

16
17
18

Procedure dominates(1 , 2 )
if N (1 ) < N (2 ) then return false
return objective(1 , N (2 ))  objective(2 , N (2 ))

visiting  in an ILS iteration is thus p  prestart
> 0. Hence, the number of runs performed with  is
||
lower-bounded by a binomial random
 variable B(k; J, p). Then, for any constant k < K we obtain
limJ B(k; J, p) = limJ Jk pk (1  p)Jk = 0. Thus, limJ P [N (J, )  K] = 1.
Definition 7 (Consistent estimator). cN () is a consistent estimator for c() iff
 > 0 : lim P (|cN ()  c()| < ) = 1.
N 

When cN () is a consistent estimator of c(), cost estimates become more and more reliable
as N approaches infinity, eventually eliminating overconfidence and the possibility of mistakes in
comparing two parameter configurations. This fact is captured in the following lemma.
Lemma 8 (No mistakes for N  ). Let 1 , 2   be any two parameter configurations with
c(1 ) < c(2 ). Then, for consistent estimators cN , limN  P (cN (1 )  cN (2 )) = 0.
Proof. Write c1 as shorthand for c(1 ), c2 for c(2 ), c1 for cN (1 ), and c2 for cN (2 ). Define
m = 21  (c2 + c1 ) as the midpoint between c1 and c2 , and  = c2  m = m  c1 > 0 as
its distance from each of the two points. Since cN is a consistent estimator for c, the estimate
c1 comes arbitrarily close to the real cost c1 . That is, limN  P (|c1  c1 | < ) = 1. Since
276

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

|m  c1 | = , the estimate c1 cannot be greater than or equal to m: limN  P (c1  m) = 0.
Similarly, limN  P (c2 < m) = 0. Since
P (c1  c2 ) = P (c1  c2  c1  m) + P (c1  c2  c1 < m)
= P (c1  c2  c1  m) + P (c1  c2  c1 < m  c2 < m)
 P (c1  m) + P (c2 < m),
we have limN  P (c1  c2 )  limN  (P (c1  m) + P (c2 < m)) = 0 + 0 = 0.
Combining our two lemmata we can now show that in the limit, FocusedILS is guaranteed to
converge to the true best parameter configuration.
Theorem 9 (Convergence of FocusedILS). When FocusedILS optimizes a cost statistic c based on
a consistent estimator cN , the probability that it finds the true optimal parameter configuration  
approaches one as the number of ILS iterations goes to infinity.
Proof. According to Lemma 6, N () grows unboundedly for each   . For each 1 , 2 , as
N (1 ) and N (2 ) go to infinity, Lemma 8 states that in a pairwise comparison, the truly better
configuration will be preferred. Thus eventually, FocusedILS visits all finitely many parameter
configurations and prefers the best one over all others with probability arbitrarily close to one.
We note that in many practical scenarios cost estimators may not be consistentthat is, they
may fail to closely approximate the true performance of a given parameter configuration even for
a large number of runs of the target algorithm. For example, when a finite training set, , is used
during configuration rather than a distribution over problem instances, D, then even for large N , cN
will only accurately reflect the cost of parameter configurations on the training set, . For small
training sets, , the cost estimate based on  may differ substantially from the true cost as defined
by performance across the entire distribution, D. The larger the training set, , the smaller the
expected difference (it vanishes as training set size goes to infinity). Thus, it is important to use
large training sets (which are representative of the distribution of interest) whenever possible.

4. Adaptive Capping of Algorithm Runs
Now we consider the last of our dimensions of automated algorithm configuration, the cutoff time
for each run of the target algorithm. We introduce an effective and simple capping technique that
adaptively determines the cutoff time for each run. The motivation for this capping technique comes
from a problem encountered by all configuration procedures considered in this article: often the
search for a performance-optimizing parameter setting spends a lot of time with evaluating a parameter configuration that is much worse than other, previously-seen configurations.
Consider, for example, a case where parameter configuration 1 takes a total of 10 seconds to
solve N = 100 instances (i.e., it has a mean runtime of 0.1 seconds per instance), and another parameter configuration 2 takes 100 seconds to solve the first of these instances. In order to compare
the mean runtimes of 1 and 2 based on this set of instances, knowing all runtimes for 1 , it is
not necessary to run 2 on all 100 instances. Instead, we can already terminate the first run of 2
after 10 +  seconds. This results in a lower bound on 2 s mean runtime of 0.1 + /100 since
the remaining 99 instances could take no less than zero time. This lower bound exceeds the mean
runtime of 1 , and so we can already be certain that the comparison will favour 1 . This insight
provides the basis for our adaptive capping technique.
277

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

4.1 Adaptive Capping in BasicILS
In this section, we introduce adaptive capping for BasicILS. We first introduce a trajectory-preserving
version of adaptive capping (TP capping) that provably does not change BasicILSs search trajectory and can lead to large computational savings. We then modify this strategy heuristically to
perform more aggressive adaptive capping (Aggr capping), potentially yielding even better performance in practice.
4.1.1 T RAJECTORY- PRESERVING C APPING
Observe that all comparisons between parameter configurations in ParamILS are pairwise. In
BasicILS(N ), these comparisons are based on Procedure betterN (1 , 2 ), where 2 is either the
best configuration encountered in this ILS iteration or the best configuration of the last ILS iteration. Without adaptive capping, these comparisons can take a long time, since a poor parameter
configuration  can easily take more than an order of magnitude longer than good configurations.
For the case of optimizing the mean of non-negative cost functions (such as runtime or solution
cost), we implement a bounded evaluation of a parameter configuration  based on N runs and a
given performance bound in Procedure objective (see Procedure 4). This procedure sequentially
performs runs for parameter configuration  and after each run computes a lower bound on cN ()
based on the i  N runs performed so far. Specifically, for our objective of mean runtime we
sum the runtimes of each of the i runs, and divide this sum by N ; since all runtimes must be
nonnegative, this quantity lower bounds cN (). Once the lower bound exceeds the bound passed
as an argument, we can skip the remaining runs for . In order to pass the appropriate bounds to
Procedure objective, we need to slightly modify Procedure betterN (see Procedure 2 on page 274)
for adaptive capping. Procedure objective now has a bound as an additional third argument, which
is set to  in line 1 of betterN , and to cN (2 ) in line 2.
Because this approach results in the computation of exactly the same function betterN as used in
the original version of BasicILS, the modified procedure follows exactly the same search trajectory
it would have followed without capping, but typically requires much less runtime. Hence, within
the same amount of overall running time, this new version of BasicILS tends to be able to search a
larger part of the parameter configuration space. Although in this work we focus on the objective of
minimizing mean runtime for decision algorithms, we note that our adaptive capping approach can
be applied easily to other configuration objectives.
4.1.2 AGGRESSIVE C APPING
As we demonstrate in Section 6.4, the use of trajectory-preserving adaptive capping can result in
substantial speedups of BasicILS. However, sometimes this approach is still less efficient than it
could be. This is because the upper bound on cumulative runtime used for capping is computed from
the best configuration encountered in the current ILS iteration (where a new ILS iteration begins
after each perturbation), as opposed to the overall incumbent. After a perturbation has resulted in
a new parameter configuration , the new iterations best configuration is initialized to . In the
frequent case that this new  performs poorly, the capping criterion does not apply as quickly as
when the comparison is performed against the overall incumbent.
To counteract this effect, we introduce a more aggressive capping strategy that can terminate
the evaluation of a poorly-performing configuration at any time. In this heuristic extension of our
adaptive capping technique, we bound the evaluation of any parameter configuration by the per278

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

Procedure 4: objective(, N, optional parameter bound)
Procedure that computes cN (), either by performing new runs or by exploiting previous cached runs.
An optional third parameter specifies a bound on the computation to be performed; when this parameter
is not specified, the bound is taken to be . For each , N () is the number of runs performed for ,
i.e., the length of the global array R . When computing runtimes, we count unsuccessful runs as 10
times their cutoff time.
Input
: Parameter configuration , number of runs, N , optional bound bound
Output
: cN () if cN ()  bound, otherwise a large constant (maxPossibleObjective) plus the
number of instances that remain unsolved when the bound was exceeded
Side Effect: Adds runs to the global cache of performed algorithm runs, R ; updates global
incumbent, inc
// ===== Maintain invariant: N (inc )  N () for any 
1
2

if  =
6 inc and N (inc ) < N then
cN (inc )  objective(inc , N, ) // Adds N  N (inc ) runs to Rinc
// ===== For aggressive capping, update bound.

3

if Aggressive capping then bound  min(bound, bm  cN (inc ))
// ===== Update the run results in tuple R .

for i = 1...N do
sum runtime  sum of runtimes in R [1], . . . , R [i  1] // Tuple indices starting at 1.
0i  max(max , N  bound sum runtime)
if N ()  i then (, i , i , oi )  R [i]
if N ()  i and ((i  0i and oi = unsuccessful) or (i < 0i and oi 6= unsuccessful))
then o0i  oi // Previous run is longer yet unsuccessful or shorter yet successful  can re-use result
9
else
10
o0i  objective from a newly executed run of A() on instance i with seed si and captime i
4
5
6
7
8

11
12
13
14

R [i]  (, i , 0i , o0i )
if 1/N  (sum runtime + o0i ) > bound then return maxPossibleObjective + (N + 1)  i
if N = N (inc ) and (sum of runtimes in R ) < (sum of runtimes in Rinc ) then inc  
return 1/N  (sum of runtimes in R )

formance of the incumbent parameter configuration multiplied by a factor that we call the bound
multiplier, bm. When a comparison between any two parameter configurations  and  0 is performed and the evaluations of both are terminated preemptively, the configuration having solved
more instances within the allowed time is taken to be the better one. (This behaviour is achieved
by line 12 in Procedure objective, which keeps track of the number of instances solved when exceeding the bound.) Ties are broken to favour moving to a new parameter configuration instead of
staying with the current one.
Depending on the bound multiplier, the use of this aggressive capping mechanism may change
the search trajectory of BasicILS. For bm =  the heuristic method reduces to our trajectorypreserving method, while a very aggressive setting of bm = 1 means that once we know a parameter
configuration to be worse than the incumbent, we stop its evaluation. In our experiments we set
bm = 2, meaning that once the lower bound on the performance of a configuration exceeds twice
the performance of the incumbent solution, its evaluation is terminated. (In Section 8.4, we revisit
this choice of bm = 2, configuring the parameters of ParamILS itself.)
279

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

4.2 Adaptive Capping in FocusedILS
The main difference between BasicILS and FocusedILS is that the latter adaptively varies the number of runs used to evaluate each parameter configuration. This difference complicates, but does
not prevent the use of adaptive capping. This is because FocusedILS always compares pairs of parameter configurations based on the same number of runs for each configuration, even though this
number can differ from one comparison to the next.
Thus, we can extend adaptive capping to FocusedILS by using separate bounds for every number
of runs, N . Recall that FocusedILS never moves from one configuration, , to a neighbouring
configuration,  0 , without performing at least as many runs for  0 as have been performed for .
Since we keep track of the performance of  with any number of runs M  N (), a bound for the
evaluation of  0 is always available. Therefore, we can implement both trajectory-preserving and
aggressive capping as we did for BasicILS.
As for BasicILS, for FocusedILS the inner workings of adaptive capping are implemented in
Procedure objective (see Procedure 4). We only need to modify Procedure betterF oc (see Procedure
3 on page 276) to call objective with the right bounds. This leads to the following changes in
Procedure betterF oc . Subprocedure dominates on line 16 now takes a bound as an additional
argument and passes it on to the two calls to objective in line 18. The two calls of dominates in
line 10 and the one call in line 11 all use the bound cmax . The three direct calls to objective in
lines 8, 9, and 12 use bounds , cmax , and , respectively.

5. Experimental Preliminaries
In this section we give background information about the computational experiments presented in
the following sections. First, we describe the design of our experiments. Next, we present the
configuration scenarios (algorithm/benchmark data combinations) studied in the following section.
Finally, we describe the low-level details of our experimental setup.
5.1 Experimental Design
Here we describe our objective function and the methods we used for selecting instances and seeds.
5.1.1 C ONFIGURATION O BJECTIVE : P ENALIZED AVERAGE RUNTIME
In Section 2, we mentioned that algorithm configuration problems arise in the context of various
different cost statistics. Indeed, in our past work we explored several of them: maximizing solution
quality achieved in a given time, minimizing the runtime required to reach a given solution quality,
and minimizing the runtime required to solve a single problem instance (Hutter et al., 2007).
In this work we focus on the objective of minimizing the mean runtime over instances from
a distribution D. This optimization objective naturally occurs in many practical applications. It
also implies a strong correlation between c() and the amount of time required to obtain a good
empirical estimate of c(). This correlation helps to make our adaptive capping scheme effective.
One might wonder whether means are the right way to aggregate runtimes. In some preliminary
experiments, we found that minimizing mean runtime led to parameter configurations with overall good runtime performance, including rather competitive median runtimes, while minimizing
median runtime yielded less robust parameter configurations that timed out on a large (but < 50%)
fraction of the benchmark instances. However, when we encounter runs that do not terminate within
280

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

the given cutoff time the mean is ill-defined. In order to penalize timeouts, we define the penalized
average runtime (PAR) of a set of runs with cutoff time max to be the mean runtime over those
runs, where unsuccessful runs are counted as p  max with penalization constant p  1. In this
study, we use p = 10.
5.1.2 S ELECTING I NSTANCES AND S EEDS
As mentioned previously, often only a finite set  of instances is available upon which to evaluate
our algorithm. This is the case in the experiments we report here. Throughout our study, all configuration experiments are performed on a training set containing half of the given benchmark instances.
The remaining instances are solely used as a test set to evaluate the found parameter configurations.
For evaluations within ParamILS that are based on N runs, we selected the N instances and
random number seeds to be used by following a common blocking technique (see, e.g., Birattari
et al., 2002; Ridge & Kudenko, 2006). We ensured that whenever two parameter configurations
were compared, their cost estimates were based on exactly the same instances and seeds. This
serves to avoid noise effects due to differences between instances and the use of different seeds. For
example, it prevents us from making the mistake of considering configuration  to be better than
configuration  0 just because  was tested on easier instances.
When dealing with randomized target algorithms, there is also a tradeoff between the number
of problem instances used and the number of independent runs performed on each instance. In
the extreme case, for a given sample size N , one could perform N runs on a single instance or a
single run on N different instances. This latter strategy is known to result in minimal variance of
the estimator for common optimization objectives such as minimization of mean runtime (which
we consider in this study) or maximization of mean solution quality (see, e.g., Birattari, 2004).
Consequently, we only performed multiple runs per instance when we wanted to acquire more
samples of the cost distribution than there were instances in the training set.
Based on these considerations, the configuration procedures we study in this article have been
implemented to take a list of hinstance, random number seedi pairs as one of their inputs. Empirical
estimates cN () of the cost statistic c() to be optimized were determined from the first N hinstance,
seedi pairs in that list. Each list of hinstance, seedi pairs was constructed as follows. Given a training
set consisting of M problem instances, for N  M , we drew a sample of N instances uniformly at
random and without replacement and added them to the list. If we wished to evaluate an algorithm
on more samples than we had training instances, which could happen in the case of randomized
algorithms, we repeatedly drew random samples of size M as described before, where each such
batch corresponded to a random permutation of the N training instances, and added a final sample
of size N mod M < M , as in the case N  M . As each sample was drawn, it was paired with
a random number seed that was chosen uniformly at random from the set of all possible seeds and
added to the list of hinstance, seedi pairs.
5.1.3 C OMPARISON OF C ONFIGURATION P ROCEDURES
Since the choice of instances (and to some degree of seeds) is very important for the final outcome
of the optimization, in our experimental evaluations we always performed a number of independent
runs of each configuration procedure (typically 25). We created a separate list of instances and seeds
for each run as explained above, where the kth run of each configuration procedure uses the same
kth list of instances and seeds. (Note, however, that the disjoint test set used to measure performance
of parameter configurations was identical for all runs.)
281

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Configuration scenario
S A P S -SWGCP
S P E A R -SWGCP
S A P S -QCP
S P E A R -QCP
C P L E X -R E G I O N S 100

Type of benchmark instances & citation
Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)
Graph colouring (Gent, Hoos, Prosser & Walsh, 1999)
Quasigroup completion (Gomes & Selman, 1997)
Quasigroup completion (Gomes & Selman, 1997)
Combinatorial Auctions (CATS) (Leyton-Brown, Pearson & Shoham, 2000)

Table 1: Overview of our five B R O A D configuration scenarios.
Algorithm
S APS
S PEAR

C PLEX

Parameter type
Continuous
Categorical
Integer
Continuous
Categorical
Integer
Continuous

# parameters of type
4
10
4
12
50
8
5

# values considered
7
220
58
36
27
57
35

Total # configurations, ||
2 401
8.34  1017

1.38  1037

Table 2: Parameter overview for the algorithms we consider. More information on the parameters for each algorithm is given in the text.
A detailed list of all parameters and the values we considered can be found in an online appendix at
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/algorithms.html.
We performed a paired statistical test to compare the final results obtained in the runs of two
configuration procedures. A paired test was required since the kth run of both procedures shared the
same kth list of instances and seeds. In particular, we performed a two-sided paired Max-Wilcoxon
test with the null hypothesis that there was no difference in the performances, considering p-values
below 0.05 to be statistically significant. The p-values reported in all tables were derived using this
test; p-values shown in parentheses refer to cases where the procedure we expected to perform better
actually performed worse.
5.2 Configuration Scenarios
In Section 6, we analyze our configurators based on five configuration scenarios, each combining a
high-performance algorithm with a widely-studied benchmark dataset. Table 1 gives an overview
of these, which we dub the B R O A D scenarios. The algorithms and benchmark instance sets used
in these scenarios are described in detail in Sections 5.2.1 and 5.2.2, respectively. In these five
B R O A D configuration scenarios, we set fairly aggressive cutoff times of five seconds per run of the
target algorithm and allowed each configuration procedure to execute the target algorithm for an
aggregate runtime of five CPU hours. These short cutoff times and fairly short times for algorithm
configuration were deliberately chosen to facilitate many configuration runs for each B R O A D scenario. In contrast, in a second set of configuration scenarios (exclusively focusing on C PLEX), we
set much larger cutoff times and allowed more time for configuration. We defer a description of
these scenarios to Section 7.
5.2.1 TARGET A LGORITHMS
Our three target algorithms are listed in Table 2 along with their configurable parameters.
282

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

S APS The first target algorithm used in our experiments was S APS, a high-performance dynamic
local search algorithm for SAT solving (Hutter, Tompkins & Hoos, 2002) as implemented in UBCSAT (Tompkins & Hoos, 2004). When introduced in 2002, S APS was a state-of-the-art solver, and
it still performs competitively on many instances. We chose to study this algorithm because it is
well known, it has relatively few parameters, and we are intimately familiar with it. S APSs four
continuous parameters control the scaling and smoothing of clause weights, as well as the probability of random walk steps. The original default parameters were set manually based on experiments
with prominent benchmark instances; this manual experimentation kept the percentage of random
steps fixed and took up about one week of development time. Having subsequently gained more
experience with S APSs parameters for more general problem classes (Hutter, Hamadi, Hoos &
Leyton-Brown, 2006), we chose promising intervals for each parameter, including, but not centered
at, the original default. We then picked seven possible values for each parameter spread uniformly
across its respective interval, resulting in 2401 possible parameter configurations (these are exactly
the same values as used by Hutter et al., 2007). As the starting configuration for ParamILS, we used
the center point of each parameters domain.
S PEAR The second target algorithm we considered was S PEAR, a recent tree search algorithm
for solving SAT problems. S PEAR is a state-of-the-art SAT solver for industrial instances, and
with appropriate parameter settings it is the best available solver for certain types of hardware and
software verification instances (Hutter, Babic, Hoos & Hu, 2007). Furthermore, configured with
ParamILS, S PEAR won the quantifier-free bit-vector arithmetic category of the 2007 Satisfiability
Modulo Theories Competition. S PEAR has 26 parameters, including ten categorical, four integer,
and twelve continuous parameters, and their default values were manually engineered by its developer. (Manual tuning required about one week.) The categorical parameters mainly control
heuristics for variable and value selection, clause sorting, resolution ordering, and enable or disable
optimizations, such as the pure literal rule. The continuous and integer parameters mainly deal with
activity, decay, and elimination of variables and clauses, as well as with the interval of randomized
restarts and percentage of random choices. We discretized the integer and continuous parameters
by choosing lower and upper bounds at reasonable values and allowing between three and eight
discrete values spread relatively uniformly across the resulting interval, including the default, which
served as the starting configuration for ParamILS. The number of discrete values was chosen according to our intuition about the importance of each parameter. After this discretization, there
were 3.7  1018 possible parameter configurations. Exploiting the fact that nine of the parameters are
conditional (i.e., only relevant when other parameters take certain values) reduced this to 8.34  1017
configurations.
C PLEX The third target algorithm we used was the commercial optimization tool C PLEX 10.1.1, a
massively parameterized algorithm for solving mixed integer programming (MIP) problems. Out of
its 159 user-specifiable parameters, we identified 81 parameters that affect C PLEXs search trajectory. We were careful to omit all parameters that change the problem formulation (e.g., by changing
the numerical accuracy of a solution). Many C PLEX parameters deal with MIP strategy heuristics (such as variable and branching heuristics, probing, dive type and subalgorithms) and with
the amount and type of preprocessing to be performed. There are also nine parameters governing
how frequently a different type of cut should be used (those parameters have up to four allowable
magnitude values and the value choose automatically; note that this last value prevents the parameters from being ordinal). A considerable number of other parameters deal with simplex and
283

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

barrier optimization, and with various other algorithm components. For categorical parameters with
an automatic option, we considered all categorical values as well as the automatic one. In contrast, for continuous and integer parameters with an automatic option, we chose that option instead
of hypothesizing values that might work well. We also identified some numerical parameters that
primarily deal with numerical issues, and fixed those to their default values. For other numerical
parameters, we chose up to five possible values that seemed sensible, including the default. For the
many categorical parameters with an automatic option, we included the automatic option as a choice
for the parameter, but also included all the manual options. Finally, we ended up with 63 configurable parameters, leading to 1.78  1038 possible configurations. Exploiting the fact that seven of
the C PLEX parameters were only relevant conditional on other parameters taking certain values, we
reduced this to 1.38  1037 distinct configurations. As the starting configuration for our configuration
procedures, we used the default settings, which have been obtained by careful manual configuration
on a broad range of MIP instances.
5.2.2 B ENCHMARK I NSTANCES
We applied our target algorithms to three sets of benchmark instances: SAT-encoded quasi-group
completion problems, SAT-encoded graph-colouring problems based on small world graphs, and
MIP-encoded winner determination problems for combinatorial auctions. Each set consisted of
2000 instances, partitioned evenly into training and test sets.
QCP Our first benchmark set contained 23 000 instances of the quasi-group completion problem (QCP), which has been widely studied by AI researchers. We generated these QCP instances
around the solubility phase transition, using the parameters given by Gomes and Selman (1997).
Specifically, the order n was drawn uniformly from the interval [26, 43], and the number of holes
H (open entries in the Latin square) was drawn uniformly from [1.75, 2.3]  n1.55 . The resulting
QCP instances were converted into SAT CNF format. For use with the complete solver, S PEAR,
we sampled 2000 of these SAT instances uniformly at random. These had on average 1497 variables (standard deviation: 1094) and 13 331 clauses (standard deviation: 12 473), and 1182 of them
were satisfiable. For use with the incomplete solver, S APS, we randomly sampled 2000 instances
from the subset of satisfiable instances (determined using a complete algorithm); their number of
variables and clauses were very similar to those used with S PEAR.
SW-GCP Our second benchmark set contained 20 000 instances of the graph colouring problem
(GCP) based on the small world (SW) graphs of Gent et al. (1999). Of these, we sampled 2000
instances uniformly at random for use with S PEAR; these had on average 1813 variables (standard
deviation: 703) and 13 902 clauses (standard deviation: 5393), and 1109 of them were satisfiable.
For use with S APS, we randomly sampled 2000 satisfiable instances (again, determined using a
complete SAT algorithm), whose number of variables and clauses were very similar to those used
with S PEAR.
Regions100 For our third benchmark set we generated 2000 instances of the combinatorial auction
winner determination problem, encoded as mixed-integer linear programs (MILPs). We used the
regions generator from the Combinatorial Auction Test Suite (Leyton-Brown et al., 2000), with
the goods parameter set to 100 and the bids parameter set to 500. The resulting MILP instances
contained 501 variables and 193 inequalities on average, with a standard deviation of 1.7 variables
and 2.5 inequalities.
284

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

Scenario
S A P S -SWGCP
S P E A R -SWGCP
S A P S -QCP
S P E A R -QCP
C P L E X -R E G I O N S 100

Default
20.41
9.74
12.97
2.65
1.61

Test performance (penalized average runtime, in CPU seconds)
mean  stddev. for 10 runs
Run with best training performance
BasicILS
FocusedILS
BasicILS
FocusedILS
0.32  0.06 0.32  0.05
0.26
0.26
8.05  0.9
8.3  1.1
6.8
6.6
4.86  0.56 4.70  0.39
4.85
4.29
1.39  0.33
1.29  0.2
1.16
1.21
0.5  0.3
0.35  0.04
0.35
0.32

Fig.
2(a)
2(b)
2(c)
2(d)
2(e)

Table 3: Performance comparison of the default parameter configuration and the configurations found
with BasicILS and FocusedILS (both with Aggr Capping and bm = 2). For each configuration scenario, we list test performance (penalized average runtime over 1000 test instances, in CPU seconds) of the algorithm default, mean  stddev of test performance across
25 runs of BasicILS(100) & FocusedILS (run for five CPU hours each), and the test performance of the run of BasicILS and FocusedILS that was best in terms of training performance. Boldface indicates the better of BasicILS and FocusedILS. The algorithm configurations found in FocusedILSs run with the best training performance are listed in an online appendix at http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html. Column Fig. gives a reference to a scatter plot comparing the performance of those configurations
against the algorithm defaults.

5.3 Experimental Setup
We carried out all of our experiments on a cluster of 55 dual 3.2GHz Intel Xeon PCs with 2MB
cache and 2GB RAM, running OpenSuSE Linux 10.1. We measured runtimes as CPU time on
these reference machines. All our configuration procedures were implemented as Ruby scripts, and
we do not include the runtime of these scripts in the configuration time. In easy configuration
scenarios, where most algorithm runs finish in milliseconds, the overhead of our scripts can be
substantial. Indeed, the longest configuration run we observed took 24 hours to execute five hours
worth of target algorithm runtime. In contrast, for the harder C PLEX scenarios described in Section
7 we observed virtually no overhead.

6. Empirical Evaluation of BasicILS, FocusedILS and Adaptive Capping
In this section, we use our B R O A D scenarios to empirically study the performance of BasicILS(N )
and FocusedILS, as well as the effect of adaptive capping. We first demonstrate the large speedups
ParamILS achieved over the default parameters and then study the components responsible for this
success.
6.1 Empirical Comparison of Default and Optimized Parameter Configurations
In this section, for each of our five B R O A D configuration scenarios, we compare the performance of
the respective algorithms default parameter configuration against the final configurations found by
BasicILS(100) and FocusedILS. Table 3 and especially Figure 2 show that the configurators led to
very substantial speedups.
In Table 3, we report the final performance achieved by 25 independent runs of each configurator. For each independent configuration run, we used a different set of training instances and seeds
(constructed as described in Section 5.1.2). We note that there was often a rather large variance
in the performances found in different runs of the configurators, and that the configuration found
285

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

4

4

10

Runtime [s], autotuned

Runtime [s], autotuned

10

3

10

2

10

1

10

0

10

1

10

2

10

2

10

1

10

0

10

1

10

2

10
2

1

10

0

10

1

10

10

2

10

3

4

10

2

10

Runtime [s], default

10

(a) S A P S -SWGCP.
531s vs 0.15s; 499 vs no timeouts
4

1

10

0

10

1

2

10

1

10

0

10

1

10

2

10

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

(c) S A P S -QCP.
72s vs 0.17s; 149 vs 1 timeouts

4

10

4

10

4

3

10

2

10

1

10

0

10

1

10

2

10
2

3

10

10

Runtime [s], autotuned

Runtime [s], autotuned

3

10

(b) S P E A R -SWGCP.
33s vs 17s; 3 vs 2 timeouts

10

10

2

10

Runtime [s], default

4

10

Runtime [s], autotuned

3

10

3

10

2

10

1

10

0

10

1

10

2

10
2

10

1

10

0

10

1

10

2

10

10

Runtime [s], default

(d) S P E A R -QCP.
9.6s vs 0.85s; 1 vs 0 timeouts

3

4

10

2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(e) C P L E X -R E G I O N S 100.
1.61s vs 0.32s; no timeouts

Figure 2: Comparison of default vs automatically-determined parameter configurations for our five B R O A D
configuration scenarios. Each dot represents one test instance; timeouts (after one CPU hour) are
denoted by circles. The dashed line at five CPU seconds indicates the cutoff time of the target
algorithm used during the configuration process. The subfigure captions give mean runtimes for
the instances solved by both of the configurations (default vs optimized), as well as the number of
timeouts for each.

in the run with the best training performance also tended to yield better test performance than the
others. For that reason, we used that configuration as the result of algorithm configuration. (Note
that choosing the configuration found in the run with the best training set performance is a perfectly
legitimate procedure since it does not require knowledge of the test set. Of course, the improvements thus achieved come at the price of increased overall running time, but the independent runs
of the configurator can easily be performed in parallel.)
In Figure 2, we compare the performance of this automatically-found parameter configuration
against the default configuration, when runs are allowed to last up to an hour. The speedups are
more obvious in this figure than in Table 3, since the penalized average runtime in that table counts
runtimes larger than five seconds as fifty seconds (ten times the cutoff of five seconds), whereas the
data in the figure uses a much larger cutoff time. The larger speedups are most apparent for scenarios
S A P S -SWGCP, S A P S -QCP, and S P E A R -QCP: their corresponding speedup factors in mean runtime are
now 3540, 416 and 11, respectively (see Figure 2).
286

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

Algorithm 5: RandomSearch(N, 0 )
Outline of random search in parameter configuration space; inc denotes the incumbent parameter
configuration, betterN compares two configurations based on the first N instances from the training
set.
Input : Number of runs to use for evaluating parameter configurations, N ; initial configuration
0  .
Output : Best parameter configuration inc found.
1 inc  0 ;
2 while not TerminationCriterion() do
3
  random   ;
4
if betterN (, inc ) then
5
inc  ;
6

return inc

6.2 Empirical Comparison of BasicILS and Simple Baselines
In this section, we evaluate the effectiveness of BasicILS(N ) against two of its components:
 a simple random search, used in BasicILS for initialization (we dub it RandomSearch(N ) and
provide pseudocode for it in Algorithm 5); and
 a simple local search, the same type of iterative first improvement search used in BasicILS(N )
(we dub it SimpleLS(N )).
To evaluate one component at a time, in this section and in Section 6.3 we study our algorithms
without adaptive capping. We then investigate the effect of our adaptive capping methods in Section
6.4.
If there is sufficient structure in the search space, we expect BasicILS to outperform RandomSearch. If there are local minima, we expect BasicILS to perform better than simple local search.
Our experiments showed that BasicILS did indeed offer the best performance.
Here, we are solely interested in comparing how effectively the approaches search the space
of parameter configurations (and not how the found parameter configurations generalize to unseen
test instances). Thus, in order to reduce variance in our comparisons, we compare the configuration
methods in terms of their performance on the training set.
In Table 4, we compare BasicILS against RandomSearch for our B R O A D configuration scenarios.
On average, BasicILS always performed better, and in three of the five scenarios, the difference was
statistically significant as judged by a paired Max-Wilcoxon test (see Section 5.1.3). Table 4 also
lists the performance of the default parameter configuration for each of the scenarios. We note that
both BasicILS and RandomSearch consistently achieved substantial (and statistically significant)
improvements over these default configurations.
Next, we compared BasicILS against its second component, SimpleLS. This basic local search
is identical to BasicILS, but stops at the first local minimum encountered. We used it in order to
study whether local minima pose a problem for simple first improvement search. Table 5 shows that
in the three configuration scenarios where BasicILS had time to perform multiple ILS iterations,
its training set performance was statistically significantly better than that of SimpleLS. Thus, we
conclude that the search space contains structure that can be exploited with a local search algorithm
as well as local minima that can limit the performance of iterative improvement search.
287

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Scenario
S A P S -SWGCP
S P E A R -SWGCP
S A P S -QCP
S P E A R -QCP
C P L E X -R E G I O N S 100

Training performance (penalized average runtime, in CPU seconds)
Default RandomSearch(100)
BasicILS(100)
19.93
0.46  0.34
0.38  0.19
10.61
7.02  1.11
6.78  1.73
12.71
3.96  1.185
3.19  1.19
2.77
0.58  0.59
0.36  0.39
1.61
1.45  0.35
0.72  0.45

p-value
0.94
0.18
1.4  105
0.007
1.2  105

Table 4: Comparison of RandomSearch(100) and BasicILS(100), both without adaptive capping. The table
shows training performance (penalized average runtime over N = 100 training instances, in CPU
seconds). Note that both approaches yielded substantially better results than the default configuration, and that BasicILS performed statistically significantly better than RandomSearch in three
of the five B R O A D configuration scenarios as judged by a paired Max-Wilcoxon test (see Section
5.1.3).
Scenario
S A P S -SWGCP
S A P S -QCP
S P E A R -QCP

SimpleLS(100)
Performance
0.5  0.39
3.60  1.39
0.4  0.39

BasicILS(100)
Performance
Avg. # ILS iterations
0.38  0.19
2.6
3.19  1.19
5.6
0.36  0.39
1.64

p-value
9.8  104
4.4  104
0.008

Table 5: Comparison of SimpleLS(100) and BasicILS(100), both without adaptive capping. The table shows
training performance (penalized average runtime over N = 100 training instances, in CPU seconds). In configuration scenarios S P E A R -SWGCP and C P L E X -R E G I O N S 100, BasicILS did not complete its first ILS iteration in any of the 25 runs; the two approaches were thus identical and are not
listed here. In all other configuration scenarios, BasicILS found significantly better configurations
than SimpleLS.

6.3 Empirical Comparison of FocusedILS and BasicILS
In this section we investigate FocusedILSs performance experimentally. In contrast to our previous comparison of RandomSearch, SimpleLS, and BasicILS using training performance, we now
compare FocusedILS against BasicILS using test performance. This is becausein contrast to BasicILS and SimpleLSFocusedILS grows the number of target algorithm runs used to evaluate a
parameter configuration over time. Even different runs of FocusedILS (using different training sets
and random seeds) do not use the same number of target algorithm runs to evaluate parameter configurations. However, they all eventually aim to optimize the same cost statistic, c, and therefore
test set performance (an unbiased estimator of c) provides a fairer basis for comparison than training performance. We only compare FocusedILS to BasicILS, since BasicILS already outperformed
RandomSearch and SimpleLS in Section 6.2.
Figure 3 compares the test performance of FocusedILS and BasicILS(N ) with N = 1, 10 and
100. Using a single target algorithm run to evaluate each parameter configuration, BasicILS(1)
was fast, but did not generalize well to the test set at all. For example, in configuration scenario
S A P S -SWGCP, BasicILS(1) selected a parameter configuration whose test performance turned out to
be even worse than the default. On the other hand, using a large number of target algorithm runs for
each evaluation resulted in a very slow search, but eventually led to parameter configurations with
good test performance. FocusedILS aims to achieve a fast search and good generalization to the test
set. For the configuration scenarios in Figure 3, FocusedILS started quickly and also led to the best
final performance.
288

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

2

2

Mean runtime [s], test

Mean runtime [s], test

10

1

10

0

10

1

10

BasicILS(1)
BasicILS(10)
BasicILS(100)
FocusedILS
0

10

2

1

0.5

0

4

10

1.5

10

BasicILS(1)
BasicILS(10)
BasicILS(100)
FocusedILS
0

10

CPU time used for tuner [s]

2

4

10

10

CPU time used for tuner [s]

(a) S A P S -SWGCP

(b) C P L E X -R E G I O N S 100

Figure 3: Comparison of BasicILS(N ) with N = 1, 10, and 100 vs FocusedILS, both without adaptive
capping. We show the median of test performance (penalized average runtime across 1 000 test
instances) across 25 runs of the configurators for two scenarios. Performance in the other three
B R O A D scenarios was qualitatively similar: BasicILS(1) was the fastest to move away from the
starting parameter configuration, but its performance was not robust at all; BasicILS(10) was a
rather good compromise between speed and generalization performance, but given enough time
was outperformed by BasicILS(100). FocusedILS started finding good configurations quickly
(except for scenario S P E A R -QCP, where it took even longer than BasicILS(100) to improve over
the default) and always was amongst the best approaches at the end of the configuration process.

Scenario
S A P S -SWGCP
S P E A R -SWGCP
S A P S -QCP
S P E A R -QCP
C P L E X -R E G I O N S 100

Test performance (penalized average runtime, in CPU seconds)
Default BasicILS(100)
FocusedILS
20.41
0.59  0.28
0.32  0.08
9.74
8.13  0.95
8.40  0.92
12.97
4.87  0.34
4.69  0.40
2.65
1.32  0.34
1.35  0.20
1.61
0.72  0.45
0.33  0.03

p-value
1.4  104
(0.21)
0.042
(0.66)
1.2  105

Table 6: Comparison of BasicILS(100) and FocusedILS, both without adaptive capping. The table shows
test performance (penalized average runtime over 1 000 test instances, in CPU seconds). For each
configuration scenario, we report test performance of the default parameter configuration, mean 
stddev of the test performance reached by 25 runs of BasicILS(100) and FocusedILS, and the pvalue for a paired Max-Wilcoxon test (see Section 5.1.3) for the difference of the two configurators
performance.

We compare the performance of FocusedILS and BasicILS(100) for all configuration scenarios in Table 6. For three S APS and C PLEX scenarios, FocusedILS performed statistically significantly better than BasicILS(100). These results are consistent with our past work in which FocusedILS achieved statistically significantly better performance than BasicILS(100) (Hutter et al.,
2007). However, we found that in both configuration scenarios involving the S PEAR algorithm,
BasicILS(100) actually performed better on average than FocusedILS, albeit not statistically significantly. We attribute this to the fact that for a complete, industrial solver such as S PEAR, the two
benchmark distributions QCP and SWGCP are quite heterogeneous. We expect FocusedILS to have
problems in dealing with highly heterogeneous distributions, due to the fact that it frequently tries
to extrapolate performance based on a few runs per parameter configuration.
289

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

2

2.5
BasicILS, no capping
BasicILS, TP capping

Mean runtime [s], train

Mean runtime [s], train

10

1

10

0

10

1

10

2

3

10

10

BasicILS, no capping
BasicILS, TP capping
2

1.5

1

0.5

0 2
10

4

10

CPU time used for tuner [s]

3

10

4

10

CPU time used for tuner [s]

(a) S A P S -SWGCP, significant.

(b) C P L E X -R E G I O N S 100, significant.

Figure 4: Speedup of BasicILS by adaptive capping for two configuration scenarios. We performed 25
runs of BasicILS(100) without adaptive capping and with TP capping. For each time step, we
computed training performance of each run of the configurator (penalized average runtime over
N = 100 training instances) and plot the median over the 25 runs.
Scenario
S A P S -SWGCP
S P E A R -SWGCP
S A P S -QCP
S P E A R -QCP
C P L E X -R E G I O N S 100

Training performance (penalized average runtime)
No capping
TP capping
p-value
0.38  0.19
0.24  0.05
6.1  105
6.78  1.73
6.65  1.48
0.01
3.19  1.19
2.96  1.13
9.8  104
0.361  0.39 0.356  0.44
0.66
0.67  0.35
0.47  0.26
7.3  104

Avg. # ILS iterations
No capping TP capping
3
12
1
1
6
10
2
3
1
1

Table 7: Effect of adaptive capping for BasicILS(100). We show training performance (penalized average runtime on N = 100 training instances, in CPU seconds). For each configuration scenario, we
report mean  stddev of the final training performance reached by 25 runs of the configurator without capping and with TP capping, the p-value for a paired Max-Wilcoxon test for their difference
(see Section 5.1.3), as well as the average number of ILS iterations performed by the respective
configurator.

6.4 Empirical Evaluation of Adaptive Capping in BasicILS and FocusedILS
We now present experimental evidence that the use of adaptive capping has a strong impact on the
performance of BasicILS and FocusedILS.
Figure 4 illustrates the extent to which TP capping sped up BasicILS for two configuration scenarios. In both cases, capping helped to improve training performance substantially; for S A P S -SWGCP,
BasicILS found the same solutions up to about an order of magnitude faster than without capping.
Table 7 quantifies the speedups for all five B R O A D configuration scenarios. TP capping enabled
up to four times as many ILS iterations (in S A P S -SWGCP) and improved average performance in all
scenarios. The improvement was statistically significant in all scenarios, except S P E A R -QCP.
Aggressive capping further improved BasicILS performance for one scenario. For scenario
S A P S -SWGCP, it increased the number of ILS iterations completed within the configuration time
from 12 to 219, leading to a significant improvement in performance. In the first ILS iteration of
BasicILS, both capping techniques are identical (the best configuration in that iteration is always
the incumbent). Thus, we did not observe a difference on configuration scenarios S P E A R -SWGCP and
C P L E X -R E G I O N S 100, for which none of the 25 runs of the configurator finished its first ILS iteration.
For the remaining two configuration scenarios, the differences were insignificant.
290

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

Scenario
S A P S -SWGCP
S P E A R -SWGCP
S A P S -QCP
S P E A R -QCP
C P L E X -R E G I O N S 100

Number of ILS iterations performed
No capping
TP capping
p-value
121  12
166  15
1.2  105
37  12
43  15
0.0026
142  18
143  22
0.54
153  49
165  41
0.03
36  13
40  16
0.26

Aggr capping
244  19
47  18
156  28
213  62
54  15

p-value
1.2  105
9  105
0.016
1.2  105
1.8  105

Number of runs performed for the incumbent parameter configuration
Scenario
No capping
TP capping
p-value
Aggr capping
p-value
S A P S -SWGCP
993  211
1258  262 4.7  104 1818  243 1.2  105
503  265
476  238
(0.58)
642  288
0.009
S P E A R -SWGCP
S A P S -QCP
1575  385 1701  318
0.065
1732  340
0.084
S P E A R -QCP
836  509
1130  557
0.02
1215  501
0.003
761  215
795  184
0.40
866  232
0.07
C P L E X -R E G I O N S 100

Table 8: Effect of adaptive capping on search progress in FocusedILS, as measured by the number of ILS
iterations performed and the number of runs performed for the incumbent parameter configuration.
For each configuration scenario, we report mean  stddev of both of these measures across 25
runs of the configurator without capping, with TP capping, and with Aggr capping, as well as the
p-values for paired Max-Wilcoxon tests (see Section 5.1.3) for the differences between no capping
and TP capping; and between no capping and Aggr capping.

We now evaluate the usefulness of capping for FocusedILS. Training performance is not a useful
quantity in the context of comparing different versions of FocusedILS, since the number of target
algorithm runs this measure is based on varies widely between runs of the configurator. Instead, we
used two other measures to quantify search progress: the number of ILS iterations performed and
the number of target algorithm runs performed for the incumbent parameter configuration. Table 8
shows these two measures for our five B R O A D configuration scenarios and the three capping schemes
(none, TP, Aggr). FocusedILS with TP capping achieved higher values than without capping for all
scenarios and both measures (although only some of the differences were statistically significant).
Aggressive capping increased both measures further for all scenarios, and most of the differences
between no capping and aggressive capping were statistically significant. Figure 5 demonstrates that
for two configuration scenarios FocusedILS with capping reached the same solution qualities more
quickly than without capping. However, after finding the respective configurations, FocusedILS
showed no further significant improvement.
Recall that the experiments in Section 6.2 and 6.3 compared our various configurators without adaptive capping. One might wonder how these comparisons change in the presence of adaptive
capping. Indeed, adaptive capping also worked out of the box for RandomSearch and enabled it to
evaluate between 3.4 and 33 times as many configurations than without capping. This improvement
significantly improved the simple algorithm RandomSearch to the point where its average performance came within 1% of the one of BasicILS for two domains (S A P S -SWGCP and S P E A R -SWGCP;
compare the much larger differences without capping reported in Table 4). For S P E A R -QCP, there
was still a 25% difference in average performance, but this result was not significant. Finally, for
S A P S -QCP and C P L E X -R E G I O N S 100 the difference was still substantial and significant (22% and 55%
difference in average performance, with p-values 5.2  105 and 0.0013, respectively).
Adaptive capping also reduced the gap between BasicILS and FocusedILS. In particular, for
S A P S -SWGCP, where, even without adaptive capping, FocusedILS achieved the best performance we
have encountered for this scenario, BasicILS caught up when using adaptive capping. Similarly,
291

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

2

2
FocusedILS, no capping
FocusedILS, TP capping
FocusedILS, Aggr capping

Mean runtime [s], test

Mean runtime [s], test

10

1

10

0

10

1

10

2

10

3

10

4

10

FocusedILS, no capping
FocusedILS, TPcapping
FocusedILS, Aggr capping
1.5

1

0.5

0 1
10

5

10

CPU time used for tuner [s]

2

10

3

10

4

10

5

10

CPU time used for tuner [s]

(a) S A P S -SWGCP

(b) C P L E X -R E G I O N S 100

Figure 5: Speedup of FocusedILS by adaptive capping for two configuration scenarios. We performed 25
runs of FocusedILS without adaptive capping, with TP capping and with Aggr capping. For each
time step, we computed the test performance of each run of the configurator (penalized average
runtime over 1000 test instances) and plot the median over the 25 runs. The differences at the
end of the trajectory were not statistically significant. However, with capping the time required to
achieve that quality was lower in these two configuration scenarios. In the other three scenarios,
the gains due to capping were smaller.

for C P L E X -R E G I O N S 100, FocusedILS already performed very well without adaptive capping while
BasicILS did not. Here, BasicILS improved based on adaptive capping, but still could not rival
FocusedILS. For the other scenarios, adaptive capping did not affect the relative performance much;
compare Tables 6 (without capping) and 3 (with capping) for details.

7. Case Study: Configuring C PLEX for Real-World Benchmarks
In this section, we demonstrate that ParamILS can improve the performance of the commercial optimization tool C PLEX for a variety of interesting benchmark distributions. To our best knowledge,
this is the first published study on automatically configuring C PLEX.
We use five C PLEX configuration scenarios. For these, we collected a wide range of MIP benchmarks from public benchmark libraries and other researchers, and split each of them 50:50 into
disjoint training and test sets; we detail them in the following.
 Regions200 This set is almost identical to the Regions100 set (described in Section 5.2.2
and used throughout the paper), but its instances are much larger. We generated 2 000 MILP
instances with the generator provided with the Combinatorial Auction Test Suite (LeytonBrown et al., 2000), based on the regions option with the goods parameter set to 200 and
the bids parameter set to 1 000. These instances contain an average of 1 002 variables and 385
inequalities, with respective standard deviations of 1.7 and 3.4.
 MJA This set comprises 343 machine-job assignment instances encoded as mixed integer
quadratically constrained programs (MIQCP). It was obtained from the Berkeley Computational Optimization Lab5 and was introduced by Akturk, Atamturk and S. Gurel (2007).
These instances contain an average of 2 769 variables and 2 255 constraints, with respective
standard deviations of 2 133 and 1 592.
5. http://www.ieor.berkeley.edu/atamturk/bcol/, where this set is called conic.sch

292

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

 CLS This set comprises 100 capacitated lot-sizing instances encoded as mixed integer linear
programs (MILP). It was also obtained from the Berkeley Computational Optimization Lab
and was introduced by Atamturk and Munoz (2004). All 100 instances contain 181 variables
and 180 constraints.
 MIK This set of 120 MILP-encoded mixed-integer knapsack instances was also obtained
from the Berkeley Computational Optimization Lab and was originally introduced by Atamturk
(2003). These instances contain an average of 384 variables and 151 constraints, with respective standard deviations of 309 and 127.
 QP This set of quadratic programs originated from RNA energy parameter optimization (Andronescu, Condon, Hoos, Mathews & Murphy, 2007). Mirela Andronescu generated 2 000 instances for our experiments. These instances contain 9 3667 165 variables and 9 1917 186
constraints. Since the instances are polynomial-time solvable quadratic programs, we set a
large number of inconsequential C PLEX parameters concerning the branch and cut mechanism to their default values, ending up with 27 categorical, 2 integer and 2 continuous parameters to be configured, for a discretized parameter configuration space of size 3.27  1017 .
To study ParamILSs behavior for these harder problems, we set significantly longer cutoff times
for these C PLEX scenarios than for the B R O A D scenarios from the previous section. Specifically, we
used a cutoff time of 300 CPU seconds for each run of the target algorithm during training, and
allotted two CPU days for every run of each of the configurators. As always, our configuration
objective was to minimize penalized average runtime with a penalization constant of 10.
In Table 9, we compare the performance of C PLEXs default parameter configuration with the
final parameter configurations found by BasicILS(100) and FocusedILS (both with aggressive capping and bm = 2). Note that, similar to the situation described in Section 6.1, in some configuration
scenarios (e.g., C P L E X -CLS, C P L E X -MIK) there was substantial variance between the different runs of
the configurators, and the run with the best training performance yielded a parameter configuration
that was also very good on the test set. While BasicILS outperformed FocusedILS in 3 of these 5
scenarios in terms of mean test performance across the ten runs, FocusedILS achieved the better test
performance for the run with the best training performance for all but one scenario (in which it performed almost as well). For scenarios C P L E X -R E G I O N S 200 and C P L E X -CLS, FocusedILS performed
substantially better than BasicILS.
Note that in all C PLEX configuration scenarios we considered, both BasicILS and FocusedILS
found parameter configurations that were better than the algorithm defaults, sometimes by over
an order of magnitude. This is particularly noteworthy since ILOG expended substantial effort to
determine strong default C PLEX parameters. In Figure 6, we provide scatter plots for all five scenarios. For C P L E X -R E G I O N S 200, C P L E X - C O N I C . S C H , C P L E X -CLS, and C P L E X -MIK, speedups were quite
consistent across instances (with average speedup factors reaching from 2 for C P L E X - C O N I C . S C H
to 23 for C P L E X -MIK). Finally, for C P L E X -QP we see an interesting failure mode of ParamILS. The
optimized parameter configuration achieved good performance with the cutoff time used for the
6. For configuration scenario C P L E X -MIK, nine out of ten runs of FocusedILS yielded parameter configurations with
average runtimes smaller than two seconds. One run, however, demonstrated an interesting failure mode of FocusedILS with aggressive capping. Capping too aggressively caused every C PLEX run to be unsuccessful, such that
FocusedILS selected a configuration which did not manage to solve a single instance in the test set. Counting unsuccessful runs as ten times the cutoff time, this resulted in an average runtime of 10  300 = 3000 seconds for this run.
(For full details, see Section 8.1 of Hutter, 2009).

293

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Scenario
C P L E X -R E G I O N S 200
CP L E X-C O N I C.S C H
C P L E X -CLS
C P L E X -MIK
C P L E X -QP

Test performance (penalized average runtime, in CPU seconds)
mean  stddev. for 10 runs
Run with best training performance
Default
BasicILS
FocusedILS BasicILS
FocusedILS
72
45  24
11.4  0.9
15
10.5
5.37
2.27  0.11
2.4  0.29
2.14
2.35
712
443  294
327  860
80
23.4
64.8
20  27
301  948 6
1.72
1.19
969
755  214
827  306
528
525

Fig.
6(a)
6(b)
6(c)
6(d)
6(e)

Table 9: Experimental results for our C PLEX configuration scenarios.

For each configuration scenario, we list test performance (penalized average runtime over test instances) of the algorithm default, mean  stddev of test performance across ten runs of BasicILS(100)
& FocusedILS (run for two CPU days each), and the test performance of the run of
BasicILS and FocusedILS that is best in terms of training performance. Boldface indicates the better of BasicILS and FocusedILS. The algorithm configurations found in
FocusedILSs run with the best training performance are listed in an online appendix
at http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/results.html.
Column
Fig. gives a reference to a scatter plot comparing the performance of those configurations against
the algorithm defaults.

configuration process (300 CPU seconds, see Figure 6(f)), but this performance did not carry over
to the higher cutoff time we used in our tests (3600 CPU seconds, see Figure 6(e)). Thus, the parameter configuration found by FocusedILS did generalize well to previously unseen test data, but
not to larger cutoff times.

8. Review of Other ParamILS Applications
In this section, we review a number of other applications of ParamILSsome of them dating back
to earlier stages of its development, others very recentthat demonstrate its utility and versatility.
8.1 Configuration of SAPS, GLS+ and SAT4J
Hutter et al. (2007), in the first publication on ParamILS, reported experiments on three target algorithms to demonstrate the effectiveness of the approach: the SAT algorithm SAPS (which has
4 numerical parameters), the local search algorithm GLS+ for solving the most probable explanation (MPE) problem in Bayesian networks (which has 5 numerical parameters; Hutter, Hoos &
Stutzle, 2005), and the tree search SAT solver SAT4J (which has 4 categorical and 7 numerical
parameters; http://www.sat4j.org). They compared the respective algorithms default performance,
the performance of the CALIBRA system (Adenso-Diaz & Laguna, 2006), and the performance
of BasicILS and FocusedILS. Out of the four configuration scenarios studied, FocusedILS significantly outperformed CALIBRA on two and performed better on average on the third. For the fourth
one (configuring SAT4J), CALIBRA was not applicable due to the categorical parameters, while
FocusedILS significantly outperformed BasicILS.
Overall, automated parameter optimization using ParamILS achieved substantial improvements
over the previous default settings: GLS+ was sped up by a factor > 360 (tuned parameters found
solutions of better quality in 10 seconds than the default found in one hour), SAPS by factors of 8
and 130 on SAPS-QWH and SAPS-SW, respectively, and SAT4J by a factor of 11.
294

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

4

4

2

10

1

10

0

10

1

10

2

10

Runtime [s], autotuned

3

10

10

3

10

2

10

1

10

0

10

1

10

2

10
2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

1

10

0

10

1

10

2

10

3

4

10

Runtime [s], default

1

10

0

10

1

10

2

10

10

1

10

0

10

1

10

2

10

10

Runtime [s], default

(d) C P L E X -MIK.
28s vs 1.2s; no timeouts

3

4

10

2
1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(c) C P L E X -CLS.
309s vs 21.5s; no timeouts
4

10

3

10

2

10

1

10

0

10

1

10

2

3

10

2

10

1

10

0

10

1

10

2

10
2

1

10

2

Runtime [s], autotuned

Runtime [s], autotuned

2

10

0

10

10

4

3

1

10

10

10

10

2

10

4

(b) C P L E X - C O N I C . S C H .
5.37s vs 2.39.5s; no timeouts

10

3

10

10
2

10

(a) C P L E X -R E G I O N S 200.
72s vs 10.5s; no timeouts

Runtime [s], autotuned

4

10

Runtime [s], autotuned

Runtime [s], autotuned

10

10
2

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

(e) C P L E X -QP.
296s vs 234s; 0 vs 21 timeouts

4

2

10

10

1

10

0

10

1

10

2

10

3

10

Runtime [s], default

4

10

(f) C P L E X -QP, with test cutoff of 300
seconds.
81s vs 44s; 305 vs 150 timeouts

Figure 6: Comparison of default vs automatically-determined parameter configuration for our five C PLEX
configuration scenarios. Each dot represents one test instance; time-outs (after one CPU hour)
are denoted by red circles. The blue dashed line at 300 CPU seconds indicates the cutoff time
of the target algorithm used during the configuration process. The subfigure captions give mean
runtimes for the instances solved by both of the configurations (default vs optimized), as well as
the number of timeouts for each.

8.2 Configuration of Spear for Industrial Verification Problems
Hutter et al. (2007) applied ParamILS to a specific real-world application domain: configuring
the 26 parameters of the tree-search DPLL solver S PEAR to minimize its mean runtime on a set
of practical verification instances. In particular, they considered two sets of industrial problem
instances, bounded model-checking (BMC) instances from Zarpas (2005) and software verification
(SWV) instances generated by the C ALYSTO static checker (Babic & Hu, 2007).
The instances from both problem distributions exhibited a large spread in hardness for S PEAR.
For the SWV instances, the default configuration solved many instances in milliseconds but failed
to solve others in days. This was despite the fact that S PEAR was specifically developed for this type
of instances, that its developer had generated the problem instances himself (and thus had intimate
domain knowledge), and that a week of manual performance tuning had been expended in order to
optimize the solvers performance.
S PEAR was first configured for good general performance on industrial SAT instances from
previous SAT competitions. This already led to substantial improvements over the default perfor295

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

mance in the 2007 SAT competition.7 While the S PEAR default solved 82 instances and ranked 17th
in the first round of the competition, an automatically configured version solved 93 instances and
ranked 8th, and a further optimized version solved 99 instances, ranking 5th (above MiniSAT). The
speedup factors due to this general optimization were 20 and 1.3 on the SWV and BMC datasets,
respectively.
Optimizing on the specific instance sets yielded further, and much larger improvements (a factor
of over 500 for SWV and 4.5 for BMC). Most encouragingly, the best parameter configuration found
for the software verification instances did not take longer than 20 seconds to solve any of the SWV
problem instances (compared to multiple timeouts after a CPU day for the original default values).
Key to good performance in that application was to perform multiple independent runs of FocusedILS, and to select the found configuration with best training performance (as also done in
Sections 6.1 and 7 of this article).
8.3 Configuration of SATenstein
KhudaBukhsh, Xu, Hoos and Leyton-Brown (2009 ) used ParamILS to perform automatic algorithm
design in the context of stochastic local search algorithms for SAT. Specifically, they introduced a
new framework for local search SAT solvers, called SATenstein, and used ParamILS to choose
good instantiations of the framework for given instance distributions. SATenstein spans three broad
categories of SLS-based SAT solvers: WalkSAT-based algorithms, dynamic local search algorithms
and G2 WSAT variants. All of these are combined in a highly parameterized framework solver with
a total of 41 parameters and 4.82  1012 unique instantiations.
FocusedILS was used to configure SATenstein on six different problem distributions, and the
resulting solvers were compared to eleven state-of-the-art SLS-based SAT solvers. The results
showed that the automatically configured versions of SATenstein outperformed all of the eleven
state-of-the-art solvers in all six categories, sometimes by a large margin.
The SAT ENSTEIN work clearly demonstrated that automated algorithm configuration methods
can be used to construct new algorithms by combining a wide range of components from existing algorithms in novel ways, and thereby go beyond simple parameter tuning. Due to the low
level of manual work required by this approach, we believe this automated design of algorithms
from components will become a mainstream technique in the development of algorithms for hard
combinatorial problems.
Key to the successful application of FocusedILS for configuring SAT ENSTEIN was the careful
selection of homogeneous instance distributions, most instances of which could be solved within a
comparably low cutoff time of 10 seconds per run. Again, the configuration with the best training
quality was selected from ten parallel independent runs of FocusedILS per scenario.
8.4 Self-Configuration of ParamILS
As a heuristic optimization procedure, ParamILS is itself controlled by a number of parameters:
the number of random configurations, r, to be sampled at the beginning of search; the perturbation
strength, s; and the probability of random restarts, prestart . Furthermore, our aggressive capping
mechanism makes use of an additional parameter: the bound multiplier, bm. Throughout this article,
we have used the manually-determined default values hr, s, prestart , bmi = h10, 3, 0.01, 2i.
7. See http://www.cril.univ-artois.fr/SAT07. S PEAR was not allowed to participate in the second round
of this competition since its source code is not publicly available.

296

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

In recent work (see Section 8.2 of Hutter, 2009), we evaluated whether FocusedILSs performance could be improved by using ParamILS to automatically find a better parameter configuration.
In this self-configuration task, configuration scenarios play the role of instances, and the configurator to be optimized plays the role of the target algorithm. To avoid confusion, we refer to this
configurator as the target configurator. Here, we set fairly short configuration times of one CPU
hour for the target configurator. However, this was still significantly longer than the cutoff times
we used in any of our other experiments, such that parallelization turned out to be crucial to finish the experiment in a reasonable amount of time. Because BasicILS is easier to parallelize than
FocusedILS, we chose BasicILS(100) as the meta-configurator.
Although the notion of having an algorithm configurator configure itself was intriguing, in this
case, it turned out to only yield small improvements. Average performance improved for four out
of the five scenarios and degraded for the remaining one. However, none of the differences was
statistically significant.
8.5 Other Applications of ParamILS
Thachuk, Shmygelska and Hoos (2007 ) used BasicILS in order to determine performance-optimizing
parameter settings of a new replica-exchange Monte Carlo algorithm for protein folding in the 2DHP and 3D-HP models.8 Even though their algorithm has only four parameters (two categorical and
two continuous), BasicILS achieved substantial performance improvements. While the manuallyselected configurations were biased in favour of either short or long protein sequences, BasicILS
found a configuration which consistently yielded good mean runtimes for all types of sequences.
On average, the speedup factor achieved was approximately 1.5, and for certain classes of protein
sequences up to 3. While all manually-selected configurations performed worse than the previous
state-of-the-art algorithm for this problem on some instances, the robust parameter configurations
selected by BasicILS yielded uniformly better performance.
In very recent work, Fawcett, Hoos and Chiarandini (2009) used several variants of ParamILS
(including a version that has been slightly extended beyond the ones presented here) to design
a modular stochastic local search algorithm for the post-enrollment course timetabling problem.
They followed a design approach that used automated algorithm configuration in order to explore
a large design space of modular and highly parameterised stochastic local search algorithms. This
quickly led to a solver that placed third in Track 2 of the 2nd International Timetabling Competition
(ITC2007) and subsequently produced an improved solver that is shown to achieve consistently
better performance than the top-ranked solver from the competition.

9. Related Work
Many researchers before us have been dissatisfied with manual algorithm configuration, and various
fields have developed their own approaches for automatic parameter tuning. We start this section
with the most closely-related workapproaches that employ direct search to find good parameter
configurationsand then describe other methods. Finally, we discuss work on related problems,
such as finding the best parameter configuration or algorithm on a per-instance basis, and approaches
that adapt their parameters during an algorithms execution (see also Hoos, 2008, for further related
work on automated algorithm design).
8. BasicILS was used, because FocusedILS had not yet been developed when that study was conducted.

297

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

9.1 Direct Search Methods for Algorithm Configuration
Approaches for automated algorithm configuration go back to the early 1990s, when a number of
systems were developed for adaptive problem solving. One of these systems is Composer (Gratch
& Dejong, 1992), which performs a hill-climbing search in configuration space, taking moves if
enough evidence has been gathered to render a neighbouring configuration statistically significantly
better than the current configuration. Composer was successfully applied to improving the five
parameters of an algorithm for scheduling communication between a collection of ground-based
antennas and spacecrafts (Gratch & Chien, 1996).
Around the same time, the MULTI-TAC system was introduced by Minton (1993, 1996). MULTITAC takes as input generic heuristics, a specific problem domain, and a distribution over problem instances. It adapts the generic heuristics to the problem domain and automatically generates
domain-specific LISP programs implementing them. A beam search is then used to choose the best
LISP program where each program is evaluated by running it on a fixed set of problem instances
sampled from the given distribution.
Another search-based approach that uses a fixed training set was introduced by Coy et al. (2001).
Their approach works in two stages. First, it finds a good parameter configuration i for each instance Ii in the training set by a combination of experimental design (full factorial or fractional
factorial) and gradient descent. Next, it combines the parameter configurations 1 , . . . , N thus determined by setting each parameter to the average of the values taken in all of them. Note that this
averaging step restricts the applicability of the method to algorithms with only numerical parameters.
A similar approach, also based on a combination of experimental design and gradient descent,
using a fixed training set for evaluation, is implemented in the CALIBRA system of Adenso-Diaz
and Laguna (2006). CALIBRA starts by evaluating each parameter configuration in a full factorial
design with two values per parameter. It then iteratively homes in to good regions of parameter
configuration space by employing fractional experimental designs that evaluate nine configurations
around the best performing configuration found so far. The grid for the experimental design is
refined in each iteration. Once a local optimum is found, the search is restarted (with a coarser
grid). Experiments showed CALIBRAs ability to find parameter settings for six target algorithms
that matched or outperformed the respective originally-proposed parameter configurations. Its main
drawback is the limitation to tuning numerical and ordinal parameters, and to a maximum of five
parameters. When we first introduced ParamILS, we performed experiments comparing its performance against CALIBRA (Hutter et al., 2007). These experiments are reviewed in Section 8.1.
Terashima-Marn et al. (1999) introduced a genetic algorithm for configuring a constraint satisfaction algorithm for large-scale university exam scheduling. They constructed and configured an
algorithm that works in two stages and has seven configurable categorical parameters. They optimized these choices with a genetic algorithm for each of 12 problem instances, and for each of them
found a configuration that improved performance over a modified Brelaz algorithm. However, note
that they performed this optimization separately for each instance. Their paper did not quantify how
long these optimizations took, but stated that Issues about the time for delivering solutions with
this method are still a matter of research.
Work on automated parameter tuning can also be found in the numerical optimization literature. In particular, Audet and Orban (2006) proposed the mesh adaptive direct search algorithm.
Designed for purely continuous parameter configuration spaces, this algorithm is guaranteed to converge to a local optimum of the cost function. Parameter configurations were evaluated on a fixed
298

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

set of large unconstrained regular problems from the CUTEr collection, using as optimization objectives runtime and number of function evaluations required for solving a given problem instance.
Performance improvements of around 25% over the classical configuration of four continuous parameters of interior point methods were reported.
Algorithm configuration is a stochastic optimization problem, and there exists a large body of
algorithms designed for such problems (see, e.g., Spall, 2003). However, many of the algorithms in
the stochastic optimization literature require explicit gradient information and are thus inapplicable
to algorithm configuration. Some algorithms approximate the gradient from function evaluations
only (e.g., by finite differences), and provably converge to a local minimum of the cost function
under mild conditions, such as continuity. Still, these methods are primarily designed to deal with
numerical parameters and only find local minima. We are not aware of any applications of general
purpose algorithms for stochastic optimization to algorithm configuration.
9.2 Other Methods for Algorithm Configuration
Sequential parameter optimization (SPO) (Bartz-Beielstein, 2006) is a model-based parameter optimization approach based on the Design and Analysis of Computer Experiments (DACE; see, e.g.,
Santner, Williams & Notz, 2003), a prominent approach in statistics for blackbox function optimization. SPO starts by running the target algorithm with parameter configurations from a Latin
hypercube design on a number of training instances. It then builds a response surface model based
on Gaussian process regression and uses the models predictions and predictive uncertainties to determine the next parameter configuration to evaluate. The metric underlying the choice of promising
parameter configurations is the expected improvement criterion used by Jones, Schonlau and Welch
(1998). After each algorithm run, the response surface is refitted, and a new parameter configuration is determined based on the updated model. In contrast to the previously-mentioned methods,
SPO does not use a fixed training set. Instead, it starts with a small training set and doubles its size
whenever a parameter configuration is determined as incumbent that has already been incumbent in
a previous iteration. A recent improved mechanism resulted in a more robust version, SPO+ (Hutter, Hoos, Leyton-Brown & Murphy, 2009). The main drawbacks of SPO and its variants, and in
fact of the entire DACE approach, are its limitation to continuous parameters and to optimizing
performance for single problem instances, as well as its cubic runtime scaling in the number of data
points.
Another approach is based on adaptations of racing algorithms in machine learning (Maron &
Moore, 1994) to the algorithm configuration problem. Birattari et al. (2002; 2004) developed a procedure dubbed F-Race and used it to configure various stochastic local search algorithms. F-Race
takes as input an algorithm A, a finite set of algorithm configurations , and an instance distribution D. It iteratively runs the target algorithm with all surviving parameter configurations on a
number of instances sampled from D (in the simplest case, each iteration runs all surviving configurations on one instance). A configuration is eliminated from the race as soon as enough statistical
evidence is gathered against it. After each iteration, a non-parametric Friedman test is used to check
whether there are significant differences among the configurations. If this is the case, the inferior
configurations are eliminated using a series of pairwise tests. This process is iterated until only
one configuration survives or a given cutoff time is reached. Various applications of F-Race have
demonstrated very good performance (for an overview, see Birattari, 2004). However, since at the
start of the procedure all candidate configurations are evaluated, this approach is limited to situations
in which the space of candidate configurations can practically be enumerated. In fact, published ex299

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

periments with F-Race have been limited to applications with only around 1200 configurations. A
recent extension presented by Balaprakash et al. (2007) iteratively performs F-Race on subsets of
parameter configurations. This approach scales better to large configuration spaces, but the version
described by Balaprakash et al. (2007) handles only algorithms with numerical parameters.
9.3 Related Algorithm Configuration Problems
Up to this point, we have focused on the problem of finding the best algorithm configuration for
an entire set (or distribution) of problem instances. Related approaches attempt to find the best
configuration or algorithm on a per-instance basis, or to adapt algorithm parameters during the
execution of an algorithm. Approaches for setting parameters on a per-instance basis have been
described by Patterson and Kautz (2001), Cavazos and OBoyle (2006), and Hutter et al. (2006).
Furthermore, approaches that attempt to select the best algorithm on a per-instance basis have been
studied by Leyton-Brown, Nudelman and Shoham (2002), Carchrae and Beck (2005), Gebruers,
Hnich, Bridge and Freuder (2005), Gagliolo and Schmidhuber (2006), and Xu, Hutter, Hoos and
Leyton-Brown (2008). In other related work, decisions about when to restart an algorithm are made
online, during the run of an algorithm (Horvitz, Ruan, Gomes, Kautz, Selman & Chickering, 2001;
Kautz, Horvitz, Ruan, Gomes & Selman, 2002; Gagliolo & Schmidhuber, 2007). So-called reactive
search methods perform online parameter modifications (Battiti, Brunato & Mascia, 2008). This
last strategy can be seen as complementary to our work: even reactive search methods tend to have
parameters that remain fixed during the search and can hence be configured using offline approaches
such as ParamILS.
9.4 Relation to Other Local Search Methods
Since ParamILS performs an iterated local search with a one-exchange neighbourhood, it is very
similar in spirit to local search methods for other problems, such as SAT (Selman, Levesque &
Mitchell, 1992; Hoos & Stutzle, 2005), CSP (Minton, Johnston, Philips & Laird, 1992), and MPE
(Kask & Dechter, 1999; Hutter et al., 2005). Since ParamILS is a local search method, existing
theoretical frameworks (see, e.g., Hoos, 2002; Mengshoel, 2008), could in principle be used for
its analysis. The main factor distinguishing our problem from the ones faced by standard local
search algorithms is the stochastic nature of our optimization problem (for a discussion of local
search for stochastic optimization, see, e.g., Spall, 2003). Furthermore, there exists no compact
representation of the objective function that could be used to guide the search. To illustrate this,
consider local search for SAT, where the candidate variables to be flipped can be limited to those
occurring in currently-unsatisfied clauses. In general algorithm configuration, on the other hand,
such a mechanism cannot be used, because the only information available about the target algorithm
is its performance in the runs executed so far. While, obviously, other (stochastic) local search
methods could be used as the basis for algorithm configuration procedures, we chose iterated local
search, mainly because of its conceptual simplicity and flexibility.

10. Discussion, Conclusions and Future work
In this work, we studied the problem of automatically configuring the parameters of complex,
heuristic algorithms in order to optimize performance on a given set of benchmark instances. We
extended our earlier algorithm configuration procedure, ParamILS, with a new capping mechanism
300

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

and obtained excellent results when applying the resulting enhanced version of ParamILS to two
high-performance SAT algorithms as well as to C PLEX and a wide range of benchmark sets.
Compared to the carefully-chosen default configurations of these target algorithms, the parameter configurations found by ParamILS almost always performed much better when evaluated on
sets of previously unseen test instances, for some configuration scenarios by as much as two orders
of magnitude. The improvements over C PLEXs default parameter configuration are particularly
noteworthy, though we do not claim to have found a new parameter configuration for C PLEX that
is uniformly better than its default. Rather, given a somewhat homogeneous instance set, we find a
configuration specific to that set that typically outperforms the default, sometimes by a factor as high
as 20. Note that we achieved these results even though we are not intimately familiar with C PLEX
and its parameters; we chose the parameters to optimize as well as the values to consider based
on a single person-day of studying the C PLEX user manual. The success of automated algorithm
configuration even under these extreme conditions demonstrates the potential of the approach.
The ParamILS source code and executable are freely available at
http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/,
along with a quickstart guide and data for the configuration scenarios studied in this article.9
In order to apply ParamILS, or other such automated algorithm configuration methods, a practitioner must supply the following ingredients.
 A parameterized algorithm A It must be possible to set As configurable parameters externally, e.g., in a command line call. Often, a search for hard-coded parameters hidden in the
algorithms source code can lead to a large number of additional parameters to be exposed.
 Domains for the parameters Algorithm configurators must be provided with the allowable
values for each parameter. Depending on the configurator, it may be possible to include additional knowledge about dependencies between parameters, such as the conditional parameters
supported by ParamILS. For the use of ParamILS, numerical parameters must be discretized
to a finite number of choices. Depending on the type of parameter, a uniform spacing of
values or some other spacing, such as uniform on a log scale, is typically reasonable.
 A set of problem instances The more homogeneous the problem set of interest is, the better
we can expect any algorithm configuration procedure to perform on it. While it is possible
to configure an algorithm for good performance on rather heterogeneous instance sets (e.g.,
on industrial SAT instances, as we did with S PEAR as reported in Section 8.2), the results for
homogeneous subsets of interest will improve when we configure on instances from that subset. Whenever possible, the set of instances should be split into disjoint training and test sets
in order to safeguard against over-tuning. When configuring on a small and/or heterogeneous
benchmark set, ParamILS (or any other configuration procedure) might not find configurations that perform well on an independent test set.
 An objective function While we used median performance in our first study on ParamILS
(Hutter et al., 2007), we have since found cases where optimizing median performance led
to parameter configurations with good median but poor overall performance. In these cases,
optimizing for mean performance yielded more robust parameter configurations. However,
when optimizing mean performance one has to define the cost for unsuccessful runs. In this
article, we have penalized such runs by counting them as ten times the cutoff time. How to
deal with unsuccessful runs in a more principled manner is an open research question.
9. ParamILS continues to be actively developed; it is currently maintained by Chris Fawcett.

301

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

 A cutoff time for unsuccessful runs The smaller the cutoff time for each run of the target
algorithm is chosen, the more quickly any configuration procedure will be able to explore the
configuration space. However, choosing too small a cutoff risks the failure mode we experienced with our C P L E X -QP scenario. Recall that there, choosing 300 seconds as a timeout
yielded a parameter configuration that was very good when judged with that cutoff time (see
Figure 6(f)), but performed poorly for longer cutoffs (see Figure 6(e)). In all of our other experiments, parameter configurations performing well with low cutoff times turned out to scale
well to harder problem instances as well. In many configuration scenarios, in fact, we noticed
that our automatically-found parameter configurations showed much better scaling behaviour
than the default configuration. We attribute this to our use of mean runtime as a configuration
objective. The mean is often dominated by the hardest instances in a distribution. However, in
manual tuning, algorithm developers typically pay more attention to easier instances, simply
because repeated profiling on hard instances takes too long. In contrast, a patient automatic
configurator can achieve better results because it avoids this bias.
 Computational resources The amount of (computational) time required for the application
of automated algorithm configuration clearly depends on the target application. If the target
algorithm takes seconds to solve instances from a homogeneous benchmark set of interest,
in our experience a single five-hour configuration run will suffice to yield good results and
for some domains we have achieved good results with configuration times as short as half an
hour. In contrast, if runs of the target algorithm are slow and only performance with a large
cutoff time can be expected to yield good results on the instances of interest, then the time
requirements of automated algorithm configuration grow. We also regularly perform multiple
parallel configuration runs and pick the one with best training performance in order to deal
with variance across configuration runs.
Overall, we firmly believe that automated algorithm configuration methods such as ParamILS
will play an increasingly prominent role in the development of high-performance algorithms and
their applications. The study of such methods is a rich and fruitful research area with many interesting questions remaining to be explored.
In ongoing work, we are currently developing methods that adaptively adjust the domains of
integer-valued and continuous parameters during the configuration process. Similarly, we plan to
enhance ParamILS with dedicated methods for dealing with continuous parameters that do not require discretization by the user. Another direction for further development concerns the strategic
selection of problem instances used during evaluation of configurations and of instance-specific cutoff times used in this context. By heuristically preventing the configuration procedure from spending inordinate amounts of time trying to evaluate poor parameter settings on very hard problem
instances, it should be possible to improve its scalability.
We believe that there is significant room for combining aspects of the methods studied here with
concepts from related work on this and similar algorithm configuration problems. In particular,
we believe it would be fruitful to integrate statistical testing methodsas used, e.g., in F-Race
into ParamILS. Furthermore, we see much potential in the use of response surface models and
active learning, and believe these can be combined with our approach. Finally, while the algorithm
configuration problem studied in this article is of significant practical importance, there is also much
to be gained from studying methods for related problems, in particular, instance-specific algorithm
configuration and the online adjustment of parameters during the run of an algorithm.
302

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

Acknowledgments
We thank Kevin Murphy for many helpful discussions regarding this work. We also thank Domagoj
Babic, the author of S PEAR, and Dave Tompkins, the author of the UBCSAT S APS implementation
we used in our experiments. We thank the researchers who provided the instances or instance
generators used in our work, in particular Gent et al. (1999), Gomes and Selman (1997), LeytonBrown et al. (2000), Babic and Hu (2007), Zarpas (2005), Le Berre and Simon (2004), Akturk
et al. (2007), Atamturk and Munoz (2004), Atamturk (2003), and Andronescu et al. (2007). Lin
Xu created the specific sets of QCP and SWGCP instances we used. Thanks also to Chris Fawcett
and Ashique KhudaBukhsh for their comments on a draft of this article. Finally, we thank the
anonymous reviewers as well as Rina Dechter and Adele Howe for their valuable feedback. Thomas
Stutzle acknowledges support from the F.R.S.-FNRS, of which he is a Research Associate. Holger
Hoos acknowledges support through NSERC Discovery Grant 238788.

References
Adenso-Diaz, B. & Laguna, M. (2006). Fine-tuning of algorithms using fractional experimental design and
local search. Operations Research, 54(1), 99114.
Akturk, S. M., Atamturk, A., & Gurel, S. (2007). A strong conic quadratic reformulation for machine-job
assignment with controllable processing times. Research Report BCOL.07.01, University of CaliforniaBerkeley.
Andronescu, M., Condon, A., Hoos, H. H., Mathews, D. H., & Murphy, K. P. (2007). Efficient parameter
estimation for RNA secondary structure prediction. Bioinformatics, 23, i19i28.
Atamturk, A. (2003). On the facets of the mixedinteger knapsack polyhedron. Mathematical Programming,
98, 145175.
Atamturk, A. & Munoz, J. C. (2004). A study of the lot-sizing polytope. Mathematical Programming, 99,
443465.
Audet, C. & Orban, D. (2006). Finding optimal algorithmic parameters using the mesh adaptive direct search
algorithm. SIAM Journal on Optimization, 17(3), 642664.
Babic, D. & Hu, A. J. (2007). Structural Abstraction of Software Verification Conditions. In W. Damm, H. H.
(Ed.), Computer Aided Verification: 19th International Conference, CAV 2007, volume 4590 of Lecture
Notes in Computer Science, (pp. 366378). Springer Verlag, Berlin, Germany.
Balaprakash, P., Birattari, M., & Stutzle, T. (2007). Improvement strategies for the F-Race algorithm: Sampling design and iterative refinement. In Bartz-Beielstein, T., Aguilera, M. J. B., Blum, C., Naujoks,
B., Roli, A., Rudolph, G., & Sampels, M. (Eds.), 4th International Workshop on Hybrid Metaheuristics (MH07), (pp. 108122).
Bartz-Beielstein, T. (2006). Experimental Research in Evolutionary Computation: The New Experimentalism. Natural Computing Series. Springer Verlag, Berlin, Germany.
Battiti, R., Brunato, M., & Mascia, F. (2008). Reactive Search and Intelligent Optimization, volume 45 of
Operations research/Computer Science Interfaces. Springer Verlag. Available online at http://reactivesearch.org/thebook.
Birattari, M. (2004). The Problem of Tuning Metaheuristics as Seen from a Machine Learning Perspective.
PhD thesis, Universite Libre de Bruxelles, Brussels, Belgium.
Birattari, M., Stutzle, T., Paquete, L., & Varrentrapp, K. (2002). A racing algorithm for configuring metaheuristics. In Langdon, W. B., Cantu-Paz, E., Mathias, K., Roy, R., Davis, D., Poli, R., Balakrishnan, K.,
Honavar, V., Rudolph, G., Wegener, J., Bull, L., Potter, M. A., Schultz, A. C., Miller, J. F., Burke, E.,
& Jonoska, N. (Eds.), Proceedings of the Genetic and Evolutionary Computation Conference (GECCO2002), (pp. 1118). Morgan Kaufmann Publishers, San Francisco, CA, USA.
303

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

Carchrae, T. & Beck, J. C. (2005). Applying machine learning to low-knowledge control of optimization
algorithms. Computational Intelligence, 21(4), 372387.
Cavazos, J. & OBoyle, M. F. P. (2006). Method-specific dynamic compilation using logistic regression. In
Cook, W. R. (Ed.), Proceedings of the ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA-06), (pp. 229240)., New York, NY, USA.
ACM Press.
Coy, S. P., Golden, B. L., Runger, G. C., & Wasil, E. A. (2001). Using experimental design to find effective
parameter settings for heuristics. Journal of Heuristics, 7(1), 7797.
Diao, Y., Eskesen, F., Froehlich, S., Hellerstein, J. L., Spainhower, L., & Surendra, M. (2003). Generic online
optimization of multiple configuration parameters with application to a database server. In Brunner, M. &
Keller, A. (Eds.), 14th IFIP/IEEE International Workshop on Distributed Systems: Operations and Management (DSOM-03), volume 2867 of Lecture Notes in Computer Science, (pp. 315). Springer Verlag,
Berlin, Germany.
Fawcett, C., Hoos, H. H., & Chiarandini, M. (2009). An automatically configured modular algorithm for post
enrollment course timetabling. Technical Report TR-2009-15, University of British Columbia, Department
of Computer Science.
Gagliolo, M. & Schmidhuber, J. (2006). Dynamic algorithm portfolios. In Amato, C., Bernstein, D., & Zilberstein, S. (Eds.), Ninth International Symposium on Artificial Intelligence and Mathematics (AI-MATH-06).
Gagliolo, M. & Schmidhuber, J. (2007). Learning restart strategies. In Veloso, M. M. (Ed.), Proceedings
of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI07), volume 1, (pp. 792
797). Morgan Kaufmann Publishers, San Francisco, CA, USA.
Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR to select solution strategies in constraint programming. In Munoz-Avila, H. & Ricci, F. (Eds.), Proceedings of the 6th International Conference on Case Based Reasoning (ICCBR05), volume 3620 of Lecture Notes in Computer Science, (pp.
222236). Springer Verlag, Berlin, Germany.
Gent, I. P., Hoos, H. H., Prosser, P., & Walsh, T. (1999). Morphing: Combining structure and randomness.
In Hendler, J. & Subramanian, D. (Eds.), Proceedings of the Sixteenth National Conference on Artificial
Intelligence (AAAI99), (pp. 654660)., Orlando, Florida. AAAI Press / The MIT Press, Menlo Park, CA,
USA.
Gomes, C. P. & Selman, B. (1997). Problem structure in the presence of perturbations. In Kuipers, B. &
Webber, B. (Eds.), Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI97),
(pp. 221226). AAAI Press / The MIT Press, Menlo Park, CA, USA.
Gratch, J. & Chien, S. A. (1996). Adaptive problem-solving for large-scale scheduling problems: A case
study. Journal of Artificial Intelligence Research, 4, 365396.
Gratch, J. & Dejong, G. (1992). Composer: A probabilistic solution to the utility problem in speed-up
learning. In Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings of the Tenth National Conference on
Artificial Intelligence (AAAI92), (pp. 235240). AAAI Press / The MIT Press, Menlo Park, CA, USA.
Hoos, H. H. (2002). A mixture-model for the behaviour of SLS algorithms for SAT. In Proceedings of the
Eighteenth National Conference on Artificial Intelligence (AAAI-02), (pp. 661667)., Edmonton, Alberta,
Canada.
Hoos, H. H. (2008). Computer-aided design of high-performance algorithms. Technical Report TR-2008-16,
University of British Columbia, Department of Computer Science.
Hoos, H. H. & Stutzle, T. (2005). Stochastic Local Search  Foundations & Applications. Morgan Kaufmann
Publishers, San Francisco, CA, USA.
Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). A Bayesian
approach to tackling hard computational problems. In Breese, J. S. & Koller, D. (Eds.), Proceedings
of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI01), (pp. 235244). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
304

fiPARAM ILS: A N AUTOMATIC A LGORITHM C ONFIGURATION F RAMEWORK

Hutter, F. (2009). Automated Configuration of Algorithms for Solving Hard Computational Problems. PhD
thesis, University Of British Columbia, Department of Computer Science, Vancouver, Canada.
Hutter, F., Babic, D., Hoos, H. H., & Hu, A. J. (2007). Boosting Verification by Automatic Tuning of Decision
Procedures. In Proceedings of Formal Methods in Computer Aided Design (FMCAD07), (pp. 2734).,
Washington, DC, USA. IEEE Computer Society.
Hutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction and automated
tuning of randomized and parametric algorithms. In Benhamou, F. (Ed.), Principles and Practice of Constraint Programming  CP 2006: Twelfth International Conference, volume 4204 of Lecture Notes in
Computer Science, (pp. 213228). Springer Verlag, Berlin, Germany.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2009). Tradeoffs in the empirical evaluation of competing algorithm designs. Technical Report TR-2009-21, University of British Columbia, Department of Computer
Science.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Murphy, K. P. (2009). An experimental investigation of
model-based parameter optimisation: SPO and beyond. In Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO-2009), (pp. 271278).
Hutter, F., Hoos, H. H., & Stutzle, T. (2005). Efficient stochastic local search for MPE solving. In Proceedings
of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI05), (pp. 169174).
Hutter, F., Hoos, H. H., & Stutzle, T. (2007). Automatic algorithm configuration based on local search.
In Howe, A. & Holte, R. C. (Eds.), Proceedings of the Twenty-second National Conference on Artificial
Intelligence (AAAI07), (pp. 11521157). AAAI Press / The MIT Press, Menlo Park, CA, USA.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling and probabilistic smoothing: Efficient dynamic
local search for SAT. In Hentenryck, P. V. (Ed.), Principles and Practice of Constraint Programming 
CP 2002: Eighth International Conference, volume 2470 of Lecture Notes in Computer Science, (pp.
233248). Springer Verlag, Berlin, Germany.
Johnson, D. S. (2002). A theoreticians guide to the experimental analysis of algorithms. In Goldwasser,
M. H., Johnson, D. S., & McGeoch, C. C. (Eds.), Data Structures, Near Neighbor Searches, and Methodology: Fifth and Sixth DIMACS Implementation Challenges, (pp. 215250). American Mathematical Society, Providence, RI, USA.
Jones, D. R., Schonlau, M., & Welch, W. J. (1998). Efficient global optimization of expensive black box
functions. Journal of Global Optimization, 13, 455492.
Kask, K. & Dechter, R. (1999). Stochastic local search for Bayesian networks. In The Seventh International
Workshop on Artificial Intelligence and Statistics (AISTATS99).
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C. P., & Selman, B. (2002). Dynamic restart policies. In Dechter,
R., Kearns, M., & Sutton, R. (Eds.), Proceedings of the Eighteenth National Conference on Artificial
Intelligence (AAAI02), (pp. 674681). AAAI Press / The MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building local search sat solvers from components. In Proceedings of the Twenty-first International Joint Conference
on Artificial Intelligence (IJCAI09), (pp. 517524).
Le Berre, D. & Simon, L. (2004). Fifty-five solvers in Vancouver: The SAT 2004 competition. In Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory and Applications of Satisfiability Testing: Proceedings of the Seventh
International Conference (SAT04), volume 3542 of Lecture Notes in Computer Science, (pp. 321344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning the empirical hardness of optimization
problems: The case of combinatorial auctions. In Hentenryck, P. V. (Ed.), Principles and Practice of
Constraint Programming  CP 2002: Eighth International Conference, volume 2470 of Lecture Notes in
Computer Science, (pp. 556572). Springer Verlag, Berlin, Germany.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards a universal test suite for combinatorial
auction algorithms. In Jhingran, A., Mason, J. M., & Tygar, D. (Eds.), EC 00: Proceedings of the 2nd
305

fiH UTTER , H OOS , L EYTON -B ROWN & S T UTZLE

ACM conference on Electronic commerce, (pp. 6676)., New York, NY, USA. ACM.
Lourenco, H. R., Martin, O., & Stutzle, T. (2002). Iterated local search. In F. Glover & G. Kochenberger
(Eds.), Handbook of Metaheuristics (pp. 321353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search for classification
and function approximation. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 5966). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding the role of noise in stochastic local search: Analysis and experiments. Artificial Intelligence, 172(8-9), 955990.
Minton, S. (1993). An analytic learning system for specializing heuristics. In Bajcsy, R. (Ed.), Proceedings of
the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI93), (pp. 922929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically configuring constraint satisfaction programs: A case study. Constraints,
1(1), 140.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: A heuristic repair
method for constraint-satisfaction and scheduling problems. Artificial Intelligence, 58(1), 161205.
Patterson, D. J. & Kautz, H. (2001). Auto-WalkSAT: a self-tuning implementation of WalkSAT. In Electronic
Notes in Discrete Mathematics (ENDM), 9.
Ridge, E. & Kudenko, D. (2006). Sequential experiment designs for screening and tuning parameters of
stochastic heuristics. In Paquete, L., Chiarandini, M., & Basso, D. (Eds.), Workshop on Empirical Methods
for the Analysis of Algorithms at the Ninth International Conference on Parallel Problem Solving from
Nature (PPSN), (pp. 2734).
Santner, T. J., Williams, B. J., & Notz, W. I. (2003). The Design and Analysis of Computer Experiments.
Springer Verlag, New York.
Selman, B., Levesque, H. J., & Mitchell, D. (1992). A new method for solving hard satisfiability problems.
In Rosenbloom, P. & Szolovits, P. (Eds.), Proceedings of the Tenth National Conference on Artificial
Intelligence (AAAI92), (pp. 440446). AAAI Press / The MIT Press, Menlo Park, CA, USA.
Spall, J. C. (2003). Introduction to Stochastic Search and Optimization. New York, NY, USA: John Wiley &
Sons, Inc.
Terashima-Marn, H., Ross, P., & Valenzuela-Rendon, M. (1999). Evolution of constraint satisfaction strategies in examination timetabling. In Proceedings of the Genetic and Evolutionary Computation Conference
(GECCO-1999), (pp. 635642). Morgan Kaufmann.
Thachuk, C., Shmygelska, A., & Hoos, H. H. (2007). A replica exchange monte carlo algorithm for protein
folding in the hp model. BMC Bioinformatics, 8, 342342.
Tompkins, D. A. D. & Hoos, H. H. (2004). UBCSAT: An implementation and experimentation environment
for SLS algorithms for SAT & MAX-SAT. In Theory and Applications of Satisfiability Testing: Proceedings of the Seventh International Conference (SAT04), volume 3542, (pp. 306320). Springer Verlag,
Berlin, Germany.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: portfolio-based algorithm selection
for SAT. Journal of Artificial Intelligence Research, 32, 565606.
Zarpas, E. (2005). Benchmarking SAT Solvers for Bounded Model Checking. In Bacchus, F. & Walsh, T.
(Eds.), Theory and Applications of Satisfiability Testing: Proceedings of the Eighth International Conference (SAT05), volume 3569 of Lecture Notes in Computer Science, (pp. 340354). Springer Verlag.

306

fiJournal of Artificial Intelligence Research 36 (2009) 513-546

Submitted 08/09; published 12/09

RoxyBot-06: Stochastic Prediction and Optimization
in TAC Travel
Amy Greenwald

amy@cs.brown.edu

Department of Computer Science, Brown University
Providence, RI 02912 USA

Seong Jae Lee

seongjae@u.washington.edu

Computer Science and Engineering, University of Washington
Seattle, WA 98195 USA

Victor Naroditskiy

vnarodit@cs.brown.edu

Department of Computer Science, Brown University
Providence, RI 02912 USA

Abstract
In this paper, we describe our autonomous bidding agent, RoxyBot, who emerged victorious in the travel division of the 2006 Trading Agent Competition in a photo finish. At
a high level, the design of many successful trading agents can be summarized as follows:
(i) price prediction: build a model of market prices; and (ii) optimization: solve for an
approximately optimal set of bids, given this model. To predict, RoxyBot builds a stochastic model of market prices by simulating simultaneous ascending auctions. To optimize,
RoxyBot relies on the sample average approximation method, a stochastic optimization
technique.

1. Introduction
The annual Trading Agent Competition (TAC) challenges its entrants to design and build
autonomous agents capable of effective trading in an online travel1 shopping game. The first
TAC, held in Boston in 2000, attracted 16 entrants from six countries in North America,
Europe, and Asia. Excitement generated from this event led to refinement of the game
rules, and continuation of regular tournaments with increasing levels of competition over
the next six years. Year-by-year, entrants improved their designs, developing new ideas
and building on previously successful techniques. Since TACs inception, the lead author
has entered successive modifications of her autonomous trading agent, RoxyBot. This paper
reports on RoxyBot-06, the latest incarnation and the top scorer in the TAC-06 tournament.
The key feature captured by the TAC travel game is that goods are highly interdependent
(e.g., flights and hotels must be coordinated), yet the markets for these goods operate
independently. A second important feature of TAC is that agents trade via three different
kinds of market mechanisms, each of which presents distinct challenges. Flights are traded
in a posted-price environment, where a designated party sets a price that the other parties
1. There are now four divisions of TAC: Travel, Supply Chain Management (SCM), CAT (TAC backwards),
and Ad Auctions (AA). This paper is concerned only with the first; for a description of the others, see the
papers by Arunachalam and Sadeh (2005), Cai et al. (2009), Jordan and Wellman (2009), respectively.
In this paper, when we say TAC, we mean TAC Travel.

c
2009
AI Access Foundation. All rights reserved.

fiGreenwald, Lee, & Naroditskiy

must take or leave. Hotels are traded in simultaneous ascending auctions, like the FCC
spectrum auctions. Entertainment tickets are traded in continuous double auctions, like
the New York Stock Exchange. In grappling with all three mechanisms while constructing
their agent strategies, participants are confronted by a number of interesting problems.
The success of an autonomous trading agent such as a TAC agent often hinges upon
the solutions to two key problems: (i) price prediction, in which the agent builds a model
of market prices; and (ii) optimization, in which the agent solves for an approximately
optimal set of bids, given this model. For example, at the core of RoxyBots 2000 architecture (Greenwald & Boyan, 2005) was a deterministic optimization problem, namely how to
bid given price predictions in the form of point estimates. In spite of its effectiveness in the
TAC-00 tournament, a weakness of the 2000 design was that RoxyBot could not explicitly
reason about variance within prices. In the years since 2000, we recast the key challenges
faced by TAC agents as several different stochastic bidding problems (see, for example, the
paper by Greenwald & Boyan, 2004), whose solutions exploit price predictions in the form
of distributions. In spite of our perseverance, RoxyBot fared unimpressively in tournament
conditions year after year, until 2006. Half a decade in the laboratory spent searching for
bidding heuristics that can exploit stochastic information at reasonable computational expense finally bore fruit, as RoxyBot emerged victorious in TAC-06. In a nutshell, the secret
of RoxyBot-06s success is: (hotel) price prediction by simulating simultaneous ascending
auctions, and optimization based on the sample average approximation method. Details of
our approach are the subject of the present article.
Overview This paper is organized as follows. Starting in Section 2, we summarize the
TAC market game. Next, in Section 3, we present a high-level view of RoxyBots 2006
architecture. In Section 4, we describe RoxyBots price prediction techniques for flights, hotels, and entertainment, in turn. Perhaps of greatest interest is our hotel price prediction
method. Following Wellman et al. (2005), we predict hotel prices by computing approximate competitive equilibrium prices. Only, instead of computing those prices by running
the tatonnement process, we simulate simultaneous ascending auctions. Our procedure is
simpler to implement than tatonnement, yet achieves comparable performance, and runs
sufficiently fast. In Section 5, we describe RoxyBots optimization technique: sample average
approximation. We argue that this approach is optimal in pseudo-auctions, an abstract
model of auctions. In Section 6.1, we describe simulation experiments in a controlled testing environment which show that our combined approachsimultaneous ascending auctions
for hotel price prediction and sample average approximation for bid optimizationperforms
well in practice in comparison with other reasonable bidding heuristics. In Section 6.2, we
detail the results of the TAC-06 tournament, further validating the success of RoxyBot-06s
strategy, and reporting statistics that shed light on the bidding strategies of other participating agents. Finally, in Section 7, we evaluate the collective behavior of the autonomous
agents in the TAC finals since 2002. We find that the accuracy of competitive equilibrium
calculations has varied from year to year and is highly dependent on the particular agent
pool. Still, generally speaking, the collective appears to be moving toward competitive
equilibrium behavior.

514

fiRoxyBot-06

2. TAC Market Game: A Brief Summary
In this section, we summarize the TAC game. For more details, see http://www.sics.se/
tac/.
Eight agents play the TAC game. Each is a simulated travel agent whose task is to
organize itineraries for its clients to travel to and from TACTown during a five day (four
night) period. In the time allotted (nine minutes), each agents objective is to procure
travel goods as inexpensively as possible, trading off against the fact that those goods are
ultimately compiled into feasible trips that satisfy its client preferences to the greatest
extent possible. The agents know the preferences of their own eight clients only, not the
other 56.
Travel goods are sold in simultaneous auctions as follows:
 Flight tickets are sold by TACAir in dynamic posted-pricing environments. There
are flights both to and from TACTown on each applicable day. No resale of flight
tickets by agents is permitted.
Flight price quotes are broadcast by the TAC server every ten seconds.
 Hotel reservations are sold by the TAC seller in multi-unit ascending call markets.
Specifically, 16 hotel reservations are sold in each hotel auction to the 16 highest
bidders at the 16th highest price. The TAC seller runs eight hotel auctions, one per
night-hotel combination (recall that travel takes place during a four night period;
moreover, there are two hotels: a good one and a bad one). No resale of hotel
reservations by agents is permitted. Nor is bid withdrawal allowed.
More specifically, the eight hotel auctions clear on the minute with exactly one auction
closing at each of minutes one through eight. (The precise auction to close is chosen
at random, with all open auctions equally likely to be selected.) For the auction that
closes, the TAC server broadcasts the final closing price, and informs each agent of its
winnings. For the others, the TAC server reports the current ask price, and informs
each agent of its hypothetical quantity won (HQW).
 Agents are allocated an initial endowment of entertainment tickets, which they trade
among themselves in continuous double auctions (CDAs). There are three entertainment events scheduled each day.
Although the event auctions clear continuously, price quotes are broadcast only every
30 seconds.
One of the primary challenges posed by TAC is to design and build autonomous agents
that bid effectively on interdependent (i.e., complementary or substitutable) goods that are
sold in separate markets. Flight tickets and hotel reservations are complementary because
flights are not useful to a client without the corresponding hotel reservations, nor vice
versa. Tickets to entertainment events (e.g., the Boston Red Sox and the Boston Symphony
Orchestra) are substitutable because a client cannot attend multiple events simultaneously.

515

fiGreenwald, Lee, & Naroditskiy

REPEAT
{start bid interval }
0. Download current prices and winnings from server
1. predict: build stochastic models
a. flights: Bayesian updating/learning
b. hotels: simultaneous ascending auctions
c. entertainment: sample historical data
2. optimize: sample average approximation
3. Upload current bids to server
(three separate threads)
{end bid interval }
UNTIL game over

Table 1: A high-level view of RoxyBot-06s architecture.

3. RoxyBot-06s Architecture: A High-Level View
In our approach to the problem of bidding on interdependent goods in the separate TAC
markets, we adopt some simplifying assumptions. Rather than tackle the game-theoretic
problem of characterizing strategic equilibria, we focus on a single agents (decision-theoretic)
problem of optimizing its own bidding behavior, assuming the other agents strategies are
fixed. In addition, we assume that the environment can be modeled in terms of the agents
predictions about market clearing prices. These prices serve to summarize the relevant information hidden in other agents bidding strategies. These two assumptionsfixed otheragent behaviors and market information encapsulated by pricessupport the modular design of RoxyBot-06 and many other successful TAC agents, which consists of two key stages:
(i) price prediction; and (ii) optimization.
The optimization problem faced by TAC agents is a dynamic one that incorporates
aspects of sequentiality as well as simultaneity in auctions. The markets operate simultaneously, but in addition, prices are discovered incrementally over time. In principle, a
clairvoyant agentone with knowledge of future clearing pricescould justifiably employ
an open-loop strategy: it could solve the TAC optimization problem once at the start of the
game and place all its bids accordingly, never reconsidering those decisions. A more practical alternative (and the usual approach taken in TAC2 ), is to incorporate into an agents
architecture a closed loop, or bidding cycle, enabling the agent to condition its behavior
on the evolution of prices. As price information is revealed, the agent improves its price
predictions, and reoptimizes its bidding decisions, repeatedly.
One distinguishing feature of RoxyBot-06 is that it builds stochastic models of market
clearing prices, rather than predicting clearing prices as point estimates. Given its stochastic
price predictions, stochastic optimization lies at the heart of RoxyBot-06. Assuming time is
2. An exception is livingagents (Fritschi & Dorer, 2002), the winner of TAC 2001.

516

fiRoxyBot-06

discretized into stages, or bid intervals, during each iteration of its bidding cycle, RoxyBot-06
faces an n-stage stochastic optimization problem, where n is the number of stages remaining
in the game. The key input to this optimization problem is a sequence of n  1 stochastic
models of future prices, each one a joint probability distribution over all goods conditioned
on past prices and past hotel closings. The solution to this optimization problem, and the
output of each iteration of the bidding cycle, is a vector of bids, one per good (or auction).
Table 1 presents a high-level view of RoxyBot-06s architecture, emphasizing its bidding
cycle. At the start of each bid interval, current prices and winnings are downloaded from
the TAC server. Next, the key prediction and optimization routines are run. In the prediction module, stochastic models of flight, hotel, and entertainment prices are built. In
the optimization module, bids are constructed as an approximate solution to an n-stage
stochastic optimization problem. Prior to the end of each bid interval, the agents bids
are uploaded to the TAC server using three separate threads: (i) the flight thread bids on
a flight only if its price is near its predicted minimum; (ii) the hotel thread bids on open
hotels only if it is moments before the end of a minute; and (iii) the entertainment thread
places bids immediately.
We discuss the details of RoxyBot-06s price prediction module first, and its optimization
module second.

4. Price Prediction
In this section, we describe how RoxyBot-06 builds its stochastic models of flight, hotel, and
event prices. Each model is a discrete probability distribution, represented by a set of scenarios. Each scenario is a vector of future pricesprices at which goods can be bought
and sold after the current stage. For flights, the price prediction model is not stochastic: the
future buy price is simply RoxyBot-06s prediction of the expected minimum price during the
current stage. For hotels, the future buy prices are predicted by Monte Carlo simulations
of simultaneous ascending auctions to approximate competitive equilibrium prices. There
are no current buy prices for hotels. For entertainment, RoxyBot-06 predicts future buy and
sell prices based on historical data. Details of these price prediction methods are the focus
of this section.
4.1 Flights
Efforts to deliberate about flight purchasing start with understanding the TAC model of
flight price evolution.
4.1.1 TAC Flight Prices Stochastic Process
Flight prices follow a biased random walk. They are initialized uniformly in the range
[250, 400], and constrained to remain in the range [150, 800]. At the start of each TAC
game instance, a bound z on the final perturbation value is selected for each flight. These
bounds are not revealed to the agents. What is revealed to the agents is a sequence of
random flight prices. Every ten seconds, TACAir perturbs the price of each flight by a
random value that depends on the hidden parameter z and the current time t as follows:
given constants c, d  R and T > 0, each (intermediate) bound on the perturbation value

517

fiGreenwald, Lee, & Naroditskiy

is a linear function of t:

t
(z  c)
(1)
T
The perturbation value at time t is drawn uniformly from one of the following ranges (see
Algorithm 1):
x(t, z) = c +

 U [c, x(t, z)], if x(t, z) > 0
 U [c, +c], if x(t, z) = 0
 U [x(t, z), +c], if x(t, z) < 0
Observe that the expected perturbation value in each case is simply the average of the
corresponding upper and lower bounds. In particular,
 if x(t, z) > c, then the expected perturbation is positive;
 if x(t, z)  (0, c), then the expected perturbation is negative;
 if x(t, z)  (c, 0), then the expected perturbation is positive;
 otherwise, if x(t, z)  {c, 0, c}, then the expected perturbation is zero.
Moreover, using Equation 1, we can compute the expected perturbation value conditioned
on z:
 if z  [0, c], then x(t, z)  [0, c], so prices are expected not to increase;
 if z  [c, c + d], then x(t, z)  [c, c + d], so prices are expected not to decrease;
 if z  [c, 0], then x(t, z)  [c, c], so prices are expected not to increase while t 
cT
.
and they are expected not to decrease while t  cz

cT
cz

The TAC parameters are set as follows: c = 10, d = 30, T = 540, and z uniformly
distributed in the range [c, d]. Based on the above discussion, we note the following:
given no further information about z, TAC flight prices are expected to increase (i.e.,
the expected perturbation is positive); however, conditioned on z, TAC flight prices may
increase or decrease (i.e., the expected perturbation can be positive or negative).
4.1.2 RoxyBot-06s Flight Prices Prediction Method
Although the value of the hidden parameter z is never revealed to the agents, recall that
the agents do observe sample flight prices, say y1 , . . . , yt , that depend on this value. This
information can be used to model the probability distribution Pt [z]  P [z | y1 , . . . , yt ].
Such a probability distribution can be estimated using Bayesian updating. Before RoxyBot06, agents Walverine (Cheng et al., 2005) and Mertacor (Toulis et al., 2006) took this approach.
Walverine uses Bayesian updating to compute the next expected price perturbation and then
compares that value to a threshold, postponing its flight purchases if prices are not expected
to increase by more than that threshold. Mertacor uses Bayesian updating to estimate the
time at which flight prices will reach their minimum value. RoxyBot uses Bayesian updating
to compute the expected minimum price, as we now describe.
518

fiRoxyBot-06

Algorithm 1 getRange(c, t, z)
compute x(t, z) {Equation 1}
if x(t, z) > 0 then
a = c; b = x(t, z)
else if x(t, z) < 0 then
a = x(t, z); b = +c
else
a = c; b = +c
end if
return [a, b] {range}
RoxyBot-06s implementation of Bayesian updating is presented in Algorithm 2. Letting

Q0 [z] =

1
c+d

= P [z], the algorithm estimates Pt+1 [z] = P [z | y1 , . . . , yt+1 ] as usual:
P [y1 , . . . , yt | z]P [z]



z  P [y1 , . . . , yt | z ]P [z ] dz

where

P [z | y1 , . . . , yt ] = P
P [y1 , . . . , yt | z] =
=

t
Y

i=1
t
Y

(2)

P [yi | y1 , . . . , yi1 , z]

(3)

P [yi | z]

(4)

i=1

Equation 4 follows from the fact that future observations are independent of past observations; observations depend only on the hidden parameter z.
The only thing left to explain is how to set the values P [yi | z], for i = 1, . . . , t. As
described in the pseudocode, this is done as follows: if yt+1 is within the appropriate range at
that time, then this probability is set uniformly within the bounds of that range; otherwise,
it is set to 0. Presumably, Walverines and Mertacors implementations of Bayesian updating
are not very different from this one.3 However, as alluded to above, how the agents make
use of the ensuing estimated probability distributions does differ.
RoxyBot-06 predicts each flights price to be its expected minimum price. This value is
computed as follows (see Algorithm 3): for each possible value of the hidden parameter z,
RoxyBot simulates an expected random walk, selects the minimum price along this walk,
and then outputs as its prediction the expectation of these minima, averaging according to
Pt [z]. We call this random walk expected, since the perturbation value  is an expectation
(i.e.,  = ba
2 ) instead of a sample (i.e.,   U [a, b]). By carrying out this computation,
RoxyBot generates flight price predictions that are point estimates. The implicit decision
to make only RoxyBot-06s hotel and event price predictions stochastic was made based on
our intuitive sense of the time vs. accuracy tradeoffs in RoxyBots optimization module, and
hence warrants further study.
3. We provide details here, because corresponding details for the other agents do not seem to be publicly
available.

519

fiGreenwald, Lee, & Naroditskiy

Algorithm 2 Flight Prediction(c, d, t, yt+1 , Qt )
for all z  {c, c + 1, . . . , d} do
[a, b] = getRange(c, t, z)
if yt+1  [a, b] then
1
P [yt+1 | z] = ba
else
P [yt+1 | z] = 0
end if
Qt+1 [z] = P [yt+1 | z]Qt [z]
end for{update probabilities}
for all z  {c, c + 1, . . . , d} do
t+1 [z]
Pt+1 [z] = P  Q


z Qt+1 [z ] dz
end for{normalize probabilities}
return Pt+1 {probabilities}
Algorithm 3 Expected Minimum Price(c, t, t , pt , Pt )
for all z  R do
min[z] = +
for  = t + 1, . . . , t do
[a, b] = getRange(c, , z)
 = ba
2 {expected perturbation}
p = p 1 +  {perturb price}
p = max(150, min(800, p ))
if p < min[z] then
min[z] = p
end if
end for
end forP
return z Pt [z] min[z] dz
4.2 Hotels
In a competitive market where each individuals effect on prices is negligible, equilibrium prices are prices at which supply equals demand, assuming all producers are profitmaximizing and all consumers are utility-maximizing. RoxyBot-06 predicts hotel prices by
simulating simultaneous ascending auctions (SimAA) (Cramton, 2006), in an attempt to
approximate competitive equilibrium (CE) prices. This approach is inspired by Walverines (Cheng et al., 2005), where the tatonnement method (Walras, 1874) is used for the
same purpose.
4.2.1 Simultaneous Ascending Auctions
Let p~ denote a vector of prices. If ~y (~
p) denotes the cumulative supply of all producers, and
if ~x(~
p) denotes the cumulative demand of all consumers, then ~z(~
p) = ~x(~
p)~y (~
p) denotes the

520

fiRoxyBot-06

excess demand in the market. The tatonnement process adjusts the price vector at iteration
n + 1, given the price vector at iteration n and an adjustment rate n as follows: p~n+1 =
p~n + n~z(~
pn ). SimAA adjusts the price vector as follows: p~n+1 = p~n +  max{~z(~
pn ), 0}, for
some fixed value of . Both of these processes continue until excess demand is non-positive:
i.e., supply exceeds demand.
Although competitive equilibrium prices are not guaranteed to exist in TAC markets (Cheng et al., 2003), the SimAA adjustment process, is still guaranteed to converge:
as prices increase, demand decreases while supply increases; hence, supply eventually exceeds demand. The only parameter to the SimAA method is the magnitude  of the price
adjustment. The smaller this value, the more accurate the approximation (assuming CE
prices exist), so the value of  can be chosen to be the lowest value that time permits.
The tatonnement process, on the other hand, is more difficult to apply as it is not
guaranteed to converge. The Walverine team dealt with the convergence issue by decaying
an initial value of . However, careful optimization was required to ensure convergence to
a reasonable solution in a reasonable amount of time. In fact, Walverine found it helpful
to set initial prices to certain non-zero values. This complexity is not present when using
simultaneous ascending auctions to approximate competitive equilibrium prices.
4.2.2 Prediction Quality
In TAC, cumulative supply is fixed. Hence, the key to computing excess demand is to
compute cumulative demand. Each TAC agent knows the preferences of its own clients, but
must estimate the demand of the others. Walverine computes a single hotel price prediction (a
point estimate) by considering its own clients demands together with those of 56 expected
clients. Briefly, the utility of an expected client is an average across travel dates and hotel
types augmented with fixed entertainment bonuses that favor longer trips (see the paper
by Cheng et al., 2005, for details). In contrast, RoxyBot-06 builds a stochastic model of
hotel prices consisting of S scenarios by considering its own clients demands together with
S random samples of 56 clients. A (random or expected) clients demand is simply the
quantity of each good in its optimal package, given current prices. The cumulative demand
is the sum total of all clients individual demands.
In Figure 1, we present two scatter plots that depict the quality of various hotel price
predictions at the beginning of the TAC 2002 final games. All price predictions are evaluated using two metrics: Euclidean distance and the expected value of perfect prediction
(EVPP). Euclidean distance is a measure of the difference between two vectors, in this case
the actual and the predicted prices. The value of perfect prediction (VPP) for a client is the
difference between its surplus (value of its preferred package less price) based on actual and
predicted prices. EVPP is the VPP averaged over the distribution of client preferences.4
On the left, we plot the predictions generated using the competitive equilibrium ap1
proximation methods, tatonnement and SimAA, both with fixed  = 24
, making expected,
random, and exact predictions. The exact predictions are computed based on the actual
clients in the games, not just the client distribution; hence, they serve as a lower bound
on the performance of these techniques on this data set. Under both metrics, and for both
expected and random, SimAAs predictions outperform tatonnements.
4. See the paper by Wellman et al. (2004) for details.

521

fiGreenwald, Lee, & Naroditskiy

44

70
livingagents
PackaTAC
Southampton
RoxyBot UMBCTAC
whitebear
SICS_baseline
ATTac

Expected Value of Perfect Prediction

Expected Value of Perfect Prediction

65
tatonnement, random

42

SimAA, random
40

tatonnement, expected
38

36

SimAA, expected

SimAA, exact
tatonnement, exact

60
55
50
harami

cuhk

45
kavayaH

SimAA, random

40
Walverine
35

34
180

190

200

210

220

30
180

230

Euclidean Distance

ATTac01
190

200

210

220

230

240

250

260

Euclidean Distance

Figure 1: EVPP and Euclidean Distance for the CE price prediction methods (tatonnement
1
; expected, random, and exact) and the TAC 2002 agents
and SimAA with  = 24
predictions in the 2002 finals (60 games). The plot on the left shows that SimAAs
predictions are better than tatonnements and that expecteds are better than
randoms. RoxyBot-06s method of hotel price prediction (SimAA, Random) is
plotted again on the right. Note the differences in scales between the two plots.

Since  is fixed, and tatonnement is not guaranteed to converge under this condition,
this outcome is not entirely surprising. What is interesting, though, is that SimAA expected
performs comparably to Walverine (see the right plot).5 This is interesting because SimAA
has fewer parameter settings than tatonnementonly a single  value as compared to
an initial  value together with a decay scheduleand moreover, we did not optimize
its parameter setting. Walverines parameter settings, on the other hand, were highly
optimized.
We interpret each prediction generated using randomly sampled clients as a sample
scenario, so that a set of such scenarios represents draws from a probability distribution
over CE prices. The corresponding vector of predicted prices that is evaluated is actually
the average of multiple (40) such predictions; that is, we evaluate an estimate of the mean
of this probability distribution. The predictions generated using sets of random clients are
not as good as the predictions with expected clients (see Figure 1 left), although with more
than 40 sets of random clients, the results might improve. Still, the predictions with random
clients comprise RoxyBot-06s stochastic model of hotel prices, which is key to its bidding
strategy. Moreover, using random clients helps RoxyBot-06 make better interim predictions
later in the game as we explain next.
4.2.3 Prediction Quality over Time: Interim Price Prediction
The graphs depicted in Figure 1 pertain to hotel price predictions made at the beginning of
the game, when all hotel auctions are open. In those CE computations, prices are initialized
to 0. As hotel auctions close, RoxyBot-06 updates the predicted prices of the hotel auctions
5. With the exception of the RoxyBot-06 data point (i.e., SimAA random), this plot was produced by the
Walverine team (Wellman et al., 2004).

522

fi22

140

tatonnement, expected clients
SimAA, expected clients
tatonnement, random clients
SimAA, random clients
tatonnement, random clients, with distribution
SimAA, random clients, with distribution

20
18
16

tatonnement, expected clients
SimAA, expected clients
tatonnement, random clients
SimAA, random clients
tatonnement, random clients, with distribution
SimAA, random clients, with distribution

120
Euclidean Distance per Hotel

Expected Value of Perfect Prediction per Hotel

RoxyBot-06

14
12
10
8
6
4

100
80
60
40
20

2
0

0
0

1

2

3

4

5

6

7

0

Minute

1

2

3

4

5

6

7

Minute

Figure 2: EVPP and Euclidean Distance in TAC 2006 finals (165 games) of the CE price
prediction methods with and without distribution as the game progresses. Distribution improves prediction quality.

that remain open. We experimented with two ways of constructing interim price predictions.
The first is to initialize and lower bound the prices in the hotel markets at their closing
(for closed auctions) or current ask (for open auctions) prices while computing competitive
equilibrium prices.6 The second differs in its treatment of closed auctions: we simulate a
process of distributing the goods in the closed auctions to the clients who want them most,
and then exclude the closed markets (i.e., fix prices at ) from further computations of
competitive equilibrium prices.
Regarding the second methodthe distribution methodwe determine how to distribute goods by computing competitive equilibrium prices again. As explained in Algorithm 4, all hotels (in both open and closed auctions) are distributed to random clients
by determining who is willing to pay the competitive equilibrium prices for what. It is not
immediately obvious how to distribute goods to expected clients; hence, we enhanced only
the prediction methods with random clients with distribution.
Figure 2, which depicts prediction quality over time, shows that the prediction methods
enhanced with distribution are better than the predictions obtained by merely initializing
the prices of closed hotel auctions at their closing prices. Hotels that close early tend to
sell for less than hotels that close late; hence, the prediction quality of any method that
makes decent initial predictions is bound to deteriorate if those predictions remain relatively
constant throughout the game.
4.2.4 Run Time
Table 2 shows the run times of the CE prediction methods on the TAC 2002 (60 games)
and TAC 2006 (165 games) finals data set at minute 0, as well as their run times during
6. At first blush, it may seem more sensible to fix the prices of closed hotels at their closing prices, rather
than merely lower bound them (i.e., allow them to increase). If some hotel closed at an artificially low
price, however, and if that price were not permitted to increase, then the predicted prices of the hotels
complementing the hotel in question would be artificially high.

523

fiGreenwald, Lee, & Naroditskiy

Algorithm 4 Distribute
1: for all hotel auctions h do
2:
initialize price to 0
3:
initialize supply to 16
4: end for
5: compute competitive equilibrium prices {Tatonnement or SimAA}
6: for all closed hotel auctions h do
7:
distribute units of h to those who demand them at the computed competitive equilibrium prices
8:
distribute any leftover units of h uniformly at random
9: end for
minutes 17 on the TAC 2006 finals data set. What the numbers in this table convey is
that SimAAs run time, even with distribution, is reasonable. For example, at minute 0,
SimAA sample takes on the order of 0.1 seconds. At minutes 1-7, this method without
distribution runs even faster. This speed increase occurs because CE prices are bounded
below by current ask prices and above by the maximum price a client is willing to pay for
a hotel, and current ask prices increase over time, correspondingly reducing the size of the
search space. SimAA sample with distribution at minutes 1-7 takes twice as long as SimAA
sample without distribution at minute 0 because of the time it takes to distribute goods, but
the run time is still only (roughly) 0.2 seconds. Our implementation of tatonnement runs
so slowly because we fixed  instead of optimizing the tradeoff between convergence rate
and accuracy, so the process did not converge, and instead ran for the maximum number
of iterations (10,000). In summary, SimAA is simpler than tatonnement to implement, yet
performs comparably to an optimized version of tatonnement (i.e., Walverine), and runs
sufficiently fast.

2002, minute 0
2006, minute 0
2006, average 17

Exp Tat
2213
2252
2248

Exp SimAA
507
508
347

Sam Tat
1345
1105
1138

Sam SimAA
157
130
97

Dist Tat

1111
2249

Dist SimAA

128
212

Table 2: Run times for the CE price prediction methods, in milliseconds. Experiments were
run on AMD Athlon(tm) 64 bit 3800+ dual core processors with 2M of RAM.

4.2.5 Summary
The simulation methods discussed in this sectionthe tatonnement process and simultaneous ascending auctionswere employed to predict hotel prices only. (In our simulations,
flight prices are fixed at their expected minima, and entertainment prices are fixed at 80.)
In principle, competitive equilibrium (CE) prices could serve as predictions in all TAC markets. However, CE prices are unlikely to be good predictors of flight prices, since flight
prices are determined exogenously. With regard to entertainment tickets, CE prices might
524

fiRoxyBot-06

have predictive power; however, incorporating entertainment tickets into the tatonnement
and SimAA calculations would have been expensive. (In our simulations, following Wellman et al., 2004, client utilities are simply augmented with fixed entertainment bonuses
that favor longer trips.) Nonetheless, in future work, it could be of interest to evaluate the
success of these or related methods in predicting CDA clearing prices.
Finally, we note that we refer to our methods of computing excess demand as clientbased because we compute the demands of each client on an individual basis. In contrast,
one could employ an agent-based method, whereby the demands of agents, not clients,
would be calculated. Determining an agents demands involves solving so-called completion, a deterministic (prices are known) optimization problem at the heart of RoxyBot-00s
architecture (Greenwald & Boyan, 2005). As TAC completion is NP-hard, the agent-based
method of predicting hotel prices is too expensive to be included in RoxyBot-06s inner
loop. In designing RoxyBot-06, we reasoned that an architecture based on a stochastic pricing model generated using the client-based method and randomly sampled clients would
outperform one based on a point estimate pricing model generated using the agent-based
method and some form of expected clients, but we did not verify our reasoning empirically.
4.3 Entertainment
During each bid interval, RoxyBot-06 predicts current and future buy and sell prices for tickets
to all entertainment events. These price predictions are optimistic: the agent assumes it
can buy (or sell) goods at the least (or most) expensive prices that it expects to see before
the end of the game. More specifically, each current price prediction is the best predicted
price during the current bid interval.
RoxyBot-06s estimates of entertainment ticket prices are based on historical data from
the past 40 games. To generate a scenario, a sample game is drawn at random from this
collection, and the sequences of entertainment bid, ask, and transaction prices are extracted.
Given such a history, for each auction a, let trade ai denote the price at which the last trade
before time i transacted; this value is initialized to 200 for buying and 0 for selling. In
addition, let bid ai denote the bid price at time i, and let ask ai denote the ask price at time
i.
RoxyBot-06 predicts the future buy price in auction a after time t as follows:
future buy at =

min

i=t+1,...,T

min{trade ai , ask ai }

(5)

In words, the future buy price at each time i = t + 1, . . . , T is the minimum of the ask price
after time i and the most recent trade price. The future buy price at time t is the minimum
across the future buy prices at all later times. The future sell price after time t is predicted
analogously:
(6)
future sell at = max max{trade ai , bid ai }
i=t+1,...,T

Arguably, RoxyBot-06s entertainment predictions are made in the simplest possible way:
past data are future predictions. It is likely one could improve upon this naive approach by
using a generalization technique capable of learning a distribution over these data, and
then sampling from the learned distribution.

525

fiGreenwald, Lee, & Naroditskiy

4.4 Summary
In this section, we described RoxyBot-06s price prediction methods. The key ideas, which
may be transferable if not beyond TAC, at least to other TAC agents, are as follows:
1. RoxyBot makes stochastic price predictions. It does so by generating a set of so-called
scenarios, where each scenario is a vector of future prices.
2. For each flight, RoxyBot uses Bayesian updating to predict its expected minimum price.
3. For hotels, RoxyBot-uses a method inspired by Walverines: it approximates competitive
equilibrium prices by simulating simultaneous ascending auctions, rather than the
usual tatonnement process.

5. Optimization
Next, we characterize RoxyBot-06s optimization routine. It is (i) stochastic, (ii) global, and
(iii) dynamic. It takes as input stochastic price predictions; it considers its flight, hotel,
and entertainment bidding decisions in unison; and it simultaneously reasons about bids to
be placed in both current and future stages of the game.
5.1 Abstract Auction Model
Recall that our treatment of bidding is decision-theoretic, rather than game-theoretic. In
particular, we focus on a single agents problem of optimizing its own bidding behavior, assuming the other agents strategies are fixed. In keeping with our basic agent architecture,
we further assume that the environment can be modeled in terms of the agents predictions
about market clearing prices. We introduce the term pseudo-auction to refer to a market
mechanism defined by these two assumptionsfixed other-agent behaviors and market information encapsulated by prices. The optimization problem that RoxyBot solves is one of
bidding in pseudo-auctions, not (true) auctions. In this section, we formally develop this
abstract auction model and relate it to TAC auctions; in the next, we define and propose
heuristics to solve various pseudo-auction bidding problems.
5.1.1 Basic Formalism
In this section, we formalize the basic concepts needed to precisely formulate bidding under uncertainty as an optimization problem, including: packagessets of goods, possibly
multiple units of each; a function that describes how much the agent values each package; pricelinesdata structures in which to store the prices of each unit of each good; and
bidspairs of vectors corresponding to buy and sell offers.
Packages Let G denote an ordered set of n distinct goods and let N  Nn represent the
multiset of these goods in the marketplace, with Ng denoting the number of units of each
good g  G. A package M is a collection of goods, that is, a submultiset of N . We write
M  N whenever Mg  Ng for all g  G.
It is instructive to interpret this notation in the TAC domain. The flights, hotel rooms,
and entertainment events up for auction in TAC comprise an ordered set of 28 distinct

526

fiRoxyBot-06

goods. In principle, the multiset of goods in the TAC marketplace is:
N TAC = h, . . . , , 16, . . . , 16, 8, . . . , 8i  N28
| {z } | {z } | {z }
8 flights

8 hotels

12 events

In practice, however, since each agent works to satisfy the preferences of only eight clients,
it suffices to consider the multiset of goods:
N TAC8 = h8 . . . , 8, 8, . . . , 8, 8, . . . , 8i  N TAC
| {z } | {z } | {z }
8 flights

8 hotels

12 events

A trip corresponds to a package, specifically some M  N TAC8 that satisfies the TAC
feasibility constraints.
Given A, B  N , we rely on the two basic operations,  and , defined as follows: for
all g  G,
(A  B)g  Ag + Bg
(A  B)g  Ag  Bg
For example, if G = {, , } and N = h1, 2, 3i, then A = h0, 1, 2i  N and B = h1, 1, 1i 
N . Moreover, (A  B) = 1, (A  B) = 2, and (A  B) = 3; and (A  B) = 1,
(A  B) = 0, and (A  B) = 1.
Value Let N denote the set of all submultisets of N : i.e., packages comprised of the goods
in N . We denote v : N  R a function that describes the value the bidding agent attributes
to each viable package.
In TAC, each agents objective is to compile packages for m = 8 individual clients. As
such, the agents value function takes special form. Each client c is characterized by its own
value function vc : N  R, and the agents value for a collection of packages is the sum of its
~ = (X1 , . . . , Xm ),
clients respective values for those packages: given a vector of packages X
~ =
v(X)

m
X

vc (Xc ).

(7)

c=1

N

Pricelines A buyer priceline for good g is a vector p~g  R+g , where the kth component,
pgk , stores the marginal cost to the agent of acquiring the kth unit of good g. For example,
if an agent currently holds four units of a good g, and if four additional units of g are
available at costs of $25, $40, $65, and $100, then the corresponding buyer priceline (a
vector of length 8) is given by p~g = h0, 0, 0, 0, 25, 40, 65, 100i. The leading zeros indicate
that the four goods the agent holds may be acquired at no cost.
We assume buyer pricelines are nondecreasing. Note that this assumption is WLOG,
since an optimizing agent buys cheaper goods before more expensive ones.
Given a set of buyer pricelines P = {~
pg | g  G}, we define costs additively, that is, the
cost of the goods in multiset Y  N is given by:
g,

Costg (Y, P ) =

Yg
X

pgk ,

X

Costg (Y, P ).

k=1

Cost(Y, P ) =

gG

527

(8)

fiGreenwald, Lee, & Naroditskiy

N

A seller priceline for good g is a vector ~g  R+g . Much like a buyer priceline, the kth
component of a seller priceline for g stores the marginal revenue that an agent could earn
from the kth unit it sells. For example, if the market demands four units of good g, which
can be sold at prices of $20, $15, $10, and $5, then the corresponding seller priceline is given
by ~g = h20, 15, 10, 5, 0, 0, 0, 0i. Analogously to buyer pricelines, the tail of zero revenues
indicates that the market demands only four of those units.
We assume seller pricelines are nonincreasing. Note that this assumption is WLOG,
since an optimizing agent sells more expensive goods before cheaper ones.
Given a set of seller pricelines  = {~g | g  G}, we define revenue additively, that is,
the revenue associated with multiset Z  N is given by:
g,

Revenueg (Z, ) =

Zg
X

gk ,

X

Revenueg (Z, ).

(9)

k=1

Revenue(Z, ) =

(10)

gG

If a priceline is constant, we say that prices are linear. We refer to the constant value
as a unit price. With linear prices, the cost of acquiring k units of good g is k times the
unit price of good g.
Bids An agent submits a bid  expressing offers to buy or sell various units of the goods
in the marketplace. We divide  into two components h~b, ~ai, where for each good g the bid
consists of a buy offer, ~bg = hbg1 , . . . , bgNg i, and a sell offer, ~ag = hag1 , . . . , agNg i. The bid
price bgk  R+ (resp. agk  R+ ) represents an offer to buy (sell) the kth unit of good g at
that price.
By definition, the agent cannot buy (sell) the kth unit unless it also buys (sells) units
1, . . . , k  1. To accommodate this fact, we impose the following constraint: Buy offers must
be nonincreasing in k, and sell offers nondecreasing. In addition, an agent may not offer to
sell a good for less than the price at which it is willing to buy that good: i.e., bg1 < ag1 .
Otherwise, it would simultaneously buy and sell good g. We refer to these restrictions as
bid monotonicity constraints.
5.1.2 Pseudo-Auction Rules
Equipped with this formalism, we can specify the rules that govern pseudo-auctions. As
in a true auction, the outcome of a pseudo-auction dictates the quantity of each good to
exchange, and at what prices, conditional on the agents bid. The quantity issue is resolved
by the winner determination rule whereas the price issue is resolved by the payment rule.
Definition 5.1 [Pseudo-Auction Winner Determination Rule] Given buyer and seller pricelines P and , and bid  = h~b, ~ai, the agent buys the multiset of goods Buy(, P ) and sells
the multiset of goods Sell(, ), where
Buyg (, P ) = max k such that bgk  pgk
k

Sellg (, ) = max k such that agk  gk
k

528

fiRoxyBot-06

Note that the monotonicity restrictions on bids ensure that the agents offer is better than
or equal to the price for every unit it exchanges, and that the agent does not simultaneously
buy and sell any good.
There are at least two alternative payment rules an agent may face. In a first-price
pseudo-auction, the agent pays its bid price (for buy offers, or is paid its bid price for sell
offers) for each good it wins. In a second-price pseudo-auction, the agent pays (or is paid) the
prevailing prices, as specified by the realized buyer and seller pricelines. This terminology
derives by analogy from the standard first- and second-price sealed bid auctions (Krishna,
2002; Vickrey, 1961). In these mechanisms, the high bidder for a single item pays its bid (the
first price), or the highest losing bid (the second price), respectively. The salient property
is that in first-price pseudo-auctions, the price is set by the bid of the winner, whereas in
second-price pseudo-auctions an agents bid price determines whether or not it wins but
not the price it pays.
In this paper, we focus on the second-price model. That is, our basic problem definitions
presume second-price auctions; however, our bidding heuristics are not tailored to this
case. As in true auctions, adopting the second-price model in pseudo-auctions simplifies the
problem for the bidder. It also provides a reasonable approximation to the situation faced
by TAC agents, as we now argue:
 In TAC entertainment auctions, agents submit bids (i.e., buy and sell offers) of the
form specified above. If we interpret an agents buyer and seller pricelines as the
current order book (not including the agents own bid), then the agents immediate winnings are as determined by the winner determination rule, and payments are
according to the second-price rule (i.e., the order-book prices prevail).
 In TAC hotel auctions, only buy bids are allowed. Assuming once again an order
book that reflects all outstanding bids other than the agents own, an accurate buyer
priceline would indicate that the agent can win k units of a good if it paysfor all
k unitsa price just above the (17  k)th existing (other-agent) offer. The actual
price it pays will be that of the 16th-highest unit offer (including its own offer). Since
the agents own bid may affect the price,7 this situation lies between the first- and
second-price characterizations of pseudo-auctions described above.
 In TAC flight auctions, agents may buy any number of units at the posted price. The
situation at any given time is modeled exactly by the second-price pseudo-auction
abstraction.
5.2 Bidding Problems
We are now ready to discuss the optimization module repeatedly employed by RoxyBot-06
within its bidding cycle to construct its bids. The key bidding decisions are: what goods to
bid on, at what price, and when?
7. It can do so in two ways. First, the agent may submit the 16th-highest unit offer, in which case it sets
the price. Second, when it bids for multiple units, the number it wins determines the price-setting unit,
thus affecting the price for all winning units. Note that this second effect would be present even if the
auction cleared at the 17th-highest price.

529

fiGreenwald, Lee, & Naroditskiy

Although RoxyBot technically faces an n-stage stochastic optimization problem, it solves
this problem by collapsing those n stages into only two relevant stages, current and
future, necessitating only one stochastic model of future prices (current prices are known).
This simplification is achieved by ignoring the potentially useful information that hotel
auctions close one by one in a random, unspecified order, and instead operating (like most
TAC agents) under the assumption that all hotel auctions close at the end of the current
stage. Hence, there is only one model of hotel prices: a stochastic model of future prices.
Moreover, the only pressing decisions regarding hotels are what goods to bid on now and
at what price. There is no need to reason about the timing of hotel bid placement.
In contrast, since flight and entertainment auctions clear continuously, a trading agent
should reason about the relevant tradeoffs in timing its placement of bids on these goods.
Still, under the assumption that all hotel auctions close at the end of the current stage,
in future stages, hotel prices, and hence hotel winnings, are known, so the only remaining
decisions are what flight and entertainment tickets to buy. A rational agent will time its
bids in these markets to capitalize on the best prices. (The best prices are the minima for
buying and the maxima for selling.) Hence, it suffices for an agents model of future prices
in these markets to predict only the best prices (conditioned on current prices). That is, it
suffices to consider only one stochastic pricing model. No further information is necessary.
Having established that it suffices for RoxyBot to pose and solve a two-stage, rather than
an n-stage, stochastic optimization problem, we now proceed to define an abstract series
of such problems that is designed to capture the essence of bidding under uncertainty in
TAC-like hybrid markets that incorporate aspects of simultaneous and sequential, one-shot
and continuously-clearing, auctions. More specifically, we formulate these problems as twostage stochastic programs with integer recourse (see the book by Birge & Louveaux, 1997,
for an introduction to stochastic programming).
In a two-stage stochastic program, there are two decision-making stages, and hence two
sets of variables: first- and second-stage variables. The objective is to maximize the sum of
the first-stage objectives (which depend only on the first-stage variables) and the expected
value of the ensuing second-stage objectives (which can depend on both the first- and secondstage variables). The objective value in the second stage is called the recourse value, and if
any of the second-stage variables are integer-valued, then the stochastic program is said to
have integer recourse.
At a high-level, the bidding problem can be formulated as a two-stage stochastic program
as follows: in the first stage, when current prices are known but future prices are uncertain,
bids are selected; in the second stage, all uncertainty is resolved, and goods are exchanged.
The objective is to maximize the expected value of the second-stage objective, namely the
sum of the inherent value of final holdings and any profits earned, less any first-stage costs.
Since the second stage involves integer-valued decisions (the bidder decides what goods to
buy and sell at known prices), the bidding problem is one with integer recourse.
In this section, we formulate a series of bidding problems as two-stage stochastic programs with integer recourse, each one tailored to a different type of auction mechanism,
illustrating a different type of bidding decision. The mechanisms we study, inspired by
TAC, are one-shot and continuously-clearing variants of second-price pseudo-auctions. In
the former, bids can only be placed in the first stage; in the latter, there is an opportunity

530

fiRoxyBot-06

for recourse. Ultimately, we combine all decision problems into one unified problem that
captures what we mean by bidding under uncertainty.
In our formal problem statements, we rely on the following notation:
 Variables:
 Q1 is a multiset of goods to buy now
 Q2 is a multiset of goods to buy later
 R1 is a multiset of goods to sell now
 R2 is a multiset of goods to sell later
 Constants:
 P 1 is a set of current buyer pricelines
 P 2 is a set of future buyer pricelines
 1 is a set of current seller pricelines
 2 is a set of future seller pricelines
Note that P 1 and 1 are always known, whereas P 2 and 2 are uncertain in the first stage
but their uncertainty is resolved in the second stage.
Flight Bidding Problem An agents task in bidding in flight auctions is to decide how
many flights to buy now at current prices and later at the lowest future prices, given (known)
current prices and a stochastic model of future prices. Although in TAC all units of each
flight sell for the same price at any one time, we state the flight bidding problem more
generally: we allow for different prices for different units of the same flight.
Definition 5.2 [Continuously-Clearing, Buying] Given a set of current buyer pricelines P 1
and a probability distribution f over future buyer pricelines P 2 ,
FLT(f ) = max
EP 2 f
1
n
Q Z




max
v(Q1  Q2 )  Cost(Q1 , P 1 ) + Cost(Q1  Q2 , P 2 )  Cost(Q1 , P 2 )
2
n

Q Z



(11)

Note that there are two cost terms referring to future pricelines (Cost(, P 2 )). The first of
these terms adds the total cost of the goods bought in the first and second stages. The
second term subtracts the cost of the goods bought in just the first stage. This construction
ensures that, if an agent buys k units of a good now, any later purchases of that good incur
the charges of units (k + 1, k + 2, ...) in the goods future priceline.
Entertainment Bidding Problem Abstractly, the entertainment buying problem is the
same as the flight bidding problem. An agent must decide how many entertainment tickets
to buy now at current prices and later at the lowest future prices. The entertainment selling
problem is the opposite of this buying problem. An agent must decide how many tickets to
sell now at current prices and later at the highest future prices.

531

fiGreenwald, Lee, & Naroditskiy

Definition 5.3 [Continuously-Clearing, Buying and Selling] Given a set of current buyer
and seller pricelines (P, )1 and a probability distribution f over future buyer and seller
pricelines (P, )2 ,

ENT(f ) = max E(P,)2 f
max v((Q1  Q2 )  (R1  R2 ))
Q1 ,R1 Zn
Q2 ,R2 Zn

 Cost(Q1 , P 1 ) + Cost(Q1  Q2 , P 2 )  Cost(Q1 , P 2 )

+ Revenue(R1 , 1 ) + Revenue(R1  R2 , 2 )  Revenue(R1 , 2 )
(12)

subject to Q1  R1 and Q1  Q2  R1  R2 , for all (P, )2 .

The constraints ensure that an agent does not sell more units of any good than it buys.
Hotel Bidding Problem Hotel auctions close at fixed times, but in an unknown order.
Hence, during each iteration of an agents bidding cycle, one-shot auctions approximate
these auctions well. Unlike in the continuous setup, where decisions are made in both the
first and second stages, in the one-shot setup, bids can only be placed in the first stage; in
the second stage, winnings are determined and evaluated.
Definition 5.4 [One-Shot, Buying] Given a probability distribution f over future buyer
pricelines P 2 ,


(13)
HOT(f ) = max EP 2 f v(Buy( 1 , P 2 ))  Cost(Buy( 1 , P 2 ), P 2 )
 1 =h~b,0i

Hotel Bidding Problem, with Selling Although it is not possible for agents to sell
TAC hotel auctions, one could imagine an analogous auction setup in which it were possible
to sell goods as well as buy them.
Definition 5.5 [One-Shot, Buying and Selling] Given a probability distribution f over
future buyer and seller pricelines (P, )2 ,


max E(P,)2 f v(Buy( 1 , P 2 )  Sell( 1 , 2 ))  Cost(Buy( 1 , P 2 ), P 2 ) + Revenue(Sell( 1 , 2 ), 2 )

 1 =h~b,~
ai

(14)

1

2

1

2

2

subject to Buy( , P )  Sell( ,  ), for all (P, ) .

Bidding Problem Finally, we present (a slight generalization of) the TAC bidding problem by combining the four previous stochastic optimization problems into one. This abstract
problem models bidding to buy and sell goods both via continuously-clearing and one-shot
second-price pseudo-auctions, as follows:
Definition 5.6 [Bidding Under Uncertainty] Given a set of current buyer and seller pricelines (P, )1 and a probability distribution f over future buyer and seller pricelines (P, )2 ,
BID(f ) =
max

Q1 ,R1 Zn , 1 =h~b,~
ai

E(P,)2 f



max

Q2 ,R2 Zn

v((Q1  Q2 )  (R1  R2 )  Buy( 1 , P 2 )  Sell( 1 , P 2 ))


 Cost(Q1 , P 1 ) + Cost(Q1  Q2 , P 2 )  Cost(Q1 , P 2 ) + Cost(Buy( 1 , P 2 ), P 2 )


+ Revenue(R1 , 1 ) + Revenue(R1  R2 , 2 )  Revenue(R1 , 2 ) + Revenue(Sell( 1 , 2 ), 2 )
(15)
532

fiRoxyBot-06

subject to Q1  R1 and Q1  Q2  R1  R2 and Buy( 1 , P 2 )  Sell( 1 , 2 ), for all (P, )2 .

Once again, this bidding problem is (i) stochastic: it takes as input a stochastic model
of future prices; (ii) global: it seamlessly integrates flight, hotel, and entertainment bidding
decisions; and (iii) dynamic: it facilitates simultaneous reasoning about current and future
stages of the game.
Next, we describe various heuristic approaches to solving the problem of bidding under
uncertainty.
5.3 Bidding Heuristics
In this section, we discuss two heuristic solutions to the bidding problem: specifically,
the expected value method (EVM), an approach that collapses stochastic information, and
sample average approximation (SAA), an approach that exploits stochastic information and
characterizes RoxyBot-06.
5.3.1 Expected Value Method
The expected value method (Birge & Louveaux, 1997) is a standard way of approximating
the solution to a stochastic optimization problem. First, the given distribution is collapsed
into a point estimate (e.g., the mean); then, a solution to the corresponding deterministic optimization problem is output as an approximate solution to the original stochastic
optimization problem. Applying this idea to the problem of bidding under uncertainty
yields:
Definition 5.7 [Expected Value Method] Given a probability distribution f over buyer
and seller pricelines, with expected values P 2 and 2 , respectively,
BID EVM(P 2 , 2 ) =
max

Q1 ,R1 Zn , 1 =h~b,~
ai,Q2 ,R2 Zn

v((Q1  Q2 )  (R1  R2 )  (Buy( 1 , P 2 )  Sell( 1 , P 2 ))


 Cost(Q1 , P 1 ) + Cost(Q1  Q2 , P 2 )  Cost(Q1 , P 2 ) + Cost(Buy( 1 , P 2 ), P 2 )


+ Revenue(R1 , 1 ) + Revenue(R1  R2 , 2 )  Revenue(R1 , 2 ) + Revenue(Sell( 1 , 2 ), 2 )
(16)
subject to Q1  R1 and Q1  Q2  R1  R2 .

In practice, without full knowledge of the distribution f , we cannot implement the
expected value method; in particular, we cannot compute P 2 or 2 so we cannot solve
BID EVM(P 2 , 2 ) exactly. We can, however, solve an approximation of this problem in
which the expected buyer and seller pricelines P 2 and 2 are replaced by an average scenario
(P 2 , 2 ) (i.e., average buyer and seller pricelines), defined as follows:
P 2 =

S
1X 2
Pi ,
S

2 =

i=1

S
1X 2
i .
S
i=1

533

fiGreenwald, Lee, & Naroditskiy

Algorithm 5 EVM(G, N, f, S)
1: sample S scenarios (P, )21 , . . . , (P, )2S  f
P

PS
S
2
2,
2:   BID EVM

P
i=1 i
i=1 i
3: return 
5.3.2 Sample Average Approximation
Like the expected value method, sample average approximation is an intuitive way of approximating the solution to a stochastic optimization problem. The idea is simple: (i) generate
a set of sample scenarios, and (ii) solve an approximation of the problem that incorporates
only the sample scenarios. Applying the SAA heuristic (see Algorithm 6) involves solving
the following approximation of the bidding problem:
Definition 5.8 [Sample Average Approximation] Given a set of S scenarios,
(P, )21 , . . . , (P, )2S  f ,
BID SAA((P, )21 , . . . , (P, )2S ) =
max

S
X

max

v((Q1  Q2 )  (R1  R2 )  (Buy( 1 , Pi2 )  Sell( 1 , Pi2 ))

2
2
n
Q1 ,R1 Zn , 1 =h~b,~
ai i=1 Q ,R Z
1
1
1


 Cost(Q , P ) + Cost(Q  Q2 , Pi2 )  Cost(Q1 , Pi2 ) + Cost(Buy( 1 , Pi2 ), Pi2 )


+ Revenue(R1 , 1 ) + Revenue(R1  R2 , 2i )  Revenue(R1 , 2i ) + Revenue(Sell( 1 , 2i ), 2i )
(17)
subject to Q1  R1 and Q1  Q2  R1  R2 .

Algorithm 6 SAA(G, N, f, S)
1: sample S scenarios (P, )21 , . . . , (P, )2S  f
2:   BID SAA((P, )21 , . . . , (P, )2S )
3: return 
Using the theory of large deviations, Ahmed and Shapiro (2002) establish the following
result: as S  , the probability that an optimal solution to the sample average approximation of a stochastic program with integer recourse is an optimal solution to the original
stochastic optimization problem approaches 1 exponentially fast. Given hard time and space
constraints, however, it is not always possible to sample sufficiently many scenarios to infer
any reasonable guarantees about the quality of a solution to a sample average approximation. Hence, we propose a modified SAA heuristic, in which SAA is fed some tailor-made
important scenarios, and we apply this idea to the bidding problem.
5.3.3 Modified Sample Average Approximation
The bids that SAA places are sample prices that appear in its scenarios. SAA never bids
higher on any good than its highest sampled price, because as far as it knows, bidding that
price is enough to win that good in all scenarios. However, there is some chance that the
534

fiRoxyBot-06

highest sampled price falls below the clearing price. Let us compute this probability in the
case of a single-unit auction, or a uniform-price multi-unit auction: i.e., one in which all
units of the good being auctioned off clear at the same price.
Let F denote the cumulative distribution function over the predicted prices, let f denote
the corresponding density function, and let G denote the cumulative distribution function
over the clearing prices. Using this notation, the term 1  G(x) is the probability the
clearing price is greater than x. Further, let X be a random variable that represents
the highest value among S sample price predictions. Then P (X  x) = F (x)S is the
probability that all S samples (and hence the highest among them) are less than x; and
P (X = x) = (F (x)S ) = S(F (x))S1 f (x) is the probability that the highest value among
the S samples equals x. Putting these two terms togethernamely, the probability the
highest sample price prediction is exactly x, and the probability the clearing price is greater
than xwe can express the probability the highest of SAAs sample price predictions is less
than the clearing price as follows:
Z 

S(F (x))S1 f (x)(1  G(x))dx
(18)



Assuming perfect prediction (so that G = F ), this complex expression simplies as follows:
Z 
S(F (x))S1 f (x)(1  F (x))dx

Z 
Z 
S1
(F (x))S f (x)dx
(F (x))
f (x)dx  S
= S






(F (x))S 
(F (x))S+1 
= S
S
S
S+1


1
=
S+1
Hence, the probability that all SAAs sample price predictions are less than the clearing
price is 1/(S + 1). In particular, assuming perfect prediction and that the clearing prices in
the TAC hotel auctions are independent, the probability that an SAA agent with 49 scenarios
bidding in TAC Travel has any chance of winning all eight hotels (i.e., the probability that
a sample price in at least one of its scenarios is greater than the clearing price) is only
8

1
= 0.988  0.85.
1  49+1
To remedy this situation, we designed and implemented a simple variant of SAA in
RoxyBot-06. The SAA* heuristic (see Algorithm 7) is a close cousin of SAA, the only difference
arising in their respective scenario sets.
P Whereas SAA samples S scenarios, SAA* samples
only S  |N | scenarios, where |N | = g Ng . SAA* creates an additional |N | scenarios as
follows: for each unit k of each good g  G, it sets the price of the kth unit of good g to the
upper limit of its range of possible prices and, after conditioning on this price setting, it sets
the prices of the other goods to their mean values. Next, we describe experiments with a
test suite of bidding heuristics, including SAA and SAA*, in a controlled testing environment.

535

fiGreenwald, Lee, & Naroditskiy

Algorithm 7 SAA(G, N, f, S)
Require: S  |N |
1: hard-code |N | scenarios (P, )21 , . . . , (P, )2|N |
2: sample S  |N | scenarios (P, )2|N |+1 , . . . , (P, )2S  f
3:   BID SAA((P, )21 , . . . , (P, )2S )
4: return 
Agent
SMU
AMU

Predictions
Average scenario
S scenarios

TMU
BE
TMU*

Average scenario
S scenarios
Average scenario

BE*

S scenarios

Bids
Marginal utilities
Calculates marginal utilities in each scenario
Bids average marginal utilities across scenarios
Marginal utilities
Best of S TMU solutions
Marginal utilities, assuming only goods
in a target set are available
Best of S TMU*solutions

On
All goods
All goods
Goods in a target set
Goods in a target set
Goods in a target set
Goods in a target set

Table 3: Marginal-utility-based agents. The marginal utility of a good is defined as the
incremental utility that can be achieved by winning that good, relative to the
utility of the set of goods already held.

5.4 Summary
In this section, we developed a series of bidding problems, and heuristics solutions to those
problems, that captures the essence of bidding in the one-shot and continuously-clearing
auctions that characterize TAC. The bulk of our presentation was deliberately abstract, so
as to suggest that our problems and their solutions are applicable well beyond the realm of
TAC: e.g., to bidding for interdependent goods in separate eBay auctions. Still, it remains
to validate our approach in other application domains.

6. Experiments
We close this paper with two sets of experimental results, the first in a controlled testing
environment, and the second the results from the final round of the 2006 TAC Travel competition. The combined strategy of hotel price prediction via SimAA and bid optimization
via SAA emerged victorious in both settings.
6.1 Controlled Experiments
To some extent at least, our approach to bidding has been validated by the success of
RoxyBot-06 in TAC-06. Nonetheless, we ran simulations in a controlled testing environment
to further validate our approach. These results are reported by Lee (2007) and Greenwald et
al. (2008), but we summarize them here as well.

536

fiRoxyBot-06

We built a test suite of agents, all of which predict using RoxyBot-06s SimAA random
mechanism with distribution. The agents differ in their bidding strategies; the possibilities
include SAA,8 SAA*, and the six marginal-utility-based heuristics studied by Wellman et
al. (2007), and summarized in Table 3.
Our experiments were conducted in a TAC Travel-like setting, modified to remove any
aspects of the game that would obscure a controlled study of bidding. Specifically, we
eliminated flight and entertainment trading, and endowed all agents with eight flights in
and eight flights out on each day. Further, we assumed all hotels closed after one round
of bidding (i.e., hotel auctions are one-shot, so that the ensuing bid optimization problem
adheres to Definition 5.4).
We designed two sets of experiments: one decision-theoretic and one game-theoretic. In
the former, hotel clearing prices are the outcome of a simulation of simultaneous ascending
auctions, but depend on the actual clients in each game, not a random sampling. (Our
simulator is more informed than the individual agents.) In the latter, hotel clearing prices
are determined by the bids the agents submit using the same mechanism as in TAC Travel:
the clearing price is the 16th highest bid (or zero, if fewer than 16 bids are submitted).
We first ran experiments with 8 agents per game, but found that hotel prices were
often zero: i.e., there was insufficient competition. We then changed the setup to include a
random number of agents drawn from a binomial distribution with n = 32 and p = 0.5, with
the requisite number of agents sampled uniformly with replacement from the set of possible
agents. The agents first sample the number of competitors from the binomial distribution,
and then generate scenarios assuming the sampled number of competitors.
Because of the game-theoretic nature of TAC, an individual agents performance can
depend heavily on the other agents included in the agent pool. In our experiments, we
attempted to mitigate any artificial effects of the specific agents we chose to include in
our pool by sampling agents from the pool to play each game, with replacement. Thus,
an agents average score from the games is a measure of the agents performance against
various combinations of opponents.
In Figures 3(a) and 3(b), we plot the mean scores obtained by each agent type in each
setting, along with 95% confidence intervals. These averages were computed based on 1000
independent observations, obtained by playing 1000 games. Scores were averaged across
agent types in each game to account for any game dependencies. SAAB and SAAT9 are
the best performing agents in the game-theoretic experiments and among the best in the
decision-theoretic setting.
6.2 TAC 2006 Competition Results
Table 4 lists the agents entered in TAC-06 and Table 5 summarizes the outcome. The
TAC-06 finals comprised 165 games over three days, with the 80 games on the last day
weighted 1.5 times as much as the 85 over the first two days. On the first day of the finals,
RoxyBot finished third, behind Mertacor and Walverinethe top scorers in 2005. As it happens,
RoxyBots optimization routine, which was designed for stochastic hotel and entertainment
8. The particular implementation details explaining how RoxyBot-06 applied SAA in the TAC domain are
relegated to Appendix A.
9. SAAB is SAA, and SAAT is a slight variant of SAA*. See the paper by Greenwald et al. (2008) for
details.

537

fiGreenwald, Lee, & Naroditskiy

1

0.85

0.95
Score (thousands)

Score (thousands)

0.8
0.75
0.7
0.65
0.6

0.9
0.85
0.8
0.75
0.7

0.55
0.65
0.5
0.6
SAAT SAAB

TMU

TMU*
BE
Agent

BE*

AMU

SMU

(a) Decision-theoretic setting

SAAT SAAB

TMU

TMU*
BE
Agent

BE*

AMU

SMU

(b) Game-theoretic setting

Figure 3: Mean scores and confidence intervals.

price predictions, was accidentally fed deterministic predictions (i.e., point price estimates)
for entertainment. Moreover, these predictions were fixed, rather than adapted based on
recent game history.
On days 2 and 3, RoxyBot ran properly, basing its bidding in all auctions on stochastic
information. Moreover, the agent was upgraded after day 1 to bid on flights not just once,
but twice, during each minute. This enabled the agent to delay its bidding somewhat at
the end of a game for flights whose prices are decreasing. No doubt this minor modification
enabled RoxyBot to emerge victorious in 2006, edging out Walverine by a whisker, below the
integer precision reported in Table 5. The actual margin was 0.22a mere 22 parts in
400,000. Adjusting for control variates (Ross, 2002) spreads the top two finishers a bit
further.10
Agent
006
kin agent
L-Agent
Mertacor
RoxyBot
UTTA
Walverine
WhiteDolphin

Affiliation
Swedish Inst Comp Sci
U Macau
Carnegie Mellon U
Aristotle U Thessaloniki
Brown U
U Tehran
U Michigan
U Southampton

Reference
Aurell et al., 2002
Sardinha et al., 2005
Toulis et al., 2006; Kehagias et al., 2006
Greenwald et al., 2003, 2004, 2005; Lee et al., 2007
Cheng et al., 2005; Wellman et al., 2005
He & Jennings, 2002; Vetsikas & Selman, 2002

Table 4: TAC-06 participants.

10. Kevin Lochner computed these adjustment factors using the method described by Wellman et al. (2007,
ch. 8).

538

fiRoxyBot-06

Agent
RoxyBot
Walverine
WhiteDolphin
006
Mertacor
L-Agent
kin agent
UTTA

Finals
4032
4032
3936
3902
3880
3860
3725
2680

Adjustment Factor
5
17
2
27
16
7
0
14

Table 5: TAC-06 final scores, with adjustment factors based on control variates.

Mean scores, utilities, and costs (with 95% confidence intervals) for the last day of the
TAC-06 finals (80 games) are plotted in Figure 4 and detailed statistics are tabulated in
Table 6. There is no single metric such as low hotel or flight costs that is responsible for
RoxyBots success. Rather its success derives from the right balance of contradictory goals.
In particular, RoxyBot incurs high hotel and mid-range flight costs while achieving mid-range
trip penalty and high event profit.11
Let us compare RoxyBot with two closest rivals: Walverine and WhiteDolphin. Comparing to
Walverine first, Walverine bids lower prices (by 55) on fewer hotels (49 less), yet wins more (0.8)
and wastes less (0.42). It would appear that Walverines hotel bidding strategy outperforms
RoxyBots, except that RoxyBot earns a higher hotel bonus (15 more). RoxyBot also gains an
advantage by spending 40 less on flights and earning 24 more in total entertainment profit.
A very different competition takes place between RoxyBot and WhiteDolphin. WhiteDolphin
bids lower prices (120 less) on more hotels (by 52) than RoxyBot. RoxyBot spends much more
(220) on hotels than WhiteDolphin but makes up for it by earning a higher hotel bonus (by
96) and a lower trip penalty (by 153). It seems that WhiteDolphins strategy is to minimize
costs even if that means sacrificing utility.
6.3 Summary
As already noted, TAC Travel bidding, viewed as an optimization problem, is an n-stage
decision problem. We solve this n-stage decision problem as a sequence of 2-stage decision
problems. The controlled experiments reported in this section establish that our bidding
strategy, SAA, is the best in our test suite in the setting for which it was designed, with
only 2 stages. The TAC competition results establish that this strategy is also effective in
an n-stage setting.

7. Collective Behavior
The hotel price prediction techniques described in Section 4.2 are designed to compute (or at
least approximate) competitive equilibrium prices without full knowledge of the client pop11. An agent suffers trip penalties to the extent that it assigns its clients packages that differ from their
preferred.

539

fiGreenwald, Lee, & Naroditskiy

# of Hotel Bids
Average of Hotel Bids
# of Hotels Won
Hotel Costs
# of Unused Hotels
Hotel Bonus
Trip Penalty
Flight Costs
Event Profits
Event Bonus
Total Event Profits
Average Utility
Average Cost
Average Score

Rox
130
170
15.99
1102
2.24
613
296
4615
110
1470
1580
9787
5608
4179

Wal
81
115
16.79
1065
1.82
598
281
4655
26
1530
1556
9847
5693
4154

Whi
182
50
23.21
882
9.48
517
449
4592
6
1529
1535
9597
5468
4130

SIC
33
513
13.68
1031
0.49
617
340
4729
-6
1498
1492
9775
5765
4010

Mer
94
147
18.44
902
4.86
590
380
4834
123
1369
1492
9579
5628
3951

L-A
58
88
14.89
987
1.89
592
388
4525
-93
1399
1306
9604
5605
3999

kin
15
356
15.05
1185
0.00
601
145
4867
-162
1619
1457
10075
6213
3862

UTT
24
498
9.39
786
0.48
424
213
3199
-4
996
992
6607
3989
2618

Table 6: 2006 Finals, Last day. Tabulated Statistics. We omit the first two days because
agents can vary across days, but cannot vary within. Presumably, the entries on
the last day are the teams preferred versions of the agents.
2006 Finals, Last Day

2006 Finals, Last Day

2006 Finals, Last Day

4.5

6.5

10

6

3.5
3

9
8
7

2.5

6

2

5

Rox Wal Whi SIC Mer LA kin UTT
Agent

Cost (thousands)

Utility (thousands)

Score (thousands)

4

5.5
5
4.5
4
3.5

Rox Wal Whi SIC Mer LA kin UTT
Agent

3

Rox Wal Whi SIC Mer LA kin UTT
Agent

Figure 4: 2006 Finals, Last day. Mean scores, utilities, and costs, and 95% confidence
intervals.

ulation. In this section, we assume this knowledge and view the output of the tatonnement
and SimAA calculations not as predictions but as ground truth. We compare the actual
prices in the final games to this ground truth in respective years since 2002 to determine
whether TAC market prices resemble CE prices. What we find is depicted in Figure 5.
Because of the nature of our methods, these calculations pertain to hotel prices only.
The results are highly correlated on both metrics (Euclidean distance and EVPP). We
observe that the accuracy of CE price calculations has varied from year to year. 2003
was the year in which TAC Supply Chain Management (SCM) was introduced. Many
participants diverted their attention away from Travel towards SCM that year, perhaps
leading to degraded performance in Travel. Things seem to improve in 2004 and 2005. We

540

fiRoxyBot-06

cannot explain the setback in 2006, except by noting that performance is highly dependent
on the particular agent pool, and in 2006 there were fewer agents in that pool.
260

45

tatonnement, exact
simAA, exact
Expected Value of Perfect Prediction

240

Euclidean Distance

220
200
180
160
140
120
100
2002

2003

2004
Year

2005

2006

tatonnement, exact
simAA, expact

40

35

30

25

20
2002

2003

2004
Year

2005

2006

Figure 5: A comparison of the actual (hotel) prices to the output of competitive equilibrium
price calculations in the final games since 2002. The label exact means: full
knowledge of the client population.

8. Conclusion
The foremost aim of trading agent research is to develop a body of techniques for effective
design and analysis of trading agents. Contributions to trading agent design include the
invention of trading strategies, together with models and algorithms for realizing their
computation and methods to measure and evaluate the performance of agents characterized
by those strategies. Researchers seek both specific solutions to particular trading problems
and general principles to guide the development of trading agents across market scenarios.
This paper purports to contribute to this research agenda. We described the design and
implementation of RoxyBot-06, an able trading agent as demonstrated by its performance in
TAC-06.
Although automated trading in electronic markets has not yet fully taken hold, the
trend is well underway. Through TAC, the trading agent community is demonstrating the
potential for autonomous bidders to make pivotal trading decisions in a most effective way.
Such agents offer the potential to accelerate the automation of trading more broadly, and
thus shape the future of commerce.

Acknowledgments
This paper extends the work of Lee et al. (2007). The material in Section 5.1 is based on
the book by Wellman et al. (2007). We are grateful to several anonymous reviewers whose
constructive criticisms enhanced the quality of this work. This research was supported by
NSF Career Grant #IIS-0133689.

541

fiGreenwald, Lee, & Naroditskiy

Appendix A. TAC Bidding Problem: SAA
The problem of bidding in the simultaneous auctions that characterize TAC can be formulated as a two-stage stochastic program. In this appendix, we present the implementation
details of the integer linear program (ILP) encoded in RoxyBot-06 that approximates an
optimal solution to this stochastic program.12
We formulate this ILP assuming current prices are known, and future prices are uncertain in the first stage but revealed in the second stage. Note that whenever prices are
known, it suffices for an agent to make decisions about the quantity of each good to buy,
rather than about bid amounts, since choosing to bid an amount that is greater than or
equal to the price of a good is equivalent to a decision to buy that good.
Unlike in the main body of the paper, this ILP formulation of bidding in TAC assumes
linear prices. Table 7 lists the price constants and decision variables for each auction type.
For hotels, the only decisions pertain to buy offers; for flights, the agent decides how many
tickets to buy now and how many to buy later; for entertainment events, the agent chooses
sell quantities as well as buy quantities.
Hotels
bid now

Price
Yas

Flights and Events
buy now
buy later
Events
sell now
sell later

Variable (bid)
apq
Price
Ma
Yas

Price
Na
Zas

Variable (qty)
a
as

Variable (qty)
a
as

Table 7: Auction types and associated price constants and decision variables.

A.1 Index Sets
a  A indexes the set of goods, or auctions.
af  Af indexes the set of flight auctions.
ah  Ah indexes the set of hotel auctions.
ae  Ae indexes the set of event auctions.
c  C indexes the set of clients.
p  P indexes the set of prices.
12. The precise formulation of RoxyBot-06s bidding ILP appears in the paper by Lee et al. (2007). The
formulation here is slightly simplified, but we expect it would perform comparably in TAC. The key
differences are in flight and entertainment bidding.

542

fiRoxyBot-06

q  Q indexes the set of quantities
(i.e., the units of each good in each auction).
s  S indexes the set of scenarios.
t  T indexes the set of trips.
A.2 Constants
Gat indicates the quantity of good a required to complete trip t.
Ma indicates the current buy price of af , ae .
Na indicates the current sell price of ae .
Yas indicates the future buy price of af , ah , ae in scenario s.
Zas indicates the future sell price of ae in scenario s.
Ha indicates the hypothetical quantity won of hotel ah .
Oa indicates the quantity of good a the agent owns.
Uct indicates client cs value for trip t.
A.3 Decision Variables
 = {cst } is a set of boolean variables indicating whether or not client c is allocated
trip t in scenario s.
 = {apq } is a set of boolean variables indicating whether to bid price p on the qth
unit of ah .
M = {a } is a set of integer variables indicating how many units of af , ae to buy now.
N = {a } is a set of integer variables indicating how many units of ae to sell now.
Y = {as } is a set of integer variables indicating how many units of af , ae to buy later
in scenario s.
Z = {as } is a set of integer variables indicating how many units of ae to sell later in
scenario s.
A.4 Objective Function


flight cost

 current}| future {
hotel cost
z }| {
z X }|
{
X z }| { z }| {
X
X


Ma a + Yas as 
Uct cts 
Yas apq +
max


,,M,N,Y,Z
Af
Ah ,Q,pYas
S  C,T
trip value

z

543

(19)

fiGreenwald, Lee, & Naroditskiy


 event revenue
event cost
}|
{ z
z
}|
{
current
future
future 
current

z
}|
{
z
z
}|
{
}|
{
}| {
z
X


 Na a + Zas as  Ma a  Yas as 


Ae 

A.5 Constraints
X

cst  1 c  C, s  S

(20)

T

allocation

buy
own
z }| { z}|{
z }| {
X
cst Gat  Oa + (a + as )

a  Af , s  S

(21)

a  Ah , s  S

(22)

C,T

buy

allocation

own
z }| { z}|{
z X}|
{
X
cst Gat  Oa +
apq
C,T

Q,pYas

allocation


  sell 
own
buy
z }| { z}|{
z }| {
X
z }| {
cst Gat  Oa + a + as   a + as 
C,T

a  Ae , s  S

X

apq  Ha

(23)

a  Ah

(24)

apq  1 a  Ah , q  Q

(25)

P,Q

X
P

Equation (20) limits each client to one trip in each scenario. Equation (21) prevents the
agent from allocating flights that it does not own or buy. Equation (22) prevents the agent
from allocating hotels that it does not own or buy. Equation (23) prevents the agent from
allocating event tickets that it does not own or buy and not sell. Equation (24) ensures the
agent bids on at least HQW units in each hotel auction. Equation (25) prevents the agent
from placing more than one buy offer per unit in each hotel auction.
An agent might also be constrained not to place sell offers on more units of each good
than it owns, and/or not to place buy (sell) offers for more units of each good than the
market supplies (demands).
Note that there is no need to explicitly enforce the bid monotonicity constraints in this
ILP formulation:
 Buy offers must be nonincreasing in k, and sell offers nondecreasing.
The ILP does not need this constraint because prices are assumed to be linear.
In effect, the only decisions the ILP makes are how many units of each good to bid
on. Hence, the bids (10, 15, 20) and (20, 15, 10) are equivalent.
 An agent may not offer to sell for less than the price it is willing to buy.
544

fiRoxyBot-06

The ILP would not choose to place both a buy offer and a sell offer on a good if
the buy price of that good exceeds the sell price, because that would be unprofitable.

References
Ahmed, S., & Shapiro, A. (2002). The sample average approximation method for
stochastic programs with integer recourse. Optimization Online, http://www.
optimization-online.org.
Arunachalam, R., & Sadeh, N. M. (2005). The supply chain trading agent competition.
Electronic Commerce Research and Applications, 4 (1), 6684.
Aurell, E., Boman, M., Carlsson, M., Eriksson, J., Finne, N., Janson, S., Kreuger, P., &
Rasmusson, L. (2002). A trading agent built on constraint programming. In Eighth
International Conference of the Society for Computational Economics: Computing in
Economics and Finance, Aix-en-Provence.
Birge, J., & Louveaux, F. (1997). Introduction to Stochastic Programming. Springer, New
York.
Cai, K., Gerding, E., McBurney, P., Niu, J., Parsons, S., & S.Phelps (2009). Overview of
CAT: A market design competition. Tech. rep. ULCS-09-005, University of Liverpool.
Cheng, S., Leung, E., Lochner, K., K.OMalley, Reeves, D., Schvartzman, L., & Wellman,
M. (2003). Walverine: A Walrasian trading agent. In Proceedings of the Second
International Joint Conference on Autonomous Agents and Multi-Agent Systems, pp.
465472.
Cheng, S., Leung, E., Lochner, K., K.OMalley, Reeves, D., Schvartzman, L., & Wellman,
M. (2005). Walverine: A Walrasian trading agent. Decision Support Systems, 39 (2),
169184.
Cramton, P. (2006). Simultaneous ascending auctions. In Cramton, P., Shoham, Y., &
Steinberg, R. (Eds.), Combinatorial Auctions. MIT Press.
Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering for successful TAC participation. In Proceedings of the First International Joint Conference on Autonomous
Agents and Multiagent Systems, pp. 4546.
Greenwald, A. (2003). Bidding marginal utility in simultaneous auctions. In Workshop on
Trading Agent Design and Analysis.
Greenwald, A., & Boyan, J. (2004). Bidding under uncertainty: Theory and experiments.
In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, pp.
209216.
Greenwald, A., Naroditskiy, V., & Lee, S. (2008). Bidding heuristics for simultaneous
auctions: Lessons from tac travel. In Workshop on Trading Agent Design and Analysis.
Greenwald, A., & Boyan, J. (2005). Bidding algorithms for simultaneous auctions: A case
study. Journal of Autonomous Agents and Multiagent Systems, 10 (1), 6789.
He, M., & Jennings, N. (2002). SouthamptonTAC: Designing a successful trading agent. In
Proceedings of the Fifteenth European Conference on Artificial Intelligence, pp. 812.
545

fiGreenwald, Lee, & Naroditskiy

Jordan, P. R., & Wellman, M. P. (2009). Designing an ad auctions game for the trading
agent competition. In Workshop on Trading Agent Design and Analysis.
Kehagias, D., Toulis, P., & Mitkas, P. (2006). A long-term profit seeking strategy for continuous double auctions in a trading agent competition. In Fourth Hellenic Conference
on Artificial Intelligence, Heraklion.
Krishna, V. (2002). Auction Theory. Academic Press.
Lee, S. J. (2007). Comparison of bidding algorithms in simultaneous auctions. B.S. honors thesis, Brown University, http://list.cs.brown.edu/publications/theses/
ugrad/.
Lee, S., Greenwald, A., & Naroditskiy, V. (2007). Roxybot-06: An (SAA)2 TAC travel agent.
In Proceedings of the 20th International Joint Conference on Artificial Intelligence,
pp. 13781383.
Ross, S. M. (2002). Simulation (Third edition). Academic Press.
Sardinha, J. A. R. P., Milidiu, R. L., Paranhos, P. M., Cunha, P. M., & de Lucena, C.
J. P. (2005). An agent based architecture for highly competitive electronic markets.
In Proceedings of the Eighteenth International Florida Artificial Intelligence Research
Society Conference, Clearwater Beach, Florida, USA, pp. 326332.
Toulis, P., Kehagias, D., & Mitkas, P. (2006). Mertacor: A successful autonomous trading
agent. In Fifth International Joint Conference on Autonomous Agents and Multiagent
Systems, pp. 11911198, Hakodate.
Vetsikas, I., & Selman, B. (2002). WhiteBear: An empirical study of design tradeoffs for autonomous trading agents. In Workshop on Game-Theoretic Decision-Theoretic Agents.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. Journal
of Finance, 16, 837.
Walras, L. (1874). Elements deconomie politique pure. L. Corbaz, Lausanne.
Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: Strategies
and Lessons from the Trading Agent Competition. MIT Press.
Wellman, M. P., Reeves, D. M., Lochner, K. M., & Suri, R. (2005). Searching for Walverine
2005. In Workshop on Trading Agent Design and Analysis, No. 3937 in Lecture Notes
on Artificial Intelligence, pp. 157170. Springer.
Wellman, M., Reeves, D., Lochner, K., & Vorobeychik, Y. (2004). Price prediction in a
Trading Agent Competition. Artificial Intelligence Research, 21, 1936.

546

fiJournal of Artificial Intelligence Research 36 (2009) 341-385

Submitted 05/09; published 11/09

Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches
Tahira Naseem
Benjamin Snyder
Jacob Eisenstein
Regina Barzilay

TAHIRA @ CSAIL . MIT. EDU
BSNYDER @ CSAIL . MIT. EDU
JACOBE @ CSAIL . MIT. EDU
REGINA @ CSAIL . MIT. EDU

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Avenue, Cambridge MA 02139

Abstract
We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the
structure of each becomes more apparent. We consider two ways of applying this intuition to the
problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for
a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian
models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily as the number of
available languages increases.

1. Introduction
In this paper, we explore the application of multilingual learning to part-of-speech tagging when no
annotation is available.1 The fundamental idea upon which our work is based is that the patterns of
ambiguity inherent in part-of-speech tag assignments differ across languages. At the lexical level, a
word with part-of-speech tag ambiguity in one language may correspond to an unambiguous word
in the other language. For example, the word can in English may function as an auxiliary verb,
a noun, or a regular verb. However, many other languages are likely to express these different
senses with three distinct lexemes. Languages also differ in their patterns of structural ambiguity.
For example, the presence of an article in English greatly reduces the ambiguity of the succeeding
tag. In languages without articles, however, this constraint is obviously absent. The key idea of
multilingual learning is that by combining natural cues from multiple languages, the structure of
each becomes more apparent.
Even in expressing the same meaning, languages take different syntactic routes, leading to
cross-lingual variation in part-of-speech patterns. Therefore, an effective multilingual model must
accurately represent common linguistic structure, yet remain flexible to the idiosyncrasies of each
language. This tension only becomes stronger as additional languages are added to the mix. Thus,
a key challenge of multilingual learning is to capture cross-lingual correlations while preserving
individual language tagsets, tag selections, and tag orderings.
1. Code,
data
sets,
and
the
raw
outputs
http://groups.csail.mit.edu/rbg/code/multiling pos.

c
2009
AI Access Foundation. All rights reserved.

of

our

experiments

are

available

at

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

In this paper, we explore two different approaches for modeling cross-lingual correlations. The
first approach directly merges pairs of tag sequences into a single bilingual sequence, employing
joint distributions over aligned tag-pairs; for unaligned tags, language-specific distributions are still
used. The second approach models multilingual context using latent variables instead of explicit
node merging. For a group of aligned words, the multilingual context is encapsulated in the value of
a corresponding latent variable. Conditioned on the latent variable, the tagging decisions for each
language remain independent. In contrast to the first model, the architecture of the hidden variable
model allows it to scale gracefully as the number of languages increases.
Both approaches are formulated as hierarchical Bayesian models with an underlying trigram
HMM substructure for each language. The first model operates as a simple directed graphical
model with only one additional coupling parameter beyond the transition and emission parameters
used in monolingual HMMs. The latent variable model, on the other hand, is formulated as a
non-parametric model; it can be viewed as performing multilingual clustering on aligned sets of
tag variables. Each latent variable value indexes a separate distribution on tags for each language,
appropriate to the given context. For both models, we perform inference using Markov Chain Monte
Carlo sampling techniques.
We evaluate our models on a parallel corpus of eight languages: Bulgarian, Czech, English,
Estonian, Hungarian, Romanian, Serbian, and Slovene. We consider a range of scenarios that vary
from combinations of bilingual models to a single model that is jointly trained on all eight languages. Our results show consistent and robust improvements over a monolingual baseline for
almost all combinations of languages. When a complete tag lexicon is available and the latent variable model is trained using eight languages, average performance increases from 91.1% accuracy
to 95%, more than halving the gap between unsupervised and supervised performance. In more realistic cases, where the lexicon is restricted to only frequently occurring words, we see even larger
gaps between monolingual and multilingual performance. In one such scenario, average multilingual performance increases to 82.8% from a monolingual baseline of 74.8%. For some language
pairs, the improvement is especially noteworthy; for instance, in complete lexicon scenario, Serbian
improves from 84.5% to 94.5% when paired with English.
We find that in most scenarios the latent variable model achieves higher performance than the
merged structure model, even when it too is restricted to pairs of languages. Moreover the hidden
variable model can effectively accommodate large numbers of languages which makes it a more
desirable framework for multilingual learning. However, we observe that the latent variable model
is somewhat sensitive to lexicon coverage. The performance of the merged structure model, on the
other hand, is more robust in this respect. In the case of the drastically reduced lexicon (with 100
words only), its performance is clearly better than the hidden variable model. This indicates that the
merged structure model might be a better choice for the languages that lack lexicon resources.
A surprising discovery of our experiments is the marked variation in the level of improvement
across language pairs. If the best pairing for each language is chosen by an oracle, average bilingual
performance reaches 95.4%, compared to average performance of 93.1% across all pairs. Our
experiments demonstrate that this variability is influenced by cross-lingual links between languages
as well as by the model under consideration. We identify several factors that contribute to the
success of language pairings, but none of them can uniquely predict which supplementary language
is most helpful. These results suggest that when multi-parallel corpora are available, a model that
simultaneously exploits all the languages  such as the latent variable model proposed here  is

342

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

preferable to a strategy that selects one of the bilingual models. We found that performance tends
to improves steadily as the number of available languages increases.
In realistic scenarios, tagging resources for some number of languages may already be available.
Our models can easily exploit any amount of tagged data in any subset of available languages.
As our experiments show, as annotation is added, performance increases even for those languages
lacking resources.
The remainder of the paper is structured as follows. Section 2 compares our approach with
previous work on multilingual learning and unsupervised part-of-speech tagging. Section 3 presents
two approaches for modeling multilingual tag sequences, along with their inference procedures and
implementation details. Section 4 describes corpora used in the experiments, preprocessing steps
and various evaluation scenarios. The results of the experiments and their analysis are given in
Sections 5, and 6. We summarize our contributions and consider directions for future work in
Section 7.

2. Related Work
We identify two broad areas of related work: multilingual learning and inducing part-of-speech tags
without labeled data. Our discussion of multilingual learning focuses on unsupervised approaches
that incorporate two or more languages. We then describe related work on unsupervised and semisupervised models for part-of-speech tagging.
2.1 Multilingual Learning
The potential of multilingual data as a rich source of linguistic knowledge has been recognized since
the early days of empirical natural language processing. Because patterns of ambiguity vary greatly
across languages, unannotated multilingual data can serve as a learning signal in an unsupervised
setting. We are especially interested in methods to leverage more than two languages jointly, and
compare our approach with relevant prior work.
Multilingual learning may also be applied in a semi-supervised setting, typically by projecting
annotations across a parallel corpus to another language where such resources do not exist (e.g.,
Yarowsky, Ngai, & Wicentowski, 2000; Diab & Resnik, 2002; Pado & Lapata, 2006; Xi & Hwa,
2005). As our primary focus is on the unsupervised induction of cross-linguistic structures, we do
not address this area.
2.1.1 B ILINGUAL L EARNING
Word sense disambiguation (WSD) was among the first successful applications of automated multilingual learning (Dagan et al., 1991; Brown et al., 1991). Lexical ambiguity differs across languages
 each sense of a polysemous word in one language may translate to a distinct counterpart in another
language. This makes it possible to use aligned foreign-language words as a source of noisy supervision. Bilingual data has been leveraged in this way in a variety of WSD models (Brown et al.,
1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002;
Bhattacharya, Getoor, & Bengio, 2004), and the quality of supervision provided by multilingual
data closely approximates that of manual annotation (Ng et al., 2003). Polysemy is one source of
ambiguity for part-of-speech tagging; thus our model implicitly leverages multilingual WSD in the
context of a higher-level syntactic analysis.

343

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Multilingual learning has previously been applied to syntactic analysis; a pioneering effort was
the inversion transduction grammar of Wu (1995). This method is trained on an unannotated parallel
corpus using a probabilistic bilingual lexicon and deterministic constraints on bilingual tree structures. The inside-outside algorithm (Baker, 1979) is used to learn parameters for manually specified
bilingual grammar. These ideas were extended by subsequent work on synchronous grammar induction and hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005).
One characteristic of this family of methods is that they were designed for inherently multilingual tasks such as machine translation and lexicon induction. While we share the goal of learning
from multilingual data, we seek to induce monolingual syntactic structures that can be applied even
when multilingual data is unavailable.
In this respect, our approach is closer to the unsupervised multilingual grammar induction work
of Kuhn (2004). Starting from the hypothesis that trees induced over parallel sentences should
exhibit cross-lingual structural similarities, Kuhn uses word-level alignments to constrain the set
of plausible syntactic constituents. These constraints are implemented through hand-crafted deterministic rules, and are incorporated in expectation-maximization grammar induction to assign zero
likelihood to illegal bracketings. The probabilities of the productions are then estimated separately
for each language, and can be applied to monolingual data directly. Kuhn shows that this form of
multilingual training yields better monolingual parsing performance.
Our methods incorporate cross-lingual information in a fundamentally different manner. Rather
than using hand-crafted deterministic rules  which may require modification for each language
pair  we estimate probabilistic multilingual patterns directly from data. Moreover, the estimation
of multilingual patterns is incorporated directly into the tagging model itself.
Finally, multilingual learning has recently been applied to unsupervised morphological segmentation (Snyder & Barzilay, 2008). This research is related, but moving from morphological to
syntactic analysis imposes new challenges. One key difference is that Snyder & Barzilay model
morphemes as unigrams, ignoring the transitions between morphemes. In syntactic analysis, transition information provides a crucial constraint, requiring a fundamentally different model structure.
2.1.2 B EYOND B ILINGUAL L EARNING
While most work on multilingual learning focuses on bilingual analysis, some models operate on
more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a
multilingual lexicon from a group of related languages. This work first induces bilingual models for
each pair of languages and then combines them. We take a different approach by simultaneously
learning from all languages, rather than combining bilingual results.
A related thread of research is multi-source machine translation (Och & Ney, 2001; Utiyama
& Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico,
& Cattoni, 2008) where the goal is to translate from multiple source languages to a single target
language. By using multi-source corpora, these systems alleviate sparseness and increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translation
systems build separate bilingual models and then select a final translation from their output. For instance, a method developed by Och and Ney (2001) generates several alternative translations from
source sentences expressed in different languages and selects the most likely candidate. Cohn and
Lapata (2007) consider a different generative model: rather than combining alternative sentence
translations in a post-processing step, their model estimates the target phrase translation distribu-

344

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

tion by marginalizing over multiple translations from various source languages. While their model
combines multilingual information at the phrase level, at its core are estimates for phrase tables that
are obtained using bilingual models.
In contrast, we present an approach for unsupervised multilingual learning that builds a single
joint model across all languages. This makes maximal use of unlabeled data and sidesteps the
difficult problem of combining the output of multiple bilingual systems without supervision.
2.2 Unsupervised Part-of-Speech Tagging
Unsupervised part-of-speech tagging involves predicting the tags for words, without annotations of
the correct tags for any word tokens. Generally speaking, the unsupervised setting does permit the
use of declarative knowledge about the relationship between tags and word types, in the form of a
dictionary of the permissible tags for the most common words. This setup is referred to as semisupervised by Toutanova and Johnson (2008), but is considered unsupervised in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of
varying levels of coverage.
Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags are
encoded as a linear chain of hidden variables, and words are treated as emitted observations. Recent
advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007),
which places prior distributions on tag transition and word-emission probabilities. Such Bayesian
priors permit integration over parameter settings, yielding models that perform well across a range
of settings. This is particularly important in the case of small datasets, where many of the counts
used for maximum-likelihood parameter estimation will be sparse. The Bayesian setting also facilitates the integration of other data sources, and thus serves as the departure point for our work.
Several recent papers have explored the development of alternative training procedures and
model structures in an effort to incorporate more expressive features than permitted by the generative HMM. Smith and Eisner (2005) maintain the HMM structure, but incorporate a large number
of overlapping features in a conditional log-linear formulation. Contrastive estimation is used to
provide a training criterion which maximizes the probability of the observed sentences compared to
a set of similar sentences created by perturbing word order. The use of a large set of features and a
discriminative training procedure led to strong performance gains.
Toutanova and Johnson (2008) propose an LDA-style model for unsupervised part-of-speech
tagging, grouping words through a latent layer of ambiguity classes. Each ambiguity class corresponds to a set of permissible tags; in many languages this set is tightly constrained by morphological features, thus allowing an incomplete tagging lexicon to be expanded. Haghighi and Klein
(2006) also use a variety of morphological features, learning in an undirected Markov Random Field
that permits overlapping features. They propagate information from a small number of labeled prototype examples using the distributional similarity between prototype and non-prototype words.
Our focus is to effectively incorporate multilingual evidence, and we require a simple model
that can easily be applied to multiple languages with widely varying structural properties. We view
this direction as orthogonal to refining monolingual tagging models for any particular language.
2. In addition to the basic HMM architecture, other part-of-speech tagging approaches have been explored (Brill, 1995;
Mihalcea, 2004)

345

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

(

(

a

b

)

)

I

l

o

v

e

f

i

s

h

I

o

r

o

e
l

J

(

c



a

e

s

i

s

s

o

l

n

o

r

o

v

e

f

l
J



a

s

h

o

e

p

d

i

e

s

i

s

s

o

n

p

d

)

l

I

l

o

v

e

f

i

s

h

J



a

d

o

r

e

s

p

e

o

i

s

s

o

n

p

a

n

i

o

h

e

v

d

a

g

i

m

M

u

j

h

e

m

a

c

h

c

h

l

i

a

s

a

n

d

h

a

i

Figure 1: Example graphical structures of (a) two standard monolingual HMMs, (b) our merged
node model, and (c) our latent variable model with three superlingual variables.

3. Models
The motivating hypothesis of this work is that patterns of ambiguity at the part-of-speech level differ
across languages in systematic ways. By considering multiple languages simultaneously, the total
inherent ambiguity can be reduced in each language. But with the potential advantages of leveraging
multilingual information comes the challenge of respecting language-specific characteristics such as
tag inventory, selection and order. To this end, we develop models that jointly tag parallel streams
of text in multiple languages, while maintaining language-specific tag sets and parameters over
transitions and emissions.

346

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Part-of-speech tags reflect the syntactic and semantic function of the tagged words. Across
languages, pairs of word tokens that are known to share semantic or syntactic function should have
tags that are related in systematic ways. The word alignment task in machine translation is to
identify just such pairs of words in parallel sentences. Aligned word pairs serve as the cross-lingual
anchors of our model, allowing information to be shared via joint tagging decisions. Research in
machine translation has produced robust tools for identifying word alignments; we use such a tool
as a black box and treat its output as a fixed, observed property of the parallel data.
Given a set of parallel sentences, we posit a hidden Markov model (HMM) for each language,
where the hidden states represent the tags and the emissions are the words. In the unsupervised
monolingual setting, inference on the part-of-speech tags is performed jointly with estimation of
parameters governing the relationship between tags and words (the emission probabilities) and between consecutive tags (the transition probabilities). Our multilingual models are built upon this
same structural foundation, so that the emission and transition parameters retain an identical interpretation as in the monolingual setting. Thus, these parameters can be learned on parallel text and
later applied to monolingual data.
We consider two alternative approaches for incorporating cross-lingual information. In the first
model, the tags for aligned words are merged into single bi-tag nodes; in the second, latent variable
model, an additional layer of hidden superlingual tags instead exerts influence on the tags of clusters
of aligned words. The first model is primarily designed for bilingual data, while the second model
operates over any number of languages. Figure 1 provides a graphical model representation of the
monolingual, merged node, and latent variable models instantiated over a single parallel sentence.
Both the merged node and latent variable approaches are formalized as hierarchical Bayesian
models. This provides a principled probabilistic framework for integrating multiple sources of
information, and offers well-studied inference techniques. Table 1 summarizes the mathematical
notation used throughout this section. We now describe each model in depth.
3.1 Bilingual Unsupervised Tagging: A Merged Node Model
In the bilingual merged node model, cross-lingual context is incorporated by creating joint bi-tag
nodes for aligned words. It would be too strong to insist that aligned words have an identical
tag; indeed, it may not even be appropriate to assume that two languages share identical tag sets.
However, when two words are aligned, we do want to choose their tags jointly. To enable this, we
allow the values of the bi-tag nodes to range over all possible tag pairs ht, t i  T  T  , where T
and T  represent the tagsets for each language.
The tags t and t need not be identical, but we do believe that they are systematically related.
This is modeled using a coupling distribution , which is multinomial over all tag pairs. The
parameter  is combined with the standard transition distribution  in a product-of-experts model.

Thus, the aligned tag pair hyi , yj i is conditioned on the predecessors yi1 and yj1
, as well as the

3
coupling parameter (yi , yj ). The coupled bi-tag nodes serve as bilingual anchors  due to the
Markov dependency structure, even unaligned words may benefit from cross-lingual information
that propagates from these nodes.
3. While describing the merged node model, we consider only the two languages  and  , and use a simplified notation

in which we write hy, y  i to mean hy  , y  i. Similar abbreviations are used for the language-indexed parameters.

347

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Notation used in both models
xi
yi

a,
t






t



0



0



The ith word token of the sentence in language .
The ith tag of the sentence in language .
The word alignments for the language pair h,  i.
The transition distribution (over tags), conditioned on tag t in language . We describe a bigram transition model, though our implementation uses trigrams (without bigram interpolations); the extension is trivial.
The emission distribution (over words), conditioned on tag t in
language .
The parameter of the symmetric Dirichlet prior on the transition
distributions.
The parameter of the symmetric Dirichlet prior on the emission
distributions.

Notation used in the merged node model




0
Ab




A coupling parameter that assigns probability mass to each pair of
aligned tags.
A Dirichlet prior on the coupling parameter.
Distribution over bilingual alignments.

Notation used in the latent variable model






zj



z = hz1 , z2 , . . . , zn i



G0



Am



A multinomial over the superlingual tags z.
The concentration parameter for , controlling how much probability mass is allocated to the first few values.
The setting of the j th superlingual tag, ranging over the set of integers, and indexing a distribution set in .
The z th set of distributions over tags in all languages 1 through
n .
A base distribution from which the z are drawn, whose form is
a set of n symmetric Dirichlet distributions each with a parameter
0 .
Distribution over multilingual alignments.

Table 1: Summary of notation used in the description of both models. As each sentence is treated
in isolation (conditioned on the parameters), the sentence indexing is left implicit.

348

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

We now present a generative account of how the words in each sentence and the parameters
of the model are produced. This generative story forms the basis of our sampling-based inference
procedure.
3.1.1 M ERGED N ODE M ODEL : G ENERATIVE S TORY
Our generative story assumes the existence of two tagsets, T and T  , and two vocabularies W and
W   one of each for each language. For ease of exposition, we formulate our model with bigram
tag dependencies. However, in our experiments we used a trigram model (without any bigram
interpolation), which is a trivial extension of the described model.
1. Transition and Emission Parameters. For each tag t  T , draw a transition distribution t
over tags T , and an emission distribution t over words W . Both the transition and emission
distributions are multinomial, so they are drawn from their conjugate prior, the Dirichlet (Gelman, Carlin, Stern, & Rubin, 2004). We use symmetric Dirichlet priors, which encode only an
expectation about the uniformity of the induced multinomials, but not do encode preferences
for specific words or tags.
For each tag t  T  , draw a transition distribution t over tags T  , and an emission distribution
t over words W  , both from symmetric Dirichlet priors.
2. Coupling Parameter. Draw a bilingual coupling distribution  over tag pairs pairs T  T  .
This distribution is multinomial with dimension |T |  |T  |, and is drawn from a symmetric
Dirichlet prior 0 over all tag pairs.
3. Data. For each bilingual parallel sentence:
(a) Draw an alignment a from a bilingual alignment distribution Ab . The following paragraph defines a and Ab more formally.
(b) Draw a bilingual sequence of part-of-speech tags (y1 , ..., ym ), (y1 , ..., yn ) according to:
P ((y1 , ..., ym ), (y1 , ..., yn )|a, ,  , ).4 This joint distribution thus conditions on the
alignment structure, the transition probabilities for both languages, and the coupling
distribution; a formal definition is given in Formula 1.
(c) For each part-of-speech tag yi in the first language, emit a word from the vocabulary W :
xi  yi ,
(d) For each part-of-speech tag yj in the second language, emit a word from the vocabulary
W  : xj  y  .
j

This completes the outline of the generative story. We now provide more detail on how alignments are handled, and on the distribution over coupled part-of-speech tag sequences.
Alignments An alignment a defines a bipartite graph between the words x and x in two parallel
sentences . In particular, we represent a as a set of integer pairs, indicating the word indices.
Crossing edges are not permitted, as these would lead to cycles in the resulting graphical model;
thus, the existence of an edge (i, j) precludes any additional edges (i + a, j  b) or (i  a, j + b),
4. We use a special end state, rather than explicitly modeling sentence length. Thus the values of m and n are determined
stochastically.

349

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

for a, b  0. From a linguistic perspective, we assume that the edge (i, j) indicates that the words
xi and xj share some syntactic and/or semantic role in the bilingual parallel sentences.
From the perspective of the generative story, alignments are treated as draws from a distribution Ab . Since the alignments are always observed, we can remain agnostic about the distribution
Ab , except to require that it assign zero probability to alignments which either: (i) align a single
index in one language to multiple indices in the other language or (ii) contain crossing edges. The
resulting alignments are thus one-to-one, contain no crossing edges, and may be sparse or even
possibly empty. Our technique for obtaining alignments that display these properties is described in
Section 4.2.
Generating Tag Sequences In a standard hidden Markov model for part-of-speech tagging, the
tags are drawn as a Markov process from the transition distribution. This permits the probability
of a tag sequence to factor across the time steps. Our model employs a similar factorization: the
tags for unaligned words are drawn from their predecessors transition distribution, while joined tag
nodes are drawn from a product involving the coupling parameter and the transition distributions
for both languages.
More formally, given an alignment a and sets of transition parameters  and  , we factor the
conditional probability of a bilingual tag sequence (y1 , ..., ym ), (y1 , ..., yn ) into transition probabilities for unaligned tags, and joint probabilities over aligned tag pairs:
Y
Y
P ((y1 , ..., ym ), (y1 , ..., yn )|a, ,  , ) =
yi1 (yi )
y (yj )
j1

unaligned i

Y

unaligned j


P (yi , yj |yi1 , yj1
, ,  , ).

(1)

(i,j)a

Because the alignment contains no crossing edges, we can still model the tags as generated
sequentially by a stochastic process. We define the distribution over aligned tag pairs to be a product
of each languages transition probability and the coupling probability:

P (yi , yj |yi1 , yj1
, ,  , )

=

yi1 (yi ) y

j1

(yj )(yi , yj )

Z

The normalization constant here is defined as:
X
yi1 (y) y
Z=

j1

.

(2)

(y  ) (y, y  ).

y,y 

This factorization allows the language-specific transition probabilities to be shared across aligned
and unaligned tags.
Another way to view this probability distribution is as a product of three experts: the two transition parameters and the coupling parameter. Product-of-expert models (Hinton, 1999) allow each
information source to exercise very strong negative influence on the probability of tags that they
consider to be inappropriate, as compared with additive models. This is ideal for our setting, as it
prevents the coupling distribution from causing the model to generate a tag that is unacceptable from
the perspective of the monolingual transition distribution. In preliminary experiments we found that
a multiplicative approach was strongly preferable to additive models.

350

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

3.1.2 M ERGED N ODE M ODEL : I NFERENCE
The goal of our inference procedure is to obtain transition and emission parameters  and  that can
be applied to monolingual test data. Ideally we would choose the parameters that have the highest
marginal probability, conditioned on the observed words x and alignments a,
Z
,  = arg max P (, , y, |x, a, 0 , 0 , 0 )dyd.
,

While the structure of our model permits us to decompose the joint probability, it is not possible to analytically marginalize all of the hidden variables. We resort to standard Monte Carlo
approximation, in which marginalization is performed through sampling. By repeatedly sampling
individual hidden variables according to the appropriate distributions, we obtain a Markov chain
that is guaranteed to converge to a stationary distribution centered on the desired posterior. Thus,
after an initial burn-in phase, we can use the samples to approximate a marginal distribution over
any desired parameter (Gilks, Richardson, & Spiegelhalter, 1996).
The core element of our inference procedure is Gibbs sampling (Geman & Geman, 1984). Gibbs
sampling begins by randomly initializing all unobserved random variables; at each iteration, each
random variable ui is then sampled from the conditional distribution P (ui |ui ), where ui refers
to all variables other than ui . Eventually, the distribution over samples drawn from this process will
converge to the unconditional joint distribution P (u) of the unobserved variables. When possible,
we avoid explicitly sampling variables which are not of direct interest, but rather integrate over
them. This technique is known as collapsed sampling; it is guaranteed never to increase sampling
variance, and will often reduce it (Liu, 1994).
In the merged node model, we need sample only the part-of-speech tags and the priors. We are
able to exactly marginalize the emission parameters  and approximately marginalize the transition
and coupling parameters  and  (the approximations are required due to the re-normalized product
of experts  see below for more details). We draw repeated samples of the part-of-speech tags, and
construct a sample-based estimate of the underlying tag sequence. After sampling, we construct
maximum a posteriori estimates of the parameters of interest for each language,  and .
Sampling Unaligned Tags For unaligned part-of-speech tags, the conditional sampling equations
are identical to the monolingual Bayesian hidden Markov model. The probability of each tag decomposes into three factors:
P (yi |yi , y , x, x , 0 , 0 )  P (xi |yi , yi , xi , 0 )P (yi |yi1 , yi , 0 )P (yi+1 |yi , yi , 0 ), (3)
which follows from the chain rule and the conditional independencies in the model. The first factor
is for the emission xi and the remaining two are for the transitions. We now derive the form of each
factor, marginalizing the parameters  and .
For the emission factor, we can exactly marginalize the emission distribution , whose prior is
Dirichlet with hyperparameter 0 . The resulting distribution is a ratio of counts, where the prior acts
as a pseudo-count:
Z
n(yi , xi ) + 0
yi (xi )P (yi |y, xi , 0 )dyi =
P (xi |y, xi , 0 ) =
.
(4)
n(yi ) + |Wyi |0
yi
Here, n(yi ) is the number of occurrences of the tag yi in yi , n(yi , xi ) is the number of occurrences of the tag-word pair (yi , xi ) in (yi , xi ), and Wyi is the set of word types in the vocabulary
351

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

W that can take tag yi . The integral is tractable due to Dirichlet-multinomial conjugacy, and an
identical marginalization was applied in the monolingual Bayesian HMM of Goldwater and Griffiths (2007).
For unaligned tags, it is also possible to exactly marginalize the parameter  governing transitions. For the transition from i  1 to i,
Z
n(yi1 , yi ) + 0
yi1 (yi )P (yi |yi , 0 )dyi1 =
P (yi |yi1 , yi , 0 ) =
.
(5)
n(y
i1 ) + |T |0
yi1
The factors here are similar to the emission probability: n(yi ) is the number of occurrences of
the tag yi in yi , n(yi1 , yi ) is the number of occurrences of the tag sequence (yi1 , yi ), and T is
the tagset. The probability for the transition from i to i + 1 is analogous.
Jointly Sampling Aligned Tags The situation for tags of aligned words is more complex. We
sample these tags jointly, considering all |T  T  | possibilities. We begin by decomposing the
probability into three factors:
P (yi , yj |yi , yj , x, x , a, 0 , 0 , ,  , )  P (xi |y, xi , 0 )P (xj |y , xj , 0 )P (yi , yj |yi , yj , a, ,  , ).
The first two factors are emissions, and are handled identically to the unaligned case (Formula 4). The expansion of the final, joint factor depends on the alignment of the succeeding tags.
If neither of the successors (in either language) are aligned, we have a product of the bilingual
coupling probability and four transition probabilities:

P (yi , yj |yi , yj
, ,  , )  (yi , yj )yi1 (yi )yi (yi+1 )y

j1


(yj )y (yj+1
).
j

Whenever one or more of the succeeding words is aligned, the sampling formulas must account
for the effect of the sampled tag on the joint probability of the succeeding tags, which is no longer
a simple multinomial transition probability. We give the formula for one such casewhen we are
sampling a joint tag pair (yi , yj ), whose succeeding words (xi+1 , xj+1 ) are also aligned to one
another:
"
#

yi (yi+1 ) y (yj+1
)
j
P (yi , yj |yi , yj , a, ,  , )  (yi , yj )yi1 (yi ) y (yj ) P
 (t ) (t, t ) . (6)
j1

(t)



y
i
t,t
y
j


= t ,
Intuitively, if  puts all of its probability mass on a single assignment yi+1 = t, yj+1
then the transitions from i to i + 1 and j to j + 1 are irrelevant, and the final factor goes to one.
Conversely, if  is indifferent and assigns equal probability to all pairs ht, t i, then the final fac
tor becomes proportional to yi (yi+1 )y (yj+1
), which is the same as if xi+1 and xj+1 were not
j
aligned. In general, as the entropy of  increases, the transition to the succeeding nodes exerts a
greater influence on yi and yj . Similar equations can be derived for cases where the succeeding tags
are not aligned to each other, but one of them is aligned to another tag, e.g., xi+1 is aligned to xj+2 .
As before, we would like to marginalize the parameters ,  , and . Because these parameters
interact as a product-of-experts model, these marginalizations are approximations. The form of the
marginalizations for  and  are identical to Formula 5. For the coupling distribution,

P (yi , yj |yi , yj
, 0 ) 

352

n(yi , yj ) + 0
,
N (a) + |T  T  |0

(7)

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

where n(yi , yj ) is the number of times tags yi and yj were aligned, excluding i and j, and N (a)
is the total number of alignments. As above, the prior 0 appears as a smoothing factor; in the
denominator it is multiplied by the dimensionality of , which is the size of the cross-product of the
two tagsets. Intuitively, this approximation would be exactly correct if each aligned tag had been
generated twice  once by the transition parameter and once by the coupling parameter  instead
of a single time by the product of experts.
The alternative to approximately marginalizing these parameters would be to sample them using
a Metropolis-Hastings scheme as in the work by Snyder, Naseem, Eisenstein, and Barzilay (2008).
The use of approximate marginalizations represents a bias-variance tradeoff, where the decreased
sampling variance justifies the bias introduced by the approximations, for practical numbers of
samples.
3.2 Multilingual Unsupervised Tagging: A Latent Variable Model
The model described in the previous section is designed for bilingual aligned data; as we will see
in Section 5, it exploits such data very effectively. However, many resources contain more than
two languages: for example, Europarl contains eleven, and the Multext-East corpus contains eight.
This raises the question of how best to exploit all available resources when multi-aligned data is
available.
One possibility would be to train separate bilingual models and then combine their output at test
time, either by voting or some other heuristic. However, we believe that cross-lingual information
reduces ambiguity at training time, so it would be preferable to learn from multiple languages jointly
during training. Indeed, the results in Section 5 demonstrate that joint training outperforms such a
voting scheme.
Another alternative would be to try to extend the bilingual model developed in the previous
section. While such an extension is possible in principle, the merged node model does not scale well
in the case of multi-aligned data across more than two languages. Recall that we use merged nodes
to represent both tags for aligned words; the state space of such nodes grows as |T |L , exponential
in the number of languages L. Similarly, the coupling parameter  has the same dimension, so
that the counts required for estimation become too sparse as the number of languages increases.
Moreover, the bi-tag model required removing crossing edges in the word-alignment, so as to avoid
cycles. This is unproblematic for pairs of aligned sentences, usually requiring the removal of less
than 5% of all edges (see Table 16 in Appendix A). However, as the number of languages grows, an
increasing number of alignments will have to be discarded.
Instead, we propose a new architecture specifically designed for the multilingual setting. As
before, we maintain HMM substructures for each language, so that the learned parameters can
easily be applied to monolingual data. However, rather than merging tag nodes for aligned words,
we introduce a layer of superlingual tags. The role of these latent nodes is to capture cross-lingual
patterns. Essentially they perform a non-parametric clustering over sets of aligned tags, encouraging
multilingual patterns that occur elsewhere in the corpus.
More concretely, for every set of aligned words, we add a superlingual tag with outgoing edges
to the relevant part-of-speech nodes. An example configuration is shown in Figure 1c. The superlingual tags are each generated independently, and they influence the selection of the part-of-speech
tags to which they are connected. As before, we use a product-of-experts model to combine these
cross-lingual cues with the standard HMM transition model.

353

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

This setup scales well. Crossing and many-to-many alignments may be used without creating
cycles, as all cross-lingual information emanates from the hidden superlingual tags. Furthermore,
the size of the model and its parameter space scale linearly with the number of languages. We now
describe the role of the superlingual tags in more detail.
3.2.1 P ROPAGATING C ROSS - LINGUAL PATTERNS WITH S UPERLINGUAL TAGS
Each superlingual tag specifies a set of distributions  one for each languages part-of-speech
tagset. In order to learn repeated cross-lingual patterns, we need to constrain the number of values
that the superlingual tags can take and thus the number of distributions they provide. For example,
we might allow the superlingual tags to take on integer values from 1 to K, with each integer
value indexing a separate set of tag distributions. Each set of distributions should correspond to a
discovered cross-lingual pattern in the data. For example, one set of distributions might favor nouns
in each language and another might favor verbs, though heterogenous distributions (e.g., favoring
determiners in one language and prepositions in others) are also possible.
Rather than fixing the number of superlingual tag values to an arbitrary size K, we leave it unbounded, using a non-parametric Bayesian model. To encourage the desired multilingual clustering
behavior, we use a Dirichlet process prior (Ferguson, 1973). Under this prior, high posterior probability is obtained only when a small number of values are used repeatedly. The actual number of
sampled values will thus be dictated by the data.
We draw an infinite sequence of distribution sets 1 , 2 , . . . from some base distribution G0 .
()
Each i is a set of distributions over tags, with one distribution per language, written i . To
weight these sets of distributions, we draw an infinite sequence of mixture weights 1 , 2 , . . . from
a stick-breaking process, which defines a distribution over the integers with most probability mass
placed on some initial set of values. The pair of sequences 1 , 2 , . . . and 1 , 2 , . . . now define
the distribution over superlingual tags and their associated distributions on parts-of-speech. Each
superlingual tag z  N is drawn with probability z , and is associated with the set of multinomials

hz , z , . . .i.
As in the merged node model, the distribution over aligned part-of-speech tags is governed by
a product of experts. In this case, the incoming edges are from the superlingual tags (if any) and
the predecessor tag. We combine these distributions via their normalized product. Assuming tag
position i of language  is connected to M superlingual tags, the part-of-speech tag yi is drawn
according to,
Q

yi1 (yi ) M
m=1 zm (yi )
yi 
,
(8)
Z

where yi1 indicates the transition distribution, zm is the value of the mth connected superlingual
tag, and zm (yi ) indicates the tag distribution for language  given by zm . The normalization Z is
obtained by summing this product over all possible values of yi .
This parameterization allows for a relatively simple parameter space. It also leads to a desirable
property: for a tag to have high probability, each of the incoming distributions must allow it. That is,
any expert can veto a potential tag by assigning it low probability, generally leading to consensus
decisions.
We now formalize this description by giving the stochastic generative process for the observed
data (raw parallel text and alignments), according to the multilingual model.

354

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

3.2.2 L ATENT VARIABLE M ODEL : G ENERATIVE S TORY
For n languages, we assume the existence of n tagsets T 1 , . . . , T n and vocabularies, W 1 , . . . , W n ,
one for each language. Table 1 summarizes all relevant parameters. For clarity the generative
process is described using only bigram transition dependencies, but our experiments use a trigram
model, without any bigram interpolations.
1. Transition and Emission Parameters. For each language  = 1, ..., n and for each tag
t  T  , draw a transition distribution t over tags T and an emission distribution t over
words W  , all from symmetric Dirichlet priors of appropriate dimension.
2. Superlingual Tag Parameters. Draw an infinite sequence of sets of distributions over tags
1 , 2 , . . ., where each i is a set of n multinomials hi1 , i2 , . . . in i, one for each of n
languages. Each multinomial i is a distribution over the tagset T  , and is drawn from a
symmetric Dirichlet prior; these priors together comprise the base distribution G0 , from which
each i is drawn.
At the same time, draw an infinite sequence of mixture weights   GEM (), where
GEM () indicates the stick-breaking distribution (Sethuraman, 1994) with concentration
parameter  = 1. These parameters define a distribution over superlingual tags, or equivalently over the part-of-speech distributions that they index:
P
k k=z
(9)
z 
Pk
(10)
 
k k =k

where =k is defined as one when  = k and zero otherwise. From Formula 10, we can
say that the set of multinomials  is drawn from a Dirichlet process, conventionally written
DP (, G0 ).
3. Data. For each multilingual parallel sentence:
(a) Draw an alignment a from multilingual alignment distribution Am . The alignment a
specifies sets of aligned indices across languages; each such set may consist of indices
in any subset of the languages.
(b) For each set of indices in a, draw a superlingual tag value z according to Formula 9.
(c) For each language , for i = 1, . . . (until end-tag reached):
i. Draw a part-of-speech tag yi  T  according to Formula 8.
ii. Draw a word wi  W  according to the emission distribution yi .

One important difference from the merged node model generative story is that the distribution
over multilingual alignments Am is unconstrained: we can generate crossing and many-to-one alignments as needed. To perform Bayesian inference under this model we again use Gibbs sampling,
marginalizing parameters whenever possible.

355

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

3.2.3 L ATENT VARIABLE M ODEL : I NFERENCE
As in section 3.1.2, we employ a sampling-based inference procedure. Again, standard closed forms
are used to analytically marginalize the emission parameters , and approximate marginalizations
are applied to transition parameters , and superlingual tag distributions i ; similar techniques are
used to marginalize the superlingual tag mixture weights . As before, these approximations would
be exact if each of the parameters in the numerator of Formula 8 were solely responsible for other
sampled tags.
We still must sample the part-of-speech tags y and superlingual tags z. The remainder of the
section describes the sampling equations for these variables.
Sampling Part-of-speech Tags
draw from:

To sample the part-of-speech tag for language  at position i we


|yi , y(,i) , a, z)P (yi |y(,i) , a, z)
P (yi |y(,i) , x, a, z)  P (xi |xi , y )P (yi+1

(11)

where y(,i) refers to all tags except yi . The first factor handles the emissions, and the latter two
factors are the generative probabilities of (i) the current tag given the previous tag and superlingual
tags, and (ii) the next tag given the current tag and superlingual tags. These two quantities are similar
to equation 8, except here we integrate over the transition parameter yi1 and the superlingual tag
parameters z . We end up with a product of integrals, each of which we compute in closed form.
Terms involving the transition distributions  and the emission distributions  are identical to
the bilingual case, as described in Section 3.1.2. The closed form for integrating over the parameter
of a superlingual tag with value z is given by:
Z
n(z, yi , ) + 0
z (yi )P (z |0 )dz =
n(z, ) + T  0
where n(z, yi , ) is the number of times that tag yi is observed together with superlingual tag z in
language , n(z, ) is the total number of times that superlingual tag z appears with an edge into
language , and 0 is a symmetric Dirichlet prior over tags for language .
Sampling Superlingual Tags For each set of aligned words in the observed alignment a we need
to sample a superlingual tag z. Recall that z is an index into an infinite sequence
h11 , . . . , 1n i, h21 , . . . , 2n i, . . . ,
where each z is a distribution over the tagset T  . The generative distribution over z is given by
Formula 9. In our sampling scheme, however, we integrate over all possible settings of the mixture
weights  using the standard Chinese Restaurant Process closed form (Escobar & West, 1995):
(
1
fi
fi

 Y
n(zi ) if zi  zi
P yi fizi , zi , y(,i)  k+
P zi fizi , y 
(12)

otherwise
k+

The first group of factors is the product of closed form probabilities for all tags connected to the
superlingual tag, conditioned on zi . Each of these factors is calculated in the same manner as
equation 11 above. The final factor is the standard Chinese Restaurant Process closed form for
posterior sampling from a Dirichlet process prior. In this factor, k is the total number of sampled
superlingual tags, n(zi ) is the total number of times the value zi occurs in the sampled superlingual
tags, and  is the Dirichlet process concentration parameter (see Step 2 in Section 3.2.2).
356

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

3.3 Implementation
This section describes implementation details that are necessary to reproduce our experiments. We
present details for the merged node and latent variable models, as well as our monolingual baseline.
3.3.1 I NITIALIZATION
An initialization phase is required to generate initial settings for the word tags and hyperparameters,
and for the superlingual tags in the latent variable model. The initialization is as follows:
 Monolingual Model
 Tags: Random, with uniform probability among tag dictionary entries for the emitted
word.
 Hyperparameters 0 , 0 : Initialized to 1.0
 Merged Node Model
 Tags: Random, with uniform probability among tag dictionary entries for the emitted
word. For joined tag nodes, each slot is selected from the tag dictionary of the emitted
word in the appropriate language.
 Hyperparameters 0 , 0 , 0 : Initialized to 1.0
 Latent Variable Model
 Tags: Set to the final estimate from the monolingual model.
 Superlingual Tags: Initially a set of 14 superlingual tag values is assumed  each value
corresponds to one part-of-speech tag. Each alignment is assigned one of these 14 values
based on the most common initial part-of-speech tag of the words in the alignment.
 Hyperparameters 0 , 0 : Initialized to 1.0
 Base Distribution G0 : Set to a symmetric Dirichlet distribution with parameter value
fixed to 1.0
 Concentration Parameter : Set to 1.0 and remains fixed throughout.
3.3.2 H YPERPARAMETER E STIMATION
Both models have symmetric Dirichlet priors 0 and 0 , for the emission and transition distributions respectively. The merged node model also has symmetric Dirichlet prior 0 on the coupling
parameter. We re-estimate these priors during inference, based on non-informative hyperpriors.
Hyperparameter re-estimation applies the Metropolis-Hastings algorithm after each full epoch
of sampling the tags. In addition, we run an initial 200 iterations to speed convergence. MetropolisHastings is a sampling technique that draws a new value u from a proposal distribution, and makes
a stochastic decision about whether to accept the new sample (Gelman et al., 2004). This decision is
based on the proposal distribution and on the joint probability of u with the observed and sampled
variables x and y .
We assume an improper prior P (u) that assigns uniform probability mass over the positive reals,
and use a Gaussian proposal distribution with the mean set to the previous value of the parameter and
357

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

variance set to one-tenth of the mean.5 For non-pathological proposal distributions, the MetropolisHastings algorithm is guaranteed to converge in the limit to a stationary Markov chain centered on
the desired joint distribution. We observe an acceptance rate of approximately 1/6, which is in line
with standard recommendations for rapid convergence (Gelman et al., 2004).
3.3.3 F INAL PARAMETER E STIMATES
The ultimate goal of training is to learn models that can be applied to unaligned monolingual data.
Thus, we need to construct estimates for the transition and emission parameters  and . Our
sampling procedure focuses on the tags y. We construct maximum a posteriori estimates y, indicating the most likely tag sequences for the aligned training corpus. The predicted tags y are then
combined with priors 0 and 0 to construct maximum a posteriori estimates of the transition and
emission parameters. These learned parameters are then applied to the monolingual test data to find
the highest probability tag sequences using the Viterbi algorithm.
For the monolingual and merged node models, we perform 200 iterations of sampling, and select
the modal tag settings in each slot. Further sampling was not found to produce different results. For
the latent variable model, we perform 1000 iterations of sampling, and select the modal tag values
from the last 100 samples.

4. Experimental Setup
We perform a series of empirical evaluations to quantify the contribution of bilingual and multilingual information for unsupervised part-of-speech tagging. Our first evaluation follows the standard
procedures established for unsupervised part-of-speech tagging: given a tag dictionary (i.e., a set of
possible tags for each word type), the model selects the appropriate tag for each token occurring in
a text. We also evaluate tagger performance when the available dictionaries are incomplete (Smith
& Eisner, 2005; Goldwater & Griffiths, 2007). In all scenarios, the model is trained using only
untagged text.
In this section, we first describe the parallel data and part-of-speech annotations used for system
evaluation. Next we describe a monolingual baseline and the inference procedure used for testing.
4.1 Data
As a source of parallel data, we use Orwells novel Nineteen Eighty Four in the original English
as well as its translation to seven languages  Bulgarian, Czech, Estonian, Hungarian, Slovene,
Serbian and Romanian.6 Each translation was produced by a different translator and published in
print separately by different publishers.
This dataset has representatives from four language families  Slavic, Romance, Ugric and
Germanic. This data is distributed as part of the publicly available Multext-East corpus, Version 3
(Erjavec, 2004). The corpus provides detailed morphological annotation at the token level, including
part-of-speech tags. In addition, a lexicon for each language is provided.
5. This proposal is identical to the parameter re-estimation applied for emission and transition priors by Goldwater and
Griffiths (2007).
6. In our initial publication (Snyder et al., 2008), we used a subset of this data, only including sentences that have
one-to-one alignments between all four languages considered in that paper. The current set-up makes use of all the
sentences available in the corpus.

358

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Percentage Aligned
Bulgarian (BG)
Czech (CS)
English (EN)
Estonian (ET)
Hungarian (HU)
Romanian (RO)
Slovene (SL)
Serbian (SR)

Sentences
6681
6750
6736
6477
6767
6519
6688
6676

Words
101175
102834
118426
94900
98428
118330
116908
112131

BG

CS

EN

ET

HU

RO

SL

SR

41.0
43.2
35.7
32.2
35.5
39.3
41.4

41.7
36.4
42.4
32.0
27.5
49.4
44.4

50.5
41.9
42.9
39.6
42.5
45.2
43.2

33.5
39.1
34.4
32.6
23.4
36.4
33.6

31.3
30.7
32.9
33.8
22.4
29.1
26.6

41.5
31.7
42.5
29.2
26.9
31.2
33.9

45.4
56.2
44.6
44.8
34.6
30.8
53.4

45.9
48.4
40.9
39.7
30.3
32.1
51.2
-

Table 2: Percentage of the words in the row language that have alignments when paired with the
column language.

The corpus consists of 118,426 English words in 6,736 sentences (see Table 3). Of these sentences, the first 75% are used for training, taking advantage of the multilingual alignments. The
remaining 25% are used for evaluation. In the evaluation, only monolingual information is made
available to the model, to simulate performance on non-parallel data.
4.2 Alignments
In our experiments we use sentence-level alignments provided in the Multext-East corpus. Wordlevel alignments are computed for each language pair using G IZA ++ (Och & Ney, 2003). The
procedures for handling these alignments are different for the merged node and latent variable models.
4.2.1 M ERGED N ODE M ODEL
We obtain 28 parallel bilingual corpora by considering all pairings of the eight languages. To
generate one-to-one alignments at the word level, we intersect the one-to-many alignments going in
each direction. This process results in alignment of about half the tokens in each bilingual parallel
corpus. We further automatically remove crossing alignment edges, as these would induce cycles in
the graphical model. We employ a simple heuristic: crossing alignment edges are removed based
on the order in which they appear from left to right; this step eliminates on average 3.62% of the
edges. Table 2 shows the number of aligned words for each language pair after removing crossing
edges. More detailed statistics about the total number of alignments are provided in Appendix A.
4.2.2 L ATENT VARIABLE M ODEL
As in the previous setting, we run GIZA ++ on all 28 pairings of the 8 languages, taking the intersection of alignments in each direction. Since we want each latent superlingual variable to span as
many languages as possible, we aggregate pairwise lexical alignments into larger sets of densely
aligned words and assign a single latent superlingual variable to each such set. Specifically, for
each word token, we consider the set of the word itself and all word tokens to which it is aligned.
If pairwise alignments occur between 2/3 of all token pairs in this set, then it is considered densely

359

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY




















Figure 2: An example of a multilingual alignment configuration. Nodes correspond to words tokens, and are labeled by their language. Edges indicate pairwise alignments produced
by GIZA ++. Boxes indicate alignment sets, though the set C1 is subsumed by C2 and
eventually discarded, as described in the text.

connected and is admitted as an alignment set. Otherwise, increasingly smaller subsets are considered until one that is densely connected is found. This procedure is repeated for all word tokens
in the corpus that have at least one alignment. Finally, the alignment sets are pruned by removing
those which are subsets of larger alignment sets. Each of the remaining sets is considered the site
of a latent superlingual variable.
This process can be illustrated by an example. The sentence I know you, the eyes seemed to
say, I see through you, appears in the original English version of the corpus. The English word
token seemed is aligned to word tokens in Serbian (cinilo), Estonian (nais), and Slovenian (zdelo).
The Estonian and Slovenian tokens are aligned to each other. Finally, the Serbian token is aligned to
a Hungarian word token (mintha), which is itself not aligned to any other tokens. This configuration
is shown in Figure 2, with the nodes labeled by the two-letter language abbreviations.
We now construct alignment sets for these words.
 For the Hungarian word, there is only one other aligned word, in Serbian, so the alignment
set consists only of this pair (C1 in the figure).
 The Serbian word has aligned partners in both Hungarian and English; overall this set has
two pairwise alignments out of a possible three, as the English and Hungarian words are
not aligned. Still, since 2/3 of the possible alignments are present, an alignment set (C2) is
formed. C1 is subsumed by C2, so it is eliminated.
 The English word is aligned to tokens in Serbian, Estonian, and Slovenian; four of six possible
links are present, so an alignment set (C3) is formed. Note that if the Estonian and Slovenian
words were not aligned to each other then we would have only three of six links, so the set
360

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

would not be densely connected by our definition; we would then remove a member of the
alignment set.
 The Estonian token is aligned to words in Slovenian and English; all three pairwise alignments
are present, so an alignment set (C4) is formed. An identical alignment set is formed by
starting with the Slovenian word, but only one superlingual tag is created.
Thus, for these five word tokens, a total of three overlapping alignment sets are created. Over
the entire corpus, this process results in 284,581 alignment sets, covering 76% of all word tokens.
Of these tokens, 61% occur in exactly one alignment set, 29% occur in exactly two alignment sets,
and the remaining 10% occur in three or more alignment sets. Of all alignment sets, 32% include
words in just two languages, 26% include words in exactly three languages, and the remaining 42%
include words in four or more languages. The sets remain fixed during sampling and are treated by
the model as observed data.

Bulgarian (BG)
Czech (CS)
English (EN)
Estonian (ET)
Hungarian (HU)
Romanian (RO)
Slovene (SL)
Serbian (SR)

Number
of Tokens
101175
102834
118426
94900
98428
118330
116908
112131

Tags per token when lexicon contains ...
all words count > 5 count > 10 top 100 words
1.39
4.61
5.48
7.33
1.35
5.27
6.37
8.24
1.49
3.11
3.81
6.21
1.36
4.91
5.82
7.34
1.29
5.42
6.41
7.85
1.55
4.49
5.53
8.54
1.33
4.59
5.49
7.23
1.38
4.76
5.73
7.61

Trigram
Entropy
1.63
1.64
1.51
1.61
1.62
1.73
1.64
1.73

Table 3: Corpus size and tag/token ratio for each language in the set. The last column shows the
trigram entropy for each language based on the annotations provided with the corpus.

4.3 Tagset
The Multext-East corpus is manually annotated with detailed morphosyntactic information. In our
experiments, we focus on the main syntactic category encoded as the first letter of the provided
labels. The annotation distinguishes between 14 parts-of-speech, of which 11 are common for all
languages in our experiments. Appendix B lists the tag repository for each of the eight languages.
In our first experiment, we assume that a complete tag lexicon is available, so that the set of
possible parts-of-speech for each word is known in advance. We use the tag dictionaries provided
in the Multext-East corpus. The average number of possible tags per token is 1.39. We also experimented with incomplete tag dictionaries, where entries are only available for words appearing more
than five or ten times in the corpus. For other words, the entire tagset of 14 tags is considered. In
these two scenarios, the average per-token tag ambiguity is 4.65 and 5.58, respectively. Finally we
also considered the case when lexicon entries are available for only the 100 most frequent words.
In this case the average tags per token ambiguity is 7.54. Table 3 shows the specific tag/token ratio
for each language for all scenarios.
361

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

In the Multext-East corpus, punctuation marks are not annotated with part-of-speech tags. We
expand the tag repository by defining a separate tag for all punctuation marks. This allows the model
to make use of any transition or coupling patterns involving punctuation marks. However, we do
not consider punctuation tokens when computing model accuracy.
4.4 Monolingual Comparisons
As our monolingual baseline we use the unsupervised Bayesian hidden Markov model (HMM) of
Goldwater and Griffiths (2007). This model, which they call BHMM1, modifies the standard HMM
by adding priors and by performing Bayesian inference. Its performance is on par with state-ofthe-art unsupervised models. The Bayesian HMM is a particularly informative baseline because
our model reduces to this baseline when there are no alignments in the data. This implies that any
performance gain over the baseline can only be attributed to the multilingual aspect of our model.
We used our own implementation after verifying that its performance on the Penn Treebank corpus
was identical to that reported by Goldwater and Griffiths.
To provide an additional point of comparison, we use a supervised hidden Markov model trained
using the annotated corpus. We apply the standard maximum-likelihood estimation and perform inference using Viterbi decoding with pseudo-count smoothing for unknown words (Rabiner, 1989).
In Appendix C we also report supervised results using the Stanford Tagger, version 1.67 . Although the results are slightly lower than our own supervised HMM implementation, we note that
this system is not directly comparable to our set-up, as it does not allow the use of a tag dictionary
to constrain part-of-speech selections.
4.5 Test Set Inference
We use the same procedure to apply all the models (the monolingual model, the bilingual merged
node model, and the latent variable model) to test data. After training, trigram transition and word
emission probabilities are computed, using the counts of tags assigned in the final training iteration.
Similarly, the final sampled values of the hyperparameters are selected as smoothing parameters. We
then apply Viterbi decoding to identify the highest probability tag sequences for each monolingual
test set. We report results for multilingual and monolingual experiments averaged over five runs and
for bilingual experiments averaged over three runs. The average standard-deviation of accuracy over
multiple runs is less than 0.25 except when the lexicon is limited to the 100 most frequent words.
In that case the standard deviation is 1.11 for monolingual model, 0.85 for merged node model and
1.40 for latent variable model.

5. Results
In this section, we first report the performance for the two models on the full and reduced lexicon
cases. Next, we report results for a semi-supervised experiment, where a subset of the languages
have annotated text at training time. Finally, we investigate the sensitivity of both models to hyperparameter values and provide run time statistics for the latent variable model for increasing numbers
of languages.
7. http://nlp.stanford.edu/software/tagger.shtml

362

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

1.
2.
3.
4.
5.
6.
7.

Random
Monolingual
M ERGED N ODE: average
L ATENT VARIABLE
Supervised
M ERGED N ODE: voting
M ERGED N ODE: best pair

Avg
83.3
91.2
93.2
95.0
97.3
93.0
95.4

BG

CS

EN

ET

HU

RO

SL

SR

82.5
88.7
91.3
92.6
96.8
91.6
94.7

86.9
93.9
96.9
98.2
98.6
97.4
97.8

80.7
95.8
95.9
95.0
97.2
96.1
96.1

84.0
92.7
93.3
94.6
97.0
94.3
94.2

85.7
95.3
96.7
96.7
97.8
96.8
96.9

78.2
91.1
91.9
95.1
97.7
91.6
94.1

84.5
87.4
89.3
95.8
97.0
87.9
94.8

83.5
84.5
90.2
92.3
96.6
88.2
94.5

Table 4: Tagging accuracy with complete tag dictionaries. The first column reports average results
across all languages (see Table 3 for language name abbreviations). The latent variable
model is trained using all eight languages, whereas the merged node models are trained on
language pairs. In the latter case, results are given by averaging over all pairings (line 3),
by having all bilingual models vote on each tag prediction (line 6), and by having an oracle
select the best pairing for each target language (line 7). All differences between L ATENTVARIABLE, M ERGED N ODE: voting, and Monolingual (lines 2, 4, and 6) are statistically
significant at p < 0.05 according to a sign test.

5.1 Full Lexicon Experiments
Our experiments show that both the merged node and latent variable models substantially improve
tagging accuracy. Since the merged node model is restricted to pairs of languages, we provide
average results over all possible pairings. In addition, we also consider two methods for combining
predictions from multiple bilingual pairings: one using a voting scheme and the other employing an
oracle to select the best pairings (see below for additional details).
As shown in Line 4 of Table 4, the merged node model achieves, on average, 93.2% accuracy,
a two percentage point improvement over the monolingual baseline.8 The latent variable model 
trained once on all eight languages  achieves 95% accuracy, nearly two percentage points higher
than the bilingual merged node model. These two results correspond to error reductions of 23% and
43% respectively, and reduce the gap between unsupervised and supervised performance by over
30% and 60%.
As mentioned above, we also employ a voting scheme to combine information from multiple
languages using the merged node model. Under this scheme, we train bilingual merged node models
for each language pair. Then, when making tag predictions for a particular language  e.g., Romanian  we consider the preferences of each bilingual model trained with Romanian and a second
language. The tag preferred by a plurality of models is selected. The results for this method are
shown in line 6 of Table 4, and do not differ significantly from the average bilingual performance.
Thus, this simple method of combining information from multiple language does not measure up to
the joint multilingual model performance.
8. The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by Goldwater and
Griffiths (2007) on the WSJ corpus. We attribute this discrepancy to the differences in tag inventory used in our
data-set. For example, when Particles and Prepositions are merged in the WSJ corpus (as they happen to be in our
tag inventory and corpus), the performance of Goldwaters model on WSJ is similar to what we report here.

363

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

improvement over random

25

20

Monolingual
Merged Node
Latent Variable

15

10

5

Full Lexicon

Counts > 5

Counts > 10

Top 100

Figure 3: Summary of model performance in full and reduced lexicon conditions. Improvement
over the random baseline is indicated for the monolingual baseline, the merged node
model (average performance over all possible bilingual pairings), and the latent variable
model (trained on all eight languages). Counts > x indicates that only words with
counts greater than x were kept in the lexicon; Top 100 keeps only the 100 most common words.

We use the sign test to assess whether there are statistically significant differences in the accuracy of the tag predictions made by the monolingual baseline (line 2 of Table 4), the latent variable
model (line 4), and the voting-based merged node model (line 6). All differences in these rows are
found to be statistically significant at p < 0.05. Note that we cannot use the sign test to compare the
average performance of the bilingual model (line 3), since this result is an aggregate over accuracies
for every language pair.
5.2 Reduced Lexicon Experiments
In realistic application scenarios, we may not have a tag dictionary with coverage across the entire lexicon. We consider three reduced lexicons: removing all words with counts of five or less;
removing all words with counts of ten or less; and keeping only the top 100 most frequent words.
Words that are removed from the lexicon can take any tag, increasing the overall difficulty of the
task. These results are shown in Table 5 and graphically summarized in Figure 3. In all cases, the
monolingual model is less robust to reduction in lexicon coverage than the multilingual models. In
the case of the 100 word lexicon, the latent variable model achieves accuracy of 57.9%, compared to
53.8% for the monolingual baseline. The merged node model, on the other hand, achieves a slightly
higher average performance of 59.5%. In the two other scenarios, the latent variable model trained
on all eight languages outperforms the bilingual merged node model, even when an oracle selects
the best bilingual pairing for each target language. For example, using the lexicon with words that
appear greater than five times, the monolingual baseline achieves 74.7% accuracy, the merged node
model using the best possible pairings achieves 81.7% accuracy, and the full latent variable model
achieves an accuracy of 82.8%.

364

fiTop 100

Counts > 10

Counts > 5

M ULTILINGUAL PART- OF -S PEECH TAGGING

Random
Monolingual
M ERGED N ODE: average
L ATENT VARIABLE
M ERGED N ODE: voting
M ERGED N ODE: best pair
Random
Monolingual
M ERGED N ODE: average
L ATENT VARIABLE
M ERGED N ODE: voting
M ERGED N ODE: best pair
Random
Monolingual
M ERGED N ODE: average
L ATENT VARIABLE
M ERGED N ODE: voting
M ERGED N ODE: best pair

Avg
63.6
74.8
80.1
82.8
80.4
81.7
57.9
70.9
77.2
79.7
77.5
79.0
37.3
53.8
59.6
57.9
62.4
63.6

BG

CS

EN

ET

HU

RO

SL

SR

62.9
73.5
80.2
81.3
80.4
82.7
57.5
71.9
77.8
78.8
78.4
80.2
36.7
60.9
60.1
65.5
61.5
64.7

62
72.2
79
83.0
78.5
79.7
54.7
66.7
75.3
79.4
75.3
76.7
32.1
44.1
52.5
49.3
55.4
55.3

71.8
87.3
90.4
88.1
90.7
90.7
68.3
84.4
88.8
86.1
89.2
89.4
48.9
69.0
73.5
71.6
74.8
77.4

61.6
72.5
76.5
80.6
76.4
77.5
56
68.3
72.9
77.9
73.1
74.9
36.6
54.8
59.5
54.3
62.2
61.5

61.3
73.5
77.3
80.8
76.8
78
55.1
69.0
73.8
76.4
73.3
75.2
36.4
56.8
59.4
51.0
60.9
60.2

62.8
77.1
82.7
86.1
84.0
84.4
57.2
73.0
80.5
83.1
81.7
82.1
33.7
51.4
61.4
57.5
64.3
69.3

64.8
75.7
78.7
83.6
79.7
80.9
59.2
70.4
76.1
80.0
76.1
77.6
39.8
49.4
56.6
53.9
62.3
63.1

61.8
66.3
75.9
78.8
76.4
79.4
55.5
63.7
72.4
75.9
73.1
76.1
33.8
44.0
53.4
60.4
57.5
56.9

Table 5: Tagging accuracy in reduced lexicon conditions. Counts > x indicates that only words
with counts greater than x were kept in the lexicon; Top 100 keeps only the 100 most
common words. The latent variable model is trained using all eight languages, whereas
the merged node models are trained on language pairs. In the latter case, results are given
by averaging over all pairings, by having all bilingual models vote on each tag prediction,
and by having an oracle select the best pairing for each target language. Other than the
three pairs of results marked with , , and , all differences between monolingual,
L ATENT VARIABLE, and M ERGED N ODE: voting are statistically significant at p <
0.05 according to a sign test.

Next we consider the performance of the bilingual merged node model when the lexicon is
reduced for only one of the two languages. This condition may occur when dealing with two languages with asymmetric resources, in terms of unannotated text. As shown in Table 6, the merged
models on average scores 5.7 points higher than the monolingual model when both tag dictionaries are reduced, but 14.3 points higher when the partner language has a full tag dictionary. This
suggests that the bilingual models effectively transfer the additional lexical information available
for the resource-rich language to the resource-poor language, yielding substantial performance improvements.
Perhaps the most surprising result is that the resource-rich language gains as much on average
from pairing with the resource-poor partner language as it would have gained from pairing with a
language with a full lexicon. In both cases, an average accuracy of 93.2% is achieved, compared to
the 91.1% monolingual baseline.

365

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Monolingual
Reduced Full
60.9
88.7
44.1
93.9
69.0
95.8
54.8
92.7
56.8
95.3
51.4
91.1
49.4
87.4
44.0
84.5
53.8
91.2

BG
CS
EN
ET
HU
RO
SL
SR

Avg.

Bilingual (Merged Node)
Reduced language Unreduced language
71.3
91.6
66.7
97.1
82.4
95.8
65.6
93.3
63.0
96.7
69.3
91.5
63.3
89.1
63.6
90.3
68.1
93.2

Both reduced
60.1
52.5
73.5
59.5
59.4
61.4
56.6
53.4
59.5

Both full
91.3
96.9
95.9
93.3
96.7
91.9
89.3
90.2
93.2

Table 6: Various scenarios for reducing the tag dictionary to the 100 most frequent terms.

5.3 Indirect Supervision
Although the main focus of this paper is unsupervised learning, we also provide some results indicating that multilingual learning can be applied to scenarios with varying amounts of annotated
data. These scenarios are in fact quite realistic, as previously trained and highly accurate taggers
will usually be available for at least some of the languages in a parallel corpus. We apply our latent
variable model to these scenarios by simply treating the tags of annotated data (in any subset of
languages) as fixed and observed throughout the sampling procedure. From a strictly probabilistic
perspective this is the correct approach. However, we note that, in practice, heuristics and objective functions which place greater emphasis on the supervised portion of the data may yield better
results. We do not explore that possibility here.
supervised language(s)...
BG

accuracy for...

BG
CS
EN
ET
HU
RO
SL
SR

Avg

50.8
62.6
57.2
50.3
62.8
55.0
64.9
57.7

CS

EN

ET

HU

RO

SL

SR

All others

None

69.1

68.0
52.2

65.9
50.2
68.1

60.4
51.2
61.8
56.1

67.1
51.0
61.9
56.4
51.1

73.9
56.6
80.6
59.8
49.8
62.9

69.6
53.1
69.5
57.1
50.0
59.2
56.2

76.2
76.6
82.8
72.5
62.3
74.9
77.7
72.5
74.4

65.5
49.3
71.6
54.3
51.0
57.5
53.9
60.4
57.9

70.5
58.0
50.0
61.6
56.8
65.9
61.7

57.7
53.1
61.3
55.6
64.1
58.9

51.4
57.8
53.2
63.5
58.6

58.5
54.4
61.6
57.7

54.7
63.4
57.9

69.9
64.8

59.2

Table 7: Performance of the latent variable model when some of the eight languages have supervised annotations and the others have only the most frequent 100 words lexicon. The
first eight columns report results when only one of the eight languages is supervised. The
penultimate column reports results when all but one of the languages are supervised. The
final column reports results when no supervision is available (repeated from Table 5 for
convenience).

366

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Table 7 gives results for two scenarios of indirect supervision: where only one of the eight
languages has annotated data, and where all but one of the languages has annotated data. In both
cases, the unsupervised languages are provided with a 100 word lexicon, and all eight languages are
trained together. When only one of the eight languages is supervised, the results vary depending on
the choice of supervised language. When one of Bulgarian, Hungarian, or Romanian is supervised,
no improvement is seen, on average, for the other seven languages. However, when Slovene is supervised, the improvement seen for the other languages is fairly substantial, with average accuracy
rising to 64.8%, from 57.9% for the unsupervised latent variable model and 53.8% for the monolingual baseline. Perhaps unsurprisingly, the results are more impressive when all but one of the
languages is supervised. In this case, the average accuracy of the lone unsupervised language rises
to 74.4%. Taken together, these results indicate that any mixture of supervised resources may be
added to the mix in a very simple and straightforward way, often yielding substantial improvements
for the other languages.
5.4 Hyperarameter Sensitivity and Runtime Statistics
Both models employ hyperparameters for the emission and transition distribution priors (0 and 0
respectively) and the merged node model employs an additional hyperparameter for the coupling
distribution prior (0 ). These hyperparameters are all updated throughout the inference procedure.
The latent variable model uses two additional hyperparameters that remained fixed: the concentration parameter of the Dirichlet process () and the parameter of the base distribution for superlingual tags (0 ). For the experiments described above we used the initialization values listed in
Section 3.3.1. Here we investigate the sensitivity of the models to different initializations of 0 ,
0 , and 0 , and to different fixed values of  and 0 . Tables 8 and 9 show the results obtained
for the merged node and latent variable models, respectively, using a full lexicon. We observe that
across a wide range of values, both models yield very similar results. In addition, we note that the
final sampled hyperparameter values for transition and emission distributions always fall below one,
indicating that sparse priors are preferred.
As mentioned in Section 3.2 one of the key theoretical benefits of the latent variable approach is
that the size of the model and its parameter space scale linearly with the number of languages. Here
we provide empirical confirmation by running the latent variable model on all possible subsets of
the eight languages, recording the time elapsed for each run9 . Figure 4 shows the average running
time as the number of languages is increased (averaged over all subsets of each size). We see that the
model running time indeed scales linearly as languages are added, and that the per-language running
time increases very slowly: when all eight languages are included, the time taken is roughly double
that for eight monolingual models run serially. Both of our models scale well with tagset size and
the number of examples. The time dependence on the former is cubic, as we use trigram models and
employ Viterbi decoding to find optimal sequences at test-time. During the training time, however,
the time scales linearly with the tagset size for the latent variable model and quadratically for the
merged node model. This is due to the use of Gibbs sampling that isolates the individual sampling
decision on tags (for the latent variable model) and tag-pairs (for the merged node model). The
dependence on the number of training examples is also linear for the same reason.
9. All experiments were single-threaded and run using an Intel Xeon 3.0 GHz processor

367

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

M ERGED N ODE: hyperparameter initializations

0
0
0
BG
CS
EN
ET
HU
RO
SL
SR

Avg

1.0
1.0
1.0
91.3
96.9
95.9
93.3
96.7
91.9
89.3
90.2
93.2

0.1
1.0
1.0
91.3
97.0
95.9
93.4
96.7
91.8
89.3
90.2
93.2

0.01
1.0
1.0
91.3
97.0
95.9
93.3
96.7
91.8
89.3
90.2
93.2

1.0
0.1
1.0
91.3
96.9
95.9
93.4
96.7
91.9
89.3
90.2
93.2

1.0
0.01
1.0
91.2
96.8
95.9
93.2
96.7
91.8
89.4
90.2
93.2

1.0
1.0
0.1
91.1
96.5
95.9
93.4
96.7
91.8
89.3
90.2
93.1

1.0
1.0
0.01
91.3
97.1
95.9
93.2
96.8
91.8
89.3
90.2
93.2

Table 8: Results for different initializations of the hyperparameters of the merged node model. 0 ,
0 and 0 are the hyperparameters for the transition, emission and coupling multinomials
respectively. The results for each language are averaged over all possible pairings with the
other languages.

L ATENT VARIABLE : hyperparameter initializations & settings


0
0
0
BG
CS
EN
ET
HU
RO
SL
SR

Avg

1.0
1.0
1.0
1.0
92.6
98.2
95.0
94.6
96.7
95.1
95.8
92.3
95.0

0.1
1.0
1.0
1.0
92.6
98.1
95.0
95.0
96.7
95.0
95.8
92.3
95.1

10
1.0
1.0
1.0
92.6
98.2
94.9
95.0
96.7
95.1
95.8
92.3
95.1

100
1.0
1.0
1.0
92.6
98.2
94.8
94.9
96.7
95.1
95.8
92.3
95.0

1.0
0.1
1.0
1.0
92.6
98.1
95.1
94.2
96.7
95.2
95.8
92.4
95.0

1.0
0.01
1.0
1.0
92.7
98.1
95.2
94.8
96.6
95.1
95.8
92.4
95.1

1.0
1.0
0.1
1.0
92.6
98.2
95.0
95.0
96.7
95.0
95.8
92.3
95.1

1.0
1.0
0.01
1.0
92.6
98.1
94.9
94.9
96.7
94.9
95.8
92.3
95.0

1.0
1.0
1.0
0.1
92.6
98.2
94.9
94.9
96.7
95.1
95.8
92.3
95.1

1.0
1.0
1.0
0.01
92.6
98.1
95.0
94.5
96.7
95.0
95.8
92.3
95.0

Table 9: Results for different initializations and settings of hyperparameters of the latent variable
model. 0 and 0 are the hyperparameters for the transition and emission multinomials respectively and are updated throughout inference.  and 0 are the concentration parameter
and base distribution parameter, respectively, for the Dirichlet process, and remain fixed.

6. Analysis
In this section we provide further analysis of: (i) factors that influence the effectiveness of language
pairings in bilingual models, (ii) the incremental value of adding more languages in the latent vari368

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Figure 4: Average running time for 1000 iterations of the latent variable model. Results are averaged over all possible language subsets of each size. The top line shows the average
running time for the entire subset, and the bottom line shows the running time divided by
the number of languages.

able model, (iii) the superlingual tags and their corresponding cross-lingual patterns as learned by
the latent variable model, and (iv) whether multilingual data is more helpful than additional monolingual data. We focus here on the full lexicon scenario, though we expect our analysis to extend to
the various reduced lexicon cases considered above as well.
6.1 Predicting Effective Language Pairings
We first analyze the cross-lingual variation in performance for different bilingual language pairings.
As shown in Table 10, the performance of the merged node model for each target language varies
substantially across pairings. In addition, the identity of the optimally helpful language pairing
also depends heavily on the target language in question. For instance, Slovene, achieves a large
improvement when paired with Serbian (+7.4), a closely related Slavic language, but only a minor improvement when coupled with English (+1.8). On the other hand, for Bulgarian, the best
performance is achieved when coupling with English (+6) rather than with closely related Slavic
languages (+2.4 and +0). Thus, optimal pairings do not correspond simply to language relatedness.
We note that when applying multilingual learning to morphological segmentation the best results
were obtained for related languages, but only after incorporating declarative knowledge about their
lower-level phonological relations using a prior which encourages phonologically close aligned
morphemes (Snyder & Barzilay, 2008). Here too, a more complex model which models lower-level
morphological relatedness (such as case) may yield better outcomes for closely related languages.
As an upper bound on the merged node model performance, line 7 of Table 10 shows the results
when selecting (with the help of an oracle) the best partner for each language. The average accuracy using this oracle is 95.4%, substantially higher than the average bilingual pairing accuracy of
93.2%, and even somewhat higher than the latent variable model performance of 95%. This gap in

369

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

performance motivates a closer examination of the relationship between languages that constitute
effective pairings.
M ERGED N ODE M ODEL

coupled with...

accuracy for...

BG
CS
EN
ET
HU
RO
SL
SR

Avg
91.3
96.9
95.9
93.3
96.7
91.9
89.3
90.2

BG

95.3
96.1
93.0
96.8
94.1
88.5
88.5

CS

EN

ET

HU

RO

SL

SR

90.2

94.7
97.5

92.3
97.8
95.8

90.6
96.3
95.8
92.2

91.2
96.4
95.8
93.0
96.8

91.1
97.4
96.1
94.2
96.5
91.3

88.7
97.4
96.0
93.9
96.7
93.9
94.8

95.9
94.0
96.6
90.6
88.1
88.2

92.9
96.8
92.0
89.2
94.5

96.9
91.3
89.8
94.2

90.3
87.5
89.5

87.5
85.0

91.4

Table 10: Merged node model accuracy for all language pairs. Each row corresponds to the performance of one language, each column indicates the language with which the performance
was achieved. The best result for each language is indicated in bold. All results other than
those marked with a  are significantly higher than the monolingual baseline at p < 0.05
according to a sign test.

6.1.1 C ROSS - LINGUAL E NTROPY
In a previous publication (Snyder et al., 2008) we proposed using cross-lingual entropy as a posthoc explanation for variation in coupling performance. This measure calculates the entropy of a
tagging decision in one language given the identity of an aligned tag in the other language. While
cross-lingual entropy seemed to correlate well with relative performance for the four languages
considered in that publication, we find that it does not correlate as strongly for all eight languages
considered here. We computed the Pearson correlation coefficient (Myers & Well, 2002) between
the relative bilingual performance and cross-lingual entropy. For each target language, we rank the
remaining seven languages based on two measures: how well the paired language contributes to
improved performance of the target, and the cross-lingual entropy of the target language given the
coupled language. We compute the Pearson correlation coefficient between these two rankings to
assess their degree of overlap. See Table 19 in the Appendix for a complete list of results. On
average, the coefficient was 0.29, indicating a weak positive correlation between relative bilingual
performance and cross-lingual entropy.
6.1.2 A LIGNMENT D ENSITY
We note that even if cross-lingual entropy had exhibited higher correlation with performance, it
would be of little practical utility in an unsupervised scenario since its estimation requires a tagged
corpus. Next we consider the density of pairwise lexical alignments between language pairs as
a predictive measure of their coupled performance. Since alignments constitute the multilingual
anchors of our models, as a practical matter greater alignment density should yield greater opportunities for cross-lingual transfer. From the linguistic viewpoint, this measure may also indirectly
370

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

capture the correspondence between two languages. Moreover, this measure has the benefit of being computable from an untagged corpus, using automatically obtained GIZA ++ alignments. As
before, for each target language, we rank the other languages by relative bilingual performance, as
well as by the percentage of words in the target language to which they provide alignments. Here
we find an average Pearson coefficient of 0.42, indicating mild positive correlation. In fact, if we
use alignment density as a criterion for selecting optimal pairing decisions for each target language,
we obtain an average accuracy of 94.67%  higher than average bilingual performance, but still
somewhat below the performance of the multilingual model.
6.1.3 M ODEL C HOICE
The choice of model may also contribute to the patterns of variability we observe across language
pairs. To test this hypothesis, we ran our latent variable model on all pairs of languages. The results
of this experiment are shown in Table 11. As in the case of the merged node model, the performance
of each target language depends heavily on the choice of partner. However, the exact patterns of
variability differ in this case from those observed for the merged node model. To measure this variability, we compare the pairing preferences for each language under each of the two models. More
specifically, for each target language we rank the remaining seven languages by their contribution
under each of our two models, and compute the Pearson coefficient between these two rankings. As
seen in the last column of Table 19 in the Appendix, we find a coefficient of 0.49 between the two
rankings, indicating positive, though far from perfect, correlation.
L ATENT VARIABLE M ODEL

coupled with...

accuracy for...

BG
CS
EN
ET
HU
RO
SL
SR

Avg
91.9
97.2
95.7
93.9
96.8
93.2
90.5
91.6

BG

97.5
95.7
94.8
97.0
94.6
88.6
94.7

CS

EN

ET

HU

RO

SL

SR

92.2

91.9
97.5

91.6
97.6
95.7

91.6
97.4
95.6
92.3

92.1
97.4
95.7
93.9
96.8

92.3
96.5
95.7
94.5
96.6
94.4

91.8
96.8
95.8
94.1
96.8
94.7
94.6

95.7
94.3
96.8
92.1
87.7
88.5

93.4
96.7
92.4
92.4
94.5

96.7
92.3
95.2
94.5

92.1
87.5
89.7

87.6
88.0

91.1

Table 11: Accuracy of latent variable model when run on language pairs. Each row corresponds
to the performance of one language, each column indicates the language with which the
performance was achieved. The best result for each language is indicated in bold. All
results other than those marked with a  are significantly higher than the monolingual
baseline at p < 0.05 according to a sign test.

6.1.4 U TILITY

OF EACH

L ANGUAGE AS A B ILINGUAL PARTNER

We also analyze the overall helpfulness of each language. As before, for each target language, we
rank the remaining seven languages by the degree to which they contribute to increased target language performance when paired in a bilingual model. We can then ask whether the helpfulness
371

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

rankings provided by each of the eight languages are correlated with one another  in other words,
whether languages tend to be universally helpful (or unhelpful) or whether helpfulness depends
heavily on the identity of the target language. We consider all pairs of target languages, and compute the Pearson rank correlation between their rankings of the six supplementary languages that
they have in common (excluding the two target languages themselves). When we average these pairwise rank correlations we obtain a coefficient of 0.20 for the merged node model and 0.21 for the
latent variable model. These low correlations indicate that language helpfulness depends crucially
on the target language in question. Nevertheless, we can still compute the average helpfulness of
each language (across all target languages) to obtain something like a universal helpfulness ranking. See Table 20 in the appendix for this ranking. We can then ask whether this ranking correlates
with language properties which might be predictive of general helpfulness. We compare the universal helpfulness rankings10 to language rankings induced by tag-per-token ambiguity (the average
number of tags allowed by the dictionary per token in the corpus) as well as trigram entropy (the
entropy of the tag distribution given the previous two tags). In both cases we assign the highest
rank to the language with lowest value, as we expect lower entropy and ambiguity to correlate with
greater helpfulness. Contrary to expectations, the ranking induced by tag-per-token ambiguity actually correlates negatively with both universal helpfulness rankings by very small amounts (-0.28 for
the merged node model and -0.23 for the latent variable model). For both models, Hungarian, which
has the lowest tag-per-token ambiguity of all eight languages, had the worst universal helpfulness
ranking. The correlations with trigram entropy were only a little more predictable. In the case
of the latent variable model, there was no correlation at all between trigram entropy and universal
helpfulness (-0.01). In the case of the merged node model, however, there was moderate positive
correlation (0.43).
6.2 Adding Languages in the Latent Variable Model
While bilingual performance depends heavily on the choice of language pair, the latent variable
model can easily incorporate all available languages, obviating the need for any choice. To test
performance as the number of languages increases, we ran the latent variable model with all possible subsets of the eight languages in the full lexicon as well as all three reduced lexicon scenarios.
Figures 5, 6, 7, and 8 plot the average accuracy as the number of available languages varies for all
four lexicon scenarios (in decreasing order of the lexicon size). For comparison, the monolingual
and average bilingual baseline results are given. In all scenarios, our latent variable model steadily
gains in accuracy as the number of available languages increases, and in most scenarios sees an
appreciable uptick when going from seven to eight languages. In the full lexicon case, the gap between supervised and unsupervised performance is cut by nearly two thirds under the unsupervised
latent variable model with all eight languages.
Interestingly, as the lexicon is reduced in size, the performance of the bilingual merged node
model gains relative to the latent variable model on pairs. In the full lexicon case, the latent variable
model is clearly superior, whereas in the two moderately reduced lexicon cases, the performance
on pairs is more or less the same for the two models. In the case of the drastically reduced lexicon
10. We note that the universal helpfulness rankings obtained from each of the two multilingual models match each other
only roughly: their correlation coefficient with one another is 0.50. In addition, universal in this context refers only
to the eight languages under consideration and the rankings could very well change in a wider multilingual context.

372

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Figure 5: The performance of the latent variable model as the number of languages varies (averaged
over all subsets of the eight languages for each size). LEFT: Average performance across
all languages. Scores for monolingual and bilingual merged node models are given for
comparison. RIGHT: The Performance for each individual language as the number of
available languages varies.

(100 words), the merged node model is the clear winner. Thus, it seems that of the two models, the
performance gains of the latent variable model are more sensitive to the size of the lexicon.
The same four figures (5, 6, 7, and 8) also show the multilingual performance broken down by
language. All languages except for English tend to increase in accuracy as additional languages are
added to the mix. Indeed, in the two cases of moderately reduced lexicons (Figures 6 and 7) all languages except for English show steady large gains which actually increase in size when going from
seven to the full set of eight languages. In the full lexicon case (Figure 5), Estonian, Romanian, and
Slovene display steady increases until the very end. Hungarian peaks at two languages, Bulgarian
at three languages, and Czech and Serbian at seven languages. In the more drastic reduced lexicon
case (Figure 8), the performance across languages is less consistent and the gains when languages
are added are less stable. All languages report gains when going from one to two languages, but
only half of them increase steadily up to eight languages. Two languages seem to trend downward
after two or three languages, and the other two show mixed behavior.
In the full lexicon case (Figure 5), English is the only language which fails to improve. In the
other scenarios, English gains initially but these gains are partially eroded when more languages are
added. It is possible that English is an outlier since it has significantly lower tag transition entropy
than any of the other languages (see Table 3). Thus it may be that internal tag transitions are simply
more informative for English than any information that can be gleaned from multilingual context.

373

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Figure 6: The performance of the latent variable model for the reduced lexicon scenario (Counts >
5), as the number of languages varies (averaged over all subsets of the eight languages
for each size). LEFT: Average performance across all languages. Scores for monolingual
and bilingual merged node models are given for comparison. RIGHT: The Performance
for each individual language as the number of available languages varies.

Figure 7: The performance of the latent variable model for the reduced lexicon scenario (Counts >
10), as the number of languages varies (averaged over all subsets of the eight languages
for each size). LEFT: Average performance across all languages. Scores for monolingual
and bilingual merged node models are given for comparison. RIGHT: The Performance
for each individual language as the number of available languages varies.

374

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Figure 8: The performance of the latent variable model for the reduced lexicon scenario (100
words), as the number of languages varies (averaged over all subsets of the eight languages for each size). LEFT: Average performance across all languages. Scores for
monolingual and bilingual merged node models are given for comparison. RIGHT: The
Performance for each individual language as the number of available languages varies.

6.3 Analysis of the Superlingual Tag Values
In this section we analyze the superlingual tags and their corresponding part-of-speech distributions,
as learned by the latent variable model. Recall that each superlingual tag intuitively represents
a discovered multilingual context and that it is through these tags that multilingual information
is propagated. More formally, each superlingual tag provides a complete distribution over partsof-speech for each language, allowing the encoding of both primary and secondary preferences
separately for each language. These preferences then interact with the language-specific context
(i.e. the surrounding parts-of-speech and the corresponding word). We place a Dirichlet process
prior on the superlingual tags, so the number of sampled values is dictated by the complexity of the
data. In fact, as shown in Table 12, the number of sampled superlingual tags steadily increases with
the number of languages. As multilingual contexts becomes more complex and diverse, additional
superlingual tags are needed.
Number of languages
Number of superlingual tag values

2
11.07

3
12.57

4
13.87

5
15.07

6
15.79

7
16.13

8
16.50

Table 12: Average number of sampled superlingual tag values as the number of languages increases.

Next we analyze the part-of-speech tag distributions associated with superlingual tag values.
Most superlingual tag values correspond to low entropy tag distributions, with a single dominant
part-of-speech tag across all languages. See, for example, the distributions associated with superlin375

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

gual tag value 6 in Table 13, all of which favor nouns by large margins. Similar sets of distributions
occur favoring verbs, adjectives, and the other primary part-of-speech categories. In fact, among the
seventeen sampled superlingual tag values, nine belong to this type, and they cover 80% of actual
superlingual tag instances. The remaining superlingual tags correspond to more complex crosslingual patterns. The associated tag distributions in those cases favor different part-of-speech tags
in various languages and tend to have higher entropy, with the probability mass spread more evenly
over two or three tags. One such example is the set of distributions associated with the superlingual
tag value 14 in Table 13, which seems to be a mixed noun/verb class. In six out of eight languages
the most favored tag is verb, while a strong secondary choice in these cases is noun. However,
for Estonian and Hungarian, this preference is reversed, with nouns being given higher probability.
This superlingual tag may have captured the phenomenon of light verbs, whereby verbs in one
language correspond to a combination of a noun and verb in another language. For example the English verb whisper/V, when translated into Urdu, becomes the collocation whisper/N do/V. In these
cases, verbs and nouns will often be aligned to one another, requiring a more complex superlingual
tag. The analysis of these examples shows that the superlingual tags effectively learns both simple
and complex cross-lingual patterns

EN
ET
HU
RO
SL
SR

P (N ) = 0.91,
P (N ) = 0.92,
P (N ) = 0.97,
P (N ) = 0.91,
P (N ) = 0.85,
P (N ) = 0.90,
P (N ) = 0.94,
P (N ) = 0.92,

P (A) = 0.04,
P (A) = 0.03,
P (V ) = 0.00,
P (V ) = 0.03,
P (A) = 0.06,
P (A) = 0.04,
P (A) = 0.03,
P (A) = 0.03,

...
...
...
...
...
...
...
...

BG

14

CS

CS

TAG VALUE

TAG VALUE

6

BG

EN
ET
HU
RO
SL
SR

P (V ) = 0.66,
P (V ) = 0.60,
P (V ) = 0.55,
P (N ) = 0.52,
P (N ) = 0.44,
P (V ) = 0.45,
P (V ) = 0.55,
P (V ) = 0.49,

P (N ) = 0.21,
P (N ) = 0.22,
P (N ) = 0.25,
P (V ) = 0.29,
P (V ) = 0.34,
P (N ) = 0.33,
P (N ) = 0.24,
P (N ) = 0.26,

...
...
...
...
...
...
...
...

Table 13: Part-of-speech tag distributions associated with two superlingual latent tag values. Probabilities of only the two most probable tags for each language are shown.

6.3.1 P ERFORMANCE WITH R EDUCED DATA
One potential objection to the claims made in this section is that the improved results may be due
merely to the addition of more data, so that the multilingual aspect of the model may be irrelevant.
We test this idea by evaluating the monolingual, merged node, and latent variable systems on training sets in which the number of examples is reduced by half. The multilingual models in this setting
have access to exactly half as much data as the monolingual model in the original experiment. As
shown in Table 14, both the monolingual baseline and our models are quite insensitive to this drop
in data. In fact, both of our models, when trained on half of the corpus, still outperform the monolingual model trained on the entire corpus. This indicates that the performance gains demonstrated
by multilingual learning cannot be explained merely by the addition of more data.

376

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

M ONOLINGUAL: full data
M ONOLINGUAL: half data
M ERGED N ODE: (avg.) full data
M ERGED N ODE: (avg.) half data
L ATENT VARIABLE: full data
L ATENT VARIABLE: half data

Avg
91.2
91.0
93.2
93.0
95.0
94.7

BG

CS

EN

ET

HU

RO

SL

SR

88.7
88.8
91.3
91.1
92.6
92.6

93.9
93.8
96.9
96.6
98.2
97.8

95.8
95.7
95.9
95.7
95.0
94.7

92.7
92.6
93.3
92.7
94.6
93.9

95.3
95.3
96.7
96.7
96.7
96.7

91.1
90.2
91.9
92.0
95.1
94.4

87.4
87.5
89.3
88.9
95.8
95.4

84.5
84.5
90.2
89.9
92.3
92.2

Table 14: Tagging accuracy on reduced training dataset, with complete tag dictionaries; results
on the full training dataset are repeated here for comparison. The first column reports
average results across all languages (see Table 3 for language name abbreviations).

7. Conclusions
The key hypothesis of multilingual learning is that by combining cues from multiple languages, the
structure of each becomes more apparent. We considered two ways of applying this intuition to the
problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a
pair of languages into a single sequence and a second model which instead incorporates multilingual
context using latent variables.
Our results demonstrate that by incorporating multilingual evidence we can achieve impressive
performance gains across a range of scenarios. When a full lexicon is available, our two models
cut the gap between unsupervised and supervised performance by nearly one third (merged node
model, averaged over all pairs) and two thirds (latent variable model, using all eight languages). For
all but one language, we observe performance gains as additional languages are added. The sole
exception is English, which only gains from additional languages in reduced lexicon settings.
In most scenarios, the latent variable model achieves better performance than the merged node
model, and has the additional advantage of scaling gracefully with the number of languages. These
observations suggest that the non-parametric latent variable structure provides a more flexible paradigm
for incorporating multilingual cues. However, the benefit of the latent variable model relative to the
merged node model (even when running both models on pairs of languages) seems to decrease with
the size of the lexicon. Thus, in practical scenarios where only a small lexicon or no lexicon is
available, the merged node model may represent a better choice.
Our experiments have shown that performance can vary greatly depending on the choice of
additional languages. It is difficult to predict a priori which languages constitute good combinations.
In particular, language relatedness itself cannot be used as a consistent predictor as sometimes
closely related languages constitute beneficial couplings and sometimes unrelated languages are
more helpful. We identify a number of features which correlate with bilingual performance, though
we observe that these features interact in complex ways. Fortunately, our latent variable model
allows us to bypass this question by simply using all available languages.
In both of our models, lexical alignments play a crucial role as they determine the typology
of the model for each sentence. In fact, we observed a positive correlation between alignment
density and bilingual performance, indicating the importance of high quality alignments. In our
experiments, we considered the alignment structure an observed variable, produced by standard MT

377

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

tools which operate over pairs of languages. An interesting alternative would be to incorporate
alignment structure into the model itself, to find alignments best tuned for tagging accuracy based
on the evidence of multiple languages rather than pairs.
Another limitation of the two models is that they only consider one-to-one lexical alignments.
When pairing isolating and synthetic languages11 it would be beneficial to align short analytical
phrases consisting of multiple words to single morpheme-rich words in the other language. To
do so would involve flexibly aligning and chunking the parallel sentences throughout the learning
process.
An important direction for future work is to incorporate even more sources of multilingual
information, such as additional languages and declarative knowledge of their typological properties
(Comrie, 1989). In this paper we showed that performance improves as the number of languages
increases. We were limited by our corpus to eight languages, but we envision future work on
massively parallel corpora involving dozens of languages as well as learning from languages with
non-parallel data.

Bibliographic Note
Portions of this work were previously presented in two conference publications (Snyder, Naseem,
Eisenstein, & Barzilay, 2008, 2009). The current article extends this work in several ways, most
notably: we present a new inference procedure for the merged node model which yields improved
results (Section 3.1.2) and we conduct extensive new empirical analyses of the multilingual results.
More specifically, we analyze properties of language combinations that contribute to successful
multilingual learning, we show that adding multilingual data provides much greater benefit than
increasing the quantity of monolingual data, we investigate additional scenarios of lexicon reduction, we investigate scalability as a function of the number of languages, and we experiment with
semi-supervised settings (Sections 5 and 6).

Acknowledgments
The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168 and grants IIS-0835445, IIS-0904684) and the Microsoft Research Faculty Fellowship. Thanks to Michael Collins, Tommi Jaakkola, Amir Globerson, Fernando Pereira, Lillian Lee,
Yoong Keok Lee, Maria Polinsky and the anonymous reviewers for helpful comments and suggestions. Any opinions, findings, and conclusions or recommendations expressed above are those of
the authors and do not necessarily reflect the views of the NSF.
11. Isolating languages are those with a morpheme to word ratio close to one, and synthetic languages are those which
allow multiple morphemes to be easily combined into single words. English is an example of an isolating language,
whereas Hungarian is a synthetic language.

378

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Appendix A. Alignment Statistics
BG
BG
CS
EN
ET
HU
RO
SL
SR

42163
51098
33849
31673
42017
45969
46434

CS

EN

ET

HU

RO

SL

SR

42163

51098
43067

33849
40207
40746

31673
31537
39012
32056

42017
32559
50289
27709
26455

45969
57789
52869
42499
34072
36442

46434
49740
48394
37681
29797
38004
59865

43067
40207
31537
32559
57789
49740

40746
39012
50289
52869
48394

32056
27709
42499
37681

26455
34072
29797

36442
38004

59865

Table 15: Number of alignments per language pair

BG
BG
CS
EN
ET
HU
RO
SL
SR

2.77
6.13
3.36
4.04
4.52
2.95
3.48

CS

EN

ET

HU

RO

SL

SR

2.77

6.13
3.67

3.36
1.92
4.35

4.04
2.73
6.12
2.88

4.52
3.61
5.59
3.88
4.13

2.95
2.59
3.54
2.44
3.09
3.78

3.48
2.64
3.86
2.21
3.06
3.92
4.11

3.67
1.92
2.73
3.61
2.59
2.64

4.35
6.12
5.59
3.54
3.86

2.88
3.88
2.44
2.21

4.13
3.09
3.06

3.78
3.92

4.11

Avg.
3.89
2.85
4.75
3.01
3.72
4.20
3.22
3.33

Table 16: Percentage of alignments removed per language pair

379

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Appendix B. Tag Repository

Adjective
Conjunction
Determiner
Interjection
Numeral
Noun
Pronoun
Particle
Adverb
Adposition
Article
Verb
Residual
Abbreviation

BG

CS

EN

ET

HU

RO

SL

SR

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x
x
x
x

Table 17: Tag repository for each language

Appendix C. Stanford Tagger Performance
Language
BG
CS
EN
ET
HU
RO
SL
SR

Avg.

Accuracy
96.1
97.2
97.6
97.1
96.3
97.6
96.6
95.5
96.7

Table 18: Performance of the (supervised) Stanford tagger for the full lexicon scenario

380

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Appendix D. Rank Correlation

Language
BG
CS
EN
ET
HU
RO
SL
SR

Avg.
Language
BG
CS
EN
ET
HU
RO
SL
SR

Avg.

Performance correlates for M ERGED N ODE model
Cross-lingual entropy Alignment density L ATENT VARIABLE performance
-0.29
0.09
-0.09
0.39
0.34
0.24
0.28
0.77
0.42
0.46
0.56
0.56
0.31
-0.02
0.29
0.34
0.83
0.89
0.59
0.66
0.95
0.21
0.13
0.63
0.29
0.42
0.49
Performance correlates for L ATENT VARIABLE model
Cross-lingual entropy Alignment density
M ERGED N ODE performance
0.58
0.44
-0.09
-0.40
-0.44
0.24
0.67
0.41
0.42
0.14
0.32
0.56
-0.14
-0.72
0.29
0.04
0.68
0.89
0.57
0.54
0.95
0.18
0.10
0.68
0.21
0.17
0.49

Table 19: Pearson correlation coefficients between bilingual performance on the target language
and various rankings of the supplementary language. For both models and for each target language, we obtain a ranking over all supplementary languages based on bilingual
performance in the target language. These rankings are then correlated with other characteristics of the bilingual pairing: cross-lingual entropy (the entropy of tag distributions
in the target language given aligned tags in the supplementary language); alignment
density (the percentage of words in the target language aligned to words in the auxiliary
language); and performance in the alternative model (target language performance when
paired with the same supplementary language in the alternative model).

381

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Appendix E. Universal Helpfulness
M ERGED N ODE model
ET
2.43
EN
2.57
SL
3.14
BG
3.43
SR
3.43
RO
4.71
CS
5.00
HU
5.71

L ATENT VARIABLE model
BG
1.86
SR
3.00
ET
3.14
CS
3.71
EN
3.71
SL
3.71
RO
4.14
HU
6.00

Table 20: Average helpfulness rank for each language under the two models

382

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

References
Baker, J. (1979). Trainable grammars for speech recognition. In Proceedings of the Acoustical
Society of America.
Banko, M., & Moore, R. C. (2004). Part-of-speech tagging in context. In Proceedings of the
COLING, pp. 556561.
Bertoldi, N., Barbaiani, M., Federico, M., & Cattoni, R. (2008). Phrase-based statistical machine
translation with pivot languages. In International Workshop on Spoken Language Translation
Evaluation Campaign on Spoken Language Translation (IWSLT), pp. 143149.
Bhattacharya, I., Getoor, L., & Bengio, Y. (2004). Unsupervised sense disambiguation using bilingual probabilistic models. In ACL 04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, p. 287, Morristown, NJ, USA. Association for Computational Linguistics.
Brill, E. (1995). Transformation-based error-driven learning and natural language processing: A
case study in part-of-speech tagging. Computational Linguistics, 21(4), 543565.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1991). Word-sense disambiguation
using statistical methods. In Proceedings of the ACL, pp. 264270.
Chen, Y., Eisele, A., & Kay, M. (2008). Improving statistical machine translation efficiency by
triangulation. In Proceedings of LREC.
Chiang, D. (2005). A hierarchical phrase-based model for statistical machine translation. In Proceedings of the ACL, pp. 263270.
Cohn, T., & Lapata, M. (2007). Machine translation by triangulation: Making effective use of multiparallel corpora. In Proceedings of ACL.
Comrie, B. (1989). Language universals and linguistic typology: Syntax and morphology. Oxford:
Blackwell.
Dagan, I., Itai, A., & Schwall, U. (1991). Two languages are more informative than one. In Proceedings of the ACL, pp. 130137.
Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging using parallel
corpora. In Proceedings of the ACL, pp. 255262.
Erjavec, T. (2004). MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora. In Fourth International Conference on Language Resources and Evaluation, LREC, Vol. 4, pp. 15351538.
Escobar, M., & West, M. (1995). Bayesian density estimation and inference using mixtures. Journal
of the american statistical association, 90(230), 577588.
Ferguson, T. (1973). A bayesian analysis of some nonparametric problems. The annals of statistics,
1, 209230.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian data analysis. Chapman
and Hall/CRC.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence,
6, 721741.
383

fiNASEEM , S NYDER , E ISENSTEIN & BARZILAY

Genzel, D. (2005). Inducing a multilingual dictionary from a parallel multitext in related languages.
In Proceedings of HLT/EMNLP, pp. 875882.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo in practice.
Chapman & Hall/CRC.
Goldwater, S., & Griffiths, T. L. (2007). A fully Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL, pp. 744751.
Haghighi, A., & Klein, D. (2006). Prototype-driven learning for sequence models. In Proceedings
of HLT-NAACL, pp. 320327.
Hinton, G. E. (1999). Products of experts. In Proceedings of the Ninth International Conference on
Artificial Neural Networks, Vol. 1, pp. 16.
Johnson, M. (2007). Why doesnt EM find good HMM POS-taggers?.
EMNLP/CoNLL, pp. 296305.

In Proceedings of

Kuhn, J. (2004). Experiments in parallel-text based grammar induction. In Proceedings of the ACL,
p. 470.
Li, C., & Li, H. (2002). Word translation disambiguation using bilingual bootstrapping. In Proceedings of the ACL, pp. 343351.
Liu, J. S. (1994). The collapsed Gibbs sampler in Bayesian computations with applications to a
gene regulation problem. Journal of the American Statistical Association, 89(427), 958966.
Merialdo, B. (1994). Tagging english text with a probabilistic model. Computational Linguistics,
20(2), 155171.
Mihalcea, R. (2004). Current Issues in Linguistic Theory: Recent Advances in Natural Language
Processing, chap. Unsupervised Natural Language Disambiguation Using Non-Ambiguous
Words. John Benjamins Publisher.
Myers, J. L., & Well, A. D. (2002). Research Design and Statistical Analysis (2nd edition).
Lawrence Erlbaum.
Ng, H. T., Wang, B., & Chan, Y. S. (2003). Exploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the ACL, pp. 455462.
Och, F. J., & Ney, H. (2001). Statistical multi-source translation. In MT Summit 2001, pp. 253258.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Pado, S., & Lapata, M. (2006). Optimal constituent alignment with edge covers for semantic projection. In Proceedings of ACL, pp. 1161  1168.
Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2), 257286.
Resnik, P., & Yarowsky, D. (1997). A perspective on word sense disambiguation methods and
their evaluation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical
Semantics: Why, What, and How?, pp. 7986.
Sethuraman, J. (1994). A constructive definition of Dirichlet priors. Statistica Sinica, 4(2), 639650.

384

fiM ULTILINGUAL PART- OF -S PEECH TAGGING

Smith, N. A., & Eisner, J. (2005). Contrastive estimation: Training log-linear models on unlabeled
data. In Proceedings of the ACL, pp. 354362.
Snyder, B., & Barzilay, R. (2008). Unsupervised multilingual learning for morphological segmentation. In Proceedings of the ACL/HLT, pp. 737745.
Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2008). Unsupervised multilingual learning
for pos tagging. In Proceedings of EMNLP.
Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2009). Adding more languages improves
unsupervised multilingual part-of-speech tagging: A bayesian non-parametric approach. In
Proceedings of NAACL/HLT.
Toutanova, K., & Johnson, M. (2008). A Bayesian LDA-based model for semi-supervised part-ofspeech tagging. In Advances in Neural Information Processing Systems 20, pp. 15211528.
MIT Press.
Utiyama, M., & Isahara, H. (2006). A comparison of pivot methods for phrase-based statistical
machine translation. In Proceedings of NAACL/HLT, pp. 484491.
Wu, D. (1995). Stochastic inversion transduction grammars, with application to segmentation,
bracketing, and alignment of parallel corpora. In IJCAI, pp. 13281337.
Wu, D., & Wong, H. (1998). Machine translation with a stochastic grammatical channel. In Proceedings of the ACL/COLING, pp. 14081415.
Xi, C., & Hwa, R. (2005). A backoff model for bootstrapping resources for non-English languages.
In Proceedings of EMNLP, pp. 851  858.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2000). Inducing multilingual text analysis tools via
robust projection across aligned corpora. In Proceedings of HLT, pp. 161168.

385

fiJournal of Artificial Intelligence Research 36 (2009) 165228

Submitted 03/09; published 10/09

Hypertableau Reasoning for Description Logics
Boris Motik
Rob Shearer
Ian Horrocks

boris.motik@comlab.ox.ac.uk
rob.shearer@comlab.ox.ac.uk
ian.horrocks@comlab.ox.ac.uk

Computing Laboratory, University of Oxford
Wolfson Building
Parks Road
Oxford OX1 3QD
United Kingdom

Abstract
We present a novel reasoning calculus for the description logic SHOIQ+ a knowledge
representation formalism with applications in areas such as the Semantic Web. Unnecessary
nondeterminism and the construction of large models are two primary sources of inefficiency
in the tableau-based reasoning calculi used in state-of-the-art reasoners. In order to reduce
nondeterminism, we base our calculus on hypertableau and hyperresolution calculi, which
we extend with a blocking condition to ensure termination. In order to reduce the size of the
constructed models, we introduce anywhere pairwise blocking. We also present an improved
nominal introduction rule that ensures termination in the presence of nominals, inverse
roles, and number restrictionsa combination of DL constructs that has proven notoriously
difficult to handle. Our implementation shows significant performance improvements over
state-of-the-art reasoners on several well-known ontologies.

1. Introduction
Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007)
are a family of knowledge representation formalisms with well-understood formal properties.
DLs have been applied to numerous problems in computer science such as information
integration and metadata management. Recent interest in DLs has been spurred by their
application in the Semantic Web: the DL SHOIQ provides the logical underpinning for
the Web Ontology Language (OWL) (Patel-Schneider, Hayes, & Horrocks, 2004), and the
DL SROIQ (Kutz, Horrocks, & Sattler, 2006) is used in OWL 2an extension of OWL
currently being standardized by the World Wide Web Consortium.
A central component of most DL applications is an efficient and scalable reasoner. Modern reasoners, such as Pellet (Parsia & Sirin, 2004), FaCT++ (Tsarkov & Horrocks, 2006),
and RACER (Haarslev & Moller, 2001), are typically based on tableau calculi (Baader &
Nutt, 2007), which demonstrate the (un)satisfiability of a knowledge base K via a constructive search for an abstraction of a model of K. Numerous optimizations have been developed
in an effort to reduce the size of the search space (Horrocks, 2007). Despite major advances
in tableau reasoning algorithms, however, ontologies are still encountered in practice that
cannot be handled by existing DL reasoners. Two main sources of complexity in tableau
calculi have been identified in the literature (Donini, 2007).
This first source of complexity is known as or-branching: given a disjunctive assertion

c
2009
AI Access Foundation. All rights reserved.

fiMotik, Shearer, & Horrocks

(C t D)(s), a tableau algorithm nondeterministically guesses that either C(s) or D(s) holds.
To show the unsatisfiability of K, every possible guess must lead to a contradiction: if
assuming that C(s) holds leads to a contradiction, the algorithm must backtrack and assume
that D(s) holds, which can give rise to exponential behavior. General concept inclusions
(GCIs)implications of the form C v Dare the main source of disjunctions: to ensure
that C v D holds, a tableau algorithm adds a disjunction (C t D)(s) to each individual s
in the model. Various absorption optimizations (Horrocks, 1998; Tsarkov & Horrocks, 2004;
Hudek & Weddell, 2006; Horrocks, 2007) have been developed to reduce the nondeterminism
in tableau calculi.
The second source of complexity in tableau calculi is known as and-branching: the
expansion of a model due to existential quantifiers can generate very large models. Apart
from memory consumption problems, and-branching can increase or-branching by increasing
the number of individuals to which GCIs are applied.
In this paper, we present a reasoning calculus that addresses both sources of complexity. We focus on the DL SHOIQ+ , which is obtained by extending SHOIQ with local
reflexivity and disjoint, reflexive, irreflexive, symmetric, and asymmetric roles. SROIQ
further extends SHOIQ+ with generalized role inclusions of the form R1  . . .  Rn v R.
Generalized role inclusions can be encoded using standard GCIs as proposed by Demri and
de Nivelle (2005); thus, by adding a suitable preprocessing phase, the results from this
paper should allow us to handle SROIQ (and hence OWL 2) as well.
Our algorithm can be viewed as a hybrid of resolution and tableau, and is related to the
hypertableau (Baumgartner, Furbach, & Niemela, 1996) and hyperresolution (Robinson,
1965) calculi. It first preprocesses a SHOIQ+ knowledge base into a set of DL-clauses
universally quantified implications containing DL concepts and roles as predicates. The
main derivation rule for DL-clauses is hyperresolution: an atom from the consequent of a
DL-clause is derived only if all atoms from the DL-clause antecedent can be matched to
already derived consequences. Hyperresolution is very effective at restricting or-branching.
Consider, for example, the following example:
(1)

R(x, y1 )  S(x, y2 )  A(x)  B(y1 )  C(y2 )

This DL-clause derives a disjunction only if it is applied to assertions of the form R(a, b)
and S(c, d) where a = c. The presence of variables in (1) allows us to simultaneously work
with individuals a, b, c and d, and to check whether a = c. In contrast, derivation rules
in tableau algorithms consider at most pairs of individuals; consequently, no absorption
technique we are aware of can localize nondeterminism only to the individuals that satisfy
the mentioned constraints. As we discuss in detail in Section 3.3.1, our calculus generalizes
all known absorption variants. Furthermore, in contrast to absorption techniques, our
algorithm is guaranteed to exhibit no nondeterminism on Horn knowledge bases (Hustadt,
Motik, & Sattler, 2005) such as GALEN, NCI, and SNOMED CT (see Section 7). Finally,
our calculus provides a uniform proof-theoretic framework that can handle several useful
extensions of commonly used DLs (see Section 4.1.3).
Hyperresolution decides many fragments of first-order logic (e.g., Fermuller, Leitsch,
Hustadt, & Tammet, 2001; Fermuller, Tammet, Zamov, & Leitsch, 1993), as well as description and modal logics (e.g., Georgieva, Hustadt, & Schmidt, 2003; Hustadt & Schmidt,
1999). Unlike most of these fragments, SHOIQ+ allows for cyclic GCIs of the form
166

fiHypertableau Reasoning for Description Logics

C v R.C, on which hyperresolution can generate infinite paths of successors. To ensure
termination, we use the pairwise blocking technique (Horrocks, Sattler, & Tobies, 2000b)
to detect cyclic computations. Due to hyper-inferences, the soundness and correctness
proofs by Horrocks et al. (2000b) do not carry over immediately to our calculus; in fact,
certain simpler blocking conditions applicable to weaker DLs cannot be straightforwardly
transferred to our setting. To limit and-branching, we extend the blocking condition by
Horrocks et al. to anywhere pairwise blocking: an individual can be blocked by another
individual that is not necessarily its ancestor, which can reduce the sizes of the constructed
models. Anywhere blocking has already been used with single blocking (Buchheit, Donini,
& Schaerf, 1993; Baader, Buchheit, & Hollunder, 1996; Donini & Massacci, 2000; Donini,
Lenzerini, Nardi, & Schaerf, 1998); however, to the best of our knowledge, it has been
neither used with the more sophisticated pairwise blocking nor tested in practice.
Ensuring termination of a tableau decision procedure for DLs with nominals, inverse
roles, and number restrictions has proven notoriously difficult. This problem was finally
solved by Horrocks and Sattler (2007) by extending the tableau calculus with a nominal
introduction rule. In certain situations, this rule guesses and introduces new nominals, and
is thus a potential source of inefficiency in practice. In this paper, we present a variant of
this rule that is simpler and more efficient.
We have implemented our calculus in a new reasoner called HermiT.1 Even with a rather
nave implementation, the deterministic treatment of GCIs significantly reduces classification times for several real-world ontologies. Furthermore, pairwise anywhere blocking seems
to be very effective in limiting model sizes and it allows HermiT to classify several ontologies
that, to the best of our knowledge, no other reasoner can handle.

2. Preliminaries
We now define the syntax and the semantics of the description logic SHOIQ+ . A signature
is a triple  = (NR , NC , NI ) consisting of mutually disjoint sets of atomic roles NR , atomic
concepts NC , and individuals NI . The set of roles is then NR  {R | R  NR }. The function inv() is defined on the set of roles as follows, where R is an atomic role: inv(R) = R
and inv(R ) = R. An RBox R is a finite set of axioms of the form R1 v R2 (role inclusion),
Dis(S1 , S2 ) (role disjointness), Ref(R) (reflexivity), Irr(S) (irreflexivity), Sym(R) (symmetry), Asy(S) (asymmetry), and Tra(R) (transitivity), where R, R1 , and R2 are roles, and
S, S1 , and S2 are simple roles, as defined next. Let vR be the reflexive-transitive closure
of the following relation: {(R1 , R2 ) | R1 v R2  R or inv(R1 ) v inv(R2 )  R}. A role R is
transitive in R if a role R0 exists such that R0 vR R, R vR R0 , and either Tra(R0 )  R or
Tra(inv(R0 ))  R. A role S is simple if no transitive role R exists such that R vR S. The
set of concepts is the smallest set containing > (the top concept),  (the bottom concept),
A (atomic concept), {a} (nominal ), C (negation), C u D (conjunction), C t D (disjunction), R.C (existential restriction), R.C (universal restriction), S.Self (local reflexivity),
 n S.C (at-least restriction), and  n S.C (at-most restriction), for A an atomic concept,
a an individual, C and D concepts, R a role, S a simple role, and n a nonnegative integer.
A TBox T is a finite set of general concept inclusions (GCIs) C v D for C and D concepts. An ABox A is a finite set of assertions of the form C(a) (concept assertion), R(a, b)
1. http://www.hermit-reasoner.com/

167

fiMotik, Shearer, & Horrocks

Table 1: Model-Theoretic Semantics of SHOIQ+
Interpretation of Concepts and Roles
= 4I
I = 
= {sI }
(C)I = 4I \ C I
I
I
=C D
(C t D)I = C I  DI
I
= {hy, xi | hx, yi  R }
(S.Self)I = {x | hx, xi  S I }
= {x | y : hx, yi  RI  y  C I }
= {x | y : hx, yi  RI  y  C I }
= {x | ]{y | hx, yi  S I  y  C I }  n}
= {x | ]{y | hx, yi  S I  y  C I }  n}
Satisfaction of Axioms in an Interpretation
I
I |= C v D iff C  DI
I |= R1 v R2 iff R1I  R2I
I
I
I |= Ref(R) iff x  4 : hx, xi  R
I |= Irr(S)
iff x  4I : hx, xi 6 S I
I |= Sym(R) iff RI  (inv(R))I
I |= Asy(S)
iff S I  (inv(S))I = 
I
+
I
I |= Tra(R) iff (R )  R
I |= Dis(S1 , S2 ) iff S1I  S2I = 
I |= C(a)
iff aI  C I
I |= R(a, b)
iff haI , bI i  RI
I
I
I |= a  b
iff a = b
I |= a 6 b
iff aI 6= bI
+
Note: ]N is the number of elements in N , and R is the transitive closure of R.
>I
{s}I
(C u D)I
(R )I
(R.C)I
(R.C)I
( n S.C)I
( n S.C)I

(role assertion), a  b (equality assertion), and a 6 b (inequality assertion), where C is a
concept, R is a role, and a and b are individuals. A SHOIQ+ knowledge base K is a triple
(R, T , A). With |K| we denote the size of Kthat is, the number of symbols required to
encode K on the input tape of a Turing machine (numbers can be coded in binary).
An interpretation for K is a tuple I = (4I , I ), where 4I is a nonempty set, and I
assigns an element aI  4I to each individual a, a set AI  4I to each atomic concept A,
and a relation RI  4I  4I to each atomic role R. The function I is extended to concepts
and roles as shown in the upper part of Table 1. I is a model of K, written I |= K, if it
satisfies all axioms of K as shown in the lower part of Table 1. The basic inference problem
for SHOIQ+ is checking whether K is satisfiablethat is, checking whether a model of K
exists. A concept C subsumes a concept D, written K |= C v D, if C I  DI for each model
I of K. It is easy to see that K |= C v D if and only if K  {(C u D)(a)} is unsatisfiable,
where a is an individual that does not occur in K (Baader & Nutt, 2007).
The negation-normal form nnf(C) of a concept C is the concept obtained from C by
using de Morgans laws, the dualities between existential and universal restrictions, and
the dualities between at-least and at-most restrictions to push negations inwards so that
they occur only in front of atomic concepts, nominals, and local reflexivity concepts. The
concept nnf(C) is logically equivalent to C, and it can be computed from C in time linear
in the size of C (Baader & Nutt, 2007). We use C
 to denote nnf(C).
As mentioned in Section 1, extending SHOIQ+ with general role inclusions would yield
SROIQ (Kutz et al., 2006)the DL that underpins OWL 2. ALCHOIQ+ is obtained
from SHOIQ+ by disallowing transitivity axioms. SHIQ+ is obtained from SHOIQ+
by disallowing nominals. SHOQ+ is obtained from SHOIQ+ by disallowing inverse roles.
SHOIQ and SHIQ are obtained from SHOIQ+ and SHIQ+ , respectively, by disallow168

fiHypertableau Reasoning for Description Logics

ing local reflexivity, role disjointness, reflexivity, irreflexivity, symmetry, and asymmetry
axioms. Finally, SHOI is obtained from SHOIQ by disallowing at-least and at-most
restrictions.

3. Motivation and Algorithm Overview
In this section, we present an overview of the main aspects of our algorithm. We explain in
Section 3.1 the root causes of the scalability problems encountered in tableau algorithms,
and in Section 3.2 we outline how we address them. Finally, in Section 3.3 we discuss the
relationship between our algorithm and some related approaches.
3.1 Causes of Scalability Problems in Tableau Algorithms
To show that a knowledge base K = (R, T , A) is satisfiable, a tableau algorithm constructs
a derivationa sequence of ABoxes A0 , A1 , . . . , An where A0 = A and each Ai is obtained
from Ai1 by an application of one derivation rule.2 The derivation rules make the information implicit in the axioms of R and T explicit, and thus evolve the ABox A towards
a (representation of a) model of K. The algorithm terminates either if no derivation rule
is applicable to some An , in which case An represents a model of K, or if An contains
an obvious contradiction, in which case the model construction has failed. The following
derivation rules are commonly used in DL tableau calculi.
 t-rule: Given (C1 t C2 )(s), derive either C1 (s) or C2 (s).
 u-rule: Given (C1 u C2 )(s), derive C1 (s) and C2 (s).
 -rule: Given (R.C)(s), derive R(s, t) and C(t) for t a fresh individual.
 -rule: Given (R.C)(s) and R(s, t), derive C(t).
 v-rule: Given a GCI C v D and an individual s, derive (C t D)(s).
The t-rule is nondeterministic: if (C1 t C2 )(s) is true, then C1 (s) or C2 (s) or both are true.
Therefore, tableau calculi make a nondeterministic guess and choose either C1 or C2 ; if
one choice leads to a contradiction, the algorithm must backtrack and try the other choice.
Thus, K is unsatisfiable only if all choices lead to a contradiction. We next discuss two
sources of complexity inherent in the tableau derivation rules.
3.1.1 Or-Branching
Handing disjunctions through reasoning by case is often called or-branching. The v-rule
adds a disjunction for each GCI to each individual in an ABox and is thus a major source of
or-branching and inefficiency (Horrocks, 2007). Consider, for example, the knowledge base
K1 = (, T1 , A1 ), with T1 and A1 specified as follows:
(2)

T1 = {R.A v A}
A1 = {A(a0 ), R(a0 , b1 ), R(b1 , a1 ), . . . , R(an1 , bn ), R(bn , an ), A(an )}

2. Some formalizations of tableau algorithms work on completion graphs (Horrocks & Sattler, 2007), which
have a natural correspondence to ABoxes.

169

fiMotik, Shearer, & Horrocks

a0
(i)
(ii)
(iii)
(iv)

A
R.A t A
R.A

R

R

b1
R.A t A
A
R.A

a1

an1

R.A t A
R.A
A

R.A t A
R.A
A

R

bn
R.A t A
A
R.A

R

an
A
R.A t A
R.A
A

Figure 1: Or-Branching Example

The ABox A1 is graphically shown in Figure 1. The individuals occurring in the ABox are
represented as black dots, an assertion of the form A(a0 ) is represented by placing A next
to the individual a0 , and an assertion of the form R(a0 , b1 ) is represented as an R-labeled
arrow from a0 to b1 . Initially, A1 contains only the concept assertions shown in line (i ).
To satisfy the GCI in T1 , a tableau algorithm applies the v-rule, thus adding the assertions shown in line (ii ) of Figure 1. Tableau algorithms are usually free to choose the order
in which they process the assertions in an ABox; in fact, finding an order that exhibits good
performance in practice requires advanced heuristics (Tsarkov & Horrocks, 2005b). Let us
assume that the algorithm chooses to process the assertions on ai before those on bj . Hence,
by applying the derivation rules to all ai , a tableau algorithm derives the assertions shown
in line (iii ) of Figure 1; after that, by applying the derivation rules to all bi , the algorithm
derives the assertions shown in line (iv ) of Figure 1. The ABox now contains both A(an )
and A(an ), which is a contradiction. Thus, the algorithm needs to backtrack its most
recent choice, so it flips its guess on bn1 to A(bn1 ). This generates a contradiction on
bn1 , so the algorithm backtracks from all guesses for bi , changes the guess on an to A(an ),
and repeats the work for all bi . This also leads to a contradiction, so the algorithm must
revise its guess for an1 ; but then, two guesses are again possible for an . In general, after
revising a guess for ai , all possibilities for aj , i < j  n, must be reexamined, which results
in exponential behavior. None of the standard backtracking optimizations (Horrocks, 2007)
are helpful: the problem arises because the order in which the individuals are processed
makes the guesses on ai independent from the guesses on aj for i 6= j.
The GCI R.A v A, however, is not inherently nondeterministic: it is equivalent to
the Horn clause x, y : [R(x, y)  A(y)  A(x)], which can be applied bottom-up to derive the assertions A(bn ), A(an1 ), . . . , A(a0 ) and eventually reveal a contradiction on a0 .
These inferences are deterministic,3 so we can conclude that K1 is unsatisfiable without
any backtracking. This example suggests that the processing of GCIs in tableau algorithms
can be unnecessarily nondeterministic. Hustadt et al. (2005) have identified a class of
knowledge bases without unnecessary nondeterminism: knowledge bases expressed in the
description logic Horn-SHIQ can always be translated into Horn clauses, suggesting that
reasoning without any nondeterminism is possible in principle. Ideally, a practical DL
reasoning procedure should exhibit no nondeterminism on Horn knowledge bases.
3. More precisely, each inference is deterministic, but the order in which the inferences are performed is
dont-care nondeterministic.

170

fiHypertableau Reasoning for Description Logics

a

a
S
S

S

S
S

S

(a) Ancestor Blocking

(b) Anywhere Blocking

Figure 2: And-Branching Example

In the context of tableau calculi, various absorption optimizations (Horrocks, 2007)
have been developed to control the nondeterminism arising in the application of GCIs. We
discuss these optimizations in depth in Section 3.3.1.
3.1.2 And-Branching
The introduction of new individuals in the -rule is often called and-branching, and it is
another major source of inefficiency in tableau algorithms (Donini, 2007). Consider, for
example, the (satisfiable) knowledge base K2 = (, T2 , A2 ), with T2 and A2 specified as
follows (where n and m are integers):

(3)

T2 = { A1 v  2 S.A2 , . . . , An1 v  2 S.An , An v A1 ,
Ai v (B1 t C1 ) u . . . u (Bm t Cm ) for 1  i  n }
A2 = { A1 (a) }

At-least restrictions are dealt with in tableau algorithms by the -rule, which is quite
similar to the -rule: from ( n R.C)(s), the -rule derives R(s, ti ) and C(ti ) for 1  i  n,
and ti 6 tj for 1  i < j  n. Thus, the assertion A1 (a) implies the existence of at least two
individuals in A2 , which imply the existence of at least two individuals in A3 , and so on.
Given K2 , a tableau algorithm thus constructs a binary tree, shown in Figure 2a, in which
each individual is labeled with some Ai and an element of  = {B1 , C1 }  . . .  {Bm , Cm }.
All individuals in the tree at depth n are instances of An ; because of the GCI An v A1 ,
these individuals must be instances of A1 as well, so we can repeat the whole construction
and generate an even deeper tree. Clearly, a nave application of the tableau rules does not
terminate if the TBox contains existential quantifiers in cycles.
To ensure termination is such cases, tableau algorithms employ blocking (Baader & Nutt,
2007), which is based on an important observation about the shape of ABoxes that can be
derived from some input ABox A. The individuals in A are called named (shown as black
circles), and they can be connected by role assertions in an arbitrary way. The individuals
introduced by the - and -rules are called blockable (shown as white circles). For example,
if R.C(a) is expanded into R(a, s) and C(s), then s is called a blockable individual and it
is an R-successor of a. It is not difficult to see that, if the knowledge base does not contain
171

fiMotik, Shearer, & Horrocks
bloc
ks
t0
t
s0
s
u0
u

Figure 3: Forest-Like Shape of ABoxes

nominals, no tableau derivation rule can connect s with an arbitrary named individual: the
individual s can participate only in inferences that derive an assertion of the form D(s) with
D a concept, create a new successor of s, connect s to an existing predecessor or successor,
or, in the presence of (local) reflexivity, connect s to itself. Hence, each ABox A0 obtained
from A can be seen as a forest of the form shown in Figure 3: each named individual can
be arbitrarily connected to other named individuals and to a tree of blockable successors.
The concept label LA (s) is defined as the set of all concepts C such that C(s)  A, and the
edge label LA (s, s0 ) as the set of all atomic roles such that R(s, s0 )  A.
The forest-like structure of ABoxes enables blocking. Description logics such as SHIQ+
and SHOIQ+ allow for inverse roles and number restrictions, which has been handled
in the literature by ancestor pairwise blocking (Horrocks et al., 2000b): for individuals
s, s0 , t, and t0 occurring in an ABox A as shown in Figure 3, t blocks s (shown by a
double border on s) if and only if LA (s) = LA (t), LA (s0 ) = LA (t0 ), LA (s, s0 ) = LA (t, t0 ),
and LA (s0 , s) = LA (t0 , t).4 In tableau algorithms, the - and -rules are applicable only
to nonblocked individuals, which ensures termination: the number of different concept and
edge labels is exponential in |K|, so an exponentially long branch in a forest-like ABox must
contain a blocked individual, thus limiting the length of each branch in an ABox. Let A
be an ABox as in Figure 3 to which no tableau derivation rule is applicable, and in which
s is blocked by t. We can construct a model from A by unravelingthat is, by replicating
the fragment between s and t infinitely often. Intuitively, blocking ensures that the part
of the ABox between s and s0 behaves just like the part between t and t0 , so unraveling
indeed generates a model. If our logic were able to connect blockable individuals in a nontree-like way, then unraveling would not generate a model; in fact, the notion of ancestors,
descendants, and blocking would itself be ill-defined.
Consider now an unlucky run of a tableau algorithm with ancestor pairwise blocking
on K2 . The number of elements in  is exponential in |K2 |, so it can happen that blocking
comes into effect only after the algorithm constructs an exponentially deep tree; since the
tree is binary, it is doubly exponential in total. In a lucky run, the algorithm can always
pick Bj instead of Cj ; then, the algorithm constructs a polynomially deep binary tree, so
4. Our blocking definition must include both edge labels in both directions because, unlike in some other
tableau formalizations, our edge labels include only atomic roles.

172

fiHypertableau Reasoning for Description Logics

the tree is exponential in total. Thus, the and-branching caused by the - and -rules can
lead to unnecessary generation of an ABox that is doubly exponential in the size of the
input, which limits the scalability of tableau algorithms in practice.
3.2 The Hypertableau Algorithm at a Glance
In this section we present an informal overview of our hypertableau algorithm that addresses
the problems due to or- and and-branching outlined in Section 3.1. We then formalize the
algorithm in Section 4.
3.2.1 Derivation Rules
The hyperresolution calculus (Robinson, 1965) has often been
V used forWfirst-order theorem proving. It works on clausesimplications V
of the form ni=1 Ui  m
j=1 Vj where Ui
n
and Vj are first-order
atoms. The conjunction i=1 Ui is called the antecedent, and the
W
V
is
called the consequent; we sometimes omit  if the antecedent is
disjunction m
j
j=1
empty. For Di a possibly empty disjunction of literals and  the most general unifier of
(A1 , B1 ), . . . , (Am , Bm ), the hyperresolution derivation rule is defined as follows (assuming
that the unifier  exists):5
A1  D 1

...

Am  Dm
B 1  . . .  B m  C1  . . .  Ck
D1   . . .  Dm   C1   . . .  Ck 

To make the calculus refutationally complete for first-order logic, one additionally needs a
factoring derivation rule, which we do not discuss any further.
The hypertableau calculus (Baumgartner et al., 1996) is based on the observation that,
if the literals in C1   . . .  Cn  do not share variables, we can replace the clause with a
nondeterministically chosen atom Ci  that we assume to be true. If we assume that all
clauses are safe (i.e., that each variable occurring in a clause also occurs in the clauses
antecedent), then Ai  Di and C1   . . .  Cn  are always ground, so they can always be
nondeterministically split into atoms. Such a hypertableau inference is written as
A1

...

Am
B 1  . . .  B m  C1  . . .  Ck
C1  | . . . | Ck 

where  is the most general unifier of (A1 , B1 ), . . . , (Am , Bm ) and | represents or-branching.
On Horn clauses, each inference is deterministic,6 and the calculus exhibits a minimal
amount of dont-known nondeterminism on general clauses.
The hypertableau calculus by Baumgartner et al. (1996) can be easily applied to DLs:
GCIs can be translated into first-order formulae (Borgida, 1996), which can then be converted into clauses, as shown in the following example.
A v R.B

x : [A(x)  y : R(x, y)  B(y)]

A(x)  B(f (x))
A(x)  R(x, f (x))

5. It is usual in resolution theorem proving to assume that the notation Ai  Di does not imply that Ai is
the left-most disjunct in the disjunction, and we follow this convention.
6. As mentioned before, the order in which inferences are applied is nevertheless dont-care nondeterministic.

173

fiMotik, Shearer, & Horrocks

Let A be an ABox containing the assertions A(a), R(a, b), and B(b). The GCI A v R.B
is clearly satisfied in A, so there is no need to perform any inference. The clauses obtained
by skolemization, however, are not satisfied in A, so the hypertableau calculus derives
R(a, f (a)) and B(f (a)). Hence, skolemization may make the calculus perform unnecessary
inferences, which may be inefficient.
Therefore, instead of working with skolemized clauses, our calculus first preprocesses a
SHOIQ+ knowledge base K into a pair (K) = (T R (K), A (K)),
A (K) is an ABox
Vn where W
and T R (K) is a set of DL-clausesimplications of the form i=1 Ui  m
j=1 Vj , where Ui
are of the form R(x, y) or A(x), and Vj are of the form R(x, y), A(x), R.C(x),  n R.C(x),
or x  y. The preprocessing step is introduced formally in Section 4.1. The DL-clauses in
T R (K) are used in the Hyp-rule, which is inspired by the hypertableau derivation rule. For
example, a GCI R.A v B is translated into a DL-clause R(x, y)  B(x)  A(y); then, if
an ABox contains R(a, b), the Hyp-rule derives either B(a) or A(b).
At-most restrictions are translated in our approach into DL-clauses containing equalities;
for example, the axiom A v  2 R.B is translated into the DL-clause
A(x)  R(x, y1 )  B(y1 )  R(x, y2 )  B(y2 )  R(x, y3 )  B(y3 )  y1  y2  y1  y3  y2  y3 .
While a concept of the form  n R.B can be encoded using O(log n) bits, the corresponding
DL-clause contains O(n2 ) literals; thus, our translation incurs an exponential blowup. We
do not believe, however, this issue to be particular to our approach: tableau algorithms
deal with at-most restrictions using a specialized -rule whose application requires O(n)
space; thus, our translation merely makes the exponential space requirement explicit. Consequently, the (hyper)tableau algorithms are unlikely to be able to handle large numbers
in number restrictions, and specialized algorithms, such as the one proposed by Faddoul,
Farsinia, Haarslev, and Moller (2008), may be required.
Because of the translation described in the previous paragraph, the Hyp-rule can derive
equalities of the form s  t. These are then dealt with using the -rule: whenever s  t  A
and s 6= t, the -rule replaces s with t or vice versa in all assertions in A; this is usually
called merging.
Apart from the Hyp- and the -rule, our calculus contains the -rule from the tableau
calculus that deals with existential quantifiers, the -rule that detects obvious contradictions (which can be of the form s 6 s, or A(s) and A(s)), and the NI -rule that ensures
termination in the presence of nominals, number restrictions, and inverse roles. We discuss
the NI -rule in more detail in Section 3.2.4.
The rules of the algorithm are formalized in Definition 7 on page 193 and Table 5 on page
196, and the reader may find it useful to briefly examine these definitions before continuing.
3.2.2 Anywhere Pairwise Blocking
We employ pairwise blocking from Section 3.1.2 to ensure termination of the calculus; to
curb and-branching, however, we extend it to anywhere pairwise blocking. The key idea
is to extend the set of potential blockers for s beyond the ancestors of s. In doing so, we
must avoid cyclic blocks: if s is allowed to block t and t can block s, then neither s nor t is
guaranteed to have all its successors constructed, which would render the calculus incomplete. Therefore, we parameterize our algorithm with a strict ordering  on individuals that
174

fiHypertableau Reasoning for Description Logics

R
a
A
R.>

R

R
b

a
A
R.>

R

b

R

R
c

R.>

a

R

c

A
R.>

Figure 4: A Yo-Yo Example

contains the ancestor relation. We allow t to block s only if, in addition to the conditions
mentioned in Section 3.1.2, we have t  s. This version of blocking is formalized in Definition 7 on page 193. Note that, if  coincides with the ancestor relation, then anywhere
blocking becomes equivalent to ancestor blocking.
Anywhere blocking can reduce and-branching in practice. Consider again the knowledge
base K2 from Section 3.1.2. After we exhaust the exponentially many members of , all
subsequently created individuals will be blocked. In the best case, we can always choose Bj
instead of Cj , so we create a polynomial path in the tree and then use the individuals from
that path to block their siblings, as shown in Figure 2b. Hence, there is a derivation for K2
with anywhere blocking that can be constructed in polynomial time.
3.2.3 Problems Due to Merging
Merging can easily lead to termination problems even for very simple DLs, as shown in the
following example. For simplicity, we present the TBox as a set of DL-clauses C3 .
(4)

A3 = { A(a), R.>(a), R(a, b), R(a, a) }
C3 = { R(x, y1 )  R(x, y2 )  y1  y2 ,
A(x)  R(x, y)  R.>(y) }

Consider now the derivation in our calculus on A3 and C3 illustrated in Figure 4: by the
second DL-clause, the Hyp-rule derives R.>(b), which the -rule expands to R(b, c); then,
by the first DL-clause, the Hyp-rule derives b  a, so the -rule merges b into a. Clearly,
the resulting ABox is isomorphic to the original one (that c is a blockable and b a named
individual is not relevant here), so we can repeat the same sequence of inferences, which
leads to nontermination. To the best of our knowledge, this problem was first identified by
Baader and Sattler (2001), and it is commonly known as a yo-yo.
This problem arises because, due to merging, a can have an unbounded number of
blockable R-successors: the blockable individual c is created as an R-successor of b, but
merging b into a makes c a blockable R-successor of a. This, in turn, allows us to apply the
DL-clauses from C3 to a an arbitrary number of times, which leads to nontermination.
This problem can be solved by always merging a descendant s into its ancestor t, and
pruning s before mergingthat is, by removing all assertions containing a blockable descendant of s and thus ensuring that t does not inherit new successors.7 Pruning is formally
defined in Definition 7 on page 193.
7. Horrocks et al. (2000b) do not physically remove successors, but mark them as not present by setting
the relevant edge labels to . This has exactly the same effect as pruning.

175

fiMotik, Shearer, & Horrocks

B
R.C

A
R.B
a

R

s1

R

C
S.{c}

A
R.B

s2

a

S

B
R.C
R

s1

c
b

R

A
R.B

s3
B
R.C

R

s4

S

b
A
R.B

C
S.{c}

R

s3

C
S.{c}
R
S
s2

c

R

B
R.C

Figure 5: Non-Tree-Like Structures Due to Merging

Thus, before merging b into a in our example, we prune bthat is, we remove the
assertion R(b, c). Merging then produces an ABox that represents a model of A3 and C3 ,
so the algorithm terminates. Note that pruning is well-defined only because our ABoxes
are forest-shaped, cf. Figure 3: if connections between individuals were arbitrary and, in
particular, cyclic, it would not be clear which part of the ABox should be pruned.
3.2.4 Nominals
With nominals, it is possible to derive ABoxes that are not forest-like, as the following
simple example demonstrates. For presentation purposes, we use the concept R.{c} in the
DL-clauses even though such concepts would be further decomposed in our algorithm.
(5)

A4 = { A(a), A(b) }
C4 = { A(x)  (R.B)(x), B(x)  (R.C)(x), C(x)  (S.{c})(x) }

Successive applications of the Hyp- and -rules to A4 and C4 can produce the ABox
A14 shown on the left-hand side of Figure 5. This ABox is clearly not forest-shaped: the
two paths of role atoms in A14 start at the named individuals a and b and end in a named
individual c. Nevertheless, if role relations between blockable individuals remain forestlike, termination of the derivation can be ensured using blocking. Some DLs that include
nominals produce only such extended forest-like ABoxes (Horrocks & Sattler, 2001).
If a DL includes inverse roles, number restrictions, and nominals, the shape of an ABox
becomes much more involved. To this end, assume now that we extend C4 with the DL-clause
S(y1 , x)  S(y2 , x)  y1  y2 (which axiomatizes S to be inverse-functional and effectively
introduces number restrictions). On A14 , the Hyp-rule then derives s2  s4 . Note that both
s2 and s4 are blockable individuals; furthermore, neither individual is an ancestor of the
other, so we can merge, say, s4 into s2 . This produces the ABox A24 shown on the righthand side of Figure 5, in which the assertion R(s3 , s2 ) makes A24 not forest-shaped. By
extending the example, it is possible to use nominals, inverse roles, and number restrictions
to arrange blockable individuals in cycles. The derived ABoxes are thus not forest-shaped,
which makes defining suitable notions of pruning and unraveling difficult and prevents us
from using blocking to ensure termination of the calculus.
176

fiHypertableau Reasoning for Description Logics

a

s1

s2

a

s1

s2

c
b

s3

s4

a

s1
s2

c
b

s3

s4

b

c

s3

Figure 6: The Introduction of Root Individuals

To solve this problem, we need to extend the arbitrarily interconnected part of A24 by
changing the status of s2 from a blockable into a root individual that is, an individual similar to the named ones in that it can be arbitrarily interconnected. Our extended forest-like
ABoxes thus consist of a set of arbitrarily interconnected root individuals each of which can
be the root of a tree (ignoring reflexive connections and connections back to root individuals) that otherwise consists entirely of blockable individuals (see Figure 3 on page 172).
Named individuals are just the subset of the root individuals that occur in the input ABox.
When we talk about individuals, we mean either root or blockable ones (see Definition 7 on
page 193 for a formal definition).
Returning to our example, after changing the status of s2 from a blockable into a root
individual, only s1 and s3 are blockable in A24 , so the ABox has the extended forest-like
shape and we can apply blocking and pruning as usual. This is schematically shown in
Figure 6. More generally, we apply the following preliminary version of the NI -rule, which
we denote with (*) for easier reference:
We change s into a root individual whenever A contains assertions R(s, a) and
A(s) where a is a root or a named individual, s is a blockable individual that is
not a successor of a, and a must satisfy an at-most restriction  n R .A.
Note that, if s is a successor of a, then the part of the ABox involving s and a is forestshaped, so the NI -rule need not be applicable.
This solution, however, introduces another problem: the number of root individuals can
now grow arbitrarily, as shown in the following example.

(6)

A5 = 
{ A(b) }

A(x)  (R.A)(x),
A(x)  (S.{a})(x),
C5 =
S(y1 , x)  S(y2 , x)  S(y3 , x)  y1  y2  y2  y3  y1  y3

On A5 and C5 , our calculus can produce the ABox A15 shown on the left-hand side of Figure
7. ABox A15 does not explicitly contain at-most restriction concepts, so the precondition
of (*) cannot be checked directly; we shall discuss this issue shortly. For the moment,
however, please note that the last DL-clause in C5 corresponds to the axiom > v  2 S  .>,
so individuals c and d can be seen as satisfying the precondition of (*); therefore, we change
them into root individuals. Furthermore, the third DL-clause from C5 is not satisfied, so the
Hyp-rule derives c  b, and the -rule can merge c into b. Since d is now not a blockable
individual, we cannot prune it, so we obtain the ABox A25 shown in the middle of Figure 7.8
8. To reduce clutter, we do not repeat the labels of individuals.

177

fiMotik, Shearer, & Horrocks

R
b
A
R.A
S.{a}

b
S

R
c
R

R

S

R
a

b
S

d

S

S

R
a

d
R

S

S

a

S
e

d
Figure 7: A Yo-Yo With Root Individuals

Since R.A(d) is not satisfied, we can extend A25 with R(d, e), A(e), R.A(e), S.{a}(e),
and S(e, a) to produce the ABox A35 shown on the right-hand side of Figure 7. Individual
e can be seen as satisfying the precondition of (*), so it is changed into a root individual.
This ABox is isomorphic to A15 , so we can repeat the same inferences forever.
We solve this problem with an NI -rule that refines (*). Assume that A contains an
individual s that satisfies the precondition of (*)that is, A contains assertions R(s, a)
and A(s), where a is a root or a named individual, s is a blockable individual that is not a
successor of a, and a must satisfy an at-most restriction  n R .A. In any model of A, there
can be at most n different individuals bi that participate in assertions of the form R(bi , a)
and A(bi ). Hence, we associate with a a set of n fresh root individuals {b1 , . . . , bn } that
represent the R -neighbors of a. We turn s into a root individual by nondeterministically
choosing bj from this set and merging s into bj . In this way, the number of new root
individuals that can be introduced as a result of the at-most restriction  n R .A on a is
limited to n. The complete definition of the NI -rule is given in Table 5 on page 196. In the
example from Figure 7, the NI -rule introduces at most two fresh root individuals. When
the NI -rule is applied for the third time, instead of introducing e, one of the previously
introduced root individuals is reused, which ensures termination of the calculus.
When formulating the NI -rule, we are faced with a technical problem: at-most restriction concepts are translated in our calculus into DL-clauses, which makes testing the
condition from the previous paragraph difficult. For example, an application of the Hyprule to the third DL-clause in (6) (obtained from the axiom > v  2 S  .>) can produce
an equality such as c  b; this equality alone does not reflect the fact that a must satisfy
the at-most restriction  2 S  .>. To enable the application of the NI -rule, we introduce
annotated equalities in which the annotations establish an association with the at-most
restriction. The third DL-clause from (6) is thus represented in our algorithm as follows:
(7)

S(y1 , x)  S(y2 , x)  S(y3 , x) 
y1  y2 @x2 S  .>  y2  y3 @x2 S  .>  y1  y3 @x2 S  .>

The Hyp-rule then derives c  b @a2 S  .> , which has the same meaning as c  b; however,
the annotation says that, since a must satisfy the at-most restriction  2 S  .>, both b and
c must also be merged with one of the (two) individuals reserved as S  -neighbors of a.
178

fiHypertableau Reasoning for Description Logics

R.B
D
S
a
R
B
R.C

b

S

C
S.D
R.B
D
S
c

C
S.D
R.B
D
S
c

R.B
D
S
a

R

R

B
R.C

R

d

S

B
R.C

b

c

e

c

C
S.D

C
S.D

C
S.D

(a) Nonterminating Variant

S

R

R
b

R

R

R.B
D
C
S.D
S
a

B
R.C

(b) Terminating Variant

Figure 8: The Caterpillar Example

3.2.5 Nominals and Merging
The introduction of the NI -rule leads to another problem: repeated merging between root
individuals can lead to nontermination in a caterpillar derivation. Consider, for example,
an application of the hypertableau calculus to the following knowledge base:

	
S(a, a), R.B(a)
A6 =


B(x)  R.C(x),
C(x)  S.D(x),
(8)
C6 =
D(x)  x  a,
S(y1 , x)  S(y2 , x)  y1  y2 @x1 S  .>
The ABox and the first DL-clause cause the introduction of two new blockable individuals b and c; the next two DL-clauses connect c with a by the role S; the last DL-clause
produces c  c @x1 S  .> ; and an application of the NI -rule to this assertion causes c to
become a root individual. The ABox A16 resulting from these inferences is shown in the
left-hand side of Figure 8a. Since S is inverse-functional, the individuals a and c must be
merged. Because individual c is a root, it is no longer a descendant of a, so we can choose
to merge a into c. The blockable individual b is then pruned (in order to avoid the problems
outlined in Section 3.2.3), and the resulting ABox is shown in the middle part of Figure
8a. The existential restriction R.B on c, however, is not satisfied, so a similar sequence
of rule applications constructs the ABox A26 shown in the right-hand side of of Figure 8a.
This ABox is isomorphic to A16 , so the same inferences can be repeated forever.
This problem can be intuitively explained by the following observation. The NI -rule
introduces fresh root individuals as neighbors of an existing root individual; thus, each
179

fiMotik, Shearer, & Horrocks

root individual in an ABox can be seen as a part of a chain showing which individual
caused the introduction of which root individual. Each chain is initially anchored at a
named individual: such individuals occur in the input ABox and are not introduced by the
NI -rule. The length of a path of blockable individuals can be used to limit the length of
the chains of root individuals. If we allow chain anchors to be removed from an ABox,
then the chains remain limited in length in any given ABox; however, over the course of
derivation, one end of the chain can be extended indefinitely as the other end is shortened.
We solve this problem by allowing named individuals to be merged only into other
named individuals, as specified by the postcondition of the -rule in Table 5 on page
196. This ensures that each chain of root individuals always remains anchored at a named
individual. In our example, instead of merging a into c, we merge c into a, which results
in the ABox shown in Figure 8b. No derivation rule is applicable to this ABox, so the
algorithm terminates.
3.2.6 The NI -Rule and Unraveling
The NI -rule is required not only to ensure that ABoxes are forest shaped, but also to
enable the application of blocking and unraveling. Consider, for example, the knowledge
base shown in (9), in which we omit the annotations on equalities for the sake of clarity.
Intuitively, the axioms of the knowledge base state that the individual a can have no R neighbors, and that there is an infinite chain of individuals each of which is an S  -neighbor
of a.

(9)

A7 = {
A(a), (R.B)(a), }

A(x)

R(y,
x)

,
B(x)

(R.B)(x),
B(x)

(S.{a})(x),






R(y1 , x)  R(y2 , x)  y1  y2 ,
C7 =
S(y1 , x)  S(y2 , x)  S(y3 , x)  S(y4 , x) 






y1  y2  y1  y3  y1  y4  y2  y3  y2  y4  y3  y4 ,

Without the NI -rule, an application of our calculus to A7 and C7 might produce the
ABox A17 shown in Figure 9a. The individual d is blocked in A17 by the individual c, so
the derivation terminates. Note that the last DL-clause from C7 (which corresponds to the
axiom > v  3 S  .>) is satisfied: a is the only individual in A17 that has S  -neighbors and
it has only two such neighbors. To construct a model from A17 , we unravel the blocked parts
of the ABoxthat is, we construct an infinite path that extends past d by duplicating
the fragment of the model between c and d an infinite number of times. This, however,
creates additional S  -neighbors of a, which invalidates the last DL-clause from C7 ; thus,
the unraveled ABox does not define a model of A7 and C7 .
The NI -rule elegantly solves this problem. Since a must satisfy an at-most restriction of
the form  3 S  .>, as soon as S(b, a), S(c, a), and S(d, a) are derived, the NI -rule is applied
to turn b, c, and d into root individuals. This corrects the problems with unraveling: root
individuals do not become blocked, so we introduce another fresh blockable individual e.
This individual is merged with another S  -neighbor of a, producing an individual with two
R -neighbors, as illustrated in Figure 9b. R is inverse-functional, however, so the neighbors
are merged. Merging continues until b has been merged into a, causing a to become its own

180

fiHypertableau Reasoning for Description Logics

A
R.B
a

R
S
R

b
B
R.B
S.{a}

S
c
B
R.B
S.{a}

R

d
B
R.B
S.{a}

(a) Premature Blocking

A
R.B
a

R
S
b
B
R.B
S.{a}

A
R.B

S

S

R

a

R

c
B
R.B
S.{a}

R

S

S
R

d
B
R.B
S.{a}

e

b

B
R.B
S.{a}

B
R.B
S.{a}

S

S

R

c
B
R.B
S.{a}

R

R
d
B
R.B
S.{a}

(b) A Correct Derivation

Figure 9: The NI -rule and Unraveling
R-neighbor, at which point our algorithm correctly determines that the knowledge base
represented by A7 and C7 is unsatisfiable.
3.3 Related Work
3.3.1 Hypertableau vs. Absorption
Absorption has been extensively used in tableau calculi to address the problems with orbranching outlined in Section 3.1.1 (Horrocks, 2007). The basic absorption algorithm tries to
rewrite GCIs into the form A v C where A is an atomic concept. After such preprocessing,
instead of deriving A t C for each individual in an ABox, C(s) is derived only if the ABox
contains A(s); thus, the nondeterminism introduced by the absorbed GCIs is localized. This
basic technique has been refined and extended in several ways. Negative absorption rewrites
GCIs into the form A v C where A is an atomic concept; then, C(s) is derived only if
an ABox contains A(s) (Horrocks, 2007). Role absorption rewrites GCIs into the form
R.> v C; then, C(s) is derived only if an ABox contains R(s, t) (Tsarkov & Horrocks,

181

fiMotik, Shearer, & Horrocks

2004). Binary absorption rewrites GCIs into the form A1 u A2 v C; then, C(s) is derived
only if an ABox contains both A1 (s) and A2 (s) (Hudek & Weddell, 2006).
These techniques have proven indispensable in practice; however, our analysis shows potential for further improvement. For example, the axiom R.A v A from (2) cannot be absorbed directly, and applying role absorption to (2) produces the axiom R.> v A t R.A
containing a disjunction in the consequent. Binary absorption is not directly applicable to
(2) since the axiom does not contain two concepts on the left-hand side of v, but the algorithm by Hudek and Weddell (2006) additionally transforms (2) into an absorbable axiom
A v R .A. Consider, however, the following axiom:
> v R.C t S.D

(10)

The binary absorption algorithm can process the two disjuncts in (10) in two ways. If
R.C is processed before S.D, then (10) is transformed into the axioms shown in (11),
both of which can be applied deterministically in a tableau algorithm. If, however, S.D is
processed before R.C, then (10) is transformed into the axioms shown in (12). The first
axiom is absorbable, but the second is not, so a tableau algorithm will be nondeterministic.
(11)
(12)

C v R .Q1

Q1 v S.D
> v D t S  .Q2

Q2 v R.C

Heuristics are used in practice to find a good absorption (see, e.g., Wu & Haarslev, 2008),
but there are no guarantees that the result will incur the least amount of nondeterminism;
this is so even on Horn knowledge bases, for which reasoning without any nondeterminism
is possible in principle (Hustadt et al., 2005). In contrast, our algorithm is guaranteed
to preprocesses a Horn knowledge base into Horn DL-clauses that will always result in
deterministic derivations. For example, (10) is transformed into a Horn DL-clause (13).
(13)

R(x, y1 )  C(y1 )  S(x, y2 )  D(y2 )

Even in the case of inherently nondeterministic knowledge bases, absorption can be
further optimized. Consider axiom (14), which is translated into DL-clause (15):
(14)
(15)

> v A t R.B t S.C
R(x, y1 )  S(x, y2 )  A(x)  B(y1 )  C(y2 )

The binary absorption algorithm transforms (14) into the following axioms:
(16)

Q1 u Q2 v A

(17)

> v B t R .Q1

(18)

> v C t S  .Q2

Axiom (16) is absorbable; however, (17) and (18) are not, so their application introduces a
nondeterministic choice point for each individual occurring in an ABox. This problem can
be ameliorated by using role absorption and transforming (17) and (18) into (19) and (20):
(19)

R .> v B t R .Q1
182

fiHypertableau Reasoning for Description Logics

(20)

S  .> v C t S  .Q2

Now (19) can be used to derive (B t R .Q1 )(b) from R(a, b), and (20) can be used to
derive (C t S  .Q2 )(d) from S(c, d); however, these two disjunctions are derived even if
a 6= c. In contrast, the DL-clause (15) derives a disjunction only if a = c; thus, literals
R(x, y1 ) and S(x, y2 ) in (15) act as guards. The presence of variables in the antecedent
(the shared variable x in this example) makes the guards more selective than if each guard
were applied in isolation. Furthermore, if a = c, we derive a disjunction A(a)  B(b)  C(d),
which involves three different individuals (a, b, and d in this case); in contrast, consequences
of tableau algorithms typically involve just one individual. Thus, through the usage of
variables, DL-clauses can be more global in their effect than tableau rules.
To the best of our knowledge, no known absorption technique can localize the effects of
axioms with number restrictions, such as (21).
(21)

 2 R.B v A

In order to ensure that only instances of B are counted, tableau algorithms need to include
a choose-rule that, for each assertion R(a, b), nondeterministically derives B(b) or B(b).
In the hypertableau setting, however, (21) is translated into the following DL-clause:
(22)

R(x, y1 )  R(x, y2 )  B(y1 )  B(y2 )  A(x)  y1  y2

No choose-rule is needed, as the DL-clause is simply applied to assertions of the form
R(a, b), B(b), R(a, c), and B(c); furthermore, the conclusion is a tautology whenever b = c.
The presence of guard atoms in the antecedent of (22) thus significantly reduces the
nondeterminism introduced by such number restrictions. Furthermore, on Horn knowledge
bases with number restrictions (which includes the common case of functional roles), our
calculus exhibits no nondeterminism; in contrast, tableau calculi still need the choose-rule,
which introduces nondeterminism even if all GCIs have been fully absorbed.
The hypertableau calculus as presented in this paper does not generalize negative absorption directly; for example, the negatively absorbed axiom (23) is translated into a
DL-clause (24) which is then applied to all individuals in an ABox.
(23)
(24)

A v B
 A(x)  B(x)

Negative absorption can, however, easily be applied in our setting: to negatively absorb an
atomic concept A, we simply replace in the input ABox and the DL-clauses all occurrences
of A with A0 where A0 is a fresh concept, and then move the literals involving A0 to the
appropriate side of DL-clauses. In our example, (24) would be thus converted into (25),
which can then be applied deterministically.
(25)

A0 (x)  B(x)

Note that this will transform a DL-clause A(x)  B(x) into  A0 (x)  B(a); however, a
similar situation arises in tableau calculi, where applying negative absorption to A v B
means that A v B cannot be absorbed.
183

fiMotik, Shearer, & Horrocks

To summarize, unlike various absorption techniques that are guided primarily by heuristics, the hypertableau calculus provides a framework that captures all variants of absorption
we are aware of, guarantees deterministic behavior whenever the input knowledge base is
Horn, eliminates the need for the nondeterministic choose-rule, and allows for a more powerful use of guard atoms to further localize any remaining nondeterminism. Furthermore,
in Section 4.1.3 we show that the our calculus provides a proof-theoretic framework for DLs
that can uniformly handle certain useful extensions of SHOIQ+ .
3.3.2 Relationship with Caching
Various caching optimizations can be used to reduce the sizes of the models constructed
during knowledge base classification (Ding & Haarslev, 2006; Horrocks, 2007). In the proposed approaches, caching is used in parallel with blockingthat is, caching alone does
not guarantee termination of the calculus, and caching must be carefully integrated with
blocking in order not to affect soundness and/or completeness. This integration is particularly problematic in the presence of inverse roles. In contrast, anywhere blocking alone is
sufficient to guarantee termination of the calculus. Furthermore, in Section 6.2 we present
an optimization of anywhere blocking that can be seen as a very simple but effective form of
general caching. Finally, as we discuss in Section 7, an efficient implementation of anywhere
blocking can be obtained using very simple techniques. Thus, anywhere blocking achieves
many of the effects of caching without much of the added complexity.
Donini and Massacci (2000) have used anywhere blocking with caching of unsatisfiable
concepts to obtain a tableau algorithm for the DL ALC that runs in single exponential time.
Gore and Nguyen (2007) have presented an algorithm for the DL SHI that also runs in exponential time and achieves termination solely by caching both satisfiable and unsatisfiable
concepts. These algorithms, however, seem to be incompatible with all absorption variants,
and the latter are essential for making tableau algorithms practical. Furthermore, it is
unclear how to extend these algorithms to DLs that provide number restrictions, nominals,
and inverse roles, such as SHOIQ+ .
3.3.3 Relationship with First-Order Calculi
The original hypertableau calculus for first-order logic was subsequently extended with
equality and has been implemented in the KRHyper theorem prover (Baumgartner, Furbach, & Pelzer, 2008). The calculus can be used for finite model generation, and it decides
function-free clause logic.
Hyperresolution with splitting has been used to decide several description and modal
logics (Georgieva et al., 2003; Hustadt & Schmidt, 1999). These approaches, however, rely
on skolemization, which, as we have discussed previously, can be inefficient in practice.
Furthermore, these approaches deal with logics that are much weaker than SHOIQ+ ; in
particular, we are not aware of a hyperresolution-based decision procedure that can handle
inverse roles, number restrictions, and nominals.
Our hypertableau calculus is related to the Extended Positive (EP) tableau calculus for
first-order logic by Bry and Torge (1998). Instead of relying on skolemization, EP satisfies
existential quantifiers by introducing new constants, and this is done in a way that makes
the calculus complete for finite satisfiability. EP is, however, unlikely to be practical due to

184

fiHypertableau Reasoning for Description Logics

a high degree of nondeterminism. Furthermore, EP does not provide a decision procedure
for DLs such as SHOIQ+ that do not enjoy the finite model property (Baader & Nutt,
2007). Consider, for example, the knowledge base whose TBox contains axioms (26) and
(27), and whose ABox contains assertion (28):
(26)

A v R.A

(27)

> v  1 R .>

(28)

(A u R.A)(a)

EP will try to satisfy the existential quantifier on a by reusing athat is, by adding
assertions R(a, a) and A(a). This leads to a contradiction, so EP will backtrack, introduce
a fresh individual b, and add assertions R(a, b) and A(b); to satisfy (26), it will then also
add R.A(b). To satisfy the existential quantifier in the latter assertion, EP will again try
to reuse a; this will fail, so it will try to reuse b by adding an assertion R(b, b). Due
to (27), however, b will be merged into a, which results in a contradiction; therefore, EP
will backtrack, introduce yet another fresh individual c and add the assertions R(b, c), A(c),
and R.A(c). By repeating the argument, it is easy to see that EP will generate ever larger
models and will not terminate. This is unsurprising since the knowledge base is satisfied
only in infinite models. To achieve termination on such knowledge bases, EP would need
to be extended with blocking techniques such as the ones described in this paper.
Baumgartner and Schmidt (2006) developed a so-called blocking transformation of firstorder clauses, which can improve the performance of bottom-up model generation methods.
Roughly speaking, the clauses are modified in a way that makes a bottom-up calculus derive
s  t or s 6 t for each term s that is a subterm of t; then, an application of paramodulation
to s  t achieves an effect that is analogous to reusing s instead of t in the EP tableau
calculus. This transformation, however, does not ensure termination for DLs that do not
have the finite model property. For example, for the same reasons as explained in the previous paragraph, hyperresolution with splitting does not terminate on the clauses obtained by
an application of the blocking transformation to (the clauses corresponding to) (26)(28).
Furthermore, even for DLs that enjoy the finite model property, an unlucky sequence of
applications of derivation rules can prevent a bottom-up model generation method with
blocking from terminating (please refer to Section 3.2.3 for more details).

4. The Satisfiability Checking Algorithm
We now present the hypertableau algorithm that can be used to check the satisfiability of
a SHOIQ+ knowledge base K. Our algorithm consists of two phases: the preprocessing
phase is described in Section 4.1, and the hypertableau phase is described in Section 4.2.
4.1 Preprocessing
The goal of the preprocessing phase is to transform a SHOIQ+ knowledge base K into an
ABox A (K) and a set of DL-clauses T R (K) that are equisatisfiable with K.
Definition 1 (DL-Clause). The concepts >, , and concepts of the form A and A for A
an atomic concept are called literal concepts. Let NV be a set of variables disjoint from the
185

fiMotik, Shearer, & Horrocks

Table 2: Satisfaction of DL-Clauses in an Interpretation
I,  |= C(s)
I,  |= R(s, t)
I,  |= s  t
Wn
V
I,  |= m
j=1 Vj
i=1 Ui 

iff
iff
iff
iff

Wn
V
I |= m
j=1 Vj
i=1 Ui 
I |= C

iff
iff

sI,  C I
hsI, , tI, i  RI
sI, = tI,
I,  |= Ui for each 1  i  m implies
I,  |= Vj for some 1  j  n
Wn
V
I,  |= m
j=1 Vj for all mappings 
i=1 Ui 
I |= r for each DL-clause r  C

set of individuals NI . An atom is an expression of the form B(s),  n S.B(s), R(s, t), or
s  t, for s and t individuals or variables, B a literal concept, R an atomic role, S a (not
necessarily atomic) role, and n a positive integer. A DL-clause is an expression of the form
U1  . . .  Um  V1  . . .  Vn
where Ui and Vj are atoms, m  0, and n  0. The conjunction U1  . . .  Um is called the
antecedent, and the disjunction V1  . . .  Vn is called the consequent. The empty antecedent
and the empty consequent of a DL-clause are written as > and , respectively.
Let I = (4I , I ) be an interpretation and  : NV  4I a mapping of variables to elements of the interpretation domain. Let aI, = aI for an individual a and xI, = (x) for
a variable x. Satisfaction of an atom, DL-clause, and a set of DL-clauses C in I and  is
defined in Table 2.

4.1.1 Elimination of Transitivity Axioms
Transitivity axioms are handled in tableau algorithms by the + -rule: if R is transitive
and an ABox contains R.C(s) and R(s, t), the + -rule derives R.C(t). In our algorithm,
however, concepts of the form R.C are translated into DL-clauses, so the + -rule cannot
be applied. Therefore, instead of handling transitivity directly, we encode a SHOIQ+
knowledge base K into an equisatisfiable ALCHOIQ+ knowledge base (K). This encoding
eliminates all transitivity axioms, but simulates their effects using additional GCIs.
Definition 2. Given a SHOIQ+ knowledge base K = (R, T , A), the concept closure of K
is the smallest set of concepts clos(K) such that
 if C v D  T , then nnf(C t D)  clos(K),
 if C(a)  A, then nnf(C)  clos(K),
 if C  clos(K) and D syntactically occurs in C, then D  clos(K),
 if  n R.C  clos(K), then C
  clos(K), and
 if R.C  clos(K), S vR R, and Tra(S)  R, then S.C  clos(K).

186

fiHypertableau Reasoning for Description Logics

The -encoding of K is the ALCHOIQ+ knowledge base (K) = (R0 , T 0 , A) where R0 is
obtained from R by removing all transitivity axioms and
T 0 = T  {R.C v S.(S.C) | R.C  clos(K), S vR R, and Tra(S)  R}.
Similar encodings are known for various description (Tobies, 2001) and modal (Schmidt
& Hustadt, 2003) logics. Note that, in order to guarantee decidability (Horrocks, Sattler,
& Tobies, 2000a), number restrictions and local reflexivity are allowed in SHOIQ+ only
on simple rolesthat is, on roles not having transitive subroles; for similar reasons, role
disjointness, irreflexivity, and asymmetry axioms are also allowed only on simple roles.
Lemma 1. A SHOIQ+ knowledge base K is satisfiable if and only if (K) is satisfiable,
and (K) can be computed in time polynomial in |K|.
The full proof of an analogous result for the DL SHIQ is given by Motik (2006) in Theorem 5.2.3, and the generalization of this result to SHOIQ+ is straightforward; therefore,
we omit the proof of Lemma 1 for the sake of brevity. After the elimination of transitivity
axioms, there is no distinction between simple and complex roles. Hence, in the rest of
this paper we assume that all roles are simple unless otherwise stated and, without loss of
generality, we treat R.B as a syntactic shortcut for  1 R.B.
4.1.2 Normalization
Before translation into a set of DL-clauses, a a knowledge base is first brought into a
normalized form. This is done in order to make all negations explicit, and to ensure that
the resulting DL-clauses are compatible with blocking.
To understand the first issue, consider the axiom A v (R.R.R.B). Converting this
axiom into DL-clauses is not straightforward because of the implicit negations; for example,
the concept A is seemingly negated but, due to the negation implicit in the implication, A
actually occurs positively in the axiom. Therefore, we replace this axiom with the following
equivalent axiom. This makes all negations explicit, so the result can be easily translated
into a DL-clause.
(29)

> v A t R.R.R.B

R(x, y1 )  R(y1 , y2 )  R(y2 , y3 )  B(y3 )  A(x)

To understand the second issue, consider the knowledge base K8 , consisting of an ABox
A8 and a TBox that corresponds to the set of DL-clauses C8 .
(30)

A8 = { A(a), B(a) }
C8 = { R(x, y1 )  R(y1 , y2 )  R(y2 , y3 )  B(y3 )  A(x),

B(x)  R.B(x) }

By applying the rules from Section 3.2, our algorithm constructs on K8 the ABox shown
in Figure 10. According to the definition of blocking introduced in Definition 7,9 c is now
blocked by b; furthermore, no rule is applicable to the ABox, so the algorithm terminates,
leading us to believe that K8 is satisfiable. The ABox, however, does not represent a model of
K8 : if we expand R.B(c) into R(c, d) and B(d), by the first DL-clause in C8 we can derive
9. The version of blocking introduced in Definition 7 differs from the one presented in Section 3.1.2 in that
the concept label LA (s) of an individual s consists only of atomic concepts A such that A(s)  A.

187

fiMotik, Shearer, & Horrocks

a
B
R.B
A

R

b
B
R.B

R

c
B
R.B

Figure 10: Incorrect Blocking due to Lack of Normalization

A(a), which then contradicts A(a). This problem arises because the antecedent of the
first DL-clause in C8 checks for a path of three R-successors, whereas the pairwise blocking
condition ensures only that all paths of length two are fully constructed. Intuitively, the
antecedents of each DL-clause should check for paths that fit into the fully constructed
model fragments. We can ensure this by renaming complex concepts into simpler ones.
Thus, we transform the culprit DL-clause into the following ones, which check only for
paths of length one.
(31)

> v A t R.Q1

R(x, y)  Q1 (y)  A(x)

(32)

> v Q1 t R.Q2

R(x, y)  Q2 (y)  Q1 (x)

(33)

> v Q2 t R.B

R(x, y)  B(y)  Q2 (x)

The application of these DL-clauses to the ABox shown in Figure 10 would additionally
derive Q2 (a), Q2 (b), and Q1 (a), so c would not be blocked. The calculus would then expand
R.B(c) and discover a contradiction.
To formalize these ideas, we define a normalized form of DL knowledge bases.
F
Definition 3 (Normalized Form). A GCI is normalized if it is of the form > v ni=1 Ci ,
where each Ci is of the form B, {a}, R.B, R.Self, R.Self,  n R.B, or  n R.B, for
B a literal concept, R a role, and n a nonnegative integer.
A TBox T is normalized if each GCI in it is normalized. An ABox A is normalized if
each concept assertion in A contains only a literal concept, each role assertion in A contains
only an atomic role, and A contains at least one assertion. An ALCHOIQ+ knowledge
base K = (R, T , A) is normalized if T and A are normalized.
The following transformation can be used to normalize a knowledge base.
Definition 4 (Normalization). For an ALCHOIQ+ knowledge base K, the knowledge base
(K) is computed as shown in Table 3.
Normalization can be seen as a variant of the well-known structural transformation
(Plaisted & Greenbaum, 1986; Nonnengart & Weidenbach, 2001). An application of the
structural transformation to (29) would replace each complex subconcept with a positive
atomic concept, eventually producing > v A t R.Q1 . This axiom cannot be translated
into a Horn DL-clause, whereas (29) can; thus, the structural transformation can destroy
188

fiHypertableau Reasoning for Description Logics

Table 3: The Functions Used in the Normalization
S
() 
(> v nnf(C1 t C2 ))
RA
C1 vC2 T
S
(> v C t C 0 ) = (> v C t C 0 ) 
(> v 
 C 0 t Ci )
(K) = {>(a)} 

S

1in

for C 0 of the form C 0 = C1 u . . . u Cn and n  2
(> v C t R.D) = (> v C t R.D )  (> v 
 D t D)
(> v C t  n R.D) = (> v C t  n R.D )  (> v 
 D t D)
(> v C t  n R.D) = 
(> v C t  n R.
 D
 D

 )  (> v 
 t D)

if C is empty,
(> v C t {s}) =
(C(s)) otherwise.
(D(s)) = {D (s)}  (> v 
 D t nnf(D))
(R (s, t)) = {R(t, s)}
() = {} for any other axiom 

QC
if pos(C) = true
C =
, where QC is a fresh atomic concept unique for C
QC if pos(C) = false
pos(>) = false
pos() = false
pos(A) = true
pos(A) = false
pos({s}) = true
pos({s}) = false
pos(R.Self) = true
pos(R.Self) = false
pos(C1 u C2 ) = pos(C1 )  pos(C2 )
pos(C1 t C2 ) = pos(C
 1 )  pos(C2 )
pos(C
 1 ) if n = 0
pos(R.C1 ) = pos(C1 )
pos( n R.C1 ) =
pos( n R.C1 ) = true
true
otherwise
Note: A is an atomic concept, C(i) are arbitrary concepts, C is a possibly empty
disjunction of arbitrary concepts, D is not a literal concept, and a is a fresh individual.
Note that t is commutative, so C 0 in C t C 0 is not necessarily the right-most disjunct.

Horn-ness. To prevent this, we introduce the function pos(C) (c.f. Table 3) that returns
false if the clausification of C does not require adding atoms into the consequent of a DLclause. We then replace an occurrence of a concept C in a concept D with a negative literal
concept QC if pos(C) = false, and with a positive literal concept QC if pos(C) = true.
Special care must be taken when replacing a concept D in a concept  n R.D: since D
occurs in  n R.D under an implicit negation, we replace D with 
 D
in order to preserve

Horn-ness. On a Horn knowledge base K (Hustadt et al., 2005), normalization performs the
same replacements as the one presented by Hustadt et al., so (K) is a Horn knowledge
base as well.
Lemma 2. The following properties hold for each ALCHOIQ+ knowledge base K and the
corresponding knowledge base (K):
 K is satisfiable if and only if (K) is satisfiable;
 (K) is normalized; and
189

fiMotik, Shearer, & Horrocks

 (K) can be computed in time polynomial in |K|.
Proof. (Sketch) Since our transformation can be seen a syntactic variant of the structural
transformation, the proof that K and (K) are equisatisfiable is completely analogous to
the ones by Plaisted and Greenbaum (1986) and Nonnengart and Weidenbach (2001), so we
omit it for the sake ofFbrevity. For the second claim, note that  essentially rewrites each
GCI into a form > v ni=1 Ci and then keeps replacing nested subconcepts of Ci until the
GCI becomes normalized; it adds >(a) to the ABox so that the ABox is not empty; and
it replaces all inverse role assertions with equivalent assertions on the atomic roles. Thus,
(K) is normalized. Finally, each occurrence of a concept in K can be replaced with a new
atomic concept at most once, and all necessary syntactic transformations can be performed
in polynomial time, so (K) can be computed in polynomial time.
4.1.3 Translation into DL-Clauses
We now introduce the notion of HT-clausessyntactically restricted DL-clauses on which
our hypertableau calculus is guaranteed to terminate. In the rest of this paper, we often
use the function ar, which, given a role R and variables or constants s and t, returns an
atom that is semantically equivalent to R(s, t) but that contains an atomic role; that is,

R(s, t) if R is an atomic role
ar(R, s, t) =
.
S(t, s) if R is an inverse role and R = S 
Definition 5 (HT-Clause). We assume that, for each individual a, the set of atomic concepts NC contains a unique nominal guard concept which we denote as Oa ; furthermore,
we assume that nominal guard concepts do not occur in any input knowledge base.
An annotated equality is an atom of the form s  t @un S.B , where s, t, and u are
constants or variables, n is a nonnegative integer, S is a role, and B is a literal concept;
the part @un S.B of the atom is called the annotation. This atom is semantically equivalent
to s  t.10
An HT-clause is a DL-clause r of the following form, for m  0 and n  0:
(34)

U1  . . .  Um  V1  . . .  Vn

Furthermore, it must be possible to separate the variables into a center variable x, a set
of branch variables yi , and a set of nominal variables zj such that the following properties
hold, for A an atomic concept, B a literal concept not containing a nominal guard concept,
Oa a nominal guard concept, R an atomic role, and S a role.
 Each atom in the antecedent of r is of the form A(x), R(x, x), R(x, yi ), R(yi , x),
A(yi ), or A(zj ).
 Each atom in the consequent of r is of the form B(x),  h S.B(x), B(yi ), R(x, x),
R(x, yi ), R(yi , x), R(x, zj ), R(zj , x), x  zj , or yi  yj @xh S.B .
 Each yi occurs in the antecedent of r in an atom of the form R(x, yi ) or R(yi , x).
10. As explained in Section 3.2.4, annotations are only used to ensure termination of the hypertableau phase.

190

fiHypertableau Reasoning for Description Logics

 Each zj occurs in the antecedent of r in an atom of the form Oa (zj ).
 Each equality yi  yj @xh S.A in the consequent of r occurs in a subclause of r of the
form (35) where y 1 , . . . , y h+1 are branch variables such that no y k with 1  k  h + 1
occurs elsewhere in r.
(35)

...

h+1
^

_

k=1

1k<`h+1

[ar(S, x, y k )  A(y k )] . . .  . . .

y k  y ` @xh S.A . . .

 Each equality yi  yj @xh S.A in the consequent of r occurs in a subclause of r of the
form (36) where y 1 , . . . , y h+1 are branch variables such that no y k with 1  k  h + 1
occurs elsewhere in r.
(36)

...

h+1
^

ar(S, x, y k ) . . .  . . .

k=1

h+1
_

A(y k ) 

k=1

_

y k  y ` @xh S.A . . .

1k<`h+1

HT-clauses are more general than what is strictly needed to capture ALCHOIQ+ knowledge bases. For example, HT-clauses of the form R(x, y)  A(y)  S(x, y) express a form of
relativized role inclusions, and HT-clauses of the form R(x, y)  S(y, x)  U (x, y)  T (y, x)
capture safe role expressions (Tobies, 2001).
We now show how to transform a normalized ALCHOIQ+ knowledge base into a set
of HT-clauses, after which we explain the need for nominal guard concepts.
Definition 6 (Clausification). The clausification of a normalized ALCHOIQ+ knowledge
base K = (R, T , A) is the pair (K) = (T R (K), A (K)) in which T R (K) is a set of DLclauses and A (K) is an ABox, both obtained as shown in Table 4.
By Definition 3, concepts of the form {a} are converted to ABox assertions during
normalization, so Table 4 need not handle them. Positive nominal concepts are naturally translated into equalities containing constants; for example, > v A t {a} corresponds to A(x)  x  a. Such DL-clauses are impractical: given an equality assertion
a  b, the -rule would need to replace all occurrences of a with b not only in the assertions, but in the DL-clauses as well; thus, the mentioned DL-clause should be replaced
with A(x)  x  b. To avoid the need for changing a set of DL-clauses in a derivation,
we extract all constants into the ABox; for example, > v A t {a} is transformed into
the DL-clause A(x)  Oa (z{a} )  x  z{a} and the assertion Oa (a). All constants are thus
pushed into the assertions, so the -rule can perform replacements only in the ABox.
Lemma 3. Let K be a normalized ALCHIQ knowledge base. Then, K is equisatisfiable
with (K) = (T R (K), A (K)), and T R (K) contains only HT-clauses.
Proof. By inspecting Table 4, T R (KB) clearly contains only HT-clauses. The following
equivalences between DL concepts and first-order formulae are well known (Borgida, 1996):
R.B(x)  y : R(x, y)  B(y)
V
 n R.B(x)  y1 , . . . , yn+1 :
[R(x, yi )  B(yi )] 
1in+1

{a}(x)  x  a
191

W
1i<jn+1

yi  yj

fiMotik, Shearer, & Horrocks

Table 4: Translation of a Normalized Knowledge Base to HT-Clauses
T (T ) = {

n
V

lhs(Ci ) 

i=1

n
W

n
F

rhs(Ci ) | for each > v

i=1

Ci in T }

i=1

R (R) = {ar(R, x, y)  ar(S, x, y) | for each R v S in R} 
{ar(S1 , x, y)  ar(S2 , x, y)   | for each Dis(S1 , S2 )  R} 
{>  ar(R, x, x) | for each Ref(R)  R} 
{ar(S, x, x)   | for each Irr(S)  R} 
{ar(R, x, y)  ar(R, y, x) | for each Sym(R)  R} 
{ar(S, x, y)  ar(S, y, x)   | for each Asy(S)  R}
T R (K) = T (T )  R (R)
A (K) = A  {Oa (a) | for each {a} occurring in K}
Note: Whenever lhs(Ci ) or rhs(Ci ) is undefined, it is omitted in the HT-clause.
C
lhs(C)
rhs(C)
A
A(x)
A

A(x)

{a}

Oa (zC )

x  zC

 n R.A

 n R.A(x)

 n R.A

 n R.A(x)

R.Self

ar(R, x, x)

R.Self

ar(R, x, x)

R.A

ar(R, x, yC )

R.A
 n R.A

A(yC )

ar(R, x, yC )  A(yC )
n+1
V

W

i=1

1i<jn+1

i )  A(y i )]
[ar(R, x, yC
C
n+1
V

 n R.A

i=1

i )
ar(R, x, yC

n+1
W
i=1

i )
A(yC

i  y j @x
yC
C n R.A

W
1i<jn+1

i  y j @x
yC
C n R.A

(i)

Note: Each yC and zC is a fresh variable unique for C (and i).
Let 0T R (K) be the set of HT-clauses defined just like T R (K), but with the difference that
lhs({a}) = > and rhs({a}) = x  a. Then, (0T R (K), A (K)) is obtained from K by replacing concepts of the form R.B,  n R.B and {a} with the equivalent first-order formulae,
so K and (0T R (K), A (K)) are clearly equisatisfiable. We now show that (0T R (K), A (K))
is equisatisfiable with (T R (K), A (K)).
() Each model I 0 of (0T R (K), A (K)) is extended to a model I of (T R (K), A (K))
0
by setting OaI = {aI } for each nominal guard concept Oa .

192

fiHypertableau Reasoning for Description Logics

() Each model I of (K) is a model of (0T R (K), A (K)): for each   0T R (K), we
have   T R (K) and Oak (ak )  A (K), where  and  are of the form shown below.
V
W
W
 = V Ui V Vj  nk=1 xk  aWk
W
=
Ui  nk=1 Oak (z{ak } )  Vj  nk=1 xk  z{ak }
W
Now if the disjunction nk=1 xk  ak in some  were not true in I for some values of
x1 , . . . , xn , then clearly  would not be true in I for the same values of x1 , . . . , xn .
4.2 The Hypertableau Calculus for HT-Clauses
We now present the hypertableau calculus for deciding the satisfiability of an ABox A
and a set of HT-clauses C. As explained in Section 3, our algorithm uses several types of
individuals. Each individual is either root or blockable as summarized next; when we refer
simply to an individual, we mean either a root or a blockable one.
 Root individuals are those that either occur in the input ABox, or are introduced by
the NI -rule. Their important characteristic is that they can be connected in arbitrary,
and not just tree-like, ways.
 Root individuals that occur in the input ABox are called named individuals.
 Root individuals that are introduced by the NI -rule are defined as finite strings
of the form a.1 . . . . .n where a is a named individual, each ` is of the form
hR.B.ii, and n  0. Root individuals introduced by applying the NI -rule to an
assertion s  t @un R.B are all of the form u.hR.B.ii with 1  i  n.
 Blockable individuals are introduced by the -rule, and make up the tree-like parts of
a model. The set of blockable individuals is disjoint from the set of root individuals.
Blockable individuals are defined as finite strings of the form s.i1 .i2 . . . . .in where s
is a root individual, each i` is an integer, and n  1. This string representation
naturally induces the parentchild relationship between individuals; for example, s.2
is the second child of the individual s, which can be either blockable or root.
We now introduce our algorithm.
Definition 7 (Hypertableau Algorithm).
Individuals. Given a set of named individuals NI , the set of root individuals NO is
the smallest set such that NI  NO and, if x  NO , then x.hR, B, ii  NO for each role
R, literal concept B, and positive integer i. The set of all individuals NA is the smallest
set such that NO  NA and, if x  NA , then x.i  NA for each positive integer i. The
individuals in NA \ NO are blockable individuals. A blockable individual x.i is a successor
of x, and x is a predecessor of x.i. Descendant and ancestor are the transitive closures of
successor and predecessor, respectively.
ABoxes. The hypertableau algorithm operates on ABoxes that are obtained by extending
the standard definition from Section 2 as follows.
 In addition to assertions from Section 2, an ABox can contain annotated equality
assertions and a special assertion  that is false in all interpretations. Furthermore,
assertions can refer to the individuals from NA and not only from NI .
193

fiMotik, Shearer, & Horrocks

 Each (in)equality s  t (s 6 t) also stands for the symmetric (in)equality t  s (t 6 s).
The same is true for annotated equalities.
 An ABox A can contain renamings of the form a 7 b where a and b are root individuals. Let 7 be the reflexive-transitive closure of 7 in A. An individual b is the
canonical name of a root individual a in A, written b = kakA , if b is the only individual such that a 7 b and there exists no individual c 6= b such that b 7 c; if no such
individual exists, then kakA = a.11
An input ABox is an ABox containing only named individuals, no annotated equalities,
and no renamings, and in which all concepts are literal and all roles are atomic.
Satisfaction of such ABoxes in an interpretation is obtained by a straightforward generalization of the definitions in Section 2: all individuals are interpreted as elements of the
interpretation domain 4I , and I |= a 7 b iff aI = bI .
Pairwise Anywhere Blocking. The labels of an individual s and of an individual
pair hs, ti in an ABox A are defined as follows:
LA (s) = { A | A(s)  A and A is an atomic concept }
LA (s, t) = { R | R(s, t)  A }
Let  be a strict ordering (i.e., a transitive and irreflexive relation) on NA containing
the ancestor relationthat is, if s0 is an ancestor of s, then s0  s. By induction on , we
assign to each individual s in A a status as follows:
 a blockable individual s is directly blocked by a blockable individual t if and only if the
following conditions are satisfied, for s0 and t0 the predecessors of s and t, respectively:
 t is not blocked,
 t  s,
 LA (s) = LA (t) and LA (s0 ) = LA (t0 ), and
 LA (s, s0 ) = LA (t, t0 ) and LA (s0 , s) = LA (t0 , t);
 s is indirectly blocked iff it has a predecessor that is blocked; and
 s is blocked iff it is either directly or indirectly blocked.
Pruning. The ABox pruneA (s) is obtained from A by removing all assertions containing
a descendant of s.
Merging. The ABox mergeA (s  t) is obtained from pruneA (s) by replacing the individual s with the individual t in all assertions and their annotations (but not in renamings)
and, if both s and t are root individuals, adding the renaming s 7 t.
Derivation Rules. Table 5 specifies derivation rules that, given an ABox A and a set
of HT-clauses C, derive one or more ABoxes A1 , . . . , An . In the Hyp-rule,  is a mapping
11. As we show in Lemma 4, the derivation rules of our calculus ensure that 7 is a functional and acyclic
relation, so an individual b satisfying the definition always exists. The second part of the definition of
kakA is thus just a technical aid necessary to make the definition complete.

194

fiHypertableau Reasoning for Description Logics

from the set of variables NV to the individuals occurring in the assertions of A, and (U )
is the result of replacing each variable x in the atom U with (x).
Rule Precedence. The -rule can be applied to a (possibly annotated) equality s  t
in an ABox A only if A does not contain an equality s  t @un R.B to which the NI-rule is
applicable (with the same s and t).
Clash. An ABox A contains a clash iff   A; otherwise, A is clash-free.
Derivation. For a set of HT-clauses C and an input ABox A, a derivation is a pair
(T, ) where T is a finitely branching tree and  is a function that labels the nodes of T with
ABoxes such that the following properties hold for each node t  T :
 (t) = A if t is the root of T ;
 t is a leaf of T if   (t) or no derivation rule is applicable to (t) and C;
 t has children t1 , . . . , tn such that (t1 ), . . . , (tn ) are exactly the results of applying
one (arbitrarily chosen, but respecting the rule precedence) applicable rule to (t) and
C in all other cases.
We stress several important aspects of Definition 7. If the preconditions of the NI -rule
are satisfied for an annotated equality s  t @un R.B , then the rule must be applied even
if s = t; hence, such an equality plays a role in a derivation even though it is a logical
tautology. Furthermore, even though the NI -rule is not applied to s  t @un R.B if u is
a blockable individual, the equality cannot be eagerly simplified into s  t because u can
subsequently be merged into a root individual so the annotation might become important.
Finally, if C has been obtained by a normalization of a DL knowledge base that does not
use nominals, inverse roles, or number restrictions, then the precondition of the NI -rule will
never be satisfied, so we need not keep track of annotations at all.
Renamings are used to keep track of root individuals that are merged into other root
individuals, which is necessary to make the NI -rule sound. For example, if a root individual
a.hR, B, 2i is merged into a named individual b, then the NI -rule must use b instead of
a.hR, B, 2i in all future inferences.
The proof of Lemma 6 shows that assertions containing at least one indirectly blocked
individual are not used to construct a model from an ABox labeling a leaf in a derivation. All
derivation rules are therefore applicable only to individuals that are either directly blocked
or not blocked, as this is sufficient for completeness. Since all rules are sound, however, one
may choose to disregard this restriction if that makes implementation easier.
We next introduce a notion of HT-ABoxes, which formalizes the idea of forest-shaped
ABoxes introduced in Section 3.1.2.
Definition 8 (HT-ABoxes). An ABox A is an HT-ABox if it satisfies the following conditions, for R an atomic role, S a role, B a literal concept not containing a nominal guard
concept, Oa a nominal guard concept, s, t, u  NA , a  NO , b  NI , and i, j integers.
1. Each role assertion in A is of the form R(a, s), R(s, a), R(s, s.i), R(s.i, s), or R(s, s).

195

fiMotik, Shearer, & Horrocks

Table 5: Derivation Rules of the Hypertableau Calculus

Hyp-rule

-rule

-rule

-rule

NI -rule

If 1. r  C, where r = U1  . . .  Um  V1  . . .  Vn , and
2. a mapping  from the variables in r to the individuals of A exists
such that
2.1 there is no x  NV such that (x) is indirectly blocked,
2.2 (Ui )  A for each 1  i  m, and
2.3 (Vj ) 6 A for each 1  j  n,
then A1 := A  {} if n = 0;
Aj := A  {(Vj )} for 1  j  n otherwise.
If 1.  n R.B(s)  A,
2. s is not blocked in A, and
3. A does not contain individuals u1 , . . . , un such that
3.1 {ar(R, s, ui ), B(ui ) | 1  i  n}  {ui 6 uj | 1  i < j  n}  A, and
3.2 for each 1  i  n, either ui is a successor of s or ui is not blocked in A,
then A1 := A  {ar(R, s, ti ), B(ti ) | 1  i  n}  {ti 6 tj | 1  i < j  n}
where t1 , . . . , tn are fresh distinct successors of s.
If 1. s  t  A (the equality can possibly be annotated),
2. s 6= t, and
3. neither s nor t is indirectly blocked
then A1 := mergeA (s  t) if t is a named individual, or t is a root individual
and s is not a named individual, or s is a descendant of t;
A1 := mergeA (t  s) otherwise.
If
s 6 s  A or {A(s), A(s)}  A where s is not indirectly blocked
then A1 := A  {}.
If 1. s  t @un R.B  A (the symmetry of  applies as usual),
2. u is a root individual,
3. s is a blockable individual that is not a successor of u,
4. t is a blockable individual, and
5. neither s nor t is indirectly blocked
then Ai := mergeA (s  ku.hR, B, iikA ) for each 1  i  n.

2. Each equality in A is either of the form s  t @an R.B with s a blockable individual
that is not a successor of a and t a blockable individual, or it is a possibly annotated
equality of the form s.i  s.j, s.i  s, s.i.j  s, s  s, or s  a. (The symmetry of 
applies in all these cases as usual.)
3. Each concept assertion in A is of the form B(s),  n S.B(s), or Oa (b).
4. If A contains s  t @un R.B , then A also contains ar(R, u, s) and ar(R, u, t).
5. If A contains a blockable individual s.i in some assertion, then A must contain an
assertion of the form R(s, s.i) or R(s.i, s).
6. A contains at least one assertion.

196

fiHypertableau Reasoning for Description Logics

Table 6: Cases in an Application of the Hyp-Rule to Role Assertions
ar(R, u, s)
ar(R, v, a)
ar(R, v, a)
ar(R, v, a)
ar(R, v.n, a)

ar(R, u, t)
ar(R, v, b)
ar(R, v, v.n)
ar(R, v, v)
ar(R, v.n, v)

s  t @uk R.B
a  b @vk R.B
a  v.n @vk R.B
a  v @vk R.B
a  v @v.n
k R.B

ar(R, v, v.m)
ar(R, v, v.m)
ar(R, v.n, v.n.m)

ar(R, v, v.n)
ar(R, v, v)
ar(R, v.n, v)

v.m  v.n @vk R.B
v.m  v @vk R.B
v.n.m  v @v.n
k R.B

ar(R, v, v)
ar(R, v.n, v.n)

ar(R, v, v)
ar(R, v.n, v)

v  v @vk R.B
v.n  v @v.n
k R.B

ar(R, v.n, v)

ar(R, v.n, v)

v  v @v.n
k R.B

7. The relation 7 in A is acyclic, A contains at most one renaming a 7 b for an
individual a, and, if A contains a 7 b, then a does not occur in any assertion in A.
Clearly, each input ABox is an HT-ABox. We now prove that, given an HT-ABox, our
calculus produces only HT-ABoxes.
Lemma 4 (HT-Preservation). For C a set of HT-clauses and A an HT-ABox, each ABox
A0 obtained by applying a derivation rule to C and A is an HT-ABox.
Proof. Let C, A, and A0 be as stated in the lemma. We now analyze each derivation rule
from Table 5 and show that A0 satisfies the remaining conditions of HT-ABoxes.
(Hyp-rule) Consider an application of the Hyp-rule to an HT-clause r of type (34) with
a mapping , deriving an assertion (V ).
Assume that V is of the form yi  yj @xk R.B , so (V ) is of the form s  t @uk R.B . By
Definition 5, the antecedent of r then contains atoms of the form ar(R, x, yi ) and ar(R, x, yj )
so, by the precondition of the Hyp-rule, A contains assertions ar(R, u, s) and ar(R, u, t). If
u is a root individual and either s or t is a blockable individual that is not a successor
of u, then (V ) clearly satisfies Property (2) of HT-ABoxes. Otherwise, since A satisfies
Property (1) of HT-ABoxes, we have the possibilities shown in Table 6, for v a blockable
individual, and a and b root individuals. For brevity, we omit the symmetric combinations
where the roles of ar(R, u, s) and ar(R, u, t) are exchanged. Clearly, (V ) satisfies Property
(2) of HT-ABoxes. Finally, (V ) obviously satisfies Property (4) of HT-ABoxes.
Assume that V is of the form x  zj , so (V ) is of the form s  t. By Definition 5,
the antecedent of r then contains an atom Oa (zj ), so either Oa (s)  A or Oa (t)  A. By
Property (3) of HT-ABoxes, either s or t is a named individual, so (V ) satisfies Property
(2) of HT-ABoxes.
Assume that V is of the form R(x, x). Then, (V ) is of the form R(s, s), and it satisfies
Property (1) of HT-ABoxes.
197

fiMotik, Shearer, & Horrocks

Assume that V is of the form R(x, yi ) or R(yi , x), so (V ) is of the form R(s, t). By
Definition 5, the antecedent of r then contains an atom of the form S(x, yi ) or S(yi , x),
and either S(s, t)  A or S(t, s)  A; these assertions satisfy Property (1) of HT-ABoxes,
so R(s, t) satisfies it as well.
Assume that V is of the form R(x, zj ) or R(zj , x), so (V ) is of the form R(s, t). By
Definition 5, the antecedent of r then contains an atom of the form Oa (zj ) for Oa a nominal
guard concept, and either Oa (s)  A or Oa (t)  A; by Property (3) of HT-ABoxes, either
s or t is a named individual, so R(s, t) satisfies Property (1) of HT-ABoxes.
Assume that V is of the form B(x),  n S.B(x), or B(yi ), so (V ) is of the form B(s)
or  n S.B(s). By Definition 5, B is a literal but not a nominal guard concept, so (V )
satisfies Property (3) of HT-ABoxes.
(-rule) Consider an application of the -rule to an assertion  n R.B(s). By Property
(3) of HT-ABoxes, B is not a nominal guard concept, so all assertions B(ti ) introduced by
the rule satisfy Property (3) of HT-ABoxes. Furthermore, all ti introduced by the rule are
fresh blockable successors of s, and all role assertions introduced by the rule are of the form
R(s, ti ) or R(ti , s), so they satisfy Properties (1) and (5) of HT-ABoxes. The inequalities
introduced by the rule trivially satisfy the properties of HT-ABoxes.
(-rule) Consider an application of the -rule to a possibly annotated equality s  t,
where s is merged into t (the annotation of the equality plays no role here). By the conditions
on the 7 relation of A, the ABox A contains no renaming for s or t, so the renaming s 7 t
is the only renaming for s in A0 , and adding this renaming to A does not introduce a cycle
in 7. Merging replaces all occurrences of s in A, so no assertion of A0 contains s. Hence,
the 7 relation in A0 satisfies Property (7) of HT-ABoxes.
The NI -rule is not applicable to s  t by the rule precedence, so, by the preconditions
of the NI -rule and Property (2) of HT-ABoxes, s  t can be of the form v  a, v.i  v.j,
v.i  v, or v.i.j  v for a  NO and v  NA ; we denote this property with (*). Since pruning
and replacements are applied to all assertions of A uniformly, A0 clearly satisfies Property
(4) of HT-ABoxes. Furthermore, pruning removes all successors of s, so A0 satisfies Property
(5) of HT-ABoxes. We next consider the types of assertions of A that change when s is
merged into t.
Consider a role assertion R(s, u)  A that is changed into R(t, u)  A0 . If either t or
u is a root individual, then R(t, u) clearly satisfies Property (1) of HT-ABoxes, so assume
that t and u are both blockable individuals. Then, u is not a successor of s, since the -rule
prunes all assertions that contain a descendant of the merged individual. But then, by (*)
and since R(s, u) satisfies Property (1) of HT-ABoxes, we have the possibilities shown in
Table 7. The cases when R(u, s)  A is changed into R(u, t)  A0 by merging are analogous.
We now consider the form of equalities that can be derived from other equalities via
merging. An equality u  v @sn R.C can be changed into u  v @tn R.C , but the resulting
equality always satisfies Property (2) of HT-ABoxes. Furthermore, for a a root individual,
s  u @an R.C can be changed into t  u @an R.C , and s  a can be changed into t  a;
however, in both cases, the resulting equality satisfies Property (2) of HT-ABoxes. For the
remaining cases, assume that a possibly annotated equality s  u is changed into a possibly
annotated equality t  u. If s is a root individual, then t is a root individual as well (the
-rule never merges a root individual into a blockable one), so t  u satisfies Property (2) of
198

fiHypertableau Reasoning for Description Logics

Table 7: Cases in an Application of the -Rule to Role Assertions
R(s, u)
R(v.i, v)
R(v.i, v)
R(t.j.i, t.j)
R(v.i, v.i)
R(v.i, v.i)
R(t.j.i, t.j.i)

st
v.i  v.j
v.i  v
t.j.i  t
v.i  v.j
v.i  v
t.j.i  t

R(t, u)
R(v.j, v)
R(v, v)
R(t, t.j)
R(v.j, v.j)
R(v, v)
R(t, t)

Table 8: Cases in an Application of the -Rule to Equalities
su
v.i  v.k
v.i  v
u.k.i  u
v.i  v.k
v.i  v
u.k.i  u
t.j.i  t.j.k
t.j.i  t.j
t.j.i  t

st
v.i  v.j
v.i  v.j
u.k.i  u.k.j
v.i  v
v.i  v
u.k.i  u.k
t.j.i  t
t.j.i  t
t.j.i  t

tu
v.j  v.k
v.j  v
u.k.j  u
v  v.k
vv
u.k  u
t  t.j.k
t  t.j
tt

HT-ABoxes. Assume that s is a blockable individual. Since the -rule prunes all assertions
that contain a descendant of the merged individual, u is not a successor of s. By (*),
Property (2) of HT-ABoxes, and the fact that the NI -rule is not applicable to A, we have
the possibilities shown in Table 8. In all cases, the resulting assertion satisfies Property (2)
of HT-ABoxes. Furthermore, replacing s with t in s  t  A results in t  t  A0 , so A0
satisfies Property (6) of HT-ABoxes.
Consider an assertion C(s)  A that is changed into C(t)  A0 . The only nontrivial case
is when C is a nominal guard concept Oa . By Property (3) of HT-ABoxes, s is then a named
individual. The -rule replaces named individuals only with other named individuals, so t
is a named individual as well. Thus, C(t) satisfies Property (3) of HT-ABoxes.
(NI -rule) Consider an application of the NI -rule to an equality s  t @un R.B that merges
s into a root individual ku.hR, B, iikA . The individual s is blockable, so no renaming is added
to A and the 7 relation in A0 satisfies Property (7) of HT-ABoxes. Since s is replaced by
a root individual in role and equality assertions, all resulting assertions satisfy Properties
(1) and (2) of HT-ABoxes. Since s is not a named individual, no assertion involving a
nominal guard concept is affected by merging, so A0 satisfies Property (3). Since pruning
and replacements are applied to all assertions of A uniformly, A0 clearly satisfies Property
(4) of HT-ABoxes. Pruning removes all successors of s, so A0 satisfies Property (5) of

199

fiMotik, Shearer, & Horrocks

HT-ABoxes. Finally, A0 is clearly not empty, so it satisfies Property (6).
We next prove soundness and completeness of our calculus. We use these notions as is
customary in resolution-based theorem proving: a calculus is sound if its derivation rules
preserve satisfiability of a theory, and it is complete if, whenever the calculus terminates
without detecting a contradiction, the theory is indeed satisfiable.
Lemma 5 (Soundness). Let C be a set of HT-clauses and A an input ABox such that
(C, A) is satisfiable. Then, each derivation for C and A contains a branch such that (t) is
clash-free for each node t on the branch.
Proof. We say that a model I of an ABox A0 is NI-compatible with A0 if the following
conditions are satisfied:
 For each root individual a occurring in A0 , each concept  n R.B, and each   4I
such that aI  ( n R.B)I , haI , i  RI , and   B I , we have  = (a.hR, B, ii)I for
some 1  i  n.12
 If s  t @un R.B  A0 , then we have huI , sI i  RI , huI , tI i  RI , sI  B I , tI  B I , and
uI  ( n R.B)I .13
To prove this lemma, we first show the following property (*): if (C, A0 ) is satisfiable in
a model that is NI -compatible with A0 and A1 , . . . , An are ABoxes obtained by applying a
derivation rule to C and A0 , then some (C, Ai ) is satisfiable in a model that is NI -compatible
with Ai . Let I be a model of (C, A0 ) that is NI -compatible with A0 , and consider all possible
derivation rules that can derive A1 , . . . , An from A0 and C.
(Hyp-rule) Consider an application of the Hyp-rule to an HT-clause r of the form (34).
Since (Ui )  A0 , we have I |= (Ui ) for all 1  i  m. But then, I |= (Vj ) for some
1  j  n. Since Aj := A0  {(Vj )}, we have I |= (C, Aj ).
If I |= (Vj ) for some atom Vj not of the form  = yk  y` @xh R.B , then I is clearly NI compatible with Aj . Furthermore, for each Vj of the form , clearly h(x)I , (yk )I i  RI ,
h(x)I , (y` )I i  RI , (yk )I  B I , and (y` )I  B I . Let (**) denote these two properties.
Assume that I is not NI -compatible with Aj for each 1  j  n. By (**), then I 6|= (Vj )
for each Vj not of the form , and (x)I 6 ( h R.B)I for each Vj of the form . Let
 : NV  4I be a variable mapping such that (x) = (x)I and (yk ) = (yk )I for each
branch variable yk not occurring in an atom of the form ; furthermore, for each set of
branch variables y1 , . . . , yh+1 occurring in an atom of the form , we set (y1 ), . . . , (yh+1 )
to arbitrarily chosen domain elements that verify (x)I 6 ( h R.B)I . Clearly, I,  6|= Vj
for each Vj not occurring in a subset (35) or (36) of r; furthermore, by the definition of ,
we have that I,  6|= Vj for each Vj occurring in a subset of (35) or (36) of r. But then, we
conclude I,  6|= (C, A0 ), which is a contradiction.
(-rule) Since  n R.B(s)  A0 , we have I |=  n R.B(s), which implies that domain
elements 1 , . . . , n  4I exist where hsI , i i  RI and i  B I for 1  i  n, and i 6= j
12. Intuitively, this condition ensures that each root individual a.hR, B, ii is interpreted as an appropriate
neighbor of aI .
13. Intuitively, this condition ensures that u, s, and t are interpreted in I in accordance with the annotation.

200

fiHypertableau Reasoning for Description Logics

0

for 1  i < j  n. Let I 0 be an interpretation obtained from I by setting tIi = i . Clearly,
I 0 |= ar(R, s, ti ), I 0 |= B(ti ), and I 0 |= ti 6 tj for i 6= j, so I 0 |= (C, A1 ). The individuals ti
are not root individuals, so I 0 is NI -compatible with A1 .
(-rule) Assume that the -rule is applied to the assertion s  t  A0 and s is merged
into t. Since I |= s  t, we have sI = tI . Pruning removes assertions, so I is a model of the
pruned ABox by monotonicity. Merging simply replaces an individual with a synonym, so
I |= (C, A1 ). Furthermore, by Property (7) of HT-ABoxes, A does not contain renamings
for s and t, so kskA1 = t; hence, I is NI -compatible with A1 .
(-rule) This rule is never applicable if (C, A0 ) is satisfiable.
(NI -rule) Assume that the NI -rule is applied to some s  t @un R.B  A0 and s is
merged into a root individual. Since I is NI -compatible with A0 , we have uI  ( n R.B)I ,
huI , sI i  RI , sI  B I , and sI = (u.hR, B, ii)I for some 1  i  n. Let vi = ku.hR, B, iikA0 ;
since I is NI -compatible, we have (u.hR, B, ii)I = viI . Thus, the NI -rule replaces s by its
synonym vi , so I |= (C, Ai ) just like in the case of the -rule. If vi does not occur in
A0 , the interpretation I may not be NI -compatible with Ai because it does not interpret
vi .hS, C, `i correctly. We then extend I to I 0 as follows. For each m, S, and C such that
viI  ( m S.C)I , let 1 , . . . , k be the elements of 4I such that hviI , j i  S I and j  C I ;
0
clearly, k  m. We then set (vi .hS, C, `i)I = ` for 1  `  k. Since none of vi .hS, C, `i
occurs in Ai , we have I 0 |= (C, Ai ), so I 0 is NI -compatible with Aj .
This completes the proof of (*). To prove the main claim of this lemma, let A be an
input ABox. Similarly as for the NI -rule in the proof of (*), we can extend I to a model I 0
of (C, A). Since A does not contain annotated equalities, I 0 is NI -compatible with A. The
claim of this lemma then follows by a straightforward inductive application of (*).
Lemma 6 (Completeness). If a derivation for a set of HT-clauses C and an input ABox A
exists in which some leaf node is labeled with a clash-free ABox A0 , then (C, A) is satisfiable.
Proof. We prove the lemma by constructing from A0 a model of (C, A). Since our logic does
not have the finite model property, we obtain this model by unraveling A0 as intuitively
explained in Section 3.1.2. As usual, elements of the unraveled model are paths (Horrocks
& Sattler, 2001, 2007), as defined next.
Given an individual s that is directly blocked in A0 , let the blocker of s be an arbitrarily
chosen but fixed individual t such that s is directly blocked by t.
A path is finite sequence of pairs of individuals p = [ ss00 , . . . , ssn0 ]. Let tail(p) = sn and
tail0 (p) = s0n . Furthermore, let q = [p |

sn+1
]
s0n+1

n

0

be the path [ ss00 , . . . , ssn0 , ssn+1
]; we say that q is a
0
0

n

n+1

successor of p, and p is a predecessor of q. The set of all paths P(A0 ) is defined inductively
as follows:
 [ aa ]  P(A0 ) for each root individual a occurring in A0 ;
0

 [p | ss0 ]  P(A0 ) if p  P(A0 ), s0 is a successor of tail(p), s0 occurs in A0 , and s0 is not
blocked in A0 ; and
 [p | ss0 ]  P(A0 ) if p  P(A0 ), s0 is a successor of tail(p), s0 occurs in A0 , s0 is directly
blocked in A0 , and s is the blocker of s0 in A0 .
201

fiMotik, Shearer, & Horrocks

Table 9: The Construction of an Interpretation from A0

4I
aI
aI
AI
RI

=
=
=
=
=

P(A0 )
[ aa ] for each root individual a that occurs in an assertion in A0
bI if a 6= b and kakA0 = b
{p  4I | A(tail(p))  A0 }
{h[ aa ], pi  4I  4I
| a is a root individual and R(a, tail(p))  A0 } 
a
I
I
{hp, [ a ]i  4  4
| a is a root individual and R(tail(p), a)  A0 } 
s
{hp, [p | s0 ]i  4I  4I | R(tail(p), s0 )  A0 } 
{h[p | ss0 ], pi  4I  4I | R(s0 , tail(p))  A0 } 
{hp, pi  4I  4I
| R(tail(p), tail(p))  A0 }

Let I be the interpretation constructed from A0 as shown in Table 9. A0 is an HT-ABox,
so 4I is not empty. We now show that, for each ps of the form [ ss0 ] or [qs | ss0 ] and each
individual w, the following claims hold (*):
 R(s, s)  A0 (resp. A(s)  A0 ) iff hps , ps i  RI (resp. ps  AI ): Immediate by the
definition of I.
 If B(w)  A0 and LA0 (w) = LA0 (s0 ) for B a literal concept, then ps  B I : The proof
is immediate if B is atomic. If B = A, since the -rule is not applicable to A0 , we
have A(w) 6 A0 ; but then, we have A(s0 ) 6 A0 and A(s) 6 A0 , which by the case for
atomic concepts implies ps 6 AI .
 If  n R.B(s)  A0 , then ps  ( n R.B)I : By the definition of paths, s is not blocked;
since the -rule is not applicable to  n R.B(s), individuals u1 , . . . , un exist such that
ar(R, s, ui )  A0 and B(ui )  A0 for 1  i  n, and ui 6 uj  A0 for 1  i < j  n.
Each assertion ar(R, s, ui ) satisfies Property (1) of HT-ABoxes, so each ui can be of
one of the following forms.
 ui = s. Let pui = ps . But then, by the previous two cases we conclude that
ar(R, s, ui )  A0 and B(ui )  A0 imply hps , pui i  RI and pui  B I .
 ui is a successor of s. If ui is directly blocked by the blocker vi , let pui = [ps | uvii ];
otherwise, ui is not blocked because s is not blocked, and let pui = [ps | uuii ].
Either way, we have ar(R, tail(ps ), ui )  A0 , which, by the definition of I, implies hps , pui i  RI . Furthermore, B(ui )  A0 and LA0 (ui ) = LA0 (tail(pui )) imply
pui  B I .
 ui is a blockable predecessor of s. Since s is blockable, we have ps = [qs | ss0 ];
hence, let pui = qs . If s0 is not blocked, then s = s0 and tail(pui ) = ui , so we have
ar(R, s0 , tail(pui ))  A0 . If s0 is blocked by the blocker s, then by the definition of
pairwise blocking LA0 (tail(pui ), s0 ) = LA0 (ui , s) and LA0 (s0 , tail(pui )) = LA0 (s, ui ),
so we again have ar(R, s0 , tail(pui ))  A0 . Either way, we have hps , pui i  RI by

202

fiHypertableau Reasoning for Description Logics

the definition of I. Furthermore, B(ui )  A0 and LA0 (ui ) = LA0 (tail(pui )) imply
pui  B I .
 ui and s do not satisfy any of the previous three conditions. If s is a blockable
individual, then ui is a root individual, so let pui = [ uuii ]. If s is a root individual,
then ui is not blocked in A0 by Condition 3.2 of the -rule, so some pui  4I
exists that has the form pui = [p | uuii ]. Either way, we have ar(R, s, ui )  A0 and
B(ui )  A0 , which imply hps , pui i  RI and pui  B I .
Consider now each 1  i < j  n. If tail0 (pui ) 6 tail0 (puj )  A0 , since  6 A0 and the
-rule is not applicable, we have tail0 (pui ) 6= tail0 (puj ), so pui 6= puj . Furthermore, if
tail0 (pui ) 6 tail0 (puj ) 
/ A0 , this is because tail0 (pui ) 6= ui , which is possible only if s0 is
directly blocked by the blocker s and ui = s or ui is a blockable predecessor of s. Note,
however, that s can have at most one blockable predecessor, and that there can be at
most one ui such that ui = s. Therefore, we have ui 6= uj , which implies pui 6= puj ,
and we conclude ps  ( n R.B)I .
For an assertion 0  A0 of the form a  b and a 6 b with a and b named individuals, it
is straightforward to see that I |= 0 . Furthermore, if 0 is of the form R(a, b) or B(a), or
 n R.B(a) with a a named individual, (*) implies I |= 0 . Consider now each   A. By
induction on the application of the derivation rules, it is straightforward to show that, if
 6 A0 , then A0 contains renamings that, when applied to , produce an assertion 0  A0 .
But then, since I |= 0 , we have I |=  by the definition of I.
It remains to be shown that I |= C. Consider each HT-clause r  C containing atoms of
the form Ai (x), Uk (x, x), ar(Ri , x, yi ), Bi (yi ), and Cj (zj ) in the antecedent. Furthermore,
consider a variable mapping  such that the antecedent of r is true in I and that is,
px  AIi , hpx , px i  UkI , hpx , pyi i  RiI , pyi  BiI , and pzj  CjI for px = (x), pyi = (yi ),
and pzj = (zj ). Let s = tail(px ), s0 = tail0 (px ), and t0i = tail0 (pyi ). By the definition of I
and the fact that LA0 (s0i ) = LA0 (si ), we have Ai (s)  A0 , Uk (s, s)  A0 , and Bi (t0i )  A0 .
Depending on the relationship between px and pyi , we define ti as follows.
 pyi is a successor of px or pyi = px . Let ti = t0i . Clearly, Bi (ti )  A0 ; furthermore, the
definition of I and hpx , pyi i  RiI imply ar(Ri , s, t0i )  A0 , so we have ar(Ri , s, ti )  A0 .
 pyi is a predecessor of px . We have the following cases.
 s directly blocks s0 . Let ti be the predecessor of s; such ti exists since s is
blockable. The definition of I and hpx , pyi i  RiI imply ar(Ri , s0 , tail(pyi ))  A0
and B(tail(pyi ))  A0 , and by the definition of pairwise blocking we conclude
that ar(Ri , s, ti )  A0 and Bi (ti )  A0 .
 s0 is not blocked. Let ti = t0i . By the definition of I, we have Bi (ti )  A0 and
ar(Ri , s, ti )  A0 .
 pyi and px do not match any of the conditions mentioned thus far. By the definition
of I, then either px or pyi is of the form [ aa ]. Let ti = tail(pyi ). By hpx , pyi i  RiI and
the definition of I, we conclude that Bi (ti )  A0 and ar(Ri , s, ti )  A0 .

203

fiMotik, Shearer, & Horrocks

By Definition 5, the antecedent of r contains an atom of the form Oa (zj ) for each nominal
variable zj . Thus, by the definition of I and Property (3) of HT-ABoxes, we have pzj is of
u
the form [ ujj ] for uj a named individual; furthermore, Cj (uj )  A0 .
Let  be a mapping such that (x) = s, (yi ) = ti , and (zj ) = uj . Clearly, neither s
nor ti are indirectly blocked, and (Uj )  A0 for each atom Uj in the antecedent of r. The
Hyp-rule is not applicable to r, A0 , and , so r contains an atom Vi in the consequent such
that (Vi )  A0 . Depending on the type of Vi , we have the following possibilities.
Assume that Vi is of the form yi  yj @xk S.B ; thus, we have ti  tj @sk S.B  A0 . Since
the -rule is not applicable to A0 , we have ti = tj . By Definition 5, r contains a subclause
of the form (35) or (36), so the antecedent of r contains atoms ar(S, x, yi ) and ar(S, x, yj );
therefore, hpx , pyi i  S I and hpx , pyj i  S I . The NI -rule is not applicable to ti  tj @sk S.B
so, by the preconditions of the NI -rule, if s is a root individual, then ti (tj ) is either a root
individual or a successor of s. This rules out the possibility when px is of the form [ aa ] and
pyi (pyj ) is neither a successor of px nor of the form [ bb ]. Hence, by the construction of I,
we have that pyi (pyj ) is either a successor of px , equal to px , the predecessor of px , or is
of the form [ aa ]. We now consider the following cases (w.l.o.g. we omit the symmetric cases
obtained by swapping pyi and pyj ):
 pyi is of the form [ aa ]. Then, ti = tj implies pyi = pyj by the definition of paths.
 pyi is a successor of px . Then, pyi = [px | utii ] for ui = ti if ti is not blocked or ui the
blocker of ti . Either way, ti is different from s and the predecessor of s (if the latter
exists). We have the following possibilities for pyj :
u

 pyj is a successor of px . Then, pyj = [px | tjj ], so ti = tj clearly implies pyi = pyj .
 pyj = px or pyj is the predecessor of px . Then tj = s or tj is the predecessor of
s, which contradicts the fact that ti 6= tj .
 pyi = px . Then ti = s. The only nontrivial case is if pyj is the predecessor of px ; but
then, tj 6= s, which contradicts the fact that ti 6= tj .
 pyi is the predecessor of px . The only remaining possibility is for pyj to be the predecessor of px . Since px can have at most one predecessor, we have pyi = pyj .
Thus, we conclude that I,  |= r.
Assume that Vi is of the form x  zj ; thus, we have s  uj  A. Since the -rule is
not applicable to A0 , we have s = uj . Since uj is a named individual, it cannot block other
individuals, so s0 = s, which implies px = pzj . Thus, I,  |= r.
Assume that Vi is of the form Ti (x, x); thus, we have Ti (s, s)  A0 . By (*), we then have
hpx , px i  RiI . Thus, we have I,  |= r.
Assume that Vi is of the form Di (x) for Di a literal concept or of the form  n T.B;
thus, we have Di (s)  A0 . By (*), we then have px  DiI . Thus, we have I,  |= r.
Assume that Vi is of the form Ei (yi ) for Ei a literal concept; thus, we have Ei (ti )  A0 .
We have already established that LA0 (ti ) = LA0 (t0i ); by (*), we then have pyi  EiI . Thus,
we have I,  |= r.

204

fiHypertableau Reasoning for Description Logics

Assume that Vi is of the form ar(Si , x, yi ), so ar(Si , s, ti )  A0 . By the definition of
blocking, we have ar(Si , s0 , t0i )  A0 . Finally, by the definition of I, we have hpx , pyi i  SiI .
Thus, we have I,  |= r.
Assume that Vi is of the form ar(Sj , x, zj ), so ar(Sj , s, uj )  A0 . Since uj is a named
individual, by the definition of I we have hpx , pzj i  SjI . Thus, we have I,  |= r.
We next prove termination of the hypertableau calculus.
Lemma 7 (Termination). For a set of HT-clauses C and an input ABox A, let |C, A| be
the sum of the size of A, of the number of concepts and roles in C, and of dlog ne for each
integer n occurring in C in an atom of the form  n R.B and yi  yj @xn R.B . The total
number of individuals introduced on each path in each derivation for C and A is at most
doubly exponential in |C, A|, and each derivation for C and A is finite.
Proof. We prove the claim by showing that (i) each derivation rule can be applied at most
once to a fixed set of individuals on a derivation path, and (ii) the number of new individuals
introduced on each derivation path is at most doubly exponential in |C, A|. The supply of
blockable individuals is infinite, so we can assume that no blockable individual is introduced
twice on a derivation path. Furthermore, if the root individual s is removed from an ABox
A0 due to merging, then a renaming is added to A0 that ensures kskA0 6= s. Once a renaming
is added to A0 , all ABoxes occurring below A0 in a derivation will contain this renaming as
well, so no subsequent application of the NI -rule can reintroduce s.
Next, we prove (i) by considering each derivation rule.
 An application of the Hyp-rule to an HT-clause r of the form (34) and a mapping
 introduces an assertion (Vi ), which prevents a subsequent reapplication of the
Hyp-rule to the same r and . Merging and pruning can remove (Vi ) in subsequent
derivation steps, but this also removes at least one individual occurring in  from the
set of potential premises of the Hyp-rule, thus preventing the reuse of the same  in
a future application of the Hyp-rule to r.
 An application of the -rule to an assertion  n R.B(s) introduces t1 , . . . , tn as fresh
successors of s and the assertions B(ti ), ar(R, s, ti ), and ti 6 tj for 1  i < j  n.
Thus, the individuals u1 , . . . , un from Condition 3 of the -rule can be matched to
t1 , . . . , tn . Furthermore, if s is a root individual, none of ti can become blocked and
Condition 3.2 is always satisfied for ti ; moreover, if s is blockable, Condition 3.2 is
trivially satisfied for ti . If some ti is merged into another individual v, then B(v),
ar(R, s, v), and v 6 tj are added to the ABox, so the ABox still contains individuals
that can be matched to Condition 3 of the -rule. Finally, if some ti becomes indirectly
blocked, then s is blocked and the -rule is not applicable to s.
 An application of the -rule to s  t removes either s or t, so the rule cannot be
reapplied to the same s and t
 An application of the -rule produces an ABox that labels a derivation leaf.
 An application of the NI -rule to an equality s  t @un R.B removes s, so the rule
cannot be reapplied to the same  n R.B, s and u.
205

fiMotik, Shearer, & Horrocks

Next, we prove (ii)that is, that the total number of individuals introduced on a derivation path is at most doubly exponential in |C, A|. A path of length n between individuals
s and t in an ABox A0 is a sequence of individuals u0 , u1 , . . . , un such that u0 = s, un = t,
and, for each 0  i  n  1, either R(ui , ui+1 )  A0 or R(ui+1 , ui )  A0 for R an atomic role.
A root path for a root individual t in an ABox A0 is a path between t and a named
individual s such that all intermediate individuals ui , 1  i  n  1, are root individuals.
The level lev(t) of t is the length of the shortest root path for t. Thus, lev(t) = 0 if t is a
named individual.
The depth dep(t) of an individual t is the number of ancestors of t. Thus, dep(t) = 0 if
t is a root individual. Due to Property (5) of HT-ABoxes, if an individual t occurs in an
ABox A0 , then A0 contains a path of length dep(t) between a root individual s and t such
that the individuals ui , 0  i  n  1, are all ancestors of t; since each individual has at
most one predecessor, these ui are also the only ancestors of t.
We now show that the maximum level of a root individual and the maximum depth of
every individual are both at most exponential in the size of C and A.
An application of an derivation rule never increases the level of an individual. This is
because a named individual is never pruned and can be merged only into another named
individual,14 and a root individual can be merged only into another root individual. Such
rule applications can only make a root path shorter, and not longer.
Let m be the number of atomic concepts and n the number of atomic roles that occur
in A and C, let  = 22m+2n + 1, and let A0 be an ABox labeling a node of a derivation for
A and C. We next show that (1) dep(t)   for each individual t occurring in A0 , and (2) if
t is a root individual, then lev(t)  .
(Claim 1) For a pair of individuals s and t occurring in A0 , there are 2m different
possible labels LA0 (s) and 2n different possible labels LA0 (s, t). Thus, if A0 contains at least
 = 2m  2m  2n  2n + 1 predecessor-successor pairs of blockable individuals, then A0 must
contain two pairs hs, s.ii and ht, t.ji such that the following conditions are satisfied:
LA0 (s.i) = LA0 (t.j)
LA0 (s, s.i) = LA0 (t, t.j)

LA0 (s) = LA0 (t)
LA0 (s.i, s) = LA0 (t.j, t)

Since  contains the ancestor relation, a path in A0 containing  blockable individuals
must include at least one blocked individual, so a blockable individual of depth  must be
blocked. The -rule is applied only to individuals that are not blocked, so the rule cannot
introduce an individual u such that dep(u) > .
(Claim 2) We show that the following stronger claim (*) holds for each root individual
s occurring in an assertion in A0 (the symmetry of  applies as usual):
1. lev(s)  ;
2. if R(s, t)  A0 or R(t, s)  A0 or t  u @sn R.B  A0 with t a blockable nonsuccessor
of s, then lev(s) + dep(t)  ; and
3. if s  t  A0 with t a blockable nonsuccessor of s (where the equality can be annotated), then lev(s) + dep(t)   + 1.
14. If a derivation rule replaced a named individual with an individual that is not named, the levels of other
root individuals could increase.

206

fiHypertableau Reasoning for Description Logics

This claim is clearly true for the input ABox A labeling the root of a derivation, which
contains only named individuals. We now assume that (*) holds for some ABox A0 and
consider all possible derivation rules that can be applied to A0 .
 Assume that the Hyp-rule derives an assertion R(s, t) or R(t, s), where s is a root
individual and t is a blockable nonsuccessor of s. Let R(x, y) or R(y, x) be the atom
from the consequent of an HT-clause r that is instantiated by the derivation rule. We
have the following two possibilities for the antecedent of r.
 The antecedent of r contains an atom of the form S(x, y) or S(y, x) that is
matched to an assertion of the form S(s, t) or S(t, s) in A0 . Since A0 satisfies (*),
the resulting ABox satisfies (*) as well.
 The antecedent of r contains an atom of the form Oa (x) or Oa (y) that is matched
to an assertion of the form Oa (s) in A0 (since t is blockable, A0 cannot contain
Oa (t) by Property 3 of HT-ABoxes). Then dep(t)   and lev(s) = 0, so the
resulting ABox satisfies (*) as well.
 Assume that the Hyp-rule derives an assertion t  u @sn R.B , where s is a root individual and t is a blockable nonsuccessor of s. By Definition 5, the antecedent of
the HT-clause then contains atoms of the form ar(R, x, yi ) and ar(R, x, yj ) that are
matched to assertions ar(R, s, t) and ar(R, s, u) in A0 . Since A0 satisfies (*), we have
lev(s) + dep(t)  , so the resulting equality satisfies Item 2 of (*). To show that
t  u @sn R.B satisfies Item 3 of (*), assume that u is a root individual and t is a
nonsuccessor of u. Since A0 contains ar(R, s, u), we have that lev(u)  lev(s) + 1; but
then, lev(u) + dep(t)   + 1, as required.
 If the Hyp-rule derives an assertion s  t, where s is a root individual and t is a
blockable nonsuccessor of s, the only remaining possibility is that the consequent of
the HT-clause then contains the equality x  zj . By Definition 5, the antecedent
then contains Oa (zj ) that is matched to an assertion Oa (s) in A0 , where s is a named
individual. Then dep(t)   and lev(s) = 0, so the resulting ABox satisfies (*).
 Assume that the -rule introduces an assertion of the form R(s, t) or R(t, s) where
t is a fresh individual. Individual t is always a successor of s, so the resulting ABox
trivially satisfies (*).
 Assume that the -rule is applied to an assertion of the form u  s and that u is
merged into s. By the definition of merging, we have that dep(u)  dep(s) and u is
pruned. If s is a blockable individual, then u is blockable as well, and the resulting
ABox satisfies (*) because u is replaced with an individual of equal or smaller depth.
Therefore, we assume that s is a root individual and consider the types of assertions
that can be added to A0 as a result of merging.
 If R(u, u) is changed into R(s, s), the resulting ABox clearly satisfies (*).
 Assume that R(u, t) where t is a root individual is changed into R(s, t). This
inference can make root paths to s and t only shorter and not longer, so the
levels of s and t can only decrease rather than increase. Thus, the resulting
ABox satisfies Item 1 of (*).
207

fiMotik, Shearer, & Horrocks

 Assume that R(u, t), where t is a predecessor of u, is changed into R(s, t); the
only nontrivial case is when t is a blockable nonsuccessor of s. Since t is a
predecessor of u, we have dep(t) + 1 = dep(u); since A0 satisfies (*), we have
lev(s) + dep(u)   + 1; but then, lev(s) + dep(t)   as required.
 The cases when R(t, u) is changed into R(t, s) are analogous.
 Assume that a possibly annotated equality v  u is changed into v  s. The
only nontrivial case is when v is a blockable nonsuccessor of s. If u is a root
individual, then the level of s after merging is bounded by min(lev(s), lev(u))
before merging, so (*) is preserved. If u and v are both blockable individuals,
then by Property (2) of HT-ABoxes, either u is an ancestor of v, or u and v
are siblings, or v is an ancestor of u. If u is an ancestor of v, then pruning u
removes v  u from A0 . If v is a sibling or an ancestor of u, then u must be
a nonsuccessor of s, so lev(s) + dep(u)   + 1; but then, dep(v)  dep(u), so
lev(s) + dep(v)   + 1 and (*) is preserved.
 Assume that v  v 0 @un R.B is changed into v  v 0 @sn R.B or v  s @sn R.B . The
only nontrivial case is when v is a blockable nonsuccessor of s. Since u is pruned
before merging, by Properties (2) and (4) of HT-ABoxes v must be a predecessor of u, so dep(v) + 1 = dep(u). Furthermore, by the same properties u
must be a blockable nonsuccessor of s, so lev(s) + dep(u)   + 1. But then,
lev(s) + dep(v)  , as required.
 An application of the -rule trivially preserves (*).
 Assume that the NI -rule is applied to an assertion s  t @un R.B replacing s with a
root individual v = ku.hR, B, iikA0 . If v already occurs in an assertion in A0 , then v
satisfies Item 1 of (*). If, however, v is fresh, by Property (4) of HT-ABoxes v will be
connected to u by a role assertion, so lev(v)  lev(u) + 1. Furthermore, since s is a
blockable nonsuccessor of u, we have lev(u) + dep(s)  . Finally, since s is blockable,
dep(s)  1, so lev(u)    1. As a consequence, we conclude that lev(v)  , which
proves Item 1 of (*). The proof that the assertions introduced through merging satisfy
(*) is analogous to the case for the -rule.
We now complete the proof of claim (ii)that is, that the total number of individuals
introduced by derivation rules is at most doubly exponential in |C, A|.
All named individuals are of level 0 and are never introduced by the derivation rules.
An application of the NI -rule to a root individual u of level ` can introduce at most n root
individuals of level ` + 1 for each concept  n R.B that occurs in C. Thus, for each named
individual, the derivation rules can create a tree of root individuals. The maximum depth
of the tree is , which is exponential in |C, A|. Furthermore, the maximum branching factor
b is equal to the sum of all numbers occurring in C in atoms of the form yi  yj @xn R.B .
Clearly, b is exponential in |C, A|, so each such tree is doubly exponential in |C, A|.15
Similarly, each root individual can become the root of a tree of blockable individuals of
depth . Each blockable individual is introduced by applying the -rule to its predecessor.
15. If numbers were coded in unary, then the branching factor would be polynomial, but each such tree
would still be doubly exponential in |C, A|.

208

fiHypertableau Reasoning for Description Logics

Furthermore, the -rule can be applied to an individual s at most once for each concept
of the form  n R.B. Thus, the branching factor is exponential assuming binary coding of
numbers, and each such tree is at most doubly exponential in |C, A|.
Thus, the total number of individuals appearing in a derivation is at most doubly exponential in |C, A|. Since the branching factor in the derivation is exponentially bounded by
|C, A|, each derivation is finite.
We now state the main theorem of this section.
Theorem 1. The satisfiability of a SHOIQ+ knowledge base K can be decided by computing
K0 = ((K)) and then checking whether some derivation for (K0 ) contains a leaf node
labeled with a clash-free ABox. Such an algorithm can be implemented such that it runs in
2NExpTime in |K|.
Proof. The first part of the theorem follows immediately from Lemmas 1, 2, 5, and 6.
By Lemma 7, the total number of individuals is doubly exponential in |A (K0 ), T R (K0 )|.
Since the structural transformation is polynomial, the total number of individuals is doubly
exponential in |K|. Thus, the existence of a leaf derivation node labeled with a clash-free
ABox can be checked by nondeterministically applying the hypertableau derivation rules to
construct an ABox that is at most doubly exponential in |K|.

5. Discussion
In this section we discuss the possibilities of optimizing the blocking condition to single and
subset blocking; furthermore, we argue that modifying the algorithm to make it optimal
w.r.t. worst-case complexity might be difficult.
5.1 Single Blocking
For DLs such as SHOQ+ that do not provide for inverse roles, pairwise blocking can be
weakened to atomic single blocking, defined as follows.
Definition 9 (Atomic Single Blocking). Atomic single blocking is obtained from pairwise
blocking (see Definition 7) by changing the notion of direct blocking: a blockable individual
s is directly blocked by a blockable individual t if and only if t is not blocked, t  s, and
LA (s) = LA (t) for LA (s) as in Definition 7.16
In some cases, this simpler blocking condition can make the hypertableau algorithm
construct smaller ABoxes, which can lead to increased efficiency. We next formalize the
notion of HT-clauses to which atomic single blocking is applicable.
Definition 10 (Simple HT-Clause). An HT-clause r is simple if it satisfies the following
restrictions, for x a center variable, yi a branch variable, zj a nominal variable, B a literal
concept, and R an atomic role:
 Each atom in the antecedent of r is of the form A(x), R(x, x), R(x, yi ), A(yi ), or
A(zj ).
16. The name atomic reflects the fact that LA (s) contains only atomic concepts.

209

fiMotik, Shearer, & Horrocks

 Each atom in the consequent of r is of the form B(x),  h R.B(x), B(yi ), R(x, x),
R(x, yi ), R(x, zj ), x  zj , or yi  yj .
It is straightforward to see that, if K is a SHOQ+ knowledge base, then T R (K) contains
only simple HT-clauses. The completeness of the hypertableau algorithm with atomic single
blocking on simple HT-clauses is straightforward to show.
Lemma 8. Let C be a set of simple HT-clauses, and A an input ABox. If a derivation with
atomic single blocking for C and A exists in which a leaf node is labeled with a clash-free
ABox A0 , then (C, A) is satisfiable.
Proof. By slightly modifying the proof of Lemma 4, it is possible to show the following
property (*): each atom in A0 involving an atomic role is of the form R(s, a), R(s, s), or
R(s, s.i), for a a named individual and s any individual.
Let I be a model constructed in the same way as in Lemma 6, but by using single
blocking. Due to (*), whenever hp1 , p2 i  RI , then p2 is either of the form [ aa ] for a a named
individual, it is a successor of p1 , or p2 = p1 . The proof that I is a model of (C, A) is a
straightforward consequence of the following observations about the proof of Lemma 6:
 In the proof that  n R.B(s)  A0 implies ps  ( n R.B)I , individual ui can never
be a blockable predecessor of s. Thus, labels LA0 (s, ui ), LA0 (ui , s), and LA0 (ui ) are
never relevant.
 In the proof that I |= C, it is not possible that pyi is a predecessor of px . Thus, labels
LA0 (s0 , tail(pyi )), LA0 (tail(pyi ), s0 ), and LA0 (tail(pyi )) are never relevant.
The proof that I is a model of (A, C) thus requires only LA0 (s) = LA0 (t) to hold when s
is blocked by the blocker t; hence, I is a model of (A, C) even if atomic single blocking is
used.
The following variant of single blocking can also be applied to DLs with inverse roles
but no number restrictions, such as SHOI.
Definition 11 (Full Single Blocking). Full single blocking is obtained from atomic single
blocking (see Definition 9) by changing the definition of LA (s) as follows:
LA (s) = { C | C(s)  A where C is of the form A or  1 R.B
with A an atomic and B a literal concept }
For t to directly block s in A under atomic single blocking, it suffices if s and t occur
in the same atomic concepts in A. Intuitively, this is because the model construction from
Lemma 6 copies all nonatomic concepts from t to s; hence, assertions of the form C(s)
where C is not atomic are not relevant. In contrast, in full single blocking, s and t must
occur in A in exactly the same concepts (apart from negated atomic concepts). Intuitively,
given a clash-free ABox A0 to which no derivation rule is applicable, a model for (A, C) is
constructed from A0 by replacing s with t; for the result to be a model, the two individuals
must occur in exactly the same concepts.

210

fiHypertableau Reasoning for Description Logics

a
T.C

T

b
C
R.D

R

c
D
S  .C

S

d
C
R.D

Figure 11: Problems with Single Blocking

Full single blocking must be applied with care in the hypertableau setting. Consider the
following knowledge base K9 , consisting of an ABox A9 and a set of HT-clauses C9 .
(37)

A9 = { T.C(a) }
C9 = {C(x)  R.D(x), D(x)  S  .C(x), R(x, y1 )  S(x, y2 )  }

On K9 , the hypertableau algorithm with full single blocking produces the ABox shown in
Figure 11. The individual d is blocked by b, so the algorithm terminates; an expansion of
R.D(d), however, would reveal that K9 is unsatisfiable. The problem arises because the
HT-clause R(x, y1 )  S(x, y2 )   contains two role atoms, which allows the HT-clause to
examine both the successor and the predecessor of x. Full single blocking, however, does
not ensure that both predecessors and successors of x have been fully built. We can correct
this problem by requiring the normalized GCIs to contain at most one R.C concept. For
example, if we replace our HT-clause with R(x, y1 )  Q(x) and Q(x)  S(x, y2 )  , then
the first HT-clause would additionally derive Q(b), so d would not be blocked by b.
We can apply full single blocking to the DL SHOI provided that each HT-clause contains at most one role atom in the antecedent. We can always ensure this by suitably
renaming complex concepts with atomic ones.
Lemma 9. Let A be an ABox and C a set of HT-clauses such that, for each r  C, ( i) r
contains no atoms of the form R(x, x), ( ii) the antecedent of r contains at most one role
atom, and ( iii) all at-least restriction concepts are of the form  1 S.B for S a role and B
a literal concept. If a derivation with full single blocking for C and A exists in which a leaf
node is labeled with a clash-free ABox A0 , then (C, A) is satisfiable.
Proof. Let A00 be obtained from A0 by removing each assertion containing an indirectly
blocked individual. Since no derivation rule is applicable to indirectly blocked individuals,
no derivation rule is applicable to A00 and C. For an individual s occurring in A00 , let
[s]A00 = s if s is not blocked in A00 , and let [s]A00 = s0 if s is blocked in A00 by the blocker s0 .
Note the following useful property (*): if A(s)  A00 , then A(s) 6 A00 since the -rule
is not applicable to A00 ; but then, A([s]A00 ) 6 A00 as well.
We now construct an interpretation I from A00 as follows.
4I
sI
AI
RI

=
=
=
=

{s | s occurs in A00 and it is not blocked in A00 }
[s]A00 for each individual s occurring in A00
{[s]A00 | A(s)  A00 }
{h[s]A00 , [t]A00 i | R(s, t)  A00 }
211

fiMotik, Shearer, & Horrocks

It is straightforward to see that I |= A00 . Consider now each HT-clause r  C that contains
in the antecedent one atom of the form R(x, y), as well as atoms of the form Ai (x), Bi (y),
Ci (zi ). Let  be a mapping from the variables of r to the individuals in A00 such that
I |= (Ui ) for each atom Ui from the antecedent of r. By the definition of I, individuals s
and t then exist such that R(s, t)  A00 , (x) = [s]A00 , and (y) = [t]A00 . By the definition
of full single blocking, then Ai (s)  A00 and Bi (t)  A00 as well. Furthermore, since each
zi occurs in a nominal guard concept, (zi ) is a named individual. Let  0 be such that
 0 (x) = s,  0 (y) = t, and  0 (zi ) = (zi ). Since the Hyp-rule is not applicable to C and A00
for  0 , we have  0 (Vj )  A00 for some atom Vj from the consequent of r. Consider now the
possible forms that Vj can have.
 If Vj = S(x, y), then I |= S((x), (y)) by the definition of I. The case Vj = S(y, x)
is analogous.
 If Vj = A(x) for A an atomic concept, then A([s]A00 )  A00 by the definition of full
single blocking; but then, I |= A((x)) by the definition of I. The case when Vj = A(y)
is analogous.
 If Vj = A(x), then A([s]A00 ) 6 A00 by (*); but then, by the definition of I we have
I |= A((x)). The case when Vj = A(y) is analogous.
 If Vj = D(x) for D =  1 R.B, then D([s]A00 )  A00 by the definition of full single
blocking. Since the -rule is not applicable to [s]A00 , an individual t exists such that
ar(R, s, t)  A00 and if B is atomic, then B(t)  A00 , and if B = A, then A(t) 6 A00 .
By the definition of full single blocking, if B is atomic, then B([t]A00 )  A00 , and if
B = A, then A([t]A00 ) 6 A00 . By the definition of I, we have h[s]A00 , [t]A00 i  RI , and
[t]A00  B I ; therefore, I |= D((x)). The case when Vj = D(y) is analogous.
 If Vj = x  zi , then  0 (x)   0 (zi )  A00 ; since the -rule is not applicable to A00 , we
have  0 (x) =  0 (zi ). But then, since named individuals cannot block other individuals,
we have (x) =  0 (x); hence, I |= (x)  (zi ).
Thus, in all cases we have I |= (Vj ). The case when r does not contain a role atom R(x, y)
in the antecedent is analogous, so I |= (A, C).
5.2 Subset Blocking
In tableau algorithms for DLs without inverse roles, full single blocking condition from
Definition 11 can be further weakened to full subset blocking (Baader et al., 1996).
Definition 12 (Full Subset Blocking). Full subset blocking is obtained from full single
blocking (see Definition 11) by changing the notion of direct blocking: a blockable individual s
is directly blocked by an individual t if and only if t is not blocked, t  s, and LA (s)  LA (t).
Full subset blocking is problematic in the hypertableau setting. Consider the knowledge
base that consists of an ABox A10 and a TBox corresponding to the HT-clauses C10 .
(38)

A10 = 
{ T.C(a) }

C(x)  R.C(x),
C(x)  S.D(x),
C10 =
S(x, y)  D(y)  E(x), R(x, y)  E(y)  
212

fiHypertableau Reasoning for Description Logics

T.C

a

C
R.C
S.D S
E
b
T
R

c D

d

C
R.C
S.D

Figure 12: Problems with Full Subset Blocking

On K10 , our algorithm can produce the ABox shown in Figure 12, in which d is blocked by
b. If, however, we expand S.D(d) into S(d, e) and D(e), we can derive E(d); together with
R(b, d) and the HT-clause R(x, y)  E(y)  , we get a contradiction.
The problem arises because, in the hypertableau setting, the syntactic distinction between atomic and inverse roles is lost: an atom R (x, y) is transformed (by the function
ar) into the semantically equivalent atom R(y, x). The HT-clause S(x, y)  D(y)  E(x)
can be seen as including an implicit inverse role, because it examines a successor of x in the
antecedent in order to derive new information about x in the consequent, thus mimicking
the behavior of tableau algorithms with the semantically equivalent GCI D v S  .E.
The semantically equivalent but inverse-free GCI S.D v E would, in our hypertableau
algorithm, be transformed into exactly the same HT-clause. In the tableau setting, however, this GCI would be treated very differently: it would result in the v-rule deriving (E t S.D)(s) for all individuals s. A similar effect could be achieved in the hypertableau setting by translating S.D v E into two HT-clauses: >  E(x)  Q(x) and
Q(x)  S(x, y)  D(y)  . This introduces nondeterminism, but solves the problem with
full subset blocking by deriving either E(c) or Q(c), the first of which leads to an immediate
contradiction, and the second of which delays blocking.
In general, it is easy to see that full subset blocking could be used in the hypertableau
setting by modifying the preprocessing phase so as to ensure that HT-clauses do not include
such implicit inverses. It is not clear, however, if this would be very useful: it would result
in (possibly) smaller ABoxes, but at the cost of (possibly) larger derivation trees.
5.3 The Number of Blockable Individuals
Buchheit et al. (1993) presented a tableau algorithm for the DL ALCN R which, due to
anywhere blocking, runs in NExpTime instead of 2NExpTime, and Donini et al. (1998)
presented a similar result for the basic DL ALC. It is interesting to compare these algorithms
to ours to see whether anywhere blocking can improve the worst-case complexity of our
algorithm when K is a SHIQ+ knowledge base. In such a case, no HT-clause in (K)
contains a nominal guard concept, which prevents the derivation of assertions satisfying the
preconditions of the NI -rule; hence, no new root individuals are introduced in a derivation,
which eliminates a significant source of complexity.
The following example shows that, unfortunately, anywhere blocking does not improve the worst-case complexity; in fact, we identify a tension between and- and or213

fiMotik, Shearer, & Horrocks

branching. In the example, we use the well-known encoding of binary numbers by concepts B0 , B1 , . . . , Bk1 : we assign to each individual s in an ABox A a binary number
`A (s) = bk1 . . . b1 b0 such that bi = 1 if and only if Bi (s)  A. Using k concepts, we can
thus encode 2k different binary numbers. Furthermore, for any atomic role R, using the
well-known R-successor counting formula (Tobies, 2000), we can ensure that, whenever an
individual t is an R-successor of s in A, then `A (t) = (`A (s) + 1) mod 2k ; we omit this
formula for the sake of brevity. Let K11 be the following knowledge base. For the sake of
brevity, we omit the HT-clauses corresponding to the axioms in K11 .
(39)

C(a)

(40)

C v L.C u R.C

(41)

(The R-successor formula for B0 , . . . , Bk1 )

(42)

(The L-successor formula for B0 , . . . , Bk1 )

(43)

B0 u . . . u Bk1 v A

(44)

L.A u R.A v A

Figure 13 schematically presents a derivation on K11 in which a doubly exponential
number of blockable individuals is introduced.17 For simplicity of presentation, we use
single anywhere blocking. Due to (39)(42), our algorithm can create individuals a.1, a.2,
a.1.1, a.1.2, a.1.1.1, a.1.1.2, and so on, such that s.1 is an L-successor of s, and s.2 is an
k
k
R-successor of s. After creating the individuals of the form a.12 1 .1 and a.12 1 .2 where
k
12 1 is a string of 2k 1 ones, each individual x.1 blocks x.2 (c.f. Figure 13a). But then, due
k
k
k
to (43), a.12 1 .1 and a.12 1 .2 become instances of A. By (44), a.12 1 is made an instance
k
k
of A as well, so it does not block its sibling a.12 2 .2 any more; hence, a.12 2 .2 is now
expanded to exponential depth (c.f. Figure 13b). By repeating this process, the algorithm
k
k
derives that a.12 2 is an instance of A, but then it does not block its sibling a.12 3 .2 (c.f.
Figure 13d). Eventually, the algorithm constructs a binary tree of exponential depth, thus
creating a doubly-exponential number of blockable nodes in total (c.f. Figure 13d).
Buchheit et al. and Donini et al. obtained the nondeterministic exponential behavior
by applying the u-, t-, -, and v-rules exhaustively before applying the -rule. Such a
strategy ensures that the label of an individual s is fully constructed before introducing
a successor of s, which prevents individuals from being indirectly blocked. On K11 , this
means that the GCI (44) is applied to each individual s before introducing its successors.
Thus, before the existentials on s are expanded, the assertion (L.A t R.A t A)(s) is
introduced and one disjunct is chosen nondeterministically. The choices (L.A)(s) and
(R.A)(s) will lead to a clash, so the algorithm eventually derives A(s), before it expands
the existentials on s and introduces s.1 and s.2. Thus, while generating at most exponential
models, this algorithm incurs a massive amount of nondeterminism.
Nondeterministic exponential behavior can be guaranteed in the hypertableau algorithm
by nondeterministically fixing the label of each individual before applying the -rule to it.
This technique is similar to the one used by Tobies (2001) in order to obtain a PSpace
17. Initially, we suggested informally that our algorithm should run in NExpTime on SHIQ (Motik, Shearer,
& Horrocks, 2007). As this example shows, this is not the case.

214

fiHypertableau Reasoning for Description Logics

a
a.2

a.1

a.2

|2 k



3|

a.1

a

x

x

A
x.1

x.2

x.1

(a) An exponential path is constructed
with each blockable individual blocking
its sibling. No individual contains A in
its label.

(b) Adding A to the label of x.1 unblocks
x.2.

a
a.1

x.2

a
a.2

A
x

A

A
x.1

x.2

(c) Adding A to the label of x.2 makes x.2
blocked, and forces the addition of A to
the label of x. This unblocks the sibling
of x so another subtree is created.

(d) Derivation terminates with an exponential number
of unblocked individuals, but a doubly-exponential
number of indirectly blocked individuals.

Figure 13: Creation of an Exponentially Deep Binary Tree of Blockable Individuals

215

fiMotik, Shearer, & Horrocks

decision procedure for concept satisfiability in a DL with inverse roles but without GCIs.
The performance results in Section 7, however, seem to suggest that this might not be
beneficial in practice. Still, it might be worth exploring whether nondeterministically adding
concepts to labels of individuals can be used as an optimization that would detect early
blocks and thus prevent the construction of large models.
5.4 The Number of Root Individuals
SHOIQ is NExpTime-complete (Tobies, 2000), and it is straightforward to extend this
result to SHOIQ+ . Thus, one might wonder whether the complexity result in Theorem 1
can be sharpened to obtain a worst-case optimal decision procedure. This, unfortunately, is
not the case: we present an example on which our algorithm generates a doubly-exponential
number of root individuals. We construct K12 by extending K11 (axioms (39)(44)) with
the following two axioms:
(45)

B0 u . . . u Bk1 v {b}

(46)

A v  2 L .> u  2 R .>

As shown in Section 5.3, the axioms of K11 can cause our algorithm to construct a
binary tree of blockable individuals with exponential depth. Axiom (45) of K12 , however,
merges the leaves of this tree into the single named individual b, and axiom (46) ensures that
the N I-rule is applied to each of the remaining blockable individuals, beginning with the
neighbors of b. If, at each application of the N I-rule, we always merge blockable individuals
into root individuals as shown in Figure 14a, then our algorithm constructs the ABox shown
in Figure 14b, which contains two binary trees of root individuals of depth 2k/2 . Unlike
the case with K11 , fully constructing individual labels does not avoid double-exponential
behavior, since the promotion of blockable individuals to root individuals prevents blocking.

6. Algorithm Optimizations
DL reasoning algorithms are often used in practice to compute a classification of a knowledge
base Kthat is, to determine whether K |= A v B for each pair of atomic concepts A and
B occurring in K. A nave classification algorithm would involve a quadratic number of calls
to the subsumption checking algorithm, each of which can potentially be highly expensive.
To obtain acceptable levels of performance, various optimizations have been developed that
reduce the number of subsumption checks and the time required for each check (Baader,
Hollunder, Nebel, Profitlich, & Franconi, 1994). The well-known dependency-directed backtracking optimization (Horrocks, 2007) can readily be used with the hypertableau calculus.
Furthermore, we have developed two simple optimizations that, to the best of our knowledge, have not been considered previously in the literature.
6.1 Reading Classification Relationships from Concept Labels
Let (A, C) be an ABox and a set of HT-clauses obtained by clausifying a knowledge base
K, and let A and B be atomic concepts for which we want to check whether K |= A v B;
since A and B are atomic, this is the case if and only if (A0 , C) is unsatisfiable where
216

fiHypertableau Reasoning for Description Logics

a

L

L

L

R

R

L

b.hL, >, 1i.hL, >, 1i

R

R

L

b.hL, >, 1i.hL, >, 2i

L

R

R

L

b.hL, >, 2i.hL, >, 1i

b.hL, >, 1i

R

b.hL, >, 2i.hL, >, 2i

b.hL, >, 2i

b
(a) A root introduction strategy for the N I-rule

a
L
L

R

R

L

R

|2k/2  1|

|2k/2  1|

b
(b) The resulting tree, containing a doubly-exponential
number of root individuals

A0 = A  {A(a), B(a)} and a is a fresh individual. Let A1 be a clash-free ABox labeling
a leaf in a derivation from (A0 , C). We can use A1 to learn the following things about
subsumption in K. The proofs of these claims are straightforward.
1. If C(a)  A1 for some concept C and the derivation of C(a) does not depend on a
nondeterministic choice, then K |= A v C.
217

fiMotik, Shearer, & Horrocks

2. If A1 has been obtained from A0 deterministically, then K |= A v C only if C(a)  A1 .
3. If C(b)  A1 but D(b) 6 A1 for C and D concepts and b an individual that is not
blocked, then K 6|= C v D.
Thus, if K is deterministic, we can classify it using a linear number of calls to the hypertableau algorithm: for each atomic concept A, we check the satisfiability of (A  {A(a)}, C);
if the algorithm produces a clash-free ABox A1 , the set of subsumers of A are contained
in LA1 (a). These optimizations are applicable in the case of tableau algorithms as well;
however, they might be less effective due to increased or-branching.
6.2 Caching Blocking Labels
Let T and R be a SHIQ+ TBox and RBox, respectively, and let C = T R (T  R); since
T does not contain nominals, no assertions involving nominal guard concepts are needed.
Furthermore, assume that the classification of T  R involves n calls to the hypertableau
algorithm for ({Ai (ai ), Bi (ai )}, C). Then, if a derivation for ({Ai (ai ), Bi (ai )}, C) contains
a leaf node labeled with a clash-free ABox Ai , we can use the nonblocked individuals from
Ai as blockers in all subsequent satisfiability checks of ({Aj (aj ), Bj (aj )}, C) for j > i.
This is a simple consequence of the following fact. Let I1 and I2 be two models of T  R
such that 4I1  4I2 = ; furthermore, let I be defined as 4I = 4I1  4I2 , AI = AI1  AI2 ,
and RI = RI1  RI2 , for each atomic concept A and each atomic role R. Then, by a simple
induction on the structure of axioms in T  R, it is trivial to show that I |= T  R. This
property does not hold in the presence of nominals, which can impose a bound on the
number of elements in the interpretation of a concept; the bound could be satisfied in I1
and I2 individually, but violated in I.
Our optimization is correct because, instead of ({Ai (ai ), Bi (ai )}, C), we can check the
satisfiability of (Ai  {Ai (ai ), Bi (ai )}, C), and in doing so we can use the individuals from
Ai as potential blockers due to anywhere blocking. This optimization can be seen as a very
simple form of model caching (Horrocks, 2007), and it has been key to obtaining the results
that we present in Section 7. For example, on GALEN only one subsumption test is costly
because it computes a substantial part of a model of the TBox; all subsequent subsumption
tests reuse large parts of that model.
In practice, we do not need to keep the entire ABox Ai around; rather, for each nonblocked blockable individual t with a predecessor t0 , we simply need to retain the sets LAi (t),
LAi (t0 ), LAi (t, t0 ), and LAi (t0 , t).

7. Implementation and Evaluation
Based on the calculus from Section 4, we have implemented a prototype DL reasoner called
HermiT. In order to estimate how well our calculus performs in practice, we have compared
HermiT with two state-of-the-art tableau reasoners on several practical problems. The
objective of this evaluation was not to establish the superiority of HermiT, but to compare
the behavior of our calculus with that of the tableau calculi used in many existing systems,
and to demonstrate the usefulness of our calculus on realistic problems.
It is important to understand that HermiT is a prototype, and as such does not always
outperform the well-established reasoners. In particular, HermiT may be uncompetitive
218

fiHypertableau Reasoning for Description Logics

on ontologies where specialized optimizations are needed for good performance. For example, HermiT cannot process the SNOMED CT ontology due to the very large number of
concepts, while many other reasoners can classify the ontology easily. These reasoners, however, employ techniques that are quite different from the standard tableau algorithm; for
example, on an EL++ ontology such as SNOMED CT, Pellet uses the reasoning algorithm
by Baader, Brandt, and Lutz (2005), and other reasoners employ specialized techniques as
well (Haarslev, Moller, & Wandelt, 2008). Similarly, artificial test problems such as those
used in the TANCS comparison at the Tableaux98 conference (Balsiger & Heuerding, 1998;
Balsiger, Heuerding, & Schwendimann, 2000) and the DL98 workshop (Horrocks & PatelSchneider, 1998b) are often either easy for reasoners employing particular optimizations or
are only difficult due to the fact that they encode large propositional satisfiability problems
(Horrocks & Patel-Schneider, 1998a). Since our goal was to demonstrate the usefulness of
the hypertableau calculus on realistic problems, we have chosen to ignore such ontologies
and test problems, as they mainly test specialized calculi and optimizations that are applicable to various sublanguages of SHOIQ+ . Instead, we focus our evaluation on practical
ontologies in which the main difficulty is due to nontrivial reasoning problems encountered
during classification.
In addition to the hypertableau calculus described in Section 4, HermiT also implements
the optimizations from Section 6 and the well-known dependency directed backtracking
optimization (Horrocks, 2007). Thus, HermiT fully supports SHOIQ+ and it can perform
both satisfiability and subsumption testing as well as knowledge base classification. An
extensive discussion of implementation techniques is beyond the scope of this paper; we
only comment briefly on the implementation of anywhere blocking. In the DL community,
it is commonly understood that anywhere blocking is more costly than ancestor blocking
because, to determine the blocking status of an individual, one may need to examine all
individuals in an ABox and not just the individuals ancestors. Our implementation avoids
this problem by maintaining a hash table in which individuals are indexed by their four
blocking labels. The table is created by scanning all individuals in A in the increasing
sequence of the ordering . For each individual s in A, if the parent of s is blocked, then
s is indirectly blocked; otherwise, the algorithm queries the hash table for an individual
whose blocking labels are equal to those of s. If the hash table contains such an individual
t, then s is directly blocked in A by t; otherwise, s is not blocked in A so it is added into
the hash table. The blocking status of all individuals in A can thus be determined with a
linear number of hash table lookups.
We used Pellet 2.0.0rc4 (Parsia & Sirin, 2004) and FaCT++ 1.2.2 (Tsarkov & Horrocks,
2006) as reference implementations of the SHOIQ tableau algorithm (Horrocks & Sattler,
2007). Pellet employs ancestor blocking, while FaCT++ has recently been extended with
anywhere blocking. At the time of writing, however, the implementation of anywhere
blocking in FaCT++ was known to be incorrect,18 so we switched this feature off and used
FaCT++ with ancestor blocking as well. To measure the effects of ancestor vs. anywhere
blocking, we also used HermiT-Anca version of HermiT with ancestor blocking.
We used a collection of 392 test ontologies that we assembled from three independent
sources.
18. Personal communication with Dmitry Tsarkov.

219

fiMotik, Shearer, & Horrocks

 The Gardiner ontology suite (Gardiner, Horrocks, & Tsarkov, 2006) is a collection of
OWL ontologies gathered from the Web and includes many of the most commonlyused OWL ontologies.
 The Open Biological Ontologies (OBO) Foundry19 is a collection of biology and life
science ontologies.
 GALEN (Rector & Rogers, 2006) is a large and complex biomedical ontology which
has proven notoriously difficult to classify with existing reasoners.
We have preprocessed all ontologies to resolve ontology imports and eliminate some trivial
syntactic errors. Thus, each test ontology can be parsed as a single file using the OWL
API. All test ontologies are available online.20
We measured the time needed to classify each test ontology using the mentioned reasoners. All tests were performed on a 2.2 GHz MacBook Pro with 2 GB of physical memory.
A classification attempt was aborted if it exhausted all available memory (Java tools were
allowed to use 1 GB of heap space), or if it exceeded a timeout of 30 minutes.
The three reasoners exhibited negligible differences in performance on most of the test
ontologies. Therefore, we discuss next only the test results for interesting ontologies
that is, ontologies that can be classified by at least one of the tested reasoners, and that
are either not trivial or on which the tested reasoners exhibited a significant difference in
performance. These include several ontologies from the OBO corpus (Molecule Role, XP
Uber Anatomy, XP Plant Anatomy, Cellular Component, Gazetteer, CHEBI), two versions
of the National Cancer Institute (NCI) Thesaurus (Hartel et al., 2005), two versions of the
GALEN medical terminology ontology, two versions of the Foundational Model of Anatomy
(FMA) (Golbreich et al., 2006), the Wine ontology from the OWL Guide,21 two SWEET
ontologies developed at NASA,22 and a version of the DOLCE ontology developed at the
Institute of Cognitive Science and Technology of the Italian National Research Council.23
Basic statistical information about these ontologies is summarized in Table 10.
We noticed that, for all three reasoners, classification times may vary from run to run.
For Pellet and HermiT, this is due to Javas collection library: the order of iteration over
collections often depends on the objects hash codes, and these may vary from run to run;
that, in turn, may change the order in which the derivation rules are applied, and some
orders may be better than others. We conjecture that FaCT++ is susceptible to similar
variations. While the times may vary, we have not noticed a case where an ontology might
be successfully classified in one run, but not in another. Therefore, in Table 11 we present
the classification times for the interesting ontologies that we obtained on one particular
run; these times can be taken as being typical. We identified four groups of ontologies,
which we delineate in Tables 10 and 11 by horizontal lines.
19.
20.
21.
22.
23.

http://obofoundry.org/
http://hermit-reasoner.com/2009/JAIR_benchmarks/
http://www.w3.org/TR/owl-guide/
http://sweet.jpl.nasa.gov/ontology/
http://www.loa-cnr.it/DOLCE.html

220

fiHypertableau Reasoning for Description Logics

Table 10: Statistics of Interesting Ontologies

Ontology Name

Classes

Roles

Individuals

Molecule Role
XP Uber Anatomy
XP Plant Anatomy
XP Regulators
Cellular Component
NCI-1
Gazetteer
GALEN-doctored
GALEN-undoctored
CHEBI
FMA-Lite
SWEET Phenomena
SWEET Numerics
Wine
DOLCE-Plans
NCI-2
FMA-Constitutional

8849
11427
19145
25520
27889
27652
150979
2748
2748
20977
75141
1728
1506
138
118
70576
41648

2
82
82
4
4
70
2
413
413
9
2
145
177
17
264
189
168

128056
88955
86099
155169
163244
0
214804
0
0
243972
46225
171
113
206
27
0
85

Number of Axioms
TBox RBox
ABox
9243
14669
35770
42896
47345
46800
167349
3937
4179
38375
119558
2419
2184
355
265
100304
122695

1
80
87
3
3
140
2
799
800
2
3
239
305
40
948
290
395

128056
88955
86099
155169
163244
0
214804
0
0
243972
46225
491
340
494
68
0
86

Expressivity
ALE+
ALEHIF +
SHIF
SH
SH
ALE
ALE+
ALEHIF +
ALEHIF +
ALE+
ALEI+
SHOIN (D)
SHOIN (D)
SHOIN (D)
SHOIN (D)
ALCH(D)
ALCOIF(D)

On the ontologies in the first group, HermiT performs similarly to HermiT-Anc, which
suggests little impact of anywhere blocking on the performance. Consequently, we believe
that HermiT outperforms the other reasoners mainly due to the reduced nondeterminism of
the hypertableau calculus. As shown in Table 10, Molecule Role, XP Uber Anatomy, and
NCI-1 do not use disjunctions, so HermiT classifies them deterministically using a linear
number of calls to the hypertableau algorithm. FaCT++ outperforms HermiT on NCI-1
because this ontology can be classified using the completely defined concepts optimization
(Tsarkov & Horrocks, 2005a), which FaCT++ implements but HermiT does not. This
optimization enables FaCT++ to use simpler structural reasoning techniques on ontologies
that satisfy certain syntactic constraints.
On the ontologies in the second group, HermiT-Anc is significantly slower than HermiT.
This suggests that anywhere blocking significantly improves the performance since it prevents the construction of large models. Pellet runs out of memory on all ontologies in this
group; furthermore, FaCT++ cannot process two of them and is significantly slower than
HermiT on CHEBI. FaCT++, however, is faster than HermiT-Anc on CHEBI and GALENdoctored, and we conjecture that this is mainly due to the ordering heuristics (Tsarkov &
Horrocks, 2005b) used by FaCT++. The superior performance of HermiT on the ontologies
in this group is mainly due to the fact that all of these ontologies can be classified deterministically using a linear number of concept satisfiability tests. Furthermore, HermiTs
classification time is in most cases dominated by only the first such test, as the caching of
blocking labels described in Section 6.2 makes subsequent tests easy.
On the ontologies in the third group, HermiT is significantly slower than the other
reasoners. As Table 10 shows, all ontologies in this group contain nominals, which prevents
HermiT from caching blocking labels. Furthermore, due to nominals, the ABox must be
221

fiMotik, Shearer, & Horrocks

Table 11: Results of Performance Evaluation
Classification Times (seconds)
HermiT
HermiT-Anc
Pellet
FaCT++
Molecule Role
3.3
3.4
25.7
304.5
XP Uber Anatomy
5.4
4.9

86.0
XP Plant Anatomy
12.8
11.2
87.2
22.9
XP Regulators
14.1
17.1
35.4
66.6
Celular Component
18.6
18.0
40.5
76.7
NCI-1
14.1
14.4
23.2
3.0
Gazetteer
131.9
132.3


GALEN-doctored
8.8
456.3

15.9
GALEN-undoctored
126.3



CHEBI
24.2


397.0
FMA-Lite
107.2



SWEET Phenomena
13.5
11.2

0.2
SWEET Numerics
76.7
72.6
3.7
0.2
Wine
343.7
524.6
19.5
162.1
DOLCE-Plans
1075.1

105.1

NCI-2


172.0
60.7
FMA-Constitutional



616.7
Note: entry  means that reasoner was unable to classify the ontology either
due to time out or memory exhaustion.
Ontology Name

taken into account during classification, and HermiT currently reapplies the hypertableau
rules to the entire ABox in each run. Effectively, HermiT does not reuse any computation
between different hypertableau runs. The other two reasoners, however, use the completion
graph caching optimization (Sirin, Cuenca Grau, & Parsia, 2006), in which the tableau rules
are first applied to the entire ABox, and the resulting completion graph is used as a starting
point in each subsequent run.
HermiT was unable to classify the two ontologies in the fourth group. Both NCI-2 and
FMA-Constitutional use disjunctions, so they cannot be classified using a linear number
of concept satisfiability tests; instead, HermiT uses the classification algorithm by Baader
et al. (1994). All classification tests are straightforward (each test takes less than 50 ms);
however, the resulting taxonomy is rather shallow, so HermiT makes an almost quadratic
number of tests. Both Pellet and FaCT++, however, use more optimized versions of the
classification algorithm that reduce the number of tests that need to be performed.
To summarize, although HermiT is not better than Pellet and FaCT++ on all ontologies,
our results clearly demonstrate the practical potential of both the reduced nondeterminism and anywhere blocking. In fact, anywhere blocking can mean the difference between
success and failure on complex ontologies, which suggests that and-branching is a more significant source of inefficiency in practice than or-branching. Anywhere blocking is applicable

222

fiHypertableau Reasoning for Description Logics

to tableau calculi as well (as mentioned earlier, FaCT++ already contains a preliminary
version of it), so we believe that our results can be used to improve the performance of
tableau reasoners as well without the need for a major redesign. Conversely, most of the
optimizations used in tableau reasoners can be used with the hypertableau algorithm, and
incorporating them into HermiT would make HermiT competitive with Pellet and FaCT++
in those cases where HermiT is currently slower.

8. Conclusion
In this paper we presented a novel reasoning algorithm for DLs. The algorithm is based on
hyperresolution with anywhere blocking, which reduces the nondeterminism due to GCIs
and the sizes of generated models. Furthermore, the algorithm uses a novel refinement of the
NI -rule which can reduce the amount of nondeterminism introduced in order to handle the
interaction between nominals, inverse roles, and number restrictions (Horrocks & Sattler,
2007). This refined version of the NI -rule is equally applicable to tableau algorithms.
We have implemented our calculus and have conducted an extensive performance comparison. Our results show that the combination of the new calculus and novel optimizations
significantly increases the performance of DL reasoning in practice: our reasoner is currently
the only one that can classify several complex ontologies.
Despite this advance in performance, there are still some ontologies, such as the full
version of GALEN,24 that defeat HermiT (and state-of-the-art tableau reasoners). This is
because the large number of cyclic axioms in these ontologies cause HermiT to construct
extremely large ABoxes and eventually exhaust all available memory. To alleviate this
problem, we have developed a reasoning technique in which the -rule is modified to nondeterministically reuse individuals from the ABox generated thus far. Initial experiments
with this technique have shown very promising results (Motik & Horrocks, 2008).
Finally, we plan to extend our technique to the DL SROIQ, which extends SHOIQ
with more expressive role inclusion axioms that allow us to express, for example, that the
brother of a persons father is also that persons uncle. This logic is of considerable interest
as it underpins OWL 2the extension of OWL currently being standardized by the W3C.

Acknowledgments
This is an extended version of a paper published at CADE 2007 (Motik et al., 2007). We
thank the anonymous reviewer for numerous comments that have contributed to the quality
of this paper.

References
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL Envelope. In Kaelbling, L. P., &
Saffiotti, A. (Eds.), Proc. of the 19th Int. Joint Conference on Artificial Intelligence
(IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan Kaufmann Publishers.
Baader, F., Buchheit, M., & Hollunder, B. (1996). Cardinality Restrictions on Concepts.
Artificial Intelligence, 88 (12), 195213.
24. http://www.co-ode.org/galen/

223

fiMotik, Shearer, & Horrocks

Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2007). The Description Logic Handbook: Theory, Implementation and Applications
(2nd edition). Cambridge University Press.
Baader, F., Hollunder, B., Nebel, B., Profitlich, H.-J., & Franconi, E. (1994). An Empirical
Analysis of Optimization Techniques for Terminological Representation systems or:
Making KRIS Get a Move on. Applied Intelligence, 4 (2), 109132.
Baader, F., & Nutt, W. (2007). Basic Description Logics. In Baader, F., Calvanese, D.,
McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), The Description Logic
Handbook: Theory, Implementation and Applications (2nd edition)., pp. 47104. Cambridge University Press.
Baader, F., & Sattler, U. (2001). An Overview of Tableau Algorithms for Description Logics.
Studia Logica, 69, 540.
Balsiger, P., & Heuerding, A. (1998). Comparison of Theorem Provers for Modal Logics
 Introduction and Summary. In de Swart, H. (Ed.), Proc. of the 2nd Int. Conf.
on Analytic Tableaux and Related Methods (TABLEAUX98), Vol. 1397 of LNAI, pp.
2526. Springer.
Balsiger, P., Heuerding, A., & Schwendimann, S. (2000). A Benchmark Method for the
Propositional Modal Logics K, KT, S4. Journal of Automated Reasoning, 24 (3), 297
317.
Baumgartner, P., Furbach, U., & Niemela, I. (1996). Hyper Tableaux. In Proc. of the
European Workshop on Logics in Artificial Intelligence (JELIA 96), No. 1126 in
LNAI, pp. 117, Evora, Portugal. Springer.
Baumgartner, P., Furbach, U., & Pelzer, B. (2008). The Hyper Tableaux Calculus with
Equality and an Application to Finite Model Computation. Journal of Logic and
Computation.
Baumgartner, P., & Schmidt, R. A. (2006). Blocking and Other Enhancements for BottomUp Model Generation Methods. In Furbach, U., & Shankar, N. (Eds.), Proc. of the
3rd Int. Joint Conf. on Automated Reasoning (IJCAR 2006), Vol. 4130 of LNCS, pp.
125139, Seattle, WA, USA. Springer.
Borgida, A. (1996). On the Relative Expressiveness of Description Logics and Predicate
Logics. Artificial Intelligence, 82 (12), 353367.
Bry, F., & Torge, S. (1998). A Deduction Method Complete for Refutation and Finite
Satisfiability. In Dix, J., del Cerro, L. F., & Furbach, U. (Eds.), Proc. European
Workshop on Logics in Artificial Intelligence (JELIA 98), Vol. 1489 of LNCS, pp.
122138, Dagstuhl, Germany. Springer.
Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable Reasoning in Terminological
Knowledge Representation Systems. Journal of Artificial Intelligence Research, 1,
109138.
Demri, S., & de Nivelle, H. (2005). Deciding Regular Grammar Logics with Converse
Through First-Order Logic. Journal of Logic, Language and Information, 14 (3), 289
329.
224

fiHypertableau Reasoning for Description Logics

Ding, Y., & Haarslev, V. (2006). Tableau Caching for Description Logics with Inverse and
Transitive Roles. In Parsia, B., Sattler, U., & Toman, D. (Eds.), Proc. of the 2006 Int.
Workshop on Description Logics (DL 2006), Vol. 189 of CEUR Workshop Proceedings,
Windermere, UK.
Donini, F. M. (2007). Complexity of Reasoning. In Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), The Description Logic Handbook:
Theory, Implementation and Applications (2nd edition)., pp. 105148. Cambridge
University Press.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1998). AL-log: Integrating Datalog
and Description Logics. Journal of Intelligent Information Systems, 10 (3), 227252.
Donini, F. M., & Massacci, F. (2000). EXPTIME tableaux for ALC. Artificial Intelligence,
124 (1), 87138.
Faddoul, J., Farsinia, N., Haarslev, V., & Moller, R. (2008). A Hybrid Tableau Algorithm for
ALCQ. In Ghallab, M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.),
Proc. of the 18th European Conf. on Artificial Intelligence (ECAI 2008), Vol. 178 of
Frontiers in Artificial Intelligence and Applications, pp. 725726, Patras, Greece. IOS
Press.
Fermuller, C., Tammet, T., Zamov, N., & Leitsch, A. (1993). Resolution Methods for the
Decision Problem, Vol. 679 of LNAI. Springer.
Fermuller, C. G., Leitsch, A., Hustadt, U., & Tammet, T. (2001). Resolution Decision Procedures. In Robinson, A., & Voronkov, A. (Eds.), Handbook of Automated Reasoning,
Vol. II, chap. 25, pp. 17911849. Elsevier Science.
Gardiner, T., Horrocks, I., & Tsarkov, D. (2006). Automated Benchmarking of Description
Logic Reasoners. In Proc. of the 2006 Description Logic Workshop (DL 2006), Vol.
189 of CEUR Workshop Proceedings.
Georgieva, L., Hustadt, U., & Schmidt, R. A. (2003). Hyperresolution for Guarded Formulae. Journal of Symbolic Computation, 36 (12), 163192.
Golbreich, C., Zhang, S., & Bodenreider, O. (2006). The Foundational Model of Anatomy
in OWL: Experience and Perspectives. Journal of Web Semantics, 4 (3), 181195.
Gore, R., & Nguyen, L. A. (2007). EXPTIME Tableaux with Global Caching for Description
Logics with Transitive Roles, Inverse Roles and Role Hierarchies. In Proc. of the 16th
Int. Conf. on Automated Reasoning with Tableaux and Related Methods (TABLEAUX
2007), Vol. 4548 of LNCS, pp. 133148, Aix en Provence, France. Springer.
Haarslev, V., & Moller, R. (2001). RACER System Description. In Gore, R., Leitsch, A., &
Nipkow, T. (Eds.), Proc. of the 1st Int. Joint Conf. on Automated Reasoning (IJCAR
2001), Vol. 2083 of LNAI, pp. 701706, Siena, Italy. Springer.
Haarslev, V., Moller, R., & Wandelt, S. (2008). The Revival of Structural Subsumption
in Tableau-Based Description Logic Reasoners. In Baader, F., Lutz, C., & Motik, B.
(Eds.), Proc. of the 21st Int. Workshop on Description Logics (DL 2008), Vol. 353 of
CEUR Workshop Proceedings, Dresden, Germany.

225

fiMotik, Shearer, & Horrocks

Hartel, F. W., de Coronado, S., Dionne, R., Fragoso, G., & Golbeck, J. (2005). Modeling a
description logic vocabulary for cancer research. Journal of Biomedical Informatics,
38 (2), 114129.
Horrocks, I. (1998). Using an Expressive Description Logic: FaCT or Fiction?. In Cohn,
A. G., Schubert, L., & Shapiro, S. C. (Eds.), Proc. of the 6th Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR 98), pp. 636647, Trento,
Italy. Morgan Kaufmann Publishers.
Horrocks, I. (2007). Implementation and Optimization Techniques. In Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.), The Description Logic Handbook: Theory, Implementation and Applications (2nd edition)., pp.
329373. Cambridge University Press.
Horrocks, I., & Patel-Schneider, P. F. (1998a). Comparing Subsumption Optimizations. In
Franconi, E., De Giacomo, G., MacGregor, R. M., Nutt, W., & Welty, C. A. (Eds.),
Proc. of the 1998 Description Logic Workshop (DL98), Vol. 11 of CEUR Workshop
Proceedings, pp. 9094, PovoTrento, Italy.
Horrocks, I., & Patel-Schneider, P. F. (1998b). DL Systems Comparison. In Franconi,
E., De Giacomo, G., MacGregor, R. M., Nutt, W., & Welty, C. A. (Eds.), Proc. of
the 1998 Int. Workshop on Description Logic (DL98), Vol. 11 of CEUR Workshop
Proceedings, pp. 5557, PovoTrento, Italy.
Horrocks, I., & Sattler, U. (2001). Ontology Reasoning in the SHOQ(D) Description Logic.
In Nebel, B. (Ed.), Proc. of the 7th Int. Joint Conf. on Artificial Intelligence (IJCAI
2001), pp. 199204, Seattle, WA, USA. Morgan Kaufmann Publishers.
Horrocks, I., & Sattler, U. (2007). A Tableau Decision Procedure for SHOIQ. Journal of
Automated Reasoning, 39 (3), 249276.
Horrocks, I., Sattler, U., & Tobies, S. (2000a). Practical Reasoning for Very Expressive
Description Logics. Logic Journal of the IGPL, 8 (3), 239263.
Horrocks, I., Sattler, U., & Tobies, S. (2000b). Reasoning with Individuals for the Description Logic SHIQ. In MacAllester, D. (Ed.), Proc. of the 17th Int. Conf. on Automated
Deduction (CADE-17), Vol. 1831 of LNAI, pp. 482496, Pittsburgh, USA. Springer.
Hudek, A. K., & Weddell, G. (2006). Binary Absorption in Tableaux-Based Reasoning
for Description Logics. In Parsia, B., Sattler, U., & Toman, D. (Eds.), Proc. of the
2006 Int. Workshop on Description Logics (DL 2006), Vol. 189 of CEUR Workshop
Proceedings, Windermere, UK.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data Complexity of Reasoning in Very Expressive Description Logics. In Proc. of the 19th Int. Joint Conf. on Artificial Intelligence
(IJCAI 2005), pp. 466471, Edinburgh, UK. Morgan Kaufmann Publishers.
Hustadt, U., & Schmidt, R. A. (1999). Issues of Decidability for Description Logics in the
Framework of Resolution. In Caferra, R., & Salzer, G. (Eds.), Selected Papers from
Automated Deduction in Classical and Non-Classical Logics, Vol. 1761 of LNAI, pp.
191205. Springer.
Kutz, O., Horrocks, I., & Sattler, U. (2006). The Even More Irresistible SROIQ. In
Doherty, P., Mylopoulos, J., & Welty, C. A. (Eds.), Proc. of the 10th Int. Conf. on the
226

fiHypertableau Reasoning for Description Logics

Principles of Knowledge Representation and Reasoning (KR 2006), pp. 6878, Lake
District, UK. AAAI Press.
Motik, B. (2006). Reasoning in Description Logics using Resolution and Deductive
Databases. Ph.D. thesis, Univesitat Karlsruhe, Germany.
Motik, B., & Horrocks, I. (2008). Individual Reuse in Description Logic Reasoning. In
Proc. of the 4th Int. Joint Conf. on Automated Reasoning (IJCAR 2008), Sydney,
Australia. Springer. To appear.
Motik, B., Shearer, R., & Horrocks, I. (2007). Optimized Reasoning in Description Logics
using Hypertableaux. In Pfenning, F. (Ed.), Proc. of the 21st Conference on Automated Deduction (CADE-21), Vol. 4603 of LNAI, pp. 6783, Bremen, Germany.
Springer.
Nonnengart, A., & Weidenbach, C. (2001). Computing Small Clause Normal Forms. In
Robinson, A., & Voronkov, A. (Eds.), Handbook of Automated Reasoning, Vol. I,
chap. 6, pp. 335367. Elsevier Science.
Parsia, B., & Sirin, E. (2004). Pellet: An OWL-DL Reasoner. Poster at the 3rd Int. Semantic
Web Conference (ISWC 2004).
Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL Web Ontology Language:
Semantics and Abstract Syntax, W3C Recommendation..
http://www.w3.org/TR/owl-semantics/.
Plaisted, D. A., & Greenbaum, S. (1986). A Structure-Preserving Clause Form Translation.
Journal of Symbolic Logic and Computation, 2 (3), 293304.
Rector, A. L., & Rogers, J. (2006). Ontological and Practical Issues in Using a Description
Logic to Represent Medical Concept Systems: Experience from GALEN. In Barahona,
P., Bry, F., Franconi, E., Henze, N., & Sattler, U. (Eds.), Tutorial Lectures of the 2nd
Int. Summer School 2006, Vol. 4126 of LNCS, pp. 197231, Lisbon, Portugal. Springer.
Robinson, A. (1965). Automatic Deduction with Hyper-Resolution. Int. Journal of Computer Mathematics, 1, 227234.
Schmidt, R. A., & Hustadt, U. (2003). A Principle for Incorporating Axioms into the FirstOrder Translation of Modal Formulae. In Baader, F. (Ed.), Proc. of the 19th Int.
Conf. on Automated Deduction (CADE-19), Vol. 2741 of LNAI, pp. 412426, Miami
Beach, FL, USA. Springer.
Sirin, E., Cuenca Grau, B., & Parsia, B. (2006). From Wine to Water: Optimizing Description Logic Reasoning for Nominals. In Doherty, P., Mylopoulos, J., & Welty, C. A.
(Eds.), Proc. of the 10th Int. Conf. on Principles of Knowledge Representation and
Reasoning (KR 2006), pp. 9099, Lake District, UK. AAAI Press.
Tobies, S. (2000). The Complexity of Reasoning with Cardinality Restrictions and Nominals
in Expressive Description Logics. Journal of Artificial Intelligence Research, 12, 199
217.
Tobies, S. (2001). Complexity Results and Practical Algorithms for Logics in Knowledge
Representation. Ph.D. thesis, RWTH Aachen, Germany.

227

fiMotik, Shearer, & Horrocks

Tsarkov, D., & Horrocks, I. (2004). Efficient Reasoning with Range and Domain Constraints.
In Haarslev, V., & Moller, R. (Eds.), Proc. of the 2004 Int. Workshop on Description
Logics (DL 2004), Vol. 104 of CEUR Workshop Proceedings, Whistler, BC, Canada.
Tsarkov, D., & Horrocks, I. (2005a). Optimised Classification for Taxonomic Knowledge
Bases. In Horrocks, I., Sattler, U., & Wolter, F. (Eds.), Proc. of the 2005 Int. Workshop
on Description Logics (DL 2005), Edinburgh, Scotland, UK.
Tsarkov, D., & Horrocks, I. (2005b). Ordering Heuristics for Description Logic Reasoning.
In Kaelbling, L. P., & Saffiotti, A. (Eds.), Proc. of the 19th Int. Joint Conf. on Artificial Intelligence (IJCAI 2005), pp. 609614, Edinburgh, UK. Morgan Kaufmann
Publishers.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ Description Logic Reasoner: System Description. In Proc. of the 3rd Int. Joint Conf. on Automated Reasoning (IJCAR 2006),
Vol. 4130 of LNAI, pp. 292297, Seattle, WA, USA. Springer.
Wu, J., & Haarslev, V. (2008). Planning of Axiom Absorption. In Baader, F., Lutz, C., &
Motik, B. (Eds.), Proc. of the 21st Int. Workshop on Description Logics (DL 2008),
Vol. 353 of CEUR Workshop Proceedings, Dresden, Germany.

228

fiJournal of Artificial Intelligence Research 36 (2009) 71

Submitted 12/08; published 10/09

Prime Implicates and Prime Implicants:
From Propositional to Modal Logic
Meghyn Bienvenu

meghyn@informatik.uni-bremen.de

Department of Mathematics and Computer Science
University of Bremen, Germany

Abstract
Prime implicates and prime implicants have proven relevant to a number of areas of
artificial intelligence, most notably abductive reasoning and knowledge compilation. The
purpose of this paper is to examine how these notions might be appropriately extended from
propositional logic to the modal logic K. We begin the paper by considering a number of
potential definitions of clauses and terms for K. The different definitions are evaluated with
respect to a set of syntactic, semantic, and complexity-theoretic properties characteristic of
the propositional definition. We then compare the definitions with respect to the properties
of the notions of prime implicates and prime implicants that they induce. While there is
no definition that perfectly generalizes the propositional notions, we show that there does
exist one definition which satisfies many of the desirable properties of the propositional
case. In the second half of the paper, we consider the computational properties of the
selected definition. To this end, we provide sound and complete algorithms for generating
and recognizing prime implicates, and we show the prime implicate recognition task to be
Pspace-complete. We also prove upper and lower bounds on the size and number of prime
implicates. While the paper focuses on the logic K, all of our results hold equally well for
multi-modal K and for concept expressions in the description logic ALC.

1. Introduction
Prime implicates and prime implicants are important notions in artificial intelligence. They
have given rise to a significant body of work in automated reasoning and have been applied
to a number of different sub-areas in AI. Traditionally, these concepts have been studied
in the context of propositional logic, but they have also been considered for many-valued
(Ramesh & Murray, 1994) and first-order logic (Marquis, 1991a, 1991b). Not much is
known, however, about prime implicates and prime implicants in other logics. In particular,
no definition of prime implicate or prime implicant has ever been proposed for a modal or
description logic, nor has it been shown that no reasonable definition can be provided.
Given the increasing interest in modal and description logics as knowledge representation
languages, one naturally wonders whether these notions can be suitably generalized to these
more expressive logics.
We recall that in propositional logic the prime implicates of a formula are defined to be
its logically strongest clausal consequences. The restriction to clauses is made in order to
reduce redundant elements from a formulas set of consequences: there is no use in keeping
around the consequence a  b when one already has the consequences a and b. The decision
to consider only the logically strongest clausal consequences is motivated by a desire to
eliminate irrelevant weaker consequences: if we already have the consequence a, there is
c
2009
AI Access Foundation. All rights reserved.

fiBienvenu

no point in retaining the consequences a  b or a  b. Prime implicates thus provide a
complete yet compact representation of the set of logical consequences of a formula. What
is particularly nice about this representation is that it makes many computational tasks
simpler: satisfiability, tautology, entailment, and equivalence queries and the conditioning
and forgetting transformations are all tractable for formulae represented by their prime
implicates (Darwiche & Marquis, 2002). This is why prime implicates are considered an
interesting target language for knowledge compilation (Cadoli & Donini, 1997; Darwiche
& Marquis, 2002). Prime implicates have also proved relevant to other sub-areas of AI,
like distributed reasoning (Adjiman, Chatalic, Goasdoue, Rousset, & Simon, 2006), belief
revision (Bittencourt, 2007; Pagnucco, 2006), non-monotonic reasoning (cf. Przymusinski,
1989), and characterizations of relevance (Lakemeyer, 1995; Lang, Liberatore, & Marquis,
2003).
The dual notion to prime implicates is prime implicants, which are defined to be the
logically weakest terms (= conjunctions of literals) which imply a given formula. The main
application domain for prime implicants is in abduction and diagnosis. We recall that in
abduction, one is given a background theory and an observation, and the objective is to
find an explanation for the observation. In logical terms, an explanation is a formula which
logically entails the observation when taken together with the background theory. As the
set of explanations for an abduction problem can be very large, an important question
is how to select a representative subset of explanations. One very common approach is
to use prime implicants: the relevant explanations for an observation o with respect to a
background theory t are taken to be the prime implicants of t  o (de Kleer, Mackworth,
& Reiter, 1992; Eiter & Makino, 2002).
For many applications in AI, the expressive power of propositional logic proves insufficient. First-order logic provides a much greater level of expressivity, but at the price of
undecidability. Modal and description logics offer an interesting trade-off between expressivity and complexity, as they are generally more expressive than propositional logic yet
are better-behaved computationally than first-order logic. This explains the growing trend
towards using such languages for knowledge representation.
A prototypical description logic is ALC, which extends propositional logic with restricted
forms of universal and existential quantification. An example expression in ALC is
F emale  hasChild.F emale  hasChild.(Doctor  P rof essor)  hasP et.Dog
which describes the set of individuals who are female, have at least one daughter and one
pet dog, and are such that all of their children are either doctors or professors. The above
concept expression can be represented equally well in the modal logic K2 by the formula:
F emale  31 F emale  21 (Doctor  P rof essor)  32 Dog
Schild (1991) proved a general result which showed that the description logic ALC with n
binary relations is in fact a notational variant of the multi-modal logic Kn . This means that
results concerning Kn can be transferred to ALC, and vice-versa.
In this paper, we investigate the notions of prime implicates and prime implicants for
the modal logic K = K1 , but actually all of our results hold for formulae in Kn , and hence
also for concept expressions in ALC. The decision to present our results in terms of K
72

fiPrime Implicates and Prime Implicants in Modal Logic

rather than in terms of Kn or ALC was motivated solely by a desire to simplify notation
and increase the readability of the proofs.
The question of how the notions of prime implicates and prime implicants can be suitably
defined for the logic K is clearly of interest from a theoretical point of view. We argue,
however, that this question is also practically relevant. To support this claim, we briefly
discuss two application areas in which the study of prime implicates and prime implicants
in K might prove useful.
The first domain of application we will consider is abductive reasoning in K. As noted
above, one of the key foundational issues in abductive reasoning is the selection of an
interesting subset of explanations. This issue is especially crucial for logics like K which
allow for an infinite number of non-equivalent formulae, since this means that the number of
non-equivalent explanations for an abduction problem is not just large but in fact infinite,
making it simply impossible to enumerate the entire set of explanations. As prime implicants
are a widely-accepted means of characterizing relevant explanations in propositional logic,
a reasonable starting point for research into abductive reasoning in the logic K is the study
of different possible definitions of prime implicant in K and their properties.
The investigation of prime implicates in K is also relevant to the development of knowledge compilation procedures for K. We recall that knowledge compilation (cf. Darwiche
& Marquis, 2002) is a general technique for coping with the intractability of reasoning
which consists in an off-line phase in which a knowledge base is rewritten as an equivalent
knowledge base which allows for tractable reasoning, followed by an online phase in which
reasoning is performed on the compiled knowledge base. The idea is that the initial cost
of compiling the knowledge base will be offset by computational savings on later queries.
Currently, most work on knowledge compilation is restricted to propositional logic, even
though this technique could prove highly relevant for modal and description logics, which
generally suffer from an even higher computational complexity than propositional logic. As
prime implicates are one of the better-known mechanisms for compiling formulae in propositional logic, it certainly makes sense to investigate whether this approach to knowledge
compilation can be fruitfully extended to logics like K.
Our paper is organized as follows. After some preliminaries, we consider how to appropriately generalize the notions of clauses and terms to K. As there is no obvious definition,
we enumerate a list of syntactic, semantic, and complexity-theoretic properties of the propositional definitions, which we then use to compare the different candidate definitions. We
next consider the different definitions in light of the notions of prime implicate and prime
implicant they induce. Once again, we list some basic properties from the propositional
case that we would like to satisfy, and we see how the different definitions measure up.
In the second half of the paper, we investigate the computational properties of the most
satisfactory definition of prime implicates. We consider the problems of prime implicate
generation and recognition, and we provide sound and complete algorithms for both tasks.
We also study the complexity of the prime implicate recognition problem, showing it to be
Pspace-complete and thus of the same complexity as satisfiability and deduction in K. We
conclude the paper with a discussion of the relevance of our results to the two application
areas cited above and some directions for future research. In order to enhance the readability of the paper, proofs have been omitted from the body of the text. Full proofs can be
found in the appendix.
73

fiBienvenu

2. Preliminary Definitions and Notation
We briefly recall the basics of the modal logic K (refer to Blackburn, de Rijke, & Venema,
2001; Blackburn, van Benthem, & Wolter, 2006, for good introductions to modal logic).
Formulae in K are built up from a set of propositional variables V, the standard logical
connectives (, , and ), and the modal operators 2 and 3. We will call a formula of
the form 2 (resp. 3) a 2-formula (resp. 3-formula). Where convenient we will use
   as an abbreviation for   . We adopt the shorthand 2k  (resp. 3k ) to refer to
the formula consisting of  preceded by k copies of 2 (resp. 3), with the convention that
20  = 30  = . We will use var() to refer to the set of propositional variables appearing
in a formula . The modal depth of a formula , written (), is defined as the maximal
number of nested modal operators appearing in , e.g. (3(a  2a)  a) = 2. We define the
length of a formula , written ||, to be the number of occurrences of propositional variables,
logical connectives, and modal operators in . For example, we would have |(a  b)| = 4
and |3(a  b)  2a| = 8.
Negation normal form (NNF) is defined just as in propositional logic: a formula is said
to be in NNF if negation only appears directly before propositional variables. Every formula
 in K can be transformed into an equivalent formula in NNF using the recursive procedure
Nnf defined as follows:
Nnf (l)=l (for propositional literals l)
Nnf (1  2 )=Nnf (1 )Nnf (2 )
Nnf ((1  2 ))=Nnf (1 )Nnf (2 )
Nnf (1  2 )=Nnf (1 )Nnf (2 )
Nnf ((1  2 ))=Nnf (1 )Nnf (2 )

Nnf (2)=2Nnf ()
Nnf (2)=3Nnf ()
Nnf (3)=3Nnf ()
Nnf (3)=2Nnf ()
Nnf ()=Nnf ()

For example, applying Nnf to the formula 2(a  3(b  c)) results in the formula 3(a 
2(b c)) which is in NNF. The transformation Nnf takes linear time, and yields a formula
which is no more than double the size of the original formula and has the same modal depth
and propositional variables as the original.
A model for K is a tuple M = hW, R, vi, where W is a non-empty set of possible worlds,
R  W  W is a binary relation over worlds, and v : W  V  {true, f alse} is a valuation
of the propositional variables at each world. Models can be understood as labelled directed
graphs, in which the vertices correspond to the elements of W, the directed edges represent
the binary relation R, and the vertices are labeled by propositional valuations which specify
the propositional variables which are true in the corresponding possible world.
Satisfaction of a formula  in a model M at the world w (written M, w |= ) is defined
inductively as follows:
 M, w |= a if and only if v(w, a) = true
 M, w |=  if and only if M, w 6|= 
 M, w |=    if and only if M, w |=  and M, w |= 
 M, w |=    if and only if M, w |=  or M, w |= 
 M, w |= 2 if and only if M, w |=  for all w such that wRw
74

fiPrime Implicates and Prime Implicants in Modal Logic

 M, w |= 3 if and only if M, w |=  for some w such that wRw
If we think of models as labeled directed graphs, then determining the satisfaction of a
formula 2 at vertex w consists in evaluating  at all of the vertices which can be reached
from w via an edge; 2 is satisfied at w just in the case that  holds in each of these
successor vertices. Similarly, in order to decide whether a formula 3 holds at a vertex w,
we consider each of the successors of w in the graph and check whether at least one of these
vertices satisfies .
A formula  is said to be a tautology, written |= , if M, w |=  for every model M and
world w. A formula  is satisfiable if there is some model M and some world w such that
M, w |= . If there is no M and w for which M, w |= , then  is called unsatisfiable, and
we write  |= .
Ladner (1977) showed that satisfiability and unsatisfiability in K are Pspace-complete.
For Pspace membership, Ladner exhibited a polynomial space tableaux-style algorithm for
deciding satisfiability of K formulae. Pspace-hardness was proven by means of a reduction
from QBF validity (the canonical Pspace-complete problem).
In modal logic, the notion of logical consequence (or entailment) can be defined in one
of two ways:
 a formula  is a global consequence of  if whenever M, w |=  for every world w of
a model M, then M, w |=  for every world w of M
 a formula  is a local consequence of  if M, w |=  implies M, w |=  for every model
M and world w
In this paper, we will only consider the notion of local consequence, and we will take  |= 
to mean that  is a local consequence of . When  |= , we will say that  entails .
Two formulae  and  will be called equivalent, written   , if both  |=  and  |= .
A formula  is said to be logically stronger than  if  |=  and  6|= .
We now highlight some basic properties of logical consequence and equivalence in K
which will play an important role in the proofs of our results.
Theorem 1. Let , 1 , ..., m , , 1 , ..., n be formulae in K, and let  be a propositional
formula. Then
1.  |=   |=        |= 
2.  |=   3 |= 3  2 |= 2
3.  31 ...3m 21 ...2n |=   ( |=  or i 1 ...n |=  for some i)
4. |=   31  ...  3m  21  ...  2n  (|=  or |= 1  ...  m  i for some i)
5. 2 |= 21  ...  2n   |= i for some i
6. 31  ...  3m  21  ...  2n
 31  ...  3m  2(1  1  ...  m )  ...  2(n  1  ...  m )
75

fiBienvenu

Statement 1 of Theorem 1 shows how the three reasoning tasks of entailment, unsatisfiability, and tautology-testing can be rephrased in terms of one another. Statement 2 tells us
how entailment between two 2- or 3-formulae can be reduced to entailment between those
formulae with the first modality removed. Statements 3 and 4 define the conditions under
which a conjunction (resp. disjunction) of propositional literals and 2- and 3-formulae
is unsatisfiable (resp. a tautology). Statement 5 gives us the conditions under which a
2-formula implies a disjunction of 2-formulae. Statement 6 demonstrates the interaction
between 2- and 3-formulae in a disjunction.
Theorem 2. Let  be a disjunction of propositional literals and 2- and 3-formulae. Then
each of the following statements holds:
1. If  |=  for some non-tautological propositional clause , then every disjunct of  is
either a propositional literal or an unsatisfiable 3-formula
2. If  |= 31  ...  3n , then every disjunct of  is a 3-formula
3. If  |= 21  ...  2n and 6|= 21  ...  2n , then every disjunct of  is either a
2-formula or an unsatisfiable 3-formula
Theorem 3. Let  =   31  ...  3m  21  ...  2n and  =    31  ...  3p 
21  ...  2q be formulae in K. If  and   are both propositional and 6|=  , then

  |=   and
1  ...  m |= 1  ...  p and
 |=  

for every i there is some j such that i |= 1  ...  p  j
Theorems 2 and 3 concern entailment relations between formulae which are disjunctions
of propositional literals and 2- and 3-formulae. Theorem 2 tells us what kinds of formulae
of this type can entail a propositional clause, a disjunction of 3-formulae, or a disjunction
of 2-formulae, while Theorem 3 outlines the conditions under which two formulae of this
type can be related to each other by the entailment relation. We illustrate Theorem 3 on a
small example.
Example 4. Consider the formula  = b  3(a  3c)  3(d  2a)  2(c  d). Then
according to Theorem 3, we have:
  |= b  d  3(a  d)  2c, since b |= b  d and (a  3c)  (d  2a) |= a  d and
c  d |= c  (a  d)
  6|= a  3c, since b 6|= a
  6|= a  b  3(a  c), since (a  3c)  (d  2a) 6|= a  c
  6|= b  3(a  2a)  2c, since c  d 6|= c  (a  2a)
76

fiPrime Implicates and Prime Implicants in Modal Logic

3. Literals, Clauses, and Terms in K
As we have seen in the introduction, the notions of prime implicates and implicants are
straightforwardly defined using the notions of clauses and terms. Thus, if we aim to provide
suitable definitions of prime implicates and implicants for the logic K, we first need to decide
upon a suitable definition of clauses and terms in K. Unfortunately, whereas clauses and
terms are standard notions in both propositional and first-order logic1 , there is no generally
accepted definition of clauses and terms in K. Indeed, several quite different notions of
clauses and terms have been proposed in the literature for different purposes.
Instead of blindly picking a definition and hoping that it is appropriate, we prefer to
list a number of characteristics of literals, clauses, and terms in propositional logic, giving
us a principled means of comparing different candidate definitions. Each of the properties
below describes something of what it is to be a literal, clause, or term in propositional logic.
Although our list cannot be considered exhaustive, we do believe that it covers the principal
syntactic, semantic, and complexity-theoretic properties of the propositional definition.
P1 Literals, clauses, and terms are in negation normal form.
P2 Clauses do not contain , terms do not contain , and literals contain neither  nor .
P3 Clauses (resp. terms) are disjunctions (resp. conjunctions) of literals.
P4 The negation of a literal is equivalent to another literal. Negations of clauses (resp.
terms) are equivalent to terms (resp. clauses).
P5 Every formula is equivalent to a finite conjunction of clauses. Likewise, every formula
is equivalent to a finite disjunction of terms.
P6 The task of deciding whether a given formula is a literal, term, or clause can be accomplished in polynomial-time.
P7 The task of deciding whether a clause (resp. term) entails another clause (resp. term)
can be accomplished in polynomial-time.
One may wonder whether there exist definitions of literals, clauses, and terms for K
satisfying all of these properties. Unfortunately, we can show this to be impossible.
Theorem 5. Any definition of literals, clause, and terms for K that satisfies properties P1
and P2 cannot satisfy P5.
The proof of Theorem 5 only makes use of the fact that  does not distribute over 3
and  does not distribute over 2, which means that our impossibility result holds equally
well for most standard modal and description logics.
We will now consider a variety of possible definitions and evaluate them with respect
to the above criteria. The first definition that we will consider is that proposed by Cialdea
1. One might wonder why we do not simply translate our formulae in K into first-order formulae and then
put them into clausal form. The reason is simple: we are looking to define clauses and terms within the
language of K, and the clauses we obtain on passing by first-order logic are generally not expressible in K.
Moreover, if we were to define clauses in K as those first-order clauses which are representable in K, we
would obtain a set of clauses containing no 3 modalities, thereby losing much of the expressivity of K.

77

fiBienvenu

Mayer and Pirri (1995) in a paper on abductive reasoning in modal logic. They define terms
to be the formulae which can be constructed from the propositional literals using only , 2,
and 3. Modal clauses and literals are not used in the paper but can be defined analogously,
yielding the following definition2 :
D1

L ::= a | a | 2L | 3L
C ::= a | a | 2C | 3C | C  C
T ::= a | a | 2T | 3T | T  T

It is easy to see by inspection that this definition satisfies properties P1-P2, P4, and P6.
Property P3 is not satisfied, however, since there are clauses that are not disjunctions of
literals  take for instance 2(a  b). From Theorem 5 and the fact that both P1 and P2 are
satisfied, we can conclude that property P5 cannot hold. At first glance, it may seem that
entailment between clauses or terms could be accomplished in polynomial time, but this is
not the case. In fact, we can show this problem to be NP-complete. The proof relies on the
very strong resemblance between terms of D1 and concept expressions in the description
logic ALE (for which both unsatisfiability and deduction are known to be NP-complete).
By using a slightly different definition, we can gain P3:
D2

L ::= a | a | 2L | 3L
C ::= L | C  C
T ::= L | T  T

It can be easily verified that definition D2 satisfies properties P1-P4 and P6. As definition
D1 does not satisfy P5, and definition D2 is even less expressive, it follows that D2 does not
satisfy P5 either. This reduced expressiveness does not however improve its computational
complexity: property P7 is still not satisfied as we can show that entailment between
clauses or terms is NP-complete using the same reduction as was used for definition D1.
The fact that even an extremely inexpressive definition like D2 does not allow for polynomial
entailment between clauses and terms suggests that property P7 cannot be satisfied by any
reasonable definition of clauses and terms for K.
Let us now consider some more expressive options. We begin with the following definition
of clauses that was proposed by Enjalbert and Farinas del Cerro (1989) for the purpose of
modal resolution:
D3
C ::= a | a | 2C | 3ConjC | C  C
ConjC ::= C | ConjC  ConjC
This definition of clauses can be extended to a definition of terms and literals which satisfies
P3 or P4, but there is no extension which satisfies both properties. Let us first consider
one of the possible extensions which satisfies P4 and a maximal subset of P1-P7:
D3a

L ::= a | a | 2L | 3L
C ::= a | a | 2C | 3ConjC | C  C
ConjC ::= C | ConjC  ConjC
T ::= a | a | 2DisjT | 3T | T  T
DisjT ::= T | DisjT  DisjT

2. Note that here and in what follows, we let a range over propositional variables and L, C, and T range
over the sets of literals, clauses, and terms, respectively.

78

fiPrime Implicates and Prime Implicants in Modal Logic

This definition satisfies P1 and P4-P6 (satisfaction of P5 was shown in Enjalbert & Farinas
del Cerro, 1989). It does not satisfy P3 as there are clauses that are not disjunctions of
literals  take for example 2(a  b). Given that definition D3a is strictly more expressive
than definitions D1 and D2, it follows that entailment between clauses or terms must be
NP-hard, which means that D3a does not satisfy P7. In fact, we can show that entailment
between clauses or terms of definition D3a is Pspace-complete. To do so, we modify the
polynomial translation of QBF into K used to prove Pspace-hardness of K so that the
translated formula is a conjunction of clauses with respect to D3a. We then notice that
a formula  is unsatisfiable if and only if 3 entails 3(a  a). We thus reduce QBF
validity to entailment between clauses, making this task Pspace-hard, and hence (being a
subproblem of entailment in K) Pspace-complete. This same idea is used to show Pspacecompleteness for definitions D3b and D5 below.
If instead we extend D3 so as to enforce property P3, we obtain the following definition:
L ::= a | a | 2C | 3ConjC
C ::= a | a | 2C | 3ConjC | C  C
ConjC ::= C | ConjC  ConjC
T ::= L | T  T
This definition satisfies all of the properties except P2, P4, and P7. Property P4 fails to
hold because the negation of the literal 3(a  b) is not equivalent to any literal. The proof
that P5 holds is constructive: we use standard logical equivalences to rewrite formulae as
equivalent conjunctions of clauses and disjunctions of terms (this is also what we do for
definitions D4 and D5 below).
We now consider two rather simple definitions that satisfy properties P3, P4, and P5.
The first definition, which is inspired by the notion of modal atom proposed by Giunchiglia
and Sebastiani (1996), defines literals as the set of formulae in NNF that cannot be decomposed propositionally.
D3b

L ::= a | a | 2F | 3F
C ::= L | C  C
T ::= L | T  T
F ::= a | a | F  F | F  F | 2F | 3F
D4 satisfies all of the properties except P2 and P7. For P7, we note that an arbitrary formula  in NNF is unsatisfiable (a Pspace-complete problem) if and only if 3 |= 3(a  a).
Definition D4 is very liberal, imposing no structure on the formulae behind modal operators. If we define literals to be the formulae in NNF that cannot be decomposed modally
(instead of propositionally), we obtain a much stricter definition which satisfies exactly the
same properties as D4.
D4

D5

L ::= a | a | 2C | 3T
C ::= L | C  C
T ::= L | T  T

A summary of our analysis of the different definitions with respect to properties P1-P7
is provided in the following table.
Theorem 6. The results in Figure 1 hold.
79

fiBienvenu

P1
P2
P3
P4
P5
P6
P7

D1
D2
yes
yes
yes
yes
no
yes
yes
yes
no
no
yes
yes
no (unless P=NP)

D3a D3b D4 D5
yes
yes
yes yes
no
no
no
no
no
yes
yes yes
yes
no
yes yes
yes
yes
yes yes
yes
yes
yes yes
no (unless P=Pspace)

Figure 1: Properties of the different definitions of literals, clauses, and terms.

Clearly deciding between different candidate definitions is more complicated than counting up the number of properties that the definitions satisfy, the simple reason being that
some properties are more important than others. Take for instance property P5 which
requires clauses and terms to be expressive enough to represent all of the formulae in K. If
we just use the standard propositional definition of clauses and terms (thereby disregarding
the modal operators), then we find that it satisfies every property except P5, and hence
more properties than any of the definitions considered in this section, and yet we would be
hard-pressed to find someone who considers the propositional definition an appropriate definition for K. This demonstrates that expressiveness is a particularly important property,
so important in fact that we should be willing to sacrifice properties P2 and P7 to keep it.
Among the definitions that satisfy P5, we prefer definitions D4 and D5 to definitions D3a
and D3b, as the latter definitions have less in common with the propositional definition
and present no advantages over D4 and D5.
Of course, when it comes down to it, the choice of a definition must depend on the
particular application in mind. There may very well be circumstances in which a less
expressive or less elegant definition may prove to be the most suitable. In this paper we
are using clauses and terms to define prime implicates and prime implicants, so for us the
most important criteria for choosing a definition will be the quality of the notions of prime
implicates and prime implicants that the definition induces.

4. Prime Implicates/Implicants in K
Once a definition of clauses and terms has been fixed, we can define prime implicates and
prime implicants in exactly the same manner as in propositional logic:
Definition 7. A clause  is an implicate of a formula  if and only if  |= .  is a prime
implicate of  if and only if:
1.  is an implicate of 
2. If  is an implicate of  such that  |= , then  |= 
Definition 8. A term  is an implicant of the formula  if and only if  |= .  is a prime
implicant of  if and only if:
1.  is an implicant of 
80

fiPrime Implicates and Prime Implicants in Modal Logic

2. If  is an implicant of  such that  |=  , then  |= 
Of course, the notion of prime implicate (resp. implicant) that we get will be determined by the definition of clause (resp. term) that we have chosen. We will compare different definitions using the following well-known properties of prime implicates/implicants
in propositional logic:
Finiteness The number of prime implicates (resp. prime implicants) of a formula is finite
modulo logical equivalence.
Covering Every implicate of a formula is entailed by some prime implicate of the formula.
Conversely, every implicant of a formula entails some prime implicant of the formula.
Equivalence A model M is a model of  if and only if M is a model of all the prime
implicates of  if and only if M is a model of some prime implicant of 3 .
Implicant-Implicate Duality Every prime implicant of a formula is equivalent to the
negation of some prime implicate of the negated formula. Conversely, every prime
implicate of a formula is equivalent to the negation of a prime implicant of the negated
formula.
Distribution If  is a prime implicate of 1  ...  n , then there exist prime implicates
1 , ..., n of 1 , ..., n such that   1  ...  n . Likewise, if  is a prime implicant
of 1  ...  n , then there exist prime implicants 1 , ..., n of 1 , ..., n such that
  1  ...  n
Finiteness ensures that the prime implicates/implicants of a formula can be finitely
represented, while Covering means the prime implicates provide a complete representation
of the formulas implicates. Equivalence guarantees that no information is lost in replacing a formula by its prime implicates/implicants, whereas Implicant-Implicate Duality
allows us to transfer results and algorithms for prime implicates to prime implicants, and
vice-versa. Finally, Distribution relates the prime implicates/implicants of a formula to
the prime implicates/implicants of its sub-formulae. This property will play a key role in
the prime implicate generation algorithm presented in the next section.
We can show that definition D4 satisfies all five properties. For Finiteness and Covering, we first demonstrate that every implicate  of a formula  is entailed by some implicate
 of  with var( )  var() and having depth at most () + 1 (and similarly for implicants). As there are only finitely many non-equivalent formulae on a finite language and
with bounded depth, it follows that there are only finitely many prime implicates/implicants
of a given formula, and that there can be no infinite chains of increasingly stronger implicates (or increasingly weaker implicants). Equivalence follows directly from Covering
and the property P5 of the previous section: we use P5 to rewrite  as a conjunction
of clauses, each of which is implied by some prime implicate of  because of Covering.
The property Implicant-Implicate Duality is an immediate consequence of the duality
3. The property Equivalence is more commonly taken to mean that a formula is equivalent to the conjunction of its prime implicates and the disjunction of its prime implicants. We have chosen a model-theoretic
formulation in order to allow for the possibility that the set of prime implicates/implicants is infinite.

81

fiBienvenu

between clauses and terms (P4). Distribution can be shown using Covering plus the
fact that a disjunction of clauses is a clause and a conjunction of terms is a term (P3).
Theorem 9. The notions of prime implicates and prime implicants induced by definition
D4 satisfy Finiteness, Covering, Equivalence, Implicant-Implicate Duality, and
Distribution.
We remark by way of contrast that in first-order logic the notion of prime implicate induced by the standard definition of clauses has been shown to falsify Finiteness, Covering,
and Equivalence (Marquis, 1991a, 1991b).
We now show that definition D4 is the only one of our definitions to satisfy all five
properties. For definitions D1 and D2, we show that Equivalence does not hold. This
is a fairly straightforward consequence of the fact that these definitions do not satisfy
property P5.
Theorem 10. The notions of prime implicates and prime implicants induced by definitions
D1 and D2 do not satisfy Equivalence.
For the notions of prime implicates induced by definitions D3a, D3b, and D5, we show
in the appendix that the clause 2 3k a  3(a  b  2k a) is a prime implicate of 2(a  b)
for every k  14 . We thereby demonstrate not only that these definitions admit formulae
with infinitely many distinct prime implicates but also that they allow seemingly irrelevant
clauses to be counted as prime implicates. This gives us strong grounds for dismissing these
definitions as much of the utility of prime implicates in applications comes from their ability
to eliminate such irrelevant consequences.
Theorem 11. The notions of prime implicates and prime implicants induced by D3a, D3b,
and D5 falsify Finiteness.
While the comparison in the last section suggested that D5 was at least as suitable as
D4 as a definition of clauses and terms, the results of this section rule out D5 as a suitable
definition for prime implicates and prime implicants. In the remainder of the paper, we will
concentrate our attention on the notions of prime implicates and prime implicants induced
by definition D4, as these have been shown to be the most satisfactory generalizations of
the propositional case.

5. Prime Implicate Generation and Recognition
In this section, we investigate the computational aspects of modal prime implicates. As we
will be primarily focusing on the notion of prime implicate induced by definition D4, for
the remainder of the paper we will use the words clause, term, and prime implicate
to mean clause, term, and prime implicate with respect to definition D4, except where
explicitly stated otherwise.
We remark that, without loss of generality, we can restrict our attention to prime implicates since by Implicant-Implicate Duality (Theorem 9) any algorithm for generating
or recognizing prime implicates can be easily adapted into an algorithm for generating or
recognizing prime implicants.
4. For D4, the only prime implicate of 2(a  b) is itself.

82

fiPrime Implicates and Prime Implicants in Modal Logic

Function Dnf -4()
Input: a formula 
Output: a set of D4-terms
Return the set of terms output by Iter-Dnf-4({Nnf ()}).
Function Iter-Dnf -4(S)
Input: a set S of formulae in NNF
Output: a set of D4-terms, which are output one-by-one
If S = {  }  S  , then do Iter-Dnf -4(S   {}  {})
Else if S = {  }  S  , then do Iter-Dnf -4(S   {}), then do Iter-Dnf -4(S   {})
V
Else output S  if it is consistent (and nothing otherwise)
Figure 2: Helper functions Dnf -4 and Iter-Dnf -4.

5.1 Generating Prime Implicates
We start by considering the problem of generating the set of prime implicates of a given
formula. This task is important if we want to produce abductive explanations, or if we want
to compile a formula into its set of prime implicates.
For our generation algorithm, we will require a means of transforming the input formula
into an equivalent disjunction of simpler formulae. To this end, we introduce in Figure 2
the helper function Dnf -4() which returns a set of satisfiable terms with respect to D4
whose disjunction is equivalent to . The function Dnf -4 is defined in terms of another
function Iter-Dnf -4 which takes an input a set S of formulae in NNF and returns in an
iterative fashion a set of satisfiable terms whose disjunction is equivalent to S. The following
lemmas highlight some important properties of these functions.
Lemma 12. Iter-Dnf-4 terminates and requires only polynomial space in the size of its
input.
Lemma 13. The output of Dnf-4 on input  is a set of satisfiable terms with respect to
D4 whose disjunction is equivalent to .
Lemma 14. There are at most 2|| terms in Dnf-4(). Each of the terms has length at
most 2||, depth at most (), and contains only those propositional variables appearing in
var().
We present in Figure 3 the algorithm GenPI which computes the set of prime implicates
of a given formula. Our algorithm works as follows: in Step 1, we check whether  is
unsatisfiable, outputting a contradictory clause if this is the case. For satisfiable , we
set T equal to a set of satisfiable terms whose disjunction is equivalent to . Because of
Distribution, we know that every prime implicate of  is equivalent to some disjunction
of prime implicates of the terms in T . In Step 2, we set (T ) equal to the propositional
83

fiBienvenu

Function GenPI()
Input: a formula 
Output: a set of clauses
(1) If  is unsatisfiable, return {3(a  a)}. Otherwise, set T = Dnf -4().
(2) For each T  T : let LT be the set of propositional literals in T and let DT
be the set of formulae  such that 3 is in T . If there are no literals of the
form 2 in T , then set (T ) = LT  {3 |   DT }. Otherwise, set (T ) =
LT  {2T }  {3(  T ) |   DT }
where T is the conjunction
of formulae  such that 2 is in T .
W
(3) Set Candidates = { T T T | T  (T )}.
(4) For each j  Candidates: remove j from Candidates if k |= j for some
k  Candidates with k < j, or if both j |= k and j 6|= k for k > j.
(5) Return Candidates.
Figure 3: Algorithm for prime implicate generation.

literals in T (LT ) plus the strongest 2-literal implied by T (2T ) plus the strongest 3literals implied by T ({3(  T ) |   DT }). It is not too hard to see that every prime
implicate of T must be equivalent to one of the elements in (T ). This means that in
Step 3 we are guaranteed that every prime implicate of the input formula is equivalent to
some candidate prime implicate in Candidates. During the comparison phase in Step 4,
non-prime candidates are eliminated, and exactly one prime implicate of each equivalence
class will be retained.
We illustrate the behavior of GenPI on an example:
Example 15. We run GenPI on input  = a  ((3(b  c)  3b)  (3b  3(c  d)  2e  2f )).
Step 1: As  is satisfiable, we call the function Dnf -4 on , and it returns the two terms
T1 = a  3(b  c)  3b and T2 = a  3b  3(c  d)  2e  2f .
Step 2: We have LT1 = {a}, DT1 = {b  c, b}, and there are no 2-literals in T1 , so we
get (T1 ) = {a, 3(b  c), 3b}. For T2 , we have LT2 = {a}, DT2 = {b, c  d}, and
T2 = e  f , giving us (T2 ) = {a, 2(e  f ), 3(b  e  f ), 3((c  d)  e  f )}.
Step 3: The set Candidates will contain all the different possible disjunctions of elements
in (T1 ) with elements in (T2 ), of which there are 12: aa, a2(ef ), a3(bef ),
a  3((c  d)  e  f ), 3(b  c)  a, 3(b  c)  2(e  f ), 3(b  c)  3(b  e  f ),
3(bc)3((cd)ef ), 3ba, 3b2(ef ), 3b3(bef ), and 3b3((cd)ef ).
Step 4: We will remove from Candidates the clauses a  2(e  f ), a  3(b  e  f ),
a  3((c  d)  e  f ), 3(b  c)  a, and 3b  a since they are strictly weaker than
a  a. We will also eliminate the clauses 3b  2(e  f ), 3b  3(b  e  f ), and
84

fiPrime Implicates and Prime Implicants in Modal Logic

3b  3((c  d)  e  f ) since they are weaker than the clauses 3(b  c)  2(e  f ),
3(b  c)  3(b  e  f ), 3(b  c)  3((c  d)  e  f ).
Step 5: GenPI will return the four remaining clauses in Candidates, which are a  a,
3(b  c)  2(e  f ), 3(b  c)  3(b  e  f ), and 3(b  c)  3((c  d)  e  f ).
Our algorithm can be shown to be a sound and complete procedure for generating prime
implicates.
Theorem 16. The algorithm GenPI always terminates and outputs exactly the set of
prime implicates of the input formula.
By examining the prime implicates produced by the algorithm, we can place an upper
bound on the length of a formulas prime implicates.
Theorem 17. The length of the smallest clausal representation of a prime implicate of a
formula is at most single exponential in the length of the formula.
This upper bound is optimal as we can find formulae with exponentially large prime
implicates. This situation contrasts with propositional logic, where the length of prime
implicates is linearly bounded by the number of propositional variables in the formula.
Theorem 18. The length of the smallest clausal representation of a prime implicate of a
formula can be exponential in the length of the formula.
It is interesting to note that the formula used in the proof of Theorem 18 has a depth
of 1, which means that we cannot avoid this worst-case spatial complexity by restricting
our attention to formulae of shallow depth. Nor can we escape this exponential worst-case
spatial complexity by dropping down to one of the less expressive notions of prime implicates
examined in the previous section, as the following theorem attests.
Theorem 19. If prime implicates are defined using either D1 or D2, then the length of
the smallest clausal representation of a prime implicate of a formula can be exponential in
the length of the formula.
An examination of the set of candidate prime implicates constructed by our algorithm
allows us to place a bound on the maximal number of non-equivalent prime implicates a
formula can possess.
Theorem 20. The number of non-equivalent prime implicates of a formula is at most
double exponential in the length of the formula.
This bound can also be shown to be optimal. This situation contrasts with propositional
logic, where there can be at most single exponentially many non-equivalent prime implicates
of a given formula.
Theorem 21. The number of non-equivalent prime implicates of a formula may be double
exponential in the length of the formula.
Again, this worst-case result is robust in that it can be improved neither by restricting the depth of formulae, nor by using less expressive notions of prime implicate, as the
following theorem demonstrates.
85

fiBienvenu

Theorem 22. If prime implicates are defined using either D1 or D2, then the number of
non-equivalent prime implicates of a formula may be double exponential in the length of the
formula.
Theorems 19 and 22 together suggest that definitions D1 or D2 do not yield especially interesting approximate notions of prime implicate, as they induce a significant loss
of expressivity without any improvement in the size or number of prime implicates in the
worst-case.
Our generation algorithm GenPI corresponds to the simplest possible implementation
of the distribution property, and it is quite clear that it does not represent a practicable
way for producing prime implicates. One major source of inefficiency is the large number
of clauses that are generated, so if we want to design a more efficient algorithm, we need
to find ways to generate fewer candidate clauses. There are a couple of different techniques
that could be used. One very simple method which could yield a smaller number of clauses
is to eliminate from (T ) those elements which are not prime implicates of T , thereby
decreasing the cardinalities of the (T ) and hence of Candidates. To do this, we simply
test whether T is a tautology (and remove it if it is) and then compare the 3-literals in
(T ), discarding any weaker elements. If we apply this technique to Example 15, we would
remove 3b from (T1 ), thereby reducing the cardinality of Candidates from 12 to 8.
More substantial savings could be achieved by using a technique developed in the framework of propositional logic (cf. Marquis, 2000) which consists in calculating the prime implicates of T1 , then the prime implicates of T1  T2 , then those of T1  T2  T3 , and so on until
we get the prime implicates of the full disjunction of terms. By interleaving comparison
and construction, we can eliminate early on a partial clause that cannot give rise to prime
implicates instead of producing all of the extensions of the partial clause and then deleting
them one by one during the comparison phase. In our example, there were only two terms,
but imagine that there was a third term T3 . Then by applying this technique, we would
first produce the 4 prime implicates of T1  T2 and then we would compare the 4|(T3 )|
candidate clauses of T1  T2  T3 . Compare this with the current algorithm which generates
and then compares 12|(T3 )| candidate clauses.
Given that the number of elements in Candidates can be double exponential in the
length of the input, cutting down on the size of the input to GenPI could yield significant
savings. One obvious idea would be to break conjunctions of formulae into their conjuncts,
and then calculate the prime implicates of each of the conjuncts. Unfortunately, however,
we cannot apply this method to every formula as the prime implicates of the conjuncts are
not necessarily prime implicates of the full conjunction. One solution which was proposed
in the context of approximation of description logic concepts (cf. Brandt & Turhan, 2002)
is to identify simple syntactic conditions that guarantee that we will get the same result
if we break the formula into its conjuncts. For instance, one possible condition is that the
conjuncts do not share any propositional variables. The formula  in our example satisfies
this condition since the variables in a and ((3(b  c)  3b)  (3b  3(c  d)  2e  2f )) are
disjoint. By generating the prime implicates of the conjuncts separately, we can directly
identify the prime implicate a, and we only have 6 candidate clauses of ((3(b  c)  3b) 
(3b  3(c  d)  2e  2f )) to compare. If we also remove weaker elements from the (Ti ) as
86

fiPrime Implicates and Prime Implicants in Modal Logic

suggested above, we get only 3 candidate clauses for ((3(bc)3b)(3b3(cd)2e2f )),
all of which are prime implicates of .
Another important source of inefficiency in our algorithm is the comparison phase in
which we compare all candidate clauses one-by-one in order to identify the strongest ones.
The problem with this is of course that in the worst-case there can be a double exponential
number of candidate clauses, simply because there may be double exponentially many
distinct prime implicates, and each prime implicate must be equivalent to some candidate
clause. Keeping all of these double exponentially many clauses in memory will generally
not be feasible. Fortunately, however, it is not necessary to keep all of the candidate clauses
in memory at once since we can generate them on demand from the sets (T ). Indeed, as
we demonstrate in the appendix, by implementing our algorithm in a more clever fashion,
we obtain an algorithm which outputs the prime implicates iteratively while requiring only
single-exponential space (the output of the algorithm could of course be double exponentially
large because of Theorem 21).
Theorem 23. There exists an algorithm which runs in single-exponential space in the size
of the input and incrementally outputs, without duplicates, the set of prime implicates of
the input formula.
Although our modified algorithm has a much better spatial complexity than the original,
it still does not yield a practicable means for generating prime implicates. The reason is that
we still need to compare each of the candidate clauses against all the other candidate clauses
in order to decide whether a candidate is a prime implicate or not. Given that the set of
candidate clauses may be double exponential in number, this means that our algorithm may
need to perform double exponentially many entailment tests before producing even a single
prime implicate. A much more promising approach would be to test directly whether or not
a candidate clause is a prime implicate without considering all of the other candidate clauses.
In order to implement such an approach, we must of course come up with a procedure for
determining whether or not a given clause is a prime implicate. This will be our objective
in the following section.
5.2 Recognizing Prime Implicates
The focus of this section is the problem of recognizing prime implicates, that is, the problem
of deciding whether a clause  is a prime implicate of a formula . As has been discussed
in the previous subsection, this problem is of central importance, as any algorithm for
generating prime implicates must contain (implicitly or explicitly) some mechanism for
ensuring that the generated clauses are indeed prime implicates.
In propositional logic, prime implicate recognition is BH2 -complete (Marquis, 2000),
being as hard as both satisfiability and deduction. In K, satisfiability and unsatisfiability are
both Pspace-complete, so we cannot hope to find a prime implicate recognition algorithm
with a complexity of less than Pspace.
Theorem 24. Prime implicate recognition is Pspace-hard.
In order to obtain a first upper bound, we can exploit Theorem 17 which tells us that
there exists a polynomial function f such that every prime implicate of a formula  is
87

fiBienvenu

equivalent to some clauses of length at most 2f (||) . This leads to a simple procedure for
determining if a clause  is a prime implicate of a formula . We simply check for every
clause  of length at most 2f (||) whether  is an implicate of  which implies  but is not
implied by . If this is the case, then  is not a prime implicate (we have found a logically
stronger implicate of ), otherwise, there exists no stronger implicate, so  is a prime
implicate. It is not too hard to see that this algorithm can be carried out in exponential
space, which gives us an Expspace upper bound.
Of course, the problem with this naive approach is that it does not at all take into
account the structure of , so we end up comparing a huge amount of irrelevant clauses,
which is exactly what we were hoping to avoid. The algorithm that we propose later in this
section avoids this problem by exploiting the information in the input formula and clause
in order to cut down on the number of clauses to test. The key to our algorithm is the
following theorem which shows how the general problem of prime implicate recognition can
be reduced to the more specialized tasks of prime implicate recognition for propositional
formulae, 2-formulae, and 3-formulae. To simplify the presentation of the theorem, we
let () refer to the set of prime implicates of , and we use the notation  \ {l1 , ..., ln }
to refer to the clause obtained by removing each of the literals li from . For example
(a  b  3c) \ {a, 3c} refers to the clause b.
Theorem 25. Let  be a formula of K, and let  = 1  ... k  31  ... 3n  21  ...
2m (j propositional literals) be a non-tautologous clause such that (a) i  i 1 ...n
for all i, and (b) there is no literal l in  such that    \ {l}. Then   () if and only
if the following conditions hold:
1. 1  ...  k  (  ( \ {1 , ..., k }))
2. 2(i  1  ...  n )  (  ( \ {2i })) for every i
3. 3(1  ...  n )  (  ( \ {31 , ..., 3n }))
We remark that the restriction to clauses for which i  i  1  ...  m for all i
and for which  6  \ {l} for all l is required. If we drop the first requirement, then there
are some non-prime implicates that satisfy all three conditions, and if we drop the second,
there are prime implicates which fail to satisfy one of the conditions5 . These restrictions are
without loss of generality however since every clause can be transformed into an equivalent
clause satisfying them. For the first condition, we replace each 2i by 2(i  1  ...  m ),
thereby transforming a clause 1  ... k  31  ...3m  21  ... 2n into the equivalent
1  ...  k  31  ...3m  2(1  1  ...  m )  ...  2(n  1  ...  m ). Then to make
the clause satisfy the second condition, we simply remove from  those literals for which
   \ {l} until no such literal remains.
Theorem 25 shows how prime implicate recognition can be split into three more specialized sub-tasks, but it does not tell us how to carry out these tasks. Thus, in order to turn
5. For the first restriction, consider the formula  = 3(abc)2a and the clause  = 3(ab)2(ab).
It can be easily shown that  is an implicate of , but  is not a prime implicate of  since there exist
stronger implicates (e.g.  itself). Nonetheless, it can be verified that both 2(a  b  (a  b)) 
(  ( \ {2(a  b)})) and 3(a  b)  (  ( \ {3(a  b)})). For the second restriction, consider the
formula 2a and the clause 2a2(ab). We have 2(ab) 6 (2a(2a)) even though 2a2(ab)  2a
is a prime implicate of 2a.

88

fiPrime Implicates and Prime Implicants in Modal Logic

this theorem into an algorithm for prime implicate recognition, we need to figure out how
to test whether a propositional clause, a 2-formula, or a 3-formula is a prime implicate of
a formula.
Determining whether a propositional clause is a prime implicate of a formula in K is
conceptually no more difficult than determining whether a propositional clause is a prime
implicate of a propositional formula. We first ensure that the clause is an implicate of the
formula and then make sure that all literals appearing in the clause are necessary.
Theorem 26. Let  be a formula of K, and let  be a non-tautologous propositional clause
such that  |=  and such that there is no literal l in  such that    \ {l}. Then   ()
if and only if  6|=  \ {l} for all l in .
We now move on to the problem of deciding whether a clause of the form 2 is a prime
implicate of a formula . We remark that if 2 is implied by , then it must also be implied
by each of the terms Ti Dnf -4(). But if Ti |= 2, then by Theorem 1, it must be the
case that the conjunction of the 2-literals in Ti implies 2. This means that the formula
21  ...  2n (where i is the conjunction of the formulae  such that 2 is in Ti ) is an
implicate of  which implies 2, and moreover it is the strongest such implicate. It follows
then that 2 is a prime implicate of  just in the case that 2 |= 21  ...  2n , which
is true if and only if  |= i for some i (by Theorem 1). Thus, by comparing the formula 
with the formulae i associated with the terms of , we can decide whether or not 2 is a
prime implicate of .
Theorem 27. Let  be a formula of K, and let  = 2 be a non-tautologous clause such
that  |= . Then   () if and only if there exists some term T Dnf-4() such that
 |= T , where T is the conjunction of formulae  such that 2 is in T .
Finally let us turn to the problem of deciding whether a clause 3 is a prime implicate
of a formula . Now we know by Covering that if 3 is an implicate of , then there must
be some prime implicate  of  which implies 3. It follows from Theorem 2 that  must
be
W a disjunction of 3-literals, and from Theorem 16 that  is equivalent to a disjunction
T Dnf -4() 3dT where 3dT is an element of (T ) for every T (refer back to Figure 3 for
the definition of (TW
)). According to Definition 7, 3 is aWprime implicate of  just in
the case that 3 |= T Dnf -4() 3dT , or equivalently  |= T Dnf -4() dT . Thus, 3 is
not a prime implicate of W
 just in the case that there isWa choice of 3dT  (T ) for each
6|= T Dnf -4() dT .
T  Dnf -4() such that T Dnf -4() dT |=  and W
Testing directly whether  entails some formula T Dnf -4() dT could take exponential
space in the worst case since there may be exponentially many terms in Dnf -4(). Luckily,
W however, we can get around this problem by exploiting the structure of the formula
T Dnf -4() dT . We remark that because of the way (T ) is defined the formula dT must
be a conjunction of formulae  such that 2 or 3 appears in Nnf () outside the scope of
modal operators  we will use X W
to denote the set of formulae  satisfying this condition. We
show in the appendix
that  6|= T Dnf -4() dT implies the existence of a subset S  X such
W
that (a)  6|= S  and (b) every dT has at least W
one conjunct from the set S. Conversely,
the existence of such a subset of X implies  6|= T Dnf -4() dT . This observation is the
basis for the algorithm Test3PI given in Figure 4. The basic idea behind the algorithm is
to try out each of the different subsets of X in order to see whether some subset satisfies the
89

fiBienvenu

Function Test3PI(3, )
Input: a clause 3 and a formula  such that  |= 3
Output: yes or no
(1) If  |= , return yes if  |=  and no otherwise.
(2) Set X equal to the set of formulae  such that 2 or 3 appears in Nnf () outside
the scope of modal operators.
(3) For each W
S  X , test whether the following two conditions hold:
(a)  6|= S 
(b) for each Ti  Dnf -4(), there exists conjuncts 3i , 2i,1 , ..., 2i,k(i) of Ti
such that:
(i) {i , i,1 , ..., i,k(i) }  S 6= 
(ii) 3(i  i,1  ...  i,k(i) ) |= 3
Return no if some S satisfies these conditions, and yes otherwise.
Figure 4: Algorithm for identifying prime implicates of the form 3.

aforementioned conditions. If we find a suitable subset, this proves that 3 is not a prime
implicate, and if no such subset exists, then we can be sure there is no stronger implicate
than 3. The algorithm can be shown to run in polynomial space since there can be at
most || elements in X , and we can consider the terms in Dnf -4() one at a time.
Theorem 28. Let  be a formula, and let 3 be an implicate of . Then the algorithm
Test3PI returns yes on input (3, ) if and only if 3 is a prime implicate of .
Theorem 29. The algorithm Test3PI runs in polynomial space.
We now illustrate the algorithm Test3PI with two examples.
Example 30. We use Test3PI to test whether the clause  = 3(ab) is a prime implicate
of  = a  (2(b  c)  2(e  f ))  3(a  b).
Step 1: As  is satisfiable, we pass directly to Step 2.
Step 2: We set X equal to the set of formulae  such that 2 or 3 appears in Nnf ()
outside the scope of modal operators. In our case, we set X = {b  c, e  f, a  b}
since  =Nnf () and b  c, e  f , and a  b are the only formulae satisfying the
requirements.
Step 3: We examine each of the different subsets of X to determine whether they satisfy
conditions (a) and (b). In particular, we consider the subset S = {b  c, e  f }. We
remark that this subset satisfies condition (a) since a  b 6|= (b  c)  (e  f ). In order
to check condition (b), we first call the function Dnf -4 on  which returns the two
terms T1 = a  2(b  c)  3(a  b) and T2 = a  2(e  f )  3(a  b). We notice that
the conjuncts 3(a  b) and 2(b  c) of T1 satisfy conditions (i) and (ii) since b  c  S
and 3(a  b  (b  c)) |= . We then notice that the conjuncts 3(a  b) and 2(e  f ) of
T2 also satisfy conditions (i) and (ii) since e  f  S and 3(a  b  (e  f )) |= . That
90

fiPrime Implicates and Prime Implicants in Modal Logic

Function TestPI(, )
Input: a clause  and a formula 
Output: yes or no
(1) If  6|= , return no.
(2) If  |= , then return yes if  |=  and no if not.
If |= , then return yes if |=  and no otherwise.
(3) For each li in  = l1  ...  ln , test if  \ {li }  , and if so, remove li from .
Let D = {31 , ..., 3m } be the set of 3-literals in . If D is non-empty, replace
each disjunct 2 of  by the literal 2(  1  ...  m ).
(4) Let P be the set of propositional literals which are disjuncts of . For each
l  P, check whether  |=  \ {l}, and return no if so.
(5) Let B be the set of 2-formulae appearing as disjuncts in . Check for each 2
in B whether there is some T in Dnf -4(  ( \ {2})) for which the formula
2(  1  ...  k ) implies the conjunction of 2-literals in T , and return no
if not.
W
(6) If D is empty, return yes, otherwise return Test3PI(3( m
i=1 i ),   ( \ D)).
Figure 5: Algorithm for identifying prime implicates.

means that we have found a subset S of X which satisfies conditions (a) and (b), so
the algorithm returns no. This is the correct output since 3(a  b  ((b  c)  (e  f )))
is an implicate of  which is strictly stronger than .
Example 31. We use Test3PI to test whether the clause  = 3(a  b  c) is a prime
implicate of  = a  (2(b  c)  2(e  f ))  3(a  b)  2(e  f  (a  b  c)).
Step 1: We proceed directly to Step 2 since  is satisfiable.
Step 2: We set X = {b  c, e  f, a  b, e  f  (a  b  c))} since Nnf ()=a  (2(b 
c)  2(e  f ))  3(a  b)  3(e  f  (a  b  c)).
Step 3: We check whether there is some subset of X satisfying conditions (a) and (b). We
claim that there is no such subset. To see why, notice that a  2(b  c)  3(a  b) 
3(e  f  (a  b  c)) is the only term in Dnf -4(). Moreover, there is only one
set of conjuncts of this term which implies 3(a  b  c), namely {3(a  b), 2(b  c)}.
But that means that S must contain either a  b or b  c in order to satisfy condition
(b)(i). As a  b  c implies both a  b and b  c, we are guaranteed that a  b  c
will imply the disjunction of elements in S, thereby falsifying condition (a). It follows
that there is no subset of X satisfying the necessary conditions, so Test3PI returns
yes, which is the desired result.
In Figure 5, we present our algorithm for testing whether a clause  is a prime implicate
of a formula . The first two steps of the algorithm treat the limit cases where  is not
an implicate or where one or both of  and  is a tautology or contradiction. In Step 3,
91

fiBienvenu

we apply equivalence-preserving transformations to  to make it satisfy the requirements
of Theorem 25. Then in Steps 4, 5, and 6 we use the procedures from Theorems 26, 27,
and 28 to test whether the three conditions in Theorem 25 are verified. If the three tests
succeed, then by Theorem 25, the clause is a prime implicate, so we return yes. If some
test fails, we return no as the clause has been shown not to be a prime implicate.
Theorem 32. The algorithm TestPI always terminates, and it returns yes on input (,
) if and only if  is a prime implicate of .
We demonstrate the use of TestPI on an example.
Example 33. We use TestPI to test if the clauses 1 = b, 2 = 2b  2(e  f ), 3 = a  3c,
4 = 3(a  b), and 5 = 3(a  b  c)  3(a  b  c  f )  2(e  f ) are prime implicates of
 = a  (2(b  c)  2(e  f ))  3(a  b).
1 : We output no in Step 1 since  6|= 1 .
2 : We skip Steps 1 and 2 since  |= 2 and neither  |=  nor |= 2 . In Step 3, we
make no changes to 2 since it contains no redundant literals nor any 3-literals. We
skip Step 4 since 2 has no propositional disjuncts. In Step 5, we return no since
Dnf -4(  (2 \ {2b})) = {a  2(b  c)  3(a  b)  3(e  f )} and 2b 6|= 2(b  c).
3 : We proceed directly to Step 3 since  |= 3 ,  6|= , and 6|= 3 . No modifications
are made to 3 in Step 3 as it does not contain any redundant literals or 2-literals.
In Step 4, we test whether or not  |= 3 \ {a}. As  6|= 3c, we proceed on to Step
5, and then directly on to Step 6 since 3 contains no 2-literals. In Step 6, we call
Test3PI(3c,   (3 \ {3c})), which outputs no since   (3 \ {3c}) |=  and
c 6|= .
4 : Steps 1-5 are all inapplicable, so we skip directly to Step 6. In this step, we call
Test3PI with as input the clause 3(a b) and the formula  (4 \{3(a b)}) = .
We have already seen in Example 30 above that Test3PI returns no on this input,
which means that TestPI also returns no.
5 : We proceed directly to Step 3, where we delete the redundant literal 3(a  b  c  f )
and then modify the literal 2(ef ). At the end of this step, we have 5 = 3(abc)
2((ef )(abc)). Step 4 is not applicable since there are no propositional disjuncts
in 5 . In Step 5, we continue since Dnf -4(  (5 \ {2((e  f  (a  b  c))})) =
{a2(ef )3(ab)2(abc)}, and 2(((ef (abc))(abc)) |=
2(e  f )  2(a  b  c). In Step 6, we return yes since we call Test3PI on input
(3(a  b  c),   (5 \ {3(a  b  c)})), and we have previously shown in Example
31 that Test3PI returns yes on this input.
We show in the appendix that the algorithm TestPI runs in polynomial space. As we
have already shown that TestPI decides prime implicate recognition, it follows that this
problem is in Pspace:
Theorem 34. Prime implicate recognition is in Pspace.
92

fiPrime Implicates and Prime Implicants in Modal Logic

By putting together Theorems 24 and 34, we obtain a tight complexity bound for the
prime implicate recognition task.
Corollary 35. Prime implicate recognition is Pspace-complete.

6. Conclusion and Future Work
The first contribution of this work is a detailed comparison of several different possible
definitions of clauses, terms, prime implicates, and prime implicants for the modal logic K.
The results of this investigation were largely positive: although we have shown that no
perfect definition exists, we did exhibit a very simple definition (D4) which satisfies most of
the desirable properties of the propositional case. The second contribution of our work is a
thorough investigation of the computational aspects of the selected definition D4. To this
end, we presented a sound and complete algorithm for generating prime implicates, as well
as a number of optimizations to improve the efficiency of the algorithm. An examination of
the structure of the prime implicates generated by our algorithm allowed us to place upper
bounds on the length of prime implicates and on the number of prime implicates a formula
can possess. We showed these bounds to be optimal by exhibiting matching lower bounds,
and we further proved that the lower bounds hold even for some much less expressive notions of prime implicates. Finally, we constructed a polynomial-space algorithm for deciding
prime implicate recognition, thereby showing this problem to be Pspace-complete, which is
the lowest complexity that could reasonably be expected. Although the focus of the paper
was on the logic K, all of our results can be easily lifted to multi-modal K and to concept
expressions in the well-known description logic ALC.
As was mentioned in the introduction, one of the main applications of prime implicants
in propositional logic is to the area of abductive reasoning, where prime implicants play
the role of abductive explanations. The results of our paper can be directly applied to the
problem of abduction in K: our notion of prime implicants can be used as a definition of
abductive explanations in K, and our prime implicate generation algorithm provides a means
of producing all of the abductive explanations to a given abduction problem. Moreover,
because the notion of term underlying our definition of abductive explanations is more
expressive than that used by Cialdea Mayer and Pirri (1995), we are able to find explanations
which are overlooked by their method. For instance, if we look for an explanation of the
observation c given the background information 2(a  b)  c, we obtain 2(a  b), whereas
their framework yields 2a and 2b. This is an argument in favor of our approach since
generally in abduction one is looking to find the weakest conditions guaranteeing the truth
of the observation given the background information.
Also of interest are our results on the size and number of prime implicates, as these
yield corresponding lower bounds on the size and number of abductive explanations. In
particular, our results imply that the abductive explanations of Cialdea Mayer and Pirri
(1995) can have exponential size and be double exponentially many in number in the worst
case, and thus behave no better in these respects than the notion of abductive explanation
induced by our preferred definition D4. Moreover, the fact that these lower bounds hold
even in the case of the extremely inexpressive notion of abductive explanations induced
by definition D2 suggests that these high worst-case complexity results really cannot be
93

fiBienvenu

avoided. In light of these intractability results, an interesting question for future research
would be to study the problem of generating a single prime implicate, since in some applications it may prove sufficient to produce a single minimal explanation for an observation.
Another interesting subject for future work which is relevant from the point of view of abduction is the investigation of the notion of prime implicate over a fixed vocabulary. The
development of generation algorithms for this more refined notion of prime implicate would
allow one to generate only those abductive explanations which are built up from a given set
of propositional variables.
The second domain of application which was mentioned in the introduction was the
area of knowledge compilation. In propositional logic, one well-known target language for
knowledge compilation is prime implicate normal form, in which a formula is represented
as the conjunction of its prime implicates. A natural idea would be to use our selected
definition of prime implicate to define in an analogous manner a notion of prime implicate
normal form for K formulae. Unfortunately, the normal form we obtain satisfies few of the
nice properties of the propositional case. For instance, we find that entailment between two
formulae in prime implicate normal form is no easier than between arbitrary K formulae.
To see why, consider any pair of formulae  and  in negation normal form. The formulae
3 and 3 are their own prime implicates and hence are in prime implicate normal form
according to the naive definition. As  |=  just in the case that 3 |= 3, we can reduce
entailment between arbitrary K formulae in NNF to entailment between formulae in prime
implicate normal form. As the former problem is known to be Pspace-complete, it follows
that the latter is Pspace-complete as well.
At first sight, this appears to be quite a disappointing result as one would hope that the
computational difficulty of representing a formula by its prime implicates would be offset
by some good computational properties of the resulting formula. As it turns out, however,
the problem lies not in our definition of prime implicates but rather in the naive way of
defining prime implicate normal form. Indeed, in a continuation of the present work (Bienvenu, 2008), we proposed a more sophisticated definition of prime implicate normal form,
in which we specify which of the many different clausal representations of a prime implicate
should be used. This normal form was shown to enjoy a number of desirable properties
which make it interesting from the viewpoint of knowledge compilation. Most notably, it
was proven that entailment between formulae in K in our prime implicate normal form can
be carried out in polynomial time using a simple structural comparison algorithm which is
reminiscent of the structural subsumption algorithms used in subpropositional description
logics. It should be noted that the proof of this and other results by Bienvenu (2008) make
ample use of the material presented in the current paper.
In this work, we studied prime implicates with respect to the local consequence relation,
so a natural direction for future work would be the investigation of prime implicates with respect to the global consequence relation. This question is particularly interesting given that
global consequence is the type of consequence used in description logic ontologies. Unfortunately, our preliminary investigations suggest that defining and generating prime implicates
with respect to the global consequence relation will likely prove more difficult than for the
local consequence relation. For one thing, if we use a definition of clause which is reasonably
94

fiPrime Implicates and Prime Implicants in Modal Logic

expressive, then the notion of prime implicate we obtain does not satisfy Covering since
we can construct infinite sequences of stronger and stronger implicates. Take for instance
the formula (a  b)  (b  3b) which implies (using the global consequence relation)
each of the increasingly stronger clauses in the infinite sequence a  3b, a  3(b  3b),
a  3(b  3(b  3b)), ... This is a familiar situation for description logic practitioners since
these infinite sequences are responsible for the non-existence of most specific concepts in
many common DLs (cf. Kusters & Molitor, 2002) and the lack of uniform interpolation for
ALC TBoxes (Ghilardi, Lutz, & Wolter, 2006). A standard solution to this problem is to
simply place a bound on the depth of formulae to be considered, effectively blocking these
problematic infinite sequences. This will not allow us to regain Covering, but it will give
us a weaker version of this property, which should be sufficient for most applications. The
development of generation algorithms for the global consequence relation may also prove
challenging, since it is unclear at this point whether we will be able to draw inspiration from
pre-existing methods. Despite these potential difficulties, we feel that this subject is worth
exploring since it could contribute to the development of more flexible ways of accessing
and structuring information in description logic ontologies.
Finally, another natural direction for future research would be to extend our investigation of prime implicates and prime implicants to other popular modal and description logics.
Particularly of interest are modal logics of knowledge and belief and expressive description
logics used for the semantic web. We are confident that the experience gained from our
investigation of prime implicates and prime implicants in K will prove a valuable asset in
the exploration of other modal and description logics.

Acknowledgments
This paper corrects and significantly extends an earlier conference publication (Bienvenu,
2007). This paper was written while the author was a PhD student working at IRIT,
Universite Paul Sabatier, France. The author would like to thank her thesis supervisors
Andreas Herzig, Jerome Lang, and Jerome Mengin, as well as the anonymous reviewers for
very helpful feedback.

Appendix A. Proofs
Theorem 1 Let , 1 , ..., m , , 1 , ..., n be formulae in K, and let  be a propositional
formula. Then
1.  |=   |=        |= 
2.  |=   3 |= 3  2 |= 2
3.  31 ...3m 21 ...2n |=   ( |=  or i 1 ...n |=  for some i)
4. |=   31  ...  3m  21  ...  2n  (|=  or |= 1  ...  m  i for some i)
5. 2 |= 21  ...  2n   |= i for some i
95

fiBienvenu

6. 31  ...  3m  21  ...  2n
 31  ...  3m  2(1  1  ...  m )  ...  2(n  1  ...  m )
Proof. The first statement is a well-known property of local consequence, but we prove it
here for completeness:
 |=  






M, w |=  implies M, w |=  for all M, w
M, w 6|=  or M, w |=  for all M, w
M, w |=  or M, w |=  for all M, w
|=   
M, w 6|=    for all M, w
   |= 

For the second statement, if  6|= , then there is some M, w such that M, w |=   .
Create a new model M from M by adding a new world w and placing a single arc from
w to w. Then M , w |= 3  2, which means that 3  2 is satisfiable and hence
3 6|= 3 (since 2  3). For the other direction, suppose 3 6|= 3. Then there
exists M, w such that M, w |= 3  3  3  2. But this means that there is some
w for which   , hence  6|= . To complete the proof, we use the following chain of
equivalences: 2 |= 2  2 |= 2  3 |= 3   |=    |= .
For 3, suppose that   31  ...  3m  21  ...  2n 6|= . Then there exist M, w
such that M, w |=   31  ...3m  21  ...  2n . As M, w |= , we cannot have
 |= , nor can we have i  1  ...  n |=  since for each i there is some w such
that M, w |= i  1  ...  n . Now for the other direction suppose that  and all of the
i  1  ...  n are satisfiable. Then there is some propositional model w of , and for
each i, we can find Mi , wi such that Mi , wi |= i  1  ...  n . Now we construct a new
Kripke structure which contains the models Mi and the world w and in which there are arcs
going from w to each of the wi . It is not hard to see that in this new model Mnew we have
Mnew , w |=   31  ...3m  21  ...2n , so   31  ...3m  21  ...  2n 6|= .
Statement 4 follows easily from the third statement. We simply notice that   31 
...  3m  21  ...  2n is a tautology just in the case that its negation   31 
...  3n  21  ...  2m is unsatisfiable.
For 5, we use statements 1 and 4 to get the following chain of equivalences:
2 |= 21  ...  2n
 |= 3  21  ...  2n
 |=   i for some i
  |= i for some i
The first implication of the equivalence in 6 is immediate since 31  ...  3m |=
31  ...  3m and 2i |= 2(i  1  ...  m ) for all i. For the other direction, we remark
that by using statements 1 and 3, we get the following equivalences:
2(i  1  ...  m ) |= 2i  31  ...  3m
 2(i  1  ...  m )  (2i  31  ...  3m ) |= 
 2(i  1  ...  m )  3i  21  ...  2m |= 
 (i  1  ...  m )  i  1  ...  m |= 
96

fiPrime Implicates and Prime Implicants in Modal Logic

As (i  1  ...  m )  i  1  ...  m is clearly unsatisfiable, it follows that
2(i  1  ...  m ) |= 2i  31  ...  3m for every i and hence that 31  ...  3m 
2(1  1  ...  m )  ...  2(n  1  ...  m ) |= 31  ...  3m  21  ...  2n ,
completing the proof.
Theorem 2 Let  be a disjunction of propositional literals and 2- and 3-formulae. Then
each of the following statements holds:
1. If  |=  for some non-tautological propositional clause , then every disjunct of  is
either a propositional literal or an unsatisfiable 3-formula
2. If  |= 31  ...  3n , then every disjunct of  is a 3-formula
3. If  |= 21  ...  2n and 6|= 21  ...  2n , then every disjunct of  is either a
2-formula or an unsatisfiable 3-formula
Proof. For (1), let  be a non-tautologous propositional clause such that  |= , and suppose
for a contradiction that  contains a disjunct 2 or a disjunct 3 where  6|= . In the
first case, we have 2 |= , and hence |= 3  . It follows from Theorem 1 that
|= , contradicting our assumption that  is not a tautology. In the second case, we have
3 |= , and thus |= 2  . By Theorem 1, either |=  or |= . In both cases, we
reach a contradiction since we have assumed that  6|=  and 6|= . It follows then that 
cannot have any 2-formulae or satisfiable 3-formulae as disjuncts.
The proofs of (2) and (3) proceed similarly.
Theorem 3 Let  =   31  ...  3m  21  ...  2n and  =    31  ...  3p 
21  ...  2q be formulae in K. If  are   are both propositional and 6|=  , then

  |=   and

1  ...  m |= 1  ...  p and
 |=  

for every i there is some j such that i |= 1  ...  p  j
Proof. Since we have 6|=  , we know that 6|=   and 6|= 1  ...  p  i for all i. Using this
information together with Theorem 1, we get the following equivalences:
 |=  


31  ...  3m |=  



2i |=  



|=      31  ...  3p  21  ...  2m
|=    
 |=  
3(1  ...  m ) |= 
|=    31  ...  3p  2(1  ...  m )  21  ...  2q
|= 1  ...  p  (1  ...  m )
1  ...  m |= 1  ...  p
|=    3(1  ...  p  i )  21  ...  2q
there is some j such that |= 1  ...  p  i  j
there is some j such that i |= 1  ...  p  j

To complete the proof, we use the fact  |=  if and only if  |=  , 31  ...  3m |=  ,
and 2i |=  for every i.
97

fiBienvenu

Theorem 5 Any definition of literals, clause, and terms for K that satisfies properties P1
and P2 cannot satisfy P5.
Proof. We remark that the set of clauses (resp. terms) with respect to definition D1 is
precisely the set of formulae in NNF which do not contain  (resp. ), i.e. D1 is the most
expressive definition satisfying both P1 and P2. Thus, to show the result, it suffices to
show that D1 does not satisfy P5.
Suppose for a contradiction that D1 does satisfy P5. Then there must exist clauses 1 , ..., n
such that 3(a  b)  1  ...  n . Each of the clauses i is a disjunction li,1  ....  li,pi . By
distributing  over , we obtain the following:
n
^

_

3(a  b) 

li,ji

(j1 ,...,jn){1,...,p1 }...{1,...,pn} i=1

from which we can infer that for each (j1 , ..., jn )  {1, ..., p1 }  ...  {1, ..., pn } we have
n
^

li,ji |= 3(a  b)

i=1

Vn

Consider some (j1 , ..., jn ) such that i=1 li,ji is consistent (there must be at least one such
tuple, otherwise we would have 3(a  b)  ). The formulae li,ji are eitherVpropositional
literals or formulae of the form 2 or 3 for some clause . It follows that ni=1 li,ji must
have the following form:
1  ...  k  31  ...  3m  21  ...  2n
where 1 , ..., k are propositional
literals and 1 , ...,
V m , 1 , ..., n are clauses with respect
V
to D1. As we know that ni=1 li,ji |= 3(a  b) and ni=1 li,ji 6|= , by Theorem 1, there must
be some 3q such that
3q  21  ...  2n |= 3(a  b)
We now show that 3q 6|= 3(a  b) (and hence that 6|= 1  ...  n ). Suppose for a
contradiction that this is not the case. Then we must have q |= a and q |= b. But by
Theorem 1, every disjunct of q (which we recall is a D1-clause) must either be unsatisfiable
or equal to both a and b. As the latter
V is impossible, it follows that q |= , which is a
contradiction since we assumed that ni=1 li,ji is satisfiable. It follows then that in order to
get 3q  21  ...  2n |= 3(a  b), there must be some r which is not a tautology.
Now let us consider the formula
_
=
2j1 ,...,jn
{(j1 ,...,jn)|

Vn

i=1 li,ji 6}

V
where 2j1 ,...,jn is a non-tautological 2-formula appearing in ni=1 li,ji (we have just shown
that such a formula must exist). Clearly it must be the case that
n
^

_

(j1 ,...,jn){1,...,p1 }...{1,...,pn} i=1

98

li,ji |= 

fiPrime Implicates and Prime Implicants in Modal Logic

from which we get:
3(a  b) |= 
But according to Theorem 2, a satisfiable 3-formula cannot imply a disjunction of 2formulae unless that disjunction is a tautology, so we must have |=  . However, this is
impossible since it would imply (Theorem 1) that there is some j1 ,...,jn which is a tautology,
contradicting our earlier assumption to the contrary. We can thus conclude that there is
no set of clauses 1 , ..., n with respect to D1 such that 3(a  b)  1  ...  n , and hence
that any definition which satisfies P1 and P2 cannot satisfy P5.
In order to prove Theorem 6, we will make use of the following lemmas:
Lemma 6.1 Definition D5 satisfies P5.
Proof. We demonstrate that any formula in K in NNF is equivalent to a conjunction of
clauses with respect to definition D5. The restriction to formulae in NNF is without loss
of generality as every formula is equivalent to a formula in NNF. The proof proceeds by
induction on the structural complexity of formulae. The base case is propositional literals,
which are already conjunctions of clauses since every propositional literal is a clause with
respect to D5. We now suppose that the statement holds for formulae 1 and 2 and show
that it holds for more complex formulae.
We first consider  = 1  2 . By assumption, we can find clauses i and j such
that 1  1  ...  n and 2  1  ...  m . Thus,  is equivalent to the formula
1  ...  n  1  ...  m , which is a conjunction of clauses with respect to D5.
Next we consider  = 1  2 . By the induction hypothesis, we have 1  1  ...  n
and 2  1  ...  m for some clauses i and j . Thus,   (1  ...  n )  (1  ...  m ),
which can be written equivalently as   (i,j){1,...,n}{1,...,m}(i  j ). Since the union of
two clauses produces another clause, all of the i  j are clauses, completing the proof.
We now consider the case where  = 21 . By assumption, 1  1  ...  n , where each
i is a clause. So   2(1 ...n ). But we also know that 2(1 ...n )  21 ...2n .
It follows that  is equivalent to 21  ...  2n , which is a conjunction of clauses since the
2i are all clauses.
Finally, we consider  = 31 . Using the induction hypothesis, we have   3(1 
...  n ) for clauses i . But since the i are clauses, each i is a disjunction of literals
li,1  ...  li,pi . After distributing  over  and  over 3, we find that  is equivalent to the
formula
_
3(l1,j1  l2,j2  ...  ln,jn )
(j1 ,...,jn){1,...,p1 }...{1,...,pn}

which is a clause with respect to D5.
The proof that every formula is equivalent to a disjunction of terms with respect to D5
proceeds analogously.
Lemma 6.2 Every clause (resp. term) with respect to D5 is a clause (resp. term) with
respect to definitions D3a, D3b, and D4.
Proof. We will show by induction on the structural complexity of formulae that:
99

fiBienvenu

1. every clause C with respect to D5 is a clause with respect to definitions D3a, D3b,
and D4 and a disjunction of terms with respect to D3a
2. every term T with respect to D5 is a term with respect to definitions D3a, D3b, and
D4 and a conjunction of clauses with respect to D3a and D3b
We require this stronger formulation of the statement to prove some of the sub-cases.
The base case for our induction is propositional literals, which are both clauses and
terms with respect to D5. It is easy to see that (1) and (2) are verified since propositional
literals are both clauses and terms with respect to definitions D3a, D3b, and D4 (and
hence they are also disjunctions of terms with respect to D3a and conjunctions of clauses
with respect to D3a and D3b).
For the induction step, we will show that the above statements hold for arbitrary clauses
or terms with respect to D5 under the assumption that the statments hold for all of their
proper sub-clauses and sub-terms.
We begin with clauses. Let C be a D5-clause such that all proper sub-clauses and subterms of C satisfy (1) and (2). Now since C is a clause with respect to D5, it can either
be a propositional literal or a formula of the form C1  C2 for clauses C1 and C2 , 2C1 for
some clause C1 , or 3T1 for some term T1 . The case where C is a propositional literal has
already been treated in the base case. Let us thus consider the case where C = C1  C2 .
The first part of (1) holds since by the induction hypothesis both C1 and C2 are clauses
with respect to definitions D3a, D3b, and D4, and for all three definitions the disjunction
of two clauses is a clause. The second half of (1) is also verified since both C1 and C2 are
disjunctions of terms with respect to D3a, and thus so is their disjunction C1  C2 . We
next consider the case where C = 2C1 for some clause C1 with respect to D5. The first
part of (1) follows easily as we know that C1 must also be a clause with respect to D3a,
D3b, and D4, and for all of these definitions putting a 2 before a clause yields another
clause. The second part of (1) holds as well since C1 is a disjunction of terms with respect
to D3a and thus 2C1 is a term with respect to this same definition. We now suppose that
C = 3T1 for some term T1 with respect to D5. For definitions D3a and D3b, we know
from the induction hypothesis that T1 is a conjunction of clauses with respect to D3a and
D3b and hence that 3T1 is a clause with respect to these definitions. For D4, the result
obviously holds since we are allowed to put any formula in NNF behind 3. The second part
of (1) holds since by the induction hypothesis T1 is a term with respect to D3a and hence
3T1 is also a term with respect to this definition.
We next consider terms. Let T be a D5-term such that all proper sub-clauses and subterms of T satisfy (1) and (2). Then T must be either a propositional literal or a formula of
the form T1  T2 for terms T1 and T2 , 2C1 for some clause C1 , or 3T1 for some term T1 . If
T = T1  T2 , the first half of (2) holds since we know T1 and T2 to be terms with respect to
D3a, D3b, and D4, and conjunctions of terms are also terms for all three definitions. The
second half is also verified since both T1 and T2 are assumed to be conjunctions of clauses
with respect to D3a and D3b, which means that T is also a conjunction of clauses with
respect to these definitions. Next suppose that T = 2C1 . For definitions D3b and D4, it is
easy to see that T is a literal and hence a term. For D3a, the induction hypothesis tells us
that C1 is a disjunction of terms, from which we can deduce that 2C1 is a term. Moreover,
since C1 is known to be a clause with respect to D3a and D3b, then 2C1 must also be a
100

fiPrime Implicates and Prime Implicants in Modal Logic

clause with respect to these definitions, so T is a conjunction of clauses with respect to both
D3a and D3b. Finally, we treat the case where T = 3T1 . For D3a, we use the fact that
T1 is a term with respect to D3a, which means that 3T1 must also be a term. For D3b,
we use the supposition that T1 is a conjunction of clauses with respect to D3b, from which
we get that 3T1 is a literal and hence a term. The first part of (2) clearly also holds for D4
since any formula behind 3 yields a literal and thus a term. The second half of (2) follows
from the fact that by the induction hypothesis T1 is a conjunction of clauses with respect
to D3a and D3b, so 3T1 is a clause (and hence a conjunction of clauses) with respect to
these definitions.

U ,S = 1,1  ...  1,m  
where the i,j are defined inductively as follows

3i+1,j , if either i  n, ui  Sj , or i > n and uin  Sj
i,j =
2i+1,j , if either i  n, ui 6 Sj , or i > n and uin 6 Sj
for i  {1, ..., 2n} and 2n+1,j = , and  = 2...2
| {z } .
2n

Figure 6: The formula U ,S which codes an instance U = {u1 , ..., un }, S = {S1 , ..., Sm } of
the exact cover problem.

Lemma 6.3 Entailment between terms or clauses is NP-complete for both definitions D1
and D2.
Proof. In the proofs of both NP-membership and NP-hardness, we will exploit the relationship between terms with respect to definitions D1 and D2 and concept expressions in
the description logic ALE (cf. Baader, McGuiness, Nardi, & Patel-Schneider, 2003). We
recall that concept expressions in this logic are constructed as follows (we use a modal logic
syntax and assume a single modal operator in order to facilitate comparison between the
formalisms):
 ::=  |  | a | a |    | 2 | 3
The semantics of the symbols  and  is as one would expect: M, w |=  and M, w 6|=  for
every model M and world w. The semantics of atomic literals, conjunctions, and universal
and existential modalities is exactly the same as for K.
It is not hard to see that every term with respect to D1 or D2 is a concept expression in
ALE. As entailment between ALE expressions is decidable in nondeterministic polynomial
time (cf. Donini, Lenzerini, Nardi, Hollunder, Nutt, & Marchetti Spaccamela, 1992), it
follows that deciding entailment between terms with respect to either D1 or D2 can also
be accomplished in nondeterministic polynomial time, i.e. these problems belong to NP.
It remains to be shown that these problems are NP-hard. To prove this, we show how
the polynomial-time reduction of Donini (2003) (adapted from the original NP-hardness
proof by Donini et al., 1992) of the NP-complete exact cover (XC) problem (Garey &
101

fiBienvenu

Johnson, 1979) to unsatisfiability in ALE can be modified so as to give a polynomial-time
reduction from XC to entailment between terms with respect to D1 or D2.
The exact cover problem is the following: given a set U = {u1 , ..., un } and a set S =
{S1 , ..., Sm } of subsets of U, determine whether there exists
Sqan exact cover, that is, a subset
{Si1 , ..., Siq } of S such that Sih  Sik =  for h 6= k and k=1 Sik = U. Donini has proven
(2003) that U, S has an exact cover if and only if the formula U ,S pictured in Figure 6 is
unsatisfiable. Notice that U ,S is not a term with respect to either D1 and D2 as it uses
the symbols  and . We would like to find a similar formula which is a term with respect
to our definitions and which is satisfiable if and only if U ,S is. Consider the formula
U ,S = 1,1  ...  1,m   
where i,j and   are defined exactly like i,j and  except that we replace  by a and
 by a. It is easy to verify that U ,S is indeed a term with respect to both D1 and
D2. Moreover, it is not too hard to see that 1,1  ...  1,m |= 32n  if and only if
1,1  ...  1,m |= 32n a and hence that U ,S and U ,S are equisatisfiable. As U, S has an
exact cover if and only if U ,S is unsatisfiable, and U ,S is unsatisfiable just in the case that
U ,S is, it follows that U, S has an exact cover if and only if U ,S is unsatisfiable. Moreover,
U ,S can be produced in linear time from U ,S , so we have a polynomial-time reduction
from XC to unsatisfiability of terms in D1 or D2. But a formula is unsatisfiable just in
the case that it entails the term a  a. So, XC can be polynomially-reduced to entailment
between terms with respect to either D1 or D2, making these problems NP-hard and hence
NP-complete.
In order to show the NP-completeness of clausal entailment, we remark that for both
definitions D1 and D2, the function Nnf transforms negations of clauses into terms and
negations of terms into clauses. This means that we can test whether a clause  entails a
clause  by testing whether the term Nnf ( ) entails the term Nnf (). Likewise, we
can test whether a term  entails another term  by testing whether the clause Nnf ( )
entails the clause Nnf (). As the NNF transformation is polynomial, it follows that
entailment between clauses is exactly as difficult as entailment between terms, so clausal
entailment is NP-complete.
Lemma 6.4 For definition D5, entailment between clauses or terms is Pspace-complete.
Proof. Membership in Pspace is immediate since entailment between arbitrary formulae
in K can be decided in polynomial space. To prove Pspace-hardness, we adapt an existing
proof of Pspace-hardness of K.
Figure 7 presents an encoding of a QBF  = Q1 p1 ...Qm pm  in a K-formula f () that
is used in section 6.7 of (Blackburn et al., 2001) to demonstrate the Pspace-hardness of
K. The formula f () has the property that it is satisfiable just in the case that  is a
QBF-validity. As the formula f () can be generated in polynomial-time from , and the
QBF-validity problem is known to be Pspace-hard, it follows that satisfiability of formulae
in K is Pspace-hard as well.
In Figure 8, we show a modified encoding. We claim the following:
(1) f () and f  () are logically equivalent
102

fiPrime Implicates and Prime Implicants in Modal Logic

(i) q0
Vm
(ii) i=0 ((qi  j6=i qj )  2(qi  j6=i qj )  ...  2m (qi  j6=i qj ))
Vm
(iiia) i=0 ((qi  3qi+1 )  2(qi  3qi+1 )  ...  2m (qi  3qi+1 ))
V
(iiib) {i|Qi =} 2i (qi  (3(qi+1  pi+1 )  3(qi+1  pi+1 )))
V
Vm1 j
(iv) m1
i=1 ( j=i 2 ((pi  2pi )  (pi  2pi )))

(v) 2m (qm  )

Figure 7: The formula f () is the conjunction of the above formulae.
(i) q0
Vm V
(ii) i=0 ( j6=i ((qi  qj )  2(qi  qj )  ...  2m (qi  qj )))
Vm
(iiia) i=0 ((qi  3qi+1 )  2(qi  3qi+1 )  ...  2m (qi  3qi+1 ))
V
(iiib) {i|Qi =} 2i (qi  3(qi+1  pi+1 ))  2i (qi  3(qi+1  pi+1 ))
Vm1 Vm1
(iv) i=1 ( j=i (2j (pi  2pi )  2j (pi  2pi )))

(v) 2m (qm  1 )  ....  2m (qm  l )

Figure 8: The formula f  () is the conjunction of the above formulae, where the formulae
i in (v) are propositional clauses such that   1  ...  l .

(2) if  is in CNF, then f  () is a conjunction of clauses with respect to D5
(3) if  is in CNF, then f  () can be generated in polynomial time from f ()
To show (1), it suffices to show that (i)(i), (ii)(ii), (iiia)(iiia), (iiib)(iiib), (iv)(iv),
and (v)(v). The first equivalence is immediate since (i) and (i) are identical. (ii)(ii)
follows from the fact that 2k (qi  j6=i qj )  j6=i 2k (qi  qj ). (iiia)(iiia) holds
since (iiia) is just (iiia) with qi  3qi+1 replaced with qi  3qi+1 . We have (iiib)(iiib)
since 2i (qi  (3(qi+1  pi+1 )  3(qi+1  pi+1 )))  2i (qi  3(qi+1  pi+1 ))  2i (qi 
3(qi+1  pi+1 )). The equivalence (iv)(iv) holds as 2j ((pi  2pi )  (pi  2pi )) 
2j (pi  2pi )  2j (pi  2pi ). Finally, we have (v)(v) since   1  ...  l . Thus, f ()
and f  () are logically equivalent.
To prove (2), we show that each of the component formulae in f  () is a conjunction
of clauses with respect to D5, provided that  is in CNF. Clearly this is the case for (i)
as (i) is a propositional literal. The formula (ii) is also a conjunction of clauses with
respect to D5 since it is a conjunction formulae of the form 2k (qi  qj ). Similarly,
(iiia), (iiib), and (iv) are all conjunctions of clauses since the formulae 2k (qi  3qi+1 ),
2i (qi  3(qi+1  pi+1 )), 2i (qi  3(qi+1  pi+1 )), 2k (pi  2pi ), and 2k (pi  2pi ) are
all clauses with respect to D5. The formula (v) must also be a conjunction of clauses since
the i are assumed to be propositional clauses, making each 2m (qm  i ) a clause with
respect to D5, and (v) a conjunction of clauses with respect to D5.
103

fiBienvenu

For (3), it is clear that we can transform (i), (iiia), (iiib), and (iv) into (i), (iiia), (iiib),
and (iv) in polynomial time as the transformations involve only simple syntactic operations
and the resulting formulae are at most twice as large. The transformation from (ii) to (ii)
is very slightly more involved, but it is not too hard to see the resulting formula is at most
m times as large as the original (and m can be no greater than the length of f ()). The
only step which could potentially result in an exponential blow-up is the transformation
from (v) to (v), as we put  into CNF. But under the assumption that  is already in CNF,
the transformation can be executed in polynomial time and space, as all we have to do is
separate  into its conjuncts and rewrite the (qm  i ) as (qm  i ).
Now let  = Q1 p1 ...Qm pm  be a QBF such that  = 1  ...  l for some propositional
clauses i . Let f  () be the formula as defined in Figure 8. By (2) above, we know that
f  () = 1  ...  p for some clauses i with respect to D5. Now consider the following
formula
 = 3(21  ...  2p  32(a  a))
We can show that f  () is satisfiable if and only if  is satisfiable as follows:
 is unsatisfiable
 21  ...  2p  32(a  a) is unsatisfiable
 1  ...  p  2(a  a) is unsatisfiable
 1  ...  p is unsatisfiable
 f  () is unsatisfiable
But we also know from (1) above that f  ()  f (), and from (Blackburn et al., 2001)
that f () is satisfiable just in the case that  is a QBF validity. It is also easy to see
that  is satisfiable if and only if  does not entail the contradiction 3(a  a). Putting
this altogether, we find that  is valid just in the case that  does not entail 3(a  a).
As  and 3(a  a) are both clauses and terms with respect to D5, we have shown that
the QBF-validity problem for QBF with propositional formulae in CNF can be reduced to
the problems of entailment of clauses or terms with respect to D5. Moreover, this is a
polynomial time reduction since it follows from (3) that the transformation from  to 
can be accomplished in polynomial time. This suffices to show Pspace-hardness, since it is
well-known that QBF-validity remains Pspace-hard even when we restrict the propositional
part  to be a formula in CNF (cf. Papadimitriou, 1994).
Theorem 6 The results in Figure 1 hold.
Proof. The satisfaction or dissatisfaction of properties P1 and P2 can be immediately
determined by inspection of the definitions, as can the satisfaction of P3 by definitions D2,
D3b, D4, and D5. Counterexamples to P3 for definitions D1 and D3a were provided in
body of the paper: the formula 2(a  b) is a clause but not a disjunction of literals with
respect to both definitions.
In order to show that definition D3b does not satisfy P4, we remark that the negation
of the literal 3(a  b) is equivalent to 2(a  b) which cannot be expressed as a literal
in D3b. For each of the other definitions, it can be shown (by a straightforward inductive
proof) that Nnf (L) is a literal whenever L is a literal, that Nnf (C) is a term whenever
104

fiPrime Implicates and Prime Implicants in Modal Logic

C is a clause, and that Nnf (T ) is a clause whenever T is a term. This is enough to prove
that these definitions satisfy P4 since Nnf () is equivalent to .
Since we know that definitions D1 and D2 satisfy both properties P1 and P2, it follows
by Theorem 5 that these definitions do not satisfy P5. We have seen in Lemma 6.1 that
definition D5 does satisfy P5, i.e. that every formula is equivalent to some conjunction of
clauses with respect to D5 and some disjunction of terms with respect to D5. As every
clause (resp. term) of D5 is also a clause (resp. term) with respect to definitions D3a, D3b,
and D4 (by Lemma 6.2), it follows that every formula is equivalent to some conjunction of
clauses and some disjunction of terms with respect to these definitions, which means they
all satisfy P5.
It is easy to see that property P6 is satisfied by all of the definitions since all of our
definitions are context-free grammars, and it is well-known that deciding membership for
context-free grammars can be accomplished in polynomial time (cf. Younger, 1967).
From Lemma 6.3, we know that deciding entailment between clauses or terms with
respect to either D1 or D2 is NP-complete (and hence not in P, unless P=NP). Entailment
between clauses/terms is Pspace-complete for D5 (Lemma 6.4). As every clause (resp.
term) of D5 is also a clause (resp. term) with respect to definitions D3a, D3b, and D4
(from Lemma 6.2), it follows that entailment between clauses or terms is Pspace-hard for
these definitions. Membership in Pspace is immediate since entailment between arbitary
K formulae is in Pspace.
We prove Theorem 9 in several steps:
Lemma 9.1 The notions of prime implicates and prime implicants induced by D4 satisfy
Implicant-Implicate Duality.
Proof. Suppose for a contradiction that we have a prime implicant  of some formula 
which is not equivalent to the negation of a prime implicate of . Let  be a clause which
is equivalent to  (there must exist such a clause because of property P4, cf. Theorem
6). The clause  is an implicate of  since  |=  and   . As we have assumed that
 is not a prime implicate, there must be some implicate  of  such that  |=  and
 6|=  . But then let  be a term equivalent to  (here again we use P4). Now  must
be an implicant of  since  |=  . Moreover,  is strictly weaker than  since  |= 
and  6|=  and    and    . But this means that  cannot be a prime implicant,
contradicting our earlier assumption. Hence, we can conclude that every prime implicant
of a formula  is equivalent to the negation of some prime implicate of . The proof that
every prime implicate of a formula  is equivalent to the negation of a prime implicant of
 proceeds analogously.
Lemma 9.2 If clauses and terms are defined according to definition D4, then every implicate  of a formula  is entailed by some implicate  of  with var( )  var() and
with depth at most () + 1, and every implicant  of  entails an implicant  of  with
var( )  var() and depth at most () + 1.
Proof. We intend to show that the following statement holds: for any formula  and any
implicate  of , there exists a clause  such that  |=  |=  and var( )  var() and
105

fiBienvenu

()  () + 1. So let  be an arbitrary formula, and let  be some implicate of . If 
is a tautology, then we can set  = a  a (where a  var()). If   , then we can set
 = 3(a  a) (where a  var()), as this clause verifies all of the necessary conditions.
Now we consider the case where neither  nor  is a tautology or a falsehood, and we
show how to construct the clause  . The first thing we do is use Dnf-4 to rewrite  as
a disjunction of satisfiable terms Ti with respect to D4 such that the Ti contain only the
variables appearing in  and have depth at most ():
 = T1  ...  Tz
As  |= , it must be the case that Ti |=  for every Ti . Our aim is to find a clause i for
each of the terms Ti such that Ti |= i |=  and var(i )  var(Ti ) and (i )  (Ti ). So
consider some Ti . Since Ti is a term, it has the form 1  ...  k  31  ...  3m  21 
...  2n , where 1 , ..., k are propositional literals. As  is a clause, it must be of the form
1  ...  p  31  ...  3q  21  ...  2r , where 1 , ..., p are propositional literals. As
Ti |= , it must be the case that the formula
1  ...  k  31  ...  3m  21  ...  2n 
1  ...  p  21  ...  2q  31  ...  3r
is unsatisfiable. By Theorem 1, one of the following must hold:
(a) there exists u and v such that u  v
(b) there exists u such that u  1  ...  n  1  ...  q |= 
(c) there exists u such that u  1  ...  n  1  ...  q |= 
Now if (a) holds, we can set i = u since Ti |= u |= , (u ) = 0  (Ti ), and var(u ) 
var(Ti ). If it is (b) that holds, then it must be the case that
u  1  ...  n |= 1  ...  q
and hence that
3(u  1  ...  n ) |= 31  ...  3q |= 
We can set i = 3(u  1  ...  n ), since Ti |= 3(u  1  ...  n ) |= , (3(u  1 
...  n ))  (Ti ), and var(3(u  1  ...  n ))  var(Ti ). Finally, if (c) holds, then it
must be the case that
1  ...  n |= 1  ...  q  u
and hence that
2(1  ...  n ) |= 31  ...  3q  2u |= 
So we can set i = 2(1  ...  n ), as Ti |= 2(1  ...  n ) |= , (2(1  ...  n ))  (Ti ),
and var(2(1  ...  n ))  var(Ti ). Thus, we have shown that for every Ti , there is some
i such that Ti |= i |=  and var(i )  var(Ti ) and (i )  (Ti ). But then 1  ...  z is
a clause implied by every Ti , and hence by , and such that var(i )  i var(Ti )  var()
and (i )  maxi (Ti )  ().
Now let  be an implicant of , and let  be the formula Nnf (). We know that
the NNF transformation is equivalence-preserving, hence   , and it is straightforward
106

fiPrime Implicates and Prime Implicants in Modal Logic

to show that  must be a clause with respect to D4. But then  is an implicate of ,
so there must be some clause  with var( )  var() = var() and depth at most
() + 1 = () + 1 such that  |=  |= . Let  be Nnf ( ). It can be easily verified
that  is a term. Moreover, by properties of the NNF transformation, we have    ,
var( ) = var( ) = var( ), and ( ) = ( ) = ( ). But then  is a term such that
var( )  var(), ( )  () + 1, and  |=  |= .
Lemma 9.3 The notions of prime implicates and prime implicants induced by D4 satisfy
Finiteness.
Proof. Consider an arbitrary formula . From Lemma 9.2, we know that for each prime
implicate  of , there must be an implicate  of  containing only those propositional
atoms appearing in  and such that ( )  () + 1 and  |= . But since  is a prime
implicate, we must also have  |=  and hence    . Thus, every prime implicate of
 is equivalent to some clause built from the finite set of propositional symbols in  and
having depth at most () + 1. As there are only finitely many non-equivalent formulae
on a finite alphabet and with fixed depth, it follows that there can be only finitely many
distinct prime implicates. By Lemma 9.1, every prime implicant of  is equivalent to the
negation of some prime implicate of . It follows then that every formula can only have
finitely many distinct prime implicants.
Lemma 9.4 The notions of prime implicates and prime implicants induced by D4 satisfy
Covering.
Proof. Let  be an arbitrary formula. From Lemma 9.2, we know that every implicate of
 is entailed by some implicate of  whose propositional variables are contained in var()
and whose depth is at most () + 1. Now consider the following set
 = { |  |= ,  is a clause, var()  var(), ()  () + 1}
and define another set  from  as follows:
 = {   | 6    .   |=  and  6|=   }
In other words,  is the set of all of the logically strongest implicates of  having depth at
most () + 1 and built from the propositional letters in . We claim the following:
(1) every    is a prime implicate of 
(2) for every implicate  of , there is some    such that  |= 
We begin by proving (1). Suppose that (1) does not hold, that is, that there is some   
which is not a prime implicate of . Since  is by definition an implicate of , it follows that
there must be some implicate  of  such that  |=  and  6|= . But by Lemma 9.2, there
is some implicate  of  such that ( )  () + 1, var( )  var(), and  |= . But
that means that  is an element of  which implies but is not implied by , contradicting
the assumption that  is in . We can thus conclude that every element of  must be a
prime implicate of .
107

fiBienvenu

For (2): let  be some implicate of . Then by Lemma 9.2, there exists some clause
   such that  |= . If   , we are done. Otherwise, there must exist some   
such that  |=  and  6|= . If   , we are done, otherwise, we find another stronger
member of . But as  has finitely many elements modulo equivalence, after a finite number
of steps, we will find some element which is in  and which implies . Since we have just
seen that all members of  are prime implicates of , it follows that every implicate of  is
implied by some prime implicate of .
For the second part of Covering, let  be an implicant of , and let  be a clause
equivalent to  (there must be one because D4 satisfies P4). Now since  |= , we must
also have  |= . According to what we have just shown, there must be some prime
implicate  of  such that  |=  |= . By Lemma 9.1,  must be equivalent to the
negation of some prime implicant  of . But since    and  |=  and   , it
follows that  |= , completing the proof.
Lemma 9.5 The notions of prime implicates and prime implicants induced by D4 satisfy
Equivalence.
Proof. Let  be some formula in K, and suppose that M is a model of every prime implicate
of . As D4 is known to satisfy property P5 (by Theorem 6), we can find a conjunction of
clauses which is equivalent to . By Covering (Lemma 9.3), each of these clauses is implied
by some prime implicate of , so M must be a model of each of these clauses. It follows
that M is a model of . For the other direction, we simply note that by the definition of
prime implicates if M is a model of , then it must also be a model of every prime implicate
of . We have thus shown that M is a model of  if and only if it is a model of every prime
implicate of . Using a similar argument, we can show that M is a model of  if and only
if it is a model of some prime implicant of .
Lemma 9.6 The notions of prime implicates and prime implicants induced by D4 satisfy
Distribution.
Proof. Let  be a prime implicate of 1  ...  n . Now for each i , we must have i |= .
From Covering, we know that there must exist some prime implicate i for each i such
that i |= . This means that the formula 1  ...  n (which is a clause because it is a
disjunction of clauses) entails . But since  is a prime implicate, it must also be the case
that  |= 1  ...  n , and hence   1  ...  n . The proof for prime implicants is entirely
similar.
Theorem 9 The notions of prime implicates and prime implicants induced by definition
D4 satisfy Finiteness, Covering, Equivalence, Implicant-Implicate Duality, and
Distribution.
Proof. Follows directly from Lemmas 9.1-9.6.
Theorem 10 The notions of prime implicates and prime implicants induced by definitions
D1 and D2 do not satisfy Equivalence.
108

fiPrime Implicates and Prime Implicants in Modal Logic

Proof. The proof is the same for both definitions. Suppose that Equivalence holds. Then
for every formula , the set  of prime implicates of  is equivalent to . But this means
that the set   {} is inconsistent, and hence by compactness of K (cf. Blackburn et al.,
2001, p. 86) that there is some finite subset S    {} which is inconsistent. If  6 ,
then we know that the set S must contain  because the set of prime implicates of 
cannot be inconsistent. But then the conjunction of elements in S \ {} is a conjunction
of clauses which is equivalent to . It follows that every formula  is equivalent to some
conjunction of clauses. As we have shown earlier in the proof of Theorem 5 that there are
formulae which are not equivalent to a conjunction of clauses with respect to D1 or D2, it
follows that Equivalence cannot hold for these definitions.
Theorem 11 The notions of prime implicates and prime implicants induced by definitions
D3a, D3b, and D5 do not satisfy Finiteness.
Proof. Suppose that clauses are defined with respect to definition D3a, D3b, or D5 (the
proof is the same for all three definitions). Consider the formula  = 2(a  b). It follows
from Theorem 3 that  implies k = 2(3k a)  3(a  b  2k a) for every k  1. As the
formulae k are clauses (with respect to D3a, D3b, and D5), the k are all implicates
of . To complete the proof, we show that every k is a prime implicate of . Since the k
are mutually non-equivalent (because 2p a 6|= 2q a whenever p 6= q), it follows that  has
infinitely many prime implicates modulo equivalence.
Consider some k and some implicate  = 31  ...  3m  21  ...  2n of  that
implies it (by Theorem 2 there cannot be any propositional literals in ). Using Theorem 3
and the fact that  |=  |= k , we get the following:
(a) a  b |= i  i  ...  m for some i
(b) i |= (3k a)  (a  b  2k a) for every i
(c) 1  ...  m |= a  b  2k a
Let i be such that a  b |= i  i  ...  m . We remark that i must be satisfiable since
otherwise we can combine (a) and (c) to get a  b |= a  b  2k a. Now by (b), we know that
i |= (3k a)  (a  b  2k a) and hence that i  (2k a)  (a  b  3ka) is inconsistent. It
follows that both i  (2k a)  a and i  (2k a)  b are inconsistent. Using Theorem 1,
we find that either i |= 3k a or i |= a  b. As i is a satisfiable clause with respect to
definitions D3a, D3b, and D5, it cannot imply a  b, so we must have i |= 3k a. By
putting (a) and (c) together, we find that
a  b  i |= 1  ...  m |= a  b  2k a
It follows that i |= 2k a, i.e. 3k a |= i . We thus have i  3k a and 1  ...  m 
a  b  2k a. As 3k a |= i and a  b  2k a |= 1  ...  m , by Theorem 3 we get
2(3k a)  3(a  b  2k a) |= 2i  3i  ...  3m |=  and hence k  . We have thus
shown that any implicate of  which implies k must be equivalent to k . This means that
each k is a prime implicate of , completing the proof.
109

fiBienvenu

Lemmas 12, 13, and 14 follow easily from known properties of the disjunctive normal
form transformation in propositional logic (cf. Bienvenu, 2009, ch. 2).
In the proof of Theorem 16, we will make use of the following lemmas:
Lemma 16.1 The algorithm GenPI always terminates.
Proof. We know from Lemma 12 that the algorithm Dnf -4 always terminates and returns
a finite set of formulae. This means that there are only finitely many terms T to consider.
For each T , the set (T ) contains only finitely many elements (this is immediate given
the definition of (T )), which means that the set Candidates also has finite cardinality.
In the final step, we compare at most once each pair of elements in Candidates. As the
comparison always terminates, and there are only finitely many pairs to check, it follows
that the algorithm GenPI terminates.
Lemma 16.2 The algorithm GenPI outputs exactly the set of prime implicates of the input
formula.
Proof. We first prove that every prime implicate of a satisfiable term T is equivalent to
some element in (T ). Let T = 1  ...  k  31  ...  3m  21  ...  2n be some
satisfiable term, and let  = 1  ...  p  31  ...  3q  21  ...  2r be one of its
prime implicates. We restrict our attention to the interesting case in which both T and 
are non-tautologous. As T |= , it must be the case that
1  ...  k  31  ...  3m  21  ...  2n 
1  ...  p  21  ...  2q  31  ...  3r
is unsatisfiable. By Theorem 1, one of the following must hold:
(a) there exists u and v such that u  v
(b) there exists u such that u  1  ...  n |= 1  ...  q
(c) there exists u such that 1  ...  n |= u  1  ...  q
If (a) holds, then u |= , so  must be equivalent to u or else we would have found a
stronger implicate, contradicting our assumption that  is a prime implicate of T . But then
the result holds since u is in (T ). If (b) holds, then the formula 3(u  1  ...  n )
is an implicate of T which implies , so   3(u  1  ...  n ). We are done since
3(u  1  ...  r ) is a member of (T ). Finally we consider the case where (c) holds. In
this case, 2(1  ...  n ) is an implicate of T which implies , and so is equivalent to  (as
 is a prime implicate). But then we have the desired result since 2(1  ...  n ) is one
of the elements in (T ). Thus we can conclude that every prime implicate of a term T is
equivalent to some element in (T ). By Lemma 13, the elements in Dnf -4() are terms,
and their disjunction is equivalent to . As D4 satisfies Distribution, it follows that every
prime implicate of the input  is equivalent to some element in Candidates. This means
that if an element i in Candidates is not a prime implicate of , then there is some prime
implicate  of  that implies but is not implied by i , and hence some j  Candidates
such that j |= i and i 6|= j . Thus, during the comparison phase, this clause will be
removed from Candidates. Now suppose that the clause  is a prime implicate of . Then
110

fiPrime Implicates and Prime Implicants in Modal Logic

we know that there must be some i  Candidates such that i  , and moreover, we
can choose i so that there is no j with j < i such that j |= i . When in the final step we
compare i with all the clauses j with j 6= i, we will never find that j |= i for j < i, nor
that j |= i 6|= j for some j > i, otherwise  would not be a prime implicate. It follows
then that i remains in the set Candidates which is returned by the algorithm. We have
thus shown that the set of formulae output by GenPI on input  is precisely the set of
prime implicates of .
Theorem 16 The algorithm GenPI always terminates and outputs exactly the set of prime
implicates of the input formula.
Proof. Follows directly from Lemmas 16.1 and 16.2.
Theorem 17 The length of the smallest clausal representation of a prime implicate of a
formula is at most single exponential in the length of the formula.
Proof. Prime implicates generated by GenPI can have at most 2|| disjuncts as there are
at most 2|| terms in Dnf -4() by Lemma 14. Moreover, each disjunct has length at most
2|| (also by Lemma 14). This gives us a total of 2||  2|| symbols, to which we must
add the at most 2||  1 disjunction symbols connecting the disjuncts. We thus find that
the length of the smallest representation of a prime implicate of a formula  is at most
2||  2|| + (2||  1).
Theorem 18 The length of the smallest clausal representation of a prime implicate of a
formula can be exponential in the length of the formula.
Proof. Consider the formula
=

n
^

(2ai,1  2ai,2 )

i=1

and the clause
=

_

(j1 ,...,jn

2(a1,j1  a2,j2  ...  an,jn )

){1,2}n

where ak,l 6= am,p whenever k 6= m or l 6= p. It is not difficult to see that  and  are
equivalent, which means that  must be a prime implicate of . All that remains to be
shown is that any clause equivalent to  must have length at least ||. This yields the result
since  clearly has size exponential in n, whereas the length of  is only linear in n.
Let  be a shortest clause which is equivalent to . As  is equivalent to , it follows
from Theorem 2 that  is a disjunction of 2-literals and of inconsistent 3-literals. But
since  is assumed to be a shortest representation of , it cannot contain any inconsistent
3-literals or any redundant 2-literals, since we could remove them to find an equivalent
shorter clause. So  must be of the form 21  ...  2m , where l 6|= k whenever l 6= k.
Now since  |= , every disjunct 2p must also imply . As  is a disjunction of 2-literals,
it follows from Theorem 3 that every disjunct 2p of  implies some disjunct 2q of . But
that means that every 2p must have length at least 2n + 1, since each p is a satisfiable
formula which implies a conjunction of n distinct propositional variables. We also know
that every disjunct 2q of  implies some disjunct 2p of  since  |=  . We now wish
111

fiBienvenu

to show that no two disjuncts of  imply the same disjunct of  . Suppose that this is not
the case, that is, that there are distinct disjuncts 21 and 22 of  and some disjunct 2p
of  such that 21 |= 2p and 22 |= 2p . Now since 21 and 22 are distinct disjuncts,
there must be some i such that 21 |= ai,1 and 22 |= ai,2 or 21 |= ai,2 and 22 |= ai,1 .
We know that 2p |= 2q for some q , and that every q implies either ai1 or ai2 , so either
2p |= 2ai1 or 2p |= 2ai2 . But we know that the 2q each imply either 2ai,1 or 2ai,2
but not both, so one of 21 and 22 must not imply 2p . This contradicts our earlier
assumption that 21 |= 2p and 22 |= 2p , so each disjunct of  must imply a distinct
disjunct of  . We have thus demonstrated that  contains just as many disjuncts as .
As we have already shown that the disjuncts of  are no shorter than the disjuncts of , it
follows that | |  ||, and hence | | = ||. We conclude that every clause equivalent to 
has length at least ||, completing the proof.
For Theorem 19, we will prove that the following clause
_
=
2q1 ...qn c
(q1 ,...,qn){3,2}n

is a prime implicate (with respect to both D1 and D2) of the formula


=

( 23(b0  b1 )  22(b0  b1 ) ) 

n
^

( 2i 3bi  2i 2bi )

i=2



n1
^

2i+1 ( (bi1  bi )  2bi )  2n+1 ( (bn1  bn )  c )

i=1

and moreover that there is no shorter way to represent .
The proof of Theorem 19 makes use of the following lemmas.
Lemma 19.1 Let l1  ...  lm be a D1-clause which implies q1 ...qn a, where qi  {2, 3} and
a is a propositional variable. Then l1  ...  lm  q1 ...qn a.
Proof. In the proof, we will make use of the fact that every D1-clause is satisfiable. This
is very straightforwardly shown by structural induction. The base case is propositional
literals, which are clearly satisfiable. For the induction step, we consider a D1-clause 
such that all its proper sub-clauses are satisfiable. There are three possibilities: either 
is of the form 2  or 3  where  is a satisfiable D1-clause, or a disjunction 1  2 of
satisfiable D1-clauses 1 and 2 . In all three cases, we find that  must also be satisfiable.
The proof of the lemma is by induction on n. When n = 0, we have just l1  ...  lm |= a.
According to Theorem 2, every disjunct of l1  ...  lm must be either a or some unsatisfiable
formula. But we have shown in the previous paragraph that every D1-clause is satisfiable,
so l1  ...  lm  a.
Now suppose the result holds whenever n  k, and suppose that we have l1  ...  lm |=
q1 ...qk+1 a. For every li , we must have li |= q1 ...qk+1 a, and hence |= li  q1 ...qk+1 a. Using
Theorem 1, we arrive at the following four possibilities:
112

fiPrime Implicates and Prime Implicants in Modal Logic

(a) |= q1 ...qk+1 a
(b) li  
(c) q1 = 3 and li  3li and li |= q2 ...qk+1 a
(d) q1 = 2 and li  2li and li |= q2 ...qk+1 a
We can eliminate case (a) since 6|= q1 ...qk+1 a for every string of modalities q1 ...qk+1 . We can
also eliminate (b) since all of the li must be satisfiable as they are D1-clauses. We remark
that if (c) holds, then according to the induction hypothesis, li  3q2 ...qk+1 a. Similarly, if
(d) holds, then li  2q2 ...qk+1 a. It follows then that each li is equivalent to q1 ...qk+1 a, and
so l1  ...  lm  q1 ...qk+1 a.
V
n , and let T = 2q (b  b )  ( n
k
Lemma
19.2
Fix
(q
,
...,
q
)

{2,
3}
1
n
1
0
1
k=2 2 qk bk ) 
Vn1 k+1
( (bk1  bk )  2bk )  2n+1 ( (bn1  bn )  c ). Then T |= 2r1 ...rn c if and
k=1 2
only if rk = qk for all 1  k  n.
Proof. We begin by showing that for all 1  i  n  1 the formula
bi1  bi  (

n
^

n1
^

2ki1 qk bk )  (

k=i+1

2ki ((bk1  bk )  2 bk ) )  2ni ((bn1  bn )  c)

k=i

entails the formula ri+1 ...rn c just in the case that qi+1 ...qn = ri+1 ...rn .
The proof is by induction on i. The base case is i = n  1. We have
bn2  bn1  qn bn  ((bn2  bn1 )  2bn1 )  2((bn1  bn )  c) |= rn c

(1)

if and only if
bn2  bn1  qn bn  2bn1  2((bn1  bn )  c) |= rn c
if and only if (Theorem 1) either
qn = 3 and rn = 2 and bn1  ((bn1  bn )  c) |= c
or
qn = rn and bn1  bn  ((bn1  bn )  c) |= c
As bn1  ((bn1  bn )  c) 6|= c, we cannot have the first alternative. It follows then that
if Equation (1) holds, then the second alternative must hold, in which case we get qn = rn ,
as desired. For the other direction, we simply note that bn1  bn  ((bn1  bn )  c) |= c
is a valid entailment, which means qn = rn implies Equation (1).
Next let us suppose that the above statement holds for all 1 < j  i  n  1, and let us
prove the statement holds when i = j  1. Then
bj2  bj1  (

n
^

kj

2

qk bk )  (

k=j

n1
^

2kj+1 (bk1  bk  2bk ) )

k=j1

 2nj+1 ((bn1  bn )  c)

|= rj ...rn c
113

(2)

fiBienvenu

if and only if one of the following holds:
(a) qj = 3 and rj = 2 and
bj1  (

n
^

n1
^

2kj1 qk bk )  (

k=j+1

2kj ((bk1  bk )  2bk ) )  2nj ((bn1  bn )  c)

k=j

|= rj+1 ...rn c

(b) qj = rj and
bj1  bj  (

n
^

n1
^

2kj1 qk bk )  (

k=j+1

2kj ((bk1  bk )  2bk ))  2nj ((bn1  bn )  c)

k=j

|= rj+1 ...rn c

We will first show that the entailment in (a) does not hold. Consider the model M =
hW, R, vi defined as follows:
 W = {wj , ..., wn }
 R = {(wj , wj+1 ), ..., (wn1 , wn )}
 v(c, w) = f alse for all w  W
 for w 6= wj : v(bk , w) = true if and only if w = wk
 v(bk , wj ) = true if and only if k = j  1
Notice that since each world (excepting wn ) has exactly one successor, the 2- and 3quantifiers have the same behaviour (except at wn ). It can easily be verified that M, wj
satisfies the left-hand side of the aboveV
entailment for any tuple qj+1 ...qn : we have M, wj |=
bj1 by definition, we have M, wj |= nk=j+1 2kj1 qk bk because M, wk |= bk for k 6= j,
Vn1 kj
we have M, wj |= k=j
2 ((bk1  bk )  2bk ) ) since M, wj 6|= bj and M, wk 6|= bk1 for
k 6= j, and finally we have M, wj |= 2nj ((bn1  bn )  c) since wn 6|= bn1 . However, the
right-hand side rj+1 ...rn c is not satisfied at wj : the only world accessible from wj in n  j
steps is wn which does not satisfy c.
We have just shown that case (a) cannot hold, which means that Equation (2) holds if
and only if (b) does. But if we apply the induction hypothesis to the entailment in (b), we
find that it holds just in the case that qj+1 ...qn = rj+1 ...rn . It follows then that Equation (2)
if and only if qj ...qn = rj ...rn , as desired. This completes our proof of the above statement.
We now proceed to the proof of the lemma. By Theorem 1,
2q1 (b0  b1 )  (

n
^

n1
^

2k qk bk )  (

k=2

2k+1 ( (bk1  bk )  2bk )  2n+1 ( (bn1  bn )  c )

k=1

|= 2r1 ...rn c

holds just in the case that
q1 (b0  b1 )  (

n
^

k=2

2k1 qk bk ) 

n1
^

2k ( (bk1  bk )  2bk )  2n ( (bn1  bn )  c )

k=1

|= r1 ...rn c
114

fiPrime Implicates and Prime Implicants in Modal Logic

which in turn holds if and only if one of the following statements holds:
(i) q1 = 3 and r1 = 2 and
(

n
^

n1
^

2k2 qk bk )  (

k=2

2k1 ((bk1  bk )  2bk ) )  2n1 ((bn1  bn )  c) |= r2 ...rn c

k=1

(ii) q1 = r1 and
b0  b1  (

n
^

k=2

n1
^

2k2 qk bk )  (

2k1 ((bk1  bk )  2bk ) )  2n1 ((bn1  bn )  c)

k=1

|= r2 ...rn c

We remark that if we set j = 1 in (a) above, then the left-hand side of the entailment in (i)
is logically weaker than that in (a), and the right-hand side matches that in (a). As we have
already shown that the entailment in (a) does not hold, it follows that the entailment in
(i) cannot hold either. Thus, we find that the desired entailment relation in the statement
of the lemma holds if and only if (ii) does. This completes the proof since we have already
shown in the induction above that the entailment in (ii) holds if and only if q2 ...qn = r2 ...rn ,
i.e. (ii) is true just in the case that q1 ...qn = r1 = rn .
Lemma 19.3 There is no D1-clause equivalent to  and with strictly smaller size than .
Proof. Let  be a D1-clause which is equivalent to . Suppose furthermore that  is a
shortest such clause. As  is non-tautologous and contains only 2-literals as disjuncts, it
follows that every disjunct of  must be either unsatisfiable or a 2-literal (cf. Theorem 2).
But D1-clauses are always satisfiable (cf. proof of Lemma 19.1), so  must contain only
2-literals.
Since  |= , every disjunct 2l of  must imply some disjunct 2q1 ...qn c of . Also,
every disjunct 2l of  must be implied by some disjunct 2q1 ...qn c of , since otherwise we
could remove 2l from  while preserving the equivalence between  and  .
It follows then that each disjunct of  is implied by some disjunct of  and implies
some disjunct of . But since the disjuncts of  do not imply each other (because of Lemma
19.1), it follows that each disjunct of  is equivalent to some disjunct of , and moreover
that every disjunct of  is equivalent to some disjunct of  .
This completes the proof since it is clear that the disjuncts 2q1 ...qn c of  cannot be
more compactly represented.
Our proof works equally well for D2, since every D2-clause is also a D1-clause.
Theorem 19 If prime implicates are defined using either D1 or D2, then the length of
the smallest clausal representation of a prime implicate of a formula can be exponential in
the length of the formula.
Proof. We begin with definition D1. Let  and  be as defined on page 112. We begin by
distributing  over  in order to transform  into an equivalent disjunction of D4-terms:
_

Tq1 ,...,qn
(q1 ,...,qn){2,3}n

115

fiBienvenu

where Tq1 ,...,qn is equal to
2q1 (b0  b1 )  (

n
^

2 i q i bi ) 

n1
^

2i+1 ( (bi1  bi )  2bi )  2n+1 ( (bn1  bn )  c )

i=1

i=2

By Lemma 19.2, Tq1 ,...,qn |= 2q1 ...qn c, and hence Tq1 ,...,qn |= . We thus have  |= .
We now show that there is no stronger clause with respect to D1 which is implied by
. Let  be a D1-clause such that  |=  |= . As  is a non-tautologous disjunction of
2-literals, we know from Lemma 2 that every disjunct of  must be of the form 2l where
l is a D1-clause such that l |= r1 ...rn c for some quantifier string r1 ...rn . But according to
Lemma 19.1, if l |= r1 ...rn c, then l is equivalent to r1 ...rn c. It follows that  is equivalent
to a clause having only disjuncts of the forms 2r1 ...rn c.
As  |=  , it must be the case that each of the terms Tq1 ,...,qn implies  , or equivalently
Tq1 ,...,qn   |= . As we have shown above that the disjuncts of  are all 2-literals, it
follows from Theorem 1 that each term implies some disjunct of  . Moreover, we know
from the preceding paragraph that each of the disjuncts of  is equivalent to some formula
of the form 2r1 ...rn c. By Lemma 19.2, the only formula of this type which is implied by
Tq1 ,...,qn is the formula 2q1 ...qn c. This means that for every tuple of quantifiers (q1 , ..., qn ),
there is a disjunct of  which is equivalent to 2q1 ...qn c. It follows that every disjunct of
 is equivalent to some disjunct of  , giving us  |=  . We can thus conclude that  is a
prime implicate of .
This completes the proof, since we have already shown in Lemma 19.3 that there is no
shorter D1-clause which is equivalent to  than  itself.
The above proof also works for definition D2 since every D2-clause is also a D1-clause.
In particular this means that any D2-clause which is a prime implicate with respect to D1
is also a prime implicate with respect to D2, and that any D2-clause which is shortest
among all equivalent D1-clauses is also shortest among D2-clauses.
Theorem 20 The number of non-equivalent prime implicates of a formula is at most double
exponential in the length of the formula.
Proof. We know from Theorem 16 that every prime implicateWof  is equivalent to some
clause returned by GenPI. Every such clause is of the form T Dnf -4() T where T 
(T ). As there can be at most 2|| terms in Dnf -4() by Lemma 14, these clauses can have
no more than 2|| disjuncts. Moreover, there are at most 2|| choices for each disjunct T
since the cardinality of (T ) is bounded above by the size of T , which we know from Lemma
||
1.3 to be no more than 2||. It follows then that there are at most (2||)2 clauses returned
||
by GenPI, hence at most (2||)2 non-equivalent prime implicates of .
Theorem 21 The number of non-equivalent prime implicates of a formula may be double
exponential in the length of the formula.
Proof. Let n be some natural number, and let a11 , a12 , ..., an1 , an2 , b11 , b12 , b12 , ..., bn1 ,
bn2 be 4n distinct propositional variables. Consider the formula  defined as
n
^

((3ai1  2bi1 )  (3ai2  2bi2 ))

i=1

116

fiPrime Implicates and Prime Implicants in Modal Logic

It is not hard to see that there will be 2n terms in Dnf -4(), corresponding to the 2n ways
of deciding for each i  {1, ..., n} whether to take the first or second disjunct. Each term
T  Dnf -4() will be of the form
n
^

(3ai f (i,T )  2bi f (i,T ) )

i=1

where f (i, T )  {1, 2} for all i. For each T , denote by D(T ) the set of formulae {3(a f (i,T ) 
b1 f (1,T )  ...  bn f (n,T ) )) | 1  i  n}. Now consider the set of clauses C defined as
{

_

dT | dT  D(T )}

T Dnf -4()
n

Notice that there are n2 clauses in C since each clause corresponds to a choice of one of
the n elements in D(T ) for each of the 2n terms T in Dnf -4(). This number is double
exponential in || since the length of  is linear in n. In order to complete the proof, we
show that (i) all of the clauses in C are prime implicates of  and (ii) that the clauses in C
are mutually non-equivalent.
We begin by showing that 1 6|= 2 for every pair of distinct elements 1 and 2 in C.
This immediately gives us (ii) and will prove useful in the proof of (i). Let 1 and 2 be
distinct clauses in C. As 1 and 2 are distinct, there must be some term T  Dnf -4()
for which 1 and 2 choose different elements from D(T ). Let d1 be the element from
D(T ) appearing as a disjunct in 1 , let d2 be the element in D(T ) which is a disjunct in
2 , and let aj,k be the a-literal which appears in d2 (and hence not in d1 ). Consider the
formula  = 2(aj,k  b1,k1  ...  bn,kn ), where the tuple (k1 , ..., kn ) is just like the tuple
associated with T except that the 1s and 2s are inversed. Clearly d1   is consistent,
since the variables in  do not appear in d1 . But  is inconsistent with every disjunct in
2 , since by construction every disjunct in 2 contains a literal whose negation appears in
. It follows that 2 |=  but 1 6|= , and hence 1 6|= 2 .
We now prove (i). Let  be a clause in C, and let  be a prime implicate of  which
implies . By Theorem 16, we know that  must be equivalent to one of the clauses output
by GenPI, and more specifically to a clause output by GenPI which is a disjunction of
3-literals (because of Theorem 2). We remark that the set C is composed of exactly those
candidate clauses which are disjunctions of 3-literals, so  must be equivalent to some
clause in C. But we have just shown that the only element in C which implies  is  itself.
It follows that   , which means that  is a prime implicate of .
Theorem 22 If prime implicates are defined using either D1 or D2, then the number of
non-equivalent prime implicates of a formula may be double exponential in the length of the
formula.
Proof. Let  and  be as defined on page 112. Set  equal to the formula obtained from 
by replacing c in the last conjunct of  by c  d. Set  equal to the set of clauses that can
be obtained from  by replacing zero or more occurrences of c by d. For example, if n = 1,
n
then  = {23c  22c, 23d  22c, 23c  22d, 23d  22d}. There are 22 elements in
 since we choose for each of the 2n disjuncts of  whether to change c into d. We intend
117

fiBienvenu

to show that the clauses in  are all pairwise non-equivalent prime implicates of  . The
proof that every element in  is indeed a prime implicate of  (with respect to both D1
and D2) proceeds quite similarly to the proof that  is a prime implicate of  (see proof
of Theorem 19), so we will not repeat it here. Instead we will show that all of the elements
in  are pairwise non-equivalent. To do so, we consider any two distinct elements  and 
of . Since  and  are distinct, there must be some string of quantifiers q1 ...qn such that
 has a disjunct 2q1 ...qn  (  {c, d}) which is not a disjunct of . Now if  |= , then we
would have 2q1 ...qn  |= , and hence 2q1 ...qn  |= 2r1 ...rn  for some disjunct r1 ...rn  of .
But by using Lemma 19.1, we see that this can only happen if r1 ...rn = q1 ...qn and  =  ,
i.e. if 2q1 ...qn is a disjunct of . This is a contradiction, so we must have  6|= . It follows
that the elements of  are pairwise non-equivalent, and hence that  possesses a double
exponential number of prime implicates.
Theorem 23 There exists an algorithm which runs in single-exponential space in the size
of the input and incrementally outputs, without duplicates, the set of prime implicates of
the input formula.
Proof. Let the sets T and Candidates and the function  be defined as in Figure 3.
We assume that T is ordered: T = {T1 , ..., Tn }. For each Ti  T , we let max i denote
the number of elements in (Ti ), and we assume an ordering on the elements of (Ti ):
(Ti ) = {i,1 , ..., i,max i }. Notice that the tuples in {1, .., max 1 }  ...  {1, ..., max n } can be
ordered using the standard lexicographic ordering <lex : (a1 , ..., an ) <lex (b1 , ..., bn ) if and
only if there is some 1  j  n such that aj < bj and ak  bk for all 1  k  j  1. Now
set maxindex = ni=1 maxi , and let f : {1, .., max 1 }  ...  {1, ..., max n }  {1, ..., maxindex }
be the bijection defined as follows: f (a1 , ..., an ) = m if and only if (a1 , ..., an ) is the m-th
tuple in the lexicographic ordering of {1, .., max 1 }  ...  {1, ..., max n }. We will denote by
m the unique clause of form 1,a1  ...  n,an such that f (a1 , ..., an ) = m. We remark
that given an index m  {1, ..., maxindex } and the sets (T1 ), ..., (Tn ), it is possible to
generate in polynomial space (in the size of the sets (T1 ), ..., (Tn )) the clause m . We
make use of this fact in our modified version of algorithm GenPI, which is defined as follows:
Function IterGenPI()
(1) Same as in GenPI.
(2) Same as in GenPI.
(3) For i = 1 to maxindex : if j 6|= i for all j < i and either j 6|= i or i |= j
for every i < j  maxindex , then output i .
The proofs of termination, correctness, and completeness of IterGenPI are very similar to corresponding results for GenPI (Theorem 16), so we will omit the details. We
will instead focus on the spatial complexity of IterGenPI. The first step of IterGenPI
clearly runs in single-exponential space in ||, since deciding the satisfiability of  takes
only polynomial space in ||, and generating the elements in Dnf -4() takes at most
single-exponential space in || (refer to Lemma 14). Step 2 also uses no more than singleexponential space in ||, since each of the sets (T ) associated with a term Ti  T has
polynomial size in Ti . Finally, for Step 3, we use the above observation that the generation of a given i from its index i can be done in polynomial space in the size of the sets
118

fiPrime Implicates and Prime Implicants in Modal Logic

(T1 ), ..., (Tn ), and hence in single-exponential space in ||. This is sufficient since for
the comparisons in Step 3, we only need to keep two candidate clauses in memory at any
one time, and deciding whether one candidate clause entails another can be accomplished
in single-exponential space (since both clauses have single-exponential size in ||).
Theorem 24 Prime implicate recognition is Pspace-hard.
Proof. The reduction is simple: a formula  is unsatisfiable if and only if 3(a  a) is a
prime implicate of . This suffices as the problem of checking the unsatisfiability of formulae
in K is known to be Pspace-complete.
We will need the following two lemmas for Theorem 25:
Lemma 25.1 Let  be a formula from K, and let  = 1  ...  k  31  ...  3m 
21  ...  2n (j propositional literals) be a non-tautologous clause. Suppose furthermore
that there is no literal l in  such that    \ {l}. If   (), then 1  ...  k 
(  ( \ {1 , ..., k })) and 3(1  ...  n )  (  ( \ {31 , ..., 3m })) and for every
i, 2(i  1  ...  m )  (  ( \ {2i })).
Proof. We will prove the contrapositive: if 1  ...  k 6 (  ( \ {1 , ..., k })) or
3(1  ...  n ) 6 (  ( \ {31 , ..., 3m })) or there is some i for which 2(i  1 
...  m ) 6 (  ( \ {2i })), then  6 (). We will only consider the case where
 |=  because if  6|=  then we immediately get  6 ().
Let us first suppose that 1 ...k 6 ((\{1 , ..., k })). Since  |= , we must also
have (\{1 , ..., k }) |= 1 ...k , so 1 ...k is an implicate of (\{1 , ..., k }).
As 1  ...  k is known not to be a prime implicate of   ( \ {1 , ..., k }), it follows that
there must be some clause  such that   ( \ {1 , ..., k }) |=  |= 1  ...  k 6|=  . Now
consider the clause  =   31  ...  3m  21  ...  2n . We know that  |=  since
  ( \ {1 , ..., k }) |=  , and that  |=  because  |= 1  ...  k . We also have  6|= 
since  must be equivalent to a propositional clause (by Theorem 2) and the propositional
part of  (namely 1  ...  k ) does not imply  . It follows then that  |=  |=  6|=  ,
so  6 ().
Next suppose that 3(1  ...  n ) 6 (  ( \ {31 , ..., 3m })). Now 3(1  ...  n )
must be an implicate of   ( \ {31 , ..., 3m }) since we have assumed that  |= .
As 3(1  ...  n ) is not a prime implicate of   ( \ {31 , ..., 3m }), it follows that
there is some  such that   ( \ {31 , ..., 3m }) |=  |= 3(1  ...  n ) 6|=  . Let
 = 1 ...k  21 ...2n . Because of Theorem 2, we know that  is a disjunction
of 3-literals, so according to Theorem 3 we must have  6|=  since 3(1  ...  n ) 6|=  .
We also know that  |=  since   ( \ {31 , ..., 3m }) |=  and that  |=  since
 |= 3(1  ...  n ). That means that  |=  |=  6|=  , so  6 ().
Finally consider the case where there is some i for which 2(i  1  ...  m ) 6
(  ( \ {2i })). We know that  |=  and hence that   ( \ {2i }) |= 2i .
Moreover, since ( \ {2i }) |= 3j for all j, we have   ( \ {2i }) |= 2(i 
1  ...  m ). Thus, if 2(i  1  ...  m ) 6 (  ( \ {2i })), it must mean
that there is some  such that   ( \ {2i }) |=  |= 2(i  1  ...  m ) 6|=  .
By assumption,  is not a tautology, so 2(i  1  ...  m ) cannot be a tautology
119

fiBienvenu

either. As  |= 2(i  1  ...  m ) and 2(i  1  ...  m ) is not a tautology,
it follows from Theorem 2 that  is equivalent to some formula 21  ...  2p . Let
 = 1  ...  k  31  ...  3m  21  ...  2i1  (21  ...  2p )  2i+1  ...  2n .
As   ( \ {2i }) |= 21  ...  2p , it must be the case that  |=  . Also, we know
that there can be no j such that i |= j  1  ...  m because otherwise we would have
i  1  ...  m |= j and hence 2(i  1  ...  m ) |= 21  ...  2p . Similarly,
there can be no k 6= i such that 2i |= 2(k  1  ...  m ) because this would mean
that    \ {2i }, contradicting our assumption that there are no superfluous disjuncts
in . It follows then by Theorem 3 that  6|=  . Thus,  |=  |=  6|=  , which means
 6 ().
Lemma 25.2 Let  be a formula of K, and let  = 1 ...k 31 ...3m 21 ...2n
(j propositional literals) be a non-tautologous clause. Suppose furthermore that there is no
literal l in  such that    \ {l}. Then if  6 (), either 1  ...  k 6 (  ( \
{1 , ..., k })) or 3(1  ...  m ) 6 (  (1  ...  k  2(1  1  ...  m )  ...  2(n 
1  ...  m ))) or 2(i  1  ...  m ) 6 (  ( \ {2i })) for some i.
Proof. We will only consider the case where  |=  because if  6|=  then we immediately
get the result. Suppose then that  6 () and  |= . By Definition 7, there must be some
 = 1  ...o  31  ...  3p  21  ...  2q such that  |=  |=  6|=  . Since  6|=  ,
by Proposition 3 we know that either 1  ...  k 6|= 1  ...  o or 1  ...  m 6|= 1  ...  p
or there is some i for which i 6|= j  1  ...  p for all j.
We begin with the case where 1  ...  k 6|= 1  ...  o . As  |= , by Theorem 3,
1 ...p |= 1 ...m and for every i there is some j such that i |= 1 ...m j . It
follows then (also by Theorem 3) that  |=  |= 1 ...o 31 ...3m 21 ...2n ,
and hence that (\{1 , ..., k }) |= 1 ...o . As 1 ...o |= 1 ...k 6|= 1 ...o ,
we have found an implicate of   ( \ {1 , ..., k }) which is stronger than 1  ...  k , so
1  ...  k 6 (  ( \ {1 , ..., k })).
Next suppose that 1 ...m 6|= 1 ...p . As  |= , it follows from Theorem 3 that

1 ...o |= 1 ...k and that for every i there is some j such that i |= 1 ...m j .
We thereby obtain  |=  |= 1  ...  k  31  ...  3p  2(1  1  ...  m )  ... 
2(n  1  ...  m ). From this, we can infer that   (1  ...  k  2(1  1  ... 
m )  ...  2(n  1  ...  m )) |= 31  ...  3p |= 31  ...  3m 6|= 31  ...  3p .
As 31  ...  3m  3(1  ...  m ), it follows that 3(1  ...  m ) 6 (  (1  ... 
k  2(1  1  ...  m )  ...  2(n  1  ...  m ))).
Finally suppose that i 6|= j  1  ...  p for all j and furthermore that 1  ...  m |=

1  ...  p (we have already shown the result holds when 1  ...  m 6|= 1  ... 
p ). Now 2(i  1  ...  m ) is an implicate of   ( \ {2i })) so to show that
2(i  1  ...  m ) is not a prime implicate of   ( \ {2i })), we must find some
stronger implicate. Consider the set S = {s  {1, ..., q} : s |= i  1  ...  m and s 6|=
k 1 ...m for k 6= i}. We note that there must be at least one element in S as we have
assumed  6|=  \ {2i }. Now since 1  ...  o |= 1  ...  k , 1  ...  p |= 1  ...  m, for
...  m , and s |= s for s  S, we
every s 6 S there is some r 6= i such that sW|= r  1 W

get  |=  W
|= 1 ...k 31 ...3m ( j6=i 2j )( W
sS 2s ). It follows that (\

{2i }) |= sS 2(s 1 ...m ), which means that sS 2(s 1 ...m ) is an
120

fiPrime Implicates and Prime Implicants in Modal Logic

W
implicate of (\{2i }). Moreover, sS 2(s 1 ...m ) |= 2(i 1 ...m )
since by construction s |= i  1  ...  m for every s  S.
W
It remains to be shown that 2(i  1  ...  m ) 6|= sS 2(s  1  ...  m ).
Suppose
for a contradiction that the contrary holds. Then 2(i  1  ...  m ) |=
W

sS 2(s  1  ...  m ), so by Theorem 1, there must be some s  S for which
i  1  ...  m |= s  1  ...  m . But then i |= s  1  ...  m , and thus
i |= s  1  ...  p since we have assumed 1  ...  m |= 1  ...  p . This contradicts
our earlier assumption that W
i 6|= j  1  ...  p for all j. Thus, we have shown that
2(i  1  ...  m ) 6|= sS 2(s  1  ...  m ), so 2(i  1  ...  m ) 6
(  ( \ {2i })).
Theorem 25 Let  be a formula of K, and let  = 1 ...k 31 ...3n 21 ...2m
(j propositional literals) be a non-tautologous clause such that (a) i  i  1  ...  n
for all i, and (b) there is no literal l in  such that    \ {l}. Then   () if and only
if the following conditions hold:
1. 1  ...  k  (  ( \ {1 , ..., k }))
2. 2(i  1  ...  n )  (  ( \ {2i })) for every i
3. 3(1  ...  n )  (  ( \ {31 , ..., 3n }))
Proof. The forward direction was shown in Lemma 25.1. The other direction follows from
Lemma 25.2 together with the hypothesis that i  i  1  ...  n for all i (which
ensures that   (1  ...  k  2(1  1  ...  m )  ...  2(n  1  ...  m )) 
  ( \ {31 , ..., 3n })).
Theorem 26 Let  be a formula of K, and let  be a non-tautologous propositional clause
such that  |=  and such that there is no literal l in  such that    \ {l}. Then   ()
if and only if  6|=  \ {l} for all l in .
Proof. Consider a formula  and a non-tautologous propositional clause  such that  |= 
and such that there is no literal l in  such that    \ {l}. Suppose that  |=  \ {l}
for some l in . As we know that  6  \ {l}, it follows that  \ {l} is an implicate of
 which is strictly stronger than , so  is not a prime implicate of . For the other
direction, suppose that  6 (). Then it must be the case that there is some clause 
such that  |=  |=  6|= . Since  |= , it follows from Theorem 2 that each literal in 
is a propositional literal of  or is inconsistent. If all of the literals in  are inconsistent,
then both  and  must be inconsistent, so clearly  |=  \ {l} for every l in . Otherwise,
 is equivalent to a propositional clause, and more specifically to a propositional clause
containing only those literals appearing in  (since  |= ). As  is strictly stronger than ,
there must be some literal l in  which does not appear in . But that means  |=  \ {l}
and so  |=  \ {l}, completing the proof.
Theorem 27 Let  be a formula of K, and let  = 2 be a non-tautologous clause such
that  |= . Then   () if and only if there exists some term T Dnf-4() such that
 |= T , where T is the conjunction of formulae  such that 2 is in T .
121

fiBienvenu

Proof. Let  be some formula, and let  = 2 be a non-tautologous clause such that  |= .
For the first direction, suppose that there is no term T Dnf -4() such that  |= T , where
T is the conjunction of formulae  such that 2 is in T . There are two cases: either there
are no terms in Dnf -4() because  is unsatisfiable, or there are terms but none satisfy the
condition. In the first case, 2 is not a prime implicate of , since any contradictory
clause
W
(e.g. 3(a  a)) is stronger. In the second case, consider the clause  = T 2T , where
T is the conjunction of formulae  such that 2 is in T . Now for every T we must
W have
2T |= 2, otherwise we would have T 6|= 2, and hence
W  6|= 2. Moreover,  |= T 2T
since T |= 2T for every T . But by Theorem 1, 2 6|= T 2T since  6|= T for all T . So
we have  |=  |=  6|=  , which means that  is not a prime implicate of .
For the other direction, suppose that 2 is not a prime implicate of  and that  6|= .
Then
must have T |= 2 for all T Dnf -4(),
W Dnf -4() is non-empty. As  |= 2, we W
so T 2T also implies 2. We now show
. We let
W that T 2T is a primeWimplicate of W
 be some implicate of  which implies T 2T . Now since  |= T 2T and T 2T is
non-tautologous, it follows from Theorem 2 that   21  ...  2n for some formulae i .
As  |= , we must have T |= 21  ...  2n for all T W
Dnf -4(). But that can only
Wbe the
case if 2T |= 21  ...  2n for all T , which W
means T 2T |= 21  ...  2n . As T 2T
implies every implicate W
of  that implies it, T 2T must be a prime implicate of . But
this means that 2 6|= T 2T , since we have assumed that 2 is not a prime implicate
of . It follows from Theorem 1 that  6|= T for all T Dnf -4().
In order to show Theorem 28 we will need the following lemmas:
Lemma 28.1 If 3 is an implicate of  which is not a prime implicate, the algorithm
Test3PI returns no on input (3, ).
Proof. Suppose that 3 is not a prime implicate of . If  is unsatisfiable, then  must
be satisfiable, so we will return no in the first step. If  is satisfiable, then since we have
assumed that 3 is an implicate of , there must be some clause  such that  |=  |= 3
but 3 6|= . As  |= 3, it follows from Theorem 2 that  is equivalent to a disjunction
of 3-formulae, and hence to some clause 3  .
We know from Lemma 13 that  is equivalent to the disjunction of terms in Dnf -4().
It must thus be the case that Ti |= 3  for all Ti  Dnf -4(). Since each Ti is a satisfiable
conjunction of propositional literals and 2- and 3-formulae, it follows that there exists a set
{3i , 2i,1 , ..., 2i,k(i) } of conjuncts of Ti such that 3(i i,1 ...i,k(i) ) |= 3  , otherwise
Ti would fail to imply 3  . Moreover, all of the elements of {3i , 2i,1 , ..., 2i,k(i) } must
appear in the NNF of  outside modal operators, so the formulae i , i,1 , ..., i,k(i) must all
be elements of the set X . It is immediate that both
3

_
(i  i,1  ...  i,k(i) ) |= 3  |= 3
i

and
3 6|= 3

_

(i  i,1  ...  i,k(i) )

i

122

(3)

fiPrime Implicates and Prime Implicants in Modal Logic

W
The latter implies that the formula 3  (3 i (i  i,1  ...  i,k(i) )) must be consistent,
which means that
_
^
  ( (i  i,1  ...  i,k(i) ))    (i  i,1  ...  i,k(i) )
i

i

must be consistent as well. But then itVmust be the case that we can select for each i some
i  {i , i,1 , ..., i,k(i) } such that   i i is consistent. Let S be the set of i . The set
S satisfies the condition of the algorithm since:
 SX
  6|=

W

S

 (because we know  

V

i

i to be consistent)

 for each Ti  Dnf -4(), the conjuncts 3i , 2i,1 , ..., 2i,k(i) of Ti are such that:
 {i , i,1 , ..., i,k(i) }  S 6=  (since S contains i  {i , i,1 , ..., i,k(i) })
 3(i  i,1  ...  i,k(i) ) |= 3 (follows from (3) above)
Since there exists a set S  X satisfying these conditions, the algorithm returns no.
Lemma 28.2 If the algorithm Test3PI returns no on input (3, ), then 3 is not a
prime implicate of .
Proof. Suppose Test3PI returns no on input (3, ). If this happens during the first
step, it must be the case that  is unsatisfiable and 3 is unsatisfiable, in which case 3 is
not a prime implicate of . The other possibility is that the algorithm returns no in Step 3,
which means W
there must be some S  X satisfying:
(a)  6|= S 
(b) for each Ti  Dnf -4(), there exist conjuncts 3i , 2i,1 , ..., 2i,k(i) of Ti
such that:
(i) {i , i,1 , ..., i,k(i) }  S 6= 
(ii) 3(i  i,1  ...  i,k(i) ) |= 3
W
Let  be the clause i 3(i  i,1 W
 ...  i,k(i)
W ). We remark that for each Ti , we have Ti |=
3(i i,1 ...i,k(i) ), and hence
T
|=
i
i
i 3(i i,1 ...i,k(i) ).
W
W From the definition of
Dnf -4(), we also have   i Ti . It immediately follows that  |= i 3(i i,1 ...i,k(i) )
and hence W
 |= . From 2 (b) (ii), we have that 3(i  i,1  ...  i,k(i) ) |= 3 for every i,
and hence i 3(i  i,1  ...  i,k(i)) |= 3 which yields  |= 3. From 2 (b) (i), we have
that {i , i,1 , ..., i,k(i) }  S 6=  and hence that for every
i there is some   S W
such that
W
i  i,1  ... i,k(i)W|= . From this we can infer that i 3(i  i,1  ... i,k(i) ) |= W
S 3,
and hence  |= 3 S . But we know from 2 (a) and Theorem 1 that 3 6|= 3 S .
It follows then that 3 6|= . Putting all this together, we see that there exists a clause 
such that  |=  |= 3 but 3 6|= , and hence that 3 is not a prime implicate of .
Theorem 28 Let  be a formula, and let 3 be an implicate of . Then the algorithm
Test3PI returns yes on input (3, ) if and only if 3 is a prime implicate of .
123

fiBienvenu

Proof. It is clear that Test3PI terminates since unsatisfiability testing and the NNF transformation always terminate, and there are only finitely many S and Ti . Lemmas 28.1 and
28.2 show us that the algorithm always gives the correct response.
Theorem 29 The algorithm Test3PI runs in polynomial space.
Proof. We remark that the sum of the lengths of the elements in X is bounded by the
length of the formula Nnf(), and hence by Lemma 14 the sum of theW lengths of the
elements of a particular S  X cannot exceed 2||. Testing whether  6|= S  can thus
be accomplished in polynomial V
space in the length of  and  as it involves testing the
satisfiability of the formula   S  whose length is clearly polynomial in  and .
Now let us turn to Step 3 (b). We notice that it is not necessary to keep all of the Ti in
memory at once, since we can generate the terms Ti one at a time using only polynomial
space by Lemma 12. By Lemma 14, the length of any Ti in Dnf -4() can be at most 2||.
It follows that checking whether {i , i,1 , ..., i,k(i) }  S 6= , or whether 3(i  i,1  ... 
i,k(i) ) |= 3 can both be accomplished in polynomial space in the length of  and . We
conclude that the algorithm Test3PI runs in polynomial space.
In order to show Theorem 32, we use the following lemmas:
Lemma 32.1 If  is a clause that is not a prime implicate of , then TestPI outputs no
on this input.
Proof. Let us begin by considering a formula  which is a clause but that is not a prime
implicate of . There are two possible reasons for this: either  is not an implicate of , or
it is an implicate but there exists some stronger implicate. In the first case, TestPI returns
no in Step 1, as desired. We will now focus on the case where  is an implicate but not a
prime implicate. We begin by treating the limit cases where one or both of  and  is a
tautology or contradiction. Given that we know  to be a non-prime implicate of , there
are only two possible scenarios: either 6|=  and |= , or  |=  and  6|= . In both cases,
the algorithm returns no in Step 2.
If  is an implicate of , and neither  nor  is a tautology or contradiction, then the
algorithm will continue on to Step 3. In this step, any redundant literals will be deleted
from , and if  contains 3-literals, we add an extra disjunct to the 2-literals so that 
satisfies the syntactic requirements of Theorem 25. Let 1  ...k  31  ...  3m  21 
...  2n be the clause  at the end of Step 3 once all modifications have been made. As
the transformations in Step 3 are equivalence-preserving (Theorem 1), the modified  is
equivalent to the original, so  is still a non-tautologous non-prime implicate of . This
means  and  now satisfy all of the conditions of Theorem 25. It follows then that one of
the following holds:
(a) 1  ...  k 6 (  ( \ {1 , ..., k })
(b) 2(i  1  ...  n ) 6 (  ( \ {2i })) for some i
(c) 3(1  ...  n ) 6 (  ( \ {31 , ..., 3n }))
124

fiPrime Implicates and Prime Implicants in Modal Logic

Suppose that (a) holds. Now 1  ...  k is a non-tautologous propositional clause implied
by   ( \ {1 , ..., k }) which contains no redundant literals. This means that   ( \
{1 , ..., k }) and 1 ...k satisfy the conditions of Theorem 26. According to this theorem,
as 1  ...  k 6 (  ( \ {1 , ..., k }), then there must be some j such that   ( \
{1 , ..., k }) |= 1  ...  j1  j+1  ...  k . This means that  |=  \ {j }, so the algorithm
returns no in Step 4.
Suppose next that (b) holds, and let i be such that 2(i  1  ...  n ) 6 ( 
( \ {2i })). By Theorem 27, this means that there is no T Dnf -4() such that
2(i  1  ...  n ) entails the conjunction of 2-formulae conjuncts of T . It follows that
the algorithm returns no in Step 5.
Finally consider the case
Then in Step 6,
Wm where neither (a) nor (b) holds but (c)Wdoes.
m
we will call Test3PI(3( i=1 i ),   ( \ {31 , ..., 3m })). As 3( i=1 i ) is not a prime
implicate of (\{31 , ..., 3m })) and we have shown Test3PI to be correct (Theorem
28), Test3PI will return no, so TestPI will return no as well. As we have covered each of
the possible cases, we can conclude that if  is a clause that is not a prime implicate of ,
then TestPI outputs no.
Lemma 32.2 If TestPI outputs no with input (, ) and  is a clause, then  is not a
prime implicate of .
Proof. There are 5 different ways for TestPI to return no (these occur in Steps 1, 2, 4, 5,
and 6). Let us consider each of these in turn. The first way that the algorithm can return
no is in Step 1 if we find that  6|= . This is correct since  cannot be a prime implicate if
it is not a consequence of . In Step 2, we return no if  is unsatisfiable but  is not, or if
 is a tautology but  is not. This is also correct since in both cases  cannot be a prime
implicate since there exist stronger implicates (any contradictory clause if   , and any
non-tautologous implicate of  if   ). In Step 3, we may modify , but the resulting
formula is equivalent to the original, and so it is a prime implicate just in the case that the
original clause was. Let 1  ...k  31  ...  3m  21  ...  2n be the clause at the end
of Step 3. Now in Step 4, we return no if we find some propositional literal l in  for which
 |= \{l}. Now since in Step 3, we have removed redundant literals from , we can be sure
that  \ {l} is strictly stronger than . So we have  |=  \ {l} |=  and  6|=  \ {l}, which
means that  is not a prime implicate of . We now consider Step 5 of TestPI. In this step,
we return no if for some disjunct 2i there is no term T in Dnf -4((\{2i })) for which
2(i  1  ...  m ) entails the conjunction of 2-literals in T . According to Theorem 27,
this means that 2(i  1  ...  m ) is not a prime implicate of   ( \ {2i }), which
means that  is not a prime implicate
W of  by Theorem 25. . In this step, we return no
if Test3PI returns no on input (3( ki=1 i ),   ( \ {31 , ..., 3m })). By Theorem 28,
W
we know that this happens just in the case that 3( ki=1 i ) is not a prime implicate of
  ( \ {31 , ..., 3m }). It follows from Theorem 25 that  is not a prime implicate
of .
Theorem 32 The algorithm TestPI always terminates, and it returns yes on input (,
) if and only if  is a prime implicate of .
125

fiBienvenu

Proof. The algorithm TestPI clearly terminates because Steps 1 to 5 involve a finite number
of syntactic operations on  and a finite number of entailment checks. Moreover, the call
to Test3PI in Step 6 is known to terminate (Theorem 28). Correctness and completeness
have already been shown in Lemmas 32.1 and 32.2.
We make use of the following lemma in the proof of Theorem 34:
Lemma 34.1 The algorithm TestPI provided in Figure 5 runs in polynomial space in the
length of the input.
Proof. It is clear that steps 1 through 5 can be carried out in polynomial space in the length
of the input, since they simply involve testing the satisfiability of formulae whose lengths are
polynomial in ||+||. Step 6 can also
W be carried out in polynomial space since by Theorem
29 deciding whether the formula 3(W m
i=1 i ) is a prime implicate of   ( \ {1 , ..., m }))
takes only polynomial space in |3( m
i=1 i )| + |  ( \ {31 , ..., 3m }))|, and hence in
|| + ||. We can thus conclude that the algorithm TestPI runs in polynomial space in the
length of the input.
Theorem 34 Prime implicate recognition is in Pspace.
Proof. We have show in Theorem 32 that TestPI always terminates and returns yes whenever the clause is a prime implicate and no otherwise. This means that TestPI is a decision
procedure for prime implicate recognition. Since the algorithm has been shown to run in
polynomial space (Lemma 34.1), we can conclude that prime implicate recognition is in
Pspace.
Corollary 35 Prime implicate recognition is Pspace-complete.
Proof. Follows directly from Theorems 24 and 34.

References
Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2006). Distributed
reasoning in a peer-to-peer setting: Application to the semantic web. Journal of
Artificial Intelligence Research, 25, 269314.
Baader, F., McGuiness, D. L., Nardi, D., & Patel-Schneider, P. (Eds.). (2003). The Description Logic Handbook. Cambridge University Press.
Bienvenu, M. (2007). Prime implicates and prime implicants in modal logic. In Proceedings
of the Twenty-Second Conference on Artificial Intelligence (AAAI07), pp. 397384.
Bienvenu, M. (2008). Prime implicate normal form for ALC concepts. In Proceedings of the
Twenty-Third Conference on Artificial Intelligence (AAAI08), pp. 412417.
Bienvenu, M. (2009). Consequence Finding in Modal Logic. Ph.D. thesis, Universite de
Toulouse.
Bittencourt, G. (2007). Combining syntax and semantics through prime form representation. Journal of Logic and Computation, 18 (1), 1333.
126

fiPrime Implicates and Prime Implicants in Modal Logic

Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal logic. Cambridge University
Press.
Blackburn, P., van Benthem, J., & Wolter, F. (Eds.). (2006). Handbook of Modal Logic.
Elsevier.
Brandt, S., & Turhan, A. (2002). An approach for optimized approximation. In Proceedings
of the KI-2002 Workshop on Applications of Description Logics (KIDLWS01).
Cadoli, M., & Donini, F. M. (1997). A survey on knowledge compilation. AI Communications, 10 (3-4), 137150.
Cialdea Mayer, M., & Pirri, F. (1995). Propositional abduction in modal logic. Logic
Journal of the IGPL, 3 (6), 907919.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial
Intelligence Research, 17, 229264.
de Kleer, J., Mackworth, A. K., & Reiter, R. (1992). Characterizing diagnoses and systems.
Artificial Intelligence, 56, 197222.
Donini, F. M. (2003). The Description Logic Handbook, chap. Complexity of Reasoning.
Cambridge University Press.
Donini, F. M., Lenzerini, M., Nardi, D., Hollunder, B., Nutt, W., & Marchetti Spaccamela,
A. (1992). The complexity of existential qualification in concept languages. Artificial
Intelligence, 53, 309327.
Eiter, T., & Makino, K. (2002). On computing all abductive explanations. In Proceedings of
the Eighteenth National Conference on Artificial Intelligence (AAAI02), pp. 6267.
Enjalbert, P., & Farinas del Cerro, L. (1989). Modal resolution in clausal form. Theoretical
Computer Science, 65 (1), 133.
Garey, M. R., & Johnson, D. S. (1979). Computers and intractability. A guide to the theory
of NP-completeness. W. H. Freeman.
Ghilardi, S., Lutz, C., & Wolter, F. (2006). Did I damage my ontology? A case for conservative extensions in description logics. In Proceedings of the Tenth International
Conference on Principles of Knowledge Representation and Reasoning (KR06), pp.
187197.
Giunchiglia, F., & Sebastiani, R. (1996). A SAT-based decision procedure for ALC. In
Proceedings of the Fifth International Conference on Principles of Knowledge Representation and Reasoning (KR96), pp. 304314.
Kusters, R., & Molitor, R. (2002). Approximating most specific concepts in logics with
existential restrictions. AI Communications, 15 (1), 4759.
Ladner, R. (1977). The computational complexity of provability in systems of modal propositional logic. SIAM Journal of Computing, 6 (3), 467480.
Lakemeyer, G. (1995). A logical account of relevance. In Proceedings of the Fourteenth
International Joint Conference on Artificial Intelligence (IJCAI95), pp. 853861.
127

fiBienvenu

Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formulavariable independence and forgetting. Journal of Artificial Intelligence Research, 18,
391443.
Marquis, P. (1991a). Contribution a letude des methodes de construction dhypotheses en
intelligence artificielle. In french, Universite de Nancy I.
Marquis, P. (1991b). Extending abduction from propositional to first-order logic. In Proceedings of Fundamentals of Artificial Intelligence Research Workshop, pp. 141155.
Marquis, P. (2000). Handbook on Defeasible Reasoning and Uncertainty Management Systems, Vol. 5, chap. Consequence Finding Algorithms, pp. 41145. Kluwer.
Pagnucco, M. (2006). Knowledge compilation for belief change. In Proceedings of the
Nineteenth Australian Conference on Artificial Intelligence (AI06), pp. 9099.
Papadimitriou, C. (1994). Computational Complexity. Addison Welsey.
Przymusinski, T. (1989). An algorithm to compute circumscription. Artificial Intelligence,
38 (1), 4973.
Ramesh, A., & Murray, N. (1994). Computing prime implicants/implicates for regular logics.
In Proceedings of the Twenty-Fourth IEEE International Symposium on MultipleValued Logic, pp. 115123.
Schild, K. (1991). A correspondence theory for terminological logics: Preliminary report.
In Proceedings of the Twelth International Joint Conference on Artificial Intelligence
(IJCAI91), pp. 466471.
Younger, D. H. (1967). Recognition and parsing of context-free languages in time n3 .
Information and Control, 10 (2), 189208.

128

fi