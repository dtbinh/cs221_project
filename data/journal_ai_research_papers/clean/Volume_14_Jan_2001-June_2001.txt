Journal of Artificial Intelligence Research 14 (2001) 105{136

Submitted 3/00; published 4/01

Partial-Order Planning with Concurrent Interacting Actions
Craig Boutilier

cebly@cs.toronto.edu

Department of Computer Science
University of Toronto
Toronto, ON, M5S 3H8, Canada

Ronen I. Brafman

brafman@cs.bgu.ac.il

Department of Computer Science
Ben-Gurion University
Beer Sheva, Israel 84105

Abstract

In order to generate plans for agents with multiple actuators, agent teams, or distributed controllers, we must be able to represent and plan using concurrent actions with
interacting effects. This has historically been considered a challenging task requiring a
temporal planner with the ability to reason explicitly about time. We show that with
simple modifications, the
action representation language can be used to represent
interacting actions. Moreover, algorithms for partial-order planning require only small
modifications in order to be applied in such multiagent domains. We demonstrate this fact
by developing a sound and complete partial-order planner for planning with concurrent interacting actions, POMP, that extends existing partial-order planners in a straightforward
way. These results open the way to the use of partial-order planners for the centralized
control of cooperative multiagent systems.
STRIPS

1. Introduction
In order to construct plans for agents with multiple actuators (such as multi-armed robots),
agent teams, or controllers distributed throughout an environment, we must be able to
model the effects and interactions of multiple actions executed concurrently, and generate plans that take these interactions into account. A viable solution to the basic
multiagent/multi-actuator planning (MAP) problem must include economical action descriptions that are convenient to specify and are easily manipulable by planning algorithms,
as well as planning methods that can deal with the interactions generally associated with
concurrent actions.
Surprisingly, despite the interest in multiagent applications|for instance, in robotics
(Donald, Jennings, & Rus, 1993; Khatib, Yokoi, Chang, Ruspini, Holmberg, Casal, &
Baader, 1996) and distributed AI (e.g., see the various proceedings of the International
Conference on Multiagent Systems)|and the large body of work on distributed multiagent
planning, very little research addresses this basic problem of planning in the context of
concurrent interacting actions. Researchers in distributed AI have considered many central
issues in multiagent planning and multiagent interaction, but much existing research is
concerned mainly with problems stemming from the distributed nature of such systems,
such as task decomposition and resource allocation (Durfee & Lesser, 1989; Wilkins &
Myers, 1998; Stone & Veloso, 1999), obtaining local plans that combine to form global plans

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBoutilier & Brafman

(Durfee & Lesser, 1991; Ephrati, Pollack, & Rosenschein, 1995), minimizing communication
needs (Wolverton & des Jardins, 1998; Donald et al., 1993), and so on. As opposed to this
form of distributed planning, our focus in this paper is on centralized planning for agent
teams (or distributed actuators).
Representation of concurrent actions has been dealt with by various researchers in the
knowledge-representation community (e.g., Lin & Shoham, 1992; Reiter, 1996; de Giacomo,
Lesperance, & Levesque, 1997; Moses & Tennenholtz, 1995; Pinto, 1998). Of particular
note are the action languages Ac (Baral & Gelfond, 1997) and C (Giunchiglia & Lifschitz,
1998) which enable the specification of concurrent interacting actions and employ a nonmonotonic override mechanism to deduce the effects of a set of actions with conicting
effects. Finally, a number of contemporary planners can handle concurrent noninteracting
actions to a certain degree|examples include Graphplan (Blum & Furst, 1995), and IPP
(Koehler, 1998), which extends Graphplan to handle resource constraints, and more recently
OBDD-based planners such as MBP (Cimatti, Giunchiglia, Giunchiglia, & Traverso, 1997)
and UMOP (Jensen & Veloso, 2000)|while Knoblock (1994) provides a good discussion of
the issue of parallelizing serial plans.
Despite these advances, one often sees in the planning community suggestions that
temporal planners are required to adequately deal with concurrent interacting actions. For
example, in his discussion of parallel execution plans, Knoblock (1994) asserts:
To handle these cases [of interacting actions] requires the introduction of an
explicit representation of time, such as that provided in temporal planning systems.
A similar perspective seems implicit in the work on parallel action execution presented by
Lingard and Richards (1998). Certainly time plays a role in planning|in any planner the
idea that sequences of actions occur embodies an implicit notion of time. However, we
disagree that time in centralized multiagent planning must be dealt with in a more explicit
fashion than in single-agent planning. The main aim of this paper is to demonstrate that the
MAP problem can be solved using very simple extensions to existing (single-agent) planners
like UCPOP (Penberthy & Weld, 1992). We provide a representation and MAP algorithm
that requires no explicit representation of time. This is not to deny that explicit temporal
representations are useful in planning|for many problems these may be necessary|but
we do not think this is the key bottleneck in planning the activities of multiagent teams.
Specifically, we view temporal issues to be orthogonal to the main concerns facing multiagent
planning.
The central issue in multiagent planning lies in the fact that individual agent actions
do interact. Sometimes planning is hindered as a result of action interaction: action X of
agent 1 might destroy the intended effect of action Y of agent 2 if executed concurrently. For
example, in a half-duplex communication line, we cannot allow simultaneous transmission
of messages from both sides. In such a case, a planning algorithm has to make sure that
X and Y are not executed at the same time. More interesting is the fact that planning
often benefits as a result of action interaction: action X of agent 1 might only achieve an
intended effect if agent 2 performs action Y concurrently. For example, opening a typical
door requires two simultaneous actions: turning the knob and pushing the door. In military
activities, different units may have to coordinate their actions in order to be effective (e.g.,
106

fiPlanning with Concurrent Interacting Actions

turn on engines or lights simultaneously, or attack at the same time). Similar situations
arise in a variety of domains. In such cases, a planning algorithm has to ensure that the
appropriate actions are executed at the same time. An action representation that makes
these interactions explicit and a planning algorithm that can, as result of these interactions,
prescribe that certain actions must or must not be executed concurrently are some of the
main features of any multiagent planner. Temporal representations may play a role in the
scheduling of such actions, but are not strictly necessary for reasoning about the effects of
interaction (or lack thereof).
To illustrate some of these issues, consider the following example which will be discussed
in detail later in the paper: two agents must move a large set of blocks from one room to
another. While they could pick up each block separately, a better solution would be to use
an existing table in the following manner. First, the agents put all blocks on the table,
then they each lift one side of the table. However, they must lift the table simultaneously;
otherwise, if only one side of the table is lifted, all the blocks will fall off. Having lifted
the table, they must move it to the other room. There they put the table down. In fact,
depending on the precise goal and effects of actions, it may be better for one agent to drop
its side of the table, causing all of the blocks to slide off at once. Notice how generating
this plan requires the agents to coordinate in two different ways: first, they must lift the
table together so that the blocks do not fall; later, one of them (and only one) must drop
its side of the table to let the blocks fall.
Since the actions of distinct agents interact, we cannot, in general, specify the effects of
an individual's actions without taking into account what other actions might be performed
by other agents at the same time. That truly concurrent actions are often desirable precludes
the oft-used trick of \interleaving semantics" (Reiter, 1996; de Giacomo et al., 1997). Agents
lifting a table on which there are a number of items must do so simultaneously or risk the
items sliding from the table, perhaps causing damage. Interleaving individual \lift my side
of table" actions will not do.
One way to handle action interactions is to specify the effects of all joint actions directly.
More specifically, let Ai be the set of actions available to agent i (assuming n agents labeled
1 : : :n), and let the joint action space be A1  A2      An . We treat each element of this
space as a separate action, and specify its effects using our favorite action representation.1
The main advantage of this reduction scheme is that the resulting planning problem can
be tackled using any standard planning algorithm. However, it has some serious drawbacks
with respect to ease of representation. First, the number of joint actions increases exponentially with the number of agents. This has severe implications for the specification and
planning process. Second, this reduction fails to exploit the fact that a substantial fraction of the individual actions may not interact at all, or at least not interact under certain
conditions. We would like a representation of actions in multiagent/multi-actuator settings
that exploits the independence of individual action effects to whatever extent possible. For
instance, while the lift actions of the two agents may interact, many other actions will not
(e.g., one agent lifting the table and another picking up a block). Hence, we do not need
1. Our discussion will center on the STRIPS action representation, but similar considerations apply to other
representations such as the situation calculus (McCarthy & Hayes, 1969; Reiter, 1991) and dynamic Bayes
nets (Dean & Kanazawa, 1989; Boutilier & Goldszmidt, 1996).

107

fiBoutilier & Brafman

to explicitly consider all combinations of these actions, and can specify certain individual
effects separately, combining the effects \as needed."
Joint actions also cause problems for the planning process itself: their use in the context
of most planners forces what seems to be an excessive degree of commitment. Whenever
the individual action of some agent can accomplish a desired effect, we must insert into our
plan a joint action, thereby committing all other agents to specific actions to be executed
concurrently, even though the actual choices may be irrelevant. For these reasons, we desire
a more \distributed" representation of actions.
We are therefore faced with the following two problems:
1. The representation problem: how do we naturally and concisely represent interactions
among concurrently executed actions.
2. The planning problem: how do we plan in the context of such a representation.
In this paper, we show how the STRIPS action representation can be augmented to
handle concurrent interacting actions and how existing nonlinear planners can be adapted
to handle such actions. In fact, it might come as a surprise that solving both problems
requires only a small number of changes to existing nonlinear planners, such as UCPOP
(Penberthy & Weld, 1992).2 The main addition to the STRIPS representation for action a
is a concurrent action list: this describes restrictions on the actions that can (or cannot) be
executed concurrently in order for a to have the specified effect (indeed, a can have a number
of different conditional effects depending on which concurrent actions are applied). In order
to handle this richer language, we must make a number of modifications to \standard"
partial-order planners: (a) we add equality (respectively, inequality) constraints on action
orderings to enforce concurrency (respectively, nonconcurrency) constraints; and (b) we
expand the definition of threats to cover concurrent actions that could prevent an intended
action effect.
We emphasize that we deal with the problem of planning the activities of multiple agents
or agents with multiple actuators in a centralized fashion, as opposed to distributed planning. Our model assumes that one has available a central controller that can decide on an
appropriate joint plan and communicate this plan to individual agents (or actuators). While
distributed planning is an important and dicult problem, it is not the problem addressed
in this work. We also assume that some mechanism is available by which individual agents
can ensure that the execution of their concurrent plans are synchronized. Again, while an
issue of significance and subtlety, it is not a task we consider in this paper.
We note that planning with parallel actions has been addressed in some detail by Lingard
and Richards (1998). Specifically, they provide a very general framework for understanding
constraint-posting, least-commitment planners that allow for concurrent action execution.
However, as mentioned above, their work takes an explicit temporal view of the problem
and focuses primarily on issues having to do with action duration. Furthermore, while
multiagent planning could presumably be made to fit within their model, this seems not
to be their main motivation. In fact, the planning algorithms they discuss deal with the
issue of ensuring that parallel actions do not have negative synergistic effects, and explicitly
2. Moreover, other planning algorithms (e.g., Blum & Furst, 1995; Kautz & Selman, 1996) should prove
amenable to extension to planning with concurrent interacting actions using similar ideas.

108

fiPlanning with Concurrent Interacting Actions

exclude the possibility of positive synergy. In our work, we abstract away from the temporal
component and focus precisely on planning in the presence of such synergies, both positive
and negative.
In the following section we describe our STRIPS-style representation for concurrent,
interacting actions and multiagent plans. In Section 3 we describe the Partial-Order Multiagent Planning algorithm (POMP), a modified version of the UCPOP algorithm that can be
used to generate plans for multiagent teams or multiactuator devices. Section 4 illustrates
the POMP algorithm on an extended example. In Section 5 we discuss the soundness and
completeness of the POMP algorithm. We conclude in Section 6 with a discussion of some
issues raised by this work.

2. Representing Concurrent Actions and Plans

We begin by considering the representation of concurrent actions and partially ordered plans
using a simple extension of traditional planning representations. We first describe a standard action representation based on the STRIPS model, specifically that used by UCPOP
(Penberthy & Weld, 1992). We then describe the extension of this representation to represent concurrent interacting actions and its semantics, and finally describe the representation
and semantics of partially ordered multiagent plans.

2.1 The STRIPS Action Representation

Variants of the STRIPS action representation language (Fikes & Nilsson, 1971) have been
employed in many planning systems. We assume a finite set of predicates and domain
objects (generally typed) that characterize the domain in question. States of this system
are truth assignments to ground atomic formulae of this language. A state is represented
as a set (or conjunction) of those ground atoms true in that state, such as
f

OnTable(B 1); Holding(A; B 2)g

thus embodying the closed world assumption (Reiter, 1978). Actions induce state transitions
and can be viewed as partial mappings from states to states. An action A is represented
using a precondition and an effect, each a conjunction of literals (sometimes referred to
as the precondition or effect list). If a state does not satisfy the conjunction of literals
in the precondition list, the effect of applying the action is undefined. Otherwise, the
state resulting from performing action A is determined by deleting from the current state
description all negative literals appearing in the effect list of A and adding all positive
literals appearing in the effect list.
As an example, the action of picking up a particular block B from the oor is described
in Figure 1, using the usual LISP-style notation of many planning systems. This action can
be executed when the agent's hand is empty and block B is clear and on the oor. After
the action is executed, the agent's hand is no longer empty (it holds B ), and B is not on
the oor.
Since the action of picking up a block from a location is essentially the same, regardless
of the particular block and location, a whole class of such actions can be described using an
action schema or operator with free variables denoting the object to be picked up and the
109

fiBoutilier & Brafman

(define (action pickup-block-B-from-floor)
:precondition (and (on floor B) (handempty) (clear B))
:effect
(and (not (handempty)) (not (on floor B)) (holding B))))

Figure 1: The Pickup-block-B-from-oor action
(define (operator pickup)
:parameters
(?x ?y)
:precondition (and (on ?x ?y) (handempty) (clear ?x) (not (= ?x ?y)))
:effect
(and (not (handempty)) (not (on ?x ?y)) (holding ?x))))

Figure 2: The Pickup action schema
pickup location. An action schema specification is similar to the specification of a single
action except for the use of free variables. The precondition list of an action schema can
contain, along with predicates (or more precisely, proposition \schemata"), equality and
inequality constraints on the variables.
Figure 2 illustrates an action schema for the pickup action. It has two variables, ?x and
?y , which stand for the object being picked up and the location of the object, respectively.
The precondition list includes the requirements that ?x be on ?y , that the hand is empty,
that ?x is clear, and that ?x and ?y designate different objects (i.e., one cannot pickup an
object from atop itself).
The STRIPS representation can be enhanced, obtaining a more expressive language that
allows for a form of universal quantification in the action description (e.g., as in UCPOP
Penberthy & Weld, 1992). In addition, conditional effects can be captured using a when
clause consisting of an antecedent and a consequent. The semantics of the action description
is similar to the original semantics except that in states s that satisfy the preconditions of
the action and the antecedent of the when clause, the actual effect of the action is the union
of the \standard" effect specified in the effect list and the consequent of the when clause.
The when clause does not change the expressiveness of the language|each conditional
action description can be expressed using separate non-conditional actions in the classic
STRIPS representation to capture each when clause. However, it allows for a more economical and natural specification of actions. For example, in the classic STRIPS blocks world,
after putting some block B1 on a destination block B2 , block B2 is no longer clear. However,
after putting B1 on the table, the table remains clear. Hence, a different putdown schema is
required to describe moving a block to the table. Using a when clause, we can use a single
schema with a conditional effect that modifies the standard effect of the action in case the
destination is not the table (i.e., the when clause will state that when the destination is not
the table, it will become unclear). In addition, conditional effects may allow us to postpone
commitment during planning (e.g., we may decide to put a block down, but we don't have
to commit to whether the destination is the table or not).

2.2 Representing Concurrent Actions in STRIPS

The introduction of concurrent interacting actions requires us to address two issues specific
to the multiagent setting: who is performing the action, and what other actions are being
110

fiPlanning with Concurrent Interacting Actions

(define (operator pickup)
:parameters
(?a1 ?x ?y)
:precondition (and (on ?x ?y)(handempty ?a1) (clear ?x)(not (= ?x ?y)))
:concurrent
(not (and (pickup ?a2 ?x ?y) (not (= ?a1 ?a2))))
:effect
(and (not (handempty ?a1)) (not (on ?x ?y)) (holding ?a1 ?x)))

Figure 3: The multiagent Pickup schema
performed at the same time. First, we deal with the identity of the performing agent by
introducing an agent variable to each action schema. When the schema is instantiated, this
variable is bound to a constant denoting the particular agent that is carrying out the action.
Second, we must take into account the fact that for an action to have a particular effect,
certain actions may or may not be performed concurrently. We capture such constraints by
adding a concurrent action list to the existing precondition and effect lists in the specification of an action. The concurrent action list is a list of action schemata and negated action
schemata, some of which can be partially instantiated. If an action schema A0 appears in
the concurrent action list of an action A then an instance of schema A0 must be performed
concurrently with action A in order to have the intended effect. If an action schema A0
appears negated in the concurrent action list of an action A then no instance of schema A0
can be performed concurrently with action A if A is to have the prescribed effect.
The concurrent action list is similar to the precondition list in the following sense:
when the constraints it specifies on the environment in which the action is performed are
satisfied, the action will have the effects specified in the effect list. Notice that positive
action schemata are implicitly existentially quantified|one instance of that schema must
occur concurrently|whereas negated action schema are implicitly universally quantified|
no instance of this schema should be performed concurrently.
A schema A0 appearing in the concurrent action list of schema A can be partially instantiated or constrained: if A0 contains free variables appearing in the parameter list of A, then
these variables must be instantiated as they are instantiated in A. In addition, constraints
that restrict the possible instantiations of the schema A can appear within the concurrent
action list. This can be seen in the description of the multiagent setting version of the action
pickup shown in Figure 3. The multiagent pickup schema has an additional parameter, ?a1,
signifying the performing agent. Its list of preconditions and effects is similar to that of the
single-agent pickup schema, but it also has the concurrent action list:
(not (and (pickup ?a2 ?x ?y) (not (= ?a1 ?a2))))

The \not" prefix restricts the set of actions that can be performed concurrently with any
instance of the schema Pickup(?a1; ?x; ?y ). In particular, we disallow concurrent execution
of any instance of the schema Pickup(?a2; ?x; ?y ) such that ?a2 is different from ?a1. That
is, no other agent should attempt to pickup the object ?x at the same time.
Using this representation, we can represent actions whose effects are modified by the
concurrent execution of other actions. For example, suppose that when agent a1 lifts up
one side of a table all blocks on it are dumped onto the oor as long as no other agent
a2 lifts the other side; but if some agent a2 does lift the other side of the table then the
effect is simply to raise the side of the table. Clearly, we can distinguish between these two
111

fiBoutilier & Brafman

(define (operator lower)
:parameters
(?a1 ?s1)
:precondition (and (holding ?a1 ?s1) (raised ?s1))
:effect
(and (not (raised ?s1))
(forall ?x
(when ((ontable ?x)
(not (and (lower ?a2 ?s2)(not (= ?s1 ?s2)))))
(and (onfloor ?x) (not (ontable ?x)))))))

Figure 4: The Lower action schema
cases using the concurrency conditions (not (lift ?a2 ?side)) and (lift ?a2 ?side).
However, treating them as standard concurrency conditions essentially splits the action into
two separate actions with similar effects. As in single-agent representations, we can treat
such \modifiers" using a when clause; but now, the antecedent of the when clause has two
parts: a list of additional preconditions and a list of additional concurrency conditions. The
general form of the when clause is now (when antecedent effect), where the antecedent
itself consists of two parts: (preconditions concurrency-constraints). The latter list
has the same form as that of the concurrent-action list, and similar semantics. Thus,
whenever the precondition part of the antecedent is satisfied in the current state and the
concurrency condition is satisfied by the actions executed concurrently, the actual effect of
the action is obtained by conjoining the standard effect with the consequent of the when
clause.
The syntax of when clauses is illustrated in the table-lowering action described in Figure 4. Notice that this operator contains a universally quantified effect, that is, an effect
of the form (forall ?x (effect ?x)). This allows us to state that the conditional effect,
described by the when clause, applies to any object ?x that satisfies its precondition (e.g., to
every object on the table in this case). The use of universally quantified conditional effects
in finite domains is well understood (see Weld's (1994) discussion). However, to simplify
our presentation, we do not treat it formally in this paper.
When we lower one side of the table, that side is no longer raised. In addition, if there
is some object on the table, then lowering one side of the table will cause that object to fall,
as long as the other side of the table is not being lowered at the same time. Here, we use
universal quantification to describe the fact that this will happen to any object that is on
the table. Notice that in the concurrent part of the antecedent we see a constrained schema
again. It stipulates that the additional effect (i.e., the objects falling to the oor from the
table) will occur if no instance of the schema lower(?a2; ?s2) is executed concurrently, where
?s1 is different than ?s2.3
An action description can have no when clause, one when clause, or multiple when
clauses. In the latter case, the preconditions of all the when clauses must be disjoint.4
One might insist that the set of when clauses be exhaustive as well; however, we do not
3. In certain cases we might also insist that ?a1 6=?a2, if agents can perform only one action at a time.
But an agent with multiple effectors (to take one example) might be able to lower one or both sides
concurrently. See below for more on this.
4. In the case of multiple clauses, the disjointness restriction can be relaxed if the effects are independent,
much like in a Bayes net action description (Boutilier & Goldszmidt, 1996).

112

fiPlanning with Concurrent Interacting Actions

require this. If no when clause is satisfied when an action is performed, we assume that the
\additional" effect is null; that is, the effect of the action is simply that given by the main
effect list. When we discuss the when clauses of a specific action in our formal definitions
below, we will generally assume the existence of an implicit when clause whose precondition
consists of the negation of preconditions of the explicitly specified when clauses, and whose
effect list is empty. This allows our definitions to be stated more concisely.5

2.3 The Semantics of Concurrent Action Specifications

The semantics of individual actions is, of course, different in our multiagent setting than in
the single-agent case. It is not individual actions that transform one state of the world into
another state of the world. Rather it is joint actions that define state transitions. Joint
actions describe the set of individual actions (some of which could be no-ops) performed by
each of the agents; that is, they are n-tuples of individual actions.
Given a joint action a = ha1 ;    ; ani, we refer to the individual actions ai as the elements
of a. We say that the concurrent action list of an element ai of a is satisfied with respect
to a just when, for every positive schema A in this list, a contains some element aj (j 6= i)
which is an instance of A, and for every negative schema A0 in the list, none of the elements
aj (1  j  n) is an instance of A0. Ignoring for the moment the existence of when clauses,
we can define the notion of joint action consistency in a straightforward manner:

Definition Let a = a1; ; an be a joint action where no individual action ai contains a
h



i

when clause. We say a is consistent if





The precondition lists pi of each ai are jointly (logically) consistent (i.e., they do
not contain a proposition and its negation).
The effect lists ei of each ai are jointly consistent.
The concurrent action list of each element of a is satisfied w.r.t. a.

Given a state s, a consistent joint action a = ha1;    ; an i can be executed in s if the
precondition lists of all elements of a are satisfied in s. The resulting state t is obtained by
taking the union of the effect lists of each of the elements of a and applying it to s, as in
the single-agent case. In fact, a consistent joint action a can be viewed as a single-agent
action whose preconditions are the union of the preconditions of the various ai and whose
effects are the union of the effects of the ai .
Notice that under this semantics, a joint action is inconsistent if some individual action
a causes Q to be true, and another b causes Q to be false. It is the responsibility of the
axiomatizer of the planning domain to recognize such conicts and either state the true
effect when a and b are performed concurrently (by imposing conditional effects with concurrent action conditions) or to disallow concurrent execution (by imposing nonconcurrency
conditions).6
5. We do not assume that such a clause is ever explicitly constructed for planning purposes|it is merely a
conceptual device.
6. One can easily preprocess actions descriptions in order to check for consistency. If actions a and b are
discovered to have conicting effects, but the specification allows them to be executed concurrently,
an algorithm could automatically add a nonconcurrency constraint to each action description, thus

113

fiBoutilier & Brafman

With when clauses the definition of consistency is a bit more involved. Consistent joint
actions without when clauses can be applied consistently at all possible states (if they are
applicable at all). In contrast, joint actions with when clauses may be consistent when
applied at some states, but inconsistent at others. Given a joint action a = ha1 ;    ; ani
and a specific state s, exactly one when clause of each action ai will be satisfied; that is,
just one clause will have its preconditions and concurrency constraints satisfied.7 Thus the
joint action and the state together determine which conditional effects are selected.

Definition Given a joint action a = a1; ; an and state s, the active when clause wi of
h



i

ai relative to s and a is the (unique) when clause that is satisfied by s and a (i.e.,
whose preconditions are satisfied by s and whose concurrency constraints are satisfied
by a).

We thus relativize the notion of consistency in this case.

Definition Let a = a1; ; an be a joint action (where individual actions ai may contain
h



i

when clauses). Let s be some state, let wi be the active when clause for ai (w.r.t. s,
a), and let wi have preconditions wpi, concurrency constraints wci, and effects wei.
We say a is consistent at state s if:





The precondition lists pi and active when-preconditions wpi of each ai are mutually consistent.
The effect lists ei and active when-effects wei of each ai are mutually consistent.
The concurrent action list of each element of a is satisfied w.r.t. a.

Note that we do not require that the concurrent action lists in the when clauses be satisfied,
since they are \selected" by a. Note also that this definition reduces to the \whenless"
definition if the individual actions have no when clauses|an action is consistent with respect
to s iff it is consistent in the original sense.
Given a state s, a joint action a = ha1 ;    ; ani (involving when clauses) that is consistent
with respect to s can be executed in s if the precondition lists of all elements of a are satisfied
in s. The resulting state t is obtained by taking the union of the effect lists of each of the
elements of a, together with the effect lists of each of the active when clauses, and applying
it to s.
Several interesting issues arise in the specification of actions for multiple agents. First,
we assume throughout the rest of the paper that each agent can perform only one action
at a time, so any possible concurrent actions must be performed by distinct agents. This
allows our action descriptions to be simpler than they otherwise might. When a single
agent can perform more than one action at a time, it can be captured using a group of
\agents" denoting its different actuators. If these agents can only perform certain actions
preventing problems from arising during the planning process. This would be valid only if a and b could
not, in fact, be (meaningfully) performed concurrently. If they can, then it is important that the domain
axiomatizer specify what the true interacting effect is (e.g., maybe action a dominates). We note that
this automatic inconsistency detection and repair admits a certain additional degree of convenience in
domain specification.
7. We assume an implicit when clause corresponding to the negation of explicitly stated clauses as described
above.

114

fiPlanning with Concurrent Interacting Actions

concurrently, this can be captured by adding extra concurrency constraints. More generally,
different agents may have different capabilities, and it would be useful to have the ability
to explicitly specify these capabilities in the form of constraints on the types of actions that
different agents can execute. One way to handle such constraints is via a preprocessing
step that augments the action descriptions with additional preconditions or concurrency
conditions that capture these constraints. An alternative is to alter the planning algorithm
to take such constraints into account explicitly. When these are simple constraints|for
instance, the fact that there are n agents might imply that only n actions can be executed
concurrently|this can be done in a simple and ecient manner. This is the approach
we take in the planning algorithm we develop in Section 3. However, when the capability
constraints are complex, the former method seems better.
Another issue that must be addressed is the precise effect of a joint action, one of
whose individual actions negates some precondition of a concurrently executed individual
action. We make no special allowances for this, simply retaining the semantics described
above. While this does not complicate the definition of joint actions, we note that some
such combinations may not make sense. For example, the concurrent writing of variable p
to q and variable q to p in a computer program might be seen as each action destroying
the preconditions of the other; yet the net effect of the individual actions is simply a swap
of values. Hence, in certain circumstances, it may be acceptable to describe the actions
this way, and in others this may not be the true effect of the joint action. Again, we can
treat this issue in several ways: we can allow the specification of such actions and design
the planner so that it excludes such combinations when forming concurrent plans unless an
explicit concurrency condition is given (this means the axiomatizer need not think about
such interactions); or we can allow such combinations, in general, but explicitly exclude
problematic cases by adding nonconcurrency constraints.
Finally, an undesirable (though theoretically unproblematic) situation can arise if we
provide \incongruous" concurrency lists. For example, we may require action a to be concurrent with b in order to have a particular effect, while b may be required to be nonconcurrent
with a (this can span a set of actions with more than two elements, naturally). Hence, a and
b cannot occur together in a consistent joint action, and we would not be able to achieve
the intended effect of a. Although the planner will eventually \recognize" this fact, such
specifications can lead to unnecessary backtracking during the planning process. Again,
this is something that is easily detected by a preprocessor, and we will generally assume
that concurrency lists are congruous.

2.4 Concurrent Plan Representation
Before moving on to discuss the planning process, we describe our representation for multiagent plans, which is a rather straightforward extension of standard single-agent, partially
ordered plan representations. A (single-agent) nonlinear plan consists of: (1) a set of action
instances; (2) various strict ordering constraints using the relations < and > on the ordering of these actions; and (3) codesignation and non-codesignation constraints on the values
of variables appearing in these actions, forcing them to have the same or different values,
respectively (Weld, 1994; Penberthy & Weld, 1992). A plan of this sort represents its set of
possible linearizations, the set of totally ordered plans formed from its action instances that
115

fiBoutilier & Brafman

do not violate any of the ordering, codesignation, and non-codesignation constraints.8 We
say a plan is consistent if it has some linearization. The set of linearizations can be seen as
the \semantics" of a nonlinear plan in some sense. A (consistent) nonlinear plan satisfies a
goal set G, given starting state s, if any linearization is guaranteed to satisfy G.
A concurrent nonlinear plan for n agents (labeled 1; : : :n) is similar: it consists of a set
of action instances (with agent arguments, though not necessarily instantiated) together
with a set of arbitrary ordering constraints over the actions (i.e., <; >; = and 6=) and the
usual codesignation and non-codesignation constraints. Unlike single-agent nonlinear plans,
we allow equality and inequality ordering constraints so that concurrent or nonconcurrent
execution of a pair of actions can be imposed. Our semantics must allow for the concurrent
execution of actions by our n agents. To this end we extend the notion of a linearization:
Definition Let P be a concurrent nonlinear plan for agents 1; : : :n. An n-linearization of
P is a sequence of joint actions A1;    Ak for agents 1; : : :n such that
1. each individual action instance in P is a member of exactly one joint action Ai ;
2. no individual action occurs in A1 ;    Ak other than those in P , or individual
No-op actions;
3. the codesignation and non-codesignation constraints in P are respected; and
4. the ordering constraints in P are respected. More precisely, for any individual
action instances a and b in P , and joint actions Aj and Ak in which a and b
occur, any ordering constraints between a and b are true of Aj and Ak ; that is,
if af<; >; =; 6=gb, then j f<; >; =; 6=gk.
In other words, the actions in P are arranged in a set of joint actions such that the ordering
of individual actions satisfies the constraints, and \synchronization" is ensured by no-ops.
Note that if we have a set of k actions (which are allowed to be executed by distinct
agents) with no ordering constraints, the set of linearizations includes the \short" plan
with a single joint action where all k actions are executed concurrently by different agents
(assuming k  n), a \strung out" plan where the k actions are executed one at a time by a
single agent, with all others doing nothing (or where different agents take turns doing the
individual actions), \longer" plans stretched out even further by joint no-ops, or anything
in between.

Example Suppose our planner outputs the following plan for a group of three agents: the
set of actions is

f

a(1); b(2); c(2); d(3); e(1); f (2)

g

with the ordering constraints

e(1) = b(2); c(2) = d(3); a(1) < e(1); d(3) < f (2)

f

6

g

Here, the numerical arguments denote the agent performing the action. Joint actions
involve one action for each of the three agents. A simple 3-linearization of this plan|
depicted as the first linearization in Figure 5, and using N to denote no-ops for the
8. Concurrent execution has also been considered in this context for non-interacting actions; see Knoblock's
discussion of this issue (Knoblock, 1994).

116

fiPlanning with Concurrent Interacting Actions

Time

1

2

3

4

5

Time

1

2

3

Agent 1

a

e

N

N

N

Agent 1

a

e

N

Agent 2

N

b

c

N

f

Agent 2

c

b

f

Agent 3

N

N

N

d

N

Agent 3

N

d

N

Linearization 1

Linearization 2

Figure 5: Two possible linearizations of a partially ordered multiagent plan
corresponding agents|is:
(1) (2) (3)i h (1) (2) (3)i

ha

;N

;N

; e

;b

;N

; hN

(1) (2) (3)i
;c

;N

; hN

(1) (2) (3)i
;N

;d

; hN

(1) (2) (3)i
;f

;N

We can insert additional tuples of the form hN (1); N (2); N (3)i in any location we
wish. Another possible 3-linearization (the second in Figure 5) is:

a(1); c(2); N (3) ; e(1); b(2); d(3) ; N (1); f (2); N (3)

h

i h

i h

i

In fact, this is the shortest 3-linearization of the plan.
The definition of n-linearization requires that no agent perform more than one action at
a time. This conforms with the assumption we made in the last section, though the definition could quite easily be relaxed to allow this. Because of no-ops, our n-linearizations do
not correspond to shortest plans, either in the concurrently on nonconcurrently executed
senses of the term. However, it is a relatively easy matter to \sweep through" a concurrent nonlinear plan and construct some shortest n-linearization, one with the fewest joint
actions, or taking the least amount of \time." Though we do not have an explicit notion
of time, the sequence of joint actions in an n-linearization implicitly determines a time line
along which each agent must execute its individual actions. The fact that concurrency and
nonconcurrency constraints are enforced in the linearizations ensure that the plan is coordinated and synchronized. We note that in order to execute such a plan in a coordinated
fashion the agents will need some synchronization mechanism. This issue is not dealt with
in this paper.

3. Planning with Concurrent Actions

In Figure 6, we present the POMP algorithm, a version of Weld's POP algorithm (Weld,
1994) modified to handle concurrent actions. To keep the discussion simple, we begin
by describing POMP without considering conditional action effects. Below we describe
the simple modifications required to add conditionals (i.e., to build the analog of CPOP).
Though we do not discuss universal quantification in this paper, our algorithm could easily
be extended to handle universally quantified effects in much the same way as Penberthy
and Weld's (1992) full UCPOP algorithm.
117

fiBoutilier & Brafman

POMP(hA; O; L; NC;B i,agenda )
Termination: If agenda is empty, return hA; O; L; NC;B i.
Goal Selection: Let hQ; Aneed i be a pair on the agenda . (Aneed is an action and Q is a conjunct from its
precondition list.)
Action Selection: Let Aadd = Choose an action one of whose effects unifies with Q subject to the constraints in B . (This may be a newly instantiated action from  or an action that is already in A
and can be ordered consistently prior to Aneed). If no such action exists, then return failure. Let
Q A g. Form B by adding to B any codesignation constraints that are needed
L = L [ fAadd !
need
in order to force Aadd to have the desired effect. Let O = O [ fAadd < Aneed g. If Aadd is newly
instantiated, then A = A [ fAadd g and O = O [ fA0 < Aadd < A g (otherwise, let A = A).
Concurrent Action Selection: If Aadd is newly instantiated then apply the following steps to all positive
actions ffconc in its concurrent list: Let Aconc = Choose a newly instantiated action from  or an
action that is already in A and can be ordered consistently concurrently with Aadd. Make sure that
there is a free agent that can perform this action concurrently with Aadd and any other concurrently
scheduled actions. If no such action exists then return failure. Let O = O [ fAconc = Aneedg. If
Aconc is newly instantiated, then A = A [ fAadd g and O = O [ fA0 < Aconc < A g (otherwise,
let A = A). If aadd is the agent variable in Aadd and aconc is the agent variable in Aconc , then
add aadd 6= aconc to B , as well as all similar non-codesignation constraints for actions A such that
A = Aadd 2 O.
Re-apply this step to Aconc , if needed.
For every negative action A conc in Aadd concurrent list let NC = NC [ fA conc 6= Aadd g. Add to
B any codesignation constraints associated with A conc .
Updating of Goal State: Let agenda = agenda , fhQ; Aneedig.
If Aadd is newly instantiated, then add fhQj ; Aaddig to agenda for every Qj that is a logical precondition of Aadd. Add the other preconditions to B . If additional concurrent actions were added, add
their preconditions as well.
Causal Link Protection: For every action At that might threaten a causal link Ap !R Ac perform one of
(a) Demotion: Add At < Ap to O .
(b) Weak Promotion: Add At  Ac to O . If no agent can perform At concurrently with Ac , add
At > Ac, instead.
If neither constraint is consistent, then return failure.
Nonconcurrency Enforcement For every action At that threatens a nonconcurrency constraint A 6= A
(i.e., At is an instance of the schema A that does not violate any constraint in B ) add a consistent
constraint, either
(a) Demotion: Add At < A to O .
(b) Promotion: Add At > A to O .
If neither constraint is consistent, then return failure.
Recursive Invocation: POMP(hA ; O ; L ; NC ; B i,agenda' )
0

0

0

0

0

0

0

1

0

0

0

0

1

0

0

0

:

0

:

:

0

0

0

0

0

0

0

0

0

0

0

0

0

Figure 6: The Partially Ordered Multiagent Planning algorithm

118

fiPlanning with Concurrent Interacting Actions

We assume the existence of a function MGU(Q; R; B ) which returns the most general
unifier of the literals Q and R with respect to the codesignation constraints in B . This is
used wherever unification of action schemata is required (see the Action Selection step in
Figure 6 and our discussion of NC-threats below). The algorithm has a number of input
variables: the set A contains all action instances inserted into the plan so far; the set O
contains ordering constraints on elements of A; the set L contains causal links; the set
NC contains nonconcurrency constraints; and the set B contains the current codesignation
constraints. The set NC does not appear in the POP algorithm and contains elements of
the form A 6= a, where A 2  is an action schema and a is an action instance from A.
Intuitively, a nonconcurrency constraint of this form requires that no action instance a0
that matches the schema A subject to the (non) codesignation constraints should appear
concurrently with a in the plan.
The agenda is a set of pairs of the form hQ; Ai, listing preconditions Q that have not
been achieved yet and the actions A that require them. Initially, the sets L, NC , and B
are empty, while A contains the two fictitious actions A0 and A1 , where A0 has the initial
state propositions as its effects and A1 has the goal state conditions as its preconditions.
The agenda contains all pairs hQ; A1 i such that Q is one of the conjuncts in the description
of the goal state. This specification of the initial agenda is identical to that used in POP
(Weld, 1994). Finally, we note that the choose operator, which appears in the Action
Selection and Concurrent Action Selection steps, denotes nondeterministic choice.
Again, this device is just that used in POP to make algorithm specification independent of
the search strategy actually used for planning. Intuitively, a complete planner will require
one to search over nondeterministic choices, backtracking over those that lead to failure.
Many of the structures and algorithmic steps of POMP correspond exactly to those
used in POP. Rather than describe these in detail, we focus our discussion on the elements
of POMP that differ from POP. Apart from the additional data structure NC mentioned
above, one key difference is the additional Concurrent Action Selection step in POMP,
which takes care of the concurrency requirements of each newly instantiated action.
One final key distinction is the notion of a threat used in POMP, which is more general
than that used by POP. Much like POP, given a plan hA; O; L; NC i, we say that At
Q A when O [ fA  A < A g is consistent, and A has :Q
threatens the causal link Ap !
c
p
t
c
t
as an effect. Threats are handled using demotion (much like in POP), or weak promotion.
The latter differs from the standard promotion technique used in POP: it allows At to be
ordered concurrently with Ac , not just after Ac .9
Apart from handling conventional threats in a different manner, we have another form of
threat in concurrent plans, namely, NC-threats . We say that action instance At threatens the
nonconcurrency constraint A 6= Ac if O [ fAt = Ac g is consistent and At is an instantiation
of A that does not violate any of the codesignation constraints. Demotion and promotion
can be used to handle NC-threats, just as they do more conventional threats. Notice that
although the set NC contains negative (inequality) constraints, they will ultimately be
grounded in the set of positive constraints in O. Following the approach suggested by Weld
9. If we wish to exclude actions that negate some precondition of another concurrent action (see discussion
in Section 2), we must use O [ fAp  At  Ac g in the definition of threat, and we must change weak
promotion to standard promotion.

119

fiBoutilier & Brafman

(1994), we do not consider an action to be a threat if some of its variables can be consistently
instantiated in a manner that would remove the threat.
The POMP algorithm must check for the consistency of ordering constraints in several
places: in Action Selection where an action chosen to achieve an effect must be consistently ordered before the consumer of that effect; in Concurrent Action Selection
where each concurrency requirement added to the plan must be tested for consistency; and
in Nonconcurrency Enforcement where demotion or promotion is used to ensure that
no nonconcurrency requirements are violated. The consistency testing of a set of ordering
constraints is very similar to that employed in POP (see Weld (1994) for a nice discussion), with one key difference: the existence of equality (=) and inequality (6=) ordering
constraints as opposed to simple strict inequalities (i.e., < and >). However, with minor
modifications, standard consistency-checking algorithms for strict ordering constraints can
be used. Equality can be dealt with by simply \merging" actions that must occur concurrently (i.e., treating them as a single action for the purposes of consistency testing).
Inequalities are easily handled by assuming all actions occur at different points whenever
possible. Non-strict inequalities (i.e.,  and ) do not arise directly in our algorithm
(though these two can be easily dealt with). We refer to Ghallab and Alaoui (1989) for
further details on processing such constraints.
The POMP algorithm as described can easily be modified to handle conditional effects,
just as the POP algorithm can be extended to CPOP. The main fact to note is that in the
action selection phase, we can use an action whose conditional effects achieve the chosen
subgoal. In that case, we do not just add the preconditions of the selected action to the
agenda, but also the antecedent of the particular conditional effect (this to ensure that
the action has this particular effect). We handle the additional concurrency conditions in
the antecedent much like the regular concurrency conditions. As in the CPOP algorithm,
we must consider the possibility that a particular conditional effect of an added action
threatens an established causal link. In this case, we can, aside from using the existing
threat resolution techniques, consider a form of confrontation , where we add the negation
of the conditional effect's antecedent to the agenda. Again, we have several ways to do
this: we could add the negation of some literal in the antecedent's condition to the agenda;
but we can also add a concurrent action to negate a negative concurrency condition in the
antecedent, or post a nonconcurrency constraint to offset a positive concurrency constraint
in the antecedent. The details of such steps are straightforward and look similar to those
involved in the unconditional algorithm.

4. An Example of the POMP Algorithm
In this section, we formalize the example alluded to in the introduction and describe the
construction of a concurrent plan for this problem using the POMP algorithm.
In the initial state, two agents, Agent1 and Agent2, are located in Room1, together with
a table and a set of blocks scattered around the room. Their goal is to ensure that all of
the blocks are in Room2 and the table is on the oor. In order to simplify this example, we
assume there is only one block B , we omit certain natural operators, and we simplify action
descriptions. In order to compactly represent the multiple block version of this, we would
require the introduction of universal quantification. As shown by Weld (1994), this can
120

fiPlanning with Concurrent Interacting Actions

be done with little diculty. Intuitively, the agents should gather the blocks in the room
(in this case only one), put them on the table, carry the table to the other room, dump
the blocks from the table, and then put the table down. While this is not the best plan
for a single block, it illustrates how such a plan would be constructed for multiple blocks
(in which case this strategy is better than that of agents making multiple trips carrying
individual blocks). We use the following actions:







Pickup(a; b): agent a picks up a block b
PutDown(a; b): agent a puts block b on the table
ToTable(a; s): agent a moves to side s (left, right) of the table
MoveTable(a; r): agent a moves to room r with the table
Lift(a; s): agent a lifts side s of the table
Lower(a; s): agent a lowers side s of the table

The a variables are of type agent , b variables are of type block , r variables are of type room ,
and s variables are of type table-side. (We omit other natural actions since they won't be
used in the plan of interest.)
The domain is described using the following predicates:









OnTable(b): block b is on the table
OnFloor(b): block b is on the oor
AtSide(a; s): agent a is at side s (left, right) of the table
Up(s): side s of the table is raised
Down(s): side s of the table is on the oor
InRoom(x; r): object x (agent, block, table) is in room r
HandEmpty(a): the hand of agent a is empty
Holding(a; x): agent a is holding x (block, side of table)

The operator descriptions are defined in Figure 7.
The initial state of our planning problem is:
InRoom(B; Room1); OnFloor(B ); InRoom(Agent1; Room1); InRoom(Agent2; Room1);
InRoom(Table; Room1); Down(LeftSide); Down(RightSide)g

f

The goal propositions are:
InRoom(B; Room2); OnFloor(B ); Down(LeftSide); Down(RightSide)g

f

We now consider how a concurrent nonlinear plan for this multiagent planning problem
might be generated by POMP.
121

fiBoutilier & Brafman

(define (operator pickup)
:parameters
(?a1 ?x)
:precondition (and (inroom ?a1 ?r1) (inroom ?x ?r1)
(handempty ?a1) (onfloor ?x))
:concurrent
(and (not (pickup ?a2 ?x)) (not (= ?a1 ?a2)))
:effect
(and (not (handempty ?a1)) (not (onfloor ?x)) (holding ?a1 ?x)))
(define (operator putdown)
:parameters
(?a1 ?x)
:precondition
(and (inroom ?a1 ?r1) (inroom ?x ?r1) (inroom Table ?r1)
(holding ?a1 ?x))
:concurrent
(not (lift ?a2 ?s1))
:effect
(and (not (holding ?a1 ?x)) (ontable ?x) (handempty ?a1)))
(define (operator totable)
:parameters
(?a1 ?s1)
:precondition (and (inroom ?a1 ?r1) (inroom Table ?r1) (not (atside ?a2 ?s1)))
:concurrent
(and (not (totable ?a2 ?s1)) (not (= ?a1 ?a2)))
:effect
(atside ?a1 ?s1))

(define (operator movetable)
:parameters
(?a1 ?r1)
:precondition (holding ?a1 Table)
:concurrent
(and (movetable ?a2 ?r1) (not (= ?a1 ?a2)))
:effect
(and (inroom ?r1 Table) (inroom ?r1 ?a1)
(when ((ontable ?x) ()) (inroom ?r1 ?x))))
(define (operator lower)
:parameters
(?a1 ?s1)
:precondition (and (holding ?a1 ?s1) (up ?s1))
:concurrent
(and (not (lift ?a2 ?s2)) (not (= ?a1 ?a2)) (not (= ?s1 ?s2)))
:effect
(and (not (up ?s1))(down ?s1) (not (holding ?a1 ?s1))
(when ((and (ontable ?x) (up ?s2) (not (= ?s1 ?s2)))
(and (not (lower ?a2 ?s2)) (not (= ?a2 ?a1))))
(and (onfloor ?x) (not (ontable x))))))
(define (operator lift)
:parameters
(?a1 ?s1)
:precondition (and (atside ?s1 ?a1) (down ?s1) (down ?s2) (not (= ?s1 ?s2)))
:concurrent
(and (not (lower ?a2 ?s2)) (not (= ?a1 ?a2)) (not (= ?s1 ?s2)))
:effect
(and (not (down ?s1)) (up ?s1) (holding ?a1 ?s1)
(when ((and (ontable ?x) (down ?s2) (not (= ?s1 ?s2)))
(and (not (lift ?a2 ?s2))))
(and (onfloor ?x) (not (ontable x))))))

Figure 7: The table movers domain

122

fiPlanning with Concurrent Interacting Actions

Suppose that InRoom(B; Room2) is the first goal selected. This can be achieved by performing A1 = MoveTable(a1; Room2) via its conditional effect (note that a1 is an agent variable, so there is no commitment to which agent performs this action).10 We must add both
Holding(a1; Table) and OnTable(B ) to the agenda and insert the appropriate causal links.
In addition, the concurrent list forces us to add the action A2 = MoveTable(a2; Room2) to
the plan together with the non-codesignation constraint a1 6= a2. The ordering constraint
A1 = A2 is added as well. When we add A2, we must add its precondition Holding(a2; Table)
to the agenda as well. The structure of the partially constructed plan might be viewed as
follows:11
InRoom(Block, R2)

A1

GOAL

MoveTable(a1,R2)

A2

C

MoveTable(a2,R2)

Next, we choose the subgoal OnTable(B ) from the agenda (which we just added). We
add the action A3 = PutDown(a3; B ) to the plan with the appropriate ordering constraint
A3 < A1; its preconditions are added to the agenda and a causal link is added to L. In
addition, we must add to NC the nonconcurrency constraint not(Lift(a; s)): no agent can
lift any side of the table while the block is being placed on it if the desired effect is to be
achieved.
InRoom(Block, R2)

OnTable(Block)

A3
PutDown(a3,Block)

A1

MoveTable(a1,R2)

GOAL

C

A2

MoveTable(a2,R2)

10. We do not pursue the notion of heuristics for action selection here; but we do note that this action is
a plausible candidate for selection in the multi-block setting. If the goal list asserts that a number of
blocks should be in the second room, the single action of moving the table will achieve all of these under
the appropriate conditions (i.e., all the blocks are on the table). If action selection favors (conditional)
actions that achieve more goals or subgoals, this action will be considered before the actions needed for
\one by one" transport of the blocks by the individual agents. So this choice is not as silly as it might
seem in the single-block setting.
11. In the plan diagrams that follow, we indicate actions as Ai with the name of the action below it. Variables
are indicated by lower-case names (we do not indicate co-designation constraints in the diagrams). An
arrow from one action to another denotes a causal link (from producer to consumer), labeled by the
proposition being produced. Large arrows labeled with a C (resp. NC) denote concurrency (resp.
nonconcurrency) constraints between actions. We use left-to-right ordering to denote the temporal
ordering of actions, if such constraints exist.

123

fiBoutilier & Brafman

Now we choose the subgoal Holding(a1; Table). This can be achieved using A4 =
Lift(a1; s1), with the ordering constraint A4 < A1 . All the preconditions are added to
the agenda, but no concurrency conditions are added (yet!) for this action, since we do not
yet need to invoke the conditional effects of that action induced by simultaneous lifting of
the other side of the table:
Holding(a1,Table)

A4
Lift(a1,LS)

InRoom(Block, R2)

NC OnTable(Block)

A3

A1

PutDown(a3,Block)

GOAL

MoveTable(a1,R2)
C

A2

MoveTable(a2,R2)

We now note that the conditional effect of A4 poses a threat to the causal link A3 ontable
!
A1; this is because lifting a single side of the table will dump the block from the table. In
addition, the nonconcurrency constraint associated with A3 , that no lifting be performed
concurrently with A3 , is threatened by A4 (an NC-threat), as indicated in the plan diagram
above. The confrontation strategy is used to handle the first threat, and the action A5 =
Lift(a4; s2) scheduled concurrently with A4 . The constraints s1 6= s2 and a4 6= a1 are also
imposed. This ensures that the undesirable effect will not occur. We resolve the NC threat
by ordering A3 before A4 .12 The resulting partially completed plan is now free of threats:
Holding(a1,Table)

A4
Lift(a1,LS)

InRoom(Block, R2)
OnTable(Block)

A3
PutDown(a3,Block)

C

A1

MoveTable(a1,R2)

GOAL

C

A5

A2

Lift(a2,RS)

MoveTable(a2,R2)

Next, we choose the subgoal Down(LeftSide). This is achieved using the action A6 =
Lower(a1; LeftSide) and its preconditions are added to the agenda. In a completely similar
way, A7 = Lower(a2; RightSide) is added to achieve Down(RightSide) (again, we anticipate
the unification of these agent variables).
12. In anticipation of a subsequent step, we use variable a2 in the plan diagram instead of a4, since they
will soon be unified. To keep things concrete, we have also replaced s1 and s2 with particular sides of
the table, LeftSide and RightSide, to make the discussion a bit less convoluted.

124

fiPlanning with Concurrent Interacting Actions

A6
Lower(a1,LS)
Down(LS)

A4

Holding(a1,Table)

Lift(a1,LS)
OnTable(Block)

InRoom(Block, R2)

A3

A1

PutDown(a3,Block) C

MoveTable(a1,R2)

GOAL

C

A5

A2

Lift(a2,RS)

MoveTable(a2,R2)

A7

Down(RS)

Lower(a2,RS)

We now choose to work on the preconditions of A6 and A7 . Both of the preconditions,
Up(s) and Holding(a; s), are effects of Lift, so we use A4 and A5 as their producers. At this
stage, both A6 and A7 are constrained to follow A4 and A5 , but there are no constraints
on the relative ordering of A6 and A7 themselves. We also see that both A6 and A7
\potentially" threaten the causal link A3 ontable
!
A1; that is, they each have a conditional
effect that would cause the block to fall from the table. There are several ways to resolve
these two threats, including confrontation. We choose strict promotion, and order both A6
and A7 to occur after A1 and A2 .
Holding(a1,LS)
Up(a1,LS)

A4

A6
Lower(a1,LS)

Holding(a1,Table)

Down(LS)

Lift(a1,LS)
OnTable(Block)

A

3

PutDown(a3,Block)

C

A5
Lift(a2,RS)

InRoom(Block, R2)

A1

MoveTable(a1,R2)

GOAL

C

A2

MoveTable(a2,R2)

Down(RS)

A7
Lower(a2,RS)
Up(a2,RS)
Holding(a2,RS)

Now, we choose the subgoal OnFloor(B ), which is a conditional effect of the Lower
action. We choose to accomplish it using an existing action, A6 . In order to obtain the
desired effect, we ensure the antecedent of the when clause for this effect holds: this involves
adding the conditions of the antecedent (OnTable(B ) and Up(LeftSide)) to the agenda, and
imposing the nonconcurrency constraint of the antecedent, namely, that no concurrent
Lower action can take place. This constraint is threatened by the action A7 , so we order
125

fiBoutilier & Brafman

A6 before A7 by posting the constraint A6 < A7.13 The conditions of the antecedent,
OnTable(B ) and Up(LeftSide), can use A3 and A5 as the producers, respectively.
OnTable(Block)
Holding(a1,LS)
Up(a1,LS)

A4

A6
Lower(a1,LS)

Holding(a1,Table)

Down(LS)
OnFloor(Block)

Lift(a1,LS)
OnTable(Block)

A

3

PutDown(a3,Block)

C

InRoom(Block, R2)

A1

MoveTable(a1,R2)

GOAL

C
Holding(a2,Table) A
2
Lift(a2,RS)
MoveTable(a2,R2)

A5

Down(RS)

A7
Up(a2,RS)

Lower(a2,RS)

Holding(a2,RS)

The only unsolved subgoal is the precondition of the initial PutDown(a3; B ) action
(others, such as Down(LeftSide) for the Lift action, are produced by the initial state). We
don't illustrate it, but it is a simple matter to introduce the Pickup(a3; B ) action before
PutDown(a3; B ).
We now have the following plan: first, the block is picked up and put on the table by
some agent a3 (either of Agent1 or Agent2 can do this). This is followed by two concurrent
lift actions and two concurrent move actions which get the table to the other room with the
block on top. Next, we have a single lower action, which makes the block fall off, followed
by another lower action which ensures that both sides of the table are on the oor. We
note that the plan does not care which of the agents (the one who lifts the LeftSide or the
RightSide) initially puts the block on the table.14

5. Soundness and Completeness of the POMP Algorithm
We say that a planning algorithm is sound if it generates only plans that are guaranteed
to achieve the goals posed to it; a complete algorithm is guaranteed to generate a plan if
a successful plan exists.15 In the case of concurrent nonlinear plans, we will say that an
algorithm is sound if each n-linearization of the plan produced for a given problem will
reach a goal state, and an algorithm is complete if it successfully generates a concurrent
nonlinear plan whenever there is a sequence of joint actions (i.e., an n-linearization of some
13. The other ordering A7 < A6 could have been used to resolve this threat; but it would cause an \unresolvable" threat to the conditions of the antecedent, which require that the other side remain up. It is,
of course, only \unresolvable" in the sense that it would require the agents to pick up the block, etc.,
essentially introducing a cycle in the plan.
14. Further examples of MAP problems, the plans produced by POMP, and code implementing the POMP
algorithm can be obtained at http://www.cs.bgu.ac.il/ishayl/project/.
15. For formal definitions of these concepts, we refer the reader to (Penberthy & Weld, 1992).

126

fiPlanning with Concurrent Interacting Actions

concurrent plan) that achieves the goal from the initial state. We now show that the POMP
algorithm is both sound and complete.
The soundness proof is straightforward. Suppose that the generated plan is not sound.
Thus, some n-linearization of the plan does not achieve the goal or some required subgoal
(i.e., a precondition of one of the plan's actions). Because of the agenda mechanism, it
is clear that for each needed goal or precondition there exists an action in the plan that
achieves that subgoal (goal or precondition). Moreover, there is an explicit causal link in
the plan for that particular subgoal as well as an ordering constraint requiring that the
producing action to appear prior to the consuming action (or the goal). Any n-linearization
of a plan is another plan obtained from the original plan by adding new, consistent, strict
(i.e. <; >) ordering constraints. Recall that the original plan's ordering constraints must
have been consistent, otherwise it would not constitute a solution, and that there were no
threats. Clearly, by adding new strict ordering constraints we cannot cause any new threats
to causal links or violate a nonconcurrency constraint. Hence, the resulting n-linearization
respects all causal links of the original plan and all ordering constraints of the original plan.
To complete the proof, we must be convinced that POMP actually considers all possible,
relevant interactions between actions. Consider some effect P of an action a needed by some
action b which is ordered after a. Given the semantics of actions, there are only two reasons
why P will not hold prior to the execution of b: (1) some action c between a and b (possibly
concurrent with a) has an effect :P ; or (2) a did not actually have P as an effect. Case (1)
contradicts the fact that there are no threats (in our extended sense, covering the possibility
of c occurring concurrently with a) in the context of this plan. Case (2) implies that either
P is an effect of a subject to some concurrency or nonconcurrency condition that is violated
in this n-linearization. Any such problem would have been taken care of by the Action
Selection or Nonconcurrency Enforcement steps (and by the ordering constraints).
Thus it should be clear that any n-linearization of a plan produced by POMP does in fact
achieve all its goals; that is, POMP is sound.
The completeness proof rests on three key elements:
1. A reduction from multiagent planning problems to single agent planning problems.
2. The fact that POMP can solve a multiagent planning problem iff POP can solve the
single agent planning problem obtained via this reduction.
3. The fact that POP is sound and complete (Penberthy & Weld, 1992).
First, we show how given a multiagent planning problem, a similar single agent planning
problem can be obtained. We shall refer to the generated problem as the equivalent single
agent planning problem (or the ESA problem). This reduction has the property that a plan
for the multiagent planning problem exists if and only if a plan for the ESA problem exists.
In the introduction, we discussed such a reduction via the use of joint actions. Here, we will
use a similar idea, but with a little more care so that both POMP and POP will perform
similar steps in the solution of the original problem and the ESA problem, respectively.
Combining these results with the fact that POP is sound and complete, we can deduce that
POMP is sound and complete as well.
127

fiBoutilier & Brafman

In the discussion below, we ignore conditional effects to avoid undue and, for the most
part, uninteresting complications. The extension of the arguments to deal with conditional
effects is straightforward. We first recall the following facts relevant to our argument:
(a) POP and POMP are nondeterministic planning algorithms and, although there
are various ways of making them deterministic, this issue is orthogonal to the
proof. Thus, in showing the correspondence between POP and POMP alluded to
in point (2) above, we can utilize the exibility awarded to us by each planner's
use of nondeterministic choice. In particular, it is sucient to show that for a
given solution path for one planner, a similar solution path exists for the other.
(b) The choice of the next agenda element to work on is immaterial for both POP
and POMP|it can affect the running time (e.g., by causing backtracking) but
not the existence of a solution. Hence, we are exible in ordering the subgoals
achieved, as long as we respect causality (i.e., we cannot achieve a goal that is
derived from a precondition of an action that was not introduced yet).
(c) By introducing additional ordering constraints consistent with current constraints
in a valid plan, we obtain a valid plan for the given problem.
(d) The precise order in which actions and ordering constraints are inserted does
not affect the validity of the solution. In fact, as is well known in the planning
community, one can postpone the threat resolution step without affecting the
soundness or completeness of the algorithm, as long as all threats are eventually
resolved.
Our proof will proceed in two stages. In the first stage, we will limit ourselves to a
restricted set of planning problems for which we can show the connections with POP in a
straightforward fashion. We then relax this restriction to show the correspondence between
the two planners in the general case.
Recall that in Section 2.3 we suggested a possible restriction on the set of actions one
is allowed to execute concurrently, namely, that no two actions a and b are permitted to
occur concurrently if one's effects negate any of the other's preconditions. We remarked
that this concurrent, non-clobbering condition, if not enforced in the action specification
itself, is easily enforced by the POMP algorithm if we modify the definition of a threat and
use promotions instead of weak promotions to resolve threats. Let us restrict attention, for
the time being, to domains respecting this condition.
We first note the following fact. Let M be some POMP plan, and consider some nlinearization of M in which a1 and a2 occur concurrently, but where M is such that no
future actions require the effects produced by the concurrent execution of these actions.
That is, actions a1 and a2 are not forced to occur concurrently by plan M . In this case,
any similar n-linearization in which a1 is ordered before a2 , or vice versa, and no other
ordering constraints are violated (some such linearization must exist) will also achieve the
goal. The only case in which this might not happen is when one of a1 or a2 clobbers the
other's preconditions; but this has been explicitly disallowed in our restricted setting (by
the imposition of a nonconcurrency constraint or \precondition").
128

fiPlanning with Concurrent Interacting Actions

Now consider the ESA problem, where the actions available to the agent are as follows:
for each individual action a that has no concurrency constraints in the multiagent problem,
we create an action corresponding to the joint action where a is performed by its \owning"
agent, and no-ops are executed by every other agent; and for each individual action a that
has concurrency conditions, requiring that actions b1;    bk be executed concurrently, we
create an action corresponding to the joint action where a and each of the bi are performed,
but no other actions apart from no-ops are performed.16 We note that nonconcurrency
constraints are ignored in the ESA problem definition.
Clearly, if a joint action sequence exists for a given problem, there also exists a concurrent nonlinear plan for that problem. In addition, by the argument above involving
the assumption that no concurrent action clobber another's precondition, it is also easy
to see that, if a concurrent nonlinear plan can be found for a problem, there also exists a
concurrent nonlinear plan in which the only concurrency constraints involve actions whose
specification requires the concurrent execution of another action (or set of actions) in order
to obtain a particular effect. This implies that, should a problem be solvable, it is solvable
by a sequence of joint actions of the type constructed above, using only single-agent individual actions together with a set of no-ops, or at most involving minimal sets of interacting
actions. In other words, a concurrent nonlinear plan exists for a given problem iff a plan for
the ESA problem exists. We note that the structure of any solution for the ESA problem
(or any linearization of a nonlinear single-agent plan for the ESA problem) is very specific:
actions occur concurrently only if they are forced to. In other words, solutions to the ESA
problem are strung out plans, in which agents \take turns" performing their actions.
Next, we want to show that (in our restricted setting) POMP's solution path for a given
planning problem and POP's solution path for its ESA problem resemble each other. This
becomes apparent once we combine POMP's action selection and concurrent action selection
steps. We obtain a step that is equivalent to the action selection step of POP for the ESA
problem (i.e., whenever POMP chooses an action which requires another concurrent action,
the required concurrent action is immediately inserted as well; this is equivalent to inserting
the proper ESA action). In fact, now POP and POMP look almost identical, except for
POMP's Nonconcurrency Enforcement step. However, because of the fashion in which
the ESA problem was defined, all nonconcurrency constraints are automatically \imposed"
in the plan produced by POP since they refer to different joint actions. Any linearization
of these joint actions enforces the nonconcurrency of all joint actions. Therefore, the only
(single-agent) actions that can occur together in POP's solution to the ESA problem are
those that have to occur together and on which there is no nonconcurrency constraint. (In
fact, on these actions there is an explicit concurrency constraint.)17
The above argument demonstrates that POP and POMP generate \identical" sets of
plans, except for two small differences. First, POMP's semantics allows for concurrent
execution of certain actions, even though they need not be executed concurrently in order
16. It is important to note that a single action schema gives rise to n individual actions, one for each
agent (e.g., Lift(Agent1; s) and Lift(Agent2; s) are distinct actions, and separate joint actions for these
will be created). Similarly, when the concurrency conditions involve action schemata, any permitted
combination of agent instantiations will give rise to a distinct joint action.
17. This assumes that concurrency lists are congruous, as described in Section 2; but if, not, a simple
redefinition of the ESA problem can be given so that no \incongruous" concurrent actions are admitted.

129

fiBoutilier & Brafman

to solve the problem, while POP (for the ESA problem) cannot generate plans that admit
this. However, this difference cannot affect the completeness argument (since it means that
POMP is more exible than POP).18 Second, POMP commits to a particular ordering of
actions for which there is a nonconcurrency constraint, while POP will not make such a
commitment if both orderings are consistent. However, if both are consistent (and remain
unordered in the final plan for the ESA problem) then the choice POMP makes cannot
impact the solution (and POMP can produce either alternative if the ordering does matter).
Now, using the fact that POP is sound and complete, the virtual equivalence of POMP and
POP steps, and our facts about strung out plans and the ESA problem, we see that POMP
is sound and complete for the special case where concurrent actions do not destroy each
other's preconditions.
Finally, we wish to remove the restrictions placed on concurrent actions, and admit
problems where a concurrent action can clobber the precondition of another. We note that
problems of this type exist that cannot be solved by a strung out plan in the sense defined
above. For instance, consider the following problem. We have two actions:


Action a: Precondition P ; effect Q



Action b: Precondition :Q; effect :P

Actions a and b have no nonconcurrency constraints, thus they are not required to be
concurrent to have their specified effects when considered in isolation. Suppose our initial
state is fP; :Qg and the goal state is f:P; Qg. The only plan that achieves this goal requires
that a and b be executed concurrently. If we order one before the other, we will destroy the
ability to perform the second, and the goal will not be reachable. Thus, POMP can solve
this problem while POP could not solve the ESA problem (as formulated above).
To deal with the more general case, we extend the construction of the ESA problem by
including (in addition to the actions used in the restricted case) a joint action in the ESA
problem for any set of actions A satisfying the following conditions:


Each element of A is permitted to be executed concurrently (but need not be forced
to be concurrent).



Each element of A clobbers the precondition of some other element of A.



No element of A can be removed without destroying this property.

In other words, we create a joint action corresponding to the concurrent execution of each
element of such a set A. We'll call these \self-clobbering" joint actions. It should be evident
that a concurrent nonlinear plan exists for an arbitrary multiagent planning problem iff there
exists a sequence of joint actions (allowing self-clobbering actions) that solve the problem,
and hence (by the soundness and completeness of POP) iff POP can find a plan for this
generalized ESA problem. We have already seen that POMP can emulate any step of POP
18. This additional exibility impacts only the soundness of POMP (and is addressed above). In fact, we
could have used the current line of reasoning as part of an integrated soundness and completeness proof
based on the POP/POMP correspondence, in which case, we would need to explain why this last point
does not hinder the soundness of POMP.

130

fiPlanning with Concurrent Interacting Actions

involving actions other than self-clobbering actions. We simply have to show that POMP
can emulate POP's introduction of self-clobbering actions to show completeness.
Let A be some self-clobbering joint action. We claim that POP is complete (for the
generalized ESA problem) if it only ever considers adding A to an incomplete plan when
each of its elements ai 2 A has an effect that satisfies some subgoal on the agenda. Suppose,
to the contrary, that ai 2 A has no consumer on the current agenda. Then either A is not
necessary in a successful plan (since some subset of the actions in A can be used), or the
actions that consume the effects of some ai have not yet been introduced. We can discount
the former case by considering only executions of POP that do not use this action. POP
will be complete even if this action is never considered, since it is able to introduce the
individual components (or concurrent subsets) of A that do produce the necessary effects.
We can discount the latter case, since there must be a valid execution of POP that introduces
the (ultimate) consumers of each element of ai before introducing A. Thus, without loss of
generality, we assume that each element ai 2 A satisfies some subgoal on the agenda if A is
introduced by POP.
Now suppose POP introduces a self-clobbering action A. Since all ai 2 A satisfy some
agenda item, POMP can simulate this step as follows: introduce each ai in turn to satisfy
some agenda item, postponing threat resolution among the ai ; resolve the self-threats among
the ai through weak promotion in the Causal Link Protection step (so that we impose
ordering constraint ai  aj for ai that threatens aj ). In the example above, for instance,
once actions a and b are added to achieve subgoals Q and :P , respectively, the only way
to resolve the mutual threat is by weak promotion of both actions; that is, we impose a  b
and b  a. In other words, they are forced to be concurrent. Thus any introduction of
a self-clobbering joint action by POP (under the assumptions stated above) has a strong
correspondence with a sequence of possible steps in POMP. Since POP can always find
a plan under these assumptions, so can POMP. Thus the completeness of POMP in the
general case of arbitrary multiagent planning problems is demonstrated.

6. Concluding Remarks
One often finds assertions in the planning literature that planning with interacting actions
is an inherently problematic affair, requiring substantial extension to existing single-agent
planning representations and algorithms. Thus, it is somewhat surprising that only minor
changes are needed to enable the STRIPS action representation language to capture interacting actions, and that relatively small modifications to existing nonlinear planners are
required to generate concurrent plans. Our solution involves the addition of a concurrent
action list to the standard action description, specifying which actions should or should
not be scheduled concurrently with the current action in order to achieve a desired effect.
The POP planner is augmented by two steps: one which handles the insertion of required
concurrent actions, and one which handles threats emanating from the potential concurrent
execution of two interfering actions. In addition, explicit reasoning with equality and inequality constraints is introduced. Because of the strong resemblance between our solution
for the multiagent case and the solution for the single agent case, little overhead is incurred
when actions do not interact. In fact, in the extreme case of non-interacting actions, both
our extension to STRIPS and to POP reduce to their single-agent equivalents.
131

fiBoutilier & Brafman

There is a close connection between our specification method and Knoblock's (1994)
approach to generating parallel execution plans. Knoblock adds to the action description a
list that describes the resources used by the action: actions that require the same resource
(e.g., access to a database) cannot be scheduled at the same time. Hence, Knoblock's
resource list actually characterizes one form of nonconcurrency constraint.19 In fact, we
believe that certain nonconcurrency constraints are more naturally described using such a
resource list than with the general method proposed here|augmenting our language with
such lists should not prove dicult.
The treatment of concurrent actions in the specification languages Ac (Baral & Gelfond, 1997) and C (Giunchiglia & Lifschitz, 1998) has many features in common with our
extension of STRIPS (although C , in particular, is a very expressive language with many
additional features). These languages allow the use of complex actions|which are sets of
primitive actions|analogous to the ability we provide to combine a number of elements
into a joint action. Typically, complex actions inherit their effects from the primitive actions contained in them. However, explicit specification of the effects of complex actions is
possible, overriding this inheritance. This overriding mechanism can extend to an arbitrary
number of levels (e.g., an action a can have some effect, which is overridden when a and b
are performed concurrently, but this effect is itself overridden when c is performed as well,
etc.). In these action description languages, an implicit view of time is adopted, much like
in our treatment, and concurrent actions are assumed to be performed simultaneously. Until quite recently, there were no tools for actually synthesizing plans for domains described
in languages such as C . However, recent progress in model-based techniques had led to
a number of new algorithms, including a SAT encoding for the language C (Giunchiglia,
2000).
When the effects of one agent's actions depend on the actions performed by other agents
at the same time, action specification becomes a complex task. The STRIPS representation is
useful because it admits a relatively simple planning algorithm. However, despite STRIPS's
semantic adequacy and its ability, in principle, to represent any set of actions, verifying that
a domain description is accurate becomes more dicult when interactions must be taken into
account. Consequently, we believe that the use of dynamic Bayes nets, in conjunction with
conditional outcome (or probability) trees (Boutilier & Goldszmidt, 1996), can provide a
more natural and concise representation of actions in multiagent settings. This specification
technique makes clear the inuence of different context conditions on an action's effects,
and allows one to exploit the independence of different effects. While this representation
can be used for stochastic domains, dynamic Bayes nets offer these advantages even in the
case of purely deterministic actions. The POMP algorithm naturally extends to this form
of domain description, and a more complete treatment of this issue would be an interesting
direction for future research.
While adapting existing nonlinear planners to handle interacting actions is conceptually
simple, we expect that the increase in domain complexity will inevitably lead to poor computational performance. Indeed, in our experiments with the POMP algorithm, we have
found that performance is greatly affected by the ordering of agenda items. Hence, adequate
heuristics for making the various choices the planner is faced with|namely, choosing sub19. In principle, any nonconcurrency constraint can be handled in this manner by introducing fictitious
resources.

132

fiPlanning with Concurrent Interacting Actions

goals, choosing actions that achieve them, and choosing threat-resolution strategies|will
become even more critical. Of course, the same issues are central for single-agent nonlinear
planners, though we anticipate that the multiagent case with its interacting actions will
require different, or additional, heuristics.
An interesting topic for future work would be extending newer planning algorithms
such as Graphplan (Blum & Furst, 1995) to handle our multiagent representation language.
Indeed, the model-based algorithm of Cimatti, et al. (1997) seems to offer promising developments in this direction. Naturally, all representational issues raised in this paper arise
regardless of the particular planning algorithm used, although with different implications.
For example, the question of whether or not to allow for concurrent actions that destroy
one another's preconditions affected which threat removal operators were valid in POMP,
whereas in Graphplan they would affect the definition of interfering actions (and consequently, the question of which actions are considered mutually exclusive).
Finally, we note that the approach we have considered is suitable for a team of agents
with a common set of goals. It assumes that some central entity generates the plan, and
that the agents have access to a global clock or some other synchronization mechanism
(this is typically the case for a single agent with multiple effectors, and applies in certain
cases to more truly distributed systems). An important research issue is how such plans
can be generated and executed in a distributed fashion, and how their execution should
be coordinated and controlled. This is an important question to which some answers have
emerged in the DAI literature (des Jardins, Durfee, Ortiz Jr., & Wolverton, 1999; Grosz,
Hunsberger, & Kraus, 1999; des Jardins & Wolverton, 1999; Boutilier, 1996, 1999; Brafman,
Halpern, & Shoham, 1998) and the distributed systems literature (Fagin, Halpern, Moses,
& Vardi, 1995).

Acknowledgments
Thanks to the referees for their suggestions on the presentation of these ideas and to Mike
Wellman for his helpful comments. We also thank Daniel Fogel, Ishay Levy, and Igor Razgon
for their implementation of the POMP algorithm. Boutilier was supported by NSERC Research Grant OGP0121843, and the NCE IRIS-II program Project IC-7. Brafman was supported by Paul Ivanier Center for Robotics and NCE IRIS-II program Project IC-7. Much
of this work was undertaken while both authors were at the University of British Columbia,
Department of Computer Science. Preliminary results in this paper were presented in
\Planning with Concurrent Interacting Actions," Proceedings of the Fourteenth National
Conference on Artificial Intelligence (AAAI-97), Providence, RI, pp.720{729 (1997).

References
Baral, C., & Gelfond, M. (1997). Reasoning about effects of concurrent actions. Journal of
Logic Programming, 85{117.
Blum, A. L., & Furst, M. L. (1995). Fast planning through graph analysis. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence, pp. 1636{
1642 Montreal.
133

fiBoutilier & Brafman

Boutilier, C. (1996). Planning, learning and coordination in multiagent decision processes.
In Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and
Knowledge, pp. 195{210 Amsterdam.
Boutilier, C. (1999). Sequential optimality and coordination in multiagent systems. In
Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,
pp. 478{485 Stockholm.
Boutilier, C., & Goldszmidt, M. (1996). The frame problem and Bayesian network action
representations. In Proceedings of the Eleventh Biennial Canadian Conference on
Artificial Intelligence, pp. 69{83 Toronto.
Brafman, R. I., Halpern, J. Y., & Shoham, Y. (1998). On the knowledge requirements of
tasks. Artificial Intelligence, 98 (1-2), 317{350.
Cimatti, A., Giunchiglia, E., Giunchiglia, F., & Traverso, P. (1997). Planning via model
checking: A decision procedure for AR. In Proceedings of the Fourth European Conference on Planning (ECP'97), pp. 130{142 Toulouse.
de Giacomo, G., Lesperance, Y., & Levesque, H. J. (1997). Reasoning about concurrent
execution, prioritized interrupts, and exogenous actions in the situation calculus. In
Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence,
pp. 1221{1226 Nagoya.
Dean, T., & Kanazawa, K. (1989). Persistence and probabilistic projection. IEEE Trans.
on Systems, Man and Cybernetics, 19 (3), 574{585.
des Jardins, M. E., Durfee, E. H., Ortiz Jr., C. L., & Wolverton, M. J. (1999). A survey of
research in distributed continual planning. AI Magazine, 20 (4), 13{22.
des Jardins, M. E., & Wolverton, M. J. (1999). Coodinating a distributed planning system.
AI Magazine, 20 (4), 13{22.
Donald, B. R., Jennings, J., & Rus, D. (1993). Information invariants for cooperating
autonomous mobile robots. In Proceedings of the International Symposium on Robotics
Research Hidden Valley, PA.
Durfee, E. H., & Lesser, V. R. (1989). Negotiating task decomposition and allocation using
partial global planning. In Huhns, M., & Gasser, L. (Eds.), Distributed AI, Vol. 2.
Morgan Kaufmann.
Durfee, E. H., & Lesser, V. R. (1991). Partial global planning: A coordination framework for distributed hypothesis formation. IEEE Transactions on System, Man, and
Cybernetics, 21 (5), 1167{1183.
Ephrati, E., Pollack, M. E., & Rosenschein, J. S. (1995). A tractable heuristic that maximizes global utility through plan combination. In Proceedings of the First International Conference on Multiagent Systems, pp. 94{101 San Francisco.
134

fiPlanning with Concurrent Interacting Actions

Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning about Knowledge.
MIT Press, Cambridge, MA.
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2 (3{4), 189{208.
Ghallab, M., & Alaoui, A. M. (1989). Managing eciently temporal relations through
indexed spanning trees. In Proceedings of the Eleventh International Joint Conference
on Artificial Intelligence, pp. 1297{1303 Detroit.
Giunchiglia, E. (2000). Planning as satisfiability with expressive action languages: Concurrency, constraints and nondeterminism. In Proceedings of the Seventh International
Conference on Principles of Knowledge Representation and Reasoning (KR'00), pp.
657{666 Breckenridge, CO.
Giunchiglia, E., & Lifschitz, V. (1998). An action language based on causal explanation:
Preliminary report. In Proceedings of the Fifteenth National Conference on Artificial
Intelligence, pp. 623{630 Madison, WI.
Grosz, B. J., Hunsberger, L., & Kraus, S. (1999). Planning and acting together. AI
Magazine, 20 (4), 13{22.
Jensen, R. M., & Veloso, M. M. (2000). OBDD-based universal planning for synchronized
agents in non-deterministic domains. Journal of Artificial Intelligence Research, 13,
189{226.
Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and
stochastic search. In Proceedings of the Thirteenth National Conference on Artificial
Intelligence, pp. 1194{1201 Portland, OR.
Khatib, O., Yokoi, K., Chang, K., Ruspini, D., Holmberg, R., Casal, A., & Baader, A.
(1996). Force strategies for cooperative tasks in multiple mobile manipulation systems.
In Giralt, G., & Hirzinger, G. (Eds.), Robotics Research 7, The Seventh International
Symposium, pp. 333{342. Springer-Verlag, Berlin.
Knoblock, C. A. (1994). Generating parallel execution plans with a partial-order planner.
In Proceedings of the Second International Conference on AI Planning Systems, pp.
98{103 Chicago.
Koehler, J. (1998). Planning under resource constraints. In Proceedings of the Thirteenth
European Conference on Artificial Intelligence, pp. 489{493 Brighton, UK.
Lin, F., & Shoham, Y. (1992). Concurrent actions in the situation calculus. In Proceedings
of the Tenth National Conference on Artificial Intelligence, pp. 590{595 San Jose.
Lingard, A. R., & Richards, E. B. (1998). Planning parallel actions. Artificial Intelligence,
99 (2), 261{324.
McCarthy, J., & Hayes, P. (1969). Some philosophical problems from the standpoint of
artificial intelligence. Machine Intelligence, 4, 463{502.
135

fiBoutilier & Brafman

Moses, Y., & Tennenholtz, M. (1995). Multi-entity models. Machine Intelligence, 14, 63{88.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner for
ADL. In Proceedings of the Third International Conference on Principles of Knowledge
Representation and Reasoning (KR'92), pp. 103{114 Cambridge, MA.
Pinto, J. (1998). Concurrent actions and interacting effects. In Proceedings of the Sixth
International Conference on Principles of Knowledge Rerpresentation and Reasoning
(KR'98), pp. 292{303 Trento.
Reiter, R. (1978). On closed world databases. In Gallaire, H., & Minker, J. (Eds.), Logic
and Databases, pp. 55{76. Plenum, New York.
Reiter, R. (1991). The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. In Lifschitz, V. (Ed.), Artificial
Intelligence and Mathematical Theory of Computation (Papers in Honor of John McCarthy), pp. 359{380. Academic Press, San Diego.
Reiter, R. (1996). Natural actions, concurrency and continuous time in the situation calculus. In Proceedings of the Fifth International Conference on Principles of Knowledge
Representation and Reasoning (KR'96), pp. 2{13.
Stone, P., & Veloso, M. M. (1999). Task decomposition, dynamic role assignment, and lowbandwidth communication for real-time strategic teamwork. Artificial Intelligence,
110 (2), 241{273.
Weld, D. S. (1994). An introduction to least commitment planning. AI Magazine, 15 (4),
27{61.
Wilkins, D. E., & Myers, K. L. (1998). A multiagent planning architecture. In Proceedings of
the Fourth International Conference on AI Planning Systems, pp. 154{162 Pittsburgh.
Wolverton, M. J., & des Jardins, M. (1998). Controlling communication in distributed planning using irrelevance reasoning. In Proceedings of the Fifteenth National Conference
on Artificial Intelligence, pp. 868{874 Madison, WI.

136

fiJournal of Artificial Intelligence Research 14 (2001) 303358

Submitted 10/00; published 6/01

GIB: Imperfect Information in a Computationally
Challenging Game
Matthew L. Ginsberg

ginsberg@cirl.uoregon.edu

CIRL
1269 University of Oregon
Eugene, OR 97405 USA

Abstract
This paper investigates the problems arising in the construction of a program to play the
game of contract bridge. These problems include both the difficulty of solving the games
perfect information variant, and techniques needed to address the fact that bridge is not, in
fact, a perfect information game. Gib, the program being described, involves five separate
technical advances: partition search, the practical application of Monte Carlo techniques to
realistic problems, a focus on achievable sets to solve problems inherent in the Monte Carlo
approach, an extension of alpha-beta pruning from total orders to arbitrary distributive
lattices, and the use of squeaky wheel optimization to find approximately optimal solutions
to cardplay problems.
Gib is currently believed to be of approximately expert caliber, and is currently the
strongest computer bridge program in the world.

1. Introduction
Of all the classic games of mental skill, only card games and Go have yet to see the appearance of serious computer challengers. In Go, this appears to be because the game is
fundamentally one of pattern recognition as opposed to search; the brute-force techniques
that have been so successful in the development of chess-playing programs have failed almost utterly to deal with Gos huge branching factor. Indeed, the arguably strongest Go
program in the world (Handtalk) was beaten by 1-dan Janice Kim (winner of the 1984 Fuji
Womens Championship) in the 1997 AAAI Hall of Champions after Kim had given the
program a monumental 25 stone handicap.
Card games appear to be different. Perhaps because they are games of imperfect information, or perhaps for other reasons, existing poker and bridge programs are extremely
weak. World poker champion Howard Lederer (Texas Holdem, 1996) has said that he would
expect to beat any existing poker program after five minutes play.1 Perennial world bridge
champion Bob Hamman, seven-time winner of the Bermuda Bowl, summarized the state of
bridge programs in 1994 by saying that, They would have to improve to be hopeless.
In poker, there is reason for optimism: the gala system (Koller & Pfeffer, 1995), if
applicable, promises to produce a computer player of unprecedented strength by reducing
the poker problem to a large linear optimization problem which is then solved to generate
a strategy that is nearly optimal in a game-theoretic sense. Schaeffer, author of the world
1. Many of the citations here are the results of personal communications. Such communications are indicated simply by the presence of a  in the accompanying text.
c
2001
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGinsberg

champion checkers program Chinook (Schaeffer, 1997), is also reporting significant success
in the poker domain (Billings, Papp, Schaeffer, & Szafron, 1998).
The situation in bridge has been bleaker. In addition, because the American Contract
Bridge League (acbl) does not rank the bulk of its players in meaningful ways, it is difficult
to compare the strengths of competing programs or players.
In general, performance at bridge is measured by playing the same deal twice or more,
with the cards held by one pair of players being given to another pair during the replay and
the results then being compared.2 A team in a bridge match thus typically consists of
two pairs, with one pair playing the North/South (N/S) cards at one table and the other
pair playing the E/W cards at the other table. The results obtained by the two pairs are
added; if the sum is positive, the team wins this particular deal and if negative, they lose
it.
In general, the numeric sum of the results obtained by the two pairs is converted to
International Match Points, or imps. The purpose of the conversion is to diminish the
impact of single deals on the total, lest an abnormal result on one particular deal have an
unduly large impact on the result of an entire match.
Jeff Goldsmith reports that the standard deviation on a single deal in bridge is about 5.5
imps, so that if two roughly equal pairs were to play the deal, it would not be surprising if one
team beat the other by about this amount. It also appears that the difference between an
average club player and an expert is about 1.5 imps (per deal played); the strongest players
in the world are approximately 0.5 imps/deal better still. Excepting gib, the strongest
bridge playing programs appear to be slightly weaker than average club players.
Progress in computer bridge has been slow. An incorporation of planning techniques into
Bridge Baron, for example, appears to have led to a performance increment of approximately
1/3 imp per deal (Smith, Nau, & Throop, 1996). This modest improvement still leaves
Bridge Baron far shy of expert-level (or even good amateur-level) performance.
Prior to 1997, bridge programs generally attempted to duplicate human bridge-playing
methodology in that they proceeded by attempting to recognize the class into which any
particular deal fell: finesse, end play, squeeze, etc. Smith et al.s work on the Bridge Baron
program uses planning to extend this approach, but the plans continue to be constructed
from human bridge techniques. Nygate and Sterlings early work on python (Sterling &
Nygate, 1990) produced an expert system that could recognize squeezes but not prepare for
them. In retrospect, perhaps we should have expected this approach to have limited success;
certainly chess-playing programs that have attempted to mimic human methodology, such
as paradise (Wilkins, 1980), have fared poorly.
Gib, introduced in 1998, works differently. Instead of modeling its play on techniques
used by humans, gib uses brute-force search to analyze the situation in which it finds itself.
A variety of techniques are then used to suggest plays based on the results of the brute-force
search. This technique has been so successful that all competitive bridge programs have
switched from a knowledge-based approach to a search-based approach.
GIBs cardplay based on brute-force techniques was at the expert level (see Section 3)
even without some of the extensions that we discuss in Section 5 and subsequently. The
weakest part of gibs game is bidding, where it relies on a large database of rules describing
2. The rules of bridge are summarized in Appendix A.

304

fiGIB: Imperfect information in a computationally challenging game

the meanings of various auctions. Quantitative comparisons here are difficult, although the
general impression of the stronger players using GIB are that its overall play is comparable
to that of a human expert.
This paper describes the various techniques that have been used in the gib project, as
follows:
1. Gibs analysis in both bidding and cardplay rests on an ability to analyze bridges
perfect-information variant, where all of the cards are visible and each side attempts
to take as many tricks as possible (this perfect-information variant is generally referred
to as double dummy bridge). Double dummy problems are solved using a technique
known as partition search, which is discussed in Section 2.
2. Early versions of gib used Monte Carlo methods exclusively to select an action based
on the double dummy analysis. This technique was originally proposed for cardplay
by Levy (Levy, 1989), but was not implemented in a performance program before
gib. Extending Levys suggestion, gib uses Monte Carlo simulation for both cardplay
(discussed in Section 3) and bidding (discussed in Section 4).
3. Section 5 discusses difficulties with the Monte Carlo approach. Frank et al. have
suggested dealing with these problems by searching the space of possible plans for
playing a particular bridge deal, but their methods appear to be intractable in both
theory and practice (Frank & Basin, 1998; Frank, Basin, & Bundy, 2000). We instead
choose to deal with the difficulties by modifying our understanding of the game so
that the value of a bridge deal is not an integer (the number of tricks that can be
taken) but is instead taken from a distributive lattice.
4. In Section 6, we show that the alpha-beta pruning mechanism can be extended to deal
with games of this type. This allows us to find optimal plans for playing bridge end
positions involving some 32 cards or fewer. (In contrast, Franks method is capable
only of finding solutions in 16 card endings.)
5. Finally, applying our ideas to the play of full deals (52 cards) requires solving an
approximate version of the overall problem. In Section 7, we describe the nature of
the approximation used and our application of squeaky wheel optimization (Joslin &
Clements, 1999) to solve it.
Concluding remarks are contained in Section 8.

2. Partition search
Computers are effective game players only to the extent that brute-force search can overcome
innate stupidity; most of their time spent searching is spent examining moves that a human
player would discard as obviously without merit.
As an example, suppose that White has a forced win in a particular chess position,
perhaps beginning with an attack on Blacks queen. A human analyzing the position will
see that if Black doesnt respond to the attack, he will lose his queen; the analysis considers
places to which the queen could move and appropriate responses to each.
305

fiGinsberg

A machine considers responses to the queen moves as well, of course. But it must also
analyze in detail every other Black move, carefully demonstrating that each of these other
moves can be refuted by capturing the Black queen. A six-ply search will have to analyze
every one of these moves five further ply, even if the refutations are identical in all cases.
Conventional pruning techniques cannot help here; using - pruning, for example, the
entire main line (Whites winning choices and all of Blacks losing responses) must be
analyzed even though there is a great deal of apparent redundancy in this analysis.3
In other search problems, techniques based on the ideas of dependency maintenance (Stallman & Sussman, 1977) can potentially be used to overcome this sort of difficulty. As an
example, consider chronological backtracking applied to a map coloring problem. When a
dead end is reached and the search backs up, no information is cached and the effect is to
eliminate only the specific dead end that was encountered. Recording information giving
the reason for the failure can make the search substantially more efficient.
In attempting to color a map with only three colors, for example, thirty countries may
have been colored while the detected contradiction involves only five. By recording the
contradiction for those five countries, dead ends that fail for the same reason can be avoided.
Dependency-based methods have been of limited use in practice because of the overhead
involved in constructing and using the collection of accumulated reasons. This problem has
been substantially addressed in the work on dynamic backtracking (Ginsberg, 1993) and its
successors such as relsat (Bayardo & Miranker, 1996), where polynomial limits are placed
on the number of nogoods being maintained.
In game search, however, most algorithms already include significant cached information
in the form of a transposition table (Greenblatt, Eastlake, & Crocker, 1967; Marsland, 1986).
A transposition table stores a single game position and the backed up value that has been
associated with it. The name reflects the fact that many games transpose in that identical
positions can be reached by swapping the order in which moves are made. The transposition
table eliminates the need to recompute values for positions that have already been analyzed.
These collected observations lead naturally to the idea that transposition tables should
store not single positions and their values, but sets of positions and their values. Continuing
the dependency-maintenance analogy, a transposition table storing sets of positions can
prune the subsequent search far more efficiently than a table that stores only singletons.
There are two reasons that this approach works. The first, which we have already mentioned, is that most game-playing programs already maintain transposition tables, thereby
incurring the bulk of the computational expense involved in storing such tables in a more
general form. The second and more fundamental reason is that when a game ends with one
player the winner, the reason for the victory is generally a local one. A chess game can be
thought of as ending when one side has its king captured (a completely local phenomenon);
a checkers game, when one side runs out of moves. Even if an internal search node is evaluated before the game ends, the reason for assigning it any specific value is likely to be
independent of some global features (e.g., is the Black pawn on a5 or a6?). Partition search
exploits both the existence of transposition tables and the locality of evaluation for realistic
games.
3. An informal solution to this is Adelson-Velskiy et al.s method of analogies (Adelson-Velskiy, Arlazarov,
& Donskoy, 1975). This approach appears to have been of little use in practice because it is restricted
to a specific class of situations arising in chess games.

306

fiGIB: Imperfect information in a computationally challenging game

X X
O
O
X

!
!!
!
!

!

O moves

aa

@

@
@

aa
aa

X X
O O
O
X

X X
O
O O X

X X
O
O
O
X

X X O
O
O
X

X X X
O O
O
X

X X X
O
O O X

X X X
O
O
O
X

X X O
O X
O
X

Figure 1: A portion of the game tree for tic-tac-toe

This section explains these ideas via an example and then describes them formally.
Experimental results for bridge are also presented.
2.1 An example
Our illustrative examples for partition search will be taken from the game of tic-tac-toe.
A portion of the game tree for this game appears in Figure 1, where we are analyzing a
position that is a win for X. We show Os four possible moves, and a winning response
for X in each case. Although X frequently wins by making a row across the top of the
diagram, - pruning cannot reduce the size of this tree because Os losing options must
all be analyzed separately.
Consider now the position at the lower left in the diagram, where X has won:
X X X
O O
O
X

(1)

The reason that X has won is local. If we are retaining a list of positions with known
outcomes, the entry we can make because of this position is:
X X X
? ? ?
? ? ?

(2)

where the ? means that it is irrelevant whether the associated square is marked with an X, an
O, or unmarked. This table entry corresponds not to a single position, but to approximately
36 because the unassigned squares can contain Xs, Os, or be blank. We can reduce the
game tree in Figure 1 to:
307

fiGinsberg

X X
O
O
X

!
!!
!
!

!

X X
O O
O
X

O moves

a
@ aa
aa
@
@
a

X X
O
O O X

X X
O
O
O
X

X X O
O
O
X

@

@
@

X X X
? ? ?
? ? ?

Continuing the analysis, it is clear that the position

X X O
O X
O
X

X X
? ? ?
? ? ?

(3)

is a win for X if X is on play.4 So is
X ? ?
?
?
? ? X

and the tree can be reduced to:
X X
O
O
X






X X
? ? ?
? ? ?

O moves

HH
H
HH

X ? ?
?
?
? ? X

X X X
? ? ?
? ? ?

X ? ?
? X ?
? ? X

Finally, consider the position
X X
?
? ? X

(4)

where it is Os turn as opposed to Xs. If O moves in the second row, we get an instance of
X X
? ? ?
? ? ?

while if O moves to the upper right, we get an instance of
X ? ?
?
?
? ? X
4. We assume that O has not already won the game here, since X would not be on play if the game were
over.

308

fiGIB: Imperfect information in a computationally challenging game

Thus every one of Os moves leads to a position that is known to be a win for X, and we
can conclude that the original position (4) is a win for X as well. The root node in the
reduced tree can therefore be replaced with the position of (4).
These positions capture the essence of the algorithm we will propose: If player x can
move to a position that is a member of a set known to be a win for x, the given position is
a win as well. If every move is to a position that is a loss, the original position is also.
2.2 Formalizing partition search
In this section, we present a summary of existing methods for evaluating positions in game
trees. There is nothing new here; our aim is simply to develop a precise framework in which
our new results can be presented.
Definition 2.2.1 An interval-valued game is a quadruple (G, pI , s, ev), where G is a finite
set of legal positions, pI  G is the initial position, s : G  2G gives the immediate
successors of a given position, and ev is an evaluation function
ev : G  {max, min}  [0, 1]
Informally, p0  s(p) means that position p0 can be reached from p in a single move, and
the evaluation function ev labels internal nodes based upon whose turn it is to play (max or
min) and values terminal positions in terms of some element of the unit interval [0, 1].
The structures G, pI , s and ev are required to satisfy the following conditions:
1. There is no sequence of positions p0 , . . . , pn with n > 0, pi  s(pi1 ) for each i and
pn = p0 . In other words, there are no loops that return to an identical position.
2. ev(p)  [0, 1] if and only if s(p) = . In other words, ev assigns a numerical value to
p if and only if the game is over. Informally, ev(p) = max means that the maximizer
is to play and ev(p) = min means that the minimizer is to play.
We use 2G to denote the power set of G, the set of subsets of G. There are two further
things to note about this definition.
First, the requirement that the game have no loops is consistent with all modern
games. In chess, for example, positions can repeat but there is a concealed counter that
draws the game if either a single position repeats three times or a certain number of moves
pass without a capture or a pawn move. In fact, dealing with the hidden counter is more
natural in a partition search setting than a conventional one, since the evaluation function
is in general (although not always) independent of the value of the counter.
Second, the range of ev includes the entire unit interval [0, 1]. The value 0 represents
a win for the minimizer, and 1 a win for the maximizer. The intermediate values might
correspond to intermediate results (e.g., a draw) or, more importantly, allow us to deal with
internal search nodes that are being treated as terminal and assigned approximate values
because no time remains for additional search.
The evaluation function ev can be used to assign numerical values to the entire set G
of positions:
309

fiGinsberg

Definition 2.2.2 Given an interval-valued game (G, pI , s, ev), we introduce a function
evc : G  [0, 1] defined recursively by

 ev(p),

evc (p) = maxp0 s(p) evc (p0 ),

minp0 s(p) evc (p0 ),

if ev(p)  [0, 1];
if ev(p) = max;
if ev(p) = min.

The value of (G, pI , s, ev) is defined to be evc (pI ).
To evaluate a position in a game, we can use the well-known minimax procedure:
Algorithm 2.2.3 (Minimax) For a game (G, pI , s, ev) and a position p  G, to compute
evc (p):
if ev(p)  [0, 1] return ev(p)
if ev(p) = max return maxp0 s(p) minimax(p0 )
if ev(p) = min return minp0 s(p) minimax(p0 )
There are two ways in which the above algorithm is typically extended. The first involves the introduction of transposition tables; we will assume that a new entry is added
to the transposition table T whenever one is computed. (A modification to cache only
selected results is straightforward.) The second involves the introduction of - pruning.
Incorporating these ideas gives us the algorithm at the top of the next page.
Each entry in the transposition table consists of a position p, the current cutoffs [x, y],
and the computed value v. Note the need to include information about the cutoffs in the
transposition table itself, since the validity of any particular entry depends on the cutoffs
in question.
As an example, suppose that the value of some node is in fact 1 (a win for the maximizer) but that when the node is evaluated with cutoffs of [0, 0.5] a value of 0.5 is returned
(indicating a draw) because the maximizer has an obviously drawing line. It is clear that
this value is only accurate for the given cutoffs; wider cutoffs will lead to a different answer.
In general, the upper cutoff y is the currently smallest value assigned to a minimizing
node; the minimizer can do at least this well in that he can force a value of y or lower.
Similarly, x is the currently greatest value assigned to a maximizing node. These cutoff
values are updated as the algorithm is invoked recursively in the lines responsible for setting
vnew , the value assigned to a child of the current position p.
Proposition 2.2.4 Suppose that v = (p, [x, y]) for each entry (p, [x, y], v) in T . Then if
evc (p)  [x, y], the value returned by Algorithm 2.2.5 is evc (p).

310

fiGIB: Imperfect information in a computationally challenging game

Algorithm 2.2.5 (- pruning with transposition tables) Given an interval-valued
game (G, pI , s, ev), a position p  G, cutoffs [x, y]  [0, 1] and a transposition table T
consisting of triples (p, [a, b], v) with p  G and a  b, v  [0, 1], to compute (p, [x, y]):
if there is an entry (p, [x, y], z) in T return z
if ev(p)  [0, 1] then vans = ev(p)
if ev(p) = max then
vans := 0
for each p0  s(p) do
vnew = (p0 , [max(vans , x), y])
if vnew  y then
T := T  (p, [x, y], vnew )
return vnew
if vnew > vans then vans = vnew
if ev(p) = min then
vans := 1
for each p0  s(p) do
vnew = (p0 , [x, min(vans , y)])
if vnew  x then
T := T  (p, [x, y], vnew )
return vnew
if vnew < vans then vans = vnew
T := T  (p, [x, y], vans )
return vans

2.3 Partitions
We are now in a position to present our new ideas. We begin by formalizing the idea of a
position that can reach a known winning position or one that can reach only known losing
ones.
Definition 2.3.1 Given an interval-valued game (G, pI , s, ev) and a set of positions S  G,
we will say that the set of positions that can reach S is the set of all p for which s(p)S 6= .
This set will be denoted R0 (S). The set of positions constrained to reach S is the set of
all p for which s(p)  S, and is denoted C0 (S).
These definitions should match our intuition; the set of positions that can reach a set S
is indeed the set of positions p for which some element of S is an immediate successor of p,
so that s(p)  S 6= . Similarly, a position p is constrained to reach S if every immediate
successor of p is in S, so that s(p)  S.
Unfortunately, it may not be feasible to construct the R0 and C0 operators explicitly;
there may be no concise representation of the set of all positions that can reach S. In
practice, this will be reflected in the fact that the data structures being used to describe
311

fiGinsberg

the set S may not conveniently describe the set R0 (S) of all situations from which S can
be reached.
Now suppose that we are expanding the search tree itself, and we find ourselves analyzing a particular position p that is determined to be a win for the maximizer because the
maximizer can move from p to the winning set S; in other words, p is a win because it is
in R0 (S). We would like to record at this point that the set R0 (S) is a win for the maximizer, but may not be able to construct or represent this set conveniently. We will therefore
assume that we have some computationally effective way to approximate the R0 and C0
functions, in that we have (for example) a function R that is a conservative implementation
of R0 in that if R says we can reach S, then so we can:
R(p, S)  R0 (S)
R(p, S) is intended to represent a set of positions that are like p in that they can reach
the (winning) set S. Note the inclusion of p as an argument to R(p, S), since we certainly
want p  R(p, S). We are about to cache the fact that every element of R(p, S) is a win
for the maximizer, and certainly want that information to include the fact that p itself has
been shown to be a win. Thus we require p  R(p, S) as well.
Finally, we need some way to generalize the information returned by the evaluation
function; if the evaluation function itself identifies a position p as a win for the maximizer,
we want to have some way to generalize this to a wider set of positions that are also wins.
We formalize this by assuming that we have some generalization function P that respects
the evaluation function in the sense that the value returned by P is a set of positions that
ev evaluates identically.
Definition 2.3.2 Let (G, pI , S, ev) be an interval-valued game. Let f be any function with
range 2G , so that f selects a set of positions based on its arguments. We will say that
f respects the evaluation function ev if whenever p, p0  F for any F in the range of f ,
ev(p) = ev(p0 ).
A partition system for the game is a triple (P, R, C) of functions that respect ev such
that:
1. P : G  2G maps positions into sets of positions such that for any position p, p 
P (p).
2. R : G  2G  2G accepts as arguments a position p and a set of positions S. If
p  R0 (S), so that p can reach S, then p  R(p, S)  R0 (S).
3. C : G  2G  2G accepts as arguments a position p and a set of positions S. If
p  C0 (S), so that p is constrained to reach S, then p  C(p, S)  C0 (S).
As mentioned above, the function P tells us which positions are sufficiently like p that
they evaluate to the same value. In tic-tac-toe, for example, the position (1) where X has
won with a row across the top might be generalized by P to the set of positions
X X X
? ? ?
? ? ?

312

(5)

fiGIB: Imperfect information in a computationally challenging game

as in (2).
The functions R and C approximate R0 and C0 . Once again turning to our tic-tac-toe
example, suppose that we take S to be the set of positions appearing in (5) and that p is
given by
X X
O O
O
X

so that S can be reached from p. R(p, S) might be
X X
? ? ?
? ? ?

(6)

as in (3), although we could also take R(p, S) = {p} or R(p, S) to be
X X
O O
O
X

X X

X

X

 ? ? ?  ? ? ?
? ?

?

? ? ?

although this last union might be awkward to represent. Note again that R and C are
functions of p as well as S; the set returned must include the given position p but can
otherwise be expected to vary as p does.
We will now modify Algorithm 2.2.5 so that the transposition table, instead of caching
results for single positions, caches results for sets of positions. As discussed in the introduction to this section, this is an analog to the introduction of truth maintenance techniques
into adversary search. The modified algorithm 2.3.3 appears in Figure 2 and returns a pair
of values  the value for the given position, and a set of positions that will take the same
value.
Proposition 2.3.4 Suppose that v = (p, [x, y]) for every (S, [x, y], v) in T and p  S.
Then if evc (p)  [x, y], the value returned by Algorithm 2.3.3 is evc (p).
Proof. We need to show that when the algorithm returns, any position in Sans will have
the value vans . This will ensure that the transposition table remains correct.
To see this, suppose that the node being expanded is a maximizing node; the minimizing
case is dual. Suppose first that this node is a loss for the maximizer, having value 0.
In showing that the node is a loss, we will have examined successor nodes that are in sets
denoted Snew in Algorithm 2.3.3; if the maximizer subsequently finds himself in a position
from which he has no moves outside of the various Snew , he will still be in a losing position.
Since Sall = Snew , the maximizer will lose in any position from which he is constrained to
next move into an element of Sall . Since every position in C(p, Sall ) has this property, it
is safe to take Sans = C(p, Sall ). This is what is done in the first line with a dagger in the
algorithm.
The more interesting case is where the eventual value of the node is nonzero; now in
order for another node n to demonstrably have the same value, the maximizer must have
no new options at n, and must still have some move that achieves the value vans at n.
The first condition is identical to the earlier case where vans = 0. For the second, note
that any time the maximizer finds a new best move, we set Sans to the set of positions that
313

fiGinsberg

Algorithm 2.3.3 (Partition search) Given a game (G, pI , s, ev) and (P, R, C) a partition
system for it, a position p  G, cutoffs [x, y]  [0, 1] and a transposition table T consisting
of triples (S, [a, b], v) with S  G and a  b, v  [0, 1], to compute (p, [x, y]):
if there is an entry (S, [x, y], z) with p  S return hz, Si
if ev(p)  [0, 1] then hvans , Sans i = hev(p), P (p)i
if ev(p) = max then
vans := 0
Sall := 
for each p0  s(p) do
hvnew , Snew i = (p0 , [max(vans , x), y])
if vnew  y then
T := T  (Snew , [x, y], vnew )
return hvnew , Snew i
if vnew > vans then hvans , Sans i = hvnew , Snew i
Sall := Sall  Snew
if vans = 0 then Sans = C(p, Sall )

else Sans = R(p, Sans )  C(p, Sall )
 
if ev(p) = min then
vans := 1
Sall := 
for each p0  s(p) do
hvnew , Snew i = (p0 , [x, min(vans , y)])
if vnew  x then
T := T  (Snew , [x, y], vnew )
return hvnew , Snew i
if vnew < vans then hvans , Sans i = hvnew , Snew i
Sall := Sall  Snew
if vans = 1 then Sans = C(p, Sall )
else Sans = R(p, Sans )  C(p, Sall )

T := T  (Sans , [x, y], vans )
return hvans , Sans i
Figure 2: The partition search algorithm

314

fiGIB: Imperfect information in a computationally challenging game

we know recursively achieve the same value. When we complete the maximizers loop in
the algorithm, it follows that Sans will be a set of positions from which the maximizer can
indeed achieve the value vans . Thus the maximizer can also achieve that value from any
position in R(p, Sans ). It follows that the overall set of positions known to have the value
vans is given by R(p, Sans )  C(p, Sall ), intersecting the two conditions of this paragraph.
This is what is done in the second daggered step in the algorithm.
2.4 Zero-window variations
The effectiveness of partition search depends crucially on the size of the sets maintained in
the transposition table. If the sets are large, many positions will be evaluated by lookup.
If the sets are small, partition search collapses to conventional - pruning.
An examination of Algorithm 2.3.3 suggests that the points in the algorithm at which
the sets are reduced the most are those marked with a double dagger in the description,
where an intersection is required because we need to ensure both that the player can make
a move equivalent to his best one and that there are no other options. The effectiveness of
the method would be improved if this possibility were removed.
To see how to do this, suppose for a moment that the evaluation function always returned
0 or 1, as opposed to intermediate values. Now if the maximizer is on play and the value
vnew = 1, a prune will be generated because there can be no better value found for the
maximizer. If all of the vnew are 0, then vans = 0 and we can avoid the troublesome
intersection. The maximizer loses and there is no best move that we have to worry about
making.
In reality, the restriction to values of 0 or 1 is unrealistic. Some games, such as bridge,
allow more than two outcomes, while others cannot be analyzed to termination and need
to rely on evaluation functions that return approximate values for internal nodes. We can
deal with these situations using a technique known as zero-window search (originally called
scout search (Pearl, 1980)). To evaluate a specific position, one first estimates the value
to be e and then determines whether the actual value is above or below e by treating any
value v > e as a win for the maximizer and any value v  e as a win for the minimizer. The
results of this calculation can then be used to refine the guess, and the process is repeated.
If no initial estimate is available, a binary search can be used to find the value to within
any desired tolerance.
Zero-window search is effective because little time is wasted on iterations where the
estimate is wildly inaccurate; there will typically be many lines showing that a new estimate
is needed. Most of the time is spent on the last iteration or two, developing tight bounds
on the position being considered. There is an analog in conventional - pruning, where
the bounds typically get tight quickly and the bulk of the analysis deals with a situation
where the value of the original position is known to lie in a fairly narrow range.
In zero-window search, a node always evaluates to 0 or 1, since either v > e or v  e.
This allows a straightforward modification to Algorithm 2.3.3 that avoids the troublesome
cases mentioned earlier.
315

fiGinsberg

2.5 Experimental results
Partition search was tested by analyzing 1000 randomly generated bridge deals and comparing the number of nodes expanded using partition search and conventional methods.
In addition to our general interest in bridge, there are two reasons why it can be expected
that partition search will be useful for this game. First, partition search requires that the
functions R0 and C0 support a partition-like analysis; it must be the case that an analysis of
one situation will apply equally well to a variety of similar ones. Second, it must be possible
to build approximating functions R and C that are reasonably accurate representatives of
R0 and C0 .
Bridge satisfies both of these properties. Expert discussion of a particular deal often
will refer to small cards as xs, indicating that it is indeed the case that the exact ranks of
these cards are irrelevant. Second, it is possible to back up xs from one position to its
predecessors. If, for example, one player plays a club with no chance of having it impact
the rest of the game, and by doing so reaches a position in which subsequent analysis shows
him to have two small clubs, then he clearly must have had three small clubs originally.
Finally, the fact that cards are simply being replaced by xs means that it is possible to
construct data structures for which the time per node expanded is virtually unchanged from
that using conventional methods.
Perhaps an example will make this clearer. Consider the following partial bridge deal
in which East is to lead and there are no trumps:












AK


10
A












AQ




KJ




An analysis of this situation shows that in the main line, the only cards that win tricks
by virtue of their ranks are the spade Ace, King and Queen. This sanctions the replacement
of the above figure by the following more general one:
316

fiGIB: Imperfect information in a computationally challenging game












xx


x
x












AQ




Kx




Note first that this replacement is sound in the sense that every position that is an
instance of the second diagram is guaranteed to have the same value as the original. We
have not resorted to an informal argument of the form Jacks and lower tend not to matter,
but instead to a precise argument of the form, In the expansion of the search tree associated
with the given deal, Jacks and lower were proven never to matter.
Bridge also appears to be extremely well-suited (no pun intended) to the kind of analysis
that we have been describing; a chess analog might involve describing a mating combination
and saying that the position of Blacks queen didnt matter. While this does happen,
casual chess conversation is much less likely to include this sort of remark than bridge
conversation is likely to refer to a host of small cards as xs, suggesting at least that the
partition technique is more easily applied to bridge than to chess (or to other games).
That said, however, the results for bridge are striking, leading to performance improvements of an order of magnitude or more on fairly small search spaces (perhaps 106 nodes).
The deals we tested involved between 12 and 48 cards and were analyzed to termination, so
that the depth of the search varied from 12 to 48. (The solver without partition search was
unable to solve larger problems.) The branching factor for minimax without transposition
tables appeared to be approximately 4, and the results appear in Figure 3.
Each point in the graph corresponds to a single deal. The position of the point on the
x-axis indicates the number of nodes expanded using - pruning and transposition tables,
and the position on the y-axis the number expanded using partition search as well. Both
axes are plotted logarithmically.
In both the partition and conventional cases, a binary zero-window search was used to
determine the exact value to be assigned to the hand, which the rules of bridge constrain
to range from 0 to the number of tricks left (one quarter of the number of cards in play).
As mentioned previously, hands generated using a full deck of 52 cards were not considered
because the conventional method was in general incapable of solving them. The program was
run on a Sparc 5 and PowerMac 6100, where it expanded approximately 15K nodes/second.
The transposition table shares common structure among different sets and as a result, uses
approximately 6 bytes/node.
The dotted line in the figure is y = x and corresponds to the breakeven point relative to
- pruning in isolation. The solid line is the least-squares best fit to the logarithmic data,
and is given by y = 1.57x0.76 . This suggests that partition search is leading to an effective
reduction in branching factor of b  b0.76 . This improvement, above and beyond that
317

fiGinsberg

107

p

1.57x0.76

105

Partition
103

10

p p

p

p

p
p p
p p pp p p
p
p
p pp p
p p p pp p p p p p p p p p p pp p
p p pp pppppppppp pp pppppppp ppppppppp p p pppp pp ppp p
pp p p p p p p
p pppp p p p pp p p p p pp ppp p p p p p p p
ppp p ppp pppp pppp pppp p pp ppppp pppppp pp p pppp ppp p pp p
p
p pp p p p p p p p pp p p p p pp p p pp p p p p
pp p p p pp pppppppp ppppp ppppp pppppppppp pp ppp p pp ppp p ppp p p p p p p p p
p p p p p p p p pp p p
p p p p ppppp p p pp pp pppppppp p p p p p p p p p p p
pp p pppppppppppppppp ppppppp pppp pp ppp ppppp pppppp pp p p p pp pp pp p p pp
p p
p p p pppp ppp p pp p ppppp pp p p p p p p p p p p
p ppp pp pp pppppp pppppp ppppppppppp pp p ppp pp p p p p ppp p ppp p p p
p
p
p
p pppp pp pppp p p p ppppp p p p p pp p p p p p
p pp pppppppppppppppppppppp pppp p pp pp p p p p pp p p
p
p
p
p
p
p
p
pppp p p p p p pp
p p p p p p ppp p p

p

10

103

105

107

Conventional
Figure 3: Nodes expanded as a function of method
provided by - pruning, can be contrasted with - pruning itself, which gives a reduction
when compared to pure minimax of b  b0.75 if the moves are ordered randomly (Pearl,
1982) and b  b0.5 if the ordering is optimal.
The method was also applied to full deals of 52 cards, which can be solved while expanding an average of 18,000 nodes per deal.5 This works out to about a second of cpu
time.

3. Monte Carlo cardplay algorithms
One way in which we might use our perfect-information cardplay engine to proceed in a
realistic situation would be to deal the unseen cards at random, biasing the deal so that it
was consistent both with the bidding and with the cards played thus far. We could then
analyze the resulting deal double dummy and decide which of our possible plays was the
strongest. Averaging over a large number of such Monte Carlo samples would allow us to
deal with the imperfect nature of bridge information. This idea was initially suggested by
Levy (Levy, 1989), although he does not appear to have realized (see below) that there are
problems with it in practice.
Algorithm 3.0.1 (Monte Carlo card selection) To select a move from a candidate set
M of such moves:
5. The version of gib that was released in October of 2000 replaced the transposition table with a data
structure that uses a fixed amount of memory, and also sorts the moves based on narrowness (suggested
by Plaat et al. (Plaat, Schaeffer, Pijls, & de Bruin, 1996) to be rooted in the idea of conspiracy search
(McAllester, 1988)) and the killer heuristic. While the memory requirements are reduced, the overall
performance is little changed.

318

fiGIB: Imperfect information in a computationally challenging game

1. Construct a set D of deals consistent with both the bidding and play of the deal thus
far.
2. For each move m  M and each deal d  D, evaluate the double dummy result of
making the move m in the deal d. Denote the score obtained by making this move
s(m, d).
3. Return that m for which

P

d s(m, d)

is maximal.

The Monte Carlo approach has drawbacks that have been pointed out by a variety of
authors, including Koller and others (Frank & Basin, 1998). Most obvious among these
is that the approach never suggests making an information gathering play. After all,
the perfect-information variant on which the decision is based invariably assumes that the
information will be available by the time the next decision must be made! Instead, the
tendency is for the approach to simply defer important decisions; in many situations this
may lead to information gathering inadvertently, but the amount of information acquired
will generally be far less than other approaches might provide.
As an example, suppose that on a particular deal, gib has four possible lines of play to
make its contract:
1. Line A works if West has the Q.
2. Line B works if East has the Q.
3. Line C defers the guess until later.
4. Line D (the clever line) works independent of who has the Q.
Assuming that either player is equally likely to hold the Q, a Monte Carlo analyzer
will correctly conclude that line A works half the time, and line B works half the time. Line
C, however, will be presumed to work all of the time, since the contract can still be made
(double dummy) if the guess is deferred. Line D will also be concluded to work all of the
time (correctly, in this case).
As a result, gib will choose randomly between the last two possibilities above, believing
as it does that if it can only defer the guess until later (even the next card), it will make
that guess correctly. The correct play, of course, is D.
We will discuss a solution to these difficulties in Sections 57; although gibs defensive
cardplay continues to be based on the above ideas, its declarer play now uses stronger techniques. Nevertheless, basing the card play on the algorithm presented leads to extremely
strong results, approximately at the level of a human expert. Since gibs introduction, all
other competitive bridge-playing programs have switched their cardplay to similar methods, although gibs double dummy analysis is substantially faster than most of the other
programs and its play is correspondingly stronger.
We will describe three tests of GIBs cardplay algorithms: Performance on a commercially available set of benchmarks, performance in a human championship designed to
highlight cardplay in isolation, and statistical performance measured over a large set of
deals.
319

fiGinsberg

For the first test, we evaluated the strength of gibs cardplay using Bridge Master (BM),
a commercial program developed by Canadian internationalist Fred Gitelman. BM contains
180 deals at 5 levels of difficulty. Each of the 36 deals on each level is a problem in declarer
play. If you misplay the hand, BM moves the defenders cards around if necessary to ensure
your defeat.
BM was used for the test instead of randomly dealt deals because the signal to noise ratio is far higher; good plays are generally rewarded and bad ones punished. Every deal also
contains a lesson of some kind; there are no completely uninteresting deals where the line
of play is irrelevant or obvious. There are drawbacks to testing gibs performance on nonrandomly dealt deals, of course, since the BM deals may in some way not be representative
of the problems a bridge player would actually encounter at the table.
The test was run under Microsoft Windows on a 200 MHz Pentium Pro. As a benchmark,
Bridge Baron (BB) version 6 was also tested on the same deals using the same hardware.6
BB was given 10 seconds to select each play, and gib was given 90 seconds to play the entire
deal with a maximum Monte Carlo sample size of 50.7 New deals were generated each time
a play decision needed to be made.
These numbers approximately equalized the computational resources used by the two
programs; BB could in theory take 260 seconds per deal (ten seconds on each of 26 plays),
but in practice took substantially less. Gib was given the auctions as well; there was no
facility for doing this in BB. This information was critical on a small number of deals.
Here is how the two systems performed:
Level
1
2
3
4
5
Total

BB
16
8
2
1
4
33
18.3%

GIB
31
23
12
21
13
100
55.6%

Each entry is the number of deals that were played successfully by the program in question.
Gibs mistakes are illuminating. While some of them involve failing to gather information, most are problems in combining multiple chances (as in case D above). As BMs deals
get more difficult, they more often involve combining a variety of possibly winning options
and that is why GIBs performance falls off at levels 2 and 3.
At still higher levels, however, BM typically involves the successful development of
complex end positions, and gibs performance rebounds. This appeared to happen to BB
as well, although to a much lesser extent. It was gratifying to see gib discover for itself the
complex end positions around which the BM deals are designed, and more gratifying still
to witness gibs discovery of a maneuver that had hitherto not been identified in the bridge
literature, as described in Appendix B.
6. The current version is Bridge Baron 10 and could be expected to perform guardedly better in a test such
as this. Bridge Baron 6 does not include the Smith enhancements (Smith et al., 1996).
7. GIBs Monte Carlo sample size is fixed at 50 in most cases, which provides a good compromise between
speed of play and accuracy of result.

320

fiGIB: Imperfect information in a computationally challenging game

Experiments such as this one are tedious, because there is no text interface to a commercial program such as Bridge Master or Bridge Baron. As a result, information regarding
the sensitivity of gibs performance to various parameters tends to be only anecdotal.
Gib solves an additional 16 problems (bringing its total to 64.4%) given additional
resources in the form of extra time (up to 100 seconds per play, although that time was
very rarely taken), a larger Monte Carlo sample (100 deals instead of 50) and hand-generated
explanations of the opponents bids and opening leads. Each of the three factors appeared
to contribute equally to the improved performance.
Other authors are reporting comparable levels of performance for gib. Forrester, working
with a different but similar benchmark (Blackwood, 1979), reports8 that gib solves 68% of
the problems given 20 seconds/play, and 74% of them given 30 seconds/play. Deals where
gib has outplayed human experts are the topic of a series of articles in the Dutch bridge
magazine IMP (Eskes, 1997, and sequels).9 Based on these results, gib was invited to
participate in an invitational event at the 1998 world bridge championships in France; the
event involved deals similar to Bridge Masters but substantially more difficult. Gib joined
a field of 34 of the best card players in the world, each player facing twelve such problems
over the course of two days. Gib was leading at the halfway mark, but played poorly on
the second day (perhaps the pressure was too much for it), and finished twelfth.
The human participants were given 90 minutes to play each deal, although they were
penalized slightly for playing slowly. GIB played each deal in about ten minutes, using a
Monte Carlo sample size of 500; tests before the event indicated little or no improvement
if gib were allotted more time. Michael Rosenberg, the eventual winner of the contest and
the pre-tournament favorite, in fact made one more mistake than did Bart Bramley, the
second place finisher. Rosenberg played just quickly enough that Bramleys accumulated
time penalties gave Rosenberg the victory. The scoring method thus favors GIB slightly.
Finally, gibs performance was evaluated directly using records from actual play. These
records are available from high levels of human competition (world and national championships, typically), so that it is possible to determine exactly how frequently humans make
mistakes at the bridge table. In Figure 4, we show the frequency with which this data
indicates that a human declarer, leading to the nth trick of a deal, makes a mistake that
causes his contract to become unmakeable on a double-dummy basis. The y axis gives the
frequency of the mistakes and is plotted logarithmically; as one would expect, play becomes
more accurate later in the deal.
We also give similar data for gib, based on large sample of deals that gib played against
itself. The error profiles of the two are quite similar.
Before turning to defensive play, let me point out that this method of analysis favors gib
slightly. Failing to make an information gathering play gets reflected in the above figure,
since the lack of information will cause gib to make a double-dummy mistake subsequently.
But human declarers often work to give the defenders problems that exploit their relative
lack of information, and that tactic is not rewarded in the above analysis. Similar results
for defensive play appear in Figure 5.
8. Posting to rec.games.bridge on 14 July 1997.
9. http://www.imp-bridge.nl

321

fiGinsberg

0.1

human
GIB

0.01
P(err)
0.001

0.0001

0

2

4

6
trick

8

10

12

Figure 4: Gibs performance as declarer

0.1

human
GIB

0.01

P(err) 0.001

0.0001

1e-05

0

2

4

6
trick

8

Figure 5: Gibs performance as defender

322

10

12

fiGIB: Imperfect information in a computationally challenging game

There are two important technical remarks that must be made about the Monte Carlo
algorithm before proceeding. First, note that we were cavalier in simply saying, Construct
a set D of deals consistent with both the bidding and play of the deal thus far.
To construct deals consistent with the bidding, we first simplify the auction as observed,
building constraints describing each of the hands around the table. We then deal hands
consistent with the constraints using a deal generator that deals unbiased hands given
restrictions on the number of cards held by each player in each suit. This set of deals is
then tested to remove elements that do not satisfy the remaining constraints, and each of the
remaining deals is passed to the bidding module to identify those for which the observed bids
would have been made by the players in question. (This assumes that gib has a reasonable
understanding of the bidding methods used by the opponents.) The overall dealing process
typically takes one or two seconds to generate the full set of deals needed by the algorithm.
Now the card play must be analyzed. Ideally, gib would do something similar to what it
does for the bidding, determining whether each player would have played as indicated on any
particular deal. Unfortunately, it is simply impractical to test each hypothetical decision
recursively against the cardplay module itself. Instead, gib tries to evaluate the probability
that West (for example) has the K (for example), and to then use these probabilities to
weight the sample itself.
To understand the source of the weighting probabilities, let us consider a specific example. Suppose that in some particular situation, gib plays the 5. The analysis indicates
that 80% of the time that the next player (say West) holds the K, it is a mistake for West
not to play it. In other words, Wests failure to play the K leads to odds of 4:1 that he
hasnt got it.
These odds are now used via Bayes rule to adjust the probability that West holds the
K at all. The probabilities are then modified further to include information revealed by
defensive signalling (if any), and the adjusted probabilities are finally used to bias the Monte
P
P
Carlo sample. The evaluation d s(m, d) in Algorithm 3.0.1 is replaced with d wd s(m, d)
where wd is the weight assigned to deal d. More heavily weighted deals thus have a larger
impact on gibs eventual decision.
The second technical point regarding the algorithm itself involves the fact that it needs
to run quickly and that it may need to be terminated before the analysis is complete. For the
former, there are a variety of greedy techniques that can be used to ensure that a move m
P
P
is not considered if we can show d s(d, m)  d s(d, m0 ) for some m0 . The algorithm also
uses iterative broadening (Ginsberg & Harvey, 1992) to ensure that a low-width answer
is available if a high-width search fails to terminate in time. Results from the low- and
high-width searches are combined when time expires.
Also regarding speed, the algorithm requires that for each deal in the Monte Carlo
sample and each possible move, we evaluate the resulting position exactly. Knowing simply
that move m1 is not as good as move m2 for deal d is not enough; m1 may be better than m2
elsewhere and we need to compare them quantitatively. This approach is aided substantially
by the partition search idea, where entries in the transposition table correspond not to single
positions and their evaluated values, but to sets of positions and values. In many cases,
m1 and m2 may fall into the same entry of the partition table long before they actually
transpose into one another exactly.
323

fiGinsberg

4. Monte Carlo bidding
The purpose of bidding in bridge is twofold. The primary purpose is to share information
about your cards with your partner so that you can cooperatively select an optimal final
contract. A secondary purpose is to disrupt the opponents attempt to do the same.
In order to achieve this purpose, a wide variety of bidding languages have been developed. In some, when you suggest clubs as trumps, it means you have a lot of them. In
others, the suggestion is only temporary and the information conveyed is quite different.
In all of these languages, some meaning is assigned to a wide variety of bids in particular
situations; there are also default rules that assign meanings to bids that have no specifically
assigned meanings. Any computer bridge player will need similar understandings.
Bidding is interesting because the meanings frequently overlap; there may be one or
more bids that are suitable (or nearly so) on any particular set of cards. Existing computer
programs have simply matched possible bids against large databases giving their meanings,
searching for that bid that best matches the cards that the machines hold. World champion
Chip Martel reports that human experts take a different approach.10,11
Although expert bidding is based on a database such as that used by existing programs,
close decisions are made by simulating the results of each candidate action. This involves
projecting how the bidding is likely to proceed and evaluating the play in one of a variety of
possible final contracts. An expert gets his judgment from a Monte Carlo-like simulation
of the results of possible bids, often referred to in the bridge-playing community as a Borel
simulation (so named after the first player to describe the method). Gib takes a similar
tack.
Algorithm 4.0.2 (Borel simulation) To select a bid from a candidate set B, given a
database Z that suggests bids in various situations:
1. Construct a set D of deals consistent with the bidding thus far.
2. For each bid b  B and each deal d  D, use the database Z to project how the auction
will continue if the bid b is made. (If no bid is suggested by the database, the player
in question is assumed to pass.) Compute the double dummy result of the eventual
contract, denoting it s(b, d).
3. Return that b for which

P

d s(b, d)

is maximal.

As with the Monte Carlo approach to card play, this approach does not take into account
the fact that bridge is not played double dummy. Human experts often choose not to make
bids that will convey too much information to the opponents in order to make the defenders
task as difficult as possible. This consideration is missing from the above algorithm.12
10. The 1994 Rosenblum Cup World Team Championship was won by a team that included Martel and
Rosenberg.
11. Frank suggests (Frank, 1998) that the existing machine approach is capable of reaching expert levels of
performance. While this appears to have been true in the early 1980s (Lindelof, 1983), modern expert
bidding practice has begun to highlight the disruptive aspect of bidding, and machine performance is no
longer likely to be competitive.
12. In theory at least, this issue could be addressed using the single-dummy ideas that we will present in
subsequent sections. Computational considerations currently make this impractical, however.

324

fiGIB: Imperfect information in a computationally challenging game

There are more serious problems also, generally centering around the development of
the bidding database Z.
First, the database itself needs to be built and debugged. A large number of rules need
to be written, typically in a specialized language and dependent upon the bridge expertise
of the author. The rules need to be debugged as actual play reveals oversights or other
difficulties.
The nature and sizes of these databases vary enormously, although all of them represent
very substantial investments on the part of the authors. The database distributed with
meadowlark bridge includes some 7300 rules; that with q-plus bridge 2500 rules
comprising 40,000 lines of specialized code. Gibs database is built using a derivative of the
Meadowlark language, and includes about 3000 rules.
All of these databases doubtless contain errors of one sort or another; one of the nice
things about most bidding methods is that they tend to be fairly robust against such problems. Unfortunately, the Borel algorithm described above introduces substantial instability
in gibs overall bidding.
To understand this, suppose that the database Z is somewhat conservative in its actions.
The projection in step 2 of Algorithm 4.0.2 now leads each player to assume its partner bids
conservatively, and therefore to bid somewhat aggressively to compensate. The partnership
as a whole ends up over compensating.
Worse still, suppose that there is an omission of some kind in Z; perhaps every time
someone bids 7, the database suggests a foolish action. Since 7 is a rare bid, a bidding system that matches its bids directly to the database will encounter this problem
infrequently.
Gib, however, will be much more aggressive, bidding 7 often on the grounds that
doing so will cause the opponents to make a mistake. In practice, of course, the bug in the
database is unlikely to be replicated in the opponents minds, and gibs attempts to exploit
the gap will be unrewarded or worse.
This is a serious problem, and appears to apply to any attempt to heuristically model
an adversarys behavior: It is difficult to distinguish a good choice that is successful because
the opponent has no winning options from a bad choice that appears successful because the
heuristic fails to identify such options.
There are a variety of ways in which this problem might be addressed, none of them
perfect. The most obvious is simply to use gibs aggressive tendencies to identify the bugs
or gaps in the bidding database, and to fix them. Because of the size of the database, this
is a slow process.
Another approach is to try to identify the bugs in the database automatically, and to be
wary in such situations. If the bidding simulation indicates that the opponents are about
to achieve a result much worse than what they might achieve if they saw each others cards,
that is evidence that there may be a gap in the database. Unfortunately, it is also evidence
that gib is simply effectively disrupting its opponents efforts to bid accurately.
Finally, restrictions could be placed on gib that require it to make bids that are close
to the bids suggested by the database, on the grounds that such bids are more likely to
reflect improvements in judgment than to highlight gaps in the database.
All of these techniques are used, and all of them are useful. Gibs bidding is substantially
better than that of earlier programs, but not yet of expert caliber.
325

fiGinsberg

The bidding was tested as part of the 1998 Baron Barclay/OKBridge World Computer
Bridge Championships, and the 2000 Orbis World Computer Bridge Championship. Each
program bid deals that had previously been bid and played by experts; a result of 0 on any
particular deal meant that the program bid to a contract as good as the average expert
result. A positive result was better, and a negative result was worse.
There were 20 deals in each contest; although card play was not an issue, the deals were
selected to pose challenges in bidding and a standard deviation of 5.5 imps/deal is still a
reasonable estimate. One standard deviation over the 20 deal set could thus be expected
to be about 25 imps.
Gibs final score in the 1998 bidding contest was +2 imps; in the 2000 contest it was +9
imps. In both cases, it narrowly edged out the expert field against which it was compared.13
The next best program in 1998, Blue Chip Bridge, finished with a score of -35 imps, not
dissimilar from the -37 imps that had been sufficient to win the bidding contest in 1997.
The second place program in 2000 (once again Blue Chip Bridge) had a score of -2 imps.

5. The value of information
In previous sections of this paper, we have described Monte Carlo methods for dealing with
the fact that bridge is a game of imperfect information, and have also described possible
problems with this approach. We now turn to ways to overcomes some of these difficulties.
For the moment, let me assume that we replace bridge with a {0, 1} game, so that we
are interested only in the question of whether declarer makes his contract. Overtricks or
extra undertricks are irrelevant. At least as a first approximation, bridge experts often look
at hands this way, only subsequently refining the analysis.
If you ask such an expert why he took a particular line on a deal, he will often say
something like, I was playing for each opponent to have three hearts, or I was playing
for West to hold the spade queen. What he is reporting is that set of distributions of the
unseen cards for which he was expecting to make the hand.
At some level, the expert is treating the value of the game not as zero or one (which
it would be if he could see the unseen cards), but as a function from the set of possible
distributions of unseen cards into {0, 1}. If we denote this set of distributions by S, the
value of the game is thus a function
f : S  {0, 1}
We will follow standard mathematical notation and denote the set {0, 1} by 2 and denote
the set of functions f : S  2 by 2S .
It is possible to extend max and min from the set {0, 1} to 2S in a pointwise fashion, so
that, for example
min(f, g)(s) = min(f (s), g(s))
(7)
for functions f, g  2S and a specific situation s  S. The maximizing function is defined
similarly.
13. This is in spite of the earlier remark that GIBs bidding is not of expert caliber. GIB was fortunate in
the bidding contests in that most of the problems involved situations handled by the database. When
faced with a situation that it does not understand, GIBs bidding deteriorates drastically.

326

fiGIB: Imperfect information in a computationally challenging game

As an example, suppose that in a particular situation, there is one line of play f that
wins if West has the Q. There is another line of play g that wins if East has exactly
three hearts. Now min(f, g) is the line of play that wins just in case both West has the Q
and East has three hearts, while max(f, g) is the line of play that wins if either condition
obtains.
It is important to realize that the set 2S is not totally ordered by these max and min
functions, like the unit interval is. Instead, 2S is an instance of a mathematical structure
known as a lattice (Gratzer, 1978, and Section 6). At this point, we note only that we can
extend Definition 2.2.1 to any set with maximization and minimization operators:
Definition 5.0.3 A game is an octuple (G, V, pI , s, ev, f+ , f ) such that:
1. G is a finite set of possible positions in the game.
2. V is the set of values for the game.
3. pI  G is the initial position of the game.
4. s : G  2G gives the successors of a given position.
5. ev : G  {max, min}  V gives the value for terminal positions or indicates which
player is to move for nonterminal positions.
6. f+ : P(V )  V and f : P(V )  V are the combination functions for the maximizer
and minimizer respectively.
The structures G, V , pI , s and ev are required to satisfy the following conditions (unchanged
from Definition 2.2.1):
1. There is no sequence of positions p0 , . . . , pn with n > 0, pi  s(pi1 ) for each i and
pn = p0 . In other words, there are no loops that return to an identical position.
2. ev(p)  V if and only if s(p) = .
This definition extends Definition 2.2.1 only in that the value set and combination
functions have been generalized. A such, Definition 5.0.3 includes both conventional
games in which the values are numeric and the combination functions are max/min, and
our more general setting where the values are functional and the combination functions
combine them as described above.
As usual, we can use the maximization and minimization functions to assign a value to
the root of the tree:
Definition 5.0.4 Given a game (G, V, pI , s, ev, f+ , f ), we introduce a function evc : G 
V defined recursively by

 ev(p),

evc (p) =

f {evc
 +

(p0 )|p0

 s(p)},
f {evc (p0 )|p0  s(p)},

if ev(p)  V ;
if ev(p) = max;
if ev(p) = min.

The value of (G, V, pI , s, ev, f+ , f ) is defined to be evc (pI ).
327

fiGinsberg

The definition is well founded because the game has no loops, and it is straightforward
to extend the minimax algorithm 2.2.3 to this more general formalism. We will discuss
extensions of - pruning in the next section.
To flesh out our previous informal description, we need to instantiate Definition 5.0.3.
We do this by having the value of any particular node correspond to the set of positions
where the maximizer can win:
1. The set G of positions is a set of pairs (p, Z) where p is a position with only two of
the four bridge hands visible (i.e., a position in the single dummy game), and Z is
that subset of S (the set of situations) that is consistent both with p and with the
cards that were played to reach p from the initial position.
2. The value set V is 2S .
3. The initial position pI is (p0 , S), where p0 is the initial single-dummy position.
4. The successor function is described as follows:
(a) If the declarer/maximizer is on play in the given position, the successors are
obtained by enumerating the maximizers legal plays and leaving the set Z of
situations unchanged.
(b) If the minimizer is on play in the given position, the successors are obtained by
playing any card c that is legal in any element of Z and then restricting Z to
that subset for which c is in fact a legal play.
5. Terminal nodes are nodes where all cards have been played, and therefore correspond
to single situations s, since the locations of all cards have been revealed. For such a
terminal position, if the declarer has made his contract, the value is S (the entire set
of positions possible at the root). If the declarer has failed to make his contract, the
value is S  {s}.
6. The maximization and minimization functions are computed pointwise, so that
f+ (U, V ) = U  V
and
f (U, V ) = U  V
Given an initial single-dummy situation p corresponding to a set S of situations, we will
call the above game the (p, S) game.
Proposition 5.0.5 Suppose that the set of situations for which the maximizer can make
his contract is T  S. Then the value of the (p, S) game is T .
It is natural to view T as an element of 2S ; it is the function mapping points in T to 1
and points outside of T to 0.
Proof. The proof proceeds by induction on the depth of the game tree. If the root node
p is also terminal, then S = {s} and the value is clearly set correctly (to s or ) by the
definition of the (p, S) game.
328

fiGIB: Imperfect information in a computationally challenging game

If p is nonterminal, suppose first that it is a maximizing node. Now let s  S be some
particular situation. If the maximizer can win in s, then there is some successor (p0 , S 0 )
to (p, S) where the maximizer wins, and hence by the inductive hypothesis, the value of
(p0 , S 0 ) is a set U with s  U . But since the maximizer moves in p, the value assigned to
(p, S) is a superset of the value assigned to any subnode, so that s  evc (p, S) = T .
If, on the other hand, the maximizer cannot win in s, then he cannot win in any child
of s. If (pi , Si ) are the successors of (p, S) in the game tree, then again by the inductive
hypothesis, we must have s 6 evc (pi , Si ) for each i. But
evc (p, S) = i evc (pi , Si )
so that s 6 evc (p, S) = T .
For the minimizing case, suppose that the maximizer wins in s. Then the maximizer
must win in every successor of s, so that s  evc (pi , Si ) for each such successor and therefore
s  evc (p, S). Alternatively, if the minimizer wins in s, he must have a legal winning option
so that s 6 evc (pi , Si ) for some i and therefore s 6 evc (p, S).
Unfortunately, Proposition 5.0.5 is in some sense exactly what we wanted not to prove:
it says that our modified game computes the set of situations in which it is possible for the
maximizer to make his contract if he has perfect information about the opponents cards,
not the set of situations in which it is possible for him to make his contract given his actual
state of incomplete information.
Before we go on to deal with this, however, let me look at an example in some detail.
The example we will use is similar to that of Section 3 and involves a situation where the
maximizer can make his contract if either West has the Q or East has three hearts. I will
denote by S the set of situations where West has the Q, and by T the set where East has
three hearts. Its possible to tie in the defer the guess example from Section 3 as well, so
I will do that also. Here is the game tree for the game in question:

min q



A
 A
T
A
q
Aq

S
1

0

q max
P
@PPP
PP
@
P
@ max PPP
@q
Pq min
min q
A
A
A

 A
 A
S A T
S
T


A
A
A
Aq min q
Aq min q
Aq
q
C
C
0
1
1
1


S C T S C T
C
C
q Cq
q Cq





1

0

0

1

At the root node, the maximizer has four choices. If he makes the move on the left
(playing for S, as it turns out), the minimizer then moves in a situation where the maximizer
wins if S holds and loses if T holds. For the second move, where the maximizer is essentially
playing for T , the reverse is true.
In the third case, the maximizer defers the guess. We suppose that he is on play again
immediately, forced to commit between playing for S and playing for T . In the last case,
he wins independent of whether T or S obtains.
329

fiGinsberg

In the Monte Carlo setting, the above tree will actually be split based on the element of
the sample in question. In some cases, S will be true and we will examine only this subtree:
q max
P
PP

@
PP


@
PP


PP

@
PPq min
@q max
min q
min q


A



 A

S
S
S


A
Aq min q
q
q
min q


1
0
1


S
S
q
q

1

0

The maximizer can win by making any move other than the second. In the cases where T
obtains, we examine:
q max
P
PP

@
PP


@
PP


PP

@
PPq min
@q max
q 
min 
min q
A
A
A
A
 A
AT
AT
AT

A
A
A
A
Aq
Aq min q
Aq min
Aq
C
C
0
1
1
CT
CT
C
C
Cq
Cq

0

1

Here, the maximizer can win by making any move other than the first. In all cases, both
of the last two moves win for the maximizer, since this approach cannot recognize the fact
that the third move simply defers the guess while the fourth wins outright.
Now let us return to the situation where we include information about the sets that it
is possible to play for. Here is the tree again:
q max
P
@PPP
PP

@
PP


@
PP
@q max
Pq min
min q
min q
A
A
A
A
 A
 A


S A T
S A T
S
T


A
A
A
A
Aq
Aq min q
Aq min q
Aq
q
q
C
C
1
0
0
1
1
1


S C T S C T
C
C
q Cq
q Cq



1

0

0

1

The first thing that we need to do is to realize that the terminal nodes should not be
labelled with 1s and 0s but instead with sets where the maximizer can win. This produces:
330

fiGIB: Imperfect information in a computationally challenging game

q max
P
@PPP
PP

@
PP


@
PP
q 
@q max
Pq min
min 
min q
A
A
A
A


 A
 A
S A T
S A T
S
T


A
A
A
A
q
Aq
q
Aq min q
Aq min q
Aq
ST
S
T
ST
ST
ST
C
C


S C T S C T
C
C
q Cq
q Cq



ST

S

T

ST

To understand the labels, consider the two leftmost fringe nodes. The leftmost node gets
labelled with T for free because T is eliminated by the fact that the minimizer chose S.
Since the maximizer wins in S, the maximizer wins in all cases.
For the second fringe node, S is included by virtue of the minimizers moving to T ; T
is not included because the minimizer actually wins on this line. Hence the label of T for
the node in question. This analysis assumes that S and T are disjoint; if they overlap, the
labels become slightly more complex but the overall analysis is little changed.
Backing up the values one step gives us:
q max
P
PP

@

PP

@
PP



@ max PPP

S 
T q
q
@q
Pq S  T
A
A
A
A
 A
 A
 A
 A




A
A
A
A
S 
q
Aq
q
Aq
q
Aq T
q
Aq
ST
S
T
ST
ST
ST
C
C
 C
 C
 C
 C
q Cq
q Cq
ST

S

T

ST

The minimizer, playing with perfect information, always does as best he can. The first
interior nodes label of S, for example, means that the maximizer wins only if S actually is
the case.
Of course, our definitions thus far imply that the maximizer is playing with perfect
information as well, and we can back up the rest of the tree to get:
qS T
P
PP

@
PP


PP
@


PP

@
PPq S  T
S 
T q
@q S  T
q 
A
A
A
A
 A
 A
 A
 A




A
A
A
A
S 
Aq
Aq
Aq T
Aq
q
q
q
q
ST
S
T
ST
ST
ST
C
C
 C
 C
 C
 C
q Cq
q Cq
ST

331

S

T

ST

fiGinsberg

0.1

declare
defend

0.01

P(err) 0.001

0.0001

1e-05

0

2

4

6
trick

8

10

12

Figure 6: Defense vs. declarer play for humans
As before, the maximizer wins with either of the last two options.
Before we address the fact that the players do not in fact have perfect information,
let me point out that in most bridge analyses, imperfect information is assumed to be an
issue for the maximizer only. The defenders are assumed to be operating with complete
information for at least the following reasons:
1. In general, there is a premium for declaring as opposed to defending, so that both
sides want to declare. Typically, the pair with greater assets in terms of high cards
wins the bidding battle and succeeds in becoming the declaring side, so that the
overall assets available to the defenders in terms of high cards are generally less than
those available to the declarer. This means that the defenders will generally be able
to predict each others hands with more accuracy than the declarer can.
2. The defenders can signal, conveying to one another information about the cards they
hold. (As an example, play of an unnecessarily high card often indicates an even
number of cards in the suit being played.) They are generally assumed to signal only
information that is useful to them but not to declarer, once again improving their
collective ability to play as if they had perfect information.
3. After the first two or three tricks, defenders play is typically closer to double dummy
than is the declarers. This is shown in Figure 6, which contrasts the quality of human
play as defender with the quality of human play as declarer; we make more mistakes
declaring than defending as of trick four. (This figure is analogous to Figures 4 and
5.)
332

fiGIB: Imperfect information in a computationally challenging game

There are some deals where it is important for declarer to exploit uncertainty on the part
of the defenders, but these are definitely the exception as opposed to the rule.
This suggests that Proposition 5.0.5 is doing a reasonable job of modeling the defenders
cardplay, but the combination function for the maximizer needs to be modified to reflect
the imperfect-information nature of his task.
To understand this, let us return to our putative expert, who suggested at the beginning
of this section that he might be playing for West to hold the spade queen. What he might
say in a bit more detail is, I could play for each opponent to hold exactly three hearts, or
I could play for West to hold the spade queen. The latter was the better chance.
This suggests that the value assigned to the position by the maximizer is not a single
set of situations (those in which he can make the contract), but a set S of sets of situations.
Each set S  S corresponds to one set of situations that the maximizer could play for, given
his incomplete knowledge of the positions of the opposing cards.
Extending the notation used earlier in this section, we will denote the set of sets of
S
S
situations by 22 . The maximizers combination function on 22 is given by
max(F, G) = F  G

(8)

where each of F and G are sets of sets of situations. This says that if the maximizer is on
play in a situation p, and he has one move that will allow him to select from a set F of
things to play for and another move that will allow him to select from a set G, then his
choice at p is to select from any element of F  G.
The minimizers function is a bit more subtle. Suppose that at a node p, the minimizer
can move to a successor with value F = {Fi }, or to a successor with value G = {Gi }. What
value should we assign to p?
Since the minimizer has perfect information, he will always guarantee that the maximizer
achieves the minimum value for the actual situation. Whatever element of Fi  F or Gj  G
is eventually selected by the maximizer, the eventual value of p will be the minimum of Fi
and Gj . In other words
min({Fi }, {Gj }) = {min(Fi , Gj )}
(9)
where the individual minima are computed using the perfect information rule (7).
Definition 5.0.6 Let G be the set of positions in an imperfect information game, a set of
pairs (p, Z) where p is a position from the point of view of the maximizing player and Z is
the set of perfect information positions consistent with p. The imperfect information game
for G is the game (G, V, pI , s, ev, f+ , f ) where:
S

1. The value set V is 22 .
2. The initial position pI is (p0 , S), where p0 is the initial imperfect information position
and S is the set of all perfect information positions consistent with it.
3. The successor function is described as follows:
(a) If the maximizer is on play in the given position, the successors are obtained by
enumerating the maximizers legal plays and leaving the elements of the set Z of
situations unchanged.
333

fiGinsberg

(b) If the minimizer is on play in the given position, the successors are obtained by
making playing any card c that is legal in any element of X and then restricting
Z to those situations for which c is in fact a legal play.
4. Terminal nodes are nodes where all cards have been played, and therefore correspond to
single situations s. For such a terminal position, if the declarer has made his contract,
the value is ({s}, {S}). If the declarer has failed to make his contract, the value is
({s}, {S  {s}}).
5. The maximization and minimization functions are given by (8) and (9) respectively.
Theorem 5.0.7 Suppose that the value of the imperfect information game for G is T .
Then a set of positions T is a subset of an element of T if and only if the maximizer has
a strategy that wins in every element of T , assuming that the minimizer plays with perfect
information.
Proof. Once again, the proof proceeds by induction on the depth of the game tree. And
once again, the case where p is a terminal position is handled easily by the definition. For
the inductive case, we consider the maximizer and minimizer separately.
For the maximizer, suppose that there is some set T of situations that satisfies the
conditions of the theorem, so that the maximizer has a strategy that caters to all of the
elements of T . Then the first move of that strategy will be some single move to a position
pi that is a successor of p and that caters to the elements of T . Thus if the value of the
successful child is F, T is a subset of some F  F by the inductive hypothesis. Thus if the
value of the original game is G, T is a subset of an element of G by virtue of (8).
Alternatively, if T is a set for which the maximizer has no such strategy, then clearly the
maximizer cannot have a strategy after making any of the moves to the successor positions
pi . This means that no superset U  T in any evc (pi ), and thus no superset of T in evc (p)
either.
The minimizing case is not really any harder. Suppose first that the maximizer has no
strategy for succeeding in every situation in T . Then the minimizer (playing with perfect
information) must have some move to a position pi with value Fi such that T is not a subset
of any element of Fi . Now if Fi = {Ti }, recall that
min({Ti }, {Ui }) = {Ti  Uj },
and T 6 Ti for each i. Thus T 6 Ti  Uj for each i and j, and there is no V  T with
V  min({Ti }, {Ui })
For the last case, suppose that the maximizer does have a strategy for succeeding in
every situation in T . That means that after any move for the minimizer, the maximizer will
still have a strategy that succeeds in T , so that if pi are the successors of p and evc (pi ) = Ti ,
then there is a Ti  Ti with T  Ti . Now T  i Ti  min(Ti ) = evc (p). Thus evc (p)
contains an element that is a superset of T .
Using this result, we can in theory compute exactly the set of things we might play for
given a single-dummy bridge problem. Before we turn to the issues involved in doing so in
practice, however, let me repeat the example of this section using the imperfect information
technique. Here is the game tree again with values assigned to the terminal nodes:
334

fiGIB: Imperfect information in a computationally challenging game

q max
P
@PPP
PP

@
PP


@
PP
q 
@q max
Pq min
min 
min q
A
A
A
A


 A
 A
S A T
S A T
S
T


A
A
A
A
q
Aq
q
Aq min q
Aq min q
Aq
{S  T }
{S}
{T }
{S  T }
{S  T } {S  T }
C
C


S C T S C T
C
C
q Cq
q Cq



{S  T }

{S} {T }

{S  T }

Backing up past the minimizers final move gives us:
q max
P
PP

@
PP


@
PP



@ max PPP {S  T }
{S} q
{T } q
@q
Pq
A
A
A
A
 A
 A
 A
 A




A
A
A {T }
A
{S} 
Aq
Aq
Aq
Aq
q
q
q
q
{S  T }
{S}
{T }
{S  T }
{S  T } {S  T }
C
C
 C
 C
 C
 C
q Cq
q Cq
{S  T }

{S} {T }

{S  T }

And we can now complete the analysis to finally get:
q {S, T, S  T }
P
@PPP
PP
@

PP


@
PP {S  T }
{S} 
{T } q
q 
@q {S, T }
Pq
A
A
A
A
 A
 A
 A
 A




A
A {T }
A
A
{S} 
q
q
Aq
q
Aq
Aq
q
Aq
{S  T }
{S}
{T }
{S  T }
{S  T } {S  T }
C
C
 C
 C
 C
 C
q Cq
q Cq



{S  T }

{S} {T }

{S  T }

Note the difference in the values assigned to the maximizers third and fourth choices at
the first ply. The third choice has value {S, T }, indicating clearly that the maximizer will
need to subsequently decide whether to play for S or for T . But the fourth choice has value
{S  T } indicating that both possibilities are catered to.
The value assigned to the root contains some redundancy (which we will deal with in
Section 7), in that one of the maximizers choices (ST ) dominates the others. Nevertheless,
this value clearly indicates that the maximizer has an option available at the root that caters
to both situations.
335

fiGinsberg

C
C

C

C

C
Cq min

AQQ
 
  A QQ

A
Q
Aq
Qq
q

q

m1

m2

m3

m4

C
Cq min
S
 S

S
Sq min
q
m2
A
 A

A
q
q Aq

m1

m3

m4

Figure 7: Equivalent games?

6. Extending alpha-beta pruning to lattices
The results of the previous section allow us to deal with imperfect information in theory.
Unfortunately, computing the value in theory is hardly the same as computing it in practice.
Some ideas, such as transposition tables and partition search, can fairly obviously be applied
to games with values taken from sets more general than total orders. But what about -
pruning, the linchpin of high-performance adversary search algorithms? The answer here is
far more subtle.
6.1 Some necessary definitions
Let us begin by considering the two small game trees in Figure 7, where the minimizer is
on play at the nonfringe nodes and none of the mi is intended to be necessarily terminal.
Are these two games always equivalent?
We would argue that they are. In the game on the left, the minimizer needs to select
among the four options m1 , m2 , m3 , m4 . In the game on the right, he needs to first select
whether or not to play m2 ; if he decides not to, he must select among the remaining
options. Since the minimizer has the same possibilities in both cases, we assume that the
values assigned to the games are the same.
From a more formal point of view, the value of the game on the left is f (m1 , m2 , m3 , m4 ),
while that of the game on the right is f (m2 , f (m1 , m3 , m4 )) where we have abused notation somewhat, writing mi for the value of the node mi as well.
Definition 6.1.1 A game will be called simple if for any x  v  V ,
f+ {x} = f {x} = x
and also
f+ (v) = f+ {x, f+ (v  x)}
and
f (v) = f {x, f (v  x)}

336

fiGIB: Imperfect information in a computationally challenging game

We have augmented the condition developed in the discussion of Figure 7 with the
assumption that if a players move in a position p is forced (so that p has a unique successor),
then the value before and after the forced move is the same.
Proposition 6.1.2 For any simple game, there are binary functions  and  from V to
itself that are commutative, associative and idempotent 14 and such that
f+ {v0 , . . . , vm } = v0      vm
and
f {v0 , . . . , vm } = v0      vm
Proof. Induction on m.
When referring to a simple game, we will typically replace the functions f+ and f by
the equivalent binary functions  and . We assume throughout the rest of this section
that all games are simple.15
The binary functions  and  now induce a partial order , where we will say that x  y
if and only if x  y = y. It is not hard to see that this partial order is reflexive (x  x),
antisymmetric (x  y and y  x if and only if x = y) and transitive. The operators 
and  behave like greatest lower bound and least upper bound operators with regard to the
partial order.
We also have the following:
Proposition 6.1.3 Whenever S  T , f+ (S)  f+ (T ) and f (S)  f (T ).
In other words, assuming that the minimizer is trying to reach a low value in the partial
order and the maximizer is trying to reach a high one, having more options is always good.
6.2 Shallow pruning
We are now able to investigate - pruning in our general framework. Let us begin with
shallow pruning, shown in Figure 8.
The idea here is that if the minimizer prefers x to y, he will never allow the maximizer
even the possibility of selecting between y and the value of the subtree rooted at T . After
all, the value of the maximizing node in the figure is y  evc (T )  y  x, and the minimizer
will therefore always prefer x.
In order for the usual correctness proof for (shallow) - pruning to hold, we need the
following condition to be satisfied:
Definition 6.2.1 (Shallow - pruning) A game G will be said to allow shallow - pruning for the minimizer if
x  (y  T ) = x
(10)
14. A binary function f is called idempotent if f (a, a) = a for all a.
15. We also assume that the games are sufficiently complex that we can find in the game tree a node with
any desired functional value, e.g., a  (b  c) for specific a, b and c. Were this not the case, none of our
results would follow. As an example, a game in which the initial position is also terminal surely admits
pruning of all kinds (since the game tree is empty) but need not satisfy the conclusions of the results in
this section.

337

fiGinsberg

C

C

C
Cq min
S
 S

S max
Sq
q

x
A
 A

A
Aq
q

y

T

Figure 8: T can be pruned (shallowly) if x  y
for all x, y, T  V with x  y. The game will be said to allow shallow - pruning for the
maximizer if
x  (y  T ) = x
(11)
for all x, y, T  V with x  y. We will say that G allows shallow pruning if it allows shallow
- pruning for both players.
The definition basically says that the backed up value at the root of the game tree is
unchanged by pruning the maximizing subtree in the figure.
As we will see shortly, the expressions (10) and (11) describing shallow pruning are
identical to what are more typically known as absorption identities.
Definition 6.2.2 Suppose V is a set and  and  are two binary operators on V . The
triple (V, , ) is called a lattice if  and  are idempotent, commutative and associative,
and satisfy the absorption identities in that for any x, y  V ,
x  (x  y) = x

(12)

x  (x  y) = x

(13)

We also have the following:
Definition 6.2.3 A lattice (V, , ) is called distributive if  and  distribute with respect
to one another, so that
x  (y  z) = (x  y)  (x  z)

(14)

x  (y  z) = (x  y)  (x  z)

(15)

Lemma 6.2.4 Each of (12) and (13) implies the other. Each of (14) and (15) implies the
other.
Proof. These are well known results from lattice theory (Gratzer, 1978).
Proposition 6.2.5 (Ginsberg & Jaffray, 2001) For a game G, the following conditions
are equivalent:
338

fiGIB: Imperfect information in a computationally challenging game

C

C

C
Cr min
S r max
 S
 r min
 
 SSr max
r


x
A
 A

A
Ar
r

y

T

Figure 9: T can be pruned (deeply) if x  y
1. G allows shallow - pruning for the minimizer.
2. G allows shallow - pruning for the maximizer.
3. G allows shallow pruning.
4. (V, , ) is a lattice.
Proof.16 We show that the first and fourth conditions are equivalent; everything else follows
easily.
If G allows shallow - pruning for the minimizer, we take x = a and y = T = a  b in
(10). Clearly x  y so we get
a  (y  y) = a  y = a  (a  b) = a
as in (13).
For the converse, if x  y, then x  y = x and
x  (y  T ) = (x  y)  (y  T )
= x  (y  (y  T ))
= xy
= x.
6.3 Deep pruning
Deep pruning is a bit more subtle. An example appears in Figure 9.
As before, assume x  y. The argument is as described previously: Given that the
minimizer has a guaranteed value of x at the upper minimizing node, there is no way that
a choice allowing the maximizer to reach y can be on the main line; if it were, then the
maximizer could get a value of at least y.
16. The proofs of this and Proposition 6.3.2 are due to Alan Jaffray.

339

fiGinsberg



r max
@
@ r min
@
@ r max

@
@
@r min

@
@r



0

Figure 10: The deep pruning counterexample
Definition 6.3.1 (Deep - pruning) A game G will be said to allow - pruning for the
minimizer if for any x, y, T, z1 , . . . , z2i  V with x  y,
x  (z1  (z2      (z2i  (y  T )))   ) =
x  (z1  (z2      z2i )   ).
The game will be said to allow - pruning for the maximizer if
x  (z1  (z2      (z2i  (y  T )))   ) =
x  (z1  (z2      z2i )   ).
We will say that G allows pruning if it allows - pruning for both players.
As before, the prune allows us to remove the dominated node (y in Figure 9) and all of its
siblings.
The fact that a game allows shallow - pruning does not mean that it allows pruning
in general, as is shown by the following counterexample. The example involves a game with
one card that is known to both players; only the suit of the card matters. The game tree
appears in Figure 10.
In this tree, a node labelled with a suit symbol is terminal and means that the maximizer
wins if and only if the suit of the card matches the given symbol. So at the root of the given
tree, the maximizer (whose turn it is to play) can choose to turn over the card, winning
if and only if its a club, or can defer to the minimizer. The minimizer can choose to turn
the card (losing just in case its a diamond  the suit symbols refer to the maximizers
result), or hand the situation back to the maximizer. If the maximizer defers yet again,
the minimizer can either turn over the card, losing if its a club, or simply declare victory
(presumably his choice).
There is one other wrinkle in this game. At any point in the game, the maximizer can
change the card from either a diamond or a spade to a club.
Now lets consider the game itself. At ply 4, the minimizer will obviously choose to win
the game. Thus at ply 3, the maximizer will need to choose , winning just in case the
card is a heart. But this means that at ply 2, the minimizer will win the game, since if the
card is not a diamond he will move to the left (and win at once) while if the card is not a
heart he can win by moving to the right. (Remember that the minimizer knows the suit
340

fiGIB: Imperfect information in a computationally challenging game

of the card.) The upshot of this is that the maximizer wins the overall game if and only if
the card in question is a club. A formal analysis proceeds similarly, labelling the nodes as
follows:
r =   0
@

@r0 =   
@
@ r =   0
@
@
@r 0 =   0

@
@r






0

Note, incidentally, that the maximizers ability to change the card does not help him win
the game.
Now suppose that we apply deep pruning to this game. The ply four node is one where
the minimizer can force a value of at most , suggesting that the siblings of the bottom 
node can be pruned. But doing so produces the following tree:
r  
@

@r =   1
@
@ r1 =   
@
@
@r 

@
@r pruned?






If the maximizer reaches ply 3, he can win by changing the card to a club if need be.
Of course, the minimizer wont let the maximizer reach ply 3; at ply 2, hell move left
so that the maximizer wins only if the card is a diamond. That means that the maximizer
wins at the root just in case the card is either a club or a diamond.
A partial graph of the values for this game is as follows:
r
1

 


r
r
r
r


Q


A

Q
 
Q A
Q A 
Q
Q
A
r


0
where we have included the crucial fact that x  y = 0 if x 6= y (since the minimizer knows
the card) and    = 1 because the maximizer can invoke his special rule. Other least
upper bounds are not shown in the diagram. The maximizing function  moves up the
figure; the minimizing function  moves down.
The deep prune fails because we cant push the value   0 past the  to get to the
 near the root. Somewhat more precisely, the problem is that
 =   (  0) 6= (  )  (  0) = 0
This suggests the following:
341

fiGinsberg

Proposition 6.3.2 (Ginsberg & Jaffray, 2001) For a game G, the following conditions
are equivalent:
1. G allows - pruning for the minimizer.
2. G allows - pruning for the maximizer.
3. G allows pruning.
4. (V, , ) is a distributive lattice.
Proof. As before, we show only that the first and fourth conditions are equivalent. Since
pruning implies shallow pruning (take i = 0 in the definition), it follows that the first
condition implies that (V, , ) is a lattice.
From deep pruning for the minimizer with i = 1, we have that if x  y, then for any
z 1 , z2 , T ,
x  (z1  (z2  (y  T ))) = x  (z1  z2 )
Now take y = T = x to get
x  (z1  (z2  x)) = x  (z1  z2 )

(16)

It follows that each top level term in the left hand side of (16) is greater than or equal to
the right hand side; specifically
z1  (z2  x)  x  (z1  z2 ).

(17)

We claim that this implies that the lattice in question is distributive.
To see this, let u, v, w  V . Now take z1 = u  w, z2 = v and x = w in (17) to get
(u  w)  (v  w)  w  ((u  w)  v)

(18)

But v  (u  w)  w  (v  u) is an instance of (17), and combining this with (18) gives us
(u  w)  (v  w)  w  ((u  w)  v)
 w  w  (v  u)
= w  (v  u)
This is the hard direction; w  (v  u)  (u  w)  (v  w) for any lattice because
w  (v  u)  u  w and w  (v  u)  v  w individually. Thus w  (v  u) = (u  w)  (v  w),
and deep pruning implies that the lattice is distributive.
For the converse, if the lattice is distributive and x  y, then
x  (z1  (z2  (y  T ))) = (x  z1 )  (x  z2  (y  T ))
= (x  z1 )  (x  z2 )
= x  (z1  z2 )
where the second equality is a consequence of the fact that x  (yT ), so that x = x(yT ).
This validates pruning for i = 1; deeper cases are similar.
Finally, note that in games where this result applies, we can continue to use Algorithms
2.2.5 or 2.3.3 without modification, since the prunes that they endorse continue to be sound
as the game tree is expanded.
342

fiGIB: Imperfect information in a computationally challenging game

6.4 Application to imperfect information
In order to apply these ideas to games of imperfect information treated as in Section 5, we
need to show that the value set introduced there is a (hopefully distributive) lattice.
S
To do this, recall that there is redundant information in an arbitrary element F of 22 ,
since if F contains both T and U with T  U (in other words, the maximizer can play
for either T or for U but U is properly better), the set T can be removed from F without
affecting the maximizers options in any interesting way. This suggests the following:
S

Definition 6.4.1 Let F  22 for an arbitrary set S. We will say that F is reduced if
there are no T, U  F with T  U . We will say that F1 is a reduction of F2 if F1 is reduced
and F1  F2 .
S

Lemma 6.4.2 Every F  22 has a unique reduction.
Proof. This is immediate; just remove the subsumed elements from F.
.
We will denote the reduction of F by r(F).
Armed with this definition, we can now modify Definition 5.0.6 in the obvious way,
replacing the value set V with the set of reduced elements of V and the maximizing and
minimizing functions (8) and (9) with the reduced versions thereof, so that
max(F, G) = r(F  G)

(19)

min({Fi }, {Gj }) = r({Fi  Gj })

(20)

and
Remember that we typically write  for max and  for min.
Proposition 6.4.3 Given the above definitions, (V, , ) is a distributive lattice.
Proof. We need to show that max and min as defined above are commutative, associative,
and idempotent, that they distribute with respect to one another, and that the absorption
identity (12) is satisfied. Since the reduction operator clearly commutes with the initial
definitions of max and min, commutativity, associativity and distributivity are obvious, as
is the fact that  is idempotent. To see that  is idempotent, we have
F  F = r({min(Fi , Fj )}) = r({Fi  Fj })
but each element of the set on the righthand side is a subset of Fi  Fi so
F  F = r({Fi }) = r(F) = F.
For the absorption identity, we need to show that
F  (F  G) = F
But
F  G = r{Fi  Gj }
343

fiGinsberg

so
F  (F  G) = r(F  r{Fi  Gj })
= r({Fi }  {Fi  Gj })
= r({Fi })
= r(F)
= F
since, once again, each element of F  G is subsumed by the corresponding Fi .
It follows that an implementation designed to compute the value of an imperfect information game as described by Theorem 5.0.7 can indeed use - pruning to speed the
computation.
6.5 Bridge implementation
Given this body of theory, we implemented a single-dummy version of gibs double-dummy
search engine. Not surprisingly, the most difficult element of the implementation was buildS
ing efficient data structures for the manipulation of elements of 22 .
To handle this, we represented each element of S as a conjunction. We first identified
one of the two hidden hands H, and then for each card c, would write c if c were held by
H and c if c were not held by H. An element of 2S was then taken to be a disjunctive
S
combination of these conjunctions, and an element of 22 was taken to be a list of such
disjunctions. The advantage of this representation was that logical inference could be used
to construct the reduction of any such list.
In order to make this inference as efficient as possible, the disjunctions themselves were
represented as binary decision diagrams, or bdds (Lind-Nielsen, 2000). There are a variety
of public domain implementations of bdds available, and we used one provided by LindNielsen (Lind-Nielsen, 2000).17
The resulting implementation solves small endings (perhaps 16 cards left in total) quickly
but for larger endings, the running times come to be dominated by the bdd computations;
this is hardly surprising, since the size of individual bdds can be exponential in the size
of S (the number of possible distributions of the unseen cards). We found that we were
generally able to solve 32-card endings in about a minute, but that the running times were
increasing by two orders of magnitude as each additional card was added.
This is both good news and bad news. Viewed positively, the performance of the system
as constructed is far superior to the performance of preceding attempts to deal with the
imperfect information arising in bridge. Frank et.al, for example, are only capable of solving
single suit combinations (13 cards left, give or take) using an algorithm that appears to take
several minutes to run (Frank, Basin, & Matsubara, 1998). They subsequently improve the
performance to an average time of 0.6 seconds (Frank et al., 2000), but are still restricted to
problems that are too small to be of much use to a program intended to play the complete
game.
17. We tried a variety of non-bdd based implementations as well. The bdd-based implementation was far
faster than any of the others.

344

fiGIB: Imperfect information in a computationally challenging game

Thats the good news. The bad news is that a program capable only of solving an 8card ending in a minute is inappropriate for production use. Gib is a production program,
expected to play bridge at human speeds. Another approach was therefore needed.

7. Solving single-dummy problems in practice
7.1 Achievable sets
The key to practical application of the ideas in the previous section is the realization that
when it comes time to make a play, a single element of F must be selected: if you can play
for West to have the Q or for each player to have three hearts but cannot cater to both
possibilities simultaneously, you eventually have to actually make the choice.
Definition 7.1.1 Suppose that the value of the imperfect information game for G is F.
Given a specific A  S, we will say that A is achievable if there is some F  F for which
A  F.
In other words, the set A of situations is achievable if the maximizer has a plan that wins
for all elements of A.
Definition 7.1.2 Given a set S of situations, a payoff function for S is any function
f : 2S  IR such that f (U )  f (T ) whenever U  T .
The payoff function evaluates potential achievable sets.
Definition 7.1.3 Let G be a game and S the associated set of situations. If f is a payoff
function for S, a solution to G under f is any achievable set A for which f (A) is maximal.
In practice, we need not find the actual value of the game; finding a solution to G under
an appropriate payoff function suffices. In bridge, the payoff function is presumably the
probability that the cards are dealt as in the set A; this function clearly increases with
increasing set size as required by Definition 7.1.2 and can be evaluated in practice using the
Monte Carlo sample of Section 3.
Instead of finding the solution to an imperfect information game, suppose instead that
we have a Monte Carlo sample for the game consisting of a set of situations S = {si } that
is ordered as i = 0, . . . , n. We can now produce an achievable set A as follows:
Algorithm 7.1.4 To construct a maximal achievable set A from a sequence hs0 , . . . , sn i of
situations:
1. Set A = .
2. For i = 0, . . . , n, if A  {si } is achievable, set A = A  {si }.
The algorithm constructs the achievable set in a greedy fashion, gradually adding elements
of S to A until no more can be added.
Definition 7.1.5 Given a game G and a sequence S of situations, the achievable set induced by S for G is the set constructed by Algorithm 7.1.4.
345

fiGinsberg

From a computational point of view, the expensive step in the algorithm is determining
whether or not the set A  {si } is achievable. This is relatively straightforward, however,
since the focus on a specific set effectively replaces the game G with a new game with
values in {0, 1}. At any particular node n, if expanding n demonstrates that A  {si } is
not achievable, the value of the game is zero. If expanding n indicates that A  {si } is
achievable once n is reached, then the value of the node n is one. Although the search space
is unchanged from that of the original imperfect information game as in Definition 5.0.6,
there is no longer any need to manipulate complex values, and the check for achievability
is therefore tractable in practice.
Let me illustrate this by returning to our usual example of Section 5. Here is the fully
evaluated tree once again:
q {S  T }
P
@PPP
PP
@
PP


@
PP {S  T }
{S} q
{T } q
@q {S, T }
Pq
A
A
A
A
 A
 A
 A
 A




A
A
A {T }
A
{S} 
q
Aq
q
Aq
q
Aq
q
Aq
{S  T }
{S}
{T }
{S  T }
{S  T } {S  T }
C
C
 C
 C
 C
 C
q Cq
q Cq




{S  T }

{S} {T }

{S  T }

Note that we have replaced the value at the root with its reduction.
Now suppose that we view the set of positions as containing only two elements, s  S
and t  T . Presumably West holds the Q in s, and East holds three hearts in t. If the
ordering chosen is hs, ti, then we first try to achieve {s}. In this context, a node n is a win
for the maximizer if either the maximizer can indeed win at n or s is no longer possible (in
which case the maximizers ability to achieve {s} is undiminished). The game tree becomes:
q max
P
PP

@
PP


@
PP



@ max PPP

q
@q
Pq min
min 
min q
A
A
A
A


 A
 A
S A T
S A T
S
T


A
A
A
A
q
Aq
q
Aq min q
Aq min q
Aq
1
1
0
1
1
1
C
C


S C T S C T
C
C
q Cq
q Cq
1

1

0

1

All of the T branches are wins for the maximizer (who is concerned with s only), and the
S branches are wins just in case the maximizer does indeed win (as he does if he guesses
right at either of the first two plies). Backing up the values gives us:
346

fiGIB: Imperfect information in a computationally challenging game

q1
P
PP

@
PP


@
PP


PP

@

PPq 1
1
0q
q
@q 1
A
A
A
A


 A
 A
S A T
S A T
S
T


A
A
A
A
0 q
q
Aq
q
Aq
Aq 1
q
Aq
1
1
0
1
1
1
C
C


S C T S C T
C
C
q Cq
q Cq
1

1

0

1

This indicates (correctly) that the maximizer can achieve s provided that he doesnt decide
to play for T at the root of the tree. Note that this analysis is a straight minimax, allowing
S
fast algorithms to be applied while avoiding the manipulation of elements of 22 described
in the previous section.
Now we add t to our achievable set, which thus becomes {s, t}. The maximizer wins
only if he really does win (and not just because he isnt interested in T any more), and the
basic tree becomes:
q max
P
PP

@

PP

@
PP



@ max PPP

q
@q
Pq min
min 
min q
A
A
A
A


 A
 A
S A T
S A T
S
T


A
A
A
A
Aq min q
Aq min q
Aq
q
Aq
q
1
0
0
1
1
1
C
C


S C T S C T
C
C
q Cq
q Cq
1

0

0

1

Backing up the values gives:
q1
P
PP

@

PP

@
PP


PP

@

PPq 1
0 q
0q
@q 0
A
A
A
A


 A
 A
S A T
S A T
S
T


A
A
A
A
0 q
q
Aq
q
Aq
Aq 0
q
Aq
1
0
0
1
1
1
C
C


S C T S C T
C
C
q Cq
q Cq
1

0

0

1

The maximizer can achieve the extended result only by making the rightmost move, as
desired.
What if the rightmost branch did not exist, so that the maximizer were unable to
combine his chances? Now the value of the root node in the above tree is 0, so that {s, t} is
not achievable. The maximal achievable set returned by the algorithm would be S; had the
347

fiGinsberg

ordering been ht, si instead, an alternative maximal achievable set of T would have been
returned instead.
In any event, we have:
Proposition 7.1.6 Given a game G and a sequence S of situations, let A be the achievable
set induced by S for G. Then no proper superset of A in S is achievable.
Proof. This is straightforward. For any element s  S  A, we know that U  {s} is not
achievable for some U  A. Thus A  {s} is not achievable as well.
Algorithm 7.1.4 allows us to construct maximal achievable sets relative to our Monte
Carlo sample; recall that we are taking our sequence S of situations to be any ordering
of the sample itself. In practice, however, it is important not to focus too sharply on the
sample itself, lest the eventual achievable set constructed overfit irrelevant probabilistic
characteristics of that sample. This can be accomplished by replacing the simple union in
step 2 of the algorithm with some more complicated operation that captures the idea of
situations that are either like si or like those already in A. In bridge, for example, A might
be all situations where West has two or three hearts, and si might be some new situation
where West has four hearts. The generalized union would be situations where West has
two, three or four hearts. If this more general set is not achievable, another attempt could
be made with the simple union. If we denote the general union by , Algorithm 7.1.4
becomes:
Algorithm 7.1.7 To construct an achievable set A from a sequence hs0 , . . . , sn i of situations:
1. Set A = .
2. For i = 0, . . . , n:
(a) If A  {si } is achievable, set A = A  {si }.
(b) Otherwise, if A  {si } is achievable, set A = A  {si }.
This algorithm can be used in practice to find achievable sets that are either maximal
or effectively so over the set of all possible instances, not just those appearing in the Monte
Carlo sample.
7.2 Maximizing the payoff
It remains to find not just maximal achievable sets, but ones that approximate the solution
to the game in question given a particular payoff function.
To understand how we do this, let me draw an analogy between the problem we are trying
to solve and resource-constrained project scheduling (rcps). In rcps, one has a list of tasks
to be performed, together with ordering constraints saying that certain tasks need to be
performed before others. In addition, each task uses a certain quantity of various resources;
there are limitations on the availability of any particular resource at any particular time.
As an example, building an aircraft wing may involve fabricating the top and bottom flight
surfaces, building the aileron, and attaching the two. It should be clear that the aileron
348

fiGIB: Imperfect information in a computationally challenging game

cannot be attached until both it and the wing have been constructed. Building each section
may involve the use of three sheetmetal workers, but only five may be available in general.
The goal in an rcps problem is typically to minimize the length of the schedule (often
called the makespan) without exceeding the resource limits. In building a wing, it is more
efficient (and more cost effective) to build it quickly than slowly.
Many production scheduling systems try to minimize makespan by building the schedule
from the initial time forward. At each point, they select a task all of whose predecessors
have been scheduled, and then schedule that task as early as possible given the previously
scheduled tasks and the resource constraints. Scheduling the tasks in this way produces a
locally optimal schedule that may be improved by modifying the order in which the tasks
are selected for scheduling.
One method for finding an appropriate modification to the selection order is known as
squeaky wheel optimization, or swo (Joslin & Clements, 1999). In swo, a locally optimal
schedule is examined to determine which tasks are scheduled most suboptimally relative to
some overall metric; those tasks are deemed to squeak and are then advanced in the task
list so that they are scheduled earlier when the schedule is reconstructed. This process is
repeated, producing a variety of candidate solutions to the scheduling problem at hand; one
of these schedules is typically optimal or nearly so.
Applying swo to our game-playing problem is relatively straightforward.18 When we
use Algorithm 7.1.7 to construct an achievable set, we also construct as a byproduct a list of
sample elements to which that achievable set cannot be extended; moving elements of this
list forward in the sequence of hs0 , . . . , sn i will cause them to be more likely to be included
in the achievable set A if the algorithm is reinvoked. The weights assigned to the failing
sequence elements can be constructed by determining how representative each particular
element is of the remainder of the sample.
Returning to our example, suppose that the set S (where West has the Q) has a single
representative s1 in the Monte Carlo sample (presumably this means it is unlikely for West
to hold the card in question), while T has five such representatives t1 , t2 , t3 , t4 and t5 .
Suppose also that the initial ordering of the six elements is hs1 , t4 , t2 , t1 , t5 , t3 i.
Assuming that the maximizer loses his rightmost option (so that he cannot cater to S
and T simultaneously), the maximal achievable set corresponding to this ordering is S. An
examination now reveals that all of the ti s could have been achieved but werent; in swo
terms, these elements of the sample squeak.
At the next iteration, the priorities of the ti s are increased by moving them forward in
the sequence, while the priority of s1 falls. Perhaps the new ordering is ht4 , t2 , s1 , t1 , t5 , t3 i.
This ordering can be easily seen to lead to the maximal achievable set T ; S  T is still
unachievable. But the payoff assigned to T is likely to be much better than that assigned
to S (a probability of 0.8 instead of 0.2, if the Monte Carlo sample itself is unweighted). It
is in this way that swo allows us to find a globally optimal (or nearly so) achievable set.
18. Squeaky wheel optimization was developed at the University of Oregon; the patent application for the
technique has been allowed by the U.S. Patent and Trademark Office. The Universitys interests in swo
are licensed exclusively to On Time Systems, Inc. for use in scheduling and related applications, and to
Just Write, Inc. for use in bridge-playing systems.

349

fiGinsberg

7.3 Results
Our implementation of gibs cardplay when declarer is based on the ideas described above.
(As a defender, a direct Monte Carlo approach appears preferable because enough information is typically available about declarers hand to make the double-dummy assumption
reasonably valid.) The implementation is fast enough to conform to the time requirements
placed on a production program (roughly one cpu minute to play each deal).
Evaluating the impact of these ideas on gibs cardplay is difficult, since declarer play is
already the strongest aspect of its game. In extended matches between the two versions of
gib, the approach based on the ideas described here beats the Monte-Carlo based version
by approximately 0.1 imps/deal, but there is a great deal of noise in the data because most
of the swings correspond to differences in bidding or defensive play. It is possible to remove
some of these differences artificially (requiring the bidding to be identical both times the
deal is played, for example), but defensive differences remain. Nevertheless, gib is currently
a strong enough player that the 0.1 imps/deal difference is significant.
The situation on problem deals, such as those from the par contests or from the Gitelman
sets, is much clearer. In addition, many of the deals that gib gets wrong are in fact deals
that gib plays correctly but that the problem composers play incorrectly (Gitelman or, in
the case of the par contests, Swiss bridge expert Pietro Bernasconi). In the following table,
we have been generous with all parties, deeming a line to be correct if it is not clearly
inferior to another. Let me point out that the designers of the problems are attempting to
construct deals where there is a unique solution (the answer to the test they are posing
the solver), so that a deal with multiple solutions is in fact one that the composer has
already misanalyzed.
Source
BM level 1
level 2
level 3
level 4
level 5
1998 par contest
1990 par contest

size
36
36
36
36
36
12
18

BB
16
8
2
1
4
0
0

GibMC
31
23
12
21
13
5
8

GibSWO
36
34
34
31
28
11
14

composer
35
34
34
34
34
12
17

ambiguous
0
1
2
4
5
2
3

The rows are in order of increasing difficulty; it was universally felt among the human
competitors that the deals in the 1990 par contest were far more difficult than those in
1998. The columns are as follows:
Source
Size
BB
GibMC
GibSWO
composer
ambiguous

is the source from which the problems were obtained.
is the number of problems available from this particular source.
gives the number of problems solved correctly by Bridge Baron 6.
gives the number solved correctly by gib using a Monte Carlo approach.
gives the number solved correctly by gib using swo and achievable sets.
gives the number solved correctly by the composer (in that the intended
solution was the best one available).
gives the number misanalyzed by the composer (in that multiple solutions
exist).
350

fiGIB: Imperfect information in a computationally challenging game

Note, incidentally, that gibs performance is still less than perfect on these problems.
The reason is that gibs sample may be skewed in some way, or that swo may fail to find
a global optimum among the set of possible achievable sets.

8. Conclusion
8.1 GIB compared
Other programs Gib participated in both the 1998 and the 2000 World Computer
Bridge Championships. (There was no 1999 event.) Play was organized with each machine
playing two hands and the competitors being trusted not to cheat by peeking at partners
cards or those of the opponents.19
Each tournament began with a complete round robin among the programs, with the top
four programs continuing to a knockout phase. The matches in the round robin were quite
short, and it was expected that bridges stochastic element would keep any program from
being completely dominant.
While this may have been true in theory, in practice gib dominated both round robins,
winning all of its matches in 1998 and all but one in 2000. The round robin results from
the 2000 event were as follows:20

Gib
WBridge
Micro
Buff
Q-Plus
Blue Chip
Baron
Meadowlark

Gib

6
9
4
13
1
4
3

WB
14

1
7
4
13
2
0

Micro
11
19

2
5
5
7
0

Buff
16
13
18

8
0
15
0

Q-Plus
7
16
15
12

9
6
9

Chip
19
7
15
20
11

9
0

Baron
16
18
13
5
14
11

6

Mlark
17
20
20
20
11
20
14


Total
100
99
91
70
66
59
57
18

Each match was converted first to imps and then to victory points, or VPs, with the two
competing programs sharing the 20 VPs available in each match. The first entry in the
above table indicates that gib beat wbridge by 14 VPs to 6; the fourth that gib lost
to q-plus bridge by 7 VPs to 13. (This is gibs only loss ever to another program in
tournament play.)
In the 1998 knockout phase, gib beat Bridge Baron in the semifinals by 84 imps over 48
deals. Had the programs been evenly matched, the imp difference could be expected to be
normally distributed, and the observed 84 imp difference would be a 2.2 standard deviation
19. Starting with the 2001 event, each computer will handle only one of the four players, although there
is still no attempt to prevent the (networked) computers from transmitting illegal information between
partners.
20. There were eight competitors in the event:
gib (www.gibware.com), Hans Lebers q-plus
(www.q-plus.com), Tomio and Yumiko Uchidas micro bridge (www.threeweb.ad.jp/mcbridge),
Mike Whittaker and Ian Trackmans blue chip bridge (www.bluechipbridge.co.uk), Rod Ludwigs meadowlark bridge (rrnet.com/meadowlark), bridge baron (www.bridgebaron.com), and
two newcomers: Doug Bannions bridge buff (www.bridgebuff.com) and Yves Costels wbridge
(ourworld.compuserve.com/homepages/yvescostel).

351

fiGinsberg

event. Gib then beat Q-Plus Bridge in the finals by 63 imps over 64 deals (a 1.4 standard
deviation event). In 2000, it beat Bridge Buff by 39 imps over 48 deals in the semifinals
(a 1.0 standard deviation event) and then beat wbridge by 101 imps over 58 deals (a 2.6
standard deviation event). The finals had been scheduled to run 64 deals, but wbridge
conceded after 58 had been played.
The most publicized deal from the final was this one, an extremely difficult deal that both
programs played moderately well. Gib reached a better contract and was aided somewhat
by wbridges misdefence in a moderately complex situation.









KQ9
AQJ
96432
86

10 6
10 9 2
10
A J 10 9 5 3 2










8732
753
AKQJ85


AJ54
K864
7
KQ74

When wbridge played the North-South cards and gib was East-West, North opened
1 and eventually played in three notrump, committing to taking nine tricks. The gib East
started with four rounds of diamonds as South discarded two clubs and . . . ?
Looking at all four hands, the contract is cold; South can discard another club and East
has none to play. There are thus nine tricks: four in each of hearts and spades, and the
diamond nine.
Give East a club, however, and the contract rates to be down no less than four since
the defense will be able to take at least four club tricks. WBridge decided to play safe,
keeping the KQ and discarding a heart. There are now only eight tricks and the contract
was down one.
The bidding and play were more interesting when gib was N-S. North opened 1NT,
showing 1114 HCP without four hearts or spades unless exactly three cards were held in
every other suit. East overcalled a natural 2 and South cue bid 3, showing weakness in
diamonds and asking North to bid a 4-card heart or spade suit if he had one.
North has no good bid at this point. Bidding 3NT with five small diamonds rates to be
wrong and 4 is clearly out of the question. Gibs simulation suggested that 3 (ostensibly
showing four of them) was the least of evils. South raised to 4, and East doubled, ending
the auction.
East led a top diamond, and shifted to the 3, won by Norths Q. Gib now cashed
the J and led the 6, which East chose (wrongly) to ruff. WBridge now led the K as
East, which was ruffed with the J. Gib was now able to cash the AK to produce:
352

fiGIB: Imperfect information in a computationally challenging game










Q
A
962
8




A J 10 9 5 3










8
7
QJ85


5
K8

KQ4

Knowing the position exactly, gib needed five more tricks with North to lead. It ruffed
a diamond, returned to the A and drew Easts trump with the Q. Now a club forced an
entry to the South hand, where the K provided the tenth trick.
Humans Gib played a 14-deal demonstration match against human world champions Zia
Mahmood and Michael Rosenberg21 in the AAAI Hall of Champions in 1998, losing by a
total of 6.4 imps (a 0.3 standard deviation event). Early versions of gib also played on
OKBridge, an internet bridge club with some 15,000 members.22 After playing thousands
of deals against human opponents of various levels, gibs ranking was comparable to the
OKBridge average.
It is probable that neither of these results is an accurate reflection of gibs current
strength. The Mahmood-Rosenberg match was extremely short and gib appeared to have
the best of the luck. The OKBridge interface has changed and the gib OKbots no longer
function. The performance figures there are thus somewhat outdated, predating various
recent improvements including all of the ideas in Sections 57. More interesting information
will become available starting in late July of 2001, when gib, paired with Gitelman and his
regular partner Brad Moss, will begin a series of 64-deal matches against human opponents
of varying skill levels.
8.2 Current and future work
Recent work on gib has focused on its weakest areas: defensive cardplay and bidding. The
bidding work has been and continues to be primarily a matter of extending the existing
bidding database, although gibs bidding language is also being changed from Standard
American (a fairly natural system) to a variant of an artificial system called Moscito developed in Australia.23 Moscito has very sharply defined meanings, making it ideal for use
21. Mahmood and Rosenberg have won, among other titles, the 1995 Cap Volmac World Top Invitational
Tournament. As remarked earlier, Rosenberg would also go on after the GIB match to win the Par
Competition in which GIB finished 12th.
22. http://www.okbridge.com
23. Gibs version of Moscito is called Moscito Byte.

353

fiGinsberg

by a computer program, and is an action system, working hard to make the opponents
bidding as difficult as possible.
With regard to defensive cardplay, the key elements of high level defense are to make it
hard for partner to make a mistake while making it easy for declarer to do so. Providing gib
with these abilities will involve an extra level of recursion in the cardplay, as each element
of the Monte Carlo sample must now be considered from other players points of view, as
they generate and then analyze their own samples. These ideas have been implemented but
currently lead to small performance degradations (approximately 0.05 imps/deal) because
the computational cost of the recursive analyses require reducing the size of the Monte
Carlo sample substantially. As processor speeds increase, it is reasonable to expect these
ideas to bear significant fruit.
In 1997, Martel, a computer scientist himself, suggested that he expected gib to be the
best bridge player in the world in approximately 2003. The work appears to be roughly on
schedule.
8.3 Other games
I have left essentially untouched the question of to what extent the basic techniques we have
discussed could be applied to games of imperfect information other than bridge.
The ideas that we have presented are likely to be the most applicable in games where
the perfect information variant is tractable but computationally challenging, and the assumption that ones opponents are playing with perfect information is a reasonable one.
This suggests that games like hearts and other trick-taking games will be amenable to our
techniques, while games like poker (where it is essential to realize and exploit the fact that
the opponents also have imperfect information) are likely to need other approaches.

Acknowledgments
A great many people have contributed to the gib project over the years. In the technical
community, I would like to thank Jonathan Schaeffer, Rich Korf, David Etherington, Bart
Massey and the other members of cirl. In the bridge community, I have received invaluable
assistance from Chip Martel, Rod Ludwig, Zia Mahmood, Andrew Robson, Alan Jaffray,
Hans Kuijf, Fred Gitelman, Bob Hamman, Eric Rodwell, Jeff Goldsmith, Thomas Andrews
and the members of the rec.games.bridge community. The work itself has been supported
by Just Write, Inc., by DARPA/Rome Labs under contracts F30602-95-1-0023 and F3060297-1-0294, and by the Boeing Company under contract AHQ569. To everyone who has
contributed, whether named above or not, I owe my deepest appreciation.

Appendix A. A summary of the rules of bridge
We give here a very brief summary of the rules of bridge. Readers wanting a more complete
description are referred to any of the many excellent texts available (Sheinwold, 1996).
Bridge is a card game for four players, who are split into two pairs. Members of a single
pair sit opposite one another, so that North-South form one pair and East-West the other.
354

fiGIB: Imperfect information in a computationally challenging game

The deck is distributed evenly among the players, so that each deal involves giving each
player a hand of 13 cards. The game then proceeds through a bidding and a playing phase.
The playing phase consists of 13 tricks, with each player contributing one card to each
trick in a clockwise fashion. The player who plays first to any trick is said to lead to that
trick. The highest card of the suit led wins the trick (Ace is high and deuce low), unless a
trump is played, in which case the highest trump wins the trick. The person who leads to a
trick is free to lead any card he wishes; subsequent players must play a card of the suit led
if they have one, and can play any card they choose if they dont. The winner of one trick
leads to the next; the person who leads to the first trick (the opening leader) is determined
during the bidding phase of the game.
The object of the card play phase is always for your partnership to take as many tricks
as possible; there is no advantage to one partners taking a trick over another, and the order
in which the tricks are taken is irrelevant. After the opening leader plays the first card to
the first trick, the player to his left places his cards face up on the table so that all of the
other players can see them. This player is called the dummy, and when it is dummys turn
to play, dummys partner (who can see the partnerships combined assets) selects the card
to be played. Dummys partner is called the declarer and the members of the other pair
are called the defenders.
The purpose of the bidding phase is to identify trumps and the declarer, and also the
contract, which will be described shortly. The opening leader is identified as well, and is
the player to the declarers left.
During the bidding phase, various contracts are proposed. The dealer has the first
opportunity to propose a contract and subsequent opportunities are given to each player
in a clockwise direction. Each player has many opportunities to suggest a contract during
this phase of the game, which is called the auction. Each partnership is required to explain
the meanings of their actions during the auction to the other side, if requested.
Each contract suggests a particular trump suit (or perhaps that there not be a trump
suit at all). Each player suggesting a contract is committing his side to winning some particular number of the 13 available tricks. The minimum commitment is 7 tricks, so there
are 35 possible contracts (each of 4 possible trumps, or no trumps, and seven possible commitments, from seven to thirteen tricks). These 35 contracts are ordered, which guarantees
that the bidding phase will eventually terminate.
After the bidding phase is complete, the side that suggested the final contract is the
declaring side. Of the two members of the declaring side, the one who first suggested the
eventual trump suit (or no trumps) is the declarer. Play begins with the player to the
declarers left leading to the first trick.
After the hand is complete, there are two possible outcomes. If the declaring side took
at least as many tricks as it committed to taking, the declaring side receives a positive
score and the defending side an equal but negative score. There are substantial bonuses
awarded for committing to taking particular numbers of tricks; in general, the larger the
commitment, the larger the bonus. There are small bonuses awarded for winning tricks
above and beyond the commitment.
If the declaring side failed to honor its commitment, it receives a negative score and
the defenders receive an equal but positive score. The overall score in this case (where the
355

fiGinsberg

declarer goes down) is generally smaller than the overall score in the case where declarer
makes it (i.e., honors his commitment).

Appendix B. A new ending discovered by GIB
This deal occurred during a short imp match between gib and Bridge Baron.









96
QJ85
AQ3
K J 10 8

KQJ875
943
7
642










43
A72
J 10 6 2
AQ73

A 10 2
K 10 6
K9854
95

With South (gib) dealing at unfavorable vulnerability, the auction went P2XP3NT
all pass. (P is pass and X is double.) The opening lead was the K, ducked by gib, and
Bridge Baron now switched to a small heart. East won the ace and returned to spades, gib
winning.
Gib cashed all the hearts, pitching a small club from its hand. It then tested the
diamonds, learning of the bad break and winning the third diamond in hand. It then led
the 9 in the following position:












K J 10 8

Q


? ? ?












J
A??

10

98
9

When gib pitched the ten of clubs from dummy (it had been aiming for this ending all
along), the defenders were helpless to take more than two tricks independent of the location
of the club queen. At the other table, Bridge Baron let gib play in 2 making exactly, and
gib picked up 12 imps.
356

fiGIB: Imperfect information in a computationally challenging game

References
Adelson-Velskiy, G., Arlazarov, V., & Donskoy, M. (1975). Some methods of controlling the
tree search in chess programs. Artificial Intelligence, 6, 361371.
Bayardo, R. J., & Miranker, D. P. (1996). A complexity analysis of space-bounded learning
algorithms for the constraint satisfaction problem. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 298304.
Billings, D., Papp, D., Schaeffer, J., & Szafron, D. (1998). Opponent modeling in poker. In
Proceedings of the Fifteenth National Conference on Artificial Intelligence, pp. 493
499.
Blackwood, E. (1979). Play of the Hand with Blackwood. Bobbs-Merrill.
Eskes, O. (1997). GIB: Sensational breakthrough in bridge software. IMP, 8 (2).
Frank, I. (1998). Search and Planning under Incomplete Information: A Study Using Bridge
Card Play. Springer-Verlag, Berlin.
Frank, I., & Basin, D. (1998). Search in games with incomplete information: A case study
using bridge card play. Artificial Intelligence, 100, 87123.
Frank, I., Basin, D., & Bundy, A. (2000). Combining knowledge and search to solve singlesuit bridge. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pp. 195200.
Frank, I., Basin, D., & Matsubara, H. (1998). Finding optimal strategies for imperfect
information games. In Proceedings of the Fifteenth National Conference on Artificial
Intelligence, pp. 500507.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal of Artificial Intelligence Research,
1, 2546.
Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Artificial Intelligence, 55,
367383.
Ginsberg, M. L., & Jaffray, A. (2001). Alpha-beta pruning under partial orders. In Games
of No Chance II. To appear.
Gratzer, G. (1978). General Lattice Theory. Birkhauser Verlag, Basel.
Greenblatt, R., Eastlake, D., & Crocker, S. (1967). The greenblatt chess program. In Fall
Joint Computer Conference 31, pp. 801810.
Joslin, D. E., & Clements, D. P. (1999). Squeaky wheel optimization. Journal of Artificial
Intelligence Research, 10, 353373.
Koller, D., & Pfeffer, A. (1995). Generating and solving imperfect information games. In
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,
pp. 11851192.
Levy, D. N. (1989). The million pound bridge program. In Levy, D., & Beal, D. (Eds.),
Heuristic Programming in Artificial Intelligence, Asilomar, CA. Ellis Horwood.
Lind-Nielsen, J. (2000). BuDDy: Binary Decision Diagram package. Tech. rep., Department of Information Technology, Technical University of Denmark, DK-2800 Lyngby,
Denmark.
357

fiGinsberg

Lindelof, T. (1983). COBRA: The Computer-Designed Bidding System. Gollancz, London.
Marsland, T. A. (1986). A review of game-tree pruning. J. Intl. Computer Chess Assn.,
9 (1), 319.
McAllester, D. A. (1988). Conspiracy numbers for min-max searching. Artificial Intelligence,
35, 287310.
Pearl, J. (1980). Asymptotic properties of minimax trees and game-searching procedures.
Artificial Intelligence, 14 (2), 113138.
Pearl, J. (1982). A solution for the branching factor of the alpha-beta pruning algorithm
and its optimality. Comm. ACM, 25 (8), 559564.
Plaat, A., Schaeffer, J., Pijls, W., & de Bruin, A. (1996). Exploiting graph properties of game
trees. In Proceedings of the Thirteenth National Conference on Artificial Intelligence,
pp. 234239.
Schaeffer, J. (1997). One Jump Ahead: Challenging Human Supremacy in Checkers.
Springer-Verlag, New York.
Sheinwold, A. (1996). Five Weeks to Winning Bridge. Pocket Books.
Smith, S. J., Nau, D. S., & Throop, T. (1996). Total-order multi-agent task-network planning for contract bridge. In Proceedings of the Thirteenth National Conference on
Artificial Intelligence, Stanford, California.
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning and dependency-directed
backtracking in a system for computer-aided circuit analysis. Artificial Intelligence,
9, 135196.
Sterling, L., & Nygate, Y. (1990). PYTHON: An expert squeezer. J. Logic Programming,
8, 2140.
Wilkins, D. E. (1980). Using patterns and plans in chess. Artificial Intelligence, 14, 165203.

358

fi
	ff
fi 
			 ! #"$ % 
'&)(*,+.-//!(102-346574/-

CEDGFIHJH

89:;< =,-?>
/!(@A:	%&=B3?>
/!(

KMLNPOQOQROGSUTWVYX$Z[F]\_^MHGN`X$ZaKML?NPOcbdF]OGFfegN,Z[RhiOjCEDGeghlkGSiD
m

F]kGenRX$ZgR?opTqF]N`egorD

sfu2
t vxwzy{u$|~}2

2fffff2$[i$f$$6x$#2

G ff
v r$vx f
f
x{9 ~
,<!!,

$i$f$$6x$#2

9x 1?ff

,g
`96?6`q91~1B66]1!
.x96f6, !qY1`[.
1 6G2n1,16G[G1992P71
B
6#1#19

$1~f6 1.g6
!669<$ 6?!<ffq191` 69#]i~] <6 1.f1Gff9
,11P6]1Ylx9B)x16ff?lW196
 1
6q6`?)9
?iG<6  11.l196
2$1M19`17g91. 1.i71
6{Yx66691911q6196
1?[P6lx ?!1 7
11.)`,61!?9`fx .G?11$l 6ff 61! 6
6~?112]
,7#1r
6iffff!6x16f69 [7
6`16`176~[[ 1!1W,[

	fiff ] ff
!#"$%'&()+*",-$').!/"0$12"3$4#51/47698"1:;4#1/8<"$>=&?><@84A1B8<*C=&ED9#"1/1/4#1/5
$%*F$%%G;$HIJ/4#$K4#1/8<-"$%L4#$JG;"4M1/A*'N/O/E%=;P1/)Q",D/D/="8R$S4M1TDB#"1T51-",4#=1H
IJ/U6B-$%V",D/DB="8-E)K"$WN/A=DXNEY:*?ZK#O/G["1BNE\/O-$%.]_^``acbc^``deRHUf_1E4AV$%G'4#1/",DB",DX
=1Thg?ikjklmklnWjop$*c$%%Gq]rZK#OBGtsu\/O-$%bV^``aeRb9*N/$8<-4AYXNT"
1)vDB#"1w51R",4A=1
%8-B1/4#xcOY9"$%Ny=1hz9{A|,}/}/~}:?%|RzBb:)k/4#8-h)K"$GhO/8Ry&"$%/"1
"1*h=%8-B1/4#xcOF1=)S1
",/4#$U4MG
HIS4AUDB",DX$%",%N;"?)S/=A.$-4A$U=&$%",-8R
<=-$/",<691/NhB4#$",D/D/="8R
Yc*wG;",F4#15;4#1!G
=P<@84A1:h]r\=snV=15bW^``cL"GhYB/"G
DB",4b/l",-b9sn"GPYB8-:b
^``deU"1/NYc*P<F%1/N/4#1/54A%=L8<=DXK)S4AG
=K<cDB$$4AKDBM"1/1/4#15k#"15O/",5$J]rE=c/Ab,okYXb
mk=,XG;"1B1bcsv?4#G;=D=OBA=$b/^``d:Bg?",1s[L1=YBA=F8bB^``d:j1/N-$%=1VbcFG;4AVbs#Nb^``c
okYbB,eRH
IJ/$%8<=1BNw",DBD/="8-T4#$Sz9{A|,}/}/~}:w|fik|r~((|~({~rr?G;=cNVb:)SB4#8-%-"1/$M",%$D9#"1/1/4#1/5
%=;D/-=D=$-4A4A=1/"$",4#$_6",YB4##4A*!]rL"O%hsF#G;"1b^``eRH.f1TDB",-4#8O/#",KL4M$KL=DX?/",
"N,"1/8<$4#13w$%",%w=&",h=&D/=DX=$4A4#=1/"U"$%=1/4M15!$%*F$%%G;$8",*3N/4#8<A*0=y%=
DB#"1B1/4#15w$%*c$%G;$EA*F4#15w=1/4M$?%8-/1=#=5*H;f1&r"8<bP"O%'"1/NcMG;"1]_^```e?D/-N/4#8<%N
/",k$%",-8R>=1>D9#"1/1/4#1/5yG;=cNB$)S4##XYX8<=G
h$ODX%9O=OB$YX8"O/$PP$",%P=&P",-S4#1
D/=DX=$4#4A=1/""$%=1/4M15;$%*F$%%G;$)S4M#X"Nfi"1B8<PGhOB8-&r"$%%SB"1>4#1TDB#"1B1/4#15y$%*F$%%G;$H
jB4A-N1/)",D/DB="8-C4#$'F<F~M<r~r|,%<!z9{7|}/}/~}:"$hD/=DX=$%NY:*3Z.=1
"1/Ng<1
]_^``cb^```eRHf1/4#$k",D/D/="8R0"wOR4#$%4#8?&rO/1/8<4A=14#$SN/-4AN&(=G+$%DX847698",4A=1!=&
DB#"1B1/4#15S4M1/$%"1/8<K"1/NyO/$%N&=5OB4#N/4#15kK$%",-8-;=O5K$%",%J$%DB"8<Hj$NG
=1/$%-",%N
Yc*'E$%*F$%%Gt\U\]r$=&=\/"$%_\=)K",-N9eJ",?DB#"1/1B4#15h8<=G;D4#4A=1T",JjSf_l.:,FbB/4#$
",D/D/-="8-D/-=N%=
YXE8<=G
DX4A4AHf_1T&"8<b9\\C=O%DX&=-G
N"M9/E=J&rO/#A*;"O%=G'",4#8
$%*F$%%G;$L"1BN)."$P1=G;4#1/",%NQK_,zCP~M<r~(}cF~MRFTS(,;|,}9R'K{A|,}/}/~}:!Bfit",L
8<=G
DX4A4A=1VH

  -//!(g %%9!=6P
=KB?
.g
!fi;f
:!	%&%		&?%2%M? =

fi

2fffff$i

f_1ym?Fl]rZ.=1svg<1b/^``eRb:5="N/4#$%"1/8<$U",.$%4#G'",%NyY:*P",DBD/=4#G;",4M15k$%=MO4A=1
A150%="w#"fi",4A=10=&yDB#"1B1/4#15
"$%]rZ.=1bWnV=c-4#1B8$bXsg<1b^``deRHh/4MAh\U\
O/$%$SP$"G;P#"fi",4A=1&=kN/-4AF4#15
4A$S/O-4#$%4M8$b4ASN/47X-$&=G+m?Fl4#1>"1:O/GhYX=&4#Gy
DX="1:N"4##$Hf$YB"$JO-4#$4#8.%8-/1/4Mx:Ok8"1
YXJ$%1'"$"1'",D/DBM4#8",4A=1y=&Vg?iSjlmlnjko
%=P/J#"fi",4A=1HIJ/4M$U*c4AMN/$5="BNB4#$%"1/8<J$4#G;",%$/",b:4#1'N/47X1/8<J%=PmEFlS$$%4#G'",%$b
N=L1=-A*P=1;"1'4#1/NDX1/N1B8<."$$-O/G
D/4A=1VH\U\O/$%$"ENB47-1$%",R8-
%8R/1/4#xcO/"1;mEFlb
1/"G
#*!"1<}<,%R-L,=&/4M#78#4#GhYB4#1/5bB8<=GPY94#1/4#15wA=F8""1/N$%*F$%%G;",4#8y$%",-8RHy\4M1/"#A*b
4AkG
DBA=*c$?"D=)-&O/WD/-O/1B4#15
%8-B1/4#xcOhB",$%#8<$"T$%=&D/-=G;4#$4#1/5;$O/88<$$%=R$k%=T"8R
$%",-8R1=FNb"1BN0"1/=?D/-O/1/4M15
%8-/1B4#x:O/h/",E8O$=OY/-"1B8-$)S-h4A?",D/DX",-$k/",
$%=G
E5="W/"$YX1"8-/4#N%=:='",-A*HZ.=>%8R/1/4#xcO$",E=Y/"4M1N"$J"
$4#N/<X8<J=&
YB"$%hO-4#$4#8kG;=cNVH
.=1B8<-1/4#15E/S$%",R8-T$%%-",%5*;B",\U\34#$YB"$%N;=1Vb:).SG'",yS&=#A=)S4#15HUjvA==&
8#"$$-4#8"DB#"1/1/4M15'",D/D/-="8-$b#4ADB",4#"7=RN?DB#"1/1/4M15]r08jk##$%%hsiJ=$%1:YB#4#%b^``F^e
=
DB#"1B1/4#155-",DB"1/"A*F$4#$]rZK#O/Gs\BO-$%b^``deRbK",-51-4#8wD/=Y9AG$%=AF4#15G
=FN/$b
NA=DXN&=#A=)S4#15T$%=G
'=4#8"8<=1B8<D/b"1BN%$%%N3=13<F"G
D9A$E&=G'#4#%-",O
",&%).",RN/$Hf10=OL",D/DB="8-bW<cDB#=-4#15;4#N"w=&/O-4#$%4M8h$%",R8-bV/
4#$E1=$O/8R8A",
N/4#$4#1/8<4A=1YX).13NA=DBG;1L"1/N0%$%4M15HIS
$%",-8-$%%-",%5*b"$?).#"$?
DB-O/1/4#1/5
%8-B1/4#xcO$b",w51-4#8G
=FN/$PB",/"TY1CG
=4A,",%NCY:*=Y9$%F4#15><"G
DBA$Hj#$%=b
N$4#51hN84M$4A=1/$)-JG;"N.=1yY9"$4#$=&98",&rO/:<cDX-4MG
1",4#=1HIS/4#$4#1%-=cN/OB8<$4#1:%=?
$%*F$%%G"
YB4#"$S%=)K",-N/$kP<"G
DBA$kO/$%N!&(=k%$%4#15'NBO-4#15;N/A=DBG
1:HKh).P%$4#15
=O"A5=-4A/G'$E=1"!-"15=&KN/=G;"4#1/$E=&%1CO/$%N4M1'D9#"1/1/4#1/5TM4A%-",OHTIJ-=O5=O
LD9",Db/)h)S4##X-&(k%=N=G;"4M1/$/",",L&x:O/1A*O/$N>4#1#4A%-",O/bB"1/N%="$%F$
&=Gp$-O/8-N=G;"4#1B$b"$KR<}9;|,-RHf_1P/.N#=DBG
1:DBB"$%b,)KO/$%NhYX1/8-/G'",?<F"G
D9A$
&=Gkw7XbK(FURrBb9.-BbB.-/Rb,Mr-b>R%XbTX-Mb"1/N>UAfi-r
N=G;"4M1/$HU/1wN/$8<-4AYB4M15L=O"A5=-4#/G;$K4#1?DB",DXb:).E4#1/N/4M8",%S?DX=4#1:$)S/k/=$%
%$%4#1/5
<F"G
D9A$DB#"*N!"
-=AE&(=kN$4A51N84M$4A=1G;",F4#15H
l#"1/1/4#1/5h4#$.F1=)S1w%=
YX?l.FlXjSK8<=G
DBA%E14#1T4A$K$4MG
DBA$%&(=RGq]rZ*F#"1/N/b^``fi:eRH
IJcO/$b4#1hK51-"8"$%b4#$1=?<@84A1:U"#5=-4A/G;4M8G
=FNHf4#$&=K).=)k/4#A
%=>A=c=!&=L"A5=-4ABG;$?/",L",y<@84A1:L",LA"$%E=1-$%%-4#8<%N$O/YX8M"$$%$HIW=$%=G

<F%1:b
/4#$4#N"/"$YX1DBO/-$ONYc*3DX=$4#1/5!$%$%*F1"8<4M8"K$%%-4#8<4#=1/$P%=0wDB#"1/1B4#15"$%
$%DX847698",4#=1/$]rZ.*F#"1/NbK^``fi:eRH?O",D/D/="8R4#$h8<=G
DBAG;1",*%=/4M$H>"G;4#1/4M15
<4#$%4#15YX1/8-BG;",F$bF=1?691/N/$./",/*bB4#1/NNVbcN=
1=<FDBA=4#./&rO/#<FD/$$-4Ac4#*;=&
O/1/N/-A*F4#15EDB#"1B1/4#15L&=-G;"M4#$GHUIJ=O5*wN=1/=&rO/769M/"1*=Y:F4A=O/$-4A54MN$%*F1:"8<4#8"X<
$%%-4M8<4A=1/$b"#G;=$%1=1=&XG4#$UDB",4#8OB#",-A*PB",-NHf1;"MG
=$%"#F=&XJ<4#$%4M15?Y1B8-/G;",-
N=G;"4M1/$b"1=1F=D/4#G'"KDB#"18"1b4#1D/R4#1/84ADB#bY51/-",%N24#1DX=A*F1=G;4M"4MG
H2$4#15
hY1B8-/G;",-c$k&(=?4#1/$DB4A-",4A=1NBO-4#15;N/A=DBG
1:b9).y/"yYX1!",YB#P%=T8<=G
O/D)S4#!"
OR4#$%4#8yG
=FN/",h4#$L1=LDB=,",YBA*0<@84A1:bY9OLN=c$L).=).#G;DB4A-4#8"MA*>=1"!#",5
8#"$$S=&D9#"1/1/4#1/5"$c$H.IJ/4#$8M"$$J4#1/8MO/N$"#G
=$S"#X=&UP8O-1SDB#"1/1/4M15hYX1/8R/G;",F$H
f_1O/4#4AA*bk"A5=-4A/G'$<cD9A=4Ak$4#G
DB#K$%-O/8<OkO/1/N-#*c4#1/5k$J"$%F$H?O=1/5=4#15
).=4#$h8<=1/8<-1NC)S4A691/NB4#15>"&(=-G'"8R/",-"8<%-4A",4#=1=&J/",%$4#G
DB#>$%%-O/8<O/b"1/N
Yc*T&(=RG;"#4A4#1/5hh8#"$$=&DB#"1/1B4#15"$%c$S/",S\U\).=c$J)#X=1VH
c8<4#=1!54A$J"
$8RG;",4#8Lc4A)v=1!\\.$J$*c$%%G",-8-/4#%8<ObB"1/N!F8<4A=1!y4#1:%=FN/O/8<$
=O.1=",4A=1/"8<=11:4A=1/$.&=KFISiJf_l.yN=G'"4#1/$Hc8<4A=1/$h%=yLN$-8<-4AYXSY9"$%kO/-4#$%4#8
%8-B1/4#xcObV$%",-8R"A5=-4A/G>bX"1BND/-O/1/4M15'G
=FN/$bX$D8<4#A*Hc8<4A=1d$/=)S$E=)
"A5=-4#/G;$E",;<F%1/NN%=!N")k4Ajk?nN=G;"4#1B$Hc*F$%%GDX&=-G;"1/8<;4#$Lfi"#OB",%N34#1



fi 9>~20$7`$22



$##!2F

c8<4A=1cbKN/G
=1/$%%-",4M15/",'\U\u51-",%$w$%=#O/4A=1/$y<F%G
A*C&"$'4#1"3#",-5>-"15>=&
DB#"1B1/4#15YX1/8-/G'",N=G'"4#1/$HUf_1=-N%='4M##O/$%%R",%=O/J4#1OB4A4A=1/$.=1Pc4#1BN=&U$%%-OB8<O
/",\U\8"1<FDBA=4#w$OB88<$$%&rO/#A*b$%8<4#=1"M$%=54A$T<F"G;DBA$w=&hN=G'"4#1/$)S0
G
=FN>4#$#$$J",D/D/-=D/-4#",%H\U4#1/"MA*b%='8#",-4#&(*LDX&=-G;"1/8<EN/47X1/8<$JY)1\U\"1/N
mEFlbc/?$8<4A=1N$8<-4#Y$."y1:OBGPYXK=&W<FDX-4#G
1:$).LG;"N?4#1w=-NK%=
$%4MG;",%?)S/4#8R=&
E1)v"A5=-4#/G;4#8S%8-/1/4Mx:O$K4#$.G
=$%O/$&O/HE$/=)[8<=1/18<4#=1/$.%=-#",%NT)=-",K
DX=4#1:$4M1h/%<c)kK*h",DBDBA*b"1/Ny=c4#)=8<=1/1/8<4A=1/$4#1;c8<4A=1`cHF8<4A=1^
=O#4M1$K=OS8O-1J"1:O/P=&$%",-8RH

 

U


	 

ff

  2f

I=
54AE?"NS"1T=F4A)[=&\U\.$J$%*F$%%Gt",R8-/4A%8<O/b/\U4A5O
^E$=)S$=)[\\.$kG
=$%
&rO/1/N/"G
1:"X%8-B1/4#xcO$J",P",--"15NH

Relaxed GRAPHPLAN
Goal Distance
State
Helpful Actions
Task Specification

Solution / "Fail"

fi

Enforced Hill-climbing

\U4A5O;^ \\.$SYB"$%P$%*F$%%Gt",R8-/4A%8<O/H
IJ/k&rO/1/N/"G;1"O-4M$%4#8%8R/1/4#xcOk4M1\U\C4#$-#"ficNg?ikjklmklnWjoPbF)S/4M8-))S4##9N<
$8<-4#Y.4#1F8<4A=1HUIS%8R/1/4#xcO.5$8"#ANh=1*h$%",R8-
$%",%KYc*w<},%R-L:~({{ %{~;~(}cb
=OU$%",-8R"#5=-4A/GHIJ/4#$4#$W"k&=).",RNh$%",R8-/4#1/5S154#1/bfi%=YXN$-8<-4AYXNL4#1yc8<4A=1
acHgE4A1
"y$%",%b/M"ficN!g?ikjklmklnWjoQ4#1/&(=-G'$E$%",R8-)S4#w"5="VNB4#$%"1/8<$%4#G;",%b9"1/NT"N/N/47
4A=1/"MA*y)S4A'"$%K=&D/=G'4#$4#15h$O/88<$$%=R$&=K$",%bc/PF{ zfiRF{V|r~,}b%=hYXN$8<-4AYXN
4#1F8<4A=1CcH>kDX=13%-G'4#1/",4A=1b1&=-8<NC/4##78M4#GPY94#15'4#P=O%D9O$P"$%=#O4A=1DB#"1b=
DX=$B",S4A/"$J&r"4#ANVH
?1L%=DP=&FYB"$",-8-B4A%8<O.$=)k1L4#1L\U4A5O-J^,b)./"4#1:%5-",%Nh"J&)3=D/4#G;4#",4A=1/$

fi

%='8<=DXL)S4#$%DX84#"8"$%$JB",S",=$%PNBO-4#15%$%4#1/5



f&L"3D9#"1/1/4#1/50"$28<=1"4M1/$$",%$&=G

)S/4#8R5="k4#$O/1"8R/",YBAC]rN"N21BN/$b

N/<691N4#1>c8<4A=1>acHeRbB1T1&=-8<N>/4#M78#4#GhYB4#15L8"1T&r"4#X%=y691BNw"
$=#O4A=1HUf_1w/",



8"$b9";8<=G
D9A%POR4#$%4#8$%",-8R>154#1/L4#$4#1:=N>%='$%=#LE"$%T&=Gt$8<-",8RH
f_1yKD/$1/8<K=&B5="=-N/-4#15$b,1&=-8<N
/4M#78#4#GhYB4#1/5K$=G
4#G
$U)K"$%%$"?A=U=&94#G

"8R/4AF4#155="#$PB",1N%=!YXw8",N&=y#",%h=1VH>I)=%8-/1B4#x:O/$P%*F4#15!%="=4MN

fi

B4#$J",L4#1:%5-",%N




k-|,{<{#r~,}/bK4#1:%=cNBO/8<N4#12c8<4A=12cHcbK8O$
=O/;Y/-"1/8R$y)S->$%=G

5="/"$J",D/D9",1#*YX1>"8R/4AN>%=c=;",-#*H
IST:|,{J|c<}9|w%8R/1/4#xcOb"N/",D/%NC&=G



)=-3Y:*

"1/"0E=c/A]_^``eRb&N/$

/E5="#$K%=
EDB#"1B1.4M1w"1T=RNJN%-G;4M1NT"$"yD/<D/=F8<$$E]c8<4A=1cHcHeRH



fi

 
U

  

g ff 

 ff

ff  ff

2fffff$i



\=S4M1%=FN/O/84M15\U\.$YB"$-4#8?%8-/1B4#x:O/$bF)L8<=1/$-4#NJ$4#G;DBALFIJiJf_l.'DBM"1/1/4#15h"$%F$b/"$).
4#1:%=FN/O/8<NTYc*w\U4A$k"1/Nok4M#$$%=13]_^`dc^eRHJ?OJ1=",4#=1/$S",h"$&(=MA=)S$H

,+

  !"u$#&%(') *

$%",%

.-

Q~MP|k}/~rP.%L{7-~-|,{U|,

h"$-$O/G
E/",k"#X=DX-",%=k$8RG;","",E5=O/1/N/Nb/4H HAb/).E=1/A*w"AT",Y=O/P|r~,}H

  !"u0/1%(')v!23,45"u *

76

FIJiJf_l."8<4A=1

68

96 ;:

!~MP|wrR~z9{#

96 ;: 96

]Mz9%fi] e |c] e <{M] e%e

96

<
GB -IH

=6> 96
.-A@
JHKML

F%z9%] ey|,
FDB8<=1/N/4A4#=1/$
_

>

WFhRF{7.%h|Rzz9{,~(}c>|'R~}:{#P

CBED

V?|,}!|,

L

.> <

R}FJRSR|fi

Fj

]

%e

96 cb

96

|] e%e ?{#] e

F}9r}

6
< ON

S<|,yc| h"8-/4#$


96 edf+
<

z9] e
<

96 gdh+i>

F<?z9%] e

96

|,}9>{7] eh~(hFLNA%

C|] e

%K3|r~,}w|'<|~(L:r}-|fi,{({A

PRQTSTUWVYX Z+:T[96ff\ ]8_^ Z+a`
]

?6
96 F> <

K|:] eh~(hF?"NBN!#4#$y_

#4#$_SE|r~,} |<w-~(}c
|hU_?|,

~M

7+A-EH

JF|r~r,}~M<|~r!-L",D/DB#4M8",YBAT~(}

!k

WyRF{7k_

|Rzz{ ,~}:>|' B<}9_L;,yc|,},}Xy|r~,}|'|h~(E%-F-~ <{T:r}-|fi

PRQTSTUWVYX Z+J:T[96 ( :OlOlOl":m6onp\ ]8 PRQ"SqUrVYX PRQ"SqUrV!X Z+:T[96 ( :OlOlOlT:m6ons ( \ ;:T[96"n\ ;l
]

%e

  !"uftu%wv  2#wyx~3qz *
;> g
.%h|r~,} .|,}9

]

]

%e

%e

|{}8 Z~::F
I


DBM"1/1/4#15"$%



~(}B~r~|,{<| |,}

]

eP~M|wr~z{A

.-

?|,{  T|,%h%P|,

<

R~

F<

q~Mh

q

?OJ/O-4#$%4M8G;=cN4M$YB"$%N=13%{7| chz{7|,}B}/~(}cT|fi-RbB)k/4#8-",-PN<691N"$J&(=##=)S$H

  !"u%w    fv  2#'rwyx,3qz * Yk
|{ M{
M{W8 Z~9::F F> <
~  8$  Q 96 ;: 96 ;:F = 96 ;:

I{8 Z~:=:F .-H

K~ ,}C|
z9{A|,}/}/~}:0|fi-

#"fi",4A=1

._

~(L:r}-|fi

:]

u]

] e |c] e

e

e

~r

t]

Ue

WFP<

96 ;: 96 eDE~,

]Mz9] e |F] e {M] e%e

f_1).=-N/$b=1y=Y/"4#1/$y#"fiFN0DB#"1/1B4#15'"$!Yc*4A51=R4#15'/yN/A%
#4#$%$=&"M"8<4A=1B$H
lM"1/$",P$4MG
DBA?$%xcO1/8<$J=&"8<4A=1B$J4#1T=OJ&-"G
).=H

 !"u01%wv
[96 ( :OlOlOlq:m6on\
mj

 2

* Yk
~

{8 Z~::F .mj 8
T> - T-> < I&
 d RP Q"SqUrVYX =:m
O{> 
k
q a{ M{,-

K~ ,}|>z9{A|,}/}/~}:|fiR

L_L|r~,}E~}

k

]

c|<,{ ,<?L|fiR ~ 

|r~r,}0 B}9Rh~(P-|,{{A-|h#"fiFNDB#"1

F,

e vDB#"1[~(!| /}9R

~r

~ ~r,{ PFh<{A| :|r~r}

q



]

_

hekc,{Afi S}

fi 9>~20$7`$22

 &) 
  7

f

g





$##!2F

7Q  n

B]  





f_1
/4#$$%8<4#=1b).S4#1:%=cNBO/8<KJYB"$%J/O-4#$%4M8.G;=cN
OB$%N;4#1
\U\.Hf4#$N-4#NyY:*y",D/DBA*F4#15
g?ikjklmklnWjo%=P#"fiFN;D9#"1/1/4#1/5k"$%F$HIJ/J$O/#4#15?5="9NB4#$%"1/8<J$4#G;",%$.N=P1=bF#4A
mEFlJ$E$4#G;",%$bA*0=13"14M1/NDX1/N1/8<y"$$OBG
D/4A=1H
D/=;/",P/;O-4#$4#8y8<=Gy
DBO",4#=14#$KDX=A*c1/=G;4#"bc54AP$%=G;L1=4A=1/$K=1>/=)QN/4M$%"1/8<E$%4#G'",%$S8"1YX?D/S8"O4#=O/$b
"1/N>N/$8<-4AYXE=)QLG
=FN>8"1>YXL4#G;DBAG
1:%NT<@84A1:A*H
.=1B$4#N?;O/-4#$%4#8hG
=FN/",P4#$?O/$N4#1m?Fl]rZ=1/hstg<1b^``eRHgE4A13"

{8 Z~:=:F

DB#"1B1/4#15"$%

t]

+

UeRbm?lv$%4#G;",%$?&=L"8R3$%",%

{  8 Z~  :;+:F
+ 
 A

$%",-8RT?$%=#O4A=1A15w=&V"$%
"8-B4A$S/L5="#$$%",4#15'=OJ&=G

]

/",L4M$k-"8-N34M10"w&=)K",-N

eRbB4H HAb:?A15w=&W"h#"fiFNwDB#"1'/",

,{ 

.HKjk$k8<=G
DBO4M15
L=D/4MG;"V$%=#O/4A=1>A15>%=

)S/4M8-
).=O/#NG;",k"1"N/G;4#$$-4AYBAOR4#$%4#8

4M$

B/",-N0]rZ.*c#"1BNb9^``fi:eRbFm?l$%4MG;",%

4#$"
-=O5>",D/D/-=4#G;",4A=1TYB"$N=1!8<=G
D9O4#15yL&=#A=)S4#15y).4A5:fi"#O/$H

BED|+
  FG 

 QqX  B ifi8  
 


4A&

 p 

4A&

] Ve

"N/N + 0

=)S4M$%

  QqX   )8 
] Xe

D/ + 0

^

]_^e

mEFl["$$-O/G
$E&r"8<$E%=>YX
"8-/4#N34#1/NDX1/N/1A*>4M1/;$%1/$%
/",E/
)4A5:E=&K"$%
=&V&r"8<$



"1T"8<4A=1$KDB8<=1/N/4A4#=1/$

$%",%$O-4M$%4#8k$%4#G;",%P4M$



4#$$%4#G;",%NT"$.hRFQ=&V/4#1BN/4AF4#N/O/"c).4A5$HIJ

 Z+ fi8  QqX   ]8    QqX  

] e

] e

]

e

]e

j$$O/G'4#15E&r"8<$%=hYXS"8-/4#Nw4M1/NDX1/N1:A*b/4#$O-4#$4#84A51=$DX=$4A4AS4M1%-"8<4#=1/$
/",8"1T=F88OH.=1/$-4#N.?&(=##=)S4#1/5h$=-<"G
DBADB#"1B1/4#15L"$9b)S/k/?4M1/4A4#"9$%",%

o ( :; - 

4#$KG
DB*bB/E5="#$S",

Ofi

bB"1BNTP",PE&(=MA=)S4M15yP"8<4A=1B$

1/"G


2 
u2 
u 2 
u

8
(

-

:
):
Tff:o ( ff:
Tff:o - ff:
: TIff:
|
]D/-

8
8

"N/N

NMe

]



]

e

e




]

mEFlJ$k).4A5,"#Oh8<=G;DBO",4A=10$OBA$S4#1

e

/"F4#15)4A5:=1b"1/N!"8R05="B"F4#15

).4A5).=HLj$$O/G;4M15'&r"8<$%=wYX"8-/4AN4M1/NDX1/N1:A*b/yN/4#$"1/8<h=&4M1/4A4#"$%",%
%=">5="$%",%w4#$E&=;$4#G;",%N%=>&(=O/HTYcF4A=O/$A*bV/=).bU;"$%34#$E$=Afi",Y9A;4#1
=1/A*P$%DB$b/"$

W2 
u

( "1/N

2 
u



- $B",EED/8<=1BN/4A4A=1


Hf_1=-N/%=;",h"88<=O/1:S=&

Z~9:;+:F

$O/8RDX=$4A4A
4#1:%-"8<4A=1B$bV=OP4MN"T4#$?%=!$%",yg?iSjlmlnjko=1;"$c$']

eRbU"1/N

<F%-"8<h"1<cDBM4#84AE$%=#O4#=1bW4H HAbW">#"fiFNDB#"1HE1'8"13/13O/$%
/4M$EDB#"1&=hO/-4#$%4#8

fi

,"#O/",4A=1H
')S4##$%'4#1/;1<ch$%8<4A=1/",P/4#$E",D/D/-="8-34#$?&"$4AY9A Pg?iSjlmlnjko
8"1>YX?D/=1>%='$=AL#"fiFN"$%F$S4M1TD=#*c1=G'4#"B4MG
H

#}v

 uvx231u2v    0x,3qz3

 2 #w

nV.O/$<"G;4#1/S=)[g?ikjklmklnWjoYB"$)S1'4#4#$$%",-%N'=1w"LDBM"1/1/4#15?"$%
B",N=c$
1=8<=1"4#1"1:*3NA%T#4#$%$HwTY/R4A<B*0c4#)YB"$4#8'1=",4A=1/$=&g?iSjlmlnjko
"A5=-4#/Gq]rZK#O/Gs\BO-$%bW^``deRH

"



fi

2fffff$i

Ofi

jD9#"1/1/4#1/55-",DBC4#$h"0N/4A-8<%Nb#"*N5R",DBB",8<=1:"4#1/$).=c4M1/N/$L=&k1=cN/$ E<|
}9:"1/N!|r~,}>}9:RHIJS#"*-$."A%-1/",%kY)1&r"8<"1/N"8<4A=1M"*-$b:)SS=1S&r"8<
"1/N"8<4A=13M"*E%=5G;",;OD"r~(';z9HPf_10y6B-$%4#G

$%%DWbV1:O/GhYXEFb);/"
&"8<y#"*y8<=$D=1BN/4#15%=4#1B4A4#"$%",%T"1/N3/"8<4A=1C#"*
8<=-$%DX=1/N/4#15%=0"#



"8<4A=1/$J/",J",L",D/DBM4#8",YBA?4#1wL4#1/4#4#"9$",%Hf_1T"8-!$-OYB$%xcO1:.4MG
E$%%D



#"*J=&"#&r"8<$K/",J8"1TDX=$$4AY9A*yYEG;"N/%ROE4#1
/",S",-LDX=$$4AYB#*;",D/DB#4#8",Y9A?54A1=$%P&"8<$H

b/).L/"L

4MG
E$%%DB$b/"1/NwE#"*J=&"M"8<4#=1/$

?1/>8<-O/84#"K/4M15/",g?iSjlmlnjkoN=c$y)S1Y9O/4##N/4M15!D9#"1/1/4#1/5!5-",DB24#$y

F
6

96:m6 

4#1&1/8<
=&?yr/|,{K {cR~,}!-#",4A=1/$HjD9"4AL=&"8<4A=1/$w]

6"

GOO/"#A*<F8MO/$4AbB4#& 
"1/N

U~}B<<<b94H HAbB4A&=1P"8<4A=10NA%$k"
D/8<=1/NB4A4A=1T=k"1"N/N

Br:FBr
B
96:m6o
6"

<X8<=&V=HjvD9"4A=&W&"8<$L]
4A&."8-C"8<4A=1C",hA

,Br

"8-B4A$



e?",h4#G
'$%%DC>4#$LG'",N

#e","4#G
?$%%D

^
/",h"8R/4A$

MH3jDB"4A=&k"8<4A=1B$T]

 
I

y4#$.G;",NGOO/"MA*y<F8#OB$4Ab

4#$L<8#O/$-4A
=&K"8-"8<4A=1",A

Meh",y"04#G;w$%%D

G

^;/",

4#$yG;",NGOO/"##*<F8#OB$4Ab

6

4A&T"8<4A=1/$y4#1:%&bU=
4A&*/"3-,kzr~(}:3}--fiRb4H HAb4#&J$%=G
wD/8<=1/N/4#4A=13=&
<8#O/$4Ak=&W$%=G
D/8<=1BN/4A4A=1
=&
"1:*w<8#O/$-4A=1T#",4A=1B$",S"#H

vWvxu2fuW3T!"uh# M{A8 Z~Y::F
 = <
vWvxuguF
TN

L
!k
TN
A 






4#$

HIJkDB#"1/1B4#15E5-",DB=&W"P#"fiFN"$'N=c$1=K8<=1:"4#1

q

]

JHKML

m

e>R>|<{A| c3

{Y> K 

%.|fi- |,-}

'

Yk q-

~{({W}9.;|,-|}/EzF|~(y_|h,y|r~,}h|,Lr/|,{{ > :{cR~ ,

J

L

IJ/Jl=DX=$4A4#=1
4#$"$4##*hD/-=1;Yc*y4#1/N/O/8<4#=1y=.JND/
=&XSDB#"1/1/4M15k5R",DBH

|fiR|fi wr~(';zFH'?1/#*4#1:%&-4#15T"8<4A=1/$L",-;G;",N3GOO/"U<8#O/$4#;",E4#G


$%%D!FH.jk$JL",P1/=;NA%L<X8<$b91/=
DB"4AJ=&U"8<4A=1/$k4#1%-&($H
R}9r~ 'R|fi ;r~'y<z

r~('
z

c

^,HlP4#1/N/OB8<4A=1!:*:DX=$-4#$bX
&r"8<$L",

1=?<8#O/$4A
"$?4#?"8R/4A-$?=1/
4#G
y$%%D3""N3",
1=H\=G/4M$4#k&=#A=)S$/",L1=



DB"4A=&"8<4A=1B$J/"$J8<=G
DX4#1/5;1N/$HIS*N=1=J4#1:%&(-E4AH

1$%",%N0=1"wDB#"1/1/4M15
"$%bVgEiSjlmlnWjo<F%1/N/$DB#"1B1/4#15
5-",D90M"*?Y:*
#"*.O/1:4#9"L&r"8<.#"*.4M$"8RNw/",8<=1:"4#1/$."#/5="&r"8<$bF"1/N'4#1')S/4#8R'1=P).=5="9&r"8<$
",
G;",N0<8#O/$4#H (

F",4#15w&(-=G+B",?M"*bV"w8OR$4AYB"8c).",RN0$",-8-"A5=R4A/G4#$

4#1:=NHI=y691/N"yDBM"1T&(=J";$=&&r"8<$S",SM"*
",?#"*

r

c

i

Fb/4#1/4A4M"#4AL$%=&U$%A8<%N!"8<4#=1/$

^L%=wPG
DB*>$%HSIS1b/&=k"8R0&"8<bX8<=1/$4#N/k"#V"8R/4AF4#15w"8<4A=1/$",k#"*

^?=1?",&(%Sk=/J"1/NT$%A8<k6B-$%.=1?/",J4#$.1=K<8#O/$4A=&"1:*"8<4#=1T/",/"$

"A"N/*0YX1$%A8<%NHf&K'<4#$%$h$O/8R"1C"8<4A=1bUD/=F8<N3)S4#1/<ch&"8<Hf&1=b

YB"8-:%-"8-
%=LJ#"$&"8<"1BN
%*%=P"8-/4#S4A)S4A
"ENB47-1"8<4A=1VHf&X"1;"8-/4#c4#1/5L"8<4A=1
/"$YX1$A8<%N0&(=?"8-&r"8<b108<=#A8<?D/8<=1/NB4A4A=1/$S=&"#$%y"8<4A=1/$%=G;",
OD"
1)[$%=&&r"8<$=1E4#G
E$%%D",R#4AHFOB88<NT)S1T&r"8<J#"*S



L4#1/4A4M"9$%",%



4#$

"8RNbF)S?1=y"8-/4#c4#1/5h"8<4A=1/$K1Nw%=yYXk$%#8<%NH?1#"fiFN"$c$b1=YB"8-:%-"8-c4M15
=F88O-$J4#1!gEiSjlmlnWjoh$S$%",-8R!"A5=R4A/GH

vWvxu2fuW3T!"u/h M{A8 Z~Y::F
 = < .k
vWvxuguF






]

q

JHKML

e>R>|<{A| c3

~{({W} <-|<,r_|

-

{Y> K 

%.|fi- |,-}

'

J

gB

ZK"8c%-"8F4#15T=1/#*=F88O-$k4A&"#V"8-B4A-$k&(=";&r"8<

L

",P<F8#OB$4AL=&$%=G;P"A"N*

$%A8<%N0"8<4A=1H4Al=DX=$4A4#=1^,b9).PF1=)p/",k1/='<8#O/$4A=1B$K<F4#$b9"1/N!:O/$bB/",k/4#$

(!oe"FO;?JZwF"w
TZo=Zo;TZF(=Zeoo;(FoAro1oTZ9w





fi 9>~20$7`$22

B

N=c$k1/=/",DBD1VHSjkM$%=b4#&

c

^P$OD/DX=4#1/54AH



34#$k4#1!5R",DBM"*



$##!2F

bX1y4#$S",EA"$%=1y"8-/4#?4M1!#"*



/4#Ah
",YX=
",5O/G
1:",4A=14#$$OF@w84A1&=?$-=)S4M15l=DX=$4A4A=10cbV4AkN=c$?1=?%#
O/$.GhOB8-T",YX=O)k/",.4M$"8<O/"##*;5=4#15=1w)S1=1$",$Jg?iSjlmlnjkov=1T"h"$%')k4A=O
NA%#4#$%$HC/",'/",D/DX1/$
4M$y/4#$H2g?4A12"$%4#$
$%=#fi",YB#b>D9#"1/1/4#1/5!5-",DB5$
<F%1/NNCO/14M$%=G
w&"8<y#"*4#$P"8-/NB",8<=1:"4#1/$"#/'5="#$HIJ13/8O/-$4A
$%",-8Rh$",$WYc*E$%A8<4#1/5k"8-B4A-$&(=5="#$",WB4#$VAHUIJU69-$%W",%%G
DB$OB88<N/$b"1/N
1)p5="#$?",h$%OD!=1/h4#G;P$%%D0",-#4AHjk5"4#1bL69-$%S$%#8<4A=1!=&"8R/4A-$$O/88<NB$b
"1/N$%=&=bO/14M/;4#1/4A4#"$%",%'4M$?"8-NVH;IJ:OB$bV$%",-8RDX&(=RG;$=1/A*"$-4#15A
$%).D
=h
5-",DBVbW$%",4#1/5&(=G/
%=D#"*h5=4#15>N=)S1%=>;4M1/4A4#"#"*b"1/N8<=##8<$P"
#"fiFNDB#"1=1w4#$)K"*Hf1DB",4#8OB#",b/kD/-=c8<N/O/S",$K=1/#*
D=#*c1=G'4#"/4#G;k4#1?$4A
=&UE"$%H

x

#0 {8 Z~9:=:F
V- H
V >e ~     -

 u2v  }



]

{A,}::<|!{ ~(S%'|,}/!|r~r,}C~(

zF,{,}9,y~|,{U~(}

vWvxuguF

k

T

JHKML
K   = <

UeP-h|
,{ ,|<{#P%{7| :;
WF}

|,}9

J



ZKO/4##NB4#15KDB#"1B1/4#155-",D94M$DX=A*F1=G;4M"4#1

=&V4#G;k$%%DB$.YBOB4#AS]rZK#O/G

T> <

%K|,-



{

~({{F}90|T,{ r~,}

V W ~   W 
b

F%hFP{#<}:T_L

b

X

"1/N

X
R ~Z

b,)S

4#$K1:OBGPYX

sQ\BO-$%bX^``deRHok=)Lb4#1=OK8"$%k%="X1cO/GhY


	

4#$h"1OD/DXPM4#G;4AE%=1cO/GhYP=&4#G
'$%%D9$H>IJ/4#$P4#$

~}r~'

=&W"8<4#=1/$

O/$%LYX8"O/$",&%h/4M$L1:OBGPYXP=&

4#G
k$%%DB$./"$YX1'YBO/4#Ab"MB"8<4A=1/$.",D/DX",",K$%=G
#"*K4#1'S5-",D9H?)S4#$b4#$
"
#"*



c

)S/L1=;1)Q"8<4#=1>8<=G
$S4M1b4H HAbB"8<4A=1>M"*

a ~

/"G;"N';D/=F8<$$h$%%=D-4#5L")."*HF4#G;4#M",-A*b"8<4A=1M"*

 ~Z 
V  ~    

"8<4A=13M"*
4#1

b

F"1/N



^E4#$J4#N1:4#8"%='"8<4A=1#"*

j$/"$4#$K$%=A,",YBAbc/4#$.4#G
D9#4A$B",K"MB5="#$",-?8<=1:"4#1N4M1'&r"8<J#"*



H

bc)k/4#8-).=O/#N

).=O/#N3Y;4MN14M8"%=

^,bW4MG
DBA*F4#15;%RG;4#1/",4A=1VHPIJy5-",DBY9O/4##N/4M15
DB/"$%
4#$?cO/$?DX=A*F1=G;4#"

H

fi

.=1B8<-1/4#15yDB#"1<c%R"8<4A=1DB/"$% S4Al=DX=$4A4A=10cbV$%",-8-%R"-$%$Lh5R",DB

&=Gt%=D0%=YX=%%=Gb8<=#A8<4#1/5"'$S=&"8-B4Ac4M15'"8<4A=1/$","8-#"*HkcA8<4#15"8-/4AR$

fiff V  ~   ? ;fi

&="$%=&&r"8<$4M$

;]

V  ~   

e Ujv$%=&X&r"8<$K/"$",KG
=$%.$4A

bG;"fi4#G;"

1cO/GPYXh=&SN/4#$%4M1/8<L&r"8<$4M13w5-",DBH0jk1C"8-B4Ac4M15"8<4A=18"1CYX'&=O/1/N%="8R&"8<y4#1

fiff m ~

8<=1/$%"1:.4#G
SO/$-4#15LkDB#"1/1B4#15?5-",DBHj$1:OBGPYX=&VM"*-$%=PYXSA=c=Nw",.4#$
$%",-8R!4#$KDX=A*F1=G;4M"4M1TPN$4#NTDB",-"G
%R$H

Z~9:;+:F

c",-4#15'g?iSjlmlnjkop=1"y$%=A,",YBAE$%",-8R>$%",%E"$%]
4#G
bfi)S4APIJ=G^



,[ff / :OlOlOl":ffs ( \

Fj

"JM"ficNL$=#O4A=1

$%A8<%NC4#1DB",-"#A",4#G
$%%D



bU"1BN

ff 

e*c4##N/$

Rb,)S"8R

;]



eRb

4M1'DX=A*F1=G;4#"

4#$/$%V=&/"8<4#=1/$

4M$E1cO/GhYXP=&
69-$%L&r"8<M"*y8<=1"4#1B4#15

"#X5="#$Hj$)h",P4#1:%$%%N>4M1>"1$%4#G'",4A=1=&. B<}Br~|,{$%=#O/4A=1A15b).PN<61?=O
OR4#$%4#8"$J&=#A=)S$H

 Z+ fi8
] e


ffF
 /    Ms (

]e

IJ/$%4#G;",4A=1,"#O$;=Y/"4#1N/4M$y)."*",-b.=1=O
%$%4#15<"G
DB#$b.OB$O/"#A*A=).
/"13mEFlS$?$%4#G;",%$']rx:OB",4A=1/$;^
"1/N3eRb"$E<F%-"8<4#15!"TD9#"10",$h"88<=O/1:L=&.DX=$4A4A
4#1:%-"8<4A=1/$hYX).1C&"8<$H3.=1/$-4#Nh",5"4M1T$=-P<"G
DBA&(=G

o ( :; - 

$%8<4A=1Vb/G
D/*T4M1/4A4#"$%",%bB)=5="#$

qfi

bB"1/N/P"8<4A=1/$





wYX54#1/1/4#1/5w=&S/4#$

fi

2fffff$i

:
):
Tff:o ( ff:
Tff:o - ff:
: TIff:

1/"G


2 
u2 
u 2 
u

]D/-

8
(

8

-

8

"N/N

NMe

]



]

e

e




]

e

c",-4#15g?ikjklmklnWjo=1'4#1B4A4#"U$%",%b;5="#$h",'8<=1:"4#1N4M1&"8<h#"*h).=b

2 

2 

8"O/$4M15h$%#8<4A=1w=& u
( "1/N u
#"*=1/b)k/4#8-4#$"8-/4AN)k4A


2  o\

- 4#1"8<4#=1wM"*=1HIJB4#$*F4A#N/$?1)5=" ",K&r"8<

H?IJh$O/#4#15;DBM"1!4#$
Rb
u
u
u
( b u
-

W2 

[ W2 ff:. 2 

54AF4#15
O/$/L8<=8<S5="N/4#$%"1B8<E$%4#G;",%L/bB"$SNB4#$%4#1/8<.&=GtmEFlJ$$4#G;",%L&=OH

 / 'gu   "u

 I2"'}"!"u

 rw 

yO/$%g?ikjklmklnWjoP$?O/-4#$%4#8$%4MG;",%$bXxcO/",4A=1cbX4#1>"'5N*>$%%R",%5*b%='YXP4#1:%=,
N/O/8<N'4#1'c8<4A=1'acH#^,bc/",N/=:$1/=",k4A$N84#$4A=1B$UYB"8y=1/8<4A/"$G;"N/GH\=G=O
<FDR4A1/8<)S4#E-O/1/1B4#15./4#$W$%-",%5*L=1L=O/W%$%4#1/5J<F"G;DBA$bfi/4#$W)=F$VYX$%W)S1LN/4M$%"1/8<
$%4#G'",%$S",L8"O4#=O/$b/4H HAbB"$SA=)Q"$JD=$-$4AYBAHUj$J"A"N*T$"4#NVbF"1Rzr~
|{/$%x:O/14#"W$%=#OF
4A=1>8"1!1=JYXE$%*c1:$4#N<@84A1:A*H/",J=1'R|,}N/=;4#$J",D/DB#*'$%=G
L%8R/1/4#xcO$%=G;",
g?ikjklmklnWjo-O-1
"$$/=$%=#O4#=1/$"$DX=$$4#YBAHWZ.A=)Lb).SN$8<R4AYXK$=G
)."*c$=&N=4#15
/",H
IJ6BR$%k%8R/1/4#xcO
4#$E"TYBOB4#A_4#1!&(",O-
=&g?ikjklmklnWjo"1/N1B$O$E"G;4M1/4#G;"#4#*
8<-4A%R4A=1T&(=SL#"fiFNDBM"1HIJE).='=/J%8-/1B4#x:O/$J",PO/-4#$%4#8=D/4MG;4A",4A=1/$H

#%$'&($)+*-,.,/10325476'890;:

|.<=<

IJE=R4A54#1/"Vg?ikjklmklnWjo"#5=-4A/GG;",$S<c%1B$4APO/$E=&$%=,8"MAN

$HIJ/$%L",

N/O/G'GP*"8<4A=1/$k/",?$4#G
D9A*DB=DB",5",%&"8<$&=G=1/P&r"8<#"*?%=1<FHk\=?"8-0&r"8<

B

/",h5$y4#1/$%%N4#1%=0$%=G
&"8<#"*b"o?P?lp8<=-$%DX=1/N/4#1/5w%=0/",h&"8<4#$P4#1B$%%N
4#1:%=;"8<4A=13M"*h",E/;$"G
;4#G;y$%DVH
IJ/4#$EoEL?l[B"$E1==L<X8<P/"1"NBN/4#15

B

7B
c

bX"1/N1/==?D/-8<=1/N/4A4A=1!/"1

>	

8<=1/$4MNN

Hk1D-&(=-G'4#15;YB"8-:)K",-N$%",-8RbXyo?LEl$?",

O/$%SM4AP"1:*w=/"8R/4AbX4H HAbB=1/L)."*>=&G;",F4#15'"'&"8<k%-OP",k4#G


%='$4MG
DBA*;D04A%-OE&=G4MG


^,H

4#$

M-<=<

f_1ygEiSjlmlnWjohb.4#G
DB#G
1",4#=1PO/$%$"$U"kN/&"O/#V

gB

h4#$";oEL?lDB$%1:&(=S"8R/4AF4#15;"
&r"8<



KR-<OR4#$%4#8,bfi4H HAbfi4#&

bF1!/4#$oEL?l4#$8<=1/$4#N/Nw6B-$%bYX&=

eB

D9#"1/1V%R4A$V$%A8<4M15J=_"#S"8<4#=1/$V/","8-/4#

HE1L#"fiFNE"$%F$b,oEL?l$_

6B-$JO-4#$4#8S1B$O$J"
G'4#1/4#G;"M4A*'8<R4A%-4A=1T&=JL-O-1NTDBM"1"$J&=#A=)k$H

q JHKML
"> <
vWvxu2fuW3T!"u0tC Z~Y::F
. <=<
o>
K i  g
<
q6
vWvxuguF
6D@ff 6D@Bff A
6
GC
B
Cfi

,B

C
|6
F E C =B
]

e-|<{A| :-

.RRr%| Fz{7|,}0c|k

Jk

k

%K|fi-



rFR}/

k q- ?

~<Q~(<,{ fi|:<{#

W~(}:

~{({-}B|,~}|<T|r~,}|;fi<

,}9

nVWOB$V"$$O/G;=D/DX=$4#%bfi4H HAb=1"8<4A=1

/",gEiSjlmlnWjop691/N/$Hh/"
ok=)Lb/w"8<4A=1

"1/N

!B"$PYX1$A8<%N",#"*

[ff / O: lOlOlT:ffs ( \
 :DC
F E C
B
GC 

K=c88O/-$)k4#8<4#1LDB#"1
L&(=k$%=G
P#"*-$

')k4A

0%=0"8R/4A$%=G
&"8<

/H

",y#"*

^,H

j$/?"#5=-4A/G4#$KO/$4#1/5P?o?P?l$_69-$%$%-",%5*b//4#$K4#G;DB#4A$B",./E4#$1/=oEL?l&=
&r"8<

8<=1:"4#1N4#10"8<4A=1#"*

$%A8<%N!&(=k"8-/4AF4#15

S=)k4#$%bXyo?LEl

H

f_1!8<=1:%-"N/4#8<4#=1>%='B4#$b/"8<4A=10#"*

YX8"O/$%L"8<4A=1

76 

1=?"8<4A=1

).=O/#N0/"
YX1

?B

wN=:$k4#1/NN!8<=1"4#1"'oEL?l2&(=k&"8<

y"A"N*w",D/DX",-$J4#1"8<4A=1!#"*

q



H.jk$

!5$k"N/NNYc*

6

H.IJB4#$4#$

:b/4#J",D/DX",-$4#1

fi 9>~20$7`$22

&r"8<k#"*

 GHIC
^

?B
 M L r

HIJ/&(=-bB"'oEL?l&(=

O-1Vb)S4##9YXL4#1/$%%N4M1%=;"8R!"8<4A=1#"*



$##!2F

) JHKC

4#$S4M1/$%%N>4M1>"8<4A=1!M"*

^

^,H

/b9"1/NVbB4#1



#%$'&($'&ON>6'47476QP(RTS;:(UVXWYRT8%60:Z6QP
4A'",YX=",5O/G;1",4A=1Vb4A&.)8"1"8-B4A"&r"8<LYc*O/$4#15">oEL?lb).'$=OB#NN=

\[

/",HIJhxcO$%4#=14#$bB)S/4M8-!"8R/4AE$=OB#N).8R=:=$h)S/11=wo?P?l4#$S"fi"4#M",YBA 3f?4#$
8<"4#1BA*"5=c=FN34#N"w%=!$%#8<P"13"8-B4AP)S/=$%yD/8<=1/N/4#4A=1/$?$%Gq%=!Y_"$%*FFH'\-=G
y5-",DB0YBOB4##N/4#1/5
DB/"$%bX).y8"1=YB"4#1"w$-4#G
DBAG
"$O-h&=?
N/47@8O/#*=&"1"8<4A=1V$
D/8<=1BN/4A4A=1/$K"$S&(=##=)S$H

96 fi8     Z   F
 +0

N/47@8O/#*9] e

4#$JG;GPYX=&UE&r"8<k#"*k",J4#G;L$%%D

D/-



]:e

IJ/PN/47@8OBA*w=&U"8-"8<4A=108"1>YXP$%k)S1>4Ak4#$K6B-$S4#1/$%-%N>4#1:%='L5R",DBHKO/-4#15
DB#"1<F%-"8<4A=1b&"84#1/50"0&r"8<
&(=y)S/4M8-C1=o?LEl4#$h"fi"4M#",YBAb).T1$4#G
D9A*$%A8<
"1
"8-B4Ac4M15P"8<4A=1)S4A;G'4#1/4#G;"FN/4A@8O/A*HUIS/4#$OR4#$%4#8K).=F$).#B4#1'$4AO/",4A=1B$)SS
",P$-"V)K"*F$J%='"8R/4AL=1/L&"8<b9YBO$%=G
P)."*c$k1N>A$$J<=S/"1=-$H

#%$'&($^]`_JPZ:Z6Qa9bdc(We:Kfg6'b9WYhT8%6QijhY:(6aTb
j$$O/G
g?iSjlmlnjko

@ff

/"$0$%%ANQ&(="D9",-"#AL$




=&;"8-/4#-$",0"4#G;$%%D



4H HAbW"8R/4AF4#15!"8<4#=1/$hB"'YX13$A8<%N&=h"M5="#$P",h4#G
'$%D

^,Hjk$P)",'=1/#*

4#1:%$%%N4#1$%xcO14M"$%=#O4#=1A15b)'$4##U/"w">8-/=4#8<'=13/=)%=#4#1",-4#/'"8

76Dkff

4A=1/$H?c=G
y#4#1/",-4A",4A=1/$k8"1A"N!%=T$=-%D9#"1/$JB"1=/-$HSf&"10"8<4A=1

6o)Dlff



"yD/8<=1BN/4A4A=1

>=&"1=/S"8<4A=1



b1T).EN=
1=J1NT%=;4#1B8#O/N

."N/N/$

4M1/L1)v$%

,6

=&&r"8<$E%=YX
"8-/4#N=1y4#G

$%%D3",R#4AbX54A1/",E).
$%%R4#8<k=O/-$%A$?%=<F8O%

=6 

YX&(=-

HISSxcO$%4A=1w1=)4#$bc=)N=h)k691/N"P#4#1/",-4A",4A=1'=&k"8<4A=1B$B",.G;4M1/4#G;4A$

[



=OS1/)[&"8<k$% IJL8<=$D=1BN/4#15yN84#$4#=1TD/=YBAG4M$

B8<=G
DB#%H

  !"udmh gnZopqsrtsrvuMo1pwnxt7p
yzr{|pw}%r(o1pwn
<
A~
B~
B0fi~  : :OlOlOs l":" ~
[B ( ;:OlOlOl":FB s ( m ~ w\
1~[
snR( o1pqsrtsrFu%opwnRx
 t7pfi y7r{pw}MrZopwnR e 
Tx   u2v  }/

p^{y(u%oyM
vWvxuguF
nZ opqsrtt7p
 yzr{r{|{MrMB By%qy) o
_8 ]: 
J~
uB}fifi   : :OlOlOlo:"  
B U E B Q
U :5 DO






B
Q

B
B

H

~
[

U
 + 0

g~
 8 +   0  i8 +   0
D 
D 
 
U 5:  
P 
 
D
l




g?4#1'"E$%

p:<}9<,{{A

p=&-#"ficNwFIJiJf_l.h"8<4A=1B$"1/N;"LD=$-4A4A4#1:%5

&rO/1/8<4A=1

^ -

<F8O4#15;/L$%x:O/1/8<

~(}chz9%<{#<

CHUf$/S"E=1<%=,=1/

>$OB8-/",PT1cO/GPYXL=&kO/1/$",4#$%6BND/8<=1BN/4A4A=1/$L)S1

]_^e

]

e K4M$J",SG
=$%

y-~,~(}c

[~(

W%-,kz9{#

GhYR$/4AD4#$y=Ycc4A=OB$Hm",-N/1$-$
4#$D/=1Y:*%-"1/$%&=-G;",4A=1&=G
q]r1Qs

t]

?e?"1BN0"TDX=$4#4A4M1%5

4A=1

^ -

] ] Fe

]

FB4#A="8-Vbh^`d,aeRHg?4A1v"N/4A-8<%N5R",DB

Cb
xcO$%4A=14#$bVN=:$?/y<F4#$$E"=1/<%=,=1'&rO/1/8

$O/8R/",

]

e

] Feh)S1/]

e

+"1BN$O/8-/",

e%e

I="!54A1N/4#8<%N5R",DBb).TN/<691"0$%y=&S"8<4A=1B$"$y&=#A=)S$H0\=
"8-1=FN

E5-",D9b)hN<691L"1>"8<4#=1!4#1T=OJ$

H\=$4#G
DBM4#84A*
=&UD/-$%1",4#=1b)h4#N1:4A&(*

"8<4A=1/$S)S4AT4#J8<=$%DX=1/N/4M15
1=cN/$HIW='Y54M1T)S4AbF).P$%JD/-]
;HTIJ1VbW&=P"8-CN5]

e

4#1

hb).8<",%T1)A=54#8"&r"8<$


H

q

e

v"N/NV]

e

y&=S"#

"1/N

&=

b

fi

2fffff$i

	

$4#15/$%;1)A=54M8"&"8<$b).;1=)"N O/$%E"#D/-8<=1/N/4A4A=1"1/N"N/N3#4#$$k%=!<cD/-$$
;8<=1/$%-"4#1:B",L4#$?54A1Yc*yN50]

U :5

U

FeRH'F"*0"8<4#=1

k

4#$?=-N/NYX&(=
"8<4A=1

fi

">#4M1",-4A",4A=1VHhw1N%=!$4MGhO/M",%
;N/47X1/8<
YX).13/;DX=$4A4A=1/$=&

0"1BN

!4#1

U

H;I=

N=/4#$b)>N<61=O/;"8<4A=1/$
4#1"0)."*C$-O/8-/",
/wY94A55
/4#$N/47X1/8<4#$bG
=
O/1/$-",4#$_6BND/8<=1/NB4A4A=1/$E",E)S/1<F8O4#1/5/?M4#1",-4A",4#=1H\4#-$%bF)_DBOB1/4#$/h"#

=

 `  + 0

"8<4A=1/$S/",S",E=RNNYX&=
D/-]

 efi8
e

DB]

     0

Bb/Yc*w54Ac4M15yG"1>OB1/$",4#$_69NTD/8<=1/N/4#4A=1H

e

4A'B4#$N<61/4A4A=1b"8<4#=1/$

+

D/8<=1BN/4A4A=1



Q efi8 Q `T  +   0   D 
1  


aB Q

 D@G:

&=

"NBNV] Fe

=RNNY&=

v"N/NV] Fe

"1/N

;4A$%#&

)k4##/"k?O/1/$-",4#$_6BN

b/)S/4MAE=$%h=-NN>",&%)S4##X5k/4#$JDB8<=1/N/4A4#=1"N/NN>Yc*

P1cO/GhYX=&O/1B$",4#$_6BNTDB8<=1/N/4A4#=1/$.)L5kP4#$K<"8<A*

] FeRH

c8<=1BN/A*bc)_54AE")K",-N/h%="8R"8<4A=1w/",4#$=-NN3-<%

U

Yc*wA%4M15
=$%P"8<4#=1/$S"N/N";D/8<=1/N/4#4A=1T=&
"NBNW]

 fi8
e

["N/NV]


D G:
 c` P  + 0  @
7  B U 
e

&=

IJ/",)."*bF))S4##B"<F"8<A*

e

HE$4#G;DBA*
N=/4#$

b/)S/4M8-T).=O/#NT=)k4#$%E5='O/1B$",4#$_6BNVH

DB]

] ]

U

BHIJcO/$b

U fi8
e

D/]

U `E P  +   0   Dl
e


P  + 0
&B

^eOB1/$",4#$_69N;D/8<=1BN/4A4A=1/$b1/"G
A*y

&r"8<$J&=S"#V"8<4A=1B$<F8<DBJ=$%LB",S",L=-N/NYX&(=-

U

H

FOBG;G;4#15>O/Dw1:O/GhYXL=&JO/1/$-",4#$_6BNDB8<=1/N/4A4#=1/$E).5&(="!#4M1",-4A",4A=1
",-4#L",



  
+ 0

B Q    B U 
] ] Fe

] ]

e

B Q  B U     m  
+  0 
1~EWfi8 ~     m 
 U :5 eD
U





+

0

+

0
+ :OlOlOlT:;+

]8



^e%e

] ] Fe

PcO/$JN/<691?=OS1)QD=$-4A4AE4#1:%5
\U4#1/"MA*bF)LG;",h$O?/",S"8<4#=1/$

]

D/] e

"N/NW]

jA%=5bB"
M4#1",-4A",4#=1

B

]

B

4#1:%=

&(=]

Fe

PHPN=

B$D/8<=1/NB4A4A=1FK"1/N

U

$



U c`|o+ ( + 0 O: lOlOlT:;+ fi+ #0 ( 
~ 

U efi8
e

^e

^eRH

fi#(

(

Q `|o+ (+   0 O: lOlOlT:;+ fi+  # 0 ( ff :

Q fi8

D/] e

e%e

5S=-NNTYX&=L"8<4A=1/$

/4#$kYc*!4#1/$%-4#15'1)uA=54#8"%$",&*F&r"8<$
"N/N>M4#$%H

]

b).

"N/NV]

e

=&=OS"8<4A=1B$JA"N/$K%='",JG;=$%

O/1B$",4#$_6BNwD/-8<=1/N/4A4A=1B$.4#&

"1/NP=1/A*E4A& 
$",4#$%6B$VxcO/4AG
1:$V&=U"SN/4A-8<%NL=D/4#G'"#4#1/",",-"1/5G
1HYcc4#=O/$A*b

~ 

P"8<4#=1>$%S"1/N



8"1>YXL8<=G
D9O%N>4#1TDX=A*F1=G;4#"4#G;H

?O/$=ADBODX=$%)S4Au#4#1",R4A4#152"1"8<4#=1$%4#1u"8<-"4#1u=-N34#$0%=v"8-B4A2"
$G;"MA;1:OBGPYX
=&EO/1/$",4M$_6BND/8<=1BN/4A4A=1/$b)S/4M8-b.4#1O-1b.G;4A5:'A"N%="3$=%<
#"fiFNT$%=#O4A=1VH -

IS:O/$b:).E",E8<"4#1/A*1=K)S4###4M15L%=yDB"*?D/-4#8</",K691BN/4#15P"1T=DB4#G;"

#4#1/",-4A",4A=1
=&Wk"8<4A=1B$.4M$#4A#*%=y8<=$%b"88<=-N/4M15P%=yIJ=G

cHIJ-k",?"P&)G
F

=FN/$k/=)=18"1",D/DB=4#G;",%y$O/8R"w#4#1",R4A",4A=1bBM4Ah4#1:%=FN/O/84#1/5;"1=RN-4#158<=1/$%%-"4#1:

6 E 6

,6

&(="8R"8<4A=1

6 

kB","N/N/$"?D/-8<=1/N/4A4A=1y=&X"1="8<4A=1

b"1/N
%*F4#15?%=L#4M1",-4A

P"8<4#=1/$S$OB8->B",SG;"1:*w=&/$%P8<=1/$%%R"4#1$k",PG
HK?O-4#1/5
=OJ<FDR4#G
1:",4A=1/$b).
&=O/1/NT/",JD9",-"#AX"8<4A=1/$k"N/N/4#1/5"8R>=$JD/8<=1BN/4A4A=1/$K=F88OS$=
-",A*T4#1T=OJ%$4#15
"$%F$J/",S1!",D/D/-=4#G;",4#15;4M$1=J).=>E<X=HPcO/$J$4#G;DBA*'#4M1",-4AE"#V"8<4#=1/$
4#1TP=-NJ*w5k$A8<%NbB8"O/$-4#15
"#G
=$%k1=;8<=G
DBO",4#=1/"=R"N",k"#H

q!"(oAoFZweoZZmr"](TZMFFZ(M"wFZwmZ(omr;MZA9ZMTO;FFOZcZ"
Z"Z\ ZT wg;"Z(gZG("ZMF; o1r (o; .Goc;g(TA"G""Z9Z(
q



fi 9>~20$7`$22

t&5  r .}&2



$##!2F

r ou

 }

  

;/";4#G
DBAG;1%N=O?=)S1R$4A=10=&Kg?ikjklmklnWjoPb/4#5/A*T=D/4#G;4#N&=E$%=AF4#15w<
#"fiFN3DBM"1/1/4#15w"$%F$HTfP<cDB#=4A$L'&"8<h/",P'DB#"1/1/4M15T5-",DB3=&"!#"fiFN"$%3N=c$
1=S8<=1:"4#1"1*w<F8MO/$4A=1-#",4A=1/$L]rl=DX=$4A4A=1^eRH?OJ4#G
D9AG
1:",4A=14#$J"#$=;/4A5/A*=D
4#G;4#N!&=k-D",%NBA*!$%=AF4#15'DB#"1/1/4M15
"$%F$?)S/4#8R!"#$/",y$"G
y$%?=&"8<4A=1/$

M{   8 Z~  :;+:F

"$%F$

]

eK"$kN$8<-4AYXNT",JPY54M1/1/4#15=&/4#$$%8<4A=1VH





l#"1/1/4#1/5"$>$%DX847698",4A=1/$SO/$O/"##*8<=1:"4#1$=G
P=DX-",%=$8-G'","FbX"1BN>"'$%k=&8<=1F
$%"1:$Hf1/$"14#",4M15/y$-8-G;",">)S4A0
8<=1/$%"1:$E*F4A#N/$k;"8<4A=1/$?%=/"$9H
?O
$%*F$%%G4#1B$%"14M",%$"#F=DX-",%=$8RG;","h4#1y"E)."*
$O/8-yB","#b"1/N;=1/A*b"8R/",YBAS"8<4#=1/$
",EYBOB4#AHiJ"8R/",YB4##4#*w=&"1>"8<4#=1>LG
"1B$/",b/)S/1$O/88<$$4#A*w",DBDBA*F4#15=DR",%=-$
%=
E4#1/4#4#"9$",%bB"#=&WL"8<4#=1$KD/8<=1/NB4A4A=1/$.",D/DX",1OB"#A*HE1TY9O/4##N')S/",
).?8"MB?8<=1/18<4#c4A*'5-",D9HIJ/4#$5-",DBT8<=1/$4#$$=&).=yM"*-$bF=1E8<=1:"4#1/4#1/5"#]"8-
",YBAek"8<4A=1B$bW"1/N0
=L"MK]-"8-/",YB#ek&"8<$Hy\=Gq"8-3"8<4A=1VbV;",yDX=4#1:%-$%=
"#.D/8<=1BN/4A4A=1/$b"N/NC<8<$"1/NNA%<X8<$HCj#.=&k\U\.$;8<=G
D9O",4A=1/$
",T<@84#1A*
4#G
D9AG
1:%NO/$-4#15PB4#$5R",DBT$%%-O/8<O-HU\/=E$OYB$%xcO1:A*;N$8<R4AYXN'4#G
D9AG
1:",4A=1'=&
#"fiFNg?ikjklmklnWjoPb).L=1/#*1N/L4#1&=-G;",4A=1",YX=OKD/8<=1BN/4A4A=1/$K"1/N!"N/NT<8<$H
j$K"PM"ficNDB#"1/1B4#15?5-",DBwN=c$.1/=8<=1:"4#1T"1*;<8#O/$-4A=1;M",4A=1/$bcS=1BA*;4#1&=-G;"fi
4A=1=1/P1N/$S%=;D/$1S4#J",P)SB",)8"#;{A|,<h'<'--R~z9RbX4H HAb/&=J"8-&"8<k=
"8<4A=1b:S1cO/GhY=&/6B-$%M"*",)S/4#8R;4A",D/DX",-$4M1
J5-",D9H"#AN
=1;"14#1:%-G
<

{   8 Z~  :;+:F

B
M6


N/4#",%"$

]

eRbc=O-$-4A=1
=&Wg?iSjlmlnjko8<=G
D9O%$$S#"*.G
GhYR$/4ADB$

Yc*O/$-4#15?J&=#A=)S4#156/FDX=4#18<=G
D9O",4A=1HIJJM"*G;GPYX-$B4ADB$=&"#F&"8<$."1/N;"8<4#=1/$
",4#1/4A4#"M4AN%=

H\=."8-w"8<4A=1bcS4M$"#$%=h"P8<=O/1:%bc)S/4#8R;4#$4#1B4A4#"#4AN%=FHIJ1b

MBEDE+

&r"8<J#"*Sy4#$.YBOB4#A4MG
DB#4#84#A*hYc*$%4#15yE#"*kG
GPYX-$-/4ADw=&"#&r"8<$
4#G
)S1"w&r"8<
1cO/GPYX=&



J6

4M$.691/4#$/Nb"#V"8<4A=1/$k$8-/N/O/ANT&=k$%%D

W



/"P4#S#"*G
GhYR$/4AD

b/"1/NT4A"N/N/$b/4A&W1=J"A-"N*'D/-$%1b/",-EDBOK%=;L#4M$%=&U$8RN/O/ANw&r"8<$&=J



1<FK&"8<#"*S",K4#G
?$%%D
$%%D

4#$?"wD/8<=1BN/4A4A=1

"8-$;T%="

c$hDB8<=1/N/4A4#=1/$b !4#$hDBOh%="0#4#$L=&S$8RN/O/ANC"8<4A=1/$&=h/w8O-1y#"*H

jk&(%"
&"8<#"*
$%%=

B
6

35$L4#$M"*LG
GhYX-$/4AD0$%bW"#U"8<4A=1B$k=&)S/4#8R

5
4A
8<=O/1:%'4#1B8<G
1:%NHj$
$%=:=1"$y>8<=OB1%
&=;"1"8<4A=1

%=;FH"8R

^,Hm"F4#15691/4#$-N;)S4#"8<4A=1M"*



b/"#$8RN/O/ANw&r"8<$",

^E/"L4AG
GhYX-$/4ADT$%bB"1BNw$=
=1HIJ?D/=F8<$$8<=1:4#1cO$JO/1:4#X"#5="#$J/"

"0#"*;G
GhYX-$/4#DA=)y/"1



H0fy$=OB#N3YXT1=4#8<Nw/",/4#$hc4A)=&SD9#"1/1/4#1/5

5-",DB!YBO/4#MN/4#158<=$%DX=1/N/$S8A=$%A*w%='h8<=G
DBO",4A=1!=&L).4A5:S,"#O$J4#1!m?FlHBIS=$%
8"13YX'8<=G;DBO%NYc*",D/DBA*F4#15T'"8<4#=1/$h4M1#"*-$"$P",YX=bUO/DXNB",4#15T)4#5L,"#O$h"1/N
D/=D9",5",4#15ES8R/"15$"8-4#G
J"1;"8<4A=18<=G
$4M1b"1/N;$%%=DBDB4#15?)S1'1=L8-/"1/5$=F88O
4#1
"LM"*Hm"F4#15?691B4#$NJ-#"ficN;-$4A=1
=&DB#"1B1/4#155-",DB
Y9O/4##N/4M15bfi"P$4#G;4#M",-A*E%-4AF4#"
-$4#=1=&gEiSjlmlnWjoh$S$%=#O/4A=1T<c%R"8<4A=1!G
8R/"1/4#$-Gt4#$4#1:=NHKFL\4#5OPcH
f_1/$%%"N=&DBO%4M15'"#W5="#$?4#1%=P%=D#"*E4#10g?ikjklmklnWjo$%*FAb"1/N!1!DB=DB"fi
5",4#15E/GN=)k1yY:*O/$4#15?oEL?l$%6B-$%b"8R;5="
",



$K69-$%#"*

"8-0#"*







i

4#$U$4#G
DB#*EDBO4#1:%=L"?5="/$%

HIJ1bF-?4M$"h&=%1<FJA=c=DN=)S1T&=G

b9"1!"8-B4Ac4M15'"8<4A=1!)S4A>#"*kG
GhYX-$/4#D



XA=F8",%N

?%=D>%=
/?4M1/4A4#"#"*HjK
^E5$$%A8<%N&=J"8-&"8<4#1

w8<=$%DX=1/N/4M15T5="$%HTf&K'4#$PG
='B"13=1w$O/8-"8-/4#b"!YX$%L=1w4#$EDB4#8-N
"88<=-N/4M15%=>N/47@8OBA*/O-4#$%4M8,HIJwD/8<=1/NB4A4A=1/$y",DBO/4#1:%=4A
8<=-$%DX=1/N/4#15
5="$$H"8-4#G
;"1"8<4#=14#$L$A8<%NbU"#U=&4A$L"N/N/$P",'G;",N%-O'",P4#G
$

r

^,HIJ?G;",J",K4#G;





"1/N

D/-1$"8R/4A-$K%=yYX$A8<%NT&(=K&r"8<$K/",",E"A"N*'%-O

q;

fi

2fffff$i

1F ;D 
 xGl 19)
  T@
 1 F 
 
  J     <19 $ 6   
 9!fQ?1s)
 <1 G\ '  Yi961 ( ff <wW?Y9
Q e  '  r 1i961" B  TW<69 2 1l e 
 
 .9 'i961   .1'l9)
 z7   
Q   Qe  s '   
1J  $ 19 ( `
  
Q






    



fi

\U4A5O-E iJ#"fiFN>DBM"1T<c%-"8<4#=1

]

"1:*:)K"*H>0",F4#15!",h4#G


fi

^'"$$O/G
$P/",h"8<4#=1/$h",-#4#1/",-4AN4#1/;=-Nh*5

$%A8<%N JjD/8<=1/NB4A4A=1>B",k)K"$?"8-/4AN0Yc*>"1"8<4A=1""N4M$k1=?8<=1/$4MNN!"$?"1)
5="H



U



  [ 




 ff

T  






F

 ff

f_1/4#$J$8<4A=1bB).P4#1:%=FN/O/8<L\U\.$JYB"$h$%",-8R!"A5=R4A/GHhN/4#$-8O/$$KP"#5=-4A/G$J=,
4#8"VD/=DX4A$K5",RN/4#15
8<=G
DB#%1$$bB"1BN>N-4AL\U\.$J=-"#$%",-8-$%%-",%5*H
f_1!69-$%Tm?Fl

R$4A=1]rZ.=1!s

g<X1/bh^``eRbEmEFlS^0"$T)."$OB$%N4#12/0jkflK:

^``8<=G
DX4A4A=1b$",-8-C$%%-",%5*4M$P">,",-4#",4#=13=&J/4#M78#4#GhYB4#15b"A)K"*F$$%A8<4#15=1
YX$%L$O/88<$-$%=L%=>;$",%'4AL4#$E8O-1#*!&r"84#15H
Z.8"O/$%'$%",%fi"MO/",4A=1/$E",;8<=$A*bW).
"#$%=>8R=$%
%=>O/$;A=c8"U$%",R8-b4#10;=DXy%="8-5="$%",%$E)S4#"$?&(),"#O/",4#=1/$E"$
DX=$$4AY9AHh$%%AN&=P">N/4A1:L$%",-8R3"A5=-4#/GbW"1[_1/&(=-8<NB&(=RG=&.B4##78#4MGPYB4M15b
)S/4M8-v8<=GhYB4#1$A=F8"E"1/N[$*c$%%G'",4#8$%",-8RHIJ$%%R",%5*[4#$G
=4A,",%NvYc*3$-4#G
DBA
$%%-OB8<OE/",J/L$%",-8-$%DB"8<$S=&=O/J%$%4#15
YX1/8R/G;",F$K%1/N%=w/"H

)#` u2vo5  MyE ' 5  '}   #w
=4#15DBM"1/1/4#15Y:*0O-4M$%4#8h&(=)K",-N$%",R8-b;$%",-8R3$%DB"8<;4#$?/;$%DB"8<
=&K"#U"8-B",YBA
$%",%$b:%=5)k4A4AO-4M$%4#8fi"#OB",4A=1HUoS=)Lb,"#O/",4#1/5$%",%$4#1=O%$%4#1/5YX1/8RF
G;",F$E)S4#;OR4#$%4#8yN<691/NY:*xcO/",4A=1cb=1
=&%1691/N/$/",h;$OBA4#15T$%",-8R
$%DB"8<$",S$4MG
DBA4#1
$%%RO/8<Obc$%DX847698"#A*bB",A=F8"/G;4#1/4MG;"?"1/N
DB#",%"OB$%1/N;%=PYJ$-G;"#H
\=S"1:*T$%",-8-$%",%b/L1/<cJ$%",%P)S4AT$%%-4#8<#*'YX%%SOR4#$%4#8Sfi"#OB",4A=1>4#$KO/$OB"#A*;=1/#*
"w&()$%%D9$E")K"*]r"10<F"G
D9Ay&(=?/4#$?4#$yMr-EN=G;"4M1N$-8<-4AYXN4M1c8<4#=13cH#^,H#^eRH
?O?4#N"w4#$k%=TD-&(=-G<FB"O/$%4Ay$%",-8R&(=?YX%%L$%",%$HLIJ"A5=-4ABG4M$k$/=)S104#1
\U4A5OPcH
n4A/4M#78#4#GhYB4#1/5b"A5=R4A/G

N/DB4#8<%N24M1\U4A5O3$%",$=Ow4#104#1/4A4#"$",%H

+

IJ1VbV&"84M15>"14#1:%-G
NB4#",%;$%",-8R$%",%

.b">8<=G;DBA%;Y/-"N6B-$L$%",-8-$%",4#1/5>=O

q



fi 9>~20$7`$22



$##!2F



 
=("'G  17
6    69 ff1X
f196
q7f
6 % <1  %     


  11
v  61P 91 B7> Z
 Y  1 P1Y1`11 % 1`91P619.
   

 =("'
6 B6P?16x1l1`9q.


fi

\U4A5O-P IJE1&=-8<N>B4##78#4MGPYB4M15P"A5=-4#/GH

&=G

+

a+J

Q4#$4#1:=NH0IS/4#$?691BN/$LT8A=$%$hYX%%y$O/88<$$%=b4H HAb/w1",-$%$",%

)S4A

$%%-4M8<A*LYX%%,"#O/",4A=1b=&r"4##$Hf_1J#",%%8"$%b:)S=AK"A5=-4#/Gu&"4M#$b4#1K&(=RG


+ |+

8"$%byDB",&(-=G
5="W$",%



%=

U4#$?"N/NN%=
8O-1EDBM"1b"1/N$%",-8R34#$?4A%-",%NH13"

"'$%",%h)S4AT,"#O/",4A=1!



4#$K"8RNb9$",-8-$%%=DB$H

?O/U4MG
DBAG
1:",4A=1=&Y/"Nh6B-$U$",-8-;$",4#15?=OU&=G

+

4#$U$%"1/NB",-Nb)S-K$",%$

a+

",;DBh4#1"x:OO/HT?1;$",-8-C4A%-",4A=13G
=$h
69-$%L$%",%

U&(-=G'xcOObU"1/N

+

,"#O/",%$4ALYc*0RO/1/1/4#1/5g?ikjklmklnWjoPHf&Kfi"MO/",4A=134#$PYX%%P/"1/",P=&

]+J

$O/88<NB$H.-)S4#$%bFP$-O/88<$$%=-$J=&

.bU$%",-8R

",PDBOK%=;L1BN=&/Px:O/OHiJDX",%N!$",%$

",;"=4#NN3Yc*!D94#15>">/"$-",YBA
=&.c4#$-4A%N$%",%$P4#1G;G
=*H
f&1=>1)$",%$P8"13YX
"8RN!"1:*cG;=b/Y/"N/T6B-$%J$",-8-!&"4#M$H

) /Yu}2



  

3T3

f&K4M1=14A%R",4A=1Y/"N/6B-$L$%",-8-&=P">YX%%h$",%&r"4##$bW131&=-8<N/4##78M4#GPY94#15
$%%=DB$S)S4A=O/691/N/4M15
"'$%=#O/4A=1HKIJ/4#$8"10/",D/DX1YX8"O/$%h=1/8<P1/&(=-8<N/4##A8#4#GhYB4#15/"$
8-/=$%1%=!4#1B8#O/Ny"1"8<4A=14M1/
DB#"1bV4AL1/L",$L/4M$EN84#$4A=10YB"8-9H;IS;G
=FN4#$
&=L=1BA*8<=G
D9A%L=1"$%F$J)kL1=
&r","#A*w)J=15;N/84#$4A=1/$8"1!YXLG;"NHIJ$P",
L"$c$B",SN='1=J8<=1:"4#1%N"N>1/NB$H

 !"u y%  $d# *  Z~::F

mj
+ 8 PR"Q SqUrVYX =:m
-F )fi&d PRQTSTUWVYX Z+:m .L]

-

,+

e-|wz9{7|}/}/~}:|,- |

.k

1/N~ ~rS~(;%-|<F|<{#|,}90}> /}9RT%|r~,}|<~ 'Fy:|,{B%,
]

heL|,}

]

Me

~(T-|{({#3|>N"N

Y> - q-> E fi

~r k~ r

J~

o",O-"##*bB"
"$%4#$J8"MAN>N"NF1BN&(-P4A&4#SN=c$J1=S8<=1:"4#1!"1:*N"N>1BN>$%",%$H.
G;",-L/",UYX4#15kN/"NF1/N&(-K4MG
DB#4A$$%=A,",YB4##4#*b,"$U=)S4#$K4#1/4#4#"$%",%J4#$%A&).=O/#N
"A"N/*YXL"
N"N!1/NH

vWvxu2fuW3T!"uf} { 8 Z~::F
r<
vWvxuguF
,+18 PR"Q SqUrVYX =:m 


~{({%{~('<~}:

]

-RL M{

e
-
|Pz{7|,}B}/~(}c>|fiR

~{({}9|'<,{r~r,}

(

(>

~(-|,<}9?- F}<},%R

jk$-$O/G
h1&(=R8<N0B4##78#4MGPYB4M15N/=:$?1="8-/h5="HEIJ/1!).
/"
$%=G
y4#1:%%

G
N/4M",%$%",%

]

heRb YX4#1/5;
8O1:SD9#"1b)ShY/"N6B-$k$%",-8R8"1

1=J4#G;D/=P=1L$4AOB",4A=1Hok=)Lb

 Z+ 
] e

'"$J$%",-8R!/"$J1/=S$%%=D/DXNT*Hf&W/L)K"$S"

q



fi

_+
 Z+  R8
?+

DB",&(-=G
=Y/"4#1

+J

3%=h$%=G
5="$%",%

]

e

E_ Z+

t

$=)S4#15y/",

2fffff$i

(bc1w8<=G
DBA%kY/"N;6B-$%$",-8-w).=O/#Ny691BN
/",.DB",b

] eRb"1/N3%RG;4#1/",%D=$-4A4AA*H>FO/8R">D9",8"1&=1/=P<4#$%b



4#$";N/"N1/N>$%",%4#18<=1:%-"N/4#8<4A=1>%=;P"$$-O/G
D/4A=1VH



G;",0/",Pl=DX=$4A4A=10!=MN/$?=1/A*0)S1

 Z+ ]8

1cO/GPYX-$W4#1/8#O/NB4#15Fb,)S

,&df+

] e 47

4#$E"&rO/1/8<4A=1&=Gq$",%$P%=!1B",O-"

.HUIJDB=DX=$4A4A=1L4MN14A6B$V"S8M"$$W=&D9#"1/1/4#1/5

"$%F$)SL).y8"1$",&A*>",D/D9A*1/&(=-8<N0/4#M78#4#GhYB4#15H.k1/&(=OB1/",%A*b94#k4#$kl.FlXjSK/",-N
%='N84MNE)Sk"y54A1>DBM"1/1/4#15h"$%TYXA=15$%=;B",S8#"$$H

  !"udh

 i Z@Kv



k

kP

{h8 Z~:=:F A{
  @JKB

g?4#1>"
DB#"1B1/4#15"$%

x

 u2v  }

vWvxuguF

t

y-~,~(}cy

]

k

UeRb94#$

P

N

~(}cPz9_:<{#<

\[

N"N1/N&

T

~(.F

q-

_R,kz9{#

m",-N/1$$4#$
D/-=1Yc*DX=A*F1=G;4#"##*N/O/84#1/53lnWjoEFjI

A{


~
~ ( fi8 ~`|T6fi8}[:=:v; 

N84#$-4A=1TD/=YB#G=&U)S/

4#$$%=#fi",YB#

$-4#G
DBA*"N/N"1=DX-",%=%=
4#1/4#4#"X$%",%H

6
{ ( 8 Z~ ( ::F

jkD/DBA*F4#15

<

v}9y<,{{A

]rZ.*F#"1/NbE^``fi:e



/

%=
LD/-=YBAG=&N84#N/4#1/5?j?ok\ikSH

/",4M$y<c8O",Y9A!4#1"#S$%",%$b"1/N<$",YB#4#$/$


96 bA\F

"N/NV] e

7{

%=!"1:*$%",%
"8-B",YBA;4#1

fi

tA"NB$?YB"80%=;4#1/4#4#"$",% L"#&"8<$P/",L8"1

SYX8<=G
P%-OP",PG
=NVb9"1/N/=$%P4#1Th4#1/4A4#"X$%",%",P"N/N/NHok=)Lb//PG
=FN/476BN
D/=Y9AG

]

E{

eK4M$JN"NF1/N>&(L4A

{ (
M{ (

4M$$%=A,",YBAH\=G+A&%='-4A5:b/4A&

{

1/N'&(bc14A4#$$%=A,",YBAb)k/4#8-'4#G
DB#4#$U/",

4#$$%=#fi",YB#b:"$)/"?1="N/N/N'"1:*;1)

{

DX=$$4AY94##4A*;=&"8-B4#15'/P5="HS\/=GR4A5S%=wA&b94#& 4#$S$=Afi",Y9AbB1"#$%=w4#$

a

$"G
$%=#O4#=1DB#"1

4#$SN"NF

{ (
,

;HJE1L8"1!/1bB&=G"MV$%",%$?4#1

%=;h4#1/4A4#"$%",%h)S4ATP1/)[=DX-",%=b/"1/N<F8O4#1/5

bBYc*

b9"8-B4Ah/L5="WYc*5=4#15'YB"8

",&%H

GPYX-$-/4AD>4#1lKFlXjSK2&=#A=)S$K&=GtE&r"8<SB",SlnjkoEFjIQ"1BN>4A$J8<=G
D9AG
1:S",
YX=L4#1ElKFlXjSKJHjC1=1FN%RG;4#1/4#$4#8"#5=-4A/G/",N84#N$/8<=G
DBAG
1:V=&F?jokk

,+

\UiS"1BN!/",?1N/$k=1/A*DX=A*F1=G;4M"$%DB"8<y8"1YX$%DX8476BN>"$&=#A=)k$Hg?O$-$k"w$%",%



+

.H

-4A&*4M1DX=A*F1=G;4M"$%D9"8<
/",

4#$"8-B",YBA
&=G;4#1/4#4#"W$%",%H
\/O/bVR4A&(*!/",

h5="8"1/1/=SYXE"8-/N!&=G

KH.f&U/4#$J"#5=-4A/G+$O/88<N/$b4AJ&=#A=)S$J/",kP4#1B$%"1/8<

4#$K1=JN"N1/Nw&(



$4#1/8<

+

+

C8<=1/$%4#O%$"
N"NT1/NHIS/4#$.4#G
DB#4#$/",J?j?ok\ik

4#$4#1ol.ljSJbB"1/N!1/8<L4#1lKFlXjSKJH



IJ/=O5).8"11/=L<@84A1:A*N84#N
)kh">54A13"$%34M$LN"NF1/N3&(bw",



"$4##*0%$",YBA$-OF@84A1:h8<-4#%-4#">4#13#4#%-",OH

=/1/$$%=13"H],ehN<61'"!1/=4A=1

=&,'rR~KDB#"1B1/4#15L"$c$b)k/4#8-w4#$K$OF@w84A1.&=KN/"NF1/Nw&(-1$$bFYBOK8<=,ol8<=G
DB#%H
IJ*
"#$=L54AS"PD=#*c1=G'4#"F$OF@84A1:8<-4#%-4A=1
&=$%*FG;G
%-*HUIS/4#$4#$bc=).bc*
%-4AF4#"H
m",-N/A*?"1*E=&/8O-1VYX1/8R/G;",F$&rO/769M#$X4AHE=c/A"1/NPmk=,G;"1/1],"e/".N/<691N

k

1=4A=1B$=&~(} ,<<r~<{#DB#"1/1/4M15L"$%F$



k

$O@84A1:.&=N"NF1/Nw&1$$bF"1BN~} <Rh|r~,}



$OF@w84A1&=?4M14#YB4##4A*b9O/1BN8<-"4#1$%%-4M8<4A=1/$HEIJ<4#$%%1/8<y=&4#1:-$%
"8<4A=1B$bV"1/N
$OF@w84A1k8<-4A%-4#";&(=h"NBN/4A4A=1/"V$%%-4#8<4#=1/$bB8"10YXPN84#N/N!4#1DX=A*F1=G;4M"X4#G
HJ0"1*
YX1/8-BG;",P"$c$N/=b4#1&"8<b:&OB769#=$%S8<-4#%-4#"?"1/N
8"1
cO/$U<@84A1:A*hYKD/-=1;N"N1/N
&H

qT

fi 9>~20$7`$22



$##!2F

?1/k8<=O/#N"N=D/.E=:BA"1/NwmS=,G;"1/1V$G
=FN=A=5*b"1/NO/$%kS<4#$%%1/8<k=&V4#1:-$%
"8<4A=1/$K%=
-8<=51/4ALN"NF1BNT&(?"$%F$Hf&W/%$&"4#M$b:=1/L8<=O/#Nw1TG
DB#=*"yN/4A1:

Wfi

$%",-8R!$%%R",%5*/"11&=-8<N>/4M#78#4#GhYB4#1/5HVh/"P)=;-"$%=1/$J&=L}95=4#15
/4M$)."*



1y"G
=15$%=OWYX1/8R/G;",F$bfiK",."$%c$/",UN=k1/=8<=1:"4#14#1R$%"8<4A=1B$bYBO
",-P1#$$SN"N1/N&Hj1<"G
DBAL4#$JwUA<fiU-
N/=G;"4#1b)S/E1&=-8<N
B4##78#4MGPYB4M15PA"N/$J%=;<F8<MA1S$O/A$H



1&=-8<N/4M#78#4#GhYB4#1/5w8"1=&(%1CxcO/4A%'$OB88<$$%&rO/#A*$%=A"$%c$h/",N=8<=1:"4#1CN"N
1BN/$b"$K4A.N/=:$1=K18<$$-",-4#A*;58"O5:J4#1=1HF"G
D9A$&=K/",J",?8<=1"4M1N4#1
/P/R_!"1/NTX-MN=G;"4#1B$bF)S/4#8RT)L)k4##XA=c=w",S4M1!c8<4A=1!cHcH#^,H

IJS=YB$%,",4A=1
/",&(=RG;$SYB"$4#$&==O/)K"*
=&N"#4#1/5E)S4A;8<=G
D9A%1$$4#$J&=#A=)
4#15Hf&B1&=-8<N;/4M#78#4#GhYB4#1/5J8"1;1=$%=AJ"?D9#"1/1/4#1/5S"$%b:4AO/$-O/"#A*E&r"4##$*yx:OB4#8FA*H?1
8"13/1$4#G;DBA*$)S4A8-3%=">N/4A1:P$%",R8-"#5=-4A/GH/"'<FDX-4#G
1:%N)S4A3-"1F
N=G;4#4#15;1&=-8<N0/4##A8#4#GhYB4#15b"1/N0N=4#15w";$",k)k1!=1/",%%G
DBk&r"4#ANHkIJ/4#$kN/4#N/1 
A"N%=;8<=1:F4#1/84#15y$-O/A$HIJ/=O5T)E%R4AN";#",5E,",-4A*=&-"1/N/=G;4A",4A=1$%%R",%54A$b/).
N/4#N!1=J691BN!";D9#"1/1/4#1/5"$!4#1=O%$%4#1/5N=G;"4M1/$)S-P=1h-"1/N=G;4#N>$%",-kN/4#N!$4A5,
1/4768"1A*YX%%S/"1!LD/F4A=O/$K=1/bB4H HAb9"M",%%G
D/$$OFXN&=GP$-"G
LD/=Y9AG;$H
IJw"$%F$y/",y1&=-8<N/4##A8#4#GhYB4#15N=c$y1=$=AT-4A5:")."*",-T",DBDB",1:A*$%=&rO/#=&
N"N1BN/$/",J=1/P8"1!1=k"=4#N!=$%PN"N!1/N/$",k-"1/N=GH/"h&=P",R"15N

Ofi

=OJ=-"#$%",-8-$%%-",%5*4#1\U\"$J&=#A=)S$

^,HS=;1/&(=-8<N/4##A8#4#GhYB4#15POB14#P5="W4#$K"8RN>=kL"A5=-4#/G

&"4M#$H

cHJf&1&=-8<N/4#M78#4#GhYB4#15y&r"4#ANbX$%F4AD0*:B4#15TN=1y$%=&r",E"1/N0%*>%=$%=Ay"$%
Yc*;"8<=G
DBA%?O-4M$%4#8$%",-8RT"A5=-4A/G>Hf_1;/8O-1.4#G
DB#G
1",4#=1b/4M$4#$)S/",
ikO/$$%X"1/Nok=F4A5!]_^``aeK%RGt-T-<rR$%",-8RHIJB4#$K$%%-",%5*$4#G
D9A*
<cD9"1/N/$
"MV$%",-8R!1=FN$KYc*w4#1B8<"$4#15y=-N/J=&U5="WN/4#$"1/8<E$%4#G;",4#=1H
I=>$O/G;G;",R4Ab\\O/$$k1/&(=-8<NB4##78#4MGPYB4M15;"$?
YB"$%
$%",R8-3G
/=cNb"1/N"T8<=G
D9A%
YX$%_6B-$h"A5=-4#/Gq%=N")S4A=$%T$%DX84#"8"$%$)S/'1&=-8<NC/4##A8#4#GhYB4#15T/"$h-O/1
4#1:%=;";N"N1BN>"1/N&r"4#ANH

Fd 2
	 D 2

 

 ff  ff

U

ff

 U

f_1/4#$
$%8<4#=1bK)!4M1%=FN/O/8<T)=CO-4#$4#8w%8-/1/4Mx:O$;/",'8"1bK4#1D/R4#1/84ADB#bYO/$N%=

fi

D/-OB1?L$%",-8R!$%DB"8<h4#1"1*T&=)K",-N>$%",%$%DB"8<P$%",R8-!"#5=-4A/G

g

^,H

<{ zF{S|r~,};$%A8<$w"3$%=&?D/=G;4#$-4#15$O/88<$$%=R$;%="$%",-8-$%",%Hjk$'))S4##

N/G
=1/$%%-",%04#1c8<4#=1cHcb!/O-4#$%4M8w4M$'8<-O/84M"&(=\\.$DX&(=RG;"1/8<>=1G;"1*
N/=G;"4#1/$H

cHk-
|{<{#r~,}w8O$=OY/R"1/8-$J)SE$%=G
P5="V/"$J",D/D9",1#*'YX1"8-/4#N
%=c=C",-A*H[I$%4#15O/-4#$%4#8,b.).&=O/1/NB",w4A8"12*F4A#N$"F4#15$'=12"$%F$/",
8<=1:"4#1!5="V=-NR4#15$b"1/N>B"$J1=
<X8<S=1"$%F$JB",SN=1 H

qo

fi

2fffff$i

Z.=T%8-B1/4#xcO$",-k=Y/"4M1NT"$."$4#N/S<X8<=&WO/$4#1/5g?iSjlmlnjkoQ"$"hO/-4#$%4#8J$%47
G;",%=U4#1?G'"1/1N/$8<-4AYXN?4#1hc8<4#=1LHUjk#$=bYX=E=&FGpN=L}9D/$%-8<=G
DBA%1/$$
=&J"1:*3*cDX=4#8"&=)K",-N$",-8-H!f18<=1:%<c=&=Oy$%",-8R"A5=R4A/Gb)w4#1:%5-",%
G$OB8-/",/*'D/-OB1k/L$%",-8-!$%DB"8<L4#1wL$4M15A1&(=R8<N>/4##A8#4#GhYB4#15?%*
4#$k1=k8<=G
D9A%h4#1!51-""1*c)K"*
4A&1&(=R8<N!/4#M78#4#GhYB4#15E&r"4#ANVH

m#
y

2  45ouc3
+
+



I=T"$",%





)S/4M8-

"1/N8<=G
DBA%#*wO/-1>G+=,N/OR4#15
YX$%_6B-$S$%",-8Rb

]Z+eS=&"8<4#=1/$?/",L$%G%=TYX
G
=$%?D/=G;4M$4#15'"G;=15

.b)
N<61"T$%

"8<4A=1/$U",D/D9#4#8",YBA4#1

KHIJ/%8R/1/4#xcO4M$NR4ANPYc*P/"c4M15"8A=$%A=c=P",U.#"fiFNPD9#"1/$

/",;g?iSjlmlnjko

<F%-"8<$=1C$%",-8-$%",%$y4#1=O%$%4#15"$%c$H0.=1/$4MNL.-/R

N=G;"4M1bW"$L4AE)K"$POB$%N4#1!^``!jSf_l.D9#"1/1/4#1/5w8<=G
DX4A4#=1H
IJ;",').=>=c=G;$bj
"1/NZbB"1/N"'8<"4#1>1cO/GhYK=&YB"##$bc)k/4#8-",-L"#4#1T=c=Gjp4M1/4A4#"##*;"1/N$/"#XYXEG
=N
4#1:%=T=:=GZHVIJ/D9#"1/1?8<=1%-=#$L"T=YX=b)S/4M8-8-/"1/5$E=c=G;$?c4M"/
"1/N!)S/4#8R/"$J)=w5-4AD/DX-$.%=

2Y5oz

=

2





}u

=DR",%=b

YB"#M$H"8R!5R4AD/DXJ8"1!=MN=1/A*w=1PY9"#",

vxu

"4#G
HJyA=c=>","w$G;"#W"$%>)S/h'YB"##$kGhO/$SYXhG
=N4#1%==c=G+ZHVF"*>/P=YX=

2JY5"z
#vxu2

/"$k"A"N*

N0OD!YX=!YB"##$b/4H HAb4#1>h8O-1k$%",-8-$%",%bh=YX=k4#$k4#1=c=G+jhb

fi }u

"1/N"8R05-4AD/DX=#N/$J=1LYB"#HIJh",Lh",D/DB#4#8",Y9AE"8<4A=1/$4#1T/4#$J$",%
=c=GZbc=



%=

=1/k=&V/SYB"#M$YB"8-4#1:%=P=c=GjPHFIJk#"fiFN$%=MO4A=1/",=O/.O/-4#$%4#8

<F%-"8<$k4M$KL&=#A=)k4#15H

E 



}u



2
2





juZ

b

vxu

YB"#^?ZvA&b

vxu

YB"#MZ-4#5

? 

IJ/4M$S4#$k"'DB",R"#AV-#"ficNDB#"18<=1/$4#$4#15'=&).=w4MG
h$%%D9$HkIJ/h"8<4A=1$%?$%A8<%N",?



6B-$P4#G
T$%%D8<=1:"4#1/$w=1/A*"8<4#=1B",yG'",$
$%1/$%T4#1$%",%",yB"1/Nb z
} u  %=
=c=GZHE&=EDBO-$-OSL4MN"P=&$%%R4#8<4#15E"8<4A=1>8R=4#8<L4#1T"1:*'DB#"1B1/4#15$%",%
%=>=1/#*=$'"8<4A=1/$P/",P",'$A8<%N4#13y6B-$%E4MG
;$%%D=&K'#"fiFNDB#"1H
w8"#
$%''"8<4A=1/$L/",h$%Gq%=!YX
AD/&rO/Hyf_1;",YX=<F"G
D9A;$%",%bU/4#$E$%%R",%5*38O$
N=)S1EY/R"1/8-/4M15y&"8<%=&(-=GE%=;=1/H
c=G;4#G
$bfi$%%-4M8<4#15J=1$A&F%=S=1/A*?."8<4A=1/$W/",",-$%A8<%NYc*/#"fiFNLD9#"1/1
8"1YX?%=:=;GO/8RH..=1/$-4#NK?&(=MA=)S4M15
.Fc-r
<"G
DBAHKF"*w)LOB$%?E)M9F1=)S1
D/-$%1",4#=1)k4A!&=O=DX-",%=-$b

3O5"z  c 3O5"z J2 Y5"z  2
2Y5oz  2
b

b

8<=1:%=#$P"$4#15#-=Y=h",-GbU"1/N;=DX-",%=R$h8"1Y;OB$%N3%=
"1==1b

 c 3O5"z

">Y9A=c8-B",P'",RG
bW"1BNYBA=F8F$Pj

">YB#=c8-&(=G
4#$L/=#N/4#15



u

"1==1b



2  #u
3q5oz

"1BN

 HIShD9#"1/1

=1;Y9A=c8-=1%=D=&

"!YBA=F8&=Gq/'",YBAb=

=1%=0;",YB#Hf_1/4A4#"MA*b'",-G

2

4#$L=MN/4#15TYBA=F8

"1/N3Z",;=13/'",YBAHwIJ;5="4#$L%=$%"8j=1:%=ZHUF",%N3=1/4#$

$%",%b#"fiFNg?iSjlmlnjko)S4#MWO/-10=1
=OE=&.
&(=MA=)S4M15/
4#G

$%%D3=DB4#G;"
$%=#O/4A=1/$H

E  2  #u  $
 2JY5"z  2 


 3q5oz

j

juZ

b

b

 

=

q;

fi 9>~20$7`$22

E  3q5oz

 2JY5"z  2
 3q5oz

j

$##!2F



b



j



b

juZ

 

=

E  3q5oz

 2JY5"z  2
 3q5oz

h


Z
j

b

b

juZ

 

j#F=&9/$%J",J,"#4#N#"fiFN;$%=MO4A=1/$b"$4#1J#"fi",4A=1
4AN/=:$1/=G;",%%/",
=1%=
j=ZNA%$.&r"8<$./",).?$%4##91NHUf&24#$=1jhbc)?8"11/=
"1/N>4#&U4#$K=1Zb/)h8"1>1=

3O5"z

jQ=1:%=Z"1:*cG
=-H

2JY5"z  2

3q5oz

4#15

jv"1:*cG;=b

IJ/?6B-$%J"8<4A=14#1T"8-!#"fiFNDB#"14M$=1/A*w4#1/$%%N>%=;5J-4#NT=&b/4H HAb/&(-EL=YX=
",-Gb/"1BN&(-=G

LDX=4#1:.=&F4A)[=&E#"fiFNDB#"1B1b"#X=&EP$",4#15;"8<4A=1B$JN=

v	



_=YWHUIS:O/$#"fiFN$%=MO4A=1;<F%-"8<%NG;4#5Yk"1*;=&WS-",YX=Hf&V4A./",D/DX1/$

%=TYXhy$%8<=1/N=?/4#-N!=1/b/1).
A=$%yDB",0%="1=DB4#G;"U$%=#O4#=1!Yc*>$%%R4#8<4#15
=O-$A$S%=h8<=-$%DX=1/N/4#1/5;"8<4A=1/$b

3O5oz

Qju=

]Z+e.=&UADB&O/X"8<4A=1B$%="
$%",%I+"$J&=#A=)S$H

$%

Z+ fi8$T6,

] e

J ( Z+


mkb

3O5oz

[ZkHXIJ-&(=b9)yN<691/L

96 df+:	 96 ff
 ( Z+ 8 fi 

DB] e

] e

] e

]ae

] e?N1=%$P;$%L=&.5="#$EB",P4#$E8<=1/$%%RO/8<%N3Yc*-#"ficNCg?iSjlmlnjko",

4#G
;$%D^

=1wA""N=&K'4#1/4#4#"U#"*



Z~  :;+:F

)S13$%",-%N=1/;"$%2]

eRHwf_1

).=-N/$b).L8<=1/$4#N/J"$SADB&O/"8<4A=1/$S"#X=$%L",D/D9#4#8",YBA=1$bB)S/4#8R"N/N",SA"$%=1/E5="
",?6B-$%k4#G

$%%DVH?f_10
",Y=
.Fc-r<"G
DBAbX&4#15wy=YX=?",-G4#$k"G;=15
$%5="M$b)SB4#8-C8"O/$%$;"M/w/>$%",4#1/5"8<4#=1/$y%=YXw#D/&OB4#1C4#1B4A4#"$",%b

]KeRHf1",YX=.-BUR
<"G
DB#b./!G
=FN/47698",4A=12N=:$1=

4H HAb%=3YX>AG
1:$'=&
8-B"15h"1:*:B4#15H

IJ/1=4#=1=&AD/&rO//"8<4A=1B$$/",$K$%=G
?$4#G'4##",-4A4#$)S4A)S/",K)v8-G
=%k8"##$

k

<| ,,'|r~,}k]r08kRG
=%bV^``cb^```eRb/4M1
k8<=1:%<F.=&V8<=G;DBO4#15P%R,
%<-~r,}
%|RzB&=wO/-4#$%4#8>$%4#G;",4A=1VHQf_1"1cO$#b.5-N*5$-$4A=15-",DB/$YB"88R/"4#1&=G
w5="#$O/14M&r"8<$",-"8RNC/",y",w8<=1:"4#1NC4#1w8O/1:h$%",%HjkG;=15$%=
/4#1/5$bE5-",DBB$D/=c4#N/L"1>$%4#G'",4A=1=&)S/4#8R>"8<4A=1/$JG'4A5SYLOB$%&OB4M15%4#15'8#=$%

fi



!k

%=!/;5=" hIS=$%;",D/DBM4#8",YBAy=1$P)S/4#8R3",'G
GhYX-$E=&K0 r~ ;RB_|Rz9bU)S/4#8R34#$
PG'4#1/4#G;"8<=$%S$-OY/5-",DB"8R/4AF4#15
P5="#$H

IJ/4#$P"#$%="$4#G;4M#",-4A*YX).1#D/&OB"8<4A=1/$O-4#$4#8
"1/N)S/",P4#$Pc1/=)S1C"$

.k

<{# fi|}9RW&=G[#4A%-",O-S]roSYXb,?4#G
=DX=O/A=$bfisE=c/Abc^``deRH.=1/$4MNV"SK(F-r
"$%0)S
cO/1/N-N/$S=&.YBA=F8F$?",
=1
",YB#y4M1/4A4#"##*b9YBO/k
5="4#$=1/A*!%=>$"8=1

]KeJ)S4##4#10/4#$?8"$%
8<=1:"4#1=1/A*>/y$-4#15A

YBA=F8j=1%=D=&"1/=EYBA=F8!ZHIJ
$%
"8<4A=1

2J!5oz  2

jhb/=)S4M15")."*C"#=$%T",D/DBM4#8",YBA'"8<4#=1/$yG
=F4#15",=O/1/NYBA=F8F$h/",

",01=G
1:4A=1N4#1/!5="bS4H HAb=)S4#15")K"*"M=$%"8<4A=1/$/",",-04A-A,"1H
IJ;G'"4#13N/47X1/8<
YX)1'ADB&O/U"8<4A=1B$POR4#$%4#8y"1/N/'8<=1/8<D/h=&.-Afi"1B8<'4#$
/",kA,"1/8<h4#1!hOB$O/"V$%1B$%P&-$S%=)SB",k4#$JOB$%&OB&(=?$%=AF4#15
/P)S=#L"$%HSZ.4#15

m

ADB&O/b=1TE=JB"1/NbF&-$K%=;$%=G
/4M15hB",J4#$KO/$%&rO/~(}0FP} z9HIJB4#$K/"$K
N/4#$-"Nfi"1:",5/",k/#D/&OBX/4#15$k1N0%=YXL8<=G;DBO%N0&(="8-$%",R8-$%",%bXYBOk

q;

fi

2fffff$i

"N,"1",5EB",DX=$$4#YBA*&",KA$$/4#15$.",?AD/&rO/F/"1T",A,"1:Hf1=OK$%DX847698S$%4#15b
).5AD/&rO/"8<4#=1/$&=&("1:*:)K"*bc"$"?$4#N.<X8<=&B-OB1/1/4#15k#"fiFN;g?iSjlmlnjkohH
?8<=1/8#O/N/J/4#$$OYB$%8<4#=1
)S4A"1;<"G
DBA$=)k4#15E/",KAD/&rO/F"8<4A=1/$D/-OB1/4#15EN=c$
}9UDB$%
8<=G
DB#%1$$bW"1BN"&)G;",-c$?=1;8O/1:?4M1%5-",4#=1=&y%8R/1/4#xcO
4#1:%=
=OS$%",R8-!"#5=-4A/GH



$)$)affYSWe:ZWYbTW(0\0

f_1/
&(=##=)S4#1/5$=-<"G
DB#b;AD/&rO/"8<4A=1/$LO/-4#$%4#8PD/-O/1/$S=OL"M$=#O4A=1/$k&=G

;:

,

K$%",%$%DB"8<HF"*PK4#1/4A4M"$%",%4#$

qfi

"8<4A=1/$

1/"G


2
u2
u 2
u 2
u 2
u 2



u



8
8
8
8



8

-

N(e
fie

]

(



"N/N

]

-



b"1/NhK",/&=#A=)S4#15

o:
):
: ;Rff:

T ff: ;Rff:

: T   ff:

:  ,ff: ; 
Teff:  ,ff:

: T  ff:

]D/-

(



b/5="#$",-

e

]

e

]

fie

]

8

e

]

e

fi

f_1/4#$PDB#"1/1/4M15"$9bU",')=)."*c$h=&J"8-/4#c4#1/5G;4M$$4#15T5="

2 



2  

2 

 

PH>?1'=&./$%b

%=0YX
( bNA%$w=y5=" wHIJ=y=1b u
- bU1/N/$DB8<=1/N/4A4#=1
"8-B4AN>69-$%JYc* u
b9"1/N!:O/$k4#1:=A$SO/$-4#15> PDB#"1/1/4M15
"8<4A=1/$S4M1/$%%"N>=&J,}XS4M1

u

<

6B-$8"$%HiS#"fiFNTg?ikjklmklnWjov8<=51/4#$=1BA*
J69-$%"#%-1/",4Ab"$K4A$k=1BA*y4#G

$%%D>=DB4#G;"=1HIJL$%J=&5="#$k",JP$4M15A?4#G
L$%%D8<",%N!Yc*w5-",DB>8<=1/$%-O/8<4A=1>4M$

 (  G8h :_
] Ke

IJ/4M$K54A$JO/$)=wAD/&rO/"8<4A=1/$b91/"G
A*

 G8$ Wu 2  ( : u2  ( 

] Ke

2 

?1?=&$%b u
( b/N=c$1=8"O/$%L"1:*w$%",%P%-"1/$4A4#=14#1wL4M1/4A4#"9$%",%HIJ/=/=1b
bA"N/$h%=0/w$%",%T)S/'=1/#* t4#$h%-OHIW=0/4#$$%",%b).T=Y/"4#1w$-"G
w$=&
u

2 

k
W2 

(

W2 

ADB&O/U"8<4A=1B$b8<=1:"4#1/4M15b",5"4#1Vb u
( "1/N u
( HwIJ/4#$E4MG
bW;6B-$%L"8<4A=1C8"O/$%$h1=
$%",%%-"1B$4A4A=1b)k/4#Aw$%8<=1/N=1A"NB$
O/$YB"8C%=4#1/4A4M"$%",%HmkAD/&rO/"8<4#=1/$

 u 2  - uW2  

cO/$S8O$S=OJ$%=#O4#=1/$K&(-=GtP$",%$%D9"8<P=&U/4#$<"G
DB#L"$%H.hG;",T/",k
"$%'4M$N"N1/N'&

2  - 
u



=1/?8"1T"A)K"*F$."8-



Y:*;",D/D9A*c4M15 u 2 

v"1BN

b

b

b"1/N

"1/NT/",S=1P8"1>"$-4#A*G'",PE"$%4#1:4AY9A?)S4A=O8R/"154#15'EYX/"F4A=H

f_1IJiJf_l.N/=G;"4#1/$bK=18<=O/#N2=4#8"MA*=-8<=G;0/4#1/8<=G;DBA%1$$'=&PADB&O/

"8<4A=1/$yDB-O/1/4#1/5Y:*C8<=1/$4#NR4#15!1=y=1/#*w6B-$%#"fiFND9#"1C/",'g?iSjlmlnjko

61/N/$b

YBO8<=G
DBO4M15!"c4#1BN3=&JO/1/4#=13=;"M#"fiFNDB#"1/$P/",
gEiSjlmlnWjot8<=O/MNDX=$$4#YBA*

+
] ~  

691/NVbJ)S1"#A=)S4#151=14#G
$%%Dv=DB4#G;"DB#"1/$H!=DB84#$%A*b4#1"$%",-8-v$",%

Z~  :;+:F
   

8<=1/$4MN/-#"ficNw"$%]
4#$h"8-/NHF"5="$%

 ~   

&r"8<SA

.b

eRHF%1/Nw?#"fiFNwDB#"1/1/4M15L5-",DBTO/1:4#9&"8<A

W",w%=DC&r"8<
A%=

   G fi8 
=


^EN=)S1T%='&"8<S#^,b/)Sb",J"8R!A

"



HIJ1VbDB=c8<N&=G

bB"
$%

=&5="#$J4#$K51R",%N

fi 9>~20$7`$22

M #(

"$E;OB1/4A=1=&

7= #(

&r"8<L4#1





$##!2F



)S4A0;D/-8<=1/N/4A4A=1B$k=&K"#"8<4A=1B$L4#1A

/",L"N/N3",LA"$L=1

HhSDX=10%-G;4#1B",4A=1bN<691y"$EADB&O/"#"8<4A=1/$?/",L"N/N",EA"$%?=1
&r"8<E4#1

+

( HTfh8"1Y
DB=1/",bU/4#$P)."*b'$%",4M15"8<4A=1B$L=&J"#=DB4#G;"$=#O4A=1/$L&(=G

",P8<=1B$4#NN/AD/&rO/HUmS=)b4#1"#X=O?FIJiJf_l.%$4#15;N=G;"4M1/$bFP8<=G
D9A%LG
=FN
"A)K"*F$S$%#8<$h|,{{",D/DB#4#8",Y9A"8<4#=1/$S"$J/AD/&rO/H

$)$'&b9:ZW98%hY:Z6Qa9bK6'b9:%adcZWYhT8%P!


j$/"$"#"N*Y1
1/=%N'",S*yY54M1/1/4#15=&/4#$$%8<4#=1b).S4#1:%5-",%k#D/&OBc"8<4#=1/$
D/-OB1/4#154#1:%=J=O$%",-8-y"A5=R4A/GvYc*E=1/A*?",D/DBA*F4#15J4#WN/O-4M15K$4M15A1&=-8<N/4##78M4#GPY94#15
%*bUA"F4#15/'8<=G
DBA%YX$%_6B-$%L$",-8-"A5=-4A/G

+

$%",%

O/1/8-B"15N]r$%Tc8<4A=1aeRHT\/"84#1/5"

N/OR4#15LY/-"N6B-$%K$%",-8RT&(="YX%%J$%",%L4#11&(=R8<NwB4##78#4MGPYB4M15b)EA=c=;=1/#*

"]Z+eRHPIS/4#$k1/N-$k=OE4#G
D9AG
1:",4A=1!=&1&=-8<N/4##7

",?=$%
$O/88<$-$%=-$?51-",%N3Y:*

8#4#GhYB4#1/5L4#1/8<=G
DB#%J1'=1w4#1:4AYB#DB#"1/1/4M15E"$%F$HmS=)b/4M1'"#B=O%$%4M15hN=G'"4#1/$b
L"$c$S/",S8"1/1=SYL$=AN>Yc*1/&(=-8<N/4##A8#4#GhYB4#15POB$4#15
ADB&O/X"8<4A=1B$JD/-OB1/4#15y",
<"8<A*T=$%L/",k8"1/1=JYXL$%=AN>Y:*w1&=-8<N!/4M#78#4#GhYB4#1/5L"1*c)."*H

m /4

 

u2   "u

IJ$%8<=1BND/-O/1/4M15>%8-B1/4#xcOT/",
)4#1:%=FN/O/8<T4#1C/4#$y$%8<4A=124#$yG
=4A,",%NY:*=Y
$%,",4A=1B",P4#1$%=G
'DB#"1/1/4M15wN/=G;"4#1/$?-'",;5="=-N-4#1/5T8<=1B$%%-"4#1:$b"$P/"$LYX1
8<=51/4#N>Yc*TxcO/4A%h";1:OBGPYXJ=&U$%",R8--$4#1TPDB"$%]f-"1/4sK15b^`d:-O/G'G
=1/N



spO-4Ab/^``c =$#4#1;s[iJ="8-VbX^``,eRHf1'=O<FDR4#G
1:$=1;"$%F$)S4A
5="B=-N-4#1/5L8<=1F
$%%-"4M1$bF\U\.$KYB"$%E",-8-B4A%8<O?$%=G
4#G;$K)."$%%NT"yA=.=&V4#G
?"8-B4Ac4M15h5="M$K/",.1/NN
%=!YX'8",-N&=h#",%=1HTw&=N#=DN"!OR4#$%4#8y%=04#1&=-Gq$%",-8R",YX=OP5="
=-NR4#15$H
IJ/h8#"$$-4#8"W<"G
DB#h&="DB#"1B1/4#15'N/=G;"4#1>)k4A!5="=-N/-4#15'8<=1B$%%-"4#1:$?4#$S/h).#
F1=)S1K(FcU-BH"*).h/"h-LYBA=F8F$Sjhb9Z["1BN0=1!P",Y9AP4#1/4#4#"#A*b"1/N!)."1:
%=$%"8G

$O/8-/",w).0/"Zt=1%=D=&yb."1BNj=1%=D=&PZH?Y:F4A=O/$#*b

4#$;1/=GO/8-2D=4M1'4M1$%"8F4#15Cj=12Z

6BR$%Hok=)PbJ4#G;",54#1>"&(=-).",-N$%",-8R/4#153D9#"1/1

R+

8<=1&=1:%N')S4#'"P$",-8-w$%",%



T	

+

.b:)kS$%=G;S5=" [/"$ O/$%YX1'"8R/4ANbF4H HAb $-O/A%N

6

&=Gq$%=G
'=h$%",%Y:*",D/DB#*c4#1/5w"1"8<4A=1

D
|

w)S4A

$4AOB",4A=1M4AB4#$h4M$b)."$
4#h"05=:=FN4#N"%="8-/4A

[

96

"NBNW] eRH'/",L=1'8"1"$%4#13"

[

+R4A5y1=)


$/=O/#N$=G
T=

5="KYX"8R/4ANC6B-$ t?Oy"1/$%).
4#$h4M1/$%DB4A-NY:*8<1y).=3=&E=:BA"1BNmk=,XG'"1/1

&


],"eRb)S/4M8-",5O$;/",;"8-/4#c4#1/5
1=
YX"8-/4#N)S4A/=O
N$%%=*c4#1/5

$=O/MNYXDX=$%%DX=1N4A&S/G;"4#1/4M15!5="#$8"1

q",5"4#1H2&8<=O-$%b691/N/4M15!=O",Y=O/B4#$
4#1:=A$

$%=AF4#15-G;"4#1/4#1/53DB#"1/1B4#15"$%Hpmk=).b)8"1",-4A0",T"*$4MG
DBA>Y9O
=OL%$4#15>N=G;"4M1/$



+
)
R ZD 96 6
]

3N] ek4#1

Ofi

.H0IJwG
=FN).w",-wO/$-4#15!4#$"$$4#G
DB#'"$/4#$

R+

f&#"fiFN$%=MO4A=1DB#"1b 
bX/",Lg?iSjlmlnjko51-",%$?&=

b/",ENA%$

4#1

$O/D/-4#$-4#15A*T"88O-",%w",D/D/=F4#G'",4A=1Y:*O/$-4#15T
#"fiFNDBM"1/",

g?ikjklmklnWjo51-",%$y&=h/w$%",%

6D 



+

$%",-8R!$%D9"8<bB4H HAbBN=;1/=J51-",%"1*T$O/88<$$=-$J%=
{Ar~r,}wO-4M$%4#8,H

6

KbX8<=1:"4#1/$?"1"8<4A=1

+

c$?1=1F-#"ficN-$4#=19eRbX10)G
=

cb

2&=G

KHh8"#B4#$G
=FN'|;:|,{

nWO/$P<cG;DB#4A&*/OR4#$%4#8
)k4Aw",Y=.Fc-r0<F"G;DBAHF"*3'D9#"1/1

B	

/"$

O/$%E"8-B4AN

6

76 

]rjPbZJeRbVY9Ok)k4A

]rZb Je$4##Y4M15&r"#$%bV4H HAb)
",;4#10
$4#O/",4A=1

"

fi

2fffff$i

)S-Lj4M$=1>%=D=&Zkb9"1/N>Z["1/N0v",P$"1/N/4#15
=1>P",YB#HIJE-#"ficN$%=#O4#=1T/",
g?ikjklmklnWjoQ61/N/$K%=;/4M$$4AO/",4A=14M$E&=#A=)k4#15H

E    3O5oz $
 2JY5"z  2 $
juZ

Z

 3q5oz
W6 

b

b

$ 

ZQ

IJS5="

%	

  3O5oz

]rjhbZeRbc)S/4M8-;/"$ %OB$%YX1'"8R/4ANbc5$N/A%N;Yc*yJ6BR$%"8<4A=1

ZHc.=1/$x:O1:A*b).J"M4A/",
D/-OB1kB4#$DX=$$-4AYB4##4#*y&(-=G

3O5"z

j

4M15Lj=1%=PZR4A51/=))K"$D/-=YB",YBA*"EYB"N
4#N"Fbc"1/N

E$%",-8-$DB"8<b/)S/4M8-$O/A$K4#1T"y$%=#O/4A=1wDB#"1w/",

=1:%=T6BR$%H

3O5oz

$JZ

n4A'4#1;D/8<NB4#15!$O/YB$%8<4A=1b)w8<=1/8#O/N/;)S4A3"1<F"G;DBA'$/=)S4#1/5>/",PDB-O/1/4#1/5
$%",-8R$%",%$
4#13/G;"1/1/N$-8<-4AYXN3",YX=TN=c$h1=D/$8<=G
D9A%1$$b"1BN)S4#"
&)[G;",F$=1=O/S8O1:J$%",-8R!"A5=R4A/Gt4#G;DBAG
1:",4A=1H

$'&($)affYSWe:ZWYbTW(0\0


f_1E&=#A=)k4#15
$G;"#X<"G
DB#b=1L=&UE5="#$Pc<YX?N/$%%=*N%G
DX=-",-4MA*'4#1T=-N/J%=
"8-B4A/=5="H0IJ/4#$E-1/N-$PD9#"1/1/4#1/5w"$O/1/$%=#fi",YB#
)S13=1/w4#$hO/$4#1/5>
"N/NN'5="9N/A4A=1'/O-4#$%4M8,HUF"*
k4#1B4A4#"$%",%?4#$G;D/*bcS5="M$",

Ofi

",L/E&(=##=)S4#1/5y"8<4#=1/$

1B"G


W2 
uW2 
u

:
):
8
: ; Rff:

8 ;Rff:  ff: ;R
u2 
uW2 

j#V$%=#O/4A=1/$%=/4#$"$%>1/N!%=w",D/DBA*

|

; :

]DB

"NBN

b"1/N'

NMe

]

e

]

fie

PbO/$%

t",&%b9"1/N!<$%",YBM4#$



hHIJ

8<-O/84M"DX=4#1V/4#$/", c<YX%G
DX=-",-4#A*N$%%=*NHIJ"N/N/NE5=":N/A4A=1LO/-4#$%4#8
4#$k1=k"NxcO/",%h&(=?$O/8R>DB#"1B1/4#15y"$%F$HSIJ/L<F"G;DBAP4#$kN"NF1/N!&(-b"1/N!=1h8"1"$4##*
G;",L/P$8<1/",-4#=;4#1-4AYBA?)S4A/=O8-/"154M15;EYX/"c4A=S=&hO-4#$4#8,H
1/#4Ah&(=E#D/&OBW"8<4A=1/$bX8<=G;DBA%1$$?8"11=YXh-5"4#1N0Y:*$%=G
=)1cO/G
R",4#15
"#XM"ficND9#"1/$K%='";$-4AO/",4A=1HUf_1L",YX=h<F"G;DBAb)S1

4;hTSz0W
 $'&($'&b9:ZW98%hY:Z6Qa9bK6'b9:%adcZWYhT8%P!

$%4#M

bF1>"#-#"ficNTD9#"1/$8<=1:"4#1

Wu 2 wb/NA4M15-PH



#

u/"$JY1"8R/4ANYBO

4#$


O/$%hh"N/N/N!5="UNA4#=1!OR4#$%4#8L4M1!")."*!$4#G;4M#",J%=wh4#1:%5-",4A=10=&yADB&O/
"8<4A=1/$KO/-4#$%4#8,HUj$4#1BN/4#8",%N'",Kk*;YX54#1/1B4#15E=&V?$%8<4A=1Vbc4A.4#$.4#1%5R",%Nw4M1%=
$4#1/5A1&=-8<Ny/4#M78#4#GhYB4#15.%*LB",U$",-8-
N=c$b"1/Ny8<=G
DB#%A*EO-1Nh=,wNBO-4#15SY$_6B-$%
$%",-8RbB4#18"$P1&=-8<N>/4M#78#4#GhYB4#1/5PN/4#N/1V KG'",P4A%=;E5="H
"#$%=OB$%!"1=w5="k=-N-4#1/5%8-/1/4Mx:ObK",1&=G#4A%-",O-Hv?1/>=&E
G
=$%w8<=G;G
=1",D/D/-="8-$%=CN/"#4#153)k4A5="=-NR4#15$;4#$'%*F4#153%=C8<=51/4A0G

4#1

"D/DB=c8<$-$4#150DB/"$%b"1/N1O/$w/G%=D/-O/1w&-"8<4A=1/$
=&k>$%",-8R$%DB"8<>NBO-4#15

J

DB#"1B1/4#15]f-"1/4sK1/5bU^`d:K1/5Tsf-"1/4bU^``c =$#4#10s

iJ="8Rb^``,eRHhIS/4#$4#$?"#$%=


YB"$-4#8yD/-4#1B84ADBAO/1/N-#*c4#1/5';$%=,8"MAN[_5="",51BN/"!",D/DB="8-2]rL=:/#b^``eRH\/=
=O.$%*c$%Gb).k/"4MG
DBAG
1:%N
"P$-#4A5:A*$-4#G
DB#4A6BNPR$4A=1;=&Xk5="9",51BN/"P"A5=R4A/Gb
"1/NTO/$%?4A.%=
&O/.1//"1/8<?DX&=-G;"1/8<Hj[-*$=-.$OBG;G;",*
=&)S/",K/",D/DX1/$.4#$./4#$H

"



fi 9>~20$7`$22



$##!2F

f_1"D/-D/=F8<$$4#15D9/"$%b;DB#"1B1PA=c=c$P",h"#DB"4A-$?=&5="#$"1/N3N/84#N$P/O-4#$%4M8"#A*
)S/-4#$"1w=-N-4#1/5P8<=1/$%%-"4M1.YX).1/GHjS&%)K",-N/$bc5="X$%4#$$%D9#4A4#1:%=
"E%="##*h=-N/N;$%-4#$=&$OYB$%$U-$%DX8<4#15E$%S=-N-4#1/5$HIJ/$%S",J/1
&(N;%=L1&=-8<N

? ( :OlOlOlT:;?n
a (
+
M ( `
i ( g`  - `? 4

/4#M78#4#GhYB4#154#1"14#1B8<G
1:"JG;"1/1Hl-84#$%A*bK4A&

4#$y!=-NN$%-4A$
=&



$OY9$%$bF1&=-8<N/4##A8#4#GhYB4#15E5$K69-$%K$%",%N=1T?=-4#54#1/"X4#1/4A4M"B$%",%P"1BN

+

).=c$K=O/b/$%",-8->1/N/$4#1T$%=G
E$%",%

C$",4#$%&*c4M15h/E5="#$4#1

1T8"#ANw",5"4#1w=11/)$",4#15$%",%

3"1BN/#",-55="$%

$",4#$&(*F4#15/4#$b$%",R8-
5$$",%N
&=K5="#$

Yk

( Hf&W/",

H1&(=R8<NwB4##78#4MGPYB4M15L4#$

- H\=G

"y$%",%

b"1/Ny$%=E=1HUIJ4#1B8<G
1:"b=

|c<}9|_,~ ,<}Bb9DBM"1/1/4#15yD/-=c8<$$S8"1!YXP",D/D9#4ANT%="1:*DB#"1B1b/4#1DB-4#1/84AD9Abc"1BN>D/$$
8<=G
DB#%1$$=1BA*P=1N"NF1/N
&J"$c$]rE=:BAsvmk=,XG'"1/1b:,"eRb/4H HAb:",5"4#1b).S/"
"11//"1/8<G
1:?/",?A=$%$8<=G
DBA%1$-$4M1>51-"HIJcO/$b9)yO/$%h5="",51BN/"w=1/A*4#1
1&=-8<N>/4M#78#4#GhYB4#1/5bA"F4#15
/P8<=G
DBA%PY$_6B-$%$%",-8R>DBB"$%LO/1/8R/"15NH
IJ/L5="",51BN/";%8R/1/4#xcOL*F4A#N/$KRO/14MG
L$"F4#15$k4#1>N=G'"4#1/$)S/L",L=-N/-4#15
8<=1/$%%R"4#1$wYX).105="#$Hf_12=O/w%$4#15$O/4A%bJ/$%",/K(F-r"1/N

H

~%

<

,R{A,Hf1DB#"1/1/4M15T"$%c$h)S4A/=OL=-NR4#15>8<=1/$%%R"4#1$b/$%-4#$L=&J$OY9$%$P8<=#M",DB$%$

4#1:%="$4#15AP1%*bX$O/8R!B",k",51/N/"wG
8-B"1/4#$G+N=:$1=8-/"15y"1:*:/4M15'-HJIJ
-O/1:4#G
?",1&(=SED/<D/-=c8<$$S4A$%A&)."$k15A8<4AYB#L4#1"#X=OJ<FDX-4#G
1:$H

 %
$  &
ff

, ff

(' 

E

c=w&",b).y/"y$%%R4#8<%N0=O-$%A$%=wDB#"1/1B4#15;"$%F$?$%DX8476BN4#1!/$4MG
DBAIJiJf_l.>#"1F
5O/",5HL)S4##1=)Q$=)[=)[=O/J",D/D/="8R!8"1YX?<c%1BNN%=;N"X)S4#jk?n]rlN/1B"O/Ab
^``eS"$%F$bG;=PD/-84#$%A*b/)k4A>jk?n$OY9$%S=&l?nv]r08kRG
=%?"HAb^``eS/",
)K"$O/$N24#120,1/N4#1:%-1/",4A=1/"SDB#"1/1B4#15$%*F$%%G;$w8<=G
DX4A4A=1p]rZK"88R:O/$b,eRHuIJ/4#$
4#1:=A$'N/"#4#15)S4#",YB4A%-",-*&rO/1/8<4A=1$%*cGhYX=K&(-6B-$%
=RNA=54M8T&(=-GO/#",b"1/N)S4A

Ofi

8<=1/N/4#4A=1/"X<X8<$HK?O/<c%1/$-4A=1).=w4M$JN/4AF4#NNT4#1:%=
E&=#A=)k4#15y&(=O/S$OYB",-"$

^,HSjkD/DB#*w"
DBD/=F8<$$4#1/5y",DBD/="8-!%='hjk?nCN=G;"4#1>"1BN"$%>N$8<R4AD/4A=1b8<=G
D94##4#15
/P$%DX8476BNT"$%TN=)S1>4#1:%='"yD/=DX=$4#4A=1/"X1=-G;"V&(=-G>H
cHSc%1BN!O/-4#$%4#8w,"#O/",4A=1=&?DBM"1/1/4#15$%",%$%=N")k4A$%!1/=-G;"K&=-G
8<=1B$%%-O/8<$H

	
	

cHSjN O/$%EDB-O/1/4#1/5h%8R/1/4#xcO$H
HSjN O/$%L$",-8-G
8-/"1B4#$G;$H

W#}vWv  2#vxu5  3q3T'rw24  $v

 yx~3qz

 2 #w

\U\.$ED/D/-=c8<$$-4#15wDB/"$%
4#$?"#G
=$L4#N1:4#8"%=
G
=FN=A=5*0/",L/"$?YX1NA=DXN
&=JLf_ll2DB#"1/1/4M15$*c$%%G>H\=kN/"4##$b).L&JE"N/S%=;P)=w/",$SY1!N=1
w]rE=:BASsmk=,XG;"1B1b9,,YeRb"1/N54AE=1/#*EY9"$4#8?D/-4M1/84ADBA$.H
IJ/0D9#"1/1T$%",$)k4A"D9#"1/1/4#1/5"$%$%DX847698",4A=154A1v4M1/$-OYB$%T=&hl?n
N<691/N&(=;/!jSf_l.c,DB#"1/1B4#158<=G
DX4A4A=1[]rZK"88-cO/$bS,eRHIJ>4#1/DBOy4#$
"3$%;=&
=DX-",%=h$-8-G;","Fb'4M1/4A4#"U$%",%bU"1/N3"5="&(=RGhO/M"FH;IJ;4#1/4#4#"$",%4#$L$-4#G
DBA*"$%
=&5=O/1BN!",%=G;$b"1/N!P5="&=-GO/#";4#$k"1",-YB4A%-",*w6BR$%J=-N?A=54#8"W&(=RGhO/M";O/$4#1/5;

|6

#",4#=1/"$%*FGhY=M$PN<691/N3&=hDB#"1/1B4#15T"$%Hj1*=DX-",%=y$8-/G;"

"

!4#$hN/<691N3Yc*3"

fi

2fffff$i

#4#$=&DB",-"G
%-$b/"hDB8<=1/N/4A4#=1bc"1BNw"
M4#$%=&<X8<$Hf_1/$%"1:4#",4#15y?DB",-"G;%-$K*c4##N/$b

	

O/$%?#4AyIJiJf_l."$%c$E",-yOB$O/"#A*$%DX84769NbBy"8<4A=1/$?%=Ty$8RG;"FHLIJD/8<=1BN/4A4A=1

,+

4#$"1",YB4A%-",-*](69-$%=-NeJ&(=RGhO/M"FHk\=L"1"8<4A=10%=TYXh",D/DBM4#8",YBAP4M1!"w54A1$%",%

+

4#1/$"14#",4#=1T=&B4#$K&(=RGhO/M"
GhO/$YL$-",4#$_6BN4#1



KH"8R><X8<

)+* / O: lOlOlT: * -n , fiB]/.  ]96e;:"NBN  ]96e;:N/  ]96e%e
* / :OlOlOlT: * 0n , ", SJ<X8< DB",-"G;%-$b.  ]96e4#$S<8<.8<=1/N/4A4A=1 

mkb

&=-GhOB#"



96

"1/N0"N/N

.b4A$

4#1>L#4#$%/"$JL&=-G

",5"4#1b"1'",YB4A%-",-*

96

] ek"1/NN ] ek",y",%=G'4#8y"N/N"1BN0N/A%h<X8<$bV-$%DX8<4AA*H?IJ

",%=G;4#8S<8<$K",$%$=&VO/1/4#1/$"14#",%N'",%=G;$bF4H HAbM",4A=1/"B$*cGhYX=#$8<=1:"4#1/4#15Pfi",-4M",YBA$H



1.  ]96eE4#$Pfi"#OB",%NH

IJy$%G;"1:4#8$E",/",bV4A&"104M1/$%"1:4#",%N"8<4A=14#$k<c8O%NVbV1bX&="8R3$4#1/5AP<X8<

.   ]96eL=#N/$P4#1w8O1:P$%",%b18<=-$%DX=1/N/4#1/5T4M1/$%"1:4#", 4A=1/$P=&./",%=G;$4#1

4#1#4M$%b"1/N&=h"8RC4M1/$%"1:4#",4A=13=&J4#$LDB",-"G;%-$bUT8<=1/N/4A4A=1
f&

"N/N

96

96

] e",E"N/NNT%=yE$%",%bB"1/NwE4#1B$%"14M",4A=1/$.=&",%=G;$S4#1N/ ] e",EG;=NT&=G

P$",%H

f_1'\U\.$O/-4#$%4#8G;=cNVb"8-T$4#1/5A$%",%k,"#O/",4#=1'8"1'4M1=AS=O/$"1/NB$=&=DX-",%=
",D/DBM4#8",4A=1/$



YBO/4##NB4#15'/M"ficNDB#"1/1B4#155-",DBVb=1w1/N/$P%=N%RG;4#1'"M",D/D9#4#8",YBA

"8<4A=1/$'",;"8-$4#15AT&r"8<'#"*H!-&(=>4M1$%
/><=-;%=38<=G
DB4##w/>=DX-",%=
N$8<R4AD/4A=1/$.N=)S14#1:%=y"yGO/8-$-4#G
DBA.D/=DX=$4#4A=1/"B1/=-G;"&(=RGb$O/8-TB",JO-4#$4#8J,"7

6

O/",4A=108"10YX4#G;DBAG
1:%N><@84#1A*HL?Ok691/"1=-G;"&=-G+"8<4#=1/$
&=-G;",H

fi

96

l-8<=1/N/4A4A=1 DB] e

Ofi

96 ;:
96 ;:

96 ;:
96 ;:

 96 ;:

 96 ;:  96

;/"/h&=#A=)S4#15

96 32
96 32

]DB / ] e "N/N / ] e N / ] e%e

X8<$

]DB ( ] e "N/N ( ] e N ( ] e%e
H
H
H
]DB

] e "N/N

] e N

] e%e

IJ/kD/-8<=1/N/4A4A=14#$."$=&5=O/1/Nw",%=G;$Hn4A)S4#$b?<X8<K8<=1BN/4A4A=1/$D/

 96

] e=&

$4#1/5AS<X8<$",?$%%-4M8<%N'%=yYXk5-=O/1/Nw",%=G;$HE"#$%=D/-$%1.5="$%",%E"$"h$.=&
",%=G;$HEIJcO/$b).y8<=G;DB4#A")K"*0*:B4#15'<8<D/?
8<=1/N/4#4A=1/"V<X8<$Hh.=G;DB4##4#1/5;")K"*
!#=54#8"&=-GhOB#",!4#1:=A$;%R"1/$%&=-G;4#15G

4#1:%=3o\.b)SB4#8-8"OB$%$'"12<cDX=11:4#"

YBA=)OD4#1
51-"Hf_1;=O/%$%4#1/5PN=G;"4#1B$b=)bF).S&=O/1/N
B",/4#$%-"1/$%&=-G;",4A=18"1
YXPN=1h4#1"$%=1B",YBAL4#G;H.=1B8<-1/4#15
/h8<=1/N/4#4A=1/"<X8<$b=$%h8"11=SYXL8<=G
D94#AN
")K"*w)S4A/=O"1=/.<FDX=11:4#"BY9A=)ODVbc54A1w/",K))K"1K%=yD/$%-$%=MO4A=1'#15H
IJ/4M$
)."$wD/-=1Y:*okYXh],eRHj$').)k4##J$%bk8<=1/N/4A4A=1B"<8<$8"1<@84#1A*YX
4#1:%5-",%N4#1:%==Oy"A5=-4#/G;4#8
&-"G
).=b$%=!/w4M$h1=01N&(=
8<=G
DB4M#4#15G

Ofi

")K"*H

IJL8<=G;DB4##",4A=1TDB=c8<$-$D/=F8<N/$J"$&=#A=)S$

^,HS%-G'4#1;D/-N/4#8",%$E/",h",T|r~bU4#1$%1/$%
/",h1==DX-",%=h/"$P"1<8<h=1
/GHkFO/8R>D/NB4#8",%$S","'8<=G;G;=1!DB/1=G
1=14#1YX1/8-BG;",T"$%F$HJjk1!<F"G;DBA

[ V [54

",-Sy]r4#1F84A*

qfi

[ V

eU&r"8<$K4#1;U#r-"$%c$ Uj1:*;A=c8",4#=1

@[4

)k4A/4#1/$"G;84A*

$%"*F$bc=&V8<=O-$%bFA=F8",%N

E=O/5=Oh)k=APD9#"1/1/4#1/5
D/=F8<$$HSy8<=51B4A$",4#8

DBN/4#8",%$Yc*"
$4#G
D9AE$%)D!=?"#X=DX-",%=k$8RG;","FH

"

<

fi 9>~20$7`$22



$##!2F

qfi

cHSI-"1/$&(=-G"#X&(=RGhO/M",L4#1%=;xcO/"1:476B%&L?o\HIJ/4#$K4#$S$OYN/4AF4#NN4#1%=
/P$%%DB$

]r"eTl<1=-G'"#4A
"#UA=54#8"U&=-GO/#",Hy\=#A=)S4#15!g?",1"1/NL1=YB#=c8-C]_^``deRb/4#$
DB=c8<$-$E<cD9"1/N/$L"#x:O/"1:476BR$bW"1/N%R"1/$#",%$L1/5",4A=1/$H'1/N3O/D3)S4A&=%

	

	

GO/#",?/",",EG;"N/ODT=OK=&8<=1 %OB1/8<4A=1/$bFN/4#$ %O/1B8<4A=1/$b"1BNw",%=G'$8<=1"4#1B4#15
,",-4#",Y9A$H

]Yef_1/$%"1:4#",%E"#9DB",-"G
%R$HIJ/4#$4#$.$4#G
DB#*N=1/kYc*'4#1/$"14#",4M15h"#B=DX-",%=J"1/N
<X8<'D9",-"G
%-$')S4A"#*:DX>8<=1/$-4#$%%1:'8<=1/$%"1:$'=1/>",&(%>=HIJ
DB=c8<$-$G'",$;O/$%w=&SF1=)kAN5T",Y=O/y$",4#8wD/N/4#8",%$b4#1/T$1/$%T/",y
4M1/$%"1:4#",%Nw&(=-GO/#",8"1T=&(%1wYXk$-4#G
DB#4A6BN>]rE=c/AJsQmk=,G;"1/1b/,,YeRH\/=

7 6

<"G
DB#bV4A&"14#1/$"14#",%N$",4#8D/N/4#8",%>]

eS=F88O-$?4#1"T&=-GhOB#"Fb"1/N0/",

1 6

4M1/$%"1:4#",4A=1'4M$J}998<=1"4M1N4M1
k4M1/4A4#"$%",%bF10]

4;hTSz0W

H

]r8eTI-"1/$&(=-G

e8"1YSDB#"8<N')S4A

&=-GhOB#",'4#1:%=0?o\.HIJB4#$P4#$PDX=$%%DX=1NO/1:4#",&%
4#1/$%"1:4#",4A=1VbYX<

8"OB$%;4A?8"1Y
8<=$%#*bV$%=>4A?$=O/#NYX",DBDB#4AN%=>"$E$G;"#&=-GhOB#",
"$?D=$-$4AYBAH
f_1"!&rO/##*4M1/$%"1:4#",%NC&(=-GO/#"Fb4#4#$#4A#*/",
G;"1*$%",4#8w=
=1<)K"*DBN/47
8",%P=c88O-1/8<$8"1YXDB#"8<NTYc*
&=-GO/#"
$%%RO/8<OH

:Z8ZRTW

=

4hTSz0W

bc$O/A4M15h4#1T"yGO/8-$4MG
DBA

Q	

cHS\U4#1B"#A*bX4A&
?ok\=&"1:*!&=-GO/#"'8<=1:"4#1/$EG;=hB"1=1/yNB4#$ O/1/8<b108<=-<
$D=1BN/4#15
<X8<b=DX-",%=bX=?5="8<=1BN/4A4A=1!5$E$%DB#4AkOD04M1>yG;"1/1D/=DX=$%N!Y:*
gE",1"1/NP1=YBA=F8]_^``deRH

W /_  
1=)

 4vqyvGe48!xYu#J our  l |  5 T3

 

$=)

=)q=O!$%DX84#"M4AN[g?iSjlmlnjko

4#G
DB#G
1",4#=1b"$>)."$>N/$8<-4AYXN4#1

c8<4A=1HcbS4#$8-/"1/5N%=CN"k)S4Aj?n8<=1/$%%-O/8<$HQZKO/4##N/4M15=1=OT1=-G;"M4AN"$%
D/-$%1",4#=1bB4AJ$O@8<$%=;",8",L=&U8<=1/N/4A4A=1B"X<8<$H

9($'&($);:GWeSh=<TW!>/Sh9bTbT6'b?A@>8%hBC0ED.6Q:A=aTb=>6":Z6Qa9bThTSGF4747W(PZ:M0
?O1B8<=cN/4M15h=&DB#"1B1/4#15h5-",DBTYBO/4M#N/4#15P&(=J-#"ficNT"$c$J"MG
=$%J4#G;G;N/4#",%A*'8",--4A$=
%=jk?nu"8<4A=1/$w4#10",YX=0D/=DX=$4A4A=1B"1=-G;"S&(=-G>Hp?1!$4MG
DBA*1N/$'%=D"1

9

"N/N/4#4A=1/"#"*G
GhYX-$/4#Dw,"#OP&(=S"M -K=&"1!"8<4A=1HKIJh#"*G
GPYX-$-/4AD=&"1
<X8<S4#1/N/4M8",%$K6B-$%#"*J)SE"#4#$K<8<S8<=1BN/4A4A=1/$.DBMO/$/L8<=$%DX=1/N/4M15h"8<4A=1V$
D/8<=1BN/4A4A=1/$",KD/$1HI=?8<=G;DBO%K$%G
GhYX-$/4AD4#1:%5-$4#1y"1y<@84A1:G;"1/1b,).

 96

DT"L8<=O/1:%&="8R'<X8<

4D0D/



6

=&"1'"8<4A=1

:bc)S/4M8-y5$.4#1/8<G
1:%N;"8R'4MG
S"L8<=1/NB4A4A=1

ED

] eYX8<=G
$SDB$%1:b|,}9P"8-4#G
h";D/8<=1BN/4A4A=1

  96 O   96 O

 96

96

!D/-] e=&Uh8<=-$%DX=1/N/4#15

"8<4A=1YX8<=G
$yDB$%1:HCIJ/w<X8<;5$;4#$
#"*'G;GPYX-$B4ADC$;"$;$%=c=1"$
4A$
8<=O/1:%
"8R$

D/-

] e

D/-] e

H.IJ?<8<$"N/NT<X8<$k"NBN

] eK",E/1>$8-/N/O/ANT&=JL1<F

#"*HIJEDB=c8<$-$J4#$J4A%R",%N>O/1:4#"#X5="#$k",L"8RN>?69-$%4#G
H

9($'&($'&H:GWeSh=<TW!>/Sh9bIFJ<9:Z8%hTP(:Z6aTbKD.6Q:+AaTbB>96":Z6aTbTh9SLF4747W(PZ:%0
IJw#"fiFNDB#"1<c%-"8<4#=1CG;8-/"1/4M$G

&(=
jnvNB47R$P&=G

4A$yIJiJf_l.8<=O/1:%DB",y4#1

G
#*>).=#4A%AN"4##$H?f_1/$%%"N0=&$A8<4#15T"8-/4#c4#1/5w"8<4A=1B$bXy<c%-"8<4#=1G
8-/"1B4#$G
$%A8<$?"8R/4AF4#15'<X8<$HP?1/8<"1!<X8<



76

=&"8<4A=1

"



;4#$$%A8<%NbX"#=&4A$S<8<E8<=1/N/4#4A=1/$

fi

6

DB#OB$

2fffff$i

 96

:$PD/8<=1/NB4A4A=1/$?1N%=>YX
DBOL4#1:%=>4Ah8<=$%DX=1/N/4M155="$%$HTjS&%)K",-N/$b1=

=1/A*/h<X8<$E=)k10"NBN!<X8<$L"N/N

] eS",-G;",-N

:(8ZRTW
FC c6

"N/NNw&r"8<$K=&"#9<X8<$/",",y~(kz9{~,bB4H HAbc/=$%?<8<$ ;=& P)k4A'DB

6

A 96 d

 96

",/h4#G;hYX4#15bYBO"#$%=w
] e

D/-

] e]r4#1

DB",4M8O/#",b/4M$U)S4##cYXSO/1B8<=1/N/4A4A=1B"<X8<$=& cb)k/4#8-
B"k"1;G
D/*y<X8<8<=1/NB4A4A=19eRH

Wt 4  hvWv   #wyx  ff5 #/M r 3
Z.=;D/RO/1/4#15k%8-/1B4#x:O/$&(-=Gc8<4A=1'E"$4#A*8",*y=%=P"8<4A=1/$)S4A
8<=1/N/4#4A=1/"c<8<$H

9($^]M$)+VXWYS	Y47RTS

_XP(:Z6aTb0

\=kFISiJf_l.b).?N<691N"$./AD/&rO//"#",D/DB#4M8",YBAS"8<4A=1B$."8R/4AF4#15",.A"$=15="",.4#G

$%%D^,b8<&HLc8<4A=1cH#^,HE\=?=OEjn1/=-G;"&(=-G>b9).$-4#G
DBA*T8-B"15y/4#$k%=|{({|Rzz9{~-|{A

k

9

.k

|r~r,}/c| ~}:h|}w|Rzz/|,~(}c; -c||<~ J|J|,{B|Vr~'.zT^,b)S/"1h<8<J|Rzz|,R
474A$K<X8<k8<=1/N/4#4A=14#$J$",4M$_6BN4#1Th8O1:J$%",%H

 96 d0+N2

96 edf+:  fi

Z+ fi8$T6,

] e

D/-] e

DB

] e

T"N/N

 96 =
a ( Z+ O8
fi 
] e

] e

]e

9($^]M$'&_&>B>9W!>A@aMhTSlN>WYSWe:Z6aTb

?+

-4#54#1/"#A*bfi).J8OU=,T"E$%",%



>	

"
5=" pB",S/"N

9

+

04A&/=1=&9P|r~,}U$%A8<%N
&=M"ficNyDBM"1h%=

0N/A%N

O/$%YX1>"8R/4ANb8<&H.F8<4A=1!cHcH.h1=)Q$4#G
D9A*;",y"$J8<-4A%-4#=1

 ?y/",w",!$%A8<%N&='>-#"ficN2DB#"1bK4H HAb"$%",%4M$;8O'=,[4A&=1=&/!<X8<$

9

	

$%A8<%N!&(=k4A$#"fiFN>$%=MO4A=1NA%$S"
5=" p/",S/"$ %O/$Y1!"8-/4ANVH

W4   'r  x,vx2c3" ouc3
\U4#1/"##*bB&=1B",YB#4#15'$",-8-"A5=-4#/G;$k%=w/"1BN/AP=O/kD/-=D=$-4A4A=1/"Vj?n1=-G;"&=-Gb
4AK4#$.$OF@84A1:.%=
N<691k|Pr%|,}R~rr~,}yRF}9r~r,}BH\=)K",-N$%",-8Rb1=
G;",%%J4A&V4AKN=c$
/4#M78#4#GhYB4#15bYX$%_6B-$y$",-8-b.=;)SB",$%=:b."A)K"*F$;&r"8<$;"38<=G;DBA%A*C$%DX8476BN$%",-8R
$%",%H 4

f8"1
&=8<=G
DBO%<"8<A*K<8<$=&9<F8O4#1/5?"E8<=1:%<FNDX1/N1:"8<4#=1H

\=##=)S4#1/5'E=c/AJ"HM]_^``deRbX)PN/<691L=Ojk?nC$%",%%-"1/$4A4#=1T&O/1B8<4A=1
$%",%$"1/NjnC1=-G'"X&(=-G"8<4A=1/$J%=$%",%$b9"$J&(=##=)S$H

PRQTS Z+:m6 ]8_^ Z+a` Z+:m6 cbQP Z+:m6
]

]

e

y]

e%e

O/1/N<61N

]

e

PRQTS

b9G'",D/DB4#15

96 edf+

4A&DB] e

=)S4#$

)S4A

 Z+:m6 ]8
y]

e

D/-


, + 0SR 

"N/N

 96

] e

"1BN

P]Z+:m6e]8

D/-


, +  0SR 

 96 El

N ] e

TTU)oo;Ti Z"A" ZF)9FZM(AoZWVowI;o;)FZ("eFZTZZAo9Z(o>)MFZ

;"A(

"O

fi 9>~20$7`$22

XU $_nM ff p&  ) f



$##!2F

ff

'/"'4#G
D9AG
1:%N;G
=FN=A=5*0D/$%1:%N34#1/
D/8<N/4#1/5T$8<4A=1/$L4#1

u

UH *

f_1/4#$

$%8<4A=1VbW).;fi"MO/",%''D-&(=-G'"1/8<
=&/;$O/#4#15TDB#"1/1B4#15T$%*c$%GHG
DB4A-4M8"NB",">4#$

qfi

N/4AF4#NNT4#1:%=
h$OYB","$

^,HSISL\U\$%*c$%G%=:=TDB",-J4#1TE&rO/#A*'"O/%=G;",%N>%-"8-=&/P,1/N>4#1:%-1/",4A=1B"XDB#"1F
1B4#15?$%*c$%G;$8<=G
DX4A4A=1Vb8",-4AN=O"A=1/5$4#NK)S4AyjkflK:,L4#1yZ.8-1-4#N5bF.=7
=R"N=HkF4A)h-$O/A$bNG
=1/$%-",4#15w\\.$5=c=cN-O/1:4#G
"1/N!$%=MO4A=10A15
YX/"c4#=4#1T8<=G
DX4A4A=1H>"#$=54AT$%=G
T4#1:O/4A4A=1B$E=1)k*3\U\pYB"$
)K"*>4#JN=:$H
cHS\/=G=Ok=)S10<cDX-4MG
1$b9).hDB$%1:k$%=G
=&$OBA$J/",)./"y=Y/"4#1N04#1
N/=G;"4#1/$B",V)-1=WOB$%NL4#1E/jSf_l.c,S8<=G
DX4A4A=1VH\U4A-$%b)Y/-4A<9*k$O/G'G;",-4A
=O/;691/NB4#15$;4#12$%=G
G;=N=G'"4#1/$
)S/!\U\).=c$).#HvIJ1b.%=4###O/$%-",%T=O
4M1O/4#4A=1/$E=1'"$=1/$P&=h\U\.$DX&(=RG;"1/8<bW).w54A"!&()<F"G
D9A$L=&JN=G'"4#1/$
)kEP",DBD/="8-!4#$JA$$",DBD/=D/-4M",%H
cHJ61/"#A*D/-$%1"N"4##N8<=G;DB",-4#$%=1=&\\.$TDX&=-G;"1/8<0%=/",T=&hmEFlb4#1
/$%1/$;/",P).'4#1:$%4A5",%w)S/4M8-3N/4A1B8<$EYX).1\U\["1/Nm?lQA"N%=!)S/4M8-
DX&=-G;"1B8<E$O/A$H

#x  4 Ov'  /!Y!YYv

 u'+Z3O  &
} 3Yu}&2  "!"u

 2 #w

\=G+",R8-%='jSDB-4#V,Fbh,1/N>4M1%-1B",4A=1/"DBM"1/1/4#15y$%*F$%%G;$k8<=G
DX4A4A=1b=5"1B4AN
Yc*>\/"/4AGZK"88-cO/$bX)K"$8",--4AN>=O/4#1!P51/-"U$%%4#15=&hjkf_l.:,T8<=1&1/8<y4#1
Z.81/-4#N5bJ.=A=-"N/=HCIJ/T)!).=G;"4#1C%-"8-c$b.=1&=;&rO/#A*:"O%=G;",%NDBM"1/1-$b
"1/N=1!&=T/"1/NF"4##=NDBM"1/1-$H[Z.=%-"8F$T).0NB4Ac4MNN4#1:%=36B0DB",$b"8R=1
8<=1/8<-1/NE)S4AE"JNB47-1DBM"1/1/4#15KN=G;"4M1H?OV\U\$%*F$%%Gv%=c=?DB",V4#1?/&OB#A*S"O%=G'",%N
%-"8-9Hf_1w?8<=G
DX4A4A=1bF\U\NG;=1/$%%-",%Nw-O/1:4#G
kYX/"F4A=K$ODX-4A=%=B",.=&W?=
&rO/#A*"O%=G;",4#8'DB#"1/1/-$L"1/N)."$h&=5-"1%Nu-g-=ODCjNB4#$%4#15OB4#$N0DX&=-G;"1/8<
lM"1/1/4#15!c*F$%%G;]rZ."88R:OB$ystok"Ob,^eRHfP"#$%=>).=13/wF8-B4#1/N/ALjk)K",-N3&=L'6B-$%
DB#"8<T4#1w04#8<=1/4M8!^0A,",%=
N=G;"4#1bUjn%R"8H0f_1/4#$P$8<4A=1b).Y/R4A<B*D/$%1:
LN/","'8<=#A8<%N>4#1T/E&O/MA*'"O%=G;",%N!%-"8b9"1BNT54Ab/&="8-0N=G;"4#1b$%=G;L4#1OB4A4A=1/$
=10"$%=1/$k&(=E\U\.$?YX/"F4A=HIJP-"N$-=O/#N!Y")K",
/",y8<=G
DX4A4A=10G;"N
1=N/4M$%4#1/8<4A=1YX).1v=D/4MG;"E"1/N[$O/Y=DB4#G;"kDB#"1/1R$bSDBO/%4#15%=50-O/1:4#G

8O$K&=KYX=5-=ODB$Hf_1wk%<F%=y"8->N/=G;"4#1bc)L$",%E)S/4#8R'DBM"1/1-$&(=OB1/N'=DB4#G;"
$%=#O/4A=1/$b"1/Ny)k/4#8-
NB4#N/1 HlDB#"1/1B4#15S"$9bc"#FDB#"1/1/-$)S54A1;/"A&"1;=ORO/1/1/4#1/5
4#G
h=1"Ta,Tmkyl1:4#O/G+fff.)S4A^g?ZG;"4#1!G;G
=*HSf&1=w$%=#O4A=1!)."$k&=O/1/N>)k4A/4#1
$%P$%=O-8<EYX=O/1BN/$bFEDB#"1B1)."$kN8#",NT%=w/"P&"4##NT=1L$D8<4#E"$%9H

[

$)$)]\TW

U#r-

Naffh96'b

IJ.6BR$%U).=LN=G;"4#1/$/",)O/$N
4#1J8<=G
DX4A4A=1).JMr-"1/N
K(F-r
N=G;"4M1/$HV6B-$%VA=c=?",V&=-G
HIJ/4#$4M$V"8#"$$4M8"N=G;"4#1Vb4#1:=AF4#15K%-"1B$%DX=",4A=1

^ U)o(TZq"iw(FFo ;R(""Foo"ffff;ow;Ri"5o.;"wZ;uZ"fi`_;A(iF
OZba cd5c %%G T FZMFZ "oWe Z""" Tc5fohgffM;"icg OZA

"T

fi

2fffff$i

=&KDB"8$hF4#"T%-O/8-c$L"1BN3"4ADBM"1$H
\U4A5O
!$-=)S$L;-O/1:4#G

8O-$E=&K=$%;DBM"1/1-$
/",J).P",Y9AE%='$8"AP%=;YB4A55S4M1/$%"1/8<$J4M1TL8<=G
DX4A4A=1H

10000
FF
HSP2
System-R
GRT
Mips
STAN

1000

100
sec.

j

10

1

0.1

fi

16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42
problem size

\U4A5OE kiSO/1:4#G
K8O$U=1
#",-5KU#r-4M1/$%"1/8<$U&=U=$%J$47hDB#"1/1R$W/",8<=OB#Ny$8"A
OD%=;GHIJ4#G
L4#$$-=)S1>=1>";A=5",-4#/G;4#8?$8"AH

IJ/L#-K"$%c$S)-L$OYN/4AF4#NN4#1%=
)=$%$J=&U4#1/$%"1B8<$bE"$%*T"1/NLB",-N
=1$HIJ=$%?DB#"1B1-$./",JN/4#Nw).#X=1"#X=&E"$%*T4#1/$%"1B8<$K)h"#$%=y-O/1w=1LB",-N
$%HIJ$%yDB#"1B1-$k);\U\.bVmEFl]rZ.=1Pstg<1b^``cb^```eRbc*F$%%GyiLbgEiIt]riS<
&r"1/4#N/4#$s



#"B","$b^```eRb04ADB$y]rNA,"G
DVb,eRbW"1/N3FIjo]rnV=15>s\=b^```c\=3s

fi

nV=1/5b9,^eRH.IJ)=
=YB$fi",4#=1/$J8"1>YXLG;"N/

^,HF*c$%%G
ipN=c$J$4#51/47698"1:A*;).=-$%LB"1L=SDB#"1/1R$H
cHSISTY%%
D9#"1/1-$h"#.YX/">xcO/4A%T$4#G;4M#",b)S4A\U\p"1/N04ADB$h%1/N/4M15%=YXw
&r"$%%$%H
ok=%;"#$%=T/",E4#G;$",-y$-=)S1=1"TA=5",R4A/G;4#8h$8"AbW$=w).
",}9UA=c=F4#15T",L#4#1",
4#G
TMr-DB#"1/1/-$HK=1/8<-1/4#1/5$%=#O/4A=1D9#"1#15b).TN/=01=y$=)"!695OTH
ok=1?=&E$/=)S1wDB#"1B1-$5O/",-"1:%$J?OR1NwDB#"1/$%=
YXk=D/4MG;"HUfKOR1/$.=OK/",
FIjkov691/N/$?$=%$%KDBM"1/$=1TG
=$K4M1/$%"1/8<$Hc*F$%%Gyi61/N/$.$4A51/4768"1A*;#=15D9#"1/$
/"1/;=-$b-"154#1/5&(=G

bk

lk

k

^d, %=0F^ =&SFIjoh$EDB#"13#15/$bV)k4A"1"-",5=&

ik

fi wH.IJLA15B$=&\U\.$JD9#"1/$",L)k4A/4#1`d

lk

lk

%=^^a =&FIjkoh$JDBM"1A15/$b)k4A"1

"R",5'A15=&S^a wHh.=1B8<-1/4#15\\.$?5=c=cN0-OB14#G;PYX/"F4A=b).h/4M1/",
",

Ofi

G;"4#1BA*')=;-"$%=1/$J&=J/",

"

fi 9>~20$7`$22



$##!2F

^,HJf_1'"M/4A%-",4A=1B$=&1&=-8<N'B4##78#4MGPYB4M15bYB"N
69-$%$%",-8R;61/N/$"h$%",%k)k4A
YX%%
,"#O/",4#=1",K*'$-G;"#BN/D//$?]rG
=4A,",4#15P=O/.$%",R8-"A5=-4#/Gb:8<&Hc8<4A=1TacH#^eRHf_1
G;=$%S8"$%$bBL1<FY%%k$O/88<$$%=k4#$",SND/^,b94H HAb/"
N/4A8<=1/HIJE",h$%=G

8"$$)S-J$/"##=).$%YX%%$O/88<$$=4#$",N/D/;cb:"1/N;=1/A*P-*h-",-A*hYB"N
69-$%J1N/$J%=;5='N=)S1%='NDB!cHIJ$E=YB$%,",4A=1/$J",-P4#1/NDX1/N/1.=&"$T$-4AH
cHSIS;AD/&rO/U"8<4A=1/$hO-4M$%4#8D/-O/1/$E#",5;&-"8<4A=1/$L=&./'$%",-8R$%DB"8<HwnV=c=c4M15>",
/k$%",%$./",K\U\1/8<=O/1:%-$.N/O-4#1/5L$%",-8-Vb:=1/#*YX)1h"1BNaEDX-8<1:=&V"M/=&V"
$",%$S$O/88<$-$%=-$).E8<=1/$4#N/NTAD/&rO/4#1w=O<cDX-4MG
1$b:)S4#T?%1/N1/8<*/",
/P#",5J/E"$%9b9LA$$SAD/&rO/$O/88<$$%=R$Jh",H
IJ/4#$"3=-4#8"k1/=%%=CYX!G;"N=12!6B-$%=YB$%,",4A=1Hv4A208<=G'G
=1



D/-$%1",4#=1P=&BU#r-W"$%F$b&=#A=)S4#15k8"1PYXDB=1HUnV YX.G;"fiF4MG;"N/4M$%"1/8<
YX).1).=#=c8",4A=1B$b4H HAb/L1cO/GhYK=&UG
="8<4A=1/$J"
G;=YB4#A?1N/$%=
",h%=
5J&=G
=1KA=F8",4A=1P%=?"1=H$4#15k"kO/-4#$%4#8&O/1B8<4A=1L/","$$4A51B$W%="8-
$%",%.KA15h=&/"1
Rzr~;|,{c#"fiFN$%=MO4A=1w"$/O/-4#$%4#8Jfi"#O/b:?N/4#$"1/8<k=&W"8-$%",%?%=y?1<c.YX%%

l 

,"#O/",%NP$",%.4#$WG;"fiF4MG;"

^,HIJcO/$b"1h"A5=-4ABG[/",WOB$%NL1&=-8<Nh/4##78M4#GPY94#15)S4#L"1

=-"8A?&rO/1/8<4A=1'O-1B4#15LEA1/5'=&W"1w=D/4MG;"B-#"ficNT$%=MO4A=1;).=O/#NYXkDX=A*F1=G;4M"=1

/Hf1w?YX1/8-BG;",F$"fi"4#M",YBAb

$%"1/NB",-NT#--D/$%1:",4A=1/$bF54A1"1O/D/DX#4#G'4A%=

	

G
=YB4MA$8"1y-"8-
"1:*hA=F8",4A=1y"88<$$4AY9AK%=/G4#1

O/$%=1$%%DVb4H HAbfi/KG'"fiF4#G'":N/4M$%"1/8<

"8^,HLjk#$=bX\U\.$EOR4#$%4#8LOB$O/"#A*3S61/N>=D/4MG;"b=L8A=$%

4#1=$%y"$%F$?4#$?8<=1/$%"1:A*

%=w=D/4#G;"b9#"fiFN$=#O4A=1/$Sb$O/8R!B",k1&=-8<N0/4##A8#4#GhYB4#15y"#G
=$%1E1N/$k%=

"  =^ 8vy$%DB$S""NVH

A=c=wG;=E/"1

[

$)$'&\TW

K(F-r

NaffhT6"b

IJK(FcU-4#$=1k=&/kYX$%F1=)S1'YX1/8R/G;",yDBM"1/1/4#15LN/=G;"4#1/$b:)SSkD9#"1/1



1N/$.%=y",R"15E"hY9O/1/8-=&WY9A=c8-c$K4#1:%=y"$D84A6BN'5="XDX=$4#4A=1b:OB$4#15"P=YX=",-GH

O/$%

#4AhhU#r-k"$%F$bXh8<=G;D4#4A=14M1/$%"1/8<$k)-N/4#c4#N/N4#1%=T"'$k=&"$-4AbX"1/N!=&
/",-N/=1/$HP\U4A5O
aw$=)S$?y-O/1:4#G
y8O$?=&yDB#"1/1R$J/",L$8"#N%=/yB",-N
=1$H
c*F$%%Gyi+$8"A$;G;=$%;$%%"N/4##*%=3>K(F-r3"$%F$;O/$N4#1C>8<=G
DX4A4A=1VHCf_1
DB",4M8O/#",b4A4#$KE=1BA*D9#"1/1K/",J8"1$%=A'|,{{:=&U=$E"$%c$Hm?ly$%=A$S$%=G
L=&
$G;"MA4#1/$"1/8<$bF"1/N'\U\$=A$",YX=O).=hB4A-N/$=&/k$%Hf&V\U\3$O/88<N/$=1w"14#1B$%"1/8<b

l

14ASN=c$S$%=wx:OB4A%L&r"$%H\=k<"G
DB#b9\U\$%=A$S=1P=&/P$4A<a,"$%F$k4#1^ d;$%8<=1BN/$b

l

)S-LF*c$%%G
iQ1N/$J` F^h$%8<=1/N/$HoS=1E=&E-EDB#"1/1R$691BN/$.=D/4#G;"XDBM"1/$H?1

ik

ik

J"$c$/",mEFlLG;"1/",5$%=h$%=Abc4A$D9#"1/$",k)S4A/4#1
`d %=^dd =&Wc*c$%GyiL$DB#"1

lk

A15B$b)k4A"1C"-",5T=&L^a wH?13/'"$%F$PB",\U\[G;"1B",5$h%=$%=Ab4A$PDB#"1/$h",

lk

lk

)S4AB4#1

lk

%=!^ =&F*c$%%G
iL$DBM"1A15/$b/"-",5
` wH

Z.*C<cDX-4#G;14#1/5!)S4AN/4A1:y8<=1F6B5O-",4#=1/$y=&k\U\.b)&=O/1/N/",
YX/"c4#==&
\U\u=1$>"$%F$'4#$;M",5A*N/O%=/>5="J=-N/-4#15OR4#$%4#8$y&=G
N/4#$"1/8<T$%4#G;",%$;",-T1/=;$%=5=:=FN
$4#1/5A
",-G





c8<4#=1cHcH[g="

TDBM"1/1y5-",YB$
"0)S=#wY9O/1/8-C=&kYBA=F8F$y)S4A4A$

"1/N14AP4M$E'#D/&OB"8<4#=1/$PO/-4#$%4#8



)S1'",-G

=#N/$E"Y9A=c8-9b"#

DX=$4A4A=1B$S)S/y
",-G8<=O/#N0DX=$$4AY9A*TDBO
YBA=F8!",-;O/$O/"##*>8<=1/$4#N-N#D/&OBHEIJ
5="",51/N/"]c8<4#=1cHcHeRbW=1/h=LB"1/NbXN/4AF4#N$J/h"$%F$?4#1:%=T$-G;"#W$-OY/"$%F$b"1/N
"N/NNh5="cNA4A=1T]c8<4A=1ycHeWD/1:$/DB#"1B1W&=GpDBO%4#15SYBA=F8F$V=1:%=?$"8F$)k

"

fi

2fffff$i

10000
FF
HSP2
System-R
1000

100

sec.

j

10

1

0.1

0.01

fi

18

20

22

24

26

28

30

32
34
36
problem size

38

40

42

44

46

48

50

\U4A5OPa kiSO/1:4#G
.8O$=1hM",5.K(F-rE4#1/$%"1B8<$W&=/=$%/D9#"1/1-$W/",U8<=O/#N

fi

$8"AKODh%=G

\U\bmEFlcb"1/N
c*F$%%GyiPHfiIJ4MG
4#$$=)S1"1"A=5",-4#/G;4#8$-8"AH

$%=G
YBA=F8YX1",$4##1N/$P%=!YX'G
=NH>mk=).b4#1$=G
'8"$%$y"8-B4Ac4M15>5="#$
&=Gt",-M4AJ1:%-4A$J4#1>L5="",51/N/"'8O/$J=,5="#$k/",k",h$%4##""NVH.ok=k").",=&U
YBA=F8F$/",4AU)S4#M1Ny%=L$%"8-h&="8-B4Ac4M15?5="#$""Nb/.D9#"1/1G;4A5:UDBO8O1:
YBA=F8F$?=1:%=>$%"8F$L/",E1N%=YX
N/4#$-"$$%GhYBAN0#",%L=1Hhf&/",E/",D/DX1/$)S4A0%=c=>G;"1*
YBA=F8F$



)k/4#8-wNDX1/N/$.G
=?=JA$-$R"1/N=G;A*'=1/L$%DX847698J"$w"1/NwE"8<4A=1B$K/",

DB#"1B18R=c=$%$



13
DB#"1/1/8"11=?61/N4A$)."*=OE=&;$4AOB",4A=1",5"4#1HhIS$%

",LDB=YB",YBA*'P4#1/$"1/8<$/",S\U\8<=O/MN/1 $%=AP4M1TP8<=G
DX4A4A=1VH

[

$)$^]m\TW3n po +q Naff1hT6'b
 9 

f_1;

n r o9sq(KN=G'"4#1bkDB#"1/1/4#$&r"84#15"LYBOB1/8-'=&=Y7	_8<$%=YS)=N=1)S4A;"h$%

1	
7	

=&UG;"8-/4M1$b4H HAbEDBM"1/14#$.xcO/4ANw%='8<",%h"

	

_=Y>$8RN/O/#?4M1)k/4#8-T/=Y _8<$k$/"#

YXS"$$4#51N'%=G;"8-/4M1$HIJ8<=G
DX4A4A=1;-D/$%1:",4A=1wG;",$KO/$%S=&V"$4#G;DBA&=-G=&
xcO/"14A6BNP8<=1/NB4A4A=1/"<8<$H\/=<"G
DB#b,4#&/"1P=Y %8<U5$DB"4#1:%NPNVbfi/1hB",4#$4A$1)
8<=A=b"1/N&=K"M98<=A=R$B",K4#4#$.8O1:A*
DB"4M1%NT4#1bc4AK4#$1/==&VB",K8<=#=K"1:*FG
=H.?1BA*
"0$OYB$%P=&D9#"1/1-$P4#13w8<=G
DX4A4A=18<=O/#NB"1/N/A;B4#$LF4#1/N=&J8<=1/N/4A4A=1B"<8<$H
IJ4#-O/1:4#G
E8O$J",-P$=)S1>4#1\U4A5O-PcH
jkDB",&(-=G

=$DBM"1/1-$h"A"N*$1bU)/"wRO/14MG
'8O$y4#1C\U4A5OT>&=fll

&=t #MN=YBAysc=t -b,eRH\\Q=O%DX%

]rE=c/A"HAbJ^``deRbl=D9l#"1b"1/NCZK?l#"1]rm

&=-G;$k=/.D9#"1/1-$Y:*'G'"1*;=RN-$=&WG;",51/4AOBN



G
GhYX/",.4#G
4#$.$=)k1'=1T"

A=5",-4#/G;4#8$8"AH.=1/8<-1B4#15?$%=#O4A=1A15Vb\\.$UDBM"1/$%1/N%=EYK$#4#5A*L#=15U/"1y





fi 9>~20$7`$22



$##!2F

10000
FF
HSP2
Mips
IPP
PropPlan
BDDPlan

1000

100

sec.

j

10

1

0.1

0.01

fi

2

4

6

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50
problem size

\U4A5OP kiSO/1:4#G
P8O/$J=1

n r o+q(4#1B$%"1/8<$J&==$%PD9#"1/1-$KB",8<=O/MN>/"1/N/#L8<=1F

N/4A4A=1/"<X8<$HIS4#G
L4#$$/=)S1=1!";A=5",-4ABG;4#8?$8"AH

DB#"1B$-O-1NwYc*'E=SDB#"1/1R$=1TL$-G;"#A4#1/$"1/8<$HD/4MG;"BD9#"1/$K",E&=O/1/NwY:*

lk

04ADB$bFl=D9l#"1bF"1/NZK??l#"1H\U\.$DBM"1w#15/$K",?)S4A/4M1^d,a =&?=D/4#G;"XA1/5/$b

lk

)S4A
"1'"-",5?=&U^^ wH?1/A*m?l$%=G
4MG
$U691/NB$A=15DBM"1/$U/"1;\U\.bYX4#15E4#1
"L-"15

lk

&=G

ik

%=^^d

bk

=&\U\$SDB#"1A1/5/$bB`fi

=1!"-",5H

iS$%DX=1/$4AYB#w&='/>=O$%"1/NB4#15-O/1:4#G
YX/"c4#=;=&L\U\u=1

n r o9sq(wN=G;"4M14#$b

",D/DB",-1A*bBLAD/&rO/X"8<4A=1/$kO-4M$%4#8,H!"$-O-4#15b&=k$=G
L<"G
DBAL$%",%$bBLDX-8<1:_
",5y=&$OB88<$$%=-$/",?).8<=1B$4#NN0ADB&O/b/)
O/$-O/"#A*T&=O/1/N04AS)K"$E8A=$%%=>'DX-8<1:b
4H HAb=1/A*)=T=O=&"cO/1/N/N!",D/D9#4#8",YBAh"8<4A=1/$k)y8<=1/$4MNNY:*DB#"1B1HS\=?<c
"G
DB#b:"#=&X?dE$%",%$./",.\U\3A=c=c$",&(=.$%=AF4#15E=1k=&Xk$4#<a,L"$%F$B"a,,
$O/88<$-$%=-$J"A%=5/bB)S/EL$O/G

=&U"#F{ zfiRF{$O/88<$$=-$S4#$K=1BA*Td,cH.jk#$=b/EYX%%

$O/88<$-$%=-$bc$4#G;4#M",%=hSU#r-N=G;"4#1b:#4A",$B"#A=)ND//$HUZ."N;6B-$%$%",R8-w1
5=c$
NDXh/"1C-w$%%D9$h=1C

n p o9+qy"$%F$y4#1/w8<=G
DX4A4#=1$OB4A%H\4M1/"#A*bU4#1

"T&)<cDX-4MG
1$).;-"1&=E%$%4#15B",bW
5="",51BN/"ADXN0Y:*",YX=OE"T&"8<%=yT4#1
%-G;$J=&RO/1/1/4#1/5P4#G
H

[

$)$#u\TW(v

IJ

v

<#

NaffhT6"b

#N=G;"4#1&=-G;"M4A$J";$%=M4A"4AL8",-N>5"G
P/",8<=G
$k)S4A04#8<=$%=&S4#1/N=)S$H

IJw#",5$%h"$%F$h1:%NC4#13/w8<=G
DX4A4#=1]r$4A0^!4M1\U4A5OdeE8<=-$%DX=1/NN/4#8<A*%=
$%=G
S"7).=-#N;$4#N
"$%F$b)SB4#A4#1yS$-G;"#A"$%F$b:S",kA$$8",-NB$%=PYXJ8<=1/$4MNNH
\U4A5Ohd
$=)k$E-O/1:4#G
E8O-$J=&UE&=OJYX$%DX&=-G;4#15DB#"1B1-$H



fi

2fffff$i

10000
FF
HSP2
Mips
STAN
1000

100

sec.

j

10

1

0.1

0.01

fi

2

3

4

\U4A5Ohd kiSO/1:4#G
8O-$V=1

v

5

6

7

8
9
problem size

10

11

12

13

<#"$%F$V&=W/=$%DB#"1B1-$X/",W$-8"ANE%=SY94A55W4#1/$"1/8<$H

IJ4#G
E4#$J$-=)S1>=1>";A=5",-4#/G;4#8?$8"AH

\-=G5=O/D
=&9&=OYX$%_$-8"#4#15?DB#"1B1-$U$=)S1
4#1
\U4A5OSd:bcm?Fl.?4#$UJ$#=).$%b
)S/4MAKFIjo24#$&r"$%%$%DBM"1/1H\\04#$51-"MA*h$%8<=1BNyDB#"8<b"1/Ny/"$"E#==&9fi",R4#",4A=1y4#1
4A$K-OB1/1/4#15P4#G
$H?1wE=/J/"1/Nb\U\4M$K?=1/A*;DBM"1/1K/",4#$K8",DB",YBAE=&$%=AF4#15
"7).=-#N!"$%F$b$-4A^cHJf?$%=A$k&(=Ok=OS=&6Bh$OB8-"$%c$HSoS=1/P=&y$=)S1>DBM"1/1-$
5O/",-"1:%$'&=O/1/N3DB#"1/$P%=!YX;=D/4#G'"b"1/NC1=1'=&$/=)S1DB#"1/1R$ENG
=1/$%%R",%$
$ODX-4#=.DX&=-G;"1B8<L8<=1/8<-1/4#1/5y$=#O4A=1A1/5HFIjkopD/=FN/O/8<$O/1B18<$$",-4MA*A=1/5D9#"1/$
4#1"y&)[8"$%$H.l84#$A*bF=1E"$%F$/",JYX=mEFl"1/N\U\G;"1B",5L%=;$%=AbBmEFlS$KDB#"1

bk

lk

A15B$P",;)k4A/4#1"!-"15'=&Sd q%=^

lk

=&\U\$hDB#"1A15/$bU"R",5`a TH>E1"$%F$

lk

lk

$%=ANwYc*;YX=04ADB$"1/NT\U\bFDBM"1'A15B$=&W04ADB$.",)S4A/4M1` %=^ =&W\U\$KA1/5/$b

k

lk

"R",5w^^ wH.\=?IjoPb/L-"15P4M$Ja

lk

lk

%=F^ wb9)S4A^^ =1!"R",5H

.=1B8<-1/4#15L\U\.$-O/1:4#G
YX/"F4A=bSYB4#5Efi",-4M",4A=1'4#1;-O/1/1/4M15k4MG
S"$)#B"$4A$8",DB"fi
YB4#M4A*L=&$%=AF4#15E#",5"$c$YX=
$%G%=E$O/AU&=Gu)K"*=R"#/$%",-8R;"A5=-4#/G4#$
",-"1/5NH?=YB$%NT&=#A=)k4#15HIJ=$%"$%F$K/",K5$%=ANwY:*'1&(=R8<N/4##78M4#GPY94#15
",E=$B",J",L$%=ANT&r"$%H.c=G;4#G
$b=)bB$%DX84#"#A*
=1TLM",5"$%F$b1&=-8<N
/4#M78#4#GhYB4#15-O/1B$k4#1:%=T"N"N01/N0$4AOB",4A=1C]r1/=w8",-NB$8"10YXhG;=N"1:*cG
=-eRHLIJ1b
DB#"1B1;$%",$'&(=G

$8<-",8-2)S4A8<=G;DBA%>YX$%_6BR$%
$%",-8-Vb)S/4#8R",$G
=->4#G
b.YBO

8"1C$%=A;YB4#5!4#1/$%"1B8<$PxcO/4A%;M4#",YBA*bW"$8"13YX'$1=1;"$%F$P=&J$-4A!^cH>mkAD/&rO/"8

	k

4A=1/$S)=-c$G
=FN-",%A*T).#b9$A8<4#15'",-=O/1/N0dfi t=&","4##",YBAh"8<4A=1/$bX"1/N>/PYX%%
$O/88<$-$%=-$S",-LO/$O/"##*'8A=$%b/Y9OJ$%=G
4#G;$J#4AL",SN/D//$K=&G
=E/"1a
$%%DB$H





fi 9>~20$7`$22

[ $)$xw\TW



$##!2F

Naffh96'b

-y

0r /r

IJ691/"N=G'"4#1LO/$%NP4#1E.8<=G
DX4A4A=1L8<=G
$&=GQ"S"7).=-#NL",D/D9#4#8",4A=1b)SG;=F4#15
$%xcO1/8<$U=&A,",%=-$1Ny%=EYXKDB#"1/1NVHWIJ$%xcO1/8<$",JNBO%=L"M:F4#1/NB$=&9$%%-4M8<4A=1/$b
#4A/",U



f_l$1N%=?Y.$%Nh6B-$HI=&=-GO/#",%."#=&B$%K$%%-4M8<4A=1/$b,8<=G
DBA<P6B-$%

=-N?D/-8<=1/N/4A4A=1B$k",;OB$%N4#10yD/$%1:",4A=1]rE=c/ALstF8R:OB$%%bW,eRHyjk$?=1/#*"
&)2DB#"1B1-$8<=O/MN;/"1/N/#JJ&rO/#j?n-D/$%1:",4A=1bckN/=G;"4#1
)K"$$-OYN/4AF4#NN4#1%=h
"$4AhIJiJf_l.>"1/Ncf_lnW]r8<=1/N/4A4#=1/"<8<$Re8M"$$%$by&rO/#jk?n8#"$-$bV"1/N"101
G
=S<cD/-$$4AJ8#"$-$)SJ1cO/G
R4#8"8<=1/$%%-"4M1$]S1cO/GPYX=&XDB"$$15-$4#1y/JA,",%=
",y"!4#G;eE1NN%=YX'8<=1/$4#N/NH>$=)/'-O/1:4#G
w8O$h&(=wDB",4#84#DB"1$h4#1

-y

h&O/Mjk?nC8#"$$4#1>\U4A5OcH.f_1>N/4A1B8<L%='hD/F4A=O/$JN/=G;"4#1/$bBP0r /rkN/=G;"4#1
)K"$-O/1=1$4A%E",JjkflK:,Fb9O/$-4#15Pa,'mkLl14#OBG

fffG;"8R/4#1$K)S4#Tay0Z.*:%hG;"4#1

G
G
=-*H

10000
FF
IPP
PropPlan
1000

100

sec.

j

10

1

0.1

0.01

fi

1

3

5

7

9

11

13

15
17
problem size

19

21

23

25

27

29

{z(-|~}FK"$%c$K&=.=$%?DB#"1/1/-$)SB4#8-w/"1/N/#N;&OB#Bj?n

\U4A5OP kiSO/1:4#G
8O$.=1

4#8<=1B4#8
^;N=G;"4M1TD/$%1:",4A=1HIJ4#G
L4#$$-=)S1>=1>";A=5",-4#/G;4#8?$8"AH

\U\=O%DX&=-G;$T).=3=;&rO/#jk?nQDB#"1/1R$h4#1C%-G'$y=&$%=MO4A=1C4#G
Hf
GO/$%
YX1=4M8<Nb=).b/",fllu"1/Nl=DBlM"1351-",%TD/-=,",YBA*=D/4#G;"DB#"1/$hb$O/8R
/",L=1/;1N/$E%=YX;8",&rO/U)S1NB4A8<A*08<=G
DB",-4#1/5w/=$%;-O/1B1/4#154#G
$H'?1'=
/"1/NVb\U\.$EDB#"1B$k",yxcO/4A%y8A=$%y%=T=D/4#G;"=10$%
4#1/$"1/8<$bXYX4#15')k4A/4#14#1"w-"1/5=&

lk

k

G;"fi4#G;"##*w^ =&=D/4#G'"B$%=#O/4A=1'A1/5/$%=y4#1/$%"1/8<$.$%=ANYc*;l=DBl#"1b^^^
=1>"-",5H

IJ/;#",5
,",-4#",4A=1=&K\U\$P-O/1/1B4#15'4MG
$E4#$E",D/DB",-1A*N/Oy%=>/;$"G

DB1/=G
1=1
"$fi",-4M",4A=14M1

v

Ofi

<#U4#$ J$%=G
4MG
$bX"$?)=YB$%NVb1/&(=-8<N/4M#78#4#GhYB4#1/5h-OB1/$S4#1:%=

;

fi

2fffff$i

"TN"N1/Nb)k/4#8-08"O/$%$P"w$)S4A8-0%=YX$%_6BR$%$",-8-b$%=AF4#15'/
"$%4#1G
=-4MG
bYBO

lk

#4M",YBA*HUIJ?AD/&rO//"8<4A=1B$DX-8<1:",5E",$*A=),"#O$=1w"-",5b9",-=O/1/N0^a wb/"1/N

\	

Y/"N/6B-$k$%",-8R!R",A*5=c$N/Dk/"1&(=Ok=S69$%%D9$bB)S/P#",5yG;" %=-4A*=&
LYX%%S$-O/88<$$%=-$k#4AE",SNDB^,H

 / 'gu}


gn2}&2  3

}u2v 

f_1T/4#$J$%8<4#=1b)PD/$%1:J$%=G
L=&E$-O/A$K/",J).P/"L=Y/"4#1/N>4#1N=G;"4M1/$K/",).
1=O/$%N34#13'jkflK:,8<=G;D4#4A=1Hw'54Aw$%=G
G;=;<"G
DBA$P=&KN/=G;"4#1/$E)k
\U\C)=F$).#b"1/NbF%=;4#M#O/$%%-",%=OJ4#1:O/4A4#=1/$=1>E"$%=1/$K&=S\U\.$YX/"F4A=b"#$=;$%=G

<"G
DBA$=&UN=G;"4M1/$)S-?\U\4#$SA$$J",D/DB=D/-4#",%H
\=,"#O/",4#=1b:).-"1T\\=1T"h8<=##8<4A=1w=&,Y1B8-/G;",-
DB#"1/1/4M15LN=G;"4#1B$b:4#1B8#O/N/4#1/5
"#:N=G;"4#1/$V&(=Gp.jSf_l.:-^``k"1/NjSf_l.c,k8<=G
DX4A4#=1/$b,"1/NP$%1yG
=-N=G;"4M1/$&=G
E#4A%R",OHl84M$%A*bcEN=G;"4#1/$.4#1=O$O/4A%).Ekw7Xbc).=;K(FcU-:$]/<

d }
q MrF Rb v Mb v -cb.-r9bK-=
=}0y9fibVMr-bW0r-y/%jk?nbV0r-y/%cf0lnJbX-y/r%FIJiSflKbXl|(bw-#b
>R%Xb n p9
o +q bJAfi-rBb"1/NUH!f_1/$%"1B8<$h).w4A",1&(-=G DBO/YB#4#$/NN/4#$_
"1/N&=O%=DX-",%=EDB$%1:",4A=19eRbW.-(  ,Rfi-rBbV

Rb

%-4AY9O4A=1/$b&=G
#4A%-",O-b=LG;=cN/4A6BN!%=$/=)$8"#4M15YX/"c4#=H 3

IJ4#G
$k&=L\\).

G
"$O/N0=10">cD9",-8hA%-"0^T-O/1B1/4#15'",La,0mSb)k4A"TG;"4M10G;G
=*>=&Ka0Z.*:%$H
iSOB1/1/4#15?4#G;$/",).k$-=)&(==D9#"1/1-$U).J",1w=1'S$"G
G;"8-B4#1b:4#&1==F
)S4M$%P4#1/NB4#8",%N!4#1!P%<FHS&=O/1/N>B",\U\$/=)S$k<c%-G
A*>8<=G
DX4A4ADX&=-G;"1/8<
=1^;=&y,wN=G;"4M1/$S#4M$%%N",YX=HE?1).=.Fc-rc$b9TX-#wb9"1/N>R%XbX4A
$%4#MW$=)S$$",4#$%&*c4M15'YX/"c4A=H?c=G
<"G
DBA$k/",E/"
1=YX1OB$%N4M1>yjSf_l.c,

fi

8<=G
DX4A4A=1!",



IS
kw7=G;"4#1Hy\U\$%=A$Paw=&.',T"$%F$L4#10;jkflK:-^``T%$%L$-O/4A%
4#1
#$$U/"16BJ$%8<=1BN/$b)S/KK6BK=-$",-4AO/1/$=Afi",Y9Ab,=/"S$D84A698",4A=1
-=-$HIJS=1/A*=DBM"1/1).SF1=)=&B",8"1'$=AS"1:*
=&/Sk-Rwy"$c$.4#$
f_llHIJE#",%$.R$4A=1Tf_llHy$%=#$K=1/A*;&=OK=&WE-*$G;"M94#1B$%"1/8<$bF",F4#15
OD
%=^w=OR$kRO/1/1/4#1/5
4#G
HE\U\$LDB#"1!#15/$?",bV4#1%-G;$=&
1cO/GhY=&"8<4A=1B$b

	k

lk

$-=%J/"1>fllJ$4#G;P$%%D>=D/4MG;"X=1$b/R"154#15
&=G`, %=T` wH

d }



IS
R(  ,fiU-=G;"4M1HPIJ/4M$k4#$?"8#"$-$4#8"UN=G;"4#1Vb)k

 	

=Y _8<$E1N%=YX

%R"1/$%DX=%NO/$4#1/5"TY/-4#&8"$%HE1PyY/-4A&r8"$%y4#$?G
=Nb"w8<=1BN/4A4A=1/"<X8<

7	

&=-8<$E"#=Y %8<$E4#1/$4MNLY/-4A&r8"$%%=G
=
)S4#4AHE\-=G=O$-O/4A%bXfllHw"$4##*
$=A$h"$c$)S4A

 H

7	

Qa;=Y %8<$bXYBOk&"4M#$J%=$%=Ay"1:*>"$%>)S/

7	

 L

d:H?\\.b=1

/h=E/"1BNb$=A$1/w^^=Y %8<$E"$%F$?4#10A$$k/"1"T$%8<=1BNHEE1!h"$%F$

bk

k

lk

B",.f_ll$=A$bFDB#"1wA15/$.=&W\U\C",?)S4A/4M1fi %=^^^ =&Wf_llS$.#15/$b`` =1
"-",5H



ISK-r3=G;"4#1H3IJ/0^``8<=G
DX4A4#=1&",ONC69T4M1/$%"1/8<$H3\/=
$%T"$%F$b
/;&"$%$%LDB#"1B1/4#15wG
8-/"1B4#$G).
c1/=)=&K&=G'#4#%-",O
4#$E"R$4A=1=&Sg?iUI
B",K4M$1/B"1/8<Nw)S4Aw"$-4#G
DBASc4#1BN;=&N/=G;"4#1wNDX1/N1:F1=)S#N5b$OD/DBM4AN
Yc*;

qCVo?F ;T( o9Fo ;oFZ;Z F;?T(M;"gmZaw(FFoa;&(ooaFe
o"ffC)U o("FZFZ"Z(Z"M ZgTZ"Z;o;(T);o"(AwmZ(9FZ;mZJ;Aw;;;"JF
OZba cd5c %%G T FZMFZ "oWeZ""" Tc5fohgffM;"icgie!T(M;" OZA





fi 9>~20$7`$22

l



$##!2F

l l l
pl pl l l

l

OB$%HfS$%=#$JL"$c$4#1^ ,b cbXF^ acbU^` `'"1BN^^ a$%8<=1/N/$S=1!"',mk



l1:4#O/G.A-=1hG;"8R/4#1.)S4AfiEZ.*c%JG;"4#1G
G
=*']riS&"1/4MN/4#$Us
\U\C$%=A$.E$"G
"$%F$K)S4A/4M1; #^acb/

l

#"/"fi"$bc,eRH

:d:b9 #^^,bV^ `y"1/N0^` afi;$%8<=1/N/$bc-$%DX8<4AA*H

lk

lk

lk

l#"1>A1/5/$K=&\\",-L)S4A/4M1w` %=!^` =&g?iUIE$kA15/$bV^^ t=1>"-",5H



IS!.-/R?=G;"4#1bWOB$%N4#10>^``8<=G
DX4A4A=1VHPIJ
1cO/GPYX?=&K$%",%$E/",L\U\
,"#O/",%$hYX&(=-
O-1/4M15"1=D/4#G;"$%x:O/14#"$%=#O4A=14M$
{ ~}-|,L4#1'$-4A;=&K

pl

"$w/HIJEY94A55$%SjkflK:-^``y<"G
DBAE5$$%=AN!4#1 #^;$%8<=1BN/$H



IS
Afi-rh=G;"4#1VHIJ=-4A54#1/"B"$%;&=-GO/#",%NwY:*FO/",KiSO/$$9"$%F$.DB#"1F
1/%=P691/N;=O/=)%=hDB#"8<"E9",4AHE=c/A"1BNmk=,XG'"1/1],"e.G
=FN/4769N

"$>$O/8->/",S"1!",-YB4A%-",*T1cO/GPYX=&



4A$S1N>%=;YXED9#"8<NHf_ll.cHcbBO/$4M15


pl pl
 L

l

5="",51/N/"L%8-/1/4Mx:Obc$%=A$P^,bcb:"1/Nw4AJ"$%F$4M1; cbc F^,b"1/N!^ P$%8<=1BN/$b
-$%DX8<4AA*bYBOh<FB"O/$%$yG
G
=*$%=O-8<$y"$y$%=c=1"$
M",5J"$%F$b/",F4#15;A$-$/"1>"
%1:=&"
$%8<=1/N!)S1

pl

H\U\p$8"A$%=GhO/8R

 H

cb9$%4##X$%=AF4#15y;^fi4#

"$>4#1> $%8<=1/N/$H\U\.$SD9#"1>A15B$J",b=1>L"$c$k/",Sf_llG;"1/",5$k%=$=Ab
xcO/"##*A=15;4M1T%-G;$J=&P1cO/GhYX=&"8<4A=1B$H

\	

j$)K"$"A-"N*;$"4#Nb=O4#1:O/4A4#=1;4#$/",kG'" _=-4#*
=&S8O/1:A*
","4##",YBASY1B8-/G;",-
DB#"1B1/4#15!N/=G;"4#1/$



",A"$/=$%D/$1%NCYc*=O
N=G;"4M1C8<=MA8<4A=1



",-%$-4#G
DBA4#1

$%%-OB8<Ob"1/N3B",4A4#$LB4#$h$-4#G
DB#4M84A*)S/4#8RG;",$G$%=A,",YBA$%=!"$4MA*Y:*3"5N*
"A5=-4#/G$-O/8-w"$K\\.HI=y4M##O/$%%R",%J=OK4#1OB4A4A=1/$b)E1=)54AEN/","h&=K"h&)N/=G;"4#1/$/",
/"h"
A$$S$4#G
DB#$%%RO/8<OHIS*",P&=P8-B"#A154#1/5&=S\U\.H

$'&($)]\TW
[

>R_

h9b=>

TX-#

NaffhT6"b0

IJ/R_"1/Nw-#TN/=G;"4#1/$
).0O/$%N24M1!jkf_l.:-^``8<=G
DX4A4A=1HvZ.=",
,",-4#",4A=1/$=&.
#r-EN=G;"4#1Vb)ky
",;"N/NB4A4A=1/"8<=1/$%%-"4M1$?=1;8",D9"84A*
=&."8-/4M8AbV"1/NbW4M1D9",4#8O/#",b=1/;"G
=O/1:E=&&rO/",L4#$E","4##",Y9AHhZ.=3N=G'"4#1/$
",E8A=$%#*'#",%NVbc/=1BA*'N/47X1/8<?YX4#15h/",J4#1TTX-#bc&rOX4A%G;$K8"1YXk%R"1/$%DX=%N
YX).1y).=LA=F8",4A=1/$b4A&B=1K=&B=$%/"$UG
=-.B"1y=1$O/8-y4A%G>Hf_1y\4#5OJ`cb)8<=G
D9",
\U\.$J$OBA$=1YX=>N=G'"4#1/$K%=;/",SDX=%NYc*T)p8-G
=%&=JP1DX=D>$%*F$%%G
]r08kRG
=%b^```eRH

-?

f_1/$%"1/8<$",JJ$"G;&(=YX=;N/=G;"4#1/$U4#1
\U4A5O-S`cHUiS$O/A$U&=

},zFRz
/"kYX1y",1

Yc*08-G
=%=1;"L,PmkSl14MO/Gyff)=F$%",4A=1]r8-G
=%bX^```eRHjN/"$
4#1BN/4#8",%$
/",J/L"$%T8<=O/#N/1 YXL$%=#NY:*wP8<=-$%DX=1/N/4#1/5hDBM"1/1H

u

4#1

fi

?1/.1NB$W%=?YX8",&rO/c)S1h8<=G
DB",-4M15k/-OB1/1/4#154MG
$U4#1P\U4A5O-K` O/1/#4A\\.b8<=FNN

lt	 

b1DX=D4#$)S-4A%%1T4#1

HIJcO/$bF?",D/DB",-1K-O/1:4#G
?$ODX-4#=-4A*y=&\\C4#1T\U4A5OE`4#$

1=S$-4A51/47698"1:H?1L8<=1:%-",*bk1/D=D>$G;$J%='$=AE$%L"$T8<=MA8<4A=1/$JG;=E#4#",Y9A*

fi

/"1\U\

/4A.691/N/$.$%=#O/4A=1/$.%=
&(=O/K>/R_>"1/NwLTX-#J4#1/$%"1B8<$K)S/4#8Rw\U\N=c$1=

Ofi

G;"1/",5k%=P$%=AHok=1J=&XSDB#"1/1R$4M$$ODX-4A=4M1y%-G;$=&X$%=#O/4A=1;A15B$ U?1'/R_b

lk

lk

lk

\U\0-"15$)S4#/4#1yaa u%=;^a u=&k1/D=DW$A15/$b/^ =1
"-",5bF=1;TX-Mb\\R"15$

lk

)S4AB4#1wa

	k

lk

%=!^a, wbX`

=1>"-",5H

/4#1w/",\\.$YX/"F4A=J=1$%h).=wN/=G;"4#1/$J4M$SN/OP%='#",5P"G
=OB1k=&N"N
1/N/$U4#1hK8<=$%DX=1/NB4#15k$",%K$DB"8<$



).%-4ANh%=?-"1/N=G;4#.\U\.$$%",-8R
$%%-",%5*bRO/1/1/4#1/5

4A=1/>R_w"1/Nww-#J$O/4A$HUiJ5",RN/A$$.=&V-"1/N/=G;4A",4A=1$%%-",%5*').%-4ANbc=1
E"$%F$JB",J=-4A54#1B"\U\8<=O/MN/1 $%=AP$",-8-!1/NNODYX4#15
$%OB84#1"
N"N1BNH"N





fi

r rr
rrrr
rrrr
rrrr
rrrr
rrrrx
rrrrx
rrrrx
rrrrx
rrrrxx
rrrrS
rrrrS
rrrrS
rrrrS
rrrrSS
rr

fi

2fffff$i

0 p lWW ~
 r      r  
r 
 rr 






r 


     r  
xSr  xx

  
  r  
xr  
  r S 
 r r 
  





r 
 rr 
rr 
 rr r
r 
Sr  x rr S

   
x








 


x  


 



0 p l 	 ~
 r      r        
xr 
 rr  x
xrr     r   x 
xr   x  rr x  xx 
Sr  x r  


rr     rr Sx  xx 

xrr  x rrr S




Sr   rr   x 
x  xx rr x Sx
Srr   r  

xr  x rr Sx x
   rr 


     

\U4A5OP` kiSO/1/1B4#154#G
$"1/N$%=#O4#=1PA15h$OBA$V=1.jSf_l.:-^``>R_"1/Nw-#
$O/4A%$H

1/N/$K",E"&x:O/1KDB1/=G
1=1w4#1'/?>/R_T"1BNwTX-MJN/=G;"4#1/$bc)S-bc&=K<F"G;DBAb
"104#G
DX="1:J/4#8Ah8"1!-OB1>=Ok=&&OH.f_1>B",k$%1/$b9h"$%F$k4#1!$%N=G;"4#1/$S/"
"
G
=
8<=G
D9A<$%%-OB8<O/"10=$%
4#1"TA=k=&=EYX1/8-BG;",>N=G;"4M1/$bX)S"$%F$
",N"NF1/N'&(HDX1/N/4#1/5LG
=J=KA$$R"1/N=G;A*=1;"$;$%%-O/8<O-S"1/N'$A8<%N'"8<4A=1B$b
\U\8"14AE$%=A
>R%"1/NTX-#wE"$%F$ExcO/4A%&"$%bW=E&"4#M$b4H HAb1/8<=O/1:%-$E"TN"N
1/N$%",%>)k4AC1&(=R8<N/4##78M4#GPY94#15HI*F4#150%=$%=AT"$%F$y)S4A8<=G
DB#%Y$_6B-$%
$%",-8R></"O/$%$SG
G
=*w$%=OR8<$J&=S#",5S4M1/$%"1/8<$H

$'&($'&H:Gh9b=>|aff cZ_J\b0:hTbP(W(0
[

?Oy#"$%y<"G
DBATN=G;"4M14#$y1=
"08#"$$4M8"D9#"1/1/4#1/5>YX1/8-/G'",9HI=54A"1<F"G
D9A=&
"DB#"1B1/4#15"$%C8<=MA8<4A=1)S-T\U\|,{{ 1B8<=O/1%R$;N/47@8OBA4A$b).>8<",%N2"D9#"1/1/4#1/5
N=G;"4M138<=1"4M1/4#15!B",-N3-"1BN=G

JEcHcH

FjI4#1/$%"1/8<$HT\U4A5O/!^!$-=)S$P-O/1:4#G
;8O-$P&=h\U\.b

f_llHFb9"1/N>ZKnWjSKPZ

IJ/."$c$4M1\U4A5O/^E",-J$%=A,",YBAJFjI4#1/$%"1B8<$U/",)--"1/N=G;#*L51-",%N;"88<=RNF

l

4#15
%=L6BcN8#"O/$%<A1/5>G
=FNV)S4# 
4#G;$S"$SG'"1*>8#"OB$%$S"$Sfi",-4M",YBA$P]r04A8-/#b
c#G'"1bsnW$xcObB^``eRHik"1/N=Gu4#1/$%"1B8<51R",4A=1
"1/N%-"1/$M",4A=1y$%=&().",-%=?l?n

7

/";YX=Y13D/=F4#NN0Yc*

O/$$4Uik4#1"1/1H'?O?6B5O-y$-=)S$L-O/1/1/4M15'4#G;$E&(=hjI4#1F

$%"1/8<$)S4#0acbU^Fb^acbV,Fbacb"1/N,fi",-4M",YBA$bB6Bh"$c$=&"8R$4AH



"MO$S&=?"$%F$k=&

S$-"G
S$4Ak",SN/4#$DB#"*N'4#1yO-1b:4H HAb:"#NB","LDX=4#1:$YXA=)Q^L=1;c"fi4#$$/=)2RO/1/1/4#1/5
4#G
$&=?a,",-4#",YB#P"$%F$b"1/N$%=w=1HEIJ=O5yN/","T$%4M$k$G'"#bh=Y9$%,",4A=10%=wYX

T

fi 9>~20$7`$22



$##!2F

1000
FF
IPP
BLACKBOX
100

10
sec.

j

1

0.1

0.01
5

10

15

20
#variables

25

30

lOCbi5/iQr?iC{QB#J  5hCl!0h`iCl
CiC~5Ji"xC~5iCCEs#iff00h0!5
iEJ505iNlClihiCiC~5!iC"`iE!iiJl55pAiC
QBQ#  i&Cs5C{C5C5C~C0QffQJ  J/i5
JC5C~!#i3CiC~5ffs/lffOi+0iff~i5+l!CsilC
h{0i00h0!iOCh{0i5OisiC5QC0J~lbi5s5~ixhi
C~00h0CiQs5il5=CQ-ixC5Jili=x#C5ii!5Qhir
OiOCChh&C0OQB#J  Cb5+5QO/i!CJ!iCOC5;5Clii
/il{iCip;i!1B`50h3i~``!i~iQi!0ih
ilC5b1{(~!l50lC5ruiKhClb!`i	ChKihlC5b
50hiiO5?CCffEi=W+Q~l!5?ilQiCi!ib5ChCh&5il!
iCil&#~sl5bi50h!i~ibC5C055i	
00h0!5QQx!xC5{ilh{0i5!!C5Q0iffCx	hi~lCQC0Jlb
iCNlJi5b5
pN~lbhiJ`lJih5	#s5CC0!iC!ECliC+CCli!i
lCl1{!iC!"iC5KN !C0hKs5C5ilI5i`
silbCl3C0 5C~x&!{(NCh5	(iOiCCC
s5Ch!0

	fiff	ff!"#%$&	('")+*-,/.'1032435ff6ff789$&:;5ff-<$&%$&ff1=?>@AffB;CD;EF>@	FGH$I	6J>>IAffBffF
8K>IL$I	FGMJEfi<$N%$Iff1ff:F		O
P!QR

fiS6TU<U<VWYXZX\[^]`_ba+_bc
d1egfihkj-lZmonplqr+stmFjBrvuxwzy{r+|Fr~}Bbr4m1

ib5lCC0ii!5is5(ii5(l/55b"0p 0ii
!i!C~lslxCffs(El5&505`ff	5bb5siu
&Ch{s@ LKC5Qii5i#i#ihi1 ?/iQ0h50hN{0!i~i
iC5Ch0iC0lElih5J Sl  s! bJCQ! +h5C~5
"EQ5G5`05OiliCxiC~5+50hh 0ii=iCLChC!5h!C5
ffiCi!x~#iCxh75hC!bC5C!h`+hih`iC~J5!lO~lC!C~5
CsQiB~sh5bl5hCCxb5#~l!+5liC{<  xCs5C5b
i5ih7i5
 (i#~lE!C0lCi5hCCxb55N5ih5IhCbiCIlxl
5i`siiC~Jl0iis!xiCCs5CC0ipC5~l5~h+
~sh5bi=C=iC"C09 lJ+CCl5Jh C0iC"Ci5ih0Ci
Ch55	0lC#i5!J0-ix0!il!0Cs5CC+
"9"A3Z1z`Z-34Y/3

(/C5lbi5l0l`l`i/505Wi	J5~5l~sh5	E

N!i0hC5~ii0ff!i0	i~lCb?ixli!5CC5?J55Es
C0?!i0hC5~6S~5~lbB=C5/ih~5&CxQiiihCN-ChJ
ili!iC~5505iC(CCC`50hh!i~`O `5C/!?i~lC5Ch0
Ji!ClCi?5CC5bih053C5i(s5ii55ffWi=CiC5 iliC5l
5iC#ili0i5CC0O~lC~h5OiliihlliCJ~l!+5p!h
50hi5iCi=h0C5i!50h!ihC`	&5i`5ClChhC
ib5hC!bC5?xCs5C5blCxli#C#  fi  C9 +5b?!iC?~l!l0lCO?5i
i=s5ii5hCCxb55C5llCEhi!h!6 /+5  iC  il
0`CliCCx0iilC!#il  ~l!lCh0lC#lCN+5!5i~i~Eil"
F ihi+lClixC (5x5~lC~5	0OliO/!CCi5bi5h!C5
J! +h5C~5#s5"ihi1 iC"?!i0hC5~E0/l
0? liCxiC~5x05&C
 0CC`055-53ll(5CiC(~l
~iN5lbJ0i5CK~hi~O~!xQ5-5NlClC~r5~l  
50hhI0i{Cx`ihi H5!G-0hx0liOiC!0h!x!x
IC5#5/ih~5"CxE!CiJQibCC~5{L5~lv
 
  ~!iCCihCx5"ii	G50h(Cis7~iCC
 hCCx5h!Ci!
l!Clh!5C0O0~lCx55!/C~r5~l  0
(C5i55b5~shx5	iff~Ch5ihGi5iiih!C s5C~5O
0ihC5`h=lhCxJI5h!C#li< J#5lbC s5b~l!l0lC
ihC555lb! +h5	C5hxQ!iC-  5`ix!h50lbh5Cp
iC~i~1E!i`0hC5~hi
  1i5< fflCb5	l3i
0xii1 ?CWiZ iJQQiJC5xCpbh! ~lsllC~C
iliffCiC~5x05OhNi!{ChC!5CC5Jh=!55b5{
ihlxCib!l!~hCO#50hh0ii	#C5&lxxOx!Cx
5l
 Q555~Jls5#0iC05NhC~5iOhiCli
P!QQ

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~
 L5ihi?!iO05J`il"~5O{CxE!!0
 QlC	b!siff~lC5i5Q`Ch!iffEb5iQChi

C55x0iQC0~lCbE~~5C&Nh5lff550h
C5lxL 5#CCxi0iOiliB!iC~5x0i

 Nx55#5ii~hlJ50#hi#x{ihJ`ilN&C~l!

0lNi?5OiOiE0i

po
 0li00h0lCi50h5h!C50!55	5=BpshlCi!~l
 CC0lQill?iCO5~i{sC5C55iBO0l00h0lC
&
Cxh"s5~+5!5ilC{x"/Hs5i=i5 iC C&5Jsl!5lL
5i
 (~l!05s/ihiC~i#lCE55b0l 1ixh5Ch51< 
G`s/ihiC~`iL O0hCCCx{`!xiChlA! LC	bs5KJJp(liC
 /
5C5/i O  ~C#`/lQli!bCOi#li5iiQsi
!i!hffrt
 Z@@@g+iCt
 !A&bl!i!li5iJi0C!05bC#&!~
5b0l"CiCllx
 !iJJi05OCilO!iCC#l`i
iEhilEiJ5i1l{0i&iLp
 M9@#F&A iC
 6<Y50h
5h!C5E5  0i"C9 +5!~i&p
 M9@#F&A! `i!i&li`i&i
CilCs5iCJiQ5#50Js/ii`lili /lCJ!iCCQli5
i&3  5I h5i&x
 6<Yl&x!55b0liCi&~isi
Cio
 !/ii#5il\
 AO#li5iO
 CxI OiiC5xl
i5&l&~i5slCC5C~s5Gs5CiOi{ EiCNl~l!lCh0l
Gih5hC5L< pGibLii=l~shx5	ChE!/!ff!lbOb{
s/ihiC~i5/ih~5KCxE!~l05K1 !`hilb/i0ICx
h0ii
ffiCi`C0b`0hiJ~i!JCi	~l	ixCiii3   !i!Ci
/lul 0s5Ch!0liC5?hi0CJslCCCOiff5~l
 C ll`iC
 NI@K6
 z~+MF&b /h~siClCish0iffhC55b0lY  &z@%I<I#F&A!< (g
~FFI5 Z@@@g IAb+`
 &! &CZ#I5M ?</# g!&# \A@AOM 4A@Zr GB J
4A@ZArJp++ bFzZ "&+ xZ!%IY YY"z3 M9@#F&A!iC
 M%?p
 ?</
v
 Li   (C&(si5~ Ail`iCNC5/l     
C s5bCiC~5Ni1hi!iC~5&0i5LGCCC~l(C0`h53
5ilx1l`bG 5~iC0li5(G(5Ci1~ CN5~lCCJCQi
slGiBl`iCJ` zhEiBiLph! hCili5 L0
l~   5~lCCp v
 z~!#F&ANCh55	0lC=hiCClNi505(i
o
 ` !CJ0i5h0iCx5b
 il!i&iC#JCi
0i  r  &I@bK3iC
 &A!?C5Opbh! GCiC~5?!CGCsEi#hi
li5Ch05Gl5Oh0Jx"9 E`E!ii5~i!5ON&~lslLC5#p
&Z#I5li~lbi5K/l  
 G!iE shi!si5(pCh5iCC4
 
~lsl"liC5&C5E0iOxCiC~5i?C5s5~i~l+l"!5p
&z@%I<I#F&A`iC
 (g~FFI5iCl`i5h05"0hlCC
 00iiCO o
 0
i~ 5~iC  
   0lCff5s5~i5ir
 &Ab+?/l   ~lC5hih!i(
P!Q

fiS6TU<U<VWYXZX\[^]`_ba+_bc

s`~hCiCi5M9@#F&A!Q  G5x55Gs`!xi~5iCxMG
0lCJC55s5C
i5i i5l	~l!l0lCOi5+EhiLEh5s5~iEiCJlG5i
iLixl~i!LhCi;JClG~l!lh0l!`Cx1hiCCl9 5ACx
hCG!ix5l(5i1i+ffiC C5Cih0i50Q0hC5JllG!ii
lbl!Ci`iCE+O@ 7Q5`i?E~l!0hC s5bJC
E!C{h0l5/imCi!5=/i~i!@ 7C`b!s553"s5il!0!
3C5h55sC0#"!iCCx"/hii+i#!x!xhi0hl 
iN0iJ00hiC~!0/lCCKs5! +h5	hxii`CCi3 l
5C3hC!Aff~l!"C"~+5b`35ilC0"iG5h~575i
~lC!Cl3K! 4Gbii!x~lChCCl3I! 0G5~lCC A!CiiJ;C5C
i"i5iKl(!xiCCGiIi5ilC0"9 i575W`i!0iN
liL`0`i7i0NslC!GiCL5~l=Lh5 C01`50i0!Nxi
lCx&0!+5!Cs+i~liC5 Li5Li0!Ci?Ch55	0lC
 Oi=C`0h9 
#5hCliC!C?Jlb5i!-0lC (J~i`5JC0!0h05
/i5iNlixiQlCQiiihCxJ5h!C5!ix0+5!5iQi#ii
!`lixr~l	iCi{sO5h5	xi+! +h5C~5Q`Cs5Ciiffi
5h!C5JxJ0C!5`ihl C s5bJliC5
"9" ?333z



z`

 iQlhCCCxOx	i5l0l~l!lCh0l`!iC`Ell!i`&li5
i+#5s5~ihCC!x-iCOxxi!05~l!CSl5
~l!lCh0l(i(h`C0SiClCCC{!i=i~i!i15i~5(C
E!C(!iC`hCCCx1b3C5i5C/ ]rGlCGC5lC0L5ih 
~l!lCh0lLb  h5
 HiC!O/i5CSC=i~l!Ol= H	iCCOi5/ih~5GC
E!CEl
 ff i!C#/iJ?K505#lrQh5< C5+5~i
!xi~5L"fi
 i0hhC5~~l!l0l
 H` lC6 OE`0l

 !iC
 HC/iJ~i!iC#!x!x
 iliffCi!~5#iCN5!/C
i~lCJChCCx?CiO&!hQxC5hl`il#CCCCh5CC5ih0i5L0i5
sJli"li0
lhOiO0i?5ihlixiCE5ih~l!lCh0l05ih0i5h!CC#xi
iCiC~5`C0CliOCi!~5`5ih{lix`0iix!i9 i!
!ixihl `i?OiE0ii5l5ih0lCii	hCCCx#50i!~lChi
i{~hC0!C50li?hCbs5CiJC0lhiC5#i5Chi
!5lilC	x5CsliQiCIli5`1/ Cx	lN!C5l5C
C0lb&+C55b3CC0Ql!0i/i~ii-iC5ElhlbCiC
C@ 7liChCC5CSCi~lC A`lEl"CE/CiCL@ Al!xiCh
~!iCCC{ixlCi#50hC5p
 &z@%I<IMFIALiC
 6bgA~FFI5BhlbCiC3
0i50Cl!KiliilixC0~lC0!15JC&
5s5i1h{/i(hlbl~l!CCCG0i5&/i!i!h&C 5!/C
i~lCiC5/ih~5NCxE!CBJC#xCC05QC0ilCh~5#isiiC~

!Y*B$IF;;	<ff75#) ";C#O@#$I		7%$Nffff>Iff>@>I:FO@GffFG3>!735ff6>I87F$I;;#G	FAffff%$&>@		FG->&8~O@ff	FB%%$&$Iff	<>&7?>@""!H$N:ffC>IH	67b#Aff$N%$&ff$O@@$&;$&$NJ;$I;	v>L$I$	x@$&>I	;$&;J	;
$&ff3ffff$# %&N% 11# 	F8K>IL$&ffD :	F"g' 8J:FO 7F(%*) F>&b+ L$I	F	,%+,
' 7>@L$&C	F !ff;9
P!.-

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~

9	:;:;<>=?A@CB
M @CN,OQPG:;RN.ST@CUGV H NEW$:
M @CN,OQPG:;RN.ST@CUGV L NEW$:
M STZ[<>\]O>^E:_<&R6NES;@`U
Ma @C@`U,N.b&<>S
3 S;S;<&Z`U,<&cEO><>< @C@
3d STZ`U
d STZCWAWe<&S
7g N.^JcEfAZCNE:_h;Z Z`O>:
i ZCO>N.fAZ`OVj95k g
i ZCO>N.fAZ`OVmlGn ipo6g 4
i ZCO>N.fAZ`OVml,qrn o l
i N*sYZC<
i WAS;ZC=t<
i BG:_h;<&S;B
lAOQuA<&U a @[<
qffZCST<>RN.ST@CU
q:;W

/0DED RG/1F H/ 
L$F -.K
-AF K.D KAF HEI
L$F <L R
KEI,F R*H
P!QAFXI P
D H QAF -.K
P,F RJI
.HAF R*K
RA5F PFR
D IJ-AF -.P,F K D
P,F R D
-AF F- P
R*HAF - 
RQAFXI*L
D HEI,FXIJD HEI,F H.L$F D.D

H /2D /
FRJ3 I
P,FXIJH
QAF Q D
I,F QJL
HA5F P*L
LEKAF -EI
IJHAFX!I Q
DED AFXIJH
D 5F P D
RJI,F -EI
D F- P,F - 
D IJ-AF -.P,F - D
P,F FH P
-AF F- P
K A5F PFR
.-AFXI.I
D H D FD P
DED -AF H Q
-AF QFP

/
45/

K /
D 46F D 3 H.-AF D.D
IJKA5F PJDED K$F PJL
D I,F RL
I D 5F PFR
I P,F K.AF D Q
D F -.HAF  D
DED.D F L R
I*L$5F PJH
D F .H
D FXIJH
-AF -FP
Q D F LEH
Q.KAF - D
D L D F <L P
D P D F H.L
-AF RJI

FP,F EI
AQ F HR
Q.-AF -FP

K.KAFXI D
H D F F- P
I*L$F D I
H D F Q
DED IAF -YI
D I,F D K
KAF5P!
RAF RR
D F- PAF IJL
P,F <L R
L$F QJL
-AF F- P
QFP,F Q
R D F K.D LEH$F I 
DED $F PFP
P,F LYI

7L<R,/2F Q / D
D FL D
D 5F P D
D IJ-$F -EQ D F EFI R,F HYI
-AF QEI
DED IAF -ED FD R
D IJ-$F -EH.K$F QFQ
D L PAF I D
D F HEI
D F L.L
-AF F- P
<L R,F F- 
RJIA5F RJH
RR,F I Q
D P D FD H
P,F FL Q

7/
3

PJ-$F PEI
-AF .Q H

D -$F D H
D IJ-$F -ED PJK$F I.L PAF FK Q
-AF K 
EIAF D -AF FL Q
RQ$F QP
H $F FK 
D P!Q$F PQ
-AF Q.K
D F- D
-AF F- P
!I Q$F LGI
EIAF PJL
H Q$F P.H
D -EIAF <K R
-AFXFI R

7545/

PJ-$F H.L
-$F PR
P.IAF D 

D IJ-$F -ED P!Q$F LYLEH$F F
D F QFQ
D Q$5F RJH
-$F D R
LeF L R
D -$F D Q
EIAF LGI
-$F IEI
-$F K.L
-$F - P
D Q$F I.K
QEIAF D H
D PAF P.H
R,F L D
-$F D I

D 7	K$48F .3 L
K$F DED
LE-$F KYI
D IJ-$F -ED L D F -.L
L D F LEL
PA5F RFR
D.D 5F RJH
-$F DED
PA5F RJD.D F .L
!I $F -E-$F I.K
-$F HEK
-$F -P
PJK$F K P
Q.K$F P D
D H$5F RFR
QEIAF K.L
-$F -<R

lCi0Oih0i5NCCCCE+JCli/iis5lb~llh0lCQiOh55
Ch!CN5CC{iC50hh0iAix(0&Cl	lvH`(iC
H?ff  ~lCC5ff!h3lC(C sE3ili!iC~50il(~l!lCh0l
0i5&0!Ci l 5ihKi~l!05 iC&~llh0lC&{i
lix
ffK!l{i!00lCl Kl!i(ii5/i{5ih]CliJ05i !i
i~l!lh0lC h=OilCbI 0x
 wzy(|{     I!ii&!i!{+hih`iC~5
i5ihhChK!i5N55KClN~l!lCh0lIs/ih5Kl!ib1s
CiC1iNli BC51l!iC~i3~lCb53CE+iiN!0Nl
~l!lCh0l li5 Si5
 (l!C"!+i5x0!&~hhlGCi !li
GC s5C~Ns5AhCC! 5`i5ihAi+;ii h9 iQh0Ci
s50i5hl si!!0l7C 5i9 i!0hihG0i!~5!5
i~l!C~l5C~b0iiQhCbC s5C~550i5lC0!iCClC
0slJ&C!liC0 (COC5NClxxl!0ih0iB5
 i 0i5hl5
 i5( ~}EL JQi5xi! b CLC0{si
!i!h iCK+hih 5b!i(lK"li5IliGJ5li5ILhiCCl !iC~
/l Cl`iO!i!0!x&C0#SiQCi;lC`s5Ci+ECi0!
C0mJSiCi"sff0i&COiCECbbsi5x OCCC0	si55!miC
 s5C5i&C s5b`lLi"CiC~iC5Nx#/iOCi" OCi0!
 | QJb!
iQ7xSiJ0C!5iJiQC s5	s5C5i#i~ihCCE
QlxiCxh!Cl
    Ql!Ci!0&iBi!i5{l~l!C
Csbbsi5xWiCBC| 0 (i#h iCAs5CiC s5biiC
 i
P!

D

fiS6TU<U<VWYXZX\[^]`_ba+_bc

h;J/i5G~l!CECi!0!xC0si~ihCC`!l`i=Ch!l
 +lhi`lC~l5`0{iCix5K
 7hxi(rC0Ci0!1
1
   | 05ip
5i#5Ci+Ci4
 0C5 5~#b!!bbsi5hQiC{5C0Js/ihl!ib
sCi Bhxiis5Ems/ihlCi	sCi " (
50hC0ilixC=~~C# bF
i?~l!lh0l!Bs5Ci5E5Ci
E~i~NClE!0~lCx"iO+&li5Lb5OiE~llh0lC5Jr  $  i
i5#O/lCCC0#l~llh0l{Asi5hlCibs#CiiCi
~l!lCh0l= i/il
 ,sCi!~5OO C s5	s5Cir
 . ff;QiSi
lNi=!Ql!hN!iC~i
(00C0x	55""!ihJmi!"i~llh0lC&5!#/l
 1hCxl`iOh5El5C"LiChCCh!ii5"5Cx"0sl
l!ibChi55bx`Ci5Q5!0slO s5~QC0Oh5s5~i5CC
CiOl"s/ihi!~"lixJJE0! `!iJi~l!lCh0lCElEOhG
hC5{ll! O#l#!ClQ{5i5

U,NE=t^.Z[f
95:T:;<>=?$@[B
M @[N,OQP,:;R6NES;@`UGV H N.We:
M @[N,OQP,:;R6NES;@`UGV L N.We:
M S;ZC<(\]O&^.:;<>RN.ST@CU
Ma @C@CU,NEb><&S
3 STST<>Z`U,<c.O(<&< @[@
3d STZCU
d STZ[WAW<>S
7g NE^.c.fAZ`N.:mhTZ ZCO&:
i Z`O(NEfAZCO(V9	k g
i Z`O(NEfAZCO(Vjl,n ipo8g 4
i Z`O(NEfAZCO(VjlAqrn o l
i N*sGZC<
i WASTZC=<
i B,:_h;<>STB
l,OQuA<U a @C<
qffZCS;<&R6NES;@`U
q:;W

3
4
7
/2 / 
/  4 7  / 57  4 2/ / 
/ 3 7  / 57  3 2/  / 
/  3 
4  / 84  3

 /  //      

/ / /
/  
 /
/ /
/ / / /
  /
/
/
/
        / / / /
   
/
  / / / /    
    /       

   
/ /    /      
  
       
   
      
   
  
     

  
 
/      

 
     /      

l!OJs5~QihCxlEl	iCihCCCii5C-
h9 5 Nil!ibCi5"iCihi5LhCCCx&s/ihiC~
+OliiCN+Jh"~l!lh0l
lhE! E&sCCb`i/lOOl5!/iffH	i!H
h5+CxhNs5~l&i~i"/l Ohl	Ci5iGi55+
0/l~l!lh0lCii5iilCC=hC5C!x5i5Gb"l~l!C
P!FP

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~

0!ip{5ih"~lCs5CiQiff5s5~i!ihbil!C~l!l0l
Gi~iGh7hC5<;~l!057C"s5C5iCLi~i(hCh5
l(rs/ihiC~{&Chi5KlCi	i{0!{O
 >l!ib
ihiC5B0!CE
 iC(ih5s5~i05b(5CiCi
~i!is~lCC&i(~ih5+h0
  {OhGi~ii=iC"C!ihbil!C
~l!lCh0l3xz
 Wi?CE!lE5CSCi~lC5"i!1
 
Cx
  CiC~55iCIiC 
 O !(mCi!~5IO+hih`iC~N
l!ib`sJCi"CC05{{
 >t
JCJl/l~lC!l! l5!i Ci!~#505i
CQKCi!~505Q50hi0J5x05#0shilJl`iC
5CC0/i5i{!iilCC~l!lCh0ls5CiiQl!ibCi5`
Eli!r~lbhihO0OlC
 i5iiiQhJsiiC~Jiii
Jl!iC~5O0bCE051ilixCiCG!ihbihlCC ~l!l0lCs
C0xE!0JiQifflCiC~55hClx	hi~lCQs55hC!bC5QC0
lCN~l	~i~iIl`iCi~i!i+hih`iC~Ci5I
(gAFFI53`iilCC~l!lh0l5CiEC5CSCi~l!!ii5
5`iilCC~l!lh0l`C5CxE!OC5CSCi~lCCCis~
CiiLCisi!hi5"Li	{ili=JC0  lixCJCs/ihiC~
Q!i5{{i+!l!ihbil!C~l!l0lOC!05b`5i5Jlb
i~ls53`5hCCxb5Chl!Gx(l+5~l!l0l
 (`5`0 C0
i/5{hCCCx5Q505#0lC&xsQCi
 J5`05
Wis5C5iiQCi55x0!&i#i!xCiC~5!lCixCSi~i( /iQi
5i+C0	~l0#JC s5C~5?s5`ih0iCCCCJ5lhi0bi~l!0
h0lCQl!CCi!~50Cii5I px
 li!C505OChi
s/ihiC~~lCx5	3ii!iilC!I~l!lh0lCxCC0CL 5i#i0ib0i
iC9 +5bO!iC~&5055r &CMIS5~h+5L(5~l   =/i~i!i
/lCCKN/lIrisi``Kl iClxClClN!ix5Ch
C/h!x{1iiii&{Nl BWii~l15iC"
0O+-i!0l
 }OCff0  !ixQ#lE5x
 J#5Ch
5`0iQ!NC1iii  0 B!5x0i#Ll m
 ]0Jb!+?C0&05i  !iO/OLl Eshbl ]is
0iC0lCC&"5lb Ii5`05OCCSC5Qi?i~l
l!0h!x!xihhCJ5/ih~5GCE!Wii0/lO~l!C
LCCil{! &i!00l OCJ&C s5b50hG5hC!bC&!
5lC0C5!iilC!&~l!lh0l&5ffCi?C5C/!li~lC0!CBiOi
5/ih~5KCE!5C~~55b&5C5v
  l5CSCi~lCs/ihiC~i
ihiC5iliQib5iQi#!i5b5i5b`5!/C!i~lC5/ih~5
CxE!Chi5s/ihi!~lC+i	   i#l
 0LliC5Bs5Nii5
lC
 &A+# (Ch57~l!Cl!?bOlCOii50hh{h0i
i5C0Qs5CCsi&ChElClixBI!~i!/iB!0ffB 6<Y	Cli
h!x!xiQ550+/ii(5i1iliK Al1i!0CCili
0~+lC5	ixib0hi!lCl{CiQsQ-i!0lCi
/0C55~lC0ffi&bhi~lEs5E5Cih~5CxE!CiC&5CSCli~lC
Ch!C{C0&Ch~lCh5b1i~l&il&ii#lC!i!ClixCJCis
~!i51b1C` s5~&!0EChC!5h!CCiElC9 +5bE50hhKh0l5
P!.H

fiS6TU<U<VWYXZX\[^]`_ba+_bc

pCE!5CSCQi~lCChCCxL!i5	`N!xiChl iC10bi	
s +lC`hC~5ihNl 5ih;hl 0(C0NK!0 i5hll C5i5"
CisilCi&5ii=C!0 s5~l5Cih~5CxE!0EC{i
hi01Ji5CSCi~l!ChC5ElCCC5~503hC~5ihiO5ih0NChCL
C5iC!h#50h!Wi#J{Chi!hCC&Si~i5C5!&+hih`iC~s5C!
~+lC5	ix{C!QC005!~lC	h5
(Ci~l!0&~lCCh0lii=i~lCJihhC~lChh0lNi?l!{5!/C
l51Bbi0l	lE/l~lC`Cil ! 1Ji!00lK& C0
5!/C?i~lCE05iL5!/C 7N!is/ihiC~hlCib"x(ilii
l!i!C"CliC1CxE5+5xi(h/il{!iilC!I~l!lh0lCE!
5/ih~51CxE!C!Chibhi~l(C0Cilx5 0+iir3l
lixCffC5CSCi~lCEChC!sl5"i(hlx(5hx~l1l350h!i~i
YY/zi&i3x5~l 0  `/lCC1!0E05iKC5i{CC5!iC~5ih
hlC0slfi AiffCl0~lCh5NC5CSCBpiOliC5lC;i~lC
0QCh!50ix
 6<Y0C0l?C#i~lC0#0C!x0!E5ih`0ilCxh0
iff~lCh5N5CSCLlOiEi55fi i5 `hi=ihEi5h~lLb5OCCix
5i7l!ib`Chi55bxs/ihiC~i]p7l`iCL &z@%I<I#F&AIiC
(gAFFI5C5CSC!i~l!i{!hCOlQbib`sl!5lWi#lxl
!0C5QCJC0!+5!#s5iChi5-5"!xiNiLliCxlJC00E~Ci/i
l5i?i+l!
 &z@%I<IMFIA!Ch !N5~h+5G15~l
 C =O
i~ 5~5(si51Cx{ChSii
 A5CiCh/ixi5i?i~ 5~
Ch0i5{O``E~l!ClCi! +5~ }5-5`iCiJ5C
0iOibi~ 5~lisJ!hSi@ 7J5J +5~C0Cii~ 5~5iC
i~ 5~Oli0J0hO0lrlihxCii~ 5~CCOChSi
ibh
"9"9 --/-#Z   

(ixNbi5l051C+5~EC0Cu5hC!bC5C5il1lCl(5i{l

!0hx~l!l0lCuiCGOsEi"iC05+5~i&lxlG5i
/iliC0si1 iC1miC0i53"li@ AiblCiBxi&Ch3+lx	
K~l!0(llI5iK5KlC!iCiCiCK(ll0i
 ("5
~lCb5`COCs
 "iiOiC`Is5Ci5`C9 +5biiCb!s	 O
ll"Qii=iC 55G0sllC+iC~Ei&5h~hs5Lx"i5~l
l  O#lJ5hC#{li5JOxlNQ5
JCEC0"lh`  0hEili!9 5LxilCiCiCiiilCOlC`! O
!h&lxiC~N00!5CC0?C 5hC!bC50Ni1C/C/i`ih5C
lCl`5Ci`~l!0hl
 O0bii!/!CiC`0/iQCxhCb
s5C5i5=C/Cl`O/ll~lC! !iC~J505ihC?3Cp
iC~E505Ji!-0lCJ0!0!{5Cih~5LCE!x{!ihbil!C
5x05i/5G5C"hiiC+iCNC0&0&l`iCJOlxl
5i!0`Ci53i~lix!iilCC~l!l0lCl!~hC{5~lC1i
00lCECCL!is5iJClix1C0&?55~il!9 5!
 
5WJ5ixC0Ji!hi50sl
 ICZ#IliCJC5~5Cx5~lM  
Nhi+iiCCC1Kl8ElCx lClN!iG?mC5hxli5!hx1
P!JL

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~

3
4
7
U,NE=t^.Z[f
/
2
/
/


4
7

/
7
5
4
/
2
/
/


3
7

/
7
5
3
/
2
/
/


3
95:T:;<>=?$@[B
  
 
  
4 / 84 3




 
M @[N,OQP,:;R6NES;@`UGV H N.We:
       
M @[N,OQP,:;R6NES;@`UGV L N.We:

 
M S;ZC<(\]O&^.:;<>RN.ST@CU
 
Ma @C@CU,NEb><&S
S3 T<><O(<&@[@


 



S3 TZ`U,c.<
/
/       
d STZCU
    
d STZ[WAW<>S
/
/
^7 .fAN.Z
 /      

g NEc.Z`:mhTZCO&:




i Z`O(NEfAZCO(V9	k g
  
  
i Z`O(NEfAZCO(Vjl,n ipo8g 4 /   
 
i Z`O(NEfAZCO(VjlAqrn o l        
i N*sGZC<
/ / / /
i WASTZC=<
i B,:_h;<>STB
l,OQuA<U a @C<

 
qffZCS;<&R6NES;@`U

/
q:;W
l  OJs5~QihCxlEl	iCihCCCii5C-
h9 5islCi	E!i5iihi5ll5is/ihiC~
+OliiCN+Jh"~l!lh0l
!iLs-i!0l3!ix"Gl JH55W&GC
JhiibC` 5hx5Cxli!xiClC~hCxQ!hi00l
!i5ll`5iC55Cih~5CxE!xiilCCO&iCi
ib ~!iC0lL/iCr5C0i5LLGCxhL5i~5(Cx
!CxC#!i!#B !#sC5N5CiC~505O0ilCCWi!
l!L/l~lC`C`7 C!iElh  O!x!xLiC5/ih~5
CxE!EJ0i	(i5C`C s5b50hhh0i(5hC(Ci
!i!M (!lC0C##CCCC s5b05is5C5i#C0#C50hhLC
~C!Wi5Qs5C5iQ
 0QllCi=50h!i~i /ih~5{CE!5	
!05iAlhiJsi`~lG50hhA/i{0 7s0iC0l=iC
iCCQCi5#!0C0O0`5b#!i{Ch~C+M  5NCxE!C5	
!05i=Qlx0l!CiOiQ5hiCCliC	x!CE0sQ-i!0l
i!i5lliiC507#C"i~lC`l? lhC1CNs0 0
iC!ii(p bFz!5Cl5lK&C3LiChl5(h5
~lCbl 
 OQ +/ihl5hCLx
 SCiis55Ch
0i5#CC~l`+5{C5Oi~lCI 5{QCi&h5Q~l!	J0li=J
5/ih~53!x!x!i!hiQ5h5~lC	E!hJ!x!x`!xiCh
P!EI

fiS6TU<U<VWYXZX\[^]`_ba+_bc

l7"i{CiC=#hiClblL5CihCh(5Ci!i!0!xi Oi7
5~l
   0ffCxili5L!ih5lG5ih(iBi!Lh5C5ih0i5=&pL!i
h550l!CCii"lxlCCN"~ih5~`ihhChC0Qii#iQ
5i0i0iQ#Ci{~i5+l!CE-iCO/i#5/ih~5CE!!xiCh
Cixib~l!0~lChh0liBix+i~lCQihCQ~lChh0liBl!5!/C
l5	5C~5&#hlbl/l~l!C?i+lC! Ql!C~s5~5
OlClClihElCl"5Ci"s/ihiC~&ihi5GhCCl
5!/Ci~lCKJCx&xCC05&C0Ni~lClKNih5!0I({iliJ0i
Si~!Ci~lCh55CSCK A!C52 Gblxl0C0hOh5Q5i!iQlx5
il!Gx1
 Iz@I<@MF&GiC
 6bgA~FFICliCz
 !`iCEQ530hlCC
CilhlC(!+ffCCC{50h!i~`35!/Ci~lCl5&5iC
l!ibGhiEll(iC=5+5xiG5(CCx50hh3C
E!CffJlClC`5lC!!0hii	h!Q~!iC0lEl!
C0i 0si`0sl!x05i +5!5iiCxE!CChiCl`"iCCxi~lC
{5bOiLx"50h is0iChG{50h 5l&O05G
C5CSCbi~lC!~5ihhl~ibCxbC5~lis#ilil	5
0iE5J!JC{Q5< 1!x05i

ff_,8A

J(l{xsii	~lCC5~lC{iC  0CChlihClil5si5A
00lC
 !ix5{i#/iQ0h10{!i~`50hhiC3C5hxE0iC0l(b1lih
C5S lC?^  5=!b 

 Hil5hi{s5i?ii{ff} SQCk Ch!b
i!NJ~lCC5~lo J5h
 h0&i=5!/C!i~lCQ5hx#GOi#S5ii5i~l!6p!  
i!`h5-i!~5~l"5hCiCx }O+5=iB!b 
 C!h0liNiCC5KiliJ5lK5Ch3iCl{bb5!iC
O < iC0 iii iCCiC0Cliiliff0i5CC0!Clih S 5C5=!b 
 `iC0Cl(iQrJChC~5{!!i\S5C <iCM0ii0/
!5{x5i#/l  < 5LiCNih4p !b 
(L!5iL!C5Ii#i5"~l!5~lCINh5s5~i"5~l!`i5iCi 3`C
/Cl1N~lC5~l(C0CiEiis5(5bl5=!is5(5~ilC9 50E
!h!iCCx~l+l10OrQbh!
 !0`ix(si55(
 Oh5~l

!0l"iC&5ChlL5i(lE50h 0i#5~5b0CCli50!i5Ll
i!h-0lGC0&s505 5~l0l 5~50h"s5iC&b5/iQ0h
50hKGiQ0h5hx0Wi?C5~l!i50hhIiCK5Ch`0{hii
J{iiEi` HC55i5h3 550!&hCiCi50hhIC5~l
il!iQ0hCOl Eili?xv
 
 SlC6   5! bJ!#Q5isN5L
P!.K

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~

~l!C5l	0i5lCGlC~iff505i3Si~&CiC~`N`!i0iiC
lCCC5lbCNiO0E0Chx&50hh y
/iCxCQiC
 iC50i
 p! bbi#Ch5~liffC5hxO!5i# C
K~l!5QCiC~5ilx/l OC5b00h!COili 7ib5Q/l 
ili"5ihSi~NJSCC~l(C0C51~hi~/i5ihK0{C{ih0h50h
&05OC5hx50i+C5JCC~l!5GCiC~5i5=il&/ih0l
l!h{Si~OCi!0!`sihCi5"&CiClCi
p	h5li5~l!C5Ci6 ffCC5hxJ/l~h0l{5ihL50hh0i
!JCi55ls/ih#iO0C!lih55JCiE5LL5~l4
   !CxJ
/iJl#!0CC?50h"h0iiCN5!/C+i~lCChC!5CCi

85 ;5Gm85z
,_ff

(CiC55b5(i(0CCli(li !s5C5b!i!CC0=0s5
ls/ihi~{5hCClii(l(``Fp ih iQ5b 50i0!+5!hC0h
lixC#l CiJO5xC 5N55~l!5l`iQ0h0!i~
50h(iCG5h0iC0lLi05b"lih55? Ci ff+
C5` { ff}LiiihC (CIi~!x&5-5llK35ih750h
0iGJlll!li{"i{0SC50liJ"0i!
 CiO&L5~lC
F iOC9+ 5!~ ?Cl5 5J`Ci5?iB50hhGh0ii~lE!CxC

 E!C~l!J50hCiiiCJ0i5C#i+sSC5h!hCCC
5h!C5Cxh0i5l~iCx5-5NllC
&{C5i5bl550hx5l	!l3xC0&5ilC&/i?& 5!~3
 h!~hi?Cis5C0C5biCCC`s5Ch!0O5! NC5iiO`0
i/i~?!iC+5!hC0hC`5C05Lii?lC~l`h5
KbiI/i{lCxs"50C0 ICZ#IiJCLl"!ix`57s
hiC+ih5/l lhbl 1iiC5~CE iiC s5b50hh!i~Nh!~
Ci	i~ii	C0hiCl CiC~55/ii	x	Ci5E!C!hC#C0
C s5b50h!00CCiC0/iihi!i0hiClCiGC5
/iC0JO 5C~i?3l{ibi=s5CC0is5{iEC0
i!h-0lNC/i~i
ff0iE~#&	si55O05L0siiECi&x	i5l05"E0&!i~5i
!xiCC&s5Ch!0liOi` p! b!Ci~l5~5"5!isC0
5b/xC0hi~h&!0hih/iC s5b!CJi!iC!i=i&CC5C
iCg iixC&iC{!05i!J{50h"!i~iJCx!iJ5iNCQ-lCl
/i#!iCCxEl`iCCCC5 ihiff~l!~`C0#C5s5~ii+
0!i~5Q~!!5s5~Q5-5{ili=CxiC~5 Ll#iBC5bQ+5!hC0h
lixC0!!05bs5l
 !> !0i=C0-lCl
 A< iC= 0i  (
i0!Clihl	si55/luCiislbilOQJ5i#JCi
ix	hi~lE!0?Si~&!iCCxJiE~C!0iCh5I~lCxClCBlQ50hh!i~
h!~`/l C0 CiliQ1!0E550hhEGLC3C0&iCl0i
5C5#C{!0#i?-llENli5!xiCC&i{s5llQ
p1C0&~l	~0l`50h"s`i`l1C0&!xiCC{550hh
5iCx i57sl	i C0iliOK{!5xIlCKis3Ci5i

.'g13	ffOI%$NffH76	!ff>	.$&-$I	6>IFff>@	6>&8G>I	E<OI:F	OffMH$NG?F>G5g'/>I	ff6T+b	FH,>J(
P!R

fiS6TU<U<VWYXZX\[^]`_ba+_bc

5h!lii"!0i5fflfiGxCCJiiliG~C5OL!iC!il!0i5
JCxQC0h!3sNsl!ii`i5!ilC0i5i{JJpi~Ch5N C!0h
Ci5#iE A!0lbJssl!ii#x5h!liiC0Ji
5slNliJC0
 &G`sli5 5bip!0xC05!i!CEiN5Ji
~lCE!hiCi5 `iNCCxi5Cili55NQC5Jl!{
l!xiCCiiihC5  LE75Ei5iN5 !5iC"C0
i~l!x5?C?C5005i?/i55	xibiCCCJJpiC< ACx
#i#0C05C5{/ilC&iCl ]xCiC~5C5ih{li5JbCi5
!ixiEC\
 &CZ#IiC
 gA&l5Bi!Gx5!5Gi{Csii
lixChCCCLliC`ill3i` !C0hJCh5CN~il	
l1C`C5!~lCiJiCH `iC5i p! bO"C+5~51C0!xiCC{5Clii
x!s5~lC+( lCs5iCi?SiJi0iC~iffC0i?0J{!isllCi
5il!5&JCi5is5 0h5"!/!B/ilNJ
lbQi5JisN0C!iCh0/i"!ihC~5ii`C0`O&iCCC
l!+x	55"

 A51_
68A
J Cx#0xiN~5C5LiCNh5ihlNi!0sAO<s`iC/0iiJC0CiJs5
!50Q~LbiiCih`Ci`Qi+lJiC5~i?sQ/iQ5
5II(I~sh5blKN~l!0lKi? ?M (NCiC 	C
x	i5iCxC3!7i/Q0 ~50 hiCCl  Ci!~5L
il!0iiiCi5iil	lC5~l`5	5OCh5s5CiO
!0s5

QA	

Chl=B?EJ# (5=?&sp!bGlC!lCis5~h0!i
pLxlC 5l/ 3/ !+BA!J>(,e,Qt&_*&,,G
 ,J***&G0t 	*jG&_C`GexY_*xQ,*+C= Y   ffO
5Z L5C?0h
QihC 0ii 6 J*Q	5tt*G2AAtGe  G JjGCsJQOrQbi
iC xQls( ( l"lp
 i
QihC ` }i&
 0i  J
 0iiO!iC x5"~ls( ( l
 81
J$E~5 CiC~lp C
QClE	 C 	 p! b Ci!i! OlliC Cih0!`iCi( Cp
 2QG

>>GGQ *&_*&,,G G  G.***& G 	J,&_[e*&  &
 C=   .   "lb5isQiCiCLiliLiSiC
QC-F! h<3p!bC iBiC Chll!iC ih0!i!i>A 	JG
&_[e*& 6$B pdb"b<  
lJ 5E`p!bJO5> (x 50hh!iCC5IrQA2`G
 ,*G 	 !hl+E
P!Q

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~

lksC5Lp!b3iCxNi5>x(50h}O5!"pxCC
s"  5s3 M AC  (	GG0tGe
ff,fiQ (G  GJJ(&p,
2`,efi   AChCiZ C& im hx0
lO p s E/ 0i BiC Ci5& ( Q50h6  *jG&_*Ce&~ ih
~l
 
lff?=> CB    5 { p! b` i!!iC(Sii~( l155~( l15C-
  /i!iC p 2QJ>>GGQp *AGG  G.***&Qp &G
>*J*j,jG*Gt 	*jG&_*C`&Q!=  Y ! " Lp]5h
xiC5Qfi p!   JN~l0( lCiJ~l!~ KiCisl( ( l!i#JJp!xiC 
t 	*jG&_[e*& ,Bp  b    0  
Q5C Cb rhi -0Z p! b+h& JChi!5Cili5bp> C!0hiE }E	+ A e 2QG

 >>GGQ A,&_*&,,G G  G.***& G 	J,&_[e*&  &
eC=Z       OhALLiliLiSiC
hClC/ Q> iCBp!b6liBihC> p N!0(iihC5N!xiCpG>CC-
iE
} ++BAJ>(,e,Q A,x&_,jGG  G  G.***&,1 	JG
&_[e*&   &Te C!=Z      A "dLili"i/`iC
C5-i=!+-0iiO5>x ( 50h"!xiC E Qpfi  &	 zG .>&  E
i5=s+Y  liBp!b<b
} ~l!55Oiihiff0hhii55bChi!5B5
h=   !05	Jil!O* 5!~iO, /Cph05
ff i5~J }x l} 1p!bJrQ+b]CA0C!lih0C!  0( liBCi5
! !i!5l C
t 	*jG&_[e*& l B!  0b
+H=lCCE-p!bJil0( x h5C~Oi?0 b00> ibx (x  GG6Q
t 	*jG&_[e*& *(,Q*	 ffi       0
+#Bl?&0iOC> 3?} =p5b( / CNiC1iC0Ax "~lE C0i>x ii
(x p 0( l(hCi5t  !i! Er2Q.&>>GeGp J1&_*&AGG  G
 ,J***&Gv 	JG&_*C`   &, 50i"1ix il1""ihli
i/`iCO~!5"/iJ!  0( l
hiCM?QC55iB?M HiC{p!bt 5(ih  /, x=i50hh3isliii
 GG 	J,&_[e*& *>G	 !Y    0
<5Ofi i!+p!b lE  C(~C5h( i55{iQ  
CN* 5C~ih0!ir5#+{ ip fiAC"(	 YG&x
2`,e 
 fi& >G  GJJ(GGe fi     Q 6 l   {i5>J
,_z 	*jG&_[e*& C=    BlClCi hi!~isC& im
 hx0
O<si!MC0iiG C5> (x il,  Cs5C5b!iC iC C 1i5
/ih~5K x  E iii> !rJ>(,e,xx,$v&_*&,,GGY&
,*.AG(,mG&_C`
Y_ >z>A CCB . bbB!> im 0
P!

fiS6TU<U<VWYXZX\[^]`_ba+_bc
O<si!0iBiC50hh`i+lii  iCCs5Ch!0ff5&iiCi(

p2QJ>>GG1Q0.&_,jGG" G  ,J***&, 	*jG5&_C`
  &(,O50i~ 1i xil~ E Lili{iSiC=+~C5{/i! 0(l
$l# i!5+Z m% i# ~ E ?- 0ii l &5	, x5	QChi!5 
 5bi!C
!(  xC0C5* ( lN 0ihir0
 J&&(GeGQ  &
&
&_,jG,  ,J***&
,  ,mAGG8(A  B`0!+50
rhi   Q5{ CL p! bICCiliih> CGiCiliJiCl5	0( lKi5C> ( 
!i!5 l ffr
 Lhi/ 1 A=  2Q.&>>GeGQ $,p&_*&,,G G
 ,J***&Gx 	JG&_*C`  &T'C=+!   b   1xi	rii~ "ihli
i/`iC
il!lJ?`
 i!J??   i# h( l# 6 0ii uiC x(3hi!lp
 9570CCli
t 	*jG&_[e*&8 A"p   
ilh &  lihL
 C#  ` p! 0 A Ci( ii!i> &i~l CC~( i~iliJCi5
t 	*jG&_[e*& 0/ b    
iE!Ci0( s0i5   BiEC5hb J
 p! b OCChiC xi!~5! 
h0!!xip5s  ip
 ? AC5 ) (&*	YG&2`G


fi2Q&(G  ,J***&,2`,efi   Q lB   i

 5>J( ,_z 	JG
&_[e*&CC=Z     b<Bl!lCi iC~is!> im 0
iH ib E A5xii~ p! b  / OQi5iCi0!!i5E!i! ip
 2Q.&>>G
e, G&_,jG, G  GJJ(& G 	JG&_[e*&  &A-
!=  !    hlx+55Z Lili"i/`iC
iH i =E-  5xi={ p!   `! 5bi5isii! sCisl( ( lCi?iA 0
i!Ci( Q50hCpp
 J&&(Ge,Q+*A,,jGG  ,J***&t&G
>*J*j,jG*Gt 	*jG&_*C`&&!=i!  Y ! 003 "p]h5
5C5 CB p! bl ~l~iC CEiOll~hi~( l"iCChi!5p
 lC55 5l   p
 s AC 2 J&&(GeG1 &_*&,,G
 ,J***&G0t 	*jG&_C`GexY_*xQ,*+C=   + ffO
5Z L5C?0h
5C5+ + < iC=+ / 0iii `5ilC0!iC/ih~5`ili!ihC> li!J C
x(i10i5CC-& i5(!i! iii> CN-  GG	Qt 	*jG6&_*Ce&.J(G	
,l       
5C5   O< s`iCL ` 0ii0"  1
 Cib( 0( lii+0ih
 bil 
0h h0!pihOih&Ci5#r
 J&&(GeG/fi  &(A0zG.&&,213&`
2`,e	4
>G[exGp&,C
5C5~  }s5O O< s`iC +  t islCl5+ &" p! b 5! !xiC i0!C
(iK;!GpK5+M  ip
  AC ff >&*	YG&fiG
e5
-fi2& >G  G.***&G12`,efi   Q lB   `i 6(J(6,_
t 	*jG&_[e*&C= b   BlClCi iC~i+C& im hx0

H.-.-

fi+WY&xcFWX46_bXZ_b~WgTYX\Z~TY4S?_bZg&~gv~_W~

5C5 hC?0ii00i~lbli(!iCGCi!5NIr7#5
siE!Ci!0( ihAC2QJ>>GGzQxA&_*&,,G
 ,J***&G *jGG&_*Ce&
2`GY_*(A&-OO55F L5C
0+
=lC&Z  5sZ 1 p! b * 5b x!55b0( li=!iih0!
 i GG8Q
t 	*jG&_[e*&*(,Q*	ff e-/ b  i! 
GO55C&!E mJl5b! &B p!  #50( ClC 50!i! ?rz
 2Q.&>>GeG

t ,AGG  GJJ(t j>Gxt&JJ,jG ,t 	*jG&_C`
&QAC=   Y+  CiJ#ZLr 5h
GOiEp!    5C> ( "5( 0i/i5iCp5!C{iCi(7!iCx p
2Q.&>>GeGvQ17 *AQ&_,jGG  GJJ(&vG 	*jG&_[e*&x2`G
E_ Q5 & C!=      sQ5hL5C0+
GOi&`i?p!b 8 	t Gev GpG j
,8GeA .e~ 7J
OpbiCx Qls( ( l"lp i
GOi&ffE
 Lp!b( CLih5( l`0hih0!C&(~lbl#50hx 3!xiC 
t 	*jG&_[e*& 8$, pdbii  !
0 h5xff&5i=ff =i5hbi#ECp!bv0h3i!15i1x > !C( lCi
!i!55	p2QJ>>GGQ2$G/
 AGG  GJJ( 	*&j>Gt&JJAG
*Gt 	*jG&_[e*&&G*- C=     ivlli+QEZ"p]5
}O+5JO(0ii 7L~l 0 ] KiC7~Ch5( iL+iChi+l> ( lCi#iC C
/ihi] ! G&GQt 	*jG&_[e*& *(,Q*	 ff,l /b   !
}O+5t islCl55C+1p!brli>   5-ibSi~#iCish0ih 
ii5Ch0( lp5b+AOi iZAC55(	 GG2pGe 

fi2Q&(G  ,J***&,2`,e fi  
   Q  lB   i
5>J( ,_z 	JG
&_[e*& CC=     0{Bl!lCi iC~is!> im 0
?5CCiC`JJ?p!b L!i>x 3(px CCLilCC s5 JpiC 
> C0( lNixCCrNhiCiBi5iYEY/mJJ !BACJ"
 	 12>Je8 T(**m,jG G9 >YGe5 2Q.&>>GeGQvY &_*&,,G
 ,J***&x  Q$- CC=   Y  &Bihl	t} Y"iliLiSiC
JSi  d{  iC50ifip!b(l`,  Cs5CC5	5>x ( iJJp
ih!!i53li51hi5( l0!55"p C!sM 5s{AC>&*
	 GG&Ge 
:,- fi& (,  GJJ(G12`G fi  ;  $ Ci
+C> im h0
JSi  ?# i!50id0iiA ( N0N~lCh, b 5> ( 0~i~
iC CBpLQ 5=+Ci!!i!0( /Hi!+-ACJ>(,e, Q 
A&_*&,,G  GJJ(&G 	*jG&_[e*&GexY_ A 
!=      -OQh5ZL5C`0+
H.- D

fiS6TU<U<VWYXZX\[^]`_ba+_bc

!5+}Oiff"p!bff 	*jG&_[e*&%J$*&AQJ$	?5b(~~%i
lbLQ] s5 }6
 i5b+b }b CbQi5i= Z p! b< GG GQ,pJ&jtm,]*8*G>=t+.jGQ,Jj*&
0!5 ( l/  L 5#% 
iCi? L  J !is+( 0i  Q{iA x( 5  *jG2&_[e*& G5i
i!   !   

H.-FP

fiJournal of Artificial Intelligence Research 14 (2001) 167{203

Submitted 12/99; published 04/01

What's in an Attribute?
Consequences for the Least Common Subsumer

Ralf Kusters

kuesters@ti.informatik.uni-kiel.de

Institut f
ur Informatik und Praktische Mathematik
Christian-Albrechts-Universit
at zu Kiel
24098 Kiel
Germany

Alex Borgida

borgida@cs.rutgers.edu

Department of Computer Science
Rutgers University
Piscataway, NJ 08855
USA

Abstract

Functional relationships between objects, called \attributes", are of considerable importance in knowledge representation languages, including Description Logics (DLs). A study
of the literature indicates that papers have made, often implicitly, different assumptions
about the nature of attributes: whether they are always required to have a value, or whether
they can be partial functions. The work presented here is the first explicit study of this
difference for subclasses of the Classic DL, involving the same-as concept constructor.
It is shown that although determining subsumption between concept descriptions has the
same complexity (though requiring different algorithms), the story is different in the case
of determining the least common subsumer (lcs). For attributes interpreted as partial
functions, the lcs exists and can be computed relatively easily; even in this case our results
correct and extend three previous papers about the lcs of DLs. In the case where attributes
must have a value, the lcs may not exist, and even if it exists it may be of exponential size.
Interestingly, it is possible to decide in polynomial time if the lcs exists.
1. Introduction

Knowledge representation systems based on Description Logics (DLs) have been the subject of continued attention in Artificial Intelligence, both as a subject of theoretical studies
(Borgida, 1994; Baader, 1996; Baader & Sattler, 2000; Giacomo & Lenzerini, 1996; Calvanese, Giacomo, & Lenzerini, 1999b) and in applications (Artale, Franconi, Guarino, &
Pazzi, 1996; Brachman, McGuinness, Patel-Schneider, & Borgida, 1999; McGuinness &
Patel-Schneider, 1998). More impressively, DLs have found applications in other areas involving information processing, such as databases (Borgida, 1995; Calvanese, Lenzerini,
& Nardi, 1999), semi-structured data (Calvanese, Giacomo, & Lenzerini, 1998, 1999a),
information integration (Calvanese, Giacomo, Lenzerini, Nardi, & Rosati, 1998; Borgida
& Kusters, 2000), as well as more general problems such as configuration (McGuinness
& Wright, 1998) and software engineering (Borgida & Devanbu, 1999; Devanbu & Jones,
1997). In fact, wherever the ubiquitous term \ontology" is used these days (e.g., for proc 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fi steres, Borgida
Ku

viding the semantics of web/XML documents), DLs are prime contenders because of their
clear semantics and well-studied computational properties.
In Description Logics, one takes an object-centered view, where the world is modeled as
individuals, connected by binary relationships (here called roles), and grouped into classes
(called concepts). For those more familiar with Predicate Logic, objects correspond to
constants, roles to binary predicates, and concepts to unary predicates. In every DL system,
the concepts of the application domain are described by concept descriptions that are built
from atomic concepts and roles using the \constructors" provided by the DL language. For
example, consider a situation where we want a concept describing individual cars that have
had frequent (at least 10) repairs, and also record the fact that for cars, their model is the
same as their manufacturer's model. Concepts can be thought of as being built up from
(possibly nested) simpler noun-phrases, so the above concept, called Lemon in the sequel,
might be captured as the conjunction of
(objects that are Cars)
(things all of whose model values are in concept Model)
(things all of whose madeBy values are in concept Manufacturer)
(things whose model value is the same as the model of the madeBy attribute)
(things with at least 10 repairs values)
(things all of whose repairs values are RepairReport).
Using the syntax of the classic language, we can abbreviate the above, while emphasizing
the term-like nature of descriptions and the constructors used in each:
(and Car
(all model Model)
(all madeBy Manufacturer)
(same-as (model) (madeBy  model))
(at-least 10 repairs)
(all repairs RepairReport))
So, for example, the concept term (at-least n p) has constructor at-least, and denotes
objects which are related by the relationship p to at least n other objects; in turn, (all p
C) has as instances exactly those objects which are related by p only to instances of C.
Finally, we present the same concept in a mathematical notation which is more succinct
and preferred in formal work on DLs:
Lemon := Car u
8model:Model u
8madeBy:Manufacturer u
madeBy # (model  madeBy) u
 10 repairs u
8repairs:RepairReport

Unlike preceding formalisms, such as semantic networks and frames (Quillian, 1968; Minsky,
1975), DLs are equipped with a formal semantics, which can be given by a translation into
168

fiWhat's in an Attribute?

first-order predicate logic (Borgida, 1994), for example. Moreover, DL systems provide their
users with various inference capabilities that allow them to deduce implicit knowledge from
the explicitly represented knowledge. For instance, the subsumption algorithms allow one to
determine subconcept-superconcept relationships: C is subsumed by D (C v D) if and only
if all instances of C are also instances of D, i.e., the first description is always interpreted
as a subset of the second description. For example, the concept Car obviously subsumes the
concept description Lemon, while (at-least 10 repairs) is subsumed by (at-least 8 repairs).
The traditional inference problems for DL systems, such as subsumption, inconsistency
detection, membership checking, are by now well-investigated. Algorithms and detailed
complexity results for realizing such inferences are available for a variety of DLs of differing
expressive power | see, e.g., (Baader & Sattler, 2000) for an overview.
1.1 Least Common Subsumer

The least common subsumer (lcs) of concepts is the most specific concept description subsuming the given concepts. Finding the lcs was first introduced as a new inference problem
for DLs by Cohen, Borgida, and Hirsh (1992). One motivation for considering the lcs is to
use it as an alternative to disjunction. The idea is to replace disjunctions like C1 t    t C
by the lcs of C1; : : : ; C . Borgida and Etherington (1989) call this operation knowledge-base
vivification. Although, in general, the lcs is not equivalent to the corresponding disjunction,
it is the best approximation of the disjunctive concept within the available language. Using
such an approximation is motivated by the fact that, in many cases, adding disjunction
would increase the complexity of reasoning.1
As proposed by Baader et al. (Baader & Kusters, 1998; Baader, Kusters, & Molitor,
1999), the lcs operation can be used to support the \bottom-up" construction of DL knowledge bases, where, roughly speaking, starting from \typical" examples an lcs algorithm
is used to compute a concept description that (i) contains all these examples, and (ii) is
the most specific description satisfying property (i). Baader and Kusters have presented
such an algorithm for cyclic ALN -concept descriptions; ALN is a relatively simple language allowing for concept conjunction, primitive negation, value restrictions, and number
restrictions. Also, Baader et al. (1999) have proposed an lcs algorithm for a DL allowing
existential restrictions instead of number restrictions.
Originally, the lcs was introduced as an operation in the context of inductive learning
from examples (Cohen et al., 1992), and several papers followed up this lead. The DLs
considered were mostly sublanguages of Classic which allowed for same-as equalities, i.e.,
expressions like (same-as (madeBy) (model  madeBy)). Cohen et al. proposed an lcs
algorithm for ALN and a language that allows for concept conjunction and same-as, which
we will call S . The algorithm for S was extended by Cohen and Hirsh (1994a) to CoreClassic, which additionally allows for value restrictions (see (Cohen & Hirsh, 1994b) for
experimental results). Finally, Frazier and Pitt (1996) presented an lcs algorithm for full
Classic.
n

n

1. Observe that if the language already allows for disjunction, we have lcs(C1 ; : : : ; Cn )  C1 t    t Cn . In
particular, this means that, for such languages, the lcs is not really of interest.

169

fi steres, Borgida
Ku

1.2 Total vs. Partial Attributes

In most knowledge representation systems, including DLs, functional relationships, here
called attributes (also called \features" in the literature), are distinguished as a subclass
of general relationships, at least in part because functional restrictions occur so frequently
in practice2 . In the above example, clearly madeBy and model are meant to be attributes,
thus making unnecessary number restrictions like (and (at-most 1 madeBy) (at-least
1 madeBy)). In addition, distinguishing attributes helps identify tractable subsets of DL
constructors: in Classic, coreferences between attribute chains (as in the above examples)
can be reasoned with eciently (Borgida & Patel-Schneider, 1994), while if we changed to
roles, e.g., allowed (same-as (repairs) (ownedBy  repairsPaidFor)), the subsumption problem becomes undecidable (Schmidt-Schau, 1989).
Whereas the distinction between roles and attributes in DLs is both theoretically and
practically well understood, we have discovered that another distinction, namely the one between attributes being interpreted as total functions (total attributes ) and those interpreted
as partial functions (partial attributes ), has \slipped through the cracks" of contemporary
research. A total attribute always has a value in \the world out there", even if we do not
know it in the knowledge base currently. A partial attribute may not have a value. This
distinction is useful in practice, since there is a difference between a car possibly, but not
necessarily, having a CD player, and the car necessarily having a manufacturer (which just
may not be known in the current knowledge base). The latter is modeled by defining the
attribute madeBy to be a total attribute. Note that with madeBy being a total attribute,
every individual in the world of discourse (not only cars) must have a filler for madeBy.
Since, however, no structural information is provided for fillers of madeBy of non-car individuals, all implications drawn about these fillers are trivial. Thus, making madeBy a total
attribute seems reasonable in this case. A car's CD player, on the other hand, should be
modeled by a partial attribute to express the fact that cars are not required to have a CD
player. To indicate that a particular car does have a CD player, one would have to add the
description (at-least 1 CDplayer).
1.3 New Results

As mentioned above, in conjunction with the same-as constructor, roles and attributes
behave very differently with respect to subsumption. The main objective of this paper is to
show that the distinction between total and partial attributes induces significantly different
behaviour in computing the lcs, in the presence of same-as. More precisely, the purpose of
this paper is twofold.
First, we show that with respect to the complexity of deciding subsumption there is no
difference between partial and total attributes. Borgida and Patel-Schneider (1994) have
shown that when attributes are total, subsumption of classic concept descriptions can
be decided in polynomial time. As shown in the present work, slight modifications of the
algorithm proposed by Borgida and Patel-Schneider suce to handle partial attributes.
2. Readers coming from the Machine Learning community should be aware of the difference between our
\attributes" (functional roles) and their \attributes", which are components of an input feature vector
that usually describes an exemplar.

170

fiWhat's in an Attribute?

Moreover, these modifications do not change the complexity of the algorithm. Thus, partial
and total attributes behave very similarly from the subsumption point of view.
Second, and this is the more surprising result of this paper, the distinction between
partial and total attributes does have a significant impact on the problem of computing the
lcs. Previous results on sublanguages of classic show that if partial attributes are used,
the lcs of two concept descriptions always exists, and can be computed in polynomial time.
If, however, only total attributes are involved, the situation is very different. The lcs need
no longer even exist, and in case it exists its size may grow exponential in the size of the
given concept descriptions. Nevertheless, the existence of the lcs of two concept descriptions
can be decided in polynomial time.
Specifically, in previous work (Cohen et al., 1992; Cohen & Hirsh, 1994a; Frazier &
Pitt, 1996) concerning the lcs computation in classic, constructions and proofs have been
made without realizing the difference between the two types of attributes. Without going
into details here, the main problem for lcs is that merely finite graphs have been employed,
making the constructions applicable only for the partial attribute case. In addition to fixing
these problems, this paper also presents the proper handling of inconsistent concepts in the
lcs algorithm for classic presented by Frazier and Pitt (1996).
Although our results about subsumption are not as intriguing, the proofs to show the
results on the lcs make extensive use of the corresponding subsumption algorithms, which
is one reason we present them beforehand in this paper.
Returning to the general differences between the cases of total and partial attributes,
one could say that the fundamental cause for the differences lies in the same-as constructor,
whose semantics normally requires that (i) the two chains of attributes each have a value,
and (ii) that these values coincide. In the case of total attributes, same-as obeys the principle
C v u # v implies C v u  w # v  w
where u,v, and w are sequences of total attributes, e.g., (madeBy  model), because condition
(i) is ensured by the total aspect of all the attributes. In the case of partial attributes, the
above implication does not hold, because w, and hence u  w, is no longer guaranteed to have
a value, implying that the same-as restriction may not hold. Clearly, this implication affects
the results of subsumption. As far as lcs is concerned, a certain graph (representing the lcs
of the two given concepts) may be infinite in the case of total attributes, thus jeopardizing
the existence of the lcs.
The more general significance of our result is that knowledge representation language
designers and users need to explicitly check at the beginning whether they deal with total or partial attributes because the choice can have significant effects. Although in some
situations total attributes are convenient, to guarantee the existence of attributes without
having to resort to number restrictions, our results show that they can have drawbacks.
All things considered, requiring all attributes to be total appears to be less desirable. Concerning classic, the technical results in this paper support the use of partial attributes
because these ensure the existence of the lcs and its computation in polynomial time as
well as the ecient decision of subsumption. Moreover, the current implementation of the
classic subsumption algorithm does not require major changes in order to handle partial
attributes.
171

fi steres, Borgida
Ku

The outline of this paper is as follows: In the following section, the basic notions necessary for our investigations are introduced. Then, in the two subsequent sections, subsumption and lcs computation in classic with partial attributes is investigated. More precisely,
in Section 3 we offer a subsumption algorithm for the sublanguage classic of classic,
which contains all main classic-constructors; in Section 4, we present an lcs algorithm
for classic concept descriptions, along the lines of that proposed by Cohen and Hirsh
(1994a), and formally prove its correctness, thereby resolving some shortcomings of previous
lcs algorithms, which did not handle inconsistencies properly. Finally, Section 5 covers the
central new result of this paper, i.e., the lcs computation in presence of total attributes.
For this section, we restrict our investigations to the sublanguage S of classic in order to
concentrate on the changes caused by going from partial to total attributes. Nevertheless,
we strongly conjecture that all the results proved in this section can easily be extended to
classic and classic using similar techniques as the one employed in the two previous
sections.
2. Formal Preliminaries

In this section, we introduce the syntax and semantics of the description languages considered in this paper and formally define subsumption and equivalence of concept descriptions.
Finally, the least common subsumer of concept descriptions is specified.
Definition 1 Let C , R, and A be disjoint finite sets representing the set of concept names,
the set of role names, and the set of attribute names. The set of all classic -concept
descriptions over C , R, and A is inductively defined as follows:
 Every element of C is a concept description (concept name, like Car).
 The symbol > is a concept description (top concept, denoting the universe of all
objects).
 If r 2 R is a role and n  0 is a nonnegative integer, then  n r and  n r are concept
descriptions (number restrictions, like  10 repairs).
 If C and D are concept descriptions, then C u D is a concept description (concept
conjunction).




If C is a concept description and r is a role or an attribute, then
description (value restriction, like 8madeBy:Manufacturer).

8r:C

is a concept

If k; h  0 are non-negative integers and a1 ; : : : ; a ; b1 ; : : : ; b 2 A are attributes, then
     a # b1      b is a concept description (same-as equality, like madeBy #
model  madeBy). Note that the two sequences may be empty, i.e., k = 0 or h = 0.
The empty sequence is denoted by ".
k

a1

k

h

h

Often we dispense with  in the composition of attributes. For example, the sequence
a1     a is simply written as a1    a . Moreover, we will use 8r1    r :C as abbreviation
of 8r1:8r2    8r :C , where we have 8":C in case n = 0, and this denotes C .
As usual, the semantics of classic is defined in a model-theoretic way by means of
interpretations.
k

n

k

n

172

fiWhat's in an Attribute?

Definition 2 An interpretation I consists of a nonempty domain I and an interpretation
function I . The interpretation function assigns extensions to atomic identifiers as follows:





The extension of a concept name E is some subset E I of the domain.
The extension of a role name r is some subset rI of I  I .

The extension of an attribute name a is some partial function aI from I to I , i.e.,
if (x; y1 ) 2 aI and (x; y2 ) 2 aI then y1 = y2 .
Given roles or attributes r , we use (r1    r )I to denote the composition of the binary
relations rI . If n = 0 then the result is "I , which denotes the identity relation, i.e., "I :=
f(d; d) j d 2 I g. For an individual d 2 I , we define rI (d) := fe j (d; e) 2 rI g. If the r 's
are attributes, we say that (r1    r )I is defined for d iff (r1    r )I (d) 6= ;; occasionally,
we will refer to (r1    r )(d)I as the image of d under (r1    r )I (d).
The extension C I of a concept description C is inductively defined as follows:
 >I := I ;
i

n

i

i

n

n

n

n

( n r)I := fd 2 I j cardinality(fe 2 I j (d; e) 2 rI g)  ng;
( n r)I := fd 2 I j cardinality(fe 2 I j (d; e) 2 rI g)  ng;
(C u D)I := C I \ DI ;
(8r:C )I := fd 2 I j rI (d)  C I g where r is a role or an attribute;
(a1    a # b1    b )I := fd 2 I j (a1    a )I and (b1    b )I are defined for d
and (a1    a )I (d) = (b1    b )I (d)g:
Note that in the above definition attributes are interpreted as partial functions. Since the
main point of this paper is to demonstrate the impact of different semantics for attributes,
we occasionally restrict the set of interpretations to those that map attributes to total
functions. Such interpretations are called t-interpretations and the attributes interpreted
in this way are called total attributes in order to distinguish them from partial ones.
We stress, as remarked in the introduction, that in the definition of (a1    a # b1    b )I ,
a1    a and b1    b must be defined on d in order for d to satisfy the same-as restriction.
Although this is the standard semantics for same-as equalities, one could also think of
relaxing this restriction. For example, the same-as condition might be specified to hold if
either both paths are undefined or both images are defined and have identical values. A
third definition might be satisfied if even just one of the paths is undefined. Each of these
definitions of the semantics of same-as might lead to different results. However, in this
paper we only pursue the standard semantics.
The subsumption relationship between concept descriptions is defined as follows.
Definition 3 A concept description C is subsumed by the concept description D (C v D
for short) if and only if for all interpretations I , C I  DI . If we consider only total
interpretations, we get t-subsumption: C v D iff C I  DI for all t-interpretations I .






k

h

k

h

k

h

k

k

h

t

173

h

fi steres, Borgida
Ku

Having defined subsumption, equivalence of concept descriptions is defined in the usual way:
 D if and only if C v D and D v C . T-equivalence C  D is specified analogously.
As already mentioned in the introduction, the main difference between partial and total
attributes with respect to subsumption is that u # v v u  w # v  w holds for all attribute
chains u; v; w, whereas it is not necessarily the case that u # v v u  w # v  w.
Finally, before introducing the lcs operation formally and concluding this section, we
comment on the expressive power of classic , since (syntactically) classic lacks some
common constructors. Although classic , as introduced here, does not contain the bottom
concept ? explicitly, it can be expressed by, e.g., ( 1 r) u ( 0 r). We will use ? as an
abbreviation for inconsistent concept descriptions. Furthermore, primitive negation, i.e.,
negation of concept names, can be simulated by number restrictions. For a concept name
E one can replace every occurrence of E by ( 1 r ) and the negation :E of E by ( 0 r )
where r is a new role name. Finally, for an attribute a the following equivalences hold:
( n a)  ? for n  2; ( 1 a)  (a # a); ( 0 a)  >; ( n a)  > for n  1; and
( 0 a)  (8a:?). These show that we do not lose any expressive power by not allowing
for number restrictions on attributes. Still, full classic is somewhat more expressive than
classic . This is mainly due to the introduction of individuals (also called nominals) in
classic. For the sake of completeness we give the syntax of the full classic language.3
This requires a further set, O, representing the set of individual names. Then we can define
two additional concept constructors
 fe1 ; :::; e g, for individuals e 2 O (enumeration as in fF all; Summer; Springg)
 p : e for a role or attribute p, and an individual e (fills as in currentSeason : Summer).
In a technical report, Kusters and Borgida (1999) extend some of the results presented in
this work to full classic, in the case when individuals have a non-standard semantics.
The least common subsumer of a set of concept descriptions is the most specific concept
subsuming all concept descriptions of the set:
Definition 4 The concept description D is the least common subsumer (lcs) of the concept
descriptions C1 ; : : : ; C (lcs(C1 ; : : : ; C ) for short) iff i) C v D for all i = 1; : : : ; n and ii)
for every D0 with that property D v D0 . Analogously, we define lcs (C1 ; : : : ; C ) using v
C

t

t

E

E

E

m

i

n

n

i

t

instead of v.

n

t

Note that the lcs of concept descriptions may not exist, but if it does, by definition it is
uniquely determined up to equivalence. In this sense, we may refer to the lcs.
In the following two sections, attributes are always interpreted as partial functions; only
in Section 5 do we consider total attributes.
3. Characterizing Subsumption in

classic

In this section we modify the characterization of t-subsumption for Classic, as proposed
by Borgida and Patel-Schneider (1994), to handle the case of partial attributes. We do
3. Even here we are omitting constructs dealing with integers and other so-called \host individuals", which
cannot have roles of their own and can only act as role/attribute fillers.

174

fiWhat's in an Attribute?

so in detail, because the tools used for deciding subsumption are intimately related to the
computation of lcs.
T-subsumption in Classic is decided by a multi-part process. First, descriptions are
turned into description graphs. Next, description graphs are put into canonical form, where
certain inferences are explicated and other redundancies are reduced by combining nodes
and edges in the graph. Finally, t-subsumption is determined between a description and a
canonical description graph.
In order to \inherit" the proofs, we have tried to minimize the necessary adjustments to
the specification in (Borgida & Patel-Schneider, 1994). For this reason, roughly speaking,
attributes are treated as roles unless they form part of a same-as equality. (Note that
attributes participating in a same-as construct must have values!) To some extent, this
will allow us to adopt the semantics of the original description graphs, which is crucial for
proofs. However, the two different occurrences of attributes, namely, in a same-as equality
vs. a role in a value-restriction, require us to modify and extend the definition of description
graphs, the normalization rules, and the subsumption algorithm itself.
In the following, we present the steps of the subsumption algorithm in detail. We start
with the definition of description graphs.
3.1 Description Graphs

Intuitively, description graphs reect the syntactic structure of concept descriptions. A
description graph is a labeled, directed multigraph, with a distinguished node. Roughly
speaking, the edges (a-edges ) of the graph capture the constraints expressed by same-as
equalities. The labels of nodes contain, among others, a set of so-called r-edges, which
correspond to value restrictions. Unlike the description graphs defined by Borgida and
Patel-Schneider, here the r-edges are not only labeled with role names but also with attribute
names. (We shall comment later on the advantage of this modification in order to deal with
partial attributes.) The r-edges lead to nested description graphs, representing the concepts
of the corresponding value restrictions.
Before defining description graphs formally, in Figure 1 we present a graph corresponding
to the concept description Lemon defined in the introduction. We use G(Manufacturer),
G(Model), as well as G(RepairReport) to denote description graphs for the concept names
Manufacturer, Model, and RepairReport. These graphs are very simple; they merely consist
of one node, labeled with the corresponding concept name. In general, such graphs can
be more complex since a value restriction like 8r:C leads to a (possibly complex) nested
concept description C .
Although number restrictions on attributes are not allowed, r-edges labeled with attributes, like model and madeBy, always have the restriction [0; 1] in order to capture the
semantics of attributes. Formally, description graphs, nodes, and edges are defined mutually
recursively as follows:
Definition 5 A description graph G is a tuple (N; E; n0 ; l), consisting of a finite set N of
nodes; a finite set E of edges (a-edges); a distinguished node n0 2 N (root of the graph);
and a function l from N into the set of labels of nodes. We will occasionally use the notation
G:N odes, G:Edges, and G:root to access the components N , E and n0 of the graph G.
175

fi steres, Borgida
Ku

f>g

madeBy

f>g

RepairReport)

G(

madeBy

repairs

model

[10; 1]

Model)

G(

model [0; 1]

fCar >g
;

madeBy [0; 1]

Manufacturer)

G(

Figure 1: A description graph for Lemon, where the large node is the root of the graph
An a-edge is a tuple of the form (n1 ; a; n2 ) where n1 , n2 are nodes and a is an attribute
name.
A label of a node is defined to be ? or a tuple of the form (C; H ), consisting of a finite
set C of concept names (the atoms of the node) and a finite set H of tuples (the r-edges of
the node). Concept names in a description graph stand for atomic concept names and >.
We will occasionally use the notation n:Atoms and n:REdges to access the components C
and H of the node n.
An r-edge is a tuple, (r; m; M; G0 ), consisting of a role or attribute name, r; a min, m,
which is a non-negative integer; a max, M , which is a non-negative integer or 1; and a
(recursively nested) description graph G0 . The graph G0 will often be called the restriction
graph of the node for the role r. We require the nodes of G0 to be distinct from all the nodes
of G and other nested description graphs of G. If r is an attribute, then we require: m = 0
and M 2 f0; 1g.

Given a description graph G and a node n 2 G:N odes, we define Gj to be the graph
(N; E; n; l); Gj is said to be rooted at n. A sequence p = n0a1 a2    a n with k  0 and
(n 1; a ; n ) 2 G:Edges, i = 1; : : : ; k, is called path in G from the node n0 to n (p 2 G
for short); for k = 0 the path p is called empty; w = a1    a is called the label of p (the
empty path has label "); p is called rooted if n0 is the root of G. Occasionally, we write
n0 a1    a n 2 G omitting the intermediate nodes.
Throughout this work we make the assumption that description graphs are connected.
A description graph is said to be connected if all nodes of the graph can be reached by a
rooted path and all nested graphs are connected. The semantics of description graphs (see
Definition 6) is not altered if nodes that cannot be reached from the root are deleted.
In order to merge description graphs we need the notion of \recursive set of nodes" of
a description graph G: The recursive set of nodes of G is the union of the nodes of G and
the recursive set of nodes of all nested description graphs of G.
Just as for concept descriptions, the semantics of description graphs is defined by means
of an interpretation I . We introduce a function  which assigns an individual of the domain
of I to every node of the graph. This ensures that all same-as equalities are satisfied.
n

k

n

i

i

i

k

k

k

k

k

176

fiWhat's in an Attribute?

Definition 6 Let G = (N; E; n0 ; l) be a description graph and let I be an interpretation.
An element, d, of I is in GI , iff there is some total function, , from N into I such
that
1.

d

= (n0);

2 N , (n) 2 nI ; and
3. for all (n1 ; a; n2 ) 2 E we have ((n1 ); (n2 )) 2 aI .
The extension nI of a node n with label ? is the empty set. An element, d, of I is in nI ,
2. for all

where

( ) = (C; H ), iff

l n

2 C , we have d 2 B I ; and
for all (r; m; M; G0 ) 2 H ,

1. for all
2.

n

B

(a) there are between m and M elements, d0 , of the domain such that
and
(b) d0 2 G0I for all d0 such that (d; d0 ) 2 rI .

(d; d0 ) 2 rI ;

Cohen and Hirsh (1994a) defined the semantics of description graphs in a different way,
avoiding the introduction of a total function . The problem with their definition is,
however, that it is only well-defined for acyclic graphs, which, for example, excludes sameas equalities of the form " # spouse  spouse, or even p # p  q.
The semantics of the graphs proposed by Borgida and Patel-Schneider (1994) is similar
to Definition 6. However, in that paper a-edges captured not only same-as equalities but
also all value restrictions on attributes. Still, in the context of partial attributes, we could
not define the semantics of description graphs by means of a total function  since some
attributes might not have fillers. Specifying the semantics of description graphs in terms
of partial mappings  would make the definition even longer. Furthermore, the proofs in
(Borgida & Patel-Schneider, 1994) would not carry over as easily. Therefore, in order to
keep  a total function, value restrictions of attributes are initially always translated into redges. The next section will present the translation of concept descriptions into description
graphs in detail.
Having defined the semantics of description graphs, subsumption and equivalence between description graphs (e.g., H v G) as well as concept descriptions and description
graphs (e.g., C v G) is defined in the same way as subsumption and equivalence between
concept descriptions.
3.2 Translating Concept Descriptions into Description Graphs

Following Borgida and Patel-Schneider (1994), a classic concept description is turned
into a description graph by a recursive process. In this process, nodes and description
graphs are often merged.
Definition 7 The merge of two nodes, n1  n2 , is a new node n with the following label:
if n1 or n2 have label ?, then the label of n is ?. Otherwise if both labels are not equal to
?, then n:Atoms = n1:Atoms [ n2:Atoms and n:REdges = n1:REdges [ n2:REdges.
177

fi steres, Borgida
Ku

If G1 = (N1 ; E1 ; n1 ; l1 ) and G2 = (N2 ; E2 ; n2 ; l2 ) are two description graphs with disjoint
recursive sets of nodes, then the merge of G1 and G2 , G := G1 G2 = (N; E; n0 ; l), is defined
as follows:

3.

:= n1  n2;
N := (N1 [ N2 [ fn0 g) n fn1 ; n2 g;
E := (E1 [ E2 )[n1 =n0 ; n2 =n0 ], i.e., E is the union of E1 and E2 where every occurrence

4.

l n

1.
2.

n0

of n1 ; n2 is substituted by n0 ;

( ) := l1(n) for all n 2 N1 n fn1g; l(n) := l2 (n) for all n 2 N2 n fn2 g; and l(n0) is

defined by the label obtained by merging n1 and

n2 .

Now, a classic -concept description C can be turned into its corresponding description
G(C ) by the following translation rules.
1. > is turned into a description graph with one node n0 and no a-edges. The only atom
of the node is > and the set of r-edges is empty.
2. A concept name is turned into a description graph with one node and no a-edges. The
atoms of the node contain only the concept name and the node has no r-edges.
3. A description of the form ( n r) is turned into a description graph with one node and
no a-edges. The node has as its atoms > and it has a single r-edge (r; n; 1; G(>))
where G(>) is specified by the first translation rule.
4. A description of the form ( n r) is turned into a description graph with one node
and no a-edges. The node has as its atom > and it has a single r-edge (r; 0; n; G(>)).
5. A description of the form a1    a # b1    b is turned into a graph with pairwise
distinct nodes n1; : : : ; n 1; m1; : : : ; m 1, the root m0 := n0, and an additional node
n = m := n; the set of a-edges consists of (n0 ; a1 ; n1 ), (n1 ; a2 ; n2 ); : : : ; (n 1 ; a ; n )
and (m0; b1 ; m1 ), (m1; b2 ; m2 ), : : :, (m 1; b ; m ), i.e., two disjoint paths which coincide on their starting point, n0, and their final point, n. (Note that for p = 0 the first
path is the empty path from n0 to n0 and for q = 0 the second path is the empty path
from n0 to n0.) All nodes have > as their only atom and no r-edges.
6. A description of the form 8r:C , where r is a role, is turned into a description graph
with one node and no a-edges. The node has the atom f>g and it has a single r-edge
(r; 0; 1; G(C )).
7. A description of the form 8a:C , where a is an attribute, is turned into a description
graph with one node and no a-edges. The node has the atom f>g and it has a single
r-edge (a; 0; 1; G(C )). (In the work by Borgida and Patel-Schneider, the concept
description 8a:C is turned into an a-edge. As already mentioned, this would cause
problems for attributes interpreted as partial functions when defining the semantics
by means of  as specified in Definition 6.)

graph

p

p

p

q

q

q

p

q

178

q

q

p

p

fiWhat's in an Attribute?

8. To turn a description of the form C u D into a description graph, construct G(C ) and
G(D ) and merge them.
Figure 1 shows the description graph built in this way for the concept Lemon of our example.
It can easily be verified that the translation preserves extensions:
Theorem 1 A concept description C and its corresponding description graph G(C ) are
equivalent, i.e.,C I = G(C )I for every interpretation I .
The main diculty in the proof of this theorem is in showing that merging two description
graphs corresponds to the conjunction of concept descriptions.
Lemma 1 For all interpretations I , if n1 and n2 are nodes, then (n1  n2 )I = nI1 \ nI2 ; if
I
G1 and G2 are description graphs then (G1  G2 )I = GI
1 \ G2 .
The proof of the preceding statement is rather simple and like the one in (Borgida & PatelSchneider, 1994).
3.3 Translating Description Graphs to Concept Descriptions

Although the characterization of subsumption does not require translating description
graphs back to concept descriptions, this translation is presented here to show that concept descriptions and description graphs are equivalent representations of classic concept
descriptions. In subsequent sections, we will in fact need to turn graphs into concept descriptions.
The translation of a description graph G can be specified in a rather straightforward
recursive definition. The main idea of the translation stems from Cohen and Hirsh (1994a),
who employed spanning trees to translate same-as equalities. A spanning tree of a (connected) graph is a tree rooted at the same node as the graph and containing all nodes of the
graph. In particular, it coincides with the graph except that some a-edges are deleted. For
example, one possible spanning tree T for G in Figure 1 is obtained by deleting the a-edge
labeled madeBy, whose origin is the root of G.
Now, let G be a connected description graph and T be a spanning tree for it. Then,
the corresponding concept description C is obtained as a conjunction of the following
descriptions:
1. C contains (i) a same-as equality v # v for every leaf n of T , where v is the label
of the rooted path in T to n; and (ii) a same-as equality v1  a # v2 for each a-edge
(n1 ; a; n2) 2 G:Edges not contained in T , where v is the label of the rooted path to
n in T , i = 1; 2.
2. for every node n in T , C contains a value restriction 8v:C , where v is the label of
the rooted path in T to n, and C denotes the translation of the label of n, i.e., C is
a conjunction obtained as follows:
 every concept name in the atoms of n is a conjunct in C ;
 for every r-edge (r; m; M; G0 ) of n, C contains (a) the number restrictions (mr)
and ( M r) (in case r is a role and M 6= 1) and (b) the value restriction 8r:C 0 ,
where C 0 is the recursively defined translation of G0 .
G

G

i

i

G

n

n

n

n

n

G

G

179

fi steres, Borgida
Ku

In case the set of atoms and r-edges of n is empty, define C := >.
Referring to the graph G in Figure 1, C contains the same-as equalities model  madeBy #
model  madeBy and madeBy # model  madeBy. Furthermore, if n0 denotes the root of G,
C has the value restrictions 8":C , 8model:>, and 8model madeBy:>, where C
corresponds to Lemon as defined in the introduction, but without the same-as equality. Note
that, although in this case the same-as equality model  madeBy # model  madeBy is not
needed, one cannot dispense with 1.(i) in the construction above, as illustrated by the following example: Without 1.(i), the description graph G(a # a) would be turned into the
description >, which is not equivalent to a # a since the same-as equality requires that the
path a has a value, which may not be the case.
It is easy to prove that the translation thus defined is correct in the following sense
(Kusters & Borgida, 1999).
n

G

n0

G

n0

Lemma 2 Every connected description graph G is equivalent to its translation C , i.e., for
all interpretations I : GI = C I .
G

G

3.4 Canonical Description Graphs

In the following we occasionally refer to \marking a node incoherent"; this means that the
label of this node is changed to ?. \Marking a description graph as incoherent" means that
the description graph is replaced by the graph G(?) corresponding to ?, i.e., the graph
consisting only of one node with label ?.
One important property of canonical description graphs is that they are deterministic,
i.e., every node has at most one outgoing edge (a-edge or r-edge) labeled with the same
attribute or role name. Following Borgida and Patel-Schneider (1994), in order to turn a
description graph into a canonical graph we need to merge a-edges and r-edges. In addition,
different from their work, it might be necessary to \lift" r-edges to a-edges.
To merge two a-edges (n; a; n1) and (n; a; n2 ) in a description graph G, replace them
with a single new edge (n; a; n0 ) where n0 is the result of merging n1 and n2. In addition,
replace n1 and n2 by n0 in all other a-edges of G.
In order to merge two r-edges (r; s1 ; k1 ; G1 ), (r; s2 ; k2 ; G2 ) replace them by the new r-edge
(r; max(s1 ; s2 ); min(k1 ; k2 ); G1  G2 ).
To lift up an r-edge (a; m; M; G ) of a node n in a concept graph G with an a-edge
(n; a; n1 ), remove it from n:REdges, and augment G by adding G :N odes to G:N odes,
G :Edges to G:Edges, as well as adding (n; a; G :Root) to G:Edges. A precondition for
applying this transformation is that M = 1, or M = 0 and G corresponds to the graph
G(?). The reason for this precondition is that if an r-edge of the form (a; 0; 0; G ) is lifted
without G being inconsistent, the fact that no a-successors are allowed is lost. Normalization rule 5 (see below) will guarantee that this precondition can always be satisfied.
A description graph G is transformed into canonical form by exhaustively applying the
following normalization rules. A graph is called canonical if none of these rules can be
applied.
1. If some node in G is marked incoherent, mark the description graph as incoherent.
(Reason: Even if the node is not a root, attributes corresponding to a-edges must always
a

a

a

a

a

a

a

180

fiWhat's in an Attribute?

have a value (since they participate in same-as equalities), and this value cannot belong to the

)
2. If some r-edge in a node has its min greater than its max, mark the node incoherent.
(Reason: 2
1
)
3. Add > to the atoms of every node, if absent.
4. If some r-edge in a node has its restriction graph marked incoherent, change its max
to 0. (Reason: ( 0 )
.)
5. If some r-edge in a node has a max of 0, mark its restriction graph as incoherent.
(Reason: See 4.)
6. If some r-edge is of the form (r; 0; 1; G0 ) where G0 only contains one node with empty
set of atoms or with the atoms set to f>g and no r-edges, then remove this r-edge.
(Reason:
.)
7. If some node has two r-edges labeled with the same role, merge the two edges, as
described above. (Reason:
(
).)
8. If some description graph has two a-edges from the same node labeled with the
same attribute, merge the two edges, as described above. (Reason:
(
).)
9. If some node in a graph has both an a-edge and an r-edge for the same attribute, then
\lift up the r-edge" if the precondition is satisfied (see above). (Reason: The value
restrictions imposed on attributes that participate in same-as equalities must be made explicit
and gathered at one place similar to the previous to cases.)
We need to show that the transformations to canonical form do not change the semantics
of the graph. The main diculty is in showing that the merging processes and the lifting
preserve the semantics. The only difference from (Borgida & Patel-Schneider, 1994) is that
in addition to merging r-edges and a-edges we also need to lift up r-edges. Therefore,
we omit the proofs showing that merging edges preserves extensions. The proofs of the
following two lemmas are routine and quite similar to the one of Lemma 5.
Lemma 3 Let G = (N; E; n0 ; l) be a description graph with two mergeable a-edges and let
G0 = (N 0 ; E 0 ; n0 ; l0 ) be the result of merging these two a-edges. Then, G  G0 .
empty set.



r u



r  ?

r

 8r:?

8r:>  >

8r:C u 8r:D  8r: C u D

8a:C u 8a:D 

8a: C u D

Lemma 4 Let n be a node with two mergeable r-edges and let n0 be the node with these
edges merged. Then, nI = n0I for every interpretation I .
Lemma 5 Let G = (N; E; n0 ; l) be a description graph with node n and a-edge (n; a; n00 ).
Suppose n has an associated r-edge (a; m; M; G ). Provided that the precondition for lifting
r-edges is satisfied and that G0 = (N 0 ; E 0 ; n0 ; l0 ) is the result of this transformation, then
G  G0 .
Proof. It is sucient to show that GIj = G0 Ij , since only the label of n is changed in G0
a

and only n obtains an additional a-edge, which points to the graph G not connected to
n

n

a

181

fi steres, Borgida
Ku

RepairReport

G

fModel >g
;

model

repairs

model

[10; 1]

fCar >g
;

madeBy

fManufacturer >g
;

Figure 2: The canonical description graph for Lemon, where the left-most node is the root.
the rest of G0 . W.l.o.g. we therefore may assume that n is the root of G, i.e., n = n0. Let
d 2 GI . Thus, there is a function  from N into I as specified in Definition 6 and an
individual e such that d = (n), e = (n00), and (d; e) 2 aI . This implies e 2 GI . Hence,
there exists a function 0 from G :N odes into I for G and e satisfying the conditions in
Definition 6. Since the sets of nodes of G and G are disjoint, we can define 00 to be the
union of  and 0, i.e., 00 (m) := (m) for all nodes m in G and 00(m) := 0(m) for all
nodes m in G . Since, by construction, for the additional a-edge (n; a; G :Root) 2 E 0 we
have (00(n); 00 (G :Root)) 2 aI , it follows that all conditions in Definition 6 are satisfied
for d and G0, and thus, d 2 G0I .
Now let d 2 G0I . Thus, there is a function 00 from N 0 into I according to Definition 6.
Let e := 00(G :Root) = 00(n00). Let G00 be the description graph we obtain from G0 by
deleting the nodes corresponding to G , which is the same graph as G without the r-edge
(a; m; M; G ). If we restrict 00 to the nodes of G00 , then it follows d 2 G00I . Furthermore,
restricting 00 to the nodes of G yields e 2 GI . In particular, G can not be marked
incoherent. Then, our precondition ensures M = 1. Thus, since e is the only a-successor of
d, we can conclude d 2 GI .
ut
a

a

a

a

a

a

a

a

a

a

a

a

a

Having dealt with the issue of merging and lifting, it is now easy to verify that \normalization" does not affect the meaning of description graphs.
Theorem 2 If G is a description graph and G0 is the corresponding canonical description
graph, then G  G0 .

As an example, the canonical description graph of the graph given in Figure 1 is depicted
in Figure 2.
3.5 Subsumption Algorithm

The final part of the subsumption process is checking to see if a canonical description graph
is subsumed by a concept description. As in Borgida and Patel-Schneider (1994), where
attributes are total, it turns out that it is not necessary to turn the potential subsumer
into a canonical description graph. The subsumption algorithm presented next can also be
considered as a characterization of subsumption.
182

fiWhat's in an Attribute?

Algorithm 1 (Subsumption Algorithm) Given a concept description D and description graph G = (N; E; n0 ; l), subsumes?(D; G) is defined to be true if and only if one of the
following conditions hold:
1. The description graph
2.

D

3.

D

4.

D

5.

D

G

is marked incoherent.

is a concept name or >, and D is an element of the atoms of n0 .

is ( n r) and i) some r-edge of n0 has r as its role, and min greater than or equal
to n; or ii) n = 0.

is ( n r) and some r-edge of n0 has r as its role, and max less than or equal to n.
is

   a # b1    b

, and there are rooted paths with label
in G ending at the same node.
a1

n

m

a1

a

n

and

b1

b

m

is 8r:C , for a role r, and either (i) some r-edge of n0 has r as its role and G0
as its restriction graph with subsumes?(C; G0 ); or (ii) subsumes?(C; G(>)). (Reason:
8r:>  >.)
7. D is 8a:C , for an attribute a, and (i) some a-edge of G is of the form (n0 ; a; n0 ), and
subsumes?(C; (N; E; n0 ; l)); or (ii) some r-edge of n0 has a as its attribute, and G0 as
its restriction graph with subsumes?(C; G0 ); or (iii) subsumes?(C; G(>)).

6.

D

8.

D

is E u F and both subsumes?(E; G) and subsumes?(F; G) are true.

There are only two differences between this algorithm and the one for total attributes presented by Borgida and Patel-Schneider (see also Algorithm 2). First, in the partial attribute
case, given D = 8a:C , one needs to look up the value restriction either in some a-edge or
some r-edge of G, since attributes can label both a-edges and r-edges. (In the total attribute
case, attributes can only label a-edges so that examining r-edges was not necessary.) The
second and most important distinction is the treatment of same-as equalities. As shown in
the above algorithm, with D = a1    a # b1    b one only needs to check whether there
exist two paths labeled v := a1    a and w := b1    b leading the same node in G. In the
total attribute case, however, it suces if there exist prefixes v0 and w0 of v and w with this
property, as long as the remaining suxes are identical.
Soundness and completeness of this algorithm is stated in the following theorem.
Theorem 3 Let C , D be classic descriptions. Then, C v D iff subsumes?(D; G ),
where G is the canonical form of G(C ).
The soundness of the subsumption algorithm, i.e., the if direction in the theorem stated
above, is pretty obvious. As in (Borgida & Patel-Schneider, 1994), the main point of the
only-if direction (proof of completeness) is that the canonical graph G is deterministic,
i.e., from any node, given a role or attribute name r, there is at most one outgoing r-edge
or a-edge with r as label. We point the reader to (Borgida & Patel-Schneider, 1994) for
the proof, since it is almost identical to the one for total attributes already published there.
These proofs reveal that, for the if direction of Theorem 3, description graphs need not be
normalized. Thus, one can also show:
n

m

n

m

C

C

C

183

fi steres, Borgida
Ku

Remark 1 Let G be some (not necessarily normalized description graph) and let
concept description. Then, subsumes?(D; G) implies G v D.

D

be a

classic

Borgida and Patel-Schneider argue that the canonical description graph G of a concept
description C can be constructed in time polynomial in the size of C . Furthermore, Algorithm 1 runs in time polynomial in the size of G and D. It is not hard to see that the
changes presented here do not increase the complexity. Thus, soundness and completeness
of the subsumption algorithm provides us with the following corollary.
Corollary 1 Subsumption for classic concept descriptions C and D, where attributes
are interpreted as partial functions, can be decided in time polynomial in the size of C and
D.
4. Computing the LCS in

classic

In this section, we will show that the lcs of two classic concept descriptions can be stated
in terms of a product of canonical description graphs. A similar result has been proven by
Cohen and Hirsh (1994a) for a sublanguage of classic , which only allows for concept
names, concept conjunction, value restrictions, and same-as equalities. In particular, this
sublanguage does not allow for inconsistent concept descriptions (which, for example, can be
expressed by conicting number-restrictions). Furthermore, the semantics of the description
graphs provided by Cohen and Hirsh restricts the results to the case when description graphs
are acyclic. This excludes, for example, same-as equalities of the form  # spouse  spouse.
In the following, we first define the product of description graphs. Then, we show that
for given concept descriptions C and D, the lcs is equivalent to a description graph obtained
as the product of G and G . Our constructions and proofs will be quite close to those in
(Cohen & Hirsh, 1994a).
C

D

4.1 The Product of Description Graphs

A description graph represents the constraints that must be satisfied by all individuals in the
extension of the graph. Intuitively, the product of two description graphs is the intersection
of these constraints|as the product of finite automata corresponds to the intersection of the
words accepted by the automata. However, in the definition of the product of description
graphs special care has to be taken of incoherent nodes, i.e., nodes labeled with ?. Also,
since attributes may occur both in r-edges and a-edges, one needs to take the product
between restriction graphs of r-edges, on the one hand, and the original graphs G1 or G2
(rooted at certain nodes), on the other hand.
Definition 8 Let G1 = (N1 ; E1 ; n1 ; l1 ) and G2 = (N2 ; E2 ; n2 ; l2 ) be two description graphs.
Then, the product G := G1  G2 := (N; E; n0 ; l) of the two graphs is recursively defined as
follows:
1.
2.
3.

:= N1  N2 ;
n0 := (n1 ; n2 );
E :=f((n; n0 ); a; (m; m0 )) j (n; a; m) 2 E1 and (n0 ; a; m0 ) 2 E2 g;
N

184

fiWhat's in an Attribute?

4. Let n 2 N1 and n0 2 N2 . If l1 (n) = ?, then let l((n; n0 )) := l2 (n0 ) and, analogously,
if l2 (n0 ) = ?, then l((n; n0 )) := l1 (n). Otherwise, for l1 (n) = (S1 ; H1 ) and l2 (n0 ) =
(S2 ; H2), define l((n; n0)) := (S; H ) where
(a)
(b)

S

:= S1 \ S2;

:=
(
(
)
(
)
) j (r; p1 ; q1 ; G1 ) 2 H1 , (r; p2 ; q2 ; G2 ) 2 H2 g [
f(a; 0; 1; G1 m  G2 ) j (n; a; m) 2 E1 , (a; p2 ; q2 ; G2 ) 2 H2 g [
f(a; 0; 1; G1  G2 m ) j (a; p1 ; q1 ; G1 ) 2 H1 , (n ; a; m) 2 E2 g.
H

0

0

0

f r; min p1 ; p2 ; max q1 ; q2 ; G1  G2
0

j

0

0

0

0

j

0

According to this definition, if in the tuple (n; n0) some node, say n, is incoherent, then
the label of (n; n0 ) coincides with the one for n0. The reason for defining the label in this
way is that lcs(?; C )  C for every concept description C . This has been overlooked by
Frazier and Pitt (1996), thus making their constructions and proofs only hold for concept
descriptions that do not contain inconsistent subexpressions.
Note that G, as defined here, might not be connected, i.e., it might contain nodes that
cannot be reached from the root n0. Even if G1 and G2 are connected this can happen
because all tuples (n1; n2 ) belong to the set of nodes of G regardless of whether they are
reachable from the root or not. However, as already mentioned in Section 3.1 we may
assume G to be connected.
Also note that the product graph can be translated back into a classic concept
description since the product of two description graphs is once again a description graph.
4.2 Computing the LCS

We now prove the main theorem of this subsection, which states that the product of two
description graphs is equivalent to the lcs of the corresponding concept descriptions.
Theorem 4 Let C1 and C2 be two concept descriptions, and let G1 and G2 be corresponding
canonical description graphs. Then, C 1  2  lcs(C1 ; C2 ).
G

G

Let G := G1  G2 . We will only sketch the proof showing that C subsumes C1 and,
by symmetry, also C2 (see (Kusters & Borgida, 1999) for details). By construction, if there
are two rooted paths to a common node in G, then G1 has corresponding paths leading to
the same node as well. Thus, by Theorem 3, the same-as equalities in C subsume the ones
in C1 . Now, let T be a spanning tree of G, (m1; m2 ) be a node in G, and v be the label of
the rooted path in T to (m1 ; m2 ). Then, by construction it follows that there exists a rooted
path in G1 to m1 labeled v. Furthermore, a rather straightforward inductive proof shows
that the concept description E corresponding to the label of (m1 ; m2 ) subsumes G1 j .
This implies 8v:E w G1 . As a result, we can conclude G w G1 .
The more interesting part of the proof is to show that C is not only a common subsumer
of C1 and C2, but the least common subsumer.
We now show by induction over the size of D, C1, and C2 that if D subsumes C1 and
C2 , then D subsumes C : We distinguish different cases according to the definition of
\subsumes?". Let G1 = (N1 ; E1 ; n1; l1 ) be the canonical description graph of C1, G2 =
(N2 ; E2 ; n2; l2 ) be the canonical description graph of C2 , and G = (N; E; n0 ; l) = G1  G2 .
In the following, we assume that C1 v D and C2 v D; thus, subsumes?(D; G1) and
Proof.

G

G

m1

G

G

185

fi steres, Borgida
Ku

?(D; G2). We show that subsumes?(D; G). Then, Remark 1 implies G v D, and
thus, C v D. Note that one cannot use Theorem 3 since G might not be a canonical
description graph.
1. If G is incoherent, then there is nothing to show.
2. If D is a concept name, >, or a number-restriction, then by definition of the label of
n0 it is easy to see that subsumes?(D; G).
3. If D is v # w, then there exist nodes m1 in G1 and m2 in G2 such that there are two
paths from n1 to m1 with label v and w, respectively, as well as two paths from n2 to
m2 with label v and w. Then, by definition of G it is easy to see that there are two
paths from n0 = (n1; n2 ) to (m1 ; m2) with label v and w, respectively. This shows
subsumes?(D; G).
4. If D is 8r:C , r a role or attribute, then one of several cases applies:
(i) n1 and n2 have r-edges with role or attribute r, and restriction graphs G01 and G02 ,
respectively, such that subsumes?(C; G01 ) and subsumes?(C; G02 );
(ii) without loss of generality, n1 has an a-edge pointing to m1 with attribute r, such
that subsumes?(C; G01 ), where G01 := G1 j ; and n2 has an r-edge with restriction
graph G02 such that subsumes?(C; G02 ).
In both cases (i) and (ii), subsumes?(C; G01  G02 ) follows by induction. Furthermore,
by definition of G there is an r-edge with role r and restriction graph G01  G02 for n0.
This implies subsumes?(D; G).
(iii) n1 and n2 have a-edges with attribute r leading to nodes m1 and m2 , respectively. Then, subsumes?(C; G1 j ) and subsumes?(C; G2 j ). By induction, we know
subsumes?(C; G1 j
 G2 j ). It is easy to see that Gj(
 G2 j . Fur) = G1 j
thermore, by definition there is an a-edge with attribute r from (n1; n2 ) to (m1 ; m2 )
in G. This shows subsumes?(D; G).
(iv) (without loss of generality) n1 has no r-edge and no a-edge with role or attribute
r . This implies subsumes?(C; G(>)), which also ensures subsumes?(D; G).
5. If D is E u F , then by definition of the subsumption algorithm, subsumes?(E; G1 ) and
subsumes?(E; G2 ) hold. By induction, we have subsumes?(E; G), and analogously,
subsumes?(F; G). Thus, subsumes?(D; G).
ut
As stated in Section 3.5, a canonical description graph for a classic concept description
can be computed in time polynomial in the size of the concept description. It is not hard
to verify that the product of two description graphs can be computed in time polynomial in
the size of the graphs. In addition, the concept description corresponding to a description
graph can be computed in time polynomial in the size of the graph. Thus, as a consequence
of Theorem 4 we obtain:
subsumes
G

m1

m1

m1

m2

m2

m1 ;m2

m1

m2

Corollary 2 The lcs of two classic concept descriptions always exists and can be computed in time polynomial in the size of the concept descriptions.
186

fiWhat's in an Attribute?

j , j 6= i

j , j 6= i

i

a

a

a

i

a

Figure 3: The canonical description graph for D , without node labels.
i

As intimated in (Cohen et al., 1992), this statement does not hold for sequences of concept
descriptions. Intuitively, generalizing the lcs algorithm to sequences of, say, n concept descriptions, means computing the product of n description graphs. The following proposition
shows that the size of such a product graph may grow exponentially in n. Thus, the lcs
computed in this way grows exponentially in the size of the given sequence. However, this
does not imply that this exponential blow-up is unavoidable. There might exist a smaller,
still equivalent representation of the lcs. Nevertheless, we can show that the exponential
growth is inevitable.
Proposition 1 For all integers n  2 there exists a sequence D1 ; : : : ; D of classic
concept descriptions such that the size of every classic concept description equivalent to
lcs(D1 ; : : : ; D ) is at least exponential in n where the size of the D 0 s is linear in n.
Proof. As in Cohen et al. (1992), for a given n, define the concept descriptions D as
follows:
D := u (" # a ) u u (a # a a ) u (" # a a )
6=
6=
n

n

i

i

i

j

j

i

j

i

i

i

j

i

i

where a1 ; : : : ; a denote attributes. The canonical description graph for D is depicted in
Figure 3. Using Algorithm 1 it is easy to see that D v v # w iff the number of a 0 s in v and
the number of a 0 s in w are equal modulo 2 where v; w are words over fa1 ; : : : ; a g. This
implies that
D1 ; : : : ; D v v # w
iff for all 1  i  n the number of a 0s in v and
(1)
0
the number of a s in w are equal modulo 2.
Let s  f1; : : : ; ng be a non-empty set. We define v := a    a k where i1 <    < i
are the elements of s and w := a 3 a 3    a k 3 with a 3 := a a a . Now let E be the lcs
of D1 ; : : : ; D , and let G be the corresponding canonical description graph with root n0.
From (1) we know that E v v # w for every s  f1; : : : ; ng. Algorithm 1 implies that
the paths from n0 in G labeled v and w exist and that they lead to the same node q .
Assume there are non-empty subsets s; t of f1; : : : ; ng, s 6= t, such that q = q . This would
imply E v v # v in contradiction to (1). Thus, s 6= t implies q 6= q . Since there are
2 1 non-empty subsets of f1; : : : ; ng, this shows that G contains at least 2 1 nodes.
The fact that the size of G is linear in the size of E completes the proof.
ut
This proposition shows that algorithms computing the lcs of sequences are necessarily worstcase exponential. Conversely, based on the polynomial time algorithm for the binary lcs
operation, an exponential time algorithm can easily be specified employing the following
identity lcs(D1 ; : : : ; D )  lcs(D ; lcs(D 1 ; lcs(   lcs(D2 ; D1 )   ).
n

i

i

i

i

n

n

i

i

i1

s

i1

s

n

i2

i

j

i

j

j

k

j

E

s

s

E

s

s

s

s

s

t

s

n

E

E

n

n

n

187

t

t

n

fi steres, Borgida
Ku

Corollary 3 The size of the lcs of sequences of classic concept descriptions can grow
exponentially in the size of the sequences and there exists an exponential time algorithm for
computing the lcs.
5. The LCS for Same-as and Total Attributes

In the previous sections, attributes were interpreted as partial functions. In this section,
we will present the significant changes in computing the lcs that occur when considering
total functions instead of partial functions. More precisely, we will look at a sublanguage
S of classic that only allows for concept conjunction and same-as equalities, but where
we have the general assumption that attributes are interpreted as total functions.
We restrict our attention to the language S in order to concentrate on the changes
caused by going from partial to total functions. We strongly conjecture, however, that the
results represented here can easily be transfered to classic by extending the description
graphs for S as in Section 4.
First, we show that in S the lcs of two concept descriptions does not always exist.
Then, we will present a polynomial decision algorithm for the existence of an lcs of two
concept descriptions. Finally, it will be shown that if the lcs of two concept descriptions
exists, then it might be exponential in the size of the given concept descriptions and it can
be computed in exponential time.
In the sequel, we will simply refer to the lcs by lcs. Since throughout the section
attributes are always assumed to be total, this does not lead to any confusion.
Once again, it may be useful to keep in mind that for total (though not partial) attributes
we have (u # v) v (u  w # v  w) for any u; w; v 2 A, where A is the set of finite words
over A, the finite set of attribute names. Indeed, all the differences between partial and
total attributes shown in this section finally trace back to this property.
t

t

t

t

t

5.1 The Existence of the LCS

In this subsection, we prove that the lcs of two concept descriptions in S does not always
exist. Nevertheless, there is always an infinite representation of the lcs, which will be used
in the next subsection to characterize the existence of the lcs.
To accomplish the above, we return to the graph-based characterization of t-subsumption
proposed by Borgida and Patel-Schneider (1994), and modified for partial attributes in Section 3. For a concept description C , let G denote the corresponding canonical description
graph, as defined in Section 3.4. Its semantics is specified as in Section 3.1, although now
the set of interpretations is restricted to allow attributes to be interpreted as total functions
only.
Since S contains no concept names and does not allow for value-restrictions, the nodes
in G do not contain concept names and the set of r-edges is empty. Therefore, G can
be defined by the triple (N; E; n0 ) where N is a finite set of nodes, E is a finite set over
N  A  N , and n0 is the root of the graph.
As a corollary of the results of Borgida and Patel-Schneider, subsumption C v D of
concept descriptions C and D in S can be decided with the following algorithm, which also
provides us with a characterization of t-subsumption.
C

C

C

t

188

fiWhat's in an Attribute?

G

C0 :

D0 :

c

G
a

a

b

b

d

d
c

Figure 4: The canonical graphs for C0 and D0
Algorithm 2 Let C , D be concept descriptions in S , and G = (N; E; n0 ) be the canonical
description graph of C . Then, subsumes ?(D; G ) is defined to be true if and only if one
of the following conditions hold:
C

t

1.

C

is v # w and there are words v0 ; w0 ; u 2 A such that v = v0 u and w = w0 u, and
there are rooted paths in G labeled v0 and w0 , respectively, ending at the same node.

D

C

) and subsumes ?(D2 ; G ) are true.
Apart from the additional constructors handled by Algorithm 1, Algorithm 2 only differs
from Algorithm 1 in that, for total attributes, as considered here, it is sucient if prefixes
of rooted paths v and w lead to a common node, as long as the remainder in both cases is
the same path.
2.

D

is D1 u D2 and both subsumes ?(D1 ; G
t

C

t

C

Theorem 5 There are concept descriptions in S such that the lcs of these concept descriptions does not exist in S .

This result corrects the statement of Cohen et al. (1992) that the lcs always exists, a
statement that inadvertently assumed that attributes were partial, not total.
As proof, we offer the following S -concept descriptions, which are shown not to have an
lcs:
C0 := a # b;
D0 := a # ac u b # bc u ad # bd:
The graphs for these concepts are depicted in Figure 4.
The following statement shows that an lcs E of C0 and D0 would satisfy a condition
which does not have a \regular structure". This statement can easily be verified using
Algorithm 2.
E v v#w
iff v = w or there exists a nonnegative integer n and u 2
A such that v = ac du and w = bc du or vice versa.
Given this description of the lcs of C0 and D0 , one can show, again, by employing Algorithm 2, that no finite description graph can be equivalent to E . However, we omit this
elementary proof here, because the absence of the lcs also follows from Theorem 6, where
infinite graphs are used to characterize the existence of an lcs. Note that in the partial
attribute case, the lcs of C0 and D0 is equivalent to a # a u b # b, a result that can be
t

n

189

n

fi steres, Borgida
Ku

obtained by the lcs algorithm presented in the previous section. The corresponding (finite) description graph consists of a root and two additional nodes, where the root has two
outgoing edges leading to the two nodes and labeled a and b, respectively.
To state Theorem 6, we first introduce infinite description graphs and show that there
always exists an infinite description graph representing the lcs of two S -concept descriptions.
An infinite description graph G is defined, like a finite graph, by a triple (N; E; n0 )
except that the set of nodes N and the set of edges E may be infinite. As in the finite case,
nvn0 2 G means that G contains a path from n to n0 labeled with the word v 2 A . The
semantics of infinite graphs is defined as in the finite case. Furthermore, infinite graphs are
translated into concept descriptions as follows: take an (infinite) spanning tree T of G, and,
as in the finite case, for every edge of G not contained in it, add to C a same-as equality.
Note that in contrast to the partial attribute case, C need not contain same-as equalities of
the form v # v since, for total attributes, v # v  >. Still, C might be a concept description
with an infinite number of conjuncts (thus, an infinite concept description). The semantics
of such concept descriptions is defined in the obvious way. Analogously to Lemma 2, one
can show that an (infinite) graph G and its corresponding (infinite) concept description C
are equivalent, i.e., C  G.
We call an (infinite) description graph G deterministic if, and only if, for every node n
in G and every attribute a 2 A there exists at most one a-successor for n in G. The graph
G is called complete if for every node n in G and every attribute a 2 A there is (at least)
one a-successor for n in G. Clearly, for a deterministic and complete (infinite) description
graph, every path is uniquely determined by its starting point and its label.
Algorithm 2 (which deals with finite description graphs G ) can be generalized to deterministic and complete (infinite) description graphs G in a straightforward way. To see
this, first note that a (finite) description graph coming from an S -concept description is
canonical iff it is deterministic in the sense just introduced. Analogously, a deterministic
infinite graph can be viewed as being canonical. Thus, requiring (infinite) graphs to be
deterministic satisfies the precondition of Algorithm 2. Now, if in addition these graphs are
complete, then (unlike the condition stated in the subsumption algorithm) it is no longer
necessary to consider prefixes of words because a complete graph contains a rooted path
for every word. More precisely, if v0 and w0 lead to the same node, then this is the case for
v = v 0 u and w = w0 u as well, thus making it unnecessary to consider the prefixes v 0 and w0
of v and w, respectively. Summing up, we can conclude:
Corollary 4 Let G = (N; E; n0 ) be a deterministic and complete (infinite) description
G

G

G

G

G

C

graph and

v; w

2 A. Then,
Gv v#w
iff
t

n0 vn

2 G and n0wn 2 G for some node n:

We shall construct an (infinite) graph representing the lcs of two concept descriptions in S
as the product of the so-called completed canonical graphs. This infinite representation of
the lcs will be used later to characterize the existence of an lcs in S , i.e., the existence of a
finite representation of the lcs.
We now define the completion of a graph. Intuitively, a graph is completed by iteratively
adding outgoing a-edges labeled with an attribute a for every node in the graph that does
not have such an outgoing a-edge. This process might extend a graph by infinite trees. As
an example, the completion of G (cf. Figure 4) is depicted in Figure 5 with A = fa; b; c; dg.
C0

190

fiWhat's in an Attribute?

G

1
C :
0

a

d



c

c

b

b

d

a

a

b

c

d

a

b

c

d

...
...
Figure 5: The complete graph for C0
Formally, completions are defined as follows: Let G be an (infinite) description graph.
The graph G0 is an extension of G if for every node n in G and for every attribute a 2 A
such that n has no outgoing edges labeled a, a new node m is added, as well as an edge
(n; a; m ). Now, let G0 ; G1 ; G2 ; : : : be a sequence of graphs such that G0 = G and G +1 is
an extension of G ; for i  0. If G = (N ; E ; n0), then
[ [
G1 := (
N;
E ; n0 )
n;a

i

n;a

i

i

i

i

0

i

0
construction, G1
i

i

i

is called the completion of G. By
is a complete graph. Furthermore, if
1
G is deterministic, then G is deterministic as well. Finally, it is easy to see that a graph
and its extension Sare equivalent. Thus, by induction, G1  G.
The nodes in 1 N , i.e., the nodes in G1 that are not in G, are called tree nodes; the
nodes of G are called non-tree nodes. By construction, for every tree node t in G1 there is
exactly one direct predecessor of t in G1, i.e., there is exactly one node n and one attribute
a such that (n; a; t) is an edge in G1 ; n is called a-predecessor of t. Furthermore, there is
exactly one youngest ancestor n in G of a tree node t in G1; n is the youngest ancestor of
t if there is a path from n to t in G1 which does not contain non-tree nodes except for n.
Note that there is only one path from n to t in G1. Finally, observe that non-tree nodes
have only non-tree nodes as ancestors.
Note that the completion of a canonical description graph is always complete and deterministic.
In the sequel, let C , D be two concept descriptions in S , G = (N ; E ; n ), G =
(N ; E ; n ) be their corresponding canonical graphs, and G1 , G1 be the completions of
G , G . The products G := G  G and G
1 := G1  G1 are specified as in Definition 1.
As usual, we may assume G and G1 are connected, i.e., they only contain nodes that are
reachable from the root (n ; n ); otherwise, one can remove all those nodes that cannot be
reached from the root without changing the semantics of the graphs.
We denote the product G1  G1 by G1 instead of G1 (or G1) because otherwise
this graph could be confused with the completion of G. In general, these graphs do not
t

i

i

C

D

C

D

D

C

D

C

C

D

C

D

C

D

191

D

C

D

C

C

D

fi steres, Borgida
Ku

coincide. As an example, take the products G  G and G1  G1 (see Figure 4 for the
graphs G and G ). The former product results in a graph that consists of a root with
two outgoing a-edges, one labeled a and the other one labeled b. (As mentioned before, this
graph corresponds to the lcs of C0 and D0 in the partial attribute case.) The product of
the completed graphs, on the other hand, is a graph that is obtained as the completion of
the graph depicted in Figure 6 (the infinite trees are omitted for the sake of simplicity).
As an easy consequence of the fact that G  G1 and Corollary 4, one can prove the
following lemma.
C0

Lemma 6

C0

D0

C

C

C0

D0

D0

C

v v # w iff n
t

C

vn

2 G1 and n
C

C

wn

2 G1 for a node n in G1 .
C

C

But then, by the construction of G1 we know:
Proposition 2 C v
for a node n in G1 .

t

v

# w and D v v # w iff (n
t

C

) 2 G1 and (n

; nD vn

C

)

; nD wn

2 G1

In particular, G1 represents the lcs of the concept descriptions C and D in the following
sense:
Corollary 5 The (infinite) concept description C 1 corresponding to G1 is the lcs of C
and D, i.e., i) C; D v C 1 and ii) C; D v E 0 implies C 1 v E 0 for every S -concept
description E 0 .
G

t

t

G

t

G

5.2 Characterizing the Existence of an LCS

Let C , D be concept descriptions in S and let the graphs G , G , G, G1 , G1, and G1
be defined as above.
We will show that G1 not only represents a (possibly infinite) lcs of the S -concept
descriptions C and D (Corollary 5), but that G1 can be used to characterize the existence
of a finite lcs. The existence depends on whether G1 contains a finite or an infinite number
of so-called same-as nodes.
Definition 9 A node n of an (infinite) description graph H is called a same-as node if
C

there exist two direct predecessors of
may be labeled differently.)

a

n

in

H.

D

(The a-edges leading to

c

c

c

d

d

d


b

d

d

d

c

c

c

Figure 6: A subgraph of G1  G1
C0

192

D0

C

n

D

from these nodes

fiWhat's in an Attribute?

For example, the graph depicted in Figure 6 contains an infinite number of same-as nodes.
We will show that this is a sucient and necessary condition for the lcs of C0 and D0 not
to exist.
It is helpful to observe that same-as nodes in G1 have one of the forms (g; f ), (f; t),
and (t; f ), where g and f are non-tree nodes and t is a tree node. There cannot exist a
same-as node of the form (t1; t2 ), where both t1 and t2 are tree nodes, since tree nodes
only have exactly one direct predecessor, and thus (t1; t2 ) does. Moreover, if G1 has an
infinite number of same-as nodes, it must have an infinite number of same-as nodes of the
form (f; t) or (t; f ), because there only exist a finite number of nodes in G1 of the form
(g; f ). For this reason, in the following lemma we only characterize same-as nodes of the
form (f; t). (Nodes of the form (t; f ) can be dealt with analogously.) To state the lemma,
recall that with n0un1vn2 2 H , for some graph H , we describe a path in H labeled uv
from n0 to n2 that passes through node n1 after u (i.e., n0un1 2 H and n1vn2 2 H ); this
is generalized the obvious way to interpret n0u1n1u2n2u3n3 2 H .
G

1

(nC ; nD )
v

w

(h1 ; p0 )

x

(h2 ; p0 )

G

..
.

6=
6
1=

v
h

w
h2

..
.x

(e1 ; q0 )

(e2 ; q0 )
a

e1

6=

e2

a
n

= (f; t)

Figure 7: same-as nodes in G1
Lemma 7 Given a node f in G and a tree-node t in G1 , the node n = (f; t) in G1 is a
same-as node iff
C




D

there exist nodes (h1 ; p0 ), (h2 ; p0 ) in G, h1 6= h2 ;
there exist nodes (e1 ; q0 ), (e2 ; q0 ) in G1 , where e1 , e2 are distinct nodes in G and
q0 is a node in G1 ; and
there exists an attribute a 2 A and v; w; x 2 A , v 6= w, where A is the set of attributes
in C ,
C

D



such that

(n

C

) (

) (

) ( ) and (n

) ( ) ( )
For the direct successors (h01 ; p00 ) and (h02 ; p00 ) of (h1 ; p0 )

; nD v h1 ; p0 x e1 ; q0 a f; t

C

) (

; nD w h2 ; p0 x e2 ; q0 a f; t

are paths in G1 (see Figure 7).
and (h2 ; p0 ) in this paths, we, in addition, require p00 to be a tree node in G1 .4
D

4. Note that since G
1 is deterministic, the successors of (h1 ; p0 ) and (h2 ; p0 ) in the two paths must in fact
be of the form (; p00 ).

193

fi steres, Borgida
Ku

The if direction is obvious. We proceed with the only-if direction and assume that
is a same-as node in G1. Let p0 be the (uniquely determined) youngest ancestor of t in
G1 . In particular, p0 is a node in G and there exists p0 xq0 at in G1 with a 2 A and
x 2 A such that the successor of p0 in this path is a tree node in G .
Since n is a same-as node and t can only be reached via q0 and the attribute a, there
must exist e1 , e2 in G , e1 6= e2 , with (e1 ; q0 )a(f; t); (e2 ; q0)a(f; t) 2 G1. Since G1 is
connected, there are paths from (n ; n ) to (e1 ; q0) and (e2 ; q0). Every path from n to q0
must pass through p0 and the sux of the label of this path is x. Consequently, there exist
nodes h1 ; h2 in G such that (h1 ; p0 )x(e1 ; q0 )a(f; t) and (h2 ; p0)x(e2 ; q0)a(f; t) are paths
in G1. In particular, xa is a label of a path from h1 to f in G , and the label xa only
consists of attributes contained in C . If h1 = h2, then this, together with the fact that G
is deterministic, would imply e1 = e2 . Hence, h1 6= h2 . Let v, w be the labels of the paths
from (n ; n ) to (h1 ; p0 ) and (h2 ; p0), respectively. As G is deterministic and h1 6= h2, it
follows that v 6= w.
ut
The main results of this section is stated in the next theorem. As a direct consequence of
this theorem, we obtain that there exists no lcs in S for the concept descriptions C0 and
D0 of our example.
Proof.
n

D

D

D

D

C

C

D

D

C

C

C

C

D

Theorem 6 The lcs of C and D exists iff the number of same-as nodes in G1 is finite.

We start by proving the only-if direction. For this purpose, we assume that G1
contains an infinite number of same-as nodes and show that there is no (finite) lcs for C
and D in S .
As argued before, we may assume that G1 contains an infinite number of same-as nodes
of the form (f; t) or (t; f ), where t is a tree node and f is a non-tree node. More precisely,
say G1 contains for every i  1 nodes n = (f ; t ) such that f is a node in G and t is
a tree node in G1. According to Lemma 7, for every same-as node n there exist nodes
h1 ; h2 ; e1 ; e2 in G , p0 in G , and q0 in G1 as well as a 2 A and x 2 A with the
properties required in Lemma 7.
Since G and G are finite description graphs, the number of tuples of the form
h1 ; h2 ; e1 ; e2 ; f ; a is finite. Thus, there must be an infinite number of i's yielding
the same tuple h1; h2 ; e1 ; e2 ; f; a. In particular, h1 6= h2 and e1 6= e2 are nodes in G and
there is an infinite number of same-as nodes of the form n = (f; t1 ). Finally, as in the
lemma, let v, w be the label of paths (in G) from (n ; n ) to (h1 ; p0 ) and (h2 ; p0 ).
Now, assume there is an lcs E of C and D in S . According to Corollary 5, E  C 1 .
Let G be the finite canonical graph for E with root n0. By Proposition 2 and Lemma 7
we know E v vx a # wx a. From Algorithm 2 it follows that there are words v0, w0 , and u
such that vx a = v0 u and wx a = w0 u, where the paths in G starting from n0 labeled v0 ,
w0 lead to the same node in G .
If u 6= ", then u = u0a for some word u0. Then, Algorithm 2 ensures E v vx # wx .
However, by Lemma 7 we know that the words vx and wx lead to different nodes in
G
1 , namely, (e1 ; q0 ) and (e2 ; q0 ), which, with Proposition 2, leads to the contradiction
E  G
1 6v vx # wx . Thus, u = ".
As a result, for every i  1 there exists a node q in G such that n0vx aq and n0wx aq
are paths in G . Because G is a finite description graph, there exist i; j  1, i 6= j , with
Proof.

i

i

i

i

C

D

;i

;i

;i

;i

C

C

;i

;i

i

i

;i

D

;i

i

D

i

D

;i

;i

i

i

C

i

C

;i

D

t

G

E

t

i

i

i

i

E

E

t

i

;i

t

i

i

;i

i

i

E

i

i

E

194

E

i

i

i

i

fiWhat's in an Attribute?

= q . By Algorithm 2, this implies E v vx a # wx a. On the other hand, the path in
starting from (n ; n ) with label vx a leads to the node n and the one for wx a leads
to n . Since n 6= n , Proposition 2 implies E 6v vx a # wx a, which is a contradiction. To
sum up, we have shown that there does not exist an lcs for C and D in S .
This shows that there is no lcs of C , D in S which completes the proof of the only-if
direction.
We now prove the if direction of Theorem 6. For this purpose, we assume that G1 has
only a finite number of same-as nodes. Note that every same-as node in G1 has only a
finite number of direct predecessors. To see this, two cases are distinguished: i) a node of
the form (g1 ; g2 ) in G has only predecessors in G; ii) if t is a tree node and g a non-tree node,
then a predecessor of (g; t) in G1 is of the form (g0 ; t0) where t0 is the unique predecessor
(tree or non-tree node) of t and g0 is a non-tree node. Since the number of nodes in G
and G is finite, in both cases we only have a finite number of predecessors. But then, the
spanning tree T of G1 coincides with G1 except for a finite number of edges because, if T
does not contain a certain edge, then this edge leads to a same-as node. As a result, C 1
is an S -concept description because it is a finite conjunction of same-as equalities. Finally,
Corollary 5 shows that C 1 is the lcs of C and D.
ut
If v # w is a conjunct in C 1 , then v and w lead from the root of G1 to a same-as node.
As mentioned before, same-as nodes are of the form (f; g); (f; t), or (t; f ), where t is a tree
node and f; g are non-tree nodes. Consequently, v and w must be paths in G or G .
Thus, they only contain attributes occurring in C or D.
qi

G
1

j

t

C

j

i

D

i

j

i

i

j

t

i

j

j

C

D

G

G

G

C

D

Corollary 6 If the lcs of two concept description C and D in S exists, then there is a
concept description in S only containing attributes occurring in C or D that is equivalent
to the lcs.

Therefore, when asking for the existence of an lcs, we can w.o.l.g. assume that the set of
attributes A is finite. This fact will be used in the following two subsections.
5.3 Deciding the Existence of an LCS

From the following corollary we will derive the desired decision algorithm for the existence
of an lcs of two concept descriptions in S . To state the corollary we need to introduce the
language L C (q1 ; q2) := fw 2 A j there is a path from the node q1 to q2 in G labeled wg.
Since description graphs can be viewed as finite automata, such a language will be regular.
Moreover, let aA denote the set faw j w 2 Ag for an attribute a 2 A, where A is a finite
alphabet.
G

C

Corollary 7 G1 contains an infinite number of same-as nodes iff either
(i) there exist nodes (h1 ; p0 ), (h2 ; p0 ) in G as well as nodes f , e1 , e2 in G , and attributes
a; b 2 A such that
C

1.

h1

6= h2 , e1 6= e2 ;

2.

p0

does not have a b-successor in G ;

3.

(e1 ; a; f ), (e2 ; a; f ) are edges in G

D

C

; and
195

fi steres, Borgida
Ku

4.

(

LGC h1 ; e1

) \ L C (h2 ; e2 ) \ bA is an infinite set of words;
G

or
(ii) the same statement as (i) but with r^oles of C and

D

switched.

We first prove the only-if direction. Assume that G1 contains an infinite number
of same-as nodes. Then, w.l.o.g., we find the configuration in G1 described in the proof
of Theorem 6. This configuration satisfies the conditions 1. and 3. stated in the corollary.
If, for i 6= j , the words x and x coincide, we can conclude n = n because G1 is a
deterministic graph. However, by definition, n 6= n . Hence, x 6= x . Because A is finite,
we can, w.l.o.g., assume that all x 's have b 2 A as their first letter for some fixed b. Thus,
condition 4. is satisfied as well. According to the configuration, the b-successor of (; p0 ) in
G
1 is of the form (; p00 ) where p00 is a tree node. Thus, p0 does not have a b-successor in
G , which means that condition 3. is satisfied.
We now prove the if direction of the corollary. For this purpose, let bx 2 L C (h1 ; e1 ) \
L C (h2 ; e2 ) \ bA . Since p0 has no b-successor in G it follows that there are tree nodes
t; t0 in G1 such that p0 bxtat0 2 G1 . Thus, we have (h1 ; p0 )bx(e1 ; t)a(f; t0 ) 2 G
1 and
0

(h2 ; p0 )bx(e2 ; t)a(f; t ) 2 G1. Since e1 6= e2, we can conclude (e1 ; t) 6= (e2 ; t). This means
that (f; t0) is a same-as node. Analogously, for by 2 L C (h1 ; e1 ) \ L C (h2 ; e2 ) \ bA there
are tree nodes s; s0 in G1 such that p0bysas0 2 G1 and (f; s0) is a same-as node in G1.
Since bx and by both start with b, and the b-successor of p0 in G1 is a tree node, x 6= y
implies s0 6= t0. Hence, (f; t0) and (f; s0) are distinct same-as nodes. This shows that if the
set L C (h1 ; e1 ) \ L C (h2 ; e2 ) \ bA is infinite, G1 must have an infinite number of same-as
nodes.
ut
For given nodes (h1 ; p0), (h2 ; p0 ) in G, attributes a; b 2 A, nodes f; e1; e2 2 G the conditions 1. to 3. in Corollary 7 can obviously be checked in time polynomial in the size of the
concept descriptions C and D. As for the last condition, note that an automaton accepting
the language L C (h1 ; e1 ) \ L C (h2 ; e2 ) \ bA can be constructed in time polynomial in the
size of C . Furthermore, for a given finite automaton it is decidable in time polynomial in
the size of the automaton if it accepts an infinite language (see the book by Hopcroft and
Ullman (1979) for details). Thus, condition 4. can be tested in time polynomial in the size
of C and D as well. Finally, since the size of G and G is polynomial in the size of C and D,
only a polynomial number of configurations need to be tested. Together with Corollary 7
these complexities provide us with the following corollary.
Proof.

i

j

i

i

j

j

i

j

i

D

G

G

D

D

D

G

D

G

D

D

G

G

C

G

G

C

Corollary 8 For given concept descriptions C and D in S it is decidable in time polynomial
in the size of C and D whether lcs of C and D exists in S .
5.4 Computing the LCS

In this subsection, we first show that the size of an lcs of two S -concept descriptions may
grow exponentially in the size of the concept descriptions. This is a stronger result than
that presented for partial attributes, where it was only shown that the lcs of a sequence of
concept descriptions in S can grow exponentially. Then, we present an exponential time lcs
algorithm for S -concept descriptions.
196

fiWhat's in an Attribute?

GC 0

:

GDk

:

a

c

c



a

b

a

d

d

c

c

b

a


d

d

k

Figure 8: The canonical description graphs for C 0 and D

k

In order to show that the lcs may be of exponential size, we consider the following
example, where A := fa; b; c; dg.We define
0 := a # b;
C
D
:= u=1 ac # ad u u=1 bc # bd u ac a # bc a:
The corresponding canonical description graphs G 0 and G k are depicted in Figure 8.
A finite graph representing the lcs of C 0 and D is depicted in Figure 9 for k = 2.
This graph can easily be derived from G10  G1k . The graph comprises two binary trees
of height k, and thus, it contains at least 2 nodes. In the following, we will show that
there is no canonical description graph G k (with root n0) representing the lcs E of C 0
and D with less than 2 nodes. Let x 2 fc; dg be a word of length k over fc; dg, and let
v := axa, w := bxa. Using the canonical description graphs G 0 and G k it is easy to see
that C 0 v v # w and D v v # w. Thus, E v v # w. By Algorithm 2, this means that
there are words v0; w0 ; u such that v = v0 u, w = w0 u, and there are paths from n0 labeled
v 0 and w0 in G k leading to the same node in G k . Suppose u 6= ". Then, Algorithm 2
implies E v ax # bx. But according to G , D 6v ax # bx. Therefore u must be the empty
k

k

i

k

i

i

i

i

k

k

i

D

C

k

D

C

k

E

k

k

k

k

D

C

t

t

k

t

k

E

k

E

t

D

a

c

t

b

d

d

c

2
c

d

a

c

a

a

d

d

a

a

c

a

d

a

c

a

Figure 9: A finite graph representing the lcs of C 0 and D2
197

fi steres, Borgida
Ku

word ". This proves that in G k there is a path from n0 labeled axa for every x 2 fc; dg .
Hence, there is a path for every ax. Now, let y 2 fc; dg be such that x 6= y. If the paths
for ax and ay from n0 in G k lead to the same node, then this implies E v ax # ay in
contradiction to C 0 6v ax # ay. As a result, ax and ay lead to different nodes in G k . Since
fc; dg contains 2 words, this shows that G k has at least 2 nodes. Finally, taking into
account that the size of a canonical description graph of a concept description in S is linear
in the size of the corresponding description we obtain the following theorem.
k

E

k

E

k

t

t

k

E

k

k

E

Theorem 7 The lcs of two S -concept descriptions may grow exponentially in the size of
the concepts.

The following (exponential time) algorithm computes the lcs of two S -concept descriptions
in case it exists.
Algorithm 3
Input: concept descriptions C , D in S , for which the lcs exists in S ;
Output: lcs of C and D in S ;
1. Compute G0 := G

G

C

D

;

2. For every combination

 of nodes (h1 ; p0 ), (h2 ; p0) in G = G  G , h1 6= h2 ;
 a 2 A, e1 ; e2 ; f in G , e1 6= e2 , where (e1 ; a; f ) and (e2 ; a; f ) are edges in G
C

D

C

extend G0 as follows: Let G
in

C

h1 ;t

,G

h2 ;t

be two trees representing the (finite) set of words

0
L := @L C (h1 ; e1 ) \ L C (h2 ; e2 ) \
G

[

G

b

62succ(

p0

)

1 (
f"g; if a 62 succ(p0)
bA A [

;

;

otherwise

where succ(p0 ) := fb j p0 has a b-successorg and the set of nodes of G 1 , G 2 , and
G0 are assumed to be disjoint. Now, replace the root of G 1 by (h1 ; p0 ), the root of
G 2 by (h2 ; p0 ), and extend G0 by the nodes and edges of these two trees. Finally,
add a new node n for every word v in L, and for each node of the trees G 1 and
G 2 reachable from the root of G 1 and G 2 by a path labeled v , add an edge with
label a from it to n . The extension is illustrated in Figure 10.
h ;t

h ;t

h ;t

h ;t

v

h ;t

h ;t

h ;t

h ;t

v

3. The same as in step 2, with r^oles of C and D switched.
4. Compute the canonical graph of G0 , which is called G0 again. Then, output the concept
description C 0 of G0 .
G

Proposition 3 The translation
of C and D.

CG0

of the graph G0 computed by Algorithm 3 is the lcs E
198

fiWhat's in an Attribute?

0

G

(h1 ; p0 )

(h2 ; p0 )

b

a

b

a

b

n

a
c

a

a
c

a

bc

n
d

a

a

d

bad

n

Figure 10: The extension at the nodes (h1 ; p0), (h2 ; p0 ) in G0 where L = fb; bc; badg
It is easy to see that if there are two paths in G0 labeled y1 and y2 leading from
the root (n ; n ) to the same node, then G1 contains such paths as well. Consequently,
(E  )G1 v G0.
Now, assume E v y1 # y2, y1 6= y2. By Proposition 2 we know that there are paths
in G1 labeled y1 and y2 leading to the same node n. W.l.o.g, we may assume that n is a
same-as node in G1. Otherwise, there exist words y10; y2 0; u with y1 = y10u, y2 = y20u such
that y10 and y20 lead to a same-as node. If we can show that G0 contains paths labeled y10
and y20 leading to the same node, then, by Algorithm 2, this is sucient for G0 v y1 # y2.
So let n be a same-as node. We distinguish two cases:
1. If n is a node in G = G  G , then the paths for y1 and y2 are paths in G. Since G
is a subgraph of G0 this holds for G0 as well. Hence, C 0 v y1 # y2.
2. Assume n is not a node in G. Then, since n is a same-as node, we know that n is of the
form (f; t) or (t; f ) where f is a non-tree node and t is a tree node. By symmetry, we
may assume that n = (f; t). Now it is easy to see that there exist nodes h1 ; h2 ; e1 ; e2 in
G , p0 in G , and a tree node q0 in G1 as well as a 2 A and x; v; w 2 A as specified
in Lemma 7 such that y1 = vxa and y2 = wxa. But then, with h1 ; h2 ; e1 ; e2 ; p0; f and
a the preconditions of Algorithm 3 are satisfied and x 2 L. Therefore, by construction
of G0 there are paths labeled y1 and y2, respectively, leading from the root to the same
node.
ut
We note that the product G of G and G can be computed in time polynomial in the
size of C and D. Furthermore, there is only a polynomial number of combinations of nodes
(h1 ; p0 ), (h2 ; p0) in G, e1 ; e2 ; f in G , a 2 A. Finally, the finite automaton for L can be
computed in time polynomial in the size of C and D. In particular, the set of states of this
automaton can polynomially be bounded in the size of C and D. If L contained a word
longer than the number of states, the accepting path in the automaton contains a cycle. But
then, the automaton would accept infinitely many words, in contradiction to the assumption
that L is finite. Thus, the length of all words in L can be bounded polynomially in the
size of C and D. In particular, this means that L contains only an exponential number of
words. Trees representing these words can be computed in time exponential in the size of
C and D .
Proof.

C

t

D

t

t

t

C

D

G

C

D

D

C

D

C

199

t

fi steres, Borgida
Ku

Corollary 9 If the lcs of two S -concept descriptions exists, then it can be computed in time
exponential in the size of the concept descriptions.
6. Conclusion

Attributes | binary relations that can have at most one value { have been distinguished
in many knowledge representation schemes and other object-centered modeling languages.
This had been done to facilitate modeling and, in description logics, to help identify tractable
sets of concept constructors (e.g., restricting same-as to attributes). In fact, same-as restrictions are quite important from a practical point of view, because they support the modeling
of actions and their components (Borgida & Devanbu, 1999).
A second distinction, between attributes as total versus partial functions, had not been
considered so essential until now. This paper has shown that this distinction can sometime
have significant effects.
In particular, we have first shown that the approach for computing subsumption of
Classic concepts with total attributes, presented by Borgida and Patel-Schneider (1994),
can be modified to accommodate partial attributes, by treating partial attributes as roles
until they participate in same-as restrictions, in which case they are \converted" to total attributes. As a result, we obtain polynomial-time algorithms for subsumption and
consistency checking in this case also.
In the case of computing least common subsumers, which was introduced as a technique
for learning non-propositional descriptions of concepts, we first noted that several of the
papers in the literature (Cohen & Hirsh, 1994a; Frazier & Pitt, 1996) (implicitly) used
partial attributes, when considering Classic. Furthermore, these papers used a weaker
version of the \concept graphs" employed in (Borgida & Patel-Schneider, 1994), which
make the results only hold for the case of same-as restrictions that do not generate \cycles".
Furthermore, the algorithm proposed by Frazier and Pitt (1996) does not handle inconsistent
concepts, which can easily arise in Classic concepts as a result of conicts between lower
and upper bounds of roles.
Therefore, we have provided an lcs algorithm together with a formal proof of correctness
for a sublanguage of Classic with partial attributes, which allows for same-as equalities
and inconsistent concepts | the algorithm and proofs can easily be extended to full Classic
(Kusters & Borgida, 1999). In this case, the lcs always exists, and it can be computed in
time polynomial in the size of the two initial concept descriptions. As shown by Cohen et al.
(1992), there are sequences of concept descriptions for which the lcs may grow exponentially
in the size of the sequence.
To complete the picture, and as the main part of the paper, we then examined the
question of computing lcs in the case of total attributes. Surprisingly, the situation here
is very different from the partial attribute case (unlike with subsumption). First, for the
language S the lcs may not even exist. (The existence of the lcs mentioned by Cohen et al.
(1992) is due to an inadvertent switch to partial semantics for attributes.) Nevertheless,
the existence of the lcs of two concept descriptions can be decided in polynomial time. But
if the lcs exists, it may grow exponentially in the size of the concept descriptions, and hence
the computation of the lcs may take time exponential in the size of the two given concept
descriptions.
200

fiWhat's in an Attribute?

As an aside, we note that it has been pointed out by Cohen et al. (1992) that concept
descriptions in S correspond to a finitely generated right congruence. Furthermore, in this
context the lcs of two concept descriptions is the intersection of right congruences. Thus,
the results presented in this paper also show that the intersection of finitely generated
right congruences is not always a finitely generated right congruence, and that there is a
polynomial algorithm for deciding this question. Finally, if the intersection can be finitely
generated, then the generating system may be exponential and can be computed with
an exponential time algorithm in the size of the generating systems of the given right
congruences.
The results in this paper therefore lay out the scope of the effect of making attributes
be total or partial functions in a description logic that supports the same-as constructor.
Moreover, we correct some problems and extend results in the previous literature.
We believe that the disparity between the results in the two cases should serve as a
warning to other researchers in knowledge representation and reasoning, concerning the
importance of explicitly considering the difference between total and partial attributes.
Acknowledgments

The authors wish to thank the anonymous reviewers for their helpful comments. This
research was supported in part by NSF Grant IRI-9619979. It was carried out while the
first author was at the Rutgers University and the RWTH Aachen.
References

Artale, A., Franconi, E., Guarino, N., & Pazzi, L. (1996). Part-Whole Relations in ObjectCentered Systems: An Overview. Data & Knowledge Engineering, 20 (3), 347{383.
Baader, F. (1996). A Formal Definition for the Expressive Power of Terminological Knowledge Representation Languages. Journal of Logic and Computation, 6 (1), 33{54.
Baader, F., & Kusters, R. (1998). Computing the Least Common Subsumer and the Most
Specific Concept in the Presence of Cyclic ALN -Concept Descriptions. In Herzog,
O., & Gunter, A. (Eds.), Proceedings of the 22nd Annual German Conference on
Artificial Intelligence, KI-98, Vol. 1504 of Lecture Notes in Computer Science, pp.
129{140 Bremen, Germany. Springer{Verlag.
Baader, F., Kusters, R., & Molitor, R. (1999). Computing Least Common Subsumers in
Description Logics with Existential Restrictions. In Dean, T. (Ed.), Proceedings of the
16th International Joint Conference on Artificial Intelligence (IJCAI'99), pp. 96{101
Stockholm, Sweden. Morgan Kaufmann Publishers.
Baader, F., & Sattler, U. (2000). Tableaux Algorithms for Description Logics. In Proceedings of the International Conference on Automated Reasoning with Analytic Tableaux
and Related Methods (TABLEAUX 2000), Vol. 1847 of Lecture Notes in Artifical Intelligence, pp. 1{18 University of St. Andrews, Scotland.
201

fi steres, Borgida
Ku

Borgida, A. (1994). On The Relationship Between Description Logic and Predicate Logic.
In Proceedings of the Third International Conference on Information and Knowledge
Management (CIKM'94), pp. 219{225 Gaithersburg, Maryland. ACM Press.
Borgida, A. (1995). Description logics in data management. IEEE Trans. on Knowledge
and Data Engineering, 7 (5), 671{682.
Borgida, A., & Devanbu, P. (1999). Adding more "DL" to IDL: towards more knowledgeable
component inter-operability. In Proceedings of the 1999 International Conference on
Software Engineering, pp. 378{387 Los Angeles, CA USA. ACM.
Borgida, A., & Etherington, D. (1989). Hierarchical Knowledge Bases and Ecient Disjunctive Reasoning. In Brachman, R., & H.J. Levesque, R. R. (Eds.), Proceedings
of the 1st International Conference on Principles of Knowledge Representation and
Reasoning (KR'89), pp. 33{43 Toronto, Canada. Morgan Kaufmann Publishers.
Borgida, A., & Kusters, R. (2000). What's not in a name: Some Properties of a Purely
Structural Approach to Integrating Large DL Knowledge Bases. In Baader, F., &
Sattler, U. (Eds.), Proceedings of the 2000 International Workshop on Description
Logics (DL2000), No. 33 in CEUR-WS Aachen, Germany. RWTH Aachen.
Borgida, A., & Patel-Schneider, P. (1994). A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic. Journal of Artificial Intelligence Research, 1, 277{308.
Brachman, R., McGuinness, D., Patel-Schneider, P., & Borgida, A. (1999). \Reducing"
CLASSIC to Practice: Knowledge Representation Theory Meets Reality. Artificial
Intelligence, 114 (1{2), 203{237.
Calvanese, D., Giacomo, G. D., & Lenzerini, M. (1998). What can Knowledge Representation do for Semi-Structured Data?. In Proceedings of the 16th National Conference
of the American Association for Artificial Intelligence, AAAI-98, pp. 205{210. AAAI
Press/The MIT Press.
Calvanese, D., Giacomo, G. D., & Lenzerini, M. (1999a). Modeling and Querying SemiStructured Data. Network and Information Systems, 2 (2), 253{273.
Calvanese, D., Giacomo, G. D., & Lenzerini, M. (1999b). Reasoning in Expressive Description Logics with Fixpoints based on Automata on Infinite Trees. In Dean, T. (Ed.),
Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI'99), pp. 84{89 Stockholm, Sweden. Morgan Kaufmann Publishers.
Calvanese, D., Giacomo, G. D., Lenzerini, M., Nardi, D., & Rosati, R. (1998). Description
Logic Framework for Information Integration. In Cohn, A., Schubert, L., & Shapiro, S.
(Eds.), Proceedings of the 6th International Conference on the Principles of Knowledge
Representation and Reasoning (KR-98), pp. 2{13 Trento, Italy. Morgan Kaufmann
Publishers.
Calvanese, D., Lenzerini, M., & Nardi, D. (1999). Unifying Class-Based Representation
Formalisms. Journal of Artificial Intelligence Research, 11, 199{240.
202

fiWhat's in an Attribute?

Cohen, W. W., & Hirsh, H. (1994a). Learnability of description logics with equality constraints. Machine Learning, 17 (2/3), 169{199.
Cohen, W. W., & Hirsh, H. (1994b). Learning the CLASSIC Description Logic: Theoretical and Experimental Results. In Doyle, J., Sandewall, E., & Torasso, P. (Eds.),
Proceedings of the Fourth International Conference on Principles of Knowledge Representation and Reasoning (KR'94), pp. 121{133 Bonn, Germany. Morgan Kaufmann
Publishers.
Cohen, W., Borgida, A., & Hirsh, H. (1992). Computing Least Common Subsumers in Description Logics. In Swartout, W. (Ed.), Proceedings of the 10th National Conference
on Artificial Intelligence, pp. 754{760 San Jose, CA. MIT Press.
Devanbu, P., & Jones, M. (1997). The Use of Description Logics in KBSE Systems. ACM
Transactions on Software Engineering and Methodology (TOSEM), 6 (2), 141{172.
Frazier, M., & Pitt, L. (1996). Classic learning. Machine Learning Journal, 25, 151{193.
Giacomo, G. D., & Lenzerini, M. (1996). TBox and ABox reasoning in expressive description logics. In Aiello, L., Doyle, J., & Shapiro, S. (Eds.), Proceedings of the 5th
International Conference on Principles of Knowledge Representation and Reasoning
(KR'96), pp. 316{327 Boston, USA. Morgan Kaufmann Publishers.
Hopcroft, J., & Ullman, J. (1979). Introduction to Automata Theory. Addison-Wesley Publ.
Co.
Kusters, R., & Borgida, A. (1999). What's in an Attribute? Consequences for the Least
Common Subsumer. Tech. rep. DCS-TR-404, Rutgers University, USA. Available via
ftp://ftp.cs.rutgers.edu/pub/technical-reports/.
McGuinness, D., & Patel-Schneider, P. (1998). Usability Issues in Knowledge Representation Systems. In Proceedings of the 15th National Conference on Artificial Intelligence
(AAAI-98) and of the 10th Conference on Innovative Applications of Artificial Intelligence (IAAI-98), pp. 608{614 Menlo Park. AAAI Press.
McGuinness, D., & Wright, J. (1998). An industrial strength Description Logic-based
configurator platform. IEEE Intelligent Systems, 13 (4), 66{77.
Minsky, M. (1975). A framework for representing knowledge. In Winston, P. (Ed.), The
Psychology of Computer Vision McGraw-Hill, New York.
Quillian, M. (1968). Semantic memory. In Minsky, M. (Ed.), Semantic Information Processing, pp. 216{270 Cambridge, Mass. MIT Press.
Schmidt-Schau, M. (1989). Subsumption in KL-ONE is undecidable. In Brachman, R. J.
(Ed.), Proceedings of the 1st International Conference on Principles of Knowledge
Representation and Reasoning (KR'89), pp. 421{431 Toronto, Ont. Morgan Kaufmann
Publishers.
203

fiJournal of Artificial Intelligence Research 14 (2001) 1-28

Submitted 2/00; published 1/01

On Reachability, Relevance, and Resolution in the Planning
as Satisfiability Approach
Ronen I. Brafman

brafman@cs.bgu.ac.il

Department of Computer Science, Ben-Gurion University
P.O.Box 653, Beer-Sheva 84105, Israel

Abstract

In recent years, there is a growing awareness of the importance of reachability and
relevance-based pruning techniques for planning, but little work specifically targets these
techniques. In this paper, we compare the ability of two classes of algorithms to propagate
and discover reachability and relevance constraints in classical planning problems. The first
class of algorithms operates on SAT encoded planning problems obtained using the linear
and Graphplan encoding schemes. It applies unit-propagation and more general resolution steps (involving larger clauses) to these plan encodings. The second class operates at
the plan level and contains two families of pruning algorithms: Reachable-k and Relevantk . Reachable-k provides a coherent description of a number of existing forward pruning
techniques used in numerous algorithms, while Relevant-k captures different grades of backward pruning. Our results shed light on the ability of different plan-encoding schemes to
propagate information forward and backward and on the relative merit of plan-level and
SAT-level pruning methods.
1. Introduction

The success of the planning as satisfiability (PAS) approach (Kautz & Selman, 1992, 1996)
has led to various attempts to refine the initial methods used and to improve our understanding of its performance. In particular, various methods for generating formulas from
planning instances have been compared (Ernst, Millstein, & Weld, 1997), and various systematic alternatives to the original stochastic method have been examined (e.g., Bayardo
& Schrag, 1997; Li & Anbulagan, 1997). Still, many issues surrounding this approach are
poorly understood. In particular, little is known about the inuence of the encoding method
on performance.
Concentrating on the two encoding methods proposed by Kautz and Selman (1996), the
linear and the Graphplan-based encodings, we examine their inuence on the ability to
propagate reachability and relevance information via unit propagation and, more generally,
k -clause resolution. We do so by comparing the pruning ability of these techniques to that
of a class of algorithms for reachability and relevance analysis that operate on the original
problem formulation: Reachable-k and Relevant-k. Reachable-k is a simplified variant of
a similar algorithm for state pruning in Markov decision processes (Boutilier, Brafman, &
Geib, 1998), while Relevant-k is a natural counterpart used for relevance analysis. Both
algorithms provide a coherent framework for discussing different grades of reachability and
relevance-based pruning methods that appear in the literature.
Our work is motivated by the growing role that forward and backward pruning methods
play in current planning algorithms and the important role of propagation techniques in

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBrafman

all SAT solvers used in the planning as satisfiability approach. Unit propagation plays
a central role in the Davis-Putnam algorithm (Davis & Putman, 1960) and many of its
offsprings (e.g., Crawford & Auton, 1993; Freeman, 1995; Gomes, Selman, & Kautz, 1998;
Li & Anbulagan, 1997) and it is used as a preprocessing step when stochastic methods are
applied. Moreover, a limited form of binary propagation is used in Crawford's Compact
program for simplifying CNF formulas which is utilized in the Blackbox planner (Kautz
& Selman, 1999). Our results shed some light on the relationship between these pruning
techniques.
The paper is organized as follows. Section 2 provides background material, describing
the basic ideas of the PAS framework and the Graphplan algorithm. In Section 3, we
discuss Reachable-k , an algorithm for performing reachability analysis, and compare its
ability to prune possible actions to that of k-clause resolution. In Section 4 we describe
Relevant-k which is similar to Reachable-k and is applied to relevance analysis. Again, we
compare it to methods based on resolution. In Section 5 we empirically compare the results
of various methods for k = 1; 2. We conclude with a discussion of future and related work
in Section 6. Proofs appear in the appendix, but their main arguments are described in the
body of the paper.
2. Background

The Graphplan algorithm (Blum & Furst, 1997) and the Satplan algorithm (Kautz
& Selman, 1996) have profoundly altered the direction of research within the planning
community. Two of the main concepts studied in this paper, reachability analysis and plan
encodings, have become central to current planning research thanks to these planners. We
briey discuss these planners, and in particular, their aspects pertaining to our topic.

2.1 Reachability Analysis in Graphplan
The purpose of reachability analysis is to discover unreachable states of the world and
infeasible actions, i.e., actions that cannot be performed in the course of a successful plan.
By discovering such actions ahead of time, we reduce the space that needs to be searched to
find a valid plan. In principle, full edged reachability analysis requires forward search in
the space of possible states. This is a very expensive operation, and instead, we can opt for
sound, but incomplete methods. Such methods do not discover all the actions that can be
ruled out. However, any action that they rule out is infeasible and need not be considered
when searching for a plan.
The Graphplan planner provides a good example of the utility of approximate reachability analysis. Graphplan has two main stages: In the first stage, approximate reachability analysis is conducted, yielding a data-structure called the planning graph which
represents a sound, but incomplete, approximation of the set of states reachable from the
initial state. In the second stage, Graphplan searches for a plan over this structure.
Graphplan's planning graph construction algorithm presents a particularly good tradeoff
between computational complexity and pruning power, and its utility in pruning the search
space is attested to by the planner's good performance.
The planning graph construction algorithm can be viewed as generating a list of annotated sets. The odd elements in this list contain sets of propositions. The even elements
2

fiReachability, Relevance, and Resolution

in this list contain sets of actions. Each such set is annotated with mutual exclusion constraints on its members. Intuitively, the i action set contains the list of actions that could
be performed at the i step of a concurrent action plan (i.e., a plan allowing for concurrent
execution of actions that do not interfere with each other). The i proposition set contains
propositions that could hold after i 1 (sets of concurrent) actions have been performed.
The mutual exclusion constraints circumscribe these sets by indicating that certain pairs of
actions or propositions cannot occur at the same time at a particular stage. Hence, if the
propositions p; q appear in the i proposition set, then it is possible (or more accurately
{ the algorithm does not rule out the possibility) that an i-step plan applied at the initial
state could lead to a world in which p and/or q hold. However, if p and q are marked
mutually exclusive then we know that p and q cannot hold together after some i-step plan
has been performed.
The sets are constructed as follows: The first proposition set contains the propositions
that hold at the initial state. The first action set contains the actions that can be performed
at the initial state. In general, the i set of propositions contains the effects of actions in
the i-1 action set, and the i action set contains actions whose preconditions appear in
the i-1 set of propositions, provided that their preconditions are not marked as mutually
exclusive. Mutual exclusion constraints are added as follows: Two actions are marked
mutually exclusive at the i action set if their preconditions are marked mutually exclusive
at the i-1 . Clearly, if all the preconditions of these actions cannot hold at this time
point, we cannot perform both actions together at this time point. Another reason for
marking actions as mutually exclusive is if they conict. That is, if one action destroys a
precondition or an effect of the other action. Two propositions at the i proposition set
are marked mutually exclusive if all pairs of actions in the i-1 action set that have them
as effects are mutually exclusive. When that is the case, there is no way for us to achieve
both of these propositions together at this stage.
The initial construction of the planning graph is terminated once all goal propositions
appear in the last proposition set. At that point, Graphplan performs a form of regressionbased search on the planning-graph structure. If this search fails, the planning graph is
extended by one additional layer of actions and one additional layer of propositions and
searched again. For more details, see the article by Blum and Furst (1997).
th

th

th

th

th

th

th

th

th

th

th

th

2.2 The Planning as Satisfiability Approach
The Planning as Satisfiability approach (PAS for short), works as follows: given a planning
problem and a bound n on the size of the plan, the plan encoder generates a propositional
formula in conjunctive normal form. This formula has the following property: it is satisfiable
iff the planning problem has a solution with at most n time steps. Intuitively, the formula
is composed of propositions describing the state of the world throughout the execution of
an n-step plan.
The propositional language on top of which the formula is defined contains a proposition
for each possible aspect of the world at each time point. For example, suppose we are looking
at the blocks' world domain where states are described using the on and clear relations.
In that case, for any pair of blocks x; y, and any time point 0  t  n we shall introduce
a proposition pon( ) which corresponds to x being on top of y at time t. Similarly, for
x;y;t

3

fiBrafman

every block x and time point 0  t  n, we shall introduce a proposition pclear( ) which
corresponds to x being clear at time t.
A truth assignment to the language described above can be viewed as describing the state
of the world during the execution of an n-step plan. For instance, if pon( 3) is assigned
true , then block A is on top of block B at time 3. Of course, most truth assignments
to this language would not correspond to anything resembling the true state of the world
during the execution of an actual plan. For example, both on(A,B) and on(B,A) could
be assigned true . The goal of the encoding scheme is to generate a formula such that any
assignment satisfying this formula will correspond to the true state of the world during
the execution of an actual plan that achieves the desired goal. Each axiom in this formula
places some constraint on the value of these propositions so that the combined effect of
these constraints is to ensure that the resulting formula has precisely the desired truth
assignments. For example, one type of axiom will be a conjunction of all the propositions
corresponding to the initial state. Any truth assignment satisfying this axiom must ensure
that these propositions hold at time 0. Another class of axioms could state that if some
action a is performed at time t then the world at time t 1 must satisfy the preconditions
of a. In the next sections we discuss two of the central plan encodings in more detail.
Once an appropriate formula has been generated, it is simplified using various well
known techniques. In particular, all simplifiers employ a unit-resolution step (Genesereth
& Nilsson, 1987). Unit resolution (also known as unit propagation ) works as follows: To
satisfy a CNF formula, we must satisfy each of its clauses. In particular, if one of the
clauses contains a single literal (such a clause is known as a unit clause ) we immediately
know that the variable appearing in this clause must be assigned an appropriate value. Any
clause containing the same literal will be satisfied now, and it can be removed from the
formula. Any clause c containing the negation of that literal can be resolved against this
unit clause, and the resulting clause (which is smaller than c) can replace c. For example,
suppose we have the formula (p) ^ (:q _:p) ^ (r _ p). The first disjunct, (p) is a unit clause.
Hence, p must be assigned true . This makes the third clause, (r _ p), satisfied. The second
clause is now resolved with the first clause, and we replace (:q _ :p) by (:q). We now
have a new unit clause, (:q), and so the proposition q must be assigned the value false . If
we had additional clauses containing q or :q, we could remove them, or simplify them, as
appropriate.
After simplification, we apply our favorite algorithm for finding satisfying assignments,
and if one is attained, a decoder is used to obtain an actual plan from this solution. If we
do not find a satisfying assignment, we increase the value of n (the size of the plan), and
try again.
Finally, we note that the Blackbox planner (Kautz & Selman, 1999) combines PAS
with Graphplan's reachability analysis. It constructs a planning graph, and uses it to
generate a particular plan encoding.
x;t

A;B;

3. Reachability and Resolution

Reachability and relevance analysis form an essential part of successful modern planning
algorithms. The most notable example of reachability analysis is Graphplan's planning
graph (Blum & Furst, 1997), and many recent planners employ either reachability analysis
4

fiReachability, Relevance, and Resolution

(e.g., Bonet, Loerincs, & Geffner, 1997), relevance analysis (e.g., McDermoot, 1996; Nebel,
Dimopoulos, & Koehler, 1997), or both (Kambhampati, Parker, & Lambrecht, 1997). The
importance of reachability and relevance analysis has been noted in the context of decisiontheoretic planning as well. For example, Boutilier and Dearden (1994) employ relevance
analysis to reduce the state-space, and Boutilier, Brafman, and Geib (1998) describe a
general method for reachability analysis for MDPs. Below, we discuss this method in a
simplified form suitable for classical planning problems described using the Strips representation language (Fikes & Nilsson, 1971). In Section 4, we present a counterpart of this
method for performing relevance analysis and relate these algorithms to k-clause resolution
in the context of SAT-encoded planning problems.

3.1 Propagating Reachability Information
Reachable-k (Boutilier, Brafman, & Geib, 1998) is an algorithm for estimating the states
reachable from a given initial state. As formulated, it is quite general and applies to domains with non-deterministic actions and conditional effects. In Figure 1, we present a
simplified version of that algorithm, Reachable-k , which deals with deterministic, unconditional actions represented in the Strips representation language. A prolog implementation
of Reachable-k is available at www.cs.bgu.ac.il/~pdm.
An important reason for our interest in Reachable-k is its similarity to the inuential planning graph construction of the Graphplan planner (Blum & Furst, 1997). In
fact, it generalizes the ideas behind Graphplan's planning graph, which is equivalent to
Reachable-2. We use A to denote the set of actions feasible i steps from the initial state,

S to denote the corresponding set of propositions, and C S to denote constraints on these
propositions, such that if fp1 ; : : : ; p g 2 C S  then these propositions cannot co-occur after

i steps. C A denotes similar constraints on actions. Here,  2 fL; P g, where L is used when
we restrict our attention to linear action sequences, and P is used when we allow concurrent
non-conicting actions (i.e., actions that do not destroy each others' effects or preconditions
and whose preconditions are not constrained not to co-occur). Of course, for k = 1 the sets


C S and C A are empty for all i  0. (Actually, as defined, C A
is not empty even when
k = 1, but it plays no real part in the algorithm, and it should be ignored). Finally, note
that in this description, the set of possible actions contains all actions of the form noop[l],
where l is a literal.
When k = 2, S and A represent the propositional and action levels of Graphplan's
planning graph, and C S and C A hold their respective mutual exclusion constraints.
We have not stated a termination condition for Reachable-k, but one can be formulated
based on the content of S or the index i itself. In the PAS framework, where the number
of time-steps is fixed, one would opt for the second alternative. Reachable-k gives us sets
of actions and propositions, A ; S , that could occur after the performance of j actions (or
j sets of concurrently non-conicting actions) from the initial state. It is easy to see that
Reachable-k is sound in the following sense:
i

i

i

m

i

i

i

L
i

i

i

i

P
i

i

i

j

j

Theorem 1 If a set of propositions or actions is excluded by Reachable-k at time j then

there is no feasible plan in which, at time j , these propositions hold or, respectively, these
actions appear.
5

fiBrafman

= literals that hold in the initial state.



S0



C S0

= fg.

= actions all of whose preconditions are in S0 .
 C AL
0 = ffai ; aj gjai ; aj 2 A0 ; i 6= j; neither ai nor aj are noops or aj is a noop whose effect
is destroyed by ai g.
 C AP = ffai ; aj gjai ; aj 2 A0 ; i 6= j; and ai deletes a precondition or an effect of aj g.


A0

0

We define Si ; Ai inductively as follows:
i = literals that appear in the effects of Ai 1 .
 C Si = l-tuples of literals, where l  k , appearing in Si such that some subset of any set
of actions from Ai 1 that has these literals appearing among their effects, appears in C Ai 1
(where  2 fL; P g as appropriate).


S

i = actions whose preconditions appear in Si and no subset of their preconditions appears
in C Si .
 C AL
i = ffal ; aj gjal ; aj 2 Ai ; l 6= j; neither al nor aj are noops, or aj is a noop whose effect is
destroyed by al g.
 C APi = ffaj1 ; : : : ; ajl gjl  k; aj1 ; : : : ; ajl 2 Ai ; jm 6= jn for m 6= n; and either (1) ajm
deletes a precondition or an effect of ajn for some m 6= n or (2) some subset of the union of
preconditions of aj1 ; : : : ; ajl appears in C Si g.


A

Figure 1: The Reachable-k Algorithm
Sometimes, all actions that can be executed at a particular time point in which p holds
have :p as an effect. In that case, we can ignore the noop[p] action, as it will not be part
of any useful plan.1 However, as formulated, p will appear in Reachable-k 's next level. We
denote by Reachable -k a variant in which noop[p] does not appear in such a case.
The computational complexity of Reachable-k is O(njAjjLj E + njLjjAj ), where n is
the number of levels we generate, jAj is the number of possible actions, jLj is the size of the
propositional language used, and E is the maximal number of actions that have a particular
shared effect. A more detailed explanation appears in Appendix B.
Example: To illustrate the parallel action version of Reachable-k, we use the following
planning domain: There are four propositions, p1 ; p2 ; p3 ; p4 . Intuitively, we can think of
them as representing a binary counter with 4 bits. There are 4 actions, each of which
increases the counter by one at different states. In this domain only a single non-noop
action is applicable at each state. a1 changes the least-significant bit, a2 changes the next
bit, a3 changes the third bit, and a4 changes the most significant bit. a is applicable only
if the first i 1 bits are one. More specifically the actions are:
k

k

k

i

1. For some planning algorithms, e.g., in the PAS approach, it may be necessary to leave the noops in.

6

fiReachability, Relevance, and Resolution

a1

Precondition:

:

a2

Precondition:

p1

a3

Precondition:

p

p

a4

Precondition:

p

p

p1

; effect:

p1

.

^ : 2; effect: : 1 ^ 2.
1 ^ 2 ^ : 3 ; effect: : 1 ^ : 2 ^ 3 .
1 ^ 2 ^ 3 ^ : 4 ; effect: : 1 ^ : 2 ^ : 3 ^
p

p

p

p

p

p

p

p

p

p
p

p

p4

.

We compare Reachable-1 and Reachable-2 using the initial state: :p1 ^:p2 ^:p3 ^:p4 ,
which corresponds to a binary representation of 0. We concentrate on the (more interesting)
parallel action version of the algorithm, and we shall not mention noops explicitly or obvious
mutual exclusion constraints between propositions and their negations.
In the context of Reachable-1, C S and C A are empty, for all practical purpose. We
start with S0 = f:p1 ; :p2 ; :p3 ; :p4 g; Only the action affecting the first bit is applicable, and
A0 = fa1 g (and all the relevant noops); We now have S1 = fp1 ; :p1 ; :p2 ; :p3 ; :p4 g. Because
the preconditions for both a1 and a2 appear in S1 , we have that A1 = fa1 ; a2 g. Consequently
S2 = fp1 ; :p1 ; p2 ; :p2 ; :p3 ; :p4 g. Now, we can also apply a3 , and we have A2 = fa1 ; a2 ; a3 g;
S3 = fp1 ; :p1 ; p2 ; :p2 ; p3 ; :p3 ; :p4 g. Finally, at this stage all preconditions of a4 appear as
well, and A3 = fa1 ; a2 ; a3 ; a4 g.
Next, consider Reachable-2. Again, S0 = f:p1 ; :p2 ; :p3 ; :p4 g, C S0 is empty, and A0 =
fa1 g. In the next step we have: S1 = fp1; :p1 ; :p2; :p3; :p4g, with no interesting constraints
in C S1 , and A1 = fa1 ; a2 g. However, now C A contains (a1 ; a2 ), which are interfering
actions. S2 = fp1 ; :p1 ; p2 ; :p2 ; p3 ; :p3 ; :p4 g, as in the case of k = 1. However, C S2 contains
(p1 ; p2 ). This follows from the fact that the only actions in A1 capable of producing p1 are
a1 and noop[p1 ], and the only action in A1 capable of producing p2 is a2 . a2 interferes
with both a1 an noop[p1]. Therefore, because C S2 contains (p1 ; p2 ), a3 is not applicable
at this stage. Hence, A2 = fa1 ; a2 g, which is one action less than the set A2 in Reachable1. S3 = S2 = fp1 ; :p1 ; p2 ; :p2 ; p3 ; :p3 ; :p4 g. However, now C S3 does not contain (p1 ; p2 )
(because noop[p2] can be used to achieve p2 , and it does not conict with, e.g., noop[p1]).
Therefore, A3 = fa1 ; a2 ; a3 g.
To see how k = 3 improves our ability to prune over k = 2, suppose that S0 =
fp1 ; p2; :p3; :p4 g (i.e., the counter's value is 0011) and consider k = 2 first. We have
A0 = a3 and S1 = f:p1; p1; :p2 ; p2 ; :p3 ; p3 ; :p4 g, with (p1 ; p3 ); (p1 ; :p2 ); (:p1 ; p2 ) and
(p2 ; p3 ) in C S1 . Each of these mutual exclusion relations stems from the fact that a3
is mutually exclusive with noop[p1] and noop[p2 ]. Therefore, A1 = fa1 ; a3 g. Again
S2 = S1 = f:p1; p1; :p2 ; p2 ; :p3 ; p3 ; :p4 g, but now C S2 contains (p1 ; p2 ) only, and so
A2 = fa1 ; a2 ; a3 g. Now, C S3 is empty, and so A3 = fa1 ; a2 ; a3 ; a4 g. However, if k = 3,
C S1 contains the ternary constraint (p1 ; p2 ; p3 ). This ternary constraint remains in C S2 as
well, and in C S3 . Because it is in C S3 for k = 3, we have that a4 62 A3 .
i

i

i

3.2 k-Clause Resolution and Reachability
-clause resolution (or propagation) refers to the resolution of pairs of clauses one of whose
length is k at most. The k = 1 variant, i.e., unit propagation, is an integral part of all
major algorithms for generating satisfying assignments.
We wish to compare the type of reachability information derived by performing k-clause
resolution on SAT-encoded planning problems, with the information obtained by running
k

7

fiBrafman

the Reachable-k algorithm. By reachability information we mean constraints on the set of
actions possible at a time point or constraints on world states (in the form of, e.g., sets of
unreachable propositions or k-tuples of propositions). Hence, for example, a constraint of
the form a(t) _ a0 (t) implies that one of the actions a or a0 must appear at time t in the
plan. A constraint of the form :a(t) _:a0(t) implies that one of the actions a or a0 must not
appear in the plan. Similar constraints on the propositions holding at a time point can also
be derived. In principle, such constraints reduce our search space and could help us attain
a solution more quickly. However, the effectiveness of such deduced constraints depends
on the precise algorithm used. Moreover, comparison over a very large class of constraints
seems quite dicult. Therefore, in this article we concentrate on a very concrete class of
reachability information of the form :a(t), i.e., the action a cannot be performed at any
state reachable via t steps. These are powerful constraint which can be utilized effectively
by almost all planners (perhaps with the exception of partial-order planners). Consequently,
we shall say that an algorithm Alg1 generates more reachability information than another
algorithm Alg2 if whenever Alg2 is able to determine that some action a cannot be performed
at some time t, Alg1 is able to reach this conclusion as well, and in addition, there are such
conclusions which Alg1 can reach but which Alg2 cannot reach. Hence, Alg1 generates a
strict superset of the constraints on actions (of the type we are interested in) generated
by the other algorithm. Note that this does not mean that Alg1 is better than Alg2 on
every instance, only that it is always as good, and in some cases better. In this section
we shall compare the pruning ability of the two Reachable-k variants and two encoding
methods discussed by Kautz and Selman (1996):2 the linear encoding and the Graphplan
encoding.
3.2.1 Linear Plan Encoding

The linear plan encoding (Kautz & Selman, 1992) is a simple and natural method for
translating a planning problem into a formula that is satisfiable iff there is a valid plan of
length n (for some given n). The clauses in the linear plan encoding fall into the following
classes:
1. an action implies its preconditions prior to its execution;
2. an action implies its effects following its execution;
3. an action does not affect any other proposition (frame axioms);
4. there is at least one action at each time point;
5. there is at most one action at each time point.
Because we have explicit frame axioms, noops are not needed in the linear encoding (as
opposed to the Graphplan encoding). In addition, the formula contains unary clauses
describing the initial and goal states. However, for the purpose of analyzing reachability
effects, we exclude the description of the goal state (which plays a role in relevance analysis).
Consider the mechanism by which resolution can yield reachability information: Given
the propositions that hold at the initial state, we can derive the negation of actions whose
2. The third (state-based) encoding method cannot be generated automatically.

8

fiReachability, Relevance, and Resolution

preconditions do not hold using unit propagation on axioms of class 1. Propagating these
unit clauses with the appropriate instance of axiom class 4, we will obtain a disjunction
of all actions that can be executed at the first time point. So far, this is identical to
what Reachable -k provides. To propagate this information forward, we can resolve these
action disjunctions with axioms of class 2 and 3. This, however, requires binary resolution
(discussed below). Hence, except for the unlikely case in which a single action is possible,
there is no more that we can derive using unit propagation alone. Reachable -1, on the
other hand, can provide us with a list of all possible effects of these actions and possibly
prune out future actions whose preconditions do not appear in this list. We conclude:
Lemma 1 In the context of the linear encoding, Reachable -1 yields more reachability in-

formation than unit propagation.

Example: Consider a blocks' world domain with a single action schema move(object,source,

destination).3 Its preconditions are: on(object,source), clear(object), clear(destination)
and its effects are: on(object,destination), clear(source), :on(object,source), : clear
(destination) (except when the destination is the table which is always clear). If we have k
stacks of blocks initially, k2 actions can be performed at the initial state (i.e., moving a block
from the top of a stack to the top of another stack or the table). This will be discovered by
both algorithms. In particular, unit propagation will yield a disjunction of all these actions.
We know that all blocks that are 2 or more blocks below the top cannot participate in
the second move action. Reachable-1 will find this out due to the fact that they are not
clear. Suppose that A is one such block. All initially feasible move actions participate in
a frame axiom of the form move(o,s,d)^:clear(A; 0) ! :clear(A; 1), which, in clausal
form is :move(o,s,d)_clear(A; 0) _ :clear(A; 1). Resolving against :clear(A; 0), we
have :move(o,s,d)_:clear(A; 1). If we could deduce :clear(A; 1), we could rule out all
actions that have it as a precondition. But if we are restricted to unit propagation, this
requires deducing move(o,s,d) for some initially feasible action, and we cannot make such
a deduction.
If we propagated information forward using axioms of class 2 and 3 and used binary
resolution (as discussed before Lemma 1), we now have a set of disjunctions of the possible
effects (including frame effects) of the initially allowable actions. The number of such
disjuncts is O(e ), where e is the maximal number of effects of an action and m is the
number of actions that can be executed initially. In some cases, these disjunctions could
contain a single literal, e.g., when all initially allowable actions leave some proposition
unchanged. When one of these disjunctions contains only literals that are negations of
some action's precondition, we can deduce the negation of this action by resolving with
axioms of class 1.
Example: In the example considered above we would generate a disjunction of the form
move(o1 ; s1 ; d1 ) _ move(o2 ; s2 ; d2 ) _ move(o3 ; s3 ; d3 ), containing all instances of the move
action for time 0 whose negations have not been deduced. As discussed above, for all such
actions, we can obtain a clause of the form :move(o ; s ; d )_:clear(A; 1). Once we resolve
these binary clauses against the clause above, we obtain a unary clause :clear(A; 1), that
m

i

i

i

3. In fact, since we use plain Strips, we need three action schemas: one for moving a block to a block, one
for moving a block to a table, and one from moving a block from the table. However, as this does not
affect our analysis, we stick to a single move action in this and the following examples.

9

fiBrafman

can be used in conjunction with class 1 axioms to deduce the negations of step 2 actions
whose preconditions include clear(A; 1).
As we saw, the effect disjunctions discussed above allow us to rule out certain propositions or combinations of propositions. These are analogous to mutual exclusion constraints.
These mutual exclusion constraints can be used to prune actions. For example, if we deduce :p1 _    _ :p and all the p are preconditions of some action a, we can deduce :a
using binary resolution (by resolving precondition axioms with this disjunction). However,
as we show below, binary resolution has trouble propagating even binary mutual exclusion
constraints forward. We believe that this is generally true, i.e., k-clause resolution will have
trouble propagating k-ary constraints. We can show the following:
m

i

Lemma 2 Reachable-2 and binary resolution (in the case of the linear encoding) are incomparable.

We prove this by providing two examples. One in which Reachable-2 is able to prune
an action that binary resolution cannot, and one in which the converse hold.
First, consider the 4-bit counter with initial value 0000 (i.e., :p1; :p2 ; :p3 ; :p4 ). After four steps we obtain the following: S4 = f:p1 ; p1 ; :p2 ; p2 ; :p3 ; p3 ; :p4 g and C S4 =
f(p1 ; p3 ); (p2 ; p3)g. Therefore, A4 = fa1 ; a2 ; a3 g. This implies that S5 = S4 . We claim
that (p2 ; p3 ) 2 C S5 as well, which means that a4 62 A5 . To see this, consider all pairs of
actions that have p2 and p3 as effects. They are: (a2 ; a3 ); (a2 ; noop[p3]); (noop[p2 ]; a3 ), and
(noop[p2]; noop[p3 ]). (a2 ; a3 ) is a pair of real actions, which are always mutually exclusive
in the linear encoding. The preconditions of (a2 ; noop[p3]) are mutually exclusive according
to C S4 , and so are the preconditions of (noop[p2]; noop[p3]). Finally, (noop[p2]; a3 ) are
interfering actions. We conclude that (p2 ; p3 ) 2 C S5 and a4 62 A5 .
When we run a binary resolution procedure on the linear encoding of this problem,
we could not deduce a4 62 A5 . This stems from the fact that ternary resolution is needed
to propagate the mutual exclusion of p2 and p3 . Recall that we obtain mutual exclusion
constraints by resolving against a disjunction of actions that have not been ruled out. In
the above case, at time 4 we would have the following disjunction: a41 _ a42 _ a43 _ noop[:p1] _
   _ noop[6= p4]. Our goal is to deduce :p52 _ :p53 using :p42 _ :p43 and the various axioms.
To do this, we will try to deduce either :p52 _:p53 from each of the actions in the disjunction.
It is easy to deduce :p52 from a43 and :p52 from a42 . However, we believe that it is impossible
to deduce :p52 _ :p53 from a41 and from some of the noops.4 The reason for this is that such
a deduction involves the use of frame axioms, which are ternary. If we know that, e.g., :p42
holds, we apply unit resolution to the frame axioms and obtain a binary clause. However,
here we only know :p42 _ :p43. Once we resolve this against a frame axiom we remain with
a ternary clause. To get our desired result we must resolve two such ternary clauses.
Finally, let us see an example in which we use binary resolution to derive a ternary
constraint. By definition, Reachable-2 cannot derive such constraints. Suppose that the
initial state is :p; :q; :r. We have four actions: a1 has p; r as effects, a2 has q; r as effects,

a3 has p; q as effects, and a4 has p; q; r as preconditions. Using Reachable -2 we deduce that
a1; a2; a3 are possible at time 0. We get as their possible effects p; q; r; :p; :q; :r (recall
4. The fact that the deduction is impossible has been verified. What we are hypothesizing here is the reason
for it.

10

fiReachability, Relevance, and Resolution

that we must include all noop actions in Reachable-k in order to capture frame effects). No
strict subset of p; q; r can appear in the set of constraints C S1 . Since we deal with binary
constraints only, the set fp; q; rg does not appear in C S1 . Therefore, we will consider a4
possible at time 1, although, in fact, it is impossible. Using binary resolution, we would
have obtained the constraint :p _ :q _ :r (referring to time 1) which would have enabled
us to deduce that a4 is impossible at time 1.
3.2.2 The Graphplan Encoding

The Graphplan encoding differs from the linear encoding by its ability to consider multiple
concurrent (non-interfering) actions, allowing one to obtain shorter plans which, in turn,
can reduce the search space size. It constructs the following sets of clauses:
1. An action implies its preconditions;
2. An effect implies one of the actions that has this effect;
3. There is at least one action at each time-point;
4. Two conicting actions cannot occur together.
Besides the obvious ability to consider multiple parallel (non-interfering) actions, the important difference between the Graphplan and Linear encoding is in axiom class 2 (referred
to in (Ernst et al., 1997) as explanatory frame axioms.) Clauses in this class will contain
positive occurrences of action literals and negative occurrences of state literals.
As in the linear case, using unit propagation we can infer which actions cannot be
applied at the initial state. Using axioms of class 2, we can propagate this information forward, deducing the negation of all effects that cannot be produced by the initially allowable
actions. This information enables us to exclude actions whose preconditions cannot be produced. This forward propagation is essentially identical to Reachable-1. We can informally
conclude:

Lemma 3 In the context of the

encoding, unit propagation and Reachable-1
yield the same reachability information, if we ignore the explicit constraints appearing in
axiom class 4. If we use these constraints, unit propagation can yield more reachability
information.
Graphplan

To be precise we have to carefully define the notion of reachability constraints in the context
of the Graphplan encoding. For example, in the Graphplan encoding we can derive a
constraint that says that one of a group of actions must appear in the plan. This constraint
will not necessarily rule out any action because the Graphplan encoding permits multiple
actions at the same time point.5 However, in the linear encoding such a constraint will
immediately rule out all other actions because only a single action is allowed at each time
point. As we mentioned earlier, in this paper we concentrate on strict exclusion constraints
5. However, because actions that interfere with each other cannot occur concurrently, if we know that action
a will occur then we can deduce that any action a that interferes with a will not occur. This is precisely
where class 4 axioms enter the picture.
0

11

fiBrafman

which lead to an immediate reduction in the search space by ruling out the need for certain
actions at certain time points.
When k > 1, the mechanism remains the same. But now, axioms of class 4 can play
a more prominent role because we can use them to exclude actions in more cases than
before. However, the same problem of propagating mutual exclusion constraints forward
which we had with the linear encoding reappear here. Consequently, k-clause resolution in
the context of the Graphplan encoding and Reachable-k are incomparable.
4. Relevance and Resolution

Relevance analysis is a complex task and it can be performed to various degrees. For
instance, considering the last action level, one can exclude actions that do not produce a
literal in the goal. However, some actions producing a goal literal can also be irrelevant.
For example, consider a blocks' world planning problem in which the color of the blocks
is specified as part of the goal. As observed by Nebel, Dimopoulos, and Koehler (1997), a
paint-block action is still, intuitively, irrelevant if the initial and final colors of the blocks
are the same. However, it does have a goal literal as an effect.
In this section, we formulate an algorithm for relevance analysis, called Relevant-k .
Relevant-k does not perform the deeper relevance analysis needed to determine that the
paint-block action is irrelevant in the above example. Rather, Relevant-k is similar in its
motivation and form to Reachable-k , and it has a similar soundness property. Relevant-k
prunes the search space by excluding states from which the goal is not reachable within a
given number of steps and actions that are not useful for achieving the goal state within a
given number of steps.
Relevant-1 is similar to a number of existing components of existing planners, such
as McDermott's greedy regression graph (McDermoot, 1996) and Nebel, Dimopoulos, and
Koehler's And-Or trees (Nebel et al., 1997). Relevant-k generalizes these ideas to arbitrary
levels of interactions, taking into consideration mutual-exclusion constraints that relevant
states must satisfy. Relevant-k is slightly more complicated then Reachable-k because the
Strips formalism allows incomplete description of goal states, and propagating this partial information raises some diculties. Naturally, if the goal state is partially specified,
fewer constraints are available to start with, and so fewer constraints will be derived. The
algorithm is described in Figure 2. We are not aware of a similar, general formulation of
these ideas. Therefore, it is worthwhile going over the central points of this algorithm,
concentrating on the more interesting and complex case in which parallel actions are allowed. However, before we do this, we point out an important assumption we shall make
on the action representation used: No proposition symbol shall appear only in the preconditions or only in the effects of an action. This restriction is not dicult to enforce, as any
Strips-based domain representation can be transformed into a description in which this
assumptions is satisfied. For example, if p is a precondition of action a that does not appear
in the effect of a, we can simply add it to the effect, as we know that it must hold after
the action is executed. If p appears in the effect of a but neither p nor :p appear in the
preconditions of a, we can decompose a into two versions of the a action, one in which p is
a precondition and one in which :p is a precondition. Note that in the worst case, such a
transformation can cause an exponential blow-up in the number of actions.
12

fiReachability, Relevance, and Resolution

r contains all actions that are useful and safe w.r.t. the goal.
 A0 contains Ar0 and all noops that are safe w.r.t. the goal.


A0

contains all pairs of interfering actions in A0 .
We define Ri ; Si ; Ari ; Ai inductively as follows:
 Ri is the union of preconditions of actions in Ari 1 .


C A0



S

i is the union of preconditions of actions in Ai 1 .

i contains sets S of literals such that S  Si , jS j  k and for any set of actions A  Ai 1
whose preconditions contain S it is the case that A 2 C Ai 1 .
 Ari contains all actions that are useful w.r.t. Ri but no subset of their effects is contained in
C Si .
 Ai contains Ari and all noops useful w.r.t. Si .
 C Ai contains all action sets A such that A  Ai and either (1) A contains two interfering
actions, or (2) Some subset of the set of effects of A is in C Si .



CS

Action descriptions must contain the same set of propositional symbols in their precondition and
effect lists.

Figure 2: The Relevant-k Algorithm
For k = 1 the algorithm is quite simple (and identical in the parallel and linear cases). In
that case, we can ignore the sets S ; C S ; A and C A (as they are degenerate) and consider
the sets R and A only. Starting with the goal literals, at each stage we have a set of
literals from which we construct the next set of actions. This action set contains actions
with an effect in the current literal set. However, if all the goal effects of an action are all
part of its preconditions, we can ignore that action as irrelevant. Next, a new literal set is
constructed, containing the set of preconditions of the current set of actions, and we repeat
the process with this new set.
When k > 1, the picture becomes a bit more complicated. We start with the set
of relevant actions, A . These are actions that achieve one of the desired literals. In
particular, A0 contains only actions that have one of the goal literals as an effect (but not
as a precondition). If the goal is partially specified, literals that are not part of it could hold
in the previous time step. Hence, we include the appropriate noop actions in a larger set,
A , which contains both A and noops that do not destroy needed propositions. A subset
of the actions in A is mutually exclusive if it contains interfering actions or actions whose
effects are mutually exclusive. Given the set A 1 , we generate the set R , which includes
the preconditions of A 1 . The set S is defined as the set of preconditions of actions in A .
If the goal is a completely specified state, there is no the sets R and S and the sets A
and A are identical, and so we need not distinguish between them.
i

i

i

i

r
i

i

r
i

r

r
i

i

i

r
i

r
i

i

i

i

i

i

13

i

r
i

fiBrafman

To facilitate the description of the Relevant-k algorithm, it would be useful to add a
few simple definitions. First, we wish to revise the definition of interfering actions in the
context of the Relevant-k algorithm. We say that actions a; a0 interfere with each other if
some effect of a conicts with some precondition or effect of a0 or (and this is beyond the
previous definition of this term) if their preconditions are inconsistent. An action a is useful
w.r.t. (with respect to) some literal l if a is the noop action preserving l or l is an effect,
but not a precondition, of a. a is useful w.r.t. some set of literals if it is useful w.r.t. one of
the set's elements. A set A of actions is safe w.r.t. some set of S of literals if no action in
A has an effect that negates an element of S .
Relevant-k embodies the intuitions described above. Note that an increased index corresponds to points earlier in time. The definition of the sets S ; R ; A ; A is quite intuitive:
S contains the preconditions of the actions in the previous A , and R contains the preconditions of actions in A . A contains actions that have a useful, but not mutually exclusive,
effects. A is defined much like A , but w.r.t. S rather than R . The set C S contains
literals that are mutually exclusive at a particular point. A set L of literals is mutually
exclusive if any set of relevant actions that have L among their preconditions are mutually
exclusive. The set C A contains mutually exclusive sets of actions. A set of actions A is
mutually exclusive if it contains interfering actions or if the set of its effects is mutually
exclusive.
Example: In order to illustrate the Relevant-k algorithm, we shall once again use the counter
example used in Section 3.1, starting with a three bit counter and using the propositions,
p1 ; p2 ; p3 . Each of the actions a1 ; a2 ; a3 can change the value of a single bit from 0 to 1,
provided the values of the lower bits are 1.
We start with the final state f:p1 ; :p2 ; p3 g and k = 1. Since the final state is fully
specified, there is no distinction between the sets S and R and between A and A . A0
contains the action a3 and the three relevant noops. S1 contains fp1 ; :p1 ; p2 ; :p2 ; p3 ; :p3 g,
A1 now contains a1 ; a2 ; a3 , and the appropriate noops, and the remaining sets look the
same.
If k = 2, A0 and S1 are as in the k = 1 case. However, C S1 contains (:p1 ; p2 ) and
(p1 ; :p2 ), which implies that a2 cannot be applied. Hence, A1 contains a1 and a3 , but not
a2 , unlike the case of k = 1. The action a2 would be introduced only in the next step.
Next, consider a partially specified goal, such as fp3 ; p2 g and with k = 2. A0 =
fa2 ; noop[p2]; noop[p3]g because a2 has p2 as an effect, and a2 does not destroy p3; whereas
a1 , for example, does not have an effect in the goal. A0 would now contain A0 as well as
the noops for p1 and :p1 . R1 = fp1 ; p2 ; :p2 ; p3 g and S1 = fp1 ; :p1 ; p2 ; :p2 ; p3 g. Next, A1
contains fa1 ; a2 g, etc.
Finally, suppose we have four bits, and the goal state is f:p1 ; :p2 ; p3 ; p4 g (i.e., the
counter's bit value is 1100). If k = 2, A0 contains a3 and S1 = fp1 ; :p1 ; p2 ; :p2 ; p3 ; :p3 ; p4 g.
However, C S1 contains pairs such as (:p1 ; :p3 ); (:p2 ; :p3 ) and others. A1 contains a1 ; a3
and some noops. S2 = S1 , but now, C S2 does not contain (:p1 ; :p3 ), it does contain
(:p2 ; :p3 ), though, and that precludes action a4 from being in A2 . In the next step, we
have S3 = S2 = S1 , and C S3 no longer contains (:p2 ; :p3 ). This implies that we can
add a4 to A3 because its effects are no longer mutually exclusive. So overall, we have
A0 = fa3 g; A1 = fa1 ; a3 g; A2 = fa1 ; a2 ; a3 g, and A3 = fa1 ; a2 ; a3 ; a4 g. However, if k = 3, at
i

i

i

i

i

r
i

r
i

i

r
i

r
i

i

i

i

i

i

i

i

r
i

i

r

r

r

14

fiReachability, Relevance, and Resolution

we would still have a mutual exclusion constraint on (:p1 ; :p2 ; :p3 ), which would not
allow us to add a4 . Hence, when k = 3, A3 = fa1 ; a2 ; a3 g.
We can prove the following soundness results:
C S3

Theorem 2 Let

be some state from which the goal is reachable using an m-step plan
(where each step can contain a number of non-interfering actions). Then (1) the set of
literals satisfied in s is a subset of S , no subset of which is in C S , and (2) there exists
an m-step plan for reaching the goal from s such that if A is the set of actions in the plan
v steps before last then A  A and no subset of A is in C A .
s

m

m

v

v

A corollary of this theorem is:

Corollary 1 For any initial state s from which the goal is reachable and any minimal (in
the number of operators) plan P = A0 ; : : : A00 (where the steps are numbered backwards) for
reaching the goal from s, we have A0  A and C A does not contain any subset of A0 .
m

i

i

i

i

The complexity of Relevant-k is O(jAj jLj + jLj
jAj), where jAj is the number of
actions, jLj is the number of proposition in the language, and m is the maximal number
of preconditions of an action. For more details, see Appendix B.
We now compare the amount of relevance information that can be propagated backwards
using k-clause resolution and the goal literals as opposed to Relevant-k . Consider unit
propagation first. In the context of the linear encoding, we see that all actions that destroy
some goal condition will be ruled out. However, actions that are irrelevant because they
produce irrelevant effects will not be pruned.6 On the other hand, Relevant-1 prunes both
actions that destroy some goal literal and actions that are simply irrelevant. There is a
slightly degenerate case in which all actions but one destroy some goal proposition. In that
case, using unit propagation we will be able to deduce the previous state. Consequently, we
have:
k

k

k
mp

p

Lemma 4 In the context of the linear encoding, unless there is a single safe, final action,

unit propagation yields less relevance information than Relevant-1.

In the context of the Graphplan encoding the situation is often worse, and unit propagation prunes even less than in the linear encoding. The goal propositions appear only in
class 2 (effect) axioms. Propagating them against these axioms, we obtain disjunctions of
positive action propositions explaining a particular goal proposition. If we assume that all
literals have more than one explanation, we see that no new unit clauses emerge. Consequently, we can prune nothing.
Example: Consider the blocks' world domain once again. Suppose that there are three
blocks A,B, and C, and that the goal is on(A,B). Clearly, any action that moves block C
or moves another block on top of block C is irrelevant as a last action. When we consider
the Graphplan encoding, the only unit clause we have is on(A,B,t) (where t is the last
time point). We can resolve it against the effect axiom that lists the the possible causes for
on(A,B,t). Aside from the noop action, there are actions such as moving A from C to B
6. In general, proving that an action should be ruled out means that we have shown that in all models, i.e.,
all plans, this action does not appear. We cannot expect to be able to do this for an irrelevant action
since it could possibly be inserted into the plan without affecting it.

15

fiBrafman

and moving A from the Table to B. This yields a new ternary clause and no additional unit
clauses. There are no other axioms in which on(A,B,t) appears negated.
Notice that we have no means of excluding actions that destroy one of the goal literals.
For example, if our goal was clear(A,t), we would not want the action move(B,C,A,t-1) as
a last action. However, as above, all that we can deduce from clear(A,t) is: move(B,A,C,t1)_move(B,A,Table,t-1)_ move(C,A,B,t-1)_move(C,A,Table,t-1)_ noop[clear(A; t 1)].
If we could use binary resolution at this stage, we could deduce the negation of any action
with the effect :clear(A,t), because any such action would be mutually exclusive from
any of the above five actions.
If a goal literal l has a single explanation it must be a noop action (which implies that
there is no \real" operator that has it as an effect). In that case, we would be able to deduce
that this noop action must hold, and using the precondition axioms, we would deduce that
l must hold at the previous step. Using the mutex axioms (class 4) we could deduce the
negation of any action that destroys l. However, we cannot deduce the negation of any
action that does not interact with l, whether it is simply irrelevant or it destroys some
other goal literal.
Example: Consider a domain such as the Rocket domain, where a rocket can have fuel,
but there is no action for fueling a rocket. Suppose that the rocket has fuel in the goal
state. Hence, fuel(t) holds. Since the explanation axiom for fuel is a binary clause (i.e.,
:fuel(t) _ noop[fuel(t 1)]. Resolving this axiom with the fact fuel(t), we derive a new
unit clause noop[fuel(t 1)]. Using the precondition axioms, we can derive fuel(t-1).
Using the mutex axiom, we can derive an action such as fly(t-1), one of whose effects is
:fuel(t 1). Notice, though, that we cannot deduce the negation of an action that does
not interact with the proposition fuel, whether or not it is irrelevant. For example, if fuel
is the only proposition in the goal, then an action such as loading the rocket, which does
not affect the value of the proposition fuel need not be considered for the final action of
the plan. However, as before, there is no way of deducing :load(t 1).
Because no action can produce fuel the same reasoning would apply to any step, and
we will be able to deduce the fact that fuel holds at each time point during the plan.
Using this fact, we will be able to prune out all actions that have :fuel as a precondition.
Relevant-1 will not be able to do so: If a has :fuel as a precondition but a has an effect
that is relevant at some point, a will be considered a relevant action.7

Lemma 5 In the context of the

encoding, if there is an action for changing the value of every literal, then unit propagation yields less relevance information than
Relevant-1.
Graphplan

Some actual values appears in Section 5. In particular, in the examples we looked at,
the Graphplan encoding could not prune any action. This follows from the (quite typical)
fact that in these domains, each of the facts that hold at the final state can be achieved
by a number of actions. Hence, unit propagation can deduce only disjunctions of possible
7. Of course, in this particular domain we do not have an action whose precondition is :fuel, but the
observation is still valid. For example, we may have a maintenance action which can be performed only
when the rocket is without fuel.

16

fiReachability, Relevance, and Resolution

actions, none of which are a unit clause. Since we have no way of deducing negated actions,
propagation stops at this point.
The general case is similar. In the linear encoding, having obtained a disjunction of
allowable actions, we can generate a disjunction of allowable preconditions. This information
is propagated backwards much like the forward case. Yet, as in the k = 1 case, all we can
expect is a form of backwards reachability analysis from the goal state, rather than true
relevance analysis. Again, Relevant-k is likely to do a much better job here, because it
takes explicit relevance issues into account. However, as in the case of reachability analysis,
because of the ability of k-clause resolution to yield constraints of order greater than k, we
cannot show that Relevant-k is always better.
In the context of the Graphplan encoding, we will generate disjunctions of relevant
actions, from which disjunctions of relevant preconditions can be deduced, etc. However,
irrelevant actions will not be excluded explicitly (since more than one action is allowed at
each step) and we will only conclude that some relevant action must appear. Nor can we
exclude actions that destroy a goal proposition. Again, because we can deduce constraints
of order greater than k via k-clause resolution, we cannot provide a general result here.
Finally, we note that (1) the Graphplan planner does not incorporate relevance analysis, but Mea-Graphplan, a more recent variant, does (Kambhampati et al., 1997), as
well as IPP (Nebel et al., 1997). (2) Ernst, Millstein, and Weld (1997) discuss an enhanced
version of the Graphplan encoding which contains effects axioms as well (i.e., axioms of
the form action ! effect). In terms of the ability to propagate reachability and relevance
information, we see an added ability to rule out actions that destroy needed propositions
(as in the linear encoding.)
5. Empirical Evaluation

In the previous sections we attempted to understand the mechanisms by which resolution
yields reachability and relevance information and to compare them to a natural class of
direct reachability and relevance algorithms. As we noted, the relationship is not always
that of subsumption, and it is of interest to examine the actual pruning abilities of these
algorithms. In this section we describe the performance of these algorithms on a number
of standard planning problems. Because of the limited number of domains used, caution
should be exercised in interpreting these results. However, some interesting results emerge.
Our first set of experiments examined the performance of unary methods on large blocks
world and logistics domain problems. We used the blocks' world problems bw-dir.a/b/c/d
from the Satplan distribution8 involving 9/11/15/19 blocks, respectively, and (minimal)
plans of length 6/9/14/18. The logistics' domain problems are based on instances described
in (Brafman & Hoos, 1999) involving 8 packages and 3 cities, with minimal plans of size
6/10/16, respectively. SAT-encodings were generated using the Medic program (Ernst
et al., 1997). We used the crse options to obtain a linear encoding and the erpe options
to obtain a Graphplan-like encodings. However, the encoding obtained via the erpe options contain explicit effect axioms, as in the linear encoding. These axiom improve the
Graphplan-encoding's ability to propagate relevance information.
8. These instances are part of the UCPOP distribution, maintained by the University of Washington, or
from http://www.research.att.com/ kautz/blackbox/index.html, the BlackBox home page.

17

fiBrafman

log.a
log.b
log.c
bw.a
bw.b
bw.c
bw.d

j j
A

4565
5941
8021
3888
10890
44100
116964

Reach Rel R+R U-rch(l) U-rel(l)
2922 617 3476
401
38
3517 680 3905
442
20
5051 2782 6214
600
32
1697 408 2105
639
300
3565 830 4395
1201
440
12818 2394 15212
3141
840
26963 5238 32201
6482
5114

Table 1: Pruning Effects of Unary Methods. jAj is the number of possible actions in
the course of a minimal length plan. The following entries hold the number of
actions pruned using: Reachable-1, Relevant-1, both combined, unit propagation
on linear encoding using initial state, and using the final state. Unit propagation
in the Graphplan encoding using the final state yielded no pruning. Execution
times for the Reach/Relevant algorithms are  0:01 seconds except for bw.c (0.03
sec.), and bw.d (0.07 sec.).
In this set of experiments we measured the number of potential actions eliminated by
the following algorithms: Reachable-1, Relevant-1, Reachable-1 and Relevant-1 combined,
reachability analysis via unit-resolution using the initial state, and relevance analysis via
unit-resolution using the goal state. We did not consider the Graphplan encoding for the
following reasons:
(1) Unit-propagation in the Graphplan encoding yields as much information as Reachable1. (2) For our particular experiments (and in most other cases), unit-resolution based on
the final state in the Graphplan encoding prunes little, if any, actions because for each
fact appearing in the goal state there are a number of potential producing actions. (3)
The version of the Graphplan-encoding produced by Medic is basically equivalent to the
linear-encoding in terms of relevance information because it contains explicit effect axioms.
The actual numbers appear in Table 1. The first column provides the size of the set of
actions for the minimal plan length. The following columns provide the number of actions
pruned by the various methods tested. It is evident that Reachable-1 is extremely effective.
Relevance analysis seems much less useful, although Relevant-1 does prune a non-negligible
number of actions. The results for unit-resolution are quite disappointing, although in line
with our theoretical analysis. Recalling that unit-resolution in the Graphplan encoding
is equivalent to Reachable-1, we see that there is a much greater potential for pruning in
the Graphplan encoding. Another interesting observation is that there is little overlap
between the reachability and relevance analysis. This stems from the fact that the pruning
effect of these algorithms is often quite shallow: most of the pruning is done on the very
first steps (in reachability) or very last steps (in relevance). Finally, we note that the
k = 1 algorithms are quite fast: Unit propagation is an important heuristic in all SAT
solution algorithms based on the David-Putnam algorithm (Davis & Putman, 1960), and
it is extremely fast, with negligible running times (i.e., < 0:01 seconds). Not surprisingly,
18

fiReachability, Relevance, and Resolution

bw-sm.a
bw-sm.b
bw-sm.c
log-sm.a
log-sm.b
log-sm.c
hanoi-3
hanoi-4
hanoi-5

j j
A

18
48
100
18
42
66
38
68
110

Rch1 Rch2 Rel1/2 rch1(l) rch2(l) rch1(gp) rch2(gp) rel1/2(l/g)
21
22
8
15
22
21
22
4
68
70
44
44
74
68
70
6
199
204
184
96
210
199
204
12
39
57
8
14
49
39
44
1
111
165
18
36
141
111
126
3
196
292
26
58
244
196
220
5
94
97
21
36
117
94
118
9
224
230
34
66
281
224
280
14
450
460
50
108
558
450
551
20

Table 2: Effects of Unary and Binary Methods. jAj is the number of possible actions per step . The following columns hold the number of actions pruned during
the course of a minimal-length (or longer) plan using Reachable-1, Reachable-2,
Relevant-1 and 2 (which yield the same value), unit propagation on the linear
encoding using initial state, binary propagation on the linear encoding using initial state, unit propagation on the Graphplan encoding using initial state, and
binary propagation on the Graphplan encoding using initial state. The final
column correspond to propagation using the goal state. All methods (i.e., unit
and binary) on both encodings yielded the same values.
Reachable-1 and Relevant-1 are also extremely fast. Execution times for these algorithms
were less than  0:01 seconds, except for bw.c (0.03 sec.), and bw.d (0.07 sec.), for which
these amount to a small fraction of the running times required by modern SAT algorithms.9
The next set of experiments, shown in Table 2, introduces binary pruning methods
as well. Here, we were limited by the slow performance of our prolog implementation of
Reachable-2 and the Medic encoder (Ernst et al., 1997). We looked at blocks world problems involving 3,4, and 5 blocks, respectively, and we looked at logistics domain problems
involving one package and two cities, three packages and two cities, and three packages and
three cities. In addition, we looked at three hanoi-tower problems with 3,4, and 5 disks.
There are a number of points worth mentioning:



In two domains (blocks' world and hanoi), Reachable-2 is only slightly more useful
than Reachable-1. In the logistics domain, on the other hand, Reachable-2 is much
more effective. However, we must remember that Reachable-2 yields mutual exclusion
constraints which we did not measure. These constraints can be quite useful and they
have an important role in the Graphplan planner.



No clear winner emerges. In the blocks-world domain, binary resolution in the linear
encoding prunes more than Reachable-2, whereas in the logistics domain, Reachable2 prunes more. Interestingly, binary resolution in the Graphplan-encoding is less

9. These experiments were conducted on a PC with a PentiumII-200 processor.

19

fiBrafman

Time
Reach-1
Reach-2
U-Res(lin)
B-Res(lin)
U/B-Res(gp)

1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
1 2 2 3 3 3 4 4 4 4
1 16 16 16 16 16 16 16 16 16
1 2 2 3 3 4 4 5 5 6
1 2 3 4 5 6 7 8 9 10

11
11
5
16
6
11

12
12
5
16
7
12

13
13
5
16
7
13

14
14
5
16
8
14

15
15
5
16
8
15

16
16
6
16
9
16

Table 3: Reachability Analysis in a 16-bit Counter. Shown are the number of unpruned actions per time step. Rows correspond to Reachable-1, Reachable-2, unit
resolution on the linear encoding, binary resolution on the linear encoding. The
last row corresponds to unit and binary resolution on the Graphplan encoding,
which had identical effect.
effective than in the linear encoding. However, the Graphplan-encoding allows for
shorter plans, and consequently, smaller search spaces. Therefore, the Graphplanencoding is still likely to be more ecient.



Relevant-2 has no advantage over relevant-1. In fact, this behavior was observed when
using resolution as well: unit and binary resolution on both the linear and Graphplan
encodings pruned the same amount of actions. Consequently, we present them in one
column. Indeed, we see from both sets of experiments reported in Tables 1 and 2,
that relevance analysis contributes little. One obvious reason is that the goal state is
often incomplete and much less constrained than the initial state (at least explicitly).
Therefore, the algorithms have diculty deriving relevance constraints. However,
one's intuition seems to indicate that this should not be the case, at least not to
the extent observed. There should be means of providing better relevance analysis,
although they may require more sophisticated derivation of state constraints.



As predicted, relevance analysis is much more useful at the state-space level than at
the truth-assignment level.



As expected, the Graphplan encoding is typically better than the linear encoding.

Finally, we ran some tests on a 16 bit version of the counter domains described in
the text. This is a very constrained domain in which only a single action is applicable
at each state and we wanted to see how much of this would be discovered by the algorithms. The results are shown in Tables 3 and 4, where the number of permisable actions
is given as a function of the the time step. Table 3 presents the results for forward pruning
using Reachable-1, Reachable-2, and unit and binary propagation using the Graphplan
and linear encodings. Table 4 presents the results for backward pruning using Relevant-1,
Relevant-2, and unit and binary propagation using the linear encodings.

20

fiReachability, Relevance, and Resolution

Time
Rel1/2,U/B-Res(gp)
U-Res(l)
B-Res(l)

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
1 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16
1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9

Table 4: Relevance Analysis in a 16-bit Counter. Shown are the number of unpruned
actions per time step. The (identical) results for Reachable-1, Reachable-2, unitresolution on the Graphplan encoding, and binary-resolution on the Graphplan
encoding appear in the first row. The next rows correspond to unit and binary
resolution on the linear encoding, respectively.

6. Conclusion

We have shown a connection between the scheme used to encode planning instances and the
ability to propagate reachability and relevance information from the initial and final steps
to other time points. We hope that these results will serve to improve our understanding of
the factors contributing to the performance of different encoding methods. In addition, we
provided a crisp and general formulation of a class of reachability and relevance algorithms
that appear in various forms in different planning algorithms. We compared the pruning
ability of resolution-based propagation methods which operate on encoded plans, to that of
the Reachable-k and Relevant-k algorithms which operate at the plan level. Our empirical
results show a complex picture, where no clear winner emerges. However, it seems that
when the domain is constrained (making parallel actions less useful) binary methods have
little advantage over unary methods. In addition, they show that relevance analysis is best
conducted at the plan level. For SAT-based planning algorithms, this would suggest the use
of a simple plan-level relevance analysis stage prior to the plan encoding. This observation
is confirmed by recent results reported by Do, Srivastava, and Kambhampati (2000).
In (Brafman, 1999), we pointed out that binary clauses form a large fraction of the
clauses in SAT-encoded planning problems. Given our results regarding the utility of binary
resolution, a natural idea is to augment standard clause simplification techniques (e.g., unit
propagation) with some limited form of binary clause preprocessing. Initial results presented
there indicated the utility of this idea: In instances where unit clauses could be derived from
this form of binary resolution, nice reductions in running time were demonstrated. When
unit clauses were not derivable via this method, only a small overhead was incurred. A
more principled, systematic, and ecient technique based on these ideas is investigated in
(Brafman, 2000).
This work is among the first attempts to theoretically analyze different encoding schemes.
We have concentrated on one particular aspect of such encodings, i.e., their ability to propagate concrete state information backwards and forwards. Naturally, this attempt is a-priori
limited in its scope, as this ability is only one factor inuencing the performance of various
algorithms, and its inuence is probably more significant in systematic methods based on
the David-Putnam procedure than in methods based on stochastic local search.
21

fiBrafman

Other authors have considered some of the ideas presented here, too. Kautz and Selman
(1999) discuss the relation between Graphplan's mutex constraint and a restricted form
of binary propagation. In particular they show that mutex computation is a limited form
of negative binary propagation. In mutex propagation, two assertions of mutual exclusion
yield a new one. Of course, each mutual exclusion statement is equivalent to a binary
clause (e.g., either action a is not performed or action b is not performed), hence we can
view this process as a limited form of binary propagation: From f:p _ :qg and fp _ :rg
deduce f:q _ :rg. Graphplan performs this operation, but in an incomplete manner. In
addition, they tested additional limited inference methods such as the failed literal strategy
(attempting to prove that a particular literal is inconsistent using unit propagation) and the
binary failed literal strategy (attempting to prove that a binary clause is inconsistent using
unit propagation). These methods do not directly correspond to the methods considered
in this paper. More closely related is one of the options in the Medic system for encoding
planning problems: a simple inference method which is referred to as simple data-ow
analysis (Ernst et al., 1997). This method is basically an instance of Reachable-1.
Haslum and Geffner (2000) present a parametrized class of admissible heuristics functions H . There is an interesting and important relation between the heuristic function
generation technique discussed in that paper and the parameterized class of reachability
analysis algorithms discussed in this paper. When a heuristic function assigns 1 to some
state s this means it believes that goal is not reachable from s. If the heuristic function is
admissible, then in fact, this is true. Thus, admissible heuristic functions provide a sound
tool for pruning { the goal is not reachable from any state to which they assign the value
1. In fact, the derivation of the heuristic functions of class H is closely related to our
computation of Reachable-k. In both cases, instead of analyzing actual states, we analyze
subsets of states of size k and their interactions. However, in designing heuristic functions,
a greater emphasis is put on the distance from the current state to a state in which some
set of literals appears without mutual exclusion constraints (i.e., the indices of the sets S
and C S ).
Finally, a recent paper by Do, Srivastava, and Khambhampati (2000) examines encoded
planning problems generated by the Blackbox planner. Blackbox utilizes mutual exclusion constraints derived from Graphplan's planning graph. The authors show that
these constraints are useful, despite the fact that they increase the size of the encoding.
In addition, the authors examine the utility of adding explicit mutual-exclusion constraints
stemming from (state-space based) relevance analysis. These constraints appear to improve
the planner's performance. In fact, it seems that the constraints described by Do, Srivastava, and Kambhampati (2000) are more powerful than those generated by Relevant-2. We
believe that Relevant-k can and should be strengthened, and we hope to examine this issue
more closely in the future.
k

k

i

i

Acknowledgments

I wish to thank Craig Boutilier and Chris Geib for valuable discussions on reachability analysis and the anonymous reviewers for very useful and detailed comments. I am particularly
grateful to Olga Rozenfeld who implemented the algorithms in Prolog, suggested the use of
22

fiReachability, Relevance, and Resolution

the counter example for illustrating the algorithms, and provided important corrections to
previous drafts. This work was supported in part by the Paul Ivanier Center for Robotics
Research and Production Management.
Appendix A. Proofs

Theorem 1 If a set of propositions or actions is excluded by Reachable-k at time j then

there is no feasible plan in which, at time j , these propositions hold, or, respectively, these
actions appear.

Proof: This is immediate: Consider any valid plan and the states of the world during the
execution of this plan. It is straightforward to show that both appear within the sets
and S without being constrained by virtue of this being a valid plan.

Ai

i

Lemma 1 In the context of the linear encoding, Reachable -1 yields more reachability information than unit propagation.

Proof: Given the definitions used earlier on, a more formal statement of this lemma is as
follows: Let k be some integer denoting the length of a plan. Let A
1 be the set of

actions pruned by Reachable -1 up to the k-th level given some planning domain and initial
state. Let A
be the set of actions that are pruned by unit-resolution on the linear
encoding of this planning domain using k steps (i.e., actions for which we can deduce a unit
clause containing the negation of their corresponding variable), but without a goal state
supplied. Then A
, and for some planning instances A
.
1A
1A
First let us consider unit resolution. The unit clauses that are available initially correspond to the propositions that hold at the initial state. The only axioms in which propositions denoting the state at time 0 appear are those of class 1 (precondition axioms) and 3
(frame axioms). However, the clauses in class 3 are ternary and contain at most one such
proposition. These ternary frame clauses can yield a unit clause only if we are able to rule
out all actions but one, which we cannot, at this stage. Therefore, unit clauses can only
be derived by resolving the current unit clauses with class 1 clauses. Such resolutions can
yield new unit clauses containing negated actions. These negated actions can be resolved
against clauses containing positive action variables. Such variables appear only in class 4
(at-least-one-action) axioms.
Now there are two cases to consider. First, suppose that we have been able to rule out
all actions but one. Using the frame and effect axioms, we can derive the state at time 1.
Our situation now is analogous to that in which we were at time 0 with knowledge of the
initial state. Since Reachable -1 puts us in the same position, our claim follows (using a
simple inductive argument). Next, suppose that we cannot rule out all actions but one. In
that case, we have no new unit clauses, and so unit propagation stops. Reachable -1 will be
able to rule out all actions ruled out by the unit propagation process. Moreover, if all the
actions that are not ruled out have some common effect, that effect can be deduced using
Reachable- -1, and it can rule out actions that require its negation as a precondition. This
type of information is not obtained via unit propagation.
reach

u

res

reach

u

res

reach

23

u

res

fiBrafman

Lemma 3 In the context of the

Graphplan encoding, unit propagation and Reachable-1
rule out the same sets of actions, if we ignore the explicit constraints appearing in axiom
class 4. If we use these constraints, unit propagation can yield more reachability information.

Proof: First, suppose we ignore the mutex axioms of class 4. Using unit propagation, we

deduce the negation of those actions whose preconditions are violated at time 0. Negated
action literals can be resolved against class 2 (effect explanation) axioms. If we have been
able to rule out all explanations of some time 1 proposition, we can deduce its negation
in this manner. The same mechanism will allow us to exclude this variable when using
Reachable-1. Similarly, negated action literals can be resolved against class 3 (at-least-oneaction) axioms, but this yields no more information. Those time 1 variables we can deduce
can be used to rule out time 1 actions.
Notice the following. If we can deduce p at time 1, then one of the actions that produce p
must hold at time 0. This information is not explicit in the Reachable-1 algorithm (although
it appear in the Graphplan's planning graph in the form of edges). However, it cannot
be used to rule out other actions if we are restricted to unit resolution.
Class 4 axioms can make a difference in the above case. Suppose we have been able
to conclude that a particular action a that produces p must occur (i.e., by deducing p and
ruling out all its causes except a). In that case, all actions that are mutually exclusive
with a cannot occur. These actions may not affect p at all, and their negation need not
necessarily be derivable using Reachable-1.

Theorem 2 Let

s be some state from which the goal is reachable using an m-step plan
(where each step can contain a number of non-interfering actions). Then (1) the set of
literals satisfied in s is a subset of S , no subset of which is in C S , and (2) there exists
an m-step plan for reaching the goal from s such that if A is the set of actions in the plan
v steps before last then A  A and no subset of A is in C A .
m

m

v

v

Proof: Recall that we assume that any proposition appearing in the effects of an action

appears in its preconditions as well. We can always enforce this requirement by converting
an action that does not satisfy it into an a set of actions that satisfy it.
Our proof proceeds by induction on the number of steps by which the goal is reachable.
Let S be some state from which the goal G is reachable by a single step. Let A be the set
of actions in such a one-step plan for reaching G from S . By definition, A does not contain
interfering actions. In addition, we know that if G is reachable from S by performing A
then the preconditions of A and G n Effects-Of (A) must hold in S .
First, suppose to the contrary that for some literal l 2 S , we have that l 62 S1 . Notice
that by definition of A0 , we have that S1 contains all literals that are consistent with
G. Therefore, l must be inconsistent with G, i.e., :l 2 G. Since l 2 S , there must be
some action a 2 A with the precondition l and the effect :l (otherwise, l would hold after
performing A). Such an action would be in A0 and its preconditions, l among them, would
be in S1 . We conclude that S  S1 .
Next, we want to show that there is a one-step plan for reaching G from S all of whose
actions are in A0 . From the discussion above we see that the plan A for reaching G from
r

24

fiReachability, Relevance, and Resolution

contains an action from A0 for changing the value of every proposition l that holds in
S and that is inconsistent with G. Clearly, none of these actions can have an effect that
is inconsistent with G. Let A0  A denote the set of such actions. By applying A0 at S
we transform all literals inconsistent with G to their value in G and we do not destroy the
value of any literal consistent with G. Since A0  A, it constitutes a valid plan (i.e., its
actions do not interfere with each other) that achieves G. By definition, A0  A0 .
To conclude the proof of the base step, we must show that no subset of S is in C S1 .
Suppose, to the contrary that some subset S 0 of S is in C S1 . We have seen that for any
such S 0 , there is some set of actions A0  A such that A0  A0 and each l 2 S 0 is either
a precondition of some action in A0 or l is consistent with G and is not destroyed by A0 .
Denote by A00 the set consisting of A0 and any noop[] corresponding to those l 2 S 0 that
are not preconditions of an element in A0 . By definition of A0 , we have that A00 2 A0 .
However, if S 0 2 C S1 then A00 2 C A0 which implies that A00 contains interfering actions.
We claim that this is impossible. First, all the effects of A00 are either in G or consistent
with G, by construction. In addition, all the preconditions of A00 are in S 0 and therefore in
S . Because S is an actual state of the world, it cannot contain conicting literals. Hence,
0
S 62 C S1 .
Next, suppose that we have established our inductive hypothesis for all i < m and let
us prove that it holds for i = m. Hence, let S be some state for which there exists an mstep plan A = A1 ; : : : ; A for attaining G. Let S +1 denote the state obtained by applying
A1 to S . We know that there is an m
1 step plan for achieving G from S +1 . By our
+1
inductive hypothesis, S satisfies the conditions of the Theorem. In particular, we know
that S +1  S 1 and no subset of S +1 is in C S 1 . To complete our proof it would be
sucient to show that S +1 is reachable by a one-step plan A0 whose actions are in A but
not in C A . The proof is similar to the base case.
r

S

m

m

m

m

m

Corollary 1 For any initial state s from which the goal is reachable and any minimal (in
the number of operators) plan P = A0 ; : : : A00 (where the steps are numbered backwards) for
reaching the goal from s, we have A0  A and C A does not contain any subset of A0 .
m

i

i

i

i

Proof: An inspection of the proof of the previous theorem shows that in every step we

have found some subset of the set of actions in each candidate plan that satisfied the
relevant conditions. In particular, consider a minimal plan, all its elements must satisfy
these conditions.

Lemma 4 In the context of the linear encoding, unless there is a single safe, final action,
unit propagation yields less relevance information than Relevant-1.

Initially, our only unit clauses are goal literals. We can resolve then against the effect
axioms only. This would yield negation of various actions (i.e., unsafe actions). These
negated action literals can be resolved only against the action disjunction (axiom class 4).
However, if there is more than one safe final action, we will not obtain a unit clause from
this disjunction, and there is nothing farther that we can do. The same information, and
more, is easily obtainable from Relevant-1.
25

fiBrafman

Lemma 5 In the context of the

Graphplan encoding, if there is an action for changing the value of every literal, then unit propagation yields less relevance information than
Relevant-1.

Proof: See text prior to this Lemma.
Appendix B. The Complexity of Reachable-k and Relevant-k

The computational complexity of Reachable-k is O(njAjjLj E + njLjjAj ), where n is the
number of levels we generate, jAj is the number of possible actions, jLj is the size of the
propositional language used, and E is the maximal number of actions that have a particular
shared effect. As we explain below, the complexity is dominated by the time required to
produce the sets C S and C A .
The set of possible effects, S , is produced in O(jAj m ) steps, where m is the maximal
number of effects.
C S requires examining all l -tuples of elements in S , for l  k , and there are at most
O (jLj ) such elements. For each such tuple we have to find the set of actions that produce
it. This can be done quickly, provided we maintained pointers to these actions. The number
of such sets of actions is O(E ) (since no more than k actions are needed). For each such
set of actions we must check whether some subset of it is a member of C A 1 . Given an
appropriate representation of C A 1 , this can be done in time O(jAj). To accomplish this,
we can use a binary tree whose leafs correspond to bit vectors. The depth of this tree is
jAj and its size is O(jC A 1j). Finally, we need to maintain C S as a similar tree of bitvectors. This can be done in O(jLj ) (or, if C S is small, at a lower cost). The overall cost
of producing C S is O(jLj jE j jAj).
To produce the set A , we go over all actions and check whether their preconditions
appear in S . This requires O(jAj  m ) steps (assuming a bit-vector representation of S ),
where m is the maximal number of preconditions of an action. We also have to check
whether the preconditions appear in C S . Since jA j  jAj and we can check whether a
subset of the set of preconditions appears in C S in time O(jLj), this requires O(jAjjLj)
steps.
Finally, we need to produce the C A . This requires generating all subsets of A of size
k or less, taking O (jAj ) steps. For each such subset we must check whether its preconditions contain an element of C S . Again, provided an appropriate data-structure for C S
is maintained, this can be done in O(jLj) for each set of preconditions. As in the case of
C S , we assumed C A is maintained as a tree of bit-vectors, which can be generated in time
O (jAj ). The overall complexity of this step is O (jLjjAj ).
Note that for small values of k other data-structures are likely to provide better performance.
Next, we address Relevant-k. Our analysis is under the assumption that the same
set of variables appear in the preconditions and effects of each operator. As we noted,
transforming a set of operators that do not satisfy this property into a set of operators that
satisfy it may cause an exponential blow-up in the worst case.
The complexity of Relevant-k is O(jAj jLj + jLj m jAj), where jAj is the number of
actions, jLj is the number of proposition in the language, and m is the maximal number of
k

i

k

k

i

i

e

i

e

i

k

K

i

i

i

i

k

k

i

i

k

i

i

p

i

p

i

i

i

i

i

k

i

i

i

k

k

k

k

k

k
p

p

26

fiReachability, Relevance, and Resolution

preconditions of an action. The analysis is quite similar to the case of Reachable-k, and we
ignore the sets R and A which are subsets of the larger S and A and whose generation
contributes constant factors:
The set of preconditions, S , is produced in O(jAj  m ) steps.
To compute C S , we iterate over O(jLj ) sets of literals. For each such set we examine
all sets of actions that have it as preconditions, and there are at most O(m ) such sets. For
each such set of actions, we need to check that it is not in C A 1 . Each such check can be
performed in O(jAj) steps. The overall complexity of this step is O(jLj m jAj).
To produce the set A , we go over all actions useful for S , which require O(jLjE ) (where
as before, E is the maximal number of actions that have a particular effect). For each action,
we check whether its effects are in C S . Since we need to perform this check at most once
for every action, the overall complexity of O(jLjE + jLjjAj).
Finally, we need to produce the sets C A . Interfering actions can be pre-computed with
the cost amortized over all steps. In any case, their computation requires no more than
2
O (mjAj ) steps, where m is the maximal sum of preconditions and effects for an action.
Next, we have to examine the effects of all l-tuples of actions, where l  k, and see whether
these effects have a subset in C S . This takes O(jAj jLj) steps.
Again, for small values of k (and in particular, k = 1; 2) a tighter analysis is possible.
r
i

i

i

i

i

p

k

i

k
p

i

k

i

k
p

i

i

i

k

i

References

Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques to solve real-world
SAT instances. In Proc. AAAI-97, pp. 203{208.
Blum, A., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90, 281{300.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism
for planning. In Proc. AAAI-97, pp. 714{719.
Boutilier, C., Brafman, R. I., & Geib, C. (1998). Structured reachability analysis for markov
decision processes. In Proc. of 14th Conference on Uncertainty in AI, pp. 24{32.
Boutilier, C., & Dearden, R. (1994). Using abstractions for decision theoretic planning with
time constraints. In Proc. of AAAI'94, pp. 1016{1022.
Brafman, R. I. (1999). Reachability, relevance, resolution, and the planning as satisfiability
approach. In IJCAI'99, pp. 976{981.
Brafman, R. I., & Hoos, H. H. (1999). To encode or not to encode - i: linear planning. In
IJCAI'99, pp. 988{993.
Brafman, R. I. (2000). A simplifier for propositional formulas with many binary clauses.
Tech. rep. 00-04, Dept. of Computer Science, Ben-Gurion University.
Crawford, J., & Auton, L. D. (1993). Experimental results on the cross-over point in
satisfiability problems. In Proc. AAAI'93, pp. 21{27.
27

fiBrafman

Davis, M., & Putman, H. (1960). A computing procedure for quantification theory. Journal
of the ACM, 7, 201{215.
Do, M. B., Srivastava, B., & Kambhampati, S. (2000). Investigating the effect of relevance
and reachability constraints on sat encodings of planning. In Proc. of the Fifth Intl.
Conf. on AI Planning and Scheduling Systems.
Ernst, M. D., Millstein, T. D., & Weld, D. S. (1997). Automatic SAT-compilation of
planning problems. In Proceedings of the International Joint Conference on Artificial
Intelligence.
Fikes, R., & Nilsson, N. (1971). Strips: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2 (3{4), 189{208.
Freeman, J. W. (1995). Improvements to Propositional Satisfiability Search Algorithms.
Ph.D. thesis, U. Pennsylvania Dept. of Computer and Information Science.
Genesereth, M. R., & Nilsson, N. J. (1987). Logical Foundations of Artificial Intelligence.
Kaufmann, Los Altos, CA.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through
randomization. In Proc. of 15th Nat. Conf. AI, pp. 431{437.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proc. of
the Fifth Intl. Conf. on AI Planning and Scheduling Systems, pp. 140{149.
Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding and exending graphplan. In Proc. 4th European Conf. on Planning, pp. 260{272.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Proc. of the 10th European
Conf. on AI, pp. 359{363.
Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and
stochastic search. In Proc. of the 13th National Conference on AI (AAAI'96), pp.
1194{1201.
Kautz, H., & Selman, B. (1999). Unifying sat-based and graph-based planning. In Proc.
16th Intl. Joint Conf. on AI (IJCAI'99), pp. 318{325.
Li, C. M., & Anbulagan (1997). Heuristics based on unit propagation for satisfiability
problems. In Proc. IJCAI-97.
McDermoot, D. (1996). A heuristic estimator for means-ends analysis in planning. In Proc.
3rd Int. Conf on AI Planning Systems, pp. 142{149.
Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts and operators in
plan generation. In Proc. 4th European Conf. on Planning.

28

fiJournal of Artificial Intelligence Research 1 (2001) 231-252

Submitted 3/00; published 5/01

Technical Paper Recommendation: A Study in Combining
Multiple Information Sources
Chumki Basu

cbasu@cs.rutgers.edu

Haym Hirsh

hirsh@cs.rutgers.edu

Department of Computer Science, Rutgers University, 110 Frelinghuysen Road,
Piscataway NJ 08854-8019 and
Telcordia Technologies, Inc., 445 South Street,
Morristown NJ 07960-6438
Department of Computer Science, Rutgers University, 110 Frelinghuysen Road,
Piscataway NJ 08854-8019

William W. Cohen

wcohen@whizbang.com

Craig Nevill-Manning

nevill@cs.rutgers.edu

WhizBang! Labs, WhizBang Labs - East, 4616 Henry Street,
Pittsburgh PA 15213

Department of Computer Science, Rutgers University, 110 Frelinghuysen Road,
Piscataway NJ 08854-8019

Abstract

The growing need to manage and exploit the proliferation of online data sources is opening up new opportunities for bringing people closer to the resources they need. For instance,
consider a recommendation service through which researchers can receive daily pointers to
journal papers in their fields of interest. We survey some of the known approaches to the
problem of technical paper recommendation and ask how they can be extended to deal
with multiple information sources. More specifically, we focus on a variant of this problem
{ recommending conference paper submissions to reviewing committee members { which
offers us a testbed to try different approaches. Using WHIRL { an information integration system { we are able to implement different recommendation algorithms derived from
information retrieval principles. We also use a novel autonomous procedure for gathering
reviewer interest information from the Web. We evaluate our approach and compare it
to other methods using preference data provided by members of the AAAI-98 conference
reviewing committee along with data about the actual submissions.

1. Introduction
We can define the paper recommendation problem as follows:
Given a representation of my interests, find me relevant papers.

In fact, if we replace papers in the above definition with the name of some other artifact
of choice, we have yet another instantiation of a recommendation problem. What then
makes paper recommendation all that interesting?
231

fiBasu, Hirsh, Cohen, & Nevill-Manning

The ability to automatically filter a large set of papers and find those that are most
aligned with one's research interests has its advantages. With the growing number of
publications, many of them online, it is dicult to keep up with the latest research, even
if it's within one's field. With the timeliness of information becoming all the more critical,
it is also desirable for a paper to reach its target audience with minimal latency. Although
a straightforward approach to finding relevant papers may look for close matches between
a person's interests and a paper's content, what is less clear is how to represent both the
interests of the researchers and the contents of the papers.
Another feature that sets paper recommendation apart is that there is a variant problem
which must be dealt with on a regular basis by numerous conference chairs. Conferences
offer a venue where a large number of fairly specific papers must be distributed to a smaller
number of reviewers, all within a very tight timeframe. Even with the scope of the problem
being constrained to some degree by topic, conference organizers and/or reviewers still must
expend a great deal of time and effort before they can begin the reviewing process. This
would suggest that there can be real value in finding ways of automating the filtering process
that would make it less burdensome to the potential consumers.
We consider algorithms for recommending focused sets of technical papers. We use
conference reviewing as a platform to explore a series of questions relating to the recommendation process. There has been new interest in the AI community for this problem
recently since it was proposed as a \challenge" task at IJCAI-97 (Geller, 1997). Our focus
on conference reviewing turns out to be a natural choice since we can obtain data about a
set of papers, i.e., the conference submissions, and we can also obtain information about
the preferences of a set of reviewers for these submissions. In the following section, we discuss related work that addresses the conference reviewing problem. We also consider how
other work in the area of recommender systems { e.g., recommending articles to newsgroup
readers or recommending Web pages to Web site visitors { can contribute to this task.
However, our focus is on varying the sources of information in our data representations,
thereby allowing us to formulate different recommendation algorithms based on how we recombine these sources when computing similarity. We show that there is indeed a difference
in performance when we vary the amount and source of data, compared to the baseline of
using a single source of information in our data representations. We also compare these
recommendation algorithms against each other, against collaborative filtering, and against
the random assignment of papers to reviewers. We apply our methods to experimental data
involving reviewer preferences and conference abstracts for the AAAI-98 conference. 1

2. What We Know about Paper Recommendation
We already know that by recommending papers to reviewers, and more generally, to the
arbitrary researcher, we are trying to be selective in choosing those papers that will ultimately reach the consumer based on relevance to interests or expertise. However, finding
papers for conference reviewers is necessarily a more complex task, since papers may be
assigned to reviewers based on other criteria. For instance, reviewer load balancing and
conict-resolution of reviewer-author aliations may be two such criteria. In addition, the
1. The data were obtained with permission from AAAI, the AAAI reviewers, and when appropriate, from
the authors of the submitted papers.

232

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

reviewer's own reviewing preferences may be inuenced by considerations such as a paper's
readability and novelty. For example, a preference for novelty may lead a reviewer to choose
a paper simply because it is not relevant to his or her interests.
Our methods are not suited to address these latter issues for a number of reasons. First,
for confidentiality purposes, we lack information related to the author identity or aliation
of the submitted conference papers. Secondly, since constraint-satisfaction is not our main
concern { we are primarily interested in finding the best papers for each person without
regard to whether multiple people receive the same paper { we do not incorporate other
criteria into our selection procedure. We also do not have a way to represent the \novelty"
of a paper with respect to any consumer, and thereby do not have a means for recognizing
it. Finally, our methods do not distinguish between the notion of interest and expertise
with respect to reviewers. For the more general recommendation problem, the researcher
may want to retrieve papers in areas outside of his or her expertise, in which case a separate
representation for each would be needed.
Previous work in the area of assigning conference papers to reviewers had approached
the problem as one of content-based information retrieval. Dumais and Nielsen (1992) used
data provided by 15 members of the reviewing committee for the HYPERTEXT '91 conference. These reviewers not only submitted abstracts of their papers and/or interests, but also
provided complete relevance assessments for the 117 papers submitted to the conference.
Using an information retrieval method known as latent semantic indexing (LSI), they compared the reviewer abstracts with the submissions, ranking the submissions from most to
least similar to each reviewer. From their results, they noticed, based on the performance
metric that evaluates the number of relevant articles returned in the Top 10, that they
could achieve an average of 48% improvement using their automated methods compared to
random assignment of articles to reviewers.
While these results are encouraging, we believe that the widespread availability of online
resources introduces opportunities for exploring some new issues. What if the reviewers
weren't asked to supply interest information? Can the process of gleaning reviewer interest
data be automated with simple methods? How well do we do at retrieving relevant papers
using this \approximation" of reviewer interests? The automatic collection of reviewer
interest information from the Web, which effectively removes the reviewer from loop, is a
novel aspect of our research.
Yarowsky and Florian (1999) attempted a similar task for the ACL'99 conference. However, their primary focus was on classification { the assignment of every paper to exactly
one of six conference committees. They used 92 papers which were submitted to the ACL
conference in electronic form and also requested committee members to provide representative papers. When the number of papers returned by these members was insucient, they
augmented the collection with other papers downloaded from online sources. They used
content-based retrieval (within the context of the vector-space model (Salton, 1989)) as one
of their routing strategies. The main algorithm first computed a centroid for each reviewer
based on representative papers and then computed a centroid for each committee as the
sum of its reviewer centroids. Then, each paper was classified (assigned to a committee) by
computing its cosine similarity with the committee centroids and choosing the one with the
highest rank. Amongst other approaches, they experimented with a Naive Bayes classifier
and the assessment of similarity between reviewing committee members and authors cited
233

fiBasu, Hirsh, Cohen, & Nevill-Manning

in the papers. Based on their system performance relative to human judges on the same
task (evaluated against the actual assignments provided by the program chair of the conference), they extrapolated that automated methods could be as effective as human judges,
especially in cases where the judges may be less experienced.
When we are dealing with large conferences with several hundred papers covering a variety of areas, the information load is even greater for conference organizers and reviewers
alike. In these cases, getting evaluative relevance judgments for all submitted (or even accepted) papers from the reviewers is not feasible. (As an example, for the AAAI conference,
reviewers do not even have to state their preferences for all the papers they can potentially
review. Instead, they can stop scanning the list as soon as they have filled up their quota
of \bids"{ papers they expressed interest in reviewing.) Therefore, we focus on building
an extensible framework for recommendation { defining a process whereby we can systematically incorporate more information in formulating recommendation algorithms, for the
purpose of generating better recommendations.
Content-based information retrieval, also known as content-based filtering, is a popular
recommendation method: consider systems that recommend Web pages such as Syskill &
Webert (Pazzani & Billsus, 1997). There are a number of other systems such as WebWatcher
and Fab that do content-based filtering, mainly as part of a hybrid approach that also
involves collaborative filtering. Whereas content-based filtering looks only at the contents
of an artifact (e.g., the words on a Web page), collaborative filtering will also consider the
opinions of other like-minded people with respect to these artifacts. Collaborative filtering
has been used to recommend NetNews articles (Konstan, Miller, Maltz, Herlocker, Gordon,
& Riedl, 1997), movies (Hill, Stead, Rosenstein, & Furnas, 1995; Basu, Hirsh, & Cohen,
1998), music (Cohen & Fan, 2000; Shardanand & Maes, 1995), and even jokes (Gupta,
Digiovanni, Narita, & Goldberg, 1999). Since both content-based and collaborative methods
use data that are orthogonal to one another, there are opportunities to come up with hybrid
approaches that use combinations of the data. Our own work on movie recommendation
provides another example of how to design a hybrid system. Hybrid systems exploit data
from multiple sources with the expectation that they can do better by compensating for
the limiting factor of data sparseness associated with any single source.
In our current study, we would like to identify different sources of information to describe
both papers and reviewers, with the expectation that the individual pieces themselves, along
with knowledge of how to combine them, can make a difference in the recommendations.
Although we do share the common goal of combining data from multiple sources with the
hybrid recommendation approaches, the algorithms that we develop are strictly contentbased. For evaluative purposes, we also compare our algorithms against the results of
applying collaborative filtering methods to the set of reviewer preferences.

3. Representing Papers and Reviewers
Our approach to recommendation is to represent each entity using a variety of information
sources, to enumerate different combinations of these sources, and to evaluate the effectiveness of these combinations using ranked-retrieval methods. For the paper recommendation
problem, we have two types of entities | papers and their consumers (reviewers, in our
case). For each entity, we can represent the salient features of that entity as a sequence
234

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

of one or more information sources. In addition, we also need another type of information
source that relates a reviewer to a paper, namely, the reviewers' actual preferences for the
papers. We begin with a discussion of our choice of information sources | some of these
choices are based on data that are typically used to assign papers to reviewers, and are
usually provided explicitly by the papers' authors, while other choices rely more on implicit
knowledge mined from the semi-structured data available on the Web.

3.1 Paper Information Sources

All of our experiments were based on a compilation of submitted abstracts obtained from
AAAI for the AAAI-98 conference. There were 466 papers submitted to this conference.
AAAI gave us a collection of 256 papers to use in our experiments | the abstracts of 144
accepted papers and the abstracts of 112 papers that had been rejected but whose authors
had granted AAAI permission to provide the abstract for this work. Also excluded were
any papers that had been authored by any of the authors of this paper.
For each submission we obtained its title, abstract, and a set of user-assigned keywords
from a prespecified list. Therefore, each paper has associated with it a set of three information sources all of which were provided by the papers' authors. Although one may consider
the body of the paper as another source, this information was not available to the reviewers
(nor us), so we do not use it as a source.

3.2 Reviewer Information Sources

So far, we have seen an example where an entity such as a paper can be represented by
multiple information sources mainly because it is composed of distinct units such as a title,
abstract, etc. However, there is another case where we may want to multiply-represent an
entity. Consider trying to automatically compose a representation of a reviewer's interests.
We may try first to go to the reviewer's home page. From there, we may decide to look
around for the reviewer's papers. Each of these sources can offer a different point-of-view
of the reviewer's interests, and therefore, can be considered as a separate unit. We focus on
these sources { the reviewer's entry-level home page and the papers that are referenced from
the home page { as a substitute for asking the reviewer to provide interest information.
We believe both home pages and online papers are credible information sources since it
is likely that a fair number of conference reviewers have stated their research interests in
either or both sources. Since one of our paper information sources is the paper abstract,
we decided to represent the reviewer as an \abstract of interests". In the case of home
pages, the entire text of the reviewers's entry-level home page was taken as an abstract of
the reviewer's interests. In the case of PostScript files, we define an abstract to be the first
300 words extracted from the paper.
We extracted all of this information from the Web using pre-existing utilities. To find
reviewers' home pages, we fed the names and aliations of the members of the review
committee into Ahoy,2 a home page finding engine (Shakes, Langheinrich, & Etzioni, 1997).
When Ahoy returned at least one match, we supplied the URL as a starting point for
w3mir,3 an HTTP service that retrieves files from the contents of Web sites. We used
2. http://ahoy.cs.washington.edu:6060.
3. http://www.math.uio.no/janl/w3mir.

235

fiBasu, Hirsh, Cohen, & Nevill-Manning

w3mir to download only HTML files and PostScript files accessible from the entry-level
home page and residing on the same site.4 Since all of a person's papers may not directly
be available from one site, we additionally retrieved cross-references to other sites which
contained PostScript files, also using w3mir. The PostScript files were then converted to
ASCII using PreScript (Nevill-Manning, Reed, & Witten, 1998).
All PostScript files retrieved for a reviewer are treated uniformly. Although it would
be desirable to attempt to do so in future work, we make no attempt to determine the
timeliness of a paper, especially with respect to a reviewer's current interests. We also do
not distinguish between journal papers, conference papers, and even lecture notes. It is
for this reason that we do not attempt to do any detailed analysis of the contents of these
files (e.g., to automatically extract titles, abstracts, etc.). Instead, we rely on heuristics
such as looking at the first N words to approximate a paper's abstract. Although detailed
analysis is likely to be valuable in the paper recommendation process, our immediate goal
is to obtain a gross sense of the usability of various sources of semi-structured information.

3.3 Reviewer Preferences
To evaluate our queries we need some \ground truth" | some set of data specifying what
papers each reviewer had selected as suitable for him or her to review. With this information,
we can evaluate how different approaches perform in making the same choices. We note that
this is only an approximation to the full set of abstracts that the reviewer might have liked
| the reviewing process only requires a reviewer to find some minimum quota of papers,
and once that quota is reached, a reviewer need not look at other papers to find more. We
view this optimistically as yielding a close approximation to what a reviewer's full set of
preferences would be, since reviewers are able to peruse abstracts by keywords and often
attempt to inspect at least the subset of papers labeled by keywords in the areas in which
they are knowledgeable.
In our experiments ground truth comes from the actual preferences stated by 122 (of
the 230) AAAI-98 reviewers who gave AAAI permission to release their preference information for the papers we considered in this work. We point out that this data only reects
the reviewers' initial preferences for reviewing. We do not have data on what papers the
reviewers actually received following the AAAI reviewer assignment process.
Of course, one potential limitation of this data is that it is based only on a portion of data
that may not be representative of the entire data for the conference. For example, we have
preference data for approximately half of all of the reviewers and are predicting preferences
for a collection of papers whose distribution is skewed towards the accepted papers. There is
also the issue of whether AAAI researchers are representative of the much larger community
of researchers at large. (We can ask a similar question of the user populations of other
conferences as well). However, we consider these as acceptable limitations resulting from
our use of conference reviewing as a platform for paper recommendation.
4. At the moment, we focus on PostScript for convenience, but there is no reason to limit ourselves to just
one file format; the main constraint is being able to extract the words from a document.

236

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

4. Recommendation Methodology

In this section, we examine both collaborative and content-based methods of recommendation. These methods allow us to explore the use of different subsets of the data described
in the previous section.

4.1 Recommending with Reviewer and Paper Information Sources

In the following sections, we outline a content-based recommendation framework that uses
data describing the papers as well as data describing the reviewers to make recommendations. The reviewer preference data is then used for evaluation purposes, but not as input
to the recommendation process.
In order to locate papers that closely match reviewer interest data we rely on ad hoc
similarity metrics commonly used in the information retrieval community. We will describe
these methods further in the section on WHIRL. In brief, for each reviewer we compare
the given reviewer representation with the appropriate paper information source(s). Each
of these comparisons can be implemented as a query that returns a rank-ordered list of
papers. We can consequently compute precision at Top N , or the proportion of the papers
returned that were actually preferred by the reviewer, for each query. Our final score for
each query is the average of this value, computed over a subset of 50 reviewers (from the
larger set of reviewers who gave us their permission).
Our recommendation algorithms take different paper and reviewer information sources
as inputs. Since our data can be plotted along two dimensions, let Reviewer be the set of
information sources describing reviewers and Paper the set of information sources describing
papers. We can construct a Reviewer  Paper matrix where each entry in this matrix is
a score measuring the effectiveness of using the respective sources, (Reviewer ; Paper ), to
compute similarity between reviewers and papers when performing a ranked-retrieval. For
instance, given the paper and reviewer representations we have described, we can construct
a 2  3 matrix, which gives us 6 possible evaluations or scores. We will refer to this matrix
as the recommendation sources matrix.
Conceptually, we can extend the recommendation sources matrix along each dimension,
by considering combinations of the rows and columns. We refer to the augmented matrix
as the source combinations matrix. We can now define a recommendation algorithm as
a combination method or procedure applied to one or more rows/columns of the source
combinations matrix. This introduces another dimension for comparison { the combination
method itself { which we consider by looking at replicates of the source combinations matrix.
Now, we can pose the following questions for experimental analysis:
i

j

 Do recommendation algorithms that incorporate more information lead to better performance?
 If so, does the method of combining data used by the algorithm make a difference?
4.1.1 WHIRL

For all of our queries, we use WHIRL, a system specifically designed for informationintegration tasks (Cohen, 1998b; Cohen & Hirsh, 1998). For these tasks, it is often necessary to manipulate in a general way information obtained from many heterogeneous online
237

fiBasu, Hirsh, Cohen, & Nevill-Manning

sources, each potentially having its own data organization and terminology. In particular,
WHIRL makes it possible to integrate information that can be decomposed and represented
in a clean, modular way. For example, we would like to have information about home pages
and PostScript papers represented separately, using the information integration tool to
resolve these sources of information.
WHIRL is a conventional DBMS that has been extended to use ad hoc similarity metrics
developed in the information retrieval community. Using these metrics, it can reason about
pieces of text culled from heterogeneous sources based on the similarity of values rather than
on strict equality. WHIRL computes similarity using the \vector-space" representation to
model text (Salton, 1989). Each text object is represented by a vector of term weights
(where the terms have been stemmed using Porter's algorithm (Porter, 1980)) based on
the TFIDF weighting scheme. Similarity between two vectors is computed using the cosine
similarity metric. The answers to a query are presented by rank-ordering the generated
tuples, with tuples having more similar pairs of attribute fields appearing first.
For example, using WHIRL, we can pose the following query:
SELECT Reviewer.Name, Paper.ID
FROM Paper AND Reviewer
WHERE Reviewer.Descriptor SIM Paper.Abstract

This query will return a list of reviewer names and paper IDs for papers whose abstracts
were similar to the reviewer's interest descriptor. Rather than returning only those tuples for
which the descriptor and abstract fields are identical, as would be performed by a traditional
database join, this query returns Name and ID pairs for those tuples whose fields contain
similar terms, ordered according to decreasing value of similarity. The advantage of doing
ad hoc joins without requiring the textual fields to be identical to one another is important
when the text comes from multiple sources and thereby may use different terminology. It is
also important from the perspective of comparing the relative importance of different fields
to one another in an ecient way.
To use WHIRL all data must be stored in the form of WHIRL relations. For our
data we constructed two relations, each one representing different information sources. For
each conference submission, we form a Paper relation containing its id, abstract, keywords,
and title. For every reviewer, we form a Reviewer relation which contains a single tuple
with attributes representing the reviewer's name and some representation of the reviewer's
interests (for example, based on the reviewer's home page).
So far, we have discussed how we can use WHIRL to formulate queries involving a single
information source for both reviewers and papers. However, an advantage of the WHIRL
approach lies in the simplicity with which we can extend these queries to incorporate multiple sources. The primary advantage of using WHIRL in our work is the ease with which
we can measure the impact of conjunctive queries incorporating data from multiple sources.
We form conjunctive queries by adding multiple conditions to a WHERE clause:
SELECT Reviewer.Name, Paper.ID
FROM Paper AND Reviewer
WHERE Reviewer.Descriptor SIM Paper.Abstract
AND Reviewer.Descriptor SIM Paper.Keywords
238

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

When a WHIRL WHERE clause contains multiple conditions, the similarity scores
of the individual conjuncts are combined by taking their product as though they were
independent probabilities. Since similarity scores are not independent probabilities, we
only use it as a convenient way to combine scores, albeit one that offers a straightforward
approach to combination which has been previously studied (Cohen, 1998a). In the above
query, WHIRL would assign a score that reects both the similarity of the submitted
paper's abstract and the reviewer's descriptor, as well as the similarity of the submitted
paper's keywords and the reviewer's descriptor.
4.1.2 Combining Information Sources by Query Expansion

What does it mean for a recommendation algorithm to combine data from multiple information sources? This means enumerating the information sources that can be used as possible
inputs to the algorithm, and then defining a way to use these sources to compute similarity.
For instance, suppose we look at 1 reviewer source and 2 paper sources for a given collection
of reviewers and papers. To decide whether a paper is likely to interest the reviewer, we
can compute the similarity between the reviewer source and each of the paper sources and
combine the two similarity scores. Alternatively, we can compute a single similarity score
by first combining the two paper sources into a single representation and then computing
its similarity with respect to the reviewer source.
The idea of combining two sources into a single representation can be implemeted by
appending terms from the sources. In information retrieval, terms from relevant sources
are often appended to a baseline representation of a query during the process of query
reformulation. This is usually referred to as query expansion. Since our methods bear a
resemblance to query expansion, we make this analogy. These expansion methods will be
further described in the following sections. Of course, we do not have prior knowledge
of the relevance of our sources, and in this sense, we differ from the information retrieval
implementation of query expansion.
When we compare the relative performance of recommendation algorithms, we have
multiple dimensions along which to compare the results. We can differentiate the results
based on the methods used to combine the data and compute similarity or we can differentiate between the results based on which information sources were used in the comparison.
In other words, on the same set of inputs, does one method of query expansion perform
better than another? If we want to compare the merit of a single source, we can consider
two groups of algorithms { those that include a given source as input to the algorithm, and
those that exclude this source. If we simply count the number of times algorithms that
include this source outperform algorithms that exclude it, we can determine the relative
merit of the source.
4.1.3 The Concatenation Method

One way to \add" information from a new data source is to append the terms appearing in
the source to the original WHIRL query. For this type of query, we always have a single
WHIRL conjunct but each of the textual fields appearing in the conjunct can \grow" with
the addition of new terms. We call this method, queryConcat.
239

fiBasu, Hirsh, Cohen, & Nevill-Manning

Suppose, for example, that we start with the base query from the previous section that
only compares reviewer descriptors with paper abstracts. Now, suppose we want to compare
reviewer descriptors not only to the paper abstracts but also to the paper keywords. One
way to do this is to use the queryConcat method. We form a new field representing the
union of the words appearing in the paper abstract and paper keywords fields which we can
substitute in the original query. Let Paper.Descriptor = Paper.Abstract [ Paper.Keywords.
Our new query is:
SELECT Reviewer.Name, Paper.ID
FROM Paper AND Reviewer
WHERE Reviewer.Descriptor SIM Paper.Descriptor

Similarly, we can replace Paper.Descriptor in the WHERE clause to represent different
combinations of the fields, Paper.Abstract, Paper.Keywords and Paper.Title using the union
operator.
4.1.4 The Conjunction Method

As we previously stated, an important motivation for using WHIRL is its ability to execute
conjunctive queries, which we can also use to combine information sources in the recommendation process. For this type of query, instead of adding terms to any particular text field,
we add conjuncts to the original WHERE. We refer this method of reformulating queries
as queryConjunct.
We enumerate the query combinations that we considered for queryConjunct as follows.
Using the same sources as for queryConcat, we can begin the queries as before,
SELECT Reviewer.Name, Paper.ID
FROM Paper AND Reviewer
WHERE

but now, replacing the body of the WHERE clause with the following:
A: Reviewer.Descriptor SIM Paper.Abstract
K: Reviewer.Descriptor SIM Paper.Keywords
T: Reviewer.Descriptor SIM Paper.Title
AK: Reviewer.Descriptor SIM Paper.Abstract
AND Reviewer.Descriptor SIM Paper.Keywords
AT: Reviewer.Descriptor SIM Paper.Abstract
AND Reviewer.Descriptor SIM Paper.Title
KT: Reviewer.Descriptor SIM Paper.Keywords
AND Reviewer.Descriptor SIM Paper.Title
AKT: Reviewer.Descriptor SIM Paper.Abstract
AND Reviewer.Descriptor SIM Paper.Keywords
AND Reviewer.Descriptor SIM Paper.Title

We assign the labels, A (abstract), K (keywords), and T (title) to the queries to identify
the paper sources used. (We use these labels in a comparable fashion for the queryConcat
method, representing the information sources that are concatenated together.)
240

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

For each of the above queries, we can also vary the source of data used to represent the
reviewers. The first variant accounts for the case where the reviewer's descriptor contains the
words from the reviewer's home page; the second accounts for the case where the descriptor
contains the union of the first 300 words extracted from each PostScript file obtained from
the reviewer's Web pages.
We decided to try yet another combination to see whether using both representations
for reviewers would improve performance. For simplicity, we chose to test this hypothesis
with an expanded conjunctive query involving a single extra conjunct. We constructed
a Reviewer table that contains two attributes: Papers (consisting of the abstracts of the
reviewer's PostScript papers) and Homepage (consisting of the reviewer's home page). We
then ran each of the above queries, but now with an additional conjunct appearing in each
WHERE clause:
Reviewer.Homepage SIM Paper.Keywords

We chose to use Keywords as the Paper.Descriptor based on our intuitions that a paper's
keywords and a reviewer's homepage would have a greater number of words in common.

4.2 Recommending with Reviewer Preferences

Since we have evaluations from the reviewers on a common set of papers, one approach
for recommending papers would be to take this information and use it for collaborative
filtering. We note that for the actual conference reviewing problem, collaborative filtering
as a method for assigning papers may not be practical. Although we have the benefit of
using all of the preferences for a set of reviewers in our study, this information will generally
not be available to the reviewers as they are making their selections, thereby making it more
dicult to base predictions on the preferences of others. Nevertheless, it is worthwhile to
measure the impact of using reviewer preferences for the purpose of recommending papers.
The recommendation methodology for the collaborative filtering approaches is implemented as follows: each reviewer is presented with a recommended paper in an online
manner. After the paper is presented the reviewer tells the system if the paper was relevant. If it was, then the paper is assigned a rating of 1 and the paper is said to be rated
positively. If the paper was not relevant, it is assigned a rating of 0 and is said to be rated
negatively. Let Rating (R; P ) represent the rating that has been assigned to paper P by
reviewer R. When the paper is not relevant, the reviewer also provides a single relevant
paper as a positive example in order to condition future recommendations. Since we know
which papers were liked by the reviewers, we can simulate this process with the data that
we have. We experiment with two collaborative filtering algorithms: kNN (Hill et al., 1995;
Cohen & Fan, 2000) and Extended Direct Bayes (Cohen & Fan, 2000). We let P1 ,P2 ,...,P ,1
represent the papers that have been previously rated by the reviewer in t , 1 trials. The
kNN algorithm uses the following distance metric to locate other reviewers, R , closest to
the current reviewer with respect to the papers that have already been rated:
t

i

Dist(R; R0) = jRating (R; P1) , Rating (R0; P1)j + ::: + jRating (R; P ,1) , Rating (R0; P ,1)j
t

t

We can then compute a score for an arbitrary paper, P , with respect to the ratings of
the k closest reviewers, R1,...,R , as follows:
k

241

fiBasu, Hirsh, Cohen, & Nevill-Manning

Score(P ) = Rating (R1; P ) + ::: + Rating (R ; P )
k

According to the above methodology, the highest scoring paper will be presented to the
reviewer as the next recommendation.
Extended Direct Bayes can be viewed as an ad hoc extension of a direct Bayesian approach to recommendation. We define R(P ; P ) to represent the Laplace-corrected estimate
of the prior probability that the reviewer will give P a positive rating. (R(P ; P ) can be
thought of as measuring the \relatedness" between two papers.) Now consider an arbitrary
trial t and let P1 ,P2,...,P ,1 represent the papers that have been rated positively by the
reviewer on previous trials and consider an arbitrary trial t.
We can now use the following scoring function to rank each paper P :
i

j

j

i

j

t

Score(P ) = 1 , ((1 , R(P; P1))  :::  (1 , R(P; P ,1)))
t

The subtrahend in the above expression represents the probability that P is not related
to any P (assuming that the P 's are independent).
i

i

4.3 Evaluation Methodology

In the following sections, we evaluate the performance of our recommendation algorithms.
For collaborative filtering, we compute recommendations for a reviewer until we run out
of positive examples to use as feedback. For each reviewer's list of recommendations, we
measure precision in the Top N ; this gives us the proportion of the items returned in the
Top N for a given reviewer that were actually preferred by the reviewer. Although it is
possible to use other evaluation metrics, we compute precision at different levels of papers
returned since it is well-suited to the conference reviewing task. Since a reviewer may get a
list of about 10 papers to review, we would like to simulate this by recommending the Top
10 papers returned by our methods. By computing precision, we measure the percentage of
papers in this list that would have matched the reviewer's preferences. This metric is also
commonly used in the literature. For instance, Dumais and Nielsen (1992) mostly used this
measure, i.e., the number of relevant articles in the Top 10, when reporting their results
since this constituted a reasonable reviewer load. We additionally report results of precision
at Top 30. For the kNN algorithm, we set k = 10 for our experiments.
Our recommendation algorithms can be seen as a choice of a query expansion method
crossed against a choice of the input data sources. For each of the methods queryConjunct
and queryConcat, we ran 3  7 queries detailed in the previous section. This resulted in 21
runs per reviewer, per method. Each run returned an ordered list of paper IDs. For each run,
we again measure precision in the Top N (for N = 10 and N = 30). In our discussion, we
refer to a run using abstracts based on a reviewer's papers as a p run. Similarly, h runs will
be based on a reviewer's home page. Finally, ph runs combine both sources of information
(using the extra conjunct). The results we will report represent precision values averaged
across the reviewers. In order for us to compare performance across different information
sources, we need to do our evaluation using the same population of reviewers. Not all of
the reviewers who provided preference data had home pages and/or papers available online.
Therefore, we performed a set of runs using 50 reviewers randomly chosen from the set of
242

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

Source(s)
p(Top10)
h(Top10)
ph(Top10)
p(Top30)
h(Top30)
ph(Top30)

A
0.248
0.210
0.334
0.194
0.169
0.245

K
0.260
0.284
0.304
0.201
0.217
0.219

T
0.234
0.232
0.332
0.177
0.183
0.233

AK
0.266
0.288
0.312
0.198
0.226
0.224

AT
0.274
0.270
0.342
0.195
0.199
0.241

KT
0.308
0.320
0.286
0.220
0.232
0.211

AKT
0.330
0.332
0.374
0.232
0.232
0.249

Table 1: Average Precision Scores at Top 10 and Top 30 Papers Returned using queryConjunct.
reviewers who had both home pages and papers available online, and report results averaged
across these 50 reviewers.
As we mentioned earlier, reviewer choices may be inuenced by a variety of factors
ranging from a person's curiosity to a paper's readability. Many of these factors are dicult
to model. Furthermore, human judges may assign papers to reviewers according to criteria
other than relevance of paper contents to reviewer interests, and their individual opinions
may vary. Therefore, it is highly unlikely that our proposed methods will achieve 100%
precision. Unfortunately, given the nature of the problem, we have not been able to get
an assessment of how human judges would have done at the same task. Nevertheless, we
can evaluate our recommendation framework built on content-based information retrieval
principles and compare relative performance to other reasonable baseline approaches.

5. Results
There are a number of questions we would like to keep in mind as we analyze the results.
In the course of our experiments we vary both the amount of information input to our
algorithms and the method of query expansion used by the algorithms. One of the questions
we would like to answer is what algorithm or set of algorithms is most suited to the task
at hand? We also ask whether the choice of inputs results in measurable differences in
performance. The tabulation of results which provides the basis for analyzing our contentbased algorithms is presented in Table 1 and Table 2. The baseline method against which we
compare all algorithms is random assignment. This method assigns each reviewer a random
collection of papers. With this method, we can expect a precision of 7.0%. In other words,
this means that if we were to select papers randomly, on average, each reviewer would like
fewer than 1 out of 14 of the papers selected.
Table 1 and Table 2 are replicates of the source combinations matrix we had discussed
earlier. Since we ran two trials for Top N papers returned, each table is actually the
concatenated representation of the matrices for the Top 10 and Top 30 experiments. In
the first three rows of Table 1 and Table 2, we report precision figures of the Top 10
papers returned for the queryConjunct method and the queryConcat method, respectively.
243

fiBasu, Hirsh, Cohen, & Nevill-Manning

Precision at Top 10 and Top 30
0.4
compare.dat
x

0.38
0.36
0.34
queryConjunct method

0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
queryConcat method

0.35

0.4

Figure 1: A Comparison of Two Query Methods
Similarly, we show the results for Top 30 papers returned in the bottom three rows of the
tables. Since we can view the rows as representing the reviewer sources used in a query and
the columns representing the paper sources, we can measure the impact of adding data in
two ways. By reading across a row, across groups of columns representing N information
sources, we can gauge how the results vary as more paper data are included in the queries.
Similarly, by reading down a column, we can gauge the differences in the results as more
reviewer data are included in the queries.
Given this information, what can we say about the performance of our recommendation
algorithms that used different methods of query expansion? We can compare the relative
performance of the two methods queryConjunct and queryConcat based on the values listed
in Table 1 and Table 2. Note that in all cases performance of these methods exceeds
that of random selection, with accuracies a factor 2 to 5 times better. In Figure 1, we
record this information as a data point for every query that uses two or more sources of
information (since the methods differ in how they combine data from two or more sources,
it is meaningless to plot points that refer to queries using a single source). In this figure, the
x-axis represents queries expanded using the queryConcat method and the y-axis represents
queries expanded using the queryConjunct method. If a point falls on the x = y line, then
the two methods yielded the same performance for a query using the same information
sources. All points that fall in the area above the x = y line mark those queries where
queryConjunct had higher precision than queryConcat. The data reveal that in almost all
cases, queryConjunct had higher precision than queryConcat, thereby making queryConjunct
the dominant of the two query expansion methods and the preferred method of the two for
the task at hand.
Our expectation is that as we increase the source data we should notice an increase
in precision. Specifically, we note that for queryConjunct, the query that uses the most
information for a paper submission in a majority of cases performs statistically significantly
244

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

Source(s)
p(Top10)
h(Top10)
ph(Top10)
p(Top30)
h(Top30)
ph(Top30)

A
0.248
0.210
0.258
0.194
0.169
0.197

K
0.260
0.284
0.272
0.201
0.217
0.212

T
0.234
0.232
0.242
0.177
0.183
0.180

AK
0.264
0.226
0.262
0.202
0.179
0.203

AT
0.266
0.226
0.260
0.202
0.179
0.203

KT
0.276
0.308
0.300
0.201
0.199
0.209

AKT
0.266
0.222
0.274
0.209
0.184
0.211

Table 2: Average Precision Scores at Top 10 and Top 30 Papers Returned using queryConcat.
better5 than queries that use less information and in no case performs statistically significantly worse.
We should note that adding information will not always lead to monotonically better
results. Notice that for queryConjunct, in the case of Top 30 papers returned, hKT is indistinguishable from hAKT. We also note that phT performs better (though not statistically
significantly better) than phKT. There are similar cases for queryConcat. How do we explain these gaps? If these are indeed gaps, i.e., they are true statistical differences, then we
may consider as an explanation that adding information may also be increasing the amount
of noise in our representations. Consider, for example, that keywords from a fixed list can
often be a poor match to the real subject matter of a paper. In these special cases, the use
of keywords as a source could lead to a degradation in retrieval performance.
Analogous to our analysis of the paper sources, we can now examine any column of
Table 1 or Table 2 and measure the effect of adding more information to the reviewer representation. For queryConjunct, a majority of the time, we find that queries incorporating
more information (ph entries) perform statistically significantly better than single source
queries (p and h entries).
So far, we have illustrated how we can move across groups of columns or blocks of
rows in the source combinations matrix, adding sources to the queries until there is no
improvement. How significant are the gains that we can realize when we do this? Focusing
on queryConjunct, for every reviewer source, we consider queries that contained data from
a single paper source and had the lowest precision. We pair each of these queries with the
corresponding query in the same row of the matrix that made use of all of the paper sources
and report the resulting improvement in precision in Table 3. For the Top 10 results, we
note that in the best case, we can gain an improvement in precision of 58% when going from
a single-source to a multi-source query, and for the Top 30 results, we gain an improvement
5. All comparisons between two queries Qi and Qj were made using a two-tailed sign test. Specifically, we
consider the set Rij of reviewers r for which precision(Qi ; r) 6= precision(Qj ; r) and then estimate the
probability
pij = P rob(precision(Qi ; r ) > precision(Qj ; r ) j r 2 Rij )
We consider a difference to be statistically significant if one can reject with confidence > 0.95 the null
hypothesis that pij was generated by j Rij j independent ips of a fair coin.

245

fiBasu, Hirsh, Cohen, & Nevill-Manning

Single-Source Queries Improvement After Adding Two Sources
pT(Top 10)
41%
hA(Top 10)
58%
phK(Top 10)
23%
pT(Top 30)
31%
hA(Top 30)
37%
phK(Top 30)
14%

Table 3: A Comparision of Single-Source vs. Multi-Source Queries.
Methods(s)

Top 10 Top 30
kNN
0.294 0.154
ExtendedDirectBayes 0.300 0.129

Table 4: Average Precision Scores at Top 10 and Top 30 Papers Returned using Collaborative Filtering Methods.
of 37%. These results do support our intuitions that by incorporating more information in
our queries, the quality of the retrieval results improves. Since we have a different paper
source for the single-source queries in each row of Table 3, we also note that the impact of
any given paper source is dependent on the reviewer representation that we use.
Can we still come up with an assessment of which sources are significant for the conference reviewing task? For queryConjunct, we present a series of figures (Figure 2 to Figure 6)
that illustrate the impact of each source by plotting precision values of queries that exclude
the source along the x-axis and precision values of queries that include the source along the
y-axis (for both N = 10 and N = 30). If a point falls on the x = y line, then the queries
have exactly the same performance | the choice of source is irrelevant. All points that fall
in the area above the x = y line mark those queries that had higher precision compared to
their query counterparts which did not contain the source.
By simply counting the number of times the queries that include a source outperform
the queries that did not include the source, we have one way of ranking the sources in
decreasing order of importance. In this case, queries that include the abstract source for
papers and the home page source for reviewers have the highest rates of success (when
compared to the other information sources for papers and reviewers, respectively).
Now, the natural question to ask is whether the trends that we noticed for queryConjunct
also hold for queryConcat. The answer is no, which also means that queryConcat does not
give us a definitive answer to the question of whether more information is really better.
Just as we have noticed that query performance is linked to both the reviewer and paper
sources, we also find that it is linked to the query expansion method.
246

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

Precision at Top 10 and Top 30
0.4
A.dat
x

0.38
0.36
0.34

Queries with Abstract

0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Abstract

0.35

0.4

Figure 2: The Role of Abstract as an Information Source

Precision at Top 10 and Top 30
0.4
K.dat
x

0.38
0.36

Queries with Keywords

0.34
0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Keywords

0.35

0.4

Figure 3: The Role of Keywords as an Information Source

247

fiBasu, Hirsh, Cohen, & Nevill-Manning

Precision at Top 10 and Top 30
0.4
T.dat
x

0.38
0.36
0.34

Queries with Titles

0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Titles

0.35

0.4

Figure 4: The Role of Title as an Information Source

Precision at Top 10 and Top 30
0.4
P.dat
x

0.38
0.36

Queries with Reviewer Papers

0.34
0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Reviewer Papers

0.35

0.4

Figure 5: The Role of Papers as an Information Source

248

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

Precision at Top 10 and Top 30
0.4
H.dat
x

0.38

Queries with Reviewer Homepages

0.36
0.34
0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Reviewer Homepages

0.35

0.4

Figure 6: The Role of Homepage as an Information Source
In Table 4, we show the results of the collaborative filtering runs. We report averages
of the precision values computed for the Top N (for N =10 and N =30) papers returned
based on the reviewer recommendation lists. Since we stop recommending after we have
exhausted the set of positive examples for a reviewer, the reviewer recommendation lists are
of varying lengths. In those cases where the size of the list is less than N , we still compute
precision at Top N , assuming the remaining items are incorrect predictions. Both methods
for collaborative filtering exceed random selection by a significant margin.
For Top 10 papers returned, the collaborative recommendation methods are competitive with the best performance of queryConcat. This is already an interesting observation,
since not only do the methods differ, but each method is using different data to make recommendations. We can further state than when we use queryConjunct and all information
sources to recommend 10 papers, on average almost four papers coincide with the reviewer's
preferences. Compared to random selection, collaborative filtering, and queryConcat, this
method yields more papers of interest to reviewers.
In summary, what have we learned from our experiments? We have found that within
the context of peer reviewing of papers, we can make the recommendation process less
\people intensive". Most recommendation systems require their users to provide samples
of their preferences which are then used to extrapolate future behaviors. Collaborative
methods go even further by using preference information across multiple users to predict
the preferences of a single user. By automatically collecting reviewer interest information
from Web sources and precomputing similarities between these profiles and paper content,
we require less input from the reviewers. Furthermore, our content-based retrieval methods
can exceed the performance of collaborative methods in this task.
We also believe that our recommendation framework provides an extensible way of
formulating queries that provides more control over the information content of the queries.
We can control not only how much information we include in our queries but also how we
incorporate that information. As new data become available, we can evaluate which data
249

fiBasu, Hirsh, Cohen, & Nevill-Manning

sources and/or combinations are more effective, thereby fine-tuning the query formulation
process.

6. Related Work on Query Reformulation

Since our work on expanding queries using WHIRL can be viewed as a type of query
reformulation, we review some related work in the information retrieval community on this
topic. Salton (1989) describes the process of query reformulation as that of \moving" a
given query towards the relevant items and away from the nonrelevant ones. In the context
of the vector-space model of retrieval, this means that given a query expression of the form
(Salton, 1997):
Q 0 = (q 1 ; q 2 ; :::; qt)

where q is a number between 0 and 1 representing the weight assigned to term , we want
to arrive at a new query expression:
i

i

Q 0 = (q 1 ; q 2 ; :::; qt)
0

0

0

0

such that the weights are adjusted so that new terms can be introduced into the vector
representation, while other terms can effectively be removed by reducing their respective
weights to 0.
Harman (1992) describes the operational procedure underlying this process as the merging of document and query vectors. More specifically, this means that query terms not in
the original query but appearing in the relevant documents are added to the initial query
expression. The expansion occurs using both positive and negative weights, depending on
whether the terms appears in a relevant or non-relevant document.
The above description assumes that we have relevance judgments for documents that
the system can return. Practically speaking, this type of information is hard to come by.
Therefore, people have been seeking to compensate for this lack of information by expanding
queries using a variety of techniques such as the use of thesauri and relevance feedback. In
the latter case, query reformulation is part of an iterative and interactive process whereby
users are presented with the results of a retrieval and are asked to supply feedback regarding
the relative importance of the results.
Comparing our approach with these methods of query reformulation, we make a couple of
observations. First, query reformulation can be driven by knowledge we have precomputed
about a data colection. Given that entities such as papers have abstracts, keywords, and
titles, does it make sense to vary the amount of this information in the queries? If we have
the equivalent of Table 3 for a collection, we can do a table lookup at run time to determine
which formulations are the most promising.
We note that the way we construct queries for the queryConjunct method combines
aspects of both the Boolean and vector-space models of query formulation into a hybrid approach. In the case of Boolean queries, relevance feedback can lead to new query expressions
consisting of term conjuncts such as (Salton, 1997):
(Term AND Term AND Term )
Notice that if we replace any Term with Vector in the above expression, we have a
query expression formulated according to our queryConjunct method.
i

j

i

k

i

250

fiTechnical Paper Recommendation: A Study in Combining Multiple Information Sources

7. Conclusions

In this paper, we have shown that we can collect information about reviewers automatically
from the Web, and we can use it as a part of a recommendation framework to route papers
to reviewers. We treat the problem as one of decomposing reviewer interest and paper
contents into information sources, and then of combining the information sources using
different query formulations. In our experiments, we compared two ways of formulating
queries using content-based information retrieval and one collaborative approach. We have
found that the recommendation algorithm using conjunctive queries outperforms the other
approaches. We have also looked at using different subsets of the information sources in our
algorithms, and in the case of our optimal algorithm, we found that using more information
generally lead to better performance.
In a practical setting, the recommendation method of choice is likely to depend on a
number of factors ranging from the availability of information to ease of use. On the one
hand, our framework provides a more exible alternative to simple keyword-based searches
and a less intrusive alternative to collaborative methods. On the other hand, our methods
assume that we can obtain data that are reliable, accurate, and timely. Based on our results,
we are optimistic that the Web can provide credible information sources that can be used
successfully in the recommendation process.

8. Acknowledgments

We extend our thanks to AAAI, the AAAI reviewers, the AAAI paper authors, members
of the Rutgers Machine Learning Research Group, and the reviewers of this paper for their
inputs in this work.
We note that the following are the property of their respective companies as listed:
WHIRL (AT&T Labs { Research), LSI (Telcordia Technologies, Inc.).

References

Basu, C., Hirsh, H., & Cohen, W. (1998). Recommendation as classification: Using social
and content-based information in recommendation. In Proceedings of AAAI-98.
Cohen, W. (1998a). Integration of heterogeneous databases without common domains using
queries based on textual similarity. In Proceedings of ACM SIGMOD-98.
Cohen, W. (1998b). The whirl approach to information integration. In IEEE Intelligent
Systems. IEEE Press.
Cohen, W., & Fan, W. (2000). Web-collaborative filtering: Recommending music by crawling the web. In Proceedings of WWW-2000.
Cohen, W., & Hirsh, H. (1998). Joins that generalize: Text classification using whirl. In
Proceedings of KDD-98.
Dillon, M., & Desper, J. (1980). Automatic relevance feedback in boolean retrieval systems.
Journal of Documentation, 36.
251

fiBasu, Hirsh, Cohen, & Nevill-Manning

Dumais, S., & Nielsen, J. (1992). Automating the assignment of submitted manuscripts to
reviewers. In Proceedings of ACM SIGIR-92.
Geller, J. (1997). Challenge: How ijcai 1999 can prove the value of ai by using ai. In
Proceedings of IJCAI-97.
Gupta, D., Digiovanni, M., Narita, H., & Goldberg, K. (1999). Jester 2.0: A new lineartime collaborative filtering algorithm applied to jokes. In Workshop on Recommender
Systems at ACM SIGIR-99.
Harman, D. (1992). Relevance feedback revisited. In Proceedings of ACM SIGIR-92.
Hill, W., Stead, L., Rosenstein, M., & Furnas, G. (1995). Recommending and evaluating
choices in a virtual community of use. In Proceedings of CHI-95.
Konstan, J., Miller, B., Maltz, D., Herlocker, L., Gordon, L., & Riedl, J. (1997). Grouplens:
Applying collaborative filtering to usenet news.. Vol. 40.
Nevill-Manning, C., Reed, T., & Witten, I. (1998). Extracting text from postscript. Software
Practice and Experience, 28 (5).
Pazzani, M., & Billsus, D. (1997). Learning and revising user profiles: The identification of
interesting web sites. Machine Learning, 27, 313{331.
Porter, M. (1980). An algorithm for sux stripping. Program, 14, 130{137.
Salton, G. (1989). Automatic Text Processing. Addison Wesley.
Salton, G. (1997). Improving retrieval performance by relevance feedback. In Readings in
Information Retrieval.
Shakes, J., Langheinrich, M., & Etzioni, O. (1997). Dynamic reference sifting: a case study
in the homepage domain. In Proceedings of WWW-97.
Shardanand, U., & Maes, P. (1995). Social information filtering: Algorithms for automating
\word of mouth". In Proceedings of CHI-95.
Yarowsky, D., & Florian, R. (1999). Taking the load off the conference chairs: towards a
digital paper-routing assistant. In Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in NLP and Very-Large Corpora.

252

fiJournal of Artificial Intelligence Research 14 (2001) 53-81

Submitted 8/00; published 3/01

Conict-Directed Backjumping Revisited
Xinguang Chen

xinguang@cs.ualberta.ca

Peter van Beek

vanbeek@uwaterloo.ca

Department of Computing Science, University of Alberta
Edmonton, Alberta, Canada T6G 2H1
Department of Computer Science, University of Waterloo
Waterloo, Ontario, Canada N2L 3G1

Abstract

In recent years, many improvements to backtracking algorithms for solving constraint
satisfaction problems have been proposed. The techniques for improving backtracking algorithms can be conveniently classified as look-ahead schemes and look-back schemes. Unfortunately, look-ahead and look-back schemes are not entirely orthogonal as it has been
observed empirically that the enhancement of look-ahead techniques is sometimes counterproductive to the effects of look-back techniques. In this paper, we focus on the relationship
between the two most important look-ahead techniques|using a variable ordering heuristic and maintaining a level of local consistency during the backtracking search|and the
look-back technique of conict-directed backjumping (CBJ). We show that there exists a
\perfect" dynamic variable ordering such that CBJ becomes redundant. We also show
theoretically that as the level of local consistency that is maintained in the backtracking
search is increased, the less that backjumping will be an improvement. Our theoretical
results partially explain why a backtracking algorithm doing more in the look-ahead phase
cannot benefit more from the backjumping look-back scheme. Finally, we show empirically
that adding CBJ to a backtracking algorithm that maintains generalized arc consistency
(GAC), an algorithm that we refer to as GAC-CBJ, can still provide orders of magnitude
speedups. Our empirical results contrast with Bessiere and Regin's conclusion (1996) that
CBJ is useless to an algorithm that maintains arc consistency.

1. Introduction

Constraint satisfaction problems (CSPs) are a generic problem solving framework. A constraint satisfaction problem consists of a set of variables, each associated with a domain of
values, and a set of constraints. Each of the constraints is expressed as a relation, defined
on some subset of the variables, denoting the consistent value assignments that satisfy the
constraint. A solution to a CSP is an assignment of a value to every variable, in such a way
that every constraint is satisfied.
Constraint satisfaction problems are usually solved by search methods, among which
the backtracking algorithm and its improvements are widely used. The techniques for
improving backtracking algorithms can be conveniently classified as look-ahead schemes
and look-back schemes (Dechter, 1992). Look-ahead schemes are invoked whenever the
algorithm is preparing to extend the current partial solution. Look-ahead schemes include
the functions that choose the next variable to be instantiated, choose the next value to
give to the current variable, and reduce the search space by maintaining a certain level of
local consistency during the search (e.g., Bacchus & van Run, 1995; Bessiere & Regin, 1996;
c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiChen & van Beek

Haralick & Elliott, 1980; McGregor, 1979; Nadel, 1989; Sabin & Freuder, 1994). Lookback schemes are invoked whenever the algorithm encounters a dead-end and prepares for
the backtracking step. Look-back schemes include the functions that decide how far to
backtrack by analyzing the reasons for the dead-end (backjumping) and decide what new
constraints to record so that the same conicts do not arise again later in the search (e.g.,
Bruynooghe, 1981; Dechter, 1990; Frost & Dechter, 1994; Gaschnig, 1978; Prosser, 1993b;
Schiex & Verfaillie, 1994).
A backtracking algorithm can be a hybrid of both look-ahead and look-back schemes
(Prosser, 1993b). In this paper, we focus on the relationship between the two most important look-ahead techniques|using a variable ordering heuristic and maintaining a level of
local consistency during the backtracking search|and the look-back technique of conictdirected backjumping (CBJ) (Prosser, 1993b). Unfortunately, these look-ahead and lookback schemes are not entirely orthogonal as it can be observed in previous experimental
work that as the level of consistency that is maintained in the backtracking search is increased and as the variable ordering heuristic is improved, the effects of CBJ are diminished
(Bacchus & van Run, 1995; Bessiere & Regin, 1996; Prosser, 1993a, 1993b). For example, it
can be observed in Prosser's (1993b) experiments that, given a static variable ordering, increasing the level of local consistency maintained from none to the level of forward checking,
diminishes the effects of CBJ. Bacchus and van Run (1995) observe from their experiments
that adding a dynamic variable ordering (an improvement over a static variable ordering)
to a forward checking algorithm diminishes the effects of CBJ. In their experiments the
effects are so diminished as to be almost negligible and they present an argument for why
this might hold in general. Bessiere and Regin (1996) observe from their experiments that
simultaneously increasing the level of local consistency even further to arc consistency and
further improving the dynamic variable ordering heuristic diminishes the effects of CBJ
so much that, in their implementation, the overhead of maintaining the data structures for
backjumping actually slows down the algorithm. They conjecture that when arc consistency
is maintained and a good variable ordering heuristic is used, \CBJ becomes useless".
In this paper, we present theoretical results that deepen our understanding of the relationship between look-ahead techniques and the CBJ look-back technique. We show that
there exists a \perfect" dynamic variable ordering for the chronological backtracking algorithm such that CBJ becomes redundant. The more that a variable ordering heuristic is
consistent with the \perfect" heuristic, the less chance CBJ has to reduce the search effort.
We also show that CBJ and an algorithm that maintains strong k-consistency in the backtracking search are incomparable in that each can be exponentially better than the other.
This result is refined by introducing the concept of backjump level in the execution of a
backjumping algorithm and showing that an algorithm that maintains strong k-consistency
never visits more nodes than a backjumping algorithm that is allowed to backjump at most
k levels. Thus, as the level of local consistency that is maintained in the backtracking search
is increased, the less that backjumping will be an improvement. Together, our theoretical
results partially explain why a backtracking algorithm doing more in the look-ahead phase
cannot benefit more from the backjumping look-back scheme. Our results also extend the
partial ordering of backtracking algorithms presented by Kondrak and van Beek (1997) to
include backtracking algorithms and their CBJ hybrids that maintain levels of local con54

fiConflict-Directed Backjumping Revisited

sistency beyond forward checking, including the important algorithms that maintain arc
consistency.
We also present empirical results that show that, although the effects of CBJ may
be diminished, adding CBJ to a backtracking algorithm that maintains generalized arc
consistency (GAC), an algorithm that we refer to as GAC-CBJ, can still provide orders
of magnitude speedups. Our empirical results contrast with Bessiere and Regin's (1996)
conclusion that CBJ is useless to an algorithm that maintains arc consistency.

2. Background

In this section, we formally define constraint satisfaction problems, and briey review local
consistency and the search tree explored by a backtracking algorithm.

2.1 Constraint Satisfaction Problems
Definition 1 (CSP) An instance of a constraint satisfaction problem is a tuple P =
(V ; D; C ), where1
 V = fx1; : : :; xng is a finite set of n variables,
 D = fdom(x1); : : :; dom(xn)g is a set of domains. Each variable x 2 V is associated with a finite domain of possible values, dom(x). The maximum domain size
maxx2V jdom(x)j is denoted by d,

 C = fC1; : : :; Cmg is a finite set of m constraints or relations. Each constraint C 2 C
is a pair (vars(C ); rel(C )), where

{ vars(C ) = fxi1 ; : : :; xiri g is an ordered subset of the variables, called the con-

straint scope or scheme, the size of vars(C ) is known as the arity of the constraint. If the arity of the constraint is equal to 2, it is called a binary constraint.
A non-binary constraint is a constraint with arity greater than 2. The maximum
arity of the constraints in C , maxC 2C jvars(C )j, is denoted by r,
{ rel(C ) is a subset of the Cartesian product dom(xi1 )  dom(xiri ) that specifies
the allowed combinations of values for the variables in vars(C ). An element of
the Cartesian product dom(xi1 )      dom(xiri ) is called a tuple on vars(C ).
Thus, rel(C ) is often regarded as a set of tuples over vars(C ).

In the following, we assume that for any variable x 2 V , there is at least one constraint
C 2 C such that x 2 vars(C ). By definition, a tuple over a set of variables X = fx1; : : :; xkg
is an ordered list of values (a1 ; : : :; ak ) such that ai 2 dom(xi ), i = 1; : : :; k. A tuple over X
can also be regarded as a set of variable-value pairs fx1 a1 ; : : :; xk ak g. Furthermore,
a tuple over X can be viewed as a function t : X ! [x2X dom(x) such that for each variable
x 2 X , t[x] 2 dom(x). For a subset of variables X 0  X , we use t[X 0] to denote a tuple over
X 0 by restricting t over X 0. We also use vars(t) to denote the set of variables for tuple t.
1. Throughout the paper, we use n, d, m, and r to denote the number of variables, the maximum domain
size, the number of constraints, and the maximum arity of the constraints in the CSP, respectively.

55

fiChen & van Beek

An assignment to a set of variables X is a tuple over X . We say an assignment t to X
is consistent with a constraint C if either vars(C ) 6 X or t[vars(C )] 2 rel(C ). A partial
solution to a CSP is an assignment to a subset of variables. We say a partial solution is
consistent if it is consistent with each of the constraints. A solution to a CSP is a consistent
partial solution over all the variables. If no solution exists, the CSP is said to be insoluble.
A CSP is empty if either one of its variables has an empty domain or one of its constraints
has an empty set of tuples. Obviously, an empty CSP is insoluble. Given two CSP instances
P1 and P2 , we say P1 = P2 if they have exactly the same set of variables, the same set of
domains and the same set of constraints; i.e., they are syntactically the same.

Definition 2 (projection) Given a constraint C and a subset of variables S  vars(C ),
the projection S C is a constraint, where vars(S C ) = S and rel(S C ) = ft[S ] j t 2
rel(C )g.
Definition 3 (selection) Given a constraint C and an assignment t to a subset of variables X  vars(C ), the selection tC is a constraint, where vars(t C ) = vars(C ) and
rel(tC ) = fs j s[X ] = t and s 2 rel(C )g.
2.2 Local Consistency

An inconsistency is a consistent partial solution over some of the variables that cannot be
extended to additional variables and so cannot be part of any global solution. If we are
using a backtracking search to find a solution, such an inconsistency can lead to a dead end
in the search. This insight has led to the definition of properties that characterize the level
of consistency of a CSP and to the development of algorithms for achieving these levels
of consistency by removing inconsistencies (e.g., Mackworth, 1977a; Montanari, 1974), and
to effective backtracking algorithms for finding solutions to CSPs that maintain a level of
consistency during the search (e.g., Gaschnig, 1978; Haralick & Elliott, 1980; McGregor,
1979; Sabin & Freuder, 1994).
Mackworth (1977a) defines three properties of binary CSPs that characterize local consistencies: node, arc, and path consistency. Mackworth (1977b) generalizes arc consistency
to non-binary CSPs.

Definition 4 (arc consistency) Given a constraint C and a variable x 2 vars(C ), a
value a 2 dom(x) is supported in C if there is a tuple t 2 rel(C ), such that t[x] = a. t
is then called a support for fx ag in C . C is arc consistent if for each of the variables
x 2 vars(C ), and each of the values a 2 dom(x), fx ag is supported in C . A CSP is arc

consistent if each of its constraints is arc consistent.
Freuder (1978) generalizes node, arc, and path consistency, to k-consistency.

Definition 5 (k-consistency) A CSP is k-consistent if and only if given any consistent
partial solution over k , 1 distinct variables, there exists an instantiation of any kth variable
such that the partial solution plus that instantiation is consistent. A CSP is strongly kconsistent if it is j -consistent for all 1  j  k.
56

fiConflict-Directed Backjumping Revisited

For binary CSPs, node, arc and path consistency correspond to one-, two- and threeconsistency, respectively. However, the definition of k-consistency does not require the CSP
to be binary and arc consistency is not the same as two-consistency for non-binary CSPs.
A strongly n-consistent CSP has the property that any consistent partial solution can be
successively extended to a full solution of the CSP without backtracking.

2.3 Search Tree and Backtracking Algorithms

The idea of a backtracking algorithm is to extend partial solutions. At each stage, an uninstantiated variable is selected and assigned a value from its domain to extend the current
partial solution2 . Constraints are used to check whether such an extension may lead to a
possible solution of the CSP and to prune subtrees containing no solutions based on the
current partial solution. During a backtracking search, the variables can be divided into
three sets: past variables (already instantiated), current variable (now being instantiated),
and future variables (not yet instantiated). A dead-end occurs when all values of the current variable are rejected as not leading to a full solution. In such a case, some instantiated
variables become uninstantiated ; i.e., they are removed from the current partial solution.
This process is called backtracking. If only the most recently instantiated variable becomes
uninstantiated then it is called chronological backtracking ; otherwise, it is called backjumping. A backtracking algorithm terminates when all possible assignments have been tested
or a certain number of solutions have been found.
A backtracking search may be seen as a search tree traversal. In this approach we
identify tuples (assignments of values to variables) with nodes: the empty tuple is the root
of the tree, the first level nodes are 1-tuples (representing an assignment of a value to a
single variable), the second level nodes are 2-tuples, and so on. The levels closer to the
root are called shallower levels and the levels farther from the root are called deeper levels.
Similarly, the variables corresponding to these levels are called shallower and deeper. We
say that a backtracking algorithm visits a node in the search tree if at some stage of the
algorithm's execution the current partial solution identifies the node. The nodes visited
by a backtracking algorithm form a subset of all the nodes belonging to the search tree.
We call this subset, together with the connecting edges, the backtrack tree generated by a
backtracking algorithm.
The backtracking algorithm conict-directed backjumping (CBJ) (Prosser, 1993b) maintains a conict set for every variable. Every time an instantiation of the current variable
xi is in conict with an instantiation of some past variable xh , the variable xh is added to
the conict set of xi . When there are no more values to be tried for the current variable xi ,
CBJ backtracks to the deepest variable xh in the conict set of xi . At the same time, the
variables in the conict set of xi , with the exception of xh , are added to the conict set of
xh , so that no information about conicts is lost.
Throughout the paper we refer to the following backtracking algorithms (see Kondrak
& van Beek, 1997; Prosser, 1993b for detailed explanations and examples of most of these
algorithms): chronological backtracking (BT), backjumping (BJ) (Gaschnig, 1978), conictdirected backjumping (CBJ) (Prosser, 1993b), forward checking (FC) (Haralick & Elliott,
1980; McGregor, 1979), forward checking and conict-directed backjumping (FC-CBJ)
2. Throughout this paper, we assume that a static value ordering is used in the backtracking search.

57

fiChen & van Beek

(Prosser, 1993b), maintaining arc consistency (MAC) (Gaschnig, 1978; Sabin & Freuder,
1994), and maintaining arc consistency and conicted-directed backjumping (MAC-CBJ)
(Prosser, 1995).

3. Variable Ordering Heuristics and Backjumping

In this section, we present theoretical results that deepen our understanding of the relationship between the look-ahead technique of using a variable ordering heuristic and the
look-back technique of CBJ.
In previous work, Kondrak and van Beek (1997) show that, given the same deterministic
static or dynamic variable ordering heuristic, CBJ never visits more nodes than BT. Bacchus
and van Run (1995) show that BJ, a restricted version of CBJ, visits exactly the same nodes
as BT if the fail-first dynamic variable ordering heuristic is used. Previous empirical work
shows that the number of nodes that CBJ saves depends on the variable ordering heuristic
used (Bacchus & van Run, 1995; Bessiere & Regin, 1996; Prosser, 1993b).
We show that, given a CSP and a variable ordering for CBJ, there exists a \perfect"
variable ordering for the chronological backtracking algorithm (BT) such that BT never
visits more nodes than CBJ. The more that a variable ordering heuristic is consistent with
the \perfect" heuristic, the less chance CBJ has to reduce the search effort.
We first consider the case of insoluble CSPs. When CBJ is applied to an insoluble CSP,
it always backjumps from a dead-end state; i.e., it does not terminate or backjump from a
situation in which a solution of the CSP was found.

Lemma 1 Given an insoluble CSP and a variable ordering for CBJ, there exists a variable

ordering for BT such that BT never visits more nodes than CBJ to show that no solution
exists.

Proof In the backtrack tree generated by CBJ under the variable ordering, let the last

backjump that terminates the execution of CBJ be from variable xj to the root of the
backtrack tree. We choose xj to be the first variable for BT. For each value a in the domain
of xj , if the current node in the backtrack tree for CBJ is consistent (not a leaf node), the
next variable chosen to be instantiated after assigning a to xj is the variable that backjumps
to xj and causes the assignment xj a to be revoked. The entire variable ordering for
BT can be worked out in a similar, recursive manner. For this variable ordering for BT to
be well-defined, it remains to show that if the current node in the backtrack tree for CBJ
is inconsistent (a leaf node), the corresponding node in the backtrack tree for BT is also
inconsistent (and therefore no next variable needs to be chosen). We show that the variables
skipped in the variable ordering constructed for BT are irrelevant to the dead-end states
encountered by CBJ. Suppose at a stage we have ordered the variables to be instantiated
for BT as xj1 ; : : :; xjk , and for value a 2 dom(xjk ) we choose the next variable xjk+1 as
the variable which backjumps to the current variable xjk in the CBJ backtrack tree. We
prove by induction that the conict set of xjk+1 used in the backjumping is subsumed by
fxj1 ; : : :; xjk g. k = 1 is the case of the last backjump that terminates the execution of CBJ.
The hypothesis is true because the conict set of xj1 is an empty set. Suppose it is true for
the case of k > 1. Because xjk+1 backjumps to xjk , the conict set of xjk+1 is merged in the
conict set of xjk . From the inductive assumption, the conict set of xjk is subsumed by
58

fiConflict-Directed Backjumping Revisited

fxj1 ; : : :; xjk,1 g, and thus the conict set of xjk+1 is subsumed by fxj1 ; : : :; xjk g. Therefore,
the hypothesis holds for the case of k + 1. If CBJ finds out that instantiation xjk a is
inconsistent with the assignments of some past variables which are added to the conict
set of xjk , BT is also able to find out the inconsistency because the conict set of xjk is
subsumed by fxj1 ; : : :; xjk,1 g. Thus, the variable ordering for BT is well-defined.
For soluble CSPs, we further distinguish the problem between finding one solution and
finding all solutions.

Lemma 2 Given a CSP and a variable ordering for CBJ to find the first solution, there

exists a variable ordering for BT such that BT never visits more nodes than CBJ to find
the first solution.

Proof Without loss of generality, let fx1 a1; : : :; xn an g be the first solution found. A

variable ordering for BT can be constructed in the following way. The first variable chosen
for BT is x1 as it is the first variable in the path from the root to the solution in the CBJ
backtrack tree. Because we assume a static value ordering in the backtracking search, all
values in the domain of x1 that precede value a1 must be rejected by CBJ and BT before
value a1 is used to instantiate x1 . Furthermore, because fx1 a1 ; : : :; xn an g is the
first solution encountered by CBJ under the above variable ordering and value ordering,
the instantiation of x1 with a value preceding a1 leads to an insoluble subproblem and
eventually CBJ backjumps from a deeper variable to x1 to revoke that assignment. Note
that x1 cannot be skipped by a backjump from a deeper variable because x1 is on the first
level of the search tree and there is a solution for the CSP. Assigning x1 with each of the
values that precede a1 in its domain leads to insoluble subproblems and the instantiation
order for BT can be arranged as in Lemma 1. Whenever xk is instantiated with value ak ,
xk+1 is chosen to be the next variable, as it follows xk in the path from the root to the
solution in the CBJ backtrack tree. Again, all values in the domain of xk+1 that precede
ak+1 in the value ordering must be rejected by CBJ and BT before ak+1 is assigned to
xk+1. The instantiation of xk+1 with each of these values leads to an insoluble subproblem
and eventually CBJ backjumps from a deeper variable to xk+1 . Similarly, xk+1 cannot
be skipped by a backjump from a deeper variable because otherwise at least one of the
assignments to x1 ; : : :; xk must be changed so that fx1 a1 ; : : :; xn an g is not the
first solution encountered by CBJ. In each of these insoluble subproblems, the instantiation
order for BT can be arranged as in Lemma 1. Finally, xn is instantiated with an and BT
finds the solution.
When CBJ is used to find all solutions, special steps must be taken to handle the conict sets. The problem here is that the conict sets of CBJ are meant to indicate which
instantiations are responsible for some previously discovered inconsistency. However, after
a solution is found, conict sets cannot always be interpreted in this way. It is the search
for other solutions, rather than an inconsistency, that causes the algorithm to backtrack.
We need to differentiate between two causes of CBJ backtracks: (1) detecting an inconsistency, and (2) searching for other solutions. In the latter case, the backtrack must be
always chronological; that is, to the immediately preceding variable. A simple solution is to
remember the number of solutions found so far when a variable is chosen to be instantiated,
59

fiChen & van Beek

and later when a dead-end state is encountered at this level, we compare the recorded number with the current number of solutions. A difference indicates that some solutions have
been found in this interval of search, and forces the algorithm to backtrack chronologically.
Otherwise the algorithm performs a normal backjumping by analyzing the conict set of
the current variable.

Lemma 3 Given a CSP and a variable ordering for CBJ to find all solutions, there exists
a variable ordering for BT such that BT never visits more nodes than CBJ to find all
solutions.
Proof Let the first solution found by CBJ be fx1

a1; : : :; xn an g in the order of
x1 ; : : :; xn. We first construct the variable ordering for BT as it is applied to find the first

solution. However, because BT follows a strict chronological backtracking, it will inevitably
visit all the nodes fx1 a1 ; : : :; xj ,1 aj ,1 ; xj a0j g, where 1  j  n and a0j comes
after aj in the domain of xj . If CBJ skips any of these nodes, for example, from a deeper
level variable xh to xj ,1 , while the instantiations of x1 ; : : :; xj have not been changed, BT
will possibly visit more nodes than CBJ. We will show this cannot happen by induction
on the distance between the current level j and the deepest level n. After CBJ has found
the solution at level n, it will try other values for xn and eventually backtrack to xn,1 . So
the nodes at level n cannot be skipped. Suppose it is true for the case of level j + 1 and
now we consider the case of level j . Because xj aj was not skipped in the backjumping,
if aj is the last value in its domain, CBJ will backtrack to xj ,1 because the number of
solutions has been changed. So it is true for the case of j . Otherwise CBJ will change
the instantiation of xj to the next value in its domain. Let the current partial solution be
t = fx1 a1; : : :; xj,1 aj,1; xj a0j g. If the subtree rooted by t contains solutions,
from the inductive hypothesis, CBJ will not skip this node because it is on level j . If
the subtree rooted by t contains no solution, there exists a backjump from a deeper level
variable xh to escape this subtree. Could it jump beyond xj such that t is skipped? In that
case, the conict set of xh is subsumed in fx1 ; : : :; xj ,1 g. From the definition of conict
set, we know that the current instantiations of the variables in the conict set cannot lead
to a solution. However the current instantiations of fx1 ; : : :; xj ,1 g do lead to a solution,
fx1 a1; : : :; xn ang. That is a contradiction. So the conict set of xh must contain
xj and thus the node t at level j cannot be skipped. After all the values in the domain
of xj have been tried, CBJ will chronologically backtrack to xj ,1 because the number of
solutions has changed. Thus, xj ,1 aj ,1 will not be skipped. The hypothesis is true for
the case of any level j . Then we construct the variable ordering for BT in the following way:
If the current partial solution t = fx1 a1 ; : : :; xj ,1 aj ,1 ; xj a0j g cannot be extended
to a solution, we construct a variable ordering for the insoluble subproblem. If t can be
extended to a solution, we construct a variable ordering for BT as the case of finding the
first solution in this subproblem, and recursively apply the above steps until a backjump
to level xj changes the instantiation xj a0j . Under the above variable ordering, BT will
never visit more nodes than CBJ.

Theorem 4 Given a CSP and a variable ordering for CBJ, there exists a variable ordering

for BT such that BT never visits more nodes than CBJ in solving the CSP.
60

fiConflict-Directed Backjumping Revisited

x1 + x2  x3
x1 + x3 > x5 + 1
x2 , x4  x5
x1 ; :: : ;x5 2 f0; 1; 2g

x1

0

CBJ backtrack tree

x2

2

x3

x3

x3
3

x4

x4

x5

x4

x5

x5
4

x5

x5

x4

x5

x4

x4
5

p

p p
x1

x2

0

p p p

BT backtrack tree
2

x3

x3

x3
3

x5

x5

x5

x5

x5

x5
4

x4

x4

x4
5

p

p p

p p p

Figure 1: An illustration of the variable ordering constructed for BT from a CBJ backtrack
tree (for the CSP shown upper left).

Proof Follows from Lemmas 1, 2, and 3.
Example 1 Figure 1 shows the BT backtrack tree based on the variable ordering constructed
from the execution of CBJ to solve a CSP under a (hypothetical) dynamic variable ordering.
The first solution found by CBJ is fx1 0; x2 0; x3 2; x5 0; x4 0g. Thus, BT
first instantiates x1 and x2 to 0. The node fx1 0; x2 0; x3 0g and fx1 0; x2
0; x2 1g in the CBJ backtrack tree lead to insoluble subproblems. The variable ordering
for BT at each of these nodes is constructed as in the case of insoluble CSPs. For example,
in the CBJ backtrack tree, the last backjump to revoke the node fx1 0; x2 0; x3 0g
61

fiChen & van Beek

is from x5 to x3 , so the next variable instantiated in BT at this node is x5 . Under such an
ordering, BT avoids instantiating x4 and visits fewer nodes than CBJ. Then BT instantiates
x3 to 2, x5 to 0, and x4 to 0, and finds the first solution.
We have shown that there exists a \perfect" variable ordering such that CBJ becomes
redundant. Of course, the \perfect" ordering would not be known a priori, and in practice,
the primary goal in designing variable ordering heuristics is not to simulate the execution of
CBJ, but to reduce the size of the overall backtrack tree. As an example, the popular failfirst heuristic selects as the next variable to be instantiated the variable with the minimal
remaining domain size (the size of the domain after removing values that are in conict
with past instantiations) as this can be shown to minimize the size of the overall tree under
certain assumptions. A secondary effect, however, is that variables that have conicts with
past instantiations are likely to be instantiated sooner, thus approximating the \perfect"
ordering and diminishing the effects of backjumping.

4. Maintaining Consistency and Backjumping

In this section, we present theoretical results that deepen our understanding of the relationship between the look-ahead technique of maintaining a level of local consistency during
the backtracking search and the look-back technique of CBJ.
In previous work, Kondrak and van Beek (1997) show that, given the same deterministic
static or dynamic variable ordering heuristic, CBJ never visits more nodes than BT and
FC-CBJ never visits more nodes than FC. Prosser (1993a) shows that the removal of an
inconsistent value from the domain of a variable can diminish the effects of CBJ and that
CBJ can visit fewer nodes than an algorithm that combines CBJ with the discovery and
removal of some inconsistent values. Previous empirical work shows that the number of
nodes that CBJ saves depends on the level of local consistency maintained (Bacchus & van
Run, 1995; Bessiere & Regin, 1996; Prosser, 1993b).
We extend the partial ordering of backtracking algorithms presented by Kondrak and
van Beek (1997) to include backtracking algorithms and their CBJ hybrids that maintain
levels of local consistency beyond forward checking, including the important algorithms that
maintain arc consistency. We show that CBJ and an algorithm that maintains strong kconsistency in the backtracking search are incomparable in that each can be exponentially
better than the other. This result is refined by using the concept of backjump level in
the execution of a backjumping algorithm and showing that an algorithm that maintains
strong k-consistency never visits more nodes than a backjumping algorithm that is allowed
to backjump at most k levels. Thus, as the level of local consistency that is maintained in
the backtracking search is increased, the less that backjumping will be an improvement.
In Section 4.1, we consider the backjumping algorithms and define the series of algorithms BJk . In Section 4.2, we consider the look-ahead algorithms that maintain a level of
local consistency and define the series of algorithms MCk . Finally, in Section 4.3, we consider the relationships between the backjumping and the look-ahead algorithms and their
hybrids. The reader who is not interested in the technical proofs of the results should jump
directly to this section.
62

fiConflict-Directed Backjumping Revisited

x1

x1 + x2  x3
x1 + x3 > x5 + 1
x2 , x4  x5
x1 ; : : : ; x5 2 f0; 1; 2g

0

x2

1

x3

d=3
x4

x5

3

x5
d=1

x4

2

d=2
d=1

4
5

p p

Figure 2: An illustration of backjump levels in a CBJ backtrack tree (for the CSP shown
upper right).

4.1 Backjump Level and BJk

To analyze the inuence of the level of consistency on the backjumping, we need the notion of
backjump level. Informally, the level of a backjump is the distance, measured in backjumps,
from the backjump destination to the \farthest" dead-end.

Definition 6 (backjump level, Kondrak & van Beek, 1997) The definition of back-

jump level is recursive:
1. A backjump from variable xi to variable xh is of level 1 if it is performed directly from a
dead-end state in which every value of xi fails a consistency check.
2. A backjump from variable xi to variable xh is of level d  2, if all backjumps performed
to variable xi are of level less than d, and at least one of them is of level d , 1.

Example 2 Figure 2 shows the backjump levels in an example CBJ backtrack tree. There is

a one-level backjump from x5 to x3 because every value in the domain of x5 fails a consistency
check. Then CBJ finds two solutions for the problem and thus it chronologically backtracks
from x4 to x5 , and later to x3. The backjumps are of level one and two respectively. At last
there is a three-level backjump from x3 to x2 .
By classifying the backjumps performed by a backjumping algorithm into different levels,
we can now weaken CBJ into a series of backjumping algorithms which perform limited
levels of backjumps. BJk is a backjumping algorithm which is allowed to perform at most
k-level backjumps and it chronologically backtracks when a j -level backjump for j > k is
encountered3 . BJn is equivalent to CBJ, which performs unlimited backjumps, and BJ1 is
3. BJk is only of theoretical interest since in practice one would use CBJ rather than artificially prevent
backjumping; i.e., one has to actually add code to prevent backjumping.

63

fiChen & van Beek

equivalent to Gaschnig's (1978) BJ, which only does the first level backjumps or backjumps
from dead-ends.
One may immediately conclude that BJk+1 is always better than BJk because it does one
more level of backjumps. However, to be more precise, we need to justify that a situation
where BJk may skip a node visited by BJk+1 does not exist. Similar to a result by Kondrak
and van Beek (Theorem 11, 1997), we can show that:

Theorem 5 BJk visits all the nodes that BJk+1 visits.
4.2 Maintaining Strong k-consistency (MCk )

Although backtracking algorithms that maintain arc consistency (or a truncated form of arc
consistency called forward checking) during the search have been well-studied, a backtracking algorithm that maintains strong k-consistency (MCk ) has never been fully addressed in
the literature. In order to study the relationship between BJk and MCk , we need to specify
precisely the MCk algorithms.
A generic scheme to maintain a level of local consistency in a backtracking search is to
perform at each node in the search tree one full cycle of consistency achievement. A consistency achievement algorithm is applied to the CSP which is induced by the current partial
solution. If, as a result, the induced CSP becomes empty after applying the consistency
algorithm, the instantiation of the current variable is a dead-end and should be rejected.
If the resulting CSP is not empty, the instantiation of the current variable is accepted and
the search continues to the next level.
The simplest form of an induced CSP is to restrict the domains of the instantiated
variables to have only one value and leave the set of constraints unchanged. This idea can
be traced back to Gaschnig's (1978) implementation of MAC, referred to as DEEB; i.e.,
Domain Element Elimination with Backtracking. However, in order to establish a relation
between BJk and MCk , we need a more restricted definition of the induced CSP, where the
constraints in the induced CSP are the selections and projections of the constraints in the
original CSP with respect to a partial solution.

Definition 7 (induced CSP) Given a consistent partial solution t of a CSP P , the CSP
induced by t, denoted by P jt , has all the variables in P except those instantiated by t,
the domain of each variable is the same as in P , and for each constraint C in P where
vars(C ) 6 vars(t), there is a constraint C 0 = vars(C),vars(t) (t[vars(C )\vars(t)](C )) in P jt.
Example 3 Consider the graph coloring problem and the corresponding CSP shown in
Figure 3. The original CSP has four variables, x1 ; : : :; x4, where x1 ; x2; x3 2 fr; g; bg and
x4 2 frg, and five binary constraints, x1 =
6 x2, x1 =6 x3, x2 =6 x3, x2 =6 x4 and x3 =6 x4.
Given a partial solution t = fx1 g; x2 bg, the CSP induced by t, P jt , has two variables,
x3 and x4 , and the unary and binary constraints shown in Figure 4.
The maintaining strong k-consistency algorithm (MCk ) at each node in the backtrack
tree applies a strong k-consistency achievement algorithm to the CSP induced by the

current partial solution. Under such an architecture, FC can be viewed as maintaining
one-consistency, and for binary CSPs, MAC can be viewed as maintaining strong twoconsistency.
64

fiConflict-Directed Backjumping Revisited

An algorithm enforcing strong k-consistency on a CSP instance should detect and remove
all those inconsistencies t = fx1 a1 ; : : :; xj aj ,1 g where 1  j  k and t is consistent
but cannot be consistently extended to some j th variable xj . To remove an inconsistency,
we make it inconsistent in the resulting CSP by removing values from domains, removing
inconsistent tuples from existing constraints, or adding new constraints to the CSP.
We use the concept of a k-proof-tree in characterizing the tuples that are removed by a
strong k-consistency achievement algorithm.

Definition 8 (k-proof-tree) A k-proof-tree for a partial solution t over at most k vari-

ables in a CSP is a tree in which each node is associated with a partial solution over at most
k variables in the CSP, where (1) the root of the k-proof-tree is associated with t, and (2)
each leaf node of the k-proof-tree is inconsistent in the CSP, and (3) each non-leaf node s
of the k-proof-tree is consistent in the CSP, and the children of s at the next level are nodes
s0 [ fx a1g; : : :; s0 [ fx alg such that s0  s, x 62 vars(s), and dom(x) = fa1 ; : : :; al g.

Example 4 Figure 3 shows a three-proof-tree (more than one is possible) for t = fx1 gg

in the given graph coloring problem. Each non-leaf node, including the root t, is consistent,
and each leaf node is inconsistent in the CSP. Since we have constructed a three-prooftree for the tuple t it cannot be part of a solution to the CSP and a strong 3-consistency
achievement algorithm would remove it.
In general, if a k-proof-tree for an inconsistency in a CSP can be constructed, an algorithm achieving strong k-consistency would deduce and remove the inconsistency. After
applying a strong k-consistency achievement algorithm on the CSP, if all the children of
a node in the k-proof-tree are inconsistent in the resulting CSP, that node is also inconsistent in the resulting CSP because one of its subtuples cannot be consistently extended
to an additional variable. Because all the leaf nodes in the k-proof-tree are inconsistent in
the original CSP, in a bottom-up manner the inconsistency of the root of the tree can be
deduced and removed from the resulting CSP. As a special case, if a k-proof-tree for the
empty inconsistency in a CSP can be constructed, the CSP will be empty after enforcing
strong k-consistency since every way to extend a variable has been shown to lead to an
inconsistency (and therefore, each value would be removed from the domain resulting in the
empty domain). On the other hand, after a CSP has been made strongly k-consistent, if a
partial solution t over at most k variables is inconsistent in the resulting CSP, a k-proof-tree
for t in the original CSP can be constructed. If t is inconsistent in the original CSP, the
k-proof-tree contains the single node t. Otherwise, t or a subtuple t0 of t cannot be extended
to an additional variable x; i.e., all the partial solutions t0 [ fx a1 g; : : :; t0 [ fx al g,
where dom(x) = fa1; : : :; alg, are inconsistent in the resulting CSP. Then we can construct
the k-proof-tree recursively for each of those inconsistencies. As a special case, if a CSP
is empty after enforcing strong k-consistency, a k-proof-tree for the empty inconsistency in
the original CSP can be constructed.
The following lemmas (Lemma 6 to Lemma 8) reveal some basic properties about induced CSPs and strong k-consistency enforcement on induced CSPs, which are used in the
proofs of Theorem 10 and Theorem 14.

65

fiChen & van Beek

x2

6=
x1

r;g; b

6=

6=

r;g; b

6=

r

x4

x1

g

x1
x2

g
g

x1 ;x2 ; x3 2 fr; g;bg; x4 2 frg
C (x1 ;x2 ) : x1 6= x2
C (x1 ;x3 ) : x1 6= x3
C (x2 ;x3 ) : x2 6= x3
C (x2 ;x4 ) : x2 6= x4
C (x3 ;x4 ) : x3 6= x4

6=

r;g; b
x3

x1
x2

g
r

x2
x4

r
r

x1
x2
x3

g
b
r

x3
x4

r
r

x1
x2

g
b

x1
x2
x3

g
b
g

x1
x2
x3

g
b
b

Figure 3: A three-proof-tree for fx1 g g in the graph coloring problem. All leaf nodes in
the proof-tree are inconsistent in the CSP.

Lemma 6 Given a CSP P and two partial solutions t and t0 of P , if t  t0, then P jt0 =
(P jt )jt0,t .
Proof Clearly P jt0 and (P jt)jt0,t have the same set of variables and the same set of domains.

Because vars(C ),vars(t0 ) t0 C = vars(C ),vars(t0 ) t0 ,t (vars(C ),vars(t) t C ), for each constraint
C in P , the same selection and projection are made in P jt0 and (P jt )jt0,t. Therefore, P jt0
and (P jt )jt0,t have the same set of constraints.

Lemma 7 Given a CSP P and a consistent partial solution t of P , if (i) P is empty after
achieving strong k-consistency, or (ii) there exists a variable x 2 vars(t) such that the value
t[x] is removed from the domain of x when achieving strong k-consistency on P , then P jt
is empty after achieving strong k-consistency,

Proof We first show that, given a consistent partial solution t of a CSP P , and a k-proof-

tree T for an inconsistency s in P , there is a corresponding well-defined k-proof-tree Tt for
the inconsistency s0 = s[vars(s) , vars(t)], in the induced CSP P jt , provided s does not
66

fiConflict-Directed Backjumping Revisited

x3 2 fr; g; bg; x4 2 frg
C (x3 ) : f(r); (b)g
C (x3 ) : f(r); (g)g
C (x4 ) : f(r)g
C (x3 ; x4 ) : x3 6= x4


x3

r

x3
x4

r
r

x3

g

x3

b

Figure 4: Proof-tree for the empty inconsistency in the CSP P jt induced by t = fx1
g; x2 bg constructed from the proof-tree for fx1 g g in the CSP P shown in
Figure 3.
contain any assignments that are inconsistent with the assignments in t. Tt is constructed
from T in three steps (see Figure 4 for an example): (Step 1) Remove from T all nodes and
their descendants which contain assignments that are inconsistent with the assignments in
t. (Step 2) Replace each remaining node t0 in T with the node t00 = t0 [vars(t0) , vars(t)];
i.e., remove those variables which occur in t and thus do not occur in P jt . If t0 is not a
leaf node in T , then by definition t0 is consistent in P . It is possible that the corresponding
node t00 in Tt is inconsistent in P jt . Should this be the case, we make t00 into a leaf node by
removing all of its descendants. If t0 is a leaf node in T , then by definition t0 is inconsistent
in P ; i.e., there exists a constraint C in P such that t0 does not satisfy C . It must be
the case that vars(C ) 6 vars(t) (since vars(C )  vars(t) contradicts the fact that t0 is
inconsistent with C and t is consistent and therefore consistent with C , but t0 and t agree on
their assignments by Step 1). Hence, there is a corresponding constraint C 0 in P jt which is
the selection and projection of C in P . Now, it is easy to verify that the corresponding node
t00 is also inconsistent with C 0 and is therefore a well-defined leaf node. (Step 3) Remove all
subsumed nodes from T , where node t2 is subsumed by node t1 if t2 is a (necessarily only)
child of t1 and vars(t2)  vars(t1). All children of a subsumed node t2 are made children
of the parent of t2 .
Now, suppose P is empty after achieving strong k-consistency. Then there is a k-prooftree for the empty inconsistency in P and we can construct a k-proof-tree for the empty
inconsistency in P jt . Therefore, P jt is empty after achieving strong k-consistency. Suppose
there exists a variable x 2 vars(t), such that the value t[x] is removed from the domain of
x when achieving strong k-consistency on P . Then there is a k-proof-tree for fx t[x]g in
P and we can construct a k-proof-tree for the empty inconsistency in P jt. Therefore P jt is
empty after achieving strong k-consistency.

67

fiChen & van Beek

Lemma 8 Given a CSP P and an assignment fx ag, a 2 dom(x), if the induced CSP
P jfx ag is empty after achieving strong (k , 1)-consistency, then the value a is removed
from the domain of x when achieving strong k-consistency on P .

Proof Suppose P jfx ag is empty after achieving strong (k , 1)-consistency. Thus, there is
a (k , 1)-proof-tree for the empty inconsistency in P jfx ag . We now convert the (k , 1)proof-tree to a k-proof-tree for fx ag in P . Each node t in the original (k , 1)-proof-tree
is replaced by t [ fx ag. Thus, the root of the tree becomes fx ag. Furthermore,
if t is not a leaf node in the original (k , 1)-proof-tree; i.e., t is consistent in P jfx ag , it
is easy to verify that t [ fx ag is consistent in P . If t is a leaf node in the original
(k , 1)-proof-tree; i.e., t is inconsistent in P jfx ag, there is a constraint C 0 in P jfx ag such

that t does not satisfy C 0. Let C 0 be the selection and projection of the constraint C in P .
Thus, t [ fx ag does not satisfy the constraint C in P and is therefore inconsistent in P .
Hence, we have constructed a k-proof-tree for fx ag in P and thus a would be removed
from the domain of x when achieving strong k-consistency on P .

MCk extends the current node if the CSP induced by the current partial solution is not
empty after achieving strong k-consistency. The node is thus called a k-consistent node.

Definition 9 (k-consistent node) A node t in the search tree is a k-consistent node if
the CSP induced by t is not empty after enforcing strong k-consistency. A node which is
not k-consistent is called k-inconsistent.

Lemma 9 If node t is k-consistent, its ancestors are also k-consistent.
Proof Let t0 be one of t's ancestors. Because t0  t, from Lemma 6, P jt = (P jt0 )jt,t0 . Thus,
P jt is an induced subproblem of P jt0 . From Lemma 7, if P jt is not empty after achieving
strong k-consistency, P jt0 is not empty either after achieving strong k-consistency. Thus, t0
is k-consistent.

The following theorem applies to the case of finding all solutions.

Theorem 10 If MCk visits a node, then its parent is k-consistent. If a node is k-consistent,
then MCk visits the node.

Proof The first part is true because MCk would not branch on this node if its parent was
found k-inconsistent. We prove the second part by induction on the depth of the search tree.
The hypothesis is trivial for j = 1. Suppose it is true for j > 1 and we have a k-consistent
node t at level j + 1. Let the current variable be x. From Lemma 9, t's parent t0 at level
j is k-consistent. Thus, MCk will visit t0 . From Lemma 6, P jt = (P jt0 )jfx t[x]g. Because
(P jt0 )jfx t[x]g is not empty after achieving strong k-consistency, from Lemma 7, value t[x]
will not be removed from the domain of x when achieving strong k-consistency in P jt0 . As
a consequence, MCk will visit t.

A necessary and sucient condition for MCk to visit a node t is that t's parent is kconsistent and the value assigned to the current variable by t has not been removed from
its domain when enforcing strong k-consistency on t's parent.
68

fiConflict-Directed Backjumping Revisited

Theorem 11 Given a CSP and a variable ordering, MCk visits all the nodes that MCk+1

visits.

Proof Follows from Theorem 10 and Lemma 7.
4.3 Relationship Between BJk and MCk

Kondrak and van Beek (1997) have shown that for binary CSPs, BJ (BJ1 ) visits all the
nodes that FC (MC1) visits, and FC-CBJ (MC1-CBJ) and CBJ are incomparable. We
extend their partial ordering of backtracking algorithms to include the relationship between
MCk , BJk , and MCk -CBJ, 1  k  n. All of our results are for the case of general CSPs;
i.e., they are not restricted to binary CSPs.
We begin by characterizing an important property of the CBJ algorithm.

Lemma 12 If CBJ performs a one-level backjump from a deeper variable xi to a shallower
variable xh , the node th at the level of xh is one-inconsistent.

Proof Let Si be the conict set of xi used in the backjumping in which xh is the deepest variable. We show that xi will experience a domain wipe out when enforcing oneconsistency on the induced CSP P jth [Si ] . Each node ti at the level of xi is a leaf node;
i.e., ti is inconsistent in P . Suppose ti does not satisfy constraint C where xi 2 vars(C )
and vars(C )  Si [ fxi g. The selection of C in P jth [Si ] , which constrains only one variable
fxig, should prohibit value ti [xi] of xi. Thus, xi will experience a domain wipe out when
enforcing one-consistency on P jth [Si ] . Note that P jth is an induced subproblem of P jth [Si ] .
From Lemma 7, P jth is empty after enforcing one-consistency. Thus, th at the level of xh
is one-inconsistent.
Lemma 13 If CBJ performs a k-level backjump from a deeper variable xi to a shallower
variable xh , the current node th at the level of xh is k-inconsistent.
Proof Let Si be the current conict set of xi in which xh is the deepest variable. We show
that if there is a k-level backjump from xi to xh , then P jth [Si ] is empty after enforcing strong

k-consistency and thus th is k-inconsistent. The proof is by induction on k. k = 1 is true
from Lemma 12. Suppose the hypothesis is true for the case of k , 1 but it is not true for
the case of k; i.e., there is a k-level backjump from xi to xh , but the induced CSP P jth [Si ]
is not empty after enforcing strong k-consistency. So there is at least one value a left in the
domain of xi after enforcing strong k-consistency on P jth [Si ] . We know that the node ti at
the level of xi instantiating xi with a is either incompatible with th (i.e., it is a leaf node)
or is l-level backjumped from some deeper variable xj , for some 1  l < k (see Figure 5).
However, ti cannot be a leaf node as otherwise a would be removed from the domain of xi
when enforcing strong k-consistency. Let Sj be the conict set of xj . From the hypothesis,
the induced CSP P jti [Sj ] is empty after achieving strong l-consistency. Because value a is
not removed from the resulting CSP, from Lemma 8, the induced CSP P jth [Si ][fxi ag is not
empty after achieving strong (k , 1)-consistency. Because ti [Sj ]  th [Si ] [ fxi ag, the
induced CSP P jti [Sj ] is not empty after achieving strong (k , 1)-consistency. That leads to
a contradiction. Thus P jth [Si ] is empty after achieving strong k-consistency and th at the
level of xh is k-inconsistent.
69

fiChen & van Beek

th

xh
k-level backjumping

:::

ti

conict set Si xi

l-level backjumping, l < k

:::

conict set Sj xj

Figure 5: A scenario in the CBJ backtrack tree used in the proof of Lemma 13.

Theorem 14 Given a CSP and a variable ordering, BJk visits all the nodes that MCk
visits.

Proof The proof is by induction on the level of the search tree. If MCk visits a node at
level j in the search tree, BJk visits the same node. j = 1 is trivial. Suppose that it is true
for the case of j > 1 and we have a node t visited by MCk at level j + 1. We know both
MCk and BJk visit t's parent at level j . The only chance that t may be skipped by BJk is
that BJk backjumps from some deeper variable xi at level i to a shallower variable xh at
level h, such that h < j + 1 < i. Thus, the node at level h is k-inconsistent (by Lemma 13).
Since the node at level h is an ancestor of t and we know t's parent is k-consistent from
Lemma 9, the node at level h is k-consistent. That is the contradiction. Therefore, BJk
visits t at level j + 1.
MCk can be combined with backjumping, namely MCk -CBJ, provided the conict sets
are computed correctly after achieving strong k-consistency on the induced CSPs.

Theorem 15 Given a CSP and a variable ordering, MCk visits all the nodes that MCk-CBJ

visits.

Proof Because MCk-CBJ behaves exactly the same as MCk in the forward phase of a

backtracking search, it is easy to verify that MCk -CBJ visits a node t only if t's parent
is k-consistent and the value assigned to the current variable by t was not removed from
its domain when achieving strong k-consistency on t's parent. Therefore, MCk -CBJ never
visits more nodes than MCk does.
In Figure 6, we present a hierarchy in terms of the size of the backtrack tree for BJk ,
MCk , and MCk -CBJ. If there is a path from algorithm A to algorithm B in the figure,
we know that A never visits more nodes than B does. For example, MCk never visits
more nodes than BJj , for all j  k. Otherwise, there are instances to show A may be
exponentially better than B, and vice versa.
70

fiConflict-Directed Backjumping Revisited

BJn
(CBJ)

..
.

+1

BJk

BJk

MCn

MCn -CBJ

..
.

..
.

+1

MCk

+1 -CBJ

MCk

MCk

MCk -CBJ

..
.

..
.

..
.

BJ

1

MC

1

MC -CBJ

(BJ)

(FC)

(FC-CBJ)

1

Figure 6: A hierarchy for BJk , MCk , and MCk -CBJ in terms of the size of the backtrack
tree.
As the following example shows, for any fixed integer k < n, there exists a CSP instance
such that CBJ visits exponentially fewer nodes than an algorithm that maintains strong
k-consistency in the backtracking search4.

Example 5 Given a fixed integer k, we can construct a binary CSP with n + k +2 variables,
x1 ; : : :; xn,k+1; y1; : : :; yk+1 ; xn,k+2; : : :; xn+1, where dom(xi ) = f1; : : :; ng for 1  i  n +1
and dom(yj ) = f1; : : :; kg for 1  j  k + 1. The constraints are: xi =
6 xj , for i =6 j , and
yi =
6 yj , for i =6 j . The problem consists of two separate pigeon-hole subproblems, one over

variables x1; : : :; xn+1 and the other over variables y1 ; : : :; yk+1 , and is insoluble. As we can
see, the pigeon-hole problem is highly locally consistent. The first subproblem is strongly nconsistent and the second is strongly k-consistent. Under the above static variable ordering,
4. Independently, Bacchus and Grove (1999) present a similar example to show that given a fixed k, CBJ
may be exponentially better than an algorithm called MIkC , which essentially maintains k-consistency
in the backtracking search.

71

fiChen & van Beek

a backtracking algorithm maintaining strong k-consistency would not encounter a dead-end
until xn,k+1 is instantiated. Then it would find that the subproblem of xn,k+2 ; : : :; xn+1
is not strongly k-consistent. Thus, the algorithm will backtrack before it reaches the second
pigeon-hole subproblem. It will explore nk!! nodes at level n , k + 1 of the search tree and
thus take an exponential number of steps to find the problem is insoluble. CBJ does not
encounter a dead-end at the level of xn,k+1 and it continues to the second pigeon-hole
problem. Eventually it will find the second-pigeon hole problem is insoluble and backjump
to the root of the search tree. The total number of nodes explored is bounded by a constant,
O((k + 1)k), for a fixed k. Therefore, CBJ can be exponentially better than an algorithm
maintaining strong k-consistency.
Example 5 also shows that, although MCk visits fewer nodes than BJk by Theorem 14,
BJk+1 can be exponentially better than MCk . However, BJk+1 can be better than MCk
only if there is a (k + 1)-level backjump that is not also a chronological backtrack. To see
that this is true, suppose that on a particular instance all (k + 1)-level backjumps are also
chronological backtracks (i.e., the backjump is to the immediately preceding variable in the
variable ordering and only that single variable becomes uninstantiated and is removed from
the current partial solution). In this case, the freedom to backjump one additional level
rather than chronologically backtrack does not make a difference and BJk+1 is effectively
BJk and thus cannot be better than MCk . Thus, BJk+1 can be better than MCk only
if there is a (k + 1)-level non-chronological backjump. We note, however, that since the
number of backjumps of level k +1 is less than or equal to the number of backjumps of level
k, as k increases this gets more and more unlikely. Thus, as the level of local consistency
that is maintained in the backtracking search is increased, the less that backjumping will
be an improvement.
Consider Example 5 again. At each level of the backtrack tree for MCk , the instantiation
of each of the past variables removes one distinct value from the domain of the current
variable (recall that MCk never instantiates the variable y1 as it reaches a dead-end at
xn,k+1 ). If we were to maintain conict sets for the variables, the conict set for the current
variable would include all of its past variables and thus when a dead-end is encountered
by the algorithm, any backjump computed from the conict sets would also necessarily be
a chronologically backtrack. Thus, as this example shows, MCk -CBJ and MCk can visit
exactly the same nodes and consequently BJk+1 can be exponentially better than MCk CBJ. Furthermore, because MCk,1 -CBJ can reach the second pigeon-hole problem without
encountering a dead-end, it can finally retreat from the second pigeon-hole problem to the
root of the search tree by backjumps. Thus, MCk,1 -CBJ may be exponentially better
than MCk -CBJ. In particular, this shows the surprising result that MAC-CBJ can visit
exponentially more nodes than FC-CBJ.
Finally, as the following example shows, for any fixed integer k < n, there exists a CSP
instance such that an algorithm that maintains strong k-consistency in the backtracking
search visits exponentially fewer nodes than CBJ.

Example 6 Consider the CSP as defined in Example 5, but searched with the static variable
ordering y1 ; : : :; yk ; x1; : : :; xn+1 ; yk+1 .

72

fiConflict-Directed Backjumping Revisited

5. Empirical Evaluation of Adding CBJ to GAC

In this section, we report on experiments that examined the effect of adding CBJ to a
backtracking algorithm that maintains generalized arc consistency (GAC), an algorithm
that we refer to as GAC-CBJ. Previous work has shown the importance of algorithms that
maintain arc consistency (e.g., Sabin & Freuder, 1994; Bessiere & Regin, 1996). We show
that adding CBJ to a backtracking algorithm that maintains generalized arc consistency
can speed up the algorithm by several orders of magnitude on hard, structured problems.
Previous empirical studies of adding CBJ to a backtracking algorithm that maintains a
level of local consistency have led to mixed conclusions. Adding CBJ to forward checking,
a truncated form of arc consistency, has been shown to give improvements but not always
significant ones. Prosser (1993b) observes that with a static variable ordering, FC-CBJ is
about three times faster than FC on the Zebra problem. Smith and Grant (1995) observe
that with a dynamic variable ordering, adding CBJ to FC led to significant savings but
only on hard random problems that occur in the easy region. Bacchus and van Run (1995)
observe that with a dynamic variable ordering, adding CBJ to FC only led to at most a
5% improvement on the Zebra problem, n-Queens problems, and random binary problems.
Bayardo and Schrag (1996, 1997) show that adding CBJ to the well-known Davis-Putnam
algorithm, the SAT version of forward checking, can be a significant improvement on hard
random and real-world 3-SAT problems.
Adding CBJ to an algorithm that maintains full arc consistency has received less attention in the literature. In the one study that we are aware of, Bessiere and Regin (1996)
observe that adding CBJ to MAC (the binary version of GAC) actually slows down the
algorithm on random binary problems due to the overhead of maintaining the conict sets.
They conjecture that \when MAC and a good variable ordering heuristic are used, CBJ
becomes useless".
Our empirical results lead us to differ with Bessiere and Regin's conclusion about the
usefulness of adding CBJ to an algorithm that maintains full arc consistency. In our implementation we were able to significantly reduce the overhead of maintaining the conict sets
through the use of additional data structures5. On problems where adding CBJ does not
lead to many savings in nodes visited, our implementation of CBJ also does not degrade performance by any significant factor. We demonstrate the improvement by re-doing Bessiere
and Regin's (1996) experiments on random binary problems. We then show through experiments in two structured domains that GAC-CBJ can sometimes improve GAC by several
orders of magnitude on hard instances.
In our experiments, we ran both GAC and GAC-CBJ on each instance of a problem
and recorded the CPU times. Comparing CPU times is appropriate as the underlying code
for GAC and GAC-CBJ is identical, with GAC-CBJ containing only additional code to
maintain the conict sets and to determine how far to jump back. Two dynamic variable
orderings were used: the popular dom+deg heuristic which chooses the next variable with
the minimal domain size and breaks ties by choosing the variable with the maximum degree
(the number of the constraints that constrain that variable) and the dom/deg heuristic
proposed by Bessiere and Regin (1996) which chooses the next variable with the minimal
5. See the online appendix for the source code and a description of the key data structures in our implementations of GAC and GAC-CBJ.

73

fiChen & van Beek

value of the domain size divided by its degree. All experiments were run on 400 MHz
Pentium II's with 256 Megabytes of memory.

5.1 Random Problems

The run time performance of GAC and GAC-CBJ were compared on sets of randomly
generated binary CSPs. A set of random problems is defined by a 5-tuple (n; d; r; m; t),
where n is the number of the variables, d is the uniform domain size, r is the uniform arity
of the constraints, m is the number of randomly generated constraints, and t is the uniform
tightness or number of tuples in each constraint. In each case, the constraint tightness t
was chosen so that approximately half of the instances in the population were insoluble;
i.e., the instances were from the phase transition region.
Table 1: Effect of domain size on average time (seconds) to solve random instances from
(50; d; 2; 95; t). Each set contained 100 random instances. Both GAC-CBJ and
GAC used the dom/deg variable ordering.

d GAC-CBJ GAC
5
0:0027 0:0030
10
0:026
0:027
0:10
0:10
15
20
0:41
0:41
25
0:79
0:78
30
2:46
2:47
35
3:82
3:80
10:98
10:75
40

ratio
0:90
0:96
1:00
1:00
1:01
1:00
1:01
1:02

Bessiere and Regin (1996) examine the effect of domain size on the average time to
solve random instances from (50; d; 2; 95; t) (see Figure 5 (right) in Bessiere & Regin, 1996).
With their implementation of CBJ, adding CBJ steadily worsens performance as domain
size increases until at d = 40 MAC-CBJ is about 1.7 times slower than MAC alone. With our
implementation, the difference in performance between GAC-CBJ and GAC was negligible
on these problems (see Table 1).
The remaining sets of random problems that Bessiere and Regin used in their experiments to compare the performance of MAC-CBJ and MAC are now too simple to provide
a meaningful comparison as they can be solved in less than 0.01 seconds on a 400 MHz
Pentium II computer. Thus, we chose harder sets of random binary problems. On each
instance we ran both GAC and GAC-CBJ and recorded the CPU times. Here we report
the average ratio of the CPU times (GAC over GAC-CBJ). Each set contained 100 random
instances. On the first set of problems, (150; 5; 2; 750; 19), the average ratio for the dom+deg
variable ordering was 0.90 and the average ratio for the dom/deg variable ordering was 0.88.
On the second set of problems, (150; 5; 2; 1500; 21), the average ratios for both the dom+deg
74

fiConflict-Directed Backjumping Revisited

and dom/deg variable orderings was 0.93. In other words, on average GAC was a little over
10% faster than GAC-CBJ on these problems.

5.2 Planning Problems

Planning, where one is required to find a sequence of actions from an initial state to a goal
state, can be formulated as a CSP. In the formulation we used in our experiments, each
state is modeled by a collection of variables and the constraints enforce the assignments of
variables to represent a consistent state or a valid transition between states. (See Kautz &
Selman, 1992; van Beek & Chen, 1999 for more details on the formulation of planning as a
CSP.)
Table 2: Time (seconds) to solve instances of the grid planning problem. The absence of an
entry indicates that the problem was not solved within 72000 seconds (20 hours)
of CPU time.
dom+deg
GAC GAC-CBJ
0.66
0.68

1
2 762.47
3
.
4
.
.
5

dom/deg
GAC GAC-CBJ
1.58
0.86

33.33 3965.10
.

1753.13

.

.
.
.

321.17

.
.
.

In the experiments we used all 130 instances used in the First AI Planning Systems
Competition, June 6{9, 1998. The instances come from five different domains: gripper,
mystery, mprime, logistics, and grid. In the experiments we report, both GAC and GACCBJ were based on AC3 (Mackworth, 1977a) as this was found to give the best performance.
For the gripper, mystery, and mprime domains, each of the instances could be solved
in under 25 seconds by both GAC and GAC-CBJ. On these easy problems, the increased
overhead of CBJ rarely led to savings, and overall GAC was 10-15% faster than GAC-CBJ.
Table 2 shows the comparison between GAC and GAC-CBJ in solving the 5 instances
of the grid problems. GAC-CBJ showed improvement on the grid problems. For example,
it solved problem 4 in about half an hour, but GAC failed to find a solution in 20 hours.
Table 3 shows the comparison between GAC and GAC-CBJ in solving the 30 instances
of the logistics problem. On about one third of the instances, GAC-CBJ improved on GAC.
For example, on instances 18, 20 and 27, GAC-CBJ ran several orders of magnitude faster
than GAC, and on instance 15, GAC exhausted the 20 hours time limit but GAC-CBJ found
a solution within 3 minutes. GAC-CBJ and GAC performed similarly on easier instances
and sometimes GAC-CBJ was about 10% slower than GAC.

75

fiChen & van Beek

Table 3: Time (seconds) to solve instances of the logistics planning problem. The absence
of an entry indicates that the problem was not solved within 72000 seconds (20
hours) of CPU time.
dom+deg
GAC
GAC-CBJ
0.03
0.03
0.03
0.05

1
2
3
10.91
4
0.16
5
1.51
6
36.49
7
0.08
0.15
8
9
0.30
.
10
11
0.04
12
0.11
13
0.54
14
0.63
15
.
16
12.49
17
264.46
18 15382.82
1.29
19
20 6268.16
0.66
21
22
.
23
.
24
0.08
25
34.03
26
.
27 12239.26
28
.
29
.
30
.

0.86

dom/deg
GAC
GAC-CBJ
0.03
0.03
0.03
0.06

9.63

0.81

35.77

16.76

182.51
.
0.42
12.32
0.32 261.33
1165.54 15157.71

8540.58
0.41
0.32
1184.67

0.17
1.54

16.86

0.08
0.15
0.33
.
0.05
0.13
0.57
0.64

1.37

27.66

0.14
1.54
0.08
0.14
0.32
.
0.05
0.11
0.54
0.64

1.33

0.18
1.57
0.09
0.16
0.33
.
0.05
0.11
0.56
0.68

1.31

6125.87

28.55

47.06 12105.62

47.76

0.70
.
.
0.09
13.03
.
.
.
.

76

0.68
.
.
0.08
11.58
.
.
.
.

0.74
.
.
0.09
12.10
.
.
.
.

fiConflict-Directed Backjumping Revisited

5.3 Crossword Puzzle Problems

Crossword puzzle generation, where one is required to fill in a grid with words from a
dictionary, can be formulated as a CSP. In the formulation we used in our experiments, each
of the unknown words is represented by a variable which takes values from the dictionary.
Binary constraints enforce that intersecting words agree on their intersecting letter and
that a word from the dictionary appears at most once in a solution. Figure 7 shows an
example 5  5 crossword puzzle grid. A CSP model of this grid has 10 variables, 21 binary
\intersection" constraints, and 13 \not equals" constraints.
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Figure 7: A crossword puzzle.
In the experiments we used 50 grids and two dictionaries for a total of 100 instances of
the problem that ranged from easy to very hard. For the grids, we used 10 instances at each
of the following sizes: 55, 1515, 1919, 2121, and 2323. For the dictionaries we used
the UK dictionary, which collects about 220,000 words and in which the largest domain for a
word variable contains about 30,000 values, and the Linux dictionary, which collects 45,000
words and in which the largest domain for a word variable has about 5,000 values. In the
experiments we report, both GAC and GAC-CBJ were based on AC7 (Bessiere & Regin,
1997) as this was found to give the best performance (see Sillito, 2000 for a discussion of
integrating AC7 into backtracking search).
Figure 8 shows approximate cumulative frequency curves for the empirical results, where
we are plotting the ratio of the time taken to solve an instance by GAC over the time
taken to solve the instance by GAC-CBJ. Thus, for example, we can read from the curve
representing the dom+deg variable ordering that for approximately 85% of the tests adding
CBJ had little effect and that for the remaining 15% of the tests it led to orders of magnitude
improvements. We can also read from the curves the 0, 10, . . . , 100 percentiles of the data
sets (where the value of the median is the 50th percentile or the value of the 50th test). The
crossover point, where GAC-CBJ starts to perform as well as or better than GAC occurs
around the 35th percentile. Tables 4 and 5 examine the data more closely by showing the
77

fiChen & van Beek

1000

ratio (GAC / GAC-CBJ)

dom+degree
dom/degree

100

10

1

0.1
10

20

30

40

50
test

60

70

80

90

100

Figure 8: Effect on execution time of GAC of adding conict-directed backjumping (GACCBJ). A curve represents 100 tests on instances of the crossword puzzle problem
where the tests are ordered by the ratio of time taken to solve the instance by
GAC over time taken to solve the instance by GAC-CBJ.
actual times to solve the instances where GAC performed best and the instances where
GAC-CBJ performed best.
Table 4: GAC versus GAC-CBJ on instances of the crossword puzzle problem. The ten
best improvements in time (seconds) of GAC over GAC-CBJ to solve an instance
are presented.
dom+deg
rank GAC GAC-CBJ
1
1.21
1.35
2
1.10
1.20
3
6.12
6.53
4
0.78
0.81
5 110.23
114.52
68.67
71.28
6
7
47.16
48.42
8
32.69
33.63
9
25.17
26.08
21.37
10 20.73
78

dom/deg
GAC GAC-CBJ
1.11
1.23
0.95
1.02
1.16
1.24
56.66
60.36
1.30
1.37
4.86
5.11
0.22
0.23
14.23
14.76
74.38
77.52
7.43
7.67

fiConflict-Directed Backjumping Revisited

Table 5: GAC versus GAC-CBJ on instances of the crossword puzzle problem. The ten
best improvements in time (seconds) of GAC-CBJ over GAC to solve an instance
are presented. The absence of an entry indicates that the problem was not solved
within 36000 seconds (10 hours) of CPU time.
dom+deg
dom/deg
rank GAC GAC-CBJ GAC GAC-CBJ
1
.
37.85
.
54.60
2
.
41.43 10311.32
33.43
3
.
67.07
.
225.92
4
.
82.58
.
244.81
.
276.00
.
308.04
5
.
542.80
.
374.72
6
7
.
939.71
.
832.68
8 2716.86
115.87
.
1486.43
9
390.91
34.90
.
1890.24
10
.
3336.37
.
3411.83

In summary, on some of the smaller, easier crossword puzzle instances GAC was slightly
faster than GAC-CBJ, on many of the puzzles there was no noticeable difference, and on
some of the larger, harder puzzles GAC-CBJ was orders of magnitude faster than GAC.

6. Conclusion

In this paper, we presented three main results. First, we showed that the choice of dynamic
variable ordering heuristic can weaken the effects of the backjumping technique. Second,
we showed that as the level of local consistency that is maintained in the backtracking
search is increased, the less that backjumping will be an improvement. Together these
results partially explain why a backtracking algorithm doing more in the look-ahead phase
cannot benefit more from the backjumping look-back scheme and they extend the partial
ordering of backtracking algorithms presented by Kondrak and van Beek (1997) to include
backtracking algorithms and their CBJ hybrids that maintain levels of local consistency
beyond forward checking. Third, and finally, we showed that adding CBJ to a backtracking
algorithm that maintains generalized arc consistency can (still) speed up the algorithm by
several orders of magnitude on hard, structured problems. Throughout the paper, we did
not restrict ourselves to binary CSPs.

Acknowledgements
The authors wish to thank the referees for their careful reading of a previous version of
the paper and their helpful comments. The financial support of the Canadian Government
through their NSERC program is gratefully acknowledged.
79

fiChen & van Beek

References

Bacchus, F., & Grove, A. (1999). Looking forward in constraint satisfaction algorithms.
Unpublished manuscript.
Bacchus, F., & van Run, P. (1995). Dynamic variable ordering in CSPs. In Proceedings of the
First International Conference on Principles and Practice of Constraint Programming,
pp. 258{275, Cassis, France. Available as: Springer Lecture Notes in Computer Science
976.
Bayardo Jr., R. J., & Schrag, R. (1996). Using CSP look-back techniques to solve exceptionally hard SAT instances. In Proceedings of the Second International Conference
on Principles and Practice of Constraint Programming, pp. 46{60, Cambridge, Mass.
Available as: Springer Lecture Notes in Computer Science 1118.
Bayardo Jr, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques to solve realworld SAT instances. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 203{208, Providence, RI.
Bessiere, C., & Regin, J.-C. (1996). MAC and combined heuristics: Two reasons to forsake
FC (and CBJ?) on hard problems. In Proceedings of the Second International Conference on Principles and Practice of Constraint Programming, pp. 61{75, Cambridge,
Mass.
Bessiere, C., & Regin, J.-C. (1997). Arc consistency for general constraint networks: Preliminary results. In Proceedings of the Sixteenth International Joint Conference on
Artificial Intelligence, pp. 398{404, Nagoya, Japan.
Bruynooghe, M. (1981). Solving combinatorial search problems by intelligent backtracking.
Information Processing Letters, 12, 36{39.
Chen, X. (2000). A Theoretical Comparison of Selected CSP Solving and Modeling Techniques. Ph.D. thesis, University of Alberta.
Dechter, R. (1990). Enhancement schemes for constraint processing: Backjumping, learning,
and cutset decomposition. Artificial Intelligence, 41, 273{312.
Dechter, R. (1992). Constraint networks. In Shapiro, S. C. (Ed.), Encyclopedia of Artificial
Intelligence, 2nd Edition, pp. 276{285. John Wiley & Sons.
Freuder, E. C. (1978). Synthesizing constraint expressions. Comm. ACM, 21, 958{966.
Frost, D., & Dechter, R. (1994). Dead-end driven learning. In Proceedings of the Twelfth
National Conference on Artificial Intelligence, pp. 294{300, Seattle, Wash.
Gaschnig, J. (1978). Experimental case studies of backtrack vs. Waltz-type vs. new algorithms for satisficing assignment problems. In Proceedings of the Second Canadian
Conference on Artificial Intelligence, pp. 268{277, Toronto, Ont.
Haralick, R. M., & Elliott, G. L. (1980). Increasing tree search eciency for constraint
satisfaction problems. Artificial Intelligence, 14, 263{313.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Proceedings of the 10th
European Conference on Artificial Intelligence, pp. 359{363, Vienna.
80

fiConflict-Directed Backjumping Revisited

Kondrak, G., & van Beek, P. (1997). A theoretical evaluation of selected backtracking
algorithms. Artificial Intelligence, 89, 365{387.
Mackworth, A. K. (1977a). Consistency in networks of relations. Artificial Intelligence, 8,
99{118.
Mackworth, A. K. (1977b). On reading sketch maps. In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, pp. 598{606, Cambridge, Mass.
McGregor, J. J. (1979). Relational consistency algorithms and their application in finding
subgraph and graph isomorphisms. Inform. Sci., 19, 229{250.
Montanari, U. (1974). Networks of constraints: Fundamental properties and applications to
picture processing. Inform. Sci., 7, 95{132.
Nadel, B. A. (1989). Constraint satisfaction algorithms. Computational Intelligence, 5,
188{224.
Prosser, P. (1993a). Domain filtering can degrade intelligent backtracking search. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence,
pp. 262{267, Chambery, France.
Prosser, P. (1993b). Hybrid algorithms for the constraint satisfaction problem. Computational Intelligence, 9, 268{299.
Prosser, P. (1995). MAC-CBJ: Maintaining arc consistency with conict-directed backjumping. Research report 177, University of Strathclyde.
Sabin, D., & Freuder, E. C. (1994). Contradicting conventional wisdom in constraint satisfaction. In Proceedings of the 11th European Conference on Artificial Intelligence,
pp. 125{129, Amsterdam.
Schiex, T., & Verfaillie, G. (1994). Nogood recording for static and dynamic constraint
satisfaction problems. International Journal on Artificial Intelligence Tools, 3, 1{15.
Sillito, J. (2000). Improving and Estimating the Cost of Backtracking Algorithms for CSPs..
MSc thesis, University of Alberta, 2000.
Smith, B. M., & Grant, S. A. (1995). Sparse constraint graphs and exceptionally hard problems. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 646{651, Montreal.
van Beek, P., & Chen, X. (1999). CPlan: A constraint programming approach to planning.
In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pp.
585{590, Orlando, Florida.

81

fiJournal of Artificial Intelligence Research 14 (2001) 137-166

Submitted 9/00; published 4/01

Reasoning within Fuzzy Description Logics
Umberto Straccia

straccia@iei.pi.cnr.it

I.E.I - C.N.R., Via G. Moruzzi, 1
I-56124 Pisa (PI), ITALY

Abstract
Description Logics (DLs) are suitable, well-known, logics for managing structured
knowledge. They allow reasoning about individuals and well defined concepts, i.e. set
of individuals with common properties. The experience in using DLs in applications has
shown that in many cases we would like to extend their capabilities. In particular, their
use in the context of Multimedia Information Retrieval (MIR) leads to the convincement
that such DLs should allow the treatment of the inherent imprecision in multimedia object
content representation and retrieval.
In this paper we will present a fuzzy extension of ALC, combining Zadehs fuzzy logic
with a classical DL. In particular, concepts becomes fuzzy and, thus, reasoning about
imprecise concepts is supported. We will define its syntax, its semantics, describe its
properties and present a constraint propagation calculus for reasoning in it.

1. Introduction
The representation of uncertainty and imprecision has received a considerable attention in
the Artificial Intelligence community in an attempt to extend existing knowledge representation systems to deal with the imperfect nature of real world information (which is likely
the rule rather than an exception). An impressive work has been carried out in the last
decades, resulting in a number of concepts being investigated, a number of problems being
identified and a number of solutions being developed (Bacchus, 1990; Dubois & Prade, 1996;
Kruse, Schwecke, & Heinsohn, 1991; Pearl, 1988).
For most knowledge representation formalisms, First-Order Logic (FOL) has been the
basis: its basic units individuals, their properties, and the relationship between them
naturally capture the way in which people encode their knowledge. Unfortunately, it is
severely limited both (i) by its ability to represent our uncertainty about the world due
to lack of knowledge about the real world a fact can only estimated to be true to e.g.
a probability degree; and (ii) by its ability to represent inherently imprecise knowledge
indeed, there are concepts, like hot, for which no exact definition exists and, thus, a fact
like 35 Celsius is hot, rather being true or false, has a truth-value in between true and
false.
In the last decade a substantial amount of work has been carried out in the context of
Description Logics (DLs).1 DLs are a logical reconstruction of the so-called frame-based
knowledge representation languages, with the aim of providing a simple well-established
Tarski-style declarative semantics to capture the meaning of the most popular features of
structured representation of knowledge. A main point is that DLs are considered as to be
1. Description Logics have also been referred to as Terminological Logics, Concept Logics, KL-ONE-like
languages. The web page of the description logic community is found at address http://dl.kr.org/dl.
c
2001
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiStraccia

attractive logics in knowledge based applications as they are a good compromise between
expressive power and computational complexity.
Nowadays, a whole family of knowledge representation systems has been build using DLs,
which differ with respect to their expressiveness, their complexity and the completeness of
their algorithms, and they have been used for building a variety of applications (Peltason,
1991; Brachman, 1992; Baader & Hollunder, 1991a; Horrocks, 1998).
Experience in using DLs in applications has also shown that in many cases we would
like to extend the representational and reasoning capabilities of them. In particular, the use
of DLs in the context of Multimedia Information Retrieval (MIR) points out the necessity
of extending DLs with capabilities which allow the treatment of the inherent imprecision
in multimedia object representation and retrieval (Meghini & Straccia, 1996; Meghini, Sebastiani, & Straccia, 1997, 1998). In fact, classical DLs are insufficient for describing real
multimedia retrieval situations, as the retrieval is usually not only a yes-no question: (i) the
representations of multimedia objects content and queries which the system (and the logic)
have access to are inherently imperfect; and (ii) the relevance of a multimedia object to a
query can thus be established only up to a limited degree. Because of this, we need a logic
in which, rather than deciding tout court whether a multimedia object satisfies a query or
not, we are able to rank the retrieved objects according to how strongly the system believes
in their relevance to a query.
To this end, we will extend DLs with fuzzy capabilities. The choice of fuzzy set theory
as a way of endowing a DL with the capability to deal with imprecision is not uncommon
(da Silva, Pereira, & Netto, 1994; Tresp & Molitor, 1998; Yen, 1991) and can be motivated
 from a semantics point of view, as fuzzy logics capture the notion of imprecise concept,
i.e. a concept for which a clear and precise definition is not possible. Fuzzy concepts
play a key role in e.g. content descriptions of multimedia objects (most of humans
concepts are imprecise). For instance, in the context of images, the (semantic) content
of an image region r may be described by means of a fuzzy statement like r is about
a Ferrari and establish that this sentence has truth-value 0.8, i.e. r is likely about a
Ferrari;
 from a proof theoretical point of view, as there exist well-known techniques for reasoning in fuzzy logics (Chen & Kundu, 1996; Lee, 1972; Xiachun, Yunfei, & Xuhua,
1995).
In the following we will present a quite general fuzzy DL, in the sense that it is based on
the DL ALC, a significant and expressive representative of the various DLs. This allows
us to adapt it easily to the different DLs presented in the literature. Another important
point is that we will show that the additional expressive power has no impact from a
computational complexity point of view. This is certainly important as the nice tradeoff between computational complexity and expressive power of DLs contributes to their
popularity.
Note that our fuzzy extension for the management of imprecise knowledge is complementary to other DL extensions for the management of uncertainty, e.g. probabilistic extension
(Heinsohn, 1994; Jager, 1994; Koller, Levy, & Pfeffer, 1997; Sebastiani, 1994) with some exceptions like shown by Hollunder (1994) where a possibilistic DL has been considered. Even
138

fiReasoning within Fuzzy DLs

though these probabilistic extensions enlarge the applicability of DLs they do not address
the issue of reasoning about individuals and imprecise concepts, as imprecise knowledge and
uncertain knowledge are orthogonal (Dubois & Prade, 1994). Moreover, reasoning in a
probabilistic framework is generally a harder task, from a computational point of view, than
the relative non probabilistic case and in most cases a complete axiomatization is missing
(Halpern, 1990; Roth, 1996). As a consequence, the computational problems have to be
addressed carefully (Koller et al., 1997).
We will proceed as follows. In the following section we first introduce ALC. In Section 3
we extend ALC to the fuzzy case and discuss some properties in Section 4, while in Section 5
we will present a constraint propagation calculus for reasoning in it. Section 6 concludes
and presents some topics for further research.

2. A Quick Look to ALC
The specific DL we will extend with fuzzy capabilities is ALC, a significant representative
of DLs. At first, we will introduce classical ALC, while in Section 3 our fuzzy extension of
ALC will be presented.
We assume three alphabets of symbols, called primitive concepts (denoted by A), primitive roles (denoted by R) and individuals (denoted by a and b).2
2.1 Concept and Role
Concepts are expressions that collect the properties, described by means of roles, of a set of
individuals. From a FOL point of view, concepts can be seen as unary predicates, whereas
roles are interpreted as binary predicates.
A concept (denoted by C or D) of the language ALC is build out of primitive concepts
according to the following syntax rules:
C, D 

>|
|
A|
C u D|
C t D|
C|
R.C|
R.C

(top concept)
(bottom concept)
(primitive concept)
(concept conjunction)
(concept disjunction)
(concept negation)
(universal quantification)
(existential quantification).

2.2 Interpretation
DLs have a clean, model-theoretic semantics, based on the notion of interpretation. An
interpretation I is a pair I = (I , I ) consisting of a non empty set I (called the domain)
and of an interpretation function I mapping different individuals into different elements of
I (called unique name assumption), primitive concepts into subsets of I and primitive
roles into subsets of I  I . The interpretation of complex concepts is defined as usual:
2. Through this work we assume that every metavariable has an optional subscript or superscript.

139

fiStraccia

>I
I
(C u D)I
(C t D)I
(C)I
(R.C)I
(R.C)I

=
=
=
=
=
=
=

I

C I  DI
C I  DI
I \ C I
{d  I : d0 .(d, d0 ) 6 RI or d0  C I }
{d  I : d0 .(d, d0 )  RI and d0  C I }.

Note that each concept C and role R can be mapped into an equivalent open first-order
formula FC (x) and FR (x, y), respectively:
F> (x) = T

(1)

F (x) = F

(2)

FA (x) = A(x)

(3)

FR (x, y) = R(x, y)

(4)

FCuD (x) = FC (x)  FD (x)

(5)

FCtD (x) = FC (x)  FD (x)

(6)

FC (x) = FC (x)

(7)

FR.C (x) = y.FR (x, y)  FC (y)

(8)

FR.C (x) = y.FR (x, y)  FC (y),

(9)

where T and F are two formulae representing the truth-value true and false, respectively
(e.g. T = p  p and F = p  p, for some letter p).
Two concepts C and D are said to be equivalent (denoted by C  D) when C I = DI
for all interpretations I. Note that, e.g. >   ; C u D  (C t D), and (R.C) 
(R.C).
2.3 Assertion
An assertion (denoted by ) is an expression of type a:C (a is C, also a is an instance of
C), or an expression of type (a, b):R ((a, b) is R, also (a, b) is an instance of R). For
instance, tom:Tall u Student asserts that Tom is a tall student, whereas (tim, tom):Friend
asserts that Tom is a friend of Tim. A primitive assertion is either an assertion of the
form a:A, where A is a primitive concept, or an assertion of the form (a, b):R. From a
semantics point of view, an interpretation I satisfies a:C (resp. (a, b):R) iff aI  C I (resp.
(aI , bI )  RI ).
2.4 Terminological Axiom
A terminological axiom (denoted by  ) is either a concept specialisation or a concept definition. A concept specialisation is an expression of the form A<C, where A is a primitive
concept and C is a concept. A specialisation allows stating the existence of a specialisation (more specific than) relation between concepts. For instance, Ferrari<SportCar u
140

fiReasoning within Fuzzy DLs

Ownedby.CarFanatic states that a Ferrari is a sport car that is owned by a car fanatic.
On the other hand, a concept definition is an expression of the form A: = C, where
A is a primitive concept and C is a concept. A concept definition allows stating the
equivalence between concepts. For instance, Tennis: = SportKind u (HasSportTool.>) u
(HasSportTool.TennisRacket) states that tennis is identified by a kind of sport having a
tennis racket as a sport tool. From a semantics point of view, an interpretation I satisfies a
concept specialisation A<C iff AI  C I . Similarly, an interpretation I satisfies a concept
definition A: = C iff AI = C I .
2.5 Knowledge Base, Entailment and Subsumption
A finite set K of assertions and terminological axioms will be called a Knowledge Base (KB).
With KA we will denote the set of assertions in K, whereas with KT we will denote the set
of terminological axioms in K, also called a terminology. A KB K is purely assertional if
KT = . Further, we will assume that a terminology KT is such that no concept A appears
more than once on the left hand side of a terminological axiom   KT and that no cyclic
definitions are present in KT .3
We will say that an interpretation I satisfies (is a model of ) a KB K iff I satisfies each
element in K. A KB K entails an assertion  (denoted by K |= ) iff every model of K
also satisfies . Furthermore, let KT be a terminology and let C, D be two concepts. We
will say that D subsumes C with respect to (w.r.t.) KT (denoted by C vKT D) iff for every
model I of KT , C I  DI holds.
The problem of determining whether K |=  is called entailment problem; the problem of determining whether C vKT D is called subsumption problem; and the problem of
determining whether K is satisfiable is called satisfiability problem.
It is well known (Buchheit, Donini, & Schaerf, 1993a; Donini, Lenzerini, Nardi, &
Schaerf, 1994; Nebel, 1990) that in ALC
K |= (a, b):R iff (a, b):R  K
K |= a:C iff K  {a:C} is not satisfiable
C v D iff a:C |= a:D, for a new a
0

C vKT D iff C v D

0

(10)
(11)
(12)
(13)

where C 0 and D0 are build from C and D by expanding the terminology KT to KT00 and
substituting every primitive concept occurring in C or D, which is defined in KT00 , with its
defining term in KT00 . The expansion of a KB K works as follows (Nebel, 1990).
1. Elimination of concept specialisation: each concept specialisation A<C  KT is replaced with a concept definition A: = C u A , where A is a new primitive concept.
A stands for the absent part of the definition of A. Let KT0 be the terminology, which
is obtained by replacing all concept specialisation by concept definitions.
3. We will say that A directly uses primitive concept B in KT , if there is   KT such that A is on the left
hand side of  and B occurs in the right hand side of  . Let uses be the transitive closure of the relation
directly uses in KT . KT is cyclic iff there is A such that A uses A in KT .

141

fiStraccia

2. Expansion of KT0 : every defined concept (i.e. the first argument of a concept definition)
which occurs in the defining term of a concept definition (i.e. the second argument of
a concept definition) is substituted by its defining term. This process is iterated until
there remain only undefined concepts in the second arguments of concept definitions.
This yields a terminology KT00 .
3. Expansion of KA : every primitive concept occurring in KA which is defined in KT00 is
0 .
substituted by its defining term in KT00 . This yields KA
0 |= 0 , where 0 is obtained by
The transformation has the nice property that K |=  iff KA
replacing every primitive concept occurring in , which is defined in KT00 , with its defining
term in KT00 . While this allows us to restrict our attention to purely assertional KBs only,
it is worth noting that the expansion process can be exponential (Nebel, 1988).
From (10)(13), it follows that the above problems can be reduced to the satisfiability
problem. There exists a well known technique based on constraint propagation solving this
problem (Schmidt-Schau & Smolka, 1991; Buchheit, Donini, & Schaerf, 1993b; Donini
et al., 1994).
We conclude with an example.

Example 1 Consider the following terminology KT .
SportKind
<
SportTool
<
IndividualSport <
TeamSport
<
Basketball
<
TennisRacket
<
Basket
:=

Tennis

>
>
SportKind
SportKind
SportTool
SportTool
SportKindu
(KindOfSport.>)u
(KindOfSport.TeamSport)u
(HasSportTool.>)u
(HasSportTool.Basketball)
: = SportKindu
(KindOfSport.>)u
(KindOfSport.IndividualSport)u
(HasSportTool.>)u
(HasSportTool.TennisRacket)

Suppose that there are two video sequences v1, v2, which are about basket and tennis, respectively. We may represent the semantic content of them through
Kv1 = {v1:Video u About.Basket}
Kv2 = {v2:Video u About.Tennis}.
Consider K = KT  Kv1  Kv2 . If we are interested in retrieving videos about sport, we may
query K through the query concept Q = Video u About.SportKind and the answer will be
the list containing both v1 and v2, as K |= v1:Q and K |= v2:Q hold.
142

fiReasoning within Fuzzy DLs

On the other hand, if we are looking for individual sport videos, then, given the query
concept Q0 = Video u About.KindOfSport.IndividualSport, it follows that only video v2 will
be retrieved. In fact, K|=
6 v1:Q0 and K |= v2:Q0 hold.
2

3. A Fuzzy DL
Our fuzzy extension directly relates to Zadehs work on fuzzy sets (Zadeh, 1965). A fuzzy set
S with respect to an universe U is characterised by a membership function S : U  [0, 1],
assigning an S-membership degree, S (u), to each element u in U . S (u) gives us an
estimation of the belonging of u to S. Typically, if S (u) = 1 then u definitely belongs to
S, while S (u) = 0.8 means that u is likely to be an element of S. Moreover, according to
Zadeh, the membership function has to satisfy three well known restrictions: for all u  U
and for all fuzzy sets S1 , S2 with respect to U
S1 S2 (u) = min{S1 (u), S2 (u)}
S1 S2 (u) = max{S1 (u), S2 (u)}
S1 (u)
= 1  S1 (u) ,
where S1 is the complement of S1 in U . Alternative restrictions on membership functions
have been proposed in the literature, but it is not our aim to investigate them here (the
interested reader may consult e.g. Dubois & Prade, 1980).
A justification of the choice of the min and the max was given by Bellman and Giertz
(1973), which have shown that under certain reasonable conditions min and max are the
unique possible choice for set intersection and set union, respectively.
When we switch to fuzzy logics, the notion of degree of membership S (u) of an element
u  U w.r.t. the fuzzy set S over U is regarded as the truth-value of the statement u is
S. Accordingly, in our fuzzy DL, (i) a concept C, rather than being interpreted as a
classical set, will be interpreted as a fuzzy set and, thus, concepts become imprecise; and,
consequently, (ii) the statement a is C, i.e. a:C, will have a truth-value in [0, 1] given by
the degree of membership of being the individual a a member of the fuzzy set C.
3.1 Fuzzy Interpretation
A fuzzy interpretation is now a pair I = (I , I ), where I is, as for the crisp case, the
domain, whereas I is an interpretation function mapping
1. individuals as for the crisp case, i.e. aI 6= bI , if a 6= b;
2. a concept C into a membership function C I : I  [0, 1];
3. a role R into a membership function RI : I  I  [0, 1].
If C is a concept then C I will naturally be interpreted as the membership degree function
of the fuzzy concept (set) C w.r.t. I, i.e. if d  I is an object of the domain I then
C I (d) gives us the degree of being the object d an element of the fuzzy concept C under
the interpretation I. Similarly for roles. Additionally, the interpretation function I has to
satisfy the following equations: for all d  I ,
143

fiStraccia

>I (d)
I (d)
(C u D)I (d)
(C t D)I (d)
(C)I (d)
(R.C)I (d)
(R.C)I (d)

=
=
=
=
=
=
=

1
0
min{C I (d), DI (d)}
max{C I (d), DI (d)}
1  C I (d)
inf d0 I {max{1  RI (d, d0 ), C I (d0 )}}
supd0 I {min{RI (d, d0 ), C I (d0 )}}.

These equations are the standard interpretation of conjunction, disjunction, negation and
quantification, respectively (see Lee, 1972; Tresp & Molitor, 1998).
Note that the semantics of R.C
(R.C)I (d) = supd0 I {min{RI (d, d0 ), C I (d0 )}}

(14)

is the result of viewing R.C as the open first order formula y.FR (x, y)  FC (y) (see (9))
and the existential quantifier  is viewed as a disjunction over the elements of the domain.
Similarly,
(R.C)I (d) = inf d0 I {max{1  RI (d, d0 ), C I (d0 )}}

(15)

is related to the open first order formula y.FR (x, y)  FC (y) (see (8)), where the universal
quantifier  is viewed as a conjunction over the elements of the domain.
We will say that two concepts C and D are said to be equivalent (denoted by C 
= D)
when C I = DI for all interpretations I. As for the crisp non fuzzy case, dual relationships
between concepts hold: e.g. > 
=  , (C u D) 
= (C t D) and (R.C) 
= (R.C).
3.2 Fuzzy Assertion
A fuzzy assertion (denoted by ) is an expression having one of the following form h  ni
or h  mi, where  is an ALC assertion, n  (0, 1] and m  [0, 1). From a semantics point
of view, a fuzzy assertion h  ni constrains the truth-value of  to be less or equal to n
(similarly for ). Consequently, e.g. hv1:Video u About.Basket  0.8i states that video v1 is
likely about basket. Formally, an interpretation I satisfies a fuzzy assertion ha:C  ni (resp.
h(a, b):R  ni) iff C I (aI )  n (resp. RI (aI , bI )  n). Similarly, an interpretation I satisfies
a fuzzy assertion ha:C  ni (resp. h(a, b):R  ni) iff C I (aI )  n (resp. RI (aI , bI )  n).
Two fuzzy assertions  1 and  2 are said to be equivalent (denoted by  1 
=  2 ) iff they are
satisfied by the same set of interpretations. Notice that the combination of both ha:C  mi
and ha:C  ni, with m  n, restricts the truth-value of a:C in between [m, n]. Moreover,
ha:C  ni 
= ha:C  1  ni. A primitive fuzzy assertion is a fuzzy assertion involving a
primitive assertion.
One might wonder why we do not allow expressions of the form h > ni or the form
h < ni. The reason simply relies on the observation that it is quite hard to imagine
situations in which we are able to assert such strict >, < relations. So we will leave them
out for ease.4
4. Of course, the whole can easily be extended in case we would like to consider these two types of assertions
too.

144

fiReasoning within Fuzzy DLs

Note that in the work of Straccia (1998), no fuzzy assertion of the form h  ni is
allowed.
3.3 Fuzzy Terminological Axiom
Fuzzy terminological axioms we will consider are a natural extension of classical terminological axioms to the fuzzy case. From a syntax point of view, a fuzzy terminological axiom
(denoted by  ) is either a fuzzy concept specialisation or a fuzzy concept definition. A fuzzy
concept specialisation is an expression of the form AC, where A is a primitive concept
and C is a concept. On the other hand, a fuzzy concept definition is an expression of the
form A: C, where A is a primitive concept and C is a concept. From a semantics point of
view, we consider the natural extension of classical set inclusion to the fuzzy case (Zadeh,
1965). A fuzzy interpretation I satisfies a fuzzy concept specialisation AC iff
d  I , AI (d)  C I (d),

(16)

whereas I satisfies a fuzzy concept definition A: C iff
d  I , AI (d) = C I (d).

(17)

Note that in the work of Straccia (1998) a fuzzy specialisation is non-standard. Indeed, Straccia (1998) considered fuzzy specialisations of the form hA  C  ni where
(A  C)I = mindI {max{1  AI (d), C I (d)}}. A drawback of this formulation is that
it is not clear where the n in hA  C  ni comes from, i.e. who defines the value n and
how it is determined. We prefer to rely here on the standard interpretation of fuzzy subsets.
3.4 Fuzzy Knowledge Base, Fuzzy Entailment and Fuzzy Subsumption
A fuzzy knowledge base is a finite set of fuzzy assertions and fuzzy terminological axioms.
As for the crisp case, with A we will denote the set of fuzzy assertions in , with T we
will denote the set of fuzzy terminological axioms in  (the terminology), if T =  then 
is purely assertional, and we will assume that a terminology T is such that no concept A
appears more than once on the left hand side of a fuzzy terminological axiom   T and
that no cyclic definitions are present in T .
An interpretation I satisfies (is a model of ) a set of fuzzy  iff I satisfies each element
of . A fuzzy KB  fuzzy entails a fuzzy assertion  (denoted by |) iff every model of
 also satisfies .
Furthermore, let T be a terminology and let C, D be two concepts. We will say
that D fuzzy subsumes C w.r.t. T (denoted by C T D) iff for every model I of T ,
d  I , C I (d)  DI (d) holds.
Finally, given a fuzzy KB  and an assertion , it is of interest to compute s best
lower and upper truth-value bounds. To this end we define the greatest lower bound of
 w.r.t.  (denoted by glb(, )) to be sup{n : |h  ni}. Similarly, we define the
least upper bound of  with respect to  (denoted by lub(, )) to be inf{n : |h  ni}
(sup  = 0, inf  = 1). Determining the lub and the glb is called the Best Truth-Value Bound
(BTVB) problem.
145

fiStraccia

4. Some Properties
In this section we discuss some properties of our fuzzy logic. Several properties described
by Straccia (2000b) for the propositional case are easily extended to our first order case too.
4.1 Concept Equivalence
 C u>=
 C, C t > =
 >, Cu =,
 Ct =
 C,
The first ones are straightforward: > =,




C = C, (C uD) = C tD, (C tD) = C uD, C1 u(C2 tC3 ) = (C1 uC2 )t(C1 uC3 )
and C1 t (C2 u C3 ) 
= (C1 t C2 ) u (C1 t C3 ). For concepts involving roles, we have R.C 
=



R.C, R.> = >, R. = and (R.C) u (R.D) = R.(C u D). Please, note that we
and
do not have C u C 
6
=
=, nor we have C t C 
= > and, thus, (R.C) u (R.C) 
I
(R.C) t (R.C) 6
= > hold. In general we can only say that (C u C) (d)  0.5, for any
interpretation I and d  I and, similarly, (C t C)I (d)  0.5, i.e. lub(, a:C u C) = 0.5
and glb(, a:C t C) = 0.5, respectively.
4.2 Entailment Relation
Of course, |h  ni iff glb(, )  n, and similarly |h  ni iff lub(, )  n hold.
Concerning roles, note that |h(a, b):R  ni iff h(a, b):R  mi   with m  n. Therefore,
glb(, R(a, b)) = max{n : hR(a, b)  ni  }

(18)

while the same is not true for the  case. While h(a, b):R  mi   and m  n imply
|h(a, b):R  ni, the converse is false (e.g. {ha:R.A  1i, hb:A  0i}|h(a, b):R  0i)).
Furthermore, from |ha:C  ni iff |ha:C  1  ni,
1  lub(, C(a)) =
=
=
=
=

1  inf{n : |ha:C  ni}
sup{1  n : |ha:C  ni}
sup{n : |ha:C  1  ni}
sup{n : |ha:C  ni}
glb(, a:C).

follows. Therefore,
lub(, a:C) = 1  glb(, a:C),

(19)

i.e. lub can be determined through glb (and vice-versa). The same reduction to glb does
not hold for lub(, (a, b):R) as (a, b):R is not an expression of our language.5
Modus ponens on concepts is supported: if m > 1n then {ha:C  mi, ha:C t D  ni}|
ha:D  ni holds.
Modus ponens on roles is supported: if m > 1  n then {h(a, b):R  mi, ha:R.D  ni}
|ha:D  ni and {ha:R.C  mi, ha:R.D  ni} |ha:R.(C u D)  min{n, m}i hold. Moreover, {ha:R.C  mi, ha:R.D  ni} |ha:R.(C u D)  min{n, m}i holds.
Modus ponens on specialisation is supported. The following degree bounds propagation
through a taxonomy is supported. If C  D then (i)   {ha:C  ni}|ha:D  ni; and
(ii)   {ha:D  ni}|ha:C  ni hold.
5. Of course, lub(, (a, b):R) = 1  glb(, (a, b):R) holds, where (R)I (d, d0 ) = 1  RI (d, d0 ).

146

fiReasoning within Fuzzy DLs

Note that, according to Straccia (1998)
if m > 1  n then {ha:A  mi, hA  C  ni}|ha:C  ni.
A drawback of the above property is that whatever the degree m is (as long as m > 1  n),
from ha:A  mi and hA  C  ni we infer ha:C  ni, where n is a priori fixed value.
4.3 Soundness of the Semantics
Our fuzzy semantics is sound w.r.t. crisp semantics. In fact, let  be a fuzzy KB in which
no h(a, b):R  ni occurs. We leave these fuzzy assertions out, as role negation is not present
in crisp ALC. Let us consider the following transformation ]() of fuzzy assertions into
assertions, where ]() takes the crisp assertional part of a fuzzy assertion:
]h  ni 7 
]ha:C  ni 7 a:C.
We extend ]() to fuzzy terminological axioms as follows: ] =  . Finally, ] = {] :  
}  {] :   T }.
It is quite easily to verify that
Proposition 1 Let  be a fuzzy KB in which no h(a, b):R  ni occurs and let  be a fuzzy
assertion h  ni. If |h  ni then ] |= ] (i.e. there cannot be fuzzy entailment
without entailment in ALC).
a
Proof: Consider a classical interpretation I satisfying ]. I is also a fuzzy interpretation
such that C I (d)  {0, 1}, RI (d, d0 )  {0, 1} hold. By induction on the structure of a
concept C it can be shown that I (classically) satisfies a:C iff C I (aI ) = 1. Similarly for
roles. Therefore, I is a fuzzy interpretation satisfying . By hypothesis, I satisfies h  ni
and n > 0. Therefore, the truth-value of  under I is 1, i.e. I satisfies . 2
For the general case,  has to be satisfiable as h(a, b):R  ni may introduce an inconsistency,
e.g. {h(a, b):R  0.3i, h(a, b):R  0.4i}|ha:A  1i, but {(a, b):R} 6|= a:A}.
The converse of Proposition 1 does not hold in general.
Example 2 Let  be the set  = {ha:A t B  0.6i, ha:A  0.3i}. It follows that ] =
{a:(A t B), a:A} which is unsatisfiable. Therefore, it can easily be verified that ] |= a:B,
but  |
6 ha:B  ni, for all n > 0.
2
Once we restrict the attention to normalised fuzzy assertions, a converse relation follows
immediately (Lee, 1972; Straccia, 2000b).
Indeed, we say that a fuzzy assertion  is KB-normalised iff
1. if  is h  ni then n > 0.5;
2. if  is h  ni then n < 0.5.
We say that a fuzzy assertion  is query-normalised iff
147

fiStraccia

1. if  is h  ni then n  0.5;
2. if  is h  ni then n  0.5.
Note that the definitions for KB-normalisation and query-normalisation are dual. The
following proposition follows from (Lee, 1972; Straccia, 2000b) and relies on the fact that
h  ni and h  mi are together inconsistent, if n > 0.5 and m < 0.5. In particular,
Straccia (2000b) shows that if  and  are a normalised fuzzy propositional KB and a
query-normalised fuzzy proposition, then Then | iff ] |= ]. The proof is given
by showing that from a deduction proving | a deduction proving ] |= ] can be
build and vice-versa. The extension to our case is straightforward as e.g. for n, m > 0.5,
ha:R.C  ni,h(a, b):R  mi|hb:C  ni iff a:R.C, (a, b):R |= b:C holds (the other firstorder cases involving  and  are similar).
Proposition 2 Let  be a fuzzy KB in which no h(a, b):R  ni occurs and each    is
KB-normalised. Let  be a query-normalised fuzzy assertion. Then | iff ] |= ]. a
Example 3 Let  be the set  = {ha:A u B  0.4i, ha:A  0.6i}. Let  be ha:B  0.7i.
Each fuzzy assertion in  is KB-normalised and  is query-normalised. It follows that
] = {a:(A u B), a:A} and ] = a:B. It is easily verified that | and that ] |= ],
thereby confirming Proposition 2.
2
4.4 Subsumption
At first, as for the classical case and with the same method seen before, subsumption
between two concepts C and D w.r.t. a terminology T , i.e. C T D, can be reduced to
the case of an empty terminology, i.e. C 0  D0 .
Example 4 Suppose we have two images i1 and i2 each being a snapshot of the car traffic
on a major street of an European city. An underlying image analysis tool recognizes, among
all the recognised objects, that in image i1 there is a Ferrari, while in image i2 there is a
Porsche. Furthermore, a semantic image indexing tool establishes that, to some degree n
image i1 is about a Ferrari, whereas to some degree m image i2 is about a Porsche. Please
note that, as a weight of a keyword in text is a quantitative description of the aboutness of
the text w.r.t. the keyword, a truth-degree gives a quantitative description of the aboutness
of an images w.r.t. an object, i.e. the aboutness is handled as an imprecise concept. So, let
us consider
 = {hi1:About.Ferrari  0.6i, hi2:About.Porsche  0.8i,
FerrariCar, PorscheCar}.
where the axioms specify that both a Ferrari and a Porsche are a car. According to the
expansion process,  will be replaced by
0 = {hi1:About.Ferrari  0.6i, hi2:About.Porsche  0.8i,
Ferrari: Car u Ferrari , Porsche: Car u Porsche },
which will be simplified to
148

fiReasoning within Fuzzy DLs

00 = {hi1:About.(Car u Ferrari )  0.6i,
hi2:About.(Car u Porsche )  0.8i}.
Now, if we are looking for images which are about cars, then from  we may infer that  |
hi1:About.Car  0.6i and |hi2:About.Car  0.8i. Furthermore, it is easily verified that
00 | hi1:About.Car  0.6i and 00 |hi2:About.Car  0.8i hold as well. Indeed, for any
fuzzy assertion , | iff 00 | holds.
2
We conclude this section with the analogue of Example 1 for the fuzzy case..
Example 5 Consider the terminology KT and the query concept Q in Example 1. Let
us define T as the fuzzy KB derived from KT in which each terminological axiom  has
been replaced with the fuzzy terminological axiom  . Moreover, let us suppose that an
underlying semantic video indexing tool furnishes the following semantic descriptions of the
two videos v1 and v2.
v1 = {hv1:Video  1i, hv1:About.Basket  0.9i}
v2 = {hv2:Video  1i, hv2:About.Tennis  0.6i},
i.e. video v1 is about basket with degree 0.9, whereas video v2 is about tennis with degree
0.6. Let us consider  = T  v1  v2 . It is easily verified that glb(, v1:Q) = 0.9,
whereas glb(, v2:Q) = 0.6 hold. Therefore, video v1 will be ranked before video v2 after
the retrieval process.
2

5. Decision Algorithms in Fuzzy ALC
Deciding whether |h  ni or |h  mi requires a calculus. Without loss of generality
we will consider purely assertional fuzzy KBs only.
We will develop a calculus in the style of the constraint propagation method, as this
method is usually proposed in the context of DLs (Buchheit et al., 1993a). The calculus
extends the fuzzy propositional calculus described by Chen and Kundu (1996) and by
Straccia (2000b) to our fuzzy DL case. We first address the entailment problem, then the
subsumption problem and finally the BTVB problem. Both the subsumption problem and
the BTVB problem will be reduced to the entailment problem.
5.1 A Decision Procedure for the Entailment Problem
Consider a new alphabet of ALC variables. An interpretation is extended to variables by
mapping these into elements of the interpretation domain. An ALC object (denoted by w)
is either an individual or a variable.6
A constraint (denoted by ) is an expression of the form w:C or (w, w0 ):R, where w, w0
are objects, C is an ALC concept and R is a role. A fuzzy constraint (denoted by ) is an
expression having one of the following four forms: h  ni, h > ni, h  ni, h < ni. Note
that assertions and fuzzy assertions are constraints and fuzzy constraints, respectively.
6. In the following, if there is no ambiguity, ALC variables and ALC objects are called variables and objects,
respectively.

149

fiStraccia

The definitions of satisfiability of a constraint, a fuzzy constraint, a set of constraints,
a set of fuzzy constraints, primitive constraint and primitive fuzzy constraint are obvious.
It is quite easily verified that the fuzzy entailment problem can be reduced to the
unsatisfiability problem of a set of fuzzy constraints:
|h  ni

iff

  {h < ni} not satisfiable

(20)

|h  ni

iff

  {h > ni} not satisfiable.

(21)

Our calculus, determining whether a finite set S of fuzzy constraints is satisfiable or not, is
based on a set of constraint propagation rules transforming a set S of fuzzy constraints into
simpler satisfiability preserving sets Si until either all Si contain a clash (indicating that
from all the Si no model of S can be build) or some Si is completed and clash-free, that is,
no rule can be further be applied to Si and Si contains no clash (indicating that from Si a
model of S can be build).
A set of fuzzy constraints S contains a clash iff it contains either one of the constraints in
Table 1 or S contains a conjugated pair of fuzzy constraints. Each entry in Table 2 says us
hw:  ni, where n > 0
hw:>  ni, where n < 1
hw: > ni, hw:> < ni, hw:C < 0i, hw:C > 1i
Table 1: Clashes

h  ni
h > ni

h < mi h  mi
nm
n>m
nm
nm

Table 2: Conjugated Pairs
under which condition the row-column pair of fuzzy constraints is a conjugated pair. Given
a fuzzy constraint , with  c we indicate a conjugate of  (if there exists one). Notice that
a conjugate of a fuzzy constraint may be not unique, as there could be infinitely many. For
instance, both ha:C < 0.6i and ha:C  0.7i are conjugates of ha:C  0.8i.
Concerning the rules, for each connective u, t, , ,  there is a rule for each relation , >
, , <, i.e. there are 20 rules. The rules have the form:
   if 

(22)

where  and  are sequences of fuzzy constraints and  is a condition. A rule fires only
if the condition  holds, if the current set S of fuzzy constraints contains fuzzy constraints
matching the precondition  and the consequence  is not already in S. After firing, the
constraints from  are added to S. The rules are the following:
150

fiReasoning within Fuzzy DLs

( )

hw:C  ni  hw:C  1  ni

(> )

hw:C > ni  hw:C < 1  ni

( )

hw:C  ni  hw:C  1  ni

(< )

hw:C < ni  hw:C > 1  ni

(u )

hw:C u D  ni  hw:C  ni, hw:D  ni

(u> )

hw:C u D > ni  hw:C > ni, hw:D > ni

(t )

hw:C t D  ni  hw:C  ni, hw:D  ni

(t< )

hw:C t D < ni  hw:C < ni, hw:D < ni

(t )

hw:C t D  ni  hw:C  ni | hw:D  ni

(t> )

hw:C t D > ni  hw:C > ni | hw:D > ni

(u )

hw:C u D  ni  hw:C  ni | hw:D  ni

(u< )

hw:C u D < ni  hw:C < ni | hw:D < ni

( )

hw1 :R.C  ni,  c  hw2 :C  ni
if  is h(w1 , w2 ):R  1  ni

(> )

hw1 :R.C > ni,  c  hw2 :C > ni
if  is h(w1 , w2 ):R < 1  ni

( )

hw1 :R.C  ni,  c  hw2 :C  ni
if  is h(w1 , w2 ):R  ni

(< )

hw1 :R.C < ni,  c  hw2 :C < ni
if  is h(w1 , w2 ):R < ni

( )

hw:R.C  ni  h(w, x):R  ni, hx:C  ni
if x new variable and there is no w0 such that both
h(w, w0 ):R  ni and hw0 :C  ni are already in the constraint set

(> )

hw:R.C > ni  h(w, x):R > ni, hx:C > ni
if x new variable and there is no w0 such that both
h(w, w0 ):R > ni and hw0 :C > ni are already in the constraint set

( )

hw:R.C  ni  h(w, x):R  1  ni, hx:C  ni
if x new variable and there is no w0 such that both
h(w, w0 ):R  1  ni and hw0 :C  ni are already in the constraint set

(< )

hw:R.C < ni  h(w, x):R > 1  ni, hx:C < ni
if x new variable and there is no w0 such that both
h(w, w0 ):R > 1  ni and hw0 :C < ni are already in the constraint set
151

(23)

fiStraccia

Examples of rule instances are the following:
( )

ha:R.C  0.7i, h(a, b):R  0.6i  hb:C  0.7i
 is h(a, b):R  0.3i
 c = h(a, b):R  0.6i is a conjugate of 

(< )

ha:R.C < 0.8i, h(a, b):R  0.9i  hb:C < 0.8i
 is h(a, b):R < 0.8i
 c = h(a, b):R  0.9i is a conjugate of 

( )

ha:R.C  0.8i  h(w, x):R  0.8i, hx:C  0.8i
x new variable

(< )

ha:R.C < 0.8i  h(w, x):R > 0.2i, hx:C < 0.8i
x new variable.

A set of fuzzy constraints S is said to be complete if no rule is applicable to it. Any complete
set of fuzzy constraints S2 obtained from a set of fuzzy constraints S1 by applying the above
rules (23) is called a completion of S1 . Due to the rules (t ), (t> ), (u ) and (u< ), more
than one completion can be obtained. These rules are called nondeterministic rules. All
other rules are called deterministic rules.
It is easily verified that the above calculus has the termination property, i.e. any completion of a finite set of fuzzy constraints S can be obtained after a finite number of rule
applications.
Example 6 Let us consider the following fuzzy KB:
 = {ha:R.D  0.7i, ha:R.C  0.4i, h(a, b):R  0.5i, hb:C  0.2i, hb:D  0.3i}
Let  be the assertion a:R.(D u C), let  be the fuzzy assertion h  0.4i, whereas let  0 be
the fuzzy assertion h  0.5i. It is easily verified that |, whereas  |
6  0 . We show that
0
 |
6  , by verifying that there is a clash-free completion of S =   {ha:R.(D u C) < 0.5i}
(precisely, there are two of them).
By applying rules (23), we have the following sequences.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)

ha:R.D  0.7i
ha:R.C  0.4i
h(a, b):R  0.5i
hb:C  0.2i
hb:D  0.3i
ha:R.(D u C) < 0.5i
h(a, x):R  0.7i, hx:D  0.7i
hx:C  0.4i
hb:D u C < 0.5i
hx:D u C < 0.5i
1 |  2
152

Hypothesis:S

( ) : (1)
( ) : (2), (7)
(< ) : (3), (6)
(< ) : (6), (7)

fiReasoning within Fuzzy DLs

where the two sequences 1 and 2 are defined as follows: for 1 we have the two sequences
(11) hb:D < 0.5i (u< ) : (9)
(12) hx:D < 0.5i (u< ) : (10)
(13) clash
(7), (12)

(14) hx:C < 0.5i (u< ) : (10)
(15) clash-free

and for 2 we have the two sequences
(16) hb:C < 0.5i (u< ) : (9)
(17) hx:D < 0.5i (u< ) : (10)
(18) clash
(7), (17)

(19) hx:C < 0.5i (u< ) : (10)
(20) clash-free

2
Example 7 Consider Example 4 and let us prove that 00 |h(About.Car)(i1)  0.6i. We
prove the above relation by verifying that all completions of S = 00  {hi1:About.Car < 0.6i}
contain a clash. In fact, we have the following sequence.
(1)
(2)
(3)
(4)
(5)
(6)
(7)

hi1:About.(Car u Ferrari )  0.6i
Hypothesis:S
hi2:About.(Car u Porsche )  0.8i
hi1:About.Car < 0.6i
h(i1, x):About  0.6i, hx:(Car u Ferrari )  0.6i ( ) : (1)
hx:Car < 0.6i
(< ) : (3), (4)
(u) : (4)
hx:Car  0.6i, hx:Ferrari  0.6i,
clash
(5), (6)

2
Proposition 3 A finite set of fuzzy constraints S is satisfiable iff there exists a clash free
completion of S.
a
Proof:
 .) Given the termination property, it is easily verified, by case analysis, that the
above rules are sound, i.e. if S1 is satisfiable then there is a satisfiable completion S2
of S1 and, thus, S2 contains no clash. For instance, let us show that the ( ) rule is
sound. Assume I is an interpretation satisfying hw1 :R.C  ni and h(w1 , w2 ):R  mi,
where m > 1  n. Let us show that I satisfies hw2 :C  ni. Since I satisfies hw1 :R.C  ni
it follows that max{1  RI (w1 I , w2 I ), C I (w2 I )}  n. But, RI (w1 I , w2 I )  m and, thus,
1  RI (w1 I , w2 I )  1  m < n. As a consequence, C I (w2 I )  n follows, i.e. I satisfies
hw2 :C  ni.
 .) Suppose that there exists a clash free completion S 0 of S. We build from S 0 an
interpretation I satisfying S 0 and, as S  S 0 , I satisfies S. I is called canonical model.
For any primitive constraint   S 0 , we collect its lower and upper bound restrictions
in S 0 as follows: let
N  [] = {n : h  ni  S 0 }
N > [] = {n : h > ni  S 0 }
N  [] = {n : h  ni  S 0 }
N < [] = {n : h < ni  S 0 }.
153

fiStraccia

We have to define I such that for each constraint , I satisfies the constraints collected
in the sets N () []: given N  [], the truth value n of  under I has to be such that
n  max N  [], whereas w.r.t. N > [], the truth value n of  under I has to be such that
n  max N > [] + , for a  > 0. Similarly, for the other cases, for instance, w.r.t. N < [],
the truth value n of  under I has to be such that n  max N > []  , for a  > 0. The
two tables below
N  [] N > []
glb[, ]


0
0

6= 
n +
6= 

n
0
6= 
6= 
if n > n then n else n0 + 
N  [] N < []



6= 
6= 

6= 
6= 

lub[, ]
1
m0  
m
if m < m0 then m else m0  

define for any   S 0 and  > 0, lub[, ] and glb[, ], the lower and upper bound constraints
which I has to satisfy. In the tables, with n, n0 , m, m0 we indicate max N  [], max N > [],
min N  [] and min N < [], respectively. In each table we distinguish between the four cases
where the sets are empty (no constraints) or not. For instance, if for a constraint w:A, only
hw:A  0.3i, hw:A > 0.4i, hw:A  0.5i and hw:A < 0.6i are in S 0 , then according to the first
table bellow (row 4), for a  > 0, glb[w:A, ] = 0.4 + , whereas lub[w:A, ] = 0.5.
We will define I such that AI (wI ) = glb[w:A, ]. To make sure that glb[w:A, ] 
I
A (wI )  lub[w:A, ], we have to choose an  > 0 small enough such that glb[w:A, ] 
lub[w:A, ], i.e. 0.4 +   0.5. The existence of such an  > 0 is guaranteed by the fact that
S 0 is clash-free. An additional condition that the choice of such an  has to satisfy concerns
the case of a constraint  of type (w, w0 ):R. Let us show the problem with an example.
Suppose S 0 is {h(w, w0 ):R > 0.3i, hw:R.B  0.6i, hw0 :B  0.5i}. Therefore, according to
the above tables, glb[(w, w0 ):R, 1 ] = 0.3 + 1 , lub[(w, w0 ):R, 1 ] = 1, glb[w0 :B, 2 ] = 0
and lub[w0 :B, 2 ] = 0.5. So, it seems that it is sufficient to choose an 1 > 0 such that
0.3 + 1  1, which is indeed not the case. In fact, hw:R.B  0.6i and hw0 :B  0.5i
introduces an upper bound on (w, w0 ):R, i.e. the truth-value of (w, w0 ):R under I has to be
less or equal to 0.4 = 1  0.6. That is, we have to choose an 1 > 0 such that 0.3 + 1  0.4.
Otherwise, the truth-value of w0 :B under I has to be greater or equal to 0.6, contradicting
lub[w0 :B, 2 ] = 0.5. Again, the existence of such an  is guaranteed as S 0 is clash-free.
Summing up: since S 0 is clash-free, it follows that for each primitive constraint , there
is [] > 0 such that
glb[, []]  lub[, []]
where if  is (w, w0 ):R then
for
for
for
for

each
each
each
each

hw:R.C
hw:R.C
hw:R.C
hw:R.C

 ni,
> ni,
 ni,
< ni,

if
if
if
if

hw0 :C
hw0 :C
hw0 :C
hw0 :C

 ni 6 S 0
> ni 6 S 0
 ni 6 S 0
< ni 6 S 0
154

then
then
then
then

glb[, []]  1  n;
glb[, []] < 1  n;
glb[, []]  n;
glb[, []] < n.

(24)

fiReasoning within Fuzzy DLs

Now, consider the following interpretation I such that
1. the domain I is the set of objects appearing in S 0 ;
2. wI = w, for all w  I ;
3. >I (wI ) = 1 and I (wI ) = 0, for all w  I ;
4. AI (wI ) = glb[w:A, [w:A]], for all primitive concepts A and for all w  I ; and
5. RI (wI , w0 I ) = glb[(w, w0 ):R, [(w, w0 ):R]], for all roles R and for all w, w0  I .
We show, on induction on the structure of fuzzy constraints   S 0 , that I satisfies S 0 .
Case hw:A > ni: By definition, AI (wI ) = glb[w:A, [w:A]] > n and, thus, I satisfies
hw:A > ni. The cases ,  and < are similar.
Case h(w, w0 ):R  ni: By definition, RI (wI , w0 I ) = glb[(w, w0 ):R, [(w, w0 ):R]]  n and,
thus, I satisfies h(w, w0 ):R  ni. The cases >,  and < are similar.
Case hw:C u D  ni: From hw:C u D  ni  S 0 and S 0 completed, hw:C  ni  S 0 and
hw:D  ni  S 0 follows. By induction, I satisfies both hw:C  ni and hw:D  ni and, thus,
I satisfies hw:C u D  ni. The cases >,  and < are similar.
The cases involving C and C t D can be shown similarly.
Case hw:R.C  ni: Let  be (w, w0 ):R and consider hw0 :C  ni. It follows that, either (i)
hw0 :C  ni  S 0 ; or (ii) hw0 :C  ni 6 S 0 . Case (i): by induction, I satisfies hw0 :C  ni and,
thus, max{1  RI (w, w0 ), C I (w0 )}  C I (w0 )  n. Case (ii): by construction RI (w, w0 ) =
glb[(w, w0 ):R, [(w, w0 ):R]] and RI (w, w0 )  1n (see Equation 24). It follows that max{1
RI (w, w0 ), C I (w0 )}  1RI (w, w0 )  n. Therefore, inf w0 I max{1RI (w, w0 ), C I (w0 )} 
n, i.e. I satisfies hw:R.C  ni.
The cases hw:R.C > ni, hw:R.C  ni and hw:R.C < ni can be shown similarly.
Case hw:R.C  ni: Since S 0 is complete, both h(w, w0 ):R  ni and hw0 :C  ni are in
S 0 . By induction, I satisfies both h(w, w0 ):R  ni and hw0 :C  ni. As a consequence,
min{RI (w, w0 ), C I (w0 )}  n follows and, thus, supw0 I min{RI (w, w0 ), C I (w0 )}  n, i.e.
I satisfies hw:R.C  ni.
The cases hw:R.C > ni, hw:R.C  ni and hw:R.C < ni can be shown similarly. 2
The following example shows how such an interpretation is build.
Example 8 Let us consider Example 6 and the fuzzy assertion  0 . We have shown that
 |
6  0 by constructing two clash-free completions from S. Let us consider the clash-free
completion S1 in branch 1 :
S1 =   { ha:R.(D u C) < 0.5i, h(a, x):R  0.7i, hx:D  0.7i, hx:C  0.4i,
hb:D u C < 0.5i, hx:D u C < 0.5i, hb:D < 0.5i, hx:C < 0.5i}
We show that S1 is satisfiable by building an interpretation as described in the proof of
Proposition 3. Accordingly, for i > 0,
155

fiStraccia

glb[(a, b):R, 1 ]
glb[b:C, 2 ]
glb[b:D, 3 ]
glb[(a, x):R, 4 ]
glb[x:D, 5 ]
glb[x:C, 6 ]

=
=
=
=
=
=

0.5
0.2
0.3
0.7
0.7
0.4

lub[(a, b):R, 1 ]
lub[b:C, 2 ]
lub[b:D, 3 ]
lub[(a, x):R, 4 ]
lub[x:D, 5 ]
lub[x:C, 6 ]

=
=
=
=
=
=

1
1
0.5  3
1
1
0.5  6 .

Therefore, we can freely choose 2 , 4 and 5 , whereas 3 and 6 have to be such that
0.3  0.5  3 and 0.4  0.5  6 . As both h(a, b):R  0.5i and ha:R.C  0.4i are in
S1 , while hb:C  0.4i 6 S1 , we have the additional restriction on the choice of 1 (see
Equation 24) that glb[(a, b):R, 1 ]  0.6. But, glb[(a, b):R, 1 ] = 0.5  0.6, for every 1 and,
thus, the choice of 1 is also free. A solution to the i is e.g. i = 0.1 and, thus, let I be the
following interpretation:
1. the domain I is the set {a, b, x};
2. wI = w, for all w  I ;
3. >I (wI ) = 1 and I (wI ) = 0, for all w  I ;
4. C I (b) = 0.2, DI (b) = 0.3, DI (x) = 0.7 and C I (x) = 0.4 (in all other cases, AI (w) =
0); and
5. RI (a, b) = 0.5 and RI (a, x) = 0.7 (in all other cases, R0 I (w, w0 ) = 0).
Now, it is easily verified that I satisfies S1 and S.

2

From a computational complexity point of view, the fuzzy entailment problem can be proven
to be a PSPACE-complete problem, as is the classical entailment problem.
Proposition 4 Let  be a fuzzy KB and let  be a fuzzy assertion. Determining whether
| is a PSPACE-complete problem.
a
Proof: We have seen that termination of the above algorithm is guaranteed. Additionally,
for a crisp KB K, define K = {h  1i :   K}. By definition, each   K is KBnormalised and h  0.5i is query-normalised. Then from Proposition 2 it follows that
K |=  iff K |h  0.5i. From the PSPACE-completeness of the entailment problem in
crisp ALC (Schmidt-Schau & Smolka, 1991), PSPACE-hardness of the fuzzy entailment
problem follows. Unfortunately, our algorithm, as it is, requires exponential space due a
well know problem inherited from the crisp case. Indeed, it easily verified that a completion
of S = {x:C}, where C is the concept
(R.A11 ) u (R.A12 ) u R.((R.A21 ) u (R.A22 ) u . . . R.((R.An1 ) u (R.An2 )) . . .)
contains at least 2n + 1 variables. In order to require polynomial space, Schmidt-Schau
and Smolka (1991) introduced the so-called trace rule (T ) for the  operator. The (T )
rule modifies the () rule as shown below.
156

fiReasoning within Fuzzy DLs

()

(T )

w:R.C  (w, x):R, x:C
if x new variable and there is no w0 such that both (w, w0 ):R and w0 :C
are already in the actual constraint set
w:R.C  (w, x):R, x:C
if x new variable and no (w, w0 ):R0 is already in the actual set of constraints.

The difference between the () rule and the (T ) is that the latter is applied only once
for an object w. We are thus compelled to make a nondeterministic choice amongst the
constraints of the form w:R.C. Furthermore, it is convenient to apply a trace rule only if
none of the other (u), (t), () and () rules are applicable.
We say that a set of constraints S 0 is a trace of a set S if S 0 obtained from S by
application of the rules where the () has been replaced by (T ). Schmidt-Schau and
Smolka (1991) have shown that a set of constraints S = {x:C} is satisfiable iff no trace S 0
of S contains a clash. As the size of a trace S 0 of S is bounded polynomially by the size of
S, polynomial space is sufficient to prove satisfiability.
The above trace rule works if we start from a constraint set of the form {x:C}. In
the general case, we have to rely on so-called pre-completions (Baader & Hollunder, 1991b;
Donini et al., 1994). A set of constraints S 0 is said to be a pre-completion of a given set
of constraints S, if it is obtained from S by the application of the (u), (t), () and ()
rules, and none of these rules is applicable to S 0 (the size of S 0 is polynomially bounded
by the size of S). As a consequence of this pre-processing step, all role relationships
(w, w0 ):R  S 0 can be ignored, i.e. removed from S 0 , because they no longer carry any
additional information. Now, in a second step we can apply the method above by checking
whether no trace from S 0 contains a clash. In summary, a set of constraints S is satisfiable
iff there is a pre-completion S 0 of S such that no trace S 00 of S 0 contains a clash.
In the fuzzy case, similar trace rules can easily be defined. For instance, the correspondent trace rule of the ( ) rule is
(T  )

hw:R.C  ni  h(w, x):R  ni, hx:C  ni
if x new variable and no h(w, w0 ):R0  mi is already in
the actual set of fuzzy constraints.

The trace rules correspondent to the rules (> ), ( ) and (< ) are defined similarly. By
proceeding as for the crisp case, it can be shown that (i) a set of fuzzy constraints S
is satisfiable iff there is a pre-completion S 0 of S such that no trace S 00 of S 0 contains a
clash; and (ii) the size of a trace S 00 of S is bounded polynomially by the size of S. As a
consequence, the satisfiability problem is in PSPACE, which completes the proof. 2
This result establishes an important property about our fuzzy DL. In effect, it says that no
additional computational cost has to be paid for the major expressive power.
5.2 A Decision Procedure for the Subsumption Problem
In this section we address the subsumption problem, i.e. deciding whether C T D, where
C and D are two concepts and  is a fuzzy terminology. As we have seen (see Example 4),
157

fiStraccia

C T D can be reduced to the case of an empty terminology by applying the KB expansion
process. So, without loss of generality, we can limit our attention to the case C  D.
At first, an analogue to relation (12) holds. In fact, it can easily be shown that
Proposition 5 Let C and D be two concepts. It follows that C  D iff for all n > 0
ha:C  ni|ha:D  ni, where a is a new individual.
a
Proof:
 .) Assume that C  D holds. Suppose to the contrary that n > 0 such that
ha:C  ni |
6 ha:D  ni. Therefore, there is an interpretation I and an n > 0 such that
C I (aI )  n and DI (aI ) < n. But, from the hypothesis n  C I (aI )  DI (aI ) < n follows.
Absurd.
 .) Assume that for all n > 0, ha:C  ni|ha:D  ni holds. Suppose to the contrary
that C 6 D holds. Therefore, there is an interpretation I and d  I such that C I (d) >
DI (d)  0. Let us extent I to a such that aI = d and consider n = C I (d) > 0. Of course,
I satisfies ha:C  ni. Therefore, from the hypothesis it follows that I satisfies ha:D  ni,
i.e. DI (d)  n = C I (d) > DI (d). Absurd. 2
How can we check whether for all n > 0 ha:C  ni|ha:D  ni holds? A solution to this
problem, restricted to the propositional case, is given by Straccia (2000a). Indeed, it is
shown that
Proposition 6 (Straccia, 2000a) Let p and q be two propositions, 0 < n1  0.5 and
1  n2 > 0.5. It follows that for all n > 0, hp  ni|hq  ni iff for both m  {n1 , n2 },
hp  mi|hq  mi holds.
a
The above proposition establishes that, at the propositional level, in order to check whether
hp  ni|hq  ni holds for all n, it is sufficient to check the entailment relation with respect
to two values n1 , n2 . The first being less or equal than 0.5, while the second being greater
than 0.5, respectively. This is due to the fact that for given values n, n0  0.5, any proof of
hp  ni|hq  ni can be converted into a proof for hp  n0 i|hq  n0 i and vice-versa. The
case where n, n0 > 0.5 is similar.
The above proposition can be extended to our fuzzy DL as well.
Lemma 1 Let C and D be two concepts, 0 < n, n0  0.5 and let a be an individual. It
follows that ha:C  ni|ha:D  ni iff ha:C  n0 i|ha:D  n0 i.
a
Proof:
It is enough to show that S = {ha:C  ni, ha:D < ni} is satisfiable iff S 0 =
{ha:C  n0 i, ha:D < n0 i} is satisfiable.
 .) Assume that S is satisfiable. So, there is a clash-free completion S of S. With
S[n/n0 ] we indicate the set of fuzzy constraints obtained from S, by replacing any value n
in S with n0 and any value 1  n in S with 1  n0 , respectively. We will show that S[n/n0 ]
is a clash-free completion of S 0 and, thus, S 0 is satisfiable.
Let r1 , . . . , rk , k  0 be the sequence of inference rule applications, which applied to S
get S. Let S0 = S, let Sk = S and for 1  i  k let Si be the set of fuzzy constraints
obtained from Si1 by the application of the ri rule to Si1 .
By induction on k, we show that (i) the sequence of inference rules r1 , . . . , rk can be
applied to S 0 as well; (ii) for S 0 0 = S 0 , S 0 k = S 0 and S 0 i the set of fuzzy constraints obtained
158

fiReasoning within Fuzzy DLs

from S 0 i1 by the application of the ri rule to S 0 i1 , we have that S 0 i = Si [n/n0 ] and, thus,
S 0 = S 0 k = Sk [n/n0 ] = S[n/n0 ]; and (iii) if Sk is a clash-free completion of S then S 0 k is a
clash-free completion of S 0 as well.
case k = 0: No rule is applicable to S and S0 = S is a completion of S. By case analysis, (i) it is easily verified that no rule is applicable to S 0 and (ii) S 0 = S 0 0 =
S0 [n/n0 ] = S[n/n0 ]. (iii) So, S 0 0 is a completion of S 0 . We show that S 0 0 is clashfree. Assume to the contrary that S 0 0 , i.e. S 0 contains a clash. As a consequence,
S 0 = {ha:C  n0 i, ha:C < n0 i}. But then, S is {ha:C  ni, ha:C < ni}, contrary to the
assumption that S is clash-free.
induction step: by case analysis on the rule rk . We limit our presentation to the ( )
rule as for the other the proof is similar.
If rk is ( ) then it can be verified that there are hw:R.C 0  ni and h(w, w0 ):R > 1  ni
in Sk1 such that hw0 :C 0  ni 6 Sk1 and hw0 :C 0  ni  Sk . By induction, Sk1 [n/n0 ] =
S 0 k1 and, thus, both hw:R.C 0  n0 i and h(w, w0 ):R > 1  n0 i are in S 0 k1 , while
hw0 :C 0  n0 i 6 S 0 k1 . Therefore, (i) rule rk is applicable to S 0 k1 and hw0 :C 0  n0 i 
S 0 k ; (ii) so, S 0 k = Sk [n/n0 ]; (iii) from S 0 k = Sk [n/n0 ] and, as Sk is a completion
of S, by case analysis, it is easily verified that no rule is further applicable to S 0 k .
Therefore, S 0 k is a completion of S 0 . Let us show that S 0 k is clash free. Assume to
the contrary that S 0 k contains a clash. If one of the cases in Table 1 holds, then from
S 0 k = Sk [n/n0 ] it follows easily that there is a clash in Sk as well, which is contrary to
assumption that Sk is clash-free. On the other hand, if there is a conjugated pair of
fuzzy constraints in S 0 k (see Table 2), then one of the following three pairs is in S 0 k :
(a) h  n0 i and h < n0 i; (b) h  1  n0 i and h > 1  n0 i; and (c) h < n0 i and
h > 1  n0 i (note that n, n0  0.5). Again, as S 0 k = Sk [n/n0 ], it follows that there is
a conjugated pair in Sk as well, which is contrary to assumption that Sk is clash-free.
 .) Can be proven similarly to  .). 2
By proceeding as for Lemma 1 it can be shown that
Lemma 2 Let C and D be two concepts, 1  n, n0 > 0.5 and let a be an individual. It
follows that ha:C  ni|ha:D  ni iff ha:C  n0 i|ha:D  n0 i.
a
From Lemma 1 and Lemma 2 it follows that
Proposition 7 Let C and D be two concepts, 0 < n1  0.5, 1  n2 > 0.5 and let a be
an individual. It follows that for all n > 0 ha:C  ni|ha:D  ni iff for both m  {n1 , n2 },
ha:C  mi|ha:D  mi holds.
a
As a consequence, the subsumption problem can be reduced to the entailment problem for
which we have a decision algorithm.
159

fiStraccia

5.3 A Decision Procedure for the BTVB Problem
We address now the problem of determining glb(, ) and lub(, ). This is important, as
computing, e.g. glb(, ), is in fact the way to answer a query of type to which degree is
 (at least) true, given the (imprecise) facts in ?.
Without loss of generality, we will assume that all concepts are in NNF (Negation Normal
Form). Straccia (2000b) has shown that, in case of fuzzy propositional logic, from a set  of
fuzzy propositions of the form hp  ni and hp  ni, where p is a proposition, it is possible
to determine a finite set N   [0, 1], where |N  | is O(||), such that glb(, q)  N  ,
i.e. the greatest lower bound of a proposition q w.r.t.  has to be an element of N  .
Therefore, glb(, q) can be determined by computing the greatest value n  N  such that
|hq  ni. An easy way to search for this n is to order the elements of N  and then
to perform a binary search among these values by successive entailment tests. Dually,
as lub(, q) = 1  glb(, q) holds, the lub can either be computed from the glb or, as
lub(, q)  1  N  , where 1  N  = {1  n : n  N  }, we can compute it by determining
the smallest value in 1  N  .

Proposition 8 (Straccia, 2000b) Let  be a set of fuzzy propositions in NNF and let q
be a proposition. Then glb(, q)  N  and lub(, q)  1  N  , where
N

= {0, 0.5, 1} 
{n : hp  ni  } 
{1  n : hp  ni  }

1  N  = {1  n : n  N  }.
a
The above Proposition 8 can easily be extended our fuzzy description logic case. Essentially,
the quantifiers do not change the possible values of glb(, F ) and lub(, F ).
Proposition 9 Let  be a set of fuzzy assertions in NNF and let  be an assertion. Then
glb(, )  N  and lub(, )  1  N  , where
N

= {0, 0.5, 1} 
{n : h  ni  } 
{1  n : h  ni  }

1  N  = {1  n : n  N  }.
a
Proof: Let us show that glb(, )  N  . Let m be glb(, ). By definition, if m = 0 then
S =   {h < ni} is satisfiable for any n > 0 and 0  N  . Otherwise, m > 0 is the largest
value such that S =   {h < mi} is not satisfiable. Let us mark each sub-expression in 
with a  , so that we can trace the components of the query assertion  during a deduction.
160

fiReasoning within Fuzzy DLs

Consider a completion S 0 of S. Starting from h < mi, by applying the rules of inference,
only  marked expressions of type h0 < mi or h0 > 1  mi can appear in S 0 . Furthermore,
as S is not satisfiable, S 0 contains a clash, i.e. the value m is the largest value such that all
completions S 0 of S contain a clash. Let us analyse S 0 . As S 0 contains a clash, then either
(i) there is a clash according to Table 1, or (ii) there is a clash according to Table 2. If
(i) is the case, i.e. there is   S 0 which is a clash, then we have to distinguish between
two cases: (a)  is not marked with  and (b)  is marked with  . In the former case, S 0
contains a clash independently of the value m and, thus, the largest possible value m for
which S 0 contains a clash is 1. In the latter case, as m > 0, either  = hw: > 1  mi or
 = hw:> < mi which are both clashes for any value of m. As a consequence, the largest
possible value m for which S 0 contains a clash according to Table 1 is 1. Assume (ii) is the
case, i.e. a conjugated pair of fuzzy constraints  and  0 is in S 0 . Similarly to the previous
case, we have to distinguish the cases for which  and  0 are marked with  . There are four
cases:
(a)  = h  ki and  0 = h0  k 0 i are in S 0 , none is marked with  and k > k 0 .
Therefore, S 0 contains a clash for any value of m and, thus, the largest choice is 1;
(b)  = h  ki and  0 = h0 < mi are in S 0 , only  0 is marked with  and k  m.
Therefore, S 0 contains a clash for any value of m  k and, thus, the largest choice for m is
k. It is easily verified by case analysis on the rules that from   S 0 , k  N  follows;
(c)  = h  ki and  0 = h0 > 1  mi are in S 0 , only  0 is marked with  and k  1m.
Therefore, S 0 contains a clash for any value of m  1  k and, thus, the largest choice for
m is 1  k. It is easily verified by case analysis on the rules that from   S 0 , 1  k  N 
follows;
(d)  = h < mi and  0 = h0 > 1  mi are in S 0 , both are marked with  and m  1m.
Therefore, S 0 contains a clash for any value of m  0.5 and, thus, the largest choice for m
is 0.5.
Summing up, we have proved that the largest possible value for m is such that m 
2

N .

The algorithms computing glb(, ) and lub(, ) are described in Table 3. For instance,
by a binary search on N  , the value of glb(, ) can be determined in at most log |N  |
fuzzy entailment tests.

6. Conclusions and Future Work
In this work, we have presented a quite general fuzzy extension of the DL ALC, a significant
and expressive representative of the various DLs. Our fuzzy DL enables us to reason in
presence of imprecise ALC concepts, i.e. fuzzy ALC concepts. From a semantics point of
view, fuzzy concepts are interpreted as fuzzy sets i.e. given a concept C and an individual
a, C(a) is interpreted as the truth-value of the sentence a is C. From a syntax point of
view, we allow to specify lower and upper bounds of the truth-value of C(a). Complete
algorithms for reasoning in it have been presented, that is, we have devised algorithms for
solving the entailment problem, the subsumption problem as well as the best truth-value
bound problem.
161

fiStraccia

Algorithm M ax(, )
Set M in := 0 and M ax := 2.
1. Pick n  N  \ {0} such that M in < n < M ax. If there is no such n, then
set glb(, ) := M in and exit.
2. If |h  ni then set M in = n, else set M ax = n. Go to Step 1.
Algorithm M in(, )
Set M in := 0 and M ax := 2.
1. Pick n  (1  N  ) \ {0} such that M in < n < M ax. If there is no such n,
then set lub(, ) := min{M ax, 1} and exit.
2. If |h  ni then set M ax = n, else set M in = n. Go to Step 1.

Table 3: Algorithms M ax(, ) and M in(, )

An important point concerns computational complexity. The complexity result shows
that the additional expressive power has no impact from a computational complexity point
of view.
The extension of DLs to the management of vagueness is not new (Tresp & Molitor,
1998; Yen, 1991). Yen was the first, to the best of my knowledge, introducing vagueness
into a simple DL. His language has two interesting points not included into our language.
Firstly, it allows the definition of vague concepts by means of explicit membership functions
over a domain, e.g. LowPressure: domain(AirPressure); membershipf x(p.low(p)). Here
the domain over p ranges is given by AirPressure. p.low(p) determines the membership
degree of being a pressure p low. Secondly, the language allows concept modifiers, like Very
or Slightly, by means of which concepts like very low pressure can be defined through
VeryLowPressure: Very(LowPressure). This last idea has been generalised to ALC by Tresp
and Molitor (1998) where a certain type of concept modifiers are allowed. Strictly speaking, the language defined by Tresp and Molitor is more expressive, as we do not consider
concept modifiers. From a semantics point of view, the extension to Tresp and Molitors
language is quite straightforward. But, the cost that we have to pay for this increasing
expressive power is that, from a computational complexity and algorithms point of view,
things changes radically. Indeed, according to Tresp and Molitor for each completion a
linear optimisation problem is generated (set of inequations of the form op1 (t1 )Rop2 (t2 ) or
op1 (t1 )Rf (op2 (t2 )), where ti is a truth-value variable, R  {, , =, opi  {id, mi }, with
id(t) the identity and mi (t) is a modifier function over truth-value variables, respectively,
and f  {min, max}-derived from the semantics of the connectors u, t) and then solved for
the best value. Then, the minimum among all computed solutions is taken. The solutions
can be computed by relying on methods from the domain of linear programming, e.g. the
162

fiReasoning within Fuzzy DLs

simplex method (Papadimitriou & Steiglitz, 1982). While it is possible to devise a similar
approach for our fuzzy DL as well, we have seen that this is not necessary.
Both aspects considered by Tresp and Molitor and by Yen, although interesting, are
not crucial w.r.t. how we model logic-based multimedia information retrieval, where underlaying text, image and video analysis tools provide us fuzzy assertions e.g. of the form
ha:About.C  ni from which we may infer ha:About.D  mi.
This work can be used as a basis both for extending existing DL based systems and for
further research. In this latter case, there are several open points. For instance, it is not clear
yet how to reason both in case of fuzzy specialisation of the general form CD and in the
case cycles are allowed in a fuzzy KB. Another interesting topic for further research concerns
the semantics of fuzzy connectives. Of course several other choices for the semantics of the
connectives u, t, , ,  can be considered. While for a huge number of proposals given in
the literature their impact from a semantics point of view is well understood, the question
how they impact from a computational complexity and algorithms point of view remains
still open.

Acknowledgements
I would like to thank the three anonymous reviewers for their helpful comments on an early
version of this paper. This is an extension and revision of the paper appeared in AAAI-98.

References
Baader, F., & Hollunder, B. (1991a). KRIS: Knowledge representation and inference system,
system description. ACM SIGART Bulletin, 2, 814.
Baader, F., & Hollunder, B. (1991b). A terminological knowledge representation system
with complete inference algorithm. In Proc. of the Workshop on Processing Declarative
Knowledge, PDK-91, No. 567 in Lecture Notes in Artificial Intelligence, pp. 6786.
Springer-Verlag.
Bacchus, F. (1990). Representing and Reasoning with Probabilistic Knowledge. The MIT
Press.
Bellman, R., & Giertz, M. (1973). On the analytic formalism of the theory of fuzzy sets.
Information Sciences, 5, 149156.
Brachman, R. J. (1992). reducing CLASSIC to practice: Knowledge representation meets
reality. In Proc. of the 3rd Int. Conf. on the Principles of Knowledge Representation
and Reasoning (KR-92), pp. 247258. Morgan Kaufmann, Los Altos.
Buchheit, M., Donini, F. M., & Schaerf, A. (1993a). Decidable reasoning in terminological
knowledge representation systems. In Proc. of the 13th Int. Joint Conf. on Artificial Intelligence (IJCAI-93), pp. 704709 Chambery, France. Morgan Kaufmann, Los
Altos.
163

fiStraccia

Buchheit, M., Donini, F. M., & Schaerf, A. (1993b). Decidable reasoning in terminological
knowledge representation systems. Journal of Artificial Intelligence Research, 1, 109
138.
Chen, J., & Kundu, S. (1996). A sound and complete fuzzy logic system using Zadehs
implication operator. In Ras, Z. W., & Maciek, M. (Eds.), Proc. of the 9th Int. Sym.
on Methodologies for Intelligent Systems (ISMIS-96), No. 1079 in Lecture Notes in
Artificial Intelligence, pp. 233242. Springer-Verlag.
da Silva, R. M., Pereira, A. E. C., & Netto, M. A. (1994). A system of knowledge representation based on formulae of predicate calculus whose variables are annotated
by expressions of a fuzzy terminological logic. In Proc. of the 5th Int. Conf. on Information Processing and Managment of Uncertainty in Knowledge-Based Systems,
(IPMU-94), No. 945 in Lecture Notes in Computer Science. Springer-Verlag.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1994). Deduction in concept languages: From subsumption to instance checking. Journal of Logic and Computation,
4 (4), 423452.
Dubois, D., & Prade, H. (1980). Fuzzy Sets and Systems. Academic Press, New York, NJ.
Dubois, D., & Prade, H. (1994). Can we enforce full compositionality in uncertainty calculi?.
In Proc. of the 11th Nat. Conf. on Artificial Intelligence (AAAI-94), pp. 149154
Seattle, Washington.
Dubois, D., & Prade, H. (1996). Approximate and commonsense reasoning: From theory to
practice. In Ras, Z. W., & Maciek, M. (Eds.), Proc. of the 9th Int. Sym. on Methodologies for Intelligent Systems (ISMIS-96), No. 1079 in Lecture Notes in Artificial
Intelligence, pp. 1933. Springer-Verlag.
Halpern, J. Y. (1990). An analysis of first-order logics of probability. Artificial Intelligence
Journal, 46, 311350.
Heinsohn, J. (1994). Probabilistic description logics. In de Mantara, R. L., & Pool, D.
(Eds.), Proceedings of the 10th Conference on Uncertainty in Artificila Intelligence,
pp. 311318.
Hollunder, B. (1994). An alternative proof method for possibilistic logic and its application to terminological logics. In 10th Annual Conference on Uncertainty in Artificial
Intelligence Seattle, Washington. R. Lopez de Mantaras and D. Pool.
Horrocks, I. (1998). Using an expressive description logic: Fact or fiction?. In Proc. of the
8th Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR-98).
Jager, M. (1994). Probabilistic reasoning in terminological logics. In Proceedings of KR94, 5-th International Conference on Principles of Knowledge Representation and
Reasoning, pp. 305316 Bonn, FRG.
164

fiReasoning within Fuzzy DLs

Koller, D., Levy, A., & Pfeffer, A. (1997). P-CLASSIC: A tractable probabilistic description
logic. In Proc. of the 14th Nat. Conf. on Artificial Intelligence (AAAI-97), pp. 390
397.
Kruse, R., Schwecke, E., & Heinsohn, J. (1991). Uncertainty and Vagueness in Knowledge
Based Systems. Springer-Verlag, Berlin, Germany.
Lee, R. C. T. (1972). Fuzzy logic and the resolution principle. Journal of the ACM, 19 (1),
109119.
Meghini, C., Sebastiani, F., & Straccia, U. (1997). Modelling the retrieval of structured
documents containing texts and images. In Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries, No. 1324 in Lecture
Notes in Computer Science, pp. 325344 Pisa, Italy.
Meghini, C., Sebastiani, F., & Straccia, U. (1998). Mirlog: A logic for multimedia information retrieval. In Crestani, F., Lalmas, M., & van Rijsbergen, C. (Eds.), Logic and
Uncertainty in Information Retrieval: Advanced models for the representation and
retrieval of information, Vol. 4 of The Kluwer International Series On Information
Retrieval, chap. 7, pp. 151185. Kluwer Academic Publishers, Boston, USA.
Meghini, C., & Straccia, U. (1996). A relevance terminological logic for information retrieval.
In Proceedings of the 19th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (ACM SIGIR-96), pp. 197205 Zurich,
Switzerland.
Nebel, B. (1988). Computational complexity of terminological reasoning in BACK. Artificial
Intelligence, 34, 371383.
Nebel, B. (1990). Reasoning and revision in hybrid representation systems. Springer, Heidelberg, FRG.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization: Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs, New Jersey.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, Los Altos.
Peltason, C. (1991). The BACK system  an overview. SIGART Bulletin, 2 (3), 114119.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence Journal,
82, 273302.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Artificial Intelligence, 48, 126.
Sebastiani, F. (1994). A probabilistic terminological logic for modelling information retrieval. In Proceedings of SIGIR-94, 17th ACM International Conference on Research
and Development in Information Retrieval, pp. 122130 Dublin, IRL. Published by
Springer Verlag, Heidelberg, FRG.
165

fiStraccia

Straccia, U. (1998). A fuzzy description logic. In Proc. of the 15th Nat. Conf. on Artificial
Intelligence (AAAI-98), pp. 594599 Madison, USA.
Straccia, U. (2000a). A note on the relationship between fuzzy logic and four-valued logic.
Technical report B4-22-10-2000, Istituto di Elaborazione dellInformazione, Consiglio
Nazionale delle Ricerche, Pisa, Italy.
Straccia, U. (2000b). Reasoning and experimenting within Zadehs fuzzy propositional logic.
Technical report 2000-b4-011, Istituto di Elaborazione dellInformazione, Consiglio
Nazionale delle Ricerche, Pisa, Italy.
Tresp, C., & Molitor, R. (1998). A description logic for vague knowledge. In Proc. of the
13th European Conf. on Artificial Intelligence (ECAI-98) Brighton (England).
Xiachun, C., Yunfei, J., & Xuhua, L. (1995). The rationality and decidability of fuzzy
implications. In Proc. of the 14th Int. Joint Conf. on Artificial Intelligence (IJCAI95), pp. 19101911 Montreal, Canada. MK.
Yen, J. (1991). Generalizing term subsumption languages to fuzzy logic. In Proc. of the 12th
Int. Joint Conf. on Artificial Intelligence (IJCAI-91), pp. 472477 Sydney, Australia.
Zadeh, L. A. (1965). Fuzzy sets. Information and Control, 8 (3), 338353.

166

fiJournal of Artificial Intelligence Research 14 (2001) 359{389

Submitted 10/00; published 6/01

Conditional Plausibility Measures and Bayesian Networks
Joseph Y. Halpern

Cornell University, Computer Science Department
Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

halpern@cs.cornell.edu

Abstract

A general notion of algebraic conditional plausibility measures is defined. Probability
measures, ranking functions, possibility measures, and (under the appropriate definitions)
sets of probability measures can all be viewed as defining algebraic conditional plausibility
measures. It is shown that algebraic conditional plausibility measures can be represented
using Bayesian networks.

1. Introduction
Pearl (1988) among others has long argued that Bayesian networks (that is, the dags without the conditional probability tables) represent important qualitative information about
uncertainty regarding conditional dependencies and independencies. To the extent that this
is true, Bayesian networks should make perfect sense for non-probabilistic representations
of uncertainty. And, indeed, Bayesian networks have been used with  rankings (Spohn,
1988) by Darwiche and Goldszmidt (1994). It follows from results of Wilson (1994) that
possibility measures (Dubois & Prade, 1990) can be represented using Bayesian networks.
The question I address in this paper is \What properties of a representation of uncertainty are required to be able to represent the uncertainty using a Bayesian network?" This
question too has been addressed in earlier work, see (Darwiche, 1992; Darwiche & Ginsberg,
1992; Friedman & Halpern, 1995; Wilson, 1994), although the characterization given here
is somewhat different. Shenoy and Shafer (1990) consider a related question|essentially,
what is required of a representation of uncertainty so that marginals can be computed using
\local computations" of the type used in Bayesian networks|and provide axioms sucient
to guarantee that this is possible.
Here I represent uncertainty using plausibility measures, as in (Friedman & Halpern,
1995). To answer the question, I must examine general properties of conditional plausibility
as well as defining a notion of plausibilistic independence. Unlike earlier papers, I enforce a
symmetry condition in the definition of conditional independence, so that, for example, A is
independent of B iff B is independent of A. While this property holds for probability, under
the asymmetric definition of independence used in earlier work it does not necessarily hold
for other formalisms. There are also subtle but important differences between this paper
and (Friedman & Halpern, 1995) in the notion of conditional plausibility. The definitions
here are simpler but more general; particular attention is paid here to conditions on when
the conditional plausibility must be defined.
The major results here are a general condition, simpler than that given in (Friedman &
Halpern, 1995; Wilson, 1994), under which a conditional plausibility measure satisfies the
c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHalpern

semi-graphoid properties (which means it can be represented using a Bayesian network).
Conditions are also given that suce for a Bayesian network to be able to quantitatively
represent a plausibility measure; more precisely, conditions are given so that a plausibility
measure can be uniquely reconstructed given conditional plausibility tables for each node in
the Bayesian network. Conditions for quantitative representation by Bayesian networks do
not seem to have been presented in the literature for representations of uncertainty other
than probability (for which the conditions are trivial). A minor additional condition also
suces to guarantee that d-separation in the network characterizes conditional independence. All these conditions clearly apply to  rankings and possibility measures. Perhaps
more interestingly, they also apply to sets of probabilities under a novel representation of
such sets as a plausibility measure. This novel representation (and the associated notion of
conditioning) is shown to have some natural properties not shared by other representations.
The rest of the paper is organized as follows. In Section 2, I discuss conditional plausibility measures. Section 3 introduces algebraic conditional plausibility measures, which
are ones where there is essentially an analogue to + and . (Putting such an algebraic
structure on uncertainty is not new; it was also done in (Darwiche, 1992; Darwiche & Ginsberg, 1992; Friedman & Halpern, 1995; Weydert, 1994).) Section 4 discusses independence
and conditional independence in conditional plausibility spaces, and shows that algebraic
conditional plausibility measures satisfy the semi-graphoid properties. Finally, in Section 5,
Bayesian networks based on (algebraic) plausibility measures are considered. Combining
the fact that algebraic plausibility measures satisfy the semi-graphoid properties with the
results of (Geiger, Verma, & Pearl, 1990), it follows that d-separation in a Bayesian network G implies conditional independence for all algebraic plausibility measures compatible
with G; a weak richness condition is shown to yield the converse. The paper concludes in
Section 6. Longer proofs are relegated to the appendix.

2. Conditional Plausibility

2.1 Unconditional Plausibility Measures

Before getting to conditional plausibility measures, it is perhaps best to consider unconditional plausibility measures. The basic idea behind plausibility measures is straightforward.
A probability measure maps subsets of a set W to [0; 1]. Its domain may not consist of
all subsets of W ; however, it is required to be an algebra. (Recall that an algebra F over
W is a set of subsets of W containing W and closed under union and complementation,
so that if U; V 2 F , then so are U [ V and U .) A plausibility measure is more general; it
maps elements in an algebra F to some arbitrary partially ordered set. If Pl is a plausibility
measure, then we read Pl(U ) as \the plausibility of set U ". If Pl(U )  Pl(V ), then V is at
least as plausible as U . Because the ordering is partial, it could be that the plausibility of
two different sets is incomparable. An agent may not be prepared to say of two sets that
one is more likely than another or that they are equal in likelihood.
Formally, a plausibility space is a tuple S = (W; F ; Pl), where W is a set of worlds, F
is an algebra over W , and Pl maps sets in F to some set D of plausibility values partially
ordered by a relation D (so that D is reexive, transitive, and anti-symmetric) that
contains two special elements >D and ?D such that ?D D d D >D for all d 2 D; these
are intended to be the analogues of 1 and 0 for probability. As usual, the ordering is defined
360

fiConditional Plausibility Measures and Bayesian Networks

<D by taking d1 <D d2 if d1 D d2 and d1 6= d2 . I omit the subscript D from D , <D , >D
and ?D whenever it is clear from context.
There are three requirements on plausibility measures. The first two are obvious analogues of requirements that hold for other notions of uncertainty: the whole space gets
the maximum plausibility and the empty set gets the minimum plausibility. The third
requirement says that a set must be at least as plausible as any of its subsets.
Pl1. Pl(;) = ?D .
Pl2. Pl(W ) = >D .
Pl3. If U  U 0 , then Pl(U )  Pl(U 0 ).
(In Pl3, I am implicitly assuming that U; U 0 2 F . Similar assumptions are made throughout.)
All the standard representations of uncertainty in the literature can be represented as
plausibility measures. I briey describe some other representations of uncertainty that will
be of relevance to this paper.
Sets of probabilities: One common way of representing uncertainty is by a set of probability measures. This set is often assumed to be convex (see, for example, (Campos & Moral,
1995; Cousa, Moral, & Walley, 1999; Gilboa & Schmeidler, 1993; Levi, 1985; Walley, 1991)
for discussion and further references), however, convex sets do not seem appropriate for representing independence assumptions, so I do not make this restriction here. For example,
if a coin with an unknown probability of heads is tossed twice, and the tosses are known to
be independent, it seems that a reasonable representation is given by the set P0 consisting
of all measures ff , where ff (hh) = ff2 ; ff (ht) = ff (th) = ff(1 , ff); ff (tt) = (1 , ff)2 .
Unfortunately, P0 is not convex. Moreover, its convex hull includes many measures for
which the coin tosses are not independent. It is argued in (Cousa et al., 1999) that a set
of probability measures is behaviorally equivalent to its convex hull. However, even if we
accept this argument, it does not follow that a set and its convex hull are equivalent insofar
as determination of independencies goes.
There are a number of ways of viewing a set P of probability measures as a plausibility
measure. One uses the lower probability P , defined as P (U ) = inf f(U ) :  2 Pg. Clearly
P satisfies Pl1{3. The corresponding upper probability P  , defined as P  (U ) = supf :  2
Pg = 1 , P (U ), is also clearly a plausibility measure.
Both P and P  give a way of comparing the likelihood of two subsets U and V of W .
These two ways are incomparable; it is easy to find a set P of probability measures on W
and subsets U and V of W such that P (U ) < P (V ) and P  (U ) > P  (V ). Rather than
choosing between P and P  , we can associate a different plausibility measure with P that
captures both. Let DP ;P  = f(a; b) : 0  a  b  1g and define (a; b)  (a0 ; b0 ) iff b  a0 .
This puts a partial order on DP ;P  ; clearly ?DP P  = (0; 0) and >DP P  = (1; 1). Define
PlP ;P  (U ) = (P (U ); P  (U )). Thus, PlP ;P  associates with a set U two numbers that can
be thought of as defining an interval in terms of the lower and upper probability of U . It
is easy to check that PlP ;P  (U )  PlP ;P  (V ) if the upper probability of U is less than or
equal to the lower probability of V . PlP ;P  satisfies Pl1{3, so it is indeed a plausibility
measure, but one which puts only a partial order on events.
;

361

;

fiHalpern

The trouble with P , P  , and even PlP ;P  is that they lose information. For example,
it is not hard to find a set P of probability measures and subsets U; V of W such that
(U )  (V ) for all  2 P and (U ) < (V ) for some  2 P , but P (U ) = P (V ) and
P  (U ) = P  (V ). Indeed, there exists an infinite set P of probability measures such that
(U ) < (V ) for all  2 P but P (U ) = P (V ) and P  (U ) = P  (V ). If all the probability
measures in P agree that U is less likely than V , it seems reasonable to conclude that U is
less likely than V . However, none of P , P  , or PlP ;P  will necessarily draw this conclusion.
Fortunately, it is not hard to associate yet another plausibility measure with P that
does not lose this important information. For technical convenience that will become clear
later, assume that there is some index set I such that P = fi : i 2 I g. Thus, for example,
if P = f1 ; : : : ; n g, then I = f1; : : : ; ng. Let DI = [0; 1]I , that is, the functions from I to
[0; 1], with the pointwise ordering, so that f  g iff f (i)  g(i) for all i 2 I .1 It is easy to
check that ?D is the function f : I ! [0; 1] such that f (i) = 0 for all i 2 I and >D is the
function g such that g(i) = 1 for all i 2 I . For U  W , let fU be the function such that
fU (i) = i (U ) for all i 2 I . For example, for the set P0 of measures representing the two
coin tosses (which is indexed by IR), the set W can be taken to be fhh; ht; tt; thg. Then,
for example, ffhhg (ff) = ff (hh) = ff2 and ffht;ttg (ff) = 1 , ff.
It is easy to see that f; = ?D and fW = >D . Now define PlP (U ) = fU . Thus,
PlP (U )  PlP (V ) iff fU (i)  fV (i) for all i 2 I iff (U )  (V ) for all  2 P . Clearly PlP
satisfies Pl1{3. Pl1 and Pl2 follow since PlP (;) = f; = ?D and PlP (W ) = fW = >D ,
while Pl3 follows since if U  V then (U )  (V ) for all  2 P . PlP captures all the
information in P (unlike, say, P , which washes much of it away by taking infs).
This way of associating a plausibility measure with a set P of probability measures
generalizes: it provides a way of associating a single plausibility measure with any set of
plausibility measures; I leave the straightforward details to the reader.
Possibility measures: A fuzzy measure (or a Sugeno measure ) f on W (Wang & Klir,
1992) is a function f : 2W 7! [0; 1], that satisfies Pl1{3. (That is, it is less general than a
plausibility measure only in that it requires the range to be [0; 1] rather than an arbitrary
partially ordered set.) A possibility measure Poss on W is a special case of a Sugeno measure;
it is a function mapping subsets of W to [0; 1] such that Poss(W ) = 1, Poss(;) = 0, and
Poss(U ) = supw2U (Poss(fwg)), so that Poss(U [ V ) = max(Poss(U ); Poss(V )) (Dubois &
Prade, 1990). Clearly a possibility measure is a plausibility measure.
Ranking functions: An ordinal ranking (or -ranking or ranking function )  on W (as
defined by (Goldszmidt & Pearl, 1992), based on ideas that go back to (Spohn, 1988))
is a function mapping subsets of W to IN  = IN [ f1g such that (W ) = 0, (;) =
1, and (U ) = minw2U ((fwg)), so that (U [ V ) = min((U ); (V )). Intuitively, a
ranking function assigns a degree of surprise to each subset of worlds in W , where 0 means
unsurprising and higher numbers denote greater surprise. It is easy to see that if  is a
ranking function on W , then (W; 2W ; ) is a plausibility space, where x IN  y if and only
if y  x under the usual ordering on the natural numbers. One standard view of a ranking
I

I

I

I

I

I

1. In the conference version of this paper (Halpern, 2000), D , the range of the plausibility measure, was
taken to be functions from to [0; 1], not from the index set I to [0; 1]. The difference is mainly cosmetic,
but this representation makes the range independent of , so that the same plausibility values can be
used for any set of probability measures indexed by I .
I

P

P

362

fiConditional Plausibility Measures and Bayesian Networks

function, going back to Spohn, is that a ranking of k can be associated with a probability
of k , for some fixed (possibly infinitesimal) . Note that this viewpoint justifies taking
(W ) = 0, (;) = 1, and (U [ V ) = min((U ); (V )).

2.2 Conditional Plausibility Measures

Since Bayesian networks make such heavy use of conditioning, my interest here is not just
plausibility measures, but conditional plausibility measures (cpm's). Given a set W of
worlds, a cpm maps pairs of subsets of W to some partially ordered set D. I write Pl(U jV )
rather than Pl(U; V ), in keeping with standard notation for conditioning. In the case of
a probability measure , it is standard to take (U jV ) to be undefined in (V ) = 0. In
general, we must make precise what the allowable second arguments are. Thus, I take
the domain of a cpm to have the form F  F 0 where, intuitively, F 0 consists of those sets
in F on which it makes sense to condition. For example, for a conditional probability
measure defined in the usual way from an unconditional probability measure , F 0 consists
of all sets V such that (V ) > 0. (Note that F 0 is not an algebra|it is not closed under
complementation.) A Popper algebra over W is a set F F 0 of subsets of W  W satisfying
the following properties:
Acc1. F is an algebra over W .
Acc2. F 0 is a nonempty subset of F .
Acc3. F 0 is closed under supersets in F ; that is, if V 2 F 0 , V  V 0 , and V 0 2 F , then
V 0 2 F 0.
(Popper algebras are named after Karl Popper, who was the first to consider formally
conditional probability as the basic notion (Popper, 1968). De Finetti (1936) also did some
early work, apparently independently, taking conditional probabilities as primitive. Indeed,
as Renyi (1964) points out, the idea seems to go back as far as Keynes (1921).)
A conditional plausibility space (cps ) is a tuple (W; F ; F 0 ; Pl), where F F 0 is a Popper
algebra over W , Pl : F  F 0 ! D, D is a partially ordered set of plausibility values, and Pl
is a conditional plausibility measure (cpm) that satisfies the following conditions:
CPl1. Pl(;jV ) = ?D .
CPl2. Pl(W jV ) = >D .
CPl3. If U  U 0 , then Pl(U jV )  Pl(U 0 jV ).
CPl4 Pl(U jV ) = Pl(U \ V jV ).
CPl1{3 are the obvious analogues to Pl1{3. CPl4 is a minimal property that guarantees
that when conditioning on V , everything is relativized to V . It follows easily from CPl1{4
that Pl(jV ) is a plausibility measure on V for each fixed V . A cps is acceptable if it satisfies
Acc4. If V 2 F 0 , U 2 F , and Pl(U jV ) 6= ?D , then U \ V 2 F 0 .
363

fiHalpern

Acceptability is a generalization of the observation that if Pr(V ) 6= 0, then conditioning on
V should be defined. It says that if Pl(U jV ) 6= ?D , then conditioning on V \ U should be
defined.
CPl1{4 are rather minimal requirements. For example, they do not place any constraints
on the relationship between Pl(U jV ) and Pl(U jV 0 ) if V 6= V 0 . One natural additional
condition is the following.
CPl5. If V \ V 0 2 F 0 and U; U 0 2 F , then Pl(U jV \ V 0 )  Pl(U 0 jV \ V 0 ) iff Pl(U \ V jV 0 ) 
Pl(U 0 \ V jV 0 ).
It is not hard to show that CPl5 implies CPl4.
Lemma 2.1: CPl5 implies CPl4.
Proof: Since clearly Pl(U \ V jV ) = Pl(U \ V \ V jV ), by CPl5 it follows that Pl(U jV \ V ) =
Pl(U \ V jV \ V ), and hence Pl(U jV ) = Pl(U \ V jV ).
CPl5 does not follow from CPl1{4 (indeed, as shown below, the standard notion of
conditioning for lower probabilities satisfies CPl1{4 but not CPl5). A cps that satisfies
CPl5 is said to be coherent. Although I do not assume CPl5 here, it in fact holds for all
plausibility measures to which one of the main results applies (see Lemma 3.5).
In any case, CPl5 is certainly not the only coherence that might be required. For
example, it may seem reasonable to require that if V and V 0 are disjoint, then it is not the
case that both Pl(U jV [ V 0 ) > Pl(U jV ) and Pl(U jV [ V 0 ) > Pl(U jV 0 ). Similarly, we may
want to require that it not be the case that Pl(U jV [ V 0 ) < Pl(U jV ) and Pl(U jV [ V 0 ) <
Pl(U jV 0 ).2 Coming up with a reasonable set of coherence conditions remains a topic for
future work. The only properties needed for the results of this paper are CPl1{4.
The notion of cps considered here is closely related to that defined in (Friedman &
Halpern, 1995). There, a cps is taken to be a family fW; DV ; PlV ) : V  W; V 6= ;g of
plausibility spaces, where each plausibility measure PlV satisfies Pl1{3 and has domain 2W
and an analogue of CPl5 holds: PlV \V 0 (U )  PlV \V 0 (U 0 ) iff PlV 0 (U \ V )  PlV 0 (U 0 \ V ). To
distinguish the definition of cps given in this paper from that given in (Friedman & Halpern,
1995), I call the latter an FH-cps. There is no analogue to Acc1{4 in (Friedman & Halpern,
1995); F is implicitly taken to be 2W , while F 0 is implicitly taken to be 2W , f;g. This is
an inessential difference between the definitions. More significantly, note that in an FH-cps,
(W; DV ; PlV ) is a plausibility space for each fixed V , and thus satisfies Pl1{3. However,
requiring CPl1{3 is a priori stronger than requiring Pl1{3 for each separate plausibility
space. Pl1 requires that Pl(;jV ) = ?D , but the elements ?D may be different for each
V . By way of contrast, CPl1 requires that Pl(?jV ) must be the same element, ?D , for all
V . Similar remarks hold for Pl2. Nevertheless, as is shown below, there is a construction
that converts an FH-cps to a coherent cps.
I now consider some standard ways of getting a cps starting with an unconditional
representation of uncertainty.
Definition 2.2: A cps (W; F ; F 0 ; Pl) extends an unconditional plausibility space (W; F ; Pl0 )
if Pl(U jW ) = Pl0 (U ). (W; F ; F 0 ; Pl) is standard if F 0 = fU : Pl(U ) 6= ?g.
All the constructions below result in standard cps's.
V

V

2. I think an anonymous referee of this paper for suggesting this condition.

364

fiConditional Plausibility Measures and Bayesian Networks

Ranking functions: Given an unconditional ranking function , there is a well-known
way of extending it to a conditional ranking function:
(
(U \ V ) , (V ) if (V ) 6= 1,
(U jV ) = undefined
if (V ) = 1.

This is consistent with the view that if (V ) = k, then (V ) = k , since then (U jV ) =
(U \V ),(V ). It is easy to check that this definition results in a coherent cps.
Possibility measures: There are two standard ways of defining a conditional possibility
measure from an unconditional possibility measure Poss. To distinguish them, I write
Poss(U jV ) for the first approach and Poss(U jjV ) for the second approach. According to the
first approach,
8
>
< Poss(V \ U ) if Poss(V \ U ) < Poss(V ),
if Poss(V \ U ) = Poss(V ) > 0,
Poss(U jV ) = > 1
: undefined if Poss(V ) = 0.
The second approach looks more like conditioning in probability:
(
V \ U )=Poss(V ) if Poss(V ) > 0,
Poss(U jjV ) = Poss(
undefined
if Poss(V ) = 0.
It is easy to show that both definitions result in a coherent cps. (Many other notions of
conditioning for possibility measures can be defined; see, for example (Fonck, 1994). I focus
on these two because they are the ones most-often considered in the literature.)
Sets of probabilities: For a set P of probabilities, conditioning can be defined for all
the representations of P as a plausibility measure. But in each case there are subtle choices
involving when conditioning is undefined. For example, one definition of conditional lower
probability is that P (U jV ) is inf f(U jV ) : (V ) 6= 0g if (V ) 6= 0 for all  2 P , and is
undefined otherwise (i.e., if (V ) = 0 for some  2 P ). It is easy to check that P defined
this way gives a coherent cpm, as does the corresponding definition of P  . The problem
with this definition is that it may result in a rather small set F 0 for which conditioning
is defined. For example, if for each set V 6= W , there is some measure  2 P such that
(V ) = 0 (which can certainly happen in some nontrivial examples), then F 0 = fW g. As
a consequence, the cps defined in this way is not acceptable (i.e., does not satisfy Acc4) in
general.
The following definition gives a lower probability which is defined on more arguments:
(
inf f(U jV ) : (V ) 6= 0g if (V ) 6= 0 for some  2 P ,
P (U jV ) = undefined
if (V ) = 0 for all  2 P .
It is easy to see that this definition agrees with the first one whenever the first is defined and
results, in general, in a larger set F 0 . Moreover, the resulting cps is acceptable. However,
the second definition does not satisfy CPl5. For example, suppose that W = fa; b; cg and
P = f; 0 g, where (a) = (b) = 0, (c) = 1, 0(a) = 2=3, 0(b) = 1=3, and 0(c) = 0.
365

fiHalpern

Taking V = fa; bg, U = fag, and U 0 = fbg, it is easy to see that according to the second
definition, P (U \ V jW ) = P (U 0 \ V jW ) = 0, but P (U jV ) > P (U 0 jV ).
For PlP , there are two analogous definitions. For the first, PlP (U jV ) is defined only if
(V ) > 0 for all  2 P , in which case PlP (U jV ) is fU jV , where fU jV (i) = i(U jV ). This
definition gives a coherent cps, but again, in general, not one that is acceptable. In this
paper, I focus on the following definition, which does result in an acceptable cps.
First extend DI by allowing functions which have value  (intuitively,  denotes undefined). More precisely, let DI0 consist of all functions f from I to [0; 1][fg such that f (i) 6= 
for at least one i 2 I . The idea is to define PlP (U jV ) = fU jV , where fU jV (i) = i (U jV )
if i (V ) > 0 and  otherwise. (Note that this agrees with the previous definition, which
applies only to the situation where (V ) > 0 for all  2 P .) There is a problem though,
one to which I have already alluded. CPl1 says that f;jV must be ? for all V . Thus, it
must be the case that f;jV1 = f;jV2 for all V1 ; V2  W . But if i 2 P and V1 ; V2  W are
such that i (V1 ) > 0 and i(V2 ) = 0, then f;jV1 (i) = 0 and f;jV2 (i) = , so f;jV1 6= f;jV2 . A
similar problem arises with CPl2.
To deal with this problem DI0 must be slightly modified. Say that f 2 DI0 is equivalent
to ?D if f (i) is either 0 or * for all i 2 I ; similarly, f is equivalent to >D if f (i) is
either 1 or * for all i 2 I . (Since, by definition, f (i) 6=  for at least one i 2 I , an
element cannot be equivalent to both >D and ?D .) Let DI be the same as DI0 except
that all elements equivalent to ?D are identified (and viewed as one element) and all
elements equivalent to >D are identified. More precisely, let DI = f?D ; >D g [ ff 2 D0 :
f is not equivalent to >D or ?D g. Define the ordering  on DI by taking f  g if one of
the following three conditions holds:
 f = ?D ,
 g = >D ,
 neither f nor g is ?D or >D and for all i 2 I , either f (i) = g(i) =  or f (i) 6= ,
g(i) 6= , and f (i)  g(i).
Now define
8? 
if (V ) 6= 0 for some  2 P and
>
D
>
>
(V ) 6= 0 implies (U jV ) = 0 for all  2 P ,
<
if 9 2 P ((V ) 6= 0) and 8 2 P ((V ) 6= 0 ) (U jV ) = 1),
PlP (U jV ) = > >D
>
if (V ) = 0 for all  2 P ,
>
: undefined
fU jV
otherwise.
I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

It is easy to check that this gives a coherent cps.
Plausibility measures: The construction for PlP can be used to convert any FH-cps
to a cps. I demonstrate the idea by showing how to construct a conditional plausibility
measure from an unconditional plausibility measure. Given an unconditional plausibility
space (W; F ; Pl) with range D, an FH-cps is constructed in (Friedman & Halpern, 1995) by
defining Pl(U jV ) = Pl(U \ V ). Thus, DV = fd 2 D : d  Pl(V )g and >D = Pl(V ). This
is not a cps because CPl2 is not satisfied, but it is an FH-cps, since Pl1{3 is satisfied for
each fixed V , and so is CPl5. As observed in (Friedman & Halpern, 1995), this is in fact
V

366

fiConditional Plausibility Measures and Bayesian Networks

the FH-cps extending Pl that makes the minimal number of comparisons, in the sense that
if Pl0 is an FH-cps extending Pl and Pl(U jV )  Pl(U 0 jV ), then Pl0 (U jV )  Pl0 (U 0 jV ).
To get a cps, let D0 = f(d; V ) : V  W; d  Pl(V ); Pl(V ) > ?D g. Say that (d; V ) is
equivalent to ?D if d = ?D ; say that (d; V ) is equivalent to >D if d = Pl(V ). Now let
D = f?D ; >D g [ ff 2 D0 : f is not equivalent to >D or ?D g. Then define d D d0
for d; d0 2 D iff d = ?D , d0 = >D , or there is some V  W such that d = (d1 ; V ),
d0 = (d2 ; V ), and d1 D d2 . Finally, for U; V 2 F , define
8
>
(Pl(U \ V ); V ) if ?D < Pl(U \ V ) < Pl(V ),
>
< >D 
if Pl(U \ V ) = Pl(V ) > ?D ,
Pl(U jV ) = > ? 
if Pl(U \ V ) = ?D , Pl(V ) > ?D ,
D
>
: undefined
if Pl(V ) = ?D .
I leave it to the reader to check that Pl is a coherent cpm. It is important that Pl(U jV ) is
undefined if Pl(V ) = ?D ; if we tried to extend the construction to V such that Pl(V ) = ?D ,
then we would have >D = ?D . This issue did not arise in (Friedman & Halpern, 1995),
since there were separate plausibility spaces for each choice of V .

3. Algebraic Conditional Plausibility Measures

To be able to carry out the type of reasoning used in Bayesian networks, it does not suce to
just have conditional plausibility. We need to have analogues of addition and multiplication.
More precisely, there needs to be some way of computing the plausibility of the union of
two disjoint sets in terms of the plausibility of the individual sets and a way of computing
Pl(U \ V jV 0 ) given Pl(U jV \ V 0 ) and Pl(V jV 0 ).

Definition 3.1: A cps (W; F ; F 0 ; Pl) where Pl has range D is algebraic if it is acceptable
and there are functions  : D  D ! D and 
 : D  D ! D such that the following

properties hold:
Alg1. If U; U 0 2 F are disjoint and V 2 F 0 then Pl(U [ U 0 jV ) = Pl(U jV )  Pl(U 0 jV ).
Alg2. If U 2 F , V \ V 0 2 F 0 , then Pl(U \ V jV 0 ) = Pl(U jV \ V 0 ) 
 Pl(V jV 0 ).
Alg3. 
 distributes over ; more precisely, a 
 (b1      bn ) = (a 
 b1 )      (a 
 bn )
if (a; b1 ); : : : ; (a; bn ); (a; b1      bn ) 2 DomPl (
) and (b1 ; : : : ; bn ); (a 
 b1 ; : : : ; a 

bn ) 2 DomPl(), where DomPl() = f(Pl(U1 jV ); : : : ; Pl(Un jV )) : U1; : : : ; Un 2 F are
pairwise disjoint and V 2 F 0 g and DomPl (
) = f(Pl(U jV \ V 0 ); Pl(V jV 0 )) : U 2
F ; V \ V 0 2 F 0g.3 (See below for a discussion of DomPl() and DomPl(
). In the
sequel, I omit the subscript Pl if it is clear from context.)
Alg4. If (a; c); (b; c) 2 Dom(
), a 
 c  b 
 c, and c 6= ?, then a  b.
3. In the conference version of this paper, Dom( ) was taken to consist only of pairs, not tuples of arbitrary
finite length, and distributivity was considered only for terms of the form a (b b0 ). The more general
version considered here is slightly stronger. The reason is that it is possible that (a; b1
b)
Dom( ) even though (a; b1
b ) = Dom( ) for k n. Note also that only left distributivity is
required here.







  




  

k

2




367



n

2

fiHalpern

I sometimes refer to the cpm Pl as being algebraic as well.
It may seem more natural to consider a stronger version of Alg4 that applies to all pairs
in D  D, such as
Alg40 . If a 
 c  b 
 c and c 6= ?, then a  b.
However, as Proposition 3.2 below shows, by requiring that Alg3 and Alg4 hold only for
tuples in Dom() and Dom(
) rather than on all tuples in D  D, some cps's of interest
become algebraic that would otherwise not be. Intuitively, we care about 
 mainly to the
extent that Alg1 and Alg2 holds, and Alg1 and Alg2 apply only to tuples in Dom() and
Dom(
), respectively. Thus, it does not seem unreasonable that Alg4 be required to hold
only for these tuples.

Proposition 3.2: The constructions for extending an unconditional probability measure,
ranking function, possibility measure (using either Poss(U jV ) or Poss(U jjV )), and the plausibility measure PlP defined by a set P of probability measures to a cps result in algebraic
cps's.4
Proof: It is easy to see that in each case the cps is acceptable. It is also easy to find
appropriate notions of 
 and  in the case of probability measures, ranking functions, and
possibility measures using Poss(U jjV ). For probability, clearly  and 
 are essentially +
and ; however, since the range of probability is [0; 1], a  b must be defined as max(1; a + b),
and Alg3 holds only for Dom() = f(a1 ; : : : ; ak ) : a1 +    + ak  1g; there is no constraint on
Dom(); it is [0; 1]  [0; 1]. For ranking,  and 
 are min and +; there are no constraints
on Dom(min) and Dom(+). For Poss(U jjV ),  is max and 
 is ; again, there are no
constraints on Dom(max) and Dom(). I leave it to the reader to check that Alg1{4 hold

in all these cases.
For Poss(U jV ),  is again max and 
 is min. There are no constraints on Dom(max);
however, note that (a; b) 2 Dom(min) iff either a < b or a = 1. For suppose that (a; b) =
(Poss(U jV \ V 0 ); Poss(V jV 0 ), where U 2 F and V \ V 0 2 F 0 . If Poss(U \ V \ V 0 ) =
Poss(V \ V 0 ) then a = Poss(U jV \ V 0 ) = 1; otherwise, Poss(U \ V \ V 0 ) < Poss(V \ V 0 ), in
which case a = Poss(U jV \ V 0 ) = Poss(U \ V \ V 0 ) < Poss(V \ V 0 )  Poss(V jV 0 ) = b. It
is easy to check Alg1{3. While min does not satisfy Alg40 |certainly min(a; c) = min(b; c)
does not in general imply that a = b|Alg4 does hold. For if min(a; c)  min(b; c) and
a = 1, then clearly b = 1. Alternatively, if a < c, then min(a; c) = a and the only way that
a  min(b; c), given that b < c or b = 1, is if a  b.
Finally, for PlP ,  and 
 are essentially pointwise addition and multiplication. But
there are a few subtleties. As in the case of probability, Dom() consists of sequences
which sum to at most 1 for each index i. Care must also be taken in dealing with ?D and
>D . More precisely, Dom() consists of all tuples (f1; : : : ; fn) such that either
I

I

1(a). fj 6= >D ; j = 1; : : : ; n,
I

1(b). if fj ; fk 6= ?D for 1  j; k  n, then fj (i) =  iff fk (i) = , for all i 2 I , and
I

4. Essentially the same result is proved in (Friedman & Halpern, 1995) for all cases but PlP .

368

fiConditional Plausibility Measures and Bayesian Networks

1(c). Pfj :f 6=?  ;f (i)6=g fj (i)  1
or
2. there exists j such that fj = >D and fk = ?D for k 6= j ;
Dom(
) consists of pairs (f; g) such that either one of f or g is in f?D ; >D g or neither
f nor g is in f?D ; >D g and g(i) 2 f0; g iff f (i) = . The definition of  is relatively
straightforward. Define f  >D = >D  f = >D and f  ?D = ?D  f = f . If
f; g \ f?D ; >D g = ;, then f  g = h, where h(i) = min(1; f (i) + g(i)) (taking a +  =
 + a =  and min(1; ) = ). In a similar spirit, define f 
 >D = >D 
 f = f
and f 
 ?D = ?D 
 f = ?D ; if ff; gg \ f?D ; >D g = ;, then f 
 g = h, where
h(i) = f (i)  g(i) (taking   a = a   =  if a 6= 0 and   0 = 0   = 0). It is important
that   0 = 0 and    = , since otherwise Alg3 may not hold. For example, according
to Alg3,
j

D

I

j

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

((1=2; ; 1=2)
(a; 0; b))((1=2; ; 1=2))
(a; 0; b)) = ((1=2; ; 1=2)(1=2; ; 1=2))
(a; 0; b) = (a; 0; b)
(since (1=2; ; 1=2)(1=2; ; 1=2) = >D ) and, similarly, ((1=2; ; 1=2)
(a; ; b))((1=2; ; 1=2))

(a; ; b)) = (a; ; b). Since   0 = 0 and    = , these equalities hold. I leave it to the
reader to check that, with these definitions, Alg1{4 hold (although note that the restrictions
to Dom() and Dom(
) are required for both Alg3 and Alg4 to hold).
Conditional lower probability is not algebraic. For example, it is not hard to construct
pairwise disjoint sets U1 , V1 , U2 , and V2 and a set P of probability measures such that
P (Ui) = P (Vi) (and P  (Ui) = P  (Vi )) for i = 1; 2, but P (U1 [ U2 ) 6= P(V1 [ V2). That
means there cannot be a function  in the case of lower probability.5
For later convenience, I list some simple properties of algebraic cpms that show that ?
and > act like 0 and 1 with respect to addition and multiplication. Let Range(Pl) = fd :
Pl(U jV ) = d for some (U; V ) 2 F  F 0 g.
I

Lemma 3.3: If (W; F ; F 0 ; Pl) is an algebraic cps, then d  ? = ?  d = d for all d 2

Range(Pl).

Proof: Suppose that d = Pl(U jV ). By Alg1, it follows that
d = Pl(U jV ) = Pl(U [ ;jV ) = Pl(U jV )  Pl(;jV ) = d  ?:
A similar argument shows that d = ?  d.
Lemma 3.4: If (W; F ; F 0 ; Pl) is an algebraic cps then, for all d 2 Range(Pl),
(a) d 
 > = d;
5. For readers familiar with Dempster-Shafer belief functions (Shafer, 1976), they provide another example
of a plausibility measure. There are two well-known ways of defining conditioning for belief functions
(see (Fagin & Halpern, 1991)), one using Dempster's rule of combination and the other treating belief
functions as lower probabilities. Neither leads to an algebraic cps, which is why I have not discussed
belief functions in this paper.

369

fiHalpern

(b) if d 6= ?, then > 
 d = d;
(c) if d 6= ?, then ? 
 d = ?;
(d) if (d; ?) 2 Dom(
), then > 
 ? = d 
 ? = ? 
 ? = ?.

Proof: Suppose that d = Pl(U jV ). By Alg2, CPl2, and CPl4, it follows that
d = Pl(U jV ) = Pl(U \ V jV ) = Pl(U jV ) 
 Pl(V jV ) = d 
 >:
Similarly, if d =
6 ?, then U \ V 2 F 0 (by Acc4), so
d = Pl(U jV ) = Pl(U \ V jV ) = Pl(U \ V jU \ V ) 
 Pl(U \ V jV ) = > 
 d:
If d =
6 ?, then by Alg2, CPl1, and CPl4
? = Pl(?jV ) = Pl(?jU \ V ) 
 Pl(U jV ) = ? 
 d:
Finally, if (d; ?) 2 Dom(
), then there exist U; V; V 0 such that V \V 0 2 F 0 , Pl(U jV \V 0 ) = d
and Pl(V jV 0 ) = ?. By Alg2, Pl(U \ V jV 0 ) = Pl(U jV \ V 0 ) 
 Pl(V jV 0 ) = d 
 ?. By CPl3,
Pl(U \ V jV 0 )  Pl(V jV 0 ) = ?, so Pl(U \ V jV 0 ) = ?. Thus, d 
 ? = ?. Replacing U
with V \ V 0 , the same argument shows that > 
 ? = ?; replacing U with ;, we get that
? 
 ? = ?.
I conclude this section by showing that a standard algebraic cps that satisfies one other
minimal property must also satisfy CPl5. Say that 
 is monotonic if d  d0 and e  e0 then
d 
 e  d0 
 e0 . A cpm (cps) is monotonic if 
 is.

Lemma 3.5: A standard algebraic monotonic cps satisfies CPl5.
Proof: Suppose that (W; F ; F 0 ; Pl) is a standard algebraic cps and that V \ V 0 2 F 0 . If
Pl(U jV \ V 0 )  Pl(U 0 jV \ V 0 ), then it follows from Alg2 and monotonicity that
Pl(U \ V jV 0 ) = Pl(U jV \ V 0 ) 
 Pl(V jV 0 )  Pl(U 0 jV \ V 0 ) 
 Pl(V jV 0 ) = Pl(U 0 \ V jV 0 ):
For the opposite implication, suppose that Pl(U \ V jV 0 )  Pl(U 0 \ V jV 0 ). Then, by Alg2,
Pl(U jV \ V 0 ) 
 Pl(V jV 0 )  Pl(U 0 jV \ V 0 ) 
 Pl(V jV 0 ):
(1)
Since V \V 0 2 F 0 and the cps is standard, it must be the case that Pl(V \V 0 ) =
6 ?. Hence (by
0
0
0
CPl3), Pl(V ) =
6 ?; moreover, Pl(V jV ) =6 ? (otherwise Pl(V \ V ) = Pl(V jV 0) 
 Pl(V 0 ) =
?). Thus, by applying Alg4 to (1), it follows that Pl(U jV \ V 0)  Pl(U 0jV \ V 0).

4. Independence

How can we capture formally the notion that two events are independent? Intuitively, it
means that they have nothing to do with each other|they are totally unrelated; the occurrence of one has no inuence on the other. None of the representations of uncertainty
that we have been considering can express the notion of \unrelatedness" (whatever it might
370

fiConditional Plausibility Measures and Bayesian Networks

mean) directly. The best we can do is to capture the \footprint" of independence on the
notion. For example, in the case of probability, if U and V are unrelated, it seems reasonable to expect that learning U should not affect the probability of V and symmetrically,
learning V should not affect the probability of U . \Unrelatedness" is, after all, a symmetric
notion.6 The fact that U and V are probabilistically independent (with respect to probability measure ) can thus be expressed as (U jV ) = (U ) and (V jU ) = (V ). There is a
technical problem with this definition: What happens if (V ) = 0? In that case (U jV ) is
undefined. Similarly, if (U ) = 0 then (V jU ) is undefined. It is conventional to say that,
in this case, U and V are still independent. This leads to the following formal definition.

Definition 4.1: U and V are probabilistically independent (with respect to probability measure ) if (V ) =
6 0 implies (U jV ) = (U ) and (U ) 6= 0 implies (V jU ) = (V ).
This does not look like the standard definition of independence in texts, but an easy
calculation shows that it is equivalent.
Proposition 4.2: The following are equivalent:
(a) (U ) 6= 0 implies (V jU ) = (V ),
(b) (U \ V ) = (U )(V ),
(c) (V ) 6= 0 implies (U jV ) = (U ).
Thus, in the case of probability, it would be equivalent to say that U and V are independent with respect to  if (U \ V ) = (U )(V ) or to require only that (U jV ) = (U )
if (V ) 6= 0 without requiring that (V jU ) = (V ) if (U ) 6= 0. However, these equivalences do not necessarily hold for other representations of uncertainty. The definition of
independence I have given here seems to generalize more appropriately.7
The definition of probabilistic conditional independence is analogous.
Definition 4.3: U and V are probabilistically independent given V 0 (with respect to probability measure ) if (V \ V 0 ) 6= 0 implies (U jV \ V 0 ) = (U jV 0 ) and (U \ V 0 ) 6= 0 implies
(V jU \ V 0) = (V jV 0).
It is immediate that U and V are (probabilistically) independent iff they are independent
conditional on W .
The generalization to conditional plausibility measures (and hence to all other representations of uncertainty that we have been considering) is straightforward.
Definition 4.4: Given a cps (W; F ; F 0 ; Pl), U; V 2 F are plausibilistically independent
given V 0 2 F (with respect to the cpm Pl), written IPl (U; V jV 0 ), if V \ V 0 2 F 0 implies
Pl(U jV \ V 0 ) = Pl(U jV 0 ) and U \ V 0 2 F 0 implies Pl(V jU \ V 0 ) = Pl(V jV 0 ).
6. Walley (1991) calls the asymmetric notion irrelevance and defines U being independent of V as U is
irrelevant to V and V is irrelevant to U . Although my focus here is independence, irrelevance is an
interesting notion in its own right; see (Cozman, 1998; Cozman & Walley, 1999).
7. Another property of probabilistic independence is that if U is independent of V then U is independent
of V . This too does not follow for the other representations of uncertainty, and Walley (1991) actually
makes this part of his definition. Adding this requirement would not affect any of the results here,
although it would make the proofs somewhat lengthier, so I have not made it part of the definition.

371

fiHalpern

We are interested in conditional independence of random variables as well as in conditional independence of events. All the standard definitions extend to plausibility in a
straightforward way. A random variable X on W is a function from W to the reals. Let
R(X ) be the set of possible values for X (that is, the set of values over which X ranges). As
usual, X = x is the event fw : X (w) = xg. If X = fX1 ; : : : ; Xk g is a set of random variables
and x = (x1 ; : : : ; xk ), let X = x be an abbreviation for the event X1 = x1 \ : : : \ Xk = xk .
A random variable is measurable with respect to cps (W; F ; F 0 ; Pl) if X = x 2 F for all
x 2 R(X ). For the rest of the paper, I assume that all random variables X are measurable and that R(X ) is finite for all random variables X . Random variables X and Y are
independent with respect to plausibility measure Pl if the events X = x and Y = y are
independent for all x 2 R(X ) and y 2 R(Y ). More generally, given sets X, Y, and Z of
random variables, X and Y are plausibilistically independent given Z (with respect to Pl),
denoted IPlrv (X; YjZ), if IPl (X = x; Y = xjZ = z) for all x, y, and z. (Note that I am using
IPl for conditional independence of events and IPlrv for conditional independence of random
variables.) If Z = ;, then IPlrv (X; YjZ) if X and Y are unconditionally independent, that
is, if IPl (X = x; Y = xjW ) for all x, y; if either X = ; or Y = ;, then IPlrv (X; YjZ) is taken
to be vacuously true.
Now consider the following four properties of random variables, called the semi-graphoid
properties (Pearl, 1988), where X, Y, and Z are pairwise disjoint sets of variables.
CIRV1. If IPlrv (X; YjZ) then IPlrv (Y; XjZ).

CIRV2. If IPlrv (X; Y [ Y0 jZ) then IPlrv (X; YjZ).

CIRV3. If IPlrv (X; Y [ Y0 jZ) then IPlrv (X; YjY0 [ Z).

CIRV4. If IPlrv (X; YjZ) and IPlrv (X; Y0 jY [ Z) then IPlrv (X; Y [ Y0 jZ).
It is well known that CIRV1{4 hold for probability measures. The following result
generalizes this. The proof is not dicult, although care must be taken to show that the
result depends only on the properties of algebraic cpms.

Theorem 4.5: CIRV1{4 hold for all algebraic cps's.
Proof: See the appendix.
Theorem 4.5, of course, is very dependent on the definition of conditional independence
given here. Other notions of independence have been studied in the literature for specific
representations of uncertainty. Perhaps the most common definition tries to generalize the
observation that if U and V are probabilistically independent, then (U \ V ) = (U )  (V ).
Zadeh (1978) considered this approach in the context of possibility measures, calling it
noninteraction, but it clearly makes sense for any algebraic cpm.

Definition 4.6: U and V do not interact given V 0 (with respect to the algebraic cpm Pl),
denoted NI Pl (U; V jV 0 ), if V 0 2 F 0 implies that Pl(U \ V jV 0 ) = Pl(U jV 0 ) 
 Pl(V jV 0 ).8
8. Shenoy (1994) defines a notion similar in spirit to noninteraction for random variables.

372

fiConditional Plausibility Measures and Bayesian Networks

Fonck (1994) shows that noninteraction is strictly weaker than independence for a number of notions of independence for possibility measures. The following result shows that
independence implies noninteraction for all algebraic cpms.

Lemma 4.7: If (W; F ; F 0 ; Pl) is an algebraic cps, then IPl(U; V jV 0) implies NI Pl(U; V jV 0 ).
Proof: Suppose that V 0 2 F 0 and IPl(U; V jV 0) holds. If V \ V 0 2 F 0 then, from Alg2, it
follows that

Pl(U \ V jV 0 ) = Pl(U jV \ V 0 ) 
 Pl(V jV 0 ) = Pl(U jV 0 ) 
 Pl(V jV 0 ):
On the other hand, if V \ V 0 2= F 0 , then by Acc4, Pl(V jV 0 ) = ?. By CPl3, Pl(U \ V jV 0 ) = ?,
and by Lemma 3.3, Pl(U jV 0 ) 
 Pl(V jV 0 ) = ?. Thus, Pl(U \ V jV 0 ) = Pl(U jV 0 ) 
 Pl(V jV 0 ).
What about the converse to Lemma 4.7? The results of Fonck show that it does not hold
in general|indeed, it does not hold for Poss(U jV ). So what is required for noninteraction
to imply independence? The following lemma provides a sucient condition.

Lemma 4.8: If (W; F ; F 0 ; Pl) is a standard algebraic cps that satisfies Alg40, then NI Pl(U; V jV 0)
implies IPl (U; V jV 0 ).
Proof: Suppose that V \ V 0 2 F 0 and NI Pl(U; V jV 0). Then by Alg2,
Pl(U \ V jV 0 ) = Pl(U jV \ V 0 ) 
 Pl(V jV 0 ):
(2)
By Acc3, V 0 2 F 0 , so NI Pl (U; V jV 0 ) implies
Pl(U \ V jV 0 ) = Pl(U jV 0 ) 
 Pl(V jV 0 ):
(3)
Since V \ V 0 2 F 0 and (W; F ; F 0 ; Pl) is standard, Pl(V \ V 0 ) =
6 ?. Since Pl(V \ V 0) =
Pl(V jV 0 ) 
 Pl(V 0 ), it follows from Lemma 3.4 that Pl(V jV 0 ) =
6 ?. So, by Alg40 , (2),
0
0
and (3), it follows that Pl(U jV \ V ) = Pl(U jV ). An identical argument shows that
Pl(V jU \ V 0 ) = Pl(V jV 0 ) if U \ V 0 2 F 0 . Thus, IPl(U; V jV 0 ).
Lemmas 4.7 and 4.8 show why noninteraction and independence coincide for conditional
probability defined from unconditional probability, ranking functions, and possibility measures using Poss(U jjV ). Moreover, they suggest why they do not coincide in general. Since
neither Poss(U jV ) nor PlP satisfy Alg40 , it is perhaps not surprising that in neither case
does noninteraction imply conditional independence. (We shall shortly see an example in
the case of PlP ; Fonck (1994) gives examples in the case of Poss(U jV ).) Indeed, noninteraction may not even imply conditional independence for an arbitrary conditional probability
measure, as the following example shows.
Example 4.9: Suppose that W = fa; bg, F = 2W , F 0 = F , f;g, (a) = 1, (b) = 0, but
(bjb) = 1. It is easy to see that fbg is not independent of itself, but fbg does not interact
with fbg, since (b) = (b)  (b). Nevertheless, it is not hard to check that this conditional
probability measure  is algebraic and, in fact, satisfies Alg40 . However, it is not standard,
since fbg 2 F 0 although (b) = 0.
373

fiHalpern

It is easy to see that the assumption of standardness is necessary in Lemma 4.8. For
suppose that (W; F ; F 0 ; Pl) is an arbitrary nonstandard algebraic cps for which > 6= ?.
Since (W; F ; F 0 ; Pl) is nonstandard, there must exist some U 2 F 0 such that Pl(U jW ) = ?.
But then
? = Pl(;jW ) = Pl(;jU ) 
 Pl(U jW ) = ? 
 ?:
Thus
Pl(U jW ) = ? = ? 
 ? = Pl(U jW ) 
 Pl(U jW );
so NI Pl(U; U jW ). But Pl(U jU ) = > 6= ? = Pl(U ), so IPl (U; U jW ) does not hold.
In general, Theorem 4.5 does not hold if we use NI Pl rather than IPl . That is, Alg1{4
do not suce to ensure that CIRV1{4 hold for NI Pl . Besides noninteraction, a number of
different approaches to defining independence for possibility measures (Campos & Huete,
1999a, 1999b; Dubois, Fari~nas del Cerro, Herzig, & Prade, 1994; Fonck, 1994) and for sets
of probability measures (Campos & Huete, 1993; Campos & Moral, 1995; Cousa et al.,
1999) have been considered. In general, Theorem 4.5 does not hold for them either. It is
beyond the scope of this paper to discuss and compare these approaches to that considered
here, but it is instructive to consider independence for sets of probability measures in a
little more detail, especially for the representation PlP .
De Campos and Moral (1995) define what the call type-1 independence. U and V
are type-1 independent conditional on V 0 with respect to P if U and V are independent
conditional on V 0 with respect to every  2 P . It is easy to check that type-1 independence
is equivalent to noninteraction in the context of sets of probability measures. Thus, by
Lemma 4.7, IPlP (U; V jV 0 ) implies that U and V are type-1 independent conditional on
V 0 (and similarly for random variables). However, the converse does not necessarily hold,
because the two approaches treat conditioning on events that have probability 0 according
to some (but not all) of the measures in P differently. To see this, consider an example
discussed by de Campos and Moral. Suppose a coin is known to be either double-headed or
double-tailed and is tossed twice. This can be represented by P = f0 ; 1 g, where 0 (hh) =
1 and 0 (ht) = 0 (th) = 0 (tt) = 0, while 1 (tt) = 1 and 1 (ht) = 1 (th) = 1 (hh) = 0.
Let X1 and X2 be the random variables representing the outcome of the first and second
coin tosses, respectively. Clearly there is a functional dependence between X1 and X2 , but
it is easy to check that X1 and X2 are type-1 independent with respect to P . Moreover,
noninteraction holds: NI Pl(X1 = i; X2 = j ) holds for i; j 2 fh; tg. On the other hand,
IPlP (X1 ; X2 ) does not hold. For example, fX1=h(1) = 0 while fX1=hjX2 =h(1) = .9
The difference between noninteraction (i.e., type-1 independence) and the definition of
independence used in this paper in the context of sets of probability measures can be summarized as follows. U and V do not interact with respect to P if U and V are independent
9. As Peter Walley [private communication, 2000] points out, this example is somewhat misleading. The
definition of independence with respect to PlP produces the same counterintuitive behavior as type-1
independence if the probabilities are modified slightly so as to make them positive, i.e., when there is
\almost functional dependence" between the two variables. For example, suppose that the coin in the
example is known to either land heads with probability :99 or :01 (rather than 1 and 0, as in the example).
Let 00 and 01 be the obvious modifications of 0 and 1 required to represent this situation, and let
0 = 00 ; 01 . It is easy to check that X1 and X2 continue to be type-1 independent, and noninteraction
continues to hold, but now IPl (X1 ; X2 ) also holds. The real problem is that this representation of
uncertainty does not enable learning.
P

f

g

P0

374

fiConditional Plausibility Measures and Bayesian Networks

with respect to all measures  2 P . On the other hand, U and V are independent with
respect to P if (1) U and V are independent for all measures  2 P such that (U ) > 0 and
(V ) > 0 and (2) (U ) = 0 iff (V ) = 0 for all  2 P . The definition of independence used
here is thus more restrictive; it does not ignore the measures that give U or V probability 0
when determining independence. The difference between the two approaches is illustrated
in the example in the previous paragraph.
As the variant of the example considered in Footnote 9 shows though, neither definition
can completely claim to represent the intuition that if U is independent of V , then learning
U gives no information about V . If the coin in the example is known to land heads with
probability either .99 or .01, then seeing the first coin toss land heads certainly seems to
give information about the second coin toss, even though both definitions would declare
the events independent. However, the definition of independence used here does have the
advantage of leading to an algebraic cps, which means, as is shown in the next section, that
using it leads to a representation of sets of probability measures that can be represented as
a Bayesian network.

5. Bayesian Networks

Throughout this section, I assume that we start with a set W of possible worlds characterized
by a set X = fX1 ; : : : ; Xn g of n binary random variables. That is, a world in W is a tuple
(x1 ; : : : ; xn ) with xi 2 f0; 1g, and Xi (x1 ; : : : ; xn ) = xi ; that is, the value of Xi in world
w = (x1 ; : : : ; xn ) = wi.10 The goal of this section is to show that many of the tools of
Bayesian network technology can be applied in this setting. The proofs of the main results
all proceed in essentially the same spirit as well-known results for probabilistic Bayesian
networks (see (Geiger & Pearl, 1988; Geiger et al., 1990; Verma, 1986)).

5.1 Qualitative Bayesian Networks

As usual, a (qualitative) Bayesian network (over X ) is a dag whose nodes are labeled by
variables in X . The standard notion of a Bayesian network representing a probability
measure (Pearl, 1988) can be generalized in the obvious way to plausibility.
Definition 5.1: Given a qualitative Bayesian network G, let ParG(X ) be the parents of the
random variable X in G; let DesG (X ) be all the descendants of X , that is, X and all those
nodes Y such that X is an ancestor of Y ; let NDG (X ), the nondescendants of X , consist
of X , DesG (X ). Note that all ancestors of X are nondescendants of X . The Bayesian
network G is compatible with the cps (W; F ; F 0 ; Pl) (or just compatible with Pl, if the other
components of the cps are clear from context) if IPlrv (X; NDG (X )jPar(X )), that is, if X is
conditionally independent of its nondescendants given its parents, for all X 2 X .
There is a standard way of constructing a Bayesian network that represents a probability measure (Pearl, 1988). I briey review the construction here, since it works without
change for an algebraic cpm. Given an algebraic cpm Pl, let Y1 ; : : : ; Yn be a permutation of the random variables in X . Construct a qualitative Bayesian network GPl;hY1 ;:::;Y i
n

10. The assumption that the random variables are binary is just for ease of exposition. It is easy to generalize
the results to the case where (X ) is finite for each X ; there is no need to assume that (X ) is a
subset of the reals.
R

i

i

375

R

i

fiHalpern

as follows: For each k, find a minimal subset of fY1 ; : : : ; Yk,1 g, call it Pk , such that
IPlrv (fY1 ; : : : ; Yk,1 g; Yk jPk ). Then add edges from each of the nodes in Pk to Yk . Verma
(1986) shows that this construction gives a Bayesian network that is compatible with Pl in
the case that Pl is a probability measure; his proof depends only on CIRV1{4. Thus, the
construction works for algebraic cpms.

Theorem 5.2: GPl;hY1 ;:::;Y i is compatible with Pl.
Proof: For ease of notation in the proof, I write G instead of GPl;hY1 ;:::;Y i . Note that
Y1 ; : : : ; Yn represents a topological sort of G; edges always go from nodes in fY1 ; : : : ; Yk,1g
n

n

to Yk . It follows that G is acyclic; i.e., it is a dag. The construction guarantees that
Pk = ParG(Yk ) and that IPlrv (fY1 ; : : : ; Yk,1g; Yk jParG(Yk )). It follows from results of (Verma,
1986) (and is not hard to verify directly) that IPlrv (NDG (Yk ); Yk jParG (Yk )) can be proved
using only CIRV1{4. The result now follows from Theorem 4.5.

5.2 Quantitative Bayesian Networks

A qualitative Bayesian network G gives qualitative information about dependence and independence, but does not actually give the values of the conditional plausibilities. To provide
the more quantitative information, we associate with each node X in G a conditional plausibility table (cpt) that quantifies the effects of the parents of X on X . A cpt for X gives,
for each setting of X 's parents in G, the plausibility that X = 0 and X = 1 given that
setting. For example, if X 's parents in G are Y and Z , then the cpt for X would have an
entry denoted dX =ijY =j \Z =k for all (i; j; k) 2 f0; 1g3 . As the notation is meant to suggest,
dX =ijY =j \Z =k = Pl(X = ijY = j \ Z = k) for the plausibility measure Pl represented by
G.11 For each fixed j and k, we assume that x0jk  x1jk = >. A quantitative Bayesian
network is a pair (G; f ) consisting of a qualitative Bayesian network G and a function f
that associates with each node X in G a cpt for X .

Definition 5.3: A quantitative Bayesian network (G; f ) represents Pl if G is compatible

with Pl and the cpts agree with Pl, in the sense that, for each random variable X , the entry
dX =ijY1 =j1 ;:::;Y =j in the cpt is Pl(X = ijY1 = j1 \: : :\Yk = jk ) if Y1 = j1 \: : :\Yk = jk 2 F 0 .
(It does not matter what dX =ijY1 =j1 ;:::;Y =j is if Y1 = j1 \ : : : \ Yk = jk 2= F 0 .)
k

k

k

k

Given a cpm Pl, it is easy to construct a quantitative Bayesian network (G; f ) that
represents Pl: simply construct G that is compatible with Pl as in Theorem 5.2 and define
f appropriately, using Pl. The more interesting question is whether there is a unique
algebraic cpm determined by a quantitative Bayesian network. As stated, this question is
somewhat undetermined. The numbers in a quantitative network do not say what  and

 ought to be for the algebraic cpm.
A reasonable way to make the question more interesting is the following. Recall that,
for the purposes of this section, I have taken W to consist of the 2n worlds characterized by
the n binary random variables in X . Let PLD;;
 consist of all standard cps's of the form
(W; F ; F 0 ; Pl), where F = 2W , so that all subsets of W are measurable, the range of Pl is
11. Of course, if the random variables are not binary, i; j; k have to range over all possible values for the
random variables.

376

fiConditional Plausibility Measures and Bayesian Networks

D, and Pl is algebraic with respect to  and 
. Thus, for example, PLIN  ;min;+ consists
of all conditional ranking functions on W defined from unconditional ranking functions by
the construction in Section 2. Since a cps (W; F ; F 0 ; Pl) 2 PLD;;
 is determined by Pl, I
often abuse notation and write Pl 2 PLD;;
.
With this notation, the question becomes whether a quantitative Bayesian network
(G; f ) such that the entries in the cpts are in D determines a unique element in PLD;;
. As
I now show, the answer is yes, provided (D; ; 
) satisfies some conditions. Characterizing
the conditions on (D; ; 
) required for this result turns out to be a little subtle. Indeed,
it is somewhat surprising how many assumptions are required to reproduce the simple
arguments that are required in the case of probability.

Definition 5.4: (D; ; 
) is a BN-compatible domain (with respect to PLD;;
) if there
are sets D(
)  D  D and D()  D [ D2 [ D3 [ : : : satisfying the following properties:
BN1.  and 
 are commutative and associative.
BN2. For all d 2 D, (>; d); (?; d) 2 D(
), (?; d) 2 D(), > 
 d = d, ? 
 d = ?, and
?  d = d.
BN3. 
 distributes over ; more precisely, a 
 (b1      bn ) = (a 
 b1 )      (a 
 bn ) if
(a; b1 ); : : : ; (a; bn ); (a; b1    bn) 2 D(
) and (b1 ; : : : ; bn ); (a 
 b1 ; : : : ; a 
 bn ) 2 D();
moreover, (a1    an ) 
 b = a1 
 b    an 
 b if (a1 ; : : : ; an ); (a1 
 b; : : : ; an 
 b) 2
D() and (a1      an; b); (a1 ; b); : : : ; (an ; b) 2 D(
).
BN4. If (a; c); (b; c) 2 D(
), a 
 c  b 
 c, and c =
6 ?, then a  b.
BN5. If (d1 ; : : : ; dk ) 2 D() and d1      dk  d, then there exists (d01 ; : : : ; d0k ) 2 D()
such that (d01 ; d); : : : ; (d0k ; d); (d01      d0k ; d) 2 D(
), di = d0i 
 d, for i = 1; : : : ; k,
and d1     dk = (d01      d0k ) 
 d.
BN6. D() is closed under permutations and prefixes, so that if (x1 ; : : : ; xk ) 2 D() and
 is a permutation of (1; : : : ; k), then (x(1) ; : : : ; x(k) ) 2 D() and if k0  k, then
(x1 ; : : : ; xk0 ) 2 D(); moreover D()  D.
BN7. If (d1 ; : : : ; dk ); (d01 ; : : : ; d0m ) 2 D(), (di ; d0j ) 2 D(
) for i = 1; : : : ; k, j = 1; : : : ; m,
then (d1 
 d01 ; : : : ; d1 
 d0m ; : : : ; dk 
 d01 ; : : : ; dk 
 d0m ) 2 D().
BN8. If (d1 ; : : : ; dk ) 2 D() and k0  k, then d1      dk0  d1      dk .
Note that all the representations of uncertainty we have considered so far have associated
with them BN-compatible domains. Indeed, the definitions of D(), D(
), , and 
 in
each case are given in the proof of Proposition 3.2. For example, for PL[0;1];max;min, the
set of conditional possibility measures determined by unconditional possibility measures,
D() = [0; 1]  [0; 1], while D(
) consists of all pairs (a; b) 2 [0; 1]  [0; 1] such that a < b
or a = 1. I leave it to the reader to check that, in all these cases, BN1{8 hold.
Given a tuple x = (x1 ; : : : ; xn ) 2 [0; 1]n , let dX ;G;x denote the value dX =x jPar (X )=y ,
where y is the restriction of x to the variables in ParG (Xi ).
i

377

i

i

G

i

fiHalpern

Definition 5.5: If (D; ; 
) is BN-compatible, then a quantitative Bayesian network (G; f )
is (D; ; 
)-representable if the values of the cpts for G lie in D and the following properties
hold:
R1. For every node X in G and every setting y of ParG (X ), (dX =0jPar (X )=y ; dX =1jPar (X )=y ) 2
Dom() and
dX =0jPar (X )=y  dX =1jPar (X )=y = >:
G

G

G

G

R2. Suppose Y1 ; : : : ; Yn is a topological sort of the nodes in G. Then for all y 2 f0; 1gn
and all 1  j < k  n, (dY ;G;y ; dY +1 ;G;y 
    
 dY ;G;y ) 2 D(
) and (dY ;G;y 
    

dY ,1 ;G;y ; dY ;G;y ) 2 D(
).
j

k

j

k

j

k

R1 is the obvious analogue of the requirement in the probabilistic case that the entries
of the cpt for X , for a fixed setting of X 's parents, add up to 1. R2 essentially says that
certain terms (the ones required to compute the plausibility of Y = y for Y = hY1 ; : : : ; Yn i)
are required to be in D(
), so that it makes sense to take their product. Since D(
) =
[0; 1]  [0; 1] in the case of probability, there is no need to make this requirement explicit.
However, it is necessary for other representations of uncertainty.
The following result shows that, as the name suggests, there is a unique cpm that
represents a representable quantitative Bayesian network.

Theorem 5.6 : If (G; f ) is (D; ; 
)-representable, then there is a unique cpm Pl 2
PLD;;
 such that (G; f ) represents Pl.
5.3 D-Separation

Just as in the case of probability, conditional independencies can be read off the Bayesian
network using the criterion of d-separation (Pearl, 1988). Recall that a set X of nodes
in G = (V; E ), is d-separated from a set Y of nodes by a set Z of nodes in G, written
d-sep G (X; YjZ), if, for every X 2 X, Y 2 Y, and a trail from X to Y (that is, a sequence
(X0 ; : : : ; Xk ) of nodes in G such that X0 = X , Xk = Y and either (Xi ; Xi+1 ) or (Xi+1 ; Xi )
is a directed edge in G) and a node Xi on the trail with 0 < i < k such that either:
(a) Xi 2 Z and there is an arrow leading into Xi and an arrow leading out (i.e., either
(Xi,1 ; Xi ); (Xi ; Xi+1 ) 2 E or (Xi ; Xi,1 ); (Xi+1 ; Xi ) 2 E
(b) Xi 2 Z and Xi is a tail-to-tail node (i.e., (Xi ; Xi,1 ); (Xi ; Xi+1 ) 2 E )
(c) Xi is a head to head node (i.e., (Xi,1 ; Xi ); (Xi+1 ; Xi ) 2 E ), and neither Xi nor any
of its descendants are in Z.
Let G;Pl consist of all statements of the form IPlrv (X; NDG (X )jParG (X )). Let PLD;;

be an arbitrary collection of cps's of the form (W; F ; F 0 ; Pl) where all components other
than Pl are fixed, and the plausibility measures Pl all have the same range D of plausibility
values. Consider the following three statements:
1. d-sep G (X; YjZ).
2. IPlrv (X; YjZ) is provable from CIRV1{4 and G;Pl.
378

fiConditional Plausibility Measures and Bayesian Networks

3. IPlrv (X; YjZ) holds for every plausibility measure in PLD;;
 compatible with G.
The implication from 1 to 2 is proved in (Geiger et al., 1990; Verma, 1986).

Theorem 5.7: (Geiger et al., 1990; Verma, 1986) If d-sep G(X; YjZ), then IPlrv (X; YjZ) is
provable from CIRV1{4 and G;Pl.

It is immediate from Theorem 4.5 that the implication from 2 to 3 holds for algebraic
cpms.
Corollary 5.8: If IPlrv (X; YjZ) is provable from CIRV1{4 and G;Pl, then IPlrv (X; YjZ)
holds for every algebraic cpm Pl compatible with G.
Finally, the implication from 3 to 1 for probability measures is proved in (Geiger & Pearl,
1988; Geiger et al., 1990). Here I generalize the proof to algebraic plausibility measures.
Notice that to prove the implication from 3 to 1, it suces to show that if X is not dseparated from Y by Z in G, then there is a plausibility measure Pl 2 PLD;;
 such that
IPlrv (X; Y jZ) does not hold. To guarantee that such a plausibility measure exists in PLD;;
,
we have to ensure that there are \enough" plausibility measures in PLD;;
 in the following
technical sense.

Definition 5.9: A BN-compatible domain (D; ; 
) is rich if there exist d; d0 2 D such
that (1) (d; d0 ) 2 D(), (2) d  d0 = > and (3) if x = x1 
 : : : 
 xk , where each xi is either
d or d0 and k < n, then (d; x), (x; d), (d0 ; x), and (x; d0 ) are all in D(
) (intuitively, D(
)

contains all products involving d and d0 of length at most n).

All the domains for the cps's we have considered are easily seen to be rich.

Theorem 5.10: Suppose that plausibility measures in PLD;;
 take values in a rich BNcompatible domain. Then if IPlrv (X; YjZ) holds for every plausibility measure in PLD;;

compatible with G, then d-sep G (X; YjZ).
I remark that independence and d-separation for various approaches to representing sets
of probability measures using Bayesian networks are discussed by Cozman (2000b, 2000a).
However, the technical details are quite different from the approach taken here.

6. Conclusion

I have considered a general notion of conditional plausibility that generalizes all other standard notions of conditioning in the literature, and examined various requirements that
could be imposed on conditional plausibility. One set of requirements, those that lead to
algebraic cps's, was shown to suce for the construction of Bayesian networks. Further
assuming that the range D of the plausibility measure is a BN-compatible domain suces
for all the more quantitative properties of Bayesian networks to hold and for d-separation
to characterize the independencies. It should also be clear that standard constructions like
belief propagation in Bayesian networks (Pearl, 1988) can also be applied to algebraic cps's
with ranges that are BN-compatible, since they typically use only basic properties of conditioning, addition, and multiplication, all of which hold in BN-compatible domains (using
379

fiHalpern

 and 
). In particular, these results apply to sets to probability measures, provided that

they are appropriately represented as plausibility measures. The particular representation
of sets of probability measures advocated in this paper was also shown to have a number of
other attractive properties.
The results of this paper show that Alg1{4 are sucient conditions for representing
a measure of uncertainty that is acceptable as a Bayesian network. They may not be
necessary. It would be interesting to see if other natural conditions also suce. Similarly, I
have focused only on acceptable cps's, that is, ones that satisfy Acc1{4. Acc3 and Acc4 are
nontrivial conditions; it would be of interest to see to what extent they could be weakened
while still being able to prove results in the spirit of this paper. I leave these questions to
future research.

Appendix A. Proofs

In this section I give the proofs of Theorems 4.5, 5.6, and 5.10. I repeat the statement of
the results for the convenience of the reader.

Lemma A.1: Suppose that (W; F ; F 0 ; Pl) is a cps, A1 ; : : : ; An is a partition of W , X; A1 ; : : : ; An 2
F , and Y 2 F 0 . Then
Pl(X jY ) = fi:A \Y 2F 0 g Pl(X jAi \ Y ) 
 Pl(Ai jY ):12
i

Proof: Using an easy induction argument, it follows from Alg1 that
Pl(X jY ) = ni=1 Pl(X \ Ai jY ):
If Ai \ Y 2= F 0 , then it follows from Acc4 that Pl(Ai jY ) = ?. Thus, by CPl3, Pl(X \ Ai jY ) =
?. Using Lemma 3.3, it follows that
Pl(X jY ) = fi:A \Y 2F 0 g Pl(X \ Ai jY ):
If Ai \ Y 2 F 0 , then it follows from Alg2 that Pl(X \ Ai jY ) = Pl(X jAi \ Y ) 
 Pl(Ai jY ).
i

Thus,

Pl(X jY ) = fi:A \Y 2F 0 g Pl(X jAi \ Y ) 
 Pl(Ai jY );
i

as desired.

Theorem 4.5: CIRV1{4 hold for all algebraic cps's.
Proof: CIRV1 is immediate from the fact that independence is symmetric.
For CIRV2, suppose that IPlrv (X; Y [ Y0 jZ). We must show IPlrv (X; YjZ). That is, we
must show that IPl (X = x; Y = yjZ = z), for all x, y, and z. This requires showing two

things.

0 , then Pl(X A Y ) Pl(A Y ) = Pl(X A Y ) by Alg2. Thus, the terms
12. Notice that if A Y
arising on the right-hand side of the equation in Lemma A.1 are in Dom( ). This means that there is
no need to put in parentheses; is associative on terms in Dom( ).
i \

2 F

j

i \




ij

\

ij







380

fiConditional Plausibility Measures and Bayesian Networks

2(a). If X = x \ Z = z 2 F 0 , then
Pl(Y = yjX = x \ Z = z) = Pl(Y = yjZ = z):
2(b). If Y = y \ Z = z 2 F 0 , then
Pl(X = xjY = y \ Z = z) = Pl(X = xjZ = z):
For 2(a), suppose that Pl(X = x \ Z = z) 2 F 0 . From IPl (X; Y [ Y0jZ), it follows that
IPl (X = x; Y = y \ Y0 = y0jZ = z) for all y0 . Hence
Pl(Y = y \ Y0 = y0 jX = x \ Z = z) = Pl(Y = y \ Y0 = y0 jZ = z)

(4)

for all y0 2 R(Y0 ). From (4) it follows that

y0 Pl(Y = y \ Y0 = y0 jX = x \ Z = z) = y0 Pl(Y = y \ Y0 = y0 jZ = z):
Thus,
Pl([y0 Y = y \ Y0 = y0 jX = x \ Z = z) = Pl([y0 Y = y \ Y0 = y0 jZ = z):
Since [y0 (Y = y \ Y0 = y0 ) = Y = y, 2(a) holds.
For 2(b), from IPlrv (X; Y [ Y0 jZ), it follows that if Y = y \ Y0 = y0 \ Z = z 2 F 0 , then
Pl(X = xjY = y \ Y0 = y0 \ Z = z) = Pl(X = xjZ = z):

(5)

From (5) and Lemma A.1, it follows that
Pl(X = xjY = y \ Z = z)
= fy0 :Y=y\Y0 =y0 \Z=z2F 0 g Pl(X = xjY = y \ Y0 = y0 \ Z = z) 
 Pl(Y0 = y0 jY = y \ Z = z)
= fy0 :Y=y\Y0 =y0 \Z=z2F 0 g Pl(X = xjZ = z) 
 Pl(Y0 = y0 jY = y \ Z = z):
(6)
0
0
0
0
0
By Acc4, it follows that if Y = y \ Y = y \ Z = z 2= F , then Pl(Y = y jY = y \
Z = z) = ?. Thus, by Lemma 3.3, Alg1, CPl2, and CPl4,

fy0:Y=y\Y0=y0\Z=z2F 0 gPl(Y0 = y0 jY = y \ Z = z)
= y0 Pl(Y0 = y0 jY = y \ Z = z)
= Pl(W jY = y \ Z = z)
= >:

(7)

The next step is to apply distributivity (Alg3) to the last line of (6). To do this, we
must show that certain tuples are in Dom() and Dom(
), respectively. Since
(Pl(X = xjY = y \ Y0 = y0 \ Z = z); Pl(Y0 = y0 jY = y \ Z = z) 2 Dom(
);
from (5) it follows that
(Pl(X = xjZ = z); Pl(Y0 = y0 jY = y \ Z = z)) 2 Dom(
):
381

fiHalpern

If fyi0 1 ; : : : ; yi0 g = fy0 2 R(Y0 ) : Y = y \ Y0 = y0 \ Z = z 2 F 0 g, then clearly
k

(Pl(Y0 = y0i1 jY = y \ Z = z); : : : ; Pl(Y0 = y0i jY = y \ Z = z) 2 Dom():
k

Moreover, using (5) again and Alg2, it follows that
Pl(X = xjZ = z) 
 Pl(Y0 = y0i jY = y \ Z = z) = Pl(X = x \ Y0 = y0i jY = y \ Z = z):
k

k

Thus, (Pl(X = xjZ = z)
Pl(Y0 = y0i1 jY = y\Z = z); : : : ; Pl(X = xjZ = z)
Pl(Y0 = y0i jY = y\
Z = z) 2 Dom(). Finally, since (7) shows that fy0:Y=y\Y0=y0\Z=z2F 0g = > and, by the
proof of Lemma 3.4, (d; >) 2 Dom(
) for all d 2 Range(Pl), it follows that
k

(Pl(XjZ = z); fy0 :Y=y\Y0 =y0 \Z=z2F 0 g Pl(Y0 = y0 jY = y \ Z = z)) 2 Dom(
):
It now follows, using Alg3, (7), and Lemma 3.4, that

fy0:Y=y\Y0=y0\Z=z2F 0gPl(X = xjZ = z) 
 Pl(Y0 = y0 jY = y \ Z = z)
= Pl(X = xjZ = z) 
 (fy0 :Y=y\Y0 =y0 \Z=z2F 0 g Pl(Y0 = y0 jY = y \ Z = z))
= Pl(X = xjZ = z) 
 >
= Pl(X = xjZ = z):
Thus, from (6), it follows that Pl(X = xjY = y \ Z = z) = Pl(X = xjZ = z). This com-

pletes the proof of 2(b) and CIRV2.
For CIRV3, suppose that IPlrv (X; Y [ Y0 jZ). We must show that IPlrv (X; YjY0 [ Z). This
again requires showing two things:
3(a). If X = x \ Y0 = y0 \ Z = z 2 F 0 , then
Pl(Y = yjX = x \ Y0 = y0 \ Z = z) = Pl(Y = yjY0 = y0 \ Z = z):
3(b). If Y = y \ Y0 = y0 \ Z = z 2 F 0 , then
Pl(X = xjY = y \ Y0 = y0 \ Z = z) = Pl(X = xjY0 = y0 \ Z = z):
For 3(a), suppose that X = x\Y0 = y0 \Z = z 2 F 0 . Thus, by Acc3, X = x\Z = z 2 F 0 .
Since IPlrv (X; Y [ Y0 jZ), it follows that
Pl(Y = y00 \ Y0 = y0 jX = x \ Z = z) = Pl(Y = y00 \ Y0 = y0 jZ = z)
for all y00 2 R(Y). Applying Alg2 to each side of (8), it follows that
Pl(Y = yjY0 = y0 \ X = x \ Z = z) 
 Pl(Y0 = y0 jX = x \ Z = z)
= Pl(Y = yjY0 = y0 \ Z = z) 
 Pl(Y0 = y0 jZ = z):
Thus, to prove 3(a), it follows from Alg4 that it suces to show that
Pl(Y0 = y0 jX = x \ Z = z) = Pl(Y0 = y0 jZ = z) 6= ?:
382

(8)

fiConditional Plausibility Measures and Bayesian Networks

But by (8) and Alg1, it follows that
Pl(Y0 = y0 jX = x \ Z = z)
= y00 2R(Y) Pl(Y = y00 \ Y0 = y0 jX = x \ Z = z)
= y00 2R(Y) Pl(Y = y00 \ Y0 = y0 jZ = z)
= Pl(Y0 = y0 jZ = z);
as desired. Moreover, since X = x \ Y0 = y0 \ Z = z 2 F 0 , it follows from Acc4 that
Pl(Y0 = y0 jZ = z) 6= ?.
For 3(b), suppose that Y = y \ Y0 = y0 \ Z = z 2 F 0 . Since IPlrv (X; Y [ Y0 jZ), it follows
that
Pl(X = xjY = y \ Y0 = y0 \ Z = z) = Pl(X = xjZ = z):
Thus, to prove 3(b), it suces to show that
Pl(X = xjY0 = y0 \ Z = z) = Pl(X = xjZ = z):
(9)
Recall that we are assuming that IPlrv (X; Y [ Y0 jZ). By CIRV2, it follows that IPlrv (X; Y0 jZ).
Thus, (9) is immediate from 2(b) (since Y = y \ Y0 = y0 \ Z = z 2 F 0 implies that Y0 = y0 \
Z = z 2 F 0).
Finally, consider CIRV4. Suppose that IPlrv (X; YjZ) and IPlrv (X; Y0 jY [ Z). We must
show that IPlrv (X; Y [ Y0 jZ). As usual, this requires showing two things:
4(a). If Y = y \ Y0 = y0 \ Z = z 2 F 0 , then
Pl(X = xjY = y \ Y0 = y0 \ Z = z) = Pl(X = xjZ = z):
4(b). If X = x \ Z = z 2 F 0 , then
Pl(Y = y \ Y0 = y0 jX = x \ Z = z) = Pl(Y = y \ Y0 = y0 jZ = z):
Both 4(a) and 4(b) are straightforward. For 4(a), suppose that Y = y\Y0 = y0 \Z = z 2
F 0 . Since IPlrv (X; Y0 jY [ Z), it follows that
Pl(X = xjY = y \ Y0 = y0 \ Z = z) = Pl(X = xjY = y \ Z = z):
And since IPlrv (X; YjZ), it follows that
Pl(X = xjY = y \ Z = z) = Pl(X = xjZ = z):
Thus we have 4(a).
For 4(b), suppose that X = x \ Z = z 2 F 0 . There are now two cases to consider. If
Pl(Y = yjX = x \ Z = z) 6= ? then, by Acc4, X = x \ Y = y \ Z = z 2 F 0 . Moreover, by
Alg2,
Pl(Y = y\Y0 = y0 jX = x\Z = z) = Pl(Y0 = y0 jX = x\Y = y\Z = z)
Pl(Y = yjX = x\Z = z):
(10)
Since IPlrv (X; Y0 jY [ Z), it follows that
Pl(Y0 = y0 jX = x \ Y = y \ Z = z) = Pl(Y0 = y0 jY = y \ Z = z):
383

fiHalpern

And since IPlrv (X; YjZ), it follows that Pl(Y = yjX = x \ Z = z) = Pl(Y = yjZ = z). Plugging this into (10) and applying Alg2 again gives
Pl(Y = y \ Y0 = y0 jX = x \ Z = z)
= Pl(Y0 = y0 jY = y \ Z = z) 
 Pl(Y = yjZ = z)
= Pl(Y = y \ Y0 = y0 jZ = z);
as desired.
Now if Pl(Y = yjX = x \ Z = z) = ?, then by CPl3, it follows that Pl(Y = y \
Y0 = y0 jX = x\Z = z) = ?. Moreover, since IPlrv (X; YjZ), it follows that Pl(Y = yjZ = z) =
?. Applying CPl3, we get that Pl(Y = y \ Y0 = y0 jZ = z) = ?. Thus, again 4(b) holds.

Theorem 5.6: If (G; f ) is (D; ; 
)-representable then there is a unique cpm Pl 2
PLD;;
 such that (G; f ) represents Pl.
Proof: Given (G; f ), suppose without loss of generality that X = hX1 ; : : : ; Xn i is a topo-

logical sort of the nodes in G. I now define the plausibility measure Pl determined by (G; f ).
I start by defining Pl(G;f ) on sets of the form X = x.
It easily follows from Alg2 that if Pl 2 PLD;;
 and Pl(X1 = x1 \ : : : \ Xn,1 = xn,1 ) 6=
?, then
Pl(X = x) = Pl(Xn = xn jX1 = x1 \ : : : \ Xn,1 = xn,1 )

Pl(Xn,1 = xn,1 jX1 = x1 \ : : : \ Xn,2 = xn,2 )

(11)
   
 Pl(X2 = x2jX1 = x1 ) 
 Pl(X1 = x1):
Thus, an algebraic plausibility measure satisfies an analogue of the chain rule for probability.
(Since 
 in D is assumed to be associative, no parentheses are required here. However, even
without this assumption, it follows easily from Alg2 that 
 is in fact associative on tuples
(a; b; c) of the form (Pl(U1 jU2 ); Pl(U2 jU3 ); Pl(U3 jU4 )), where U1  U2  U3  U4 , which are
the only types of tuples that arise in (11). Associativity will be more of an issue below.)
If Pl is compatible with G, then in fact
Pl(X = x) = Pl(Xn = xn j \X 2Par (X ) Xj = xj )

Pl(Xn,1 = xn,1 j \X 2Par (X ,1) Xj = xj )

(12)
   
 (X1 = x1):
(If ParG (Xk ) = ;, then Pl(Xk = xk j\X 2Par (X ) Xj = xj ) is just taken to be Pl(Xk = xk ).)
It is clear from (12) that Pl(G;f ) (X = x) must be dX ;G;x 
    
 dX1 ;G;x.
Note that every subset of W can be written as a disjoint union of events of the form
X = x. Thus, if U 2 F , define
Pl(G;f ) (U ) = fx:X=xU gdX ;G;x 
    
 dX1 ;G;x:
For conditional plausibilities, suppose that Pl(G;f ) (V ) 6= ?, so that V 2 F 0 . Let
fx1 ; : : : ; xk g = fx : X = x  V g. It follows easily from BN6, BN7, R1, and R2 that
(Pl(G;f ) (X = x1 ); : : : ; Pl(G;f ) (X = xk )) 2 D(). Thus, by BN8, if X = x  V , then
Pl(G;f ) (X = x)  Pl(G;f ) (V ). By BN5, for each j , there exists dX=x jV such that
(dX=x jV ; Pl(G;f ) (V )) 2 D(
) and dX=x jV 
 Pl(G;f ) (V ) = Pl(G;f ) (X = x);
j

n

G

j

j

G

G

n

k

n

n

j

j

j

384

fiConditional Plausibility Measures and Bayesian Networks

it follows from BN4 that dX=x jV is the unique element in D with this property. Moreover,
by BN5, (dX=x1 jV ; : : : ; dX=x jV ) 2 D(). Define Pl(G;f ) (U jV ) = fx:X=xU \V g dX=xjV
(where Pl(G;f ) (;jV ) is taken to be ?). Note for future reference that it follows from BN5
that (Pl(G;f ) (U jV ); Pl(G;f ) (V )) 2 D(
) and
Pl(G;f ) (U jV ) 
 Pl(G;f ) (V ) = Pl(G;f ) (U \ V ):
(13)
This completes the definition of Pl(G;f ) . It remains to check that it is an algebraic cpm
that is represented by (G; f ). Thus, we must check that Alg1{4 and CPl1{4 hold. Alg1
is immediate from the definitions and BN1 and BN2 (BN2 is necessary for the case that
one of the disjoint sets is empty); Alg3 is immediate from BN3 and Alg4 is immediate
from BN4. For Alg2, note that if Pl(G;f ) (V ) 6= ? and Pl(G;f ) (V 0 ) 6= ? then, by (13),
Pl(G;f ) (U \ V jV 0 ) 
 Pl(G;f ) (V 0 ) = Pl(G;f ) (U \ V \ V 0 ) and
(Pl(G;f )(U jV \ V 0 ) 
 Pl(G;f ) (V jV 0 )) 
 Pl(G;f ) (V 0 )
= Pl(G;f )(U jV \ V 0 ) 
 (Pl(G;f ) (V jV 0 ) 
 Pl(G;f ) (V 0 ))
= Pl(G;f ) (U jV \ V 0 ) 
 Pl(G;f ) (V \ V 0 )
= Pl(G;f ) (U \ V \ V 0 ):
(Note that the associativity of 
 is being used here.) Thus, by BN4,
Pl(G;f ) (U \ V jV 0 ) = Pl(G;f )(U jV \ V 0 ) 
 Pl(G;f ) (V jV 0 ):
CPl1 is immediate by definition (the empty sum is taken to be ?). For CPl2, note that
by (13), Pl(G;f ) (W jV ) 
 Pl(G;f ) (V ) = Pl(G;f ) (V ). Since > 
 Pl(G;f ) (V ) = Pl(G;f ) (V ) by
BN2, it follows from BN4 that Pl(G;f ) (W jV ) = >. CPl3 follows readily from the definitions
together with BN1, BN6, and BN7. CPl4 also follows by definition.
Next we must show that (G; f ) represents Pl(G;f ) . The first step is to show that
Pl(G;f ) (X = xjParG (X ) = z) = dX =xjPar (X )=z . Note that by (13),
Pl(G;f ) (X = xjParG (X ) = z) 
 Pl(G;f ) (ParG (X ) = z) = Pl(G;f ) (X = x \ ParG (X ) = z):
By definition,
Pl(G;f ) (X = x \ ParG (X ) = z) = fx:X=x0(X =x\Par (X )=~y)g Pl(G;f ) (X = x0 ):
Each term in the \sum" on the right is the \product" of terms; indeed, the sum is over all
possible products that include dX =yjPar (X )=z as one of the terms and a term dY =yjPar (Y )=z0
for each Y 2 ParG (X ), where y is the component of z corresponding to Y . By using BN1,
BN3, R1, and R2, it is not hard to show that
Pl(G;f ) (X = y \ ParG (X ) = z)
= f:X=x0 (X =x\Par (X )=~y) Pl(G;f ) (X = x0 )
(14)
= dX =xjPar =z 
 Pl(G;f ) (ParG (X ) = z):
It now follows from BN4 that Pl(G;f ) (X = xjParG (X ) = z) = dX =xjPar (X )=z .
To show that Pl(G;f ) (X = xjNDG (X ) = ~y \ ParG (X ) = z) = dX =xjPar (X )=z , it suces
to show that
Pl(G;f ) (X = x \ NDG (X ) = ~y \ ParG (X ) = z)
(15)
= dX =xjPar (X )=z 
 Pl(G;f ) (NDG (X ) = ~y \ ParG (X ) = z);
j

k

G

G

G

G

G

G

G

G

G

385

fiHalpern

for then the result follows by BN5. (15) can be shown much like (14), but now the commutativity of 
 (BN1) is essential. That is, the expressions for Pl(G;f ) (X = x \ NDG (X ) =
~y \ ParG (X ) = z) and dX =xjPar (X )=z 
 Pl(G;f ) (NDG (X ) = ~y \ ParG(X ) = z) involve
the same terms, but not necessarily in the same order. With commutativity, they can be
permuted so that they are in the same order.
Similar arguments, which I leave to the reader, show that Pl(G;f ) (NDG (X ) = ~yjX =
x \ ParG (X ) = z) = Pl(G;f )(NDG(X ) = ~yjParG (X ) = z). Thus, (G; f ) represents Pl(G;f ) .
G

Theorem 5.10: Suppose that (D; ; 
) is a rich BN-compatible domain. Then if IPlrv (X; YjZ)
holds for every plausibility measure in PLD;;
 compatible with G, then d-sep G (X; YjZ).
Proof: Suppose that X is not d-separated from Y by Z in G. Then there is some X 2 X
and Y 2 Y such that X is not d-separated from Y by Z in G. I construct a cpm in
Pl 2 PLD;;
 such that IPlrv (X; Y jZ) does not hold, using the techniques of (Geiger et al.,

1990).
As shown in (Geiger et al., 1990, Lemma 9), if X is not d-separated from Y in G, there
exists a subgraph G0 of G such that
1. G0 includes all the nodes in G but only a subset of the edges in G,
2. X is not d-separated from Y by Z in G0 .
3. the edges E 0 in G0 consist only of those specified below:
(a) a trail q from X to Y ,
(b) for every head-to-head node Xi on the trail q, there is a directed path pi in G0 to
a node in Z; moreover, the paths pi do not share any nodes and the only node
that pi shares with q is Xi .
Note that every node in G0 has either 0, 1, or 2 parents in G0 . Let (G0 ; f ) be a quantitative
Bayesian network such that for each node in X in G0 with no parents in G0 , the cpt f (X )
is such that dX =0 = d and dX =1 = d0 . If a node X in G0 has one parent, say X 0 , then the
cpt f (X ) is such that dX =ijX 0 =j is > if i = j and ? if i 6= j . Finally, if X has two parents,
say X 0 and X 00 , the cpt f (X ) is such that dX =kjX 0 =i\X 00 =k is > if k = i  j ( mod2) and is ?
otherwise. Since d  d0 = > and BN2 guarantees that >  ? = >, the construction satisfies
R1. The richness of D guarantees that R2 holds. By Theorem 5.6, there is a (unique)
plausibility measure in Pl 2 PLD;;
 that is represented by (G0 ; f ). It is easy to check that
Pl is compatible with G as well. There are three cases to consider:
 Suppose that X has no parents in G0. Then it is easy to see that IPlrv (X; YjZ) for all
Y and Z (and, in particular, if Y = NDG(X ) and Z = ParG(X )).
 Suppose that X has one parent in G0, say X 0 . Then it is easy to see that IPlrv (X; YjZ)
holds for all Y and Z such that X 0 2 Z. Since X 0 is a parent of X in G, again
IPlrv (X; NDG(X )jParG(X )) must hold.
 Finally, if X has two parents in G0, say X 0 and X 00 , then it is easy to see that
IPlrv (X; YjZ) holds for all Y and Z such that fX 0 ; X 00 g  Z. Since X 0 and X 00 are
parents of X in G, again IPlrv (X; NDG (X )jParG (X )) must hold.
386

fiConditional Plausibility Measures and Bayesian Networks

Acknowledgments
A preliminary version of this paper appears in Uncertainty in Artificial Intelligence, Proceedings of the Sixteenth Conference, 2000. I thank Serafn Moral, Fabio Cozman, Peter
Walley, and the anonymous referees of both the UAI and journal version of the paper for
very useful comments. This work was supported in part by the NSF, under grants IRI-9625901 and IIS-0090145.

References

Campos, L., & Huete, J. F. (1993). Independence concepts in upper and lower probabilities. In Bouchon-Meunier, B., Valverde, L., & Yager, R. R. (Eds.), Uncertainty in
Intelligent Systems, pp. 85{96. North-Holland, Amsterdam.
Campos, L., & Huete, J. F. (1999a). Independence concepts in possibility theory: Part I.
Fuzzy Sets and Systems, 103 (1), 127{152.
Campos, L., & Huete, J. F. (1999b). Independence concepts in possibility theory: Part II.
Fuzzy Sets and Systems, 103 (3), 487{505.
Campos, L., & Moral, S. (1995). Independence concepts for sets of probabilities. In
Proc. Eleventh Conference on Uncertainty in Artificial Intelligence (UAI '95), pp.
108{115.
Cousa, I., Moral, S., & Walley, P. (1999). Examples of independence for imprecise probabilities. In Proc. First Intl. Symp. Imprecise Probabilities and Their Applications.
Cozman, F. G. (1998). Irrelevance and independence relations in Quasi-Bayesian networks.
In Proc. Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI '98),
pp. 89{96.
Cozman, F. G. (2000a). Credal networks. Artificial Intelligence, 120 (2), 199{233.
Cozman, F. G. (2000b). Separation properties of setes of probability measures. In Proc. Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI 2000).
Cozman, F. G., & Walley, P. (1999). Graphoid properties of epistemic irrelevance and
independence. Unpublished manuscript.
Darwiche, A. (1992). A Symbolic Generalization of Probability Theory. Ph.D. thesis, Stanford University.
Darwiche, A., & Ginsberg, M. L. (1992). A symbolic generalization of probability theory.
In Proceedings, Tenth National Conference on Artificial Intelligence (AAAI '92), pp.
622{627.
Darwiche, A., & Goldszmidt, M. (1994). On the relation between kappa calculus and probabilistic reasoning. In Proc. Tenth Conference on Uncertainty in Artificial Intelligence
(UAI '94), pp. 145{153.
387

fiHalpern

Dubois, D., Fari~nas del Cerro, L., Herzig, A., & Prade, H. (1994). An ordinal view of
independence with applications to plausible reasoning. In Proc. Tenth Conference on
Uncertainty in Artificial Intelligence (UAI '94), pp. 195{203.
Dubois, D., & Prade, H. (1990). An introduction to possibilistic and fuzzy logics. In
Shafer, G., & Pearl, J. (Eds.), Readings in Uncertain Reasoning, pp. 742{761. Morgan
Kaufmann, San Francisco, Calif.
Fagin, R., & Halpern, J. Y. (1991). A new approach to updating beliefs. In Bonissone, P.,
Henrion, M., Kanal, L., & Lemmer, J. (Eds.), Uncertainty in Artificial Intelligence 6,
pp. 347{374. Elsevier Science Publishers, Amsterdam.
Finetti, B. d. (1936). Les probabilites nulles. Bulletins des Science Mathematiques (premiere
partie), 60, 275{288.
Fonck, P. (1994). Conditional independence in possibility theory. In Proc. Tenth Conference
on Uncertainty in Artificial Intelligence (UAI '94), pp. 221{226.
Friedman, N., & Halpern, J. Y. (1995). Plausibility measures: a user's guide. In
Proc. Eleventh Conference on Uncertainty in Artificial Intelligence (UAI '95), pp.
175{184.
Geiger, D., & Pearl, J. (1988). On the logic of causal models. In Proc. Fourth Workshop
on Uncertainty in Artificial Intelligence (UAI '88), pp. 136{147.
Geiger, D., Verma, T., & Pearl, J. (1990). Identifying independence in bayesian networks.
Networks, 20, 507{534.
Gilboa, I., & Schmeidler, D. (1993). Updating ambiguous beliefs. Journal of Economic
Theory, 59, 33{49.
Goldszmidt, M., & Pearl, J. (1992). Rank-based systems: A simple approach to belief
revision, belief update and reasoning about evidence and actions. In Principles of
Knowledge Representation and Reasoning: Proc. Third International Conference (KR
'92), pp. 661{672. Morgan Kaufmann, San Francisco, Calif.
Halpern, J. Y. (2000). Conditional plausibility measures and Bayesian networks. In
Proc. Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI 2000), pp.
247{255. To appear, Journal of A.I. Research.
Keynes, J. M. (1921). A Treatise on Probability. Macmillan, London.
Levi, I. (1985). Imprecision and uncertainty in probability judgment. Philosophy of Science,
52, 390{406.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Francisco, Calif.
Popper, K. R. (1968). The Logic of Scientific Discovery (revised edition). Hutchison,
London. The first version of this book appeared as Logik der Forschung, 1934.
388

fiConditional Plausibility Measures and Bayesian Networks

Renyi, A. (1964). Sur les espaces simples de probabilites conditionelles. Annales de l'Institut
Henri Poincare, Nouvelle serie, Section B, 1, 3{21. Reprinted as paper 237 in Selected
Papers of Alfred Renyi, III: 1962{1970, Akademia Kiado, 1976, pp. 284{302.
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton University Press, Princeton, N.J.
Shenoy, P. P. (1994). Conditional independence in valuation based systems. International
Journal of Approximate Reasoning, 10, 203{234.
Shenoy, P. P., & Shafer, G. (1990). An axiomatic framework for Bayesian and belief-function
propagation. In Shachter, R., Levitt, T., Kanal, L., & Lemmer, J. (Eds.), Uncertainty
in Artificial Intelligence 4, pp. 169{198.
Spohn, W. (1988). Ordinal conditional functions: a dynamic theory of epistemic states.
In Harper, W., & Skyrms, B. (Eds.), Causation in Decision, Belief Change, and
Statistics, Vol. 2, pp. 105{134. Reidel, Dordrecht, Netherlands.
Verma, T. (1986). Causal networks: semantics and expressiveness. Technical report R{103,
UCLA Cognitive Systems Laboratory.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities, Vol. 42 of Monographs
on Statistics and Applied Probability. Chapman and Hall, London.
Wang, Z., & Klir, G. J. (1992). Fuzzy Measure Theory. Plenum Press, New York.
Weydert, E. (1994). General belief measures. In Proc. Tenth Conference on Uncertainty in
Artificial Intelligence (UAI '94), pp. 575{582.
Wilson, N. (1994). Generating graphoids from generalized conditional probability. In
Proc. Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), pp. 583{
591.
Zadeh, L. A. (1978). Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems,
1, 3{28.

389

fiJournal of Artificial Intelligence Research 14 (2001) 205-230

Submitted 12/00; published 5/01

Domain Filtering Consistencies
Romuald Debruyne

Romuald.Debruyne@emn.fr
Member of the Coconut group
Ecole des Mines de Nantes,
La Chantrerie, 4, Rue Alfred Kastler, 44307 Nantes Cedex 3 - France

Christian Bessiere

bessiere@lirmm.fr

Member of the Coconut group
LIRMM - CNRS UMR 5506, 161 rue Ada, 34392 Montpellier Cedex 5 - France

Abstract
Enforcing local consistencies is one of the main features of constraint reasoning. Which
level of local consistency should be used when searching for solutions in a constraint network
is a basic question. Arc consistency and partial forms of arc consistency have been widely
studied, and have been known for sometime through the forward checking or the MAC
search algorithms. Until recently, stronger forms of local consistency remained limited to
those that change the structure of the constraint graph, and thus, could not be used in
practice, especially on large networks. This paper focuses on the local consistencies that
are stronger than arc consistency, without changing the structure of the network, i.e., only
removing inconsistent values from the domains. In the last five years, several such local
consistencies have been proposed by us or by others. We make an overview of all of them,
and highlight some relations between them. We compare them both theoretically and
experimentally, considering their pruning efficiency and the time required to enforce them.

1. Introduction
There are more and more applications in artificial intelligence that use constraint networks
(CNs) to solve combinatorial problems, ranging from design to diagnosis, resource allocation
to car sequencing, natural language understanding to machine vision. Finding a solution in
a constraint network involves looking for a set of value assignments, one for each variable,
so that all the constraints are simultaneously satisfied (Meseguer, 1989; Tsang, 1993). This
task is NP-hard and many exponential time algorithms have been proposed to solve this
problem. These algorithms, which make a systematic exploration of the search space, all
have backtracking as a basis. As long as the unassigned variables have values consistent
with the partial instantiation, they extend it by assigning values to variables. Otherwise,
a dead-end is reached and some previous assignments have to be changed before going on
with the partial instantiation extension. The explicit constraints of the network together
induce some implicit constraints. Since basic search algorithms do not record these implicit
constraints, they waste time by repeatedly detecting the local inconsistencies caused by
them. Filtering techniques are essential to reduce the size of the search space and so to
improve the efficiency of search algorithms. They can be used during a preprocessing step to
remove once and for all some local inconsistencies that otherwise would have been repeatedly
found during search (Dechter & Meiri, 1994). They can also be maintained during search.
c
2001
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDebruyne & Bessiere

Search algorithms differ in the kind of local consistency they achieve after each choice
of a value for a variable. Most of them enforce partial arc consistency, going from forward
checking (FC,Golomb & Baumert, 1965; Haralick & Elliott, 1980), which only removes the
values directly arc inconsistent with the last assignment, to really full look-ahead (RFL,
Gaschnig, 1974), which achieves full arc consistency. Arc consistency (AC) and partial
forms of arc consistency are widely used for two reasons. First, they have low space and
time complexities, while path consistency and higher levels of k-consistency, which were
for a long time the only other options, are prohibitive and can change the structure of the
network. Moreover, until 1995, more pruningful local consistencies seemed uninteresting
since experimental evaluations of search algorithms showed that the limited local consistency
used by forward checking was the best choice (Nadel, 1988; Kumar, 1992; Bacchus & van
Run, 1995). This conclusion is not surprising since the comparisons were made on very
small and easy constraint networks. On such problems, the additional cost of pruning more
values could not be outweighed by the search savings.
However, the harder a constraint network is, the more useful filtering techniques are.
More recent works (Bessiere & Regin, 1996; Sabin & Freuder, 1994; Grant & Smith, 1996)
testing search algorithms at the threshold (Cheeseman, Kanefsky, & Taylor, 1991), where
most of the hard problems are expected to be, show that using more pruningful local
consistencies can be worthwhile. Their conclusion is that maintaining arc consistency during
search (MAC), namely achieving AC both after the choice of a value for a variable and after
the refutation of such a choice, outperforms forward checking on hard problems. The good
behavior of MAC is even more significant on large problems, especially when domains are
large. It is conceivable that on very difficult instances, maintaining an even more pruningful
local consistency may pay off. Obviously, such an algorithm would waste seconds on easy
CNs but it would save many minutes (hours ?) on very hard problems, reducing the set of
pathological CNs on which search is really prohibitive.
In this paper we study the local consistencies as preprocessing filtering techniques. This
is a preliminary work before trying to determine which local consistency is the most advantageous to be maintained during search. In the last five years, many new local consistencies
have been proposed. In the remaining of this paper, we focus our attention on those that
leave unchanged the structure of the network since they are the only able to be used on large
CNs. In addition to an overview of these local consistencies that only remove inconsistent
values, we both compare, theoretically and experimentally, their pruning efficiency and the
time needed to enforce them.

2. Definitions and Notations
A network of binary constraints P = (X , D, C) is defined by a set X = {i, j, . . . } of n
variables, each taking value in its respective finite domain Di , Dj , . . . elements of D, and a
set C of e binary constraints. d is the size of the largest domain. A binary constraint Cij
is a subset of the Cartesian product Di  Dj that denotes the compatible pairs of values
for i and j. We denote Cij (a, b) = true to specify that ((i, a), (j, b))  Cij . We then say
that (j, b) is a support for (i, a) on Cij . Checking whether a pair of values is allowed by
a constraint is called a constraint check . With each CN we associate a constraint graph
in which nodes represent variables and arcs connect pairs of variables that are constrained
206

fiDomain Filtering Consistencies

explicitly. c is the number of 3-cliques in the constraint graph and g is the maximum degree
of a node in the constraint graph. The neighborhood of i is the set of variables adjacent to i
in the constraint graph. A domain D0 = {Di0 , Dj0 , . . .} is a sub-domain of D = {Di , Dj , . . .}
if i, Di0  Di . An instantiation I of a set of variables S is a set of value assignments
{Ij }jS , one for each variable belonging to S, s.t. j  S, Ij  Dj . An instantiation I of
S satisfies a constraint Cij if {i, j} 6 S or Cij (Ii , Ij ) is true. An instantiation is consistent
if it satisfies all the constraints. A pair of values ((i, a), (j, b)) is path consistent if for all
k  X s.t. j 6= k 6= i 6= j, this pair of values can be extended to a consistent instantiation
of {i, j, k}. (j, b) is a path consistent support for (i, a) if (a, b)  Cij and ((i, a), (j, b)) is
path consistent. A solution of P = (X , D, C) is a consistent instantiation of X . A value
(i, a) is consistent if there is a solution I such that Ii = a, and a CN is consistent if it has
at least one solution. We denote by P |Di ={a} the CN obtained by restricting Di to {a} in
P . If LC is a local consistency, a CN P is not LC-consistent iff LC does not hold on P .
A CN P is LC-inconsistent iff we cannot obtain a LC-consistent constraint network by
deletion of some local inconsistencies in P . If a local consistency LC is used to detect the
inconsistency of instantiations no longer than 1, we can say that a CN P = (X , D, C) is
LC-inconsistent iff there is no sub-domain D0 of D such that LC holds on (X , D0 , C).

3. The Local Consistencies Studied
Filtering techniques can be used to detect the inconsistency of a CN, and under some
assumptions, they can ensure a backtrack-free search (Freuder, 1982, 1985). However,
their usual purpose is not to find a solution in a constraint network. They remove some
local inconsistencies and so delete some regions of the search space that do not contain
any solution. The resulting CN is equivalent to the initial one since the set of solutions
is unchanged, but if substantial reductions are made the search becomes easier. In this
section we review the basis of arc consistency, k-consistency, restricted path consistency,
and inverse consistencies. Furthermore, we extend the idea of restricted path consistency
to k-restricted path consistency and Max-restricted path consistency. We propose a new
class of local consistencies called singleton consistencies. We also show a property of path
inverse consistency that can be used to have an optimal worst case time complexity in a
path inverse consistency algorithm (Debruyne, 2000).
Arc consistency The most widely used local consistency is arc consistency. It is based
on the notion of support. A value is viable as long as it has at least one compatible value in
the domain of each neighboring variable. An AC algorithm has to remove the values that
have no support on a constraint. As in most of the filtering techniques, the value deletions
have to be propagated through the network since they can lead to the arc inconsistency of
some values that were previously viable.
k-consistency A consistent instantiation of length k-1 is k-consistent (i.e., (k-1, 1)consistent in the formalism of Freuder, 1985) if it can be extended to any additional k th
variable. The time and space complexities of enforcing k-consistency are polynomial with
the exponent dependent on k. If k  3, the constraints have to be represented in extension
to store the (k-1)-tuples deleted, and the structure of the network can be changed. This
leads to huge space requirements and subsequently important cpu time costs. In practice,
207

fiDebruyne & Bessiere

only 2-consistency, which is arc consistency in binary CNs, can be used. Although path
consistency (PC, namely 3-consistency) cannot be used on large CNs, our experiments will
involve strong path consistency (namely enforcing both arc and path consistency) because
PC has been widely studied.
Restricted path consistency The aim of Berlandier when he proposed restricted path
consistency (RPC, Berlandier, 1995) was to remove more inconsistent values than AC
while avoiding the drawbacks of path consistency. Even the most efficient PC algorithms
have to try to extend all the pairs of values (even those between two variables that are
not neighbors) to any third variable. The basis of RPC is to avoid most of this prohibitive
work. RPC performs only the most pruningful path consistency checks, namely those able
to directly delete a value. In addition to AC, an RPC algorithm checks the path consistency
of the pairs of values ((i, a), (j, b)) such that (j, b) is the only support for (i, a) in Dj . If such
a pair is path inconsistent, its deletion would lead to the arc inconsistency of (i, a). Thus
(i, a) can be removed. These few additional path consistency checks allow detecting more
inconsistent values than AC without having to delete any pair of values, and so leaving
unchanged the structure of the network.
k-restricted path consistency We can extend the idea of RPC to a more pruningful
local consistency. If RPC holds, the values that have only one support on a constraint are
such that this support is path consistent. Checking the path consistency of more supports
can remove even more values without falling into the traps of PC. k-restricted path consistency (k-RPC, Debruyne & Bessiere, 1997a) looks for a path consistent support on a
constraint for the values that have at most k supports on this constraint. RPC is 1-RPC
and AC corresponds to 0-RPC. If k-RPC holds, to achieve (k+1)-RPC we only have to
check the values that have exactly (k+1) supports on a constraint and to propagate their
possible deletion. So, it is possible to achieve AC, RPC, 2-RPC and so on, each time reusing
previous filtering effort. This adaptive enforcement can be stopped as soon as each value
has at least one path consistent support on each constraint, the constraint network being
d-RPC where d is the size of the largest domain. Indeed, if after achieving k-RPC all the
values have at most k supports on each constraint, k 0 -RPC holds for all k 0 > k.
Max-restricted path consistency A constraint network is Max-restricted path consistent (Max-RPC, Debruyne & Bessiere, 1997a) if all the values have at least one path
consistent support on each constraint, whatever is the number of supports. From the pruning efficiency point of view, Max-RPC is an upper bound for k-RPC. Achieving Max-RPC
involves deleting all the k-restricted path inconsistent values for all k. However, achieving
Max-RPC can be less expensive than enforcing a high level of k-RPC. As opposed to MaxRPC, to achieve k-RPC we have to look for the values that have at most k supports on a
constraint to know the values for which a path consistent support has to be found. This
can be expensive if k is great, the algorithm having to look for k+1 supports for each value
on each constraint. Unconditionally looking for a path consistent support avoids this costly
extra work.
k inverse consistency The aim of Freuder and Elfe when they proposed inverse consistency (Freuder & Elfe, 1996) was to achieve high order local consistencies with a good space
complexity. A k-consistency algorithm removes the instantiation of length k-1 that cannot
208

fiDomain Filtering Consistencies

be extended to any k th variable. It requires O(nk1 dk1 ) space to keep track of the deleted
instantiations. Space requirements are no longer a problem with k inverse consistency ((1,
k-1)-consistency), which removes the values that cannot be extended to a consistent instantiation including any k-1 additional variables. Its linear space complexity would allow using
it on large CNs. However, its worst case time complexity is polynomial with the exponent
dependent on k, which restricts its use to small values of k.
Path inverse consistency The first level of k inverse consistency removing more values
than AC is path inverse consistency (PIC, k = 3). By definition, (i, a) is path inverse
consistent if it can be extended to all the 3-tuples of variables containing i. However, as
said in (Freuder & Elfe, 1996), not all the 3-tuples need to be checked to enforce PIC. Only
one of the tuples (i, j, k) and (i, k, j) has to be checked. Moreover, if i is linked to neither j
nor k, (i, a) can be deleted because of (i, j, k) only if all the values of j or k are path inverse
inconsistent. In such a case, checking (i, j, k) is useless since PIC detects the inconsistency
of the network when processing j or k.
Neighborhood inverse consistency Since k inverse consistency is polynomial with the
exponent dependent on k, checking the k inverse consistency of all the values is prohibitive if
k is great. However, if the variables are not uniformly constrained, it would be worthwhile to
adapt the level of k inverse consistency to the number of constraints involving them, focusing
filtering effort on the most constrained variables (as it is done for adaptive consistency
Dechter & Pearl, 1988). This is the basis of neighborhood inverse consistency (NIC, Freuder
& Elfe, 1996), which removes the values that cannot be extended to a consistent instantiation
including all the neighboring variables. We have to point out that the behavior of NIC
is dependent on the structure of the constraint graph. If two variables i and j are not
neighbors, we can add a universal constraint allowing all the pairs of values (a, b)  Di  Dj
between i and j. The resulting CN is equivalent to the initial one since it has the same set of
solutions. However, as opposed to the other filterings studied in this paper, NIC is affected
by this change since it can remove more values. Obviously, this process increases time
complexity. On complete constraint networks, NIC tries to extend all the values to a whole
solution, namely deleting all the globally inconsistent values (named variable completability
in Freuder, 1991). This is a far more difficult task than looking for one solution. To be cost
effective, NIC has to be used only on sparse CNs, where the degree of the variables is small.
Singleton consistencies If a value (i, a) is consistent, the constraint network obtained
by restricting Di to the singleton {a} is consistent. Singleton consistencies are a class of
filtering techniques based on this remark. To detect the inconsistency of a value (i, a), a
singleton consistency filtering technique checks whether a given local consistency can detect
the possible inconsistency of P |Di ={a} . For example, singleton arc consistency (SAC, Debruyne & Bessiere, 1997b) deletes the values (i, a) such that P |Di ={a} has no arc consistent
sub-domain.1 SAC has been inspired by the strong path consistency algorithm proposed
by McGregor(McGregor, 1979). A SAC algorithm is obtained by omitting the deletions

1. Any AC algorithm can be used to know whether enforcing AC on P |Di ={a} leads to a domain wipe out,
but a lazy approach (such as LAC7 Schiex, Regin, Gaspin, & Verfaillie, 1996) is sufficient.

209

fiDebruyne & Bessiere



A binary CN is (i, j)-consistent iff i  X , Di 6=  and any consistent instantiation of i variables can be extended to a consistent instantiation including
any j additional variables.



A value a
b  Dj s.t.
consistent.
non empty



A pair of values ((i, a), (j, b)) is path consistent iff k  X , there exists c  Dk
s.t. Cik (a, c) and Cjk (b, c), otherwise it is path inconsistent. A CN is path
consistent ((2, 1)-consistent) iff no path inconsistent pair of values is allowed.



A binary CN is strongly path consistent iff it is node consistent, arc consistent
and path consistent.



A binary CN is path inverse consistent iff it is (1, 2)-consistent i.e.,
(i, a)D j, kX s.t. j 6= i 6= k 6= j, (j, b)D and (k, c)D s.t.
Cij (a, b)  Cik (a, c)  Cjk (b, c)



A binary CN is neighborhood inverse consistent iff (i, a)D, (i, a) can be
extended to a consistent instantiation including the neighborhood of i.



A binary CN is restricted path consistent iff
i  X , Di is a non empty arc consistent domain and,
(i, a)  D, for all j  X s.t. (i, a) has an unique support b in Dj ,
for all k  X linked to both i and j, c  Dk s.t. Cik (a, c) Cjk (b, c).



A binary CN is k-restricted path consistent iff
i  X , Di is a non empty arc consistent domain and,
(i, a)  D, for all Cij  C s.t. (i, a) has at most k supports in Dj ,
b  Dj s.t. Cij (a, b) and
k  X linked to both i and j, c  Dk s.t. Cik (a, c) Cjk (b, c).



A binary CN is max-restricted path consistent iff
i  X , Di is a non empty arc consistent domain and,
(i, a)  D, for all Cij  C,
b  Dj s.t. Cij (a, b) and
k  X linked to both i and j, c  Dk s.t. Cik (a, c) Cjk (b, c).



A binary CN P is singleton arc consistent iff i  X , Di 6=  and (i, a)  D,
P |Di ={a} has an arc consistent sub-domain.



A binary CN P is singleton restricted path consistent iff i  X , Di 6=  and
(i, a)  D, P |Di ={a} has a restricted path consistent sub-domain.

 Di is arc consistent iff, j  X s.t. Cij  C, there exists
Cij (a, b). A domain Di is arc consistent iff, a  Di , (i, a) is arc
A CN is arc consistent ((1, 1)-consistent) iff Di  D, Di is a
arc consistent domain.

Figure 1: The mentioned local consistencies.
210

fiDomain Filtering Consistencies

Name of
the algorithm
AC7 (Bessiere, Freuder, & Regin, 1999)
RPC2 (Debruyne & Bessiere, 1997a)
Max RPC1 (Debruyne & Bessiere, 1997a)
PC5 (Singh, 1995)
PC8 (Chmeiss & Jegou, 1996)
PIC1 (Freuder & Elfe, 1996)
PIC2 (Debruyne, 2000)
NIC1 (Freuder & Elfe, 1996)
SAC1 (Debruyne & Bessiere, 1997b)
SRPC1 (Debruyne & Bessiere, 1997b)
()

Worst case
time complexity
O(ed2 )()
O(en + ed2 + cd2 )()
O(en + ed2 + cd3 )()
O(n3 d3 )()
O(n3 d4 )
O(en2 d4 )
O(en + ed2 + cd3 )()
O(g 2 (n + ed)dg+1 )
O(en2 d4 )
O(en + n2 (e + c)d4 )

Worst case
space complexity
O(ed)
O(ed + cd)
O(ed + cd)
O(n3 d2 )
O(n2 d)2
O(n)
O(ed + cd)
O(n)
O(ed)
O(ed + cd)

optimal worst case time complexity.
Table 1: The most efficient algorithms achieving the mentioned local consistencies.3

of pairs of values in that algorithm. Many other singleton consistencies can be considered
since any local consistency can be used to detect the possible inconsistency of the CNs
P |Di ={a} with (i, a)  D. If a local consistency can be enforced in a polynomial time, the
corresponding singleton consistency also has a polynomial worst case time complexity.
The formal definitions of the local consistencies studied in this paper are presented in
Figure 1. Table 1 recalls the time and space complexities of the most efficient algorithms
enforcing them. The worst case time complexity of SAC1, SRPC1, and NIC1 have not been
proved to be optimal.

4. Relations between PIC, RPC and Max-RPC
To highlight the relations between PIC, RPC and Max-RPC, let us show a property of path
inverse consistency. As shown in (Debruyne, 2000), if we assume that the constraint network
is arc consistent, enforcing PIC requires checking even less 3-tuples than those mentioned in
(Freuder & Elfe, 1996). If (i, a) is arc consistent, it can be extended to any 3-tuple (i, j, k)
such that there is no constraint between j and k. Indeed, (i, a) has a support (j, b) on Cij
and a support (k, c) on Cik , and since j is not linked to k, ((i, a), (j, b), (k, c)) is consistent.
Furthermore, (i, a) can be extended to (i, j, k) if there is no constraint between i and k
(resp. between i and j). Indeed, (i, a) has a support b in Dj (resp. c in Dk ) and this value
being arc consistent too, it has a support c in Dk (resp. b in Dj ). So, ((i, a), (j, b), (k, c))
is consistent. Consequently, if the constraint network is arc consistent, the only 3-tuples
that have to be checked to achieve PIC correspond to the 3-cliques of the constraint graph.

2. However a O(n2 d2 ) data structure is required for the constraint representation.
3. See Section 2 for a definition of n, d, e, c, and g.

211

fiDebruyne & Bessiere

0 support

1 support
a

k

b
a

i
(A)

j

a

b

i

a

a

b

b

AC, RPC, PIC, and
Max-RPC delete (i,a)

(B) RPC, PIC, and Max-RPC
delete (i,a) because its unique support
is not path consistent

2 supports

2 supports
a

b
c
a
b

k

b

a

i

j

k

i

b
c

b

a
b
c

a

a
a

j

j

l

b

(C) (i, a) is RPC-consistent w.r.t. C ij
b u t PIC and Max-RPC
delete this value

(D) (i, a) is RPC-co nsistent w.r.t. C ij
and PIC-consistent w.r.t. C ij but
Max -RPC d eletes th is v alu e

A forbidden pair of values.

Figure 2: Examples showing the relations between PIC, RPC and Max-RPC.
Furthermore, the definition of PIC shows that any constraint network involving less than
three variables is path inverse consistent, even though it is not arc consistent.

Property 1 A CN is path inverse consistent iff
 it involves less than three variables, or
 it is arc consistent and for each value (i, a) in D, for any 3-clique {i, j, k},
(i, a) can be extended to a consistent instantiation of {i, j, k}.
This property allows us to see the relations between PIC, RPC and Max-RPC. If a
value (i, a) has no support on a constraint Cij , the three local consistencies delete this
arc inconsistent value (see Figure 2A). If (i, a) has only one support b in Dj , PIC, RPC,
and Max-RPC delete (i, a) because of Cij if ((i, a), (j, b)) is path inconsistent (see Figure
2B). The difference between these three local consistencies appears if (i, a) has at least two
supports on Cij . In such a case, (i, a) is restricted path consistent w.r.t. Cij but PIC can
delete it if there is a 3-clique {i, j, k} such that all the supports of (i, a) in Dj are path
inconsistent because of k (see Figure 2C). So, PIC is more pruningful than RPC. But it
212

fiDomain Filtering Consistencies

often deletes only few additional values because the supports of a value are seldom all path
inconsistent because of the same third variable. Max-RPC is far more pruningful since it
deletes (i, a) because of Cij if all its supports in Dj are path inconsistent, even if they are
not path inconsistent because of the same third variable (see Figure 2D).

5. Pruning Efficiency
5.1 Qualitative Study
To compare the pruning efficiency of the local consistencies presented above, we use the
transitive relation stronger introduced in (Debruyne & Bessiere, 1997b). A local consistency LC is stronger than another local consistency LC 0 if in any CN in which LC holds,
LC 0 holds too. Consequently, if LC is stronger than LC 0 , any algorithm achieving LC
deletes at least all the values removed by an algorithm achieving LC 0 . For instance, since
by definition of restricted path consistency RPC is stronger than AC, an RPC algorithm
removes at least all the arc inconsistent values. A local consistency LC is strictly stronger
than another local consistency LC 0 if LC is stronger than LC 0 and there is at least one CN
in which LC 0 holds and LC does not.
Theorem 1 Restricted path consistency is strictly stronger than AC.
Proof By definition of restricted path consistency, RPC is stronger than arc consistency.
Figure 3a shows that there exists a constraint network on which AC holds and RPC does
not. Therefore, RPC is strictly stronger than AC. 2
Theorem 2 If k > k 0 0, k-RPC is strictly stronger than k 0 -RPC.
Proof The proof that k-RPC is stronger than k 0 -RPC if k > k 0 0 is trivial. Figure 3g
shows that there exists a constraint network on which k 0 -RPC holds and k-RPC (k > k 0 0)
does not. Therefore, k-RPC is strictly stronger than k 0 -RPC if k > k 0 0. 2
Theorem 3 Max-RPC is strictly stronger than k-RPC, k 0.
Proof The proof that Max-RPC is stronger than k-RPC k 0 is trivial. Figure 3g shows
that for any k 0 there exists a constraint network on which k-RPC holds and Max-RPC
does not. Therefore, Max-RPC is strictly stronger than k-RPC k 0. 2
Theorem 4 If |X | 3, path inverse consistency is strictly stronger than restricted path
consistency.
Proof From property 1, PIC is stronger than AC if |X | 3. Now, consider a value (i, a)
having only one support (j, b) on Cij . If PIC holds, for any third variable k, (i, a) can be
extended to a consistent instantiation I including {i, j, k} and since b is the only support of
(i, a) in Dj , Ij = b. So ((i, a), (j, b)) is path consistent and (i, a) is restricted path consistent
w.r.t. Cij . Furthermore, Figure 3b shows that there exists a constraint network on which
213

fiDebruyne & Bessiere

RPC holds and PIC does not. Therefore, path inverse consistency is strictly stronger than
restricted path consistency if |X | 3. 2
Theorem 5 If |X | 3, max-restricted path consistency is strictly stronger than path inverse
consistency.
Proof Suppose there is a max-restricted path consistent CN P with a value (i, a) which
is not path inverse consistent. Since the CN is max-restricted path consistent, it is also
arc consistent by definition of max-restricted path consistency. Thus, because of property
1 we know there exist two variables j and k such that {i, j, k} is a clique in the constraint
graph and (i, a) cannot be extended to a consistent instantiation on {i, j, k}. As a result,
none of the supports of (i, a) on Cij is path consistent, which contradicts the assumption
that the CN P is max-restricted path consistent. Furthermore, Figure 3i shows that there
exists a constraint network on which path inverse consistency hold and max-restricted path
consistency does not. Therefore, if |X | 3, max-RPC is strictly stronger 2
Theorem 6 Singleton arc consistency is strictly stronger than Max-RPC.
Proof Suppose that there exists a CN P with a singleton arc consistent value (i, a) that
is not max-restricted path consistent. Let j  X be a variable such that (i, a) has no
path consistent support in Dj . For each support b of (i, a) in Dj , there exists a variable k
such that 6 c  Dk such that Cik (a, c)  Cjk (b, c). Therefore, all the values of Dj are arc
inconsistent w.r.t. P |Di ={a} and (i, a) is not singleton arc consistent. So, SAC is stronger
than Max-RPC. Figure 3e shows that there exists a constraint network on which Max-RPC
holds and SAC does not. Therefore, SAC is strictly stronger than Max-RPC. 2
Theorem 7 Neighborhood inverse consistency is strictly stronger than max-restricted path
consistency.
Proof Let (i, a) be any value of a neighborhood inverse consistent CN P . There exists a
consistent instantiation I including i and its neighborhood s.t. Ii = a. For any Cij  C, Ij
is a path consistent support of (i, a). Indeed, let k be any third variable. If k is linked to i,
((i, a), (j, Ij ), (k, Ik )) is a consistent instantiation since P is neighborhood inverse consistent.
Otherwise, there are two cases: First, if k is not linked to j, ((i, a), (j, Ij ), (k, c)) is consistent
c  Dk ; Second, if Cjk  C, there exists a consistent instantiation I 0 of j and its neighborhood s.t. Ij0 = Ij and ((i, a), (j, Ij0 ), (k, Ik0 )) is consistent. So, (i, a) is max-restricted path
consistent. Furthermore, Figure 3d shows that there exists a constraint network on which
Max-RPC holds and NIC does not. Therefore, NIC is strictly stronger than Max-RPC. 2

Theorem 8 Strong path consistency is strictly stronger than singleton arc consistency.
Proof Consider a problem that is strong path consistent. Any pair of values can be extended to any third variable. Furthermore, since the problem is strong path consistent, it
is also arc consistent and a sub-problem obtained by restricting a domain Di to a singleton
214

fiDomain Filtering Consistencies

{(i, a)} can be made arc consistent. The initial problem is therefore singleton arc consistent.
Figure 3c shows that there exists a constraint network on which SAC holds and strong PC
does not. Therefore, strong PC is strictly stronger than SAC. 2
Theorem 9 Singleton restricted path consistency is strictly stronger than singleton arc
consistency.
Proof Singleton restricted path consistency is stronger than singleton arc consistency since
RPC is stronger than AC. Figure 3d shows that there exists a constraint network on which
SAC holds and SRPC does not. Therefore, SRPC is strictly stronger than SAC. 2
The stronger relation does not induce a total ordering. Some local consistencies are
incomparable.
Theorem 10
1. If |X | 3, path inverse consistency and k-restricted path consistency are incomparable.
2. Neighborhood inverse consistency and singleton arc consistency are incomparable.
3. Neighborhood inverse consistency and strong path consistency are incomparable.
4. Neighborhood inverse consistency and singleton restricted path consistency are incomparable.
Proof
1. cf. Figure 3h and Figure 3j.
2. cf. Figure 3d and Figure 3e.
3. cf. Figure 3d and Figure 3e.
4. cf. Figure 3e and Figure 3f.
Figure 4 summarizes the relations between the local consistencies. There is an arrow
from LC to LC 0 iff LC is strictly stronger than LC 0 . A crossed line between two local
consistencies means that they are not comparable w.r.t. the stronger relation. When
LC is not stronger than LC 0 (LC 0 is strictly stronger than LC, or LC and LC 0 are not
comparable), a CN in which LC holds and LC 0 does not can be found in Figure 3. Obviously,
the stronger relation is transitive. In Figure 4 we omit the transitivity arcs.

215

fiDebruyne & Bessiere

(a)

(d)

AC

(b)

RPC

SAC
Strong PC
SAC

NIC Strong PC
Max-RPC
NIC
SRPC

(c)

PIC
2-RPC

RPC
RP C

SRPC
NIC

(e)

SAC

SAC NIC
SAC NIC

NIC
Max-RPC

Strong PC

Strong PC
SRPC

k+ 1

k+ 1

k+ 1

k+ 1

(f)

SRPC

NIC

(h)

2-R PC

PIC

...

in A

B

(g)

(i)

PIC
PIC

2-RP C
Max-RPC

The domain of a variable.
A forbidden pair of values.
A is not stronger than B (B deletes the value

k+1

k-RPC
k-RPC

(j)

Max-RPC
k'-RPC if k'>k>0

S RP C

Strong PC

on this A consistent network)

Figure 3: Some CNs proving the not stronger relations between some of the mentioned
local consistencies.

216

fiDomain Filtering Consistencies

SRPC
Strong PC

NIC
SAC
Max-RPC
k-RPC
(k>1)

RPC

PIC
AC

A
A

B : A is strictly stronger than B.
B : A and B are incomparable w.r.t. the stronger relation.
Figure 4: Relations between the mentioned local consistencies.

5.2 Experimental Evaluation
Figure 4 does not give any quantitative information. A local consistency LC can remove
more values than another local consistency LC 0 on most of the CNs even though it is
incomparable with LC because of some particular CNs. When they are comparable, it does
not show if a local consistency is far more pruningful than another or if it performs only
few additional value deletions. To have some quantitative information about the pruning
efficiency of these local consistencies, we performed an experimental evaluation. The aim of
this evaluation is to show how pruningful a local consistency is on random CNs, with a fixed
number of variables and values, when the number of constraints and the constraint tightness
217

fiDebruyne & Bessiere

.01
.01

.1

.2

.3

.4

.5

.6

.7

.8

.9

1

Tightness

.1
.2
.3
D
.4 e
n
.5 s
i
.6 t
y

AC

.7

Max-RPC

RPC
PIC
2-RPC

SAC
Strong PC

.8

SRPC
NIC
Variable
completability

.9

1
Figure 5: The T0 bounds for random CNs having 40 variables and 15 values in each domain.

are changing. We used the random uniform CN generator of (Frost, Bessiere, Dechter, &
Regin, 1996) which produces instances according to the Model B (Prosser, 1996). It involves
four parameters: n the number of variables, d the common size of the initial domains, p1
the proportion of constraints in the network (the density p1=1 corresponds to the complete
graph) and p2 the proportion of forbidden pairs of values in a constraint (the tightness).
The generated problems have 40 variables and 15 values in each domain. For each local
consistency and each density p1, two particular values of the tightness have been determined.
On the one hand, T0 (p1) is the tightness such that the local consistency does not delete any
value on 50% of the CNs generated with p1 for density. For values of tightness lower than
T0 (p1), the local consistency seldom deletes many values. On the other hand, Tall (p1) is the
tightness such that the local consistency finds the inconsistency of 50% of the CNs generated
with density p1. On constraint networks with tighter constraints, the local consistency
often removes all the values. For all the mentioned local consistencies, the values T0 (p1)
218

fiDomain Filtering Consistencies

.01
.01

.1

.2

.3

.4

.5

.6

.7

.8

.9

1

Tightness

.1
.2
.3
.4
.5
.6

D
e
n
s
i
t
y

AC
RPC
PIC
2-RPC
Max-RPC

.7

SAC
Strong PC

.8

SRPC
NIC
Variable
completability

.9
1

Figure 6: The Tall bounds for random CNs having 40 variables and 15 values in each domain.

and Tall (p1) for any density p1 are given in Figure 5 and Figure 6 respectively. We also
show these bounds for the variable completability filtering which removes all the globally
inconsistent values, and thus is the strongest filtering we can have when we limit filtering to
the domains. To determine the T0 and Tall bounds, 300 CNs have been generated for each
(density, tightness) pair. This explains why the generated problems are relatively small.
As already proved theoretically, PIC is stronger than RPC. Their pruning efficiencies
are closed. RPC deletes most of the path inverse inconsistent values and is halfway between
AC and Max-RPC in terms of pruning efficiency. k-RPC with k > 1 is incomparable
with PIC with regard to the stronger relation. However, Figure 5 and Figure 6 show that
2-RPC is more pruningful than PIC. SAC and strong PC have almost the same pruning
efficiency. Their T0 limits merge and their Tall limits show a slight difference. This confirms
the similitude between SAC and strong PC pointed out in Section 3. Although SRPC and
strong PC are not comparable w.r.t. the stronger relation, SRPC removes is more pruningful
than strong PC. As predicted in (van Beek, 1994), these polynomial filterings have more
219

fiDebruyne & Bessiere

.0 1

.1

.2

.3

.4

.5

.6

.7

.8

.9

1

Tightness

AC
RPC
PIC
2-RPC
Ma x - RP C
SAC
Str ong PC

SR PC
NIC
Varia ble
c ompletability

Figure 7: The T0 (black points) and Tall (white points) bounds for random CNs having 40
variables, 15 values in each domain, and density 1.

difficulties to delete inconsistent values on dense problems with loose constraints. On sparse
CNs, the polynomial local consistencies studied are close to variable completability, whereas
on very dense CNs, Figure 5 and Figure 6 show a large range of tightnesses between them and
variable completability. NIC behaves very differently since on complete constraint networks
it corresponds to variable completability. So, on dense CNs, NIC is far more pruningful
than the other local consistencies. On CNs generated with a density lower than .28 NIC
is less pruningful than SRPC, strong PC and SAC. The more important the propagation
through the network is, the closer T0 and Tall are. If a filtering (such as AC) uses a very
local property to delete inconsistent values, there is a large set of CNs on which it removes
some but not all the values. More pruningful local consistencies consider a more important
part of the network to know whether a value is consistent or not. So, they seldom delete
few values. On most of the CNs, they do not delete any value, or detect inconsistency: the
propagation of the first value deletions often leads to a domain wipe out.

6. Time Efficiency
6.1 Radio Link Frequency Assignment Problems
An experimental evaluation has been done on the radio link frequency assignment problems
described in (Cabon, de Givry, Lobjois, Schiex, & Warners, 1999), namely the instances
of the CELAR4 named Scen01 to Scen11, and the GRAPH instances generated using the
GRAPH generator at Delft University named Graph01 to Graph14. In these problems we
have to assign frequencies to a set of radio links defined between pairs of sites in order to
avoid interferences5 . These problems have from 200 to 916 variables and there are 40 values
in average in each domain. The constraints are binary and have a cost of violation specified
4. We thanks the Centre dElectronique de lArmement (France).
5. See http://www-bia.inra.fr/T/schiex/Doc/CELARE.html for a more detailed presentation of these
problems.

220

fiDomain Filtering Consistencies

Scen02
Scen03
Scen11

AC7
0.27
0.58
0.89

RPC2
0.7
1.55
2.53

PIC2
4.38
9.13
13.79

Max-RPC1
6.33
14.21
25.84

SAC1
45.5
99.49
144.3

SRPC1
434.93
946.31
1362.18

NIC1
10.45
26.58
time out

Table 2: Cpu time performances on some RLFAP instances on which all the local consistencies studied hold.

by a level from 0 to 4. The level 0 corresponds to hard constraints, and levels from 1 to 4
have a decreasing cost of violation. For each problem ScenXX (resp. GraphXX), we call
ScenXX.3, ScenXX.2, ScenXX.1 and ScenXX.0 (resp. GraphXX.3, GraphXX.2, GraphXX.1
and GraphXX.0) the problems of satisfaction obtained by considering the problem ScenXX
(resp. GraphXX) with only the constraints of level 0 to 3, 0 to 2, 0 to 1, and 0 respectively.
In this experimental evaluation, we consider both the cpu time performances and the
percentage of values deleted by the local consistencies studied. The algorithms used are AC7
(Bessiere, Freuder, & Regin, 1995), RPC2 (Debruyne & Bessiere, 1997a), PIC2 (Debruyne,
2000), Max-RPC1 (Debruyne & Bessiere, 1997a), the singleton arc consistency algorithm
of (Debruyne & Bessiere, 1997b) (SAC1) based on AC6, a SRPC algorithm based on RPC2
(SRPC1), and the NIC algorithm proposed in (Freuder & Elfe, 1996) (NIC1) using FCCBJ (Prosser, 1993) (as in Freuder & Elfe, 1996) with dom+deg dynamic variable ordering
heuristic (minimal domain first, in which ties are broken by choosing the variable with the
highest degree in the constraint graph Frost & Dechter, 1995; Bessiere & Regin, 1996). All
these algorithms have been modified to stop as soon as a domain wipe out occurs. We do
not show results on strong PC in this section because on these large problems it requires
often more than our 2 hours time out limit. These algorithms have been tested on each
ScenXX, Scen XX.X, GraphXX, and GraphXX.X problem using a Sun UltraSparc IIi 440
Mhz. For sake of clarity, we only show the results on some representative problems.
6.1.1 Results on problems on which all the studied local consistencies hold
(cf. Table 2)
If all the local consistencies studied hold on a constraint network, all the corresponding
filtering algorithms are useless. They waste time to check whether the local consistencies
hold without deleting any inconsistent value. On these problems, the stronger the local
consistency is, the more important is the time wasted.
We can see the consequence of the exponential worst case time complexity of NIC1. On
most of these problems, NIC1 requires a reasonable cpu time. But as we can see on the
problem Scen11, a combinatorial explosion can lead to really prohibitive cpu time for NIC1.
6.1.2 Results on arc inconsistent problems (cf. Table 3)
When arc consistency is sufficient to detect the inconsistency of the problem, stronger local
consistencies are not always more costly. On Figure 3 we can see that Max-RPC1 has
often the best cpu time performances and on Graph06 for example, AC7 is one of the
221

fiDebruyne & Bessiere

Scen07
Graph07
Scen08
Graph06

AC7
0.42
0.11
0.75
0.48

RPC2
0.43
0.14
0.48
0.27

PIC2
0.44
0.12
0.73
0.44

Max-RPC1
0.09
0.16
0.4
0.26

SAC1
0.59
0.24
0.52
0.27

SRPC1
0.47
0.14
0.47
0.27

NIC1
1.89
1.08
time out
10.13

Table 3: Cpu time performances on some arc inconsistent RLFAP instances.

Scen06.1
Scen09.1
Graph04
Graph10
Graph06.1
Graph12.1

cpu time
% of DV
cpu time
% of DV
cpu time
% of DV
cpu time
% of DV
cpu time
% of DV
cpu time
% of DV

AC7
0.27
7.88
0.8
22.48
0.81
4.97
1.43
1.43
0.39
14.96
0.73
10.42

RPC2
0.48
8.33
1.52
25.79
2.07
6.67
3.32
1.62
0.81
17.69
1.35
12.23

PIC2
0.96
17.85
1.87
29.79
18.65
6.95
37.7
1.68
0.9
100
2.83
15.28

MaxRPC1
2.04
19.7
5.88
31.03
25.39
10.35
51.42
5.42
0.8
100
5.41
100

SAC1
66.32
42.47
167.85
35.86
2238.13
18.44
3984.13
9.53
6.69
100
9.47
100

SRPC1
227.13
42.57
568.08
35.86
time out
?
time out
?
3.21
100
32.12
100

NIC1
time out
?
318.38
31.57
101.77
13.14
2033.39
7.35
8.54
100
3.97
100

Table 4: Cpu time performances and percentages of values deleted by the local consistencies
studied (% of DV) on some RLFAP instances.

most expensive local consistencies. When enforcing AC requires propagation to find the
arc inconsistency of the problem, a stronger local consistency can wipe out a domain more
quickly than AC7.
On these constraint networks, all the algorithms used have very low cpu time requirements, except NIC1, which can be very expensive on some instances, such as Scen08.
6.1.3 Results on the other problems (cf. Table 4)
On many of the RLFAP problems the local consistencies do not delete the same sets of
inconsistent values. We can see an important difference between the pruning efficiencies
especially on the problems ScenXX.1 and GraphXX.1.
Obviously, on most of these problems, the more pruningful the local consistency is, the
more important is the time required. We can see this on the problems Scen06.1 and Scen09.1
for example. However, AC7, RPC2, PIC2, and Max-RPC1 have cpu time performances in
the same order of magnitude while SAC1, SRPC1, and NIC1 are often far more expensive.
222

fiDomain Filtering Consistencies

This is especially obvious on Graph04 and Graph10. However, it is difficult to say which
is the most interesting local consistency on these problems since even if SAC1, and SRPC1
are costly, we can see on Scen06.1 and Graph04 that they can be far more pruningful.
These problems highlight that NIC1 is not very stable. It sometimes shows good performances, but an exponential explosion can lead to a prohibitive cost on some instances.
When NIC1 requires a reasonable time, its pruning efficiency is closer to the one of MaxRPC1 than to the one of SAC1. These results confirm that if the neighborhoods of the
variables are not small, NIC1 can be really prohibitive.
On Graph06.1, PIC2 (and obviously the algorithms enforcing a stronger local consistency) finds the inconsistency of the problem whereas AC7, and RPC2 remove only a part
of the inconsistent values. We can see a similar behavior on Graph12.1 where Max-RPC1
wipes out a domain whereas AC7, RPC2 and PIC2 do not find the inconsistency of the
problem. On these instances, Max-RPC1 is the best choice.
6.2 Randomly Generated Problems
The random uniform CN generator of section 5.2 is used to compare the cpu time required
to enforce the local consistencies. We have to point out that NIC has not been designed
to be used on uniform CNs but to adapt filtering effort to the degree of the variables in
the constraint graph. So, NIC would have better performances on non-uniform CNs than
those presented in this section. The generated problems have 200 variables and 30 values
in each initial domain. Figure 8 shows the results on CNs with density of .02. These CNs
are relatively sparse since the variables have four neighbors on average. Figure 9 presents
performances at density .15 (the variables have 30 neighbors on average). Because of the
set of parameters, there are no flawed variables (MacIntyre, Prosser, Smith, & Walsh,
1998) in the generated problems.6 In addition to the algorithms of the previous section, we
use a strong path consistency algorithm based on PC8 (Chmeiss & Jegou, 1996) and AC6.
This algorithm stops as soon as a domain wipe out occurs or as soon as a constraint no
longer allows any pair of values. In addition to the percentage of deleted values and cpu
time performances, Figure 8 and Figure 9 show the cpu time to number of deleted values
ratio for each tightness where the local consistency removes at least one value on average.
For each tightness, 50 instances were generated. Figure 8 and Figure 9 show mean values
obtained on a Pentium II-266 Mhz with 32 Mb of memory under Linux.
As observed in (Gent, MacIntyre, Prosser, Shaw, & Walsh, 1997) for arc consistency,
the filtering algorithms tested have a complexity peak. For low values of the tightness, they
easily prove that the values are locally consistent, and when constraints are very tight, they
quickly wipe out a domain. Each local consistency has a phase transition where most of
the hardest problems for an algorithm achieving this local consistency tend to occur.
6.3 Experiments on Sparse CNs
Even on sparse CNs (see Figure 8), the cpu time results are so different between the algorithms (7h 48min for strong PC at its peak when AC7 requires at most .22 seconds on
average) that a logarithmic scale has to be used. Strong PC is really prohibitive, even for
6. In Section 5.2,the tightness reaching 1, there was obviously flawed variables for some sets of parameters.

223

fiDebruyne & Bessiere

low values of tightness. SRPC and SAC have bad cpu time to number of deleted values
ratios, except SAC on CNs having very tight constraints because the SAC algorithm used
is based on AC6 which can be more efficient than AC7 on such problems. On these sparse
CNs, NIC has often better cpu time performances than SAC but it does not remove more
values than Max-RPC. Consequently, NIC has a bad cpu time to number of deleted values
ratio. Unlike strong PC, SRPC, SAC, and NIC, the cpu time requirements of AC7, PIC2,
RPC2 and Max-RPC are of the same order of magnitude. The cpu time to number of
deleted values ratios of these four last filterings are also very close, with a little advantage
for PIC2. Although PIC is stronger than RPC, PIC2 can be less expensive than RPC2 on
sparse CNs. If there are few 3-cliques in the constraint graph, PIC2 does not require far
more cpu time than AC7 whereas RPC2 is about two times as expensive as AC7 since it
looks for two supports for each value on each constraint.
6.4 Experiments on more Dense CNs
On more dense CNs (see Figure 9), the complexity peaks of AC7, RPC2, PIC2, and MaxRPC stay close to each other. PIC2 is less worthwhile since it deletes few additional values
compared to RPC2 while its cpu time requirements are close to those of Max-RPC. MaxRPC has one of the best cpu time to number of deleted values ratios. As soon as RPC
leads to a domain wipe out, the cpu time performances of SRPC and RPC2 merge. Indeed,
the SRPC algorithm used enforces RPC2 before checking the restricted path consistency of
the sub-problems P |Di ={a} for each (i, a)  D. If all the values of a domain are restricted
path inconsistent, the RPC preprocessing finds the global inconsistency of the problem and
the SRPC algorithm stops. SRPC is less expensive than strong PC although it is more
pruningful. These two filterings remain the most expensive. NIC is the most pruningful
local consistency on these CNs. Hence, on a large range of tightnesses, NIC has the best
cpu time to number of deleted values ratio. However, on some instances, NIC cannot avoid
the combinatorial explosion. Although NIC requires only fifteen minutes on average
at tightness .52, more than two hours are required on some instances. It is conceivable
that instances on which NIC requires far more cpu time exist for this set of parameters.
Obviously, the set of CNs on which NIC is prohibitive grows when the density increases.
The results on SAC have a lower standard deviation. SAC never requires more than fifty
two minutes on the problems generated for these experiments.
6.5 Discussion
What can we conclude from these results? Strong PC is by far the least interesting filtering
technique. Compared to SAC, which removes most of the strong path inconsistent values,
strong PC is really prohibitive.7 Achieving SAC or SRPC is costly as long as these two
local consistencies do not delete any value. Obviously, although SAC and SRPC are more
expensive than Max-RPC on almost all the generated problems, we cannot say that it is
better to use Max-RPC. Indeed, at density .15 for example, Max-RPC is useless for

7. We can point out that when the path consistency of a constraint can be expressed without explicitly
storing the set of forbidden tuples, path consistency can be used (e.g., temporal networks Allen, 1983,
constraint networks Smith, 1992).

224

fiDomain Filtering Consistencies

1E+5

n=200, d=30, and p1=.02

cpu time (in sec.)

7h48m in

1E+4

16m in15sec
Stron g PC

1E+3

2min3 6sec

1E+2
SR PC
9.33s ec
1E+1
SA C

NI C

1E+0

Ma x-R PC

PI C2

1 E -1

R PC 2
0.36s ec
0. 37s ec

AC 7

0.22s ec
1 E- 2
0.24s ec

Tig htne ss

1 E -3

1
10 0

5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

40

45

50

55

60

65

70

75

80

85

50

55

60

65

70

75

80

85

90

95

99

Percentage of values deleted

Tightness
0
1
1E+2

5

10

15

20

25

30

35

90

95

99

cpu time to number of deleted values ratio
1 E -1

1E+1

SRPC

1E+0

Strong PC

1E -2

AC 7
SAC
P IC 2

1E -1
Ma x-RP C
RP C2

NIC

1E -3

1 E -2

1 E -3

1E -4
1 E -4

1 E -5

1E -5

1 E -6

75

80

85

90

95

Tightness

1E -7
1

5

10
AC 7

15

20
25
R PC 2

30
35
PIC 2

40

45

Max-R PC

SA C

SRPC

Strong PC

90

95

99

N IC

Figure 8: Experimental evaluation on random CNs with n=200, d=30, and p1=.02.
225

fiDebruyne & Bessiere

1E+5

cpu time (in sec.)

n=200, d=30, and p1=.15
12h40m in
3h53 m in

1E+4

39m in43 sec

Strong PC
1E+3

15min 21sec

SRPC
1E+2
SA C
NIC
8.63 sec
1E+1
Ma x -RP C
2. 44s ec

6.25s ec
R PC 2

1E+0

P IC 2

1. 11s ec

AC 7
1 E -1

1 E -2

Tightness

1E -3
1
100

10

5

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

90

95

99

Percentage of values deleted
Tightness

0
1
1E+2

5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

60

65

70

75

80

85

90

95

99

95

99

cpu time to number of deleted values ratio

R PC 2

1E+1

1 E -2

SRPC
AC7

SA C
1E+0

1 E -3

1 E- 1

P IC 2

1 E -2

1E -4
Strong PC
N IC

1 E -3

1 E -5

1 E -4

1 E -5

Max-R PC

1E -6

1 E- 6

65

70

75

80

85

Tightness

90

1 E -7

1

5

10
AC7

15

20
R PC 2

25

30

35

PIC 2

40

45

50

Max-R PC

55

SA C

SRPC

85

Strong PC

90

N IC

Figure 9: Experimental evaluation on random CNs with n=200, d=30, and p1=.15.
226

fiDomain Filtering Consistencies

tightnesses lower than .63 since it does not delete any value, while for SRPC the limit is .57
of tightness. Furthermore, for singleton consistencies we can argue that the algorithm used
to achieve them is not optimal. An algorithm reusing part of the filtering performed on
P |Di ={a} to process other sub-problems P |Dj ={b} , ((i, a) and (j, b) belonging to D) would
improve cpu time performances. However, the cpu time to number of deleted values ratios of
SAC and SRPC algorithms are often among the worst ones, especially on sparse CNs. SAC
and SRPC are so expensive that it is hardly likely that enhancements of these algorithms
could lead them to be the most worthwhile filterings. On sparse uniform CNs, NIC is not
the best choice. Compared to Max-RPC, it does not delete enough values to offset the
additional cpu time cost. Furthermore, NIC cannot be used on dense CNs since its cpu
time requirements become greater than those of a search algorithm. So, NIC has to be used
only on relatively dense CNs, as those of Figure 9 on which NIC is worthwhile on average
(although on some instances a combinatorial explosion cannot be avoided). On very dense
CNs, the worst case time complexity of Max-RPC and PIC2 is close to the one of the best
path consistency algorithm (O(en + ed2 + cd3 ) against O(n3 d3 )). However, the experiments
underline that achieving Max-RPC and PIC2 is far less expensive in practice. Compared
to RPC2 and Max-RPC, PIC2 is not a good solution in-between. The cpu time to number
of deleted values ratios of RPC2 and Max-RPC are better than the one of PIC2 (except on
very sparse CNs on which PIC2 can be less expensive than RPC2). Indeed, PIC2 deletes
only few additional values compared to RPC2, while its cpu time performances are close to
those of Max-RPC.
Cpu time performances are even more essential when the aim is to maintain a local consistency during search. Maintaining a local consistency during search requires to repeatedly
propagate the choice of a value for a variable (namely the restriction of a domain to a
singleton) or the refutation of a value. To be worthwhile, a local consistency has to require
less time to detect that a branch of the search tree does not lead to a solution than a search
algorithm to explore this branch. So, maintaining a local consistency during search can
outperform MAC on hard problems only if this local consistency is more pruningful than
AC while requiring only a little additional cpu time. With regard to this criterion, we can
discard strong PC, SAC, SRPC, and NIC on dense CNs because they are too expensive. It
is conceivable that we can find instances on which maintaining these consistencies during
search outperforms MAC, but the more expensive the maintained local consistency is, the
more seldom the problems on which MAC is outperformed will be. On sparse CNs, NIC is
not prohibitive, but it deletes only few additional values compared to Max-RPC and it has
therefore a bad cpu time to number of deleted values ratio. Finally, The most promising
local consistencies are RPC and Max-RPC. If we exclude arc consistency, RPC is the least
expensive local consistency we studied. Furthermore, the RPC algorithms delete most of
the path inverse inconsistent values. Although Max-RPC is far more pruningful than arc
consistency, experiments show that in practice, Max-RPC has very good cpu time results.
Therefore, it seems very likely that maintaining RPC or Max-RPC during search could
outperform MAC on very hard problems.
To confirm these results, an algorithm called Quick that maintains an adaptation of
Max-RPC has been compared to MAC. The results of these experiments (Debruyne, 1999)
show that Quick has better cpu time performances than MAC on large and hard randomly
generated CNs that are relatively sparse. More interestingly, Quick has a more impor227

fiDebruyne & Bessiere

tant stability than MAC (the cpu time performances of Quick have a very low standard
deviation). It would be very interesting to propose efficient algorithms that maintain the
local consistencies studied in this paper and to compare these algorithms. Such a study
would allow us to know whether during search, the more advantageous local consistencies
remain RPC and Max-RPC as during a preprocessing step. First results on the effect of
maintaining SAC during search are given in (Prosser, Stergiou, & Walsh, 2000).

7. Conclusion
In this paper we extended the idea of restricted path consistency to k-RPC and MaxRPC, which are more pruningful local consistencies. We proposed a new class of local
consistencies called singleton consistencies. We studied these new local consistencies and
the other local consistencies that alike can be used on large CNs while removing more values
than arc consistency. We showed some relations between them and we compared both
theoretically and experimentally their pruning and time efficiencies. The most pruningful
are neighborhood inverse consistency and singleton restricted path consistency. However,
SRPC is expensive in time and the exponential worst case time complexity of NIC makes it
unusable on dense CNs. If we are looking for a local consistency that would advantageously
be maintained during search, RPC and Max RPC seem to be the most promising local
consistencies. Indeed, they are relatively inexpensive and far more pruningful than arc
consistency.

8. Acknowledgements
We would like to thank Toby Walsh for his suggestions for improving the presentation of
the figures in Section 5.

References
Allen, J. (1983). Maintaining Knowledge about Temporal Intervals. Communications of
the ACM, 26(11), 832843.
Bacchus, F., & van Run, P. (1995). Dynamic variable ordering in csps. In Proceedings of
CP-95, Cassis, France, pp. 258275.
Berlandier, P. (1995). Improving domain filtering using restricted path consistency. In
Proceedings of IEEE CAIA-95.
Bessiere, C., Freuder, E., & Regin, J. (1995). Using inference to reduce arc-consistency
computation. In Proceedings of IJCAI-95, Montreal, Canada, pp. 592598.
Bessiere, C., Freuder, E., & Regin, J. (1999). Using constraint metaknowledge to reduce
arc consistency computation. Artificial Intelligence, 107(1), 125148.
Bessiere, C., & Regin, J. (1996). MAC and combined heuristics: Two reasons to forsake FC
(and CBJ?) on hard problems. In Proceedings of CP-96, Cambridge MA, pp. 6175.
228

fiDomain Filtering Consistencies

Cabon, C., de Givry, S., Lobjois, L., Schiex, T., & Warners, J. (1999). Radio link frequency
assignment benchmarks. CONSTRAINTS, 4(1), 7989.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are. In
Proceedings of IJCAI-91, Sydney, Australia, pp. 294299.
Chmeiss, A., & Jegou, P. (1996). Two new constraint propagation algorithms requiring
small space complexity. In Proceedings of IEEE ICTAI-96, Toulouse, France, pp.
286289.
Debruyne, R. (1999). A strong local consistency for constraint satisfaction. In Proceedings
of IEEE ICTAI-99, Chicago IL, pp. 202209.
Debruyne, R. (2000). A property of path inverse consistency leading to an optimal algorithm. In Proceedings of ECAI-00, Berlin, Germany, pp. 8892.
Debruyne, R., & Bessiere, C. (1997a). From restricted path consistency to max-restricted
path consistency. In Proceedings of CP-97, Linz, Austria, pp. 312326.
Debruyne, R., & Bessiere, C. (1997b). Some practicable filtering techniques for the constraint satisfaction problem. In Proceedings of IJCAI-97, Nagoya, Japan, pp. 412417.
Dechter, R., & Meiri, I. (1994). Experimental evaluation of preprocessing algorithms for
constraint satisfaction problems. Artificial Intelligence, 68, 211241.
Dechter, R., & Pearl, J. (1988). Network-based heuristics for constraint-satisfaction problems. Artificial Intelligence, 34, 138.
Freuder, E. (1982). A sufficient condition for backtrack-free search. Journal of the ACM,
29(1), 2432.
Freuder, E. (1985). A sufficient condition for backtrack-bounded search. Journal of the
ACM, 32(4), 755761.
Freuder, E. (1991). Completable representations of constraint satisfaction problems. In
Proceedings of KR-91, Cambridge MA, pp. 186195.
Freuder, E., & Elfe, C. (1996). Neighborhood inverse consistency preprocessing. In Proceedings of AAAI-96, Portland OR, pp. 202208.
Frost, D., Bessiere, C., Dechter, R., & Regin, J. (1996). Random uniform csp generators.
In http://www.ics.uci.edu/ frost/csp/generatotr.html.
Frost, D., & Dechter, R. (1995). Look-ahead value ordering for constraint satisfaction
problems. In Proceedings of IJCAI-95, Montreal, Canada, pp. 572578.
Gaschnig, J. (1974). A constraint satisfaction method for inference making. In Proceedings
of the 12th Annual Allerton Conf. Circuit System Theory, U.I.L., Urbana-Champaign
IL, pp. 866874.
229

fiDebruyne & Bessiere

Gent, I., MacIntyre, E., Prosser, P., Shaw, P., & Walsh, T. (1997). The constrainedness of
arc consistency. In Proceedings of CP-97, Linz, Austria, pp. 327340.
Golomb, S., & Baumert, I. (1965). Backtrack programming. Journal of the ACM, 12(4),
516524.
Grant, S., & Smith, B. (1996). The phase transition behaviour of maintaining arc consistency. In Proceedings of ECAI-96, Budapest, Hungary, pp. 175179.
Haralick, R., & Elliott, G. (1980). Increasing tree search efficiency for constraint satisfaction
problems. Artificial Intelligence, 14, 263313.
Kumar, V. (1992). Algorithms for constraint satisfaction problems: A survey. AI Magazine,
13(1), 3244.
MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (1998). Random constraint satisfaction:
theory meets practice. In Proceedings of CP-98, Pisa, Italy, Vol. 19, pp. 325339.
McGregor, J. (1979). Relational consistency algorithms and their application in finding
subgraph and graph isomorphisms. Information Sciences, 19, 229250.
Meseguer, P. (1989). Constraint satisfaction problems: An overview. AICOM, 2, 317.
Nadel, B. (1988). Tree search and arc consistency in constraint satisfaction algorithms. in
L. Kanal and V. Kumar, editors, Search in Artificial Intelligence, Springer-Verlag,
287342.
Prosser, P. (1993). Hybrid algorithms for the constraint satisfaction problem. Computational
Intelligence, 9(3), 268299.
Prosser, P. (1996). An empirical study of phase transition in binary constraint satisfaction
problems. Artificial Intelligence, 81, 81109.
Prosser, P., Stergiou, K., & Walsh, T. (2000). Singleton consistencies. In Proceedings of
CP-00, Singapore, pp. 353368.
Sabin, D., & Freuder, E. (1994). Contradicting conventional wisdom in constraint satisfaction. In Proceedings of ECAI-94, Amsterdam, Netherlands.
Schiex, T., Regin, J., Gaspin, C., & Verfaillie, G. (1996). Lazy arc consistency. In Proceedings of AAAI-96, Portland OR, pp. 216221.
Singh, M. (1995). Path consistency revisited. In Proceedings of IEEE ICTAI-95, Washington
D.C.
Smith, B. (1992). How to Solve the Zebra Problem, or Path Consistency the Easy Way. In
Proceedings of ECAI-92, pp. 3637.
Tsang, E. (1993). Foundations of Constraint Satisfaction. London, Academic Press.
van Beek, P. (1994). On the inherent level of local consistency in constraint networks. In
Proceedings of AAAI-94, Seattle WA, pp. 368373.

230

fi
	ff
fi 
			 ! #"$ % 
'&)(*,+.-//!(1032465'( /4

789:;  <)( ->=
??!@BA9	%&<C4>=
/!(

DFEHGJILKMKJNOEQPSR8TUILVMR8WR
XZY\[^]Q_a`MW
X_cbBEdN
efIgNXaR>ILW8WYihjVJ_]QNakgILVMW>]ml

IgNOn)Eokqpr]QsQR>_aR>EHG\etNOEus;]Q_v_]Q_

wSx#y{z|}~3xyC#|B#

v$aHZ3vO$3#3

 
u >
B
v
H'Z'O >C>
>36{'BS{ 
 #~#|fz}x
 
v
u >
B
H'Z'O >C>
>36{'BS{ 
$y}z'




xZ#

3{v8.vHZ3vO$3#3

$33$3O)3$ Ov

v
  g
a8>B  ) >{>
Q  

,O
)11BQ1
BB6.1;BO 6 81BL 
 1J686 !
ff
	.Bfiff6fi	fi66	a{16>8#B6fi)6L 	 
B
B{188B
 
 
  >8;6
  6{
 6Q
 >
B{Q 		JBa6fi	!
"6$
 #fi 
 %	8
 #86 
 	&
 

 	(
 '
 118)
  
  %);6o1*
 >.B !+
 ,
 ff
1 6
 u1&
 -/.102-,-3.4-+5,-7698+:;ff;
 -/.1:<2-;>=6?
 !1#681&
 18B6o1;
6g 7ffBff@
 >{16fi ff @
 !  8>
 	@ ?
 
ff{ > 
8  	
 >;
 B
B{188aB
 
 A8{9

 >

 	
 66

B>CEDGF H;I2JKLH F
MNPO!QRNPSUTVWXQZY([(\,NPV^]?Y(Q_\,`baLc2d,Y`fe1\ggh`PNfWZ\[(NbdVSd,O*\Y([(NijWZNf\,`NPV[kQ_`P`PNbl,Q_VWXQEWXdV[(\,NPVnmR\,Vo4TVhWXQZY([(\,NPVpa
[(NbQ_SZqsrtVu[(vNPS@c2d,Ywjxjc2QyS(v7d
cz[(v\[ThVWXQZY([(\,NPV[ou{YQZQ_eS|TVWXQZY[(\,NPV[oj}srtV/\~WXdV[kY(d`P`PQ_e3Sk[kdpWv\,S([(NPW
SkopSk[kQ_mc@Nb[(v~ThVWXQZY([(\,NPV[o\,SsmEdpe7Q_`bQ_e~{oR\$gh\Y([(Nf\,`P`boyd,{hSkQZY(\{h`bQ\Y(w,de7Q_WZNfS(NbdVEgYdWXQ_S(S_xOd,Y
QX7\,mEgh`bQxgj`P\,VS*WZ\,Vu{Qd,{[(\,NPVQ_euQXWZNbQ_V[(`bod,Y|c@Nb[(vuT\,`PNb[olTh\Y\,V[kQZQ_S_xh{hT7[sY\YQ_`bo{d,[(v>q
2`P\,VVNPVld,QZYuSk[kdWv\,Sk[(NPWe7dmR\,NPVhSc@Nb[(vTVhWXQZY([(\,NPV[oNPSRvh\YeNPVS(dmEQWZ\,SkQ_S&7@*;a
v\YeRd,Y9QZ,Q_V~ThVe7Q_WZNPe\{j`bQ,xSkQZQ)\gh\,ehNPmRNb[kYNPdT|S(Nb[(SNbw`fNPSZxp_,]\,e\,VhNx\,V7wS_xzsdVe7dV>x
_,,qNb,Q_V1[(v\[Nb[NPSv\Ye[kdijVhe\,Vd,gh[(NPmR\,`)gh`P\,Vd,Ygd`PNPWXo,x^NP[NfSVh\[(T7Y\,`)[kdu[kY(o[kdijVe
dV7Q[(v\[NPStl,ddeQ_V7dTlvpqrtV[(vQy{Q_Sk[d,O2\,`f`?gdS(SNb{h`bQc&d,Y`PehSZx[(vNPSc2dT`femRQ_\,V4v\_pNPV7l\,V
\,`bl,d,YNP[(vm[(vh\[NfSUlT\Y\,V[kQZQ_e[kd{QO\,Sk[\,Ve[kdgYdeThWXQ\gd`fNPWXo4[(v\[NPSUYQ_\,SkdV\{h`boWZ`bdSkQ
[kdu[(v7Qd,g[(NPm~\,`?gd`PNfWXo,qV7Od,Y([(TVh\[kQ_`bo,x>c2QRSv7d
cv7QZYQy[(vh\[STWv4\,V4\,`bl,d,YNb[(vmNPSTV`PNPw,Q_`boud,Y_x
NPVSkdmEQWZ\,SkQ_SZxNPmEgdS(SNb{h`bQ,q|v7QNfmEgh`PNPWZ\[(NPdV4O!d,Y\,`bl,d,YNP[(vme7QZ,Q_`Pd,ghmEQ_V[yNPSU[(v\[e7QZ,Q_`bd,gQZYS
S(v7dTh`PeV7d,[|cs\,Sk[kQU[(NPmRQc2d,Y(wpNPV7l[kdc&\Ye{d,[(vlT\Y\,V[kQZQ_S\[@dVWXQ,q
|vQ~gh\Y([(NfWZT`P\Y$m~\[(v7Q_mR\[(NPWZ\,`smEdpe7Q_`PS$c2QWXdVWXQ_V[kY\[kQudVNPV1[(vhNPSUgj\gQZYy\Y(Q/1X
Z! 1jt        \,Vep,  fi  XGZb1,,1 _f jk      93  q
4QWXdVhS(NPe7QZY{d,[(vE[(vQ@Sk[kY\,Nblv[kOd,Y(c&\YeRY(QZgYQ_SkQ_V[(\[(NPdVSd,O>2S2\,VheR&9SZx\,VeRS(ThWZWZNPVWX[
Y(QZgYQ_SkQ_V[(\[(NPdVSZxpS(NPVWXQ[(v7QWXdmEgj`bQXpNP[oEd,OijVehNPV7lgd`PNPWZNbQ_S9NPS9mEQ_\,ST7Y(Q_eV7d,[2NPV~[kQZYmRS2d,O^[(v7QS(NbZQ
d,O)[(v7QSkopSk[kQ_mux{jT7[|NPV[kQZYmRS|d,O?[(v7Q$S(NbZQd,O)[(v7QE(j(  XhL,  1d,O)[(v7Q$SkoS([kQ_muq
|vQZY(Q|v\,S){QZQ_VE\S(NblVhNijWZ\,V[?{dpe7oUd,Oc2d,Yw$dVEv7Q_TYNPSk[(NPWZS+Od,Y9S(TWZWZNfVWX[(`boUYQZgY(Q_SkQ_V[kQ_eE2S
SkQZQ~&dT7[(Nf`PNbQZY_x>Q_\,V>x+\,V7wpSZx9_,,];s`bo[(v7Q,x9_,,uOd,YS(T7Y,QZoSq~dmRQEd,O2[(vNfSc2d,Y(wl,Y(d
cS

  -//!(O %%j!<6L
<sC>
&O
!fi:Q
99!	%&%		&>%3%f> <

fi 38?>v{v8.^j

$33$3

dT7[d,O[(v7Q|Q_VlNPV7QZQZYNPVl[kY\,eNb[(NbdV/SkQZQ,xOd,YQXp\,mRgh`bQ,x@S(Nb[(S(Nbwp`PNPS+\,VeE\,V|dojSt_,,\Y([(NfWZ`bQ*dV
OQ_\[(T7Y(QXaL{h\,SkQ_emEQZ[(v7dpeS?c@vhNPWvEe7QZgQ_VehS?dV~Q_mEghNbYNPWZ\,`QZNPeQ_VWXQ*[kdQZ\,`PT\[kQ@\,`bl,d,YNb[(vmRSZq+vNP`bQ
[(v7QZY(Qy\Y(Qyd,{pNbdTSe7Y\_c@{h\,W(wpS[kd[(vNfS@\ggYd\,Wv>xdT7Yc2d,Y(w/\YlT7Q_S@[(vh\[[(vNfS@mR\
o/{Q[(v7QmEdSk[
\ggYd,gYNP\[kQcs\_o3[kd~,QZYNbOou[(v7QyT\,`fNb[od,O\,V\ggY(d
pNPm~\[(NbdV3\,`bl,d,YNb[(vmuxh\[`bQ_\,Sk[NbO)dV7QUcs\,V[(S
[kd~e7dRS(d~NPVY(Q_\,SkdVh\{h`bQ[(NPmEQ,q
|vQ|S(\,mEQ|gYd,{h`bQ_mRS+[(v\[gj`P\lT7Q|\ggYd_7NPmR\[(NbdVE\,`bl,d,YNb[(vmRS)Od,Y9TVWXdmRgY(Q_S(SkQ_eYQZgY(Q_SkQ_V[(\Ga
[(NbdVS*WZ\YY(o~d,QZY@[kdR[(v7QS(ThWZWZNPVWX[*Y(QZgYQ_SkQ_V[(\[(NPdVSZx7\,Ve[(v7QWXdmEghY(Q_S(S(NbdVuNfV[kY(dpeTWXQ_Ss\,eehNb[(NbdV\,`
WXdmEgh`PQXpNb[o,q97d,YQXp\,mEgj`bQ,xNbO[(vQZY(QUNPS*V7dRWXdmRghT7[(\{h`bQ\ghgY(d_7NPmR\[(NPdVd,O+[(vQd,g[(NPmR\,`>gd`PNfWXo~NPV
[(v7QTVWXdmEghY(Q_S(SkQ_eWZ\,SkQ,x>[(v7Q_VWXdmEgY(Q_SS(NbdVc@NP`P`^Vd,[Wv\,V7l,Q[(vNPS_qd
c&QZ,QZY_xNb[NPS@Q_\,S(o3[kdijVe
[(v7Qd,g[(NPm~\,`gd`fNPWXoEO!d,Y|\,VuNfVpijVNb[kQXavd,YNbZdVROT`P`PoEd,{hSkQZY(\{h`bQ&Q_`P`PmR\,V>x>_,,xo,QZ[;a
v\YeghY(d
\{h`bov\Ye7QZY~[(v\,Vgd`bopV7dmRNP\,`&[(NPmEQ$[kd1ijVe\ggYd_7NPmR\[kQ_`bond,g[(NPmR\,`|gd`fNPWZNbQ_SuNPV
[(NPmEQmEQ_\,S(T7Y(Q_e~NPVE[(v7Q@SNbZQ|d,O[(v7Q@NPVghT7[?NbO[(v7Q@NPV7gjT7[;NPS;Y(QZgYQ_SkQ_V[kQ_eS(TWZWZNPVhWX[(`boS(QZQQ_WX[(NbdVq
d,[kQ~[(v\[$[(v7QZY(Q~\YQ~[c&dNfV[kQZY(ghY(QZ[(\[(NbdVS[kd3ijVeNfV7lu\,V\ggYd_7NPmR\[(NbdV>}ijVeNPVl\3gd`fNPWXo
c@Nb[(v\,`PT7QWZ`bdS(Q[kdE[(v\[|d,O+[(v7Qd,g[(NPmR\,`gd`PNPWXo,xpd,Y@SNPmEgh`boRWZ\,`fWZT`P\[(NPV7lE\fihU[(v\[|NPS*WZ`PdSkQ[kd
[(v7Qd,g[(NPm~\,`j\,`PT7Q,q)rOc&QWZ\,Vue7d$[(vQOd,YmEQZY*\,VeuWZ\,VQZ\,`PT\[kQgd`PNPWZNPQ_SZx[(v7Q_Vc&QWZ\,VuWXQZY([(\,NPV`Po
e7d[(v7Q$`P\[k[kQZY
q@|v7QZY(QZOd,Y(Q,xc2QySkdmEQZ[(NPmRQ_SS(vd
c[(v\[[(vQ$`P\[k[kQZYWZ\,VV7d,[{QUe7dV7Q,xd,YWZ\,VhV7d,[{Q
e7dV7QNfV[(NPmEQgd`bopV7dmRNP\,`NPV[(v7Q$S(NbZQd,O+[(v7QUNPV7gjT7[ThV`bQ_S(S*SkdmRQZ[(vNPV7lRTVh`PNbw,Q_`bo~NfS*[kYT7Qq
|vQWXdmEgh`PQXpNb[oWZ`P\,S(S&p@sWXdVhS(NPSk[(Sd,O[(v7dSkQ`P\,V7lT\l,Q_SY(Q_WXd,lVhNb_\{h`bQ{o\n+T7YNPV7l
mR\,WvNPV7Qu[(v\[TSkQ_SRdVh`bo/+?EmEQ_mEd,Y(onO!d,YSkdmEQ/gd`bopV7dmRNf\,`?x2c@vQZY(QuNPSE[(v7QS(NbZQud,O[(v7Q
NPV7gjT7[Zq/&Q_WZ\,TSkQQ_\,Wv[(NPmRQSk[kQZgTSkQ_Sy\[EmEdSk[ydV7QTVNb[d,OmEQ_mEd,Y(o,x&p@s@x+[(vdT7lv
c&QRe7duV7d,[wVd
cc@v7QZ[(vQZY[(v\[NPS\gY(d,gQZYNPVWZ`PThS(NbdV/d,Y$\,VQ_T\,`fNb[o,q&Q_WZ\,TSkQ,xlNP,Q_V4\u`PNPmRNb[
dV[(v7QE\,mEdTV[d,O2mRQ_mEd,Y(oThSkQ_e>x>[(v7QZY(QE\Y(QEdVh`bo/QXpgdVQ_V[(NP\,`f`bo/mR\,VoWXdVpijlT7Y\[(NbdVSd,O2[(v\[
mEQ_mEd,Yo3gdS(S(Nb{j`bQ$cNb[(v\iQ_eiVNb[kQ\,`bghv\{QZ[Zx^&7@*z|qyr[NPSV7d,[wVd
c@V4cv7QZ[(v7QZY
[(vNPS@NPS*\EgY(d,gQZY@NfVWZ`PTS(NPdV~d,Y\,V/Q_T\,`PNb[oQ_Nb[(vQZY_x\,`b[(v7dT7lv3Nb[@NPSswpV7d
cV/[(v\[@z 

9@q2|vTSZx

\&p@sav\YeVQ_S(S9YQ_S(T`b[&S(\_opSs[(v\[s[(v7QgY(d,{h`bQ_mNPS&\ggh\Y(Q_V[(`bo~Vd,[2[kY\,WX[(\{j`bQ,x7{hT7[s\,V;a
v\YehV7Q_S(SsY(Q_S(Th`b[*S(\_opS|[(v\[@[(v7QgY(d,{j`bQ_mNPS*WXQZY([(\,NPVh`boV7d,[@[kY\,WX[(\{h`bQ,q
@Q_SkQ_\YWvQZYSs\,`PSkdWXdVS(NPe7QZY2gY(d,{h`PQ_mRS;[(v\[s\Y(Q;aWXdmEgh`PQZ[kQTVhe7QZY2`bd,lS(gh\,WXQd,Y&d,[(v7QZY*vNblvh`bo
Y(Q_Sk[kYNPWX[kQ_eEY(Q_eThWX[(NbdVSq)7d,Y9QX7\,mEgh`PQ,x[(v7Q|gd`PNPWXoUQXpNPS([kQ_VWXQ|gY(d,{j`bQ_mO!d,Y2NPVpijVNP[kQXav7d,YNbZdVy2S
NPS;aWXdmEgj`bQZ[kQ)\gh\,ehNPmRNb[kYNPdT|S(Nb[(S(NPw`PNfSZx^_,,q|vNPSNPSTSkQZOT`NPVO!d,Ym~\[(NbdV>xh{Q_WZ\,TS(QyNb[NPS
l,Q_V7QZY\,`f`bo~[(v7dTlv[*[(v\[|aWXdmEgh`bQZ[kQUgY(d,{h`bQ_m~S2\Y(Q$V7d,[*S(TS(WXQZg[(NP{h`bQ[kd~S(NblVNbijWZ\,V[sSkgQZQ_epaT7gpNP\
gh\Y\,`f`bQ_`PNb_\[(NbdV^q7d,Y\RmEd,Y(Q$[(v7d,Y(dT7lv/ehNPS(WZTS(SNbdVd,O;;aWXdmEgh`PQZ[kQ_V7Q_S(SZxSkQZQYQZQ_V`P\_cUxh@dd
,QZY
x
T7ZZd7x_,,qfi
4Q3\,`fSkd4wVd
c[(v\[R&7@*|x2S(d1
\,Vo3\Y(lTmEQ_V[d,Y{Q_`PNPQZO+[(vh\[ 

&p@sc&dT`PeNfmEgh`bo1
|q|vTSZx


NPmRgh`PNbQ_S*[(vh\[  sp@s|qE7d,YQ_`P\{d,Y\[(NPdVS@d,O9[(vNPS

WXdmEgh`PQXpNb[o[(vQZd,Y(ogYNPmEQZY
xSkQZQ$\,VouWXdmEgh`PQXpNb[o[(vQZd,Y(o[kQX[ZxhSTWv\,S|)\gh\,eNPmRNP[kYNbdT>x_,G7qfi
rtV[(vNPSgh\gQZY_x>c&QRS(v7dc[(v\[[(v7QZY(QRNPS\uV7Q_WXQ_SS(\Y(o[kY\,e7QXaLd{QZ[c&QZQ_V4YTVVhNPV7l~[(NfmEQlT\Yka
\,V[kQZQ_Sy\,Vhe1gQZY(Od,YmR\,VWXQElT\Y\,V[kQZQ_SyOd,Y$\,Vo4l,Q_V7QZY\,`&&z\gghY(d_7NPmR\[(NbdV\,`bl,d,YNb[(vhm
TV`PQ_S(SU

d,YE
sp@s|q?\{j`bQlNb,Q_S\,Vd
,QZY(pNbQZcd,O|dT7YyY(Q_S(Th`b[(SZqfid,[kQ[(v\[


\,S(S(ThmRNPV7l  d,Y  sp@s&@[(vNfS[kQ_`P`PSTS[(v\[[(v7QZYQNfSV7du\,`bl,d,YNb[(vm[(v\[YTVSNPV

[(NPmEQEgd`bopV7dmRNf\,`?NPV[(vQRS(NbZQEd,Os[(v7QEY(QZgYQ_SkQ_V[(\[(NPdV4d,Os[(v7QR&[(v\[ijVeS\ugd`PNPWXo3[(v\[
NPSWZ`bdSkQ[kdd,g[(NPm~\,`?Z3X,X

   L  Xq1r[Ee7dQ_SEVd,[S\_o[(vh\[yO\,Sk[E\,`bl,d,YNP[(vmRSyc@NP`P`9gY(dpeTWXQ
O\YkaLO!YdmaLd,g[(NPmR\,`9G\,`fT7Q_SOd,Yy\,`f`9&9S_][(v7QZY(Q\Y(Qm~\,Vo1NPVhSk[(\,VWXQ_Scv7QZY(Q~[(vQ\,`bl,d,YNP[(vmRS
\,`bY(Q_\,eoNPVTSkQd,Y{Q_NPV7le7QZ,Q_`bd,gQ_ec@NP`P`{Qy{d,[(vO\,Sk[\,Ve4WZ`bdSkQ,q$4QRS(NPmEgh`PoWZ\,V^[lT\Y\,V[kQZQ
[(v\[|[(vQU\,`bl,d,YNb[(vhmRSsc@NP`P`;9 

ijVheu\RWZ`bdSkQXaL[kdaLd,g[(NfmR\,`^gd`PNPWXoThNPW(wp`bo,q

_

fi



Q~v'z 

C

vh73Qh{.L8$>${3

y{#y{|}$}z~v

+\Y[(NP\,`>d,{hSkQZYG\{hNf`PNb[o





Sk[(\[(NbdV\Yo

_a\gg^q





[(NfmEQXae7QZgQ_Ve7Q_V[



Sk[(\[(NbdV\Yo



T`P`d,{hS(QZY(G\{jNP`PNb[o
Sk[(\[(NbdV\Yo
Sk[(\[(NbdV\Yo

 a\gg^q
_
_a\gg^q
 a\gg^q
_
 a\gg^q
_






[(NfmEQXae7QZgQ_Ve7Q_V[

[(NfmEQXae7QZgQ_Ve7Q_V[

\{d,QXa\_l7qj\,`PT7Q




vhNPSk[kd,Y(oae7QZgQ_Ve7Q_V[

~vfiffz}8

y~ Z





Sk[(\[(NbdV\Yo

V7d,{jSkQZY(\{hNP`PNb[o


	

x~vy{z ~




V7d,[@ThV`bQ_S(Ss  
V7d,[@ThV`bQ_S(Ss  &p@s
V7d,[@ThV`bQ_S(Ss  
TVhWXdmEghT7[(\{h`PQ

\,e\,VN>QZ[\,`qbx_,,




_a\gg^q


@&


a\,eeNb[(Nb,Q\ghg^q

V7d,[@ThV`bQ_S(Ss 

a\,eeNb[(Nb,Q\ghg^q


V7d,[@ThV`bQ_S(Ss 
V7d,[@ThV`bQ_S(Ss 



;av\Ye
;av\Ye

+\{h`bQR}2\YehV7Q_S(S*Od,Y|gh\Y([(Nf\,`P`bo~\,Ve/OT`f`boRd,{hSkQZYG\{h`PQ9S





#y{~ffz}z~|

#y{z|}{z |3#

|v7QSk[(\[kQud,O|[(vQ\Y[ycNb[(vYQ_SkgQ_WX[$[kd1sgd`PNPWXoaijVehNPV7l/\,`bl,d,YNP[(vmRSyNPSy[(v\[[(v7QZY(Qu\Y(Q
[(v7Y(QZQ$[ogQ_S@d,O\,`bl,d,YNb[(vmRSNPV3TS(QUd,YTVe7QZY@NfV,Q_Sk[(NPl\[(NbdV>}sQXp\,WX[\,`bl,d,YNP[(vmRSZx\ggY(d
pNPm~\[(NbdVSZx
\,Ve

v7Q_T7YNfSk[(NPWZSZq;7\,WX[\,`bl,d,YNb[(vm~S\[k[kQ_mEgh[[kdijVeQXp\,WX[uSkd`PT[(NbdVSZqrtV[(v7Q3ijVNb[kQXavd,YNbZdV

WZ\,SkQ_SZx[(v7QZoYTVNPV

c&d,YSk[taWZ\,SkQ4[(NPmRQ\[`bQ_\,Sk[QXpgdVQ_V[(NP\,`NPV

[(v7Qv7d,YNbZdV\,S(S(Tm~NPV7l\nSk[kY\,NPlv[kOd,Y(cs\YeYQZgY(Q_SkQ_V[(\[(NbdV

[(v7QS(NbZQd,O[(v7Q&\,Ve

d,O[(v7Q&&q&rV

[(v7QNfVpijVNb[kQ

d,O

v7d,YNPZdV>x?[(vQZoe7dVd,[$V7Q_WXQ_S(S\YNP`bo4v\,`b[Zx){hT[$WZ\,V{QSk[kd,ggQ_e1cv7Q_V1[(v7Qgd`PNPWXoNPS$c@Nb[(vNPV

d,g[(NPm~\,`\Wv7Q_W(w\{h`bQ/WXdVehNb[(NbdVjq4ggY(d
pNPm~\[(NbdV\,`Pl,d,YNb[(vmRSWXdVS([kYTWX[E\ggY(d
pNfmR\[(NbdVS$[kd

c@v\[2[(v7Q@QX7\,WX[s\,`bl,d,YNb[(vm~S;ijVe^qsp\,mEgj`bQ_Sd,O>[(vNfS9NPVWZ`fTe7Q*l,YNPe7aL{h\,SkQ_eRmEQZ[(vdeS_x\,TSkwY(Q_Wv[Zx



_,]>M^d,Q tdo,x_,p]vNb[kQ,x+_,pqfiQ_T7YNPSk[(NfWZS|WXdmEQ$NfVu[c&d

\_,d,YSZ}s[(v7dS(QU[(v\[WXdVSk[kYTWX[d,Y

ijVe\,WX[(T\,`9gd`PNPWZNbQ_S[(v\[UWZ\,V4{QEQZ\,`PT\[kQ_e>x?\,Ve[(v7dSkQR[(vh\[SkgQ_WZNbO!o\umEQ_\,VSd,OsWvddS(NfV7l/\,V
\,WX[(NbdV1Od,Y|QX7\,mEgh`bQ,x;kmEdSk[|`fNbw,Q_`boRSk[(\[kQ_xjc@vNPWvedVd,[&opNbQ_`Pegd`PNPWZNbQ_S2[(v\[|WZ\,V{QQZG\,`fT\[kQ_e
TS(NfV7l[(v7QSk[(\,Ve\Ye>x`PNPV7Q_\Y|\,`Pl,QZ{Y\GaL{h\,SkQ_e3mEQZ[(v7dpeSZq

|vQ*{Q_S([WZT7Y(Y(Q_V[;QXp\,WX[&\,`bl,d,YNb[(vhmNPSNPVWXYQ_mEQ_V[(\,`pghYTVNPVlrt&?c@NP[(vgdNfV[taL{h\,S(Q_eRNPmEgYd
,QXa

"!
#!>v\,V7l7x_,,q@M?Nb[k[(mR\,V>S\,V\,`bopS(NPS*d,O;[(vQc@Nb[(V7Q_SS|\,`bl,d,YNb[(vhm MNP[k[(mR\,V>x
Q_\,V>x>%$\Q_`P{h`PNPV7l7x^_,,]s\,SS(\,Ve7Y\px&$\Q_`b{h`fNPV7l7xhMNb[k[(mR\,V^x?_,,Sk[(NP`P`>\ghgh`PNbQ_SZ}&|vNPS@\,`bl,da
mEQ_V[y vh\,V7l7xhM^QZQ,x>

YNb[(vhmY(Q_TNbY(Q_S>QXpgdV7Q_V[(NP\,`[(NPmRQ2NPV[(vQ9c&d,YSk[?WZ\,S(Q,q)@v7Q9TVeQZY`bopNPV7ls[(v7QZd,Y(od,O7[(v7Q_S(Q2\,`bl,d,YNP[(vmRS
Nb[(V7Q_S(SZx+rt?xQZ[(WqfiOd,Y$NfVpijVNb[kQXavd,YNbZdVWZ\,SkQ_Sye7QZgQ_VeSdV&Q_`P`fmR\,V>S\,VedVeNPwjSc&d,Y(wdV

\,`PT7Q2NP[kQZY\[(NbdVOd,Y)9S+\,Vey&9Ss&Q_`P`PmR\,V>xp_,]ppdVeNbwx_]7pmR\,`P`bc&ddeUdVehNbwjx

('

_ q
|vQ{Q_Sk[ywVd
c@VnO\,mRNP`bo4d,O\ggY(d
pNPm~\[(NbdV\,`bl,d,YNb[(vm~S$NPSywpV7d
cVn\,Sl,YNPemRQZ[(v7dehSZq4|v7Q
{h\,S(NfWENPe7Q_\/NPS[kdTSkQR\/ijVNb[kQEl,YNPe4d,OsgdNPV[(SUNPV[(v7Q~{Q_`PNPQZO2Skgh\,WXQ[(v7QSkgj\,WXQRd,O|\,`P`)gY(d,{j\{hNP`PNb[o
eNPS([kYNb{hT7[(NPdVSd
,QZYy[(v7QSk[(\[kQ_Syd,O*[(v7Q&

[(vNPS$NPS[(v7QTVe7QZY`PoNPVlSkgj\,WXQ~Od,YU[(v7Q\,`bl,da

YNb[(vhmRSmEQ_V[(NPdV7Q_e1\{d
,Q[kd3e7QXijV7QE\ugd`PNfWXo,qEVhWXQE[(v7QRl,YNPegdNPV[(S\Y(QRWv7dSkQ_V>x+\,`P`)d,O2[(vQ_SkQ
\,`bl,d,YNP[(vmRS|TS(QG\,`fT7QUNb[kQZY\[(NPdV/dV/[(vQUgdNPV[(S*[kd~d,{[(\,NfV3\Rgd`PNfWXoOd,Y@[(v7dS(QU{Q_`PNbQZO+Sk[(\[kQ_SZx[(v7Q_V
NPV[kQZY(gd`P\[kQ[kd[(v7QUcv7d`bQ{Q_`PNbQZO)Skgh\,WXQ,q|@v7Q$eNbQZY(Q_VhWXQUNPVu[(v7Qy\,`bl,d,YNb[(vhmRS@`PNPQ_S@NPVu[(vQ$WvdNPWXQ$d,O
l,YNPegdNPV[(SZqsVEQX7WXQ_`P`bQ_V[S(T7Y,QZoy\ggQ_\YS;NfVy\,TSkwY(Q_Wv[Zx_,qfi|v7Q_SkQ|\,`Pl,d,YNb[(vmRS;\YQ|WZ\,`P`bQ_e

*)


fi 38?>v{v8.^j

$33$3

\ggYd_7NPmR\[(NbdV\,`bl,d,YNP[(vmRS{Q_WZ\,TSkQ[(v7QZo\ggY(d
pNfmR\[kQ[(v7QEgY(dpWXQ_S(Sd,O9\,`PT7QENb[kQZY\[(NbdV>x>c@vNfWv
[(v7QQX7\,WX[@\,`bl,d,YNb[(vmRS|\,`Pl,d,YNb[(vmRS*WZ\YY(odT[|QXp\,WX[(`Po,q
Q_T7YNPS([(NPWZS9[(vh\[sedVd,[2opNbQ_`PeQ_\,S(NP`boEQZ\,`PT\[kQ_egd`PNfWZNbQ_S9\Y(QS(TY(,QZo,Q_euNPVLs\,S(S(\,VeY\px>_,,q
|v7Q_S(Q\Y(Qd,O[kQ_V,QZY(o~Q_\,S(o~[kdRNPmEgj`bQ_mEQ_V[Zx\,VeNPVWZ`fTe7Q@[kQ_WvVNPT7Q_SsS(TWv\,SEkmEdSk[*`PNbw,Q_`boRS([(\[kQ_
WvddS(NfV7l4\Sk[(\[kQucNb[(v[(vQvhNblv7Q_Sk[$gY(d,{h\{hNf`PNb[oOY(dm
SkopSk[kQ_m

c2QZY(QOT`P`bod,{hSkQZY(\{h`bQx;\,VenmRNPVhNPm$ThmQ_V[kY(d,go

[(v7Q{Q_`PNbQZO*Sk[(\[kQ,x2\,Vhen\,WX[(NfV7l4\,SyNPO|[(v7Q
Wv7ddSNPV7l3[(vQ\,WX[(NbdV[(v\[ylNb,Q_Sy[(v7Q

mEdSk[/NfV7O!d,YmR\[(NbdV\{dT7[u[(v7Q1WZT7Y(YQ_V[/Sk[(\[kQq[(v7QZYSue7QZgQ_Ve

dVt,d,[(NfV7l7xc@v7QZY(Q1S(QZ,QZY\,`

v7Q_T7YNPSk[(NPWZS&d,Y@d,gh[(NbdVS*\Y(QUWXdm${hNPVQ_e>q
|vQZY(Q\Y(QUv7Q_TYNPSk[(NPWZS&{h\,S(Q_eudVijVNP[kQvhNPSk[kd,YNbQ_Ssd,Y@d,[(v7QZYTS(Q_Ssd,O?iVNb[kQ\,mEdTV[(S|d,O)mEQ_mEd,Y(o
c@Nb[(vhNPVU[(vQ|\,`bl,d,YNb[(vmLpdVeNbwx7_]p2`P\[k_mR\,V>xj_,]p\,VSkQ_V^xh_,,\pxj_,,G{^]pMThSkQ_V\pxMNLxpNb[ta


$

$

[(NPV7l,QZY
xh4Q_`P`PS_xjd`feS(mRNb[(v^x_,,]>3Q_T`PQ_\,T>x Nfmux \Q_`b{h`fNPV7l7xh*\,S(S(\,Ve7Y\px?_,,]>3Q_Th`bQ_\,T>x

+$ ,$ \Q_`b{j`PNPV7l7x_,,];+Q_S(v7wpNPV>xQ_T`bQ_\,T^x?-$ \Q_`b{j`PNPV7l7x;_,,]\,VSkQ_VQ_V7l7x
(././.p]0$NPmuxQ_\,V>xQ_T`bQ_\,T>x1(././.q|@dVQUd,O)[(v7Q_SkQyWXdmEQ_S@cNb[(vugY(dd,OS*d,O;WZ`bdSkQ_V7Q_S(S_xhQX7WXQZg[

+Q_S(v7wpNPV>x Nfmux?

Od,Y9SkdmEQ|d,O>\,VSkQ_V>S9c2d,Ywjq;7d,Y2[(v7Q|Y(Q_Sk[Zx[(v7Q|[kY\,eQXaLduvh\,S;{QZQ_VRmR\,e7Q|{QZ[c2QZQ_V~O\,Sk[2SkQ_\YWvhNPV7l
[(v7Y(dTlvugd`PNPWXo~Skgj\,WXQU\,VeulT\Y\,V[kQZQ_SZq

3254 }yff6}{y{ ~
798Sx#z|;:H$Qy
rtVEpQ_WX[(NbdV;xc&QslNb,Q*O!d,YmR\,`pe7QXijVNb[(NPdVSd,O2S\,VeE&9S;\,Vhe$gd`PNfWZNbQ_SZ],[c&daLghv\,S(Q*[kQ_ma
gd,Y\,`ps\_o,Q_S9VQZ[(S|
 |sS;\Y(Q*e7QXijV7Q_eNPVRQ_WX[(NbdVRq)rtVRQ_WX[(NbdV;' xc&Q|e7QXijV7Q<Z a\ghgY(d_7NPmR\[(NPdVS

\,Ve\,eeNb[(Nb,Q\ggY(d
pNfmR\[(NbdVSZx\,VeSv7d
c\Y(Q_`f\[(NbdVS(vNPgR{QZ[c&QZQ_V~[(v7Q[c&dy[ogQ_S2d,O^\ggYd_7NPmR\Ga

{hNP`fNb[oROd,Y9S|\,Vhe/&9SZq
4Q~S(QZgh\Y\[kQR[(v7QRWXdmRgh`bQX7Nb[oY(Q_S(T`b[(SOd,YijVNP[kQXav7d,YNbZdVgd`PNPWXo3\ghgY(d_7NPmR\[(NPdVO!Y(dm[(vdSkQ
Od,Y2NPVpiVNb[kQXav7d,YNPZdV$gd`PNfWZNbQ_SZqQ_WX[(NbdVR$WXdV[(\,NPVhS9V7dV\ghgY(d_7NPmR\{jNP`PNb[oUY(Q_S(Th`b[(S;Od,Y;ijVNb[kQXavd,YNbZdV
&gd`PNfWZNbQ_SZ]Q_WX[(NbdV3WXdV[(\,NPVS|VdV\ggY(d
pNfmR\{hNP`PNP[oO!d,YNPVpijVNP[kQXav7d,YNbZdV&gd`PNa
WZNbQ_SZq;`b[(v7dT7lvNb[NfS?YQ_`P\[(Nb,Q_`bo$Q_\,Sko$[kdijVeyd,g[(NPmR\,`pgd`PNPWZNPQ_SZxc&Q|WXdVS(Nfe7QZY\ggY(d
pNfmR\[(NPV7l


gd`PNPWZNPQ_S*NPVQ_WX[(NbdVxjS(NfVWXQ[(v7Q$STWZWZNPVWX[(`boY(QZgYQ_SkQ_V[kQ_eWZ\,SkQ,x\[`bQ_\,Sk[ZxjNPS|gYd
\{h`bov\Ye

[kd~\gghY(d_7NPmR\[kQ,q
dmRQ@d,O[(v7QmRd,Y(Q@[kQ_WvVNPWZ\,`hgY(dd,OS2\Y(Q@NPVWZ`fTe7Q_eENPVR\ghgQ_VheNPWXQ_SNPVRd,Ye7QZY9[kdymR\w,Q[(v7Q@{dpe7o
d,O&[(v7QEgh\gQZYmEd,Y(QEY(Q_\,e\{j`bQ,qE@dc2QZ,QZY_x)SkdmEQEghY(dd,OSOY(dmd,[(v7QZYgh\gQZYS\Y(QRS(w,QZ[(Wv7Q_eNPV[(v7Q
{deo4d,O|[(v7Qgh\gQZYyNfVd,YeQZYy[kd4mEd,[(NPG\[kQ{d,[(vn[(v7QYQ_S(T`b[(Sy\,Ven[(v7QgY(dd,OSyV7QZc@`Po4gY(Q_SkQ_V[kQ_e
v7QZY(Q,q

=)C;>?@&F

KKLH F 

d,[kQ[(v\[2S\Y(QNPVO\,WX[SkgQ_WZNP\,`|WZ\,S(Q_Sd,Os2SZq|vQWXdmEgh`PQXpNb[od,OijVeNfV7l1\,Ve
\ggYd_7NPmR\[(NPV7ld,g[(NfmR\,`+gd`fNPWZNbQ_Se7QZgQ_VeSdV4[(v7QEd,{hS(QZY(G\{jNP`PNb[o/d,O9[(v7QRS(oSk[kQ_m/xSkdudT7YY(Q_ST`b[(S
\Y(QUS(QZl,Y(QZl\[kQ_e3{od,{hSkQZY(\{hNP`PNP[o,qd
c&QZ,QZY_xhdV7Q$SkQZ[|d,O)e7QXijVNb[(NPdVSsS(TpWXQ_SZq

21 :H$y}z3'BA;	|ByDC	


CFE  z|z~v:y{~0|B|B|

$y{3~

gh\Y[(NP\,`P`bod,{hSkQZYG\{h`PQ\Y(w,d
e7Q_WZNPS(NPdV~gYdWXQ_S(Sss9eQ_S(WXYNb{Q_S9\yWXdV[kY(d`P`bQ_eSk[kdpWv\,S([(NPW

G

SkopSk[kQ_m{o~NP[(S9Sk[(\[kQ_S*\,Ve[(v7QWXdVSkQ_T7Q_VWXQ_S&d,O\,WX[(NbdVS2dV[(v7QS(oSk[kQ_m/q;r[&NPS9e7Q_Vd,[kQ_e\,S&\$[(T7gh`bQ


HJILK / INMOIQPOINRSIUTINVxhc@v7QZYQ
W H xM\,VeXP\Y(QijVhNb[kQSkQZ[(S*d,O


 L  x)     

DY


\,Vhe  XG,    ]

fiC

vh73Qh{.L8$>${3


W K /[Z HNPSs[(v7Q        L,]
W &R }\H^]_M]`Hbadc .eI_SfNPS)[(v7Q  L,k    p    xc@v7QZY(QRZK\IUg
ILKDhb)NfS;[(v7Q*gY(d,{j\{hNP`PNb[o
[(vh\[Sk[(\[kQOK h NPSY(Q_\,Wv7Q_e4OY(dm
Sk[(\[kQOKRdV4\,WX[(NbdVignO!d,YQZ,QZYojK Z Hz\,Vejg Z M]^Q_NP[(v7QZY
kml"n3o*pqZR KrIUgqILKDbh   xhNbO+\,WX[(NbdVXgWZ\,V/{Q\ggh`fNbQ_edV/Sk[(\[kQsKx7d,YkltnuovpqRZK\IUg
ILKDhb  .]
W T }/HwaxP NPS&[(v7Q  Z,  $p    x7cv7QZY(Q97T K 2NPS9[(vQd,{jSkQZY(\[(NbdVmR\,e7QNPVS([(\[kQyKx (
V }qHz]^M5a|{ NPS[(v7Q(X9kyp    ,x^c@v7QZYQsV KrIUpg NfS[(vQEY(QZc&\Yel\,NPV7Q_e{o3[(\wpNPV7l
W 3
\,WX[(NPdVX
g NPVuSk[(\[kQsK q
rO|Sk[(\[kQ_SE\,Ved,{hSkQZY(\[(NbdVS\Y(QNPe7Q_V[(NPWZ\,`x)NqQ,q}P
H \,Ve~3

T NPSU[(v7QNfe7Q_V[(NP[o4OTVWX[(NbdVd,Y

\{hN k
 Q_WX[(NbdVjxh[(vQ_V/[(v7QNPSWZ\,`P`bQ_e~pfiu  XGZbZqV7d,[(v7QZYSkgQ_WZNP\,`WZ\,S(QyNPSUpj  XGZb

3  xc@v7QZY(Q*[(vQ@SkQZ[d,Od,{hSkQZYG\[(NbdVhSWXdV[(\,NfVSdV`PoUdV7Q|Q_`PQ_mEQ_V[ZxNqQ,qNPVQZ,QZYoRSk[(\[kQ@[(v7QS(\,mEQ
d,{hSkQZYG\[(NbdV3NPS*mR\,e7Q,xh\,Vheu[(v7QZY(QZOd,Y(Q[(v7Qd,{jSkQZY(\[(NbdVuOTVWX[(NbdVuNPS@WXdVSk[(\,V[Zq

H]H[(\{h`PQ_SZxdVQOd,Y&Q_\,Wvu\,WX[(NbdV>q2@dc2QZ,QZY_x7c&Qc@NP`P`
\,`PSkd1ehNPS(WZTS(SymEd,Y(Q/STWZWZNPVWX[EY(QZghY(Q_SkQ_V[(\[(NbdVSZ}uNPVngh\Y([(NfWZT`P\Y_x@9,Pqp
   @ 7t,<   _ 

 |sqh|v7Q_S(Qc@NP`P`j{Qe7QXijV7Q_euNfV3Q_WX[(NbdV3q
2132 H: ~v'z z|3#H: yv7 ~vy{3 |
d,YmR\,`P`Po,x9Ss\Y(QY(QZgY(Q_S(Q_V[kQ_e{o

gd`PNPWXo4e7Q_SWXYNb{Q_SUv7dc[kd\,WX[Ee7QZgQ_VehNPV7l/dVd,{hSkQZY(\[(NbdVS_q4Q/eNfSk[(NPV7lTNfS(v[(v7Y(QZQ[ogQ_S$d,O
gd`PNPWZNbQ_S_q

W

 l


 L,  j,Rp  
[kd\,V3\,WX[(NbdV>q

_GNPS\OTVWX[(NbdV^ l &} P-aMx>mR\gghNPVl~Q_\,Wvd,{jSkQZY(\[(NbdV

Od,Y

  ~
 Xk7XjXh$p,  q NPS\ OTVhWX[(NbdVq
 }OP] a M xUmR\ggjNPV7lQ_\,Wvgh\,NbY
 d,{jSkQZY(\[(NbdV>x[(NPmRQ*2[kd~\,V3\,WX[(NbdV^q
W  f LGk7XjXhhp  y
@NPS+\@OTVWX[(NPdVyq}eP<aMx,mR\ghghNPV7l|Q_\,WvyijVNb[kQ&SkQ_T7Q_VWXQ

W



d,O)d,{hS(QZY(G\[(NPdVS*[kd~\,V/\,WX[(NPdV>q

d,[(NPWXQu[(v\[Zx2O!d,YE\,VTV7d,{jSkQZY(\{h`bQ|x\vNPS([kd,Y(oaeQZgQ_Vhe7Q_V[$gd`PNPWXo1NPSyQ_TNb\,`bQ_V[y[kd1\
[(NPmEQXaeQZgQ_Vhe7Q_V[sdV7Q,q
@Q_WXQ_V[\,`bl,d,YNb[(vm~NPWe7QZ,Q_`bd,ghmEQ_V[Rv\,SRNfVWZ`PTe7Q_eWXdVSNPe7QZY\[(NbdVd,OijVhNb[kQumEQ_mEd,Y(ongd`PNPWZNbQ_S
\,Sc2Q_`f`s\,VSkQ_V^x)_,,G{x_,,\p]?MThSkQ_V\pxMNxpNb[k[(NfV7l,QZY_xh4Q_`f`PSZx^d`PeS(m~Nb[(v>x?_,,]+3Q_Th`bQ_\,T>x

$NPm/x&$\Q_`P{h`PNPV7l7xs\,S(S(\,VeY\px;_,,]?Q_T`bQ_\,T>x>+Q_S(v7wpNPV>x0$NPm/x$\Q_`b{j`PNPV7l7x+_,,]?+Q_S(v7wpNPV>x
Q_T`bQ_\,T>x>%U
$ \Q_`b{h`PNPVl7x_,,]^\,VhSkQ_V4 7Q_Vl7x0(././p. ]+U$ NPmuxjQ_\,V>x^3Q_Th`bQ_\,T>x0(././. q@v7Q_SkQ
\Y(Q/gd`PNfWZNbQ_Sy[(v\[R\Y(Q3\,`P`Pd
c&Q_eSkdmEQuiVNb[kQu\,mEdTV[Ed,OmEQ_mEd,Y(o]|S(TpWZNPQ_V[E\,`P`bdc&\,VhWXQ_SRc&dT`Pe

Q_V\{h`PQS(ThWv\gd`fNPWXo4[kd4S(NPmyT`P\[kQ\3OT`P`&vNPSk[kd,Yoae7QZgQ_VeQ_V[gd`PNfWXo4d
,QZY~\ijVNP[kQv7d,YNPZdV>x;d,Y

kTheNPWZNbdTS`bo,qV7Q3\YNP\,V[
d,OijVNb[kQumEQ_mEd,Yogd`PNPWZNbQ_S_xc@vNPWvnc2Q/WZ\,`P`)(y  ~E,/p _  L ipQ_SE[(v7Q/\,mRdTV[Ed,O
mEQ_mEd,Yoj  ,  q
d,Y(QO!d,YmR\,`P`bo,x@\nOY(QZQijVNb[kQmEQ_mEd,Y(ogd`fNPWXoc@Nb[(v [(v7QijVNb[kQSkQZ[
d,OUmRQ_mEd,Y(o S([(\[kQ_S
Od,Y~&%G
H9INMOIQPOINRSIUTINVENPSE\OTVWX[(NbdVw&}P]
a M] x2m~\gghNPV7lQ_\,Wv

SqL"0"(L&mQvQv"QL"Qv1D"S(Lv"QJ*1*Q&S*JL</N1
"qq6D"S(Lv"Q
S*"QQvNLJ/+"*"*N"}Sv++"D""v"+S*"SU"Sv
+"mQvm/SDvQL(v"Q
"+S
gQZYv\ghS\[(NfmEQXae7QZgQ_Ve7Q_V[Rgd`PNPWXo,x*d,Y[kdnThSkQ`bQ_SSmEQ_mRd,Y(omEd,Y(Q

v


fi 38?>v{v8.^j

$33$3

 d,{hSkQZYG\[(NbdV^x&mEQ_mRd,Y(onS([(\[kQ*gh\,NbY~[kd\gh\,NbY  \,WX[(NbdV^xsmRQ_mEd,Y(oSk[(\[kQ*q
 WZ\,V/{QSkQZQ_V3\,S\yijVNb[kQ/kS(WXY\[(WvmEQ_mEd,Y(o,q

@v7Q/SkQZ[Rd,OmEQ_mEd,Y(o

Sk[(\[kQ_S

7YQZQiVNb[kQ@mEQ_mRd,Y(oRgd`PNPWZNbQ_S9WZ\,V\,`fSkd$S(Nfm$T`f\[kQSk[(\[(NbdVh\Y(oRgd`PNPWZNbQ_S_]\,`P`v\YeV7Q_SSY(Q_S(Th`b[(SOd,Y

Sk[(\[(NbdVh\Y(ogd`PNfWZNbQ_S\ggh`bo[kduO!Y(QZQRijVNb[kQmEQ_mEd,Y(ogd`PNPWZNbQ_S\,Sc&Q_`P`qE&Q_WZ\,TSkQ~dV7Q~WZ\,V1WXdVSNPe7QZY
\OY(QZQijVNb[kQmEQ_mEd,Yo



X 

H#]w


gd`PNPWXo[kd{Q4\Sk[(\[(NPdV\Y(ogd`PNfWXod
,QZY[(v7Q1Sk[(\[kQSkgj\,WXQ

( U  

7Z(pj 
   jP
   ~ Rup Z  

 b 
 
2X



~~X    k  p 

X,

x,!

 j47 Z   pZ(
 L,  
|v7Q/\,eG\,V[(\l,Q_S~d,OOY(QZQuijVNP[kQmRQ_mEd,Y(ongd`fNPWZNbQ_S\ggQ_\YRNPV

 
[(v7Q3WXdVhSk[(\,V[(Sd,O[(v7Q/\,`bl,d,YNP[(vmRSZxs\,VeNPVS(gQ_WZNf\,`x9gYd,{h\{h`bon`P\Y(l,Q,x*S(T7{>WZ`P\,S(S(Q_Sd,Os2SZx
c@v7QZYQ@\ijVhNb[kQ|\,mEdTV[9d,O^mEQ_mRd,Y(oES(TpWXQ_S2O!d,Y&\,VRd,g[(NfmR\,`gd`PNPWXo,q)|v7QmR\ZQNPVhSk[(\,VWXQ_S9STWv\,S

/'

W
s\,`P`PTm/S|mR\ZQW
s\,`P`fTmux^_, ]MNb[k[(mR\,V^x_,G|\YQ$S(TWv/QX7\,mEgh`bQ_S_}9W
*\,`P`PTmuS@mR\ZQ
Y(Q_TNbY(Q_SsdVh`bo{jNb[*d,O+mRQ_mEd,Y(o[kdEijVeu\,V3d,g[(NPmR\,`gd`PNPWXo,q

JG

HJILK / INMOIQPOINRSIUTINV*{Q\~&|q
k_  L,;4
 t$bZfi ZG NfS@\~S(Q_T7Q_VhWXQUd,OSk[(\[kQ_S[ z / I  ( I  - I*I  B.px
*
H
*

@
c
h
v
P
N

W

v
k
S
(
[
\Y([(Sc@NP[(v3[(v7QNPVNP[(NP\,`Sk[(\[kQd,O
G x^NqQ,q  /  K / q4QRThSkQy0pKG|[kdue7Q_V7d,[kQ[(v7Q
 Z
SkQZ[|d,O)`bQ_V7l,[(v7a
 [kY\*k Q_WX[kd,YNbQ_S|cvNPWvQ_Vhe/NPVuSk[(\[kQK q
|vQuQXgQ_WX[kQ_eYQZc&\Yed,{[(\,NPV7Q_eNfVSk[(\[kQ^
K \O![kQZYQXp\,WX[(`Pow
 Sk[kQZghSRTVhe7QZYEgd`PNPWXo
 NfSE[(v7Q
Y(QZcs\Ye/d,{h[(\,NPV7Q_e3NfVU
K {o[(\wNfV7l~[(v7Q$\,WX[(NbdVS(gQ_WZNbihQ_e{o) xjc2Q_Nblv[kQ_e{o[(v7Q$gY(d,{h\{hNf`PNb[o~[(v\[
K NPS*\,WX[(T\,`P`boY(Q_\,Wv7Q_e3\O[kQZYu S([kQZghSZx
$
 NPS&\ySk[(\[(NbdV\Y(o
W V KrIL&IU   V KrIU pT K kk
 +S   !0 o* + l 0v u  ( XR  q ( IU 7T  q ( kQI  x7NbO0
MQZ[





gd`PNfWXo,x

 oD l   ( ZR    ( IUT7  ( QINz
QI  x&NbOJNPSE\
+    !0
+ 0 
[(NfmEQXae7QZgQ_Ve7Q_V[*gd`PNPWXo,x7\,Ve
W VKrIL&IU  +       0 oD  + l 0 VKrIUT7  / 
QT7  kk    ( RZ   ( IUT7  / 
QTp   ( kQI   x
NPO4NPS*\~vNPSk[kd,Y(oae7QZgQ_Ve7Q_V[sgd`PNPWXo,q
W VKrIL&IU



VKrIUTpKQILkm

sm~\_o/{Q_v\
,QyeNbQZY(Q_V[(`bo/TVhe7QZY@d,g[(NfmR\,`gd`PNfWZNbQ_S*Od,YQ_\,Wv[ogQyd,Ogd`PNPWXo,q|v7Q

Q

T\,`PNb[oud,O2\gd`fNPWXouNPSe7QZ[kQZYmRNPVQ_e/{o3Nb[(S@7XZ R  Xx?NqQ,qj{o/[(v7QQXpgQ_WX[kQ_eYQZc&\YehS\,WZWXYT7Q_e
{oNP[ZqR4QeNfSk[(NPV7lTNfS(v3{QZ[c2QZQ_VeNQZY(Q_V[gQZY(Od,YmR\,VhWXQRmEQZ[kYNPWZSOd,Y$s2S[(v\[UYTVOd,Y$\
ijVNP[kQVTm${QZY*d,O)S([kQZghS|\,Veu[(vdSkQ[(v\[|YTVuNPVe7QXiVNb[kQ_`bo,q

t u
S

G
 G-

 GIU

(
Fu  /   lUo*p 7V KrINUIUq@L[(vQZY&c&d,Y(wR\,S(STmEQ_S2[(v\[2[(vQv7d,YNbZdV~NPSp^U G,x7NPVhSk[kQ_\,e~d,O
 G-fi q2@vNPSse7dQ_S|V7d,[@Wv\,V7l,Q[(v7Q$WXdmEgh`bQX7Nb[od,O;\,Vod,O)dT7Y|gYd,{h`bQ_mRSZqfi
W @v7Q3NPVpiVNb[kQXav7d,YNPZdV LL !X ph(3XXSR   lNb,Q_SY(QZcs\YeSEd,{[(\,NfV7Q_eQ_\Y`PNPQZY~NPV
[(vQ3gY(dpWXQ_S(S\vNblvQZYRc2Q_NPlv[[(v\,V[(vdSkQ3d,{[(\,NfV7Q_e`P\[kQZY
qd,Y.-
x*[(v7Q[kd,[(\,`
^aeNPS(WXdTV[kQ_euY(QZcs\Ye/NfS|e7QXijV7Q_eu\,Ss7X1u GIU   u  /  lNovp   V K\INUIU q
W @v7QNPV7ijVNb[kQXav7d,YNbZdV/ZtD s7XX,QR   Nf- S9[(v7Q`PNPmRNP[;d,O^\,`P`hYQZc&\YehSd,{( [(\,NPVQ_e~c@NP[(vNPV
S([kQZghS@eNPNPeQ_e~{o;x7O!d,Y|4l,dNfV7lE[kd~NPVpiVNb[o} 7X&L
 GIU   `PNfm/   7Xh GkI IU q
qfi"v+6+vQ6D(*Nr"*/L"mQ*D(*N9L6O*
W

9

@v7Q 9  Z p  ,/7Z!Z R  kp   Od,Y&
NPS[(v7QQXpgQ_WX[kQ_eS(Thmd,O
YQZc&\YehS>Y(Q_WXQ_Nb,Q_e$eT7YNPVl&[(vQ)ihYSk[
XSk[kQZghS{oOd`P`bd
cNPV7ls[(v7Qgd`PNPWXo )xGNLqQ,qbxZ7Z! 
 




fiC

vh73Qh{.L8$>${3


`bQZ[
 {Q\,Vogd`PNPWXon[ogQ,x*Q_NP[(v7QZY
9G,fih~G\,` ^Gd,O<G TVhe7QZY[(v7Q
mEQZ[kYNfW@WvdSkQ_Vj2NPS2[(v7Qm~\GpNPm~\,`gQZY(Od,YmR\,VWXQd,O\,Vo~gd`fNPWXo3d,O^[ogQ[4Od,Y<GxpNqQ,q\,` >G 
mR\G o
7Z!;GIUxc@v7QZY(Q
uNPS*[(vQSkQZ[|d,O;\,`P`0ngd`PNPWZNbQ_SZq
+d,YS(NPmEgh`fNPWZNb[o,xhc2Qy\,S(S(TmEQ$[(v\[[(v7Q$S(NPZQ G-pd,O9\&G
NfS@e7QZ[kQZYm~NPV7Q_e/{ou[(v7QyS(NbZQ
MQZ[R7Z!{Q3\,Vod,O[(v7Q_S(QgQZY(Od,YmR\,VWXQmEQZ[kYNPWZSZx|\,Vhe

Sk[(\[(NbdVh\Y(o,x[(NPmRQXae7QZgQ_Ve7Q_V[Zx>d,YUvNPSk[kd,Yoae7QZgQ_VeQ_V[Zqy|v7Q









	



1d,ONb[(SSk[(\[kQSkgh\,WXQ,q*4Q\,S(S(TmEQ$[(v\[@[(vQZY(Q$\Y(QyV7dmRd,Y(Q$\,WX[(NbdVhS|[(v\,VSk[(\[kQ_SZx\,Ve/[(vh\[Q_\,Wv

Sk[(\[kQy[kY\,VS(NP[(NbdVugY(d,{j\{hNP`PNb[oNPS@lNb,Q_V\,S\~{hNPV\Y(oOY\,WX[(NbdV/cNb[(vu{jNb[(S|\,Ve3Q_\,WvY(QZcs\Ye3NPS\,V
NPV[kQZl,QZYUd,O*\[$mRdSk[{hNb[(S_q|vNfSNPSV7d/Y(Q_\,`9Y(Q_Sk[kYNfWX[(NbdV>xS(NfVWXQR\,eeNPVl/TV7Y(Q_\,Wv\{h`bQ1keThmRmUop
Sk[(\[kQ_S/\,`f`bd
c@SdVQ[kdThSkQ4mEd,Y(Q4{hNb[(SOd,Yu[kY\,VS(NP[(NbdVghY(d,{h\{hNP`fNb[(NbQ_S~\,VheY(QZc&\YeSZq`PSkd7xNb[uNPS
Sk[kY\,NPlv[kOd,Y(cs\Ye[kd[kY\,VhSkO!d,Ym

GIL

Y(QZcs\YeSSTWv[(v\[9G\,`>

GILq

\,Ve/Vd,[*dV/\,`^

4QWXdVS(NPe7QZYUgY(d,{h`bQ_m

G c@Nb[(vnV7dVpaNPV[kQZl,QZYY(QZcs\YeS$[kd^Ghc@Nb[(vnNPV[kQZl,QZY
\,` ^G h IL+Od,YSkdmEQWXdVSk[(\,V[ e7QZgQ_VeNPVl@dV`boydV3GIL

\&
ff







NfVSk[(\,VWXQ_SU[(v\[U\Y(QRYQZgY(Q_SkQ_V[kQ_eNPV\/Sk[kY\,Nblv[kOd,Y(c&\Ye1cs\_o,qsa

]3[(\{h`bQ_SOd,Y[(v7Q$[kY\,VS(NP[(NbdVuOTVWX[(NbdVndV7Q

cNb[(v/S([(\[kQ_SNfS@Y(QZghY(Q_SkQ_V[kQ_e3{o/\~SkQZ[d,O;

[(\{h`bQEOd,YQ_\,Wv\,WX[(NbdVj\,Ve4\uSNPmRNP`P\Y[(\{h`bQEOd,Y[(v7QEY(QZcs\Ye4OTVWX[(NPdV\,VheO!d,Y[(v7QEd,{jSkQZY(\[(NbdV
OTVWX[(NbdV>q?4Q*\,S(S(TmEQ&[(v\[+[(v7QsVTm${QZY?d,Oj\,WX[(NbdVS)\,Ve$[(v7Q&VTm${QZYd,O{hNP[(S?V7QZQ_eQ_e$[kdSk[kd,Y(QsQ_\,Wv



[kY\,VSNb[(NbdVgY(d,{h\{hNf`PNb[od,YsY(QZcs\Yee7dQ_S*V7d,[2QX7WXQZQ_eu;xpSkdES(TWv\yY(QZghY(Q_SkQ_V[(\[(NbdVY(Q_ThNbY(Q_SfiR * 
{hNb[(S_q|vhNPSWZ\,V{QmEdehNihQ_e/[kdu\,`P`Pd
c

{hNb[(Sc@Nb[(v7dT[Wvh\,V7lNPV7l[(v7QWXdmEgh`bQX7Nb[o3Y(Q_S(T`b[(S_qfi/rtV

[(v7Q|S(\,mRQ&cs\_o,xSk[(\[(NbdV\Yo$gd`PNPWZNPQ_S?WZ\,VE{Q&Q_VWXdpe7Q_eE\,S`PNPS([(S?cNb[(vyQ_V[kYNbQ_S_x\,Ve[(NPmEQXae7QZgQ_VeQ_V[

i]u4[(\{h`PQ_SZq

gd`PNPWZNbQ_S&Od,Y@v7d,YNPZdVu1\,S*

7d,Y*Q_\,Wv[ogQd,Os@xQ_\,Wv[ogQd,O^gd`PNfWXo,x\,VeQ_\,Wv[ogQd,O^gQZY(Od,YmR\,VWXQmEQZ[kYNPW[(v7Q



G,fih|jkXP NPSZx

C

z 


\u&|x?\gQZYO!d,Ym~\,VWXQRmEQZ[kYNPWu!ijVhNb[kQXav7d,YNbZdV^x^[kd,[(\,`eNPSWXdTV[kQ_e^x^d,YU\_,QZY\l,QgQZYka
Od,YmR\,VhWXQxj\,Veu\Egd`PNfWXo~[ogQ~Sk[(\[(NbdV\Yo,xh[(NPmEQXaeQZgQ_Vhe7Q_V[Zx7d,YvNPSk[kd,Y(oae7QZgQ_Ve7Q_V[x

3}

[(v7QR\,`PT7QEd,O&[(v7QR{Q_Sk[gd`PNfWXo3d,Os[(v7Q~SkgQ_WZNijQ_e[ogQ~ThVe7QZY[(v7QRlNP,Q_V1gQZY(Od,YmR\,VWXQ

mRQZ[kYNPWq

U f X

|v7Q7  /



 *jtXP

NPSZx

C \Rs@xh\gQZY(O!d,YmR\,VWXQmEQZ[kYNfWx\,Ve/\Egd`PNPWXo[ogQ,x
fiz c@v7QZ[(v7QZY[(v7QE\,`PT7Qyd,O2[(vQy{Q_Sk[gd`PNPWXoud,O&[(v7QESkgQ_WZNihQ_e[ogQRTVe7QZY[(v7QElNb,Q_VgQZY(Od,Yka
m~\,VWXQUmEQZ[kYNfWNPS*l,Y(Q_\[kQZYp
.q
z 



)C

HK

a2KK

rtVghY(QZNPdTS&c2d,Y(w)\gh\,eNPmRNP[kYNbdT|S(Nb[(S(NPw`PNfSZxh_,,x_,]hTVehv7Q_V7wx7d`feS(mRNb[(v^x`P`bQ_Vpa

+(././.p];\,e\,VN)QZ[\,`qbx9_,,x)Nb[cs\,S

e7QZY_x_,];TVevQ_V7wjx?d`PeS(mRNP[(v>x>MTSkQ_Vh\px`f`bQ_Ve7QZY_x

S(v7dc@V[(v\[*[(v7Qgd`PNfWXoEQXpNPS([kQ_VWXQgY(d,{h`PQ_mNfS*WXdmEghT7[(\[(NbdVh\,`P`bo~NPV[kY\,WX[(\{h`PQO!d,Y|mEdS([*G\YNf\[(NbdVS
d,O&9S_xhd,Y@QZ,Q_VTVeQ_WZNPe\{h`bQOd,YSkdmRQ$NPVpiVNb[kQXav7d,YNPZdVuWZ\,SkQ_SZq|7d,YQXp\,mRgh`bQ,xhc&Q$Sv7d
c&Q_e
[(v\[[(v7QSk[(\[(NbdV\Yogd`PNPWXo4QX7NPSk[kQ_VWXQgY(d,{j`bQ_mRS$O!d,YE&9Syc@NP[(vd,YEc@NP[(v7dT7[$VQZl\[(Nb,QuY(QXa
cs\YeS\Y(QR;aWXdmEgj`bQZ[kQ,q&dmEghT[(NPV7l\,V4d,g[(NPmR\,`)gd`PNfWXo3NPS\[U`bQ_\,Sk[\,SUv\Ye4\,Se7Q_WZNfeNPV7l[(v7Q
QX7NPSk[kQ_VWXQgY(d,{h`bQ_m/qrtVSk[kQ_\,ed,O|\,SkwpNPV7lO!d,Y\,Vd,gh[(NPmR\,`gd`PNfWXo,xc&QmRNPlv[cNPS(v1[kdWXdmEghT7[kQ\
gd`PNPWXo4[(v\[ENPS$lT\Y\,V[kQZQ_e[kd4v\_,Q/\\,`PT7Q[(v\[ENPSy\[`PQ_\,Sk[E\`P\Yl,QOY\,WX[(NbdVnd,O|[(vQd,gh[(NPmR\,`
\,`PT7Q,q



fi 38?>v{v8.^j

$33$3

WXdmEghT7[(NfV7l4S(TWv\4V7Q_\Y`bod,g[(NPmR\,`sgd`PNPWXoNPSEWZ\,`P`bQ_e\,Vw_
a

x?c@v7QZY(QRNPVheNPWZ\[kQ_S[(v7QET\,`PNb[o3d,O&[(v7QE\ggY(d
pNPm~\[(NbdV4NPV[(v7Q
Od`P`bd
cNPV7lc&\
o,q9M^QZ[
{Qs\gd`PoV7dm~NP\,`aL[(NPmEQ&\,`bl,d,YNb[(vhmc@vNPWv$Od,YQZ,QZY(oEsG
WXdmEgjT7[kQ_S
\,V^^aLgd`PNPWXo Gq|d,[(NPWXQy[(v\[@7X)GI 
G k G\,` 
G |O!d,YQZ,QZY(oX
G q||v7QU\,`bl,d,YNb[(vm
NPS*WZ\,`P`PQ_e/\,Vkt  R
 ,  NbOOd,Y|QZ,QZY(ousG x
G\,` 
G 
 X)GI y
G k FO
G\,` ^
G 
gd`boVdmRNP\,`aL[(NPmRQ\,`bl,d,YNP[(vm

y.

\ggYd_7NPmR\[(NbdVOd,Y







!

"

%

#

$

%

!

%
%

% 
%

#

%

%

LQZQ,x?Q,ql7qbx+)\gh\,ehNPmRNb[kYNPdT>x;_,G3Od,YmEd,Y(QRe7QZ[(\,NP`PQ_e4e7QXijVNb[(NPdVSZqfi@gghY(d_7NPmR\{hNf`PNb[o/ehNPSk[(NPVpa

`Za\ggYd_7NPmR\{h`bQOd,Y@\,`P`1x7Od,Y
WXQZY([(\,NPVOxd,Y9Od,Y9V7dyTV`bQ_SS;  &qd,[kQ@[(v\[9[(vNPSe7QXiVNb[(NbdVyd,O0Z
 a\gghY(d_7NPmR\[(NbdVRY(Q_ThNbY(Q_S
[(v\[G\,` G9.pqrO\gd`fNPWXoc@Nb[(v/gdS(NP[(Nb,QUgQZY(Od,YmR\,VWXQUQXpNPS([(SZxj[(vh\,VQZ,QZYo3\ggY(d
pNfmR\[(NbdV
\,`bl,d,YNP[(vm
opNbQ_`PehS|S(TWv\~gd`PNPWXo,x{Q_WZ\,ThSkQ$\gd`PNPWXoc@Nb[(v3gQZYO!d,Ym~\,VWXQ
. d,YS(mR\,`P`bQZYWZ\,VV7d,[\g7a
lTNPSv7Q_SsaWXdmEgh`bQZ[kQghY(d,{h`bQ_mRS_}+|vQZY(Q\Y(QgY(d,{h`PQ_mRS&c@vNPWv\YQ
#

gY(d
pNfmR\[kQ\Ugd`fNPWXo$cNb[(vRgdS(Nb[(NP,Q|gQZY(O!d,YmR\,VWXQ,q)@Q_VWXQ,x7\,VoR\ggY(d
pNfmR\[(NbdV~S([kY\,Nblv[kO!d,Y(cs\Ye`Po
Skd`b,Q_S@[(v7Qe7Q_WZNPSNbdVgY(d,{h`PQ_muq

iZa\ghgY(d_7NPmR\[(NPdVO!d,Y\Y({jNb[kY\Y(oX .pq$rO9[(vQZY(QRNPS\
wG \,Ve dT7[kghT7[(Ss\,VZa\ghgY(d_7NPmR\[(NPdVd,O
[(v7Q\,`PT7Q,xjNPVu[(NPmEQgd`bopV7dmRNf\,`>NPV[(v7QUS(NPZQd,OG
[(vQ_V/c&QUS(\_ou[(vQgY(d,{h`PQ_mv\,S@\@G(  
 R sjk  R,  
 p~  
 qprO[(v7Q\,`Pl,d,YNb[(vmYTVhS&NfV[(NfmEQgd`PoV7dm~NP\,`jNfV[(vQS(NbZQ
d,OG
\,Vhe ( x[(v7QUSWv7Q_mEQ$NPS*\ pfiE|j(  fi
  &jtD  E     p ~   )  q`P`
 R
V4\ggY(d
pNfmR\[(NbdV4S(Wv7Q_mEQEopNbQ_`PeS\,V

gd`bopV7dmRNP\,`aL[(NfmEQ@\,`bl,d,YNP[(vm
*)

(

'&

[(vh\[&dVNPVghT7[2s

,+

( )-+

0/

.

d,O[(v7Q9+pSWXdVSk[kYThWX[kQ_e

(

1)

2+

/

( )-+

v7QZYQ\Y(Q+2?pSZ]c&QSk[(\[kQ[(v7Q3[(vQZd,Y(Q_mRSNPV[kQZYm~S~d,O2?pS

{Q_WZ\,TSkQ[(v\[*lNP,Q_S|Sk[kY(dV7l,QZY*YQ_S(T`b[(S*NPVSkdmRQWZ\,SkQ_SZxh\,Ve{Q_WZ\,TS(Qc&Qe7dRV7d,[sQXpgh`PNPWZNP[(`bo~\,V\,`boZQ
[(v7QUWXdmRgh`bQX7Nb[oNPV[kQZYmRS|d,O
.

( q

rO)[(v7QZY(Q$NfS|\Rgd`bopV7dmRNP\,`baL[(NPmEQ\,`bl,d,YNb[(vm

G   \,` >Gk*c@NP[(v
t  R,    fi
5

d,O

5

5

 

[(v\[@dT[kghT7[(S@\,V\ggY(d
pNfmR\[(NbdV>x43jxh[kd[(v7Q\,`PT7Q

bx[(v7Q_V3c2QUS\_ou[(v\[[(v7QgY(d,{j`bQ_mv\,S@\Ot    
rtV[(vQWXdV[kQX[d,O@s2SZx)QXpNPS([kQ_VWXQd,O\
 a\,eeNP[(Nb,Q\gghY(d_7NPmR\[(NbdV\,`bl,d,YNb[(vhm \,Ven\


63

5

9+\Y(QEd,O[kQ_V4Q_ThNbG\,`PQ_V[ZqE|vNfSm~Nblv[S(QZQ_m

S(T7Y(ghYNPS(NPVl~[kduY(Q_\,e7QZYSc@v7d/\Y(QRmRd,Y(QEO\,m~NP`PNP\Y

c@Nb[(vYQZc&\YeWXYNb[kQZYNP\n[(v\[v\_,QiQ_eTggQZY/\,Ve`bd
c&QZY3{dTVheSdV[(v7Q1gQZY(Od,YmR\,VWXQd,OE\
Skd`PT[(NbdV>xOd,YEQXp\,mRgh`bQ,x[(v7QughY(d,{h\{hNP`fNb[o4d,OY(Q_\,WvhNPV7l4\l,d\,`|Sk[(\[kQ,qrtV[(v7Q_SkQ/WZ\,SkQ_SZx&[(v7QipQ_e
{dTVeSdVgQZY(O!d,YmR\,VWXQuc@NP`f`slNb,QeNbQZY(Q_V[EY(Q_S(Th`b[(SZq

d
c&QZ,QZY_x|c&Q\YQ3\,ee7Y(Q_SS(NPV7l4[(v7QWZ\,SkQ

c@v7QZYQU[(v7QZY(QNPSV7d$    T7ggQZY@{dTVheudV3[(vQ$gQZY(Od,YmR\,VWXQ$d,O;gd`PNPWZNbQ_SZxhQZ,Q_V[(v7dT7lv[(v7QZY(Q
\Y(QUWXdmRghT7[(\{h`bQTggQZY&{dTVheS&dVu[(v7QgQZY(Od,YmR\,VWXQd,O)\Egd`PNPWXoO!d,Y@Q_\,Wv    L  

D

8 x~3y{  ^9    zjht,  kZ9k  j *,u(k  XjL,  ,  X
S
  X u L,L3L,L fZ ph  j  D  X  L,L !X ph(n(X9,t~~Z
 ZLs ypX(L !   ;t    ,Rt  R,    pX2  ,ZZQ    p,fij(  9  ~
p_pXypX( ! p,    y7Z!ZSR  ,((X$p /
: y{~O~q7 |vQ[(v7QZd,Y(Q_m O!d`P`Pd
c@S@O!Y(dm [c&d~O\,WX[(SZ}t
slNb,Q_V\~sG c@Nb[(vu\,`PT7Q xjc2QyWZ\,V

WXdVSk[kYTWX[\,V7d,[(v7QZY9s\G
cNb[(vG\,`fT7Q
 TSk[;{o$myT`b[(Nbgj`boNfV7l\,`P`Y(QZcs\YeS9NPVy[(v7Q*O!d,YmEQZY
(
&{o 7
 ]> 9ThVe7QZY&[(v7Q_SkQY(QZc&\YemRQZ[kYNPWZS2c2QWZ\,VijVe\y`Pd
c&QZY&{dTVheRdV NPO^Nb[2NPS&V7d,[<.pq
87

/

,9

;:

5

5

5

|vQ|WXdmEghT7[(\[(NbdVRd,Oj[(v7Q`bd
c&QZY{dTVe>x=<xdVR[(v7Q*G\,`fT7Q*d,O

5

eQZgQ_VheS)dVE[(v7Q*Y(QZcs\YeRmRQZ[kYNPWq

&Q_WZ\,TSkQ*[(v7QZYQs\YQ|V7dV7QZl\[(Nb,Q*Y(QZcs\YeSZxNfVUd,YeQZY;O!d,Y)[(v7QsQXpgQ_WX[kQ_eRY(QZc&\Ye$[kd{Q&gdS(Nb[(Nb,QsNPVy[(v7Q
ijVNP[kQXav7d,YNbZdVuWZ\,SkQ,x\,V/\,WX[(NbdVc@Nb[(vgdS(NP[(Nb,QY(QZcs\Ye3myTSk[*{Q[(\w,Q_Vc@Nb[(vuV7dVZQZY(dRgY(d,{j\{hNP`PNb[o

>

fi

C

vh73Qh{.L8$>${3

{o[(vQ`P\,Sk[$Sk[kQZg^q&dVSNPe7QZYdV`boY(Q_\,Wv\{h`bQSk[(\[kQ_S$d,O*[(v7Qs+x?\,Vhe`bQZ[1?{QE[(v7Q`Pd
c&Q_Sk[

 7q*|vQ_V 

V7dV7ZQZYd[kY\,VSNb[(NbdVgYd,{h\{hNP`PNP[on[kddV7Qd,O[(v7Q_SkQSk[(\[kQ_SZx"@

[(v7Qv7d,YNbZdV>x*\,VeffA[(v7QS(m~\,`P`bQ_Sk[

V7dV7ZQZYd~Y(QZcs\Ye>x\,Ve3SkQZ[*<  ? A
B?
NfS@\~`Pd
c&QZY{dTVe/dV/[(v7Q$gY(d,{h\{hNf`PNb[od,O\,WX[(T\,`P`bo
Y(Q_\,WvNPV7l3\,Vogh\Y([(NPWZTh`P\YSk[(\[kQ\O[kQZYC@Sk[kQZghSNbO&[(vNPSgY(d,{j\{hNP`PNb[o3NPSVdV7ZQZY(dx)NPV4gh\Y([(NfWZT`P\Y\
Sk[(\[kQ*c@NP[(v$YQZc&\YeDAq?rO[(v7QsY(QZcs\YeEmEQZ[kYNfW2NPS)eNfS(WXdTV[kQ_e>x[(v7Q_VE`bQZ[E<


GFH?h

NPSs[(v7Q$eNPS(WXdTV[*O\,WX[kd,Y_q
d
cWXdVS(NPeQZYR[(v7Q3NPV7ijVNb[kQXav7d,YNbZdVTVe7QZYR\Sk[(\[(NbdV\Yongd`PNfWXo,q
gY(dpWXQ_S(SZx\,Ve

 x,c@v7QZY(Q Z  .eI_Sf
A

F

|vNfSENPVeTWXQ_S~\1\Yw,d


[(v7Qgd`fNPWXov\,SV7dV7ZQZY(dY(QZcs\YeNbO[(v7QZYQNfS\nV7dV7ZQZYdnghY(d,{h\{hNP`fNb[ogh\[(v

[kd\

Y(QZcs\YeV7deQ,xNqQ,qbxp\ySk[(\[kQO!Y(dmc@vNfWv[(v7QZY(QNfS2\$gdS(Nb[(Nb,QXaLY(QZcs\Ye\,WX[(NbdVgdSS(Nb{h`bQ,q)|vNfS9NPS9[kYT7Q



NbO;\,VedV`bouNbO)[(v7QZY(QyNPS\~V7dV7ZQZYdaLgY(d,{h\{hNf`PNb[o  jPygh\[(vNfS(Nb[(NPV7l~Q_\,WvV7dpe7QU\[mEdSk[dVWXQ
[kd~\EYQZc&\Ye3V7deQ,q2pTWv3\gh\[(v3\,WZWXYTQ_S*Y(QZcs\Ye/\[`PQ_\,Sk[I< 
GFH?h J AEOd,Y@Sk[(\[(NbdV\Yogd`PNPWZNPQ_SZq

 q

pNfVWXQ;Sk[(\[(NbdV\Y(ogd`PNPWZNPQ_Svh\_,Q\,`PT7Q_S>{dTVhe7Q_e{o[(v7Q[(NPmRQXae7QZgQ_Ve7Q_V[\,VhevNPSk[kd,Yoae7QZgQ_Ve7a
Q_V[U\,`PT7Q_SOd,Y$NPV7ijVNb[kQXav7d,YNbZdV4&9SZx+[(vNPSU`bd
c&QZY${dTVheO!d,Y$[(v7Q~Sk[(\[(NbdVh\Y(o4G\,`fT7QEd,O*[(v7Q
&

NPS*\,`PSkdR\R`Pd
c&QZY@{dTVeOd,Y|d,[(v7QZY@gd`PNfWZNbQ_SZq

V7dVpaV7QZl\[(Nb,QY(QZcs\YeSNPS p
. x*[(v7Q_V \
a\,eeNP[(Nb,Q\ghgY(d_7NPmR\[(NPdVnWZ\,VhV7d,[Y(QZ[(T7YVn\gdSNb[(Nb,Q~\,`PT7Q,q1?d4e7QZ[kQZYmRNfV7Q~c@vQZ[(v7QZY[(v7QZY(QuNPS
\ugd`PNPWXoc@Nb[(vY(QZcs\Ye4l,YQ_\[kQZYU[(v\,Vi/
. O!d,YU\/lNb,Q_Vs+xWXdmEgjT7[kQ \,Ve4[(v7Q_VSkQZ[yu S(TWv
G q*|v7Q
[(v\[[ 9~ p
. xNqQ,qbx&  xj\,Vhe3YThVu[(v7Qs a\,eheNb[(Nb,Qy\ggY(d
pNPm~\[(NbdV3\,`bl,d,YNb[(vmdV^\
)NPV\,`f`bo,x&Vd,[kQ[(v\[NbO[(v7Q3\,`PT7Q3d,O\&c@Nb[(v

=<

K&

&

'&

0<

L

v\,S*gdS(Nb[(Nb,Q\,`PT7QNbO+\,VeudV`PoNbO+[(v7Q\ggYd_7NPmR\[(NbdVuYQZ[(T7YVS*\EgdS(NP[(Nb,QG\,`PTQ,q

M

d,[kQ|[(v\[;[(vNfS;e7dQ_S;Vd,[WXdV[kY\,ehNPWX[[(v7Q*TVeQ_WZNPe\{hNP`fNb[oY(Q_ST`b[+d,O\,e\,VNpQZ[9\,`Lq7t_,,q2|v7Q
gY(d,{j`bQ_m[(v\[*[(v7QZogY(d,Q_e/TVeQ_WZNPe\{h`bQNPSsc@vQZ[(v7QZY|\R&cNb[(vuV7dV7gdS(Nb[(NP,QYQZc&\YehS*v\,S

.

.

\vNPSk[kd,Yoae7QZgQ_VeQ_V[d,Y[(NfmEQXae7QZgQ_Ve7Q_V[\,`PT7Qyd,O pq4Q,YQR\,SkwNfV7l~c@vQZ[(v7QZYNb[vh\,S\,`PT7Q'&

NPV[(v7QV7dVpaV7QZl\[(Nb,QY(QZcs\YeWZ\,SkQ,]^\,VS(c2QZYNfV7l~[(vNfST7Q_Sk[(NbdVQZ,Q_V1NbO)c&QEm$T`P[(Nbgh`bo[(v7QyY(QZc&\YeS

U
*e7dQ_S@V7d,[@\,VSkc&QZY|[(v7Q_NPY|T7Q_S([(NbdV>q

{o

3 2 ^9     s2(k  XjL,  ,  j" ,  $(X9k L p s
3fihyjtXP
pjZ9  Ztp u ,   D  X   L,L,* fZ ph1(X9k f 
    Ejk  RXP  $jhfi  7X(EU !   E
yZy,;93Gh|jkXP;
w

~vy~$y67

/

-9

( )-+



'qPq
ya\,eeNb[(NP,Q@\ggY(d
pNfmR\[(NbdV

d,[kQ*[(v\[+[(v7Q*WXd,Y(d`P`P\Y(oeQZgQ_VheSdVh`bodV\,WX[(S@t
?\,Veu O!Y(dm[(v7Q&gY(dd,O7d,Oj@v7QZd,Y(Q_m
|vTSZx\,VoEd,g[(NPm~Nb_\[(NbdVEgYd,{h`bQ_mc@Nb[(vR[(v7dSkQ@ghY(d,gQZY([(NbQ_S)c@NP`f`vh\_,Q\
NbO+\,VeudV`boNbO+Nb[|v\,S|\R2?q

:

q7

G

y
.


dT7[kghT[(S6.pq[(v7QZY(c@NfSkQ,x
lNb,Q_VxNb[Wv7ddS(Q_S|STWvU[(v\[
 x\,Ve[(vTS+ [ ztr
v7d`PehSZq;M^QZ[
y


r


G


@
q




,
d
k
[

Q
(
[

v

\
[




G
*

P
N
&
S
(
[
7
v

Q

\

g

g
(
Y


d
p

f
N
R
m

\
[(NbdV[kdE[(v7Q\,`PT7Qd,OG
OdTVe{o


YTVhVNPV7lE\,`bl,d,YNP[(vm
$qfi|v7Q_V NfS|\,V_a\ggY(d
pNfmR\[(NbdV/[kd
xhS(d
NPS@\,V^Za\gghY(d_7NPmR\[(NbdV3[kd

y{~O~
M^QZ[ 5 
\,`  x^\,Vhe`PQZ[	{Q\gd`PoV7dm~NP\,`aL[(NPmEQ a\,eheNb[(Nb,Q\ggY(d
pNfmR\[(NbdV\,`bl,da
YNb[(vhmuq&)NbYSk[Zxh[(v7Q$9+3WXdmEghT7[kQ_S	<E\,SNPV/|v7QZd,YQ_m
qP$\,Ve3Wv7Q_W(wpSc@v7QZ[(v7QZY 5  pq*rO;Skd7xNb[

'

L

D&

3



$

5

.

P

5

.ON

0&

5

Q

K3

5

q

pTggdSkQ,xNPVhSk[kQ_\,e>x[(v\[uc2Q1vh\_,Q\2?

O!d,Yud,gh[(NPmR\,`gd`PNPWZNbQ_SOd,Y/[(vNPSgY(d,{j`bQ_muqMQZ[

GIU{Q\,Vn\,`Pl,d,YNb[(vm [(v\[e7Q_mEdVSk[kY\[kQ_S$[(vhNPSZqMQZ[  G\,` ^Gx\,Ve yGIU.e   jq
  - qrO  ./[( v7Q_V   .\,Ve1 c&QWZ\,Vn
Sk[kd,g^qu2`PSkQRc&QWv7ddS(Q\,VSTWv[(v\[

tG
 ix7lNbpNPV7l q&pNPVhWXQ -  x7\,Ve -  NPSsgd`boVdmRNP\,`S(NbZQ\,VeuNPS&gd`bopV7dmRNP\,`a
[(NPmEQWXdmEghT7[(\{h`PQRNPV G-
 S(NPVhWXQ NfS[(v7QRdT7[kgjT7[d,O yGIU.e kx;c&QWZ\,VWv7ddSkQOX -   x)\,Ve
YTV GIUG
 q2@vNPSslNb,Q_S\; a\,eeNb[(Nb,Q\ghgY(d_7NPmR\[(NPdV>q


5

|vTS

N

5

S3

5

5



R

3

;3

5

T



N

N

D3





M

VU

fi 38?>v{v8.^j



$33$3

Za\ggYd_7NPmR\{h`bQOd,Y3SkdmEQWZ\,VVd,[3v\_,Q\9+q||v7QZY(QZOd,Y(Q,x

gY(d,{j`bQ_m

[(v\[3NPSuV7d,[

\,Vo3m$Th`b[(Nbgh`PNfWZ\[(Nb,QV7dV\gghY(d_7NPmR\{hNf`PNb[oY(Q_S(T`P[@opNbQ_`PeS|\,V\,eehNb[(Nb,QUV7dVh\ggY(d
pNPm~\{hNP`PNb[oYQ_S(T`b[Zq
d
c&QZ,QZY_x\,VE\,eheNb[(Nb,QsV7dV\gghY(d_7NPmR\{hNf`PNb[oY(Q_ST`b[+dV`boUS(vd
c@S)[(v\[;[(vQZY(Q*NPS;V7d9+x\,`P[(v7dT7lv

Za\ggYd_7NPmR\[(NbdVuOd,Y@SkdmRQipQ_eq

[(v7QZY(Q$mRNblv[*{Q\,V

H F[Z \oH]K^

WCYX

a&KKff_Ha`K F K

?

HKfe7H Fhg

bZdc

>

iKj

,g



|vNfSSkQ_WX[(NbdVO!dpWZTSkQ_SUdV4ijVhNb[kQXav7d,YNbZdV1gd`fNPWZNbQ_SZq&Q_WZ\,TSkQ[(v\[$NPS$WXdVS(NPS([kQ_V[$[(v7Y(dT7lv7dT[[(v7Q
SkQ_WX[(NbdV^xc&Q|e7dV7d,[QXpgh`PNPWZNP[(`bomEQ_V[(NbdVRNb[NPVQ_\,WvR[(v7QZd,YQ_muq;@dc2QZ,QZY_x7\,S2pQ_WX[(NbdV~S(vd
c@SZx[(v7QZY(Q
\Y(QUSNblVNijWZ\,V[*WXdmEghT7[(\[(NPdV\,`^eNQZY(Q_VWXQ_S|{QZ[c2QZQ_V/ijVNb[kQXa\,Ve3NPVpijVNP[kQXav7d,YNbZdVWZ\,`PWZT`f\[(NbdVSZq
|vQgd`PNPWXoQX7NPSk[kQ_VWXQgYd,{h`bQ_m

Od,Yu&9Sc@Nb[(v

V7QZl\[(Nb,Q4\,Ve

V7dVpaVQZl\[(Nb,QY(QZc&\YeS

[Za\gghY(d_7NPmR\[(NbdV^q@rO\~gd`PNPWXoc@NP[(v/gdS(Nb[(Nb,Q$gQZY(O!d,YmR\,VWXQUQX7NPSk[(S_xj[(v7Q_VQZ,QZY(o
}.
WZ\,VV7d,[R\ghgY(d_7NPmR\[kQ/\gd`PNfWXo1c@Nb[(vngdS(Nb[(Nb,QgQZY(Od,YmR\,VWXQ,qQ_VWXQ,x2[(vQ/e7Q_WZNPS(NbdVngYd,{h`bQ_m
NPS
Sk[kY\,NPlv[kOd,Y(cs\Ye`bo/Skd`P,Q_e{o\,VoX_
 a\ggY(d
pNPm~\[(NbdV>q|v7QZY(QZOd,Y(Q,xc&QWXdVhWXQ_V[kY\[kQ~dVs2S
c@Nb[(vV7dV7aV7QZl\[(Nb,QRY(QZcs\YeS_qy@Q_S(T`b[(SOd,Y&9ScNb[(v4TV7Y(Q_S([kYNPWX[kQ_eY(QZcs\YeS\Y(QRSk[(\[kQ_e\,S
WXd,Y(d`P`f\YNbQ_SZq/&dVS(Nfe7QZYU\,V_
 a\ggY(d
pNfmR\[(NbdV\,`Pl,d,YNb[(vm [(vh\[Zx+dVnNPV7gjT7[\3& G c@Nb[(v
V7dVpaVQZl\[(Nb,QY(QZcs\YeSZx7dT[kghT7[(S|\Egd`PNfWXoO 
d,O)[ogQ`2
 q9|vQ_V/Nb[|v7d`PehSs[(v\[
7XGIU  
 tG Z G\,` 
G Q
4QihYS([2WXdVS(Nfe7QZY9[(v7QT7Q_Sk[(NbdVd,O^c@v7QZ[(vQZY&\,Vd,g[(NPmR\,`Sk[(\[(NbdVh\Y(oEgd`PNPWXoEWZ\,V{QZ
 a\gghY(d_a
NPmR\[kQ_e1O!d,Yy&9ScNb[(v4V7dVpaV7QZl\[(NP,Q~Y(QZcs\YeSZqr[$NPSwpV7d
c@V MNb[k[(mR\,V^x2_,G7]9TVevQ_V7w
QZ[|\,`qbx(././
. &[(v\[s[(v7QY(Q_`f\[kQ_eue7Q_WZNPS(NbdVgY(d,{j`bQ_mNfS&;aWXdmRgh`bQZ[kQ,q4QNPVhWZ`PTe7Q\yS(w,QZ[(Wvud,O?[(v\[
NPSV7d,[STNb[kQ_eO!d,Y

\ggYd_7NPmR\[(NbdV\,`Pl,d,YNb[(vmopNbQ_`PehS>S(TWv$\*gd`PNPWXo,x{Q_WZ\,ThSkQ9\*gd`PNPWXoc@Nb[(vgQZY(Od,YmR\,VhWXQ *d,Y?S(m~\,`P`bQZY

k

gY(dd,O)v7QZY(Q,xS(NPVhWXQ`P\[kQZY|gY(dd,OS*{hTNP`fe~dV/NP[Zq9|v7QOd,YmR\,`^e7QZ[(\,Nf`PS*WZ\,V/{QOdTVeuNPVuggQ_VeNUq

8Sx~3y{   ) R 
pj(pX7ZR, 
p  L,  j,1p   U f X  

jkXP
 Z93    /j,"  ,$kZ9t $! @  (jb_
: y{~O~q7 Q_m${QZYS(vNbgUNfVUNfSSk[kY\,NPlv[kOd,Y(cs\Ye>x{Q_WZ\,TSkQs\|gd`PNPWXoWZ\,V${Q2lT7Q_S(SkQ_ey\,Ve$QZG\,`fT\[kQ_e

NPVgd`bopV7dmRNf\,`[(NfmEQ,q??dS(vd
c;avh\YeV7Q_S(S_x
c&QY(Q_eTWXQ9[(v7Q9;aWXdmEgh`PQZ[kQ2S(\[(NPSkij\{hNP`PNP[o@ghY(d,{h`bQ_m
[kdNb[ZqM^QZ[ ) ( II q
 {QSTWv\Od,YmyT`P\nc@Nb[(v \YNP\{h`PQ_S ( IDI 
 \,VeWZ`P\,ThSkQ_S
I( DI  x&cv7QZY(Q3WZ`P\,ThSkQ    +(N 0  +-S 0  +4S 0 EO!d,Y  Z  I  q4QS(\
o[(v\[
\YNP\{h`bQ
 7        e .  (  1^   NbO  Y(Q_Skgq  )NPS\U`PNb[kQZY\,`NfV q) NP[(v7dT7[
`bdS(S|d,O;l,Q_V7QZY\,`PNb[o,xhc&QU\,S(S(ThmEQ[(v\[QZ,QZY(ouG\YNP\{h`bQ\ggQ_\YS\[mEdSk[@dVWXQ$NPV/Q_\,WvWZ`P\,TSkQ,q&|v7Q
NPe7Q_\NPS[kd4WXdVS([kYTWX[$\&
G  Uv\
NPVl3dV7QSk[(\[kQO!d,YQ_\,Wv\ggQ_\Y\,VhWXQd,O|\G\YNP\{h`bQ
ml

on

Bz

v]wx4y
~

0u :=:V:

pVqVqdrts

o{

|{

~

~[

}{













~[

{

(



{



'
V{



{

|{

[{

O

~[

z

NPV\WZ`f\,TSkQ,q|v7QySkQZ[d,Od,{hSkQZY(\[(NbdVSNPS[(v7QESkQZ[d,OG\YNP\{h`bQ_SZq9\,Wv\,WX[(NbdV4WXd,Y(Y(Q_SkgdVehS|[kdu\,V

\,S(S(NPlVmEQ_V[d,O&\~\,`PT7Qy[kdu\\YNP\{j`bQ,q@v7Q$[kY\,VS(Nb[(NbdVOTVWX[(NbdVNPSe7QZ[kQZYmRNPVhNPSk[(NPWq|v7Q$gY(dpWXQ_S(S
Sk[(\Y([(S+c@NP[(vU[(v7Q2ihYS([\YNP\{h`bQ2NPVU[(vQ9ihYSk[?WZ`P\,ThSkQ,q?rO7[(v7Qs\,WX[(NbdVEWvdSkQ_VNPVU\WXQZY([(\,NPVySk[(\[kQ*S(\[(NPSkihQ_S
[(v7QWXd,Y(Y(Q_SkgdVeNfV7lu`PNb[kQZY\,`x?[(v7Q~ghY(dWXQ_SSgY(dpWXQZQ_eSU[kd[(v7QRihYSk[\YNP\{j`bQRd,Os[(v7QVQX[yWZ`P\,TSkQ,x+d,Y

NbO\,`P`+WZ`P\,TSkQ_Sc&QZY(QEWXdVS(NPe7QZYQ_e>qrO[(v7Q\,WX[(NbdV4e7dQ_SV7d,[

c@Nb[(vY(QZcs\Yey[kd\~ijV\,`+S(NPVw/Sk[(\[kQ

S(\[(NPS(O!o[(v7QR`PNb[kQZY\,`Lx>[(v7QEgY(dpWXQ_S(SgYdWXQZQ_eS[kd3[(v7QRV7QXp[G\YNf\{h`bQd,O2[(vQRWZ`P\,TSkQ,x^d,YUc@Nb[(vY(QZcs\Ye

.[kd/\uS(NPVwSk[(\[kQ

RqyS(NPV7wSk[(\[kQRc@NP`f`?V7QZ,QZYU{QE`bQZO[ZqE|v7Qgh\Y([(NP[(NbdVd,O2[(v7QRS([(\[kQ~Skgh\,WXQRNPV[kd

d,{hSkQZYG\[(NbdVWZ`P\,S(SkQ_SUlT\Y\,V[kQZQ_SU[(v\[U[(v7QRS(\,mEQ~\,SS(NblVmEQ_V[UNPSm~\,e7QROd,YQZ,QZY(o4\ggQ_\Y\,VWXQRd,O

mG >Q_T\,`PS~N

[(v7QS(\,mEQ~\YNP\{j`bQ,q@v7QZY(QZOd,Y(Q,x?[(vQ~\,`PT7QRd,O
Y(Q_eThWX[(NbdVuNPS|NPVuggQ_VeNb~Uq

z

6zNPSS\[(NPStij\{h`PQ,q@v7QROd,YmR\,`
M



fiC

vh73Qh{.L8$>${3


.pqurm~mEQ_eNP\[kQ_`boc&Q~l,QZ[y[(v7Q

d,[kQ[(vh\[y\,`P`gd`fNPWZNbQ_Sv\
,QQXpgQ_WX[kQ_eYQZc&\Yed,O*Q_Nb[(v7QZYRd,Y

*

V7dV\ghgY(d_7NPmR\{jNP`PNb[o/Y(Q_S(T`P[O!d,Yy&9SZx?QZ,Q_VNbOs\,`P`;[kY\ tQ_WX[kd,YNPQ_S$v\
,QV7dV7aV7QZl\[(Nb,Q~gQZY(Od,Yka
mR\,VWXQ,q

8Sx~3y{

32 Z
. F\   R
kt  RZb  Uj  |

ml

(X9k $!

:y{~O~q7

D

n





@v7QS([(\[(NbdV\Y(o\,`PT7Qd,OU\&WZ\,V{QWZ\,`fWZT`P\[kQ_eNPV

{hNPVh\Y(oSkQ_\YWv

"

 L,  j,@p  9Z&93     Ej,   ,
 

)

gd`PoV7dm~NP\,`*[(NPmEQ{o

TS(NPV7l4\,Vd,Y\,WZ`bQ3Od,YR[(v7QSk[(\[(NbdV\Yongd`PNfWXoQX7NPSk[kQ_VWXQgY(d,{h`bQ_m

\

Od,Ys2SZq

G }$

|v7QVTmU{QZYsd,O^{hNP[(S2[kd{QWZ\,`PWZT`P\[kQ_euNPS&gd`bopV7dmRNP\,`jNPV[(v7QS(NbZQd,O q V7d
cNPV7ly[(v7QG\,`PTQ,xc&Q
WZ\,V[kY(o[kd$ih\,Vu\,WX[(NbdVOd,Y|\,Vd,{jSkQZY(\[(NbdV>q;rO^[(v7QmEdpeNihQ_e&Sk[(NP`f`j\,WvNbQZ,Q_S*[(v7Q\,`PT7Q
WZ\,`PWZT`f\[kQ_e1{QZO!d,YQ,x?c&QWZ\,VWXdV[(NPVT7Qc@Nb[(v4[(v7QV7QX[$d,{hSkQZY(\[(NbdV>x)TV[(NP`;\3S([(\[(NbdV\Y(ogd`PNPWXoNPS
OdTVeRcvNPWvv\,S2[(v7Qd,g[(NPmR\,`hgQZYO!d,Ym~\,VWXQ,q|vNPS9\,`bl,d,YNb[(vmYTVhS9NPV~gd`PoV7dm~NP\,`7[(NPmEQc@Nb[(v\,V
d,Y\,WZ`bQySkd`bpNPV7lR[(vQ$Sk[(\[(NbdVh\Y(ougd`PNPWXoQXpNPS([kQ_VWXQUgYd,{h`bQ_mOd,Ys2SZqpNPVhWXQ[(v7QUd,Y\,WZ`bQ$NPSNPV
?xh{o|v7QZd,Y(Q_m7qPxh[(v7QU\,`Pl,d,YNb[(vmYThVS*NPVgd`bopV7dmRNP\,`[(NPmEQNPO?

 @q
Za\gghY(d_7NPmR\[kQ_S$[(v7Q~d,gh[(NPmR\,`

[(v\[s
q4Q~S(vd
c[(v\[[(vhNPSNPmEgh`PNPQ_S@[(v\[  {o
S(v7dc@NPV7lv7dc[kd1Skd`b,Q[(v7QuaWXdmEgh`bQZ[kQughY(d,{h`bQ_m
qSENPV[(vQugY(dd,O|d,O|vQZd,Y(Q_m
7qPx
lNb,Q_V\,VRNPVSk[(\,VhWXQ d,O
xc2Q@WXdVhSk[kYTWX[9\U&
G  >q|v7Q*dV`boWvh\,V7l,Q@[kd[(vQ|Y(QZcs\Ye
d
cx;\,S(STmEQR[(v\[CNPS\3gd`PoV7dm~NP\,`aL[(NPmEQE\,`bl,d,YNb[(vm

;Rc@NP[(v^.

Sk[(\[(NbdVh\Y(ogd`PNfWXo/O!d,YS(dmEQ

B

v]wx4y

"z

z

v]wx4y

OTVWX[(NbdV3d,O;[(v7Q$sWXdVS([kYTWX[kQ_eNPV/[(v7Q$gY(dd,O)d,O9|v7QZd,Y(Q_m
c@Nb[(vgdS(Nb[(Nb,QRgQZY(Od,YmR\,VhWXQ_SZq3d
cY(QZcs\Ye

7qPNPS|[kdmR\w,QNb[@\&

NPSUd,{[(\,NPV7Q_enNbO*Sk[(\[kQ

NfSUY(Q_\,Wv7Q_e>x\,VeY(QZcs\Ye

yNPSsY(Q_\,WvQ_e>q9Q_VWXQ NPS*S(\[(NPSkij\{h`bQNbO+\,VedV`boNbOG >sv\,Ss\,`PT7Q
q
( 
S(S(TmRQ;[(v\[^gd`PNPWXoENPS[(v7QdT7[kgjT7[>d,Op[(v7Q}Z
 a\gghY(d_7NPmR\[(NbdVU\,`bl,d,YNb[(vm Uq?rO yNPS^S(\[(NPSkij\{h`bQ,x
[(v7Q_V7Z!;G QIUyt~
 } (  -   qE&Q_WZ\,TSkQE[(vQEgQZYO!d,Ym~\,VWXQd,O9QZ,QZY(ogd`PNPWXo/Od,Y
G ENPSyQ_Nb[(v7QZY3uNbO NfSEV7d,[RS(\[(NPSti\{h`bQ,xd,Y (  - NbO NPSES\[(NPStij\{h`PQ,x9Nb[O!d`f`bd
c@S[(vh\[;v\,S
gQZY(Od,YmR\,VWXQ ~NbOs\,VedV`bo4NbO NPSS\[(NPStij\{h`PQ,q/pd7x+NPVd,Ye7QZY$[kde7Q_WZNPeQ
x+c2QWZ\,V
Z
WXdVSk[kYTWX[JG xjYTV[(v7Q$\ggY(d
pNPm~\[(NbdV/\,`bl,d,YNP[(vm
dV3Nb[Zx7[(\w,QyNb[(S|dT7[kgjT7[m4\,Ve3WZ\,`fWZT`P\[kQ
7Z!;
G  >QIU qy|v\[dT7[kghT7[S(v7dc@Sc@v7QZ[(v7QZY NPSNPV
q`P`?[(v7Q_SkQESk[kQZghS\Y(Qgd`bopV7dmRNP\,`a




(

-

NPSsd,{[(\,NPV7Q_e3NbOSk[(\[kQ

.

1z

z

.

h

z

z

.

	z

&

tz

K&



"z

.

Pz

z

0z

v]wx4y



z

z

[(NPmEQ{dTVhe7Q_eWXdmEghT7[(\[(NbdVhSZqr[*O!d`P`Pd
c@S*[(vh\[

v]wx4y

v]wx4y

NPS|NPVu@x\,Ve/v7Q_VWXQ



|q

M

OWXdT7YSkQ,x&[(v7Q/S(\,mRQ/V7dV\ggYd_7NPmR\{hNP`fNb[oY(Q_S(Th`b[Ev7d`PeSyOd,Y~&9Sc@NP[(vgdSNb[(Nb,Qu\,Ve
V7QZl\[(Nb,Q$Y(QZcs\YeSZq

w



~vy~$yl

RXP

.

_

7
n

 Uju

X\


)



 |




 R  L,  jp  $Zy93 !

kjk  

S(NPV7l[(v7QS\,mEQghY(dd,Os[kQ_WvVNPT7Q\,Sy\{d
,Q,xc&QWZ\,VS(vd
c[(v\[$[(vQ~\,`PT7QNfSUV7dV\ghgY(d_7Na
mR\{h`PQ,x7[kdd7q

w

~vy~$yl


 @




l





n

6. Fy\ p

_

D

(

 L  Gh9X93 f

kt  RZb

 

S(NfmRNP`P\YU\YlTmEQ_V[yWZ\,V{QTSkQ_e[kdSv7d
c[(v\[\/gd`PNPWXoc@Nb[(vgQZY(O!d,YmR\,VWXQ~\[`bQ_\,Sk[y[(v7Q
\_,QZY\l,Qyd,O;\,`P`^gQZYO!d,Ym~\,VWXQ_S*Od,Y@\~s


WZ\,VV7d,[{QWXdmRghT7[kQ_e3NfVgd`PoV7dm~NP\,`[(NPmEQ,xhThV`bQ_S(S

@q)d,[kQ@[(vh\[NPV[(v7Q|gYdd,Ojd,O|vQZd,Y(Q_m7qPx[(v7Q*dV`bo$gQZYO!d,Ym~\,VWXQ*l,Y(Q_\[kQZY9[(vh\,VEd,YQ_T\,`


[kdR[(v7Q$\_,QZY\l,Qyd,O+\,`f`gQZYO!d,Ym~\,VWXQ_S|NPSs[(v\[@d,O)\,V/d,g[(NPmR\,`>gd`PNfWXo,q



fi 38?>v{v8.^j

w

 p2Z!b  fi/(E _  bZh"
 7X(4U !   /p,fij(    ~1  r p,&X  ,X93%G  (j7  
  7Z!ZSE,  9,((,Zp ZLp,Xk
 L,  jp  X  G
L

,




j





$

7

X



X

,

Q


R



R

t

9


G




  9

~vy~$yl
p

$33$3

!

(

#

(

!

u

|vTSZxQZ,Q_VRWZ\,`PWZTh`P\[(NPV7l\gd`PNPWXoc@v7dS(Q&gQZY(Od,YmR\,VhWXQ&NPS)\{d
,Q\_,QZY\l,Q@NfS+`fNbw,Q_`bo[kd{QsNPV7OQ_\,S(Nb{h`bQ,q
4QV7dc[(T7YV[kd[(NPmEQXae7QZgQ_VeQ_V[&gd`PNPWZNbQ_SZq)|v7Q[(NPmEQXaeQZgQ_Vhe7Q_V[&gd`PNPWXoEQX7NPSk[kQ_VWXQghY(d,{h`bQ_m
Od,Y@&9SNPSswVd
c@Vu[kd~{Q;aWXdmRgh`bQZ[kQ,xh\,SNPSs[(v7QUSk[(\[(NPdV\Y(odV7Q,q

8Sx~3y{

(



 

ml 
 pj pX71Z u
pj  XGXP3 ! |
    @b_

/



:V:V:

(



p/  ~Zt7XXhsp  nU f X

)\gh\,eNfmRNb[kYNbdT\,Ve@S(Nb[(S(Nbwp`PNPSRt_,,$ghY(d
,Q_e\[(v7QZd,Y(Q_m



 $jkXP Z

S(NPmRNf`P\Y[kd|v7QZd,Y(Q_m

7q q4|v7Q_NbY

2S$vh\,edV`PoVdVpaLgdS(Nb[(Nb,QRYQZc&\YehSZx+\,Vhe1[(v7Q_NbY$O!d,Ym$T`f\[(NbdV1d,O*[(v7Qe7Q_WZNPSNbdV4gY(d,{j`bQ_m
c@v7QZ[(vQZYU[(v7QZY(QNPS\3gd`fNPWXocNb[(v4gQZY(Od,YmR\,VWXQ
[(v7Q_NbYSZx9TSkQ_S~\Y(Q_eTWX[(NbdVnOY(dm

v]wx4y

cs\,S

.pq/@v7QRgY(dd,Os{o4TVevQ_V7wQZ[y\,`qbx}(././.px`PNbw,Q

q4Q3mEdpeNbOo1[(vNPSyY(Q_ehTWX[(NbdV[kd1S(v7dc[(v\[R\,Vd,gh[(NPmR\,`

[(NPmEQXaeQZgQ_Vhe7Q_V[sgd`PNPWXoNPS|v\Ye[kd~\ggY(d
pNfmR\[kQQZ,Q_V/Od,Y@TVd,{hSkQZY(\{h`bQ2SZq

8Sx~3y{  Z.  \ /  R;  ~Zt7XXh)p  Xpj  ZXPU 
  
 ",  ,$kZ9k y! kjk  RZb  U,ju  @  
:y{~O~q7 4Q/lNb,Q\Y(Q_eTWX[(NbdVOY(dm
c@NP[(v[(vQ/O!d`f`bd
c@NfV7lghY(d,gQZY([(NbQ_SZq7d,Y\1O!d,YmyT`P\
c@Nb[(v
WZ`P\,TS(Q_S2c&QS(v7dcv7d
c[kdRWXdVhSk[kYTWX[*\,VuTV7d,{jSkQZY(\{h`bQFG
 >2c@Nb[(v\,`PT7QNbO NPS
S(\[(NPSkij\{h`bQ,x\,Vec@Nb[(vG\,`fT7QO
t9bG
 NbO NPSVd,[S\[(NPStij\{h`PQ,q|vQZY(QZO!d,YQ,x\,Vj_ a\ggY(d
pNPm~\[(NbdV
ml

o

}

n

T)

,z

v]wx4y

.

z

z

\z

WXdT`Pe{Q~TSkQ_e[kdeNPS([(NPV7lTNPSv{QZ[c2QZQ_VS(\[(NPSkij\{h`bQ~\,VheTVS\[(NPStij\{h`PQyOd,YmyT`P\,S$NPV1gd`bopV7dmRNP\,`

[(NPmEQ,q
7d,YsOd,YmyT`P\1z?xc2QihYS([9S(v7dc

mG >;OY(dmc@vNPWv G  >
sG >;S(Nfm$T`f\[kQ_S;[(v7Q

v7dc[kdyWXdVS([kYTWX[2\,VTVd,{hSkQZY(\{h`bQ

z

c@NP`f`{Q|WXdVS([kYTWX[kQ_e>q@|vQ*O!d,Ym~\,`7gY(Q_SkQ_V[(\[(NbdV\ggQ_\YSNPVEggQ_VeNyqfi
Od`P`bd
cNPV7luSk[kY\[kQZl,o,qus[[(v7QRihYSk[Sk[kQZgxdV7QRd,Os[(v7Q
c@Nb[(vghY(d,{h\{hNP`fNb[o



X

( q2s[Sk[kQZg



.

z

z

WZ`P\,TSkQ_S$NPSWv7dS(Q_VTVhNbO!d,YmR`bo/\[UY\,Ve7dm

_&NPS*e7QZ[kQZYmRNfV7Q_e>q9&Q_WZ\,TSkQ$[(v7Q

x[(v7QU\,SS(NblVmEQ_V[*d,O)G\YNP\{h`bQ

gY(dpWXQ_S(S*NPS*ThV7d,{hSkQZY(\{h`bQ,xpNb[*NPSslT\Y\,V[kQZQ_eu[(vh\[*Q_\,Wv/\YNP\{j`bQl,QZ[(S@[(v7QS(\,mEQ\,S(SNblVmEQ_V[|NPV\,`P`
WZ`P\,TS(Q_SZx;{Q_WZ\,TSkQNb[(SU\,`PT7QNPSye7QZ[kQZYmRNfV7Q_eNPV[(v7QS(\,mEQuSk[kQZg^qrO@\WZ`f\,TSkQNPS$S\[(NPStihQ_e{o[(vNPS
\,S(S(NPlVmEQ_V[Zx\ijV\,`>Sk[(\[kQ$c@NP`P`{QY(Q_\,WvQ_e>q9rO+Vd,[Zxh\,VuQZY(Y(d,YSk[(\[kQUc@Nf`P`h{QY(Q_\,Wv7Q_e>q

`G

yG ( IDIUG  d,O<G xS(TWv4[(v\[[(v7QRNPVhNb[(NP\,`+Sk[(\[kQ
d,O}G
 >*NPS|[(v7Q$NPVhNb[(NP\,`>Sk[(\[kQyd,OG ( xj[(v7Q$NPVhNb[(NP\,`>Sk[(\[kQ$d,O}G  ( NPS@[(v7QijVh\,`^Sk[(\[kQd,OG  x\,Ve
Y(QZcs\Yen$NPS@l\,NPV7Q_eNbO)[(v7QiV\,`Sk[(\[kQd,O}G
NPS|Y(Q_\,Wv7Q_e>q|v7QUQZYY(d,YS([(\[kQ_Sd,O2\,`P`^[(v7QsG  S\Y(Q

NPe7Q_V[(NihQ_e\,S\RTVNPT7QS(NPV7wSk[(\[kQ Rq
d
cx+WXdVSk[kYThWX[
.

.

zOY(dm



-

WXd,ghNbQ_S





z





1

?dNP`f`PTSk[kY\[kQ[(vQ$WXdVSk[kYTWX[(NbdV>xNPV/)NblT7Y(Qc&QUlNb,Qy\,V3QX7\,mEgh`PQU&WXdVS(NPSk[(NfV7lRd,O\

}G

Wvh\,NPV3d,O;WXd,ghNbQ_Sd,O z|d,{[(\,NPVQ_e3Od,Y[(vQUOd,Ym$Th`P\z


|v7Qe\,S(v7Q_e\Y(Y(d
cS2NPVheNPWZ\[kQ|\[kY\,VS(Nb[(NbdV~c@Nb[(vEghY(d,{h\{hNP`fNb[o

f[{ (
{
4  { * 1o{ (  { -  { * q

( q|v7Q@e7d,[k[kQ_eY(Q_SkgqSkd`PNfej+\Y(Y(dc@S

.Y(Q_S(g^q+
q|vQy\,- WX[(NbdVhSWXd,YY(Q_SkgdVe[kd\,S(SNblVmEQ_V[(S
[kdR[(v7QUG\YNf\{h`bQ_SZq
rO 4NPSS(\[(NPStij\{j`bQ,x[(v7Q_V\R[(NPmRQXae7QZgQ_Ve7Q_V[|gd`PNPWXouS(NPmyT`P\[(NPVl - Y(QZgQZ[(Nb[(NbdVS|d,O9\,Vo/S\[(NPSta
\Y(QygY(d,{h\{jNP`PNb[o$[kY\,VSNb[(NbdVSdV\,WX[(NPdV
\z

OoNPVl\,S(S(NPlVmEQ_V[vh\,SgQZY(Od,YmR\,VhWXQqrOzNPSV7d,[S\[(NPStij\{h`PQ,x[(vQ_VThVe7QZY\,Vo\,S(S(NblVmRQ_V[\[

_

fi

  

C

vh73Qh{.L8$>${3

   

O  

O  


[




 

   

O 

!

O 





1

|




1
1
1

)NblT7Y(QR}9VuQX7\,mEgh`bQThV7d,{hSkQZY(\{h`bQOd,Y"z


f{ (


{

4


{

* 4o{ (


{



{

* 

WZ`P\,TS(Q_S2d,O

`bQ_\,Sk[sdV7Qd,O^[(vQ

[z3NfS2V7d,[*S(\[(NfStihQ_e>q;Q_VWXQ,x7[(v7QgY(d,{h\{jNP`PNb[oy[(vh\[sThVe7QZYs\,Vo[(NPmEQXa

yd,OG >sNPSsYQ_\,Wv7Q_eNPS*\[mEdS([  ( q2sdVSkQ_T7Q_V[(`Po,x[(v7Q
(  
  q;|vNPS9gY(d,{7a
gY(d,{j\{hNP`PNb[o$[(vh\[9[(v7QijV\,`jSk[(\[kQd,O+G
 >9NPSYQ_\,Wv7Q_eNPS2\[&mEdS([t

  t9x
\{hNP`fNb[oQ_T\,`PS[(v7Q$QXpgQ_WX[kQ_e3YQZc&\Ye^qpNPVhWXQUOd,Y`P\Yl,Q$Q_V7dTlvX
Nb[v7d`feS|[(v\[
e7QZgQ_Ve7Q_V[sgd`PNPWXo~[(vQijV\,`>Sk[(\[kQ

z

.



z



-

[(v7Q[(vQZd,Y(Q_mOd`P`bd
cSZq

M

d,[kQ[(vh\[2[(v7Q[(NPmEQXae7QZgQ_VeQ_V[gd`PNfWXoyQX7NPSk[kQ_VhWXQ@gY(d,{j`bQ_mOd,Y&s2S9c@NP[(v~V7dVpaVQZl\[(Nb,Q

(././.q&|vQWZ`f\,S(S*MWXdVS(NPSk[(S&d,O+[(v7dSkQ`P\,VlT\l,Q_S

Y(QZcs\YeS*NPSsM>aWXdmEgj`bQZ[kQ~ThVev7Q_V7w~QZ[|\,`qbx
Y(Q_WXd,lVNP_\{h`bQ{o

V7dVe7QZ[kQZYmRNPVNPS([(NPW3+T7YNPVlnmR\,WvNPV7Q_S[(vh\[ThSkQ4\Y(Q_\,epaLdV`bo

\,eeNP[(NbdV\,`jY(Q_\,epaLc|YNP[kQ[(\gQ_Ssc@Nb[(v

NPV7ghT[~[(\gQ1\,Ve

fiR`Pd,l9?9[(\gQWXQ_`f`PSZq)r[*NPS&wpV7d
cV[(vh\[|M\,Ve[(v\[*M

NPSgYd,gQZY`bo/WXdV[(\,NfV7Q_e1NPV4sp@s|qVh`PNbw,Q[(v7QRWZ\,SkQRd,OsSk[(\[(NbdVh\Y(ogd`PNfWZNbQ_SZx\ggY(d
pNPm~\{hNP`PNb[o
d,O)[(NPmEQXae7QZgQ_VeQ_V[sgd`PNPWZNbQ_SsNPS@v\Ye7QZY*[(v\,V/[(v7Qgd`PNPWXoRQX7NPSk[kQ_VhWXQgY(d,{h`PQ_m

TV`bQ_S(S*M  sq
V7d,{hSkQZYG\{hNf`PNb[o/NfS\SkgQ_WZNP\,`+WZ\,SkQEd,O2gh\Y([(NP\,`?d,{hSkQZY(\{hNP`fNb[o,qQ_VWXQ,x>c&Qyl,QZ[[(vQS\,mEQEV7dVpa

\ggYd_7NPmR\{hNP`fNb[oRYQ_S(T`b[sOd,Y@&9SZxhQZ,Q_V/Od,Y@TV7Y(Q_S([kYNPWX[kQ_eY(QZcs\YeSZq

w

~vy~$yl

  RXP
w

~vy~$yl
ju,fi


. y\ y  E,  ~Xk7ZjZhhp  &Z&93 s! kjk
 |  
 \ p y  R Zt7ZjXj2yt9 !  kjk  RZb  
 _. yz
 9


n



n

_



D

)

 $j

 |

D

(

d,[kQu[(v\[[(v7QgY(dd,O|d,O|vQZd,Y(Q_m

7qfi3\,SS(TmEQ_e\[kd,[(\,`sQXgQ_WX[kQ_eY(QZcs\YeWXYNb[kQZYNbdV>q|v7Q

eNPSWXdTV[kQ_enY(QZcs\YeWXYNb[kQZYNbdVNfSy\,`fSkd4TSkQZOT`2NfV[(v7QijVhNb[kQv7d,YNbZdV>q1?dSv7d
c[(v7QY(Q_ST`b[UOd,YR\

*)


fi 38?>v{v8.^j

X   + 

$33$3

eNPSWXdTV[kQ_eRY(QZc&\Ye~WXYNb[kQZYNbdV>xc&Q@dV`PoyV7QZQ_e~[kd$Wv\,V7l,Q[(v7Q|Y(QZcs\Ye~NfVE[(v7Q|gYdd,Od,O>|v7QZd,Y(Q_m


\,SOd`P`bd
cSZ}T`b[(Nbgj`bo[(v7QyijV\,`+Y(QZcs\Ye{o



NPS[(v7QEeNPSWXdTV[O\,WX[kd,Y_x+

(10 x>c@v7QZY(Q

7qfi
[(v7Q

VTmU{QZY*d,O;WZ`P\,TS(Q_SZx\,Veu4[(v7QVTm${QZY*d,O+\YNP\{j`bQ_Ssd,O?[(vQO!d,Ym$T`f\z?q
)\gh\,eNfmRNb[kYNbdT\,Vhe|S(Nb[(SNbw`fNPS4t_,,/ghY(d
,Q_e[(v\[\ghY(d,{h`bQ_m

,QZYoSNPmRNP`P\Yu[kd

vNPSk[kd,Yoa

e7QZgQ_Ve7Q_V[sgd`PNPWXo~QX7NPSk[kQ_VhWXQUNPS*&7@*;aWXdmEgh`PQZ[kQ,q

8Sx~3y{





v

L

 


ml
 p 
(    G ! pVq4s 4 pX4Z, Tu

 @
   
7XjXhp  / !  Z  |tZb zZ93 ! &p@s@   jb_

U



(

:V:=:

(



p f L

d,{hS(QZY(G\[(NPdVS
9 u (  q*4Q@e7dV7d,[;\,ee7Y(Q_SS?[(vQ|WZ\,SkQ*d,OjS(ThWZWZNPVWX[(`boYQZgY(Q_SkQ_V[kQ_egd`PNPWZNbQ_S
Od,Y&9Sv7QZYQ,q+7d,Y2\,VR\,V\,`bopS(NPS+d,Oh[(vQ_NbYWXdmEgh`bQX7Nb[o,xSkQZQ|TVev7Q_Vwjx\(././.,\pqfiyrO
x[(vNPS
?de7Q_S(WXYNb{Q\1vd,YNbZdVvNPS([kd,Y(oaeQZgQ_Vhe7Q_V[gd`PNPWXoO!d,Y~\&c@Nb[(v

QXpgh`PNPWZNb[(`Po[(\w,Q_SSkgh\,WXQ










&

NPSQXgdV7Q_V[(NP\,`Skgh\,WXQ,q)|v7QZY(QZOd,Y(Q,xc2QsWZ\,VV7d,[+QXpgQ_WX[+[(v\[)\|gd`bopV7dmRNP\,`baL[(NPmEQ\,`bl,d,YNP[(vmdT7[kgjT7[(S
\|vNfSk[kd,Y(oae7QZgQ_Ve7Q_V[>gd`fNPWXo,x
\,VeUc2QYQ_Sk[kYNPWX[WXdVhS(NPe7QZY\[(NPdV[kd|gd`bopV7dmRNP\,`aL[(NfmEQ;\,`bl,d,YNb[(vm~S[(v\[
\ggYd_7NPmR\[kQ~[(vQvNPS([kd,Y(oaeQZgQ_Vhe7Q_V[E
e7QZgQ_Ve7Q_V[gd`PNPWXoU

[(v7Qd,g[(NPmR\,`9gQZY(O!d,YmR\,VWXQ~TVhe7QZYU\,VovNPSk[kd,Yoa

d,O^\$&|qpsT7Y\l,d7xpe7Q@dT7l,Q_mEdV[Zxp\,Vhep`PNfS(SkQ_V7w,d~t_,,2WXdVS(Nfe7QZY(Q_e

[(v7QWZ`P\,S(Sd,O$s2Sc@Nb[(v

\{dTVed,O1dV

[(vQVTm${QZYd,O$S([(\[kQ_SuWXd,Y(Y(Q_SkgdVeNfV7l1[kd\,V

d,{hSkQZYG\[(NbdV^xc@vQZY(Q9[(v7Q2Y(QZcs\YeSWXd,YY(Q_SkgdVe7Q_e[kd[(v7Q2gY(d,{h\{hNf`PNb[od,O7Y(Q_\,WvhNPV7l\|ipQ_eUSkQZ[?d,O7l,d\,`
Sk[(\[kQ_S@\,Vhe[(vTS)c&QZY(Qs{dTVe7Q_e${o
q;|v7QZoyS(v7d
c&Q_e[(v\[;Od,Y;\,Vo$ipQ_eCx,[(v7Q*d,gh[(NPmR\,`pvNPSk[kd,Yoa
e7QZgQ_Ve7Q_V[gd`PNPWZNPQ_SO!d,Y&9S$NPV1[(vhNPSUWZ`P\,SS$WZ\,V{Q~\ggY(d
pNfmR\[kQ_e[kdc@Nb[(vhNPV4\,V\,eheNb[(Nb,Q

q$4Q~S(v7dc2Q_e1NfV9Y(d,gdS(Nb[(NbdVj'q[(v\[szvNPSk[kd,Y(oae7QZgQ_Ve7Q_V[ehNPS(WXdTV[kQ_ed,Y
O
v\
,QEgd`PoV7dm~NP\,`aL[(NPmEQy\ggY(d
pNfmR\[(NbdVS(WvQ_mEQ_SRY(d,gdS(NP[(NbdV
' q x&  ,fi
   7 X(,k~j
j  ,  (pj    p
 XU7 $eR X$k  L,  7X  XG,  $p $kZ9k  q
WXdVSk[(\,V[

[kd,[(\,`aLY(QZcs\Ye\,`PT7QgYd,{h`bQ_mRSy[(v\[RWZ\,V{Qu\ggY(d
pNfmR\[kQ_e[kd4c@Nb[(vNfV\,V\,eheNb[(Nb,QWXdVSk[(\,V[

d,[(NPWXQ,x?vd
c&QZ,QZY_x?[(vh\[|v7QZd,Y(Q_m7qP,e7dQ_SVd,[lNb,QRTSNPVO!d,Ym~\[(NbdV\{dT[[(vQRWZ`P\,S(SkQ_Sd,O

&9Ss[(vh\[|&TY\l,dEQZ[|\,`qt_,,sWXdVSNPe7QZY(Q_e>}&Q_WZ\,TS(Qd,O?[(v7QY(Q_S([kYNPWX[(NbdVSs\,S(S(dWZNP\[kQ_e/c@Nb[(v
[(v7Qgj\Y\,mEQZ[kQZYIxdT7Yv\YeV7Q_SSsY(Q_S(T`b[(S*edRV7d,[@WXdV[kY\,eNPWX[|[(vQ_NbY*Y(Q_S(T`P[Zq
)NPV\,`f`bo,xc&QSv7d
c[(v\[$[(vQvNPS([kd,Y(oaeQZgQ_Vhe7Q_V[\,`PT7Q~d,O|&9S$c@Nb[(vVdVpaV7QZl\[(Nb,QY(QXa

Za\ghgY(d_7NPmR\{j`bQnThVe7QZY[kd,[(\,`EQXpgQ_WX[kQ_ed,Y1ehNPS(WXdTV[kQ_eY(QZc&\YeSZxETV`bQ_SS 
a\,eeNP[(Nb,Q\ghgY(d_7NPmR\[(NPdVnThVe7QZY[(v7Q

cs\YeS4NPSV7d,[

&7@*|qn&dVhSkQ_TQ_V[(`bo,x9[(v7QG\,`PTQvh\,SEV7d2?d,Y
S(\,mEQ$\,S(S(TmEgh[(NbdV>q

|vQvNfSk[kd,Y(oae7QZgQ_Ve7Q_V[~gd`PNPWXoQX7NPSk[kQ_VWXQgY(d,{j`bQ_m

Od,Yu&9Sc@Nb[(v

V7dVpaVQZl\[(Nb,Q4Y(QXa


(././.q*Q_VWXQ,xh{Q_WZ\,TS(QUMNPS|\EghY(d,gQZY@S(T7{>WZ`P\,S(Ssd,O

cs\YeS|NPS@M^aWXdmEgh`bQZ[kQTVhev7Q_V7wQZ[\,`qbx

&7@*|x\ggYd_7NPmR\{hNP`fNb[oEd,O?[(v7QvNPSk[kd,Yoae7QZgQ_VeQ_V[&G\,`fT7QNfS

   fiv\Ye7QZY*[(v\,V[(vQgd`fNPWXo

QX7NPSk[kQ_VWXQghY(d,{h`bQ_muq

8Sx~3y{

q. y\ pm ! Lt7ZjXj?t293    Ej,"  ,
(X9k $! kt  R
 Zb  Uj  |  &p@s
:y{~O~q7 |v7Q|vhNPSk[kd,Y(oae7QZgQ_Ve7Q_V[;G\,`fT7Q*d,O^\&~G WZ\,VR{Q|WZ\,`PWZT`P\[kQ_eTS(NPVl{hNPV\Y(oySkQ_\YWv
d
,QZY2[(v7Q@vNfSk[kd,Y(oae7QZgQ_Ve7Q_V[?gd`PNPWXo$QXpNfSk[kQ_VWXQ*gY(d,{j`bQ_muq)|v7Q*VThmU{QZY;d,O{hNb[(S)[kd{Q|WZ\,`PWZT`f\[kQ_eRNPS
gd`bopV7dmRNP\,`NPV[(v7Q|S(NPZQ*d,O1
G q;|v7QZY(QZOd,Y(Q,x{o|v7QZd,Y(Q_m7qPp. x[(vNPS;WZ\,`PWZTh`P\[(NbdVRWZ\,VE{QsgQZY(Od,YmEQ_e
NPVgd`bopV7dmRNP\,`?[(NPmEQETS(NfV7l\usp@sd,Y\,WZ`PQ,qyrO2 
&p@s@x^Nb[O!d`f`bd
c@S[(vh\[[(v7QRvNPSk[kd,Yoa
e7QZgQ_Ve7Q_V[sG\,`fT7Qd,O;\R&FG
WZ\,V/{QU
   WZ\,`PWZT`P\[kQ_e/NfVgd`PoV7dm~NP\,`[(NPmEQ,q
|vQS(QZ[
d,OU[kYT7Q4T\,V[(NihQ_e&dd`bQ_\,VOd,YmyT`P\QNPSdV7Q4d,OU[(vQS([(\,Ve\Ye&7@*
WXdmEgh`PQZ[kQ&SkQZ[(S_q+?dWXdVhWZ`PTe7Q&  sp@sO!Ydmz\,VsZ
 a\gghY(d_7NPmR\[(NbdV$d,O[(v7QsvNPS([kd,Y(oaeQZgQ_Vhe7Q_V[
ml

q

n

_

D

(

BY x4y

DY


fiC

vh73Qh{.L8$>${3


\,`PT7QRgYd,{h`bQ_mux?c2QTSkQ\3[kY\,VSkOd,YmR\[(NPdV1d,O|NPVS([(\,VWXQ_Sd,O*Y

.

(././.{q

[kds2SUS(NfmRNP`P\Y[kd[(v7Q

x4y

gY(dd,O+d,O+|vQZd,Y(Q_m7qP ~NfV1TVhev7Q_V7wx
|vQySkQZ[CY

x4y

WZ\,V{QNPV[kQZYgY(QZ[kQ_e\,S\[c&daLgh`P\_o,QZYUl\,mEQ,}9`P\
o,QZYR$SkQZ[(S[(vQyQX7NPSk[kQ_V[(NP\,`P`Po

ESkQZ[(S@[(v7QUTVhNb,QZYS(\,`P`Po~Th\,V[(NijQ_eG\YNf\{h`bQ_SZq2`P\_o,QZYyc@NPVS*NPO
[(v7Q\,`b[kQZYV\[(NfV7lyWv7dNPWXQ_S*e7QZ[kQZYmRNPV7Q\yS\[(NPSkOoNPVl$\,S(S(NPlVmEQ_V[2[kd[(v7QO!d,Ym$T`f\px\,Vegh`f\_o,QZYm$
 c@NPVS
NbO^[(vQe7QZ[kQZYmRNPVQ_e\,SS(NblVmEQ_V[*NPSsV7d,[|S(\[(NPS(O!opNPV7l7q;Od,YmyT`P\NPSsNPV
NbO+\,VedV`boNbO^gh`P\
o,QZYU
v\,S\Rc@NPVhVNPV7lESk[kY\[kQZl,o,q|vNPS|mRQ_\,VS@gj`P\_o,QZYyUv\,S\RY(Q_S(gdVhSkQ[kdQZ,QZYo/Wv7dNPWXQyd,O)gh`P\
o,QZY[
 xSkd
[(v\[@NfV[(v7QQ_Ve[(v7QUO!d,YmyT`P\yc@Nf`P`{QS\[(NPStihQ_e>q
|vQ,QZYSNbdVcv7QZY(Qgh`P\_o,QZY;
 mR\w,Q_SY\,Vedm Wv7dNPWXQ_SE\,Vegh`P\
o,QZYSl,d\,`sNPS$[kdc@NPVc@Nb[(v
(
gY(d,{j\{hNP`PNb[o
WXd,YY(Q_SkgdVeS2[kd
  L  p
    $ , ! &     pxhc@vNPWvNPS&\,`PSkd&7@*WXdma
T\,V[(NbihQ_eG\YNP\{h`bQ_SZx\,Vheugh`P\
o,QZY

BD x4y

}&

w

gh`bQZ[kQ,q)|v7Q*NPVhSk[(\,VWXQ_S+d,O

w

 x4y

\Y(QsOd,Ym$Th`P\,S+c@vNPWv$\Y(Q*T\,V[(NihQ_e\,`b[kQZYV\[(NfV7l`boc@Nb[(v$QX7NPSk[kQ_V[(NP\,`

 x4y

T\,V[(NbihQZYS11\,Ve4Y\,VedmT\,V[(NbihQZYSY?qu|v7QRmEQ_\,VNfV7l/d,Os[(v7Q~Y\,Ve7dm
\,V\,SS(NblVmEQ_V[R[kd[(v7Q/Y(Q_SkgQ_WX[(Nb,QG\YNP\{h`bQ/NPS~Wv7dSkQ_V
Sk[kdpWv\,S([(NPW&dd`PQ_\,V/Od,Ym$Th`P\


NPS*NPV
w

{



{

(

-

{

{

4

[(v7QZY(QQX7NPSk[(S" (

Od,Y|Y\,Vhe7dm$ -

VY\,Ve7dm

rO1v\,S

QXpNPS([(SI 4

O!d,Y|Y\,Ve7dm$ *

T\,V[(NbihQZYSZx*[(v7Q_V

[(v7QSk[kY\[kQZl,o

OY(dm


.eI_



q


*

z

NbO+\,VhedV`boNbO

 x4y

T\,V[(NihQZYNPS[(v\[

TVNbOd,YmR`Po\[RY\,Vhe7dm

_V/T (c ) ( IDI L&NPS*[kYT7Qf

qZqZq

 z





&

-

( q


  (

d,Ogh`P\
o,QZYe7QZ[kQZYm~NPV7Q_S\SkQZ[d,O



 _V/T (c ) ( IDI Qp
 *NPSs[kYT7Qf y \,S(S(NPlVmEQ_V[(S|S(\[(NPSkOo q
7Ydm
[(v7QEgY(dd,O9d,O2r 
&p@s{o4pv\,mRNPYEt_,/Nb[O!d`f`bd
c@S[(v\[UO!d,YQZ,QZY(o&7@*
SkQZ[ \,VeuWXdVSk[(\,V[
 [(v7QZYQNPS*\gd`boVdmRNP\,`aL[(NPmRQ@Y(Q_eThWX[(NbdV 3OY(dm z[kd

S(TWv[(v\[
Od,Y|QZ,QZY(ouNPVSk[(\,VhWXQ \,VeOd,YmyT`P\ ; > 





E
(
[
7
v

Q

O

d
P
`
b
`

d
@
c
P
N
7
V
E
l
7
v

d
f
`

e
Z
S
q
(
W rO Z $x[(v7Q_V ( Od,Y|Y\,Vedm -  _V/T vc  ( II Q7 &NfS*[kYT7Qf ztm  xh\,Ve
W NPO Z  Ux7[(vQ_V ( O!d,Y@Y\,Ve7dm -  _V/T (c  ( I*I L7 sNPSs[kYTQf+  q
\,S(S(NPlVmEQ_V[(S[kdKz?qU@v7Q$[kQZYm

d,O)[(v7Q_SkQ

|



 z





\&

( mEQ_\,VS[(v\[mEd,Y(Q[(v\,V

P






-{

{



{



z



Y

o{

{

{

B4

$



 z

'H





 z



 x4y

w

z













]

&

]

|vNfS|mEQ_\,VS@[(v\[gj`P\_o,QZYyQ_Nb[(v7QZYv\,S\~Sk[kY\[kQZl,oTVe7QZY|cvNPWv3S(v7Qc@NfVS*c@Nb[(v/,QZY(o/vNblv3gY(d,{7a
\{hNP`fNb[o,xd,Y[(v7QygY(d,{h\{hNf`PNb[od,OcNPVVNPVlTVe7QZY\,VoSk[kY\[kQZl,oNPS,QZY(oSmR\,`P`q4QRS(v7dcvd
c[kd
[kY\,VS(O!d,Ym

\uS([kdWv\,Sk[(NPWE&dd`bQ_\,V4Od,Ym$Th`P\}NfV[kd/\uscNb[(v\u`P\Y(l,QRvNPS([kd,Y(oaeQZgQ_Vhe7Q_V[

[yc@NPVSZq

\,`PT7QNbO?gh`P\_o,QZYyv\,S@\cNPVVNPVlySk[kY\[kQZl,o,xj\,Ve/\~m$TWv/S(mR\,`f`bQZY*G\,`fT7QNbOgj`P\_o,QZY

+d,Y\,VNPVSk[(\,VWXQ
xc@vQZY(Q NPS/\Od,YmyT`P\c@NP[(vG\YNf\{h`bQ_S

(
-  d,O
I( *I qhx@c2QWXdVSk[kYThWX[\n&-G |R\,SO!d`P`Pd
c@SZqz|v7Q3Yd`bQ3d,Ogh`f\_o,QZYNPS[(\w,Q_V {o
[(v7QEWXdV[kY(d`P`bQZYd,O9[(vQyghY(dWXQ_SSZqU
S([kY\[kQZl,od,O2gh`P\
o,QZYRyeQZ[kQZYmRNPV7Q_S\gd`PNPWXoud,O2[(v7QEWXdV[kY(d`f`bQZY_x
\,VeNPWXQ~,QZYS(\pq/9`f\_o,QZY/
 \ggQ_\YS$\,SUgYd,{h\{hNP`PNfSk[(NPW$[kY\,VS(Nb[(NPdVSNPV4[(vQ~gY(dpWXQ_S(SZq|v7Q~gY(dpWXQ_S(S
G  |&v\,S&[(v7Y(QZQSk[(\l,Q_S_q9|v7QihYS([&Sk[(\l,Q$WXdVS(NPSk[(Ssd,O?dV7QSk[kQZg^q9@v7QghY(dWXQ_SS*Wv7ddSkQ_S|TVhNbO!d,YmR`bo

\[yY\,Ve7dm
dV7Qd,O|[(v7Q~\YNP\{j`bQ_S$\,Vhe\,Vn\,S(S(NblVhmEQ_V[$[kd4Nb[Zx;\,VeS([kd,Y(Q_Sy[(v7Q~\YNP\{h`PQ\,Ve[(v7Q
\,S(S(NPlVmEQ_V[Zq2d,Y(QOd,YmR\,`P`bo,x7OY(dm[(vQUNPVNb[(Nf\,`S([(\[kQsK / xhdV7Qd,O)[(v7QUSk[(\[kQ_S ? t
 ;x
e
.
I
_



P
N
$
S
(
Y
_
Q
,
\

W
7
v
_
Q
^
e
9
x
_
Q
,
\

W

v
@
c
b
N
(
[

v

g
(
Y
,
d
h
{

\
h
{
f
N
P
`
b
N

[

o

p



G
?



n
q

r

[
P
N

S
7
V
,
d

[
,
d
h
{
k
S
Z
Q

Y
G


\
h
{
P
`

Q
@
c
h
v
P
N

W

v
G


\
YNP\{h`bQ
Z
\,S(S(NPlVmEQ_V[|c&\,SSk[kd,YQ_e/{ou[(v7QUgYdWXQ_S(S_qsd
c&QZ,QZY_xc@v7Q_VQZ,QZY[(v\[G\YNP\{h`bQU\ghgQ_\YS|`P\[kQZY_x[(v7Q


{

|{

{

{

z

 x4y

w



f

f

{















gY(dpWXQ_S(SWv7Q_WwS[(v\[[(v7QNfVNb[(NP\,`P`Po4ipQ_e\,S(S(NPlVmEQ_V[~NPS~Wv7dSkQ_V

\l\,NPV>qrO[(v7Q/gd`PNPWXonlNb,Q_S\

~.pq|v7QZY(QNPS
;K ( x;d,Y`PQ_S(SUOd,YmR\,`f`bo,x;[(v7Q

eNQZY(Q_V[$\,S(SNblVmEQ_V[UeT7YNPV7lu[(v7QSkQ_WXdVeSk[(\l,Q,x;[(v7QgY(dpWXQ_S(S$vh\,`b[(Sc@NP[(v1Y(QZcs\Ye
\e7QZ[kQZYmRNfVNPSk[(NPWR[kY\,VS(Nb[(NPdV1[kd4\ijV\,`&Sk[(\[kQc@vNPWvc&QY(QZO!QZY[kd\,S



4XjSk[(\[kQ,qfirO2S(ThWv4\,V1NPVhWXdVS(NPSk[kQ_VhWXo/dWZWZTYSeT7YNfV7l~[(v7Q[(vNPYeSk[(\l,Q,x[(v7QEghY(dWXQ_SSv\,`b[(S

v


fi 38?>v{v8.^j

$33$3

l








/

9% f6







(





""
/

"




(


y}9|v7QihYSk[*Sk[(\l,QUd,OG |q

)NblT7YQ

f

X.\,Ve3Vd,[(NPWXQ_S@[(vh\[[(v7Q$gd`PNPWXo  p,  q|v7QZY(QUNfS@\e7QZ[kQZYmRNPVhNPSk[(NPW[kY\,VSNb[(NbdV/[kd\
S(NPVw~Sk[(\[kQc@vNfWvc2QYQZO!QZYs[kdR\,SJK    x7d,Y|`bQ_S(SsOd,YmR\,`P`PoR\,Sps7Xj,/(/{Q_WZ\,TSkQ[(v7Qgh`P\
o,QZY
SkQ_V[2[(vQZY(QWZ\,VVd,[2Y(QXaLQ_V[kQZY*[(v7Ql\,mEQ`P\[kQZY_qfi$rOQZ,Q_V[(T\,`P`PoR[(v7Q@cv7d`bQ@Od,YmyT`P\NfS9gh\,SSkQ_e>xQ_NP[(v7QZY
Y(QZcs\Yed,YUY(QZc&\Yeiu
. NPSd,{[(\,NfV7Q_e4e7QZgQ_Ve7Q_V[dV4c@v7QZ[(vQZY[(v7QROd,Ym$Th`P\c&\,S$S(\[(NPStihQ_ed,Y$V7d,[Zq
|v7QihYS([ySk[(\l,Q/NfSyS(w,QZ[(Wv7Q_eNfVn)NblTY(Q
 qrVn[(vNPSy\,Ve[(v7QOd`P`bdc@NPV7l3ihlT7Y(Q_SZxeh\,S(v7Q_e\Y(Y(dc@S
c@Nb[(v3Y(QZcs\Ye





Y(QZgYQ_SkQ_V[Y\,Vhe7dm

[kY\,VS(NP[(NbdVS\,`P`sd,O@Q_T\,`sgY(d,{j\{hNP`PNb[o,xNbY(Y(QZl\Ye`bQ_S(S$d,O[(vQ\,WX[(NPdVWv7dS(Q_Vjx

Skd`PNfe\YY(d
c@SY(QZgY(Q_SkQ_V[e7QZ[kQZYmRNfVNPSk[(NPW$[kY\,VSNb[(NbdVSWXd,Y(Y(Q_S(gdVheNPV7l[kdu[(v7QR\,WX[(NbdV/?YT7Qx+\,Ve

X.3\,`PSkQq

e7d,[k[kQ_e\Y(Y(d
cS*Y(QZgY(Q_S(Q_V[@eQZ[kQZYmRNPVNfSk[(NPW[kY\,VS(Nb[(NPdVS*WXd,Y(Y(Q_SkgdVehNPV7ly[kdR[(v7QU\,WX[(NbdV



|vQS(Q_WXdVeuSk[(\l,QSk[(\Y([(S@NPVQ_\,Wvud,O?[(v7QSk[(\[kQ_S~{ ^ \,Vev\,Ss4Sk[kQZgjSZx7eT7YNPVlUc@vNfWv\,V
\,S(S(NPlVmEQ_V[@[kd~[(vQU\YNP\{h`bQ_SP{ ( |{ |{ uNPS*ihQ_e>q*MQZ["
e7Q_V7d,[kQ$[(v7QUgj\Y([|d,O;[(v7Q$gY(dpWXQ_S(SZ
 
SkQ_WXdVeSk[(\l,QeT7YNfV7lc@vNPWv~Nb[2NPS9\,S(STmEQ_e[(v\[9\,`PT7Q*sNPS9\,S(SNblV7Q_e[kdU\YNP\{h`bQI{  q)rO^\UG\YNP\{h`bQ

I I*I q



{



NPS?QXpNPS([kQ_V[(NP\,`f`boUT\,V[(NbihQ_e>x[(v7Q_Vy[(v7Q*\,S(S(NblVhmEQ_V[;NPS?[(v7Q*\,WX[(NbdVENPV

rO?\\YNP\{h`bQ*{





.eI_



Wv7dSkQ_VE{o[(v7Qsgd`PNPWXo,q

NPS&Y\,Vhe7dmR`boT\,V[(NihQ_e^x[(vQ_V[(v7Q\,S(S(NblVhmEQ_V[*NPS*Wv7dSkQ_VTVNbOd,YmR`boE\[|Y\,Ve7dm

{oE[(v7QghY(dWXQ_SSZxNfVe7QZgQ_Ve7Q_V[;d,O>[(v7Q\,WX[(NbdVd,O>[(v7Qgd`PNfWXo,q?rtV~[(vQSkQ_WXdVeSk[(\l,Q,x7Nb[&NPS2d,{hSkQZY(\{h`bQ
c@vNfWv\,S(S(NPlVmEQ_V[ycs\,SRmR\,eQu[kd1QZ,QZY(onG\YNf\{h`bQ,qnrO[(v7QuG\YNP\{h`bQu\,S(S(NPlVmEQ_V[yOY(dm

[(vQihYSk[

Sk[(\l,Qe7dQ_SsV7d,[2WXdNPVhWZNPe7Q@cNb[(v~[(vQ\,S(SNblVmEQ_V[2mR\,eQ[kdy[(v\[sG\YNP\{h`bQ@ehT7YNPV7lU[(v7QSkQ_WXdVheS([(\l,Q,x

*
V4{Q/[(v7QVTm${QZYRd,OY\,Vedm

.pq9MQZ[
O

[(v7Q[kY\ tQ_WX[kd,Y(odV3c@vNPWv[(v\[@v\ggQ_VS&Q_VehS|NPV[(v7Qe7Q_\,e/Q_Ve/Sk[(\[kQ$[(v\[|opNbQ_`PeS&Y(QZcs\Ye

T\,V[(NbihQZYSEd,O1q9,QZY(oSk[kY\[kQZl,od,Ogh`f\_o,QZY3e7QZ[kQZYm~NPV7Q_S

\,S(S(NPlVmEQ_V[(SZq9,QZY(o3\,S(S(NPlVmEQ_V[yo{ (





Od,Ym

(

IDI 

|{

Qp@NPVeTWXQ_SG[kY\*tQ_WX[kd,YNPQ_SZ}|vh\_,Q[(v7Q







K / I f  DI c ( I ( f"I*IDc  I  f"I*IDc q
I Lfif
Od,Y 
(ILIvIk?+[(v\[9gh\,S(SSk[(\l,Q[c@Nb[(v7dT[;Y(Q_\,WvNfV7l[(v7Q@e7Q_\,eRQ_VeRSk[(\[kQ\,VeRWXdV[(NPVTQ*c@Nb[(v
[(v7QUihYSk[@S([(\[kQUd,O;[(v7Q$[(vNbYe/S([(\l,Q,x\,Ve31[(v\[e7Q_\,epaLQ_VeNPV/Sk[(\l,Q
 q||v7QU`f\[k[kQZY1[kY\*t Q_WX[kd,YNPQ_S
[(v\[@edRV7d,[|Y(Q_\,Wv3Sk[(\l,QE
' \Y(Qd,O)[(v7QOd,Ym
K / I P  IDc ( I ( f"IIDc  _I   f"ILK
Od,Y` 
(ILIvk
I ?qyWZWXd,YeNPVl`bo,x+
G  @v\,SX\ U[kY\*t Q_WX[kd,YNPQ_S[(v\[Y(Q_\,Wv4[(v7QE[(vNPYeS([(\l,Q,q
|v7QS([kYTWX[(T7Y(Qd,O)Sk[(\l,QE
 NPS*Skw,QZ[(Wv7Q_e3Od,Y 4  E. NPV/)NblT7YQ_' q
|vQ[(vNPYe S([(\l,QWv7Q_WwS/c@v7QZ[(v7QZY zNPSS(\[(NPStihQ_e {o[(v\[[kY\*k
 Q_WX[kd,Y(oS3\,S(S(NPlVmEQ_V[Zq|v7Q
|{



|{



{



{

{





{

f

{





=^



P{

z

gY(dpWXQ_S(S?gh\,S(SkQ_S)SkQ_T7Q_V[(NP\,`P`bo[(v7YdT7lvU[(vQ9c@v7d`PQ9Od,Ym$Th`P\@Wv7Q_W(wpNPV7lQ_\,WvE`PNP[kQZY\,`NPVUQ_\,WvEWZ`P\,ThSkQ9Od,Y




fi

C

vh73Qh{.L8$>${3



9%!
6










/



/




O

9%!
6C/



(



(


(



l

!



9!%!
6H(

y'}9|v7QSkQ_WXdVhe3Sk[(\l,Q$d,OG |} S4  /

)NblT7YQ

(






O







O








/




/

O







/






f

\,V\,S(SNblVmEQ_V[^[kd|[(v7Q;YQ_SkgQ_WX[(Nb,Q\YNP\{h`bQ,q 4

O!d,Y@[(v7QUT\,V[(NbihQZY*gY(QXi{ (

\

|v7Q;WZ\,SkQ9d,O7\



{

-

{

4

* q

{

fi

 p(,   sp  Gx,NqQ,qbxdV7Q[(v\[?\,VSkc&QZYS
eT7YNPV7l[(v7Q|[(vNPYeESk[(\l,Q@cNb[(vR\,V7d,[(v7QZY&\,S(S(NblVhmEQ_V[9[(v\,VEipQ_eReTYNPV7l[(v7Q@S(Q_WXdVe~Sk[(\l,Q,x7myTSk[{Q
tghTVhNPS(v7Q_epq+v7Q_V7QZ,QZYs[(v7QG\YNf\{h`bQWXd,YY(Q_SkgdVeNPVl[kd$[(vQNPVhNb[(NP\,`xS([kd,Y(Q_e\,S(S(NblVmRQ_V[s\ggQ_\YS
[(v7QgYdWXQ_S(S*Wv7Q_W(wpS|[(vh\[|[(v7QSk[kd,Y(Q_e3\,S(S(NblVmRQ_V[*NPS*WXdVSNPSk[kQ_V[*c@Nb[(v[(v7QWZT7YY(Q_V[*\,S(SNblVmEQ_V[Zq;rO

XRd,Y@Y(QZcs\YeX.NPS*d,{[(\,NfV7Q_e>x

QZ,Q_V[(T\,`P`bo[(v7Q$c@v7d`bQOd,YmyT`P\Egh\,SSkQ_S|[(v7Q$Wv7Q_WwNPVl7xjQ_Nb[(vQZY@Y(QZcs\Ye

e7QZgQ_VeNfV7ldV1c@v7QZ[(vQZY[(v7QROd,YmyT`P\ucs\,SUS(\[(NfStihQ_e\,Vhe4[(v7QRgd`PNPWXocs\,SUV7d,[yWv7Q_\[(NPVl7x?d,YyV7d,[Zq
M^QZ[
~






{Qu[(v\[NPVhSk[(\,VWXQ/d,O[(v7Q/[(vhNbYeSk[(\l,Qc@v7QZY(Q3NP[RNPSRWvQ_W(w,Q_e

c@v7QZ[(v7QZY0{


\,`bcs\_opSl,QZ[(S

\,S(S(NPlVmEQ_V[
qr[RNPSQ_S(S(Q_V[(NP\,`f`bo[(v7QS(\,mEQ/e7QZ[kQZYm~NPVNPSk[(NfW~gYdWXQ_S(S~\,SRe7QXijV7Q_eNfV[(v7QugY(dd,Od,O
|v7QZd,YQ_m7qPx{hT7[*cv7Q_V7QZ,QZY@\,V/\,SS(NblVmEQ_V[*[kd~\E`PNP[kQZY\,`>WXdV[(\,NfVNPV7lD{

;K

V7d,[l,QZ[\,SS(NblVmEQ_V[	[(v7Q$ghY(dWXQ_SS|l,dQ_S[kdSk[(\[kQ

K



NfS*\,Skw,Q_e/Od,Y_xNbO{


e7dQ_S

=|kq[(v7QZY(c@NfSkQ,x[(v7QygY(dpWXQ_S(Sl,dQ_S[kdSk[(\[kQ

bNPSd,{[(\,NfV7Q_e>]

=q/rO*[(vQ\,S(S(NPlVmEQ_V[yWv7dSkQ_V{o1[(vQ~gd`PNPWXo4S(\[(NPSkihQ_e1[(v7QOd,Ym$Th`P\px?YQZc&\Ye

m.pq

d,[(v7QZY(cNPSkQ[(v7QY(QZcs\Ye/NfS

9G |UNPSySkw,QZ[(WvQ_eNPVn+NPlT7Y(Q~7q4d,[kQu[(v\[y[(v7Que\,S(v7Q_en\Y(Y(dc@S

|vQd,QZY\,`P`sSk[kYTWX[(TY(Qd,O
Y(QZgYQ_SkQ_V[Y\,Vhe7dm

f

[kY\,VS(NP[(NbdVS\,`P`sd,O@Q_T\,`sgY(d,{j\{hNP`PNb[o,xNbY(Y(QZl\Ye`bQ_S(S$d,O[(vQ\,WX[(NPdVWv7dS(Q_Vjx

Skd`PNfe\Y(Yd
c@SyY(QZgYQ_SkQ_V[Ee7QZ[kQZYmRNPVNPS([(NPWE[kY\,VSNb[(NbdVSyWXd,Y(Y(Q_SkgdVehNPV7l3[kd[(v7Qu\,WX[(NbdV?YT7Q,xed,[k[kQ_e
\Y(Y(dc@SY(QZgY(Q_SkQ_V[e7QZ[kQZYmRNPVhNPSk[(NPW[kY\,VhS(Nb[(NbdVS@WXd,Y(Y(Q_SkgdVeNfV7lR[kd[(v7Q$\,WX[(NbdV\,`fSkQ,x\,Vhee7d,[taeh\,S(v
\Y(Y(dc@S*Y(QZghY(Q_SkQ_V[|[kY\,VS(NP[(NbdVSs[(v\[@\YQO!d,YWXQ_e^xc@v\[kQZ,QZY[(v7QUWvdNPWXQd,O)\,WX[(NbdV>q

c@Nb[(v$G\YNP\{h`bQ_S
 \,Ves|V Y\,Ve7dmTh\,V[(NijQZYSZx,\,Ve
Z
( I - I*I q
WXdVS(Nfe7QZYG |q;2Q_WZ\,ThSkQ@[(v7Q@[(vNbYeESk[(\l,QNPSe7QZ[kQZYmRNPVNPS([(NPWx,[(v7Q|gYdWXQ_S(S9v\,SG` &[kY\*tQ_WX[kd,YNPQ_SZx
*
d,O)c@vhNPWvuYQ_\,WvSk[(\l,Q
' qsd
cUx\,S(STmEQ[(v\[1
 NPS\Rgd`PNPWXo,xcvNPWv3NPS  , ! Zh|c@NP[(vu[(v7Q
D NLQJ"Uty"v"tQ"S/&`"v*"}L(_"v/SQ}L"tQ"S*"L0D"[
+v""v/Q9*""*"Q Q(J"vD"6*0"+Q""v
&dVhS(NPe7QZY)\@Od,YmyT`P\-

w

 x4y

{

f

|{

|{





 4



fi 38?>v{v8.^j



 =








 b






 

O^# O\


 #



$33$3

 

^^# O |





 


^^# O




 

 



  





^^# O


	fiff

ff


G

)NblTY(Q7}9Skw,QZ[(Wv3d,O f|q

d,{hSkQZYG\[(NbdVhS)O!Y(dm[(v7Q*uSk[kQZghSeTYNPV7l[(v7Q|SkQ_WXdVe~Sk[(\l,Q,xNqQ,qbxc@v7Q_V7QZ,QZYNP[NPSk\,Skw,Q_e[kdUlNb,Q|\,V
\,S(S(NPlVmEQ_V[@[kd\~\YNP\{h`PQeT7YNPV7lE[(v7Qy[(vNbYe3Sk[(\l,Qx^Nb[edQ_SSkd\,WZWXd,YeNPV7l[kd[(v7QUd,{hS(QZY(G\[(NPdVS
eT7YNPV7l[(v7Q~SkQ_WXdVeSk[(\l,Q\,Vhe4[(v7QZY(QZOd,Y(Q~Nb[\,SS(NblVS[(vQRS(\,mEQR\,`PT7QE[kdQZ,QZY(o4\ggQ_\Y\,VWXQRd,O*\

   q&Q_WZ\,TS(Q

} 

d,O[(v7Q[kY\*t
 Q_WX[kd,YNPQ_Ss[(v\[
'WXd,Y(Y(Q_S(gdVhe/[kd\S(\[(NfSkO!opNPV7l\,S(S(NblVmRQ_V[\,Ve3\Y(QWXdV[(NPVTQ_eTVhe7QZY@[(vNfS|gd`PNPWXo
[kdSk[(\[kQ;K
c@v7QZY(Q$[(v7QZouY(Q_WXQ_Nb,QY(QZcs\YejqQ_VWXQ,x[(v7QyvhNPSk[kd,Y(oae7QZgQ_Ve7Q_V[|\,`PT7Qd,OG |@NPS
( tm  *
~  q

7d,Y

x2\,VNPVhWXdVS(NPSk[kQ_V[/d,Y  7
 (,  fi gd`PNfWXo1dV 
G  |ymR\_ov\
,Q3gQZY(Od,YmR\,VWXQ
Z

l,Y(Q_\[kQZY[(vh\,Vn9 
q|v7QZY(QZOd,Y(Q,x>c&QEv\_,QE[kdugQZY(Od,Ym\ghY(d,{h\{hNP`fNb[ou\,mEgh`fNijWZ\[(NbdV\,SNPV[(v7Q
gY(dd,Od,O|v7QZd,Y(Q_m7qfis[(v\[?ghTVNPSv7Q_S>Wv7Q_\[(NfV7l7q?4Q&WXdVSk[kYThWX[G~
  |O!Y(dm  WXd,ghNbQ_SG I*IUG
d,O0
G  @*[(v7Q|QX7\,WX[2\,`PT7Q*d,O+ cNP`P`{Q|e7QZ[kQZYmRNPVQ_eR`P\[kQZYxS(TWvR[(v\[9[(v7QNPVNb[(NP\,`pSk[(\[kQ(d,O0Gp  |
NPS[(v7QRNfVNb[(NP\,`+Sk[(\[kQ~d,O<G ( x\,Ve[(v7Q~NfVNb[(NP\,`+Sk[(\[kQRd,O<G  ( NPS[(vQijV\,`;Sk[(\[kQOK
~d,O<G  qErO&NPV
SkdmEQUY(QZgQZ[(Nb[(NbdVu\R[kY\*t
 Q_WX[kd,Y(o/NfS*WZ\,T7lv[Wv7Q_\[(NfV7l7xh[(v7Q_V3Nb[@NPS*SkQ_V[@[kd~[(v7Q3tgQ_V\,`b[o{d
pR\,Vhe/NPS
V7d,[9WXdV[(NPVT7Q_eRNPVR[(v7Q|Od`P`bd
cNPV7lY(QZgQZ[(Nb[(NbdVS_q?Q_VWXQ,xNP[WZ\,VV7d,[2WXd`f`bQ_WX[9\,VomRd,Y(Q|Y(QZcs\YeSZq)d,Y(Q
Od,YmR\,`P`bo,x7[(vQSk[(\[kQ_SK
;d,O)\,`f`[(v7QyG  S|\YQUNPe7Q_V[(NihQ_eu\,S|\~TVNPT7QS(NPV7wSk[(\[kQUd,OG~
  |q

rO
)
x
(
[
7
v
_
Q

V
P
N

V
_
Q
,
\

W

v
(
Y

d

T
h
V
e


,
d

Y
(
Y
Z
Q

g
Z
Q
(
[
b
N
(
[
b
N

d
j
V


)
x
X
Q
p


g
_
Q
X
W
k
[
_
Q

e

Y
Z
Q
&
c

\

Y
h
e
S

[




WZ\,V{Q
Z

WXd`P`bQ_WX[kQ_e^xh\,Ve/v7Q_VhWXQ[(v7Q\,`PT7Qd,OGp
  |sNPS  t q
&dVhS(NPe7QZY@\~O!d,Ym$T`f\ Z 
q*|v7Q_V/\V7dVpaWv7Q_\[(NPV7lgd`fNPWXoOd,Y[G
  |*v\,SgQZY(Od,YmR\,VWXQ

`bQ_S(Ss[(vh\,Vy
q&*v7Q_\[(NPV7lgd`PNPWZNbQ_S&mR\
ovh\_,Q{QZ[k[kQZY|gQZY(Od,YmR\,VWXQ_SZq;4Q$WZ`P\,NPm[(v\[|Od,Y|\,`P`+
x
[(v7Q|\,`PT7Q*d,O0G~
  @)NPS\[&mEdSk[<  G ;q;|vQ*gY(dd,ONfS;\,V~NPVehTWX[(NbdVdV  q2&dVhS(NPe7QZY}G (  |x
c@vNfWvv\,S)[(v7Q|S(\,mRQ*G\,`PTQs\,S
G  |q;Q_VWXQ,x[(v7Q*\,`PT7Q&d,O1G (  |?NPS;\[mRdSk[|q;S;\,VENPVheTWX[(Nb,Q
vogd,[(v7Q_S(NfSZx`PQZ[&TSs\,S(STmEQ[(v\[Gp
  |2v\,S&G\,`fT7Q\[*mEdS([`Q  G ;q;rV[(v7QNPVheTWX[(Nb,QSk[kQZg^x
c&Q$WXdVS(Nfe7QZY9G~
G  ||O!d`P`Pd
c&Q_e/{oG  |q@S(S(TmEQU[(v\[\~gd`fNPWXo Wv7Q_\[(SdV
(  @xNqQ,q
d,O[(v7Q $\,SS(NblVmEQ_V[(SZq7Y(dm[(v7Qyn[kY\*t
 Q_WX[kd,YNbQ_S[(v\[WXd,Y(Y(Q_SkgdVe[kdu\,V\,SS(NblVmEQ_V[Zx>\[`bQ_\,Sk[
NPS|[kY\ggQ_euOd,Y@Wv7Q_\[(NfV7lTVhe7QZY|\RWvQ_\[(NPV7l~gd`PNfWXo,x\,Ve/\[mRdSk[|
 m~\_oud,{[(\,NPV/Y(QZc&\YeX q
|v7Q_V[(vQEY(QZc&\Ye4d,{[(\,NPV7Q_eNPV[(v7QihYS([YdTVe4NPS\[UmRdSk[y 
 - +     - (10 x?\,Ve[(vQEY(QZc&\YeS
\YNP\{h`bQNPV

~



Y(Q_\,WvS([(\l,Q

Z

]

x7\UOY\,WX[(NbdVd,OmRd,Y(Q[(vh\,V

 x4y

w

V^

f

]

&



w

]

 x4y

f

]

f

f

f

=^



=f|

"

w

f

 x4y

]

&

f

0

w

]

*&

 x4y

f

]

] 

f

f

f

f

]

f



f

f





f



]

U>>





fiC

vh73Qh{.L8$>${3


 -   -


d,{[(\,NPVQ_eNPV/[(v7Q$Od`P`bdc@NPV7l~Y(dTVeS@\Y(QymyT`b[(NPgh`PNbQ_e{o

*

-   -


x{Q_WZ\,ThSkQ$\O!Y\,WX[(NbdV3d,O

d,O

[(v7Q[kY\ kQ_WX[kd,YNbQ_S@\Y(QSkQ_V[*[kdE[(v7QgQ_V\,`b[o{d_qS(NfV7l[(v7QNPVeTWX[(NPdVvogd,[(v7Q_SNPSZxpc2Qd,{h[(\,NPV[(v7Q

G (  |sThVe7QZYm O!d,Y\,V/\Y({hNb[kY\Y(oq
y
 
 j< 
*
G*  _\,`LG7 |k

y
 
 j< 

*
G*  rsv wG 


* 
wG        s*G  

* 
wG 

Od`P`bd
cNPV7lET7ggQZYs{dTVeO!d,Y[(v7QgQZY(Od,YmR\,VWXQd,O

 G

7Z! 7

QIU

( f|






r 
r 




]





f





]















]





f



]

]

T







]

T



Z

|vNfSWXdmEgj`bQZ[kQ_S[(v7QENPVeTWX[(NPdV3Sk[kQZg^qQ_VWXQ,x^c&Q$ghY(d
,Q_e[(v\[Zx>Od,YT

G~ |&NfS|\[@mEdSk[s*  wG;q

[(v7Q\,`PT7Qd,O

]

f

9,Q_V[(T\,`f`bo,xc2Qvh\_,Q1[kdih



[(v7QWXdVhSk[(\,V[(S_q4QWvddSkQ

lT\Y\,V[kQZQ_S[(v\[




w

]

]

x

\,VeO!d,YQZ,QZY(o

 x4y

X

S(TWv[(v\[





.
&

.

mGtm     F.e

t

sS(TWvu[(v\[





- 
q
|vNPS
(

&

QX[Zxc&QUWv7ddS(Q

Gsktt  ~  Q
[(v\[WXdVS(NPSk[(S/d,O
 YQZgQZ[(NP[(NbdVSd,OG @\,S3e7Q_S(WXYNb{Q_e
]

]

G!  |{Q4[(vQ1&
\{d,Q,q
 NPS|`PNPVQ_\Y|NPV[(v7QUVTm${QZY
x;x7d,O)\YNP\{h`PQ_Ssd,O \,Veuv7Q_VWXQU`fNPV7Q_\Y|NPV[(v7Q$`bQ_V7l,[(vd,O x
WZ\,V3{Q[kY\,VhSkO!d,YmEQ_eu[kd 
G !  |sNPVgd`bopV7dmRNf\,`j[(NfmEQ,q|v7Q\{d
,QUQ_Sk[(NPmR\[kQ_S@lT\Y\,V[kQZQU[(v\[
\,`PT7Qd,O"
G !  |2Od,Y Z 

D  wG   t Ds t  [
|v7QYNPlv[tav\,VheSNPe7Qd,O[(vNfS*NPV7Q_T\,`PNb[o~NPSs\E`bd
c&QZY|{dTVeOd,Y@\,VZ
 a\gghY(d_7NPmR\[(NbdVd,O+[(v7Q\,`PT7Q
d,O 
G !  @2O!d,Y Z
qQ_VWXQ,x
x[(v7Q_V 
G !  |&v\,S*\,`PT7Q t  xh\,Vhe
W NPO Z
x[(v7Q_V 
G !  |&v\,S*\,`PT7Q
 tG *s t  q
W NPO Z 
Q_VWXQ,x*\4gd`boVdmRNP\,`aL[(NPmRQ_
 a\ggY(d
pNfmR\[(NbdVd,O[(v7Q/\,`PT7Qud,O 
G !  |S(vd
c@SEcv7QZ[(v7QZY NPSENPV
M^QZ[

f

f

&Q_WZ\,TSkQ

E



\

f

f

f

	

	

w

 x4y

]



]



 x4y

w

E

w

 x4y

f

E

w

 x4y

f

]

]

f

'

q

 x4y
w

&dVhWZ`PTeNPVl7x?`bQZ[C{Q\,Vo1SkQZ[NPV&p@s@q?|v7QZYQRQXpNfSk[(S$\gd`bopV7dmRNP\,`aL[(NfmEQOTVhWX[(NbdV
c@vNfWvmR\ghSQZ,QZYoNPVhSk[(\,VWXQ{d,O[kd3\{dTVhe7Q_e3QZYY(d,YSk[kdpWv\,Sk[(NfWEO!d,Ym$T`f\};o{>

 

QZY(Y(d,Y

]

\,Ve4YQ_eTWXQ_ST[kd

G

\ggYd_7NPmR\[kQ|\,`PT7Q*d,O ! f


w

 x4y

q?Y\,VSkOd,YmkNfV[kd/[(vQ&

xdV7Q@WZ\,V\,VSkc&QZYU


Z

[(NPmEQ,q@vNPS*S(v7dc@S*[(v\["NfS*NPVu|x\,Ve3WXdVSkQ_T7Q_V[(`Po

w

 x4y$#



\,VheRv7Q_VWXQP{

&7@*|q

e2

n

Z

6

(

U>VU



c@Nb[(v

fqSNPV7lu[(v7Q

Z

_a

NPVEgd`bopV7dmRNP\,`

J. ~ \ p ! Lt7ZjXj$t93
kt  R Zb  Uj  |  &p@s

~vy~$yl
(X9k $!

w

G! 







M

iZXk

  

fi 38?>v{v8.^j

>

%)CYj

$33$3



2g

s\,`PWZTh`P\[(NPV7l4[(v7Q3ijVNb[kQXavd,YNbZdVgQZY(Od,YmR\,VWXQ3d,OSk[(\[(NbdV\Y(ogd`PNfWZNbQ_SRNPSRNfV

\ghMTVev7Q_Vw

(././.x7c@vNPWvNPS2\S(T7{>WZ`P\,S(S;d,O[(v7QWZ`f\,S(S&d,O>gd`PoV7dm~NP\,`7[(NPmEQWXdmEghT[(\{h`bQ@OTVWX[(NPdVSZq)|v7Q

QZ[s\,`qbx

Sk[(\[(NbdVh\Y(oEgd`PNPWXoQXpNfSk[kQ_VWXQgY(d,{h`bQ_m

O!d,Ys2S2NfS2S(vd
c@V~[kd${Qav\Ye~{o~)\gh\,ehNPmRNb[kYNPdTR\,Ve

|S(NP[(S(Nbwp`PNPSt_,,xOY(dmc@vNPWvNb[sO!d`P`Pd
c@Ss[(v\[siVeNPV7ly\,Vud,g[(NPm~\,`S([(\[(NbdV\Y(ogd`PNPWXoROd,Y|2S
NPSs;avh\Ye>q9dENb[sNPSsV7d,[*S(T7Y(gYNPS(NPV7l[(vh\[s\ghgY(d_7NPmR\[(NfV7lU[(v7Qd,g[(NfmR\,`gd`fNPWXoRNPSs\,`PSkd;av\Ye>q;4Q
NPVWZ`fTe7Qy[(v7QROd`P`bd
cNPV7l[(v7QZd,Y(Q_m

{Q_WZ\,TS(QRNb[\,`P`bdc@STS[kd/ghY(Q_SkQ_V[dV7QR\,SkgQ_WX[d,Os[(v7QEY(Q_ehTWX[(NbdV



TSkQ_e3NPV[(v7QgY(dd,O+d,O+|vQZd,Y(Q_m

q RNfVuNPSkd`P\[(NbdV^q

8Sx~3y{ 1 pjkXP tk    ujtD  E  fip  R
3 ! |
 k\
(



 L,  jp  $Z

|vQghY(dd,O?S(v7d
cSs[(vNPS&O!d,Y*[(vQWZ\,S(Qd,O?V7dVpaVQZl\[(Nb,QY(QZcs\YeSZ]7[(v7QTVY(Q_Sk[kYNPWX[kQ_eWZ\,SkQOd`P`bdc@S

'qx[(vNPS$S(v7d
cSU[(v\[yijVheNPV7l/\m$Th`b[(Nbgh`PNfWZ\[(Nb,QR\ggY(d
pNfmR\[(NbdV

NPmRmRQ_eNP\[kQ_`bo,q3&o9Y(d,gdS(Nb[(NbdV

S(Wv7Q_mEQOd,Y|[(vNPS|gY(d,{h`bQ_mNPS@\,`PSkdR;avh\Ye>q

:y{~O~q7
~

&dVS(Nfe7QZY@[(v7Qy;aWXdmEgj`bQZ[kQ$ghY(d,{h`bQ_m'&)(*>}*lNb,Q_V\&dd`bQ_\,VWZNbYWZTNb[
 #

o{>



\,VeNPV7ghT[P{xNPS
~

&dd`bQ_\,V4WZNbYWZTNP[\,VeNb[(SNPV7gjT7[@WZ\,V{QyS(QZQ_V4\,S\eNPY(Q_WX[kQ_e\,WXoWZ`PNfW$l,Y\ghv>q9\,Wv


.
_.$d,Y\Y(Q[(v7QNPVghT7[l\[kQ_SZx7cvNPWvY(QZgYQ_SkQ_V[s[(v7Q{hNb[(S&d,O^[(vQ@ipQ_eNPV7ghT[ /[kd[(v7Q
WZNbYWZThNb[Zq)rV7gjT7[2l\[kQ_S@v\_,QNPVhe7QZl,Y(QZQ_p
. q`f` l\[kQ_S@vh\_,QNPVeQZl,Y(QZQyx\,Vhe\,`f`\,Vheu
l\[kQ_S|v\
,QNPVe7QZl,YQZQ[
 q|vQZY(QNfS2dVQl\[kQvh\_pNPV7lydT7[(e7QZl,Y(QZQ`p. q|vhNPS9l\[kQNfS&WZ\,`f`bQ_e[(v7QdT[kghT7[
l\[kQ,xhOY(dmcvNPWv[(vQY(Q_S(T`P[&d,O)[(v7QWXdmRghT7[(\[(NbdVud,O;WZNbYWZTNb[
dV/NfV7ghT7[ WZ\,V/{QYQ_\,e>q
7Ydm
S(ThWv\3WZNbYWZTNb[
x+\,V
G WZ\,V1{Q~WXdVS([kYTWX[kQ_en\,SOd`P`bd
cSZq&Q_WZ\,TS(Q~[(v7Q{h\,S(NPW

V7dpe7QUYQZgY(Q_SkQ_V[(S\l\[kQ,x^\,Ve3QZ,QZY(o3l\[kQEv\,SdV7Qyd,O;[(v7Q$[ogQ_Syxxx d,Yyq|v7Q

l\[kQ_S*d,O^[ogQ

{

~

{

~

NPe7Q_\yd,O?[(v7QWXdVSk[kYThWX[(NbdVuNPS&,QZY(oS(NPmRNP`f\Y2[kdEdV7QS(vd
c@VuNPV4)\gh\,eNfmRNb[kYNbdT|S(NP[(S(Nbwp`PNPSZx_,,x

c&Q`bQ_\
,QdT[9[kQ_WvhVNPWZ\,`e7QZ[(\,NP`PSZq;S2\,VNPVNP[(NP\,`7S(NPmEgj`PNbOoNPVl\,S(STmEg[(NbdV^x\,S(S(TmRQ@[(v\[&[(v7QWZNPYWZTNb[
v\,SV7dzl\[kQ_S_qU9\,Wvl\[kQyd,O2[(v7QWZNbYWZTNb[{Q_WXdmEQ_S\S([(\[kQEd,O[(v7QE@q@v7QyS([(\Y([Sk[(\[kQ

G

NPS@[(v7QUdT7[kgjT7[|l\[kQ,q@QZ,QZYSkQy\,`P`^Q_e7l,Q_Sd,O)[(v7QyWZNbYWZTNb[Zq*Q_VWXQ,x\R[kY\,VSNb[(NbdV3NfV
l\[kQENPV

`bQ_\,eS@O!Y(dm\

[kddV7Qyd,ONb[(S@ghY(Q_e7Q_WXQ_S(Skd,YSZq[kY\,VhS(Nb[(NbdV3O!Y(dm\,V4l\[kQeQZgQ_VheS|dV[(v7Q\,WX[(NbdV
~

i.Nb[(S`bQZO[gY(Q_e7Q_WXQ_S(S(d,YNPSY(Q_\,Wv7Q_e^x\,VedV4\,WX[(NbdVENb[(SYNblv[

\,VeNPSe7QZ[kQZYmRNPVNPS([(NPWqUV\,WX[(NbdV
gY(Q_eQ_WXQ_S(Skd,Y9NPSY(Q_\,Wv7Q_e^q)
dV4[(v7QR\,WX[(NPdV>qE

[kY\,VhS(Nb[(NbdVyOY(dm\,VRl\[kQNPS;gYd,{h\{hNP`PNfSk[(NPW\,Ve~e7dQ_S9V7d,[9e7QZgQ_Ve

NP[(v4gY(d,{h\{jNP`PNb[o

[(v7QYNblv[*ghY(Q_e7Q_WXQ_S(Skd,YNPSsY(Q_\,WvQ_e>q

-

(

[(v7QR`bQZO[gY(Q_e7Q_WXQ_S(S(d,YUNPSY(Q_\,Wv7Q_e>x?\,Vhe4c@Nb[(vgY(d,{j\{hNP`PNb[o

-

(

&dV[(NPVT7Q/WXdVS(Nfe7QZYNPV7l\1WZNbYWZTNb[c@Nb[(v7dT[l\[kQ_S_qrO\,VNfV7ghT7[l\[kQ/c@NP[(vG\,`PTQ1/NPS

.RNPS|YQ_\,Wv7Q_e>xV7d
Y(QZcs\YeRNPSl\,NPV7Q_e>xcvNPWv~mR\w,Q_S[(v7Q@[kd,[(\,`hQXpgQ_WX[kQ_eRY(QZcs\YeRV7d,[(NfWXQ_\{h`boES(mR\,`P`PQZY;[(v\,VEd,[(v7QZY(cNPSkQ,q
rO
 > 
x[(vQ_VE[(v7Q|\,WX[(NbdVS9WZ\,VE{QsWv7dSkQ_VR\[;[(vQl\[kQ_S2Skd[(v\[;QZ,QZY(o[kY\*kQ_WX[kd,Y(oY(Q_\,WvQ_S
Y(Q_\,Wv7Q_e>xj\`P\Y(l,Q$gdSNb[(Nb,QY(QZcs\Ye3NfS*l\,NPV7Q_e>xj\,Ve3NbO)\,VNPV7ghT7[sl\[kQyc@Nb[(vu\,`PT7Q
~

o{

\,VNPV7ghT7[*l\[kQyc@Nb[(v3G\,`fT7Q~]NbO)[(vNPS@WXdVheNb[(NbdV/vd`PeSZx[(vQ_VNb[m$TS([@{Q[(v\[

~

[(v7QU

o{> 

q@Q_VWXQ,x

vh\,S|\E`P\Y(l,QgdS(NP[(Nb,QG\,`PTQNbO?\,Ve/dV`boNbO ~ o{ 
q
rO9[(v7QRWZNPYWZTNb[v\,Sl\[kQ_SZx+c2QRV7QZQ_e[kduYQ_mEQ_mU{QZY[(v7QEgj\YNb[od,O2[(vQRVTm${QZYd,O2

*

l\[kQ_SdVQ_\,Wv4[kY\ kQ_WX[kd,Y(o,qrO2[(v7Qgh\YNb[oNPSQZ,Q_V>xQZ,QZY(o[(vhNPV7ll,dQ_SU\,Se7Q_S(WXYNP{Q_e\{d
,Q,qyrO2[(v7Q
gh\YNP[oRNPS2dehe>x[(v7Q_V[(v7QY(d`bQd,O\,Vhe

.

l\[kQ_S|NfS9Skc@NP[(Wv7Q_e>xp\,Ve[(v7QY(d`bQ@d,O $\,Ve|l\[kQ_S



NPSS(c@Nb[(WvQ_e>q?rO\l\[kQNPSY(Q_\,Wv7Q_e^x[(v7Q*gj\YNb[oy{hNb[NfS jNbggQ_e>q?7d,Y9QZ,QZY(oEl\[kQNPV[(v7Q@WZNPYWZTNb[Zx
c&QV7dc[(\w,Q[c&dSk[(\[kQ_SZ}9dV7QOd,YsQZ,Q_V\,VedV7QOd,YsdeheRgh\YNP[o,qQ_VWXQ,xNbO,+zNPS2[(v7QS(QZ[&d,O
l\[kQ_SNPV
~

x7[(v7Q

] .eI_

vh\,S@Sk[(\[kQ_S-+





qs|v7QSk[(\[kQ$[kY\,VS(Nb[(NPdVOTVhWX[(NbdVuNPS

U>

fi

C

vh73Qh{.L8$>${3

.//
//
//

( x

//
-

//


NbO NPS\,Vl\[kQ\,Vheu
\,Ve

//

RZkKrIQIUgqI
K h I h k

K
.d,YKNPS\,Vl\[kQ

h
h





,
x



x
,
\

V

e
K
P
N
*
S

g
YQ_e7Q_WXQ_S(Skd,YJgd,OK]


NbOKNPS\,Vl\[kQ\,Vheu 

d,YKNPS\,Vl\[kQ
\,Ve  
. x,   h x\,VeK h NPS*gYQ_e7Q_WXQ_S(Skd,YJ.Rd,OK]
NbO
K NPS\,Vl\[kQ\,Vheu  
d,YKNPS\,Vl\[kQ
\,Ve  
. x,  
!h x\,VeKhNPS*gYQ_e7Q_WXQ_S(Skd,YUd,OK]
NbOy
K NPS\nl\[kQ4\,VeBK h NfS\gY(Q_e7Q_WXQ_SSkd,Yd,OsK\,Ve
qh 
u
 ?]
NbO+
K NPS&\,VNfV7ghT7[l\[kQd,Y&[(v7QS(NPV7wESk[(\[kQ,x\,VeKDh NPS9[(vQS(NPVw

x

//

0

//
-

//
//
//

( x

x

//
//
//
//
//1

x

Sk[(\[kQ,q

  2 

d
cc&Q|v\
,Q*[kdSkgQ_WZNbO!o[(vQ*Y(QZc&\YeOTVhWX[(NbdV>qrOj\,VENfV7ghT7[?l\[kQ*c@Nb[(vyG\,`fT7Q&NfS?Q_VhWXdTV[kQZYQ_e

*



dV1\[kY\ kQ_WX[kd,Y(oc@v7QZYQy[(vQEgh\YNb[o3d,Osl\[kQ_S$NPSQZ,Q_V>x[(vQ_VY(QZc&\Ye



( NPSd,{[(\,NfV7Q_e>x

s NfS;[(v7Q@S(NPZQ@d,O>WZNbYWZTNP[ q;|v7Q|S(\,mRQ@Y(QZcs\Ye~NfS;d,{[(\,NPV7Q_eNbO\,V~NPV7gjT7[)l\[kQc@NP[(vEG\,`fT7QJ.
*
l\[kQ_S|NPS&dpee>q;`f`hd,[(v7QZY*[kY\*tQ_WX[kd,YNPQ_S
d,{[(\,NPV/Y(QZc&\Yep
.q
|vTSQ_\,Wv1[kY\*t
 Q_WX[kd,YoY(Q_WXQ_NP,Q_SY(QZcs\YeQ_Nb[(v7QZY_.d,Yy  2  ( q$|v7QZY(Q\Y(QR\[mEdSk[y  2 [kY\Ga
c@v7QZYQ

~

~

NPS&Q_VWXdTV[kQZY(Q_edV\y[kY\ kQ_WX[kd,Y(ocv7QZY(Q[(v7Qgh\YNb[oRd,O?





kQ_WX[kd,YNbQ_SO!d,YQ_\,Wvgd`PNPWXo,q*rO\Rgd`PNfWXoWv7ddSkQ_S[(v7Q$WXd,Y(YQ_WX[\,`PT7Q_S@O!d,Y\,`P`[(v7Q$l\[kQ_SNfVud,Ye7QZY[kd
gY(d,Q~[(v\[
 > 
O|\,V1d,gh[(NPmR\,`
 ( q[(x)v7NPQZV4Y(cd,[(NPSkv7Q,QZxYy[(v7c2Q$d,YQXeSUgQ_NbO WX[kQ_e >G \,`fT7QUNPx)S[(\v7[Q_V1`bQ_\,[(vS([9Q~ QX p2g Q_ WX[kQ_( e( G 2\,`f T7BQR/d,3
gd`PNPWXoNPS9  2

`bd
c&QZY[(v\,V


(
(
  2 xjNqQ,qbxh\[@mEdS([J  2 / q
|vTSZxNPO@\,V\ggY(d
pNfmR\[(NbdV\,`bl,d,YNb[(vhm
 d,O@[(vQd,gh[(NPmR\,`
 ( NfS$ cd,NbY[(vNPV  \,2V \,e(eNb[( Nb,qQ&WXodVNPVSkSk[(g\,VQ_WX[O[(Nb
gd`PNPWXo,x
NP[^c@NP`P`Q_Nb[(v7QZYlNb,Q9\*\,`PT7Q  2
dVd,O[(v7QdT[kghT7[Zx
dV7QWZ\,VNPmRmEQ_eNf\[kQ_`boRe7QZ[kQZYmRNfV7Q@c@vQZ[(v7QZY
  
q2|vThSZx\,Vo
 a\,eehNb[(Nb,Q\ghgY(d_7NPmR\[(NPdV~Od,Y
~





~

o{



o{

















~





o{

[(vNPSsghY(d,{h`bQ_mmyTSk[*[(\w,Qy\[@`bQ_\,Sk[|gd`bopV7dmRNf\,`j[(NfmEQ,q

rtV+NPlT7Y(Q*x\,VQXp\,mEgj`bQ&WZNPYWZTNb[+\,Vey[(v7Q*4[kdc@vNfWvyNb[+NfS?[kY\,VSkOd,YmEQ_ey\Y(QslNb,Q_V>q;,QZY(o
l\[kQd,O*[(v7QWZNPYWZTNb[UNfS[kY\,VS(O!d,YmRQ_e[kd[c&dS([(\[kQ_Sd,O*[(v7Q|}+dV7QWXd,go1Od,YZXgj\YNb[o4d,O

*

l\[kQ_S2gh\,S(SkQ_e~dVE[(v\[9[kY\ tQ_WX[kd,Y(o3NfVeNPWZ\[kQ_e{oE\[(vNPVdT7[(`fNPV7Q*d,O[(v7Q@Sk[(\[kQ\,VeEdV7QWXd,go

*

Od,Yy_gh\YNb[od,Ol\[kQ_Sgh\,S(S(Q_e/dV/[(vh\[[kY\ kQ_WX[kd,Y(oNPVheNPWZ\[kQ_e/{ou\R[(vNfW(wudT7[(`PNfV7Qd,O;[(v7Q
Sk[(\[kQqS(d`PNPe3\YY(d
cNPVehNPWZ\[kQ_S@[(vQ$dT7[(WXdmRQyd,O2\,WX[(NbdVkWv7ddSkQE[(v7Q`bQZO[@gY(Q_eQ_WXQ_S(Skd,Ypx>\,Ve\
e\,S(vQ_ey\YY(d
cNPVheNPWZ\[kQ_S?[(v7QsdT7[(WXdmEQ*d,Oj\,WX[(NPdV4kWv7ddSkQ|[(v7Q&YNblv[)gY(Q_e7Q_WXQ_SSkd,Ypq;d,[k[kQ_eR\Y(Y(dc@S
NPVehNPWZ\[kQR\/[kY\,VhS(Nb[(NbdVc@NP[(v4gY(d,{h\{jNP`PNb[o

-

(

dV\,Vo4\,WX[(NbdV>q|v7QWZNbYWZTNb[NfV1)NblT7Y(Q~3v\,S\,`PT7Q

*

q||v7QUgd`fNPWXo,xhc@vhNPWv/Wv7ddSkQ_S[(v7QUYNPlv[|ghY(Q_e7Q_WXQ_S(Skd,YNPVu[(vQ$Sk[(\Y([(NfV7l~Sk[(\[kQ,xopNbQ_`PeS*[kY\ tQ_WX[kd,YNPQ_S
c@vNfWvu\,`P`Q_VeuNfVu\,V/NPV7ghT[2l\[kQ$c@Nb[(v\,`PT7QE\,Vec@vhNPWv[(v7QZYQZO!d,Y(Qd,{h[(\,NPVS*[(v7Qd,gh[(NPmR\,`\,`PT7Q,q
M

|vQZY(Qv\
,Q{QZQ_VSkQZ,QZY\,`Y(Q_WXQ_V[\gghY(d_7NPmR\[(NbdV\,`bl,d,YNb[(vm~S/NPV[kY(dehTWXQ_eOd,YSk[kYThWX[(T7Y(Q_e
2SZx^mR\,Vo/d,O9cvNPWv\Y(QES(T7Y,QZo,Q_eNfV&dT7[(Nf`PNbQZYQZ[\,`Lqbx)_,,qy3d,YQyYQ_WXQ_V[c&d,Y(w3NPVhWZ`PTe7Q_S

$d`P`bQZY\,Ve/)\Y(Yy(././.@\,Vev7Q_T7YNPS([(NPWSkQ_\YWvNPVu[(v7Q$Skgh\,WXQUd,O
(././. &\,VeO$ NPmQZ[s\,`qj(././. q2 vhNP`bQ|[(v7Q_S(Q\,`bl,d,YNP[(vmRS

\E\YNP\,V[|d,O)gd`PNPWXoNb[kQZY\[(NbdVu{o

ijVNP[kQ@WXdV[kYd`P`bQZYS9{oR\,VSkQ_V\,VeQ_V7l

\Y(Qd,O![kQ_VvhNblv`boQXQ_WX[(NP,Q/NPVYQ_eTWZNPV7l3[(v7Q\,S(omEgh[kd,[(NPWWXdmEgh`bQX7Nb[o1\,Vhen\,WX[(Th\,`2YTV1[(NPmRQ_S$d,O
gd`PNPWXo~WXdVhSk[kYTWX[(NbdV^x7[(v7QZo\,`f`YTVNPV[(NPmEQQXpgdV7Q_V[(Nf\,`>NPV[(v7QS(NbZQd,O+[(v7QUSk[kYTWX[(T7Y(Q_eY(QZgYQ_SkQ_Vpa
[(\[(NbdV>xd,YdQZYdVh`bo/c&Q_\wgQZY(Od,YmR\,VWXQlT\Y\,V[kQZQ_SZq4QRS(vd
c[(v\[QXpgdV7Q_V[(Nf\,`;\,SkomRg[kd,[(NPW

S(WvQ_mEQ[(v\[$ghY(dehTWXQ_Sy_
 a\ggY(d
pNPm~\[(NbdVS$O!d,Y\,`P`
q7d,Y[(vNPSZxjc2QWXdVS(Nfe7QZY9SY(QZgY(Q_S(Q_V[kQ_e{oX|sSE&dT7[(NP`PNPQZY_xjQ_\Ye7Q_V^xd`PeS(_mRNPe7[Zx
WXdmEgh`PQXpNb[o1NPSyV7Q_WXQ_S(S(\Y(oO!d,Y\,Vo\,`bl,d,YNb[(vm

_,,qV[(NP`Vd
cxjc2Qv\_,Qe7Q_S(WXYNb{Q_eu[(v7Q$S([(\[kQy[kY\,VS(Nb[(NbdV/OTVhWX[(NbdV/Od,Y9S{o/\~OThVWX[(NbdV

U>

fi 38?>v{v8.^j

3
3 3 33 3 3
33 3 3 3 3  3 3 3 3 3 3 3 3 3 3 3 3
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3
3 333333333
333333 3333
3
3
3
33333333
3
3
3
3
33333 333
33333
3
3
3
3
3
3
33333 33
3
3
3
3
3
3
3
3
3
3
3
33 3 3 3 3  3 3 3 3 3 3 3
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
3
3
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 33 3 3 3 3 3 3 3
3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3
3333333333333333
3 33 3
3 333 3
3333333333333333
33
3
3333333333333333
333
3 3 3 3 3 3 3 3 3 3 3 3 3333 3
33 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3
33 3 3 3 3 3 3 3 3 3 3 3 3 3
3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3
3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
3 33 3 3 3 3 3 3 3 3 3 3 3 3 3
3 3 3 3 3 3 3 3 3 3 3 3 3333 3 3
3
3
3
3
33333333
3
333
3
3333
333333333 3 3333333333333
333
33333333
3

.



.

5

$33$3

5
5

7
4

4

7

7

5

5

4

7



8

6


5
6
6



6

*

)NblT7Y(QU}WZNbYWZTNP[Zx[(v7QNb[&NfS9Y(Q_ehTWXQ_e~[kd7x\,Vhe~[(v7Q[kY\ kQ_WX[kd,YNbQ_S*\,WZWXd,YeNPV7ly[kdE\,Vd,gh[(NPmR\,`
gd`fNPWXo~Od,Y|[(v7Q

RZK\IUg
ILKhf[(v\[EWXdmEghT7[kQ_Sy[(v7QgY(d,{j\{hNP`PNb[od,O|Y(Q_\,WvhNPV7lSk[(\[kQKhOY(dm Sk[(\[kQKuTVhe7QZY$\,WX[(NbdVwgjq
4Q\,S(S(TmEQ_e3[(v\[@[(vQU[kY\,VSNb[(NbdVuOTVWX[(NbdV3c&\,SY(QZgYQ_SkQ_V[kQ_eQXpgh`PNPWZNb[(`Po,q29fq  Rpk
   Zy |sNfS\S(ThWZWZNPVWX[Y(QZgY(Q_S(Q_V[(\[(NbdVd,O&\,V4d,Ys@q2\,Wv4Sk[(\[kQEd,O2[(v7Q

SkopSk[kQ_m
NPSeQ_S(WXYNb{Q_e{o\u,Q_WX[kd,Y$d,O&\,`PT7Q_SWZ\,`f`bQ_e 2hZh  qd,[kQR[(v\[$NbO9Q_\,Wv1d,Os~j
 Sk[(\[kQ_SZqfi4WX[(NbdVhS\YQeQ_S(WXYNb{Q_e/{o/[(v7QQXQ_WX[[( v7T7QZoQ_Vv[(S\
,NPQ S
[c&daL\,`PT7Q_e>x>[(v7Q_V[(v7QSkopSk[kQ_m
v\,S_
dV4Q_\,Wvj
 T7Q_V[{omEQ_\,VhSd,O&[c&d3e\[(\/Sk[kYTWX[(T7Y(Q_SZqR@v7QZo\YQR\/e7QZgQ_Ve7Q_VWXol,Y\ghv4\,Ve\uSkQZ[
}9

d,O|OTVWX[(NbdVhSQ_VWXdpe7Q_e\,SWXdVeNb[(NbdVh\,`gY(d,{j\{hNP`PNb[o[(\{h`bQ_SZx;e7Q_WZNPSNbdV[kYQZQ_SZx\YNb[(vmRQZ[(NPW~e7Q_WZNfS(NbdV
eNP\l,Y\,mRSZx7d,Y@NPVuS(dmEQd,[(v7QZYe\[(\RSk[kYThWX[(T7Y(Q,q

|vQUe7QZgQ_Ve7Q_VWXol,Y\ghvNPS|\eNbY(Q_WX[kQ_e\,WXoWZ`PNfWl,Y\ghv3c@Nb[(v/Vde7Q_S|gh\Y([(Nb[(NbdVQ_e/NPV[kd~[c&dSkQZ[(S

3

( IDI (
|3

\,Ve

fih( IDI fih
3

|3

Rxs[(v7Q

q@v7QuihYS([RSkQZ[~d,OV7dpe7Q_SRYQZgY(Q_SkQ_V[(SR[(v7QSk[(\[kQ\[[(NPmEQ

mR q;|v7QQ_e7l,Q_S2\YQ@OY(dm[(v7Q@ihYSk[2SkQZ[9d,O^V7dp e7 Q_S9[kdU[(v7QSkQ_WXdVe\,SkoVhWv7Y(dVdTS
m j T7Q_V[+\[;[(NfmEQR /&ThVe7QZY)\,WX[(NbdV
gEe7QZgQ_VeS9gY(d,{h\{jNP`PNPSk[(NfWZ\,`P`bodV[(v7Q@\,`PT7Q_S2d,O>[(v7QgY(Q_e7Q_WXQ_SSkd,YS2d,O rh NfVR[(vNPS9l,Y\ghv>q|d,[kQ[(v\[




SkQ_WXdVe\[9[(NfmEQ







d,Y)c@Nb[(vNfV[(v7Q*SkQ_WXdVeSkQZ[|SkopVWv7Y(dV7dTSq)@v7Q9\,`PT7Q&d,Oh[(v7Q



3

[(v7QSkopVWvY(dV7dTS2Q_e7l,Q_S*myTSk[&O!d,Ym\eNbYQ_WX[kQ_e>x\,WXopWZ`PNPWl,Y\ghvNPVd,Ye7QZYsOd,Ys[(v7Qe7QZgQ_Ve7Q_VWZNbQ_S9[kd

{QQZG\,`PTh\[kQ_e>qfiy|v7QgY(d,{j\{hNP`PNb[(NPQ_S9\Y(QSkgQ_`P`PQ_eRdT7[Zx7Od,Y*Q_\,Wv\,WX[(NbdV>xNPV[(v7QWXd,Y(Y(Q_S(gdVheNPV7lye\[(\

h

Xghq4QUc@NP`P`NPVehNPWZ\[kQ[(v\[$Sk[kdpWv\,S([(NPW
sOTVWX[(NbdV{o (q
` |sSZqrtV3O\,WX[Zx>[(v7Q$ijV\,`
|sWXdVhSk[kYTWX[kQ_e/NfV~[(v7QgYdd,O^d,O+|v7QZd,Y(Q_mq vh\,Ss,QZY(o`P\Yl,QYQZc&\YehS2cvNPWv\Y(QWXdmEgjT7[kQ_e
NPmEgj`PNPWZNb[(`bo]sNPV[(NPmEQ/gd`bopV7dmRNf\,`&NfV[(v7Q3S(NPZQ/d,O[(v7Q
 |s$x9dV7Q3WZ\,V QXpgh`PNPWZNb[(`PoWXdmEghT7[kQ\,Vo
NPVehNbNfeT\,`^  d,O^[(vQY(QZcs\Ye>q@vNPS9vh\,S2[(vQQXQ_WX[*d,OmR\wpNPV7l$[(v7Qgd,[kQ_V[(NP\,`+Ghd,O^[(vQ[
 |s
Sk[kYThWX[(T7Y(QOd,YP3

\,Ve



4QEmR\w,QRV7d\,S(S(ThmEg[(NbdVS\{dT7[[(vQySk[kYTWX[(T7Y(Qyd,OY(QZcs\YeSO!d,Y

[kdd~`P\Y(l,Q[kd~c|YNb[kQe7dc@Vuc@Nb[(vgd`bopV7dmRNf\,`P`boRmR\,Vo{hNb[(SZq

8Sx~3y{ 132
:y{~O~q7

 

u

pjkZb t`t    ,Rt  R,  fi1,/  E,
:9(k  XjL,  , f 9@tptr
(



33

(

@v7Ql,Q_V7QZY\,`Sk[kY\[kQZl,oNPSSNPmRNP`P\Y[kdu[(v7QEgY(dd,Od,Os|v7QZd,Y(Q_m

 L,  jE7  Z

qPqy4QRlNb,QE\Y(Q_ehTWX[(NbdV

|s@a

OY(dm[(v7Q;aWXdmEgh`PQZ[kQS(TWZWZNPVWX[sWZNbYWZThNb[2\,`PT7QgY(d,{j`bQ_m[kdy[(vQghY(d,{h`bQ_mOd,Ys9S*NPV

Y(QZgYQ_SkQ_V[(\[(NPdV>q;VRNfVSk[(\,VWXQsd,O[(v7Q@S(TWZWZNfVWX[;WZNbYWZTNb[)\,`PT7QsgY(d,{j`bQ_mNPS;\&dd`PQ_\,V~WZNbYWZThNb[<;[(v\[
e7Q_S(WXYNb{Q_S&\~WZNbYWZTNb[
~



\,Vheu\,V/NPV7ghT[{?xNqQ,q=;e7Q_S(WXYNb{Q_S&\,V3NPVSk[(\,VWXQd,O)[(v7Q/ j\[(~WZNbYWZTNb[&\,`PT7Q

U>_

fiC

vh73Qh{.L8$>${3


gY(d,{j`bQ_muq)4QWZ\,Vu\,S(S(TmEQ[(v\[*NPV
|v7Q_V3QZ,QZY(o3l\[kQENPV

~

xpQ_\,Wvul\[kQNPSs\ygY(Q_e7Q_WXQ_S(S(d,Y&[kdR\[*mEdS([&[c2dEd,[(vQZY*l\[kQ_SZq *

v\,S@OdT7YV7Q_Nblv{d,YSZx[c2dd,Oc@vhNPWv3dT7[kghT7[[(v7QyNfV7ghT7[*[kd
~

c@vNfWvl,QZ[|[(v7QdT7[kgjT7[sd,O

~

~

x\,Ve[c&dd,O

\,S|NfV7ghT7[NbO[(vQZY(Q\Y(QO!QZc&QZY@V7Q_NPlv{d,YSZx[(v7QmRNfS(S(NPV7lV7Q_Nblv{d,YSs\Y(Q

9.qsdVS(NPe7QZY)\l\[kQm>d,O q;7\_oU[(vh\[;[(v7QsdT7[kghT7[)d,OjVQ_Nblv{d,YS.\,VeusNPS
[(v7QNPV7ghT[[kdl\[kQxp\,Vhe~[(v7QdT7[kghT[2d,O^l\[kQ[;
 NPS&NPV7ghT[[kdEV7Q_Nblv{d,YS$\,Ve'q@dcxp[(v7QWZNPYWZTNb[
; dV1NfV7ghT7[EUILdT[kghT7[(SR>/IL
K x+c@v7QZY(QEl\[kQ?/NPS[(v7Q   V7Q_Nblv{d,Yd,O&l\[kQ x?\,VeiK~Q_VWXdpe7Q_S
[(v7Q[ogQ$d,O2l\[kQ
 yx?xx&p. x^\,Vhen
q@v7QyNfe7Q_\NfS[kduWXdVS([kYTWX[O!Y(dm \,V
G \,S@NPV[(v7QgY(dd,Od,O?@v7QZd,Y(Q_mqPq9d
c&QZ,QZY_xhc&Qe7dENb[*S(TWZWZNfVWX[(`bo,qQ_VWXQ,x7c2QWXdVhSk[kYTWX[sOY(dm
;\s|s@aLY(QZgY(Q_S(Q_V[(\[(NbdVd,O^\,Vw
G ;q9|vQ@\,WX[(NbdVS&d,O+
G ;\YQ$. \,VexOd,Y&Wv7ddSNPV7l
V7Q_Nblv{d,Y`u
. d,Os[(v7Q~WZT7YY(Q_V[$Sk[(\[kQXaLl\[kQ,xd,YY(Q_S(gQ_WX[(NP,Q_`bo,x?V7Q_NPlv{d,Y~q|v7QRSk[(\[kQ_S$d,O
G ;\Y(Q
[(T7gh`PQ_SUI 6INRSIN
V 9c@v7QZY(Q9) NfS2\$l\[kQd,O xuNfS[(v7Qgh\YNb[oE{hNb[ \,SsNPV~[(v7QghY(dd,O>d,O^@v7QZd,Y(Q_mqPx
RNPS$[(v7Q[ogQd,O|l\[kQ( x\,Ve3
V NPS$TSkQ_eO!d,Y\3Y\,Ve7dm {hNb[Zq39,QZY(o1l\[kQuVThmU{QZYy NPS$lNb,Q_VnNPV
{hNPVh\Y(o1TS(NPVlu
S(\_o
|{hNP[(SZq1|v7Q_V>x)[(v7Q
 |sv\,S z'Xj T7Q_V[(Ss ( IN - IIN @tI 6INRSINV q1MQZ[
I( - IDI A@I CBI vNI {Q[(v7QuSk[kdWv\,Sk[(NPWOTVWX[(NbdVhS$[(vh\[WZ\,`fWZT`P\[kQ"h( IN"h- IINth@ I qhINRhINV(h q1|v7Q
S(NPmRgh`bQ_Sk[*NPS
Od,Y@[(v7Q_
 T7Q_V[JV h [(v\[NPS|TSkQ_e3\,S|Y\,Vedm{hNb[|NbO+OY(dm Sk[(\[kQ   ( U @?[(v7QUV7QXp[
Sk[(\[kQNPS@Wv7dSkQ_VTVNbOd,YmR`Po~\[Y\,Ve7dmOY(dmdVQUd,O;[(v7Q$[c&dgYQ_e7Q_WXQ_S(Skd,YS@d,O;l\[kQs
 NPV q*|vNPS
v\ggQ_VSNbO2[(v7QE[ogQ@
R d,O&l\[kQ @ NfS\,Ve[(v7QEgh\YNP[oNPS_p. x^d,YUNbOR NfS \,VeNPSEqErtV
[(v7Q_SkQWZ\,SkQ_S_xV h e7QZ[kQZYm~NPV7Q_SsNb[(S&G\,`fT7Q_$
. d,Y{o; NbgghNPVlU\WXdNPV>q2[(v7QZYc@NPSkQ,xV h Q_T\,`PSq9d,[(NPWXQ
[(v\[mV(
h NPS|NPVhe7QZgQ_Ve7Q_V[2d,O+[(v7QU\,WX[(NbdV^q
|vQOTVWX[(NPdVS
Od,Y*[(v7Q[j
 T7Q_V[(S h eQZ[kQZYmRNPV7Q[(v7Q{hNP[(S2d,O+[(v7QV7QXp[*Sk[(\[kQ_SZq9rO6&R NPSs\,Vu
\,Ve[(v7Qgh\YNP[oENPSQZ,Q_V>x>[(vQ_VtY\,Ve7dmR`Po\gY(Q_e7Q_WXQ_S(S(d,Yd,O9l\[kQ|
 NPSWvdSkQ_V>q3k\,Ve7dmR`bop
mEQ_\,VSv7QZYQ~[(v\[[(v7QY\,Ve7dm{hNb[V h e7QZ[kQZYmRNPVQ_Sc@v7QZ[(vQZYyghY(Q_e7Q_WXQ_S(Skd,Y
. d,YyghY(Q_e7Q_WXQ_S(Skd,YuNPS


h NPSu[(vQ {hNb[ud,OD7xc@vQZY(Q>/ILK NPSu[(vQ4dT7[kghT7[/d,OE;dVNPV7ghT7[4UINV h q
WvdSkQ_V>qQ_VWXQ,x
WZWXd,YeNPV7l`Po,xrR h  
K NPS[(v7Q@[ogQ|d,O>[(v7QWv7dSkQ_Vl\[kQ,x7\,Ve h  Y(Q_mR\,NPVhSTVWv\,V7l,Q_e>q;|v7QS(\,mEQ
v\ggQ_VS*NPO6*
R NPS*\,V\,Veu[(vQgh\YNb[oUNPS*dpee>q;rOsR NPS|\~x7[(v7QZY(QUNPSsdVh`bodVQgY(Q_e7Q_WXQ_SSkd,Y
d,O0(
 x\,VeR[(v\[2dV7QmyTSk[9{QWv7dSkQ_VOd,Y"h \,Ve Rh q;|v7Q|gj\YNb[o{hNP[qh NPSj NPggQ_e[kd+$ q;rO
R NfS\,V
 \,VeR[(vQ@gh\YNP[oNPS9QZ,Q_V>x[(v7Q_VRdV\,WX[(NbdVOg Z
.e_I  xp[(v7QgY(Q_e7Q_WXQ_SSkd,Yg d,O>l\[kQ+ NfSWv7dS(Q_V>q


Q_VWXQ,x rh NPS[(v7Q
{hNb[d,OF7x)c@vQZY(Q>/IL
K NfS[(v7QRdT[kghT7[d,OG;dVNPV7gjT7[RUIUpg q/WZWXd,YeNfV7l`bo,x
R h  EK \,Ve h  q@v7QyS\,mEQEv\ggQ_VSNbO}@R NPS\,V\,Ve[(v7Qygh\YNb[oR1NPSdpee>q@Q_VhWXQ,x^[(v7Q
SkQZ[)[kd\ijWX[(Nb[(NbdThS?l\[kQ

~

~

~











I





D





~



3

-3










,

[3



OTVWX[(NbdV

WZ\,Vu{QWZ\,`PWZTh`P\[kQ_e/\,S|Od`P`bdc@SZq



z'#}

7  R

UIINRQINV(hIUg

. R
}x#Z
UIUg \ILK
Z|Bz7 R
R
}x#Z
UINV h /ILK
Z|Bz7 R  
}x#Z WZ\,`PWZT`f\[kQ;*UIU
.   >/ILK
Z|B  
  {hNP[*d,O$
~}} [(v7Q
z








\,Ve



 &d,Yy  \,Vhe 
WZ\,`PWZT`f\[kQ;*
7 
> G]
  \,VeE 

&d,Y  \,Ve 
WZ\,`PWZT`f\[k
Q ;*
 
>  

.






|vQS([(\[kQ



NPS\uS(NfV7w3Sk[(\[kQRcvNPWvNPSY(Q_\,Wv7Q_e4OY(dm[(v7QRNPVghT7[l\[kQ_Sc@Nb[(vNfV3dV7QRS([kQZg\,Ve

_R h d,O)[(v7QUVQX[|Sk[(\[kQ$d,Y|l\[kQ$NfS|WZ\,`PWZT`P\[kQ_e3\,WZWXd,YeNPV7l`Po,q
H q/"*&1vL&"*6QS*Qv:I
"*mKJDvjQ*L*Q"6QL"Q1Q<"*L"*"v1<Qv++"
mMJD*bSDL*L"Qv_<LSQN I9U"SNP OvQ I-RTS U:S rvt`mvv""D"Q_+vLK V\
"vQD*"L"S*WJ*mQqQJQeS*LQQ""v

c@vNfWvuNPS|VQZ,QZY@`bQZO[Zq9|v7Q[ogQ

*)

U>

fi 38?>v{v8.^j

$33$3

(

VQUWZ\,V3\,`PS(dRS(NPmyT`P\[kQ[(v7QUWZNPYWZTNb[G;O!d,Y|OTVWX[(NPdVK $NfV[(v7QU\{d
,Qy\,`bl,d,YNb[(vm{ou\
d,[kQ[(v\[ZxNPVl,Q_V7QZY\,`x|WZNbYWZTNP[(S~WZ\,V

v\_,Q4mEd,Y(Q[(v\,V

|s$q

dV7Q3dT7[kgjT7[Zq4QWXdVS(NPe7QZY[(vNPSmEd,Y(Q

l,Q_V7QZY\,`mEdeQ_`^v7QZY(Q,q

YX ,X_P(
/  )  ~D

wS3z'



_       ,u  pb,(RZ$

 ~7fij

:y{~O~q7

(
u

Z9  LU  

S

   3k  X ~(

0h

4Q3Skw,QZ[(Wv[(v7QuWXdVSk[kYTWX[(NbdVNPeQ_\pqnMQZ[E[{Qu\4WZNbYWZThNb[$cNb[(vnNfV7ghT7[yl\[kQ_S~\,Ve

I*I L

dT7[kghT[l\[kQ_SZqu|v7QEdT7[(WXdmRQ~d,O*[(v7Q~WZNPYWZTNb[dV\,Vo1NPV7gjT7[ (
 NPSThS(T\,`P`boWZ\,`PWZT`P\[kQ_e\,S
Od`P`bd
cSZqRs[ihYS([ZxWZ\,`PWZT`P\[kQ~[(v7QEdT7[(WXdmEQd,O&\,`P`)l\[kQ_S$[(v\[l,QZ[$NPV7ghT7[dV`Po3OY(dm
NPV7gjT7[l\[kQ_S_q
QX[ZxWZ\,`PWZT`P\[kQy[(v7Q$dT7[(WXdmEQyd,O\,`P`^l\[kQ_S[(v\[l,QZ[[(vQ_NbYNPVghT7[(S*dV`PoOY(dm

[(v7dSkQyl\[kQ_Scv7dSkQ

dT7[(WXdmEQNPSE\,`bY(Q_\,e7oWZ\,`PWZT`f\[kQ_e>x2\,VeSkd4dV>q|vNPSyopNbQ_`PeS\,VQ_VTmEQZY\[(NbdVd,O[(v7Qul\[kQ_SRd,O\
WZNbYWZThNb[NPV4[kd,gd`bd,lNPWZ\,`d,Ye7QZY_x)NqQ,qbx)S(TWv1[(v\[U[(v7Q~dT7[(WXdmEQd,Os\/l\[kQWZ\,V{QWZ\,`PWZT`P\[kQ_ec@v7Q_V
\,`P`>[(vQdT7[(WXdmEQ_Sd,O)l\[kQ_S@c@NP[(v/\RS(mR\,`f`bQZY|NPVe7QXu\Y(QU\,`PY(Q_\,e7oWZ\,`PWZT`P\[kQ_e>q&4Q$\,SS(TmEQ[(v\[[(v7Q

tII l

II (

l\[kQ_S&\Y(Q*Q_VThmEQZY\[kQ_e~NfV[(vNPS)cs\_o,x\,VheE[(v\[<\ (
]\ y\Y(Q@[(v7Q|NPV7ghT[?l\[kQ_SZxp\,VeE[(vh\[^\A@
]\
\Y(Q[(v7QEd,[(v7QZYl\[kQ_S_xc@v7QZYQ$[(v7QRSmR\,`P`bQ_Sk[NPVeQX/d,O&\l\[kQRc@vhNPWvNPSV7Q_Nb[(vQZY\,VdT[kghT7[V7d,Y\,V

mR\G^k
I  h  q
|s SNPm$Th`P\[(NPV7l_[\,SOd`P`bdc@SZq^v\,SE\jT7Q_V[$Od,YQZ,QZY(ol\[kQud,O
[xhS(\
ojT7Q_V[(S ( IDI l q&|v7Q{h\,SNPWNPe7Q_\ENPS|[(v\[mjTQ_V[(S ( II (Y(QZgYQ_SkQ_V[*[(vQ$NPV7gjT7[2l\[kQ_S
d,OQ[qrVdV7Qy[(NPmEQSk[kQZg^xG\,`fT7Q_S\Y(Q$ghY(d,gh\l\[kQ_eO!Ydm[(vQyNPVghT7[@V7dpe7Q_S
( I*I  [(v7Y(dT7lv\,`P`
l\[kQUV7dpe7Q_S r@h II filh x\,Ve[(v7QdT7[kghT7[(SsWXd,ghNPQ_e[kd rh I*I r h n q@v7Qe7QZgQ_Ve7Q_VWXoEl,Y\gjvuv\,Ss[(v7Q
(
Od`P`bd
cNPV7lQ_e7l,Q_S\,WZWXd,YeNPVlu[kdu[(v7Q4tc@NbY(Q_S(d,O2[(v7QRWZNbYWZTNP[G[qUrO2\,V4NfV7ghT7[l\[kQ`\  t
 ?
dT7[kghT[(S2\,VNPV7ghT[;[kd$l\[kQa\ x[(v7Q_V~c&Q@l,QZ[*\,VQ_e7l,QO!Y(dm
k
[
d
fi
h
;
q

r
>
O
(
[
7
v

Q

d
7
T
k
[
h
g
7
T
2
[
,
d

O
U
\
7
V

d
7
V
aNPV7ghT7[

l\[kQa\   }
K ;NPS2NPV7ghT[[kdUl\[kQa\ x[(v7Q_V~c&Ql,QZ[&\,VQ_e7l,QO!Y(dm h [kd h q)NPVh\,`P`bo,x[(v7QV7dpe7Q_S
h( II h n Sk[(\,Ve3O!d,Y|[(vQG\,`fT7Q{hNb[(SZq;rO+l\[kQb\ gYdeThWXQ_Ss[(v7Q`L [(vudT [kghT7[*{hNb[Zx7[(vQ_V/[(v7QZY(QNfS|\,V
Q_e7l,QOY(dm
fih [kd rh q9&Q_WZ\,TS(QU[(v7QWZNbYWZTNb[c[v\,S|V7d`bdd,gx[(vQl,Y\ghv/NfS*`bdd,gaLO!Y(QZQ,x7[kdd7q
h( IDI lh e7QZgQ_VeudV3[(v7QUOTVWX[(NbdVhS|WZ\,`PWZT`P\[kQ_e3{o
|vQOThVWX[(NbdVS|\,SSkdWZNf\[kQ_e3[kd[(v7QUV7dpe7Q_S
h O!d,Y  (ILI*kI  h xpc@vNfWv
[(v7QY(Q_SkgQ_WX[(Nb,Ql\[kQ\,Vheu\Y(Q\,S*O!d`f`bd
c@S_q)2\,Wvud,O?[(v7Q\,`PT7QV7deQ_S

Sk[(\,VehS9Od,Ys[(v7QNPVghT7[{hNP[(SZxvh\,S9QX7\,WX[(`boRdV7QgY(Q_eQ_WXQ_S(Skd,Y_xpc@v7dS(Q@\,`PT7QNfS2WXd,ghNPQ_eNPV[kd rh qQ_VWXQ,x

 NPS*[(v7QUdV7QXaLgh`P\,WXQyNPe7Q_V[(Nb[oOTVWX[(NbdV>x     c@Nb[(vughY(d,{h\{hNP`fNb[o3xjO!d,Y9  (ILI*kI  h qs@dc
c&QUWXdVS(NPeQZY|[(v7QUV7dpe7Q_S*cvNPWvuWXdmRQUOY(dmNPV[kQZYV\,`l\[kQ_Sd,O+[(v7QUWZNbYWZThNb[ZqrO\  NPS|\,V3l\[kQ,x
[(v7Q_V
  6I d  edjx^cv7QZY(Q \,Vefd\Y(QE[(v7QEgY(Q_eQ_WXQ_S(Skd,YSd,O&l\[kQ`\  qyrOQ\  NPS\,Vl\[kQ,x
[(v7Q_V
dx\,Ve/NbO\  NPS|\El\[kQ,xj[(v7Q_V
  6I d 
    x\,`P`cNb[(vgY(d,{h\{jNP`PNb[o3q
&o[(vNfS?WXdVS([kYTWX[(NbdV>x,Nb[+Od`P`bd
cS[(v\[)[(v7Q
 |sb
 S(NPmyT`P\[kQ_S?[(v7Qs&dd`bQ_\,VWZNbYWZTNb[,[q)d,[(NPWXQ
[(v\[$[(v7Q~VTmU{QZYd,O<
 T7Q_V[(Sd,O<
 NPS\[ymEdSk[$[(v7Q~e7dT7{j`bQEd,Os[(v7QVTm${QZYUd,O&l\[kQ_Syd,Oc[q|v7Q
[kY\,VS(O!d,Ym~\[(NbdVuOY(dmg[[kdz
 WZ\,V/{QgQZY(Od,YmEQ_euNfVgd`PoV7dm~NP\,`[(NPmEQ,q
VQXp\,mRgh`bQd,O?\E&dd`bQ_\,V/WZNbYWZTNb[*\,Ve[(v7Qy
 |sz[kdEc@vNPWvuNb[*NPS&[kY\,VSkOd,YmEQ_eu\,S@e7Q_S(WXYNb{Q_e
NPV7gjT7[2l\[kQ$Q_Th\,`PSP 
d
cx2c2Que7QXiV7Q\
P3



|3

3

|3

"3

3

|3

D3

|3

|3

Y



3



3

	

13 

3

13 



|3

3 

Y3

P3

|3

3

C3



]

2

o{

K

o{

{0

{

o{

{

{

K



o{

{

PM

\{d
,QyNPSslNb,Q_V/NPVu)NblTY(QUq

d
cx>c&QWZ\,V4WXdVSk[kYTWX[OY(dm[(v7QyWZNPYWZTNb[D;[(vh\[NPS\S(TWZWZNPVhWX[@Y(QZgYQ_SkQ_V[(\[(NPdVd,O9\WZNPYWZTNb[

jTQ_V[(SJ ( IN - IIN@tI6INRSINVR\,S\,`bYQ_\,e7oeQXijV7Q_e>xgj`PTS*\,eeNP[(NbdV\,`&jT7Q_V[(S*Od,Y
[(v7Q$l\[kQ_Sd,OP;sxjTS(NfV7lE[(v7QU[kQ_WvVNPT7Q$O!Y(dm
[(v7Q$\{d,QEs`f\,NPmuqs?\wpNPV7lR[(vQ$\,WX[(NbdV^h
g xj[(v7QUgj\YNb[o
xp[(v7Ql\[kQ[ogQ[R2\,Vhe[(v7QY\,Ve7dm{hNP[V h NfV[kdR\,WZWXdTV[Zxc&QWZ\,V/WXdVSk[kYThWX[<

\,WZWXd,YehNPV7l[kd
[(v7Qye7Q_S(WXYNbgh[(NbdVd,O;OThVWX[(NbdV
\{d,Q
Skd~[(v\[Jj
 T7Q_V[ rWh h WXdV[(\,NPVS[(v7Q{hNb[e7Q_S(WXYNb{Q_e{ou[(v7Q
OTVWX[(NbdV
 ( IN - IDINW
 @"I 6INRSINV \{d
,Q,q2@d,[(NPWXQ[(v\[s[(v7QOTVWX[(NbdV h Od,Y h h NPS&e7QZgQ_Ve7Q_V[9dV`Po
dV[(v7QgY(Q_e7Q_WXQ_SSkd,YS&d,O^[(v7Ql\[kQd,Oi;Y(QZghY(Q_SkQ_V[kQ_e{o fiWh h xp[(v7Q9j
 T7Q_V[(SINRQINV(hpI \,Vhe~[(v7Q\,WX[(NbdVjg q
~

|s 
\

J

c@Nb[(v

K

}

J

P3 



} 



Y3 

DY

U>

3 

fi

C

vh73Qh{.L8$>${3

dT7[kgjT7[
l\[kQ

dT7[kgjT7[
l\[kQ

s

3 3333 3 3
3 3333 3 3
33 3 3 3 3 3 3
3
3 3 33 3 3
3333333  333333
33333333 33333333
333 33 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 33 3 
333 33 33 3 3 3 3 3 3 3  3 3 3 3 3 3 3 3 3 3 3 3 33 33 3 33 
333333
3
3 3 333 3 3 3 3 333 3 3
33 33 33
333 3333
3333 3333
3 33 3 3
3 3 333
3
3
3
3
33 333 3
3
3
3
3333 3333
3
3
3
333 33
3
3
3333 3333
3 3 3 33 33 3 3 3 3 3 3
3
3
3
3
333333333333
3
3
3
333 
3 3 3 3  3 3 3 3 3 33 
3333 3333
33 3 3
3
3
3
3 3 3 3 3 3 3 333 33 33 33 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3
3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3
3333 3333
3 3 33 3 3 3 3
3 3 3 3 3 3 3 33 3 3
3333 3333
3
3
3
3
3
3
3
3
3
333 3333
3333 33333 3
3333 3333
3 333 33 3333  333333
333333  33333 33 33
33 3 3 3 3 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 33 3 3 
33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3
3
3333 3333
3333 33333
3333 3333
3 3 333 3 3
3 3 333 3 3
33 333
3
3
333
3333 3333
33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33 3 3 3 3 3 3 3 33
3
3
3333 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 333 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 333 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 33
333
NPV7gj3 T7[
NPV7gh3 T[

'

l\[kQ

s

l\[kQ

3

j

jAk


jms

jAs k

jmr

jAr k

jq

jAq k

.2

( }




jAp k

jmo

jAo k

jmn

jAn k

jml

jAl k

4 }





Sf

 


3







3





fith fiuh _V/T vc fi2h
. .
.
. 


.

3

2 }

3

3

At}







(

3

jmp



`V(T (c 4h  Sf
.


.
fih _V/T vc fith  Sf
. .4
.
. 
.
.

.

3


h `V(T (c h(
.



3







3



Sf




|s

)NblT7Y(QU}&dd`bQ_\,V1WZNbYWZThNb[c@vNPWvdT7[kghT7[(S[(v7QE{jNPV\Y(oS(Tmd,OsNb[(SNPV7ghT[{hNP[(SZx\,Ve4\

Y(QZgY(Q_SkQ_V[(NPV7l$[(v7QWZNPYWZTNb[ZqV`boOTVWX[(NbdVhS ( [(v7QNPe7Q_V[(Nb[oyOTVWX[(NPdVjx 4 S(NPmyT`P\[(NPV7l
\Rl\[kQ
x AtES(NPmyT`P\[(NfV7lE\,V3l\[kQx\,VeK 2 S(NPmyT`P\[(NPVl\,Vl\[kQ@\Y(Q
e7Q_S(WXYNb{Q_ejq

v

U>

fi 38?>v{v8.^j

$33$3

Q_VWXQ,xNb[v\,S\[mEdS([\YlTmEQ_V[(S\,Ve4WZ\,V{QeQ_S(WXYNb{Q_e{o\SmR\,`P`+[(\{h`bQ,qy|vNPSv7d`PeSOd,Y

1



rh q2Q_VWXQ,x
OY(dmv;c2QyWZ\,VWXdVS([kYTWX[\NPVX@&YQZgY(Q_SkQ_V[(\[(NbdV3SNPmRNP`P\Y*[kd[(v7Q$NPVu[(v7Q$gY(dd,O
 NbO\,Vo
d,O9|vQZd,Y(Q_m
qPqQXp[Zxc&QESkgQ_WZNbOo[(vQ$Y(QZcs\YeSd,O[(vNfS|q>|v7Q$Y(QZc&\YeNPS[ -Cw xAw y y
\,WX[(NbdVRNfS)[(\w,Q_VEdVR\Sk[(\[kQ|YQZgY(Q_SkQ_V[(NPV7l\,VENPV7gjT7[?l\[kQ@c@Nb[(v\,`PT7Q*\,Vegh\YNP[op
. xd,Yc@Nb[(vy\,`PT7Q
.\,Vegh\YNb[oqy[(vQZY(c@NPSkQ,x[(v7QY(QZcs\YeQ_T\,`PS[.pqU|vNfS@  Y(QZcs\YeOTVWX[(NbdVWZ\,V{Q$YQZgY(Q_SkQ_V[kQ_e
{oR\UWZNbYWZThNb[Zxc@vhNPWvEdV{hNPVh\Y(oyNfV7ghT7[UIUg
I *dT[kghT7[(S[(vQ
{hNP[d,O>[(v7Q@YQZc&\Yed,{[(\,NPVQ_e~NPVSk[(\[kQ

(
dV/\,WX[(NPdVXgjq@LpNPVWXQNb[*Y(Q_TNbYQ_Sm   {hNP[(Ss[kdRY(QZgY(Q_S(Q_V[*[(v7QYQZc&\Ye^x WZ\,V3{QYQZgY(Q_SkQ_V[kQ_e
TS(NfV7ldV`boj
;J w {jNb[(SZqfi
 dV
rO
  
x[(v7Q_VE[(vQZY(Q@NPS;\Wv7dNPWXQd,O\,WX[(NPdVS;Od,Y9Q_\,WvSk[(\[kQ@[(vh\[9lNb,Q_S9Y(QZc&\Ye  - w xAw y y
QZ,QZY(o[kY\*k
 Q_WX[kd,Y(o,xhS(NPm~NP`P\Y9[kdE[(vQghY(dd,Od,O?|v7QZd,YQ_mqPq9@dc2QZ,QZY
xjNbO    p. x\,Vo~gd`PNfWXoRv\,S
\[`bQ_\,Sk[dV7Qy[kY\*t
 Q_WX[kd,Yo[(v\[Y(Q_WXQ_Nb,Q_S\. Y(QZc&\Ye> q@dcx[(v7QZY(Q\Y(QE\[mEdS([_ -Cw xAw [kY\*t Q_WX[kd,YNPQ_SZx
\,Ven[(v7QZY(QZOd,Y(Qu[(v7QZYQNfS$\l\gd,O\[`PQ_\,Sk[; -Cw xzw y y ( -Cw xzw 5/n
 {QZ[c&QZQ_VgdS(S(Nb{h`PQ~\,`PT7Q_SZq1S
\{d
,Q,xjc2QWXdVWZ`fTe7Q[(v\[@\,Vo
 a\,eheNb[(Nb,Q\gghY(d_7NPmR\[(NbdV[kd~[(v7QO\,WX[kd,YQ_e3gY(d,{h`PQ_mlNb,Q_S
\,`P` T7Q_V[(S|d,O

J

q&+NfV\,`P`bo,x7[(v7QUOTVhWX[(NbdVK



 kThSk[|WXd,ghNbQ_S@[(v7Q\,`PT7Qd,O rh h

Od,Y

[3 

J

NPV[kdY3







J



~



	



]





o{

~



o{



\e7Q_WZNPS(NbdV\,`bl,d,YNb[(vhmOd,Y[(v7QyS(TWZWZNPVhWX[WZNbYWZThNb[|\,`PT7QUghY(d,{h`bQ_muq|@v7QZY(QZOd,Y(Q,xj[(vQ$`bdc2QZY{dTVeud,O
;avh\YeV7Q_S(SOd,Y[(v7QO\,WX[kd,Y(Q_e\,`PT7QgY(d,{j`bQ_mvd`PeSO!d,Y[(vNfS\ghgY(d_7NPmR\[(NPdVghY(d,{h`bQ_m
\,S|c&Q_`P`q
M

|vQUOd`P`bd
cNPV7lS([kYTWX[(T7Y(Q_eY(QZgYQ_SkQ_V[(\[(NPdVNfS@mEd,Y(Qyl,Q_V7QZY\,`+[(v\,V[(v7QyY(QZgYQ_SkQ_V[(\[(NPdVSmEd,Y(Q
WXdmRmEdV$[kd[(v7Q2r{gj`P\,VVNPVlsWXdm~m$TVhNb[o,q+4Q&S\_o[(v\[+\,V$v\,S?\

[(v7Q

~

j\,Ve

  
 _   (j(  XhL,  x
S(TWvU[(vh\[ ~ 
^gY(dpeTWXQ_S

 KrIUgqILKDhIN
`[(vu{hNb[*d,O)[(v7Q[kY\,VhS(Nb[(NbdVgYd,{h\{hNP`PNP[o RZKrIUgqILK h *\,Ve K\IUg
INsghY(dehTWXQ_Ss[(v7Q`L[(v/{hNb[*d,O)[(v7Q
Y(QZcs\Ye
V KrIUpg q*pNPmRNf`P\Ys[kd~[(v7QghY(dd,O+d,O|v7QZd,YQ_m qxjc&QUWZ\,V\,`PSkdRgYd
,Q$VdV\ggY(d
pNfmR\{hNP`PNP[o
d,Y)NPS?\S(TWZWZNfVWX[?|xNbOp[(v7QZY(Qs\Y(Q&&dd`bQ_\,VyWZNbYWZTNb[(S

~

~





d,O;G\,`PTQ_S*O!d,YS(TWZWZNPVWX[(`PoY(QZghY(Q_SkQ_V[kQ_e/2SZq

8Sx~3y{ 1

p2jkZb tk    jk  R,  p  R
 tr
 ~(j(  Xh~3 f 9@tp


  Z 

|)CYX

7

(

H F[Z \oH]K^

a&KKff_H

@

DGF &F K

?

bZdc

HKfe7H F
g

iKj

>

,g

 L,  j,7  *Z



|v7QeNfS(WXdTV[kQ_e\,`PT7Qd,O+\,VuNPVpijVNP[kQXav7d,YNbZdV&NPS&[(v7QmR\G7NPmyTm[kd,[(\,`^eNPS(WXdThV[kQ_egQZYka
Od,YmR\,VWXQ,q

v7Q_Vc&QyeNfS(WZTS(S*[(vQ$gd`PNPWXoQX7NPSk[kQ_VWXQygY(d,{j`bQ_md,Y[(v7Q$\
,QZY\l,QRWZ\,SkQgQZY(Od,YmR\,VWXQ

NPV[(v7QNPVpijVNP[kQ~v7d,YNbZdV^x)NP[$NPSyV7Q_WXQ_S(S(\Yo[kd4SkgQ_WZNbOo[(vQY(QZcs\YeWXYNb[kQZYNbdV>q34Qul,Q_VQZY\,`PNbZQ[(v7Q
\,`PT7QOTVWX[(NbdVu\,S@O!d`P`Pd
c@SZq

IN9fih\,`   Gd,OG NPS`GSUm~\GpNPm~\,`>aLgQZY(Od,YmR\,VWXQ~ThVe7QZY\,Vogd`PNfWXo^d,O
[ogQy2xhNqQ,q7\,`  
 G   mR\G o 7Z!  GIUq
d,[kQs[(v\[?\[(NPmEQXae7QZgQ_VeQ_V[^d,Y;vNfSk[kd,Y(oae7QZgQ_Ve7Q_V[^NPVpijVNP[kQXav7d,YNbZdVgd`PNPWXoOd,Y+\&NPS
|vQ







V7d,[VQ_WXQ_S(S(\YNP`PoiVNb[kQ_`boY(QZgY(Q_SkQ_V[(\{h`bQ,q+7d,Y+OT`f`boaLd,{jSkQZY(\{h`bQ;2SZx,Nb[^[(T7YVQ_edT7[2SkQZQ2Q,ql7qG2T7[ta
QZYmR\,V^xj_,G)[(vh\[9[(v7Q|eNfS(WXdTV[kQ_eEd,Y2\
,QZY\l,Q\,`PT7QNPS)[(v7Q|gQZY(Od,YmR\,VWXQ*d,O>\S([(\[(NbdV\Y(ogd`PNPWXo,q
|vNfS9mEQ_\,VS&[(v\[sV7dvNPSk[kd,Yoae7QZgQ_VeQ_V[gd`PNfWXoygQZY(Od,YmRS9{QZ[k[kQZY&[(vh\,V[(vQ{Q_Sk[&Sk[(\[(NbdV\Y(o~dV7Q,q
S$\,VnNPmEgd,Y([(\,V[UWXdVSkQ_T7Q_VWXQ,x;\,Vd,g[(NPmR\,`9gd`PNPWXoNPSijVhNb[kQ_`boY(QZgY(Q_SkQ_V[(\{h`bQ,q/d,Ys2SZx
[(vNPSe7dQ_SV7d,[v7d`fe>q$\,e\,VN)QZ[\,`q9t_,,S(v7dc2Q_e[(v\[[(vQE[(NPmEQXae7QZgQ_Ve7Q_V[NPVpijVhNb[kQXav7d,YNbZdV
gd`PNPWXoaLQXpNfSk[kQ_VWXQsgY(d,{h`PQ_mO!d,Y9s2SNPSVd,[e7Q_WZNPe\{h`PQsThVe7QZY\_,QZY\l,QgQZY(Od,YmR\,VWXQsd,Y9ThVe7QZY
[kd,[(\,`eNfS(WXdTV[kQ_e3gQZY(Od,YmR\,VWXQ,qUrVWXdV[kY\,Sk[Zx^c&QES(v7d
c[(vh\[[(vQS\,mEQgY(d,{h`bQ_mOd,YSk[(\[(NbdV\Y(o
gd`PNPWZNbQ_SsNPS@aWXdmEgh`bQZ[kQ,q

U>


fi

8Sx~3y{ 0



C

vh73Qh{.L8$>${3


Q

t u

L



 j     Z ,  Up  k ! Z  ;jkXP
( p  L,  

L,L) !X ,j(/yZt |
 7XZ R   !  |   jPZ



|vQ1v\YeV7Q_SSgY(dd,OyNPS/Q_S(SkQ_V[(NP\,`P`bo

(

D

[(v7QS(\,mEQ\,SuO!d,Y|v7QZd,Y(Q_m

X,93  X

7qPqd,[kQ[(v\[3NfV[(v\[

WXdVSk[kYTWX[(NbdV>xQZ,QZY(oSk[(\[(NbdV\Y(o3gd`fNPWXoud,{[(\,NPVSY(QZcs\YeUOd,Y\[mEdSk[dV7QSk[kQZg^x>V\,mEQ_`bo3c@v7Q_V

y
 NPS|Y(Q_\,Wv7Q_e^xjmEQ_\,VNfV7lE[(v\[@[(vQO!d,Ym$T`f\ENPS|S(\[(NPSkihQ_e>q2`P`>d,[(vQZYSk[kQZghS@oNPQ_`PeY(QZcs\Ye
.pq;|v7QZY(QZOd,Y(Q,xO!d,Y[(vhNPSWXdVSk[kYTWX[(NbdV>x[(vQ|[kd,[(\,`heNfS(WXdTV[kQ_eEG\,`fT7Q|NPS)l,Y(Q_\[kQZY&[(v\,V;.NPOj\,VeEdVh`boyNPO
[(v7QijVhNb[kQXav7d,YNbZdV\,`PT7QNPS*S(d7q9dRm~\w,Q[(v7QWXdVSk[kYTWX[(NbdVuc&d,Y(wO!d,Y|\
,QZY\l,Q$\,`PT7Q,x7c&Qv\
,Q[kd
mEdpeNbOoENb[2S(ThWv[(v\[2dVhWXQ[(vQS(NPVwRSk[(\[kQJ
 NPS9Y(Q_\,WvQ_e>xpQZ,QZY(oS(T{hSkQ_T7Q_V[&\,WX[(NbdV{YNPVlS;Y(QZcs\Ye
q*|v7QZY(QZOd,Y(Q,x[(vQ$\_,QZY\l,Q$\,`PT7QQ_T\,`PS$NbO)[(v7QOd,YmyT`P\ENPSS(\[(NPSti\{h`bQ,x\,VeNb[|Q_T\,`PSm
. NbO+NP[@NPS
S(NPVwSk[(\[kQ

TVS\[(NPStij\{h`PQ,q)Q_VWXQ,x{d,[(vu[(vQgY(d,{h`bQ_m~S*\Y(QU;av\Ye^q

&dV[(\,NPVmRQ_V[yNPVO!d,Yy[(v7Q[kd,[(\,`&ehNPS(WXdTV[kQ_egQZY(Od,YmR\,VhWXQRO!d`f`bd
c@S$OY(dm

[(v7QlT7Q_S(Ska\,Vepa

WvQ_W(w~\gghY(d\,Wv>}9TQ_S(S2\$Sk[(\[(NbdV\YoEgd`fNPWXo,xWZ\,`PWZTh`P\[kQNb[(S9gQZY(O!d,YmR\,VWXQ@\,Ve\,WZWXQZg[sNbO>\,VeRdV`Po
NbO^[(vQgQZYO!d,Ym~\,VWXQNPS&gdSNb[(Nb,Q,q|v7Q[kd,[(\,`^ehNPS(WXdTV[kQ_eu\,Ve[(v7Q\_,QZY\l,QUgQZY(Od,YmR\,VWXQWZ\,Vu{d,[(v
{QWZ\,`PWZT`P\[kQ_e3NPVgd`boVdmRNP\,`[(NPmEQ,q
rtV[(v7QES(\,mEQc&\
o,x[(v7Q[kQ_WvVNPT7Q_SgY(dNPVlV7dVh\ggY(d
pNPm~\{hNP`PNb[oY(Q_S(T`b[(SOd,Y[(v7QESk[(\[(NbdV\Y(o



gd`PNPWXo3NfV[(vQ$iVNb[kQv7d,YNbZdV4WZ\,SkQ3Lsd,Y(d`P`P\Y(o37q WZ\,V{QmRdeNbihQ_e3[kdud,{h[(\,NPV4V7dV\gghY(d_7NPmR\Ga
{hNP`fNb[oRYQ_S(T`b[(S*Od,Y@NPV7ijVNb[kQv7d,YNbZdVhSZq

8Sx~3y{ 032 p  L,  ,j  9  Ztp u ,3G,fihks9  pjZL,L,> fZ ph~
,Xk
 |7Z!ZSR    9kjk  R,(  Uju  @  
(



|vQ|NPVpijVNP[kQXav7d,YNbZdV$[(NPmEQXae7QZgQ_VeQ_V[)gd`PNPWXoaLQXpNfSk[kQ_VWXQsgY(d,{h`PQ_mRS)\Y(Q@TVeQ_WZNPe\{h`bQ\,eh\,VN
QZ[s\,`qbx_,,q94QSv7d
c

[(v\[2Vd$WXdmEghT[(\{h`bQ@OTVWX[(NPdVRWZ\,VQZ,Q_V\ghgY(d_7NPmR\[kQd,g[(NPmR\,`hgd`PNPWZNbQ_SZq

8Sx~3y{ 0 p  RZt7ZjXj    Zt, u GhtUpj  ZXP9
,Xk
 |7Z!ZSR    hj,@9tjtD  E(\
7



(

X


|vQgY(dd,O?Od`P`bdc@SsO!Ydm[(v7QgYdd,O?{o\,eh\,VN>QZ[@\,`q+t_,,*S(v7dc@NPV7lE[(vQTVWXdmEghT[(\{hNP`PNb[o
d,O9[(vQE[(NPmEQXae7QZgQ_Ve7Q_V[G\,`PTQ,qUrtV\,e\,VN)QZ[\,`Lq;t_,,x+OY(dm

s

\lNb,Q_V1+T7YNPV7lmR\,WvhNPV7Q

\,V

.q
p
mv\,`b[(SydVQ_mEg[onNfV7ghT7[Zx)[(v7Q_V[(v7QZYQNfS$QX7\,WX[(`bodV7Qu[(NPmEQXae7QZgQ_VeQ_V[NPVpijVhNb[kQXav7d,YNbZdV
gd`PNPWXoc@Nb[(v4gQZY(Od,YmR\,VWXQ
 9 x* \,`P`;d,[(vQZYU[(NPmEQXaeQZgQ_Vhe7Q_V[Ugd`fNPWZNbQ_Sv\_,QgQZY(Od,YmR\,VWXQ
x\,Vhe
' @[(v7Q\_,QZY\l,QEG\,`PTQ$NPS{QZ[c&QZQ_V. \,Venq|vNPS@Y(Q_eTWXQ_S[(v7QTVe7Q_WZNfe\{h`bQghY(d,{h`bQ_m
TV7d,{jSkQZY(\{h`bQE&zNfSWXdVSk[kYThWX[kQ_ev\
NPVl/[(v7QROd`P`bdc@NPV7lgY(d,gQZY([(NPQ_SO!d,Y$\Y({hNb[kY\Yo<K&
t
$rO

6<

h<

d,O9cv7QZ[(v7QZY\+T7YNfV7lmR\,WvNPV7Qv\,`b[(SdVQ_mEg[o3NPV7gjT7[@[kd[(v7Qy[(NPmEQXae7QZgQ_VeQ_V[NPVpijVhNb[kQXav7d,YNbZdV

gd`PNPWXoQXpNPS([kQ_VWXQgY(d,{h`PQ_mO!d,Y@ThV7d,{hSkQZY(\{h`bQs2S|TVe7QZY|\
,QZY\l,QygQZYO!d,Ym~\,VWXQ,qWX[(Th\,`P`bo,x

mZa\gghY(d_7NPmR\{h`PQ,x,c2Q|WXdT`feyWv7ddSkQ
v\,`P[(S^dV$Q_mEg[o$NPV7ghT7[Zq

\,S(S(ThmRNPV7l[(v\[;[(v7Qs\,`PT7Qsd,Oh[(v7Q*TVd,{hSkQZY(\{h`bQs&4c&QZY(Q

<*NPV$\|cs\_o[(vh\[?QZ,Q_Vy[(v7Q&\ggY(d
pNPm~\[(NbdVQ_V\{j`bQ_S?ThS^[kde7Q_WZNPe7Qcv7QZ[(v7QZY

Za\ggYd_7NPmR\[(NbdV/NfS*NPmEgdS(S(Nb{j`bQ,q
w ~vy~$y 0
p  ~Xk7XjXh~,jb ! L,k7ZjZh    Zt, u G,fih1kus
3  pjX$,Xk|XXSR
     ,j,| tjk  R ,(\
pNPVhWXQ[(vhNPS|NPS*TVhe7Q_WZNPe\{h`PQ,x\,V
l





~} F HB

?

IF

(

? ?hF




/' /'

.\'GMThSkQ_V\
'

|vNfS@c&d,Y(w3cs\,SS(Tggd,Y([kQ_eNfVgj\Y([{opl,Y\,V[$|s*a p_ ,G\,Ve1|s|a,p

\,Ve$d`PeS(mRNP[(vjx
\,Ve{oQ_T[(S(Wv7QZY+@w\,e7Q_mRNfS(Wv7QZY?TS([(\,TS(WvheNbQ_VSk[|^l,Y\,V[ p_~{G92Z{
lTp
 a\{TVhev7Q_V7wq;|v7Q[(vNbYeu\,T7[(v7d,Y|gQZY(Od,YmEQ_egh\Y[*d,O?[(v7QUc2d,Y(w\[@\Y([(mEdT[(v&d`P`PQZl,Q,q

U>

fi 38?>v{v8.^j

^$d`P`bQZY\,Ve

4Qc&dT`Pe`PNPw,Q3[kdn[(v\,V7w\ghvV7Q

$33$3

S(QZ,QZY\,`\,V7dVopmEdTSY(QZOQZY(QZQ_SO!d,YWZ\[(WvNfV7l

QZY(Y(d,YS|NPVQ_\Y`PNPQZY*,QZYS(NbdVS|d,O?[(vNfS*gh\gQZY_q

?

\ jF I9Kfj

? H ?

H+H_H_

Cg

W9CB


(././.q2M^QZ[ ) I*I qp){Q|\,V~NPVSk[(\,VWXQ*d,O
IDI  x7c@v7QZYQWZ`f(\,TSkQ    +(N 0  +-S 0
 7(, E      e,.  k  1&  NPO

4Q|gYQ_SkQ_V[9[(v7Q*Y(Q_eTWX[(NPdVEO!YdmTVhev7Q_V7w$QZ[9\,`qbx

II q

 +4S 0
 Z I 
 Y(Q_S(g^q  sNPS|\E`fNb[kQZY\,`>NPV



z

c@Nb[(v\YNP\{h`bQ_S{ (
|{ \,VeuWZ`P\,TSkQ_S ~
(
|O!d,Y 
{
{

q
4

QES(\
o3[(v\[\YNP\{h`bQ1{



v]wx4y


{

t{

q


~

o{

|{

~

~[





]





4

~[

FG >  H9ILK / INMIQPOINRQIUTINVsc@NP[(v
H  UI9p F I_ T  : I
K /   t(I_
QI_
QI
M  .e_I  I
P 
( I*I qqI mI
NbOK 
 UIQI QILKh 
 t(I  
QI 
QIO I_
 'I
.// (I
//
//
 +   0 \ggQ_\YS*NPV c@Nb[(vuS(NblVTm-g
\,Ve
//
// (I
h
b
N

O
K




U

N
I


Q

N
I


Q

L
I
K
I  w 'I
//

 J_
//
 +    0 \ggQ_\YS*NPV  c@Nb[(vuS(NblVTm-g
\,Ve
//
(

I
b
N

O
K




U

I



Q

I



Q

L
I

K
h


 (I QI Q_
I  'I


RZKrIUgqILK h   0 //

\,Ve
\ggQ_\YS*NPV
c@Nb[(vuS(NblVTm
g
//
+  0
// (I
NbOK 
 'I QI QILK h  I
//
//
 +4S 0 \ggQ_\YS*NPV c@Nb[(vuS(NPlVTm mg
\,Ve
//
//
//1 (I
NbOK  K h  d,YK  K h  
.eI d,[(v7QZY(c@NfSkQ
./
 I NbOK  UI 
(I
NPO6RZK\IUg
I 
(ILK  
0
VK\IUg7   .eI d,[(vQZY(c@NPSkQ
I
T GK   /1 JI NbOK  
7
I NbOK  
d,[kQ[(v\[|\,`P`[kY\,VhS(Nb[(NbdVSsNPV
G  >s\Y(Qe7QZ[kQZYmRNfVNPSk[(NPWxp\,VeQZ,QZY(o[kY\*k Q_WX[kd,Y(ouv\,Ss\,`PT7Q_.d,Y
qE|v7QZY(QENPS\uWXd,Y(YQ_SkgdVe7Q_VWXQ{QZ[c&QZQ_V4gd`PNPWZNbQ_SOd,Y`
G  >\,Vhe4\,S(S(NblVmRQ_V[(Sd,O&G\,`fT7Q_S[kd/[(v7Q
\YNP\{h`bQ_SUd,O ?x)STWv[(v\[$gd`PNfWZNbQ_STVeQZYc@vNPWv
G  >v\,S$G\,`fT7QWXd,Y(Y(Q_SkgdVe[kdS(\[(NPS(O!opNPV7l
7Ydm

z?xc2Q$WXdVSk[kYTWX[\R&
*



\

*

z



o3















o3

o3

{

|{



!



-





~[

{



o3

-



~

{

o3

o3

{



C

~[

o3





{

~[





{





z

z

Iz

z

\,S(S(NPlVmEQ_V[(S*O!d,Y	zxj\,VeupNPWXQ,QZYS(\pq

?

\ jF I9Kf

Cg

H+H_H_

? H ?


W9Cfi|

&(././.q*M^QZ[

l\,NPV>xc&QUgYQ_SkQ_V[@[(v7QY(Q_eThWX[(NbdVuOY(dmTVev7Q_VwQZ[@\,`Lqbx

IDI  \,Ve  WZ`P\,TS(Q_S
HJILK / INMOINRSINV&c@v7QZYQ

\YNP\{h`bQ_S{ (
 z 


G

M

~







UI   I_
.eI_

H

|{

 

-



(

II 

*T

~



*z{Q\~O!d,Ym$T`f\c@Nb[(v

q)@vNPS;[(NPmRQ,xc&Qe7QXijVQ*[(v7QTVd,{hSkQZY(\{h`bQ*

:



UU>



K*grR   
-



Z


K / I9I

!


fiC

vh73Qh{.L8$>${3


.//
//
//
//
//
//
//

RXKrIUg
ILK h 

0

//



//

//
//
//
//
//
//
//1

VKrIUgp
? ?  h? F  ?






_

(( I I
(I
(I
(I
(I
(I
(I
.eI
I
.eI

K  K / ILKDh  t(IQI_  
K  UIQILK h  K*grR  ( INI  \ggQ_\YS*NfV c@Nb[(vuSNblVThm-g
NbOK 
UIQILKDh 
 (IQIN<I
 e7dQ_S@V7d,[|\ggQ_\Y@NPV c@Nb[(vuS(NblVTm-g
 \ggQ_\YS|NPV c@Nb[(vuS(NPlVTm-g
NbOK 
IQILK h  JI
NbOK 
I QILKDh  I qe7dQ_S|V7d,[@\ggQ_\Y@NfV
c@Nb[(vuSNblVThm-g
NbOK  KDgfiR  L
I K h  K*gfiR  ( IN
NbOK  KDgfiRqL
I Kh  
NbOK  Kh  d,Y9K  KDh  JIUg  .Ed,YJg 

NbO

T

NbO

Y

~[

{



H

~[

{





~

P{

~[

P{




d,[(v7QZY(cNPSkQ

K



RZK\IUg
I .

NbO R  z\,Ve
d,[(v7QZYc@NPSkQEq

&



/

U (   r|9YNPVWXQZ[kdVuVNb,QZYS(Nb[oYQ_S(SZq
s`bo[(v7Q,x7qt_,,qsQ_WZNPS(NbdV7aL[(v7QZd,Y(QZ[(NPWgj`P\,VVNPVl7q a@
  Z  Xx ? x0'  G7q
&Q_`P`PmR\,V^xq@t_,,q|j

jk ,k



E)

u :

&dT7[(NP`PNPQZY_x9qbxQ_\,V>x2q)Mqbx9\,V7wpSZx2qt_,,qQ_WZNPS(NPdVpaL[(v7QZd,Y(QZ[(NPWgh`f\,VVNPV7l7}R[kYThWX[(T7Y\,`



\,SS(TmEg[(NPdVS*\,Ve/WXdmEgjT7[(\[(NbdV\,`^`PQZ,QZY\l,Q,q)7p?t	)a)@    x

pVp

Q

x+ G7q

&dT7[(NP`PNPQZY_xqbx^Q_\Ye7Q_V>x^Uqbxd`PeSk_m~NPe7[Zx>qt_,,qgj`bdNb[(NPV7lSk[kYTWX[(TY(QyNfVgd`PNfWXo/WXdVpa
S([kYTWX[(NbdV>q?rV



fi p   

X Xj  

( 

@7Z  k
 p
)\Y(wxs\,`fNbO!d,YVNP\pq

(

pVqVq

)")	)a'+,j

7( sk     &j      
   = ^,QR
fi h(p  e gg^q'/'D',qrRYQ_S(SZx|Q_V`bd

tI)
 

/



(

)

R+

sT7Y\l,d7x9yqbx9eQ@dT7l,Q_mEdV[Zx9qbx&7`PNPS(SkQ_Vw,d7x)$qt_,,qnVn[(v7QWXdmEgh`bQX7Nb[od,O|gh\Y([(NP\,`f`bo

p((Z  ,Z;(j7X Z X  Xx ~ x+_pQj_/'q
  ,jjk  R,/ ,  r  ZpX  ,!  ZXPE1,

d,{jSkQZY(,Q_eu\Y(w,d
ue7Q_WZNPSNbdV~ghY(dWXQ_SSkQ_SZq
s\,S(S\,Ve7Y\px^$qt_,,q



X

*+

(

p t

 Z! jk      sV7gjT7{h`PNPSv7Q_eedWX[kd,Y\,`eNPS(SkQZY[(\[(NbdV>x7&Y(d
c@VVhNb,QZYS(Nb[o,q

}$

s\,S(S\,Ve7Y\px;$qbx \Q_`b{h`PNfV7l7x?M9qbx;MNP[k[(mR\,V>x;q)Mqt_,,q

X$

p, U
  ~pX    ZXP1,~ _f jk     
9Y(d
pNPe7Q_VhWXQ,xh@v7dpe7QrtS(`P\,Ve>};&Y(dc@V3VNb,QZYSNb[o,q
Y(QZQ_Vh`P\_cxqbx@dd,QZY_xUqc7qbx@

(

*   jPZX 
\,VSkQ_V>x+|q/t_,,\q



\

@TZZd7x|zq*M9qt_,,q

 p( |pO!d,YeuVNb,QZYS(Nb[oYQ_S(SZq

n)



(

U ( fi

Z Xj,G
 Pjk t
 
?Q_Wv^qh|QZg^qd7q|a,a_q

  

 

L3pk!PX

(

  j7L,  =

 

/    X R R
 ,ht,&tp,  fi1  XGXP    
 V7ghT{h`PNPS(vQ_e
edWX[kd,Y\,`yeNPSSkQZY([(\[(NbdV>xQZg[Zqd,O&dmEghT[kQZYpWZNbQ_VhWXQ,xVNP,QZYS(Nb[o ,
d OR\,S(S(\,WvThS(SkQZ[(S\[

mRv7QZYSk[Zq

fi 

\,VSkQ_V>x|qt_,,G{qd`PNPVlR&9S{ouSkQ_\YWvNPV7lNPVugd`PNPWXoSkgh\,WXQ,q*rtV*t    
;,_ZZkZ  f
 ?  XXL  h  K
 )X  Z N
 hZ!  X  gg^q p, p_q





UUU

q Q



t$ 7

fi 38?>v{v8.^j

$33$3

!+q (././.qzopV\,mRNPW3gY(d,l,Y\,m~mRNPV7l1Od,Ys2SThS(NPV7l\O\,WX[kd,YQ_e
fi     hXj,  j  ,_ZZkZ  ,,   Z 

fi
 fi
'(.*jXq*2YQ_W(w,Q_V7YNfe7l,Q,x|q
\,TSkwY(Q_Wv[Zx?q3t_,,qsb  fi
   hk    L  p     (R          7Z!_    _Z
R,  
s V7ghT{h`PNPS(vQ_e~e7dpWX[kd,Y\,`ehNPS(SkQZY([(\[(NPdV>x7\,S(S\,WvTSkQZ[k[(SrVS([(Nb[(T7[kQd,O;Q_WvV7d`bd,l,o,q
$NPm/xr$Eq a|qbxpQ_\,V>xpqM9qbxQ_T`bQ_\,T^x$q9(././. q@ghgY(d_7NPmR\[kQSkd`PT7[(NPdVS;[kdUO\,WX[kd,Y(Q_e\Yw,d

eQ_WZNPS(NbdVgY(dpWXQ_S(SkQ_SNf\~l,Y(QZQ_eo3SkQ_\YWvNfV3[(v7QySkgh\,WXQd,O;ijVhNb[kQUWXdV[kY(d`P`bQZYS_qrtV*t    fi 
k~p
     hXj,  j,  ,_ZZkZ  /   _   jX   Z  jb  fi1
 ,j X 7 (p  fi
ghg^qq'//'D'/'(
. q*2YQ_W(w,Q_V7YNfe7l,Q,x|q
$d`P`PQZY_x^yqbx+)\Y(Y_x?q/(././. q+d`PNPWXoNb[kQZY\[(NbdV1O!d,YO\,WX[kd,Y(Q_e9S_qyrtV*t      t~7 
 ;__X(X  E+  ZL  h      Z NjX   Z  D
 T
\,VSkQ_V>x|@qbx@7Q_Vl7x

S([(\[kQ~Y(QZgYQ_SkQ_V[(\[(NPdV>qErVnsk  (    t~ p
 hZ!  X  |jb   uj Z pp   1ghg^q^

K)

p

MNP[k[(mR\,V>xjqjM9qt_,Gq3Q_mRd,Y(o`PQ_S(S|gd`PNPWZNPQ_SZ}9|v7QZd,YQZ[(NPWZ\,`?`fNPmRNb[(\[(NbdVhS*\,Ve/gY\,WX[(NPWZ\,`^Y(Q_ST`b[(SZq

(

rtVuyqjs`PN)xp?q7TSk{h\,VheSZx7q a$q3QZo,QZY_xqpzq7NP`PSkdV42eSZqfix/k

)



 )  
 E,  L
 ypb,  tI)  

R, b **t      tp   k?hXj,  jQ;,_ZZkZ  y 
  s3rt9Y(Q_S(SZq
MNP[k[(mR\,V>xqGMqbx,Q_\,V>x,q
MqbxF
$ \Q_`b{h`PNfV7l7xM9q
+qt_,,qV[(v7Q&WXdmEgh`bQX7Nb[od,OS(d`bNfV7l|\Yw,d

eQ_WZNPS(NbdVgY(d,{j`bQ_mRSZqrtV1sk  (  fi  kRp
   );,_ZZkZ  ?  XXL  h  
X   _ ,NhX  
 X  ghg^qq,' Gv \.\ q
M^d,Qt
 do,x|qsqt_,p
q&dmEgjT7[(\[(NbdV\,`P`PoOQ_\,S(Nb{h`bQu{dTVheSyO!d,Ygh\Y([(NP\,`P`Po1d,{hSkQZY(,Q_e \Yw,d

eQ_WZNPS(NbdVgYdWXQ_S(S_q?7Xk,    @    
 x  t
x+_/Dj 
q
K+

(

p=p

)

)

=q

MThSkQ_V\px@qbx|MNLxsqbxpNb[k[(NPV7l,QZY
x*qbx*4Q_`f`PSZx@qbx@

fi 

v7Q_VmEd,Y(QymEQ_mEd,Y(o/v7Q_`PghSZq*rtVsk  (  
)X 
 hX  X  ghg^q  ,p
q
_ ,N





q' v'

\,e\,VNx,yqbxG\,V7wpSZxqbx

d`PeS(m~Nb[(v>xc7qt_,,qzo{Y\,NPVNPSROT`P`}



ky p

p~



 ;,_ZZkZ  ~+  ZL  h

&dVe7dV^x$qht_,,q7V[(v7Q9TVeQ_WZNPe\{hNP`fNb[o*d,OpgY(d,{j\{hNP`PNPS([(NPWgj`P\,VVNPVl

fi 

\,VheNfVpijVNb[kQXavd,YNbZdVRgj\Y([(NP\,`P`bod,{hS(QZY(G\{j`bQ\Y(w,de7Q_WZNfS(NbdVgY(d,{h`bQ_m~SZq^rtVu*t    
 7 p  9$,  ,jZ
 ;,_ZZkZ  RK
 )X  Z N
 hZ!  X  gg^qhG Gq





\$

 

/'





Q

t

W
s\,`P`PTm/xUq Eq)t_, q,QZYWXdmRNPVlNPVWXdmEgh`PQZ[kQ&gQZYWXQZgh[(NbdVc@Nb[(vT7[(NP`PQsehNPSk[(NPVWX[(NPdV$mEQ_mRd,Y(o,q

fi

 
   (  fi
 gg^q+_(.*
X$
Q_T`bQ_\,T>xUqbx1$NPm/x
E
$ q a@qbx&$\Q_`P{h`PNPV7l7xjMq?qbx>s\,S(S(\,VeY\pxj$qUq$t_,,qUd`bpNPV7l~s2S
{oS(Q_\YWvNfV7l4[(v7QuSkgh\,WXQud,O@ijVNb[kQgd`PNPWZNbQ_S_q4rVsk  (  fi  tup
  ~ 
 ;__X(X  3,
+  XXL  j   X   _ ,NhX  
 X  ghg^q7
 r, q
Q_T`bQ_\,T>x7$qbxp+Q_S(vwNPV^xM9qbxeU
$ NPmuxfiR$ q a|qbx $ \Q_`b{h`PNfV7l7xM9q+q*t_,,q)MQ_\YVNPVlijVNP[kQXaSk[(\[kQWXdVpa
[kYd`P`bQZYS@O!d,Ygh\Y([(Nf\,`P`bod,{hSkQZY(\{h`bQ$Q_VpNbY(dVmRQ_V[(SZq@rV4sk  (  fi  tp
  ~ 
 ;,_ZZkZ  
,+  XXL  j      Z NjX  
 Z  gg^q7r  r,' q
TVev7Q_Vwjxq(././,
. \q|v7Q$WXdmEgh`bQX7Nb[od,O)d,g[(NPm~\,`S(mR\,`f`>gd`PNPWZNbQ_SZqs1,p R , Z t?7Xk
    |  (  
 x  t
x+,_Dj , q
rtV1sk  (    ky p p : ehXj,  j,G;__X(X  ~,1 
_,qsd,Y(l\,V \,TOmR\,VhVu9T7{h`fNPS(v7QZYS_q

n

p

)

p

)

u

UU

fiC

vh73Qh{.L8$>${3


(././.{q 7  ( jPU  k&j,  fi  $pX  !t  ZXP,R _ 
Q_Wv>qj@QZg^qhd7qh|(././G
. a' qs\Y([(mRdT7[(v&d`f`bQZl,Q,q

TVev7Q_Vwjxpq|

(

 ,~t     

TVev7Q_VwjxGqbxd`PehS(mRNb[(v>xA7qbx`P`bQ_Vhe7QZY_x@qjt_,,q|v7Q2WXdmEgh`bQX7Nb[od,O7[(v7Q2gd`PNPWXoQXpNPS([kQ_VWXQ
ghY(d,{h`bQ_m

O!d,Y2gh\Y([(NP\,`P`PoaLd,{hS(QZY(G\{j`bQsijVNb[kQXavd,YNbZdVR\Y(w,d
e7Q_WZNPS(NPdVEgY(dpWXQ_S(SkQ_SZq>rtV*t  
 
 /^,,    t
 ; @pZ*
 + Z X    ,_ZZkZ  ggq , j ,
 q
  	u  ~, p E  
pgYNPV7l,QZY(aL;QZY`P\l7q

fi



 

/

 D '

(././.qE&dmEgh`PQXpNb[ouY(Q_S(T`P[(S@Od,Y

x,pQp
 ((p. q

TVev7Q_Vwjxqbxd`feS(mRNb[(v^x$qbxM?TSkQ_V\pxqbx^`P`bQ_Ve7QZY
xj|q~

iVNb[kQXav7d,YNPZdVU\Y(w,d
ye7Q_WZNPS(NbdVUgY(dpWXQ_S(S+gY(d,{h`PQ_mRSZq7pt* p\)`^4x

(

r

( U  \seeNPS(dVpaL4Q_S(`bQZo,q

)\gh\,eNPm~Nb[kYNbdT>xq$qt_,Gq`; j7L,  ,
 j   jP

|S(Nb[(S(NPw`PNfSZx 7q$qt_,,qurtV[kY\,WX[(\{h`bQgY(d,{h`bQ_m~SNPV4WXdV[kY(d`9[(vQZd,Y(o,q

)\gh\,eNPm~Nb[kYNbdT>x)q$qbx)

+|)7pj,+t;,ht,+j?

 u ,  x



|SNb[(S(Nbwp`PNPSZx)q*Uq

)\gh\,eNPm~Nb[kYNbdT>xq|Uqbx

 

ghY(dWXQ_SSkQ_SZq&, p R, X

tR?7Xk,  
  

/' D

x> , ,G7q

ur

t_,,q

|vQWXdmRgh`bQX7Nb[o



|  (  x

pu

'

Q (.

d,Oy\Y(w,d
e7Q_WZNfS(NbdV

 xj,  pq

$ \Q_`b{h`fNPV7l7x|Mq|+q t_,,qM^Q_\YVNfV7lgd`fNPWZNbQ_Sc@Nb[(v QX[kQZYV\,`
  k~p  _jXj,  jD;__X(X  1       fi
$\,T7OmR\,VV>xj7\,Vu7Y\,VWZNPSWXd7xs$q
 R     E     hk&t<  jk(   !     
   Z~

+Q_S(v7wpNPV>xMqbx3Q_T`PQ_\,T>x$qbx
mRQ_mEd,Y(o,q~rtV*t    
ghg^q  pXq*3d,Y(l\,V

q'(. '
9`f\[k_mR\,V>xM9q1R
$ qt_,,q
  
 V7ghT7{h`fNPS(v7Q_ee7dpWX[kd,Y\,`9eNfS(SkQZY([(\[(NbdV^xQZgh\Y([(mEQ_V[Ud,O*9`PQ_WX[kYNPWZ\,`92V7lNPV7QZQZYNPV7lu\,Ve
0/

p

n

sdmEghT7[kQZY7WZNbQ_VWXQ,x\,S(S(\,WvTSkQZ[k[(SrtVSk[(Nb[(T[kQd,O;?Q_WvVd`bd,l,o,xs\,m${YNfe7l,Q,x$q



9T[kQZYmR\,V>xqhMq@t_,Gq|1,,~ _f ,jt      c,dvhVu

/

pv\,m~NbY_x7Uq@t_, q&rt



pdVSZxQZcd,Y(wq

Nf`bQZo

D

sp@s|q7pj+k p-)`^x q x>,, ,q


('

pmR\,`f`bc2ddpe>xGUqGyqbxdVehNbwjx|qz7q^t_ qh@v7Q2d,gh[(NPmR\,`WXdV[kY(d`d,Ojgh\Y([(NP\,`P`Pod,{jSkQZY(,Q_e\Yw,d

ghY(dWXQ_SSkQ_S*d
,QZY[(v7QiVNb[kQv7d,YNbZdV^q?7Xk,  
  
dVehNbwjx|q7q9t_
q







@    x

up

. Q .

x?  j ,q



( p  R  hkt9pX  ,!  XGXP,*t      )VghT7{7a
`fNPS(v7Q_ee7dpWX[kd,Y\,`ehNPS(SkQZY([(\[(NPdV>xh[(\,V7Od,Ye/VNb,QZYS(NP[o,q

|S(NP[(S(Nbwp`PNPSZx,q?$qbx\,V|dojxqt_,,q47Q_\[(TY(QXaL{h\,SkQ_emEQZ[(vdeSUO!d,Y`P\Y(l,QS(WZ\,`bQe7opV\,mRNPW

  

ghY(d,l,Y\,mRmRNfV7l7q21 

n

fi

(,   x

uVu

D

x, G7q

vNb[kQ,xqjqbxr(r(rq@t_,p
q*+\Y[(NP\,`P`bod,{hSkQZY(,Q_e\Y(w,d/e7Q_WZNPS(NPdVgY(dpWXQ_S(SkQ_SZ}9S(TY(,QZo,q)hj 
k~?7Xk,    |  (  x u x p_
pq



& D/'(.

!v\,Vl7xp$q7Mqbx7M^QZQ,xjqqbx7!vh\,V7l7xpzq*t_,,q9mRQZ[(v7deOd,Y|SkgQZQ_eNPV7lyT7gG\,`fT7QNP[kQZY\[(NbdVNPV
gj\Y([(NP\,`P`bod,{hSkQZY\{j`bQ\Yw,d
3e7Q_WZNfS(NbdVgY(dpWXQ_S(SkQ_S_q9rtVsk  (  fi  t$p
  ~ 
 ;,_ZZkZ  
,+  XXL  j   X   _ ,NhX  
 X  ghg^qh,,Dp v.\' q
p

)

UU

fiJournal of Artificial Intelligence Research 14 (2001) 29-51

Submitted 6/00; published 2/01

Speeding Up the Convergence of Value Iteration
in Partially Observable Markov Decision Processes
Nevin L. Zhang
Weihong Zhang

Department of Computer Science
Hong Kong University of Science & Technology
Clear Water Bay Road, Kowloon, Hong Kong, CHINA

lzhang@cs.ust.hk
wzhang@cs.ust.hk

Abstract

Partially observable Markov decision processes (POMDPs) have recently become popular among many AI researchers because they serve as a natural model for planning under
uncertainty. Value iteration is a well-known algorithm for finding optimal policies for
POMDPs. It typically takes a large number of iterations to converge. This paper proposes
a method for accelerating the convergence of value iteration. The method has been evaluated on an array of benchmark problems and was found to be very effective: It enabled
value iteration to converge after only a few iterations on all the test problems.

1. Introduction
POMDPs model sequential decision making problems where effects of actions are nondeterministic and the state of the world is not known with certainty. They have attracted
many researchers in Operations Research and Artificial Intelligence because of their potential applications in a wide range of areas (Monahan 1982, Cassandra 1998b), one of which is
planning under uncertainty. Unfortunately, there is still a significant gap between this potential and actual applications, primarily due to the lack of effective solution methods. For
this reason, much recent effort has been devoted to finding ecient algorithms for POMDPs
(e.g., Parr and Russell 1995, Hauskrecht 1997b, Cassandra 1998a, Hansen 1998, Kaelbling
et al. 1998, Zhang et al. 1999).
Value iteration is a well-known algorithm for POMDPs (Smallwood and Sondik 1973,
Puterman 1990). It starts with an initial value function and iteratively performs dynamic
programming (DP) updates to generate a sequence of value functions. The sequence converges to the optimal value function. Value iteration terminates when a predetermined
convergence condition is met.
Value iteration performs typically a large number of DP updates before it converges and
DP updates are notoriously expensive. In this paper, we develop a technique for reducing
the number of DP updates.
DP update takes (the finite representation of) a value function as input and returns (the
finite representation of) another value function. The output value function is closer to the
optimal than the input value function. In this sense, we say that DP update improves its
input. We propose an approximation to DP update called point-based DP update. Pointbased DP update also improves its input, but possibly to a lesser degree than standard DP
update. On the other hand, it is computationally much cheaper. During value iteration, we
c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiZhang & Zhang
perform point-based DP update a number of times in between two standard DP updates.
The number of standard DP updates can be reduced this way since point-based DP update
improves its input. The reduction does not come with a high cost since point-based DP
update takes little time.
The rest of this paper is organized as follows. In the next section we shall give a brief
review of POMDPs and value iteration. The basic idea behind point-based DP update will
be explained in Section 3. After some theoretical preparations in Section 4, we shall work
out the details of point-based DP update in Section 5. Empirical results will be reported
in Section 6 and possible variations evaluated in Section 7. Finally, we shall discuss related
work in Section 8 and provide some concluding remarks in Section 9.

2. POMDPs and Value Iteration
2.1 POMDPs
A partially observable Markov decision process (POMDP) is a sequential decision model
for an agent who acts in a stochastic environment with only partial knowledge about the
state of its environment. The set of possible states of the environment is referred to as
the state space and is denoted by S . At each point in time, the environment is in one of
the possible states. The agent does not directly observe the state. Rather, it receives an
observation about it. We denote the set of all possible observations by Z . After receiving the
observation, the agent chooses an action from a set A of possible actions and executes that
action. Thereafter, the agent receives an immediate reward and the environment evolves
stochastically into a next state.
Mathematically, a POMDP is specified by: the three sets S , Z , and A; a reward function
r(s; a); a transition probability function P (s0 js; a); and an observation probability function
P (z js0 ; a). The reward function characterizes the dependency of the immediate reward on
the current state s and the current action a. The transition probability characterizes the
dependency of the next state s0 on the current state s and the current action a. The
observation probability characterizes the dependency of the observation z at the next time
point on the next state s0 and the current action a.

2.2 Policies and Value Functions
Since the current observation does not fully reveal the identity of the current state, the agent
needs to consider all previous observations and actions when choosing an action. Information about the current state contained in the current observation, previous observations,
and previous actions can be summarized by a probability distribution over the state space
(Astrom 1965). The probability distribution is sometimes called a belief state and denoted
by b. For any possible state s, b(s) is the probability that the current state is s. The set of
all possible belief states is called the belief space. We denote it by B.
A policy prescribes an action for each possible belief state. In other words, it is a
mapping from B to A. Associated with a policy  is its value function V  . For each belief
state b, V  (b) is the expected total discounted reward that the agent receives by following
30

fiSpeeding Up Value Iteration in POMDPS
the policy starting from b, that is

1
X
t r ];

V  (b) = E;b[

t=0

t

(1)

where rt is the reward received at time t and
 (0<1) is the discount factor. It is known
that there exists a policy  such that V  (b)V  (b) for any other policy  and any belief
state b (Puterman 1990). Such a policy is called an optimal policy. The value function of an
optimal policy is called the optimal value function. We denote it by V  . For any positive
number , a policy  is -optimal if

V  (b) +   V  (b) 8b 2 B:

2.3 Value Iteration

To explain value iteration, we need to consider how belief state evolves over time. Let b
be the current belief state. The belief state at the next point in time is determined by the
current belief state, the current action a, the next observation z . We denote it by baz . For
any state s0 , baz (s0 ) is given by

baz (s0 ) =

Ps P (z; s0js; a)b(s)

;
(2)
P (z jb; a)
P
where P (z; s0 js; a)=P (z js0 ; a)P (s0 js; a) and P (z jb; a)= s;s0 P (z; s0 js; a)b(s) is the renormal-

ization constant. As the notation suggests, the constant can also be interpreted as the
probability of observing z after taking action a in belief state b.
Define an operator T that takes a value function V and returns another value function
TV as follows:
X
TV (b) = max
[r(b; a) +  P (z jb; a)V (baz )] 8b 2 B
(3)
a
z

P
where r(b; a) = s r(s; a)b(s) is the expected immediate reward for taking action a in belief
state b. For a given value function V , a policy  is said to be V -improving if
X P (zjb; a)V (ba)] 8b 2 B:
(b) = arg max
[
r
(
b;
a
)
+

z
a
z

(4)

Value iteration is an algorithm for finding -optimal policies. It starts with an initial
value function V0 and iterates using the following formula:
Vn = TVn,1 :
It is known (e.g., Puterman 1990, Theorem 6.9) that Vn converges to V  as n goes to
infinity. Value iteration terminates when the Bellman residual maxb jVn (b) , Vn,1 (b)j falls
below (1 , )=2. When it does, a Vn -improving policy is -optimal (e.g., Puterman 1990).
Since there are infinitely many belief states, value functions cannot be explicitly represented. Fortunately, the value functions that one encounters in the process of value iteration
admit implicit finite representations. Before explaining why, we first introduce several technical concepts and notations.
31

fiZhang & Zhang

1
2
3
4

(1, 0)

(0, 1)

Figure 1: Illustration of Technical Concepts.

2.4 Technical and Notational Considerations

For convenience, we view functions over the state space vectors of size jSj. We use lower
case Greek letters ff and fi to refer to vectors and script letters V and U to refer to sets of
vectors. In contrast, the upper case letters V and U always refer to value functions, that is
functions over the belief space B. Note that a belief state is a function over the state space
and hence can be viewed as a vector.
A set V of vectors induces a value function as follows:

f (b) = max
ffb 8b 2 B;
ff2V

P

where ffb is the inner product of ff and b, that is ffb= s ff(s)b(s). For convenience, we
shall abuse notation and use V to denote both a set of vectors and the value function induced
by the set. Under this convention, the quantity f (b) can be written as V (b).
A vector in a set is extraneous if its removal does not affect the function that the set
induces. It is useful otherwise. A set of vectors is parsimonious if it contains no extraneous
vectors.
Given a set V and a vector ff in V , define the open witness region R(ff; V ) and closed
witness region R(ff; V ) of ff w.r.t V to be regions of the belief space B respectively given by

R(ff; V ) = fb 2 Bjffb > ff0 b; 8ff0 2 Vnfffgg
R(ff; V ) = fb 2 Bjffb  ff0 b; 8ff0 2 Vnfffgg
In the literature, a belief state in the open witness region R(ff; V ) is usually called a witness
point for ff since it testifies to the fact that ff is useful. In this paper, we shall call a belief
state in the closed witness region R(ff; V ) a witness point for ff.
Figure 1 diagrammatically illustrates the aforementioned concepts. The line at the
bottom depicts the belief space of a POMDP with two states. The point at the left end
represents the probability distribution that concentrates all its masses on one of the states,
while the point at the right end represents the one that concentrates all its masses on the
other state. There are four vectors ff1 , ff2 , ff3 , and ff4 . The four slanting lines represent
32

fiSpeeding Up Value Iteration in POMDPS

V ; ):

VI(

1.
2.
3.
4.
5.
6.
7.



(1 , )=2.

do f
U

DP-UPDATE(V ).
maxb jU (b) , V (b)j;
if (r > ) V U .
g while ( r > ).
return U .

r

Figure 2: Value Iteration for POMDPs.
the linear functions ffi b (i=1; 2; 3; 4) of b. The value function induced by the four vectors
is represented by the three bold line segments at the top. Vector ff3 is extraneous as its
removal does not affect the induced function. All the other vectors are useful. The first
segment of the line at the bottom is the witness region of ff1 , the second segment is that of
ff2 , and the last segment is that of ff4 .

2.5 Finite Representation of Value Functions and Value Iteration

A value function V is represented by a set of vectors if it equals the value function induced
by the set. When a value function is representable by a finite set of vectors, there is a
unique parsimonious set of vectors that represents the function (Littman et al. 1995a).
Sondik (1971) has shown that if a value function V is representable by a finite set
of vectors, then so is the value function TV . The process of obtaining the parsimonious
representation for TV from the parsimonious representation of V is usually referred to as
dynamic programming (DP) update. Let V be the parsimonious set of vectors that represents
V . For convenience, we use T V to denote the parsimonious set of vectors that represents
TV .
In practice, value iteration for POMDPs is not carried out directly in terms of value
functions themselves. Rather, it is carried out in terms of sets of vectors that represent the
value functions (Figure 2). One begins with an initial set of vectors V . At each iteration,
one performs a DP update on the previous parsimonious set V of vectors and obtains a new
parsimonious set of vectors U . One continues until the Bellman residual maxb jU (b) ,V (b)j,
which is determined by solving a sequence of linear programs, falls below a threshold.

3. Point-Based DP Update: The Idea
This section explains the intuitions behind point-based DP update. We begin with the
so-called backup operator.

3.1 The Backup Operator
Let V be a set of vectors and b be a belief state. The backup operator constructs a new

vector in three steps:

33

fiZhang & Zhang
1. For each action a and each observation z , find the vector in V that has maximum inner
product with bza | the belief state for the case when z is observed after executing action
a in belief state b. If there are more than one such vector, break ties lexicographically
(Littman 1996). Denote the vector found by fia;z .
2. For each action a, construct a vector fia by:
X
fia (s) = r(s; a) +  P (s0 ; zjs; a)fia;z (s0); 8s 2 S :
z;s0

3. Find the vector, among the fia 's, that has maximum inner product with b. If there
are more than one such vector, break ties lexicographically. Denote the vector found
by backup(b; V ).
It has been shown (Smallwood and Sondik 1973, Littman 1996) that backup(b; V ) is a
member of T V | the set of vectors obtained by performing DP update on V . Moreover, b
is a witness point for backup(b; V ).
The above fact is the corner stone of several DP update algorithms. The one-pass
algorithm (Sondik 1971), the linear-support algorithm (Cheng 1988), and the relaxed-region
algorithm (Cheng 1988) operate in the following way: They first systematically search for
witness points of vectors in T V and then obtain the vectors using the backup operator. The
witness algorithm (Kaelbling et al. 1998) employs a similar idea.

3.2 Point-Based DP Update

Systematically searching for witness points for all vectors in T V is computationally expensive. Point-based DP update does not do this. Instead, it uses heuristics to come up with
a collection of belief points and backs up on those points. It might miss witness points for
some of the vectors in T V and hence is an approximation of standard DP update.
Obviously, backing up on different belief states might result in the same vector. In other
words, backup(b; V ) and backup(b0 ; V ) might be equal for two different belief states b and
b0. As such, it is possible that one gets only a few vectors after many backups. One issue in
the design of point-based DP update is to avoid this. We address this issue using witness
points.
Point-based DP update assumes that one knows a witness point for each vector in its
input set. It backs up on those points.1 The rationale is that witness points for vectors in
a given set \scatter all over the belief space" and hence the chance of creating duplicate
vectors is low. Our experiments have confirmed this intuition.
The assumption made by point-based DP update is reasonable because its input is
either the output of a standard DP update or another point-based DP update. Standard
DP update computes, as by-products, a witness point for each of its output vectors. As will
be seen later, point-based DP update also shares this property by design.

3.3 The Use of Point-Based DP Update

As indicated in the introduction, we propose to perform point-based DP update a number
of times in between two standard DP updates. To be more specific, we propose to modify
1. As will be seen later, point-based DP update also backs up on other points.

34

fiSpeeding Up Value Iteration in POMDPS

VI1(

1.
2.
3.
4.
5.
6.
7.

V ; ):



(1 , )=2.

do f
U

DP-UPDATE(V ).
maxb jU (b) , V (b)j;
if (r > ) V POINT-BASED-VI(U ; ).
g while ( r > ).
return U .

r

U ; ):
1. do f
2.
V U.
3.
U POINT-BASED-DPU(V )
4. g while (STOP(U ; V ; )= false).
5. return V .
POINT-BASED-VI(

Figure 3: Modified Value Iteration for POMDPs.
value iteration in the way as shown in Figure 3. Note that the only change is at line
5. Instead of assigning U directly to V , we pass it to a subroutine POINT-BASED-VI and
assign the output of the subroutine to V . The subroutine functions in the same way as
value iteration, except that it performs point-based DP updates rather than standard DP
updates. Hence we call it point-based value iteration.
Figure 4 illustrates the basic idea behind modified value iteration in contrast to value
iteration. When the initial value function is properly selected,2 the sequence of value functions produced by value iteration converges monotonically to the optimal value function.
Convergence usually takes a long time partially because standard DP updates, indicated
by fat upward arrows, are computationally expensive. Modified value iteration interleaves
standard DP updates with point-based DP updates, which are indicated by the thin upward
arrows. Point-based DP update does not improve a value function as much as standard DP
update. However, its complexity is much lower. As a consequence, modified value iteration
can hopefully converge in less time.
The idea of interleaving standard DP updates with approximate updates that back up
at a finite number of belief points is due to Cheng (1988). Our work differs from Cheng's
method mainly in the way we select the belief points. A detailed discussion of the differences
will be given in Section 8.
The modified value iteration algorithm raises three issues. First, what stopping criterion
do we use for point-based value iteration? Second, how can we guarantee the stopping
criterion can eventually be satisfied? Third, how do we guarantee the convergence of the
modified value iteration algorithm itself? To address those issues, we introduce the concept
of uniformly improvable value functions.
2. We will show how in Section 5.5.

35

fiZhang & Zhang

.
.
.

.
.
.

standard update

point-based update

Value iteration

Modified value iteration

Figure 4: Illustration of the Basic Idea behind Modified Value Iteration.

4. Uniformly Improvable Value Functions

Suppose V and U are two value functions. We say that U dominates V and write V U if
V (b)U (b) for every belief state b. A value function V is said to be uniformly improvable if
V TV . A set U of vectors dominates another set V of vectors if the value function induced
by U dominates that induced by V . A set of vectors is unformly improvable if the value
function it induces is.

Lemma 1 The operator T is isotone in the sense that for any two value functions V and
U , V U implies TV TU . 2
This lemma is obvious and is well known in the MDP community (Puterman 1990).
Nonetheless, it enables us to explain the intuition behind the term \uniformly improvable".
Suppose V is a uniformly improvable value function and suppose value iteration starts
with V . Then the sequence of value functions generated is monotonically increasing and
converges to the optimal value function V  . This implies V TV V  . That is, TV (b) is
closer to V  (b) than V (b) for all belief states b.
The following lemma will be used later to address the issues listed at the end of the
previous section.

Lemma 2 Consider two value functions V and U . If V is uniformly improvable and
V U TV , then U is also uniformly improvable.
Proof: Since V U , we have TV TU by Lemma 1. We also have the condition U TV .
Consequently, U TU . That is, U is uniformly improvable. 2
Corollary 1 If value function V is uniformly improvable, so is TV . 2

5. Point-Based DP Update: The Algorithm

Point-based DP update is an approximation of standard DP update. When designing
point-based DP update, we try to strike a balance between quality of approximation and
36

fiSpeeding Up Value Iteration in POMDPS
computational complexity. We also need to guarantee that the modified value iteration
algorithm converges.

5.1 Backing Up on Witness Points of Input Vectors
Let V be a set of vectors on which we are going to perform point-based DP update. As
mentioned earlier, we can assume that we know a witness point for each vector in V . Denote

the witness point for a vector ff by w(ff). Point-based DP update first backs up on these
points and thereby obtains a new set of vectors. To be more specific, it begins with the
following subroutine:

V ):
1. U ;.
2. for each fi 2 V
3.
ff backup(w(fi ); V ).
4.
if ff 2= U
5.
w(ff)
w(fi ).
6.
U U [ fffg.
7. return U .
backUpWitnessPoints(

In this subroutine, line 4 makes sure that the resulting set U contains no duplicates and
line 5 takes note of the fact that w(fi ) is also a witness point for ff (w.r.t T V ).

5.2 Retaining Uniform Improvability

To address convergence issues, we assume that the input to point-based DP update is
uniformly improvable and require its output to be also uniformly improvable. We will
explain later how the assumption can be facilitated and how the requirement guarantees
convergence of the modified value iteration algorithm. In this subsection, we discuss how
the requirement can be fulfilled.
Point-based DP update constructs new vectors by backing up on belief points and the
new vectors are all members of T V . Hence the output of point-based DP update is trivially
dominated by T V . If the output also dominates V , then it must be uniformly improvable
by Lemma 2. The question is how to guarantee that the output dominates V .
Consider the set U resulted from backUpWitnessPoints. If it does not dominate V , then
there must exist a belief state b such U (b)<V (b). Consequently, there must exist a vector
fi in V such that U (b)<fi b. This gives us the following subroutine for testing whether
U dominates V and for, when this is not the case, adding vectors to U so that it does.
The subroutine is called backUpLPPoints because belief points are found by solving linear
programs.

U ; V ):
1. for each fi 2 V
2.
do f
backUpLPPoints(

3.
4.
5.

b

if b 6

dominanceCheck(
= NULL,
backup(
).

ff

fi; U ).

b; V

37

fiZhang & Zhang
6.
7.
8.

w(

ff)

b.

U U [ fffg.
g while (b 6= NULL).

The subroutine examines vectors in V one by one. For each fi in V , it calls another subroutine
try to find a belief point b such that U (b)<fi b. If such a point is found,
it backs up on it, resulting in a new vector ff (line 5). By the property of the backup
operator, b is a witness point of ff w.r.t T V (line 6). There cannot be any vector in U that
equals ff.3 Consequently, the vector is simply added to U without checking for duplicates
(line 7). The process repeats for fi until dominanceCheck returns NULL, that is when there
are no belief points b such that U (b)<fi b. When backUpLPPoints terminates, we have
U (b)fi b for any vector fi in V and any belief point b. Hence U dominates V .
The subroutine dominanceCheck(fi; U ) first checks whether there exists a vector ff in U
that pointwise dominates fi , that is ff(s)fi (s) for all states s. If such an ff exists, it returns
NULL right away. Otherwise, it solves the following linear program LP(fi; U ). It returns the
solution point b when the optimal value of the objective function is positive and returns
NULL otherwise:4
dominanceCheck to

fi; U ):
1. Variables: x, b(s) for each state s
2. Maximize: x.
3. Constraints:
Ps fi(s)b(s)  x+ Ps ff(s)b(b) for all ff2U
4.
Ps b(s) = 1, b(s)  0 for all states s.
5.
LP(

5.3 The Algorithm

Here is the complete description of point-based DP update. It first backs up on the witness
points of the input vectors. Then, it solves linear programs to identify more belief points and
backs up on them so that its output dominates its input and hence is uniformly improvable.
POINT-BASED-DPU(V ):
1. U backUpWitnessPoints(V )
2. backUpLPPoints(U ; V )
3. return U .
In terms of computational complexity, point-based DP update performs exactly jVj
backups in the first step and no more than jT Vj backups in the second step. It solves linear
programs only in the second step. The number of linear programs solved is upper bounded
by jT Vj+jVj and is usually much smaller than the bound. The numbers of constraints in
the linear programs are upper bounded by jT Vj + 1.
3. Since b is a witness of ff w.r.t T V , we have ffb=T V (b). Since V is uniformly improvable, we also
have T V (b)V (b). Together with the obvious fact that V (b)fi b and the condition fi b>U (b), we have
ffb>U (b). Consequently, there cannot be any vector in U that equals ff.
4. In our actual implementation, the solution point b is used for backup even when the optimal value of
the objective function is negative. In this case, duplication check is needed.

38

fiSpeeding Up Value Iteration in POMDPS
There are several algorithms for standard DP update. Among them, the incremental
pruning algorithm (Zhang and Liu 1997) has been shown to be the most ecient both
theoretically and empirically (Cassandra et al. 1997). Empirical results (Section 6) reveal
that point-based DP update is much less expensive than incremental pruning on a number
of test problems. It should be noted, however, that we have not proved this is always the
case.

5.4 Stopping Point-Based Value Iteration

Consider the do-while loop of POINT-BASED-VI (Figure 2). Starting from an initial set of
vectors, it generates a sequence of sets. If the initial set is uniformly improvable, then the
value functions represented by the sets are monotonically increasing and are upper bounded
by the optimal value function. As such, they converge to a value function (which is not
necessarily the optimal value function). The question is when to stop the do-while loop.
A straightforward method would be to compute the distance maxb jU (b) ,V (b)j between
two consecutive sets U and V and stop when the distance falls below a threshold. To compute
the distance, one needs to solve jUj+jVj linear programs, which is time consuming. We use
a metric that is less expensive to compute. To be more specific, we stop the do-while loop
when
max
jU (w(ff)) , V (w(ff))j  1 :
ff2U
In words, we calculate the maximum difference between U and V at the witness points of
vectors in U and stop the do-while loop when this quantity is no larger than 1 . Here 
is the threshold on the Bellman residual for terminating value iteration and 1 is a number
between 0 and 1. In our experiments, we set it at 0:1.

5.5 Convergence of Modified Value Iteration
Let Vn and Vn0 be sets of vectors respectively generated by VI (Figure 1) and VI1 (Figure

2) at line 3 in iteration n. Suppose the initial set is uniformly improvable. Using Lemma 2
and Corollary 1, one can prove by induction that Vn and Vn0 are uniformly improvable for
all n and their induced value functions increase with n. Moreover, Vn0 dominates Vn and is
dominated by the optimal value function. It is well known that Vn converges to the optimal
value function. Therefore, Vn0 must also converge to the optimal value function.
The question now is how to make sure that the initial set is uniformly improvable. The
following lemma answers this question.

Lemma 3 Let m= mins;a r(s; a), c = m=(1 , ), and ffc be the vector whose components
are all c. Then the singleton set fffc g is uniformly improvable.
Proof: Use V to denote the value function induced by the singleton set. For any belief

state b, we have

TV (b) = max
[r(b; a) + 
a
39

X P (zjb; a)V (ba)]
z

z

fiZhang & Zhang
= max
[r(b; a) + 
a

X P (zjb; a)c]
z

= max
[r(b; a) + m=(1 , )]
a
 m + m=(1 , )
= m=(1 , ) = V (b):
Therefore the value function, and hence the singleton set, is uniformly improvable. 2
Experiments (Section 6) have shown that VI1 is more ecient VI on a number of test
problems. It should be noted, however, that we have not proved this is always the case.
Moreover, complexity results by Papadimitriou and Tsitsiklis (1987) implies that the task
of finding -optimal policies for POMDPs is PSPACE-complete. Hence, the worst-case
complexity should remain the same.

5.6 Computing the Bellman Residual

In the modified value iteration algorithm, the input V to standard DP update is always
uniformly improvable. As such, its output U dominates its input. This fact can be used to
simplify the computation of the Bellman residual. As a matter of fact, the Bellman residual
maxb jU (b),V (b)j reduces maxb (U (b),V (b)).
To compute the latter quantity, one goes through the vectors in U one by one. For each
vector, one solves the linear program LP(ff; V ). The quantity is simply the maximum of
the optimal values of the objective functions of the linear programs. Without uniformly
improvability, we would have to repeat the process one more time with the roles of V and
U exchanged.

6. Empirical Results and Discussions

Experiments have been conducted to empirically determine the effectiveness of point-based
DP update in speeding up value iteration. Eight problems are used in the experiments.
In the literature, the problems are commonly referred to as 4x3CO, Cheese, 4x4, Part
Painting, Tiger, Shuttle, Network, and Aircraft ID. We obtained the problem files from
Tony Cassandra. Information about their sizes is summarized in the following table.
Problem jSj jZj jAj
4x3CO 11
4 11
4x4 16
2
4
Tiger 2
2
3
Network 7
2
4

Problem jSj jZj jAj
Cheese 11
4
7
Painting 4
4
2
Shuttle 8
2
3
Aircraft ID 12
5
6

The effectiveness of point-based DP update is determined by comparing the standard
value iteration algorithm VI and the modified value iteration algorithm VI1. The implementation of standard value iteration used in our experiments is borrowed from Hansen.
Modified value iteration is implemented on top of Hansen's code.5 The discount factor is
set at 0:95 and round-off precision is set at 10,6 . All experiments are conducted on an
UltraSparc II machine.
5. The implementation is available on request.

40

fiSpeeding Up Value Iteration in POMDPS
Table 1 shows the amounts of time VI and VI1 took to compute 0.01-optimal policies
for the test problems. We see that VI1 is consistently more ecient than VI, especially on
the larger problems. It is about 1.3, 2.8, 5, 62, 141, 173, and 49 times faster than VI on the
first seven problems respectively. For the Aircraft ID problem, VI1 was able to compute a
0.01-optimal policy in less than 8 hours, while VI was only able to produce a 33-optimal
policy after 40 hours.
4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
VI
3.2
13.9 27.15 37.84 79.14 5,199 12,478
VI1
2.4
5.0 5.30
.61
.56
30
253 27,676
Table 1: Time for Computing 0.01-Optimal Policies in Seconds.
Various other statistics are given in Table 2 to highlight computational properties of
VI1 and to explain its superior performance. The numbers of standard DP updates carried
out by VI and VI1 are shown at rows 1 and 3. We see that VI1 performed no more than 5
standard updates on the test problems, while VI performed more than 125. This indicates
that point-based update is very effective in cutting down the number of standard updates
required to reach convergence. As a consequence, VI1 spent much less time than VI in
standard updates (row 2 and 4).6
Problem
4x3CO Cheese 4x4 Paint Tiger Shuttle Network
DPU #
125
129 130 127 163
174
214
VI
Time
2.00
7.63 17.83 33.39 70.44 3,198
8,738
DPU #
4
4
3
3
3
5
5
Time
.05
.09
.15
.21
.09
13
82
VI1 PBDPU #
377
219 173 244 515
455
670
Time
2.32
4.86 5.09
.37
.45
10
139
Quality Ratio
.33
.58
.74
.51
.31
0.31
.32
Complexity Ratio
.38
.37
.21 .0057 .002 .0012
.005
Table 2: Detailed Statistics.
Row 5 shows the numbers of point-based updates carried out by VI1. We see that those
numbers are actually larger than the numbers of standard updates performed by VI. This
is expected. To see why, recall that point-based update is an approximation of standard
update. Let V be a set of vectors that is uniformly improvable. Use T 0 V to denote the
sets of vectors resulted from performing point-based update on V . For any belief state b,
we have V (b)T 0 V (b)T V (b). This means that point-based update improves V but not as
much as standard update. Consequently, the use of point-based update increases the total
6. Note that times shown there do not include time for testing the stopping condition.

41

fiZhang & Zhang
number of iterations, i.e the number of standard updates plus the number of point-based
updates.
Intuitively, the better point-based update is as an approximation of standard update,
the less the difference between the total number iterations that VI1 and VI need take. So,
the ratio between those two numbers in a problem can be used, to certain extent, as a
measurement of the quality of point-based update in that problem. We shall refer to it as
the quality ratio of point-based update. Row 7 shows the quality ratios in the seven test
problems. We see that the quality of point-based update is fairly good and stable across all
the problems.
Row 8 shows, for each test problem, the ratio between the average time of a standard
update performed by VI and that of a point-based update performed by VI1. Those ratios
measure, to certain extent, the complexity of point-based update relative to standard update
and hence will be referred to as the complexity ratios of point-based update. We see that,
as predicted by the analysis in Section 5.3, point-based update is consistently less expensive
than standard update. The differences are more than 200 times in the last four problems.
In summary, the statistics suggest that the quality of point-based update relative to
standard update is fairly good and stable and its complexity is much lower. Together with
the fact that point-based update can drastically reduces the number of standard updates,
those explain the superior performance of VI1.
To close this section, let us note that while VI finds policies with quality7 very close to
the predetermined criterion, VI1 usually finds much better ones (Table 3). This is because
VI checks policy quality after each (standard) update, while VI1 does not do this after
point-based updates.

Problem 4x3CO Cheese 4x4 Paint Tiger Shuttle Network
VI
.0095 .0099 .0099 .01
.0098 .0097 .0098
VI1
.0008 .0008 .0009 .0007 .0007 .00015 .001
Table 3: Quality of Policies Found by VI and VI1.

7. Variations of Point-Based DP Update
We have studied several possible variations of point-based update. Most of them are based
on ideas drawn from the existing literature. None of the variations were able to significantly
enhance the effectiveness of the algorithm in accelerating value iteration. Nonetheless a brief
discussion of some of them is still worthwhile. The discussion provides further insights about
the algorithm and shows how it compares to some of the related work to be discussed in
detail in the next section.
The variations can be divide into two categories: those aimed at improving the quality
of point-based update and those aimed at reducing complexity. We shall discuss them one
by one.
7. Quality of a policy is estimated using the Bellman residual.

42

fiSpeeding Up Value Iteration in POMDPS

7.1 Improving the Quality of Point-Based DP Update
A natural way to improve the quality of point-based update is to back up on additional
belief points. We have explored the use of randomly generated points (Cassandra 1998a),
additional by-product points, and projected points (Hauskrecht 2000). Here additional byproduct points refer to points generated at various stages of standard update, excluding the
witness points that are already being used. Projected points are points that are reachable
in one step from points that have given rise to useful vectors.
Table 4 shows, for each test problem, the number of standard updates and the amount
of time that VI1 took with and without using projected points. We see that the use of
projected points did reduce the number of standard updates by one in 4x3CO, Cheese, and
Shuttle. However, it increased the time complexity in all test problems except for Network.
The other two kinds of points and combinations of the three did not significantly improve
VI1 either. On the contrary, they often significantly degraded the performance of VI1.
w/o
with
w/o
with

4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
4
4
3
3
3
5
5
7
3
3
3
3
3
4
5
7
2.4
5.0 5.3
.61
.56
30
253 27,676
3.2
5.6 7.4
.69
2.3
33
140 35,791

Table 4: Number of Standard DP Updates and Time That VI1 Took With and Without
Using Projected Points.
A close examination of experimental data reveals a plausible explanation. Point-based
update, as it stands, can already reduce the number of standard updates down to a just few
and among them the last two or three are the most time-consuming. As such, the possibility
of further reducing the number standard updates is low and even when it is reduced, the
effect is roughly to shift the most time-consuming standard updates earlier. Consequently,
it is unlikely to achieve substantial gains. On the other hand, the use of additional points
always increases overheads.

7.2 Reducing the Complexity of Point-Based DP Update
Solving linear programs is the most expensive operation in point-based update. An obvious
way to speed up is to avoid linear programs. Point-based update solves linear programs
and backs up on the belief points found so as to guarantee uniform improvability. If the
linear programs are to be skipped, there must be some other way to guarantee uniform
improvability. There is an easy solution to this problem. Suppose V is the set of vectors
that we try to update and it is uniformly improvable. Let U be the set obtained from V by
backing up only on the witness points, which can be done without solving linear programs.
The set U might or might not be uniformly improvable. However, the union V [ U is
guaranteed to be uniformly improvable. Therefore we can reprogram point-based update
43

fiZhang & Zhang
to return the union in hope to reduce complexity. The resulting variation will be called
non-LP point-based DP update.
Another way to reduce complexity is to simplify the backup operator (Section 3.1) using
the idea behind modified policy iteration (e.g., Puterman 1990). When backing up from
a set of vectors V at a belief point, the operator considers all possible actions and picks
the one that is optimal according to the V -improving policy. To speed up, one can simply
use the action found for the belief point by the previous standard update. The resulting
operator will be called the MPI backup operator, where MPI stands for modified policy
iteration. If V is the output of the previous standard update, the two actions often are the
same. However, they are usually different if V is the result of several point-based updates
following the standard update.
Table 5 shows, for each test problem, the number of standard updates and the amount of
time that VI1 took when non-LP point-based update was used (together with the standard
backup operator). Comparing the statistics with those for point-based update (Tables 1
and 2), we see that the number of standard updates is increased on all test problems and the
amount of time is also increased except for the first three problems. Here are the plausible
reasons. First, it is clear that non-LP point-based update does not improve a set of vectors
as much as point-based update. Consequently, it is less effective in reducing the number of
standard updates. Second, although it does not solve linear programs, non-LP point-based
update produces extraneous vectors. This means that it might need to deal with a large
number of vectors at later iterations and hence might not be as ecient as point-based
update after all.
4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
4
5
8
4
4
7
10
8
2.38
2.38 3.4
.75
.88
44
599 32,281
Table 5: Number of Standard DP Updates and Time That VI1 Took When Non-LP PointBased Update is Used.
Extraneous vectors can be pruned. As a matter of fact, we did prune vectors that are
pointwise-dominated by others (hence extraneous) in our experiments. This is inexpensive.
Pruning of other extraneous vectors, however, requires the solution of linear programs and
is expensive. In Zhang et al. (1999), we have discussed how this can be done the most
ecient way. Still the results were not as good as those in Table 5. In that paper, we
have also explored the combination of non-LP point-based update with the MPI backup
operator. Once again, the results were not as good as those in Table 5. The reason is that
the MPI backup operator further compromises the quality of point-based update.
The quality of non-LP point-based update can be improved by using the Gauss-Seidel
asynchronous update (Denardo 1982). Suppose we are updating a set V . The idea is to,
after a vector is created by backup, add a copy of the vector to the set V right away. The
hope is to increase the components of later vectors. We have tested this idea when preparing
Zhang et al. (1999) and found that the costs almost always exceed the benefits. A reason
44

fiSpeeding Up Value Iteration in POMDPS
is that asynchronous update introduces many more extraneous vectors than synchronous
update.
In conclusion, point-based is conceptually simple and clean. When compared to its more
complex variations, it seems to be the most effective in accelerating value iteration.

8. Related Work
Work presented in this paper has three levels: point-based DP update at the bottom, pointbased value iteration in the middle, and modified value iteration at the top. In this section,
we discuss previous relevant work at each of the three levels.

8.1 Point-Based DP Update and Standard DP Update

As mentioned in Section 3.1, point-based update is closely related to several exact algorithms for standard update, namely one-pass (Sondik 1971), linear-support (Cheng 1988),
and relaxed-region (Cheng 1988). They all backup on a finite number of belief points.
The difference is that these exact algorithms generate the points systematically, which is
expensive, while point-based update generate the points heuristically.
There are several other exact algorithms for standard DP update. The enumeration/reduction algorithms (Monahan 1982, Eagle 1984) and incremental pruning (Zhang
and Liu 1997, Cassandra et al. 1997) first generate a set of vectors that are not parsimonious and then prune extraneous vectors by solving linear programs. Point-based DP
update never generates extraneous vectors. It might generate duplicate vectors. However,
duplicates are pruned without solving linear programs. The witness algorithm (Kaelbling
et al. 1998) has two stages. In the first stage, it considers actions one by one. For each
action, it constructs a set of vectors based on a finite number of systematically generated
belief points using an operator similar to the backup operator. In the second stage, vectors
for different actions are pooled together and extraneous vectors are pruned.
There are proposals to carry out standard update approximately by dropping vectors
that are marginally useful (e.g., Kaelbling et al. 1998, Hansen 1998). Here is one idea
along this line that we have empirically evaluated. Recall that to achieve -optimality, the
stopping threshold for the Bellman residual should be  = (1 , )=2. Our idea is to drop
marginally useful vectors at various stages of standard update while keeping the overall
error under =2 and to stop when the Bellman residual falls below =2. It is easy to see
that -optimality is still guaranteed this way. We have also tried to start with a large error
tolerance in hope to prune more vectors and gradually decrease the tolerance level to =2.
Reasonable improvements have been observed especially when one does not need quality
of policy to be high. However such approximate updates are much more expensive than
point-based updates. In the context of the modified value iteration algorithm, they are
more suitable alternatives to standard updates than point-based update.

8.2 Point-Based Value Iteration and Value Function Approximation

Point-based value iteration starts with a set of vectors and generates a sequence of vector
sets by repeatedly applying point-based update. The last set can be used to approximate
the optimal value function.
45

fiZhang & Zhang
Various methods for approximating the optimal value function have been developed
previously.8 We will compare them against point-based value iteration along two dimensions: (1) Whether they map one set of vectors to another, that is whether the can be
interleaved with standard updates, and (2) if they do, whether they can guarantee convergence when interleaved with standard updates.
Lovejoy (1993) proposes to approximate the optimal value function V  of a POMDP
using the optimal value function of the underlying Markov decision process (MDP). The
latter is a function over the state space. So V  is being approximated by one vector.
Littman et al. (1995b) extend this idea and approximate V  using jAj vectors, each of
which corresponds to a Q-function of the underlying MDP. A further extension is recently
introduced by Zubek and Dietterich (2000). Their idea is to base the approximation not on
the underlying MDP, rather on a so-called even-odd POMDP that is identical to the original
POMDP except that the state is fully observable at even time steps. Platzman (1980)
suggests approximating V  using the value functions of one or more fixed suboptimal policies
that are constructed heuristically. Those methods do not start with a set of vectors and
hence do not map a set of vectors to another. However, they can easily be adapted to do so.
However, they all put a predetermined limit on the number of output vectors. Consequently,
convergence is not guaranteed when they are interleaved with standard updates.
Fast informed bound (Hauskrecht 1997a), Q-function curve fitting (Littman et al. 1995b),
and softmax curve fitting (Parr and Russell 1995) do map a set of vectors to another. However, they differ drastically from point-based value iteration and from each other in their
ways of deriving the next set of vectors from the current one. Regardless of the size of the
current set, fast informed bound and Q-function curve fitting always produces jAj vectors,
one for each action. In softmax curve fitting, the number of vectors is also determined a
priori, although it is not necessarily related to the number of actions. Those methods can be
interleaved with standard DP updates. Unlike point-based value iteration, they themselves
may not converge (Hauskrecht 2000). Even in cases where they do converge themselves,
the algorithms resulting from interleaving them with standard updates do not necessarily
converge due to the a priori limits on the number of vectors.
Grid-based interpolation/extrapolation methods (Lovejoy 1991, Brafman 1997, Hauskrecht
1997b) approximate value functions by discretizing the belief space using a fixed or variable
grid and by maintaining values only for the grid points. Values at non-grid points are estimated by interpolation/extrapolation when needed. Such methods cannot be interleaved
with standard DP updates because they do not work with sets of vectors.
There are grid-based methods that work with sets of vectors. Lovejoy's method to lower
bound the optimal value function (Lovejoy 1991), for instance, falls into this category. This
method is actually identical to point-based value iteration except for the way it derives the
next set of vectors from the current one. Instead of using point-based update, it backs up on
grid points in a regular grid. Convergence of this method is not guaranteed. The algorithm
resulting from interleaving it with standard updates may not converge either.
8. Hauskrecht (2000) has conducted an extensive survey on previous value function approximation methods
and has empirically compared them in terms of, among other criteria, complexity and quality. It would
be interesting to also include point-based value iteration in the empirical comparison. This is not done
in the present paper because our focus is on using point-based value iteration to speed value iteration,
rather than using as a value function approximation method.

46

fiSpeeding Up Value Iteration in POMDPS
The incremental linear-function method (Hauskrecht 2000) roughly corresponds to a
variation of point-based value iteration that uses non-LP point-based update (Section 7.2)
augmented with the Gauss-Seidel asynchronous update. The method does not have access to
witness point. So it starts, for the purpose of backup, with extreme points of the belief space
and supplement them with projected points. This choice of points appears poor because it
leads to a large number of vectors and consequently the backup process is \usually stopped
well before" convergence (Hauskrecht 2000).

8.3 Previous Work Related to Modified Value Iteration
The basic idea of our modified value iteration algorithm VI1 is to add, in between two
consecutive standard updates, operations that are inexpensive. The hope is that those
operations can significantly improve the quality of a vector set and hence reduce the number
of standard updates.
Several previous algorithms work in the same fashion. The differences lie in the operations that are inserted between standard updates. The reward revision algorithm (White
et al. 1989) constructs, at each iteration, a second POMDP based on the current set of
vectors. It runs value iteration on the second POMDP for a predetermined number of steps.
The output is used to modify the current set of vectors and the resulting set of vectors is
fed to the next standard update.
Why is reward revision expected to speed up value iteration? Let V be the value function
represented by the current set of vectors. The second POMDP is constructed in such way
that it shares the same optimal value function as the original POMDP if V is optimal.
As such, one would expect the two POMDPs to have similar optimal value functions if V
is close to optimal. Consequently, running value iteration on the second POMDP should
improve the current value function. And it is inexpensive to do so because the second
POMDP is fully observable.
Reward revision is conceptually much more complex than VI1 and seems to be less
ecient. According to White et al. (1989), reward revision can, on average, reduce the
number of standard updates by 80% and computational time by 85%. From Tables 1 and
2, we see that the differences between VI1 and VI are much larger.
The iterative discretization procedure (IDP) proposed by Cheng (1988) is very similar to
VI1. There are two main differences. While VI1 uses point-based update, IDP uses non-LP
point-based update. While point-based update in VI1 backs up on witness points and belief
points found by linear programs, non-LP point-based update in IDP backs up on extreme
points of witness regions found as by-products by Cheng's linear-support or relaxed region
algorithms.
Cheng has conducted extensive experiments to determine the effectiveness of IDP in
accelerating value iteration. It was found that IDP can cut the number of standard updates
by as much as 55% and the amount of time by as much as 80%. Those are much less
significant than the reductions presented in Tables 1 and 2.
Hansen's policy iteration (PI) algorithm maintains a policy in the form of a finite-state
controller. Each node in the controller represents a vector. At each iteration, a standard
update is performed on the set of vectors represented in the current policy. The resulting
47

fiZhang & Zhang
set of vectors is used to improve the current policy9 and the improved policy is evaluated
by solving a system of linear equations. This gives rise to a third set of vectors, which is
fed to the next standard update.
We compared the performance of Hansen's PI algorithm to VI1. Table 6 shows, for each
test problem, the number of standard updates and the amount of time the algorithm took.
Comparing with the statistics for VI1 (Table 4), we see that PI performed more standard
updates than VI1. This indicates that policy improvement/evaluation is less effective than
point-based value iteration in cutting down the number of standard updates. In terms of
time, PI is more ecient than VI1 on the first three problems but significantly less ecient
on all other problems.
4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
3
7
7
10
14
9
18
9
.14
.87 3.4
3.8
4.5
60
1,109 66,964
Table 6: Number of Standard Updates and Time That PI Took to Compute 0.01-Optimal
Policies.
It might be possible to combine VI1 and PI. To be more specific, one can probably
insert a policy improvement/evaluation step between two point-based updates in pointbased value iteration (Figure 2). This should accelerate point-based value iteration and
hence VI1. This possibility and its benefits are yet to be investigated.

9. Conclusions and Future Directions
Value iteration is a popular algorithm for finding -optimal policies for POMDPs. It typically performs a large number of DP updates before convergence and DP updates are
notoriously expensive. In this paper, we have developed a technique called point-based DP
update for reducing the number of standard DP updates. The technique is conceptually
simple and clean. It can easily be incorporated into most existing POMDP value iteration algorithms. Empirical studies have shown that point-based DP update can drastically
cut down the number of standard DP updates and hence significantly speeding up value
iteration. Moreover, point-based DP update compares favorably with its more complex
variations that we can think also. It also compares favorably with policy iteration.
The algorithm presented this paper still requires standard DP updates. This limits its
capability of solving large POMDPs. One future direction is to investigate the properties
of point-based value iteration as an approximation algorithm by itself. Another direction is
to design ecient algorithms for standard DP updates in special models. We are currently
exploring the latter direction.
9. In Hansen's writings, policy improvement includes DP update as a substep. Here DP update is not
considered part of policy improvement.

48

fiSpeeding Up Value Iteration in POMDPS

Acknowledgments
Research is supported by Hong Kong Research Grants Council Grant HKUST6125/98E.
The authors thank Tony Cassandra and Eric Hansen for sharing with us their programs.
We are also grateful for the three anonymous reviewers who provided insightful comments
and suggestions on an earlier version of this paper.

References

Astrom, K. J. (1965). Optimal control of Markov decision processes with the incomplete
state estimation. Journal of Computer and System Sciences, 10, 174-205.
Brafman, R. I. (1997). A heuristic variable grid solution for POMDPs. In Proceedings of
the Fourteenth National Conference on Artificial Intelligence(AAAI-97), 727-733.
Cassandra, A. R., Littman, M. L., and Zhang, N. L. (1997). Incremental pruning: A
simple, fast, exact method for partially observable Markov decision processes. In
Proceedings of Thirteenth Conference on Uncertainty in Artificial Intelligence, 54-61.
Cassandra, A. R. (1998a). Exact and approximate algorithms for partially observable
Markov decision processes, PhD thesis, Department of Computer Science, Brown
University.
Cassandra, A. R. (1998b). A survey of POMDP applications, in Working Notes of AAAI
1998 Fall Symposium on Planning with Partially Observable Markov Decision Processes, 17-24.
Denardo, E. V. (1982). Dynamic Programming: Models and Applications Prentice-Hall.
Eagle, J. N.(1984). The optimal search for a moving target when the search path is
constrained. Operations Research, 32(5), 1107-1115.
Cheng, H. T.(1988). Algorithms for partially observable Markov decision processes. Ph D
thesis, University of British Columbia.
Hansen, E. A. (1998). Solving POMDPs by searching in policy space. In Proceedings of
Fourteenth Conference on Uncertainty in Artificial Intelligence, 211-219.
Hauskrecht, M.(1997a). Incremental methods for computing bounds in partially observable
Markov decision processes. in Proceedings of the Fourteenth National Conference on
Artificial Intelligence (AAAI-97), 734-749.
Hauskrecht, M.(1997b). Planning and control in stochastic domains with imperfect information. PhD thesis, Department of Electrical Engineering and Computer Science,
Massachusetts Institute of Technology.
Hauskrecht, M. (2000). Value function approximations for partially observable Markov
decision processes, Journal of Artificial Intelligence Research, 13, 33-95.
49

fiZhang & Zhang
Littman, M. L., Cassandra, A. R. and Kaelbling, L. P. (1995a). Ecient dynamicprogramming updates in partially observable Markov decision processes. Technical
Report CS-95-19, Brown University.
Littman, M. L., Cassandra, A. R. and Kaelbling, L. P. (1995b). Learning policies for partially observable environments, scaling up. In Proceedings of the Fifteenth Conference
on Machine Learning, 362-370.
Littman, M. L. (1996). Algorithms for sequential decision making. Ph D thesis, Department of Computer Science, Brown University.
Kaelbling, L. P., Littman. M. L. and Cassandra, A. R.(1998). Planning and acting in
partially observable stochastic domains, Artificial Intelligence, Vol 101.
Lovejoy, W. S. (1991). Computationally feasible bounds for partially observed Markov
decision processes. Operations Research, 39, 192-175.
Lovejoy, W. S. (1993). Suboptimal policies with bounds for parameter adaptive decision
processes. Operations Research, 41, 583-599.
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: theory, models, and algorithms. Management Science, 28 (1), 1-16.
Parr, R., and Russell, S. (1995). Approximating optimal policies for partially observable
stochastic domains. In Proceedings of the Fourteenth International Joint Conference
on Artificial Intelligence 1088-1094.
Papadimitriou, C. H., Tsitsiklis, J. N.(1987). The complexity of Markov decision processes.
Mathematics of Operations Research, 12(3), 441-450.
Platzman, L. K.(1980). Optimal infinite-horizon undiscounted control of finite probabilistic systems. SIAM Journal of Control and Optimization, 18, 362-380.
Puterman, M. L. (1990), Markov decision processes, in D. P. Heyman and M. J. Sobel
(eds.), Handbooks in OR & MS., Vol. 2, 331-434, Elsevier Science Publishers.
Smallwood, R. D. and Sondik, E. J. (1973). The optimal control of partially observable
processes over a finite horizon. Operations Research, 21, 1071-1088.
Sondik, E. J. (1971). The optimal control of partially observable Markov processes. PhD
thesis, Stanford University.
Sondik, E. J. (1978). The optimal control of partially observable Markov processes over
the infinite horizon, Operations Research, 21, 1071-1088.
White, C. C. III and Scherer, W. T. (1989). Solution procedures for partially observed
Markov decision processes, Operations Research, 37(5), 791-797.
Zhang, N. L., Lee, S. S., and Zhang, W.(1999). A method for speeding up value iteration
in partially observable Markov decision processes, in Proc. of the 15th Conference on
Uncertainties in Artificial Intelligence.
50

fiSpeeding Up Value Iteration in POMDPS
Zhang, N. L. and W. Liu (1997). A model approximation scheme for planning in stochastic
domains, Journal of Artificial Intelligence Research, 7, 199-230.
Zubek, V. B. and Dietterich, T. G.(2000). A POMDP approximation algorithm that anticipates the need to observe. To appear in Proceedings of the Pacific Rim Conference
on Artificial Intelligence (PRICAI-2000), Lecture Notes in Computer Science, New
York: Springer-Verlag.

51

fi